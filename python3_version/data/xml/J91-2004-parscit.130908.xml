<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.42718575">
Review Article
Does Conversation Analysis Have a Role in
Computational Linguistics?
Computers and Conversation
</title>
<author confidence="0.955317">
Paul Luff, Nigel Gilbert, and David Frohlich (editors)
</author>
<affiliation confidence="0.799539">
(University of Surrey)
London: Academic Press, 1990, 284 pp.
</affiliation>
<figure confidence="0.558221285714286">
(Computers and People Series)
Hardbound, ISBN 0-12-459560-X,
$45.00, £23.95
Reviewed by
Graeme Hirst
University of Toronto
1. Conversation Analysis
</figure>
<bodyText confidence="0.999818904761905">
Computational linguists are a vicious warrior tribe. Unconstrained by traditional dis-
ciplinary boundaries, they invade and plunder neighboring disciplines for attractive
theories to incorporate into their own. The latest victim of these attacks is sociology—
in particular, a branch of ethnomethodological sociolinguistics called Conversation
Analysis (hereafter, CA).
The book reviewed here, Computers and Conversation, is the first significant result
of this. It is based on a symposium held at the University of Surrey in September
1989. Its purpose is to show what CA has to offer to the study of human–computer
interaction (hereafter, HCI), especially interaction in natural language. In this review
article, I want to use the book to explore at some length the question of whether CA
has indeed anything to say to computational linguistics.
Up to now, almost all research in computational linguistics on dialogs between
humans and machines has taken the approach of Discourse Analysis. Discourse Anal-
ysis concerns itself with talk at the level of linguistic constituents (Levinson 1983,
pp. 286ff; Stubbs 1983, pp. 1, 9-10). The disfluencies of spoken language are ignored.
Utterances are treated as meaningful units of language, and are analyzed with regard
to their semantic content. So the human and the machine are both seen as rational
agents whose utterances are always perfectly formed and fully in accordance with
Gricean rules. These interlocutors like to spend their time seeking information from
each other, testing each other&apos;s abilities in plan inference. Typical interchanges are the
following:
</bodyText>
<listItem confidence="0.93381">
1. Human: The 8:50 train to Montreal?
</listItem>
<bodyText confidence="0.8945886">
Computer: The railroad station is three blocks west on Main
Street. But it&apos;s closed on weekends. In any case, the last train
to Montreal ran in 1988. You should call Air Canada
reservations at 1-800-AIR-LINE. Thank you for using
Mr Askme. Have a nice day!
</bodyText>
<note confidence="0.6357135">
C) 1991 Association for Computational Linguistics
Computational Linguistics Volume 17, Number 2
</note>
<listItem confidence="0.956972">
2. Computer: Attach the flageolet to the pump shaft by means of
the compressor module.
</listItem>
<bodyText confidence="0.986028461538462">
Human: It doesn&apos;t fit inside it.
Computer: No, the blue one.
Conversation Analysis, however, is a contrasting tradition. From its name, Con-
versation Analysis sounds like it should be the same thing as Discourse Analysis. In
practice, however, the two are distinct. CA tends to view talk as a human noise-making
behavior. It concerns itself with talk at the sound-stream level, without constituent
analysis, and it worries about such questions as how people take turns in conversa-
tion, where they look when they speak, and when and how they interrupt each other
(Goodwin, 1981, pp. 6-9). Boden (1990b, p. 248) remarks that &amp;quot;a better name for the
field would, in fact, probably be something like &apos;interactional analysis,&apos; as everything
in the interaction, from a quiet in-breath to the entire spatial and temporal organi-
zation of the scene, may be subject to analysis.&amp;quot; In Conversation Analysis, typical
interchanges look more like these:
</bodyText>
<figure confidence="0.758409666666667">
3. A: How much did you . in fact . see of the English department
upstairs
C: Very little
A: Very little
C: Very littlel
4. A: Richard hallo . I&apos;ve just *[s]* set out
B: *thank you*
A: ((2 syllables)) make some
B: thanks
A: I&apos;ve just boiled some water for having coffee cos I haven&apos;t
had time for tea — *would* you like some
B: *yes* yes2
5. Patient: An ah cun take thi:s hand an li:ft it and it duh&apos;unt hurt
Doctor: Yuh mean when yuh lif the ar:m up using yer other
han:d=
Patient: =Uh-huh3
6. A: How many tubes would you like sir
B: U:hm . what&apos;s the price now eh with V.A.T. do you know eh
A: Er I&apos;ll just work that out for you
B: =Thanks
(Pause of 10.0 seconds)
A: Three pounds nineteen a tube sir
B: Three nineteen is it=
A: =Yeah&apos;
</figure>
<footnote confidence="0.706622166666667">
1 From Svartvik and Quirk (1980, S.1.5: 836-848). I have omitted their extensive prosodic markings.
2 From Svartvik and Quirk (1980, S.1.4: 1-11), simplified. The asterisks mark simultaneous speech; e.g.,
B&apos;s thank you is said at the same time as A&apos;s [s].
3 From West (1984, p. 133). The colon indicates a lengthening of a sound, the equal-sign that the two
utterances are linked without a discernible pause.
4 Levinson 1983, p. 305.
</footnote>
<page confidence="0.993897">
212
</page>
<figure confidence="0.88303875">
Hirst Conversation Analysis
7. (Silence)
A: Huh?
B: I didn&apos;t say anything.&apos;
</figure>
<bodyText confidence="0.999969857142857">
The difference between the two research traditions is perhaps best illustrated by
the difference in the way they approach the matter of indirect speech acts. In Discourse
Analysis and traditional computational linguistics, indirect speech acts are seen as an
important phenomenon in discourse, and dealing with them requires considerable
effort in plan recognition on the part of the comprehender (e.g., Perrault and Allen
1980). On the other hand, &amp;quot;on the CA view, the problem doesn&apos;t even exist&amp;quot; (Levinson
1983, p. 356); rather, the analysis is in terms of pre-requests and dis preferred responses
(for details, see Levinson 1983, pp. 356ff).
Perhaps the most important points that CA makes are that there is much more to
human conversation than is studied in Discourse Analysis or theoretical linguistics,
and that empirical research methods are required. But it&apos;s not immediately obvious
that this has any consequences for HCI. After all, human conversation is spoken,
whereas HCI, to the extent that it is linguistic at all, is generally textual, research in
speech recognition notwithstanding. But on the other hand, both speech and HCI are
highly interactive forms of communication. Perhaps, as Frohlich and Luff argue in
their chapter of the book, that&apos;s what really matters.
For example, one area of particular concern to CA is turn-taking and interruptions.
CA studies how it is that speakers signal to others either that they have finished their
turn or that they wish to &apos;retain the floor&apos; for another turn construction unit; and it
studies the circumstances under which speakers will interrupt one another. In present-
day human—machine dialogues, such matters are simply not an issue. Users signal the
end of their turn by means such as hitting the RETURN key; the machine signals by
displaying some prompt to the user, usually a character such as &apos;*&apos; or &apos;&gt;&apos;. Neither
side can interrupt the other. The suggestion of this book is that this is not how things
should be. Since elements like turn-taking cues and interruptions are so important
in human conversation, if we want to build natural, flexible interfaces, these matters
should also be an issue in HCI.
Similarly, Pirkko Raudaskoski, in her chapter, contrasts conventional research in
HCI on recovery from misunderstandings with that in CA. The HCI research empha-
sizes the analysis and classification of the failure (cf. Ringle and Bruce 1982, Goodman
1986), while CA emphasizes the conversational patterns (e.g., Sacks et al. 1974) and the
active role of both interlocutors in negotiating a meaning (cf. Clark and Wilkes-Gibbs
1986, Heeman 1991). Raudaskoski sees value in a synthesis of the two approaches. And
Hugh Robinson points out in a short chapter that HCI would get more respect within
the field of software engineering if it used CA to make itself a little less airy-fairy
</bodyText>
<sectionHeader confidence="0.876153" genericHeader="method">
2. The Intellectual Background of Conversation Analysis
</sectionHeader>
<bodyText confidence="0.997225">
Conversation Analysis follows the tenets of ethnomethodology, a radical movement that
developed in sociology in the early 1960s under the leadership of Harold Garfinkel
of the University of California, Los Angeles. In reaction to conventional sociology,
ethnomethodology stresses the questions of what the foundations of social interaction
</bodyText>
<page confidence="0.8464275">
5 Schegloff, quoted by West (1984).
213
</page>
<note confidence="0.300006">
Computational Linguistics Volume 17, Number 2
</note>
<bodyText confidence="0.9706925">
are, and how people interpret their everyday social experience:
My purposes ... are to demonstrate the essential relevance, to sociological
enquiries, of a concern for common sense activities, as a topic of enquiry in its
own right ... : the socially standardized and standardizing, &amp;quot;seen but unnoticed,&amp;quot;
expected background features of everyday scenes. (Garfinkel 1967, p. 36)
Ethnomethodology thus places great emphasis on its being descriptive, empirical
work, detailed and (purportedly) atheoretical: a researcher&apos;s task is to gather data
and avoid theoretically loaded interpretations.
One &amp;quot;seen but unnoticed&amp;quot; object of study is the mechanism of ordinary conversa-
tion: not formal discourse or written text, but common phatic chit-chat and utilitarian
exchanges. Conversation Analysis arose in the 1960s from the work of Harvey Sacks,
and is associated with Emanuel Schegloff and Gail Jefferson in the U.S. and with John
Heritage and J. Maxwell Atkinson in the U.K. For a balanced, easy-to-read introduc-
tion to CA that emphasizes the linguistic issues, see Levinson (1983, ch. 6); for an
introduction that emphasizes the ethnomethodological roots, see Heritage (1984, ch. 8)
or Boden (1990a).
Sacks believed deeply that the world is actually a very orderly place if one just
looks closely enough. As Robin Wooffitt says in his introduction to CA in Computers
and Conversation:
A fundamental assumption informing CA is that ordinary talk is a highly
organized, orderly phenomenon. The goal of analysis is to reveal the basis of this
phenomenon. ... [Sacks] did not treat an utterance as something that just
happened to be said that way. Rather, he worked on the assumption that the
things people said were methodically designed. (pp. 10-12)
[In CA,] words used in talk are not studied as semantic units, but as products
or objects which are designed and used in respect of the interactions being
negotiated through the talk: requests, proposals, accusations, complaints, and so
on. ... The analytic objective is to explicate the procedures on which participants
rely to produce utterances, and by which they make sense of other people&apos;s talk:
to describe the &apos;technology of conversation&apos; (Sacks 1984, p. 413). (p. 10)
Conversation Analysts thus spend their time tape-recording or videotaping con-
versations like (3)-(7) above, and seeing what sort of structures they contain. Many
conversational exchanges are found to be various kinds of adjacency pairs: a greeting
followed by a greeting in response, a question followed by an answer, and so on.
But variations are possible. For example, the response to a question might be another
question, thereby initiating a side sequence (Jefferson 1972), as seen in dialog (6) above.
If there is some sort of trouble with the initial utterance, the response might be a next
turn repair initiator, as in (5) above.
Throughout all, researchers working with everyday conversational materials
have uncovered a veritable gold mine of ... precise and patterned procedures for
producing talk that reveal, in their instantiation, the sort of fine-grained order in
the social world that so amazed early naturalists in the nineteenth century as
they began to systematically observe the natural environment. (Boden 1990b,
pp. 252-253)
Wooffitt says (p. 27) that such conversational structures are not fixed and hard-wired
cognitive phenomena, but rather are normative and socially organized. There is some
dispute on this point. Boden (1990b, p. 250) claims, for example, that the turn-taking
model of Sacks, Schegloff, and Jefferson (1974) &amp;quot;has held up across a range of languages
</bodyText>
<page confidence="0.998458">
214
</page>
<subsectionHeader confidence="0.819023">
Hirst Conversation Analysis
</subsectionHeader>
<bodyText confidence="0.996077159090909">
and cultures [and] it now seems quite reasonable to claim that this core machinery
for talk transcends both language and culture.&amp;quot; Conversely, Levinson (1983, P. 301)
uses the claim that the model does not hold in Burundi to defend the model against
the charge of being trivial or content-free! Regardless, conversational structures are
so fundamental and become so deeply internalized that, as David Good points out
in his chapter of the book, they are present even in the disordered conversation of
schizophrenics. Good suggests that just as studying aphasics can help us learn about
syntactic mechanisms, so studying schizophrenics can help us learn about conversa-
tion. He exhibits an extract of a conversation in which the patient maintains normal
repair and functional cooperation even while the content is seriously awry
There is a tension in CA between the desire to find neat, universal, conversational
structures and the recognition that conversation, like all social interaction, is situated
and indexical—that is, it depends upon, and, a fortiori, helps to create, the situation or
context in which it occurs:
The organizational features of conversation are treated as structures in their own
right and are taken to operate—like other social structural factors—
independently of specific actors, psychological dispositions, or attributions of
particular individuals. This is not to say, of course, that there isn&apos;t variation
across individuals, but rather that these conversational structures are &amp;quot;context
free.&amp;quot; Secondly and simultaneously, the structures of talk are assumed to be
&amp;quot;context sensitive&amp;quot; in the sense that their instantiation at particular moments and
in particular contexts, as well as at specific points in interactional time,
constitutes that moment and shapes that interaction. (Boden 1990b, pp. 250-251)
This research is not without its critics of course. Goldthorpe, in his review of early
ethnomethodology (1973), finds little if anything to prefer over conventional sociol-
ogy. He complains of ethnomethodology&apos;s impenetrable jargon, and this can certainly
be seen in the CA literature—e.g., transition relevance place instead of potential end of
utterance. Presumably, such locutions are intended to distance one from theoretically
loaded interpretations of the data.
More recently, O&apos;Connell et al. (1990) have presented a critique of CA research on
turn-taking in conversations, a critique so vehement and unrelenting that one is left
with the impression that the authors believe that no Conversation Analyst has ever
uttered a true word on that topic or any other. The criticisms are largely aimed at the
assumptions and methodology of the research. Perhaps the most important, however,
is that CA completely ignores the goals and purposes of speakers in a conversation.
Even if this is true, it&apos;s not clear to me that it matters, at least for CA. A speaker&apos;s
intents are, after all, not open to direct observation. CA can only describe the external
behavior, the structures that conversants have available to them. To expect otherwise
is to misunderstand the goals and purposes of CA. For HCI and computational lin-
guistics, however, the point is more serious. If we are in the business of building
purposeful machines that respond to purposeful users, the notion of communicative
intent seems to be central to the enterprise (cf. Cohen et al. 1990). This casts doubt on
the idea that CA could play a useful role in HCI. I&apos;ll return to this point at greater
length in Section 4 below.
</bodyText>
<listItem confidence="0.75331475">
3. Applications of Conversation Analysis in Human–Computer Interaction
Computers and Conversation is implicitly divided into two parts. Chapters in the first
part debate the question of what exactly CA is and whether it has any application
at all to the study of human–machine interaction. Those in the second part report
</listItem>
<page confidence="0.996058">
215
</page>
<note confidence="0.301395">
Computational Linguistics Volume 17, Number 2
</note>
<bodyText confidence="0.99954">
research that indeed purports to apply CA to HCI. I&apos;ll take the two parts in reverse
order, as the applications described in the second are used as evidence in the debate
in the first.
</bodyText>
<subsectionHeader confidence="0.999819">
3.1 The Advice System
</subsectionHeader>
<bodyText confidence="0.999966326086957">
The chapter that is perhaps most convincingly pro-CA is the one by David Frohlich
and Paul Luff that describes a CA-inspired interface to an advice system. The advice
system, which is named Advice System, gives information and assistance on British
government welfare benefits, and is intended for use directly by clients.
The interface is primarily mouse-driven, with the user clicking on &apos;buttons&apos; on the
screen, or picking up phrases from menus to assemble natural language sentences,
rather like the Texas Instruments NLMenu system (Tennant et al. 1983a, 1983b). Un-
fortunately, the authors are fuzzy about many of the details, and it&apos;s not clear whether
there is also direct keyboard input. (Page 190 and Figure 9.1 suggest that there isn&apos;t, yet
page 207 and Tables 9.5 and 9.7 suggest that the user might make spelling errors.) It&apos;s
also unclear whether the input menus are dynamically generated from a true grammar
and lexicon, as in NLMenu, or whether they are just a static, pre-determined selec-
tion of useful fragments. Likewise, the authors don&apos;t say whether the Advice System&apos;s
responses are just pre-stored text or dynamically generated. (It&apos;s characterized as an
expert system, so one expects something closer to the latter, but Frohlich and Luff
provide no details.)
Frohlich and Luff&apos;s explicit claim (p. 188) is that they have tried to build the
principles of CA into the system &amp;quot;in such a way as to generate and support orderly
sequences of talk.&amp;quot; In this design, advice or explanation is not something the system
dumps out in a single turn, but is an &amp;quot;emergent property&amp;quot; of a number of turns (p. 219).
For example, CA is applied in formulating dialogue control policies, in which the
authors try to &amp;quot;recreate some of the dynamics of ordinary conversation&amp;quot; (p. 201) by
using CA rules for turn-taking in the conversation and for allowable adjacency pairs
(Sacks et al. 1974). Specifically, the types of utterances possible are questions, state-
ments, and answers, both by the user and by the system. That means that there are
(3 x 2)2 = 36 possible kinds of adjacency pairs (p. 193). For example, a user&apos;s question
can be followed by an answer by the system (such a pair is denoted UQ-SA) or by a
question from the system (UQ-SQ). But each &apos;speaker&apos; can also &apos;retain the floor&apos; after
an utterance, perhaps adding a statement after an answer, or asking a question (e.g.,
UA-US, UA-UQ, SA-SS, SA-SQ).
But not all 36 kinds of pairs are allowed in the system (despite the assertion to the
contrary in the caption of Table 9.1); 10 or 11 kinds are ruled out by CA&apos;s constraints
on turn-taking and turn construction (the exact count depends on whether you prefer
to believe Table 9.4 or the text on page 205). For example, UQ-US and SQ-SS are
eliminated by the rule that a question may not be followed by a statement. In fact, a
question always causes a turn transition. This strikes me as too restrictive. A UQ-US
sequence seems quite natural, e.g., &amp;quot;Can I get disability benefits? I broke my arm at
work last week.&amp;quot; But an SQ-UQ sequence is among those allowed; that is, the user can
respond to answer a question with another question—although the menu structure is
said to make this somewhat difficult to do (p. 204).
The key point that arises is that, unlike ordinary human–computer dialogs, the
user and system can interrupt each other at the end of any turn construction unit,
even if the turn is incomplete. For the user, selecting the key marked &amp;quot;.&amp;quot; (period) after
constructing an utterance indicates that they wish to end the turn, while selecting &amp;quot;... &amp;quot;
(ellipsis) indicates that they wish to say more, that is, to retain the floor. Nevertheless,
the system is allowed to interrupt at such points—but only if it requires clarification
</bodyText>
<page confidence="0.995173">
216
</page>
<subsectionHeader confidence="0.695784">
Hirst Conversation Analysis
</subsectionHeader>
<bodyText confidence="0.998803375">
of what&apos;s been said so far in the turn, that is, if it wants to elicit a self-repair by the user
(see below). Similarly, the user can interrupt the system. After each turn construction
unit uttered by the system, there is a pause during which a button labeled &amp;quot;LET ME
SPEAK&amp;quot; becomes available to the user for a short time (p. 204).
Another aspect of conversation studied by CA and incorporated in the Advice
System is repair, that is, clarifications, corrections of errors, and the like. Schegloff et
al. (1977) have shown that there is a strong preference in conversation for self-repair—
for people to correct their own errors. As already mentioned, the system can use next
turn repair initiators; that is, it can interrupt the user and ask &amp;quot;What?&amp;quot;. (It&apos;s not smart
enough to attempt the repair itself: &amp;quot;You mean ... ?&amp;quot;) In addition, it enables the user to
do the same: it makes checking moves, asking &amp;quot;OK?&amp;quot; every so often. If the user answers
&amp;quot;No&amp;quot;, the system will try to repair the problem. (Similarly, users can ask the system
&amp;quot;OK?&amp;quot; if they are not sure that they are being understood.)
But it is unclear to me why the Advice System should ever need to initiate repairs
by the user at all. After all, menu-based interfaces are supposed to prevent the user
from uttering anything that is beyond the system&apos;s ability—a well-structured menu
system precludes problematic choices. Extending that principle to natural language
input was one motivation of the original NLMenu system. But then repair would
never be necessary, and Frohlich and Luff&apos;s goal of having a system that performs
humanlike repair initiations would be undermined! (Frohlich and Luff merely say
that the use of menus ensures &amp;quot;high levels of input recognition&amp;quot; (p. 191).)
The closing of the dialogue also follows principles derived by CA. The user can&apos;t
just walk away from the machine, but must go through an official pre-closing (Sche-
gloff and Sacks 1973) in which &amp;quot;each party [must] decline at least one opportunity to
continue talking&amp;quot; (p. 214) before the closing and terminal exchange (a &amp;quot;Bye&amp;quot;–&amp;quot;Bye&amp;quot;
adjacency pair) is reached. The user can shorten this sequence by clicking on a button
labeled &amp;quot;I HAVE TO GO&amp;quot;. The pre-closing sequence starts after the system has fully dealt
with all questions that the user has asked. In this sequence, the system can volunteer
further information and encourage the user to continue to ask questions.
The system also has a deliberate opening sequence intended to help familiarize
the user with the system (p. 213). This seems to be a pretty rigid sequence, and it&apos;s not
clear that the invocation of CA results (such as Schegloff&apos;s (1968) summons–answer
sequences) is useful or an improvement upon conventional systems.
The rules that control all this are expressed in a Prolog-like notation. Some exam-
ples (from the authors&apos; Table 9.3) are shown in Figure 1. It is tempting to view these
rules as the rules of a grammar for conversation, a point that I will return to below.
There are some very interesting ideas in this work, but I am not sure what to make
of the result. First, the authors are so vague at times, even self-contradictory, that one
cannot get an adequate impression of the system as a whole. Are the interface and the
underlying advice system genuinely intelligent, or is it nothing more than &apos;Eliza with
a mouse&apos;?6
Second, whether the interesting ideas work in practice is an empirical question. The
system had not been tried out on real users at the time the chapter was written, though
field tests were planned. This is an important point. Many systems that have been
designed to be &amp;quot;helpful&amp;quot; have been found to cause more problems and misconceptions
than simple, straightforward systems. Examples include the photocopier studied by
Suchman (1987) and Raudaskoski&apos;s message system, to be described below. Moreover,
testing the system is likely to be difficult, because there is so much in it that is novel
</bodyText>
<page confidence="0.739081">
6 Thanks to George Ferguson for this apt phrase. The allusion is to Weizenbaum (1966).
217
</page>
<figure confidence="0.998703916666667">
Computational Linguistics Volume 17, Number 2
conversation &lt;- opening
body
body &lt;- if (OR (user-questions) (system-questions))
then adjacency-pair
else preclosingl
adjacency-pair &lt;- user-open-floor
system-floor
body
user-open-floor &lt;- if (system-questions)
then user-answer-turn
else user-statement-turn
</figure>
<figureCaption confidence="0.991815">
Figure 1
</figureCaption>
<subsectionHeader confidence="0.76104">
Examples of rules in the Advice System.
</subsectionHeader>
<bodyText confidence="0.9921185">
that unless it is a runaway success, it might be difficult to determine just which ideas
worked and which didn&apos;t.
</bodyText>
<subsectionHeader confidence="0.998245">
3.2 Repair in Human–Computer Dialog
</subsectionHeader>
<bodyText confidence="0.9999174">
The second of the two chapters that best show the potential applicability of CA to
HCI is that by Pirldw Raudaskoski on repair sequences. Humans not infrequently
misunderstand one another and have to initiate repairs or clarifications. Given the
imperfect nature of present NLU and speech recognition techniques, the need for
a computer to be able to initiate repairs is even greater than it is for people. And
similarly, the user should be able to initiate a repair by the system if necessary.
Raudaskoski carried out an interesting experiment with a system that could use
a number of different repair-initiation strategies whenever it failed to comprehend an
input. The domain was a speech interface to a telephone message store-and-forward
system in which users could ask to leave messages for others, or ask to hear their own
messages. To finesse the problem of speech recognition, the interface was simulated.
Users spoke over the phone to the experimenter, who typed in what they said and
read back the system&apos;s responses. It&apos;s not clear how this might have affected the results
of the experiment.
The system had five kinds of repair initiators:
</bodyText>
<listItem confidence="0.999671">
• Parroting back, indicating the source of trouble: e.g., &amp;quot;Leave a message
for who?&amp;quot;
• Implying not hearing or understanding: e.g., &amp;quot;Sorry?&amp;quot;
• Explicitly guessing: e.g., &amp;quot;Do you mean ... ?&amp;quot;
• Narrowing the user&apos;s choices: e.g., &amp;quot;You can either ... or ...&amp;quot;
• Checking: e.g., &amp;quot;You want to ... , right?&amp;quot;
</listItem>
<bodyText confidence="0.9998295">
The first two of these were also available to users to initiate a repair by the system.
The experiment was pretty much a failure—but it failed in an interesting way.
Raudaskoski found that subjects never availed themselves of repair initiators; and
the system didn&apos;t go through its whole repertoire either. In one interesting segment
</bodyText>
<page confidence="0.993248">
218
</page>
<subsectionHeader confidence="0.419442">
Hirst Conversation Analysis
</subsectionHeader>
<bodyText confidence="0.999614777777778">
(p. 168), reprinted as (8) below, the system initiates a repair (line 8.1), but the user&apos;s
response (line 8.2) is not understood either and it tries again (line 8.3). But the user
misinterprets this as a prompt for the message to be left (line 8.4). The system keeps
trying (line 8.5), but is again misunderstood: what the user repairs is her previous
utterance (line 8.6). More patient than any human would be, the system tries yet
again (line 8.7). The user takes the fact that the system is ready to start a task as
confirmation that the previous task is complete, and starts something new (line 8.8).
So the user ends up thinking that she has achieved her goal of leaving a message for
Eve, whereas the system has understood nothing at all!
</bodyText>
<listItem confidence="0.999283545454546">
8. (8.1) System: So leave for Eve—is that
right?
you want to a message
(8.2) User: A message for Eve, that&apos;s correct.
(8.3) System: What is it that you want to do then?
(8.4) User: Can you say that Judy is at home?
(8.5) System: Do what?
(8.6) User: Judy is at home.
(8.7) System: You can leave messages or listen to
messages—select either.
(8.8) User: I want to leave a message for Fred.
</listItem>
<bodyText confidence="0.9987705625">
What Raudaskoski found was that in speech mode, humans tend to be verbose;
they often don&apos;t just say &amp;quot;yes&amp;quot; or &amp;quot;no.&amp;quot; The system was unable to channel their re-
sponses narrowly enough that it could be sure of understanding them. It needed to
initiate repairs to repairs, but it couldn&apos;t.
I suspect that a similar problem might be found with Frohlich and Luff&apos;s Advice
System. As we saw earlier, its only way of initiating repairs is to ask &amp;quot;What?&amp;quot; But
this is probably a very bad initiator for a computer system that aspires to natural
conversation. Valian and Wales (1976) have shown that people&apos;s responses to &amp;quot;What?&amp;quot;
are generally to offer a syntactically simpler form of the sentence uttered, if it is
structurally complex, or to repeat essentially the same utterance, if it is already simple.
So when the Advice System says &amp;quot;What?&amp;quot;, it is likely to be seen as initiating a syntactic
repair, whereas the problem is presumably more likely to be semantic.
Raudaskoski concludes her chapter by suggesting that a computer&apos;s next turn
repair initiators should not be like those used by people, but rather should emphasize
exactly where the misunderstanding is. That would certainly be good advice for the
Advice System.
</bodyText>
<subsectionHeader confidence="0.999472">
3.3 Other Applications
</subsectionHeader>
<bodyText confidence="0.999916916666667">
Two other papers in the book also try to apply CA to human–computer interaction. Al-
ison Cawsey is interested in tutorial systems in which an expert (the system) teaches
a novice about a domain (here, electronics) and asks test questions. She focuses on
explanations that the system gives to the user. While acknowledging that such ex-
planations need to be planned (p. 223), she claims that it is also necessary to allow
for interaction within the explanation. She does this with CA-based rules not unlike
those of Frohlich and Luff, above. The planning is incremental, and based on each
exchange. It occurs at both the high level (discourse structure; e.g., do a pre-closing)
and the lower level (e.g., adding a pre-closing to another utterance). In extensions to
the work, Cawsey hopes to allow the user to initiate repairs, in effect saying &amp;quot;Huh?&amp;quot;
to the system (pp. 232-3). Moore (1989) has described a system with a similar ability,
but Cawsey claims that Moore&apos;s system, because it&apos;s not based on CA&apos;s observations
</bodyText>
<page confidence="0.990031">
219
</page>
<note confidence="0.481396">
Computational Linguistics Volume 17, Number 2
</note>
<bodyText confidence="0.999402586206897">
of natural conversations, is limited in two ways: the user can&apos;t interrupt in the middle
of a turn, and the system doesn&apos;t ask the user to confirm its guess as to what is wrong.
Whether these limitations make the system less effective than Cawsey&apos;s is, of course,
an empirical question that is yet to be tested.
Nigel Gilbert, Robin Wooffitt, and Norman Fraser describe a project in progress:
Sundial, a speech recognition—based system for giving, for example, flight information
over the phone. Utterances are not to be processed singly and in isolation, as in present-
day speech recognition systems, but rather as part of a CA-informed discourse. The
authors argue that speech is the best HCI medium (p. 236). Of course, the system is
still a couple of years off.
A rather different use of CA, in the interpretation of software specifications, is
suggested by Anthony Finkelstein and Hugo Fuks. One task of the systems analyst
is to take vague, waffly, and often contradictory statements from people as to what is
required in a system, and turn them into clear, unambiguous, formal specifications (cf.
Regoczei and Hirst 1989). Finkelstein and Fuks suggest that explicit use of CA might
ease the analyst&apos;s task, allowing one to sort it all out, including the retractions, the
iterative clarifications, and so on, to resolve different perspectives on the problem.
They use rules based in part on CA; for example, all challenged statements must be
affirmed, denied, or withdrawn. This is as much Discourse Analysis as Conversation
Analysis, and the authors claim that (contra some other suggestions, e.g., Levinson
1983, p. 294) the two are quite compatible. The examples given by the authors are
made up, not actual dialogue, and it&apos;s unclear as to whether they are intended to
represent real speech or merely formalizations thereof. (If it&apos;s the former, then the
authors are advised not to seek careers as playwrights.)
This looks promising at first glance, and the authors hope that it could lead to
automated support. But it is unclear that the rules could really do that. They describe
a rigid discourse, saying, for example, that all questions must be answered. Real con-
versations needn&apos;t be like that. Nor is it immediately clear how these rules would be
used by a human analyst in practice.
</bodyText>
<sectionHeader confidence="0.573423" genericHeader="method">
4. Is This All Really Possible?
</sectionHeader>
<bodyText confidence="0.988368894736842">
I now turn to the chapters that debate whether CA in fact has useful things to say
at all concerning human—computer conversations, and whether the research described
above should count as evidence that it does.
Paul McIlvenny invokes Suchman&apos;s (1987) notion of the computer as an interactive
artifact, to emphasize the user&apos;s view of it as a reactive, language-using entity that
produces seemingly purposeful, social actions (p. 96). CA is thus relevant to HCI
&amp;quot;wherever the second person metaphor is present&amp;quot; (p. 99).
The case for the practical usefulness of informing HCI design with CA is put, some-
what unconvincingly, by Michael Norman and Peter Thomas. Norman and Thomas
say that CA is important both for its methodology and for its findings. It tells us
about normative formats and expectations in conversations, and we can use that, for
example, in analyzing situations where users have problems with an interface. CA can
&amp;quot;sensitize the designer to matters of the interactional process&amp;quot; (p. 61); it can tell us,
for example, not just that feedback is needed, &amp;quot;an often heard and particularly vague
piece of design guidance,&amp;quot; but that &amp;quot;the next action ... may in fact be ... a repair
of the user&apos;s previous action,&amp;quot; and thus &amp;quot;provide us with a view of the normative
ways in which repairs are constructed&amp;quot; (ibid.). The chapter closes with some vague
pieces of design guidance for HCI from CA, the authors admitting that determining
how or whether they can be applied will require the development of a much better
</bodyText>
<page confidence="0.99197">
220
</page>
<subsectionHeader confidence="0.48587">
Hirst Conversation Analysis
</subsectionHeader>
<bodyText confidence="0.9786955625">
understanding of human—computer interaction.
The opposing case is put by Graham Button, in a rather long and repetitious paper.
He says that one might at best be able to produce a &amp;quot;simulacrum&amp;quot; (p. 67) of a naturally
occurring conversation. For Button (in the ethnomethodological tradition), talk is an
&amp;quot;achievement&amp;quot; (p. 79) or an accomplishment (cf. Garfinkel 1967, p. 1) that uses the
rules of CA, but is not caused by them.
His argument is that the rules of CA are not like the rules of a computer model:
CA rules for sequencing, turn-taking, and the like might look like causal rules at first
glance, but actually they are &amp;quot;resources&amp;quot; (p. 84) that people &amp;quot;display an orientation
to&amp;quot; (p. 78) in situated instances, namely social interactions. Button&apos;s argument can be
understood in two slightly different ways, both of which are, I think, valid in CA. The
first way is this: A conversant, desiring a particular effect, can choose to invoke one
of the rules that will have that effect in the current situation. In this way, conversants
can make what they want of the contributions of others. For example, a speaker who
says &amp;quot;Can you tell me how to get to your place by bus?&amp;quot; could be heard to be asking
for bus information or requesting to be picked up by car; the listener makes it one or
the other by the way she or he replies.7 Button emphasizes the idea, due originally to
Sacks, that it is the response that provides for recognition of the first utterance, and
sees this as inherently incompatible with rule-based behavior:
If we formulated a rule that upon the production of a question, the next
utterance should be an answer, we could not have the case where a second
transforms a prior utterance into a first and thereby achieves coherence with the
first. (p. 85, emphasis added)
Indeed, any aspect of an interaction can be &amp;quot;retrospectively reread to find out in the
light of present practical circumstances what [it] &apos;really&apos; consisted of &apos;in the first place&apos;
and &apos;all along&amp;quot; (Garfinkel 1967, p. 74).8
The second interpretation is this: A conversant, desiring a particular situation, can
quite literally create the desired situation by invoking a rule that would be appropriate
only therein. For example,
Suppose there is a rule for greetings which runs to the effect: do not initiate
greetings except with persons who are acquaintances. And suppose we
subsequently see a man greeting another whom we know is not an acquaintance.
We can either conclude that he broke the rule or we can infer that, via the use of
the rule, he was seeking to treat the other as an acquaintance. The second
interpretation is more likely when, for example, our man is greeting a new
colleague at the office. (Heritage 1984, p. 126, emphasis supplied)
(Obviously, there are limits to this; you can&apos;t make a 17-year-old eligible to vote sim-
ply by singing Happy Birthday to them, nor can you kill someone just by holding a
memorial service for them.)
Now, from this, Button tries to argue that CA&apos;s rules of conversation are not
7 The example is from Blum-Kulka and Weizman (1988).
8 For example, in one of Garfinkel&apos;s experiments (1967, pp. 79ff), students who sought advice from a
counselor were given completely random answers to their yes/no questions. &amp;quot;Over the course of the
exchange, subjects sometimes started with the [random] reply as an answer and altered the previous
sense of their question to accommodate ... the reply as the answer to the retrospectively revised
question&amp;quot; (p. 90). (This kind of revision is covert, and is not to be confused with the overt interactional
reconstruction of an utterance that can follow a misunderstanding in conversation (Fox 1987, McRoy
1989, Cawsey forthcoming)).
</bodyText>
<page confidence="0.989922">
221
</page>
<note confidence="0.485773">
Computational Linguistics Volume 17, Number 2
</note>
<bodyText confidence="0.995304065217392">
amenable to direct use by computer, that they are not &amp;quot;codifiable&amp;quot; or &amp;quot;reduc[ible1 to an
algorithm&amp;quot; (p. 82). His first argument is that rules in a computer are causal, meaning
that they are designed into the hardware itself (p. 77), or into the program it executes.
This is, I gather, intended to contrast with the use of rules as resources, as described
above. However, I am at a loss to see the contradiction. Any backward-chaining Horn
clause theorem prover or planner with appropriate meta-rules can be viewed as using
its rules as resources in exactly the ways I described. Indeed, the ability of a second
utterance to transform a prior utterance surely requires having rules such as &amp;quot;that upon
the production of a question, the next utterance should be an answer&amp;quot;! Button seems
to be saying nothing more than that CA rules must be represented declaratively, not
procedurally, so that they can be deliberately and explicitly invoked with a view to
their effect. But far from precluding their use by a computer, this argument suggests
that they fit very nicely into present-day Al reasoning systems!
Button&apos;s second argument is that CA rules are not complete; for example, the rules
for turn-taking don&apos;t say how to identify a possible transition relevance place:
The fact that these ... places are possible transition relevance places means that it
is a contextually decidable matter whether or not actual transition takes place.
Thus, because the rules do not, so to speak, determine their own application
(using the rules is not determined by the rules themselves), there have to be
other features involved in the application of the rules. This is the orientation to
possible transition places, and in [Sacks et al. (1974)] there is no description of
these places being formulated in terms of the rules for their production. Possible
transition places are the contextual achievement of the unfolding structuring of
the turn in progress. Possible completion places are not, then, provided for in
advance by codifiable rules, they are situatedly achieved in and for a particular
turn. (pp. 80-81, emphasis supplied)
Button&apos;s intent here is not, as it might seem, to claim that CA chooses to remain
willfully ignorant about certain elements of its domain; rather, it is, presumably, to
tacitly invoke what&apos;s called in ethnomethodology the et cetera principle (Garfinkel 1967,
pp. 73-75): that the situations in which a rule is and isn&apos;t applicable can never be
finitely enumerated. In effect, every rule has an implicit et cetera at the end of its
list of triggering conditions, and the agent must judge each time whether the current
situation is one in which the rule may be applied, or, possibly, broken.&apos; Heritage (1984,
pp. 121-122) makes an analogy with the legal system. No matter how carefully a law is
written, a complex judicial decision will sometimes be required to determine whether
or how the law applies in some particular new situation; and then another judicial
decision might be required to determine whether a second situation is covered by the
precedent established by the first.
But again, this is hardly an in-principle argument against computer use of CA
rules. Case-based reasoning, while far from being a completed science, is nevertheless
a well-established subfield of artificial intelligence, and seems to hold considerable
promise for being able to do what Button says can&apos;t ever be done.
Underneath, this discussion seems to be little more than a bad version of the old,
familiar anti-AI arguments about Turing tests and Chinese rooms, about the limitations
of physical symbol systems, about whether one needs to distinguish between a system
that is explicitly following a certain rule from one that merely acts as if it is. It seems
</bodyText>
<footnote confidence="0.92756275">
9 Button seems to be making the stronger point that CA rules contain no triggering conditions at all. But
I don&apos;t think that this can be his intent, for as we saw above, it is only through the presence of such
conditions that the use of rules as resources becomes possible; a second can transform a first only to
the extent that the transformed first is a usual condition for the second.
</footnote>
<page confidence="0.993967">
222
</page>
<subsectionHeader confidence="0.488601">
Hirst Conversation Analysis
</subsectionHeader>
<bodyText confidence="0.997281803921568">
to me that for HCI the question is whether we can build computer systems whose
conversations are usefully similar to those of humans, and it doesn&apos;t matter whether
or not Button wants to call them merely simulacra.
For Button, the bad guys are Frohlich and Luff, and Gilbert, Wooffitt, and Fraser,
whose work I described above. (Cawsey and Raudaskoski are apparently exempted.)
Button says that they are wrong in their claims to be &amp;quot;using&amp;quot; CA in their systems;
the systems might work, but only because they are not truly based on CA or directly
using CA rules. Norman and Thomas, incidentally, say that they agree with Button on
this point, but think the criticism to be irrelevant—that one can still gain inspiration
and help from CA without claiming to be using CA rules directly in one&apos;s system
(p. 58). Frohlich and Luff, defending their work, seem to make the stronger claim,
however: that aspects of CA are indeed directly built into their system (p. 188). The
applicability of CA to HCI is, at least, a hypothesis to be refuted, and is the best bet
presently available to us (ibid.).
Gilbert, Wooffitt, and Fraser also seek to counter Button&apos;s arguments, but are not
convincing. They give a lengthy argument showing the obvious: that the constraints
on adjacency pairs, with nesting and interruption, can be written as a phrase structure
grammar (like the rules of Frohlich and Luff that we saw earlier), and such a grammar
can be parsed by a chart-parser—provided, of course, that the individual constituents
can be recognized! And because the parser is nondeterministic, it can recognize an
ambiguity, and either maintain it (p. 251) or &apos;choose&apos; from among the different possible
interpretations (p. 254). But it&apos;s not at all clear to me what this proves. It&apos;s one thing to
use a grammar of CA rules to perform post hoc recognition of the high-level structure
of a conversation, or to generate an example of (both sides of) a conversation. It&apos;s quite
another to use such rules to actually participate in a conversation without making a
mess of things—especially if one takes the et cetera principle seriously. It&apos;s the other
papers in the book that show that this might be possible.
Gilbert, Wooffitt, and Fraser conclude that &amp;quot;the idea that the findings of Conver-
sation Analysis are not susceptible to computer implementation depends on a rather
simple-minded notion of what a computer model can do&amp;quot; (p. 256). And indeed, this
is the feeling one gets from Button&apos;s paper. Button retorts that &amp;quot;rather, the idea that
Conversation Analysis lends itself to computer modelling depends upon a simple-
minded notion of what conversation is, and of what order of description of human
action Conversation Analysis provides&amp;quot; (p. 70). And he&apos;s right, too. Conversation is a
complex, highly context-dependent, behavioral phenomenon. The systems described
in the book don&apos;t come close to it. But that&apos;s not to say that they are trivial. Button
accuses the systems of simply responding with &amp;quot;set pieces&amp;quot; (p. 82), by which he ap-
parently means canned responses. As I mentioned before, it&apos;s unclear whether the
Advice System&apos;s responses are just pre-stored text or dynamically generated; but even
if the responses are canned, if it turns out that informants judge the system to be easy
or &apos;natural&apos; to use, if the response selected is always judged to be suitable, even in a
variety of complex conversational situations, then something interesting is going on.
While Button&apos;s criticism is, in effect, that CA is too rich and fine for HCI, the
criticisms of CA by O&apos;Connell et al. (1990) imply the converse: that by ignoring com-
municative intent, CA is too impoverished for use in HCI. On this view, CA is nothing
more than a kind of linguistic behaviorism. Unless its descriptions of external behav-
ior can be related to the intents and purposes of speakers, CA can be used neither
for understanding intelligent agents nor for constructing them. I see several replies to
this. The first is to grant all this, but to point out that these enterprises do, nonetheless,
require these descriptions of external behavior, and CA has them available. The chal-
lenge is then to use them.
</bodyText>
<page confidence="0.994865">
223
</page>
<note confidence="0.593228">
Computational Linguistics Volume 17, Number 2
</note>
<bodyText confidence="0.999952585365853">
The second reply is to deny the premise that intelligence and understanding re-
quire explicit analysis and representation of the intent of other agents. Rather, meaning
can be seen as no more than the reader&apos;s response to a text, or, more generally, the
perceiver&apos;s response to perception. This sits well with the notion mentioned above of
a response being able to transform a first utterance without regard to the intent of its
speaker, and is supported by deconstructionist and reader-response theories in literary
studies (e.g., Iser 1974; see also Holub 1984). These theories are highly controversial,
and are a current Hot Topic in literary studies (Matthews 1991). But, as Corriveau&apos;s
reader-response model of text understanding shows (Corriveau 1991), they are not
necessarily incompatible with artificial intelligence.
The third reply is to deny the other premise, and claim that intent is indeed
accounted for in CA—in fact, is pervasive in CA. When CA uses terms such as repair
or closing or summons, or speaks of conversants using rules as resources, it is, in fact,
explicitly ascribing intent! When analysts say that a particular utterance is, say, a
next turn repair initiator, they are using their own knowledge of the language of
the conversation to interpret that utterance in context, and hence they are indeed
determining the speaker&apos;s intent.
Perhaps the strongest use of intent in CA occurs in the notion of accountability:
in interaction, agents hold each other accountable, or responsible, for their choice of
actions and inactions. For example (Heritage 1984, pp. 106ff), consider again the rules
about greetings. If two acquaintances approach each other in the hallway and the first
greets the second, the second can choose either to return the greeting, and thereby be
&amp;quot;friendly&amp;quot; or &amp;quot;polite,&amp;quot; or not to, and thereby be &amp;quot;impolite&amp;quot; or &amp;quot;snub&amp;quot; the first. But
he or she cannot just opt out and avoid making the choice, or &amp;quot;whatever they do will
be intelligible and accountable as a sustaining of, or a development or violation, etc.,
of, some order of activity. ... [Their actions] are condemned to be meaningful&amp;quot; (Heritage
1984, p. 110, second emphasis added).
One must be careful not to overstate this position, however. CA should not be
viewed as just some kind of teenage mutant theory of semantics. Rather, it should be
seen as describing the framework that agents use to communicate their meanings and
intents. So to study such a framework is necessarily to study aspects of meaning and
intent.
I believe that this third reply is correct; but it is problematic for CA for two reasons:
it weakens CA&apos;s claim to being purely empirical and atheoretical, and it undermines
the listener-response orientation of CA. On the first point, CA&apos;s defense is to distin-
guish interpretation from theory, and say that analysts can use their knowledge of the
language of the conversation without requiring any theoretical framework. The sec-
ond point is trickier. Personally, I don&apos;t see any incompatibility in the notions of intent
and conversational flexibility; we just need to note that the exercise of flexibility by
the second speaker can subvert the intent of the first speaker. But I doubt that Button
would accept this.
</bodyText>
<sectionHeader confidence="0.970385" genericHeader="conclusions">
5. Conclusion
</sectionHeader>
<bodyText confidence="0.99388825">
Before I finish, a few remarks should be made about the organization and production of
the book under review. The chapters that debate whether CA can be applied in HCI
seem to have been written iteratively, with each author seeing drafts of the others&apos;
chapters in order that their arguments and rebuttals mesh well. This is good, but it
</bodyText>
<page confidence="0.996906">
224
</page>
<subsectionHeader confidence="0.609419">
Hirst Conversation Analysis
</subsectionHeader>
<bodyText confidence="0.998661852941176">
has the awkward consequence that each side&apos;s chapters require the other side&apos;s as
pre-reading. And the chapters are widely separated in the book.
Many of the chapters have cutesy titles that lack the keywords necessary to de-
scribe the content adequately (e.g., Frohlich and Luff, &amp;quot;Applying the technology of
conversation to the technology for conversation&amp;quot;; Button, &amp;quot;Going up a blind alley&amp;quot;).
Some of the chapters seem to be not so much position papers or project reports as
memory dumps. David Good, for example, goes out of his way to tell us that his wife
isn&apos;t always logically omniscient (pp. 142-3),10 and that before ennui set in, he was
pretty good at cryptic crosswords (p. 144).
The typography and layout are mostly fine, except in Chapter 9, where tables
are placed badly with respect to where they are cited in the text; parts of pages are
left blank for no apparent reason; and one table is split, quite unnecessarily, so that
the column headings are at the bottom of a recto page, and most of the body of the
table is on the following verso! Conversations are presented in different formats in
different chapters. In Chapter 9, they are presented in many different formats within
the same chapter; and one fragment, illustrating pre-closing sequences, is given a
very large heading, &amp;quot;Our conversation.&amp;quot; A number of references cited in the text are
missing from the bibliography (e.g., &amp;quot;Schegloff 1981,&amp;quot; p. 154; &amp;quot;Hamblin 1971,&amp;quot; p. 177;
&amp;quot;Heritage 1987,&amp;quot; p. 183).
While I have been very critical of much of Computers and Conversation, it is clear,
I hope, from the length of my comments that I take the work very seriously. I believe
that the role of CA in computational linguistics and HCI is definitely worth exploring,
and the core of my criticisms is that the faltering first steps in this direction often
falter more than they reasonably ought to. There is a sense in which it is clear that CA
must have a role in NLU, because there is a sense in which ethnomethodology is just
a small subfield of artificial intelligence (although that might come as a surprise to
the ethnomethodologists), as its goal is simply to explicate and represent some areas
of commonsense knowledge, rather like naïve physics and similar projects (cf. Hayes
1985; Hobbs and Moore 1985; Hobbs et al. 1985).
Obviously, it is too early to say that the results of Conversation Analysis research
do have a place in the design of human—computer interaction. Button may yet be
proved right. But the work described in this book is reason to be optimistic that a
valuable synthesis will emerge. And if it doesn&apos;t, determining why not would be
equally valuable.
</bodyText>
<sectionHeader confidence="0.987134" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.990180208333333">
I am grateful to Deirdre Boden, Richard
Frankel, and Tom Smith for discussions that
shaped my understanding of
ethnomethodology and Conversation
Analysis, and for pointers to relevant
literature. I am especially grateful to
Deirdre for her comments on an earlier
draft of this review. My thoughts on the
nature of rules were developed in
discussions with David Traum. I have also
benefitted from advice from Jeff Pelletier,
Susan McRoy, Chrysanne DiMarco, Diane
Horton, James Allen, and Stephen Regoczei.
The opinions that I express herein, however,
are not necessarily anyone&apos;s but my own.
This review was written while I was a
visitor at the Department of Computer
Science, University of Rochester, using
facilities that the department kindly made
available to me.
10 One of the premises that she was supposed to reason from is the self-contradictory proposition that
she knows that he knows that she doesn&apos;t know that he knows that one of her acquaintances has a
twin, where know is to be construed factively. Since everything follows from a contradiction, perhaps
her inference engine just became overloaded.
</bodyText>
<page confidence="0.996188">
225
</page>
<reference confidence="0.987726552845529">
Computational Linguistics Volume 17, Number 2
References
Blum-Kulka, Shoshana and Weizman, Elda
(1988). &amp;quot;The inevitability of
misunderstanding: Discourse
ambiguities.&amp;quot; Text, 8(3), 219-241.
Boden, Deirdre (1990a). &amp;quot;The world as it
happens: Ethnomethodology and
conversation analysis.&amp;quot; In: Ritzer, George
(ed.) Frontiers of social theory. New York:
Columbia University Press, 185-213.
Boden, Deirdre (1990b). &amp;quot;People are talking:
Conversation analysis and symbolic
interaction.&amp;quot; In: Becker, Howard S. and
McCall, Michal (eds.) Symbolic interaction
and cultural studies. Chicago: The
University of Chicago Press, 244-273.
Cawsey, Alison J. (in press). &amp;quot;A belief
revision model of repair sequences in
dialogue.&amp;quot; In: Costa, Ernesto (ed.) New
directions in intelligent tutoring systems.
Springer-Verlag.
Clark, Herbert H. and Wilkes-Gibbs,
Deanna (1986). &amp;quot;Referring as a
collaborative process.&amp;quot; Cognition, 22(1),
1-39. Reprinted in: Cohen, Morgan, and
Pollack (1990), 463-493.
Cohen, Philip R.; Morgan, Jerry; and
Pollack, Martha E. (eds.) (1990). Intentions
in communication. Cambridge, MA: The
MIT Press (System Development
Foundation benchmark series).
Corriveau, Jean-Pierre (1991).
Time-constrained memory for reader-based
text comprehension. Doctoral dissertation,
Department of Computer Science,
University of Toronto (published as
technical report CSRI-246).
Fox, Barbara (1987). &amp;quot;Interactional
reconstruction in real-time language
processing.&amp;quot; Cognitive Science, 11(3),
365-387.
Garfinkel, Harold (1967). Studies in
ethnomethodology. Englewood Cliffs, NJ:
Prentice Hall. (Reprinted: Cambridge,
England: Polity Press, in association with
Basil Blackwell, 1984.)
Goldthorpe, John H. (1973). &amp;quot;A revolution
in sociology?&amp;quot; Sociology, 7(3), 449-462.
Goodman, Bradley A. (1986). &amp;quot;Reference
identification and reference identification
failures.&amp;quot; Computational Linguistics, 12(4),
273-305.
Goodwin, Charles (1981). Conversational
organization: Interaction between speakers
and hearers. New York: Academic Press
(Language, thought, and culture:
Advances in the study of cognition).
Hayes, Patrick J. (1985). &amp;quot;The second naive
physics manifesto.&amp;quot; In Hobbs and Moore
(1985), 1-36. Reprinted in Brachman,
Ronald J. and Levesque, Hector J. (eds.)
Readings in knowledge representation. Los
Altos, CA: Morgan Kaufmann, 468-485.
Heeman, Peter (1991). A system that
collaborates on referring expressions. Thesis,
Department of Computer Science,
University of Toronto (published as
technical report CSRI-251).
Heritage, John (1984). Garfinkel and
ethnomethodology. Cambridge, England:
Polity Press.
Hobbs, Jerry R. and Moore, Robert C. (eds.)
(1985). Formal theories of the commonsense
world. Norwood, NJ: Ablex.
Hobbs, Jerry R.; Blenko, Tom; Croft, Bill;
Hager, Greg; Kautz, Henry A.; Kube,
Paul; and Shoham, Yoav (1985).
&amp;quot;Commonsense summer: Final report.&amp;quot;
Report 85-35, Center for the Study of
Language and Information, Stanford
University.
Holub, Robert (1984). Reception theory: A
critical introduction. New York: Methuen.
Iser, Wolfgang (1974). The implied reader.
Baltimore: The Johns Hopkins University
Press.
Jefferson, Gail (1972). &amp;quot;Side sequences.&amp;quot; In:
Sudnow, David (ed.) Studies in social
interaction. New York: Free Press, 294-338.
Levinson, Stephen C. (1983). Pragmatics.
Cambridge, England: Cambridge
University Press (Cambridge textbooks in
linguistics).
Matthews, Anne (1991). &amp;quot;Deciphering
Victorian underwear and other seminars.&amp;quot;
New York Times Magazine, 10 February
1991, p. 42 et seq.
McRoy, Susan Weber (1989).
&amp;quot;Nonmonotonic reasoning in natural
language.&amp;quot; MS, Department of Computer
Science, University of Toronto.
McTear, Michael (1985). &amp;quot;Breakdown and
repair in naturally occurring conversation
and human-computer dialogue.&amp;quot; In:
Gilbert, G. Nigel and Heath, Christian
(eds.) Social action and artificial intelligence.
Aldershot: Gower (Surrey conferences on
sociological theory and method 3),
104-123.
Moore, Johanna D. (1989). &amp;quot;Responding to
&apos;huh?&apos;: Answering vaguely articulated
follow-up questions.&amp;quot; Proceedings of the
Conference on Human Factors in Computing
Systems, Austin.
O&apos;Connell, Daniel C.; Kowal, Sabine; and
Kaltenbacher, Erika (1990). &amp;quot;Turn-taking:
A critical analysis of the research
tradition.&amp;quot; Journal of Psycholinguistic
Research, 19(6), 345-373.
Perrault, C. Raymond and Allen, James F.
(1980). &amp;quot;A plan-based analysis of indirect
speech acts.&amp;quot; American Journal of
</reference>
<page confidence="0.981197">
226
</page>
<reference confidence="0.999459693181818">
Hirst Conversation Analysis
Computational Linguistics, 6(3-4), 167-182.
Regoczei, Stephen and Hirst, Graeme (1989).
&amp;quot;On &apos;Extracting knowledge from text&apos;:
Modelling the architecture of language
users.&amp;quot; Proceedings, Third European
Workshop on Knowledge Acquisition for
Knowledge-Based Systems, Paris, 196-211.
Also published as technical report
CSRI-225, Computer Systems Research
Institute, University of Toronto.
Ringle, Martin H. and Bruce, Bertram C.
(1982). &amp;quot;Conversation failure.&amp;quot; In:
Lehnert, Wendy G. and Ringle, Martin H.
(eds.) Strategies for natural language
processing. Hillsdale, NJ: Lawrence
Erlbaum Associates, 203-221.
Sacks, Harvey (1984). &amp;quot;On doing &apos;being
ordinary&apos;.&amp;quot; In: Atkinson, J. Maxwell and
Heritage, John C. (eds.) Structures of social
action: Studies in conversation analysis.
Cambridge, England: Cambridge
University Press (Studies in emotion and
social interaction).
Sacks, Harvey; Schegloff, Emanuel A.; and
Jefferson, Gail (1974). &amp;quot;A simplest
systematics for the organization of
turn-taking for conversation.&amp;quot; Language,
50, 1974,696-735.
Schegloff, Emanuel A. (1968). &amp;quot;Sequencing
in conversational openings.&amp;quot; American
Anthropologist, 70, 1075-1095.
Schegloff, Emanuel A. and Sacks, Harvey
(1973). &amp;quot;Opening up closings.&amp;quot; Semiotica,
7, 289-327.
Schegloff, Emanuel A.; Jefferson, Gail; and
Sacks, Harvey (1977). &amp;quot;The preference for
self-correction in the organization of
repair in conversation.&amp;quot; Language, 53(2),
361-382.
Stubbs, Michael (1983). Discourse analysis:
The sociolinguistic analysis of natural
language. Chicago: The University of
Chicago Press.
Suchman, Lucy A. (1987). Plans and situated
actions: The problem of human—machine
communication. Cambridge, England:
Cambridge University Press.
Svartvik, Jan and Quirk, Randolph (eds.)
(1980). A corpus of English conversation.
Lund: C W K Gleerup (Lund studies in
English 56).
Tennant, Harry R.; Ross, Kenneth M.; and
Thompson, Craig W. (1983a). &amp;quot;Usable
natural language interfaces through
menu-based natural language
understanding.&amp;quot; Proceedings of the
Conference on Human Factors in Computing
Systems, 1983,154-160.
Tennant, Harry R.; Ross, Kenneth M.; Saenz,
Richard M.; Thompson, Craig W.; and
Miller, James R. (1983b). &amp;quot;Menu-based
natural language understanding.&amp;quot;
Proceedings, 21st Annual Meeting of the
Association for Computational Linguistics,
Cambridge, MA, 151-158.
Valian, Virginia and Wales, Roger (1976).
&amp;quot;What&apos;s what: Talkers help listeners hear
and understand by clarifying sentential
relations.&amp;quot; Cognition, 4, 155-176.
Weizenbaum, Joseph (1966). &amp;quot;ELIZA: A
computer program for the study of
natural language communication between
man and machine.&amp;quot; Communications of the
ACM, 9(1), 36-45.
West, Candace (1984). &amp;quot;Medical misfires:
Misgivings, mishearings, and
misunderstandings in physician—patient
dialogues.&amp;quot; Discourse Processes, 7(2),
107-134. A revised version appears in:
West, Candace. Routine complications:
Troubles with talk between doctors and
patient. Bloomington, IN: Indiana
University Press, 1984.
Graeme Hirst is the author of Semantic interpretation and the resolution of ambiguity (Cambridge
University Press, 1987). His current research concerns repair and reconstruction in human—
machine conversations. Hirst&apos;s address is: Department of Computer Science, University of
Toronto, Toronto, Canada M5S 1A4; e-mail: gh@cs.toronto.edu
</reference>
<page confidence="0.997931">
227
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.035595">
<title confidence="0.959309">Review Article Does Conversation Analysis Have a Role in Computational Linguistics? Computers and Conversation</title>
<author confidence="0.988628">Paul Luff</author>
<author confidence="0.988628">Nigel Gilbert</author>
<author confidence="0.988628">David Frohlich</author>
<affiliation confidence="0.725008">(University of Surrey)</affiliation>
<note confidence="0.8759526">London: Academic Press, 1990, 284 pp. (Computers and People Series) Hardbound, ISBN 0-12-459560-X, $45.00, £23.95 Reviewed by</note>
<author confidence="0.996583">Graeme Hirst</author>
<affiliation confidence="0.995406">University of Toronto</affiliation>
<abstract confidence="0.955376276119403">1. Conversation Analysis Computational linguists are a vicious warrior tribe. Unconstrained by traditional disciplinary boundaries, they invade and plunder neighboring disciplines for attractive theories to incorporate into their own. The latest victim of these attacks is sociology— in particular, a branch of ethnomethodological sociolinguistics called Conversation (hereafter, book reviewed here, and Conversation, the first significant result of this. It is based on a symposium held at the University of Surrey in September 1989. Its purpose is to show what CA has to offer to the study of human–computer (hereafter, interaction in natural language. In this review article, I want to use the book to explore at some length the question of whether CA has indeed anything to say to computational linguistics. Up to now, almost all research in computational linguistics on dialogs between humans and machines has taken the approach of Discourse Analysis. Discourse Analysis concerns itself with talk at the level of linguistic constituents (Levinson 1983, pp. 286ff; Stubbs 1983, pp. 1, 9-10). The disfluencies of spoken language are ignored. Utterances are treated as meaningful units of language, and are analyzed with regard to their semantic content. So the human and the machine are both seen as rational agents whose utterances are always perfectly formed and fully in accordance with Gricean rules. These interlocutors like to spend their time seeking information from each other, testing each other&apos;s abilities in plan inference. Typical interchanges are the following: Human: 8:50 train to Montreal? railroad station is three blocks west on Main Street. But it&apos;s closed on weekends. In any case, the last train to Montreal ran in 1988. You should call Air Canada reservations at 1-800-AIR-LINE. Thank you for using Mr Askme. Have a nice day! C) 1991 Association for Computational Linguistics Computational Linguistics Volume 17, Number 2 the flageolet to the pump shaft by means of the compressor module. doesn&apos;t fit inside it. the blue one. Conversation Analysis, however, is a contrasting tradition. From its name, Conversation Analysis sounds like it should be the same thing as Discourse Analysis. In practice, however, the two are distinct. CA tends to view talk as a human noise-making behavior. It concerns itself with talk at the sound-stream level, without constituent analysis, and it worries about such questions as how people take turns in conversation, where they look when they speak, and when and how they interrupt each other (Goodwin, 1981, pp. 6-9). Boden (1990b, p. 248) remarks that &amp;quot;a better name for the field would, in fact, probably be something like &apos;interactional analysis,&apos; as everything in the interaction, from a quiet in-breath to the entire spatial and temporal organization of the scene, may be subject to analysis.&amp;quot; In Conversation Analysis, typical interchanges look more like these: much did you . in fact . see of the English department upstairs little little littlel hallo . I&apos;ve just *[s]* set out you* ((2 make some B: thanks A: I&apos;ve just boiled some water for having coffee cos I haven&apos;t had time for tea — *would* you like some *yes* ah cun take thi:s hand an li:ft it and it duh&apos;unt hurt mean when yuh ar:m up using yer other han:d= many tubes would you like sir . what&apos;s the price now eh with V.A.T. do you know eh I&apos;ll just work that out for you B: =Thanks (Pause of 10.0 seconds) pounds nineteen a tube sir nineteen is it= 1 From Svartvik and Quirk (1980, S.1.5: 836-848). I have omitted their extensive prosodic markings. 2 From Svartvik and Quirk (1980, S.1.4: 1-11), simplified. The asterisks mark simultaneous speech; e.g., you said at the same time as A&apos;s [s]. 3 From West (1984, p. 133). The colon indicates a lengthening of a sound, the equal-sign that the two utterances are linked without a discernible pause. 4 Levinson 1983, p. 305. 212 Hirst Conversation Analysis 7. (Silence) A: Huh? B: I didn&apos;t say anything.&apos; The difference between the two research traditions is perhaps best illustrated by the difference in the way they approach the matter of indirect speech acts. In Discourse Analysis and traditional computational linguistics, indirect speech acts are seen as an important phenomenon in discourse, and dealing with them requires considerable effort in plan recognition on the part of the comprehender (e.g., Perrault and Allen 1980). On the other hand, &amp;quot;on the CA view, the problem doesn&apos;t even exist&amp;quot; (Levinson p. 356); rather, the analysis is in terms of preferred responses (for details, see Levinson 1983, pp. 356ff). Perhaps the most important points that CA makes are that there is much more to human conversation than is studied in Discourse Analysis or theoretical linguistics, and that empirical research methods are required. But it&apos;s not immediately obvious that this has any consequences for HCI. After all, human conversation is spoken, whereas HCI, to the extent that it is linguistic at all, is generally textual, research in speech recognition notwithstanding. But on the other hand, both speech and HCI are highly interactive forms of communication. Perhaps, as Frohlich and Luff argue in their chapter of the book, that&apos;s what really matters. For example, one area of particular concern to CA is turn-taking and interruptions. CA studies how it is that speakers signal to others either that they have finished their or that they wish to &apos;retain the floor&apos; for another construction unit; it studies the circumstances under which speakers will interrupt one another. In presentday human—machine dialogues, such matters are simply not an issue. Users signal the of their turn by means such as hitting the the machine signals by displaying some prompt to the user, usually a character such as &apos;*&apos; or &apos;&gt;&apos;. Neither side can interrupt the other. The suggestion of this book is that this is not how things should be. Since elements like turn-taking cues and interruptions are so important in human conversation, if we want to build natural, flexible interfaces, these matters should also be an issue in HCI. Similarly, Pirkko Raudaskoski, in her chapter, contrasts conventional research in HCI on recovery from misunderstandings with that in CA. The HCI research emphasizes the analysis and classification of the failure (cf. Ringle and Bruce 1982, Goodman 1986), while CA emphasizes the conversational patterns (e.g., Sacks et al. 1974) and the active role of both interlocutors in negotiating a meaning (cf. Clark and Wilkes-Gibbs 1986, Heeman 1991). Raudaskoski sees value in a synthesis of the two approaches. And Hugh Robinson points out in a short chapter that HCI would get more respect within the field of software engineering if it used CA to make itself a little less airy-fairy 2. The Intellectual Background of Conversation Analysis Analysis follows the tenets of radical movement that developed in sociology in the early 1960s under the leadership of Harold Garfinkel of the University of California, Los Angeles. In reaction to conventional sociology, ethnomethodology stresses the questions of what the foundations of social interaction 5 Schegloff, quoted by West (1984). 213 Computational Linguistics Volume 17, Number 2 are, and how people interpret their everyday social experience: My purposes ... are to demonstrate the essential relevance, to sociological enquiries, of a concern for common sense activities, as a topic of enquiry in its own right ... : the socially standardized and standardizing, &amp;quot;seen but unnoticed,&amp;quot; expected background features of everyday scenes. (Garfinkel 1967, p. 36) Ethnomethodology thus places great emphasis on its being descriptive, empirical work, detailed and (purportedly) atheoretical: a researcher&apos;s task is to gather data and avoid theoretically loaded interpretations. One &amp;quot;seen but unnoticed&amp;quot; object of study is the mechanism of ordinary conversation: not formal discourse or written text, but common phatic chit-chat and utilitarian exchanges. Conversation Analysis arose in the 1960s from the work of Harvey Sacks, and is associated with Emanuel Schegloff and Gail Jefferson in the U.S. and with John Heritage and J. Maxwell Atkinson in the U.K. For a balanced, easy-to-read introduc-</abstract>
<intro confidence="0.578909">tion to CA that emphasizes the linguistic issues, see Levinson (1983, ch. 6); for an</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shoshana Blum-Kulka</author>
<author>Weizman</author>
</authors>
<title>The inevitability of misunderstanding: Discourse ambiguities.&amp;quot;</title>
<date>1988</date>
<journal>Text,</journal>
<volume>8</volume>
<issue>3</issue>
<pages>219--241</pages>
<location>Elda</location>
<contexts>
<context position="36261" citStr="Blum-Kulka and Weizman (1988)" startWordPosition="5952" endWordPosition="5955">. We can either conclude that he broke the rule or we can infer that, via the use of the rule, he was seeking to treat the other as an acquaintance. The second interpretation is more likely when, for example, our man is greeting a new colleague at the office. (Heritage 1984, p. 126, emphasis supplied) (Obviously, there are limits to this; you can&apos;t make a 17-year-old eligible to vote simply by singing Happy Birthday to them, nor can you kill someone just by holding a memorial service for them.) Now, from this, Button tries to argue that CA&apos;s rules of conversation are not 7 The example is from Blum-Kulka and Weizman (1988). 8 For example, in one of Garfinkel&apos;s experiments (1967, pp. 79ff), students who sought advice from a counselor were given completely random answers to their yes/no questions. &amp;quot;Over the course of the exchange, subjects sometimes started with the [random] reply as an answer and altered the previous sense of their question to accommodate ... the reply as the answer to the retrospectively revised question&amp;quot; (p. 90). (This kind of revision is covert, and is not to be confused with the overt interactional reconstruction of an utterance that can follow a misunderstanding in conversation (Fox 1987, M</context>
</contexts>
<marker>Blum-Kulka, Weizman, 1988</marker>
<rawString>Blum-Kulka, Shoshana and Weizman, Elda (1988). &amp;quot;The inevitability of misunderstanding: Discourse ambiguities.&amp;quot; Text, 8(3), 219-241.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Boden</author>
</authors>
<title>Deirdre (1990a). &amp;quot;The world as it happens: Ethnomethodology and conversation analysis.&amp;quot;</title>
<booktitle>Frontiers of social theory.</booktitle>
<pages>185--213</pages>
<editor>In: Ritzer, George (ed.)</editor>
<publisher>Columbia University Press,</publisher>
<location>New York:</location>
<marker>Boden, </marker>
<rawString>Boden, Deirdre (1990a). &amp;quot;The world as it happens: Ethnomethodology and conversation analysis.&amp;quot; In: Ritzer, George (ed.) Frontiers of social theory. New York: Columbia University Press, 185-213.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Boden</author>
</authors>
<title>Deirdre (1990b). &amp;quot;People are talking: Conversation analysis and symbolic interaction.&amp;quot;</title>
<booktitle>Symbolic interaction and cultural studies.</booktitle>
<pages>244--273</pages>
<editor>In: Becker, Howard S. and McCall, Michal (eds.)</editor>
<publisher>The University of Chicago Press,</publisher>
<location>Chicago:</location>
<marker>Boden, </marker>
<rawString>Boden, Deirdre (1990b). &amp;quot;People are talking: Conversation analysis and symbolic interaction.&amp;quot; In: Becker, Howard S. and McCall, Michal (eds.) Symbolic interaction and cultural studies. Chicago: The University of Chicago Press, 244-273.</rawString>
</citation>
<citation valid="false">
<title>A belief revision model of repair sequences in dialogue.&amp;quot;</title>
<editor>Cawsey, Alison J. (in press).</editor>
<publisher>Springer-Verlag.</publisher>
<marker></marker>
<rawString>Cawsey, Alison J. (in press). &amp;quot;A belief revision model of repair sequences in dialogue.&amp;quot; In: Costa, Ernesto (ed.) New directions in intelligent tutoring systems. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
<author>Deanna Wilkes-Gibbs</author>
</authors>
<title>Referring as a collaborative process.&amp;quot;</title>
<date>1986</date>
<journal>Cognition,</journal>
<volume>22</volume>
<issue>1</issue>
<pages>1--39</pages>
<note>Reprinted in: Cohen, Morgan, and Pollack</note>
<contexts>
<context position="7254" citStr="Clark and Wilkes-Gibbs 1986" startWordPosition="1173" endWordPosition="1176">ld be. Since elements like turn-taking cues and interruptions are so important in human conversation, if we want to build natural, flexible interfaces, these matters should also be an issue in HCI. Similarly, Pirkko Raudaskoski, in her chapter, contrasts conventional research in HCI on recovery from misunderstandings with that in CA. The HCI research emphasizes the analysis and classification of the failure (cf. Ringle and Bruce 1982, Goodman 1986), while CA emphasizes the conversational patterns (e.g., Sacks et al. 1974) and the active role of both interlocutors in negotiating a meaning (cf. Clark and Wilkes-Gibbs 1986, Heeman 1991). Raudaskoski sees value in a synthesis of the two approaches. And Hugh Robinson points out in a short chapter that HCI would get more respect within the field of software engineering if it used CA to make itself a little less airy-fairy 2. The Intellectual Background of Conversation Analysis Conversation Analysis follows the tenets of ethnomethodology, a radical movement that developed in sociology in the early 1960s under the leadership of Harold Garfinkel of the University of California, Los Angeles. In reaction to conventional sociology, ethnomethodology stresses the question</context>
</contexts>
<marker>Clark, Wilkes-Gibbs, 1986</marker>
<rawString>Clark, Herbert H. and Wilkes-Gibbs, Deanna (1986). &amp;quot;Referring as a collaborative process.&amp;quot; Cognition, 22(1), 1-39. Reprinted in: Cohen, Morgan, and Pollack (1990), 463-493.</rawString>
</citation>
<citation valid="true">
<title>Intentions in communication.</title>
<date>1990</date>
<editor>Cohen, Philip R.; Morgan, Jerry; and Pollack, Martha E. (eds.)</editor>
<publisher>The MIT Press</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="13976" citStr="(1990)" startWordPosition="2205" endWordPosition="2205"> interactional time, constitutes that moment and shapes that interaction. (Boden 1990b, pp. 250-251) This research is not without its critics of course. Goldthorpe, in his review of early ethnomethodology (1973), finds little if anything to prefer over conventional sociology. He complains of ethnomethodology&apos;s impenetrable jargon, and this can certainly be seen in the CA literature—e.g., transition relevance place instead of potential end of utterance. Presumably, such locutions are intended to distance one from theoretically loaded interpretations of the data. More recently, O&apos;Connell et al. (1990) have presented a critique of CA research on turn-taking in conversations, a critique so vehement and unrelenting that one is left with the impression that the authors believe that no Conversation Analyst has ever uttered a true word on that topic or any other. The criticisms are largely aimed at the assumptions and methodology of the research. Perhaps the most important, however, is that CA completely ignores the goals and purposes of speakers in a conversation. Even if this is true, it&apos;s not clear to me that it matters, at least for CA. A speaker&apos;s intents are, after all, not open to direct </context>
<context position="44708" citStr="(1990)" startWordPosition="7351" endWordPosition="7351">sponding with &amp;quot;set pieces&amp;quot; (p. 82), by which he apparently means canned responses. As I mentioned before, it&apos;s unclear whether the Advice System&apos;s responses are just pre-stored text or dynamically generated; but even if the responses are canned, if it turns out that informants judge the system to be easy or &apos;natural&apos; to use, if the response selected is always judged to be suitable, even in a variety of complex conversational situations, then something interesting is going on. While Button&apos;s criticism is, in effect, that CA is too rich and fine for HCI, the criticisms of CA by O&apos;Connell et al. (1990) imply the converse: that by ignoring communicative intent, CA is too impoverished for use in HCI. On this view, CA is nothing more than a kind of linguistic behaviorism. Unless its descriptions of external behavior can be related to the intents and purposes of speakers, CA can be used neither for understanding intelligent agents nor for constructing them. I see several replies to this. The first is to grant all this, but to point out that these enterprises do, nonetheless, require these descriptions of external behavior, and CA has them available. The challenge is then to use them. 223 Comput</context>
</contexts>
<marker>1990</marker>
<rawString>Cohen, Philip R.; Morgan, Jerry; and Pollack, Martha E. (eds.) (1990). Intentions in communication. Cambridge, MA: The MIT Press (System Development Foundation benchmark series).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Pierre Corriveau</author>
</authors>
<title>Time-constrained memory for reader-based text comprehension.</title>
<date>1991</date>
<institution>Department of Computer Science, University of Toronto</institution>
<note>Doctoral dissertation,</note>
<contexts>
<context position="46104" citStr="Corriveau 1991" startWordPosition="7576" endWordPosition="7577">er agents. Rather, meaning can be seen as no more than the reader&apos;s response to a text, or, more generally, the perceiver&apos;s response to perception. This sits well with the notion mentioned above of a response being able to transform a first utterance without regard to the intent of its speaker, and is supported by deconstructionist and reader-response theories in literary studies (e.g., Iser 1974; see also Holub 1984). These theories are highly controversial, and are a current Hot Topic in literary studies (Matthews 1991). But, as Corriveau&apos;s reader-response model of text understanding shows (Corriveau 1991), they are not necessarily incompatible with artificial intelligence. The third reply is to deny the other premise, and claim that intent is indeed accounted for in CA—in fact, is pervasive in CA. When CA uses terms such as repair or closing or summons, or speaks of conversants using rules as resources, it is, in fact, explicitly ascribing intent! When analysts say that a particular utterance is, say, a next turn repair initiator, they are using their own knowledge of the language of the conversation to interpret that utterance in context, and hence they are indeed determining the speaker&apos;s in</context>
</contexts>
<marker>Corriveau, 1991</marker>
<rawString>Corriveau, Jean-Pierre (1991). Time-constrained memory for reader-based text comprehension. Doctoral dissertation, Department of Computer Science, University of Toronto (published as technical report CSRI-246).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Fox</author>
</authors>
<title>Interactional reconstruction in real-time language processing.&amp;quot;</title>
<date>1987</date>
<journal>Cognitive Science,</journal>
<volume>11</volume>
<issue>3</issue>
<pages>365--387</pages>
<contexts>
<context position="36858" citStr="Fox 1987" startWordPosition="6049" endWordPosition="6050">zman (1988). 8 For example, in one of Garfinkel&apos;s experiments (1967, pp. 79ff), students who sought advice from a counselor were given completely random answers to their yes/no questions. &amp;quot;Over the course of the exchange, subjects sometimes started with the [random] reply as an answer and altered the previous sense of their question to accommodate ... the reply as the answer to the retrospectively revised question&amp;quot; (p. 90). (This kind of revision is covert, and is not to be confused with the overt interactional reconstruction of an utterance that can follow a misunderstanding in conversation (Fox 1987, McRoy 1989, Cawsey forthcoming)). 221 Computational Linguistics Volume 17, Number 2 amenable to direct use by computer, that they are not &amp;quot;codifiable&amp;quot; or &amp;quot;reduc[ible1 to an algorithm&amp;quot; (p. 82). His first argument is that rules in a computer are causal, meaning that they are designed into the hardware itself (p. 77), or into the program it executes. This is, I gather, intended to contrast with the use of rules as resources, as described above. However, I am at a loss to see the contradiction. Any backward-chaining Horn clause theorem prover or planner with appropriate meta-rules can be viewed </context>
</contexts>
<marker>Fox, 1987</marker>
<rawString>Fox, Barbara (1987). &amp;quot;Interactional reconstruction in real-time language processing.&amp;quot; Cognitive Science, 11(3), 365-387.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold Garfinkel</author>
</authors>
<title>Studies in ethnomethodology. Englewood Cliffs,</title>
<date>1967</date>
<publisher>Prentice Hall.</publisher>
<location>NJ:</location>
<note>in association with</note>
<contexts>
<context position="8357" citStr="Garfinkel 1967" startWordPosition="1341" endWordPosition="1342">rsity of California, Los Angeles. In reaction to conventional sociology, ethnomethodology stresses the questions of what the foundations of social interaction 5 Schegloff, quoted by West (1984). 213 Computational Linguistics Volume 17, Number 2 are, and how people interpret their everyday social experience: My purposes ... are to demonstrate the essential relevance, to sociological enquiries, of a concern for common sense activities, as a topic of enquiry in its own right ... : the socially standardized and standardizing, &amp;quot;seen but unnoticed,&amp;quot; expected background features of everyday scenes. (Garfinkel 1967, p. 36) Ethnomethodology thus places great emphasis on its being descriptive, empirical work, detailed and (purportedly) atheoretical: a researcher&apos;s task is to gather data and avoid theoretically loaded interpretations. One &amp;quot;seen but unnoticed&amp;quot; object of study is the mechanism of ordinary conversation: not formal discourse or written text, but common phatic chit-chat and utilitarian exchanges. Conversation Analysis arose in the 1960s from the work of Harvey Sacks, and is associated with Emanuel Schegloff and Gail Jefferson in the U.S. and with John Heritage and J. Maxwell Atkinson in the U.K</context>
<context position="33558" citStr="Garfinkel 1967" startWordPosition="5482" endWordPosition="5483">nstructed&amp;quot; (ibid.). The chapter closes with some vague pieces of design guidance for HCI from CA, the authors admitting that determining how or whether they can be applied will require the development of a much better 220 Hirst Conversation Analysis understanding of human—computer interaction. The opposing case is put by Graham Button, in a rather long and repetitious paper. He says that one might at best be able to produce a &amp;quot;simulacrum&amp;quot; (p. 67) of a naturally occurring conversation. For Button (in the ethnomethodological tradition), talk is an &amp;quot;achievement&amp;quot; (p. 79) or an accomplishment (cf. Garfinkel 1967, p. 1) that uses the rules of CA, but is not caused by them. His argument is that the rules of CA are not like the rules of a computer model: CA rules for sequencing, turn-taking, and the like might look like causal rules at first glance, but actually they are &amp;quot;resources&amp;quot; (p. 84) that people &amp;quot;display an orientation to&amp;quot; (p. 78) in situated instances, namely social interactions. Button&apos;s argument can be understood in two slightly different ways, both of which are, I think, valid in CA. The first way is this: A conversant, desiring a particular effect, can choose to invoke one of the rules that </context>
<context position="35195" citStr="Garfinkel 1967" startWordPosition="5771" endWordPosition="5772">at it is the response that provides for recognition of the first utterance, and sees this as inherently incompatible with rule-based behavior: If we formulated a rule that upon the production of a question, the next utterance should be an answer, we could not have the case where a second transforms a prior utterance into a first and thereby achieves coherence with the first. (p. 85, emphasis added) Indeed, any aspect of an interaction can be &amp;quot;retrospectively reread to find out in the light of present practical circumstances what [it] &apos;really&apos; consisted of &apos;in the first place&apos; and &apos;all along&amp;quot; (Garfinkel 1967, p. 74).8 The second interpretation is this: A conversant, desiring a particular situation, can quite literally create the desired situation by invoking a rule that would be appropriate only therein. For example, Suppose there is a rule for greetings which runs to the effect: do not initiate greetings except with persons who are acquaintances. And suppose we subsequently see a man greeting another whom we know is not an acquaintance. We can either conclude that he broke the rule or we can infer that, via the use of the rule, he was seeking to treat the other as an acquaintance. The second int</context>
<context position="39322" citStr="Garfinkel 1967" startWordPosition="6448" endWordPosition="6449">places being formulated in terms of the rules for their production. Possible transition places are the contextual achievement of the unfolding structuring of the turn in progress. Possible completion places are not, then, provided for in advance by codifiable rules, they are situatedly achieved in and for a particular turn. (pp. 80-81, emphasis supplied) Button&apos;s intent here is not, as it might seem, to claim that CA chooses to remain willfully ignorant about certain elements of its domain; rather, it is, presumably, to tacitly invoke what&apos;s called in ethnomethodology the et cetera principle (Garfinkel 1967, pp. 73-75): that the situations in which a rule is and isn&apos;t applicable can never be finitely enumerated. In effect, every rule has an implicit et cetera at the end of its list of triggering conditions, and the agent must judge each time whether the current situation is one in which the rule may be applied, or, possibly, broken.&apos; Heritage (1984, pp. 121-122) makes an analogy with the legal system. No matter how carefully a law is written, a complex judicial decision will sometimes be required to determine whether or how the law applies in some particular new situation; and then another judic</context>
</contexts>
<marker>Garfinkel, 1967</marker>
<rawString>Garfinkel, Harold (1967). Studies in ethnomethodology. Englewood Cliffs, NJ: Prentice Hall. (Reprinted: Cambridge, England: Polity Press, in association with Basil Blackwell, 1984.)</rawString>
</citation>
<citation valid="true">
<authors>
<author>John H Goldthorpe</author>
</authors>
<title>A revolution in sociology?&amp;quot;</title>
<date>1973</date>
<journal>Sociology,</journal>
<volume>7</volume>
<issue>3</issue>
<pages>449--462</pages>
<marker>Goldthorpe, 1973</marker>
<rawString>Goldthorpe, John H. (1973). &amp;quot;A revolution in sociology?&amp;quot; Sociology, 7(3), 449-462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley A Goodman</author>
</authors>
<title>Reference identification and reference identification failures.&amp;quot;</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<volume>12</volume>
<issue>4</issue>
<pages>273--305</pages>
<contexts>
<context position="7079" citStr="Goodman 1986" startWordPosition="1148" endWordPosition="1149">prompt to the user, usually a character such as &apos;*&apos; or &apos;&gt;&apos;. Neither side can interrupt the other. The suggestion of this book is that this is not how things should be. Since elements like turn-taking cues and interruptions are so important in human conversation, if we want to build natural, flexible interfaces, these matters should also be an issue in HCI. Similarly, Pirkko Raudaskoski, in her chapter, contrasts conventional research in HCI on recovery from misunderstandings with that in CA. The HCI research emphasizes the analysis and classification of the failure (cf. Ringle and Bruce 1982, Goodman 1986), while CA emphasizes the conversational patterns (e.g., Sacks et al. 1974) and the active role of both interlocutors in negotiating a meaning (cf. Clark and Wilkes-Gibbs 1986, Heeman 1991). Raudaskoski sees value in a synthesis of the two approaches. And Hugh Robinson points out in a short chapter that HCI would get more respect within the field of software engineering if it used CA to make itself a little less airy-fairy 2. The Intellectual Background of Conversation Analysis Conversation Analysis follows the tenets of ethnomethodology, a radical movement that developed in sociology in the e</context>
</contexts>
<marker>Goodman, 1986</marker>
<rawString>Goodman, Bradley A. (1986). &amp;quot;Reference identification and reference identification failures.&amp;quot; Computational Linguistics, 12(4), 273-305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Goodwin</author>
</authors>
<title>Conversational organization: Interaction between speakers and hearers.</title>
<date>1981</date>
<publisher>Academic Press</publisher>
<location>New York:</location>
<contexts>
<context position="3042" citStr="Goodwin, 1981" startWordPosition="465" endWordPosition="466">ft by means of the compressor module. Human: It doesn&apos;t fit inside it. Computer: No, the blue one. Conversation Analysis, however, is a contrasting tradition. From its name, Conversation Analysis sounds like it should be the same thing as Discourse Analysis. In practice, however, the two are distinct. CA tends to view talk as a human noise-making behavior. It concerns itself with talk at the sound-stream level, without constituent analysis, and it worries about such questions as how people take turns in conversation, where they look when they speak, and when and how they interrupt each other (Goodwin, 1981, pp. 6-9). Boden (1990b, p. 248) remarks that &amp;quot;a better name for the field would, in fact, probably be something like &apos;interactional analysis,&apos; as everything in the interaction, from a quiet in-breath to the entire spatial and temporal organization of the scene, may be subject to analysis.&amp;quot; In Conversation Analysis, typical interchanges look more like these: 3. A: How much did you . in fact . see of the English department upstairs C: Very little A: Very little C: Very littlel 4. A: Richard hallo . I&apos;ve just *[s]* set out B: *thank you* A: ((2 syllables)) make some B: thanks A: I&apos;ve just boile</context>
</contexts>
<marker>Goodwin, 1981</marker>
<rawString>Goodwin, Charles (1981). Conversational organization: Interaction between speakers and hearers. New York: Academic Press (Language, thought, and culture: Advances in the study of cognition).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick J Hayes</author>
</authors>
<title>The second naive physics manifesto.&amp;quot;</title>
<date>1985</date>
<booktitle>In Hobbs and Moore</booktitle>
<pages>1--36</pages>
<editor>Brachman, Ronald J. and Levesque, Hector J. (eds.)</editor>
<publisher>Morgan Kaufmann,</publisher>
<location>Los Altos, CA:</location>
<note>Reprinted in</note>
<contexts>
<context position="51224" citStr="Hayes 1985" startWordPosition="8433" endWordPosition="8434">that the role of CA in computational linguistics and HCI is definitely worth exploring, and the core of my criticisms is that the faltering first steps in this direction often falter more than they reasonably ought to. There is a sense in which it is clear that CA must have a role in NLU, because there is a sense in which ethnomethodology is just a small subfield of artificial intelligence (although that might come as a surprise to the ethnomethodologists), as its goal is simply to explicate and represent some areas of commonsense knowledge, rather like naïve physics and similar projects (cf. Hayes 1985; Hobbs and Moore 1985; Hobbs et al. 1985). Obviously, it is too early to say that the results of Conversation Analysis research do have a place in the design of human—computer interaction. Button may yet be proved right. But the work described in this book is reason to be optimistic that a valuable synthesis will emerge. And if it doesn&apos;t, determining why not would be equally valuable. Acknowledgments I am grateful to Deirdre Boden, Richard Frankel, and Tom Smith for discussions that shaped my understanding of ethnomethodology and Conversation Analysis, and for pointers to relevant literature</context>
</contexts>
<marker>Hayes, 1985</marker>
<rawString>Hayes, Patrick J. (1985). &amp;quot;The second naive physics manifesto.&amp;quot; In Hobbs and Moore (1985), 1-36. Reprinted in Brachman, Ronald J. and Levesque, Hector J. (eds.) Readings in knowledge representation. Los Altos, CA: Morgan Kaufmann, 468-485.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Heeman</author>
</authors>
<title>A system that collaborates on referring expressions.</title>
<date>1991</date>
<tech>Thesis,</tech>
<institution>Department of Computer Science, University of Toronto</institution>
<note>(published as technical report CSRI-251).</note>
<contexts>
<context position="7268" citStr="Heeman 1991" startWordPosition="1177" endWordPosition="1178">rn-taking cues and interruptions are so important in human conversation, if we want to build natural, flexible interfaces, these matters should also be an issue in HCI. Similarly, Pirkko Raudaskoski, in her chapter, contrasts conventional research in HCI on recovery from misunderstandings with that in CA. The HCI research emphasizes the analysis and classification of the failure (cf. Ringle and Bruce 1982, Goodman 1986), while CA emphasizes the conversational patterns (e.g., Sacks et al. 1974) and the active role of both interlocutors in negotiating a meaning (cf. Clark and Wilkes-Gibbs 1986, Heeman 1991). Raudaskoski sees value in a synthesis of the two approaches. And Hugh Robinson points out in a short chapter that HCI would get more respect within the field of software engineering if it used CA to make itself a little less airy-fairy 2. The Intellectual Background of Conversation Analysis Conversation Analysis follows the tenets of ethnomethodology, a radical movement that developed in sociology in the early 1960s under the leadership of Harold Garfinkel of the University of California, Los Angeles. In reaction to conventional sociology, ethnomethodology stresses the questions of what the </context>
</contexts>
<marker>Heeman, 1991</marker>
<rawString>Heeman, Peter (1991). A system that collaborates on referring expressions. Thesis, Department of Computer Science, University of Toronto (published as technical report CSRI-251).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Heritage</author>
</authors>
<title>Garfinkel and ethnomethodology.</title>
<date>1984</date>
<publisher>Polity Press.</publisher>
<location>Cambridge, England:</location>
<contexts>
<context position="9159" citStr="Heritage (1984" startWordPosition="1461" endWordPosition="1462">retically loaded interpretations. One &amp;quot;seen but unnoticed&amp;quot; object of study is the mechanism of ordinary conversation: not formal discourse or written text, but common phatic chit-chat and utilitarian exchanges. Conversation Analysis arose in the 1960s from the work of Harvey Sacks, and is associated with Emanuel Schegloff and Gail Jefferson in the U.S. and with John Heritage and J. Maxwell Atkinson in the U.K. For a balanced, easy-to-read introduction to CA that emphasizes the linguistic issues, see Levinson (1983, ch. 6); for an introduction that emphasizes the ethnomethodological roots, see Heritage (1984, ch. 8) or Boden (1990a). Sacks believed deeply that the world is actually a very orderly place if one just looks closely enough. As Robin Wooffitt says in his introduction to CA in Computers and Conversation: A fundamental assumption informing CA is that ordinary talk is a highly organized, orderly phenomenon. The goal of analysis is to reveal the basis of this phenomenon. ... [Sacks] did not treat an utterance as something that just happened to be said that way. Rather, he worked on the assumption that the things people said were methodically designed. (pp. 10-12) [In CA,] words used in tal</context>
<context position="35906" citStr="Heritage 1984" startWordPosition="5892" endWordPosition="5893">uite literally create the desired situation by invoking a rule that would be appropriate only therein. For example, Suppose there is a rule for greetings which runs to the effect: do not initiate greetings except with persons who are acquaintances. And suppose we subsequently see a man greeting another whom we know is not an acquaintance. We can either conclude that he broke the rule or we can infer that, via the use of the rule, he was seeking to treat the other as an acquaintance. The second interpretation is more likely when, for example, our man is greeting a new colleague at the office. (Heritage 1984, p. 126, emphasis supplied) (Obviously, there are limits to this; you can&apos;t make a 17-year-old eligible to vote simply by singing Happy Birthday to them, nor can you kill someone just by holding a memorial service for them.) Now, from this, Button tries to argue that CA&apos;s rules of conversation are not 7 The example is from Blum-Kulka and Weizman (1988). 8 For example, in one of Garfinkel&apos;s experiments (1967, pp. 79ff), students who sought advice from a counselor were given completely random answers to their yes/no questions. &amp;quot;Over the course of the exchange, subjects sometimes started with th</context>
<context position="39670" citStr="Heritage (1984" startWordPosition="6509" endWordPosition="6510">upplied) Button&apos;s intent here is not, as it might seem, to claim that CA chooses to remain willfully ignorant about certain elements of its domain; rather, it is, presumably, to tacitly invoke what&apos;s called in ethnomethodology the et cetera principle (Garfinkel 1967, pp. 73-75): that the situations in which a rule is and isn&apos;t applicable can never be finitely enumerated. In effect, every rule has an implicit et cetera at the end of its list of triggering conditions, and the agent must judge each time whether the current situation is one in which the rule may be applied, or, possibly, broken.&apos; Heritage (1984, pp. 121-122) makes an analogy with the legal system. No matter how carefully a law is written, a complex judicial decision will sometimes be required to determine whether or how the law applies in some particular new situation; and then another judicial decision might be required to determine whether a second situation is covered by the precedent established by the first. But again, this is hardly an in-principle argument against computer use of CA rules. Case-based reasoning, while far from being a completed science, is nevertheless a well-established subfield of artificial intelligence, an</context>
<context position="46929" citStr="Heritage 1984" startWordPosition="7709" endWordPosition="7710">rms such as repair or closing or summons, or speaks of conversants using rules as resources, it is, in fact, explicitly ascribing intent! When analysts say that a particular utterance is, say, a next turn repair initiator, they are using their own knowledge of the language of the conversation to interpret that utterance in context, and hence they are indeed determining the speaker&apos;s intent. Perhaps the strongest use of intent in CA occurs in the notion of accountability: in interaction, agents hold each other accountable, or responsible, for their choice of actions and inactions. For example (Heritage 1984, pp. 106ff), consider again the rules about greetings. If two acquaintances approach each other in the hallway and the first greets the second, the second can choose either to return the greeting, and thereby be &amp;quot;friendly&amp;quot; or &amp;quot;polite,&amp;quot; or not to, and thereby be &amp;quot;impolite&amp;quot; or &amp;quot;snub&amp;quot; the first. But he or she cannot just opt out and avoid making the choice, or &amp;quot;whatever they do will be intelligible and accountable as a sustaining of, or a development or violation, etc., of, some order of activity. ... [Their actions] are condemned to be meaningful&amp;quot; (Heritage 1984, p. 110, second emphasis added).</context>
</contexts>
<marker>Heritage, 1984</marker>
<rawString>Heritage, John (1984). Garfinkel and ethnomethodology. Cambridge, England: Polity Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
<author>Robert C Moore</author>
</authors>
<date>1985</date>
<booktitle>Formal theories of the commonsense world.</booktitle>
<publisher>Ablex.</publisher>
<location>Norwood, NJ:</location>
<contexts>
<context position="51246" citStr="Hobbs and Moore 1985" startWordPosition="8435" endWordPosition="8438">e of CA in computational linguistics and HCI is definitely worth exploring, and the core of my criticisms is that the faltering first steps in this direction often falter more than they reasonably ought to. There is a sense in which it is clear that CA must have a role in NLU, because there is a sense in which ethnomethodology is just a small subfield of artificial intelligence (although that might come as a surprise to the ethnomethodologists), as its goal is simply to explicate and represent some areas of commonsense knowledge, rather like naïve physics and similar projects (cf. Hayes 1985; Hobbs and Moore 1985; Hobbs et al. 1985). Obviously, it is too early to say that the results of Conversation Analysis research do have a place in the design of human—computer interaction. Button may yet be proved right. But the work described in this book is reason to be optimistic that a valuable synthesis will emerge. And if it doesn&apos;t, determining why not would be equally valuable. Acknowledgments I am grateful to Deirdre Boden, Richard Frankel, and Tom Smith for discussions that shaped my understanding of ethnomethodology and Conversation Analysis, and for pointers to relevant literature. I am especially grat</context>
</contexts>
<marker>Hobbs, Moore, 1985</marker>
<rawString>Hobbs, Jerry R. and Moore, Robert C. (eds.) (1985). Formal theories of the commonsense world. Norwood, NJ: Ablex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
<author>Tom Blenko</author>
<author>Bill Croft</author>
<author>Greg Hager</author>
<author>Henry A Kautz</author>
<author>Paul Kube</author>
<author>Yoav Shoham</author>
</authors>
<title>Commonsense summer: Final report.&amp;quot;</title>
<date>1985</date>
<tech>Report 85-35,</tech>
<institution>Center for the Study of Language and Information, Stanford University.</institution>
<contexts>
<context position="51266" citStr="Hobbs et al. 1985" startWordPosition="8439" endWordPosition="8442">al linguistics and HCI is definitely worth exploring, and the core of my criticisms is that the faltering first steps in this direction often falter more than they reasonably ought to. There is a sense in which it is clear that CA must have a role in NLU, because there is a sense in which ethnomethodology is just a small subfield of artificial intelligence (although that might come as a surprise to the ethnomethodologists), as its goal is simply to explicate and represent some areas of commonsense knowledge, rather like naïve physics and similar projects (cf. Hayes 1985; Hobbs and Moore 1985; Hobbs et al. 1985). Obviously, it is too early to say that the results of Conversation Analysis research do have a place in the design of human—computer interaction. Button may yet be proved right. But the work described in this book is reason to be optimistic that a valuable synthesis will emerge. And if it doesn&apos;t, determining why not would be equally valuable. Acknowledgments I am grateful to Deirdre Boden, Richard Frankel, and Tom Smith for discussions that shaped my understanding of ethnomethodology and Conversation Analysis, and for pointers to relevant literature. I am especially grateful to Deirdre for </context>
</contexts>
<marker>Hobbs, Blenko, Croft, Hager, Kautz, Kube, Shoham, 1985</marker>
<rawString>Hobbs, Jerry R.; Blenko, Tom; Croft, Bill; Hager, Greg; Kautz, Henry A.; Kube, Paul; and Shoham, Yoav (1985). &amp;quot;Commonsense summer: Final report.&amp;quot; Report 85-35, Center for the Study of Language and Information, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Holub</author>
</authors>
<title>Reception theory: A critical introduction.</title>
<date>1984</date>
<publisher>Methuen.</publisher>
<location>New York:</location>
<contexts>
<context position="45910" citStr="Holub 1984" startWordPosition="7549" endWordPosition="7550">23 Computational Linguistics Volume 17, Number 2 The second reply is to deny the premise that intelligence and understanding require explicit analysis and representation of the intent of other agents. Rather, meaning can be seen as no more than the reader&apos;s response to a text, or, more generally, the perceiver&apos;s response to perception. This sits well with the notion mentioned above of a response being able to transform a first utterance without regard to the intent of its speaker, and is supported by deconstructionist and reader-response theories in literary studies (e.g., Iser 1974; see also Holub 1984). These theories are highly controversial, and are a current Hot Topic in literary studies (Matthews 1991). But, as Corriveau&apos;s reader-response model of text understanding shows (Corriveau 1991), they are not necessarily incompatible with artificial intelligence. The third reply is to deny the other premise, and claim that intent is indeed accounted for in CA—in fact, is pervasive in CA. When CA uses terms such as repair or closing or summons, or speaks of conversants using rules as resources, it is, in fact, explicitly ascribing intent! When analysts say that a particular utterance is, say, a</context>
</contexts>
<marker>Holub, 1984</marker>
<rawString>Holub, Robert (1984). Reception theory: A critical introduction. New York: Methuen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Iser</author>
</authors>
<title>The implied reader. Baltimore: The Johns Hopkins</title>
<date>1974</date>
<publisher>University Press.</publisher>
<contexts>
<context position="45888" citStr="Iser 1974" startWordPosition="7545" endWordPosition="7546"> then to use them. 223 Computational Linguistics Volume 17, Number 2 The second reply is to deny the premise that intelligence and understanding require explicit analysis and representation of the intent of other agents. Rather, meaning can be seen as no more than the reader&apos;s response to a text, or, more generally, the perceiver&apos;s response to perception. This sits well with the notion mentioned above of a response being able to transform a first utterance without regard to the intent of its speaker, and is supported by deconstructionist and reader-response theories in literary studies (e.g., Iser 1974; see also Holub 1984). These theories are highly controversial, and are a current Hot Topic in literary studies (Matthews 1991). But, as Corriveau&apos;s reader-response model of text understanding shows (Corriveau 1991), they are not necessarily incompatible with artificial intelligence. The third reply is to deny the other premise, and claim that intent is indeed accounted for in CA—in fact, is pervasive in CA. When CA uses terms such as repair or closing or summons, or speaks of conversants using rules as resources, it is, in fact, explicitly ascribing intent! When analysts say that a particula</context>
</contexts>
<marker>Iser, 1974</marker>
<rawString>Iser, Wolfgang (1974). The implied reader. Baltimore: The Johns Hopkins University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gail Jefferson</author>
</authors>
<title>Side sequences.&amp;quot;</title>
<date>1972</date>
<booktitle>Studies in social interaction.</booktitle>
<pages>294--338</pages>
<editor>In: Sudnow, David (ed.)</editor>
<publisher>Free Press,</publisher>
<location>New York:</location>
<contexts>
<context position="10686" citStr="Jefferson 1972" startWordPosition="1707" endWordPosition="1708">tterances, and by which they make sense of other people&apos;s talk: to describe the &apos;technology of conversation&apos; (Sacks 1984, p. 413). (p. 10) Conversation Analysts thus spend their time tape-recording or videotaping conversations like (3)-(7) above, and seeing what sort of structures they contain. Many conversational exchanges are found to be various kinds of adjacency pairs: a greeting followed by a greeting in response, a question followed by an answer, and so on. But variations are possible. For example, the response to a question might be another question, thereby initiating a side sequence (Jefferson 1972), as seen in dialog (6) above. If there is some sort of trouble with the initial utterance, the response might be a next turn repair initiator, as in (5) above. Throughout all, researchers working with everyday conversational materials have uncovered a veritable gold mine of ... precise and patterned procedures for producing talk that reveal, in their instantiation, the sort of fine-grained order in the social world that so amazed early naturalists in the nineteenth century as they began to systematically observe the natural environment. (Boden 1990b, pp. 252-253) Wooffitt says (p. 27) that su</context>
</contexts>
<marker>Jefferson, 1972</marker>
<rawString>Jefferson, Gail (1972). &amp;quot;Side sequences.&amp;quot; In: Sudnow, David (ed.) Studies in social interaction. New York: Free Press, 294-338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen C Levinson</author>
</authors>
<date>1983</date>
<publisher>Cambridge University Press</publisher>
<location>Pragmatics. Cambridge, England:</location>
<note>(Cambridge textbooks in linguistics).</note>
<contexts>
<context position="1466" citStr="Levinson 1983" startWordPosition="212" endWordPosition="213"> symposium held at the University of Surrey in September 1989. Its purpose is to show what CA has to offer to the study of human–computer interaction (hereafter, HCI), especially interaction in natural language. In this review article, I want to use the book to explore at some length the question of whether CA has indeed anything to say to computational linguistics. Up to now, almost all research in computational linguistics on dialogs between humans and machines has taken the approach of Discourse Analysis. Discourse Analysis concerns itself with talk at the level of linguistic constituents (Levinson 1983, pp. 286ff; Stubbs 1983, pp. 1, 9-10). The disfluencies of spoken language are ignored. Utterances are treated as meaningful units of language, and are analyzed with regard to their semantic content. So the human and the machine are both seen as rational agents whose utterances are always perfectly formed and fully in accordance with Gricean rules. These interlocutors like to spend their time seeking information from each other, testing each other&apos;s abilities in plan inference. Typical interchanges are the following: 1. Human: The 8:50 train to Montreal? Computer: The railroad station is thre</context>
<context position="4564" citStr="Levinson 1983" startWordPosition="744" endWordPosition="745"> the price now eh with V.A.T. do you know eh A: Er I&apos;ll just work that out for you B: =Thanks (Pause of 10.0 seconds) A: Three pounds nineteen a tube sir B: Three nineteen is it= A: =Yeah&apos; 1 From Svartvik and Quirk (1980, S.1.5: 836-848). I have omitted their extensive prosodic markings. 2 From Svartvik and Quirk (1980, S.1.4: 1-11), simplified. The asterisks mark simultaneous speech; e.g., B&apos;s thank you is said at the same time as A&apos;s [s]. 3 From West (1984, p. 133). The colon indicates a lengthening of a sound, the equal-sign that the two utterances are linked without a discernible pause. 4 Levinson 1983, p. 305. 212 Hirst Conversation Analysis 7. (Silence) A: Huh? B: I didn&apos;t say anything.&apos; The difference between the two research traditions is perhaps best illustrated by the difference in the way they approach the matter of indirect speech acts. In Discourse Analysis and traditional computational linguistics, indirect speech acts are seen as an important phenomenon in discourse, and dealing with them requires considerable effort in plan recognition on the part of the comprehender (e.g., Perrault and Allen 1980). On the other hand, &amp;quot;on the CA view, the problem doesn&apos;t even exist&amp;quot; (Levinson 19</context>
<context position="9064" citStr="Levinson (1983" startWordPosition="1448" endWordPosition="1449">, detailed and (purportedly) atheoretical: a researcher&apos;s task is to gather data and avoid theoretically loaded interpretations. One &amp;quot;seen but unnoticed&amp;quot; object of study is the mechanism of ordinary conversation: not formal discourse or written text, but common phatic chit-chat and utilitarian exchanges. Conversation Analysis arose in the 1960s from the work of Harvey Sacks, and is associated with Emanuel Schegloff and Gail Jefferson in the U.S. and with John Heritage and J. Maxwell Atkinson in the U.K. For a balanced, easy-to-read introduction to CA that emphasizes the linguistic issues, see Levinson (1983, ch. 6); for an introduction that emphasizes the ethnomethodological roots, see Heritage (1984, ch. 8) or Boden (1990a). Sacks believed deeply that the world is actually a very orderly place if one just looks closely enough. As Robin Wooffitt says in his introduction to CA in Computers and Conversation: A fundamental assumption informing CA is that ordinary talk is a highly organized, orderly phenomenon. The goal of analysis is to reveal the basis of this phenomenon. ... [Sacks] did not treat an utterance as something that just happened to be said that way. Rather, he worked on the assumption</context>
<context position="11793" citStr="Levinson (1983" startWordPosition="1878" endWordPosition="1879">to systematically observe the natural environment. (Boden 1990b, pp. 252-253) Wooffitt says (p. 27) that such conversational structures are not fixed and hard-wired cognitive phenomena, but rather are normative and socially organized. There is some dispute on this point. Boden (1990b, p. 250) claims, for example, that the turn-taking model of Sacks, Schegloff, and Jefferson (1974) &amp;quot;has held up across a range of languages 214 Hirst Conversation Analysis and cultures [and] it now seems quite reasonable to claim that this core machinery for talk transcends both language and culture.&amp;quot; Conversely, Levinson (1983, P. 301) uses the claim that the model does not hold in Burundi to defend the model against the charge of being trivial or content-free! Regardless, conversational structures are so fundamental and become so deeply internalized that, as David Good points out in his chapter of the book, they are present even in the disordered conversation of schizophrenics. Good suggests that just as studying aphasics can help us learn about syntactic mechanisms, so studying schizophrenics can help us learn about conversation. He exhibits an extract of a conversation in which the patient maintains normal repai</context>
<context position="30932" citStr="Levinson 1983" startWordPosition="5045" endWordPosition="5046">o what is required in a system, and turn them into clear, unambiguous, formal specifications (cf. Regoczei and Hirst 1989). Finkelstein and Fuks suggest that explicit use of CA might ease the analyst&apos;s task, allowing one to sort it all out, including the retractions, the iterative clarifications, and so on, to resolve different perspectives on the problem. They use rules based in part on CA; for example, all challenged statements must be affirmed, denied, or withdrawn. This is as much Discourse Analysis as Conversation Analysis, and the authors claim that (contra some other suggestions, e.g., Levinson 1983, p. 294) the two are quite compatible. The examples given by the authors are made up, not actual dialogue, and it&apos;s unclear as to whether they are intended to represent real speech or merely formalizations thereof. (If it&apos;s the former, then the authors are advised not to seek careers as playwrights.) This looks promising at first glance, and the authors hope that it could lead to automated support. But it is unclear that the rules could really do that. They describe a rigid discourse, saying, for example, that all questions must be answered. Real conversations needn&apos;t be like that. Nor is it </context>
</contexts>
<marker>Levinson, 1983</marker>
<rawString>Levinson, Stephen C. (1983). Pragmatics. Cambridge, England: Cambridge University Press (Cambridge textbooks in linguistics).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne Matthews</author>
</authors>
<title>Deciphering Victorian underwear and other seminars.&amp;quot;</title>
<date>1991</date>
<pages>42</pages>
<location>New York Times Magazine,</location>
<note>et seq.</note>
<contexts>
<context position="46016" citStr="Matthews 1991" startWordPosition="7565" endWordPosition="7566">nce and understanding require explicit analysis and representation of the intent of other agents. Rather, meaning can be seen as no more than the reader&apos;s response to a text, or, more generally, the perceiver&apos;s response to perception. This sits well with the notion mentioned above of a response being able to transform a first utterance without regard to the intent of its speaker, and is supported by deconstructionist and reader-response theories in literary studies (e.g., Iser 1974; see also Holub 1984). These theories are highly controversial, and are a current Hot Topic in literary studies (Matthews 1991). But, as Corriveau&apos;s reader-response model of text understanding shows (Corriveau 1991), they are not necessarily incompatible with artificial intelligence. The third reply is to deny the other premise, and claim that intent is indeed accounted for in CA—in fact, is pervasive in CA. When CA uses terms such as repair or closing or summons, or speaks of conversants using rules as resources, it is, in fact, explicitly ascribing intent! When analysts say that a particular utterance is, say, a next turn repair initiator, they are using their own knowledge of the language of the conversation to int</context>
</contexts>
<marker>Matthews, 1991</marker>
<rawString>Matthews, Anne (1991). &amp;quot;Deciphering Victorian underwear and other seminars.&amp;quot; New York Times Magazine, 10 February 1991, p. 42 et seq.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Weber McRoy</author>
</authors>
<title>Nonmonotonic reasoning in natural language.&amp;quot;</title>
<date>1989</date>
<tech>MS,</tech>
<institution>Department of Computer Science, University of Toronto.</institution>
<contexts>
<context position="36870" citStr="McRoy 1989" startWordPosition="6051" endWordPosition="6052">). 8 For example, in one of Garfinkel&apos;s experiments (1967, pp. 79ff), students who sought advice from a counselor were given completely random answers to their yes/no questions. &amp;quot;Over the course of the exchange, subjects sometimes started with the [random] reply as an answer and altered the previous sense of their question to accommodate ... the reply as the answer to the retrospectively revised question&amp;quot; (p. 90). (This kind of revision is covert, and is not to be confused with the overt interactional reconstruction of an utterance that can follow a misunderstanding in conversation (Fox 1987, McRoy 1989, Cawsey forthcoming)). 221 Computational Linguistics Volume 17, Number 2 amenable to direct use by computer, that they are not &amp;quot;codifiable&amp;quot; or &amp;quot;reduc[ible1 to an algorithm&amp;quot; (p. 82). His first argument is that rules in a computer are causal, meaning that they are designed into the hardware itself (p. 77), or into the program it executes. This is, I gather, intended to contrast with the use of rules as resources, as described above. However, I am at a loss to see the contradiction. Any backward-chaining Horn clause theorem prover or planner with appropriate meta-rules can be viewed as using its</context>
</contexts>
<marker>McRoy, 1989</marker>
<rawString>McRoy, Susan Weber (1989). &amp;quot;Nonmonotonic reasoning in natural language.&amp;quot; MS, Department of Computer Science, University of Toronto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael McTear</author>
</authors>
<title>Breakdown and repair in naturally occurring conversation and human-computer dialogue.&amp;quot; In:</title>
<date>1985</date>
<booktitle>Social action and artificial intelligence. Aldershot: Gower (Surrey conferences on sociological theory and method</booktitle>
<volume>3</volume>
<pages>104--123</pages>
<editor>Gilbert, G. Nigel and Heath, Christian (eds.)</editor>
<marker>McTear, 1985</marker>
<rawString>McTear, Michael (1985). &amp;quot;Breakdown and repair in naturally occurring conversation and human-computer dialogue.&amp;quot; In: Gilbert, G. Nigel and Heath, Christian (eds.) Social action and artificial intelligence. Aldershot: Gower (Surrey conferences on sociological theory and method 3), 104-123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johanna D Moore</author>
</authors>
<title>Responding to &apos;huh?&apos;: Answering vaguely articulated follow-up questions.&amp;quot;</title>
<date>1989</date>
<booktitle>Proceedings of the Conference on Human Factors in Computing Systems,</booktitle>
<location>Austin.</location>
<contexts>
<context position="29133" citStr="Moore (1989)" startWordPosition="4752" endWordPosition="4753">ystem gives to the user. While acknowledging that such explanations need to be planned (p. 223), she claims that it is also necessary to allow for interaction within the explanation. She does this with CA-based rules not unlike those of Frohlich and Luff, above. The planning is incremental, and based on each exchange. It occurs at both the high level (discourse structure; e.g., do a pre-closing) and the lower level (e.g., adding a pre-closing to another utterance). In extensions to the work, Cawsey hopes to allow the user to initiate repairs, in effect saying &amp;quot;Huh?&amp;quot; to the system (pp. 232-3). Moore (1989) has described a system with a similar ability, but Cawsey claims that Moore&apos;s system, because it&apos;s not based on CA&apos;s observations 219 Computational Linguistics Volume 17, Number 2 of natural conversations, is limited in two ways: the user can&apos;t interrupt in the middle of a turn, and the system doesn&apos;t ask the user to confirm its guess as to what is wrong. Whether these limitations make the system less effective than Cawsey&apos;s is, of course, an empirical question that is yet to be tested. Nigel Gilbert, Robin Wooffitt, and Norman Fraser describe a project in progress: Sundial, a speech recognit</context>
</contexts>
<marker>Moore, 1989</marker>
<rawString>Moore, Johanna D. (1989). &amp;quot;Responding to &apos;huh?&apos;: Answering vaguely articulated follow-up questions.&amp;quot; Proceedings of the Conference on Human Factors in Computing Systems, Austin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel C O&apos;Connell</author>
<author>Sabine Kowal</author>
<author>Erika Kaltenbacher</author>
</authors>
<title>Turn-taking: A critical analysis of the research tradition.&amp;quot;</title>
<date>1990</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>19</volume>
<issue>6</issue>
<pages>345--373</pages>
<contexts>
<context position="13976" citStr="O&apos;Connell et al. (1990)" startWordPosition="2202" endWordPosition="2205">pecific points in interactional time, constitutes that moment and shapes that interaction. (Boden 1990b, pp. 250-251) This research is not without its critics of course. Goldthorpe, in his review of early ethnomethodology (1973), finds little if anything to prefer over conventional sociology. He complains of ethnomethodology&apos;s impenetrable jargon, and this can certainly be seen in the CA literature—e.g., transition relevance place instead of potential end of utterance. Presumably, such locutions are intended to distance one from theoretically loaded interpretations of the data. More recently, O&apos;Connell et al. (1990) have presented a critique of CA research on turn-taking in conversations, a critique so vehement and unrelenting that one is left with the impression that the authors believe that no Conversation Analyst has ever uttered a true word on that topic or any other. The criticisms are largely aimed at the assumptions and methodology of the research. Perhaps the most important, however, is that CA completely ignores the goals and purposes of speakers in a conversation. Even if this is true, it&apos;s not clear to me that it matters, at least for CA. A speaker&apos;s intents are, after all, not open to direct </context>
<context position="44708" citStr="O&apos;Connell et al. (1990)" startWordPosition="7348" endWordPosition="7351">tems of simply responding with &amp;quot;set pieces&amp;quot; (p. 82), by which he apparently means canned responses. As I mentioned before, it&apos;s unclear whether the Advice System&apos;s responses are just pre-stored text or dynamically generated; but even if the responses are canned, if it turns out that informants judge the system to be easy or &apos;natural&apos; to use, if the response selected is always judged to be suitable, even in a variety of complex conversational situations, then something interesting is going on. While Button&apos;s criticism is, in effect, that CA is too rich and fine for HCI, the criticisms of CA by O&apos;Connell et al. (1990) imply the converse: that by ignoring communicative intent, CA is too impoverished for use in HCI. On this view, CA is nothing more than a kind of linguistic behaviorism. Unless its descriptions of external behavior can be related to the intents and purposes of speakers, CA can be used neither for understanding intelligent agents nor for constructing them. I see several replies to this. The first is to grant all this, but to point out that these enterprises do, nonetheless, require these descriptions of external behavior, and CA has them available. The challenge is then to use them. 223 Comput</context>
</contexts>
<marker>O&apos;Connell, Kowal, Kaltenbacher, 1990</marker>
<rawString>O&apos;Connell, Daniel C.; Kowal, Sabine; and Kaltenbacher, Erika (1990). &amp;quot;Turn-taking: A critical analysis of the research tradition.&amp;quot; Journal of Psycholinguistic Research, 19(6), 345-373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Raymond Perrault</author>
<author>James F Allen</author>
</authors>
<title>A plan-based analysis of indirect speech acts.&amp;quot;</title>
<date>1980</date>
<journal>American Journal of Hirst Conversation Analysis Computational Linguistics,</journal>
<volume>6</volume>
<issue>3</issue>
<pages>167--182</pages>
<contexts>
<context position="5082" citStr="Perrault and Allen 1980" startWordPosition="821" endWordPosition="824">of a sound, the equal-sign that the two utterances are linked without a discernible pause. 4 Levinson 1983, p. 305. 212 Hirst Conversation Analysis 7. (Silence) A: Huh? B: I didn&apos;t say anything.&apos; The difference between the two research traditions is perhaps best illustrated by the difference in the way they approach the matter of indirect speech acts. In Discourse Analysis and traditional computational linguistics, indirect speech acts are seen as an important phenomenon in discourse, and dealing with them requires considerable effort in plan recognition on the part of the comprehender (e.g., Perrault and Allen 1980). On the other hand, &amp;quot;on the CA view, the problem doesn&apos;t even exist&amp;quot; (Levinson 1983, p. 356); rather, the analysis is in terms of pre-requests and dis preferred responses (for details, see Levinson 1983, pp. 356ff). Perhaps the most important points that CA makes are that there is much more to human conversation than is studied in Discourse Analysis or theoretical linguistics, and that empirical research methods are required. But it&apos;s not immediately obvious that this has any consequences for HCI. After all, human conversation is spoken, whereas HCI, to the extent that it is linguistic at all</context>
</contexts>
<marker>Perrault, Allen, 1980</marker>
<rawString>Perrault, C. Raymond and Allen, James F. (1980). &amp;quot;A plan-based analysis of indirect speech acts.&amp;quot; American Journal of Hirst Conversation Analysis Computational Linguistics, 6(3-4), 167-182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Regoczei</author>
<author>Graeme Hirst</author>
</authors>
<title>On &apos;Extracting knowledge from text&apos;: Modelling the architecture of language users.&amp;quot; Proceedings, Third European Workshop on Knowledge Acquisition for Knowledge-Based Systems,</title>
<date>1989</date>
<institution>Computer Systems Research Institute, University of Toronto.</institution>
<location>Paris,</location>
<note>Also published as technical report CSRI-225,</note>
<contexts>
<context position="30441" citStr="Regoczei and Hirst 1989" startWordPosition="4966" endWordPosition="4969">ces are not to be processed singly and in isolation, as in presentday speech recognition systems, but rather as part of a CA-informed discourse. The authors argue that speech is the best HCI medium (p. 236). Of course, the system is still a couple of years off. A rather different use of CA, in the interpretation of software specifications, is suggested by Anthony Finkelstein and Hugo Fuks. One task of the systems analyst is to take vague, waffly, and often contradictory statements from people as to what is required in a system, and turn them into clear, unambiguous, formal specifications (cf. Regoczei and Hirst 1989). Finkelstein and Fuks suggest that explicit use of CA might ease the analyst&apos;s task, allowing one to sort it all out, including the retractions, the iterative clarifications, and so on, to resolve different perspectives on the problem. They use rules based in part on CA; for example, all challenged statements must be affirmed, denied, or withdrawn. This is as much Discourse Analysis as Conversation Analysis, and the authors claim that (contra some other suggestions, e.g., Levinson 1983, p. 294) the two are quite compatible. The examples given by the authors are made up, not actual dialogue, a</context>
</contexts>
<marker>Regoczei, Hirst, 1989</marker>
<rawString>Regoczei, Stephen and Hirst, Graeme (1989). &amp;quot;On &apos;Extracting knowledge from text&apos;: Modelling the architecture of language users.&amp;quot; Proceedings, Third European Workshop on Knowledge Acquisition for Knowledge-Based Systems, Paris, 196-211. Also published as technical report CSRI-225, Computer Systems Research Institute, University of Toronto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin H Ringle</author>
<author>Bertram C Bruce</author>
</authors>
<title>Conversation failure.&amp;quot;</title>
<date>1982</date>
<booktitle>Strategies for natural language processing. Hillsdale, NJ: Lawrence Erlbaum Associates,</booktitle>
<pages>203--221</pages>
<editor>In: Lehnert, Wendy G. and Ringle, Martin H. (eds.)</editor>
<contexts>
<context position="7064" citStr="Ringle and Bruce 1982" startWordPosition="1144" endWordPosition="1147">als by displaying some prompt to the user, usually a character such as &apos;*&apos; or &apos;&gt;&apos;. Neither side can interrupt the other. The suggestion of this book is that this is not how things should be. Since elements like turn-taking cues and interruptions are so important in human conversation, if we want to build natural, flexible interfaces, these matters should also be an issue in HCI. Similarly, Pirkko Raudaskoski, in her chapter, contrasts conventional research in HCI on recovery from misunderstandings with that in CA. The HCI research emphasizes the analysis and classification of the failure (cf. Ringle and Bruce 1982, Goodman 1986), while CA emphasizes the conversational patterns (e.g., Sacks et al. 1974) and the active role of both interlocutors in negotiating a meaning (cf. Clark and Wilkes-Gibbs 1986, Heeman 1991). Raudaskoski sees value in a synthesis of the two approaches. And Hugh Robinson points out in a short chapter that HCI would get more respect within the field of software engineering if it used CA to make itself a little less airy-fairy 2. The Intellectual Background of Conversation Analysis Conversation Analysis follows the tenets of ethnomethodology, a radical movement that developed in soc</context>
</contexts>
<marker>Ringle, Bruce, 1982</marker>
<rawString>Ringle, Martin H. and Bruce, Bertram C. (1982). &amp;quot;Conversation failure.&amp;quot; In: Lehnert, Wendy G. and Ringle, Martin H. (eds.) Strategies for natural language processing. Hillsdale, NJ: Lawrence Erlbaum Associates, 203-221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harvey Sacks</author>
</authors>
<title>On doing &apos;being ordinary&apos;.&amp;quot; In:</title>
<date>1984</date>
<editor>Atkinson, J. Maxwell and Heritage, John C. (eds.)</editor>
<publisher>Cambridge University Press</publisher>
<location>Cambridge, England:</location>
<note>(Studies in emotion and social interaction).</note>
<contexts>
<context position="10191" citStr="Sacks 1984" startWordPosition="1630" endWordPosition="1631">mething that just happened to be said that way. Rather, he worked on the assumption that the things people said were methodically designed. (pp. 10-12) [In CA,] words used in talk are not studied as semantic units, but as products or objects which are designed and used in respect of the interactions being negotiated through the talk: requests, proposals, accusations, complaints, and so on. ... The analytic objective is to explicate the procedures on which participants rely to produce utterances, and by which they make sense of other people&apos;s talk: to describe the &apos;technology of conversation&apos; (Sacks 1984, p. 413). (p. 10) Conversation Analysts thus spend their time tape-recording or videotaping conversations like (3)-(7) above, and seeing what sort of structures they contain. Many conversational exchanges are found to be various kinds of adjacency pairs: a greeting followed by a greeting in response, a question followed by an answer, and so on. But variations are possible. For example, the response to a question might be another question, thereby initiating a side sequence (Jefferson 1972), as seen in dialog (6) above. If there is some sort of trouble with the initial utterance, the response </context>
</contexts>
<marker>Sacks, 1984</marker>
<rawString>Sacks, Harvey (1984). &amp;quot;On doing &apos;being ordinary&apos;.&amp;quot; In: Atkinson, J. Maxwell and Heritage, John C. (eds.) Structures of social action: Studies in conversation analysis. Cambridge, England: Cambridge University Press (Studies in emotion and social interaction).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harvey Sacks</author>
<author>Emanuel A Schegloff</author>
<author>Gail Jefferson</author>
</authors>
<title>A simplest systematics for the organization of turn-taking for conversation.&amp;quot;</title>
<date>1974</date>
<journal>Language,</journal>
<volume>50</volume>
<pages>1974--696</pages>
<contexts>
<context position="7154" citStr="Sacks et al. 1974" startWordPosition="1157" endWordPosition="1160">de can interrupt the other. The suggestion of this book is that this is not how things should be. Since elements like turn-taking cues and interruptions are so important in human conversation, if we want to build natural, flexible interfaces, these matters should also be an issue in HCI. Similarly, Pirkko Raudaskoski, in her chapter, contrasts conventional research in HCI on recovery from misunderstandings with that in CA. The HCI research emphasizes the analysis and classification of the failure (cf. Ringle and Bruce 1982, Goodman 1986), while CA emphasizes the conversational patterns (e.g., Sacks et al. 1974) and the active role of both interlocutors in negotiating a meaning (cf. Clark and Wilkes-Gibbs 1986, Heeman 1991). Raudaskoski sees value in a synthesis of the two approaches. And Hugh Robinson points out in a short chapter that HCI would get more respect within the field of software engineering if it used CA to make itself a little less airy-fairy 2. The Intellectual Background of Conversation Analysis Conversation Analysis follows the tenets of ethnomethodology, a radical movement that developed in sociology in the early 1960s under the leadership of Harold Garfinkel of the University of Ca</context>
<context position="17627" citStr="Sacks et al. 1974" startWordPosition="2811" endWordPosition="2814">etails.) Frohlich and Luff&apos;s explicit claim (p. 188) is that they have tried to build the principles of CA into the system &amp;quot;in such a way as to generate and support orderly sequences of talk.&amp;quot; In this design, advice or explanation is not something the system dumps out in a single turn, but is an &amp;quot;emergent property&amp;quot; of a number of turns (p. 219). For example, CA is applied in formulating dialogue control policies, in which the authors try to &amp;quot;recreate some of the dynamics of ordinary conversation&amp;quot; (p. 201) by using CA rules for turn-taking in the conversation and for allowable adjacency pairs (Sacks et al. 1974). Specifically, the types of utterances possible are questions, statements, and answers, both by the user and by the system. That means that there are (3 x 2)2 = 36 possible kinds of adjacency pairs (p. 193). For example, a user&apos;s question can be followed by an answer by the system (such a pair is denoted UQ-SA) or by a question from the system (UQ-SQ). But each &apos;speaker&apos; can also &apos;retain the floor&apos; after an utterance, perhaps adding a statement after an answer, or asking a question (e.g., UA-US, UA-UQ, SA-SS, SA-SQ). But not all 36 kinds of pairs are allowed in the system (despite the asserti</context>
<context position="38673" citStr="Sacks et al. (1974)" startWordPosition="6345" endWordPosition="6348">&apos;s second argument is that CA rules are not complete; for example, the rules for turn-taking don&apos;t say how to identify a possible transition relevance place: The fact that these ... places are possible transition relevance places means that it is a contextually decidable matter whether or not actual transition takes place. Thus, because the rules do not, so to speak, determine their own application (using the rules is not determined by the rules themselves), there have to be other features involved in the application of the rules. This is the orientation to possible transition places, and in [Sacks et al. (1974)] there is no description of these places being formulated in terms of the rules for their production. Possible transition places are the contextual achievement of the unfolding structuring of the turn in progress. Possible completion places are not, then, provided for in advance by codifiable rules, they are situatedly achieved in and for a particular turn. (pp. 80-81, emphasis supplied) Button&apos;s intent here is not, as it might seem, to claim that CA chooses to remain willfully ignorant about certain elements of its domain; rather, it is, presumably, to tacitly invoke what&apos;s called in ethnome</context>
</contexts>
<marker>Sacks, Schegloff, Jefferson, 1974</marker>
<rawString>Sacks, Harvey; Schegloff, Emanuel A.; and Jefferson, Gail (1974). &amp;quot;A simplest systematics for the organization of turn-taking for conversation.&amp;quot; Language, 50, 1974,696-735.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emanuel A Schegloff</author>
</authors>
<title>Sequencing in conversational openings.&amp;quot;</title>
<date>1968</date>
<journal>American Anthropologist,</journal>
<volume>70</volume>
<pages>1075--1095</pages>
<marker>Schegloff, 1968</marker>
<rawString>Schegloff, Emanuel A. (1968). &amp;quot;Sequencing in conversational openings.&amp;quot; American Anthropologist, 70, 1075-1095.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emanuel A Schegloff</author>
<author>Harvey Sacks</author>
</authors>
<title>Opening up closings.&amp;quot;</title>
<date>1973</date>
<journal>Semiotica,</journal>
<volume>7</volume>
<pages>289--327</pages>
<contexts>
<context position="21450" citStr="Schegloff and Sacks 1973" startWordPosition="3467" endWordPosition="3471"> system&apos;s ability—a well-structured menu system precludes problematic choices. Extending that principle to natural language input was one motivation of the original NLMenu system. But then repair would never be necessary, and Frohlich and Luff&apos;s goal of having a system that performs humanlike repair initiations would be undermined! (Frohlich and Luff merely say that the use of menus ensures &amp;quot;high levels of input recognition&amp;quot; (p. 191).) The closing of the dialogue also follows principles derived by CA. The user can&apos;t just walk away from the machine, but must go through an official pre-closing (Schegloff and Sacks 1973) in which &amp;quot;each party [must] decline at least one opportunity to continue talking&amp;quot; (p. 214) before the closing and terminal exchange (a &amp;quot;Bye&amp;quot;–&amp;quot;Bye&amp;quot; adjacency pair) is reached. The user can shorten this sequence by clicking on a button labeled &amp;quot;I HAVE TO GO&amp;quot;. The pre-closing sequence starts after the system has fully dealt with all questions that the user has asked. In this sequence, the system can volunteer further information and encourage the user to continue to ask questions. The system also has a deliberate opening sequence intended to help familiarize the user with the system (p. 213). Th</context>
</contexts>
<marker>Schegloff, Sacks, 1973</marker>
<rawString>Schegloff, Emanuel A. and Sacks, Harvey (1973). &amp;quot;Opening up closings.&amp;quot; Semiotica, 7, 289-327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emanuel A Schegloff</author>
<author>Gail Jefferson</author>
<author>Harvey Sacks</author>
</authors>
<title>The preference for self-correction in the organization of repair in conversation.&amp;quot;</title>
<date>1977</date>
<journal>Language,</journal>
<volume>53</volume>
<issue>2</issue>
<pages>361--382</pages>
<contexts>
<context position="20034" citStr="Schegloff et al. (1977)" startWordPosition="3231" endWordPosition="3234">em is allowed to interrupt at such points—but only if it requires clarification 216 Hirst Conversation Analysis of what&apos;s been said so far in the turn, that is, if it wants to elicit a self-repair by the user (see below). Similarly, the user can interrupt the system. After each turn construction unit uttered by the system, there is a pause during which a button labeled &amp;quot;LET ME SPEAK&amp;quot; becomes available to the user for a short time (p. 204). Another aspect of conversation studied by CA and incorporated in the Advice System is repair, that is, clarifications, corrections of errors, and the like. Schegloff et al. (1977) have shown that there is a strong preference in conversation for self-repair— for people to correct their own errors. As already mentioned, the system can use next turn repair initiators; that is, it can interrupt the user and ask &amp;quot;What?&amp;quot;. (It&apos;s not smart enough to attempt the repair itself: &amp;quot;You mean ... ?&amp;quot;) In addition, it enables the user to do the same: it makes checking moves, asking &amp;quot;OK?&amp;quot; every so often. If the user answers &amp;quot;No&amp;quot;, the system will try to repair the problem. (Similarly, users can ask the system &amp;quot;OK?&amp;quot; if they are not sure that they are being understood.) But it is unclear t</context>
</contexts>
<marker>Schegloff, Jefferson, Sacks, 1977</marker>
<rawString>Schegloff, Emanuel A.; Jefferson, Gail; and Sacks, Harvey (1977). &amp;quot;The preference for self-correction in the organization of repair in conversation.&amp;quot; Language, 53(2), 361-382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Stubbs</author>
</authors>
<title>Discourse analysis: The sociolinguistic analysis of natural language.</title>
<date>1983</date>
<publisher>The University of Chicago Press.</publisher>
<location>Chicago:</location>
<contexts>
<context position="1490" citStr="Stubbs 1983" startWordPosition="216" endWordPosition="217">versity of Surrey in September 1989. Its purpose is to show what CA has to offer to the study of human–computer interaction (hereafter, HCI), especially interaction in natural language. In this review article, I want to use the book to explore at some length the question of whether CA has indeed anything to say to computational linguistics. Up to now, almost all research in computational linguistics on dialogs between humans and machines has taken the approach of Discourse Analysis. Discourse Analysis concerns itself with talk at the level of linguistic constituents (Levinson 1983, pp. 286ff; Stubbs 1983, pp. 1, 9-10). The disfluencies of spoken language are ignored. Utterances are treated as meaningful units of language, and are analyzed with regard to their semantic content. So the human and the machine are both seen as rational agents whose utterances are always perfectly formed and fully in accordance with Gricean rules. These interlocutors like to spend their time seeking information from each other, testing each other&apos;s abilities in plan inference. Typical interchanges are the following: 1. Human: The 8:50 train to Montreal? Computer: The railroad station is three blocks west on Main St</context>
</contexts>
<marker>Stubbs, 1983</marker>
<rawString>Stubbs, Michael (1983). Discourse analysis: The sociolinguistic analysis of natural language. Chicago: The University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucy A Suchman</author>
</authors>
<title>Plans and situated actions: The problem of human—machine communication. Cambridge, England:</title>
<date>1987</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="23300" citStr="Suchman (1987)" startWordPosition="3780" endWordPosition="3781">ot get an adequate impression of the system as a whole. Are the interface and the underlying advice system genuinely intelligent, or is it nothing more than &apos;Eliza with a mouse&apos;?6 Second, whether the interesting ideas work in practice is an empirical question. The system had not been tried out on real users at the time the chapter was written, though field tests were planned. This is an important point. Many systems that have been designed to be &amp;quot;helpful&amp;quot; have been found to cause more problems and misconceptions than simple, straightforward systems. Examples include the photocopier studied by Suchman (1987) and Raudaskoski&apos;s message system, to be described below. Moreover, testing the system is likely to be difficult, because there is so much in it that is novel 6 Thanks to George Ferguson for this apt phrase. The allusion is to Weizenbaum (1966). 217 Computational Linguistics Volume 17, Number 2 conversation &lt;- opening body body &lt;- if (OR (user-questions) (system-questions)) then adjacency-pair else preclosingl adjacency-pair &lt;- user-open-floor system-floor body user-open-floor &lt;- if (system-questions) then user-answer-turn else user-statement-turn Figure 1 Examples of rules in the Advice Syste</context>
</contexts>
<marker>Suchman, 1987</marker>
<rawString>Suchman, Lucy A. (1987). Plans and situated actions: The problem of human—machine communication. Cambridge, England: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Svartvik</author>
<author>Randolph Quirk</author>
</authors>
<title>A corpus of English conversation.</title>
<date>1980</date>
<journal>Lund: C W K Gleerup (Lund studies in English</journal>
<volume>56</volume>
<contexts>
<context position="4171" citStr="Svartvik and Quirk (1980" startWordPosition="677" endWordPosition="680">&apos;ve just *[s]* set out B: *thank you* A: ((2 syllables)) make some B: thanks A: I&apos;ve just boiled some water for having coffee cos I haven&apos;t had time for tea — *would* you like some B: *yes* yes2 5. Patient: An ah cun take thi:s hand an li:ft it and it duh&apos;unt hurt Doctor: Yuh mean when yuh lif the ar:m up using yer other han:d= Patient: =Uh-huh3 6. A: How many tubes would you like sir B: U:hm . what&apos;s the price now eh with V.A.T. do you know eh A: Er I&apos;ll just work that out for you B: =Thanks (Pause of 10.0 seconds) A: Three pounds nineteen a tube sir B: Three nineteen is it= A: =Yeah&apos; 1 From Svartvik and Quirk (1980, S.1.5: 836-848). I have omitted their extensive prosodic markings. 2 From Svartvik and Quirk (1980, S.1.4: 1-11), simplified. The asterisks mark simultaneous speech; e.g., B&apos;s thank you is said at the same time as A&apos;s [s]. 3 From West (1984, p. 133). The colon indicates a lengthening of a sound, the equal-sign that the two utterances are linked without a discernible pause. 4 Levinson 1983, p. 305. 212 Hirst Conversation Analysis 7. (Silence) A: Huh? B: I didn&apos;t say anything.&apos; The difference between the two research traditions is perhaps best illustrated by the difference in the way they appr</context>
</contexts>
<marker>Svartvik, Quirk, 1980</marker>
<rawString>Svartvik, Jan and Quirk, Randolph (eds.) (1980). A corpus of English conversation. Lund: C W K Gleerup (Lund studies in English 56).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harry R Tennant</author>
<author>Kenneth M Ross</author>
<author>Craig W Thompson</author>
</authors>
<title>Usable natural language interfaces through menu-based natural language understanding.&amp;quot;</title>
<date>1983</date>
<booktitle>Proceedings of the Conference on Human Factors in Computing Systems,</booktitle>
<pages>1983--154</pages>
<contexts>
<context position="16293" citStr="Tennant et al. 1983" startWordPosition="2586" endWordPosition="2589"> as evidence in the debate in the first. 3.1 The Advice System The chapter that is perhaps most convincingly pro-CA is the one by David Frohlich and Paul Luff that describes a CA-inspired interface to an advice system. The advice system, which is named Advice System, gives information and assistance on British government welfare benefits, and is intended for use directly by clients. The interface is primarily mouse-driven, with the user clicking on &apos;buttons&apos; on the screen, or picking up phrases from menus to assemble natural language sentences, rather like the Texas Instruments NLMenu system (Tennant et al. 1983a, 1983b). Unfortunately, the authors are fuzzy about many of the details, and it&apos;s not clear whether there is also direct keyboard input. (Page 190 and Figure 9.1 suggest that there isn&apos;t, yet page 207 and Tables 9.5 and 9.7 suggest that the user might make spelling errors.) It&apos;s also unclear whether the input menus are dynamically generated from a true grammar and lexicon, as in NLMenu, or whether they are just a static, pre-determined selection of useful fragments. Likewise, the authors don&apos;t say whether the Advice System&apos;s responses are just pre-stored text or dynamically generated. (It&apos;s </context>
</contexts>
<marker>Tennant, Ross, Thompson, 1983</marker>
<rawString>Tennant, Harry R.; Ross, Kenneth M.; and Thompson, Craig W. (1983a). &amp;quot;Usable natural language interfaces through menu-based natural language understanding.&amp;quot; Proceedings of the Conference on Human Factors in Computing Systems, 1983,154-160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harry R Tennant</author>
<author>Kenneth M Ross</author>
<author>Richard M Saenz</author>
<author>Craig W Thompson</author>
<author>James R Miller</author>
</authors>
<title>Menu-based natural language understanding.&amp;quot;</title>
<date>1983</date>
<booktitle>Proceedings, 21st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>151--158</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="16293" citStr="Tennant et al. 1983" startWordPosition="2586" endWordPosition="2589"> as evidence in the debate in the first. 3.1 The Advice System The chapter that is perhaps most convincingly pro-CA is the one by David Frohlich and Paul Luff that describes a CA-inspired interface to an advice system. The advice system, which is named Advice System, gives information and assistance on British government welfare benefits, and is intended for use directly by clients. The interface is primarily mouse-driven, with the user clicking on &apos;buttons&apos; on the screen, or picking up phrases from menus to assemble natural language sentences, rather like the Texas Instruments NLMenu system (Tennant et al. 1983a, 1983b). Unfortunately, the authors are fuzzy about many of the details, and it&apos;s not clear whether there is also direct keyboard input. (Page 190 and Figure 9.1 suggest that there isn&apos;t, yet page 207 and Tables 9.5 and 9.7 suggest that the user might make spelling errors.) It&apos;s also unclear whether the input menus are dynamically generated from a true grammar and lexicon, as in NLMenu, or whether they are just a static, pre-determined selection of useful fragments. Likewise, the authors don&apos;t say whether the Advice System&apos;s responses are just pre-stored text or dynamically generated. (It&apos;s </context>
</contexts>
<marker>Tennant, Ross, Saenz, Thompson, Miller, 1983</marker>
<rawString>Tennant, Harry R.; Ross, Kenneth M.; Saenz, Richard M.; Thompson, Craig W.; and Miller, James R. (1983b). &amp;quot;Menu-based natural language understanding.&amp;quot; Proceedings, 21st Annual Meeting of the Association for Computational Linguistics, Cambridge, MA, 151-158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Virginia Valian</author>
<author>Roger Wales</author>
</authors>
<title>What&apos;s what: Talkers help listeners hear and understand by clarifying sentential relations.&amp;quot;</title>
<date>1976</date>
<journal>Cognition,</journal>
<volume>4</volume>
<pages>155--176</pages>
<contexts>
<context position="27576" citStr="Valian and Wales (1976)" startWordPosition="4494" endWordPosition="4497">her. (8.8) User: I want to leave a message for Fred. What Raudaskoski found was that in speech mode, humans tend to be verbose; they often don&apos;t just say &amp;quot;yes&amp;quot; or &amp;quot;no.&amp;quot; The system was unable to channel their responses narrowly enough that it could be sure of understanding them. It needed to initiate repairs to repairs, but it couldn&apos;t. I suspect that a similar problem might be found with Frohlich and Luff&apos;s Advice System. As we saw earlier, its only way of initiating repairs is to ask &amp;quot;What?&amp;quot; But this is probably a very bad initiator for a computer system that aspires to natural conversation. Valian and Wales (1976) have shown that people&apos;s responses to &amp;quot;What?&amp;quot; are generally to offer a syntactically simpler form of the sentence uttered, if it is structurally complex, or to repeat essentially the same utterance, if it is already simple. So when the Advice System says &amp;quot;What?&amp;quot;, it is likely to be seen as initiating a syntactic repair, whereas the problem is presumably more likely to be semantic. Raudaskoski concludes her chapter by suggesting that a computer&apos;s next turn repair initiators should not be like those used by people, but rather should emphasize exactly where the misunderstanding is. That would ce</context>
</contexts>
<marker>Valian, Wales, 1976</marker>
<rawString>Valian, Virginia and Wales, Roger (1976). &amp;quot;What&apos;s what: Talkers help listeners hear and understand by clarifying sentential relations.&amp;quot; Cognition, 4, 155-176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Weizenbaum</author>
</authors>
<title>ELIZA: A computer program for the study of natural language communication between man and machine.&amp;quot;</title>
<date>1966</date>
<journal>Communications of the ACM,</journal>
<volume>9</volume>
<issue>1</issue>
<pages>36--45</pages>
<contexts>
<context position="23544" citStr="Weizenbaum (1966)" startWordPosition="3822" endWordPosition="3823">empirical question. The system had not been tried out on real users at the time the chapter was written, though field tests were planned. This is an important point. Many systems that have been designed to be &amp;quot;helpful&amp;quot; have been found to cause more problems and misconceptions than simple, straightforward systems. Examples include the photocopier studied by Suchman (1987) and Raudaskoski&apos;s message system, to be described below. Moreover, testing the system is likely to be difficult, because there is so much in it that is novel 6 Thanks to George Ferguson for this apt phrase. The allusion is to Weizenbaum (1966). 217 Computational Linguistics Volume 17, Number 2 conversation &lt;- opening body body &lt;- if (OR (user-questions) (system-questions)) then adjacency-pair else preclosingl adjacency-pair &lt;- user-open-floor system-floor body user-open-floor &lt;- if (system-questions) then user-answer-turn else user-statement-turn Figure 1 Examples of rules in the Advice System. that unless it is a runaway success, it might be difficult to determine just which ideas worked and which didn&apos;t. 3.2 Repair in Human–Computer Dialog The second of the two chapters that best show the potential applicability of CA to HCI is t</context>
</contexts>
<marker>Weizenbaum, 1966</marker>
<rawString>Weizenbaum, Joseph (1966). &amp;quot;ELIZA: A computer program for the study of natural language communication between man and machine.&amp;quot; Communications of the ACM, 9(1), 36-45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Candace West</author>
</authors>
<title>Medical misfires: Misgivings, mishearings, and misunderstandings in physician—patient dialogues.&amp;quot;</title>
<date>1984</date>
<booktitle>Discourse Processes,</booktitle>
<volume>7</volume>
<issue>2</issue>
<pages>107--134</pages>
<publisher>Indiana University Press,</publisher>
<location>Bloomington, IN:</location>
<contexts>
<context position="4413" citStr="West (1984" startWordPosition="719" endWordPosition="720">unt hurt Doctor: Yuh mean when yuh lif the ar:m up using yer other han:d= Patient: =Uh-huh3 6. A: How many tubes would you like sir B: U:hm . what&apos;s the price now eh with V.A.T. do you know eh A: Er I&apos;ll just work that out for you B: =Thanks (Pause of 10.0 seconds) A: Three pounds nineteen a tube sir B: Three nineteen is it= A: =Yeah&apos; 1 From Svartvik and Quirk (1980, S.1.5: 836-848). I have omitted their extensive prosodic markings. 2 From Svartvik and Quirk (1980, S.1.4: 1-11), simplified. The asterisks mark simultaneous speech; e.g., B&apos;s thank you is said at the same time as A&apos;s [s]. 3 From West (1984, p. 133). The colon indicates a lengthening of a sound, the equal-sign that the two utterances are linked without a discernible pause. 4 Levinson 1983, p. 305. 212 Hirst Conversation Analysis 7. (Silence) A: Huh? B: I didn&apos;t say anything.&apos; The difference between the two research traditions is perhaps best illustrated by the difference in the way they approach the matter of indirect speech acts. In Discourse Analysis and traditional computational linguistics, indirect speech acts are seen as an important phenomenon in discourse, and dealing with them requires considerable effort in plan recogn</context>
<context position="7936" citStr="West (1984)" startWordPosition="1279" endWordPosition="1280">ches. And Hugh Robinson points out in a short chapter that HCI would get more respect within the field of software engineering if it used CA to make itself a little less airy-fairy 2. The Intellectual Background of Conversation Analysis Conversation Analysis follows the tenets of ethnomethodology, a radical movement that developed in sociology in the early 1960s under the leadership of Harold Garfinkel of the University of California, Los Angeles. In reaction to conventional sociology, ethnomethodology stresses the questions of what the foundations of social interaction 5 Schegloff, quoted by West (1984). 213 Computational Linguistics Volume 17, Number 2 are, and how people interpret their everyday social experience: My purposes ... are to demonstrate the essential relevance, to sociological enquiries, of a concern for common sense activities, as a topic of enquiry in its own right ... : the socially standardized and standardizing, &amp;quot;seen but unnoticed,&amp;quot; expected background features of everyday scenes. (Garfinkel 1967, p. 36) Ethnomethodology thus places great emphasis on its being descriptive, empirical work, detailed and (purportedly) atheoretical: a researcher&apos;s task is to gather data and a</context>
</contexts>
<marker>West, 1984</marker>
<rawString>West, Candace (1984). &amp;quot;Medical misfires: Misgivings, mishearings, and misunderstandings in physician—patient dialogues.&amp;quot; Discourse Processes, 7(2), 107-134. A revised version appears in: West, Candace. Routine complications: Troubles with talk between doctors and patient. Bloomington, IN: Indiana University Press, 1984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
</authors>
<title>is the author of Semantic interpretation and the resolution of ambiguity (Cambridge</title>
<date>1987</date>
<publisher>University Press,</publisher>
<institution>Department of Computer Science, University of Toronto,</institution>
<location>Toronto, Canada</location>
<note>M5S 1A4; e-mail: gh@cs.toronto.edu</note>
<marker>Hirst, 1987</marker>
<rawString>Graeme Hirst is the author of Semantic interpretation and the resolution of ambiguity (Cambridge University Press, 1987). His current research concerns repair and reconstruction in human— machine conversations. Hirst&apos;s address is: Department of Computer Science, University of Toronto, Toronto, Canada M5S 1A4; e-mail: gh@cs.toronto.edu</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>