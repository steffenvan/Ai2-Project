<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.034918">
<title confidence="0.9860095">
Finding Hedges by Chasing Weasels: Hedge Detection Using
Wikipedia Tags and Shallow Linguistic Features
</title>
<author confidence="0.859516">
Viola Ganter and Michael Strube
</author>
<affiliation confidence="0.677348">
EML Research gGmbH
</affiliation>
<address confidence="0.746737">
Heidelberg, Germany
</address>
<email confidence="0.934216">
http://www.eml-research.de/nlp
</email>
<sectionHeader confidence="0.99234" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999994333333333">
We investigate the automatic detection of
sentences containing linguistic hedges us-
ing corpus statistics and syntactic pat-
terns. We take Wikipedia as an already
annotated corpus using its tagged weasel
words which mark sentences and phrases
as non-factual. We evaluate the quality of
Wikipedia as training data for hedge detec-
tion, as well as shallow linguistic features.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999970647058824">
While most research in natural language process-
ing is dealing with identifying, extracting and clas-
sifying facts, recent years have seen a surge in re-
search on sentiment and subjectivity (see Pang &amp;
Lee (2008) for an overview). However, even opin-
ions have to be backed up by facts to be effective
as arguments. Distinguishing facts from fiction re-
quires to detect subtle variations in the use of lin-
guistic devices such as linguistic hedges which in-
dicate that speakers do not back up their opinions
with facts (Lakoff, 1973; Hyland, 1998).
Many NLP applications could benefit from
identifying linguistic hedges, e.g. question an-
swering systems (Riloff et al., 2003), information
extraction from biomedical documents (Medlock
&amp; Briscoe, 2007; Szarvas, 2008), and deception
detection (Bachenko et al., 2008).
While NLP research on classifying linguistic
hedges has been restricted to analysing biomedi-
cal documents, the above (incomplete) list of ap-
plications suggests that domain- and language-
independent approaches for hedge detection need
to be developed. We investigate Wikipedia as a
source of training data for hedge classification. We
adopt Wikipedia’s notion of weasel words which
we argue to be closely related to hedges and pri-
vate states. Many Wikipedia articles contain a spe-
cific weasel tag, so that Wikipedia can be viewed
as a readily annotated corpus. Based on this data,
we have built a system to detect sentences that
contain linguistic hedges. We compare a base-
line relying on word frequency measures with one
combining word frequency with shallow linguistic
features.
</bodyText>
<sectionHeader confidence="0.999781" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999862818181818">
Research on hedge detection in NLP has been fo-
cused almost exclusively on the biomedical do-
main. Light et al. (2004) present a study on an-
notating hedges in biomedical documents. They
show that the phenomenon can be annotated ten-
tatively reliably by non-domain experts when us-
ing a two-way distinction. They also perform first
experiments on automatic classification.
Medlock &amp; Briscoe (2007) develop a weakly
supervised system for hedge classification in a
very narrow subdomain in the life sciences. They
start with a small set of seed examples known
to indicate hedging. Then they iterate and ac-
quire more training seeds without much manual
intervention (step 2 in their seed generation pro-
cedure indicates that there is some manual inter-
vention). Their best system results in a 0.76 pre-
cision/recall break-even-point (BEP). While Med-
lock &amp; Briscoe use words as features, Szarvas
(2008) extends their work to n-grams. He also ap-
plies his method to (slightly) out of domain data
and observes a considerable drop in performance.
</bodyText>
<sectionHeader confidence="0.994068" genericHeader="method">
3 Weasel Words
</sectionHeader>
<bodyText confidence="0.9999636">
Wikipedia editors are advised to avoid weasel
words, because they “offer an opinion without re-
ally backing it up, and ... are really used to ex-
press a non-neutral point of view.”1 Examples
for weasel words as given by the style guide-
</bodyText>
<footnote confidence="0.988304">
1http://en.wikipedia.org/wiki/
Wikipedia:Guide_to_writing_better_
articles
</footnote>
<page confidence="0.945431">
173
</page>
<note confidence="0.931112">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 173–176,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.9910492">
lines2 are: “Some people say ... ”, “I think... ”,
“Clearly ... ”, “...is widely regarded as ... ”,
“It has been said/suggested/noticed... ”, “It may
be that ... ” We argue that this notion is sim-
ilar to linguistic hedging, which is defined by
Hyland (1998) as “...any linguistic means used
to indicate either a) a lack of complete com-
mitment to the truth value of an accompany-
ing proposition, or b) a desire not to express
that commitment categorically.” The Wikipedia
style guidelines instruct editors to, if they notice
weasel words, insert a {{weasel-inline}} or
a {{weasel-word}} tag (both of which we will
hereafter refer to as weasel tag) to mark sentences
or phrases for improvement, e.g.
</bodyText>
<listItem confidence="0.988396857142857">
(1) Others argue {{weasel-inline}} that
the news media are simply catering
to public demand.
(2) ...therefore America is viewed by
some {{weasel-inline}} technology
planners as falling further behind
Europe ...
</listItem>
<sectionHeader confidence="0.545713" genericHeader="method">
4 Data and Annotation
</sectionHeader>
<bodyText confidence="0.999973666666667">
Weasel tags indicate that an article needs to be im-
proved, i.e., they are intended to be removed after
the objectionable sentence has been edited. This
implies that weasel tags are short lived, very sparse
and that – because weasels may not have been
discovered yet – not all occurrences of linguistic
hedges are tagged. Therefore we collected not one
but several Wikipedia dumps3 from the years 2006
to 2008. We extracted only those articles that con-
tained the string {{weasel. Out of these articles,
we extracted 168,923 unique sentences containing
437 weasel tags.
We use the dump completed on July 14, 2008
as development test data. Since weasel tags are
very sparse, any measure of precision would have
been overwhelmed by false positives. Thus we
created a balanced test set. We chose one random,
non-tagged sentence per tagged sentence, result-
ing (after removing corrupt data) in a set of 500
sentences. We removed formatting, comments and
links to references from all dumps. As testing data
we use the dump completed on March 6, 2009.
It comprises 70,437 sentences taken from articles
containing the string {{weasel with 328 weasel
</bodyText>
<footnote confidence="0.999706333333333">
2http://en.wikipedia.org/wiki/
Wikipedia:Avoid_weasel_words
3http://download.wikipedia.org/
</footnote>
<table confidence="0.9867585">
S M C
K 0.45 0.71 0.6
S 0.78 0.6
M 0.8
</table>
<tableCaption confidence="0.999894">
Table 1: Pairwise inter-annotator agreement
</tableCaption>
<bodyText confidence="0.999953888888889">
tags. Again, we created a balanced set of 500 sen-
tences.
As the number of weasel tags is very low con-
sidering the number of sentences in the Wikipedia
dumps, we still expected there to be a much higher
number of potential weasel words which had not
yet been tagged leading to false positives. There-
fore, we also annotated a small sample manu-
ally. One of the authors, two linguists and one
computer scientist annotated 100 sentences each,
50 of which were the same for all annotators to
enable measuring agreement. The annotators la-
beled the data independently and following anno-
tation guidelines which were mainly adopted from
the Wikipedia style guide with only small adjust-
ments to match our pre-processed data. We then
used Cohen’s Kappa (κ) to determine the level
of agreement (Carletta, 1996). Table 4 shows the
agreement between each possible pair of annota-
tors. The overall inter-annotator agreement was
κ = 0.65, which is similar to what Light et al.
(2004) report but worse than Medlock &amp; Briscoe’s
(2007) results. As Gold standard we merged all
four annotations sets. From the 50 overlapping in-
stances, we removed those where less than three
annotators had agreed on one category, resulting
in a set of 246 sentences for evaluation.
</bodyText>
<sectionHeader confidence="0.99866" genericHeader="method">
5 Method
</sectionHeader>
<subsectionHeader confidence="0.997251">
5.1 Words Preceding Weasel Tags
</subsectionHeader>
<bodyText confidence="0.999996230769231">
We investigate the five words occurring right be-
fore each weasel tag in the corpus (but within the
same sentence), assuming that weasel phrases con-
tain at most five words and weasel tags are mostly
inserted behind weasel words or phrases.
Each word within these 5-grams receives an in-
dividual score, based a) on the relative frequency
of this word in weasel contexts and the corpus in
general and b) on the average distance the word
has to a weasel tag, if found in a weasel context.
We assume that a word is an indicator for a weasel
if it occurs close before a weasel tag. The final
scoring function for each word in the training set
</bodyText>
<page confidence="0.911124">
174
</page>
<equation confidence="0.931308625">
is thus:
Score(w) = RelF(w) + AvgDist(w) (1)
with
W(w)
RelF(w) = (2)
log2(C(w))
(3)
W (w) denotes the number of times word w oc-
</equation>
<bodyText confidence="0.99810475">
curred in the context of a weasel tag, whereas
C(w) denotes the total number of times w oc-
curred in the corpus. The basic idea of the RelF
score is to give those words a high score, which oc-
cur frequently in the context of a weasel tag. How-
ever, due to the sparseness of tagged instances,
words that occur with a very high frequency in the
corpus automatically receive a lower score than
low-frequent words. We use the logarithmic func-
tion to diminish this effect.
In equation 3, for each weasel context j,
dist(w, weaseltagj) denotes the distance of word
w to the weasel tag in j. A word that always ap-
pears directly before the weasel tag will receive
an AvgDist value of 1, a word that always ap-
pears five words before the weasel tag will receive
an AvgDist value of 15. The score for each word
is stored in a list, based on which we derive the
classifier (words preceding weasel (wpw)): Each
sentence S is classified by
</bodyText>
<equation confidence="0.925456">
S → weasel if wpw(S) &gt; Q (4)
</equation>
<bodyText confidence="0.999896">
where Q is an arbitrary threshold used to control
the precision/recall balance and wpw(S) is the
sum of scores over all words in S, normalized by
the hyperbolic tangent:
</bodyText>
<equation confidence="0.98174325">
|S|
wpw(S) = tanh L Score(wi) (5)
i=0
with |S |= the number of words in the sentence.
</equation>
<subsectionHeader confidence="0.998965">
5.2 Adding shallow linguistic features
</subsectionHeader>
<bodyText confidence="0.999891">
A great number of the weasel words in Wikipedia
can be divided into three categories:
</bodyText>
<listItem confidence="0.9949782">
1. Numerically underspecified subjects (“Some
people”, “Experts”, “Many”)
2. Passive constructions (“It is believed”, “It is
considered”)
3. Adverbs (“Often”, “Probably”)
</listItem>
<bodyText confidence="0.9994604">
We POS-tagged the test data with the TnT tagger
(Brants, 2000) and developed finite state automata
to detect such constellations. We combine these
syntactic patterns with the word-scoring function
from above. If a pattern is found, only the head
of the pattern (i.e., adverbs, main verbs for passive
patterns, nouns and quantifiers for numerically un-
derspecified subjects) is assigned a score. The
scoring function adding syntactic patterns (asp)
for each sentence is:
</bodyText>
<equation confidence="0.974283666666667">
headss
asp(S) = tanh E Score(wi) (6)
i=0
</equation>
<bodyText confidence="0.999963">
where headsS = the number of pattern heads
found in sentence S.
</bodyText>
<sectionHeader confidence="0.999706" genericHeader="evaluation">
6 Results and Discussion
</sectionHeader>
<bodyText confidence="0.992541">
Both, the classifier based on words preceding
weasel (wpw) and the one based on added syntac-
tic patterns (asp) perform comparably well on the
development test data. wpw reaches a 0.69 preci-
sion/recall break-even-point (BEP) with a thresh-
old of Q = 0.99, while asp reaches a 0.70 BEP with
a threshold of Q = 0.76.
Applied to the test data these thresholds yield an
F-Score of 0.70 for wpw (prec. = 0.55/rec. = 0.98)
and an F-score of 0.68 (prec. = 0.69/rec. = 0.68)
for asp (Table 2 shows results at a few fixed thresh-
olds allowing for a better comparison). This indi-
cates that the syntactic patterns do not contribute
to the regeneration of weasel tags. Word frequency
and distance to the weasel tag are sufficient.
The decreasing precision of both approaches
when trained on more tagged sentences (i.e., com-
puted with a higher threshold) might be caused by
the great number of unannotated weasel words. In-
deed, an investigation of the sentences scored with
the added syntactic patterns showed that many
high-ranked sentences were weasels which had
not been tagged. A disadvantage of the weasel
tag is its short life span. The weasel tag marks a
phrase that needs to be edited, thus, once a weasel
word has been detected and tagged, it is likely to
get removed soon. The number of tagged sen-
tences is much smaller than the actual number of
weasel words. This leads to a great number of
false positives.
and
</bodyText>
<equation confidence="0.860894">
W(w)
AvgDist(w) =
EW(w)
j=0 dist(w, weaseltagj)
</equation>
<page confidence="0.993341">
175
</page>
<table confidence="0.998869857142857">
σ .60 .70 .76 .80 .90 .98
balanced set
wpw .68 .68 .68 .69 .69 .70
asp .67 .68 .68 .68 .61 .59
manual annot.
wpw - .59 - - - .59
asp .68 .69 .69 .69 .70 .65
</table>
<tableCaption confidence="0.968969">
Table 2: F-scores at different thresholds (bold at
</tableCaption>
<bodyText confidence="0.983627964285714">
the precision/recall break-even-points determined
on the development data)
The difference between wpw and asp becomes
more distinct when the manually annotated data
form the test set. Here asp outperforms wpw by
a large margin, though this is also due to the fact
that wpw performs rather poorly. asp reaches an
F-score of 0.69 (prec. = 0.61/rec. = 0.78), while
wpw reaches only an F-Score of 0.59 (prec. = 0.42/
rec. = 1). This suggests that the added syntactic
patterns indeed manage to detect weasels that have
not yet been tagged.
When humans annotate the data they not only
take specific words into account but the whole
sentence, and this is why the syntactic patterns
achieve better results when tested on those data.
The word frequency measure derived from the
weasel tags is not sufficient to cover this more in-
telligible notion of hedging. If one is to be re-
stricted to words, it would be better to fall back
to the weakly supervised approaches by Medlock
&amp; Briscoe (2007) and Szarvas (2008). These ap-
proaches could go beyond the original annotation
and learn further hedging indicators. However,
these approaches are, as argued by Szarvas (2008)
quite domain-dependent, while our approach cov-
ers the entire Wikipedia and thus as many domains
as are in Wikipedia.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999723862068966">
We have described a hedge detection system based
on word frequency measures and syntactic pat-
terns. The main idea is to use Wikipedia as a read-
ily annotated corpus by relying on its weasel tag.
The experiments show that the syntactic patterns
work better when using a broader notion of hedg-
ing tested on manual annotations. When evalu-
ating on Wikipedia weasel tags itself, word fre-
quency and distance to the tag is sufficient.
Our approach takes a much broader domain into
account than previous work. It can also easily be
applied to different languages as the weasel tag ex-
ists in more than 20 different language versions of
Wikipedia. For a narrow domain, we suggest to
start with our approach for deriving a seed set of
hedging indicators and then to use a weakly super-
vised approach.
Though our classifiers were trained on data
from multiple Wikipedia dumps, there were only
a few hundred training instances available. The
transient nature of the weasel tag suggests to
use the Wikipedia edit history for future work,
since the edits faithfully record all occurrences of
weasel tags.
Acknowledgments. This work has been par-
tially funded by the European Union under the
project Judicial Management by Digital Libraries
Semantics (JUMAS FP7-214306) and by the
Klaus Tschira Foundation, Heidelberg, Germany.
</bodyText>
<sectionHeader confidence="0.999331" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999604046511628">
Bachenko, Joan, Eileen Fitzpatrick &amp; Michael Schonwet-
ter (2008). Verification and implementation of language-
based deception indicators in civil and criminal narratives.
In Proceedings of the 22nd International Conference on
Computational Linguistics, Manchester, U.K., 18–22 Au-
gust 2008, pp. 41–48.
Brants, Thorsten (2000). TnT – A statistical Part-of-Speech
tagger. In Proceedings of the 6th Conference on Applied
Natural Language Processing, Seattle, Wash., 29 April –
4 May 2000, pp. 224–231.
Carletta, Jean (1996). Assessing agreement on classifica-
tion tasks: The kappa statistic. Computational Linguistics,
22(2):249–254.
Hyland, Ken (1998). Hedging in scientific research articles.
Amsterdam, The Netherlands: John Benjamins.
Lakoff, George (1973). Hedges: A study in meaning criteria
and the logic of fuzzy concepts. Journal of Philosophical
Logic, 2:458–508.
Light, Marc, Xin Ying Qiu &amp; Padmini Srinivasan (2004). The
language of Bioscience: Facts, speculations, and state-
ments in between. In Proceedings of the HLT-NAACL
2004 Workshop: Biolink 2004, Linking Biological Liter-
ature, Ontologies and Databases, Boston, Mass., 6 May
2004, pp. 17–24.
Medlock, Ben &amp; Ted Briscoe (2007). Weakly supervised
learning for hedge classification in scientific literature. In
Proceedings of the 45th Annual Meeting of the Association
for Computational Linguistics, Prague, Czech Republic,
23–30 June 2007, pp. 992–999.
Pang, Bo &amp; Lillian Lee (2008). Opinion mining and sen-
timent analysis. Foundations and Trends in Information
Retrieval, 2(1-2):1–135.
Riloff, Ellen, Janyce Wiebe &amp; Theresa Wilson (2003). Learn-
ing subjective nouns using extraction pattern bootstrap-
ping. In Proceedings of the 7th Conference on Compu-
tational Natural Language Learning, Edmonton, Alberta,
Canada, 31 May – 1 June 2003, pp. 25–32.
Szarvas, Gy¨orgy (2008). Hedge classification in biomedical
texts with a weakly supervised selection of keywords. In
Proceedings of the 46th Annual Meeting of the Association
for Computational Linguistics: Human Language Tech-
nologies, Columbus, Ohio, 15–20 June 2008, pp. 281–
289.
</reference>
<page confidence="0.998741">
176
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.742284">
<title confidence="0.9909035">Finding Hedges by Chasing Weasels: Hedge Detection Using Wikipedia Tags and Shallow Linguistic Features</title>
<author confidence="0.995623">Ganter Strube</author>
<affiliation confidence="0.997054">EML Research gGmbH</affiliation>
<address confidence="0.954945">Heidelberg, Germany</address>
<web confidence="0.907986">http://www.eml-research.de/nlp</web>
<abstract confidence="0.9870184">We investigate the automatic detection of sentences containing linguistic hedges using corpus statistics and syntactic patterns. We take Wikipedia as an already annotated corpus using its tagged weasel words which mark sentences and phrases as non-factual. We evaluate the quality of Wikipedia as training data for hedge detection, as well as shallow linguistic features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Joan Bachenko</author>
<author>Eileen Fitzpatrick</author>
<author>Michael Schonwetter</author>
</authors>
<title>Verification and implementation of languagebased deception indicators in civil and criminal narratives.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>41--48</pages>
<location>Manchester, U.K.,</location>
<contexts>
<context position="1408" citStr="Bachenko et al., 2008" startWordPosition="209" endWordPosition="212">ty (see Pang &amp; Lee (2008) for an overview). However, even opinions have to be backed up by facts to be effective as arguments. Distinguishing facts from fiction requires to detect subtle variations in the use of linguistic devices such as linguistic hedges which indicate that speakers do not back up their opinions with facts (Lakoff, 1973; Hyland, 1998). Many NLP applications could benefit from identifying linguistic hedges, e.g. question answering systems (Riloff et al., 2003), information extraction from biomedical documents (Medlock &amp; Briscoe, 2007; Szarvas, 2008), and deception detection (Bachenko et al., 2008). While NLP research on classifying linguistic hedges has been restricted to analysing biomedical documents, the above (incomplete) list of applications suggests that domain- and languageindependent approaches for hedge detection need to be developed. We investigate Wikipedia as a source of training data for hedge classification. We adopt Wikipedia’s notion of weasel words which we argue to be closely related to hedges and private states. Many Wikipedia articles contain a specific weasel tag, so that Wikipedia can be viewed as a readily annotated corpus. Based on this data, we have built a sys</context>
</contexts>
<marker>Bachenko, Fitzpatrick, Schonwetter, 2008</marker>
<rawString>Bachenko, Joan, Eileen Fitzpatrick &amp; Michael Schonwetter (2008). Verification and implementation of languagebased deception indicators in civil and criminal narratives. In Proceedings of the 22nd International Conference on Computational Linguistics, Manchester, U.K., 18–22 August 2008, pp. 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>TnT – A statistical Part-of-Speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th Conference on Applied Natural Language Processing,</booktitle>
<volume>4</volume>
<pages>224--231</pages>
<location>Seattle, Wash.,</location>
<contexts>
<context position="9553" citStr="Brants, 2000" startWordPosition="1575" endWordPosition="1576">here Q is an arbitrary threshold used to control the precision/recall balance and wpw(S) is the sum of scores over all words in S, normalized by the hyperbolic tangent: |S| wpw(S) = tanh L Score(wi) (5) i=0 with |S |= the number of words in the sentence. 5.2 Adding shallow linguistic features A great number of the weasel words in Wikipedia can be divided into three categories: 1. Numerically underspecified subjects (“Some people”, “Experts”, “Many”) 2. Passive constructions (“It is believed”, “It is considered”) 3. Adverbs (“Often”, “Probably”) We POS-tagged the test data with the TnT tagger (Brants, 2000) and developed finite state automata to detect such constellations. We combine these syntactic patterns with the word-scoring function from above. If a pattern is found, only the head of the pattern (i.e., adverbs, main verbs for passive patterns, nouns and quantifiers for numerically underspecified subjects) is assigned a score. The scoring function adding syntactic patterns (asp) for each sentence is: headss asp(S) = tanh E Score(wi) (6) i=0 where headsS = the number of pattern heads found in sentence S. 6 Results and Discussion Both, the classifier based on words preceding weasel (wpw) and </context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Brants, Thorsten (2000). TnT – A statistical Part-of-Speech tagger. In Proceedings of the 6th Conference on Applied Natural Language Processing, Seattle, Wash., 29 April – 4 May 2000, pp. 224–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: The kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="6733" citStr="Carletta, 1996" startWordPosition="1071" endWordPosition="1072">to be a much higher number of potential weasel words which had not yet been tagged leading to false positives. Therefore, we also annotated a small sample manually. One of the authors, two linguists and one computer scientist annotated 100 sentences each, 50 of which were the same for all annotators to enable measuring agreement. The annotators labeled the data independently and following annotation guidelines which were mainly adopted from the Wikipedia style guide with only small adjustments to match our pre-processed data. We then used Cohen’s Kappa (κ) to determine the level of agreement (Carletta, 1996). Table 4 shows the agreement between each possible pair of annotators. The overall inter-annotator agreement was κ = 0.65, which is similar to what Light et al. (2004) report but worse than Medlock &amp; Briscoe’s (2007) results. As Gold standard we merged all four annotations sets. From the 50 overlapping instances, we removed those where less than three annotators had agreed on one category, resulting in a set of 246 sentences for evaluation. 5 Method 5.1 Words Preceding Weasel Tags We investigate the five words occurring right before each weasel tag in the corpus (but within the same sentence)</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Carletta, Jean (1996). Assessing agreement on classification tasks: The kappa statistic. Computational Linguistics, 22(2):249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Hyland</author>
</authors>
<title>Hedging in scientific research articles.</title>
<date>1998</date>
<location>Amsterdam, The Netherlands: John Benjamins.</location>
<contexts>
<context position="1141" citStr="Hyland, 1998" startWordPosition="175" endWordPosition="176">r hedge detection, as well as shallow linguistic features. 1 Introduction While most research in natural language processing is dealing with identifying, extracting and classifying facts, recent years have seen a surge in research on sentiment and subjectivity (see Pang &amp; Lee (2008) for an overview). However, even opinions have to be backed up by facts to be effective as arguments. Distinguishing facts from fiction requires to detect subtle variations in the use of linguistic devices such as linguistic hedges which indicate that speakers do not back up their opinions with facts (Lakoff, 1973; Hyland, 1998). Many NLP applications could benefit from identifying linguistic hedges, e.g. question answering systems (Riloff et al., 2003), information extraction from biomedical documents (Medlock &amp; Briscoe, 2007; Szarvas, 2008), and deception detection (Bachenko et al., 2008). While NLP research on classifying linguistic hedges has been restricted to analysing biomedical documents, the above (incomplete) list of applications suggests that domain- and languageindependent approaches for hedge detection need to be developed. We investigate Wikipedia as a source of training data for hedge classification. W</context>
<context position="3948" citStr="Hyland (1998)" startWordPosition="620" endWordPosition="621">thout really backing it up, and ... are really used to express a non-neutral point of view.”1 Examples for weasel words as given by the style guide1http://en.wikipedia.org/wiki/ Wikipedia:Guide_to_writing_better_ articles 173 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 173–176, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP lines2 are: “Some people say ... ”, “I think... ”, “Clearly ... ”, “...is widely regarded as ... ”, “It has been said/suggested/noticed... ”, “It may be that ... ” We argue that this notion is similar to linguistic hedging, which is defined by Hyland (1998) as “...any linguistic means used to indicate either a) a lack of complete commitment to the truth value of an accompanying proposition, or b) a desire not to express that commitment categorically.” The Wikipedia style guidelines instruct editors to, if they notice weasel words, insert a {{weasel-inline}} or a {{weasel-word}} tag (both of which we will hereafter refer to as weasel tag) to mark sentences or phrases for improvement, e.g. (1) Others argue {{weasel-inline}} that the news media are simply catering to public demand. (2) ...therefore America is viewed by some {{weasel-inline}} techno</context>
</contexts>
<marker>Hyland, 1998</marker>
<rawString>Hyland, Ken (1998). Hedging in scientific research articles. Amsterdam, The Netherlands: John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Lakoff</author>
</authors>
<title>Hedges: A study in meaning criteria and the logic of fuzzy concepts.</title>
<date>1973</date>
<journal>Journal of Philosophical Logic,</journal>
<pages>2--458</pages>
<contexts>
<context position="1126" citStr="Lakoff, 1973" startWordPosition="173" endWordPosition="174">aining data for hedge detection, as well as shallow linguistic features. 1 Introduction While most research in natural language processing is dealing with identifying, extracting and classifying facts, recent years have seen a surge in research on sentiment and subjectivity (see Pang &amp; Lee (2008) for an overview). However, even opinions have to be backed up by facts to be effective as arguments. Distinguishing facts from fiction requires to detect subtle variations in the use of linguistic devices such as linguistic hedges which indicate that speakers do not back up their opinions with facts (Lakoff, 1973; Hyland, 1998). Many NLP applications could benefit from identifying linguistic hedges, e.g. question answering systems (Riloff et al., 2003), information extraction from biomedical documents (Medlock &amp; Briscoe, 2007; Szarvas, 2008), and deception detection (Bachenko et al., 2008). While NLP research on classifying linguistic hedges has been restricted to analysing biomedical documents, the above (incomplete) list of applications suggests that domain- and languageindependent approaches for hedge detection need to be developed. We investigate Wikipedia as a source of training data for hedge cl</context>
</contexts>
<marker>Lakoff, 1973</marker>
<rawString>Lakoff, George (1973). Hedges: A study in meaning criteria and the logic of fuzzy concepts. Journal of Philosophical Logic, 2:458–508.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Light</author>
</authors>
<title>Xin Ying Qiu &amp; Padmini Srinivasan</title>
<date>2004</date>
<booktitle>In Proceedings of the HLT-NAACL 2004 Workshop: Biolink 2004, Linking Biological Literature, Ontologies and Databases,</booktitle>
<pages>17--24</pages>
<location>Boston, Mass., 6</location>
<marker>Light, 2004</marker>
<rawString>Light, Marc, Xin Ying Qiu &amp; Padmini Srinivasan (2004). The language of Bioscience: Facts, speculations, and statements in between. In Proceedings of the HLT-NAACL 2004 Workshop: Biolink 2004, Linking Biological Literature, Ontologies and Databases, Boston, Mass., 6 May 2004, pp. 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Medlock</author>
<author>Ted Briscoe</author>
</authors>
<title>Weakly supervised learning for hedge classification in scientific literature.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>992--999</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1343" citStr="Medlock &amp; Briscoe, 2007" startWordPosition="200" endWordPosition="203">nt years have seen a surge in research on sentiment and subjectivity (see Pang &amp; Lee (2008) for an overview). However, even opinions have to be backed up by facts to be effective as arguments. Distinguishing facts from fiction requires to detect subtle variations in the use of linguistic devices such as linguistic hedges which indicate that speakers do not back up their opinions with facts (Lakoff, 1973; Hyland, 1998). Many NLP applications could benefit from identifying linguistic hedges, e.g. question answering systems (Riloff et al., 2003), information extraction from biomedical documents (Medlock &amp; Briscoe, 2007; Szarvas, 2008), and deception detection (Bachenko et al., 2008). While NLP research on classifying linguistic hedges has been restricted to analysing biomedical documents, the above (incomplete) list of applications suggests that domain- and languageindependent approaches for hedge detection need to be developed. We investigate Wikipedia as a source of training data for hedge classification. We adopt Wikipedia’s notion of weasel words which we argue to be closely related to hedges and private states. Many Wikipedia articles contain a specific weasel tag, so that Wikipedia can be viewed as a </context>
<context position="2596" citStr="Medlock &amp; Briscoe (2007)" startWordPosition="398" endWordPosition="401">ed on this data, we have built a system to detect sentences that contain linguistic hedges. We compare a baseline relying on word frequency measures with one combining word frequency with shallow linguistic features. 2 Related Work Research on hedge detection in NLP has been focused almost exclusively on the biomedical domain. Light et al. (2004) present a study on annotating hedges in biomedical documents. They show that the phenomenon can be annotated tentatively reliably by non-domain experts when using a two-way distinction. They also perform first experiments on automatic classification. Medlock &amp; Briscoe (2007) develop a weakly supervised system for hedge classification in a very narrow subdomain in the life sciences. They start with a small set of seed examples known to indicate hedging. Then they iterate and acquire more training seeds without much manual intervention (step 2 in their seed generation procedure indicates that there is some manual intervention). Their best system results in a 0.76 precision/recall break-even-point (BEP). While Medlock &amp; Briscoe use words as features, Szarvas (2008) extends their work to n-grams. He also applies his method to (slightly) out of domain data and observe</context>
<context position="12739" citStr="Medlock &amp; Briscoe (2007)" startWordPosition="2130" endWordPosition="2133">ile wpw reaches only an F-Score of 0.59 (prec. = 0.42/ rec. = 1). This suggests that the added syntactic patterns indeed manage to detect weasels that have not yet been tagged. When humans annotate the data they not only take specific words into account but the whole sentence, and this is why the syntactic patterns achieve better results when tested on those data. The word frequency measure derived from the weasel tags is not sufficient to cover this more intelligible notion of hedging. If one is to be restricted to words, it would be better to fall back to the weakly supervised approaches by Medlock &amp; Briscoe (2007) and Szarvas (2008). These approaches could go beyond the original annotation and learn further hedging indicators. However, these approaches are, as argued by Szarvas (2008) quite domain-dependent, while our approach covers the entire Wikipedia and thus as many domains as are in Wikipedia. 7 Conclusions We have described a hedge detection system based on word frequency measures and syntactic patterns. The main idea is to use Wikipedia as a readily annotated corpus by relying on its weasel tag. The experiments show that the syntactic patterns work better when using a broader notion of hedging </context>
</contexts>
<marker>Medlock, Briscoe, 2007</marker>
<rawString>Medlock, Ben &amp; Ted Briscoe (2007). Weakly supervised learning for hedge classification in scientific literature. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, Prague, Czech Republic, 23–30 June 2007, pp. 992–999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="811" citStr="Pang &amp; Lee (2008)" startWordPosition="116" endWordPosition="119">arch.de/nlp Abstract We investigate the automatic detection of sentences containing linguistic hedges using corpus statistics and syntactic patterns. We take Wikipedia as an already annotated corpus using its tagged weasel words which mark sentences and phrases as non-factual. We evaluate the quality of Wikipedia as training data for hedge detection, as well as shallow linguistic features. 1 Introduction While most research in natural language processing is dealing with identifying, extracting and classifying facts, recent years have seen a surge in research on sentiment and subjectivity (see Pang &amp; Lee (2008) for an overview). However, even opinions have to be backed up by facts to be effective as arguments. Distinguishing facts from fiction requires to detect subtle variations in the use of linguistic devices such as linguistic hedges which indicate that speakers do not back up their opinions with facts (Lakoff, 1973; Hyland, 1998). Many NLP applications could benefit from identifying linguistic hedges, e.g. question answering systems (Riloff et al., 2003), information extraction from biomedical documents (Medlock &amp; Briscoe, 2007; Szarvas, 2008), and deception detection (Bachenko et al., 2008). W</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Pang, Bo &amp; Lillian Lee (2008). Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
</authors>
<title>Learning subjective nouns using extraction pattern bootstrapping.</title>
<date>2003</date>
<booktitle>In Proceedings of the 7th Conference on Computational Natural Language Learning,</booktitle>
<volume>1</volume>
<pages>25--32</pages>
<location>Edmonton, Alberta, Canada, 31</location>
<contexts>
<context position="1268" citStr="Riloff et al., 2003" startWordPosition="191" endWordPosition="194">sing is dealing with identifying, extracting and classifying facts, recent years have seen a surge in research on sentiment and subjectivity (see Pang &amp; Lee (2008) for an overview). However, even opinions have to be backed up by facts to be effective as arguments. Distinguishing facts from fiction requires to detect subtle variations in the use of linguistic devices such as linguistic hedges which indicate that speakers do not back up their opinions with facts (Lakoff, 1973; Hyland, 1998). Many NLP applications could benefit from identifying linguistic hedges, e.g. question answering systems (Riloff et al., 2003), information extraction from biomedical documents (Medlock &amp; Briscoe, 2007; Szarvas, 2008), and deception detection (Bachenko et al., 2008). While NLP research on classifying linguistic hedges has been restricted to analysing biomedical documents, the above (incomplete) list of applications suggests that domain- and languageindependent approaches for hedge detection need to be developed. We investigate Wikipedia as a source of training data for hedge classification. We adopt Wikipedia’s notion of weasel words which we argue to be closely related to hedges and private states. Many Wikipedia ar</context>
</contexts>
<marker>Riloff, Wiebe, Wilson, 2003</marker>
<rawString>Riloff, Ellen, Janyce Wiebe &amp; Theresa Wilson (2003). Learning subjective nouns using extraction pattern bootstrapping. In Proceedings of the 7th Conference on Computational Natural Language Learning, Edmonton, Alberta, Canada, 31 May – 1 June 2003, pp. 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gy¨orgy Szarvas</author>
</authors>
<title>Hedge classification in biomedical texts with a weakly supervised selection of keywords.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>281--289</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1359" citStr="Szarvas, 2008" startWordPosition="204" endWordPosition="205">e in research on sentiment and subjectivity (see Pang &amp; Lee (2008) for an overview). However, even opinions have to be backed up by facts to be effective as arguments. Distinguishing facts from fiction requires to detect subtle variations in the use of linguistic devices such as linguistic hedges which indicate that speakers do not back up their opinions with facts (Lakoff, 1973; Hyland, 1998). Many NLP applications could benefit from identifying linguistic hedges, e.g. question answering systems (Riloff et al., 2003), information extraction from biomedical documents (Medlock &amp; Briscoe, 2007; Szarvas, 2008), and deception detection (Bachenko et al., 2008). While NLP research on classifying linguistic hedges has been restricted to analysing biomedical documents, the above (incomplete) list of applications suggests that domain- and languageindependent approaches for hedge detection need to be developed. We investigate Wikipedia as a source of training data for hedge classification. We adopt Wikipedia’s notion of weasel words which we argue to be closely related to hedges and private states. Many Wikipedia articles contain a specific weasel tag, so that Wikipedia can be viewed as a readily annotate</context>
<context position="3093" citStr="Szarvas (2008)" startWordPosition="481" endWordPosition="482">sing a two-way distinction. They also perform first experiments on automatic classification. Medlock &amp; Briscoe (2007) develop a weakly supervised system for hedge classification in a very narrow subdomain in the life sciences. They start with a small set of seed examples known to indicate hedging. Then they iterate and acquire more training seeds without much manual intervention (step 2 in their seed generation procedure indicates that there is some manual intervention). Their best system results in a 0.76 precision/recall break-even-point (BEP). While Medlock &amp; Briscoe use words as features, Szarvas (2008) extends their work to n-grams. He also applies his method to (slightly) out of domain data and observes a considerable drop in performance. 3 Weasel Words Wikipedia editors are advised to avoid weasel words, because they “offer an opinion without really backing it up, and ... are really used to express a non-neutral point of view.”1 Examples for weasel words as given by the style guide1http://en.wikipedia.org/wiki/ Wikipedia:Guide_to_writing_better_ articles 173 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 173–176, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP li</context>
<context position="12758" citStr="Szarvas (2008)" startWordPosition="2135" endWordPosition="2136">re of 0.59 (prec. = 0.42/ rec. = 1). This suggests that the added syntactic patterns indeed manage to detect weasels that have not yet been tagged. When humans annotate the data they not only take specific words into account but the whole sentence, and this is why the syntactic patterns achieve better results when tested on those data. The word frequency measure derived from the weasel tags is not sufficient to cover this more intelligible notion of hedging. If one is to be restricted to words, it would be better to fall back to the weakly supervised approaches by Medlock &amp; Briscoe (2007) and Szarvas (2008). These approaches could go beyond the original annotation and learn further hedging indicators. However, these approaches are, as argued by Szarvas (2008) quite domain-dependent, while our approach covers the entire Wikipedia and thus as many domains as are in Wikipedia. 7 Conclusions We have described a hedge detection system based on word frequency measures and syntactic patterns. The main idea is to use Wikipedia as a readily annotated corpus by relying on its weasel tag. The experiments show that the syntactic patterns work better when using a broader notion of hedging tested on manual an</context>
</contexts>
<marker>Szarvas, 2008</marker>
<rawString>Szarvas, Gy¨orgy (2008). Hedge classification in biomedical texts with a weakly supervised selection of keywords. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Columbus, Ohio, 15–20 June 2008, pp. 281– 289.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>