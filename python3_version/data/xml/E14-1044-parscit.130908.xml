<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000582">
<title confidence="0.9997555">
A Knowledge-based Representation
for Cross-Language Document Retrieval and Categorization
</title>
<author confidence="0.996463">
Marc Franco-Salvador1 2, Paolo Rosso2 and Roberto Navigli1
</author>
<affiliation confidence="0.986807">
1 Department of Computer Science
</affiliation>
<address confidence="0.425076">
Sapienza Universit`a di Roma, Italy
</address>
<email confidence="0.726177">
{francosalvador,navigli}@di.uniroma1.it
</email>
<address confidence="0.546039">
2 Natural Language Engineering Lab - PRHLT Research Center
Universitat Polit`ecnica de Val`encia, Spain
</address>
<email confidence="0.993884">
{mfranco,prosso}@dsic.upv.es
</email>
<sectionHeader confidence="0.993769" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999718">
Current approaches to cross-language doc-
ument retrieval and categorization are
based on discriminative methods which
represent documents in a low-dimensional
vector space. In this paper we pro-
pose a shift from the supervised to the
knowledge-based paradigm and provide a
document similarity measure which draws
on BabelNet, a large multilingual knowl-
edge resource. Our experiments show
state-of-the-art results in cross-lingual
document retrieval and categorization.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999929014705882">
The huge amount of text that is available on-
line is becoming ever increasingly multilingual,
providing an additional wealth of useful informa-
tion. Most of this information, however, is not eas-
ily accessible to the majority of users because of
language barriers which hamper the cross-lingual
search and retrieval of knowledge.
Today’s search engines would benefit greatly
from effective techniques for the cross-lingual re-
trieval of valuable information that can satisfy
a user’s needs by not only providing (Landauer
and Littman, 1994) and translating (Munteanu and
Marcu, 2005) relevant results into different lan-
guages, but also by reranking the results in a lan-
guage of interest on the basis of the importance of
search results in other languages.
Vector-based models are typically used in the
literature for representing documents both in
monolingual and cross-lingual settings (Manning
et al., 2008). However, because of the large size
of the vocabulary, having each term as a compo-
nent of the vector makes the document represen-
tation very sparse. To address this issue several
approaches to dimensionality reduction have been
proposed, such as Principal Component Analysis
(Jolliffe, 1986), Latent Semantic Indexing (Hull,
1994), Latent Dirichlet Allocation (LDA) (Blei et
al., 2003) and variants thereof, which project these
vectors into a lower-dimensional vector space. In
order to enable multilinguality, the vectors of com-
parable documents written in different languages
are concatenated, making up the document ma-
trix which is then reduced using linear projection
(Platt et al., 2010; Yih et al., 2011). However, to
do so, comparable documents are needed as train-
ing. Additionally, the lower dimensional represen-
tations are not of easy interpretation.
The availability of wide-coverage lexical
knowledge resources extracted automatically
from Wikipedia, such as DBPedia (Bizer et al.,
2009), YAGO (Hoffart et al., 2013) and BabelNet
(Navigli and Ponzetto, 2012a), has considerably
boosted research in several areas, especially where
multilinguality is a concern (Hovy et al., 2013).
Among these latter are cross-language plagiarism
detection (Potthast et al., 2011; Franco-Salvador
et al., 2013), multilingual semantic relatedness
(Navigli and Ponzetto, 2012b; Nastase and
Strube, 2013) and semantic alignment (Navigli
and Ponzetto, 2012a; Matuschek and Gurevych,
2013). One main advantage of knowledge-based
methods is that they provide a human-readable,
semantically interconnected, representation of
the textual item at hand (be it a sentence or a
document).
Following this trend, in this paper we provide
a knowledge-based representation of documents
which goes beyond the lexical surface of text,
while at the same time avoiding the need for train-
ing in a cross-language setting. To achieve this
we leverage a multilingual semantic network, i.e.,
BabelNet, to obtain language-independent repre-
sentations, which contain concepts together with
semantic relations between them, and also include
semantic knowledge which is just implied by the
input text. The integration of our multilingual
graph model with a vector representation enables
us to obtain state-of-the-art results in comparable
</bodyText>
<page confidence="0.976725">
414
</page>
<note confidence="0.9932205">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 414–423,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.96594">
document retrieval and cross-language text cate-
gorization.
</bodyText>
<sectionHeader confidence="0.999245" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999193">
The mainstream representation of documents
for monolingual and cross-lingual document re-
trieval is vector-based. A document vector, whose
components quantify the relevance of each term in
the document, is usually highly dimensional, be-
cause of the variety of terms used in a document
collection. As a consequence, the resulting docu-
ment matrices are very sparse. To address the data
sparsity issue, several approaches to the reduc-
tion of dimensionality of document vectors have
been proposed in the literature. A popular class of
methods is based on linear projection, which pro-
vides a low-dimensional mapping from a high di-
mensional vector space. A historical approach to
linear projection is Principal Component Analysis
(PCA) (Jolliffe, 1986), which performs a singular
value decomposition (SVD) on a document matrix
D of size n × m, where each row in D is the term
vector representation of a document. PCA uses
an orthogonal transformation to convert a set of
observations of possibly correlated variables into
a set of values of linearly uncorrelated variables
called principal components, which make up the
low-dimensional vector. Latent Semantic Analy-
sis (LSA) (Deerwester et al., 1990) is very simi-
lar to PCA but performs the SVD using the cor-
relation matrix instead of the covariance matrix,
which implies a lower computational cost. LSA
preserves the amount of variance in an eigenvector
v� by maximizing its Rayleigh ratio: ��T ���
</bodyText>
<equation confidence="0.995337">
��T �� , where
C = DT D is the correlation matrix of D.
</equation>
<bodyText confidence="0.999196364864865">
A generalization of PCA, called Oriented Prin-
cipal Component Analysis (OPCA) (Diamantaras
and Kung, 1996), is based on a noise covari-
ance matrix to project the similar components of
D closely. Other projection models such as La-
tent Dirichlet Allocation (LDA) (Blei et al., 2003)
are based on the extraction of generative models
from documents. Another approach, named Ex-
plicit Semantic Analysis (ESA) (Gabrilovich and
Markovitch, 2007), represents each document by
its similarities to a document collection. Using a
low domain specificity document collection such
as Wikipedia, the model has proven to obtain com-
petitive results.
Not only have these methods proven to be suc-
cessful in a monolingual scenario (Deerwester
et al., 1990; Hull, 1994), but they have also
been adapted to perform well in tasks at a cross-
language level (Potthast et al., 2008; Platt et al.,
2010; Yih et al., 2011). Cross-language Latent Se-
mantic Indexing (CL-LSI) (Dumais et al., 1997)
was the first linear projection approach used in
cross-lingual tasks. CL-LSI provides a cross-
lingual representation for documents by reducing
the dimensionality of a matrix D whose rows are
obtained by concatenating comparable documents
from different languages. Similarly, PCA and
OPCA can be adapted to a multilingual setting.
LDA was also adapted to perform in a multilingual
scenario with models such as Polylingual Topic
Models (Mimno et al., 2009), Joint Probabilistic
LSA and Coupled Probabilistic LSA (Platt et al.,
2010), which, however, are constrained to using
word counts, instead of better weighting strate-
gies, such as log(tf)-idf, known to perform bet-
ter with large vocabularies (Salton and McGill,
1986). Another variant, named Canonical Cor-
relation Analysis (CCA) (Thompson, 2005), uses
a cross-covariance matrix of the low-dimensional
vectors to find the projections. Cross-language
Explicit Semantic Analysis (CL-ESA) (Potthast et
al., 2008; Cimiano et al., 2009; Potthast et al.,
2011), instead, adapts ESA to be used at cross-
language level by exploiting the comparable doc-
uments across languages from Wikipedia. CL-
ESA represents each document written in a lan-
guage L by its similarities with a document collec-
tion in the same language L. Using a multilingual
document collection with comparable documents
across languages, the resulting vectors from dif-
ferent languages can be compared directly.
An alternative unsupervised approach, Cross-
language Character n-Grams (CL-CNG) (Mc-
namee and Mayfield, 2004), does not draw upon
linear projections and represents documents as
vectors of character n-grams. It has proven to ob-
tain good results in cross-language document re-
trieval (Potthast et al., 2011) between languages
with lexical and syntactic similarities.
Recently, a novel supervised linear projec-
tion model based on Siamese Neural Networks
(S2Net) (Yih et al., 2011) achieved state-of-the-
art performance in comparable document retrieval.
S2Net performs a linear combination of the terms
of a document vector d� to obtain a reduced vector
r, which is the output layer of a neural network.
Each element in r� has a weight which is a linear
combination of the original weights of d, and cap-
tures relationships between the original terms.
However, linear projection approaches need a
high number of training documents to achieve
state-of-the-art performance (Platt et al., 2010;
Yih et al., 2011). Moreover, although they are
good at identifying a few principal components,
</bodyText>
<page confidence="0.998507">
415
</page>
<bodyText confidence="0.9997565">
the representations produced are opaque, in that
they cannot explicitly model the semantic content
of documents with a human-interpretable repre-
sentation, thereby making the data analysis diffi-
cult. In this paper, instead, we propose a language-
independent knowledge graph representation for
documents which is obtained from a large multi-
lingual semantic network, without using any train-
ing information. Our knowledge graph represen-
tation explicitly models the semantics of the docu-
ment in terms of the concepts and relations evoked
by its co-occurring terms.
</bodyText>
<sectionHeader confidence="0.9926845" genericHeader="method">
3 A Knowledge-based Document
Representation
</sectionHeader>
<bodyText confidence="0.999848214285714">
We propose a knowledge-based document rep-
resentation aimed at expanding the terms in a doc-
ument’s bag of words by means of a knowledge
graph which provides concepts and semantic rela-
tions between them. Key to our approach is the
use of a graph representation which does not de-
pend on any given language, but, indeed, is multi-
lingual. To build knowledge graphs of this kind we
utilize BabelNet, a multilingual semantic network
that we present in Section 3.1. Then, in Section
3.2, we describe the five steps needed to obtain our
graph-based multilingual representation of docu-
ments. Finally, we introduce our knowledge graph
similarity measure in Section 3.3.
</bodyText>
<subsectionHeader confidence="0.997179">
3.1 BabelNet
</subsectionHeader>
<bodyText confidence="0.999803689655172">
BabelNet (Navigli and Ponzetto, 2012a) is a
multilingual semantic network whose concepts
and relations are obtained from the largest avail-
able semantic lexicon of English, WordNet (Fell-
baum, 1998), and the largest wide-coverage
collaboratively-edited encyclopedia, Wikipedia,
by means of an automatic mapping algorithm. Ba-
belNet is therefore a multilingual “encyclopedic
dictionary” that combines lexicographic informa-
tion with wide-coverage encyclopedic knowledge.
Concepts in BabelNet are represented similarly to
WordNet, i.e., by grouping sets of synonyms in
the different languages into multilingual synsets.
Multilingual synsets contain lexicalizations from
WordNet synsets, the corresponding Wikipedia
pages and additional translations output by a sta-
tistical machine translation system. The relations
between synsets are collected from WordNet and
from Wikipedia’s hyperlinks between pages.
We note that, in principle, we could use any
multilingual network providing a similar kind of
information, e.g., EuroWordNet (Vossen, 2004).
However, in our work we chose BabelNet be-
cause of its larger size, its coverage of both lex-
icographic and encyclopedic knowledge, and its
free availability.1 In our work we used BabelNet
1.0, which encodes knowledge for six languages,
namely: Catalan, English, French, German, Italian
and Spanish.
</bodyText>
<subsectionHeader confidence="0.996283">
3.2 From Document to Knowledge Graph
</subsectionHeader>
<bodyText confidence="0.971671083333333">
We now introduce our five-step method for repre-
senting a given document d from a collection D of
documents written in language L as a language-
independent knowledge graph.
Building a Basic Vector Representation Ini-
tially we transform a document d into a traditional
vector representation. To do this, we score each
term ti E d with a weight wi. This weight is usu-
ally a function of term and document frequency.
Following the literature, one method that works
well is the log tf-idf weighting (Salton et al., 1983;
Salton and McGill, 1986):
</bodyText>
<equation confidence="0.999334">
wi = log2(fi + 1)log2(n/ni). (1)
</equation>
<bodyText confidence="0.9940864">
where fi is the number of times term i occurs in
document d, n is the total number of documents in
the collection and ni is the number of documents
that contain ti. We then create a weighted term
vector v� = (w1, ..., wn), where wi is the weight
corresponding to term ti. We exclude stopwords
from the vector.
Selecting the Relevant Document Terms We
then create the set T of base forms, i.e., lemmas2,
of the terms in the document d. In order to keep
only the most relevant terms, we sort the terms T
according to their weight in vector v� and retain a
maximum number of K terms, obtaining a set of
terms TK.3 The value of K is calculated as a func-
tion of the vector size, as follows:
</bodyText>
<equation confidence="0.99995">
K = (log2(1 + |V|))2, (2)
</equation>
<bodyText confidence="0.9969516">
The rationale is that K must be high enough to
ensure a good conceptual representation but not
too high, so as to avoid as much noise as possi-
ble in the set TK.
Populating the Graph with Initial Concepts
Next, we create an initially-empty knowledge
graph G = (V, E), i.e., such that V = E = ∅.
We populate the vertex set V with the set SK of
all the synsets in BabelNet which contain any term
in TK in the document language L, that is:
</bodyText>
<footnote confidence="0.996512857142857">
1http://babelnet.org
2Following the setup of (Platt et al., 2010), our initial data
is represented using term vectors. For this reason we lemma-
tize in this step.
3Since the vector v� provides weights for all the word
forms, and not only lemmas, occurring in d, we take the best
weight among those word forms of the considered lemma.
</footnote>
<page confidence="0.995219">
416
</page>
<figureCaption confidence="0.970965">
Figure 1: (a) initial graph from TK = {“European”, “apple”, “tree”, “Malus”, “species”, “America”}; (b)
knowledge graph obtained by retrieving all paths from BabelNet. Gray nodes are the original concepts.
</figureCaption>
<equation confidence="0.994639">
�5K = 5ynsetsL(t), (3)
tETK
</equation>
<bodyText confidence="0.989287906666667">
where 5ynsetsL(t) is the set of synsets in Ba-
belNet which contain a term t in the language
of interest L. For example, in Figure 1(a) we
show the initial graph obtained from the set TK =
{“European”, “apple”, “tree”, “Malus”, “species”,
“America”}. Note, however, that each retrieved
synset is multilingual, i.e., it contains lexicaliza-
tions for the same concept in other languages too.
Therefore, the nodes of our knowledge graph pro-
vide a language-independent representation of the
document’s content.
Creating the Knowledge Graph Similarly to
Navigli and Lapata (2010), we create the knowl-
edge graph by searching BabelNet for paths con-
necting pairs of synsets in V . Formally, for each
pair v, v&apos; E V such that v and v&apos; do not share any
lexicalization4 in TK, for each path in BabelNet
v → v1 → .. . → vn → v&apos;, we set: V := V U
{v1, ... , vn} and E := EU{(v, v1), ... , (vn, v&apos;)},
that is, we add all the path vertices and edges to
G. After prototyping, the path length is limited
to maximum length 3, so as to avoid an excessive
semantic drift.
As a result of populating the graph with inter-
mediate edges and vertices, we obtain a knowl-
edge graph which models the semantic context of
document d. We point out that our knowledge
graph might have different isolated components.
We view each component as a different interpreta-
tion of document d. To select the main interpre-
tation, we keep only the largest component, i.e.,
the one with the highest number of vertices, which
we consider as the most likely semantic represen-
tation of the document content.
Figure 1(b) shows the knowledge graph ob-
tained for our example term set. Note that our
approach retains, and therefore weights, only the
subgraph focused on the “apple fruit” meaning.
4This prevents different senses of the same term from be-
ing connected via a path in the resulting knowledge graph.
Knowledge Graph Weighting The final step
consists of weighting all the concepts and se-
mantic relations of the knowledge graph G. For
weighting relations we use the original weights
from BabelNet, which provide the degree of re-
latedness between the synset end points of each
edge (Navigli and Ponzetto, 2012a). As for con-
cepts, we weight them on the basis of the origi-
nal weights of the terms in the vector V. In or-
der to score each concept in our knowledge graph
G, we applied the topic-sensitive PageRank al-
gorithm (Haveliwala et al., 2003) to G. While
the well-known PageRank algorithm (Page et al.,
1998) calculates the global importance of vertices
in a graph, topic-sensitive PageRank is a variant
in which the importance of vertices is biased us-
ing a set of representative “topics”. Formally, the
topic-sensitive PageRank vector p�is calculated by
means of an iterative process until convergence as
follows: p� = cMp+(1−c)u, where c is the damp-
ing factor (conventionally set to 0.85), 1− c repre-
sents the probability of a surfer randomly jumping
to any node in the graph, M is the transition proba-
bility matrix of graph G, with Mei = degree(i)−1
if an edge from i to j exists, 0 otherwise, u is
the random-jumping transition probability vector,
where each ui represents the probability of jump-
ing randomly to the node i, and p� is the resulting
PageRank vector which scores the nodes of G. In
contrast to vanilla PageRank, the “topic-sensitive”
variant gives more probability mass to some nodes
in G and less to others. In our case we perturbate
u by concentrating the probability mass to the ver-
tices in 5K, which are the synsets corresponding
to the document terms TK (cf. Formula 3).
</bodyText>
<subsectionHeader confidence="0.998821">
3.3 Similarity between Knowledge Graphs
</subsectionHeader>
<bodyText confidence="0.9996">
We can now determine the similarity between two
documents d, d&apos; E D in terms of the similarity of
their knowledge graph representations G and G&apos;.
Following the literature (Montes y G´omez et
al., 2001) we calculate the similarity between the
vertex sets in the two graphs using Dice’s coeffi-
cient (Jackson et al., 1989):
</bodyText>
<page confidence="0.995461">
417
</page>
<figureCaption confidence="0.998828">
Figure 2: Knowledge graph examples from two comparable documents in different languages.
</figureCaption>
<equation confidence="0.99984075">
2 · X w(c)
cEV (G)nV (G0)
X w(c) + X
cEV (G) cEV (G0)
</equation>
<bodyText confidence="0.999887666666667">
where w(c) is the weight of a concept c (see Sec-
tion 3.2). Likewise, we calculate the similarity be-
tween the two edge sets as:
</bodyText>
<equation confidence="0.9996376">
2 · X w(r)
rEE(G)nE(G0)
X
w(r) +
rEE(G0)
</equation>
<bodyText confidence="0.9998324">
where w(r) is the weight of a semantic relation
edge r.
We combine the two above measures of concep-
tual (Sc) and relational (Sr) similarity to obtain an
integrated measure Sg(G, G&apos;) between knowledge
graphs:
Notably, since we are working with a language-
independent representation of documents, this
similarity measure can be applied to the knowl-
edge graphs built from documents written in any
language. In Figure 2 we show two knowledge
graphs for comparable documents written in dif-
ferent languages (for clarity, labels are in English
in both graphs). As expected, the graphs share sev-
eral key concepts and relations.
</bodyText>
<sectionHeader confidence="0.99296" genericHeader="method">
4 A Multilingual Vector Representation
</sectionHeader>
<subsectionHeader confidence="0.940773">
4.1 From Document to Multilingual Vector
</subsectionHeader>
<bodyText confidence="0.999661625">
Since our knowledge graphs will only cover the
most central concepts of a document, we comple-
ment this core representation with a more tradi-
tional vector-based representation. However, as
we are interested in the cross-language compari-
son of documents, we translate our monolingual
vector VL of a document d written in language L
into its corresponding vector VL&apos; in language L&apos;
</bodyText>
<table confidence="0.76048475">
Algorithm 1 Dictionary-based term-vector translation.
Input: a weighted document vector ~vL = (w1, ... , wn), a
source language L and a target language L&apos;
Output: a translated vector ~vL0
</table>
<listItem confidence="0.813546071428571">
1: ~vL0 +— (0, ... , 0) of length n
2: fori=1ton
3: if wi = 0 continue
4: // let ti be the term corresponding to wi in ~vL
5: SL +— SynsetsL(ti)
6: for each synset s E SL
7: T +— getTranslations(s, L&apos;)
8: if T =� 0 then
9: for each tr E T
10: wnew = wi · conftdence(tr, ti)
11: // let index(tr) be the index of tr in ~vL
12: if 3 index(tr) then
13: vL0(index(tr)) = wnew
14: return ~vL0
</listItem>
<bodyText confidence="0.991736105263158">
using BabelNet as our multilingual dictionary. We
detail the document-vector translation process in
Algorithm 1.
The translated vector VL&apos; is obtained as follows:
for each term tz with non-zero weight in vL we
obtain all the possible meanings of tz in BabelNet
(see line 5) and, for each of these, we retrieve all
the translations (line 7), i.e., lexicalizations of the
concept, in language L&apos; available in the synset. We
set a non-zero value in the translation vector vL&apos;,5
in correspondence with each such translation tr,
proportional to the weight of tz in the original vec-
tor and the confidence of the translation (line 10),
as provided by the BabelNet semantic network.6
In order to increase the amount of information
available in the vector and counterbalance possible
wrong translations, we avoid translating all vec-
tors to one language. Instead, in the present work
we create a multilingual vector representation of a
</bodyText>
<footnote confidence="0.980148125">
5To make the translation possible, while at the same time
keeping the same number of dimensions in our vector repre-
sentation, we use a shared vocabulary which covers both lan-
guages. See Section 6 for details on the experimental setup.
6Non-English lexicalizations in BabelNet have confi-
dence 1 if originating from Wikipedia inter-language links
and &lt; 1 if obtained by means of statistical machine transla-
tion (Navigli and Ponzetto, 2012a).
</footnote>
<equation confidence="0.99753625">
Sg(G, G&apos;) = Sc(G, G&apos;) + Sr(G, G&apos;)
(6)
2
.
Sc(G, G&apos;) =
, (4)
w(c)
Sr(G,G&apos;) =
X
rEE(G)
w(r)
, (5)
</equation>
<page confidence="0.982854">
418
</page>
<bodyText confidence="0.9999322">
document d written in language L by concatenat-
ing the corresponding vector VL with the translated
vector VL0 of d for language L&apos;. As a result, we
obtain a multilingual vector VLL0, which contains
lexicalizations in both languages.
</bodyText>
<subsectionHeader confidence="0.99667">
4.2 Similarity between Multilingual Vectors
</subsectionHeader>
<bodyText confidence="0.999736">
Following common practice for document similar-
ity in the literature (Manning et al., 2008), we use
the cosine similarity as the similarity measure be-
tween multilingual vectors:
</bodyText>
<sectionHeader confidence="0.992786" genericHeader="method">
5 Knowledge-based Document Similarity
</sectionHeader>
<bodyText confidence="0.9997248">
Given a source document d and a target docu-
ment d&apos;, we calculate the similarities between the
respective knowledge-graph and multilingual vec-
tor representations, and combine them to obtain a
knowledge-based similarity as follows:
</bodyText>
<equation confidence="0.9949615">
KBSim(d, d0) = c(G)Sg(G, G0) + (1 − c(G))Sv(�vLL0 , �v0LL0 ),
(8)
</equation>
<bodyText confidence="0.999578">
where c(G) is an interpolation factor calculated as
the edge density of knowledge graph G:
</bodyText>
<equation confidence="0.994454">
c(G) JE(G)J
( ) = JV (G)J(JV (G)J − 1). (9)
</equation>
<bodyText confidence="0.999886833333334">
Note that, using the factor c(G) to interpolate
the two similarities in Eq. 8, we determine the rel-
evance for the knowledge graphs and the multi-
lingual vectors in a dynamic way. Indeed, c(G)
makes the contribution of graph similarity depend
on the richness of the knowledge graph.
</bodyText>
<sectionHeader confidence="0.998877" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.9999462">
In this section we compare our knowledge-
based document similarity measure, KBSim,
against state-of-the-art models on two different
tasks: comparable document retrieval and cross-
lingual text categorization.
</bodyText>
<subsectionHeader confidence="0.992836">
6.1 Comparable Document Retrieval
</subsectionHeader>
<bodyText confidence="0.999972444444444">
In our first experiment we determine the effective-
ness of our knowledge-based approach in a com-
parable document retrieval task. Given a docu-
ment d written in language L and a collection DL0
of documents written in another language L&apos;, the
task of comparable document retrieval consists of
finding the document in DL0 which is most simi-
lar to d, under the assumption that there exists one
document d&apos; ∈ DL0 which is comparable with d.
</bodyText>
<subsubsectionHeader confidence="0.546947">
6.1.1 Corpus and Task Setting
</subsubsectionHeader>
<bodyText confidence="0.999909243902439">
Dataset We followed the experimental setting
described in (Platt et al., 2010; Yih et al., 2011)
and evaluated KBSim on the Wikipedia dataset
made available by the authors of those papers.
The dataset is composed of Wikipedia compara-
ble encyclopedic entries in English and Spanish.
For each document in English there exists a “real”
pair in Spanish which was defined as a compara-
ble entry by the Wikipedia user community. The
dataset of each language was split into three parts:
43,380 training, 8,675 development and 8,675 test
documents. The documents were tokenized, with-
out stemming, and represented as vectors using a
log(tf)-idf weighting (Salton and Buckley, 1988).
The vocabulary of the corpus was restricted to
20,000 terms, which were the most frequent terms
in the two languages after removing the top 50
terms.
Methodology To evaluate the models we com-
pared each English document against the Spanish
dataset and vice versa. Following the original set-
ting, the results are given as the average perfor-
mance between these two experiments. For eval-
uation we employed the averaged top-1 accuracy
and Mean Reciprocal Rank (MMR) at finding the
real comparable document in the other language.
We compared KBSim against the state-of-the-art
supervised models S2Net, OPCA, CCA, and CL-
LSI (cf. Section 2). In contrast to these models,
KBSim does not need a training step, so we ap-
plied it directly to the testing partition.
In addition we also included the results of
CL-ESA7, CL-C3G8 and two simple vector-based
models which translate all documents into English
on a word-by-word basis and compared them us-
ing cosine similarity: the first model (CosSimE)
uses a statistical dictionary trained with Europarl
using Wavelet-Domain Hidden Markov Models
(He, 2007), a model similar to IBM Model 4;
the second model (CosSimBN) instead uses Algo-
rithm 1 to translate the vectors with BabelNet.
</bodyText>
<sectionHeader confidence="0.901247" genericHeader="evaluation">
6.1.2 Results
</sectionHeader>
<bodyText confidence="0.99997">
As we can see from Table 1,9 the CosSimBN
model, which uses BabelNet to translate the docu-
ment vectors, achieves better results than CCA and
CL-LSI. We hypothesize that this is due to these
linear projection models losing information during
the projection. CosSimE yields results similar to
CosSimBN, showing that BabelNet is a good al-
ternative statistical dictionary. In contrast to CCA
</bodyText>
<footnote confidence="0.938065">
7Document collections with sizes higher than 105 provide
high performance (Potthast et al., 2008). Here we used 15k
documents from the training set to index the test documents.
8CL-C3G is CL-CNG using character 3-grams, which has
proven to be the best length (Mcnamee and Mayfield, 2004).
9In this work, statistically significant results according to
a x2 test are highlighted in bold.
</footnote>
<equation confidence="0.9864336">
Sv(�vLL0,�v0LL0) =
(7)
JJ�vLL0JJ JJ�v0LL0JJ.
�v0LL0
VLL0 ·
</equation>
<page confidence="0.997132">
419
</page>
<table confidence="0.9992964">
Model Dimension Accuracy MMR
S2Net 2000 0.7447 0.7973
KBSim N/A 0.7342 0.7750
OPCA 2000 0.7255 0.7734
CosSimE N/A 0.7033 0.7467
CosSimBN N/A 0.7029 0.7550
CCA 1500 0.6894 0.7378
CL-LSI 5000 0.5302 0.6130
CL-ESA 15000 0.2660 0.3305
CL-C3G N/A 0.2511 0.3025
</table>
<tableCaption confidence="0.976512">
Table 1: Test results for comparable document re-
trieval in Wikipedia. S2Net, OPCA, CosSimE,
CCA and CL-LSI are from (Yih et al., 2011).
</tableCaption>
<bodyText confidence="0.995516038461538">
and CL-LSI, OPCA performs better thanks to its
improved projection method using a noise covari-
ance matrix, which enables it to obtain the main
components in a low-dimensional space.
CL-C3G and CL-ESA obtain the lowest results.
Considering that English and Spanish do not have
many lexical similarities, the low performance of
CL-C3G is justified because these languages do
not share many character n-grams. The reason be-
hind the low results of CL-ESA can be explained
by the low number of intersecting concepts be-
tween Spanish and English in Wikipedia, as con-
firmed by Potthast et al. (2008). Despite both us-
ing Wikipedia in some way, KBSim obtains much
higher performance than CL-ESA thanks to the
use of our multilingual knowledge graph repre-
sentation of documents, which makes it possible
to expand and semantically relate its original con-
cepts. As a result, in contrast to CL-ESA, KB-
Sim can integrate conceptual and relational simi-
larity functions which provide more accurate per-
formance. Interestingly, KBSim also outperforms
OPCA which, in contrast to our system, is super-
vised, and in terms of accuracy is only 1 point be-
low S2Net, the supervised state-of-the-art model
using neural networks.
</bodyText>
<subsectionHeader confidence="0.99996">
6.2 Cross-language Text Categorization
</subsectionHeader>
<bodyText confidence="0.999972571428571">
The second task in which we tested the differ-
ent models was cross-language text categorization.
The task is defined as follows: given a document
dL in a language L and a corpus D&apos;L, with docu-
ments in a different language L&apos;, and C possible
categories, a system has to classify dL into one of
the categories C using the labeled collection D&apos;L,.
</bodyText>
<subsectionHeader confidence="0.937785">
6.2.1 Corpus and Task Setting
</subsectionHeader>
<bodyText confidence="0.9990688">
Dataset To perform this task we used the Mul-
tilingual Reuters Collection (Amini et al., 2009),
which is composed of five datasets of news from
five different languages (English, French, German,
Spanish and Italian) and classified into six possi-
</bodyText>
<table confidence="0.999343111111111">
Model Dim. EN News ES News
Accuracy Accuracy
KBSim N/A 0.8189 0.6997
Full MT 50 0.8483 0.6484
CosSimBN N/A 0.8023 0.6737
OPCA 100 0.8412 0.5954
CCA 150 0.8388 0.5323
CL-LSI 5000 0.8401 0.5105
CosSimE N/A 0.8046 0.4481
</table>
<tableCaption confidence="0.981876666666667">
Table 2: Test results for cross-language text cat-
egorization. Full MT, OPCA, CCA, CL-LSI and
CosSimE are from (Platt et al., 2010).
</tableCaption>
<bodyText confidence="0.99960585">
ble categories. In addition, each dataset of news
is translated into the other four languages using
the Portage translation system (Sadat et al., 2005).
As a result, we have five different multilingual
datasets, each containing source news documents
in one language and four sets of translated doc-
uments in the other languages. Each of the lan-
guages has an independent vocabulary. Document
vectors in the collection are created using TFIDF-
based weighting.
Methodology To evaluate our approach we used
the English and Spanish news datasets. From
the English news dataset we randomly selected
13,131 news as training and 1,875 as test docu-
ments. From the Spanish news dataset we selected
all 12,342 news as test documents. To classify
both test sets we used the English news training
set. We performed the experiment at cross-lingual
level using Spanish and English languages avail-
able for both Spanish and English news datasets,
therefore we classified each test set selecting the
documents in English and using the Spanish doc-
uments in the training dataset, and vice versa. We
followed Platt et al. (2010) and averaged the val-
ues obtained from the two comparisons for each
test set to obtain the final result. To categorize
the documents we applied k-NN to the ranked
list of documents according to the similarity mea-
sure employed for each model. We evaluated each
model by estimating its accuracy in the classifica-
tion of the English and Spanish test sets.
We compared our approach against the state-
of-the-art supervised models in this task: OPCA,
CCA and CL-LSI (Platt et al., 2010). In addi-
tion, we include the results of the CosSimBN and
CosSimE models that we introduced in Section
6.1.1, as well as the results of a full statistical ma-
chine translation system trained with Europarl and
post-processed by LSA (Full MT), as reported by
Platt et al. (2010).
</bodyText>
<page confidence="0.997064">
420
</page>
<sectionHeader confidence="0.862245" genericHeader="evaluation">
6.2.2 Results
</sectionHeader>
<bodyText confidence="0.995660433962264">
Table 2 shows the cross-language text categoriza-
tion accuracy. CosSimE obtained the lowest re-
sults. This is because there is a significant number
of untranslated terms in the translation process that
the statistical dictionary cannot cover. This is not
the case in the CosSimBN model which achieves
higher results using BabelNet as a statistical dic-
tionary, especially on the Spanish news corpus.
On the other hand, however, the linear projec-
tion methods as well as Full MT obtained the high-
est results on the English corpus. The differences
between the linear projection methods are evident
when looking at the Spanish corpus results; OPCA
performed best with a considerable improvement,
which indicates again that it is one of the most ef-
fective linear projection methods. Finally, our ap-
proach, KBSim, obtained competitive results on
the English corpus, performing best among the un-
supervised systems, and the highest results on the
Spanish news, surpassing all alternatives.
Since KBSim does not need any training for
document comparison, and because it based,
moreover, on a multilingual lexical resource, we
performed an additional experiment to demon-
strate its ability to carry out the same text cate-
gorization task in many languages. To do this, we
used the Multilingual Reuters Collection to cre-
ate a 3,000 document test dataset and 9,000 train-
ing dataset10 for five languages: English, German,
Spanish, French and Italian. Then we calculated
the classification accuracy on each test set using
each training set. Results are shown in Table 3.
The best results for each language were ob-
tained when working at the monolingual level,
which suggests that KBSim might be a good
untrained alternative in monolingual tasks, too.
In general, cross-language comparisons produced
similar results, demonstrating the general applica-
bility of KBSim to arbitrary language pairs in mul-
tilingual text categorization. However, we note
that German, Italian and Spanish training parti-
tions produced low results compared to the oth-
ers. After analyzing the length of the documents
in the different datasets we discovered that they
have different average lengths in words: 79 (EN),
76 (FR), 75 (DE), 60 (ES) and 55 (IT). German,
Spanish and especially Italian documents have the
lowest average length, which makes it more diffi-
cult to build a representative knowledge graph of
the content of each document when it is perform-
ing at cross-language level.
10Note that training is needed for the k-NN classifier, but
not for document comparison.
</bodyText>
<table confidence="0.998581">
Testing Training datasets
datasets
DE EN ES FR IT
DE 0.8053 0.6872 0.5373 0.6417 0.5920
EN 0.5827 0.8463 0.5540 0.6530 0.5820
ES 0.5883 0.6153 0.8707 0.6237 0.7010
FR 0.6867 0.7103 0.6667 0.8227 0.6887
IT 0.5973 0.5487 0.6263 0.5973 0.8317
</table>
<tableCaption confidence="0.999402">
Table 3: KBSim accuracy in a multilingual setup.
</tableCaption>
<sectionHeader confidence="0.991447" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999648459459459">
In this paper we introduced a knowledge-based
approach to represent and compare documents
written in different languages. The two main
contributions of this work are: i) a new graph-
based model for the language-independent rep-
resentation of documents based on the Babel-
Net multilingual semantic network; ii) KBSim, a
knowledge-based cross-language similarity mea-
sure between documents, which integrates our
multilingual graph-based model with a traditional
vector representation.
In two different cross-lingual tasks, i.e., compa-
rable document retrieval and cross-language text
categorization, KBSim has proven to perform on
a par or better than the supervised state-of-the-art
models which make use of linear projections to
obtain the main components of the term vectors.
We remark that, in contrast to the best systems in
the literature, KBSim does not need any parameter
tuning phase nor does it use any training informa-
tion. Moreover, when scaling to many languages,
supervised systems need to be trained on each pair,
which can be very costly.
The gist of our approach is in the knowl-
edge graph representation of documents, which re-
lates the original terms using expanded concepts
and relations from BabelNet. The knowledge
graphs also have the nice feature of being human-
interpretable, a feature that we want to exploit in
future work. We will also explore the integration
of linear projection models, such as OPCA and
S2Net, into our multilingual vector-based similar-
ity measure. Also, to ensure a level playing field,
following the competing models, in this work we
did not use multi-word expressions as vector com-
ponents. We will study their impact on KBSim in
future work.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.965955833333333">
The authors gratefully acknowledge the support
of the ERC Starting Grant MultiJEDI No. 259234,
EC WIQ-EI IRSES (Grant No. 269180) and
MICINN DIANA-Applications (TIN2012-38603-
C02-01). Thanks go to Yih et al. for their support
and Jim McManus for his comments.
</bodyText>
<page confidence="0.996884">
421
</page>
<note confidence="0.711799">
References Workshop on Statistical Machine Translation, pages
80–87. Association for Computational Linguistics.
</note>
<reference confidence="0.999245333333334">
Massih-Reza Amini, Nicolas Usunier, and Cyril
Goutte. 2009. Learning from multiple partially ob-
served views - an application to multilingual text
categorization. In Advances in Neural Information
Processing Systems 22 (NIPS 2009), pages 28–36.
Christian Bizer, Jens Lehmann, Georgi Kobilarov,
S¨oren Auer, Christian Becker, Richard Cyganiak,
and Sebastian Hellmann. 2009. Dbpedia - a crys-
tallization point for the web of data. J. Web Sem.,
7(3):154–165.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993–1022.
Philipp Cimiano, Antje Schultz, Sergej Sizov, Philipp
Sorg, and Steffen Staab. 2009. Explicit versus la-
tent concept models for cross-language information
retrieval. In Proceedings of the International Joint
Conference on Artificial Intelligence (IJCAI), vol-
ume 9, pages 1513–1518.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American society for information science,
41(6):391–407.
Konstantinos I. Diamantaras and Sun Y. Kung. 1996.
Principal component neural networks. Wiley New
York.
Susan T. Dumais, Todd A. Letsche, Michael L.
Littman, and Thomas K. Landauer. 1997. Auto-
matic cross-language retrieval using latent seman-
tic indexing. In Proc. of AAAI Spring Symposium
on Cross-language Text and Speech Retrieval, pages
18–24.
Christiane Fellbaum. 1998. WordNet: An electronic
lexical database. Bradford Books.
Marc Franco-Salvador, Parth Gupta, and Paolo Rosso.
2013. Cross-language plagiarism detection using
a multilingual semantic network. In Proc. of the
35th European Conference on Information Retrieval
(ECIR’13), volume LNCS(7814), pages 710–713.
Springer-Verlag.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In Proc. of the 20th
International Joint Conference on Artifical Intelli-
gence (IJCAI), pages 1606–1611.
Taher Haveliwala, Sepandar Kamvar, and Glen Jeh.
2003. An analytical comparison of approaches to
personalizing pagerank. Technical Report 2003-35,
Stanford InfoLab, June.
Xiaodong He. 2007. Using word dependent transition
models in hmm based word alignment for statistical
machine translation. In Proceedings of the Second
Johannes Hoffart, Fabian M. Suchanek, Klaus
Berberich, and Gerhard Weikum. 2013. Yago2: A
spatially and temporally enhanced knowledge base
from wikipedia. Artificial Intelligence, 194:28–61.
Eduard H. Hovy, Roberto Navigli, and Simone Paolo
Ponzetto. 2013. Collaboratively built semi-
structured content and Artificial Intelligence: The
story so far. Artificial Intelligence, 194:2–27.
David Hull. 1994. Improving text retrieval for the
routing problem using latent semantic indexing. In
Proceedings of the 17th Annual International ACM
SIGIR Conference on Research and Development
in Information Retrieval (SIGIR), pages 282–291.
Springer.
Donald A. Jackson, Keith M. Somers, and Harold H.
Harvey. 1989. Similarity coefficients: measures of
co-occurrence and association or simply measures of
occurrence? American Naturalist, pages 436–453.
Ian T. Jolliffe. 1986. Principal component analysis,
volume 487. Springer-Verlag New York.
Thomas K. Landauer and Michael L. Littman. 1994.
Computerized cross-language document retrieval
using latent semantic indexing, April 5. US Patent
5,301,109.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch¨utze. 2008. Introduction to Information
Retrieval. Cambridge University Press, New York,
NY, USA.
Michael Matuschek and Iryna Gurevych. 2013.
Dijkstra-WSA: A graph-based approach to word
sense alignment. Transactions of the Association for
Computational Linguistics (TACL), 1:151–164.
Paul Mcnamee and James Mayfield. 2004. Charac-
ter n-gram tokenization for european language text
retrieval. Information Retrieval, 7(1-2):73–97.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing: Volume 2-Volume 2, pages
880–889. Association for Computational Linguis-
tics.
Manuel Montes y G´omez, Alexander F. Gelbukh, Au-
relio L´opez-L´opez, and Ricardo A. Baeza-Yates.
2001. Flexible comparison of conceptual graphs.
In Proc. of the 12th International Conference on
Database and Expert Systems Applications (DEXA),
pages 102–111.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguis-
tics, 31(4):477–504.
</reference>
<page confidence="0.980409">
422
</page>
<reference confidence="0.999469806451613">
Vivi Nastase and Michael Strube. 2013. Transform-
ing wikipedia into a large scale multilingual concept
network. Artificial Intelligence, 194:62–85.
Roberto Navigli and Mirella Lapata. 2010. An ex-
perimental study of graph connectivity for unsuper-
vised word sense disambiguation. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
32(4):678–692.
Roberto Navigli and Simone Paolo Ponzetto. 2012a.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217–
250.
Roberto Navigli and Simone Paolo Ponzetto. 2012b.
BabelRelate! a joint multilingual approach to com-
puting semantic relatedness. In Proceedings of the
Twenty-Sixth AAAI Conference on Artificial Intelli-
gence (AAAI-12), pages 108–114, Toronto, Canada.
Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1998. The PageRank Citation
Ranking: Bringing Order to the Web. Technical re-
port, Stanford Digital Library Technologies Project.
John C. Platt, Kristina Toutanova, and Wen-tau Yih.
2010. Translingual document representations from
discriminative projections. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 251–261.
Martin Potthast, Benno Stein, and Maik Anderka.
2008. A wikipedia-based multilingual retrieval
model. In Advances in Information Retrieval, pages
522–530. Springer.
Martin Potthast, Alberto Barr´on-Cede˜no, Benno Stein,
and Paolo Rosso. 2011. Cross-language plagia-
rism detection. Language Resources and Evalua-
tion, 45(1):45–62.
Fatiha Sadat, Howard Johnson, Akakpo Agbago,
George Foster, Joel Martin, and Aaron Tikuisis.
2005. Portage: A phrase-based machine translation
system. In Proceedings of the ACL Workshop on
Building and Using Parallel Texts, Ann Arbor, USA.
Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval. In-
formation processing &amp; management, 24(5):513–
523.
Gerard Salton and Michael J. McGill. 1986. Intro-
duction to Modern Information Retrieval. McGraw-
Hill, Inc., New York, NY, USA.
Gerard Salton, Edward A. Fox, and Harry Wu. 1983.
Extended boolean information retrieval. Communi-
cations of the ACM, 26(11):1022–1036.
Bruce Thompson. 2005. Canonical correlation analy-
sis. Encyclopedia of statistics in behavioral science.
Piek Vossen. 2004. EuroWordNet: A multilin-
gual database of autonomous and language-specific
wordnets connected via an inter-lingual index. In-
ternational Journal of Lexicography, 17(2):161–
173.
Wen-tau Yih, Kristina Toutanova, John C. Platt, and
Christopher Meek. 2011. Learning discriminative
projections for text similarity measures. In Proceed-
ings of the Fifteenth Conference on Computational
Natural Language Learning, pages 247–256.
</reference>
<page confidence="0.999396">
423
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.911759">
<title confidence="0.998797">A Knowledge-based for Cross-Language Document Retrieval and Categorization</title>
<author confidence="0.998668">Paolo</author>
<affiliation confidence="0.98128175">1Department of Computer Sapienza Universit`a di Roma, 2Natural Language Engineering Lab - PRHLT Research Universitat Polit`ecnica de Val`encia,</affiliation>
<abstract confidence="0.998809307692308">Current approaches to cross-language document retrieval and categorization are based on discriminative methods which represent documents in a low-dimensional vector space. In this paper we propose a shift from the supervised to the knowledge-based paradigm and provide a document similarity measure which draws on BabelNet, a large multilingual knowledge resource. Our experiments show state-of-the-art results in cross-lingual document retrieval and categorization.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Massih-Reza Amini</author>
<author>Nicolas Usunier</author>
<author>Cyril Goutte</author>
</authors>
<title>Learning from multiple partially observed views - an application to multilingual text categorization.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems 22 (NIPS</booktitle>
<pages>28--36</pages>
<contexts>
<context position="28380" citStr="Amini et al., 2009" startWordPosition="4575" endWordPosition="4578">d in terms of accuracy is only 1 point below S2Net, the supervised state-of-the-art model using neural networks. 6.2 Cross-language Text Categorization The second task in which we tested the different models was cross-language text categorization. The task is defined as follows: given a document dL in a language L and a corpus D&apos;L, with documents in a different language L&apos;, and C possible categories, a system has to classify dL into one of the categories C using the labeled collection D&apos;L,. 6.2.1 Corpus and Task Setting Dataset To perform this task we used the Multilingual Reuters Collection (Amini et al., 2009), which is composed of five datasets of news from five different languages (English, French, German, Spanish and Italian) and classified into six possiModel Dim. EN News ES News Accuracy Accuracy KBSim N/A 0.8189 0.6997 Full MT 50 0.8483 0.6484 CosSimBN N/A 0.8023 0.6737 OPCA 100 0.8412 0.5954 CCA 150 0.8388 0.5323 CL-LSI 5000 0.8401 0.5105 CosSimE N/A 0.8046 0.4481 Table 2: Test results for cross-language text categorization. Full MT, OPCA, CCA, CL-LSI and CosSimE are from (Platt et al., 2010). ble categories. In addition, each dataset of news is translated into the other four languages using</context>
</contexts>
<marker>Amini, Usunier, Goutte, 2009</marker>
<rawString>Massih-Reza Amini, Nicolas Usunier, and Cyril Goutte. 2009. Learning from multiple partially observed views - an application to multilingual text categorization. In Advances in Neural Information Processing Systems 22 (NIPS 2009), pages 28–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Bizer</author>
<author>Jens Lehmann</author>
<author>Georgi Kobilarov</author>
<author>S¨oren Auer</author>
<author>Christian Becker</author>
<author>Richard Cyganiak</author>
<author>Sebastian Hellmann</author>
</authors>
<title>Dbpedia - a crystallization point for the web of data.</title>
<date>2009</date>
<journal>J. Web Sem.,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="2785" citStr="Bizer et al., 2009" startWordPosition="393" endWordPosition="396"> 2003) and variants thereof, which project these vectors into a lower-dimensional vector space. In order to enable multilinguality, the vectors of comparable documents written in different languages are concatenated, making up the document matrix which is then reduced using linear projection (Platt et al., 2010; Yih et al., 2011). However, to do so, comparable documents are needed as training. Additionally, the lower dimensional representations are not of easy interpretation. The availability of wide-coverage lexical knowledge resources extracted automatically from Wikipedia, such as DBPedia (Bizer et al., 2009), YAGO (Hoffart et al., 2013) and BabelNet (Navigli and Ponzetto, 2012a), has considerably boosted research in several areas, especially where multilinguality is a concern (Hovy et al., 2013). Among these latter are cross-language plagiarism detection (Potthast et al., 2011; Franco-Salvador et al., 2013), multilingual semantic relatedness (Navigli and Ponzetto, 2012b; Nastase and Strube, 2013) and semantic alignment (Navigli and Ponzetto, 2012a; Matuschek and Gurevych, 2013). One main advantage of knowledge-based methods is that they provide a human-readable, semantically interconnected, repre</context>
</contexts>
<marker>Bizer, Lehmann, Kobilarov, Auer, Becker, Cyganiak, Hellmann, 2009</marker>
<rawString>Christian Bizer, Jens Lehmann, Georgi Kobilarov, S¨oren Auer, Christian Becker, Richard Cyganiak, and Sebastian Hellmann. 2009. Dbpedia - a crystallization point for the web of data. J. Web Sem., 7(3):154–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>the Journal of machine Learning research,</journal>
<pages>3--993</pages>
<contexts>
<context position="2172" citStr="Blei et al., 2003" startWordPosition="303" endWordPosition="306">f interest on the basis of the importance of search results in other languages. Vector-based models are typically used in the literature for representing documents both in monolingual and cross-lingual settings (Manning et al., 2008). However, because of the large size of the vocabulary, having each term as a component of the vector makes the document representation very sparse. To address this issue several approaches to dimensionality reduction have been proposed, such as Principal Component Analysis (Jolliffe, 1986), Latent Semantic Indexing (Hull, 1994), Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and variants thereof, which project these vectors into a lower-dimensional vector space. In order to enable multilinguality, the vectors of comparable documents written in different languages are concatenated, making up the document matrix which is then reduced using linear projection (Platt et al., 2010; Yih et al., 2011). However, to do so, comparable documents are needed as training. Additionally, the lower dimensional representations are not of easy interpretation. The availability of wide-coverage lexical knowledge resources extracted automatically from Wikipedia, such as DBPedia (Bizer </context>
<context position="6155" citStr="Blei et al., 2003" startWordPosition="905" endWordPosition="908">is (LSA) (Deerwester et al., 1990) is very similar to PCA but performs the SVD using the correlation matrix instead of the covariance matrix, which implies a lower computational cost. LSA preserves the amount of variance in an eigenvector v� by maximizing its Rayleigh ratio: ��T ��� ��T �� , where C = DT D is the correlation matrix of D. A generalization of PCA, called Oriented Principal Component Analysis (OPCA) (Diamantaras and Kung, 1996), is based on a noise covariance matrix to project the similar components of D closely. Other projection models such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) are based on the extraction of generative models from documents. Another approach, named Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), represents each document by its similarities to a document collection. Using a low domain specificity document collection such as Wikipedia, the model has proven to obtain competitive results. Not only have these methods proven to be successful in a monolingual scenario (Deerwester et al., 1990; Hull, 1994), but they have also been adapted to perform well in tasks at a crosslanguage level (Potthast et al., 2008; Platt et al., 2010; Yih e</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. the Journal of machine Learning research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Cimiano</author>
<author>Antje Schultz</author>
<author>Sergej Sizov</author>
<author>Philipp Sorg</author>
<author>Steffen Staab</author>
</authors>
<title>Explicit versus latent concept models for cross-language information retrieval.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),</booktitle>
<volume>9</volume>
<pages>1513--1518</pages>
<contexts>
<context position="7821" citStr="Cimiano et al., 2009" startWordPosition="1158" endWordPosition="1161">in a multilingual scenario with models such as Polylingual Topic Models (Mimno et al., 2009), Joint Probabilistic LSA and Coupled Probabilistic LSA (Platt et al., 2010), which, however, are constrained to using word counts, instead of better weighting strategies, such as log(tf)-idf, known to perform better with large vocabularies (Salton and McGill, 1986). Another variant, named Canonical Correlation Analysis (CCA) (Thompson, 2005), uses a cross-covariance matrix of the low-dimensional vectors to find the projections. Cross-language Explicit Semantic Analysis (CL-ESA) (Potthast et al., 2008; Cimiano et al., 2009; Potthast et al., 2011), instead, adapts ESA to be used at crosslanguage level by exploiting the comparable documents across languages from Wikipedia. CLESA represents each document written in a language L by its similarities with a document collection in the same language L. Using a multilingual document collection with comparable documents across languages, the resulting vectors from different languages can be compared directly. An alternative unsupervised approach, Crosslanguage Character n-Grams (CL-CNG) (Mcnamee and Mayfield, 2004), does not draw upon linear projections and represents do</context>
</contexts>
<marker>Cimiano, Schultz, Sizov, Sorg, Staab, 2009</marker>
<rawString>Philipp Cimiano, Antje Schultz, Sergej Sizov, Philipp Sorg, and Steffen Staab. 2009. Explicit versus latent concept models for cross-language information retrieval. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), volume 9, pages 1513–1518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American society for information science,</journal>
<pages>41--6</pages>
<contexts>
<context position="5571" citStr="Deerwester et al., 1990" startWordPosition="803" endWordPosition="806">which provides a low-dimensional mapping from a high dimensional vector space. A historical approach to linear projection is Principal Component Analysis (PCA) (Jolliffe, 1986), which performs a singular value decomposition (SVD) on a document matrix D of size n × m, where each row in D is the term vector representation of a document. PCA uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components, which make up the low-dimensional vector. Latent Semantic Analysis (LSA) (Deerwester et al., 1990) is very similar to PCA but performs the SVD using the correlation matrix instead of the covariance matrix, which implies a lower computational cost. LSA preserves the amount of variance in an eigenvector v� by maximizing its Rayleigh ratio: ��T ��� ��T �� , where C = DT D is the correlation matrix of D. A generalization of PCA, called Oriented Principal Component Analysis (OPCA) (Diamantaras and Kung, 1996), is based on a noise covariance matrix to project the similar components of D closely. Other projection models such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) are based on th</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American society for information science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Konstantinos I Diamantaras</author>
<author>Sun Y Kung</author>
</authors>
<title>Principal component neural networks.</title>
<date>1996</date>
<publisher>Wiley</publisher>
<location>New York.</location>
<contexts>
<context position="5982" citStr="Diamantaras and Kung, 1996" startWordPosition="875" endWordPosition="878">of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components, which make up the low-dimensional vector. Latent Semantic Analysis (LSA) (Deerwester et al., 1990) is very similar to PCA but performs the SVD using the correlation matrix instead of the covariance matrix, which implies a lower computational cost. LSA preserves the amount of variance in an eigenvector v� by maximizing its Rayleigh ratio: ��T ��� ��T �� , where C = DT D is the correlation matrix of D. A generalization of PCA, called Oriented Principal Component Analysis (OPCA) (Diamantaras and Kung, 1996), is based on a noise covariance matrix to project the similar components of D closely. Other projection models such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) are based on the extraction of generative models from documents. Another approach, named Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), represents each document by its similarities to a document collection. Using a low domain specificity document collection such as Wikipedia, the model has proven to obtain competitive results. Not only have these methods proven to be successful in a monolingual scenar</context>
</contexts>
<marker>Diamantaras, Kung, 1996</marker>
<rawString>Konstantinos I. Diamantaras and Sun Y. Kung. 1996. Principal component neural networks. Wiley New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan T Dumais</author>
<author>Todd A Letsche</author>
<author>Michael L Littman</author>
<author>Thomas K Landauer</author>
</authors>
<title>Automatic cross-language retrieval using latent semantic indexing.</title>
<date>1997</date>
<booktitle>In Proc. of AAAI Spring Symposium on Cross-language Text and Speech Retrieval,</booktitle>
<pages>18--24</pages>
<contexts>
<context position="6839" citStr="Dumais et al., 1997" startWordPosition="1014" endWordPosition="1017">. Another approach, named Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), represents each document by its similarities to a document collection. Using a low domain specificity document collection such as Wikipedia, the model has proven to obtain competitive results. Not only have these methods proven to be successful in a monolingual scenario (Deerwester et al., 1990; Hull, 1994), but they have also been adapted to perform well in tasks at a crosslanguage level (Potthast et al., 2008; Platt et al., 2010; Yih et al., 2011). Cross-language Latent Semantic Indexing (CL-LSI) (Dumais et al., 1997) was the first linear projection approach used in cross-lingual tasks. CL-LSI provides a crosslingual representation for documents by reducing the dimensionality of a matrix D whose rows are obtained by concatenating comparable documents from different languages. Similarly, PCA and OPCA can be adapted to a multilingual setting. LDA was also adapted to perform in a multilingual scenario with models such as Polylingual Topic Models (Mimno et al., 2009), Joint Probabilistic LSA and Coupled Probabilistic LSA (Platt et al., 2010), which, however, are constrained to using word counts, instead of bet</context>
</contexts>
<marker>Dumais, Letsche, Littman, Landauer, 1997</marker>
<rawString>Susan T. Dumais, Todd A. Letsche, Michael L. Littman, and Thomas K. Landauer. 1997. Automatic cross-language retrieval using latent semantic indexing. In Proc. of AAAI Spring Symposium on Cross-language Text and Speech Retrieval, pages 18–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An electronic lexical database.</title>
<date>1998</date>
<publisher>Bradford Books.</publisher>
<contexts>
<context position="10810" citStr="Fellbaum, 1998" startWordPosition="1623" endWordPosition="1625">ation which does not depend on any given language, but, indeed, is multilingual. To build knowledge graphs of this kind we utilize BabelNet, a multilingual semantic network that we present in Section 3.1. Then, in Section 3.2, we describe the five steps needed to obtain our graph-based multilingual representation of documents. Finally, we introduce our knowledge graph similarity measure in Section 3.3. 3.1 BabelNet BabelNet (Navigli and Ponzetto, 2012a) is a multilingual semantic network whose concepts and relations are obtained from the largest available semantic lexicon of English, WordNet (Fellbaum, 1998), and the largest wide-coverage collaboratively-edited encyclopedia, Wikipedia, by means of an automatic mapping algorithm. BabelNet is therefore a multilingual “encyclopedic dictionary” that combines lexicographic information with wide-coverage encyclopedic knowledge. Concepts in BabelNet are represented similarly to WordNet, i.e., by grouping sets of synonyms in the different languages into multilingual synsets. Multilingual synsets contain lexicalizations from WordNet synsets, the corresponding Wikipedia pages and additional translations output by a statistical machine translation system. T</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An electronic lexical database. Bradford Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Franco-Salvador</author>
<author>Parth Gupta</author>
<author>Paolo Rosso</author>
</authors>
<title>Cross-language plagiarism detection using a multilingual semantic network.</title>
<date>2013</date>
<booktitle>In Proc. of the 35th European Conference on Information Retrieval (ECIR’13),</booktitle>
<volume>7814</volume>
<pages>710--713</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="3090" citStr="Franco-Salvador et al., 2013" startWordPosition="436" endWordPosition="439">latt et al., 2010; Yih et al., 2011). However, to do so, comparable documents are needed as training. Additionally, the lower dimensional representations are not of easy interpretation. The availability of wide-coverage lexical knowledge resources extracted automatically from Wikipedia, such as DBPedia (Bizer et al., 2009), YAGO (Hoffart et al., 2013) and BabelNet (Navigli and Ponzetto, 2012a), has considerably boosted research in several areas, especially where multilinguality is a concern (Hovy et al., 2013). Among these latter are cross-language plagiarism detection (Potthast et al., 2011; Franco-Salvador et al., 2013), multilingual semantic relatedness (Navigli and Ponzetto, 2012b; Nastase and Strube, 2013) and semantic alignment (Navigli and Ponzetto, 2012a; Matuschek and Gurevych, 2013). One main advantage of knowledge-based methods is that they provide a human-readable, semantically interconnected, representation of the textual item at hand (be it a sentence or a document). Following this trend, in this paper we provide a knowledge-based representation of documents which goes beyond the lexical surface of text, while at the same time avoiding the need for training in a cross-language setting. To achieve</context>
</contexts>
<marker>Franco-Salvador, Gupta, Rosso, 2013</marker>
<rawString>Marc Franco-Salvador, Parth Gupta, and Paolo Rosso. 2013. Cross-language plagiarism detection using a multilingual semantic network. In Proc. of the 35th European Conference on Information Retrieval (ECIR’13), volume LNCS(7814), pages 710–713. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing semantic relatedness using wikipediabased explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In Proc. of the 20th International Joint Conference on Artifical Intelligence (IJCAI),</booktitle>
<pages>1606--1611</pages>
<contexts>
<context position="6312" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="927" endWordPosition="930">which implies a lower computational cost. LSA preserves the amount of variance in an eigenvector v� by maximizing its Rayleigh ratio: ��T ��� ��T �� , where C = DT D is the correlation matrix of D. A generalization of PCA, called Oriented Principal Component Analysis (OPCA) (Diamantaras and Kung, 1996), is based on a noise covariance matrix to project the similar components of D closely. Other projection models such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) are based on the extraction of generative models from documents. Another approach, named Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), represents each document by its similarities to a document collection. Using a low domain specificity document collection such as Wikipedia, the model has proven to obtain competitive results. Not only have these methods proven to be successful in a monolingual scenario (Deerwester et al., 1990; Hull, 1994), but they have also been adapted to perform well in tasks at a crosslanguage level (Potthast et al., 2008; Platt et al., 2010; Yih et al., 2011). Cross-language Latent Semantic Indexing (CL-LSI) (Dumais et al., 1997) was the first linear projection approach used in cross-lingual tasks. CL</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using wikipediabased explicit semantic analysis. In Proc. of the 20th International Joint Conference on Artifical Intelligence (IJCAI), pages 1606–1611.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taher Haveliwala</author>
<author>Sepandar Kamvar</author>
<author>Glen Jeh</author>
</authors>
<title>An analytical comparison of approaches to personalizing pagerank.</title>
<date>2003</date>
<tech>Technical Report 2003-35,</tech>
<institution>Stanford InfoLab,</institution>
<contexts>
<context position="16676" citStr="Haveliwala et al., 2003" startWordPosition="2628" endWordPosition="2631">of the same term from being connected via a path in the resulting knowledge graph. Knowledge Graph Weighting The final step consists of weighting all the concepts and semantic relations of the knowledge graph G. For weighting relations we use the original weights from BabelNet, which provide the degree of relatedness between the synset end points of each edge (Navigli and Ponzetto, 2012a). As for concepts, we weight them on the basis of the original weights of the terms in the vector V. In order to score each concept in our knowledge graph G, we applied the topic-sensitive PageRank algorithm (Haveliwala et al., 2003) to G. While the well-known PageRank algorithm (Page et al., 1998) calculates the global importance of vertices in a graph, topic-sensitive PageRank is a variant in which the importance of vertices is biased using a set of representative “topics”. Formally, the topic-sensitive PageRank vector p�is calculated by means of an iterative process until convergence as follows: p� = cMp+(1−c)u, where c is the damping factor (conventionally set to 0.85), 1− c represents the probability of a surfer randomly jumping to any node in the graph, M is the transition probability matrix of graph G, with Mei = d</context>
</contexts>
<marker>Haveliwala, Kamvar, Jeh, 2003</marker>
<rawString>Taher Haveliwala, Sepandar Kamvar, and Glen Jeh. 2003. An analytical comparison of approaches to personalizing pagerank. Technical Report 2003-35, Stanford InfoLab, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
</authors>
<title>Using word dependent transition models in hmm based word alignment for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second</booktitle>
<contexts>
<context position="25304" citStr="He, 2007" startWordPosition="4077" endWordPosition="4078">l comparable document in the other language. We compared KBSim against the state-of-the-art supervised models S2Net, OPCA, CCA, and CLLSI (cf. Section 2). In contrast to these models, KBSim does not need a training step, so we applied it directly to the testing partition. In addition we also included the results of CL-ESA7, CL-C3G8 and two simple vector-based models which translate all documents into English on a word-by-word basis and compared them using cosine similarity: the first model (CosSimE) uses a statistical dictionary trained with Europarl using Wavelet-Domain Hidden Markov Models (He, 2007), a model similar to IBM Model 4; the second model (CosSimBN) instead uses Algorithm 1 to translate the vectors with BabelNet. 6.1.2 Results As we can see from Table 1,9 the CosSimBN model, which uses BabelNet to translate the document vectors, achieves better results than CCA and CL-LSI. We hypothesize that this is due to these linear projection models losing information during the projection. CosSimE yields results similar to CosSimBN, showing that BabelNet is a good alternative statistical dictionary. In contrast to CCA 7Document collections with sizes higher than 105 provide high performan</context>
</contexts>
<marker>He, 2007</marker>
<rawString>Xiaodong He. 2007. Using word dependent transition models in hmm based word alignment for statistical machine translation. In Proceedings of the Second</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Hoffart</author>
<author>Fabian M Suchanek</author>
<author>Klaus Berberich</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago2: A spatially and temporally enhanced knowledge base from wikipedia.</title>
<date>2013</date>
<journal>Artificial Intelligence,</journal>
<pages>194--28</pages>
<contexts>
<context position="2814" citStr="Hoffart et al., 2013" startWordPosition="398" endWordPosition="401">, which project these vectors into a lower-dimensional vector space. In order to enable multilinguality, the vectors of comparable documents written in different languages are concatenated, making up the document matrix which is then reduced using linear projection (Platt et al., 2010; Yih et al., 2011). However, to do so, comparable documents are needed as training. Additionally, the lower dimensional representations are not of easy interpretation. The availability of wide-coverage lexical knowledge resources extracted automatically from Wikipedia, such as DBPedia (Bizer et al., 2009), YAGO (Hoffart et al., 2013) and BabelNet (Navigli and Ponzetto, 2012a), has considerably boosted research in several areas, especially where multilinguality is a concern (Hovy et al., 2013). Among these latter are cross-language plagiarism detection (Potthast et al., 2011; Franco-Salvador et al., 2013), multilingual semantic relatedness (Navigli and Ponzetto, 2012b; Nastase and Strube, 2013) and semantic alignment (Navigli and Ponzetto, 2012a; Matuschek and Gurevych, 2013). One main advantage of knowledge-based methods is that they provide a human-readable, semantically interconnected, representation of the textual item</context>
</contexts>
<marker>Hoffart, Suchanek, Berberich, Weikum, 2013</marker>
<rawString>Johannes Hoffart, Fabian M. Suchanek, Klaus Berberich, and Gerhard Weikum. 2013. Yago2: A spatially and temporally enhanced knowledge base from wikipedia. Artificial Intelligence, 194:28–61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard H Hovy</author>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>Collaboratively built semistructured content and Artificial Intelligence: The story so far.</title>
<date>2013</date>
<journal>Artificial Intelligence,</journal>
<pages>194--2</pages>
<contexts>
<context position="2976" citStr="Hovy et al., 2013" startWordPosition="421" endWordPosition="424">guages are concatenated, making up the document matrix which is then reduced using linear projection (Platt et al., 2010; Yih et al., 2011). However, to do so, comparable documents are needed as training. Additionally, the lower dimensional representations are not of easy interpretation. The availability of wide-coverage lexical knowledge resources extracted automatically from Wikipedia, such as DBPedia (Bizer et al., 2009), YAGO (Hoffart et al., 2013) and BabelNet (Navigli and Ponzetto, 2012a), has considerably boosted research in several areas, especially where multilinguality is a concern (Hovy et al., 2013). Among these latter are cross-language plagiarism detection (Potthast et al., 2011; Franco-Salvador et al., 2013), multilingual semantic relatedness (Navigli and Ponzetto, 2012b; Nastase and Strube, 2013) and semantic alignment (Navigli and Ponzetto, 2012a; Matuschek and Gurevych, 2013). One main advantage of knowledge-based methods is that they provide a human-readable, semantically interconnected, representation of the textual item at hand (be it a sentence or a document). Following this trend, in this paper we provide a knowledge-based representation of documents which goes beyond the lexi</context>
</contexts>
<marker>Hovy, Navigli, Ponzetto, 2013</marker>
<rawString>Eduard H. Hovy, Roberto Navigli, and Simone Paolo Ponzetto. 2013. Collaboratively built semistructured content and Artificial Intelligence: The story so far. Artificial Intelligence, 194:2–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Hull</author>
</authors>
<title>Improving text retrieval for the routing problem using latent semantic indexing.</title>
<date>1994</date>
<booktitle>In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR),</booktitle>
<pages>282--291</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="2117" citStr="Hull, 1994" startWordPosition="297" endWordPosition="298">ut also by reranking the results in a language of interest on the basis of the importance of search results in other languages. Vector-based models are typically used in the literature for representing documents both in monolingual and cross-lingual settings (Manning et al., 2008). However, because of the large size of the vocabulary, having each term as a component of the vector makes the document representation very sparse. To address this issue several approaches to dimensionality reduction have been proposed, such as Principal Component Analysis (Jolliffe, 1986), Latent Semantic Indexing (Hull, 1994), Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and variants thereof, which project these vectors into a lower-dimensional vector space. In order to enable multilinguality, the vectors of comparable documents written in different languages are concatenated, making up the document matrix which is then reduced using linear projection (Platt et al., 2010; Yih et al., 2011). However, to do so, comparable documents are needed as training. Additionally, the lower dimensional representations are not of easy interpretation. The availability of wide-coverage lexical knowledge resources extracte</context>
<context position="6622" citStr="Hull, 1994" startWordPosition="978" endWordPosition="979">nce matrix to project the similar components of D closely. Other projection models such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) are based on the extraction of generative models from documents. Another approach, named Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), represents each document by its similarities to a document collection. Using a low domain specificity document collection such as Wikipedia, the model has proven to obtain competitive results. Not only have these methods proven to be successful in a monolingual scenario (Deerwester et al., 1990; Hull, 1994), but they have also been adapted to perform well in tasks at a crosslanguage level (Potthast et al., 2008; Platt et al., 2010; Yih et al., 2011). Cross-language Latent Semantic Indexing (CL-LSI) (Dumais et al., 1997) was the first linear projection approach used in cross-lingual tasks. CL-LSI provides a crosslingual representation for documents by reducing the dimensionality of a matrix D whose rows are obtained by concatenating comparable documents from different languages. Similarly, PCA and OPCA can be adapted to a multilingual setting. LDA was also adapted to perform in a multilingual sce</context>
</contexts>
<marker>Hull, 1994</marker>
<rawString>David Hull. 1994. Improving text retrieval for the routing problem using latent semantic indexing. In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 282–291. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald A Jackson</author>
<author>Keith M Somers</author>
<author>Harold H Harvey</author>
</authors>
<title>Similarity coefficients: measures of co-occurrence and association or simply measures of occurrence? American Naturalist,</title>
<date>1989</date>
<pages>436--453</pages>
<contexts>
<context position="18187" citStr="Jackson et al., 1989" startWordPosition="2885" endWordPosition="2888">e “topic-sensitive” variant gives more probability mass to some nodes in G and less to others. In our case we perturbate u by concentrating the probability mass to the vertices in 5K, which are the synsets corresponding to the document terms TK (cf. Formula 3). 3.3 Similarity between Knowledge Graphs We can now determine the similarity between two documents d, d&apos; E D in terms of the similarity of their knowledge graph representations G and G&apos;. Following the literature (Montes y G´omez et al., 2001) we calculate the similarity between the vertex sets in the two graphs using Dice’s coefficient (Jackson et al., 1989): 417 Figure 2: Knowledge graph examples from two comparable documents in different languages. 2 · X w(c) cEV (G)nV (G0) X w(c) + X cEV (G) cEV (G0) where w(c) is the weight of a concept c (see Section 3.2). Likewise, we calculate the similarity between the two edge sets as: 2 · X w(r) rEE(G)nE(G0) X w(r) + rEE(G0) where w(r) is the weight of a semantic relation edge r. We combine the two above measures of conceptual (Sc) and relational (Sr) similarity to obtain an integrated measure Sg(G, G&apos;) between knowledge graphs: Notably, since we are working with a languageindependent representation of </context>
</contexts>
<marker>Jackson, Somers, Harvey, 1989</marker>
<rawString>Donald A. Jackson, Keith M. Somers, and Harold H. Harvey. 1989. Similarity coefficients: measures of co-occurrence and association or simply measures of occurrence? American Naturalist, pages 436–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian T Jolliffe</author>
</authors>
<title>Principal component analysis, volume 487.</title>
<date>1986</date>
<publisher>Springer-Verlag</publisher>
<location>New York.</location>
<contexts>
<context position="2078" citStr="Jolliffe, 1986" startWordPosition="292" endWordPosition="293">elevant results into different languages, but also by reranking the results in a language of interest on the basis of the importance of search results in other languages. Vector-based models are typically used in the literature for representing documents both in monolingual and cross-lingual settings (Manning et al., 2008). However, because of the large size of the vocabulary, having each term as a component of the vector makes the document representation very sparse. To address this issue several approaches to dimensionality reduction have been proposed, such as Principal Component Analysis (Jolliffe, 1986), Latent Semantic Indexing (Hull, 1994), Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and variants thereof, which project these vectors into a lower-dimensional vector space. In order to enable multilinguality, the vectors of comparable documents written in different languages are concatenated, making up the document matrix which is then reduced using linear projection (Platt et al., 2010; Yih et al., 2011). However, to do so, comparable documents are needed as training. Additionally, the lower dimensional representations are not of easy interpretation. The availability of wide-covera</context>
<context position="5123" citStr="Jolliffe, 1986" startWordPosition="733" endWordPosition="734">, whose components quantify the relevance of each term in the document, is usually highly dimensional, because of the variety of terms used in a document collection. As a consequence, the resulting document matrices are very sparse. To address the data sparsity issue, several approaches to the reduction of dimensionality of document vectors have been proposed in the literature. A popular class of methods is based on linear projection, which provides a low-dimensional mapping from a high dimensional vector space. A historical approach to linear projection is Principal Component Analysis (PCA) (Jolliffe, 1986), which performs a singular value decomposition (SVD) on a document matrix D of size n × m, where each row in D is the term vector representation of a document. PCA uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components, which make up the low-dimensional vector. Latent Semantic Analysis (LSA) (Deerwester et al., 1990) is very similar to PCA but performs the SVD using the correlation matrix instead of the covariance matrix, which implies a lower computational cost. LS</context>
</contexts>
<marker>Jolliffe, 1986</marker>
<rawString>Ian T. Jolliffe. 1986. Principal component analysis, volume 487. Springer-Verlag New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Michael L Littman</author>
</authors>
<title>Computerized cross-language document retrieval using latent semantic indexing,</title>
<date>1994</date>
<tech>US Patent 5,301,109.</tech>
<contexts>
<context position="1418" citStr="Landauer and Littman, 1994" startWordPosition="188" endWordPosition="191">-of-the-art results in cross-lingual document retrieval and categorization. 1 Introduction The huge amount of text that is available online is becoming ever increasingly multilingual, providing an additional wealth of useful information. Most of this information, however, is not easily accessible to the majority of users because of language barriers which hamper the cross-lingual search and retrieval of knowledge. Today’s search engines would benefit greatly from effective techniques for the cross-lingual retrieval of valuable information that can satisfy a user’s needs by not only providing (Landauer and Littman, 1994) and translating (Munteanu and Marcu, 2005) relevant results into different languages, but also by reranking the results in a language of interest on the basis of the importance of search results in other languages. Vector-based models are typically used in the literature for representing documents both in monolingual and cross-lingual settings (Manning et al., 2008). However, because of the large size of the vocabulary, having each term as a component of the vector makes the document representation very sparse. To address this issue several approaches to dimensionality reduction have been pro</context>
</contexts>
<marker>Landauer, Littman, 1994</marker>
<rawString>Thomas K. Landauer and Michael L. Littman. 1994. Computerized cross-language document retrieval using latent semantic indexing, April 5. US Patent 5,301,109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA.</location>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to Information Retrieval. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Matuschek</author>
<author>Iryna Gurevych</author>
</authors>
<title>Dijkstra-WSA: A graph-based approach to word sense alignment.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics (TACL),</journal>
<pages>1--151</pages>
<contexts>
<context position="3264" citStr="Matuschek and Gurevych, 2013" startWordPosition="458" endWordPosition="461">rpretation. The availability of wide-coverage lexical knowledge resources extracted automatically from Wikipedia, such as DBPedia (Bizer et al., 2009), YAGO (Hoffart et al., 2013) and BabelNet (Navigli and Ponzetto, 2012a), has considerably boosted research in several areas, especially where multilinguality is a concern (Hovy et al., 2013). Among these latter are cross-language plagiarism detection (Potthast et al., 2011; Franco-Salvador et al., 2013), multilingual semantic relatedness (Navigli and Ponzetto, 2012b; Nastase and Strube, 2013) and semantic alignment (Navigli and Ponzetto, 2012a; Matuschek and Gurevych, 2013). One main advantage of knowledge-based methods is that they provide a human-readable, semantically interconnected, representation of the textual item at hand (be it a sentence or a document). Following this trend, in this paper we provide a knowledge-based representation of documents which goes beyond the lexical surface of text, while at the same time avoiding the need for training in a cross-language setting. To achieve this we leverage a multilingual semantic network, i.e., BabelNet, to obtain language-independent representations, which contain concepts together with semantic relations bet</context>
</contexts>
<marker>Matuschek, Gurevych, 2013</marker>
<rawString>Michael Matuschek and Iryna Gurevych. 2013. Dijkstra-WSA: A graph-based approach to word sense alignment. Transactions of the Association for Computational Linguistics (TACL), 1:151–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Mcnamee</author>
<author>James Mayfield</author>
</authors>
<title>Character n-gram tokenization for european language text retrieval.</title>
<date>2004</date>
<journal>Information Retrieval,</journal>
<pages>7--1</pages>
<contexts>
<context position="8364" citStr="Mcnamee and Mayfield, 2004" startWordPosition="1240" endWordPosition="1244">age Explicit Semantic Analysis (CL-ESA) (Potthast et al., 2008; Cimiano et al., 2009; Potthast et al., 2011), instead, adapts ESA to be used at crosslanguage level by exploiting the comparable documents across languages from Wikipedia. CLESA represents each document written in a language L by its similarities with a document collection in the same language L. Using a multilingual document collection with comparable documents across languages, the resulting vectors from different languages can be compared directly. An alternative unsupervised approach, Crosslanguage Character n-Grams (CL-CNG) (Mcnamee and Mayfield, 2004), does not draw upon linear projections and represents documents as vectors of character n-grams. It has proven to obtain good results in cross-language document retrieval (Potthast et al., 2011) between languages with lexical and syntactic similarities. Recently, a novel supervised linear projection model based on Siamese Neural Networks (S2Net) (Yih et al., 2011) achieved state-of-theart performance in comparable document retrieval. S2Net performs a linear combination of the terms of a document vector d� to obtain a reduced vector r, which is the output layer of a neural network. Each elemen</context>
<context position="26120" citStr="Mcnamee and Mayfield, 2004" startWordPosition="4207" endWordPosition="4210"> which uses BabelNet to translate the document vectors, achieves better results than CCA and CL-LSI. We hypothesize that this is due to these linear projection models losing information during the projection. CosSimE yields results similar to CosSimBN, showing that BabelNet is a good alternative statistical dictionary. In contrast to CCA 7Document collections with sizes higher than 105 provide high performance (Potthast et al., 2008). Here we used 15k documents from the training set to index the test documents. 8CL-C3G is CL-CNG using character 3-grams, which has proven to be the best length (Mcnamee and Mayfield, 2004). 9In this work, statistically significant results according to a x2 test are highlighted in bold. Sv(�vLL0,�v0LL0) = (7) JJ�vLL0JJ JJ�v0LL0JJ. �v0LL0 VLL0 · 419 Model Dimension Accuracy MMR S2Net 2000 0.7447 0.7973 KBSim N/A 0.7342 0.7750 OPCA 2000 0.7255 0.7734 CosSimE N/A 0.7033 0.7467 CosSimBN N/A 0.7029 0.7550 CCA 1500 0.6894 0.7378 CL-LSI 5000 0.5302 0.6130 CL-ESA 15000 0.2660 0.3305 CL-C3G N/A 0.2511 0.3025 Table 1: Test results for comparable document retrieval in Wikipedia. S2Net, OPCA, CosSimE, CCA and CL-LSI are from (Yih et al., 2011). and CL-LSI, OPCA performs better thanks to its</context>
</contexts>
<marker>Mcnamee, Mayfield, 2004</marker>
<rawString>Paul Mcnamee and James Mayfield. 2004. Character n-gram tokenization for european language text retrieval. Information Retrieval, 7(1-2):73–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Hanna M Wallach</author>
<author>Jason Naradowsky</author>
<author>David A Smith</author>
<author>Andrew McCallum</author>
</authors>
<title>Polylingual topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>2</volume>
<pages>880--889</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7293" citStr="Mimno et al., 2009" startWordPosition="1083" endWordPosition="1086"> tasks at a crosslanguage level (Potthast et al., 2008; Platt et al., 2010; Yih et al., 2011). Cross-language Latent Semantic Indexing (CL-LSI) (Dumais et al., 1997) was the first linear projection approach used in cross-lingual tasks. CL-LSI provides a crosslingual representation for documents by reducing the dimensionality of a matrix D whose rows are obtained by concatenating comparable documents from different languages. Similarly, PCA and OPCA can be adapted to a multilingual setting. LDA was also adapted to perform in a multilingual scenario with models such as Polylingual Topic Models (Mimno et al., 2009), Joint Probabilistic LSA and Coupled Probabilistic LSA (Platt et al., 2010), which, however, are constrained to using word counts, instead of better weighting strategies, such as log(tf)-idf, known to perform better with large vocabularies (Salton and McGill, 1986). Another variant, named Canonical Correlation Analysis (CCA) (Thompson, 2005), uses a cross-covariance matrix of the low-dimensional vectors to find the projections. Cross-language Explicit Semantic Analysis (CL-ESA) (Potthast et al., 2008; Cimiano et al., 2009; Potthast et al., 2011), instead, adapts ESA to be used at crosslanguag</context>
</contexts>
<marker>Mimno, Wallach, Naradowsky, Smith, McCallum, 2009</marker>
<rawString>David Mimno, Hanna M. Wallach, Jason Naradowsky, David A. Smith, and Andrew McCallum. 2009. Polylingual topic models. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume 2, pages 880–889. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manuel Montes y G´omez</author>
<author>Alexander F Gelbukh</author>
<author>Aurelio L´opez-L´opez</author>
<author>Ricardo A Baeza-Yates</author>
</authors>
<title>Flexible comparison of conceptual graphs.</title>
<date>2001</date>
<booktitle>In Proc. of the 12th International Conference on Database and Expert Systems Applications (DEXA),</booktitle>
<pages>102--111</pages>
<marker>G´omez, Gelbukh, L´opez-L´opez, Baeza-Yates, 2001</marker>
<rawString>Manuel Montes y G´omez, Alexander F. Gelbukh, Aurelio L´opez-L´opez, and Ricardo A. Baeza-Yates. 2001. Flexible comparison of conceptual graphs. In Proc. of the 12th International Conference on Database and Expert Systems Applications (DEXA), pages 102–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragos Stefan Munteanu</author>
<author>Daniel Marcu</author>
</authors>
<title>Improving machine translation performance by exploiting non-parallel corpora.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>4</issue>
<contexts>
<context position="1461" citStr="Munteanu and Marcu, 2005" startWordPosition="194" endWordPosition="197"> retrieval and categorization. 1 Introduction The huge amount of text that is available online is becoming ever increasingly multilingual, providing an additional wealth of useful information. Most of this information, however, is not easily accessible to the majority of users because of language barriers which hamper the cross-lingual search and retrieval of knowledge. Today’s search engines would benefit greatly from effective techniques for the cross-lingual retrieval of valuable information that can satisfy a user’s needs by not only providing (Landauer and Littman, 1994) and translating (Munteanu and Marcu, 2005) relevant results into different languages, but also by reranking the results in a language of interest on the basis of the importance of search results in other languages. Vector-based models are typically used in the literature for representing documents both in monolingual and cross-lingual settings (Manning et al., 2008). However, because of the large size of the vocabulary, having each term as a component of the vector makes the document representation very sparse. To address this issue several approaches to dimensionality reduction have been proposed, such as Principal Component Analysis</context>
</contexts>
<marker>Munteanu, Marcu, 2005</marker>
<rawString>Dragos Stefan Munteanu and Daniel Marcu. 2005. Improving machine translation performance by exploiting non-parallel corpora. Computational Linguistics, 31(4):477–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivi Nastase</author>
<author>Michael Strube</author>
</authors>
<title>Transforming wikipedia into a large scale multilingual concept network.</title>
<date>2013</date>
<journal>Artificial Intelligence,</journal>
<pages>194--62</pages>
<contexts>
<context position="3181" citStr="Nastase and Strube, 2013" startWordPosition="447" endWordPosition="450">ining. Additionally, the lower dimensional representations are not of easy interpretation. The availability of wide-coverage lexical knowledge resources extracted automatically from Wikipedia, such as DBPedia (Bizer et al., 2009), YAGO (Hoffart et al., 2013) and BabelNet (Navigli and Ponzetto, 2012a), has considerably boosted research in several areas, especially where multilinguality is a concern (Hovy et al., 2013). Among these latter are cross-language plagiarism detection (Potthast et al., 2011; Franco-Salvador et al., 2013), multilingual semantic relatedness (Navigli and Ponzetto, 2012b; Nastase and Strube, 2013) and semantic alignment (Navigli and Ponzetto, 2012a; Matuschek and Gurevych, 2013). One main advantage of knowledge-based methods is that they provide a human-readable, semantically interconnected, representation of the textual item at hand (be it a sentence or a document). Following this trend, in this paper we provide a knowledge-based representation of documents which goes beyond the lexical surface of text, while at the same time avoiding the need for training in a cross-language setting. To achieve this we leverage a multilingual semantic network, i.e., BabelNet, to obtain language-indep</context>
</contexts>
<marker>Nastase, Strube, 2013</marker>
<rawString>Vivi Nastase and Michael Strube. 2013. Transforming wikipedia into a large scale multilingual concept network. Artificial Intelligence, 194:62–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Mirella Lapata</author>
</authors>
<title>An experimental study of graph connectivity for unsupervised word sense disambiguation.</title>
<date>2010</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="14851" citStr="Navigli and Lapata (2010)" startWordPosition="2293" endWordPosition="2296">re the original concepts. �5K = 5ynsetsL(t), (3) tETK where 5ynsetsL(t) is the set of synsets in BabelNet which contain a term t in the language of interest L. For example, in Figure 1(a) we show the initial graph obtained from the set TK = {“European”, “apple”, “tree”, “Malus”, “species”, “America”}. Note, however, that each retrieved synset is multilingual, i.e., it contains lexicalizations for the same concept in other languages too. Therefore, the nodes of our knowledge graph provide a language-independent representation of the document’s content. Creating the Knowledge Graph Similarly to Navigli and Lapata (2010), we create the knowledge graph by searching BabelNet for paths connecting pairs of synsets in V . Formally, for each pair v, v&apos; E V such that v and v&apos; do not share any lexicalization4 in TK, for each path in BabelNet v → v1 → .. . → vn → v&apos;, we set: V := V U {v1, ... , vn} and E := EU{(v, v1), ... , (vn, v&apos;)}, that is, we add all the path vertices and edges to G. After prototyping, the path length is limited to maximum length 3, so as to avoid an excessive semantic drift. As a result of populating the graph with intermediate edges and vertices, we obtain a knowledge graph which models the sem</context>
</contexts>
<marker>Navigli, Lapata, 2010</marker>
<rawString>Roberto Navigli and Mirella Lapata. 2010. An experimental study of graph connectivity for unsupervised word sense disambiguation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(4):678–692.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network.</title>
<date>2012</date>
<journal>Artificial Intelligence,</journal>
<volume>193</volume>
<pages>250</pages>
<contexts>
<context position="2855" citStr="Navigli and Ponzetto, 2012" startWordPosition="404" endWordPosition="407"> lower-dimensional vector space. In order to enable multilinguality, the vectors of comparable documents written in different languages are concatenated, making up the document matrix which is then reduced using linear projection (Platt et al., 2010; Yih et al., 2011). However, to do so, comparable documents are needed as training. Additionally, the lower dimensional representations are not of easy interpretation. The availability of wide-coverage lexical knowledge resources extracted automatically from Wikipedia, such as DBPedia (Bizer et al., 2009), YAGO (Hoffart et al., 2013) and BabelNet (Navigli and Ponzetto, 2012a), has considerably boosted research in several areas, especially where multilinguality is a concern (Hovy et al., 2013). Among these latter are cross-language plagiarism detection (Potthast et al., 2011; Franco-Salvador et al., 2013), multilingual semantic relatedness (Navigli and Ponzetto, 2012b; Nastase and Strube, 2013) and semantic alignment (Navigli and Ponzetto, 2012a; Matuschek and Gurevych, 2013). One main advantage of knowledge-based methods is that they provide a human-readable, semantically interconnected, representation of the textual item at hand (be it a sentence or a document)</context>
<context position="10650" citStr="Navigli and Ponzetto, 2012" startWordPosition="1598" endWordPosition="1601">in a document’s bag of words by means of a knowledge graph which provides concepts and semantic relations between them. Key to our approach is the use of a graph representation which does not depend on any given language, but, indeed, is multilingual. To build knowledge graphs of this kind we utilize BabelNet, a multilingual semantic network that we present in Section 3.1. Then, in Section 3.2, we describe the five steps needed to obtain our graph-based multilingual representation of documents. Finally, we introduce our knowledge graph similarity measure in Section 3.3. 3.1 BabelNet BabelNet (Navigli and Ponzetto, 2012a) is a multilingual semantic network whose concepts and relations are obtained from the largest available semantic lexicon of English, WordNet (Fellbaum, 1998), and the largest wide-coverage collaboratively-edited encyclopedia, Wikipedia, by means of an automatic mapping algorithm. BabelNet is therefore a multilingual “encyclopedic dictionary” that combines lexicographic information with wide-coverage encyclopedic knowledge. Concepts in BabelNet are represented similarly to WordNet, i.e., by grouping sets of synonyms in the different languages into multilingual synsets. Multilingual synsets c</context>
<context position="16441" citStr="Navigli and Ponzetto, 2012" startWordPosition="2583" endWordPosition="2586">he document content. Figure 1(b) shows the knowledge graph obtained for our example term set. Note that our approach retains, and therefore weights, only the subgraph focused on the “apple fruit” meaning. 4This prevents different senses of the same term from being connected via a path in the resulting knowledge graph. Knowledge Graph Weighting The final step consists of weighting all the concepts and semantic relations of the knowledge graph G. For weighting relations we use the original weights from BabelNet, which provide the degree of relatedness between the synset end points of each edge (Navigli and Ponzetto, 2012a). As for concepts, we weight them on the basis of the original weights of the terms in the vector V. In order to score each concept in our knowledge graph G, we applied the topic-sensitive PageRank algorithm (Haveliwala et al., 2003) to G. While the well-known PageRank algorithm (Page et al., 1998) calculates the global importance of vertices in a graph, topic-sensitive PageRank is a variant in which the importance of vertices is biased using a set of representative “topics”. Formally, the topic-sensitive PageRank vector p�is calculated by means of an iterative process until convergence as f</context>
<context position="21519" citStr="Navigli and Ponzetto, 2012" startWordPosition="3459" endWordPosition="3462">the vector and counterbalance possible wrong translations, we avoid translating all vectors to one language. Instead, in the present work we create a multilingual vector representation of a 5To make the translation possible, while at the same time keeping the same number of dimensions in our vector representation, we use a shared vocabulary which covers both languages. See Section 6 for details on the experimental setup. 6Non-English lexicalizations in BabelNet have confidence 1 if originating from Wikipedia inter-language links and &lt; 1 if obtained by means of statistical machine translation (Navigli and Ponzetto, 2012a). Sg(G, G&apos;) = Sc(G, G&apos;) + Sr(G, G&apos;) (6) 2 . Sc(G, G&apos;) = , (4) w(c) Sr(G,G&apos;) = X rEE(G) w(r) , (5) 418 document d written in language L by concatenating the corresponding vector VL with the translated vector VL0 of d for language L&apos;. As a result, we obtain a multilingual vector VLL0, which contains lexicalizations in both languages. 4.2 Similarity between Multilingual Vectors Following common practice for document similarity in the literature (Manning et al., 2008), we use the cosine similarity as the similarity measure between multilingual vectors: 5 Knowledge-based Document Similarity Given</context>
</contexts>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2012a. BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network. Artificial Intelligence, 193:217– 250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>BabelRelate! a joint multilingual approach to computing semantic relatedness.</title>
<date>2012</date>
<booktitle>In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence (AAAI-12),</booktitle>
<pages>108--114</pages>
<location>Toronto, Canada.</location>
<contexts>
<context position="2855" citStr="Navigli and Ponzetto, 2012" startWordPosition="404" endWordPosition="407"> lower-dimensional vector space. In order to enable multilinguality, the vectors of comparable documents written in different languages are concatenated, making up the document matrix which is then reduced using linear projection (Platt et al., 2010; Yih et al., 2011). However, to do so, comparable documents are needed as training. Additionally, the lower dimensional representations are not of easy interpretation. The availability of wide-coverage lexical knowledge resources extracted automatically from Wikipedia, such as DBPedia (Bizer et al., 2009), YAGO (Hoffart et al., 2013) and BabelNet (Navigli and Ponzetto, 2012a), has considerably boosted research in several areas, especially where multilinguality is a concern (Hovy et al., 2013). Among these latter are cross-language plagiarism detection (Potthast et al., 2011; Franco-Salvador et al., 2013), multilingual semantic relatedness (Navigli and Ponzetto, 2012b; Nastase and Strube, 2013) and semantic alignment (Navigli and Ponzetto, 2012a; Matuschek and Gurevych, 2013). One main advantage of knowledge-based methods is that they provide a human-readable, semantically interconnected, representation of the textual item at hand (be it a sentence or a document)</context>
<context position="10650" citStr="Navigli and Ponzetto, 2012" startWordPosition="1598" endWordPosition="1601">in a document’s bag of words by means of a knowledge graph which provides concepts and semantic relations between them. Key to our approach is the use of a graph representation which does not depend on any given language, but, indeed, is multilingual. To build knowledge graphs of this kind we utilize BabelNet, a multilingual semantic network that we present in Section 3.1. Then, in Section 3.2, we describe the five steps needed to obtain our graph-based multilingual representation of documents. Finally, we introduce our knowledge graph similarity measure in Section 3.3. 3.1 BabelNet BabelNet (Navigli and Ponzetto, 2012a) is a multilingual semantic network whose concepts and relations are obtained from the largest available semantic lexicon of English, WordNet (Fellbaum, 1998), and the largest wide-coverage collaboratively-edited encyclopedia, Wikipedia, by means of an automatic mapping algorithm. BabelNet is therefore a multilingual “encyclopedic dictionary” that combines lexicographic information with wide-coverage encyclopedic knowledge. Concepts in BabelNet are represented similarly to WordNet, i.e., by grouping sets of synonyms in the different languages into multilingual synsets. Multilingual synsets c</context>
<context position="16441" citStr="Navigli and Ponzetto, 2012" startWordPosition="2583" endWordPosition="2586">he document content. Figure 1(b) shows the knowledge graph obtained for our example term set. Note that our approach retains, and therefore weights, only the subgraph focused on the “apple fruit” meaning. 4This prevents different senses of the same term from being connected via a path in the resulting knowledge graph. Knowledge Graph Weighting The final step consists of weighting all the concepts and semantic relations of the knowledge graph G. For weighting relations we use the original weights from BabelNet, which provide the degree of relatedness between the synset end points of each edge (Navigli and Ponzetto, 2012a). As for concepts, we weight them on the basis of the original weights of the terms in the vector V. In order to score each concept in our knowledge graph G, we applied the topic-sensitive PageRank algorithm (Haveliwala et al., 2003) to G. While the well-known PageRank algorithm (Page et al., 1998) calculates the global importance of vertices in a graph, topic-sensitive PageRank is a variant in which the importance of vertices is biased using a set of representative “topics”. Formally, the topic-sensitive PageRank vector p�is calculated by means of an iterative process until convergence as f</context>
<context position="21519" citStr="Navigli and Ponzetto, 2012" startWordPosition="3459" endWordPosition="3462">the vector and counterbalance possible wrong translations, we avoid translating all vectors to one language. Instead, in the present work we create a multilingual vector representation of a 5To make the translation possible, while at the same time keeping the same number of dimensions in our vector representation, we use a shared vocabulary which covers both languages. See Section 6 for details on the experimental setup. 6Non-English lexicalizations in BabelNet have confidence 1 if originating from Wikipedia inter-language links and &lt; 1 if obtained by means of statistical machine translation (Navigli and Ponzetto, 2012a). Sg(G, G&apos;) = Sc(G, G&apos;) + Sr(G, G&apos;) (6) 2 . Sc(G, G&apos;) = , (4) w(c) Sr(G,G&apos;) = X rEE(G) w(r) , (5) 418 document d written in language L by concatenating the corresponding vector VL with the translated vector VL0 of d for language L&apos;. As a result, we obtain a multilingual vector VLL0, which contains lexicalizations in both languages. 4.2 Similarity between Multilingual Vectors Following common practice for document similarity in the literature (Manning et al., 2008), we use the cosine similarity as the similarity measure between multilingual vectors: 5 Knowledge-based Document Similarity Given</context>
</contexts>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2012b. BabelRelate! a joint multilingual approach to computing semantic relatedness. In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence (AAAI-12), pages 108–114, Toronto, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Page</author>
<author>Sergey Brin</author>
<author>Rajeev Motwani</author>
<author>Terry Winograd</author>
</authors>
<title>The PageRank Citation Ranking: Bringing Order to the Web.</title>
<date>1998</date>
<tech>Technical report,</tech>
<institution>Stanford Digital Library Technologies Project.</institution>
<contexts>
<context position="16742" citStr="Page et al., 1998" startWordPosition="2639" endWordPosition="2642">dge graph. Knowledge Graph Weighting The final step consists of weighting all the concepts and semantic relations of the knowledge graph G. For weighting relations we use the original weights from BabelNet, which provide the degree of relatedness between the synset end points of each edge (Navigli and Ponzetto, 2012a). As for concepts, we weight them on the basis of the original weights of the terms in the vector V. In order to score each concept in our knowledge graph G, we applied the topic-sensitive PageRank algorithm (Haveliwala et al., 2003) to G. While the well-known PageRank algorithm (Page et al., 1998) calculates the global importance of vertices in a graph, topic-sensitive PageRank is a variant in which the importance of vertices is biased using a set of representative “topics”. Formally, the topic-sensitive PageRank vector p�is calculated by means of an iterative process until convergence as follows: p� = cMp+(1−c)u, where c is the damping factor (conventionally set to 0.85), 1− c represents the probability of a surfer randomly jumping to any node in the graph, M is the transition probability matrix of graph G, with Mei = degree(i)−1 if an edge from i to j exists, 0 otherwise, u is the ra</context>
</contexts>
<marker>Page, Brin, Motwani, Winograd, 1998</marker>
<rawString>Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1998. The PageRank Citation Ranking: Bringing Order to the Web. Technical report, Stanford Digital Library Technologies Project.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Platt</author>
<author>Kristina Toutanova</author>
<author>Wen-tau Yih</author>
</authors>
<title>Translingual document representations from discriminative projections.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>251--261</pages>
<contexts>
<context position="2478" citStr="Platt et al., 2010" startWordPosition="349" endWordPosition="352">as a component of the vector makes the document representation very sparse. To address this issue several approaches to dimensionality reduction have been proposed, such as Principal Component Analysis (Jolliffe, 1986), Latent Semantic Indexing (Hull, 1994), Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and variants thereof, which project these vectors into a lower-dimensional vector space. In order to enable multilinguality, the vectors of comparable documents written in different languages are concatenated, making up the document matrix which is then reduced using linear projection (Platt et al., 2010; Yih et al., 2011). However, to do so, comparable documents are needed as training. Additionally, the lower dimensional representations are not of easy interpretation. The availability of wide-coverage lexical knowledge resources extracted automatically from Wikipedia, such as DBPedia (Bizer et al., 2009), YAGO (Hoffart et al., 2013) and BabelNet (Navigli and Ponzetto, 2012a), has considerably boosted research in several areas, especially where multilinguality is a concern (Hovy et al., 2013). Among these latter are cross-language plagiarism detection (Potthast et al., 2011; Franco-Salvador e</context>
<context position="6748" citStr="Platt et al., 2010" startWordPosition="1000" endWordPosition="1003">(LDA) (Blei et al., 2003) are based on the extraction of generative models from documents. Another approach, named Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), represents each document by its similarities to a document collection. Using a low domain specificity document collection such as Wikipedia, the model has proven to obtain competitive results. Not only have these methods proven to be successful in a monolingual scenario (Deerwester et al., 1990; Hull, 1994), but they have also been adapted to perform well in tasks at a crosslanguage level (Potthast et al., 2008; Platt et al., 2010; Yih et al., 2011). Cross-language Latent Semantic Indexing (CL-LSI) (Dumais et al., 1997) was the first linear projection approach used in cross-lingual tasks. CL-LSI provides a crosslingual representation for documents by reducing the dimensionality of a matrix D whose rows are obtained by concatenating comparable documents from different languages. Similarly, PCA and OPCA can be adapted to a multilingual setting. LDA was also adapted to perform in a multilingual scenario with models such as Polylingual Topic Models (Mimno et al., 2009), Joint Probabilistic LSA and Coupled Probabilistic LSA</context>
<context position="9238" citStr="Platt et al., 2010" startWordPosition="1378" endWordPosition="1381">ecently, a novel supervised linear projection model based on Siamese Neural Networks (S2Net) (Yih et al., 2011) achieved state-of-theart performance in comparable document retrieval. S2Net performs a linear combination of the terms of a document vector d� to obtain a reduced vector r, which is the output layer of a neural network. Each element in r� has a weight which is a linear combination of the original weights of d, and captures relationships between the original terms. However, linear projection approaches need a high number of training documents to achieve state-of-the-art performance (Platt et al., 2010; Yih et al., 2011). Moreover, although they are good at identifying a few principal components, 415 the representations produced are opaque, in that they cannot explicitly model the semantic content of documents with a human-interpretable representation, thereby making the data analysis difficult. In this paper, instead, we propose a languageindependent knowledge graph representation for documents which is obtained from a large multilingual semantic network, without using any training information. Our knowledge graph representation explicitly models the semantics of the document in terms of t</context>
<context position="13774" citStr="Platt et al., 2010" startWordPosition="2119" endWordPosition="2122">et of terms TK.3 The value of K is calculated as a function of the vector size, as follows: K = (log2(1 + |V|))2, (2) The rationale is that K must be high enough to ensure a good conceptual representation but not too high, so as to avoid as much noise as possible in the set TK. Populating the Graph with Initial Concepts Next, we create an initially-empty knowledge graph G = (V, E), i.e., such that V = E = ∅. We populate the vertex set V with the set SK of all the synsets in BabelNet which contain any term in TK in the document language L, that is: 1http://babelnet.org 2Following the setup of (Platt et al., 2010), our initial data is represented using term vectors. For this reason we lemmatize in this step. 3Since the vector v� provides weights for all the word forms, and not only lemmas, occurring in d, we take the best weight among those word forms of the considered lemma. 416 Figure 1: (a) initial graph from TK = {“European”, “apple”, “tree”, “Malus”, “species”, “America”}; (b) knowledge graph obtained by retrieving all paths from BabelNet. Gray nodes are the original concepts. �5K = 5ynsetsL(t), (3) tETK where 5ynsetsL(t) is the set of synsets in BabelNet which contain a term t in the language of </context>
<context position="23620" citStr="Platt et al., 2010" startWordPosition="3805" endWordPosition="3808">eval and crosslingual text categorization. 6.1 Comparable Document Retrieval In our first experiment we determine the effectiveness of our knowledge-based approach in a comparable document retrieval task. Given a document d written in language L and a collection DL0 of documents written in another language L&apos;, the task of comparable document retrieval consists of finding the document in DL0 which is most similar to d, under the assumption that there exists one document d&apos; ∈ DL0 which is comparable with d. 6.1.1 Corpus and Task Setting Dataset We followed the experimental setting described in (Platt et al., 2010; Yih et al., 2011) and evaluated KBSim on the Wikipedia dataset made available by the authors of those papers. The dataset is composed of Wikipedia comparable encyclopedic entries in English and Spanish. For each document in English there exists a “real” pair in Spanish which was defined as a comparable entry by the Wikipedia user community. The dataset of each language was split into three parts: 43,380 training, 8,675 development and 8,675 test documents. The documents were tokenized, without stemming, and represented as vectors using a log(tf)-idf weighting (Salton and Buckley, 1988). The </context>
<context position="28879" citStr="Platt et al., 2010" startWordPosition="4657" endWordPosition="4660">.1 Corpus and Task Setting Dataset To perform this task we used the Multilingual Reuters Collection (Amini et al., 2009), which is composed of five datasets of news from five different languages (English, French, German, Spanish and Italian) and classified into six possiModel Dim. EN News ES News Accuracy Accuracy KBSim N/A 0.8189 0.6997 Full MT 50 0.8483 0.6484 CosSimBN N/A 0.8023 0.6737 OPCA 100 0.8412 0.5954 CCA 150 0.8388 0.5323 CL-LSI 5000 0.8401 0.5105 CosSimE N/A 0.8046 0.4481 Table 2: Test results for cross-language text categorization. Full MT, OPCA, CCA, CL-LSI and CosSimE are from (Platt et al., 2010). ble categories. In addition, each dataset of news is translated into the other four languages using the Portage translation system (Sadat et al., 2005). As a result, we have five different multilingual datasets, each containing source news documents in one language and four sets of translated documents in the other languages. Each of the languages has an independent vocabulary. Document vectors in the collection are created using TFIDFbased weighting. Methodology To evaluate our approach we used the English and Spanish news datasets. From the English news dataset we randomly selected 13,131 </context>
<context position="30465" citStr="Platt et al., 2010" startWordPosition="4918" endWordPosition="4921"> selecting the documents in English and using the Spanish documents in the training dataset, and vice versa. We followed Platt et al. (2010) and averaged the values obtained from the two comparisons for each test set to obtain the final result. To categorize the documents we applied k-NN to the ranked list of documents according to the similarity measure employed for each model. We evaluated each model by estimating its accuracy in the classification of the English and Spanish test sets. We compared our approach against the stateof-the-art supervised models in this task: OPCA, CCA and CL-LSI (Platt et al., 2010). In addition, we include the results of the CosSimBN and CosSimE models that we introduced in Section 6.1.1, as well as the results of a full statistical machine translation system trained with Europarl and post-processed by LSA (Full MT), as reported by Platt et al. (2010). 420 6.2.2 Results Table 2 shows the cross-language text categorization accuracy. CosSimE obtained the lowest results. This is because there is a significant number of untranslated terms in the translation process that the statistical dictionary cannot cover. This is not the case in the CosSimBN model which achieves higher</context>
</contexts>
<marker>Platt, Toutanova, Yih, 2010</marker>
<rawString>John C. Platt, Kristina Toutanova, and Wen-tau Yih. 2010. Translingual document representations from discriminative projections. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 251–261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Potthast</author>
<author>Benno Stein</author>
<author>Maik Anderka</author>
</authors>
<title>A wikipedia-based multilingual retrieval model.</title>
<date>2008</date>
<booktitle>In Advances in Information Retrieval,</booktitle>
<pages>522--530</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="6728" citStr="Potthast et al., 2008" startWordPosition="996" endWordPosition="999">t Dirichlet Allocation (LDA) (Blei et al., 2003) are based on the extraction of generative models from documents. Another approach, named Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), represents each document by its similarities to a document collection. Using a low domain specificity document collection such as Wikipedia, the model has proven to obtain competitive results. Not only have these methods proven to be successful in a monolingual scenario (Deerwester et al., 1990; Hull, 1994), but they have also been adapted to perform well in tasks at a crosslanguage level (Potthast et al., 2008; Platt et al., 2010; Yih et al., 2011). Cross-language Latent Semantic Indexing (CL-LSI) (Dumais et al., 1997) was the first linear projection approach used in cross-lingual tasks. CL-LSI provides a crosslingual representation for documents by reducing the dimensionality of a matrix D whose rows are obtained by concatenating comparable documents from different languages. Similarly, PCA and OPCA can be adapted to a multilingual setting. LDA was also adapted to perform in a multilingual scenario with models such as Polylingual Topic Models (Mimno et al., 2009), Joint Probabilistic LSA and Coupl</context>
<context position="25930" citStr="Potthast et al., 2008" startWordPosition="4175" endWordPosition="4178">model similar to IBM Model 4; the second model (CosSimBN) instead uses Algorithm 1 to translate the vectors with BabelNet. 6.1.2 Results As we can see from Table 1,9 the CosSimBN model, which uses BabelNet to translate the document vectors, achieves better results than CCA and CL-LSI. We hypothesize that this is due to these linear projection models losing information during the projection. CosSimE yields results similar to CosSimBN, showing that BabelNet is a good alternative statistical dictionary. In contrast to CCA 7Document collections with sizes higher than 105 provide high performance (Potthast et al., 2008). Here we used 15k documents from the training set to index the test documents. 8CL-C3G is CL-CNG using character 3-grams, which has proven to be the best length (Mcnamee and Mayfield, 2004). 9In this work, statistically significant results according to a x2 test are highlighted in bold. Sv(�vLL0,�v0LL0) = (7) JJ�vLL0JJ JJ�v0LL0JJ. �v0LL0 VLL0 · 419 Model Dimension Accuracy MMR S2Net 2000 0.7447 0.7973 KBSim N/A 0.7342 0.7750 OPCA 2000 0.7255 0.7734 CosSimE N/A 0.7033 0.7467 CosSimBN N/A 0.7029 0.7550 CCA 1500 0.6894 0.7378 CL-LSI 5000 0.5302 0.6130 CL-ESA 15000 0.2660 0.3305 CL-C3G N/A 0.2511</context>
<context position="27265" citStr="Potthast et al. (2008)" startWordPosition="4391" endWordPosition="4394">I are from (Yih et al., 2011). and CL-LSI, OPCA performs better thanks to its improved projection method using a noise covariance matrix, which enables it to obtain the main components in a low-dimensional space. CL-C3G and CL-ESA obtain the lowest results. Considering that English and Spanish do not have many lexical similarities, the low performance of CL-C3G is justified because these languages do not share many character n-grams. The reason behind the low results of CL-ESA can be explained by the low number of intersecting concepts between Spanish and English in Wikipedia, as confirmed by Potthast et al. (2008). Despite both using Wikipedia in some way, KBSim obtains much higher performance than CL-ESA thanks to the use of our multilingual knowledge graph representation of documents, which makes it possible to expand and semantically relate its original concepts. As a result, in contrast to CL-ESA, KBSim can integrate conceptual and relational similarity functions which provide more accurate performance. Interestingly, KBSim also outperforms OPCA which, in contrast to our system, is supervised, and in terms of accuracy is only 1 point below S2Net, the supervised state-of-the-art model using neural n</context>
</contexts>
<marker>Potthast, Stein, Anderka, 2008</marker>
<rawString>Martin Potthast, Benno Stein, and Maik Anderka. 2008. A wikipedia-based multilingual retrieval model. In Advances in Information Retrieval, pages 522–530. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Potthast</author>
<author>Alberto Barr´on-Cede˜no</author>
<author>Benno Stein</author>
<author>Paolo Rosso</author>
</authors>
<title>Cross-language plagiarism detection.</title>
<date>2011</date>
<journal>Language Resources and Evaluation,</journal>
<volume>45</volume>
<issue>1</issue>
<marker>Potthast, Barr´on-Cede˜no, Stein, Rosso, 2011</marker>
<rawString>Martin Potthast, Alberto Barr´on-Cede˜no, Benno Stein, and Paolo Rosso. 2011. Cross-language plagiarism detection. Language Resources and Evaluation, 45(1):45–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fatiha Sadat</author>
<author>Howard Johnson</author>
<author>Akakpo Agbago</author>
<author>George Foster</author>
<author>Joel Martin</author>
<author>Aaron Tikuisis</author>
</authors>
<title>Portage: A phrase-based machine translation system.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Building and Using Parallel Texts,</booktitle>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="29032" citStr="Sadat et al., 2005" startWordPosition="4681" endWordPosition="4684">ets of news from five different languages (English, French, German, Spanish and Italian) and classified into six possiModel Dim. EN News ES News Accuracy Accuracy KBSim N/A 0.8189 0.6997 Full MT 50 0.8483 0.6484 CosSimBN N/A 0.8023 0.6737 OPCA 100 0.8412 0.5954 CCA 150 0.8388 0.5323 CL-LSI 5000 0.8401 0.5105 CosSimE N/A 0.8046 0.4481 Table 2: Test results for cross-language text categorization. Full MT, OPCA, CCA, CL-LSI and CosSimE are from (Platt et al., 2010). ble categories. In addition, each dataset of news is translated into the other four languages using the Portage translation system (Sadat et al., 2005). As a result, we have five different multilingual datasets, each containing source news documents in one language and four sets of translated documents in the other languages. Each of the languages has an independent vocabulary. Document vectors in the collection are created using TFIDFbased weighting. Methodology To evaluate our approach we used the English and Spanish news datasets. From the English news dataset we randomly selected 13,131 news as training and 1,875 as test documents. From the Spanish news dataset we selected all 12,342 news as test documents. To classify both test sets we </context>
</contexts>
<marker>Sadat, Johnson, Agbago, Foster, Martin, Tikuisis, 2005</marker>
<rawString>Fatiha Sadat, Howard Johnson, Akakpo Agbago, George Foster, Joel Martin, and Aaron Tikuisis. 2005. Portage: A phrase-based machine translation system. In Proceedings of the ACL Workshop on Building and Using Parallel Texts, Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Christopher Buckley</author>
</authors>
<title>Termweighting approaches in automatic text retrieval.</title>
<date>1988</date>
<journal>Information processing &amp; management,</journal>
<volume>24</volume>
<issue>5</issue>
<pages>523</pages>
<contexts>
<context position="24214" citStr="Salton and Buckley, 1988" startWordPosition="3900" endWordPosition="3903">escribed in (Platt et al., 2010; Yih et al., 2011) and evaluated KBSim on the Wikipedia dataset made available by the authors of those papers. The dataset is composed of Wikipedia comparable encyclopedic entries in English and Spanish. For each document in English there exists a “real” pair in Spanish which was defined as a comparable entry by the Wikipedia user community. The dataset of each language was split into three parts: 43,380 training, 8,675 development and 8,675 test documents. The documents were tokenized, without stemming, and represented as vectors using a log(tf)-idf weighting (Salton and Buckley, 1988). The vocabulary of the corpus was restricted to 20,000 terms, which were the most frequent terms in the two languages after removing the top 50 terms. Methodology To evaluate the models we compared each English document against the Spanish dataset and vice versa. Following the original setting, the results are given as the average performance between these two experiments. For evaluation we employed the averaged top-1 accuracy and Mean Reciprocal Rank (MMR) at finding the real comparable document in the other language. We compared KBSim against the state-of-the-art supervised models S2Net, OP</context>
</contexts>
<marker>Salton, Buckley, 1988</marker>
<rawString>Gerard Salton and Christopher Buckley. 1988. Termweighting approaches in automatic text retrieval. Information processing &amp; management, 24(5):513– 523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Michael J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1986</date>
<publisher>McGrawHill, Inc.,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7559" citStr="Salton and McGill, 1986" startWordPosition="1123" endWordPosition="1126">ngual representation for documents by reducing the dimensionality of a matrix D whose rows are obtained by concatenating comparable documents from different languages. Similarly, PCA and OPCA can be adapted to a multilingual setting. LDA was also adapted to perform in a multilingual scenario with models such as Polylingual Topic Models (Mimno et al., 2009), Joint Probabilistic LSA and Coupled Probabilistic LSA (Platt et al., 2010), which, however, are constrained to using word counts, instead of better weighting strategies, such as log(tf)-idf, known to perform better with large vocabularies (Salton and McGill, 1986). Another variant, named Canonical Correlation Analysis (CCA) (Thompson, 2005), uses a cross-covariance matrix of the low-dimensional vectors to find the projections. Cross-language Explicit Semantic Analysis (CL-ESA) (Potthast et al., 2008; Cimiano et al., 2009; Potthast et al., 2011), instead, adapts ESA to be used at crosslanguage level by exploiting the comparable documents across languages from Wikipedia. CLESA represents each document written in a language L by its similarities with a document collection in the same language L. Using a multilingual document collection with comparable doc</context>
<context position="12526" citStr="Salton and McGill, 1986" startWordPosition="1877" endWordPosition="1880">an, English, French, German, Italian and Spanish. 3.2 From Document to Knowledge Graph We now introduce our five-step method for representing a given document d from a collection D of documents written in language L as a languageindependent knowledge graph. Building a Basic Vector Representation Initially we transform a document d into a traditional vector representation. To do this, we score each term ti E d with a weight wi. This weight is usually a function of term and document frequency. Following the literature, one method that works well is the log tf-idf weighting (Salton et al., 1983; Salton and McGill, 1986): wi = log2(fi + 1)log2(n/ni). (1) where fi is the number of times term i occurs in document d, n is the total number of documents in the collection and ni is the number of documents that contain ti. We then create a weighted term vector v� = (w1, ..., wn), where wi is the weight corresponding to term ti. We exclude stopwords from the vector. Selecting the Relevant Document Terms We then create the set T of base forms, i.e., lemmas2, of the terms in the document d. In order to keep only the most relevant terms, we sort the terms T according to their weight in vector v� and retain a maximum num</context>
</contexts>
<marker>Salton, McGill, 1986</marker>
<rawString>Gerard Salton and Michael J. McGill. 1986. Introduction to Modern Information Retrieval. McGrawHill, Inc., New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Edward A Fox</author>
<author>Harry Wu</author>
</authors>
<title>Extended boolean information retrieval.</title>
<date>1983</date>
<journal>Communications of the ACM,</journal>
<volume>26</volume>
<issue>11</issue>
<contexts>
<context position="12500" citStr="Salton et al., 1983" startWordPosition="1873" endWordPosition="1876">guages, namely: Catalan, English, French, German, Italian and Spanish. 3.2 From Document to Knowledge Graph We now introduce our five-step method for representing a given document d from a collection D of documents written in language L as a languageindependent knowledge graph. Building a Basic Vector Representation Initially we transform a document d into a traditional vector representation. To do this, we score each term ti E d with a weight wi. This weight is usually a function of term and document frequency. Following the literature, one method that works well is the log tf-idf weighting (Salton et al., 1983; Salton and McGill, 1986): wi = log2(fi + 1)log2(n/ni). (1) where fi is the number of times term i occurs in document d, n is the total number of documents in the collection and ni is the number of documents that contain ti. We then create a weighted term vector v� = (w1, ..., wn), where wi is the weight corresponding to term ti. We exclude stopwords from the vector. Selecting the Relevant Document Terms We then create the set T of base forms, i.e., lemmas2, of the terms in the document d. In order to keep only the most relevant terms, we sort the terms T according to their weight in vector v</context>
</contexts>
<marker>Salton, Fox, Wu, 1983</marker>
<rawString>Gerard Salton, Edward A. Fox, and Harry Wu. 1983. Extended boolean information retrieval. Communications of the ACM, 26(11):1022–1036.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruce Thompson</author>
</authors>
<title>Canonical correlation analysis. Encyclopedia of statistics in behavioral science.</title>
<date>2005</date>
<contexts>
<context position="7637" citStr="Thompson, 2005" startWordPosition="1135" endWordPosition="1136">ows are obtained by concatenating comparable documents from different languages. Similarly, PCA and OPCA can be adapted to a multilingual setting. LDA was also adapted to perform in a multilingual scenario with models such as Polylingual Topic Models (Mimno et al., 2009), Joint Probabilistic LSA and Coupled Probabilistic LSA (Platt et al., 2010), which, however, are constrained to using word counts, instead of better weighting strategies, such as log(tf)-idf, known to perform better with large vocabularies (Salton and McGill, 1986). Another variant, named Canonical Correlation Analysis (CCA) (Thompson, 2005), uses a cross-covariance matrix of the low-dimensional vectors to find the projections. Cross-language Explicit Semantic Analysis (CL-ESA) (Potthast et al., 2008; Cimiano et al., 2009; Potthast et al., 2011), instead, adapts ESA to be used at crosslanguage level by exploiting the comparable documents across languages from Wikipedia. CLESA represents each document written in a language L by its similarities with a document collection in the same language L. Using a multilingual document collection with comparable documents across languages, the resulting vectors from different languages can be</context>
</contexts>
<marker>Thompson, 2005</marker>
<rawString>Bruce Thompson. 2005. Canonical correlation analysis. Encyclopedia of statistics in behavioral science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piek Vossen</author>
</authors>
<title>EuroWordNet: A multilingual database of autonomous and language-specific wordnets connected via an inter-lingual index.</title>
<date>2004</date>
<journal>International Journal of Lexicography,</journal>
<volume>17</volume>
<issue>2</issue>
<pages>173</pages>
<contexts>
<context position="11652" citStr="Vossen, 2004" startWordPosition="1733" endWordPosition="1734">th wide-coverage encyclopedic knowledge. Concepts in BabelNet are represented similarly to WordNet, i.e., by grouping sets of synonyms in the different languages into multilingual synsets. Multilingual synsets contain lexicalizations from WordNet synsets, the corresponding Wikipedia pages and additional translations output by a statistical machine translation system. The relations between synsets are collected from WordNet and from Wikipedia’s hyperlinks between pages. We note that, in principle, we could use any multilingual network providing a similar kind of information, e.g., EuroWordNet (Vossen, 2004). However, in our work we chose BabelNet because of its larger size, its coverage of both lexicographic and encyclopedic knowledge, and its free availability.1 In our work we used BabelNet 1.0, which encodes knowledge for six languages, namely: Catalan, English, French, German, Italian and Spanish. 3.2 From Document to Knowledge Graph We now introduce our five-step method for representing a given document d from a collection D of documents written in language L as a languageindependent knowledge graph. Building a Basic Vector Representation Initially we transform a document d into a traditiona</context>
</contexts>
<marker>Vossen, 2004</marker>
<rawString>Piek Vossen. 2004. EuroWordNet: A multilingual database of autonomous and language-specific wordnets connected via an inter-lingual index. International Journal of Lexicography, 17(2):161– 173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-tau Yih</author>
<author>Kristina Toutanova</author>
<author>John C Platt</author>
<author>Christopher Meek</author>
</authors>
<title>Learning discriminative projections for text similarity measures.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>247--256</pages>
<contexts>
<context position="2497" citStr="Yih et al., 2011" startWordPosition="353" endWordPosition="356">e vector makes the document representation very sparse. To address this issue several approaches to dimensionality reduction have been proposed, such as Principal Component Analysis (Jolliffe, 1986), Latent Semantic Indexing (Hull, 1994), Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and variants thereof, which project these vectors into a lower-dimensional vector space. In order to enable multilinguality, the vectors of comparable documents written in different languages are concatenated, making up the document matrix which is then reduced using linear projection (Platt et al., 2010; Yih et al., 2011). However, to do so, comparable documents are needed as training. Additionally, the lower dimensional representations are not of easy interpretation. The availability of wide-coverage lexical knowledge resources extracted automatically from Wikipedia, such as DBPedia (Bizer et al., 2009), YAGO (Hoffart et al., 2013) and BabelNet (Navigli and Ponzetto, 2012a), has considerably boosted research in several areas, especially where multilinguality is a concern (Hovy et al., 2013). Among these latter are cross-language plagiarism detection (Potthast et al., 2011; Franco-Salvador et al., 2013), multi</context>
<context position="6767" citStr="Yih et al., 2011" startWordPosition="1004" endWordPosition="1007">2003) are based on the extraction of generative models from documents. Another approach, named Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), represents each document by its similarities to a document collection. Using a low domain specificity document collection such as Wikipedia, the model has proven to obtain competitive results. Not only have these methods proven to be successful in a monolingual scenario (Deerwester et al., 1990; Hull, 1994), but they have also been adapted to perform well in tasks at a crosslanguage level (Potthast et al., 2008; Platt et al., 2010; Yih et al., 2011). Cross-language Latent Semantic Indexing (CL-LSI) (Dumais et al., 1997) was the first linear projection approach used in cross-lingual tasks. CL-LSI provides a crosslingual representation for documents by reducing the dimensionality of a matrix D whose rows are obtained by concatenating comparable documents from different languages. Similarly, PCA and OPCA can be adapted to a multilingual setting. LDA was also adapted to perform in a multilingual scenario with models such as Polylingual Topic Models (Mimno et al., 2009), Joint Probabilistic LSA and Coupled Probabilistic LSA (Platt et al., 201</context>
<context position="8731" citStr="Yih et al., 2011" startWordPosition="1297" endWordPosition="1300">ngual document collection with comparable documents across languages, the resulting vectors from different languages can be compared directly. An alternative unsupervised approach, Crosslanguage Character n-Grams (CL-CNG) (Mcnamee and Mayfield, 2004), does not draw upon linear projections and represents documents as vectors of character n-grams. It has proven to obtain good results in cross-language document retrieval (Potthast et al., 2011) between languages with lexical and syntactic similarities. Recently, a novel supervised linear projection model based on Siamese Neural Networks (S2Net) (Yih et al., 2011) achieved state-of-theart performance in comparable document retrieval. S2Net performs a linear combination of the terms of a document vector d� to obtain a reduced vector r, which is the output layer of a neural network. Each element in r� has a weight which is a linear combination of the original weights of d, and captures relationships between the original terms. However, linear projection approaches need a high number of training documents to achieve state-of-the-art performance (Platt et al., 2010; Yih et al., 2011). Moreover, although they are good at identifying a few principal componen</context>
<context position="23639" citStr="Yih et al., 2011" startWordPosition="3809" endWordPosition="3812">l text categorization. 6.1 Comparable Document Retrieval In our first experiment we determine the effectiveness of our knowledge-based approach in a comparable document retrieval task. Given a document d written in language L and a collection DL0 of documents written in another language L&apos;, the task of comparable document retrieval consists of finding the document in DL0 which is most similar to d, under the assumption that there exists one document d&apos; ∈ DL0 which is comparable with d. 6.1.1 Corpus and Task Setting Dataset We followed the experimental setting described in (Platt et al., 2010; Yih et al., 2011) and evaluated KBSim on the Wikipedia dataset made available by the authors of those papers. The dataset is composed of Wikipedia comparable encyclopedic entries in English and Spanish. For each document in English there exists a “real” pair in Spanish which was defined as a comparable entry by the Wikipedia user community. The dataset of each language was split into three parts: 43,380 training, 8,675 development and 8,675 test documents. The documents were tokenized, without stemming, and represented as vectors using a log(tf)-idf weighting (Salton and Buckley, 1988). The vocabulary of the c</context>
<context position="26672" citStr="Yih et al., 2011" startWordPosition="4294" endWordPosition="4297">ich has proven to be the best length (Mcnamee and Mayfield, 2004). 9In this work, statistically significant results according to a x2 test are highlighted in bold. Sv(�vLL0,�v0LL0) = (7) JJ�vLL0JJ JJ�v0LL0JJ. �v0LL0 VLL0 · 419 Model Dimension Accuracy MMR S2Net 2000 0.7447 0.7973 KBSim N/A 0.7342 0.7750 OPCA 2000 0.7255 0.7734 CosSimE N/A 0.7033 0.7467 CosSimBN N/A 0.7029 0.7550 CCA 1500 0.6894 0.7378 CL-LSI 5000 0.5302 0.6130 CL-ESA 15000 0.2660 0.3305 CL-C3G N/A 0.2511 0.3025 Table 1: Test results for comparable document retrieval in Wikipedia. S2Net, OPCA, CosSimE, CCA and CL-LSI are from (Yih et al., 2011). and CL-LSI, OPCA performs better thanks to its improved projection method using a noise covariance matrix, which enables it to obtain the main components in a low-dimensional space. CL-C3G and CL-ESA obtain the lowest results. Considering that English and Spanish do not have many lexical similarities, the low performance of CL-C3G is justified because these languages do not share many character n-grams. The reason behind the low results of CL-ESA can be explained by the low number of intersecting concepts between Spanish and English in Wikipedia, as confirmed by Potthast et al. (2008). Despi</context>
</contexts>
<marker>Yih, Toutanova, Platt, Meek, 2011</marker>
<rawString>Wen-tau Yih, Kristina Toutanova, John C. Platt, and Christopher Meek. 2011. Learning discriminative projections for text similarity measures. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 247–256.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>