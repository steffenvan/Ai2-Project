<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.996095">
Hypothesis Mixture Decoding for Statistical Machine Translation
</title>
<author confidence="0.999674">
Nan Duan, Mu Li, and Ming Zhou
</author>
<affiliation confidence="0.860187666666667">
School of Computer Science and Technology Natural Language Computing Group
Tianjin University Microsoft Research Asia
Tianjin, China Beijing, China
</affiliation>
<email confidence="0.997742">
v-naduan@microsoft.com {muli,mingzhou}@microsoft.com
</email>
<sectionHeader confidence="0.998588" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9990806">
This paper presents hypothesis mixture decoding
(HM decoding), a new decoding scheme that
performs translation reconstruction using hypo-
theses generated by multiple translation systems.
HM decoding involves two decoding stages:
first, each component system decodes indepen-
dently, with the explored search space kept for
use in the next step; second, a new search space
is constructed by composing existing hypotheses
produced by all component systems using a set
of rules provided by the HM decoder itself, and
a new set of model independent features are
used to seek the final best translation from this
new search space. Few assumptions are made by
our approach about the underlying component
systems, enabling us to leverage SMT models
based on arbitrary paradigms. We compare our
approach with several related techniques, and
demonstrate significant BLEU improvements in
large-scale Chinese-to-English translation tasks.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961591836735">
Besides tremendous efforts on constructing more
complicated and accurate models for statistical
machine translation (SMT) (Och and Ney, 2004;
Chiang, 2005; Galley et al., 2006; Shen et al., 2008;
Chiang 2010), many researchers have concentrated
on the approaches that improve translation quality
using information between hypotheses from one or
more SMT systems as well.
System combination is built on top of the N-best
outputs generated by multiple component systems
(Rosti et al., 2007; He et al., 2008; Li et al., 2009b)
which aligns multiple hypotheses to build confu-
sion networks as new search spaces, and outputs
the highest scoring paths as the final translations.
Consensus decoding, on the other hand, can be
based on either single or multiple systems: single
system based methods (Kumar and Byrne, 2004;
Tromble et al., 2008; DeNero et al., 2009; Kumar
et al., 2009) re-rank translations produced by a
single SMT model using either n-gram posteriors
or expected n-gram counts. Because hypotheses
generated by a single model are highly correlated,
improvements obtained are usually small; recently,
dedicated efforts have been made to extend it from
single system to multiple systems (Li et al., 2009a;
DeNero et al., 2010; Duan et al., 2010). Such me-
thods select translations by optimizing consensus
models over the combined hypotheses using all
component systems’ posterior distributions.
Although these two types of approaches have
shown consistent improvements over the standard
Maximum a Posteriori (MAP) decoding scheme,
most of them are implemented as post-processing
procedures over translations generated by MAP
decoders. In this sense, the work of Li et al. (2009a)
is different in that both partial and full hypotheses
are re-ranked during the decoding phase directly
using consensus between translations from differ-
ent SMT systems. However, their method does not
change component systems’ search spaces.
This paper presents hypothesis mixture decoding
(HM decoding), a new decoding scheme that per-
forms translation reconstruction using hypotheses
generated by multiple component systems. HM
decoding involves two decoding stages: first, each
component system decodes the source sentence
independently, with the explored search space kept
for use in the next step; second, a new search
space is constructed by composing existing hypo-
</bodyText>
<page confidence="0.934014">
1258
</page>
<note confidence="0.992708">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1258–1267,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<figureCaption confidence="0.8132315">
Figure 1: A decoding example of a phrase-based
SMT system. Each hypothesis is annotated with a
feature vector, which includes a logarithmic probabil-
ity feature and a word count feature.
</figureCaption>
<bodyText confidence="0.999984214285714">
theses produced by all component systems using a
set of rules provided by the HM decoder itself, and
a new set of component model independent fea-
tures are used to seek the final best translation
from this new constructed search space.
We evaluate by combining two SMT models
with state-of-the-art performances on the NIST
Chinese-to-English translation tasks. Experimental
results show that our approach outperforms the
best component SMT system by up to 2.11 BLEU
points. Consistent improvements can be observed
over several related decoding techniques as well,
including word-level system combination, colla-
borative decoding and model combination.
</bodyText>
<sectionHeader confidence="0.873291" genericHeader="method">
2 Hypothesis Mixture Decoding
</sectionHeader>
<subsectionHeader confidence="0.996349">
2.1 Motivation and Overview
</subsectionHeader>
<bodyText confidence="0.995468071428572">
SMT models based on different paradigms have
emerged in the last decade using fairly different
levels of linguistic knowledge. Motivated by the
success of system combination research, the key
contribution of this work is to make more effective
use of the extended search spaces from different
SMT models in decoding phase directly, rather
than just post-processing their final outputs. We
first begin with a brief review of single system
based SMT decoding, and then illustrate major
challenges to this end.
Given a source sentence f , an SMT decoder
seeks for a target translation a that best matches f
as its translation by maximizing the following
conditional probability:
where q5(•) is the feature vector that includes a set
of system specific features, 9 is the weight vector,
D(a) is a derivation that can yield a and is defined
as a sequence of translation rule applications {r).
Figure 1 illustrates a decoding example, in which
the final translation is generated by recursively
composing partial hypotheses that cover different
ranges of the source sentence until the whole input
sentence is fully covered, and the feature vector of
the final translation is the aggregation of feature
vectors of all partial hypotheses used.1
However, hypotheses generated by different
SMT systems cannot be combined directly to form
new translations because of two major issues:
The first one is the heterogeneous structures of
different SMT models. For example, a string-to-
tree system cannot use hypotheses generated by a
phrase-based system in decoding procedure, as
such hypotheses are based on flat structures, which
cannot provide any additional information needed
in the syntactic model.
The second one is the incompatible feature
spaces of different SMT models. For example,
even if a phrase-based system can use the lexical
forms of hypotheses generated by a syntax-based
system without considering syntactic structures,
the feature vectors of these hypotheses still cannot
be aggregated together in any trivial way, because
the feature sets of SMT models based on different
paradigms are usually inconsistent.
To address these two issues discussed above, we
propose HM decoding that performs translation
reconstruction using hypotheses generated by mul-
tiple component systems.2 Our method involves
two decoding stages depicted as follows:
1. Independent decoding stage, in which each
component system decodes input sentences
independently based on its own model and
search algorithm, and the explored search
spaces (translation forests) are kept for use in
the next stage.
</bodyText>
<footnote confidence="0.8267861">
1 There are also features independent of translation deriva-
tions, such as the language model feature.
2 In this paper, we will constrain our discussions within CKY-
style decoders, in which we find translations for all spans of
the source sentence. Although standard implementations of
phrase-based decoders fall out of this scope, they can be still
re-written to work in the CKY-style bottom-up manner at the
cost of 1) only BTG-style reordering allowed, and 2) higher
time complexity. As a result, any phrase-based SMT system
can be used as a component in our HM decoding method.
</footnote>
<figure confidence="0.954784285714286">
China ’s economic growth
[-2.48, 4]
China ‘s
[-1.05, 2]
economic growth
[-1.43, 2]
China
[-0.36, 1]
’s
[-0.69, 1]
economic
[-0.51, 1]
growth
[-0.92, 1]
rp19
n
0VF
AN
1259
China ‘s economic growth
development of economy of China
</figure>
<figureCaption confidence="0.97068">
Figure 2: An example of HM decoding, in which the
translations surrounded by the dotted lines are newly
generated hypotheses. Hypotheses light-shaded come
from a phrase-based system, and hypotheses dark-
shaded come from a syntax-based system.
</figureCaption>
<bodyText confidence="0.999101857142857">
2. HM decoding stage, where a mixture search
space is constructed for translation derivations
by composing partial hypotheses generated by
all component systems, and a new decoding
model with a set of enriched feature functions
are used to seek final translations from this
newly generated search space.
HM decoding can use lexicalized hypotheses of
arbitrary SMT models to derive translation, and a
set of component model independent features are
used to compute translation confidence. We dis-
cuss mixture search space construction, details of
model and feature designs as well as HM decoding
algorithms in Section 2.2, 2.3 and 2.4 respectively.
</bodyText>
<subsectionHeader confidence="0.999169">
2.2 Mixture Search Space Construction
</subsectionHeader>
<bodyText confidence="0.992936192307692">
Let denote component MT systems,
denote the span of a source sentence starting
at position and ending at position . We use
denoting the search space of predicted
by , and denoting the mixture search
space of constructed by the HM decoder, which
is defined recursively as follows:
 . This rule adds all compo-
nent systems’ search spaces into the mixture
search space for use in HM decoding. Thus
hypotheses produced by all component sys-
tems are still available to the HM decoder.
 , in which
and . is a translation
rule provided by HM decoder that composes a
new hypothesis using smaller hypotheses in
the search spaces . These
rules further extend with hypotheses
generated by the HM decoder itself.
Figure 2 shows an example of HM decoding, in
which hypotheses generated by two SMT systems
are used together to compose new translations.
Since search space pruning is the indispensable
procedure for all SMT systems, we will omit its
explicit expression in the following descriptions
and algorithms for convenience.
</bodyText>
<subsectionHeader confidence="0.998009">
2.3 Models and Features
</subsectionHeader>
<bodyText confidence="0.999877235294118">
Following the common practice in SMT research,
we use a linear model to formulate the preference
of translation hypotheses in the mixture search
space . Formally, we are to find a translation
that maximizes the weighted linear combination
of a set of real-valued features as follows:
where is an HM decoding feature with its
corresponding feature weight .
In this paper, the HM decoder does not assume
the availability of any internal knowledge of the
underlying component systems. The HM decoding
features are independent of component models as
well, which fall into two categories:
The first category contains a set of consensus-
based features, which are inspired by the success
of consensus decoding approaches. These features
are described in details as follows:
</bodyText>
<listItem confidence="0.366353">
1) : the n-gram posterior feature of
</listItem>
<bodyText confidence="0.929355142857143">
computed based on the component search
space generated by :
is
the posterior probability of an n-gram in
, is the number of times that
occurs in , equals to 1 when occurs
in , and 0 otherwise.
</bodyText>
<figure confidence="0.935271">
China ’s development of economy
economic growth of China
China ’s
of China
economic growth
development of economy
... Rules provided by
the HM decoder
0VF AN
rp19 n
</figure>
<page confidence="0.806564">
1260
</page>
<listItem confidence="0.956920277777777">
2) h.?Cn( )(es, f): the stemmed n-gram posterior
feature of e computed based on the stemmed
component search space .?Cns (f). A word stem
dictionary that includes 22,660 entries is used
to convert e and .?Cn (f) into their stem forms
es and .?Cns (f) by replacing each word into its
stem form. This feature is computed similarly
to that of h.?Cn(f) (e, f).
3) h.?C(n (e, f): the n-gram posterior feature of e
computed based on the mixture search space
.?C (f) generated by the HM decoder:
1) hLength(e, f ): the word count feature.
2) hLM (e, f ): the language model feature.
3) hDict (e, f) : the dictionary-based feature that
counts how many lexicon pairs can be found
in a given translation pair (e, f).
4) h[.l (e, f) and h&lt;., (e, f) : reordering features
that penalize the uses of straight and inverted
</listItem>
<bodyText confidence="0.6698564">
BTG rules during the derivation of e in HM
decoding. These two features are specific to
BTG-based HM decoding (Section 2.4.1):
P(wJ.?C(f)) _ �e&apos;E.?C(f) 6 u,(e&apos;)P(e&apos;J f) is the
posterior probability of an n-gram w in .?C(f ),
</bodyText>
<equation confidence="0.814336666666667">
&apos;
(e J f) is the posterior probability of one
translation e&apos; given f based on .?C (f).
</equation>
<bodyText confidence="0.994608538461538">
4) hl (e, f) : the length posterior feature of the
specific target hypothesis with length I based
on the mixture search space .?C (f) generated
by the HM decoder:
5) hRule (e, f) and hciue (e, f) : reordering fea-
tures that penalize the uses of hierarchical and
glue rules during the derivation of e in HM
decoding. These two features are specific to
SCFG-based HM decoding (Section 2.4.2):
Note here that features in h.?C(f) (e, f) and hl (e, f)
will be computed when the computations of all the
remainder features in two categories have already
finished for each e in .?C (f), and they will be used
to update current HM decoding model scores.
Consensus features based on component search
spaces have already shown effectiveness (Kumar
et al., 2009; DeNero et al., 2010; Duan et al.,
2010). We leverage consensus features based on
the mixture search space newly generated in HM
decoding as well. The length posterior feature (Zen
and Ney, 2006) is used to adjust the preference of
HM decoder for longer or shorter translations, and
the stemmed n-gram posterior features are used to
provide more discriminative power for HM decod-
ing and to decrease the effects of morphological
changes in words for more accurate computation
of consensus statistics.
The second feature category contains a set of
general features. Although there are more features
that can be incorporated into HM decoding besides
the ones we list below, we only utilize the most
representative ones for convenience:
R is the hierarchical rule set provided by the
HM decoder itself, 6,.(R) equals to 1 when r
is provided by R, and 0 otherwise.
6) hNe (e, f ): the feature that counts how many
n-grams in e are newly generated by the HM
decoder, which cannot be found in all existing
component search spaces:
</bodyText>
<equation confidence="0.599194">
6u, (UIV
n=1 .?Cn (f )) equals to 1 when w does
not exist in U .?Cn(f)Nn=1, and 0 otherwise.
</equation>
<bodyText confidence="0.9996305">
The MERT algorithm (Och, 2003) is used to
tune weights of HM decoding features.
</bodyText>
<subsectionHeader confidence="0.997774">
2.4 Decoding Algorithms
</subsectionHeader>
<bodyText confidence="0.999898">
Two CKY-style algorithms for HM decoding are
presented in this subsection. The first one is based
on BTG (Wu, 1997), and the second one is based
on SCFG, similar to Chiang (2005).
</bodyText>
<page confidence="0.957359">
1261
</page>
<subsectionHeader confidence="0.399077">
2.4.1 BTG-based HM Decoding
</subsectionHeader>
<bodyText confidence="0.965677333333333">
The first algorithm, BTG-HMD, is presented in
Algorithm 1, where hypotheses of two consecutive
source spans are composed using two BTG rules:
</bodyText>
<listItem confidence="0.826728333333333">
• Straight rule . It combines translations of
two consecutive blocks into a single larger
block in a straight order.
• Inverted rule . It combines translations of
two consecutive blocks into a single larger
block in an inverted order.
</listItem>
<bodyText confidence="0.9777665">
These two rules are used bottom-up until the
whole source sentence is fully covered. We use
two reordering rule penalty features, and
, to penalize the uses of these two rules.
</bodyText>
<construct confidence="0.426714">
Algorithm 1: BTG-based HM Decoding
</construct>
<listItem confidence="0.937163347826087">
1: for each component model do
2: output the search space for the input
3: end for
4: for to do
5: for all s.t. do
7: for all s.t. do
8: for and do
9: add to
10: add to
11: end for
12: end for
13: for each hypothesis do
14: compute HM decoding features for
15: add to
16: end for
17: for each hypothesis do
18: compute the n-gram and length posterior
features for based on
19: update current HM decoding score of
20: end for
21: end for
22: end for
23: return with the maximum model score
</listItem>
<bodyText confidence="0.993540625">
In BTG-HMD, in order to derive translations for
a source span , we compose hypotheses of any
two smaller spans and using two BTG
rules in line 9 and 10, denotes the
operations that firstly combine and using one
BTG rule and secondly compute HM decoding
features for the newly generated hypothesis . We
compute HM decoding features for hypotheses
contained in all existing component search spaces
as well, and add them to .
From line 17 to 20, we update current HM decod-
ing scores for all hypotheses in using the
n-gram and length posterior features computed
based on . When the whole source sentence
is fully covered, we return the hypothesis with the
maximum model score as the final best translation.
</bodyText>
<subsectionHeader confidence="0.611362">
2.4.2 SCFG-based HM Decoding
</subsectionHeader>
<bodyText confidence="0.999262375">
The second algorithm, SCFG-HMD, is presented
in Algorithm 2. An additional rule set , which is
provided by the HM decoder, is used to compose
hypotheses. It includes hierarchical rules extracted
using Chiang (2005)’s method and glue rules. Two
reordering rule penalty features, and
, are used to adjust the preferences of
using hierarchical rules and glue rules.
</bodyText>
<construct confidence="0.444665">
Algorithm 2: SCFG-based HM Decoding
</construct>
<listItem confidence="0.921174454545454">
1: for each component model do
2: output the search space for the input
3: end for
4: for to do
5: for all s.t. do
7: for each rule that matches do
8: for and do
9: add to
10: end for
11: end for
12: for each hypothesis do
13: compute HM decoding features for
14: add to
15: end for
16: for each hypothesis do
17: compute the n-gram and length posterior
features for based on
18: update current HM decoding score of
19: end for
20: end for
21: end for
22: return with the maximum model score
</listItem>
<bodyText confidence="0.96439125">
Compared to BTG-HMD, the key differences in
SCFG-HMD are located from line 7 to 11, where
the translation for a given span is generated by
replacing the non-terminals in a hierarchical rule
with their corresponding target translations,
is the source span that is covered by the th non-
terminal of , is the search space for
predicted by the HM decoder.
</bodyText>
<page confidence="0.990572">
1262
</page>
<sectionHeader confidence="0.992569" genericHeader="method">
3 Comparisons to Related Techniques
</sectionHeader>
<subsectionHeader confidence="0.925849">
3.1 Model Combination and Mixture Model
based MBR Decoding
</subsectionHeader>
<bodyText confidence="0.9999842">
Model combination (DeNero et al., 2010) is an
approach that selects translations from a conjoint
search space using information from multiple SMT
component models; Duan et al. (2010) presents a
similar method, which utilizes a mixture model to
combine distributions of hypotheses from different
systems for Bayes-risk computation, and selects
final translations from the combined search spaces
using MBR decoding. Both of these two methods
share a common limitation: they only re-rank the
combined search space, without the capability to
generate new translations. In contrast, by reusing
hypotheses generated by all component systems in
HM decoding, translations beyond any existing
search space can be generated.
</bodyText>
<subsectionHeader confidence="0.999558">
3.2 Co-Decoding and Joint Decoding
</subsectionHeader>
<bodyText confidence="0.9999914">
Li et al. (2009a) proposes collaborative decoding,
an approach that combines translation systems by
re-ranking partial and full translations iteratively
using n-gram features from the predictions of other
member systems. However, in co-decoding, all
member systems must work in a synchronous way,
and hypotheses between different systems cannot
be shared during decoding procedure; Liu et al.
(2009) proposes joint-decoding, in which multiple
SMT models are combined in either translation or
derivation levels. However, their method relies on
the correspondence between nodes in hypergraph
outputs of different models. HM decoding, on the
other hand, can use hypotheses from component
search spaces directly without any restriction.
</bodyText>
<subsectionHeader confidence="0.998989">
3.3 Hybrid Decoding
</subsectionHeader>
<bodyText confidence="0.999897214285714">
Hybrid decoding (Cui et al., 2010) resembles our
approach in the motivation. This method uses the
system combination technique in decoding directly
to combine partial hypotheses from different SMT
models. However, confusion network construction
brings high computational complexity. What’s
more, partial hypotheses generated by confusion
network decoding cannot be assigned exact feature
values for future use in higher level decoding, and
they only use feature values of 1-best hypothesis
as an approximation. HM decoding, on the other
hand, leverages a set of enriched features, which
are computable for all the hypotheses generated by
either component systems or the HM decoder.
</bodyText>
<sectionHeader confidence="0.999853" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998916">
4.1 Data and Metric
</subsectionHeader>
<bodyText confidence="0.999761909090909">
Experiments are conducted on the NIST Chinese-
to-English MT tasks. The NIST 2004 (MT04) data
set is used as the development set, and evaluation
results are reported on the NIST 2005 (MT05), the
newswire portions of the NIST 2006 (MT06) and
2008 (MT08) data sets. All bilingual corpora
available for the NIST 2008 constrained data track
of Chinese-to-English MT task are used as training
data, which contain 5.1M sentence pairs, 128M
Chinese words and 147M English words after pre-
processing. Word alignments are performed using
GIZA++ with the intersect-diag-grow refinement.
The English side of bilingual corpus plus Xinhua
portion of the LDC English Gigaword Version 3.0
are used to train a 5-gram language model.
Translation performance is measured in terms of
case-insensitive BLEU scores (Papineni et al.,
2002), which compute the brevity penalty using
the shortest reference translation for each segment.
Statistical significance is computed using the boot-
strap re-sampling approach proposed by Koehn
(2004). Table 1 gives some data statistics.
</bodyText>
<tableCaption confidence="0.996108">
Table 1: Statistics on dev and test data sets
</tableCaption>
<subsectionHeader confidence="0.996013">
4.2 Component Systems
</subsectionHeader>
<bodyText confidence="0.99678975">
For convenience of comparing HM decoding with
several related decoding techniques, we include
two state-of-the-art SMT systems as component
systems only:
</bodyText>
<listItem confidence="0.98139475">
• PB. A phrase-based system (Xiong et al.,
2006) with one lexicalized reordering model
based on the maximum entropy principle.
• DHPB. A string-to-dependency tree-based
system (Shen et al., 2008), which translates
source strings to target dependency trees. A
target dependency language model is used as
an additional feature.
</listItem>
<figure confidence="0.7179332">
Data Set #Sentence #Word
MT04(dev) 1,788 48,215
MT05 1,082 29,263
MT06 616 17,316
MT08 691 17,424
</figure>
<page confidence="0.911655">
1263
</page>
<bodyText confidence="0.99970175">
Phrasal rules are extracted on all bilingual data,
hierarchical rules used in DHPB and reordering
rules used in SCFG-HMD are extracted from a
selected data set3. Reordering model used in PB is
trained on the same selected data set as well. A
trigram dependency language model used in
DHPB is trained with the outputs from Berkeley
parser on all language model training data.
</bodyText>
<subsectionHeader confidence="0.998915">
4.3 Contrastive Techniques
</subsectionHeader>
<bodyText confidence="0.9996985">
We compare HM decoding with three multiple-
system based decoding techniques:
</bodyText>
<listItem confidence="0.953519058823529">
• Word-Level System Combination (SC). We
re-implement an IHMM alignment based sys-
tem combination method proposed by Li et al.
(2009b). The setting of the N-best candidates
used is the same as the original paper.
• Co-decoding (CD). We re-implement it based
on Li et al. (2009a), with the only difference
that only two models are included in our re-
implementation, instead of three in theirs. For
each test set, co-decoding outputs three results,
two for two member systems, and one for the
further system combination.
• Model Combination (MC). Different from co-
decoding, MC produces single one output for
each input sentence. We re-implement this
method based on DeNero et al. (2010) with
two component models included.
</listItem>
<subsectionHeader confidence="0.987345">
4.4 Comparison to Component Systems
</subsectionHeader>
<bodyText confidence="0.9999586875">
We compared HM decoding with two component
SMT systems first (in Table 2). 30 features are
used to annotate each hypothesis in HM decoding,
including: 8 n-gram posterior features computed
from PB/DHPB forests for 1 &lt; n &lt; 4; 8 stemmed
n-gram posterior features computed from stemmed
PB/DHPB forests for 1 &lt; n &lt; 4; 4 n-gram post-
erior features and 1 length posterior feature com-
puted from the mixture search space of HM de-
coder for 1 &lt; n &lt; 4; 1 LM feature; 1 word count
feature; 1 dictionary-based feature; 2 grammar-
specified rule penalty features for either BTG-
HMD or SCFG-HMD; 4 count features for newly
generated n-grams in HM decoding for 1 &lt; n &lt; 4.
All n-gram posteriors are computed using the effi-
cient algorithm proposed by Kumar et al. (2009).
</bodyText>
<footnote confidence="0.806850666666667">
3 LDC2003E07, LDC2003E14, LDC2005T06, LDC2005T10,
LDC2005E83, LDC2006E26, LDC2006E34, LDC2006E85
and LDC2006E92
</footnote>
<tableCaption confidence="0.998613">
Table 2: HM decoding vs. single component system
decoding (*: significantly better than each component
system with p &lt; 0.01)
</tableCaption>
<bodyText confidence="0.9998615625">
From table 2 we can see, both BTG-HMD and
SCFG-HMD outperform decoding results of the
best component system (DHPB) with significant
improvements: +1.50, +1.76, and +1.26 BLEU
points on MT05, MT06, and MT08 for BTG-HMD;
+1.43, +1.63 and +1.09 BLEU points on MT05,
MT06, and MT08 for SCFG-HMD. We also notice
that BTG-HMD performs slight better than SCFG-
HMD on test sets. We think the potential reason is
that more reordering rules are used in SCFG-HMD
to handle phrase movements than BTG-HMD do;
however, current HM decoding model lacks the
ability to distinguish the qualities of different rules.
We also investigate on the effects of different
HM-decoding features. For the convenience of
comparison, we divide them into five categories:
</bodyText>
<listItem confidence="0.983631692307692">
• Set-1. 8 n-gram posterior features based on 2
component search spaces plus 3 commonly
used features (1 LM feature, 1 word count
feature and 1 dictionary-based feature).
• Set-2. 8 stemmed n-gram posterior features
based on 2 stemmed component search spaces.
• Set-3. 4 n-gram posterior features and 1
length posterior feature based on the mixture
search space of the HM decoder.
• Set-4. 2 grammar-specified reordering rule
penalty features.
• Set-5. 4 count features for unseen n-grams
generated by HM decoder itself.
</listItem>
<bodyText confidence="0.999363375">
Except for the dictionary-based feature, all the
features contained in Set-1 are used by the latest
multiple-system based consensus decoding tech-
niques (DeNero et al., 2010; Duan et al., 2010).
We use them as the starting point. Each time, we
add one more feature set and describe the changes
of performances by drawing two curves for each
HM decoding algorithm on MT08 in Figure 3.
</bodyText>
<figure confidence="0.86265155">
SCFG-HMD 41.31*
BTG-HMD 41.24*
DHPB 39.90
Model BLEU%
MT04 MT05 MT06 MT08
PB 38.93
41.26* 36.76* 31.69*
41.19* 36.63* 31.52*
38.21 33.59 29.62
39.76 35.00 30.43
1264
31.9
31.7
BTG-HMD
31.3
SCFG-HMD
30.9
30.7
30.5
Set-1 Set-2 Set-3 Set-4 Set-5
</figure>
<figureCaption confidence="0.999263">
Figure 3: Effects of using different sets of HM decoding
features on MT08
</figureCaption>
<bodyText confidence="0.999478384615385">
With Set-1 used only, HM-decoding has already
outperformed the best component system, which
shows the strong contributions of these features as
proved in related work; small gains (+0.2 BLEU
points) are achieved by using 8 stemmed n-gram
posterior features in Set-2, which shows consensus
statistics based on n-grams in their stem forms are
also helpful; n-gram and length posterior features
based on mixture search space bring improvements
as well; reordering rule penalty features and count
features for unseen n-grams boost newly generated
hypotheses specific for HM decoding, and they
contribute to the overall improvements.
</bodyText>
<subsectionHeader confidence="0.999578">
4.5 Comparison to System Combination
</subsectionHeader>
<bodyText confidence="0.999761833333333">
Word-level system combination is state-of-the-art
method to improve translation performance using
outputs generated by multiple SMT systems. In
this paper, we compare our HM decoding with the
combination method proposed by Li et al. (2009b).
Evaluation results are shown in Table 3.
</bodyText>
<tableCaption confidence="0.9877045">
Table 3: HM decoding vs. system combination (+: sig-
nificantly better than SC with p &lt; 0.05)
</tableCaption>
<bodyText confidence="0.999126">
Compared to word-level system combination,
both BTG-HMD and SCFG-HMD can provide
significant improvements. We think the potential
reason for these improvements is that, system
combination can only use a small portion of the
component systems’ search spaces; HM decoding,
on the other hand, can make full use of the entire
translation spaces of all component systems.
</bodyText>
<subsectionHeader confidence="0.997645">
4.6 Comparison to Consensus Decoding
</subsectionHeader>
<bodyText confidence="0.999721363636364">
Consensus decoding is another decoding technique
that motivates our approach. We compare our HM
decoding with two latest multiple-system based
consensus decoding approaches, co-decoding and
model combination. We list the comparison results
in Table 4, in which CD-PB and CD-DHPB denote
the translation results of two member systems in
co-decoding respectively, CD-Comb denotes the
results of further combination using outputs of
CD-PB and CD-DHPB, MC denotes the results of
model combination.
</bodyText>
<table confidence="0.99936775">
Model BLEU%
MT04 MT05 MT06 MT08
CD-PB 40.39 40.34 35.20 30.39
CD-DHPB
CD-Comb
MC
40.81 40.56 35.73 30.87
41.27 41.02 36.37 31.54
41.19 40.96 36.30 31.43
BTG-HMD 41.24 41.26+ 36.76+ 31.69
SCFG-HMD
41.31 41.19 36.63+ 31.52
</table>
<tableCaption confidence="0.852190666666667">
Table 4: HM decoding vs. consensus decoding (+: sig-
nificantly better than the best result of consensus decod-
ing methods with p &lt; 0.05)
</tableCaption>
<bodyText confidence="0.9780975">
Table 4 shows that after an additional system
combination procedure, CD-Comb performs slight
better than MC. Both BTG-HMD and SCFG-
HMD perform consistent better than CD and MC
on all blind test sets, due to its richer generative
capability and usage of larger search spaces.
</bodyText>
<subsectionHeader confidence="0.8570415">
4.7 System Combination over BTG-HMD
and SCFG-HMD Outputs
</subsectionHeader>
<bodyText confidence="0.999942166666667">
As BTG-HMD and SCFG-HMD are based on two
different decoding grammars, we could perform
system combination over the outputs of these two
settings (SCBTG+SCFG) for further improvements as
well, just as Li et al. (2009a) did in co-decoding.
We present evaluation results in Table 5.
</bodyText>
<tableCaption confidence="0.97462525">
Table 5: System combination based on the outputs of
BTG-HMD and SCFG-HMD (+: significantly better
than the best HM decoding algorithm (SCFG-HMD)
with p &lt; 0.05)
</tableCaption>
<figure confidence="0.971478055555556">
SCFG-HMD 41.31+
BTG-HMD 41.24
Model BLEU%
MT04 MT05 MT06 MT08
SC 41.14
41.26+ 36.76+ 31.69+
41.19+ 36.63+ 31.52+
40.70 36.04 31.16
SCFG-HMD 41.31
BTG-HMD 41.24
SCBTG+SCFG 41.74+
Model BLEU%
MT04 MT05 MT06 MT08
41.53+ 37.11+ 32.06+
41.26 36.76 31.69
41.19 36.63 31.52
31.5
31.1
</figure>
<page confidence="0.978097">
1265
</page>
<bodyText confidence="0.999743857142857">
After system combination, translation results are
significantly better than all decoding approaches
investigated in this paper: up to 2.11 BLEU points
over the best component system (DHPB), up to
1.07 BLEU points over system combination, up to
0.74 BLEU points over co-decoding, and up to
0.81 BLEU points over model combination.
</bodyText>
<subsectionHeader confidence="0.999639">
4.8 Evaluation of Oracle Translations
</subsectionHeader>
<bodyText confidence="0.992344875">
In the last part, we evaluate the quality of oracle
translations on the n-best lists generated by HM
decoding and all decoding approaches discussed in
this paper. Oracle performances are obtained using
the metric of sentence-level BLEU score proposed
by Ye et al. (2007), and each decoding approach
outputs its 1000-best hypotheses, which are used
to extract oracle translations.
</bodyText>
<table confidence="0.999614">
Model BLEU%
MT04 MT05 MT06 MT08
PB 49.53 48.36 43.69 39.39
DHPB
50.66 49.59 44.68 40.47
SC 51.77 50.84 46.87 42.11
CD-PB
CD-DHPB
CD-Comb
MC
50.26 50.10 45.65 40.52
51.91 50.61 46.23 41.01
52.10 51.00 46.95 42.20
52.03 51.22 46.60 42.23
BTG-HMD 52.69+ 51.75+ 47.08 42.71+
SCFG-HMD
SCBTG+SCFG
52.94+ 51.40 47.27+ 42.45+
53.58+ 52.03+ 47.90+ 43.07+
</table>
<tableCaption confidence="0.979607333333333">
Table 6: Oracle performances of different methods (+:
significantly better than the best multiple-system based
decoding method (CD-Comb) with p &lt; 0.05)
</tableCaption>
<bodyText confidence="0.9990088">
Results are shown in Table 6: compared to each
single component system, decoding methods based
on multiple SMT systems can provide significant
improvements on oracle translations; word-level
system combination, collaborative decoding and
model combination show similar performances, in
which CD-Comb performs best; BTG-HMD,
SCFG-HMD and SCBTG+SCFG can obtain significant
improvements than all the other approaches, and
SCBTG+SCFG performs best on all evaluation sets.
</bodyText>
<sectionHeader confidence="0.999183" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999686">
In this paper, we have presented the hypothesis
mixture decoding approach to combine multiple
SMT models, in which hypotheses generated by
multiple component systems are used to compose
new translations. HM decoding method integrates
the advantages of both system combination and
consensus decoding techniques into a unified
framework. Experimental results across different
NIST Chinese-to-English MT evaluation data sets
have validated the effectiveness of our approach.
In the future, we will include more SMT models
and explore more features, such as syntax-based
features, helping to improve the performance of
HM decoding. We also plan to investigate more
complicated reordering models in HM decoding.
</bodyText>
<sectionHeader confidence="0.998907" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999858736842106">
David Chiang. 2005. A Hierarchical Phrase-based
Model for Statistical Machine Translation. In Pro-
ceedings of the Association for Computational Lin-
guistics, pages 263-270.
David Chiang. 2010. Learning to Translate with Source
and Target Syntax. In Proceedings of the Association
for Computational Linguistics, pages 1443-1452.
Lei Cui, Dongdong Zhang, Mu Li, Ming Zhou, and
Tiejun Zhao. 2010. Hybrid Decoding: Decoding with
Partial Hypotheses Combination over Multiple SMT
Systems. In Proceedings of the International Confe-
rence on Computational Linguistics, pages 214-222.
John DeNero, David Chiang, and Kevin Knight. 2009.
Fast Consensus Decoding over Translation Forests.
In Proceedings of the Association for Computational
Linguistics, pages 567-575.
John DeNero, Shankar Kumar, Ciprian Chelba and
Franz Och. 2010. Model Combination for Machine
Translation. In Proceedings of the North American
Association for Computational Linguistics, pages
975-983.
Nan Duan, Mu Li, Dongdong Zhang, and Ming Zhou.
2010. Mixture Model-based Minimum Bayes Risk
Decoding using Multiple Machine Translation Sys-
tems. In Proceedings of the International Conference
on Computational Linguistics, pages 313-321.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable Inference and Training of
Context-Rich Syntactic Translation Models. In Pro-
ceedings of the Association for Computational Lin-
guistics, pages 961-968.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-HMM-
based Hypothesis Alignment for Combining Outputs
from Machine Translation Systems. In Proceedings
of the Conference on Empirical Methods on Natural
Language Processing, pages 98-107.
</reference>
<page confidence="0.789926">
1266
</page>
<reference confidence="0.999896681159421">
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
the Conference on Empirical Methods on Natural
Language Processing, pages 388-395.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-Risk Decoding for Statistical Machine Trans-
lation. In Proceedings of the North American Asso-
ciation for Computational Linguistics, pages 169-
176.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient Minimum Error Rate
Training and Minimum Bayes-Risk Decoding for
Translation Hypergraphs and Lattices. In Proceed-
ings of the Association for Computational Linguis-
tics, pages 163-171.
Mu Li, Nan Duan, Dongdong Zhang, Chi-Ho Li, and
Ming Zhou. 2009a. Collaborative Decoding: Partial
Hypothesis Re-Ranking Using Translation Consen-
sus between Decoders. In Proceedings of the Associ-
ation for Computational Linguistics, pages 585-592.
Chi-Ho Li, Xiaodong He, Yupeng Liu, and Ning Xi.
2009b. Incremental HMM Alignment for MT system
Combination. In Proceedings of the Association for
Computational Linguistics, pages 949-957.
Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009.
Joint Decoding with Multiple Translation Models. In
Proceedings of the Association for Computational
Linguistics, pages 576-584.
Franz Och. 2003. Minimum Error Rate Training in Sta-
tistical Machine Translation. In Proceedings of the
Association for Computational Linguistics, pages
160-167.
Franz Och and Hermann Ney. 2004. The Alignment
Template Approach to Statistical Machine Transla-
tion. Computational Linguistics, 30(4): 417-449.
Kishore Papineni, Salim Roukos, Todd Ward, and
Weijing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the Association for Computational Linguistics, pages
311-318.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new String-to-Dependency Machine Translation Al-
gorithm with a Target Dependency Language Model.
In Proceedings of the Association for Computational
Linguistics, pages 577-585.
Antti-Veikko Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved Word-Level System
Combination for Machine Translation. In Proceed-
ings of the Association for Computational Linguistics,
pages 312-319.
Roy Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-Risk
Decoding for Statistical Machine Translation. In
Proceedings of the Conference on Empirical Me-
thods on Natural Language Processing, pages 620-
629.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Corpora.
Computational Linguistics, 23(3): 377-404.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum Entropy based Phrase Reordering Model for
Statistical Machine Translation. In Proceedings of
the Association for Computational Linguistics, pages
521-528.
Yang Ye, Ming Zhou, and Chin-Yew Lin. 2007. Sen-
tence Level Machine Translation Evaluation as a
Ranking Problem: one step aside from BLEU. In
Proceedings of the Second Workshop on Statistical
Machine Translation, pages 240-247.
</reference>
<page confidence="0.992511">
1267
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.948717">
<title confidence="0.999576">Hypothesis Mixture Decoding for Statistical Machine Translation</title>
<author confidence="0.998417">Mu Li Duan</author>
<author confidence="0.998417">Zhou</author>
<affiliation confidence="0.9998245">School of Computer Science and Technology Natural Language Computing Group Tianjin University Microsoft Research Asia</affiliation>
<address confidence="0.965441">Tianjin, China Beijing, China</address>
<email confidence="0.998942">v-naduan@microsoft.com{muli,mingzhou}@microsoft.com</email>
<abstract confidence="0.999327666666667">paper presents mixture decoding (HM decoding), a new decoding scheme that performs translation reconstruction using hypotheses generated by multiple translation systems. HM decoding involves two decoding stages: first, each component system decodes independently, with the explored search space kept for use in the next step; second, a new search space is constructed by composing existing hypotheses produced by all component systems using a set of rules provided by the HM decoder itself, and a new set of model independent features are used to seek the final best translation from this new search space. Few assumptions are made by our approach about the underlying component systems, enabling us to leverage SMT models based on arbitrary paradigms. We compare our approach with several related techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A Hierarchical Phrase-based Model for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="1399" citStr="Chiang, 2005" startWordPosition="194" endWordPosition="195">y the HM decoder itself, and a new set of model independent features are used to seek the final best translation from this new search space. Few assumptions are made by our approach about the underlying component systems, enabling us to leverage SMT models based on arbitrary paradigms. We compare our approach with several related techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple s</context>
<context position="14474" citStr="Chiang (2005)" startWordPosition="2316" endWordPosition="2317">oder itself, 6,.(R) equals to 1 when r is provided by R, and 0 otherwise. 6) hNe (e, f ): the feature that counts how many n-grams in e are newly generated by the HM decoder, which cannot be found in all existing component search spaces: 6u, (UIV n=1 .?Cn (f )) equals to 1 when w does not exist in U .?Cn(f)Nn=1, and 0 otherwise. The MERT algorithm (Och, 2003) is used to tune weights of HM decoding features. 2.4 Decoding Algorithms Two CKY-style algorithms for HM decoding are presented in this subsection. The first one is based on BTG (Wu, 1997), and the second one is based on SCFG, similar to Chiang (2005). 1261 2.4.1 BTG-based HM Decoding The first algorithm, BTG-HMD, is presented in Algorithm 1, where hypotheses of two consecutive source spans are composed using two BTG rules: • Straight rule . It combines translations of two consecutive blocks into a single larger block in a straight order. • Inverted rule . It combines translations of two consecutive blocks into a single larger block in an inverted order. These two rules are used bottom-up until the whole source sentence is fully covered. We use two reordering rule penalty features, and , to penalize the uses of these two rules. Algorithm 1</context>
<context position="16533" citStr="Chiang (2005)" startWordPosition="2683" endWordPosition="2684">r hypotheses contained in all existing component search spaces as well, and add them to . From line 17 to 20, we update current HM decoding scores for all hypotheses in using the n-gram and length posterior features computed based on . When the whole source sentence is fully covered, we return the hypothesis with the maximum model score as the final best translation. 2.4.2 SCFG-based HM Decoding The second algorithm, SCFG-HMD, is presented in Algorithm 2. An additional rule set , which is provided by the HM decoder, is used to compose hypotheses. It includes hierarchical rules extracted using Chiang (2005)’s method and glue rules. Two reordering rule penalty features, and , are used to adjust the preferences of using hierarchical rules and glue rules. Algorithm 2: SCFG-based HM Decoding 1: for each component model do 2: output the search space for the input 3: end for 4: for to do 5: for all s.t. do 7: for each rule that matches do 8: for and do 9: add to 10: end for 11: end for 12: for each hypothesis do 13: compute HM decoding features for 14: add to 15: end for 16: for each hypothesis do 17: compute the n-gram and length posterior features for based on 18: update current HM decoding score of</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A Hierarchical Phrase-based Model for Statistical Machine Translation. In Proceedings of the Association for Computational Linguistics, pages 263-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Learning to Translate with Source and Target Syntax.</title>
<date>2010</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>1443--1452</pages>
<contexts>
<context position="1453" citStr="Chiang 2010" startWordPosition="204" endWordPosition="205">ndent features are used to seek the final best translation from this new search space. Few assumptions are made by our approach about the underlying component systems, enabling us to leverage SMT models based on arbitrary paradigms. We compare our approach with several related techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, </context>
</contexts>
<marker>Chiang, 2010</marker>
<rawString>David Chiang. 2010. Learning to Translate with Source and Target Syntax. In Proceedings of the Association for Computational Linguistics, pages 1443-1452.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Cui</author>
<author>Dongdong Zhang</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Tiejun Zhao</author>
</authors>
<title>Hybrid Decoding: Decoding with Partial Hypotheses Combination over Multiple SMT Systems.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics,</booktitle>
<pages>214--222</pages>
<contexts>
<context position="19198" citStr="Cui et al., 2010" startWordPosition="3111" endWordPosition="3114">ures from the predictions of other member systems. However, in co-decoding, all member systems must work in a synchronous way, and hypotheses between different systems cannot be shared during decoding procedure; Liu et al. (2009) proposes joint-decoding, in which multiple SMT models are combined in either translation or derivation levels. However, their method relies on the correspondence between nodes in hypergraph outputs of different models. HM decoding, on the other hand, can use hypotheses from component search spaces directly without any restriction. 3.3 Hybrid Decoding Hybrid decoding (Cui et al., 2010) resembles our approach in the motivation. This method uses the system combination technique in decoding directly to combine partial hypotheses from different SMT models. However, confusion network construction brings high computational complexity. What’s more, partial hypotheses generated by confusion network decoding cannot be assigned exact feature values for future use in higher level decoding, and they only use feature values of 1-best hypothesis as an approximation. HM decoding, on the other hand, leverages a set of enriched features, which are computable for all the hypotheses generated</context>
</contexts>
<marker>Cui, Zhang, Li, Zhou, Zhao, 2010</marker>
<rawString>Lei Cui, Dongdong Zhang, Mu Li, Ming Zhou, and Tiejun Zhao. 2010. Hybrid Decoding: Decoding with Partial Hypotheses Combination over Multiple SMT Systems. In Proceedings of the International Conference on Computational Linguistics, pages 214-222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>David Chiang</author>
<author>Kevin Knight</author>
</authors>
<title>Fast Consensus Decoding over Translation Forests.</title>
<date>2009</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>567--575</pages>
<contexts>
<context position="2100" citStr="DeNero et al., 2009" startWordPosition="306" endWordPosition="309">entrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hypotheses generated by a single model are highly correlated, improvements obtained are usually small; recently, dedicated efforts have been made to extend it from single system to multiple systems (Li et al., 2009a; DeNero et al., 2010; Duan et al., 2010). Such methods select translations by optimizing consensus models over the combined hypotheses using all component systems’ posterior distributions. Although these two types of approaches have shown consi</context>
</contexts>
<marker>DeNero, Chiang, Knight, 2009</marker>
<rawString>John DeNero, David Chiang, and Kevin Knight. 2009. Fast Consensus Decoding over Translation Forests. In Proceedings of the Association for Computational Linguistics, pages 567-575.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Shankar Kumar</author>
<author>Ciprian Chelba</author>
<author>Franz Och</author>
</authors>
<title>Model Combination for Machine Translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the North American Association for Computational Linguistics,</booktitle>
<pages>975--983</pages>
<contexts>
<context position="2476" citStr="DeNero et al., 2010" startWordPosition="364" endWordPosition="367">utputs the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hypotheses generated by a single model are highly correlated, improvements obtained are usually small; recently, dedicated efforts have been made to extend it from single system to multiple systems (Li et al., 2009a; DeNero et al., 2010; Duan et al., 2010). Such methods select translations by optimizing consensus models over the combined hypotheses using all component systems’ posterior distributions. Although these two types of approaches have shown consistent improvements over the standard Maximum a Posteriori (MAP) decoding scheme, most of them are implemented as post-processing procedures over translations generated by MAP decoders. In this sense, the work of Li et al. (2009a) is different in that both partial and full hypotheses are re-ranked during the decoding phase directly using consensus between translations from d</context>
<context position="13095" citStr="DeNero et al., 2010" startWordPosition="2075" endWordPosition="2078"> HM decoder: 5) hRule (e, f) and hciue (e, f) : reordering features that penalize the uses of hierarchical and glue rules during the derivation of e in HM decoding. These two features are specific to SCFG-based HM decoding (Section 2.4.2): Note here that features in h.?C(f) (e, f) and hl (e, f) will be computed when the computations of all the remainder features in two categories have already finished for each e in .?C (f), and they will be used to update current HM decoding model scores. Consensus features based on component search spaces have already shown effectiveness (Kumar et al., 2009; DeNero et al., 2010; Duan et al., 2010). We leverage consensus features based on the mixture search space newly generated in HM decoding as well. The length posterior feature (Zen and Ney, 2006) is used to adjust the preference of HM decoder for longer or shorter translations, and the stemmed n-gram posterior features are used to provide more discriminative power for HM decoding and to decrease the effects of morphological changes in words for more accurate computation of consensus statistics. The second feature category contains a set of general features. Although there are more features that can be incorporate</context>
<context position="17700" citStr="DeNero et al., 2010" startWordPosition="2894" endWordPosition="2897"> for based on 18: update current HM decoding score of 19: end for 20: end for 21: end for 22: return with the maximum model score Compared to BTG-HMD, the key differences in SCFG-HMD are located from line 7 to 11, where the translation for a given span is generated by replacing the non-terminals in a hierarchical rule with their corresponding target translations, is the source span that is covered by the th nonterminal of , is the search space for predicted by the HM decoder. 1262 3 Comparisons to Related Techniques 3.1 Model Combination and Mixture Model based MBR Decoding Model combination (DeNero et al., 2010) is an approach that selects translations from a conjoint search space using information from multiple SMT component models; Duan et al. (2010) presents a similar method, which utilizes a mixture model to combine distributions of hypotheses from different systems for Bayes-risk computation, and selects final translations from the combined search spaces using MBR decoding. Both of these two methods share a common limitation: they only re-rank the combined search space, without the capability to generate new translations. In contrast, by reusing hypotheses generated by all component systems in H</context>
<context position="22740" citStr="DeNero et al. (2010)" startWordPosition="3661" endWordPosition="3664">alignment based system combination method proposed by Li et al. (2009b). The setting of the N-best candidates used is the same as the original paper. • Co-decoding (CD). We re-implement it based on Li et al. (2009a), with the only difference that only two models are included in our reimplementation, instead of three in theirs. For each test set, co-decoding outputs three results, two for two member systems, and one for the further system combination. • Model Combination (MC). Different from codecoding, MC produces single one output for each input sentence. We re-implement this method based on DeNero et al. (2010) with two component models included. 4.4 Comparison to Component Systems We compared HM decoding with two component SMT systems first (in Table 2). 30 features are used to annotate each hypothesis in HM decoding, including: 8 n-gram posterior features computed from PB/DHPB forests for 1 &lt; n &lt; 4; 8 stemmed n-gram posterior features computed from stemmed PB/DHPB forests for 1 &lt; n &lt; 4; 4 n-gram posterior features and 1 length posterior feature computed from the mixture search space of HM decoder for 1 &lt; n &lt; 4; 1 LM feature; 1 word count feature; 1 dictionary-based feature; 2 grammarspecified rule</context>
<context position="25231" citStr="DeNero et al., 2010" startWordPosition="4067" endWordPosition="4070">us 3 commonly used features (1 LM feature, 1 word count feature and 1 dictionary-based feature). • Set-2. 8 stemmed n-gram posterior features based on 2 stemmed component search spaces. • Set-3. 4 n-gram posterior features and 1 length posterior feature based on the mixture search space of the HM decoder. • Set-4. 2 grammar-specified reordering rule penalty features. • Set-5. 4 count features for unseen n-grams generated by HM decoder itself. Except for the dictionary-based feature, all the features contained in Set-1 are used by the latest multiple-system based consensus decoding techniques (DeNero et al., 2010; Duan et al., 2010). We use them as the starting point. Each time, we add one more feature set and describe the changes of performances by drawing two curves for each HM decoding algorithm on MT08 in Figure 3. SCFG-HMD 41.31* BTG-HMD 41.24* DHPB 39.90 Model BLEU% MT04 MT05 MT06 MT08 PB 38.93 41.26* 36.76* 31.69* 41.19* 36.63* 31.52* 38.21 33.59 29.62 39.76 35.00 30.43 1264 31.9 31.7 BTG-HMD 31.3 SCFG-HMD 30.9 30.7 30.5 Set-1 Set-2 Set-3 Set-4 Set-5 Figure 3: Effects of using different sets of HM decoding features on MT08 With Set-1 used only, HM-decoding has already outperformed the best comp</context>
</contexts>
<marker>DeNero, Kumar, Chelba, Och, 2010</marker>
<rawString>John DeNero, Shankar Kumar, Ciprian Chelba and Franz Och. 2010. Model Combination for Machine Translation. In Proceedings of the North American Association for Computational Linguistics, pages 975-983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nan Duan</author>
<author>Mu Li</author>
<author>Dongdong Zhang</author>
<author>Ming Zhou</author>
</authors>
<title>Mixture Model-based Minimum Bayes Risk Decoding using Multiple Machine Translation Systems.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics,</booktitle>
<pages>313--321</pages>
<contexts>
<context position="2496" citStr="Duan et al., 2010" startWordPosition="368" endWordPosition="371">oring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hypotheses generated by a single model are highly correlated, improvements obtained are usually small; recently, dedicated efforts have been made to extend it from single system to multiple systems (Li et al., 2009a; DeNero et al., 2010; Duan et al., 2010). Such methods select translations by optimizing consensus models over the combined hypotheses using all component systems’ posterior distributions. Although these two types of approaches have shown consistent improvements over the standard Maximum a Posteriori (MAP) decoding scheme, most of them are implemented as post-processing procedures over translations generated by MAP decoders. In this sense, the work of Li et al. (2009a) is different in that both partial and full hypotheses are re-ranked during the decoding phase directly using consensus between translations from different SMT systems</context>
<context position="13115" citStr="Duan et al., 2010" startWordPosition="2079" endWordPosition="2082"> (e, f) and hciue (e, f) : reordering features that penalize the uses of hierarchical and glue rules during the derivation of e in HM decoding. These two features are specific to SCFG-based HM decoding (Section 2.4.2): Note here that features in h.?C(f) (e, f) and hl (e, f) will be computed when the computations of all the remainder features in two categories have already finished for each e in .?C (f), and they will be used to update current HM decoding model scores. Consensus features based on component search spaces have already shown effectiveness (Kumar et al., 2009; DeNero et al., 2010; Duan et al., 2010). We leverage consensus features based on the mixture search space newly generated in HM decoding as well. The length posterior feature (Zen and Ney, 2006) is used to adjust the preference of HM decoder for longer or shorter translations, and the stemmed n-gram posterior features are used to provide more discriminative power for HM decoding and to decrease the effects of morphological changes in words for more accurate computation of consensus statistics. The second feature category contains a set of general features. Although there are more features that can be incorporated into HM decoding b</context>
<context position="17843" citStr="Duan et al. (2010)" startWordPosition="2916" endWordPosition="2919">-HMD, the key differences in SCFG-HMD are located from line 7 to 11, where the translation for a given span is generated by replacing the non-terminals in a hierarchical rule with their corresponding target translations, is the source span that is covered by the th nonterminal of , is the search space for predicted by the HM decoder. 1262 3 Comparisons to Related Techniques 3.1 Model Combination and Mixture Model based MBR Decoding Model combination (DeNero et al., 2010) is an approach that selects translations from a conjoint search space using information from multiple SMT component models; Duan et al. (2010) presents a similar method, which utilizes a mixture model to combine distributions of hypotheses from different systems for Bayes-risk computation, and selects final translations from the combined search spaces using MBR decoding. Both of these two methods share a common limitation: they only re-rank the combined search space, without the capability to generate new translations. In contrast, by reusing hypotheses generated by all component systems in HM decoding, translations beyond any existing search space can be generated. 3.2 Co-Decoding and Joint Decoding Li et al. (2009a) proposes colla</context>
<context position="25251" citStr="Duan et al., 2010" startWordPosition="4071" endWordPosition="4074">atures (1 LM feature, 1 word count feature and 1 dictionary-based feature). • Set-2. 8 stemmed n-gram posterior features based on 2 stemmed component search spaces. • Set-3. 4 n-gram posterior features and 1 length posterior feature based on the mixture search space of the HM decoder. • Set-4. 2 grammar-specified reordering rule penalty features. • Set-5. 4 count features for unseen n-grams generated by HM decoder itself. Except for the dictionary-based feature, all the features contained in Set-1 are used by the latest multiple-system based consensus decoding techniques (DeNero et al., 2010; Duan et al., 2010). We use them as the starting point. Each time, we add one more feature set and describe the changes of performances by drawing two curves for each HM decoding algorithm on MT08 in Figure 3. SCFG-HMD 41.31* BTG-HMD 41.24* DHPB 39.90 Model BLEU% MT04 MT05 MT06 MT08 PB 38.93 41.26* 36.76* 31.69* 41.19* 36.63* 31.52* 38.21 33.59 29.62 39.76 35.00 30.43 1264 31.9 31.7 BTG-HMD 31.3 SCFG-HMD 30.9 30.7 30.5 Set-1 Set-2 Set-3 Set-4 Set-5 Figure 3: Effects of using different sets of HM decoding features on MT08 With Set-1 used only, HM-decoding has already outperformed the best component system, which </context>
</contexts>
<marker>Duan, Li, Zhang, Zhou, 2010</marker>
<rawString>Nan Duan, Mu Li, Dongdong Zhang, and Ming Zhou. 2010. Mixture Model-based Minimum Bayes Risk Decoding using Multiple Machine Translation Systems. In Proceedings of the International Conference on Computational Linguistics, pages 313-321.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable Inference and Training of Context-Rich Syntactic Translation Models.</title>
<date>2006</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>961--968</pages>
<contexts>
<context position="1420" citStr="Galley et al., 2006" startWordPosition="196" endWordPosition="199">er itself, and a new set of model independent features are used to seek the final best translation from this new search space. Few assumptions are made by our approach about the underlying component systems, enabling us to leverage SMT models based on arbitrary paradigms. We compare our approach with several related techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable Inference and Training of Context-Rich Syntactic Translation Models. In Proceedings of the Association for Computational Linguistics, pages 961-968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
<author>Mei Yang</author>
<author>Jianfeng Gao</author>
<author>Patrick Nguyen</author>
<author>Robert Moore</author>
</authors>
<title>Indirect-HMMbased Hypothesis Alignment for Combining Outputs from Machine Translation Systems.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods on Natural Language Processing,</booktitle>
<pages>98--107</pages>
<contexts>
<context position="1749" citStr="He et al., 2008" startWordPosition="248" endWordPosition="251"> demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hypotheses generated by a single model are highly correlated, improvements obtained are usually small; recent</context>
</contexts>
<marker>He, Yang, Gao, Nguyen, Moore, 2008</marker>
<rawString>Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen, and Robert Moore. 2008. Indirect-HMMbased Hypothesis Alignment for Combining Outputs from Machine Translation Systems. In Proceedings of the Conference on Empirical Methods on Natural Language Processing, pages 98-107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Significance Tests for Machine Translation Evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods on Natural Language Processing,</booktitle>
<pages>388--395</pages>
<contexts>
<context position="20891" citStr="Koehn (2004)" startWordPosition="3368" endWordPosition="3369">ontain 5.1M sentence pairs, 128M Chinese words and 147M English words after preprocessing. Word alignments are performed using GIZA++ with the intersect-diag-grow refinement. The English side of bilingual corpus plus Xinhua portion of the LDC English Gigaword Version 3.0 are used to train a 5-gram language model. Translation performance is measured in terms of case-insensitive BLEU scores (Papineni et al., 2002), which compute the brevity penalty using the shortest reference translation for each segment. Statistical significance is computed using the bootstrap re-sampling approach proposed by Koehn (2004). Table 1 gives some data statistics. Table 1: Statistics on dev and test data sets 4.2 Component Systems For convenience of comparing HM decoding with several related decoding techniques, we include two state-of-the-art SMT systems as component systems only: • PB. A phrase-based system (Xiong et al., 2006) with one lexicalized reordering model based on the maximum entropy principle. • DHPB. A string-to-dependency tree-based system (Shen et al., 2008), which translates source strings to target dependency trees. A target dependency language model is used as an additional feature. Data Set #Sent</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical Significance Tests for Machine Translation Evaluation. In Proceedings of the Conference on Empirical Methods on Natural Language Processing, pages 388-395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Minimum Bayes-Risk Decoding for Statistical Machine Translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the North American Association for Computational Linguistics,</booktitle>
<pages>169--176</pages>
<contexts>
<context position="2057" citStr="Kumar and Byrne, 2004" startWordPosition="298" endWordPosition="301">008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hypotheses generated by a single model are highly correlated, improvements obtained are usually small; recently, dedicated efforts have been made to extend it from single system to multiple systems (Li et al., 2009a; DeNero et al., 2010; Duan et al., 2010). Such methods select translations by optimizing consensus models over the combined hypotheses using all component systems’ posterior distributions. Although the</context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>Shankar Kumar and William Byrne. 2004. Minimum Bayes-Risk Decoding for Statistical Machine Translation. In Proceedings of the North American Association for Computational Linguistics, pages 169-176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>Wolfgang Macherey</author>
<author>Chris Dyer</author>
<author>Franz Och</author>
</authors>
<title>Efficient Minimum Error Rate Training and Minimum Bayes-Risk Decoding for Translation Hypergraphs and Lattices.</title>
<date>2009</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>163--171</pages>
<contexts>
<context position="2121" citStr="Kumar et al., 2009" startWordPosition="310" endWordPosition="313">aches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hypotheses generated by a single model are highly correlated, improvements obtained are usually small; recently, dedicated efforts have been made to extend it from single system to multiple systems (Li et al., 2009a; DeNero et al., 2010; Duan et al., 2010). Such methods select translations by optimizing consensus models over the combined hypotheses using all component systems’ posterior distributions. Although these two types of approaches have shown consistent improvements ov</context>
<context position="13074" citStr="Kumar et al., 2009" startWordPosition="2071" endWordPosition="2074">(f) generated by the HM decoder: 5) hRule (e, f) and hciue (e, f) : reordering features that penalize the uses of hierarchical and glue rules during the derivation of e in HM decoding. These two features are specific to SCFG-based HM decoding (Section 2.4.2): Note here that features in h.?C(f) (e, f) and hl (e, f) will be computed when the computations of all the remainder features in two categories have already finished for each e in .?C (f), and they will be used to update current HM decoding model scores. Consensus features based on component search spaces have already shown effectiveness (Kumar et al., 2009; DeNero et al., 2010; Duan et al., 2010). We leverage consensus features based on the mixture search space newly generated in HM decoding as well. The length posterior feature (Zen and Ney, 2006) is used to adjust the preference of HM decoder for longer or shorter translations, and the stemmed n-gram posterior features are used to provide more discriminative power for HM decoding and to decrease the effects of morphological changes in words for more accurate computation of consensus statistics. The second feature category contains a set of general features. Although there are more features th</context>
<context position="23560" citStr="Kumar et al. (2009)" startWordPosition="3807" endWordPosition="3810">M decoding, including: 8 n-gram posterior features computed from PB/DHPB forests for 1 &lt; n &lt; 4; 8 stemmed n-gram posterior features computed from stemmed PB/DHPB forests for 1 &lt; n &lt; 4; 4 n-gram posterior features and 1 length posterior feature computed from the mixture search space of HM decoder for 1 &lt; n &lt; 4; 1 LM feature; 1 word count feature; 1 dictionary-based feature; 2 grammarspecified rule penalty features for either BTGHMD or SCFG-HMD; 4 count features for newly generated n-grams in HM decoding for 1 &lt; n &lt; 4. All n-gram posteriors are computed using the efficient algorithm proposed by Kumar et al. (2009). 3 LDC2003E07, LDC2003E14, LDC2005T06, LDC2005T10, LDC2005E83, LDC2006E26, LDC2006E34, LDC2006E85 and LDC2006E92 Table 2: HM decoding vs. single component system decoding (*: significantly better than each component system with p &lt; 0.01) From table 2 we can see, both BTG-HMD and SCFG-HMD outperform decoding results of the best component system (DHPB) with significant improvements: +1.50, +1.76, and +1.26 BLEU points on MT05, MT06, and MT08 for BTG-HMD; +1.43, +1.63 and +1.09 BLEU points on MT05, MT06, and MT08 for SCFG-HMD. We also notice that BTG-HMD performs slight better than SCFGHMD on te</context>
</contexts>
<marker>Kumar, Macherey, Dyer, Och, 2009</marker>
<rawString>Shankar Kumar, Wolfgang Macherey, Chris Dyer, and Franz Och. 2009. Efficient Minimum Error Rate Training and Minimum Bayes-Risk Decoding for Translation Hypergraphs and Lattices. In Proceedings of the Association for Computational Linguistics, pages 163-171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mu Li</author>
<author>Nan Duan</author>
<author>Dongdong Zhang</author>
<author>Chi-Ho Li</author>
<author>Ming Zhou</author>
</authors>
<title>Collaborative Decoding: Partial Hypothesis Re-Ranking Using Translation Consensus between Decoders.</title>
<date>2009</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>585--592</pages>
<contexts>
<context position="1766" citStr="Li et al., 2009" startWordPosition="252" endWordPosition="255">ificant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hypotheses generated by a single model are highly correlated, improvements obtained are usually small; recently, dedicated eff</context>
<context position="18426" citStr="Li et al. (2009" startWordPosition="3002" endWordPosition="3005">onent models; Duan et al. (2010) presents a similar method, which utilizes a mixture model to combine distributions of hypotheses from different systems for Bayes-risk computation, and selects final translations from the combined search spaces using MBR decoding. Both of these two methods share a common limitation: they only re-rank the combined search space, without the capability to generate new translations. In contrast, by reusing hypotheses generated by all component systems in HM decoding, translations beyond any existing search space can be generated. 3.2 Co-Decoding and Joint Decoding Li et al. (2009a) proposes collaborative decoding, an approach that combines translation systems by re-ranking partial and full translations iteratively using n-gram features from the predictions of other member systems. However, in co-decoding, all member systems must work in a synchronous way, and hypotheses between different systems cannot be shared during decoding procedure; Liu et al. (2009) proposes joint-decoding, in which multiple SMT models are combined in either translation or derivation levels. However, their method relies on the correspondence between nodes in hypergraph outputs of different mode</context>
<context position="22189" citStr="Li et al. (2009" startWordPosition="3569" endWordPosition="3572"> 1263 Phrasal rules are extracted on all bilingual data, hierarchical rules used in DHPB and reordering rules used in SCFG-HMD are extracted from a selected data set3. Reordering model used in PB is trained on the same selected data set as well. A trigram dependency language model used in DHPB is trained with the outputs from Berkeley parser on all language model training data. 4.3 Contrastive Techniques We compare HM decoding with three multiplesystem based decoding techniques: • Word-Level System Combination (SC). We re-implement an IHMM alignment based system combination method proposed by Li et al. (2009b). The setting of the N-best candidates used is the same as the original paper. • Co-decoding (CD). We re-implement it based on Li et al. (2009a), with the only difference that only two models are included in our reimplementation, instead of three in theirs. For each test set, co-decoding outputs three results, two for two member systems, and one for the further system combination. • Model Combination (MC). Different from codecoding, MC produces single one output for each input sentence. We re-implement this method based on DeNero et al. (2010) with two component models included. 4.4 Comparis</context>
<context position="26663" citStr="Li et al. (2009" startWordPosition="4293" endWordPosition="4296">statistics based on n-grams in their stem forms are also helpful; n-gram and length posterior features based on mixture search space bring improvements as well; reordering rule penalty features and count features for unseen n-grams boost newly generated hypotheses specific for HM decoding, and they contribute to the overall improvements. 4.5 Comparison to System Combination Word-level system combination is state-of-the-art method to improve translation performance using outputs generated by multiple SMT systems. In this paper, we compare our HM decoding with the combination method proposed by Li et al. (2009b). Evaluation results are shown in Table 3. Table 3: HM decoding vs. system combination (+: significantly better than SC with p &lt; 0.05) Compared to word-level system combination, both BTG-HMD and SCFG-HMD can provide significant improvements. We think the potential reason for these improvements is that, system combination can only use a small portion of the component systems’ search spaces; HM decoding, on the other hand, can make full use of the entire translation spaces of all component systems. 4.6 Comparison to Consensus Decoding Consensus decoding is another decoding technique that motiv</context>
<context position="28599" citStr="Li et al. (2009" startWordPosition="4597" endWordPosition="4600">er than the best result of consensus decoding methods with p &lt; 0.05) Table 4 shows that after an additional system combination procedure, CD-Comb performs slight better than MC. Both BTG-HMD and SCFGHMD perform consistent better than CD and MC on all blind test sets, due to its richer generative capability and usage of larger search spaces. 4.7 System Combination over BTG-HMD and SCFG-HMD Outputs As BTG-HMD and SCFG-HMD are based on two different decoding grammars, we could perform system combination over the outputs of these two settings (SCBTG+SCFG) for further improvements as well, just as Li et al. (2009a) did in co-decoding. We present evaluation results in Table 5. Table 5: System combination based on the outputs of BTG-HMD and SCFG-HMD (+: significantly better than the best HM decoding algorithm (SCFG-HMD) with p &lt; 0.05) SCFG-HMD 41.31+ BTG-HMD 41.24 Model BLEU% MT04 MT05 MT06 MT08 SC 41.14 41.26+ 36.76+ 31.69+ 41.19+ 36.63+ 31.52+ 40.70 36.04 31.16 SCFG-HMD 41.31 BTG-HMD 41.24 SCBTG+SCFG 41.74+ Model BLEU% MT04 MT05 MT06 MT08 41.53+ 37.11+ 32.06+ 41.26 36.76 31.69 41.19 36.63 31.52 31.5 31.1 1265 After system combination, translation results are significantly better than all decoding appr</context>
</contexts>
<marker>Li, Duan, Zhang, Li, Zhou, 2009</marker>
<rawString>Mu Li, Nan Duan, Dongdong Zhang, Chi-Ho Li, and Ming Zhou. 2009a. Collaborative Decoding: Partial Hypothesis Re-Ranking Using Translation Consensus between Decoders. In Proceedings of the Association for Computational Linguistics, pages 585-592.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-Ho Li</author>
<author>Xiaodong He</author>
<author>Yupeng Liu</author>
<author>Ning Xi</author>
</authors>
<title>Incremental HMM Alignment for MT system Combination.</title>
<date>2009</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>949--957</pages>
<contexts>
<context position="1766" citStr="Li et al., 2009" startWordPosition="252" endWordPosition="255">ificant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hypotheses generated by a single model are highly correlated, improvements obtained are usually small; recently, dedicated eff</context>
<context position="18426" citStr="Li et al. (2009" startWordPosition="3002" endWordPosition="3005">onent models; Duan et al. (2010) presents a similar method, which utilizes a mixture model to combine distributions of hypotheses from different systems for Bayes-risk computation, and selects final translations from the combined search spaces using MBR decoding. Both of these two methods share a common limitation: they only re-rank the combined search space, without the capability to generate new translations. In contrast, by reusing hypotheses generated by all component systems in HM decoding, translations beyond any existing search space can be generated. 3.2 Co-Decoding and Joint Decoding Li et al. (2009a) proposes collaborative decoding, an approach that combines translation systems by re-ranking partial and full translations iteratively using n-gram features from the predictions of other member systems. However, in co-decoding, all member systems must work in a synchronous way, and hypotheses between different systems cannot be shared during decoding procedure; Liu et al. (2009) proposes joint-decoding, in which multiple SMT models are combined in either translation or derivation levels. However, their method relies on the correspondence between nodes in hypergraph outputs of different mode</context>
<context position="22189" citStr="Li et al. (2009" startWordPosition="3569" endWordPosition="3572"> 1263 Phrasal rules are extracted on all bilingual data, hierarchical rules used in DHPB and reordering rules used in SCFG-HMD are extracted from a selected data set3. Reordering model used in PB is trained on the same selected data set as well. A trigram dependency language model used in DHPB is trained with the outputs from Berkeley parser on all language model training data. 4.3 Contrastive Techniques We compare HM decoding with three multiplesystem based decoding techniques: • Word-Level System Combination (SC). We re-implement an IHMM alignment based system combination method proposed by Li et al. (2009b). The setting of the N-best candidates used is the same as the original paper. • Co-decoding (CD). We re-implement it based on Li et al. (2009a), with the only difference that only two models are included in our reimplementation, instead of three in theirs. For each test set, co-decoding outputs three results, two for two member systems, and one for the further system combination. • Model Combination (MC). Different from codecoding, MC produces single one output for each input sentence. We re-implement this method based on DeNero et al. (2010) with two component models included. 4.4 Comparis</context>
<context position="26663" citStr="Li et al. (2009" startWordPosition="4293" endWordPosition="4296">statistics based on n-grams in their stem forms are also helpful; n-gram and length posterior features based on mixture search space bring improvements as well; reordering rule penalty features and count features for unseen n-grams boost newly generated hypotheses specific for HM decoding, and they contribute to the overall improvements. 4.5 Comparison to System Combination Word-level system combination is state-of-the-art method to improve translation performance using outputs generated by multiple SMT systems. In this paper, we compare our HM decoding with the combination method proposed by Li et al. (2009b). Evaluation results are shown in Table 3. Table 3: HM decoding vs. system combination (+: significantly better than SC with p &lt; 0.05) Compared to word-level system combination, both BTG-HMD and SCFG-HMD can provide significant improvements. We think the potential reason for these improvements is that, system combination can only use a small portion of the component systems’ search spaces; HM decoding, on the other hand, can make full use of the entire translation spaces of all component systems. 4.6 Comparison to Consensus Decoding Consensus decoding is another decoding technique that motiv</context>
<context position="28599" citStr="Li et al. (2009" startWordPosition="4597" endWordPosition="4600">er than the best result of consensus decoding methods with p &lt; 0.05) Table 4 shows that after an additional system combination procedure, CD-Comb performs slight better than MC. Both BTG-HMD and SCFGHMD perform consistent better than CD and MC on all blind test sets, due to its richer generative capability and usage of larger search spaces. 4.7 System Combination over BTG-HMD and SCFG-HMD Outputs As BTG-HMD and SCFG-HMD are based on two different decoding grammars, we could perform system combination over the outputs of these two settings (SCBTG+SCFG) for further improvements as well, just as Li et al. (2009a) did in co-decoding. We present evaluation results in Table 5. Table 5: System combination based on the outputs of BTG-HMD and SCFG-HMD (+: significantly better than the best HM decoding algorithm (SCFG-HMD) with p &lt; 0.05) SCFG-HMD 41.31+ BTG-HMD 41.24 Model BLEU% MT04 MT05 MT06 MT08 SC 41.14 41.26+ 36.76+ 31.69+ 41.19+ 36.63+ 31.52+ 40.70 36.04 31.16 SCFG-HMD 41.31 BTG-HMD 41.24 SCBTG+SCFG 41.74+ Model BLEU% MT04 MT05 MT06 MT08 41.53+ 37.11+ 32.06+ 41.26 36.76 31.69 41.19 36.63 31.52 31.5 31.1 1265 After system combination, translation results are significantly better than all decoding appr</context>
</contexts>
<marker>Li, He, Liu, Xi, 2009</marker>
<rawString>Chi-Ho Li, Xiaodong He, Yupeng Liu, and Ning Xi. 2009b. Incremental HMM Alignment for MT system Combination. In Proceedings of the Association for Computational Linguistics, pages 949-957.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Haitao Mi</author>
<author>Yang Feng</author>
<author>Qun Liu</author>
</authors>
<title>Joint Decoding with Multiple Translation Models.</title>
<date>2009</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>576--584</pages>
<contexts>
<context position="18810" citStr="Liu et al. (2009)" startWordPosition="3055" endWordPosition="3058">ty to generate new translations. In contrast, by reusing hypotheses generated by all component systems in HM decoding, translations beyond any existing search space can be generated. 3.2 Co-Decoding and Joint Decoding Li et al. (2009a) proposes collaborative decoding, an approach that combines translation systems by re-ranking partial and full translations iteratively using n-gram features from the predictions of other member systems. However, in co-decoding, all member systems must work in a synchronous way, and hypotheses between different systems cannot be shared during decoding procedure; Liu et al. (2009) proposes joint-decoding, in which multiple SMT models are combined in either translation or derivation levels. However, their method relies on the correspondence between nodes in hypergraph outputs of different models. HM decoding, on the other hand, can use hypotheses from component search spaces directly without any restriction. 3.3 Hybrid Decoding Hybrid decoding (Cui et al., 2010) resembles our approach in the motivation. This method uses the system combination technique in decoding directly to combine partial hypotheses from different SMT models. However, confusion network construction b</context>
</contexts>
<marker>Liu, Mi, Feng, Liu, 2009</marker>
<rawString>Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009. Joint Decoding with Multiple Translation Models. In Proceedings of the Association for Computational Linguistics, pages 576-584.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="14222" citStr="Och, 2003" startWordPosition="2272" endWordPosition="2273">a set of general features. Although there are more features that can be incorporated into HM decoding besides the ones we list below, we only utilize the most representative ones for convenience: R is the hierarchical rule set provided by the HM decoder itself, 6,.(R) equals to 1 when r is provided by R, and 0 otherwise. 6) hNe (e, f ): the feature that counts how many n-grams in e are newly generated by the HM decoder, which cannot be found in all existing component search spaces: 6u, (UIV n=1 .?Cn (f )) equals to 1 when w does not exist in U .?Cn(f)Nn=1, and 0 otherwise. The MERT algorithm (Och, 2003) is used to tune weights of HM decoding features. 2.4 Decoding Algorithms Two CKY-style algorithms for HM decoding are presented in this subsection. The first one is based on BTG (Wu, 1997), and the second one is based on SCFG, similar to Chiang (2005). 1261 2.4.1 BTG-based HM Decoding The first algorithm, BTG-HMD, is presented in Algorithm 1, where hypotheses of two consecutive source spans are composed using two BTG rules: • Straight rule . It combines translations of two consecutive blocks into a single larger block in a straight order. • Inverted rule . It combines translations of two cons</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of the Association for Computational Linguistics, pages 160-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
<author>Hermann Ney</author>
</authors>
<title>The Alignment Template Approach to Statistical Machine Translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<pages>417--449</pages>
<contexts>
<context position="1385" citStr="Och and Ney, 2004" startWordPosition="190" endWordPosition="193">of rules provided by the HM decoder itself, and a new set of model independent features are used to seek the final best translation from this new search space. Few assumptions are made by our approach about the underlying component systems, enabling us to leverage SMT models based on arbitrary paradigms. We compare our approach with several related techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Och and Hermann Ney. 2004. The Alignment Template Approach to Statistical Machine Translation. Computational Linguistics, 30(4): 417-449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Weijing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="20694" citStr="Papineni et al., 2002" startWordPosition="3339" endWordPosition="3342"> newswire portions of the NIST 2006 (MT06) and 2008 (MT08) data sets. All bilingual corpora available for the NIST 2008 constrained data track of Chinese-to-English MT task are used as training data, which contain 5.1M sentence pairs, 128M Chinese words and 147M English words after preprocessing. Word alignments are performed using GIZA++ with the intersect-diag-grow refinement. The English side of bilingual corpus plus Xinhua portion of the LDC English Gigaword Version 3.0 are used to train a 5-gram language model. Translation performance is measured in terms of case-insensitive BLEU scores (Papineni et al., 2002), which compute the brevity penalty using the shortest reference translation for each segment. Statistical significance is computed using the bootstrap re-sampling approach proposed by Koehn (2004). Table 1 gives some data statistics. Table 1: Statistics on dev and test data sets 4.2 Component Systems For convenience of comparing HM decoding with several related decoding techniques, we include two state-of-the-art SMT systems as component systems only: • PB. A phrase-based system (Xiong et al., 2006) with one lexicalized reordering model based on the maximum entropy principle. • DHPB. A string</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the Association for Computational Linguistics, pages 311-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A new String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model.</title>
<date>2008</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>577--585</pages>
<contexts>
<context position="1439" citStr="Shen et al., 2008" startWordPosition="200" endWordPosition="203">set of model independent features are used to seek the final best translation from this new search space. Few assumptions are made by our approach about the underlying component systems, enabling us to leverage SMT models based on arbitrary paradigms. We compare our approach with several related techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kum</context>
<context position="21346" citStr="Shen et al., 2008" startWordPosition="3435" endWordPosition="3438">y using the shortest reference translation for each segment. Statistical significance is computed using the bootstrap re-sampling approach proposed by Koehn (2004). Table 1 gives some data statistics. Table 1: Statistics on dev and test data sets 4.2 Component Systems For convenience of comparing HM decoding with several related decoding techniques, we include two state-of-the-art SMT systems as component systems only: • PB. A phrase-based system (Xiong et al., 2006) with one lexicalized reordering model based on the maximum entropy principle. • DHPB. A string-to-dependency tree-based system (Shen et al., 2008), which translates source strings to target dependency trees. A target dependency language model is used as an additional feature. Data Set #Sentence #Word MT04(dev) 1,788 48,215 MT05 1,082 29,263 MT06 616 17,316 MT08 691 17,424 1263 Phrasal rules are extracted on all bilingual data, hierarchical rules used in DHPB and reordering rules used in SCFG-HMD are extracted from a selected data set3. Reordering model used in PB is trained on the same selected data set as well. A trigram dependency language model used in DHPB is trained with the outputs from Berkeley parser on all language model traini</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model. In Proceedings of the Association for Computational Linguistics, pages 577-585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko Rosti</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
</authors>
<title>Improved Word-Level System Combination for Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>312--319</pages>
<contexts>
<context position="1732" citStr="Rosti et al., 2007" startWordPosition="244" endWordPosition="247">ated techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks. 1 Introduction Besides tremendous efforts on constructing more complicated and accurate models for statistical machine translation (SMT) (Och and Ney, 2004; Chiang, 2005; Galley et al., 2006; Shen et al., 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hypotheses generated by a single model are highly correlated, improvements obtained are usua</context>
</contexts>
<marker>Rosti, Matsoukas, Schwartz, 2007</marker>
<rawString>Antti-Veikko Rosti, Spyros Matsoukas, and Richard Schwartz. 2007. Improved Word-Level System Combination for Machine Translation. In Proceedings of the Association for Computational Linguistics, pages 312-319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Tromble</author>
<author>Shankar Kumar</author>
<author>Franz Och</author>
<author>Wolfgang Macherey</author>
</authors>
<title>Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods on Natural Language Processing,</booktitle>
<pages>620--629</pages>
<contexts>
<context position="2079" citStr="Tromble et al., 2008" startWordPosition="302" endWordPosition="305"> researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al., 2007; He et al., 2008; Li et al., 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs the highest scoring paths as the final translations. Consensus decoding, on the other hand, can be based on either single or multiple systems: single system based methods (Kumar and Byrne, 2004; Tromble et al., 2008; DeNero et al., 2009; Kumar et al., 2009) re-rank translations produced by a single SMT model using either n-gram posteriors or expected n-gram counts. Because hypotheses generated by a single model are highly correlated, improvements obtained are usually small; recently, dedicated efforts have been made to extend it from single system to multiple systems (Li et al., 2009a; DeNero et al., 2010; Duan et al., 2010). Such methods select translations by optimizing consensus models over the combined hypotheses using all component systems’ posterior distributions. Although these two types of approa</context>
</contexts>
<marker>Tromble, Kumar, Och, Macherey, 2008</marker>
<rawString>Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang Macherey. 2008. Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation. In Proceedings of the Conference on Empirical Methods on Natural Language Processing, pages 620-629.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<pages>377--404</pages>
<contexts>
<context position="14411" citStr="Wu, 1997" startWordPosition="2304" endWordPosition="2305">ence: R is the hierarchical rule set provided by the HM decoder itself, 6,.(R) equals to 1 when r is provided by R, and 0 otherwise. 6) hNe (e, f ): the feature that counts how many n-grams in e are newly generated by the HM decoder, which cannot be found in all existing component search spaces: 6u, (UIV n=1 .?Cn (f )) equals to 1 when w does not exist in U .?Cn(f)Nn=1, and 0 otherwise. The MERT algorithm (Och, 2003) is used to tune weights of HM decoding features. 2.4 Decoding Algorithms Two CKY-style algorithms for HM decoding are presented in this subsection. The first one is based on BTG (Wu, 1997), and the second one is based on SCFG, similar to Chiang (2005). 1261 2.4.1 BTG-based HM Decoding The first algorithm, BTG-HMD, is presented in Algorithm 1, where hypotheses of two consecutive source spans are composed using two BTG rules: • Straight rule . It combines translations of two consecutive blocks into a single larger block in a straight order. • Inverted rule . It combines translations of two consecutive blocks into a single larger block in an inverted order. These two rules are used bottom-up until the whole source sentence is fully covered. We use two reordering rule penalty featu</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora. Computational Linguistics, 23(3): 377-404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum Entropy based Phrase Reordering Model for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>521--528</pages>
<contexts>
<context position="21199" citStr="Xiong et al., 2006" startWordPosition="3414" endWordPosition="3417">nguage model. Translation performance is measured in terms of case-insensitive BLEU scores (Papineni et al., 2002), which compute the brevity penalty using the shortest reference translation for each segment. Statistical significance is computed using the bootstrap re-sampling approach proposed by Koehn (2004). Table 1 gives some data statistics. Table 1: Statistics on dev and test data sets 4.2 Component Systems For convenience of comparing HM decoding with several related decoding techniques, we include two state-of-the-art SMT systems as component systems only: • PB. A phrase-based system (Xiong et al., 2006) with one lexicalized reordering model based on the maximum entropy principle. • DHPB. A string-to-dependency tree-based system (Shen et al., 2008), which translates source strings to target dependency trees. A target dependency language model is used as an additional feature. Data Set #Sentence #Word MT04(dev) 1,788 48,215 MT05 1,082 29,263 MT06 616 17,316 MT08 691 17,424 1263 Phrasal rules are extracted on all bilingual data, hierarchical rules used in DHPB and reordering rules used in SCFG-HMD are extracted from a selected data set3. Reordering model used in PB is trained on the same select</context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum Entropy based Phrase Reordering Model for Statistical Machine Translation. In Proceedings of the Association for Computational Linguistics, pages 521-528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Ye</author>
<author>Ming Zhou</author>
<author>Chin-Yew Lin</author>
</authors>
<title>Sentence Level Machine Translation Evaluation as a Ranking Problem: one step aside from BLEU.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>240--247</pages>
<contexts>
<context position="29744" citStr="Ye et al. (2007)" startWordPosition="4778" endWordPosition="4781">on, translation results are significantly better than all decoding approaches investigated in this paper: up to 2.11 BLEU points over the best component system (DHPB), up to 1.07 BLEU points over system combination, up to 0.74 BLEU points over co-decoding, and up to 0.81 BLEU points over model combination. 4.8 Evaluation of Oracle Translations In the last part, we evaluate the quality of oracle translations on the n-best lists generated by HM decoding and all decoding approaches discussed in this paper. Oracle performances are obtained using the metric of sentence-level BLEU score proposed by Ye et al. (2007), and each decoding approach outputs its 1000-best hypotheses, which are used to extract oracle translations. Model BLEU% MT04 MT05 MT06 MT08 PB 49.53 48.36 43.69 39.39 DHPB 50.66 49.59 44.68 40.47 SC 51.77 50.84 46.87 42.11 CD-PB CD-DHPB CD-Comb MC 50.26 50.10 45.65 40.52 51.91 50.61 46.23 41.01 52.10 51.00 46.95 42.20 52.03 51.22 46.60 42.23 BTG-HMD 52.69+ 51.75+ 47.08 42.71+ SCFG-HMD SCBTG+SCFG 52.94+ 51.40 47.27+ 42.45+ 53.58+ 52.03+ 47.90+ 43.07+ Table 6: Oracle performances of different methods (+: significantly better than the best multiple-system based decoding method (CD-Comb) with p </context>
</contexts>
<marker>Ye, Zhou, Lin, 2007</marker>
<rawString>Yang Ye, Ming Zhou, and Chin-Yew Lin. 2007. Sentence Level Machine Translation Evaluation as a Ranking Problem: one step aside from BLEU. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 240-247.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>