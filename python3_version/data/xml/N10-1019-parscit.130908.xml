<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000353">
<title confidence="0.998083">
Using Mostly Native Data to Correct Errors in Learners&apos; Writing:
A Meta-Classifier Approach
</title>
<author confidence="0.986856">
Michael Gamon
</author>
<affiliation confidence="0.962894">
Microsoft Research
</affiliation>
<address confidence="0.9404785">
One Microsoft Way
Redmond, WA 98052
</address>
<email confidence="0.997013">
mgamon@microsoft.com
</email>
<sectionHeader confidence="0.993848" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.968043130434783">
We present results from a range of experi-
ments on article and preposition error correc-
tion for non-native speakers of English. We
first compare a language model and error-
specific classifiers (all trained on large Eng-
lish corpora) with respect to their performance
in error detection and correction. We then
combine the language model and the classifi-
ers in a meta-classification approach by com-
bining evidence from the classifiers and the
language model as input features to the meta-
classifier. The meta-classifier in turn is trained
on error-annotated learner data, optimizing the
error detection and correction performance on
this domain. The meta-classification approach
results in substantial gains over the classifier-
only and language-model-only scenario. Since
the meta-classifier requires error-annotated
data for training, we investigate how much
training data is needed to improve results over
the baseline of not using a meta-classifier. All
evaluations are conducted on a large error-
annotated corpus of learner English.
</bodyText>
<sectionHeader confidence="0.999013" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999923480769231">
Research on the automatic correction of grammati-
cal errors has undergone a renaissance in the past
decade. This is, at least in part, based on the recog-
nition that non-native speakers of English now
outnumber native speakers by 2:1 in some esti-
mates, so any tool in this domain could be of tre-
mendous value. While earlier work in both native
and non-native error correction was focused on the
construction of grammars and analysis systems to
detect and correct specific errors (see Heift and
Schulze, 2005 for a detailed overview), more re-
cent approaches have been based on data-driven
methods.
The majority of the data-driven methods use a
classification technique to determine whether a
word is used appropriately in its context, continu-
ing the tradition established for contextual spelling
correction by Golding (1995) and Golding and
Roth (1996). The words investigated are typically
articles and prepositions. They have two distinct
advantages as the subject matter for investigation:
They are a closed class and they comprise a sub-
stantial proportion of learners’ errors. The investi-
gation of preposition corrections can even be
narrowed further: amongst the more than 150 Eng-
lish prepositions, the usage of the ten most fre-
quent prepositions accounts for 82% of preposition
errors in the 20 million word Cambridge Universi-
ty Press Learners’ Corpus. Learning correct article
use is most difficult for native speakers of an L1
that does not overtly mark definiteness and indefi-
niteness as English does. Prepositions, on the oth-
er hand, pose difficulties for language learners
from all L1 backgrounds (Dalgish, 1995; Bitchener
et al., 2005).
Contextual classification methods represent the
context of a preposition or article as a feature vec-
tor gleaned from a window of a few words around
the preposition/article. Different systems typically
vary along three dimensions: choice of features,
choice of classifier, and choice of training data.
Features range from words and morphological in-
formation (Knight and Chander, 1994) to the inclu-
sion of part-of-speech tags (Minnen et al., 2000;
Han et al., 2004, 2006; Chodorow et al., 2007;
Gamon et al., 2008, 2009; Izumi et al., 2003, 2004;
Tetrault and Chodorow, 2008) to features based on
linguistic analysis and on WordNet (Lee, 2004;
DeFelice and Pulman, 2007, 2008). Knight and
Chander (1994) and Gamon et al. (2008) used de-
cision tree classifiers but, in general, maximum
entropy classifiers have become the classification
</bodyText>
<page confidence="0.98705">
163
</page>
<note confidence="0.752403">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 163–171,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.998945138888889">
algorithm of choice. Training data are normally
drawn from sizeable corpora of native English text
(British National Corpus for DeFelice and Pulman
(2007, 2008), Wall Street Journal in Knight and
Chander (1994), a mix of Reuters and Encarta in
Gamon et al. (2008, 2009). In order to partially
address the problem of domain mismatch between
learners’ writing and the news-heavy data sets of-
ten used in data-driven NLP applications, Han et
al. (2004, 2006) use 31.5 million words from the
MetaMetrics corpus, a diverse corpus of fiction,
non-fiction and textbooks categorized by reading
level.
In addition to the classification approach to error
detection, there is a line of research - going back to
at least Atwell (1987) - that uses language models.
The idea here is to detect errors in areas where the
language model score is suspiciously low. Atwell
(1987) uses a part-of-speech tag language model to
detect errors, Chodorow and Leacock (2000) use
mutual information and chi square statistics to
identify unlikely function word and part-of-speech
tag sequences, Turner and Charniak (2007) employ
a language model based on a generative statistical
parser, and Stehouwer and van Zaanen (2009) in-
vestigate a diverse set of language models with
different backoff strategies to determine which
choice, from a set of confusable words, is most
likely in a given context. Gamon et al. (2008,
2009) use a combination of error-specific classifi-
ers and a large generic language model with hand-
tuned heuristics for combining their scores to max-
imize precision. Finally, Yi et al. (2008) and Her-
met et al. (2008) use n-gram counts from the web
as a language model approximation to identify
likely errors and correction candidates.
</bodyText>
<sectionHeader confidence="0.932889" genericHeader="method">
2 Our Approach
</sectionHeader>
<bodyText confidence="0.999913862068965">
We combine evidence from the two kinds of data-
driven models that have been used for error detec-
tion and correction (error-specific classifiers and a
language model) through a meta-classifier. We use
the term primary models for both the initial error-
specific classifiers and a large generic language
model. The meta-classifier takes the output of the
primary models (language model scores and class
probabilities) as input. Using a meta-classifier for
ensemble learning has been proven effective for
many machine learning problems (see e.g. Diette-
rich 1997), especially when the combined models
are sufficiently different to make distinct kinds of
errors. The meta-classification approach also has
an advantage in terms of data requirements: Our
primary models are trained on large sets of widely
available well-formed English text. The meta-
classifier, in contrast, is trained on a smaller set of
error-annotated learner data. This allows us to ad-
dress the problem of domain mismatch: We can
leverage large well-formed data sets that are sub-
stantially different from real-life learner language
for the primary models, and then fine-tune the out-
put to learner English using a much smaller set of
expensive and hard-to-come-by annotated learner
writing.
For the purpose of this paper, we restrict our-
selves to article and preposition errors. The ques-
tions we address are:
</bodyText>
<listItem confidence="0.992229428571428">
1. How effective is the meta-classification ap-
proach compared to either a classifier or a lan-
guage model alone?
2. How much error-annotated data are sufficient
to produce positive results above the baseline
of using either a language model or a classifier
alone?
</listItem>
<bodyText confidence="0.99273">
Our evaluation is conducted on a large data set
of error-annotated learner data.
</bodyText>
<sectionHeader confidence="0.996896" genericHeader="method">
3 Experimental Design
</sectionHeader>
<subsectionHeader confidence="0.991582">
3.1 Primary Models
</subsectionHeader>
<bodyText confidence="0.9999256">
Our error-specific primary models are maximum
entropy classifiers (Rathnaparki 1997) for articles
and for prepositions. Features include contextual
features from a window of six tokens to the right
and left, such as lexical features (word), part-of-
speech tags, and a handful of “custom features”,
for example lexical head of governing VP or go-
verned NP (as determined by part-of-speech-tag
based heuristics). For both articles and preposi-
tions, we employ two classifiers: the first deter-
mines the probability that a preposition/article is
present in a given context (presence classifier), the
second classifier determines the probability that a
specific article or preposition is chosen (choice
classifier). A training event for the presence clas-
sifier is any noun phrase boundary that is a poten-
tial location for a preposition or article. Whether a
location is an NP boundary and a potential site for
an article/preposition is determined by a simple
heuristic based on part-of-speech tags.
</bodyText>
<page confidence="0.997684">
164
</page>
<bodyText confidence="0.99902838">
The candidates for article choice are the and
a/an, and the choice for prepositions is limited to
twelve very frequent prepositions (in, at, on, for,
since, with, to, by, about, from, of, as) which ac-
count for 86.2 % of preposition errors in our learn-
er data. At prediction time, the presence and choice
classifiers produce a list of potential changes in
preposition/article usage for the given context.
Since the application of our system consists of
suggesting corrections to a user, we do not consid-
er identity operations where the suggested word
choice equals the actual word choice. For a poten-
tial preposition/article location where there is no
preposition/article, each of the candidates is consi-
dered for an insertion operation. For a potential
location that contains a preposition/article, the
possible operations include deletion of the existing
token or substitution with another preposi-
tion/article from the candidate set. Training data
for the classifiers is a mix of primarily well-formed
data sources: There are about 2.5 million sen-
tences, distributed roughly equally across Reuters
newswire, Encarta encyclopedia, UN proceedings,
Europarl and web-scraped general domain data1.
From the total set of candidate operations (substi-
tutions, insertions, and deletions) that each combi-
nation of presence and choice classifier produces
for prepositions, we consider only the top three
highest-scoring operations2.
Our language model is trained on the Gigaword
corpus (Linguistic Data Consortium, 2003) and
utilizes 7-grams with absolute discount smoothing
(Gao, Goodman, and Miao, 2001; Nguyen, Gao,
and Mahajan, 2007). Each suggested revision from
the preposition/article classifiers (top three for pre-
positions, all revisions from the article classifiers)
are scored by the language model: for each revi-
sion, the language model score of the original and
the suggested rewrite is recorded, as is the lan-
guage model entropy (defined as the language
model probability of the sentence, normalized by
sentence length).
1 We are not able to train the error-specific classifiers on a
larger data set like the one we use for the language model.
Note that the 2.5 million sentences used in the classifier train-
ing already produce 16.5 million training vectors.
2 This increases runtime performance because fewer calls need
to be made to the language model which resides on a server. In
addition, we noticed that overall precision is increased by not
considering the less likely suggestions by the classifier.
</bodyText>
<subsectionHeader confidence="0.999537">
3.2 Meta-Classifier
</subsectionHeader>
<bodyText confidence="0.9636121">
For the meta-classifier we chose to use a decision
tree, trained with the WinMine toolkit (Chickering
2002). The motivation for this choice is that deci-
sion trees are well-suited for continuously valued
features and for non-linear decision surfaces. An
obvious alternative would be to use a support vec-
tor machine with non-linear kernels, a route that
we have not explored yet. The feature set for the
meta-classifier consists of the following scores
from the primary models, including some arithmet-
ic combinations of scores:
Ratio and delta of Log LM score of the origi-
nal word choice and the suggested revision (2
features)
Ratio and delta of the LM entropy for origi-
nal and suggested revision (2 features).
Products of the above ratios/deltas and clas-
sifier choice/presence probabilities
Type of operation: deletion, insertion, substi-
tution (3 features)
</bodyText>
<equation confidence="0.767294">
P(presence) (1 feature)
</equation>
<bodyText confidence="0.996586">
For each preposition/article choice:
P(choice): 13 features for prepositions (12
prepositions and other for a preposition not
in that set), 2 for articles
Original token: none (for insertion) or the
original preposition/article (13 features for
prepositions, 2 for articles)
Suggested token: none (for deletion) or the
suggested preposition/article (13 features for
prepositions, 2 for articles)
The total number of features is 63 for preposi-
tions and 36 for articles.
The meta-classifier is trained by collecting sug-
gested corrections from the primary models on the
error annotated data. The error-annotation provides
the binary class label, i.e. whether the suggested
revision is correct or incorrect. If the suggested
revision matches an annotated correction, it counts
as correct, if it does not match it counts as incor-
rect. To give an example, the top three preposition
operations for the position before this test in the
sentence I rely to this test are:
Change_to_on
Delete_to
Change_to_of
The class label in this example is &amp;quot;suggestion
correct&amp;quot;, assuming that the change of preposition is
</bodyText>
<page confidence="0.993699">
165
</page>
<bodyText confidence="0.998673">
annotated in the data. The operation Change—to—on
in this example has the following feature values for
</bodyText>
<table confidence="0.4562348">
the basic classifier and LM scores:
classifier P(choice): 0.755
classifier P(presence): 0.826
LM logP(original): -17.373
LM logP(rewrite): -14.184
</table>
<bodyText confidence="0.813974">
An example of a path through the decision tree
meta-classifier for prepositions is:
</bodyText>
<equation confidence="0.4240225">
LMLogDelta is Not &lt; -8.59 and
LMLogDelta is Not &lt; -3.7 and
ProductRewriteLogRatioConf is Not &lt; -
0.00115 and
LMLogDelta is Not &lt; -1.58 and
ProductOrigEntropyRatioChoiceConf is Not &lt; -
0.00443 and
choice_prob is Not &lt; 0.206 and
</equation>
<bodyText confidence="0.939975823529412">
Original_of is 0 and
choice_prob is Not &lt; 0.329 and
to_prob is &lt; 0.108 and
Suggested_on is 1 and
Original_in is 0 and
choice_prob is Not &lt; 0.497 and
choice_prob is Not &lt; 0.647 and
presence_prob is Not &lt; 0.553
The leaf node at the end of this path has a 0.21
probability of changing “to” to “on” being a cor-
rect rewrite suggestion.
The features selected by the decision trees range
across all of the features discussed above. For both
the article and preposition meta-classifiers, the
ranking of features by importance (as measured by
how close to the root the decision tree uses the fea-
ture) follows the order in which features are listed.
</bodyText>
<subsectionHeader confidence="0.997113">
3.3 Data
</subsectionHeader>
<bodyText confidence="0.996920384615385">
In contrast to the training data for the primary
models, the meta-classifier is trained on error-
annotated data from the Cambridge University
Press Learners’ Corpus (CLC). The version of
CLC that we have licensed currently contains a
total of 20 million words from learner English es-
says written as part of one of Cambridge’s English
Language Proficiency Tests (ESOL) – at all profi-
ciency levels. The essays are annotated for error
type, erroneous span and suggested correction.
We first perform a random split of the essays in-
to 70% training, 20% test and 10% for parameter
tuning. Next, we create error-specific training, tun-
ing and test sets by performing a number of clean-
up steps on the data. First, we correct all errors that
were flagged as being spelling errors, since we
presume that the user will perform a spelling check
on the data before proceeding to grammatical
proofing. Spelling errors that were flagged as mor-
phology errors were left alone. By the same token,
we corrected confused words that are covered by
MS Word. We then revised British English spel-
ling to American English spelling conventions. In
addition, we eliminated all annotations for non-
pertinent errors (i.e., non-preposition/article errors,
or errors that do not involve any of the targeted
prepositions), but we maintained the original (er-
roneous) text for these. This makes our task harder
since we will have to learn how to make predic-
tions in text containing multiple errors, but it also
is a more realistic scenario given real learner writ-
ing. Finally, we eliminated sentences containing
nested errors and immediately adjacent errors
when they involve pertinent (preposition/article)
errors. For example, an annotated error &amp;quot;take a pic-
ture&amp;quot; with the correction &amp;quot;take pictures&amp;quot; is anno-
tated as two consecutive errors: &amp;quot;delete a&amp;quot; and
&amp;quot;rewrite picture as pictures&amp;quot;. Since the error in-
volves operations on both the article and the noun,
which our article correction module is not designed
to cover, we eliminated the sentence from the data.
(This last step eliminated 31% of the sentences
annotated with preposition errors and 29% or the
sentences annotated with article errors.) Sentences
that were flagged for a replacement error but con-
tained no replacement were also eliminated from
the data.
The final training, tuning and test set sizes are as
follows (note that for prepositions we had to re-
duce the size of the training set by an additional
20% in order to avoid memory limitations of our
decision tree tools).
Prepositions:
train: 584,485 sentences, 68,806 prep errors
tuning: 105,166 sentences, 9918 prep errors
test: 208,724 sentences, 19,706 prep errors
Articles:
train: 737,091 sentences, 58,356 article errors
tuning: 106,052 sentences, 8341 article errors
test: 210,577 sentences, 16,742 article errors
This mix is strongly biased towards “correct”
usage. After all, there are many more correct uses
of articles and prepositions in the CLC data than
incorrect ones. Again, this is likely to make our
task harder, but more realistic, since both at train-
</bodyText>
<page confidence="0.99465">
166
</page>
<bodyText confidence="0.9973325">
ing and test time we are working with the error
distribution that is observed in learner data.
</bodyText>
<subsectionHeader confidence="0.892508">
3.4 Evaluation
</subsectionHeader>
<bodyText confidence="0.979695627118644">
To evaluate, we run our meta-classifier system on
the preposition and article test sets described in
above and calculate precision and recall. Precision
and recall for the overall system are controlled by
thresholding the meta-classifier class probability.
As a point of comparison, we also evaluate the per-
formance of the primary models (the error-specific
classifier and the language model) in isolation.
Precision and recall for the error-specific classifier
is controlled by thresholding class probability. To
control the precision-recall tradeoff for the lan-
guage model, we calculate the difference between
the log probabilities of the original user input and
the suggested correction. We then vary that differ-
ence across all observed values in small incre-
ments, which affects precision and recall: the
higher the difference, the fewer instances we find,
but the higher the reliability of these instances is.
This evaluation differs from many of the evalua-
tions reported in the error detection/correction lite-
rature in several respects. First, the test set is a
broad random sample across all proficiency levels
in the CLC data. Second, it is far larger than any
sets that have been so far to report results of prepo-
sition/article correction on learner data. Finally, we
are only considering cases in which the system
suggests a correction. In other words, we do not
count as correct instances where the system&apos;s pre-
diction matches a correct preposition/article.
This evaluation scheme, however, ignores one
aspect of a real user scenario. Of all the suggested
changes that are counted as wrong in our evalua-
tion because they do not match an annotated error,
some may in fact be innocuous or even helpful for
a real user. Such a situation can arise for a variety
of reasons: In some cases, there are legitimate al-
ternative ways to correct an error. In other cases,
the classifier has identified the location of an error
although that error is of a different kind (which can
be beneficial because it causes the user to make a
correction - see Leacock et al., 2009). Gamon et al.
(2009), for example manually evaluate preposition
suggestions as belonging to one of three catego-
ries: (a) properly correcting an existing error, (b)
offering a suggestion that neither improves nor
degrades the user sentence, (c) offering a sugges-
tion that would degrade the user input. Obviously,
(c) is a more serious error than (b). Similarly, Te-
trault and Chodorow (2008) annotate their test set
with preposition choices that are valid alternatives.
We do not have similar information in the CLC
data, but we can perform a manual analysis of a
random subset of test data to estimate an &amp;quot;upper
bound&amp;quot; for our precision/recall curve. Our annota-
tor manually categorized each suggested correction
into one of seven categories.
Details of the distribution of suggested correc-
tions into the seven categories are shown in Table
1.
</bodyText>
<table confidence="0.963349833333333">
Category preps. articles
Corrects a CLC error 32.87% 33.34%
Corrects an error that 11.67% 12.16%
was not annotated as be-
ing that error type in CLC
Corrects a CLC error, but 3.62% 2.26%
uses an alternative cor-
rection
Original and suggested 9.60% 11.30%
correction are equally
good
Error correctly detected, 8.73% 5.03%
but the correction is
wrong
Identifies an error site, 19.17% 12.64%
but the actual error is not
a preposition error
Introduces an error 14.65% 23.26%
</table>
<tableCaption confidence="0.994098">
Table 1: Manual analysis of suggested corrections on
CLC data.
</tableCaption>
<bodyText confidence="0.999946133333333">
This analysis involves costly manual evaluation,
so we only performed it at one point of the preci-
sion/recall curve (our current runtime system set-
ting). The sample size was 6,000 sentences for
prepositions and 5981 sentences for articles (half
of the sentences were flagged as containing at least
one article/preposition error while the other half
were not). On this manual evaluation, we achieve
32.87% precision if we count all flags that do not
perfectly match a CLC annotation as a false posi-
tive. Only counting the last category (introduction
of an error) as a false positive, precision is at
85.34%. Similarly, for articles, the manual estima-
tion arrives at 76.74% precision, where pure CLC
annotation matching gives us 33.34%.
</bodyText>
<page confidence="0.998062">
167
</page>
<sectionHeader confidence="0.999767" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.999456">
Figure 1 and Figure 2 show the evaluation results
of the meta-classifier for prepositions and articles,
compared to the performance of the error-specific
classifier and language model alone. For both pre-
positions and articles, the first notable observation
is that the language model outperforms the clas-
sifier by a large margin. This came as a surprise to
us, given the recent prevalence of classification
approaches in this area of research and the fact that
our classifiers produce state-of-the art performance
when compared to other systems, on well-formed
data. Second, the combination of scores from the
classifier and language model through a meta-
classifier clearly outperforms either one of them in
isolation. This result, again, is consistent across
prepositions and articles.
We had previously used a hand-tuned score
combination instead of a meta-classifier. We also
established that this heuristic performs worse than
the language model for prepositions, and just about
at the same level as the language model for ar-
ticles. Note, though, that the manual tuning was
performed to optimize performance against a dif-
ferent data set (the Chinese Learners of English
Corpus: CLEC), so the latter point is not really
comparable and hence is not included in the charts.
</bodyText>
<figureCaption confidence="0.9999545">
Figure 1: Precision and recall for prepositions.
Figure 2: Precision and recall for articles.
</figureCaption>
<bodyText confidence="0.999984294117647">
We now turn to the question of the required
amount of annotated training data for the meta-
classifier. CLC is commercially available, but it is
obvious that for many researchers such a corpus
will be too expensive and they will have to create
or license their own error-annotated corpus. Thus
the question of whether one could use less anno-
tated data to train a meta-classifier and still achieve
reasonable results becomes important. Figure 3 and
Figure 4 show results obtained by using decreasing
amounts of training data. The dotted line shows the
language model baseline. Any result below the
language model performance shows that the train-
ing data is insufficient to warrant the use of a meta-
classifier. In these experiments there is a clear dif-
ference between prepositions and articles. We can
reduce the amount of training data for prepositions
to 10% of the original data and still outperform the
language model baseline. 10% of the data corres-
ponds to 6,800 annotated preposition errors and
58,400 sentences. When we reduce the training
data to 1% of the original amount (680 annotated
errors, 5,800 sentences) we clearly see degraded
results compared to the language model. With ar-
ticles, the system is much less data-hungry. Reduc-
ing the training data to 1% (580 annotated errors,
7,400 sentences) still outperforms the language
model alone. This result can most likely be ex-
plained by the different complexity of the preposi-
tion and article tasks. Article operations include
only six distinct operations: deletion of the, dele-
tion of a/an, insertion of the, insertion of a/an,
change of the to a/an, and change of a/an to the.
For the twelve prepositions that we work with, the
</bodyText>
<figure confidence="0.998743205882353">
Prepositions
0 0.2 0.4 0.6
Recall
LM only classifier only
learned thresholds
Precision
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
Articles
0 0.2 0.4 0.6 0.8 1
Recall
Learned thresholds classifier only
LM only
Precision
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
</figure>
<page confidence="0.992831">
168
</page>
<bodyText confidence="0.9995195">
total number of insertions, deletions and substitu-
tions that require sufficient training events and
might need different score combinations is 168,
making the problem much harder.
</bodyText>
<figureCaption confidence="0.9839005">
Figure 3: Using different amounts of annotated training
data for the preposition meta-classifier.
Figure 4: Using different amounts of annotated training
data for the article meta-classifier.
</figureCaption>
<bodyText confidence="0.999935818181818">
To find out if it is possible to reduce the re-
quired amount of annotated preposition errors for a
system that still covers more than one third of the
preposition errors, we ran the same learning curve
experiments but now only taking the four most
frequent prepositions into account: to, of, in, for. In
the CLC, these four prepositions account for
39.8% of preposition error flags. As in the previous
experiments, however, we found that we are not
able to outperform the baseline by using just 1% of
annotated data.
</bodyText>
<sectionHeader confidence="0.996136" genericHeader="method">
5 Error Analysis
</sectionHeader>
<bodyText confidence="0.999978530612245">
We have conducted a failure analysis on examples
where the system produces a blatantly bad sugges-
tion in order to see whether this decision could be
attributed to the error-specific classifier or to the
language model, or both, and what the underlying
cause is. This preliminary analysis highlights two
common causes for bad flags. One is that of fre-
quent lower order n-grams that dominate the lan-
guage model score. Consider the CLEC sentence I
get to know the world outside the campus by news-
paper and television. The system suggests deleting
by. The cause of this bad decision is that the bi-
gram campus newspaper is extremely likely,
trumping all other n-grams, and leading to a high
probability for the suggested string compared to
the original: Log (P(original)) _ -26.2 and Log
(P(suggestion)) _ -22.4. This strong imbalance of
the language model score causes the meta-
classifier to assign a relatively high probability to
this being a correct revision, even though the error-
specific classifier is on the right track and gives a
relatively high probability for the presence of a
preposition and the choice of by. A similar exam-
ple, but for substitution, occurs in They give dis-
counts to their workers on books. Here the bigram
in books has a very high probability and the system
incorrectly suggests replacing on with in. An ex-
ample for insertion is seen in Please send me the
letter back writing what happened. Here, the bi-
gram back to causes the bad suggestion of inserting
to after back. Since the language model is general-
ly more accurate than the error-specific classifier,
the meta-classifier tends to trust its score more than
that of the classifier. As a result we see this kind of
error quite frequently.
Another common error class is the opposite situ-
ation: the language model is on the right track, but
the classifier makes a wrong assessment. Consider
Whatever direction my leg fought to stretch, with
the suggested insertion of on before my leg. Here
Log (P(original)) _ -31.5 and Log (P(suggestion))
_ -32.1, a slight preference for the original string.
The error-specific classifier, however, assigns a
probability of 0.65 for a preposition to be present,
and 0.80 for that preposition to be on. The contex-
tual features that are important in that decision are:
the insertion site is between a pronoun and a noun,
it is relatively close to the beginning of the sen-
tence, and the head of the NP my leg has a possible
</bodyText>
<figure confidence="0.998833852941177">
Prepositions
0 0.2 0.4 0.6
Recall
100% training data LM only
10% training data 1% training data
Precision
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
Articles
0 0.2 0.4 0.6 0.8 1
Recall
100% training data 10% training data
Language model alone 1% training data
Precision
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
</figure>
<page confidence="0.994187">
169
</page>
<bodyText confidence="0.99990975">
mass noun sense. An example involving deletion is
in Someone came to sort of it. While the language
model assigns a high probability for deleting of,
the error-specific classifier does not. Similarly, for
substitution, in Your experience is very interesting
for our company, the language model suggests
substituting for with to while the classifier gives
the substitution a very low probability.
As can be seen from the learner sentences cited
above, often, even though the sentences are gram-
matical, they are not idiomatic, which can confuse
all of the classifiers.
</bodyText>
<sectionHeader confidence="0.996597" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9558674375">
We have addressed two questions in this paper:
1. How effective is a meta-classification ap-
proach that combines language modeling and
error-specific classification to the detection
and correction of preposition and article errors
by non-native speakers?
2. How much error-annotated data is sufficient to
produce positive results using that approach?
We have shown that a meta-classifier approach
outperforms using a language model or a classifier
alone. An interesting side result is that the lan-
guage model solidly outperforms the contextual
classifier for both article and preposition correc-
tion, contrary to current practice in the field. Train-
ing data requirements for the meta-classifier vary
significantly between article and preposition error
detection. The article meta-classifier can be trained
with as few as 600 annotated errors, but the prepo-
sition meta-classifier requires more annotated data
by an order of magnitude. Still, the overall amount
of expensive error-annotated data is relatively
small, and the meta-classification approach makes
it possible to leverage large amounts of well-
formed text in the primary models, tuning to the
non-native domain in the meta-classifier.
We believe that the logical next step is to com-
bine more primary models in the meta-classifier.
Candidates for additional primary models include
(1) more classifiers trained either on different data
sets or with a different classification algorithm, and
(2) more language models, such as skip models or
part-of-speech n-gram language models.
</bodyText>
<sectionHeader confidence="0.998622" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99974225">
We thank Claudia Leacock from the Butler Hill
Group for detailed error analysis and the anonym-
ous reviewers for helpful and constructive feed-
back.
</bodyText>
<sectionHeader confidence="0.962506" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999272363636363">
Eric Steven Atwell. 1987. How to detect grammatical
errors in a text without parsing it. In Proceedings of
the 3rd EACL (pp 38 – 45). Copenhagen.
John Bitchener, Stuart Young, and Denise Cameron.
2005. The effect of different types of corrective feed-
back on ESL student writing. Journal of Second Lan-
guage Writing, 14(3), 191-205.
David Maxwell Chickering. 2002. The WinMine Tool-
kit. Microsoft Technical Report 2002-103. Redmond.
Martin Chodorow, Joel Tetreault, and Na-Rae Han.
2007. Detection of grammatical errors involving pre-
positions. In Proceedings of the Fourth ACL-
SIGSEM Workshop on Prepositions (pp. 25-30). Pra-
gue.
Gerard M. Dalgish. 1985. Computer-assisted ESL re-
search and courseware development. Computers and
Composition, 2(4), 45-62.
Rachele De Felice and Stephen G. Pulman. 2007. Au-
tomatically acquiring models of preposition use. In
Proceedings of the Fourth ACL-SIGSEM Workshop
on Prepositions (pp. 45-50). Prague.
Rachele De Felice and Stephen Pulman. 2008. A clas-
sifier-based approach to preposition and determiner
error correction in L2 English. In Proceedings of
COLING. Manchester, UK.
Thomas G. Dietterich. 1997. Machine learning research:
Four current directions. AI Magazine, 18(4), 97-136.
Ted Dunning. 1993. Accurate Methods for the Statistics
of Surprise and Coincidence. Computational Linguis-
tics, 19, 61-74.
Michael Gamon, Claudia Leacock, Chris Brockett, Wil-
liam B. Dolan, Jianfeng Gao, Dmitriy Belenko, and
Alexandre Klementiev,. 2009. Using statistical tech-
niques and web search to correct ESL errors.
CALICO Journal, 26(3).
Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-
der Klementiev, William Dolan, Dmitriy Belenko,
and Lucy Vanderwende. 2008. Using contextual
speller techniques and language modeling for ESL
error correction. In Proceedings of IJCNLP, Hydera-
bad, India.
Jianfeng Gao, Joshua Goodman, and Jiangbo Miao.
2001. The use of clustering techniques for language
modeling—Application to Asian languages. Compu-
</reference>
<page confidence="0.969536">
170
</page>
<reference confidence="0.999802829787234">
tational Linguistics and Chinese Language
Processing, 6(1), 27-60.
Andrew Golding. 1995. A Bayesian Hybrid for Context
Sensitive Spelling Correction. In Proceedings of the
3rd Workshop on Very Large Corpora (pp. 39–53).
Cambridge, USA.
Andrew R. Golding and Dan Roth. 1996. Applying
Winnow to context-sensitive spelling correction. In
Proceedings of the Int. Conference on Machine
Learning (pp 182 –190).
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2004. Detecting errors in English article usage with a
maximum entropy classifier trained on a large, di-
verse corpus. In Proceedings of the 4th International
Conference on Language Resources and Evaluation.
Lisbon.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineering,
12(2), 115-129.
Trude Heift and Mathias Schulze. 2007. Errors and
Intelligence in Computer-Assisted Language Learn-
ing: Parsers and Pedagogues. New York &amp; London:
Routledge.
Matthieu Hermet, Alain Désilets, and Stan Szpakowicz.
2008. Using the web as a linguistic resource to auto-
matically correct lexico-yyntactic errors. In Proceed-
ings of the 6th Conference on Language Resources
and Evaluation (LREC), (pp. 874 - 878).
Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thep-
chai Supnithi and Hitoshi Isahara. 2003. Automatic
error detection in the Japanese learners&apos; English spo-
ken data. In Proceedings of the 41st Annual Meeting
of the Association for Computational Linguistics
(pp. 145-148).
Emi Izumi, Kiyotaka Uchimoto and Hitoshi Isahara.
2004. SST speech corpus of Japanese learners&apos; Eng-
lish and automatic detection of learners&apos; errors. In
Proceedings of the 4th International Conference on
Language Resources and Evaluation (LREC), (Vol 4,
pp. 31-48).
Kevin Knight and Ishwar Chander,. 1994. Automatic
postediting of documents. In Proceedings of the 12th
National Conference on Artificial Intelligence (pp.
779-784). Seattle: Morgan Kaufmann.
Claudia Leacock, Michael Gamon, and Chris Brockett.
2009. User Input and Interactions on Microsoft ESL
Assistant. In Proceedings of the Fourth Workshop on
Innovative Use of NLP for Building Educational Ap-
plications (pp. 73-81).
John Lee. 2004. Automatic article restoration. In Pro-
ceedings of the Human Language Technology Confe-
rence of the North American Chapter of the
Association for Computational Linguistics, (pp. 31-
36). Boston.
Guido Minnen, Francis Bond, and Anne Copestake.
2000. Memory-based learning for article generation.
In Proceedings of the Fourth Conference on Compu-
tational Natural Language Learning and of the
Second Learning Language in Logic Workshop (pp.
43-48). Lisbon.
Patrick Nguyen, Jianfeng Gao, and Milind Mahajan.
2007. MSRLM: A scalable language modeling tool-
kit. Microsoft Technical Report 2007-144. Redmond.
Adwait Ratnaparkhi. 1997. A simple introduction to
maximum entropy models for natural language
processing. Technical Report IRCS Report 97-98, In-
stitute for Research in Cognitive Science, University
of Pennsylvania.
Herman Stehouwer and Menno van Zaanen. 2009. Lan-
guage models for contextual error detection and cor-
rection. In Proceedings of the EACL 2009 Workshop
on Computational Linguistic Aspects of Grammatical
Inference ( pp. 41-48). Athens.
Joel Tetreault and Martin Chodorow. 2008a. The ups
and downs of preposition error detection in ESL. In
Proceedings of COLING. Manchester, UK.
Joel Tetreault and Martin Chodorow. 2008b. Native
judgments of non-native usage: Experiments in pre-
position error detection. In Proceedings of the Work-
shop on Human Judgments in Computational
Linguistics, 22nd International Conference on Com-
putational Linguistics (pp 43-48). Manchester, UK.
Jenine Turner and Eugene Charniak. 2007. Language
modeling for determiner selection. In Human Lan-
guage Technologies 2007: NAACL; Companion Vo-
lume, Short Papers (pp. 177-180). Rochester, NY.
Wikipedia. English Language.
http://en.wikipedia.org/wiki/English_language
Xing Yi, Jianfeng Gao, and Bill Dolan. 2008. A web-
based English proofing system for English as a
second language users. In Proceedings of the Third
International Joint Conference on Natural Language
Processing (IJCNLP). Hyderabad, India.
</reference>
<page confidence="0.998203">
171
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.550158">
<title confidence="0.9997525">Using Mostly Native Data to Correct Errors in Learners&apos; A Meta-Classifier Approach</title>
<author confidence="0.990137">Michael</author>
<affiliation confidence="0.955188">Microsoft</affiliation>
<address confidence="0.8815255">One Microsoft Redmond, WA</address>
<email confidence="0.999891">mgamon@microsoft.com</email>
<abstract confidence="0.989192291666667">We present results from a range of experiments on article and preposition error correction for non-native speakers of English. We first compare a language model and errorspecific classifiers (all trained on large English corpora) with respect to their performance in error detection and correction. We then combine the language model and the classifiers in a meta-classification approach by combining evidence from the classifiers and the language model as input features to the metaclassifier. The meta-classifier in turn is trained on error-annotated learner data, optimizing the error detection and correction performance on this domain. The meta-classification approach results in substantial gains over the classifieronly and language-model-only scenario. Since the meta-classifier requires error-annotated data for training, we investigate how much training data is needed to improve results over the baseline of not using a meta-classifier. All evaluations are conducted on a large errorannotated corpus of learner English.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eric Steven Atwell</author>
</authors>
<title>How to detect grammatical errors in a text without parsing it.</title>
<date>1987</date>
<booktitle>In Proceedings of the 3rd EACL (pp 38 – 45).</booktitle>
<location>Copenhagen.</location>
<contexts>
<context position="4639" citStr="Atwell (1987)" startWordPosition="718" endWordPosition="719">National Corpus for DeFelice and Pulman (2007, 2008), Wall Street Journal in Knight and Chander (1994), a mix of Reuters and Encarta in Gamon et al. (2008, 2009). In order to partially address the problem of domain mismatch between learners’ writing and the news-heavy data sets often used in data-driven NLP applications, Han et al. (2004, 2006) use 31.5 million words from the MetaMetrics corpus, a diverse corpus of fiction, non-fiction and textbooks categorized by reading level. In addition to the classification approach to error detection, there is a line of research - going back to at least Atwell (1987) - that uses language models. The idea here is to detect errors in areas where the language model score is suspiciously low. Atwell (1987) uses a part-of-speech tag language model to detect errors, Chodorow and Leacock (2000) use mutual information and chi square statistics to identify unlikely function word and part-of-speech tag sequences, Turner and Charniak (2007) employ a language model based on a generative statistical parser, and Stehouwer and van Zaanen (2009) investigate a diverse set of language models with different backoff strategies to determine which choice, from a set of confusa</context>
</contexts>
<marker>Atwell, 1987</marker>
<rawString>Eric Steven Atwell. 1987. How to detect grammatical errors in a text without parsing it. In Proceedings of the 3rd EACL (pp 38 – 45). Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Bitchener</author>
<author>Stuart Young</author>
<author>Denise Cameron</author>
</authors>
<title>The effect of different types of corrective feedback on ESL student writing.</title>
<date>2005</date>
<journal>Journal of Second Language Writing,</journal>
<volume>14</volume>
<issue>3</issue>
<pages>191--205</pages>
<contexts>
<context position="2881" citStr="Bitchener et al., 2005" startWordPosition="440" endWordPosition="443">y comprise a substantial proportion of learners’ errors. The investigation of preposition corrections can even be narrowed further: amongst the more than 150 English prepositions, the usage of the ten most frequent prepositions accounts for 82% of preposition errors in the 20 million word Cambridge University Press Learners’ Corpus. Learning correct article use is most difficult for native speakers of an L1 that does not overtly mark definiteness and indefiniteness as English does. Prepositions, on the other hand, pose difficulties for language learners from all L1 backgrounds (Dalgish, 1995; Bitchener et al., 2005). Contextual classification methods represent the context of a preposition or article as a feature vector gleaned from a window of a few words around the preposition/article. Different systems typically vary along three dimensions: choice of features, choice of classifier, and choice of training data. Features range from words and morphological information (Knight and Chander, 1994) to the inclusion of part-of-speech tags (Minnen et al., 2000; Han et al., 2004, 2006; Chodorow et al., 2007; Gamon et al., 2008, 2009; Izumi et al., 2003, 2004; Tetrault and Chodorow, 2008) to features based on lin</context>
</contexts>
<marker>Bitchener, Young, Cameron, 2005</marker>
<rawString>John Bitchener, Stuart Young, and Denise Cameron. 2005. The effect of different types of corrective feedback on ESL student writing. Journal of Second Language Writing, 14(3), 191-205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Maxwell Chickering</author>
</authors>
<title>The WinMine Toolkit.</title>
<date>2002</date>
<tech>Microsoft Technical Report 2002-103. Redmond.</tech>
<contexts>
<context position="11041" citStr="Chickering 2002" startWordPosition="1718" endWordPosition="1719">). 1 We are not able to train the error-specific classifiers on a larger data set like the one we use for the language model. Note that the 2.5 million sentences used in the classifier training already produce 16.5 million training vectors. 2 This increases runtime performance because fewer calls need to be made to the language model which resides on a server. In addition, we noticed that overall precision is increased by not considering the less likely suggestions by the classifier. 3.2 Meta-Classifier For the meta-classifier we chose to use a decision tree, trained with the WinMine toolkit (Chickering 2002). The motivation for this choice is that decision trees are well-suited for continuously valued features and for non-linear decision surfaces. An obvious alternative would be to use a support vector machine with non-linear kernels, a route that we have not explored yet. The feature set for the meta-classifier consists of the following scores from the primary models, including some arithmetic combinations of scores: Ratio and delta of Log LM score of the original word choice and the suggested revision (2 features) Ratio and delta of the LM entropy for original and suggested revision (2 features</context>
</contexts>
<marker>Chickering, 2002</marker>
<rawString>David Maxwell Chickering. 2002. The WinMine Toolkit. Microsoft Technical Report 2002-103. Redmond.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Chodorow</author>
<author>Joel Tetreault</author>
<author>Na-Rae Han</author>
</authors>
<title>Detection of grammatical errors involving prepositions.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth ACLSIGSEM Workshop on Prepositions</booktitle>
<pages>25--30</pages>
<location>Prague.</location>
<contexts>
<context position="3374" citStr="Chodorow et al., 2007" startWordPosition="517" endWordPosition="520">ions, on the other hand, pose difficulties for language learners from all L1 backgrounds (Dalgish, 1995; Bitchener et al., 2005). Contextual classification methods represent the context of a preposition or article as a feature vector gleaned from a window of a few words around the preposition/article. Different systems typically vary along three dimensions: choice of features, choice of classifier, and choice of training data. Features range from words and morphological information (Knight and Chander, 1994) to the inclusion of part-of-speech tags (Minnen et al., 2000; Han et al., 2004, 2006; Chodorow et al., 2007; Gamon et al., 2008, 2009; Izumi et al., 2003, 2004; Tetrault and Chodorow, 2008) to features based on linguistic analysis and on WordNet (Lee, 2004; DeFelice and Pulman, 2007, 2008). Knight and Chander (1994) and Gamon et al. (2008) used decision tree classifiers but, in general, maximum entropy classifiers have become the classification 163 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 163–171, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics algorithm of choice. Training data are normally drawn fr</context>
</contexts>
<marker>Chodorow, Tetreault, Han, 2007</marker>
<rawString>Martin Chodorow, Joel Tetreault, and Na-Rae Han. 2007. Detection of grammatical errors involving prepositions. In Proceedings of the Fourth ACLSIGSEM Workshop on Prepositions (pp. 25-30). Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard M Dalgish</author>
</authors>
<title>Computer-assisted ESL research and courseware development.</title>
<date>1985</date>
<journal>Computers and Composition,</journal>
<volume>2</volume>
<issue>4</issue>
<pages>45--62</pages>
<marker>Dalgish, 1985</marker>
<rawString>Gerard M. Dalgish. 1985. Computer-assisted ESL research and courseware development. Computers and Composition, 2(4), 45-62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rachele De Felice</author>
<author>Stephen G Pulman</author>
</authors>
<title>Automatically acquiring models of preposition use.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth ACL-SIGSEM Workshop on Prepositions</booktitle>
<pages>45--50</pages>
<location>Prague.</location>
<marker>De Felice, Pulman, 2007</marker>
<rawString>Rachele De Felice and Stephen G. Pulman. 2007. Automatically acquiring models of preposition use. In Proceedings of the Fourth ACL-SIGSEM Workshop on Prepositions (pp. 45-50). Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rachele De Felice</author>
<author>Stephen Pulman</author>
</authors>
<title>A classifier-based approach to preposition and determiner error correction in L2 English.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING.</booktitle>
<publisher>Manchester, UK.</publisher>
<marker>De Felice, Pulman, 2008</marker>
<rawString>Rachele De Felice and Stephen Pulman. 2008. A classifier-based approach to preposition and determiner error correction in L2 English. In Proceedings of COLING. Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G Dietterich</author>
</authors>
<title>Machine learning research: Four current directions.</title>
<date>1997</date>
<journal>AI Magazine,</journal>
<volume>18</volume>
<issue>4</issue>
<pages>97--136</pages>
<contexts>
<context position="6210" citStr="Dietterich 1997" startWordPosition="967" endWordPosition="969">ify likely errors and correction candidates. 2 Our Approach We combine evidence from the two kinds of datadriven models that have been used for error detection and correction (error-specific classifiers and a language model) through a meta-classifier. We use the term primary models for both the initial errorspecific classifiers and a large generic language model. The meta-classifier takes the output of the primary models (language model scores and class probabilities) as input. Using a meta-classifier for ensemble learning has been proven effective for many machine learning problems (see e.g. Dietterich 1997), especially when the combined models are sufficiently different to make distinct kinds of errors. The meta-classification approach also has an advantage in terms of data requirements: Our primary models are trained on large sets of widely available well-formed English text. The metaclassifier, in contrast, is trained on a smaller set of error-annotated learner data. This allows us to address the problem of domain mismatch: We can leverage large well-formed data sets that are substantially different from real-life learner language for the primary models, and then fine-tune the output to learne</context>
</contexts>
<marker>Dietterich, 1997</marker>
<rawString>Thomas G. Dietterich. 1997. Machine learning research: Four current directions. AI Magazine, 18(4), 97-136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate Methods for the Statistics of Surprise and Coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<pages>61--74</pages>
<marker>Dunning, 1993</marker>
<rawString>Ted Dunning. 1993. Accurate Methods for the Statistics of Surprise and Coincidence. Computational Linguistics, 19, 61-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
<author>Claudia Leacock</author>
<author>Chris Brockett</author>
<author>William B Dolan</author>
<author>Jianfeng Gao</author>
<author>Dmitriy Belenko</author>
<author>Alexandre Klementiev</author>
</authors>
<title>Using statistical techniques and web search to correct ESL errors.</title>
<date>2009</date>
<journal>CALICO Journal,</journal>
<volume>26</volume>
<issue>3</issue>
<contexts>
<context position="19380" citStr="Gamon et al. (2009)" startWordPosition="3063" endWordPosition="3066">is evaluation scheme, however, ignores one aspect of a real user scenario. Of all the suggested changes that are counted as wrong in our evaluation because they do not match an annotated error, some may in fact be innocuous or even helpful for a real user. Such a situation can arise for a variety of reasons: In some cases, there are legitimate alternative ways to correct an error. In other cases, the classifier has identified the location of an error although that error is of a different kind (which can be beneficial because it causes the user to make a correction - see Leacock et al., 2009). Gamon et al. (2009), for example manually evaluate preposition suggestions as belonging to one of three categories: (a) properly correcting an existing error, (b) offering a suggestion that neither improves nor degrades the user sentence, (c) offering a suggestion that would degrade the user input. Obviously, (c) is a more serious error than (b). Similarly, Tetrault and Chodorow (2008) annotate their test set with preposition choices that are valid alternatives. We do not have similar information in the CLC data, but we can perform a manual analysis of a random subset of test data to estimate an &amp;quot;upper bound&amp;quot; fo</context>
</contexts>
<marker>Gamon, Leacock, Brockett, Dolan, Gao, Belenko, Klementiev, 2009</marker>
<rawString>Michael Gamon, Claudia Leacock, Chris Brockett, William B. Dolan, Jianfeng Gao, Dmitriy Belenko, and Alexandre Klementiev,. 2009. Using statistical techniques and web search to correct ESL errors. CALICO Journal, 26(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
<author>Jianfeng Gao</author>
<author>Chris Brockett</author>
<author>Alexander Klementiev</author>
<author>William Dolan</author>
<author>Dmitriy Belenko</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Using contextual speller techniques and language modeling for ESL error correction.</title>
<date>2008</date>
<booktitle>In Proceedings of IJCNLP,</booktitle>
<location>Hyderabad, India.</location>
<contexts>
<context position="3394" citStr="Gamon et al., 2008" startWordPosition="521" endWordPosition="524">, pose difficulties for language learners from all L1 backgrounds (Dalgish, 1995; Bitchener et al., 2005). Contextual classification methods represent the context of a preposition or article as a feature vector gleaned from a window of a few words around the preposition/article. Different systems typically vary along three dimensions: choice of features, choice of classifier, and choice of training data. Features range from words and morphological information (Knight and Chander, 1994) to the inclusion of part-of-speech tags (Minnen et al., 2000; Han et al., 2004, 2006; Chodorow et al., 2007; Gamon et al., 2008, 2009; Izumi et al., 2003, 2004; Tetrault and Chodorow, 2008) to features based on linguistic analysis and on WordNet (Lee, 2004; DeFelice and Pulman, 2007, 2008). Knight and Chander (1994) and Gamon et al. (2008) used decision tree classifiers but, in general, maximum entropy classifiers have become the classification 163 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 163–171, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics algorithm of choice. Training data are normally drawn from sizeable corpora </context>
<context position="5303" citStr="Gamon et al. (2008" startWordPosition="822" endWordPosition="825"> to detect errors in areas where the language model score is suspiciously low. Atwell (1987) uses a part-of-speech tag language model to detect errors, Chodorow and Leacock (2000) use mutual information and chi square statistics to identify unlikely function word and part-of-speech tag sequences, Turner and Charniak (2007) employ a language model based on a generative statistical parser, and Stehouwer and van Zaanen (2009) investigate a diverse set of language models with different backoff strategies to determine which choice, from a set of confusable words, is most likely in a given context. Gamon et al. (2008, 2009) use a combination of error-specific classifiers and a large generic language model with handtuned heuristics for combining their scores to maximize precision. Finally, Yi et al. (2008) and Hermet et al. (2008) use n-gram counts from the web as a language model approximation to identify likely errors and correction candidates. 2 Our Approach We combine evidence from the two kinds of datadriven models that have been used for error detection and correction (error-specific classifiers and a language model) through a meta-classifier. We use the term primary models for both the initial error</context>
</contexts>
<marker>Gamon, Gao, Brockett, Klementiev, Dolan, Belenko, Vanderwende, 2008</marker>
<rawString>Michael Gamon, Jianfeng Gao, Chris Brockett, Alexander Klementiev, William Dolan, Dmitriy Belenko, and Lucy Vanderwende. 2008. Using contextual speller techniques and language modeling for ESL error correction. In Proceedings of IJCNLP, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Joshua Goodman</author>
<author>Jiangbo Miao</author>
</authors>
<title>The use of clustering techniques for language modeling—Application to Asian languages.</title>
<date>2001</date>
<booktitle>Computational Linguistics and Chinese Language Processing,</booktitle>
<volume>6</volume>
<issue>1</issue>
<pages>27--60</pages>
<contexts>
<context position="9997" citStr="Gao, Goodman, and Miao, 2001" startWordPosition="1548" endWordPosition="1552">rs is a mix of primarily well-formed data sources: There are about 2.5 million sentences, distributed roughly equally across Reuters newswire, Encarta encyclopedia, UN proceedings, Europarl and web-scraped general domain data1. From the total set of candidate operations (substitutions, insertions, and deletions) that each combination of presence and choice classifier produces for prepositions, we consider only the top three highest-scoring operations2. Our language model is trained on the Gigaword corpus (Linguistic Data Consortium, 2003) and utilizes 7-grams with absolute discount smoothing (Gao, Goodman, and Miao, 2001; Nguyen, Gao, and Mahajan, 2007). Each suggested revision from the preposition/article classifiers (top three for prepositions, all revisions from the article classifiers) are scored by the language model: for each revision, the language model score of the original and the suggested rewrite is recorded, as is the language model entropy (defined as the language model probability of the sentence, normalized by sentence length). 1 We are not able to train the error-specific classifiers on a larger data set like the one we use for the language model. Note that the 2.5 million sentences used in th</context>
</contexts>
<marker>Gao, Goodman, Miao, 2001</marker>
<rawString>Jianfeng Gao, Joshua Goodman, and Jiangbo Miao. 2001. The use of clustering techniques for language modeling—Application to Asian languages. Computational Linguistics and Chinese Language Processing, 6(1), 27-60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Golding</author>
</authors>
<title>A Bayesian Hybrid for Context Sensitive Spelling Correction.</title>
<date>1995</date>
<booktitle>In Proceedings of the 3rd Workshop on Very Large Corpora</booktitle>
<pages>39--53</pages>
<location>Cambridge, USA.</location>
<contexts>
<context position="2058" citStr="Golding (1995)" startWordPosition="312" endWordPosition="313">r native speakers by 2:1 in some estimates, so any tool in this domain could be of tremendous value. While earlier work in both native and non-native error correction was focused on the construction of grammars and analysis systems to detect and correct specific errors (see Heift and Schulze, 2005 for a detailed overview), more recent approaches have been based on data-driven methods. The majority of the data-driven methods use a classification technique to determine whether a word is used appropriately in its context, continuing the tradition established for contextual spelling correction by Golding (1995) and Golding and Roth (1996). The words investigated are typically articles and prepositions. They have two distinct advantages as the subject matter for investigation: They are a closed class and they comprise a substantial proportion of learners’ errors. The investigation of preposition corrections can even be narrowed further: amongst the more than 150 English prepositions, the usage of the ten most frequent prepositions accounts for 82% of preposition errors in the 20 million word Cambridge University Press Learners’ Corpus. Learning correct article use is most difficult for native speaker</context>
</contexts>
<marker>Golding, 1995</marker>
<rawString>Andrew Golding. 1995. A Bayesian Hybrid for Context Sensitive Spelling Correction. In Proceedings of the 3rd Workshop on Very Large Corpora (pp. 39–53). Cambridge, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew R Golding</author>
<author>Dan Roth</author>
</authors>
<title>Applying Winnow to context-sensitive spelling correction.</title>
<date>1996</date>
<booktitle>In Proceedings of the Int. Conference on Machine Learning (pp 182</booktitle>
<pages>190</pages>
<contexts>
<context position="2086" citStr="Golding and Roth (1996)" startWordPosition="315" endWordPosition="318">y 2:1 in some estimates, so any tool in this domain could be of tremendous value. While earlier work in both native and non-native error correction was focused on the construction of grammars and analysis systems to detect and correct specific errors (see Heift and Schulze, 2005 for a detailed overview), more recent approaches have been based on data-driven methods. The majority of the data-driven methods use a classification technique to determine whether a word is used appropriately in its context, continuing the tradition established for contextual spelling correction by Golding (1995) and Golding and Roth (1996). The words investigated are typically articles and prepositions. They have two distinct advantages as the subject matter for investigation: They are a closed class and they comprise a substantial proportion of learners’ errors. The investigation of preposition corrections can even be narrowed further: amongst the more than 150 English prepositions, the usage of the ten most frequent prepositions accounts for 82% of preposition errors in the 20 million word Cambridge University Press Learners’ Corpus. Learning correct article use is most difficult for native speakers of an L1 that does not ove</context>
</contexts>
<marker>Golding, Roth, 1996</marker>
<rawString>Andrew R. Golding and Dan Roth. 1996. Applying Winnow to context-sensitive spelling correction. In Proceedings of the Int. Conference on Machine Learning (pp 182 –190).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Na-Rae Han</author>
<author>Martin Chodorow</author>
<author>Claudia Leacock</author>
</authors>
<title>Detecting errors in English article usage with a maximum entropy classifier trained on a large, diverse corpus.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation.</booktitle>
<location>Lisbon.</location>
<contexts>
<context position="3345" citStr="Han et al., 2004" startWordPosition="512" endWordPosition="515">s English does. Prepositions, on the other hand, pose difficulties for language learners from all L1 backgrounds (Dalgish, 1995; Bitchener et al., 2005). Contextual classification methods represent the context of a preposition or article as a feature vector gleaned from a window of a few words around the preposition/article. Different systems typically vary along three dimensions: choice of features, choice of classifier, and choice of training data. Features range from words and morphological information (Knight and Chander, 1994) to the inclusion of part-of-speech tags (Minnen et al., 2000; Han et al., 2004, 2006; Chodorow et al., 2007; Gamon et al., 2008, 2009; Izumi et al., 2003, 2004; Tetrault and Chodorow, 2008) to features based on linguistic analysis and on WordNet (Lee, 2004; DeFelice and Pulman, 2007, 2008). Knight and Chander (1994) and Gamon et al. (2008) used decision tree classifiers but, in general, maximum entropy classifiers have become the classification 163 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 163–171, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics algorithm of choice. Traini</context>
</contexts>
<marker>Han, Chodorow, Leacock, 2004</marker>
<rawString>Na-Rae Han, Martin Chodorow, and Claudia Leacock. 2004. Detecting errors in English article usage with a maximum entropy classifier trained on a large, diverse corpus. In Proceedings of the 4th International Conference on Language Resources and Evaluation. Lisbon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Na-Rae Han</author>
<author>Martin Chodorow</author>
<author>Claudia Leacock</author>
</authors>
<title>Detecting errors in English article usage by non-native speakers.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>12</volume>
<issue>2</issue>
<pages>115--129</pages>
<marker>Han, Chodorow, Leacock, 2006</marker>
<rawString>Na-Rae Han, Martin Chodorow, and Claudia Leacock. 2006. Detecting errors in English article usage by non-native speakers. Natural Language Engineering, 12(2), 115-129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trude Heift</author>
<author>Mathias Schulze</author>
</authors>
<title>Errors and Intelligence in Computer-Assisted Language Learning: Parsers and Pedagogues.</title>
<date>2007</date>
<location>New York &amp; London: Routledge.</location>
<marker>Heift, Schulze, 2007</marker>
<rawString>Trude Heift and Mathias Schulze. 2007. Errors and Intelligence in Computer-Assisted Language Learning: Parsers and Pedagogues. New York &amp; London: Routledge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthieu Hermet</author>
<author>Alain Désilets</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Using the web as a linguistic resource to automatically correct lexico-yyntactic errors.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>874--878</pages>
<contexts>
<context position="5520" citStr="Hermet et al. (2008)" startWordPosition="858" endWordPosition="862">re statistics to identify unlikely function word and part-of-speech tag sequences, Turner and Charniak (2007) employ a language model based on a generative statistical parser, and Stehouwer and van Zaanen (2009) investigate a diverse set of language models with different backoff strategies to determine which choice, from a set of confusable words, is most likely in a given context. Gamon et al. (2008, 2009) use a combination of error-specific classifiers and a large generic language model with handtuned heuristics for combining their scores to maximize precision. Finally, Yi et al. (2008) and Hermet et al. (2008) use n-gram counts from the web as a language model approximation to identify likely errors and correction candidates. 2 Our Approach We combine evidence from the two kinds of datadriven models that have been used for error detection and correction (error-specific classifiers and a language model) through a meta-classifier. We use the term primary models for both the initial errorspecific classifiers and a large generic language model. The meta-classifier takes the output of the primary models (language model scores and class probabilities) as input. Using a meta-classifier for ensemble learni</context>
</contexts>
<marker>Hermet, Désilets, Szpakowicz, 2008</marker>
<rawString>Matthieu Hermet, Alain Désilets, and Stan Szpakowicz. 2008. Using the web as a linguistic resource to automatically correct lexico-yyntactic errors. In Proceedings of the 6th Conference on Language Resources and Evaluation (LREC), (pp. 874 - 878).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emi Izumi</author>
</authors>
<title>Kiyotaka Uchimoto, Toyomi Saiga, Thepchai Supnithi and Hitoshi Isahara.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics</booktitle>
<pages>145--148</pages>
<marker>Izumi, 2003</marker>
<rawString>Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thepchai Supnithi and Hitoshi Isahara. 2003. Automatic error detection in the Japanese learners&apos; English spoken data. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (pp. 145-148).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emi Izumi</author>
<author>Kiyotaka Uchimoto</author>
<author>Hitoshi Isahara</author>
</authors>
<title>SST speech corpus of Japanese learners&apos; English and automatic detection of learners&apos; errors.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC),</booktitle>
<volume>4</volume>
<pages>31--48</pages>
<marker>Izumi, Uchimoto, Isahara, 2004</marker>
<rawString>Emi Izumi, Kiyotaka Uchimoto and Hitoshi Isahara. 2004. SST speech corpus of Japanese learners&apos; English and automatic detection of learners&apos; errors. In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC), (Vol 4, pp. 31-48).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Ishwar Chander</author>
</authors>
<title>Automatic postediting of documents.</title>
<date>1994</date>
<booktitle>In Proceedings of the 12th National Conference on Artificial Intelligence</booktitle>
<pages>779--784</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>Seattle:</location>
<contexts>
<context position="3266" citStr="Knight and Chander, 1994" startWordPosition="497" endWordPosition="500">or native speakers of an L1 that does not overtly mark definiteness and indefiniteness as English does. Prepositions, on the other hand, pose difficulties for language learners from all L1 backgrounds (Dalgish, 1995; Bitchener et al., 2005). Contextual classification methods represent the context of a preposition or article as a feature vector gleaned from a window of a few words around the preposition/article. Different systems typically vary along three dimensions: choice of features, choice of classifier, and choice of training data. Features range from words and morphological information (Knight and Chander, 1994) to the inclusion of part-of-speech tags (Minnen et al., 2000; Han et al., 2004, 2006; Chodorow et al., 2007; Gamon et al., 2008, 2009; Izumi et al., 2003, 2004; Tetrault and Chodorow, 2008) to features based on linguistic analysis and on WordNet (Lee, 2004; DeFelice and Pulman, 2007, 2008). Knight and Chander (1994) and Gamon et al. (2008) used decision tree classifiers but, in general, maximum entropy classifiers have become the classification 163 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 163–171, Los Angeles, California, June 201</context>
</contexts>
<marker>Knight, Chander, 1994</marker>
<rawString>Kevin Knight and Ishwar Chander,. 1994. Automatic postediting of documents. In Proceedings of the 12th National Conference on Artificial Intelligence (pp. 779-784). Seattle: Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Michael Gamon</author>
<author>Chris Brockett</author>
</authors>
<title>User Input and Interactions on Microsoft ESL Assistant.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Innovative Use of NLP for Building Educational Applications</booktitle>
<pages>73--81</pages>
<contexts>
<context position="19359" citStr="Leacock et al., 2009" startWordPosition="3059" endWordPosition="3062">preposition/article. This evaluation scheme, however, ignores one aspect of a real user scenario. Of all the suggested changes that are counted as wrong in our evaluation because they do not match an annotated error, some may in fact be innocuous or even helpful for a real user. Such a situation can arise for a variety of reasons: In some cases, there are legitimate alternative ways to correct an error. In other cases, the classifier has identified the location of an error although that error is of a different kind (which can be beneficial because it causes the user to make a correction - see Leacock et al., 2009). Gamon et al. (2009), for example manually evaluate preposition suggestions as belonging to one of three categories: (a) properly correcting an existing error, (b) offering a suggestion that neither improves nor degrades the user sentence, (c) offering a suggestion that would degrade the user input. Obviously, (c) is a more serious error than (b). Similarly, Tetrault and Chodorow (2008) annotate their test set with preposition choices that are valid alternatives. We do not have similar information in the CLC data, but we can perform a manual analysis of a random subset of test data to estimat</context>
</contexts>
<marker>Leacock, Gamon, Brockett, 2009</marker>
<rawString>Claudia Leacock, Michael Gamon, and Chris Brockett. 2009. User Input and Interactions on Microsoft ESL Assistant. In Proceedings of the Fourth Workshop on Innovative Use of NLP for Building Educational Applications (pp. 73-81).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lee</author>
</authors>
<title>Automatic article restoration.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>31--36</pages>
<location>Boston.</location>
<contexts>
<context position="3523" citStr="Lee, 2004" startWordPosition="544" endWordPosition="545">ds represent the context of a preposition or article as a feature vector gleaned from a window of a few words around the preposition/article. Different systems typically vary along three dimensions: choice of features, choice of classifier, and choice of training data. Features range from words and morphological information (Knight and Chander, 1994) to the inclusion of part-of-speech tags (Minnen et al., 2000; Han et al., 2004, 2006; Chodorow et al., 2007; Gamon et al., 2008, 2009; Izumi et al., 2003, 2004; Tetrault and Chodorow, 2008) to features based on linguistic analysis and on WordNet (Lee, 2004; DeFelice and Pulman, 2007, 2008). Knight and Chander (1994) and Gamon et al. (2008) used decision tree classifiers but, in general, maximum entropy classifiers have become the classification 163 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 163–171, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics algorithm of choice. Training data are normally drawn from sizeable corpora of native English text (British National Corpus for DeFelice and Pulman (2007, 2008), Wall Street Journal in Knight and Chander (</context>
</contexts>
<marker>Lee, 2004</marker>
<rawString>John Lee. 2004. Automatic article restoration. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, (pp. 31-36). Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Minnen</author>
<author>Francis Bond</author>
<author>Anne Copestake</author>
</authors>
<title>Memory-based learning for article generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the Fourth Conference on Computational Natural Language Learning and of the Second Learning Language in Logic Workshop</booktitle>
<pages>43--48</pages>
<location>Lisbon.</location>
<contexts>
<context position="3327" citStr="Minnen et al., 2000" startWordPosition="508" endWordPosition="511"> and indefiniteness as English does. Prepositions, on the other hand, pose difficulties for language learners from all L1 backgrounds (Dalgish, 1995; Bitchener et al., 2005). Contextual classification methods represent the context of a preposition or article as a feature vector gleaned from a window of a few words around the preposition/article. Different systems typically vary along three dimensions: choice of features, choice of classifier, and choice of training data. Features range from words and morphological information (Knight and Chander, 1994) to the inclusion of part-of-speech tags (Minnen et al., 2000; Han et al., 2004, 2006; Chodorow et al., 2007; Gamon et al., 2008, 2009; Izumi et al., 2003, 2004; Tetrault and Chodorow, 2008) to features based on linguistic analysis and on WordNet (Lee, 2004; DeFelice and Pulman, 2007, 2008). Knight and Chander (1994) and Gamon et al. (2008) used decision tree classifiers but, in general, maximum entropy classifiers have become the classification 163 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 163–171, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics algorithm</context>
</contexts>
<marker>Minnen, Bond, Copestake, 2000</marker>
<rawString>Guido Minnen, Francis Bond, and Anne Copestake. 2000. Memory-based learning for article generation. In Proceedings of the Fourth Conference on Computational Natural Language Learning and of the Second Learning Language in Logic Workshop (pp. 43-48). Lisbon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Nguyen</author>
<author>Jianfeng Gao</author>
<author>Milind Mahajan</author>
</authors>
<title>MSRLM: A scalable language modeling toolkit.</title>
<date>2007</date>
<tech>Microsoft Technical Report 2007-144. Redmond.</tech>
<contexts>
<context position="10029" citStr="Nguyen, Gao, and Mahajan, 2007" startWordPosition="1553" endWordPosition="1557">formed data sources: There are about 2.5 million sentences, distributed roughly equally across Reuters newswire, Encarta encyclopedia, UN proceedings, Europarl and web-scraped general domain data1. From the total set of candidate operations (substitutions, insertions, and deletions) that each combination of presence and choice classifier produces for prepositions, we consider only the top three highest-scoring operations2. Our language model is trained on the Gigaword corpus (Linguistic Data Consortium, 2003) and utilizes 7-grams with absolute discount smoothing (Gao, Goodman, and Miao, 2001; Nguyen, Gao, and Mahajan, 2007). Each suggested revision from the preposition/article classifiers (top three for prepositions, all revisions from the article classifiers) are scored by the language model: for each revision, the language model score of the original and the suggested rewrite is recorded, as is the language model entropy (defined as the language model probability of the sentence, normalized by sentence length). 1 We are not able to train the error-specific classifiers on a larger data set like the one we use for the language model. Note that the 2.5 million sentences used in the classifier training already pr</context>
</contexts>
<marker>Nguyen, Gao, Mahajan, 2007</marker>
<rawString>Patrick Nguyen, Jianfeng Gao, and Milind Mahajan. 2007. MSRLM: A scalable language modeling toolkit. Microsoft Technical Report 2007-144. Redmond.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A simple introduction to maximum entropy models for natural language processing.</title>
<date>1997</date>
<tech>Technical Report IRCS Report 97-98,</tech>
<institution>Institute for Research in Cognitive Science, University of Pennsylvania.</institution>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Adwait Ratnaparkhi. 1997. A simple introduction to maximum entropy models for natural language processing. Technical Report IRCS Report 97-98, Institute for Research in Cognitive Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herman Stehouwer</author>
<author>Menno van Zaanen</author>
</authors>
<title>Language models for contextual error detection and correction.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference (</booktitle>
<pages>41--48</pages>
<location>Athens.</location>
<marker>Stehouwer, van Zaanen, 2009</marker>
<rawString>Herman Stehouwer and Menno van Zaanen. 2009. Language models for contextual error detection and correction. In Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference ( pp. 41-48). Athens.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>The ups and downs of preposition error detection in ESL.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING.</booktitle>
<publisher>Manchester, UK.</publisher>
<marker>Tetreault, Chodorow, 2008</marker>
<rawString>Joel Tetreault and Martin Chodorow. 2008a. The ups and downs of preposition error detection in ESL. In Proceedings of COLING. Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>Native judgments of non-native usage: Experiments in preposition error detection.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on Human Judgments in Computational Linguistics, 22nd International Conference on Computational Linguistics (pp</booktitle>
<pages>43--48</pages>
<location>Manchester, UK.</location>
<marker>Tetreault, Chodorow, 2008</marker>
<rawString>Joel Tetreault and Martin Chodorow. 2008b. Native judgments of non-native usage: Experiments in preposition error detection. In Proceedings of the Workshop on Human Judgments in Computational Linguistics, 22nd International Conference on Computational Linguistics (pp 43-48). Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenine Turner</author>
<author>Eugene Charniak</author>
</authors>
<title>Language modeling for determiner selection.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: NAACL; Companion Volume, Short Papers</booktitle>
<pages>177--180</pages>
<location>Rochester, NY.</location>
<contexts>
<context position="5009" citStr="Turner and Charniak (2007)" startWordPosition="773" endWordPosition="776">illion words from the MetaMetrics corpus, a diverse corpus of fiction, non-fiction and textbooks categorized by reading level. In addition to the classification approach to error detection, there is a line of research - going back to at least Atwell (1987) - that uses language models. The idea here is to detect errors in areas where the language model score is suspiciously low. Atwell (1987) uses a part-of-speech tag language model to detect errors, Chodorow and Leacock (2000) use mutual information and chi square statistics to identify unlikely function word and part-of-speech tag sequences, Turner and Charniak (2007) employ a language model based on a generative statistical parser, and Stehouwer and van Zaanen (2009) investigate a diverse set of language models with different backoff strategies to determine which choice, from a set of confusable words, is most likely in a given context. Gamon et al. (2008, 2009) use a combination of error-specific classifiers and a large generic language model with handtuned heuristics for combining their scores to maximize precision. Finally, Yi et al. (2008) and Hermet et al. (2008) use n-gram counts from the web as a language model approximation to identify likely erro</context>
</contexts>
<marker>Turner, Charniak, 2007</marker>
<rawString>Jenine Turner and Eugene Charniak. 2007. Language modeling for determiner selection. In Human Language Technologies 2007: NAACL; Companion Volume, Short Papers (pp. 177-180). Rochester, NY.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Wikipedia</author>
</authors>
<title>English Language.</title>
<note>http://en.wikipedia.org/wiki/English_language</note>
<marker>Wikipedia, </marker>
<rawString>Wikipedia. English Language. http://en.wikipedia.org/wiki/English_language</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xing Yi</author>
<author>Jianfeng Gao</author>
<author>Bill Dolan</author>
</authors>
<title>A webbased English proofing system for English as a second language users.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third International Joint Conference on Natural Language Processing (IJCNLP).</booktitle>
<location>Hyderabad, India.</location>
<contexts>
<context position="5495" citStr="Yi et al. (2008)" startWordPosition="853" endWordPosition="856">ormation and chi square statistics to identify unlikely function word and part-of-speech tag sequences, Turner and Charniak (2007) employ a language model based on a generative statistical parser, and Stehouwer and van Zaanen (2009) investigate a diverse set of language models with different backoff strategies to determine which choice, from a set of confusable words, is most likely in a given context. Gamon et al. (2008, 2009) use a combination of error-specific classifiers and a large generic language model with handtuned heuristics for combining their scores to maximize precision. Finally, Yi et al. (2008) and Hermet et al. (2008) use n-gram counts from the web as a language model approximation to identify likely errors and correction candidates. 2 Our Approach We combine evidence from the two kinds of datadriven models that have been used for error detection and correction (error-specific classifiers and a language model) through a meta-classifier. We use the term primary models for both the initial errorspecific classifiers and a large generic language model. The meta-classifier takes the output of the primary models (language model scores and class probabilities) as input. Using a meta-class</context>
</contexts>
<marker>Yi, Gao, Dolan, 2008</marker>
<rawString>Xing Yi, Jianfeng Gao, and Bill Dolan. 2008. A webbased English proofing system for English as a second language users. In Proceedings of the Third International Joint Conference on Natural Language Processing (IJCNLP). Hyderabad, India.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>