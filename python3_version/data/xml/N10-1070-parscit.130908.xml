<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000558">
<title confidence="0.994589">
Term Weighting Schemes for Latent Dirichlet Allocation
</title>
<author confidence="0.963239">
Andrew T. Wilson
</author>
<affiliation confidence="0.921907">
Sandia National Laboratories
</affiliation>
<address confidence="0.9653865">
PO Box 5800, MS 1323
Albuquerque, NM 87185-1323, USA
</address>
<email confidence="0.997169">
atwilso@sandia.gov
</email>
<note confidence="0.595024">
Peter A. Chew
</note>
<address confidence="0.799151333333333">
Moss Adams LLP
6100 Uptown Blvd. NE, Suite 400
Albuquerque, NM 87110-4489, USA
</address>
<email confidence="0.996505">
Peter.Chew@MossAdams.com
</email>
<sectionHeader confidence="0.994671" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.943890368421053">
Many implementations of Latent Dirichlet Al-
location (LDA), including those described in
Blei et al. (2003), rely at some point on the
removal of stopwords, words which are as-
sumed to contribute little to the meaning of
the text. This step is considered necessary be-
cause otherwise high-frequency words tend to
end up scattered across many of the latent top-
ics without much rhyme or reason. We show,
however, that the ‘problem’ of high-frequency
words can be dealt with more elegantly, and
in a way that to our knowledge has not been
considered in LDA, through the use of appro-
priate weighting schemes comparable to those
sometimes used in Latent Semantic Indexing
(LSI). Our proposed weighting methods not
only make theoretical sense, but can also be
shown to improve precision significantly on a
non-trivial cross-language retrieval task.
</bodyText>
<sectionHeader confidence="0.99905" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999633739130435">
Latent Dirichlet Allocation (LDA) (Blei et al., 2003),
like its more established competitors Latent Seman-
tic Indexing (LSI) (Deerwester et al., 1990) and
Probabilistic Latent Semantic Indexing (PLSI) (Hof-
mann, 1999), is a model which is applicable to the
analysis of text corpora. It is claimed to differ from
LSI in that LDA is a generative Bayesian model (Blei
et al., 2003), although this may depend upon the
manner in which one approaches LSI (see for exam-
ple Chew et al. (2010)). In LDA as applied to text
analysis, each document in the corpus is modeled as
a mixture over an underlying set of topics, and each
topic is modeled as a probability distribution over the
terms in the vocabulary.
As the newest among the above-mentioned tech-
niques, LDA is still in a relatively early stage of de-
velopment. It is also sufficiently different from LSI,
probably the most popular and well-known compres-
sion technique for information retrieval (IR), that
many practitioners of LSI may perceive a ‘barrier to
entry’ to LDA. This in turn perhaps explains why no-
tions such as term weighting, which have been com-
monplace in LSI for some time (Dumais, 1991), have
not yet found a place in LDA. In fact, it is often as-
sumed that weighting is unnecessary in LDA. For
example, Blei et al. (2003) contrast the use of tf-
idf weighting in both non-reduced space (Salton and
McGill, 1983) and LSI on the one hand with PLSI
and LDA on the other, where no mention is made of
weighting. Ramage et al. (2008) propose a simple
term-frequency weighting scheme for tagged docu-
ments within the framework of LDA, although term
weighting is not their focus and their scheme is in-
tended to incorporate document tags into the same
model that represents the documents themselves.
In this paper, we produce evidence that term
weighting should be given consideration within
LDA. First and foremost, this is shown empiri-
cally through a non-trivial multilingual retrieval task
which has previously been used as the basis for
tests of variants of LSI. We also show that term
weighting allows one to avoid maintenance of stop-
lists, which can be awkward especially for multilin-
gual data. With appropriate term weighting, high-
frequency words (which might otherwise be elimi-
nated as stopwords) are assigned naturally to topics
</bodyText>
<page confidence="0.994193">
465
</page>
<note confidence="0.7526715">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 465–473,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.995924928571429">
by LDA, rather than dominating and being scattered
across many topics as happens with the standard uni-
form weighting. Our approach belies the usually
unstated, but widespread, assumption in papers on
LDA that the removal of stopwords is a necessary
pre-processing step (see e.g. Blei et al. (2003); Grif-
fiths and Steyvers (2004)).
It might seem that to demonstrate this it would be
necessary to perform a test that directly compares the
results when stoplists are used to those when weight-
ing are used. However, we believe that stopwords
are highly ad-hoc to begin with. Assuming a vocab-
ulary of n words and a stoplist of x items, there are
(at least in theory) (n) possible stoplists. To be sure
</bodyText>
<equation confidence="0.557434">
x
</equation>
<bodyText confidence="0.999932173913044">
that no stoplist improves on a particular term weight-
ing scheme we would have to test every one of these.
In addition, our tests are with a multilingual dataset,
which raises the issue that a domain-appropriate sto-
plist for a particular corpus and language may not be
available. This is even more true if we pre-process
the dataset morphologically (for example, with stem-
ming). Therefore, rather than attempting a direct
comparison of this type, we take the position that it
is possible to sidestep the need for stoplists and to do
so in a non-ad-hoc way.
The paper is organized as follows. Section 2 de-
scribes the general framework of LDA, which has
only very recently been applied to cross-language
IR. In Section 3, we look at alternatives to the
‘standard’ uniform weighting scheme (i.e., lack of
weighting scheme) commonly used in LDA. Sec-
tion 4 discusses the framework we use for empiri-
cal testing of our hypothesis that a weighting scheme
would be beneficial. We present the results of this
comparison in Section 5 along with an impressionis-
tic comparison of the output of the different alterna-
tives. We conclude in Section 6.
</bodyText>
<sectionHeader confidence="0.959678" genericHeader="method">
2 Latent Dirichlet Allocation
</sectionHeader>
<bodyText confidence="0.9999195">
Our IR framework is multilingual Latent Dirich-
let Allocation (LDA), first proposed by Blei et al.
(2003) as a general Bayesian framework with initial
application to topic modeling. It is only very recently
that variants of LDA have been applied to cross-
language IR: examples are Cimiano et al. (2009) and
Ni et al. (2009).
As an approach to topic modeling, LDA relies on
the idea that the tokens in a document are drawn in-
dependently from a set of topics where each topic is
a distribution over types (words) in the vocabulary.
The mixing coefficients for topics within each docu-
ment and weights for types in each topic can be spec-
ified a priori or learned from a training corpus. Blei
et al. initially proposed a variational model (2003)
for learning topics from data. Griffiths and Steyvers
(2004) later developed a Markov chain Monte Carlo
approach based on collapsed Gibbs sampling.
In this model, the mixing weights for topics within
each document and the multinomial coefficients for
terms within each topic are hidden (latent) and must
be learned from a training corpus. Blei et al. (2003)
proposed LDA as a general Bayesian framework and
gave a variational model for learning topics from
data. Griffiths and Steyvers (2004) subsequently de-
veloped a stochastic learning algorithm based on col-
lapsed Gibbs sampling. In this paper we will focus
on the Gibbs sampling approach.
</bodyText>
<subsectionHeader confidence="0.548331">
2.1 Generative Document Model
</subsectionHeader>
<bodyText confidence="0.999907391304348">
The LDA algorithm models the D documents in a
corpus as mixtures of K topics where each topic is
in turn a distribution over W terms. Given 0, the
matrix of mixing weights for topics within each doc-
ument, and 0, the matrix of multinomial coefficients
for each topic, we can use this formulation to de-
scribe a generative model for documents (Alg. 1).
Restating the LDA model in linear-algebraic
terms, we can say that the product of 0 (the K × W
column-stochastic topic-by-type matrix) and 0 (the
D × K column-stochastic topic-by-document ma-
trix) is the original D×W term-by-document matrix.
In this sense, LDA computes a matrix factorization
of the term-by-document matrix in the same way that
LSI or non-negative matrix factorization (NMF) do.
In fact, LDA is a special case of NMF, but unlike in
NMF, there is a unique factorization in LDA. We see
this as a feature recommending LDA above NMF.
Our objective is to reverse the generative model to
learn the contents of 0 and 0 given a training corpus
D, a number of topics K, and symmetric Dirichlet
prior distributions over both 0 and 0 with hyperpa-
rameters α and β, respectively.
</bodyText>
<page confidence="0.983106">
466
</page>
<equation confidence="0.918894222222222">
fork=1toKdo
Draw ϕk — Dirichlet(β)
end for
ford= 1toDdo
Draw θ — Dirichlet(α)
Draw N — Poisson(ξ)
for i = 1 to N do
Draw z — Multinomial(θ)
Draw w — Multinomial(ϕ(z))
</equation>
<tableCaption confidence="0.426149444444444">
end for
end for
Algorithm 1: Generative algorithm for LDA. This will
generate D documents with N tokens each. Each token
is drawn from one of K topics. The distributions over
topics and terms have Dirichlet hyperparameters α and
β respectively. The Poisson distribution over the token
count may be replaced with any other convenient distri-
bution.
</tableCaption>
<subsectionHeader confidence="0.995846">
2.2 Learning Topics via Collapsed Gibbs
Sampling
</subsectionHeader>
<bodyText confidence="0.995095157894737">
Rather than learn 0 and 0 directly, we use collapsed
Gibbs sampling (Geman et al. (1993), Chatterji and
Pachter (2004)) to learn the latent assignment of to-
kens to topics z given the observed tokens x.
The algorithm operates by repeatedly sampling
each zij from a distribution conditioned on the val-
ues of all other elements of z. This requires main-
taining counts of tokens assigned to topics globally
and within each document. We use the following no-
tation for these sums:
Nijk: Number of tokens of type wi in document dj
assigned to topic k
N−st
ijk : The sum Nijk with the contribution of token
xst excluded
We indicate summation over all values of an index
with (�).
Given the current state of z the conditional proba-
bility of zij is:
</bodyText>
<equation confidence="0.998358333333333">
p(zij = k|z−ij,x,d,α,β) =
p(xij|ϕk) p(k|dj) a
N−ij
(·)jk + α
N−ij
(·)(·)k + Wβ
</equation>
<bodyText confidence="0.999994272727273">
As Griffiths and Steyvers (2004) point out, this is
an intuitive result. The first term, p(xij|ϕk), indi-
cates the importance of term xij in topic k. The sec-
ond term, p(k|dj), indicates the importance of topic
k in document j. The sum of the terms is normalized
implicitly to 1 when we draw each new zij.
We sample a new value for zij for every token xij
during each iteration of Gibbs sampling. We run the
sampler for a burn-in period of a few hundred itera-
tions to allow it to reach its converged state and then
estimate 0 and 0 from z as follows:
</bodyText>
<equation confidence="0.980246">
(2)
N(·)j(·) + Tα
Ni(·)k + β (3)
</equation>
<subsectionHeader confidence="0.995353">
2.3 Classifying New Documents
</subsectionHeader>
<bodyText confidence="0.999962448275862">
In LSI, new documents not in the original training
set can be ‘projected’ into the semantic space of the
training set. The equivalent process in LDA is one
of classification: given a corpus D′ of one or more
new documents we use the existing topics 0 to com-
pute a maximum a posteriori estimate of the mixing
coefficients 0′. This follows the same Monte Carlo
process of repeatedly resampling a set of token-to-
topic assignments z′ for the tokens x′ in the new doc-
uments. These new tokens are used to compute the
first term p(k|dj) in Eq. 1. We re-use the topic as-
signments z from the training corpus to compute the
second term p(xij|ϕk). Tokens with new types that
were not present in the vocabulary of the training
corpus do not participate in classification.
The resulting distribution 0′ essentially encodes
how likely each new document is to relate to each of
the K topics. We can use this matrix to compute
pairwise similarities between any two documents
from either corpus (training or newly-classified).
Whereas in LSI it may make sense to compute sim-
ilarity between documents using the cosine met-
ric (since the ‘dimensions’ defining the space are
orthogonal), we compute similarities in LDA us-
ing either the symmetrized Kullback-Leibler (KL)
or Jensen-Shannon (JS) divergences (Kullback and
Leibler (1951), Lin (2002)) since these are methods
of measuring the similarity between probability dis-
tributions.
</bodyText>
<equation confidence="0.978299222222222">
N−ij
i(·)k +
β
(1)
N(·)j(·) + Tα
θjk =
ϕki =
N(·)(·)k + Wβ
N(·)jk + α
</equation>
<page confidence="0.997974">
467
</page>
<sectionHeader confidence="0.967749" genericHeader="method">
3 Term Weighting Schemes and LDA
</sectionHeader>
<bodyText confidence="0.999875181818182">
The standard approach presented above assumes, ef-
fectively, that each token is equally important in cal-
culating the conditional probabilities. From both an
information-theoretic and a linguistic point of view,
however, it is clear that this is not the case. In En-
glish, a term such as ‘the’ which occurs with high
frequency in many documents does not contribute as
much to the meaning of each document as a lower-
frequency term such as ‘corpus’. It is an axiom of
information theory that an event a’s information con-
tent (in bits) is equal to log2 p(a) = − log2 p(a).
</bodyText>
<equation confidence="0.645088">
1
</equation>
<bodyText confidence="0.999941769230769">
Treating tokens as events, we can say that the in-
formation content of a particular token of type t is
− log2 p(t). Furthermore, as is well-known, we can
estimate p(t) from observed frequencies in a corpus:
it is simply the number of tokens of type t in the cor-
pus, divided by the total number of tokens in the cor-
pus. For high-probability terms such as ‘the’, there-
fore, − log2 p(t) is low. Our basic hypothesis is that
recalculating p(zij|z, x, α, β) to take the information
content of each token into account will improve the
results of LDA. Specifically, we have incorporated
a weighting term into Eq. 1 by replacing the counts
denoted N with weights denoted M.
</bodyText>
<equation confidence="0.9982774">
p(zij = k|z−ij, x, d, α, β) a
M−ij
i(·)k + β
M−ij
(·)(·)k + Wβ
</equation>
<bodyText confidence="0.982091611111111">
Here Mijk is the total weight of tokens of type i
in document j assigned to topic k instead of the total
number of tokens. All of the machinery for Gibbs
sampling and the estimation of θ and ϕ from z re-
mains unchanged.
We appeal to an urn model to explain the intuition
behind this approach. In the original LDA formula-
tion, each topic ϕ can be modeled as an urn contain-
ing a large number of balls of uniform size. Each
ball assumes one of W different colors (one color for
each term in the vocabulary). The frequency of oc-
currence of each color in the urn is proportional to the
corresponding term’s weight in topic ϕ. We incor-
porate a term weighting scheme by making the size
of each ball proportional to the weight of its corre-
sponding term. This makes the probability of draw-
ing the ball for a term w proportional to both the term
weight m(w) and its multinomial weight ϕw:
</bodyText>
<equation confidence="0.9969345">
p(w|ϕ,β, m) =rϕw m(w) (5)
E-∈W m(w)
</equation>
<bodyText confidence="0.9967305">
We can now expand Eq. 4 to obtain a new sampling
equation for use with the Gibbs sampler.
</bodyText>
<equation confidence="0.99900675">
p(zij = k|z−ij, x, d, m, α, β) =
Ew m(w)N−ij wjk+ α
E wm(w)N−ij Ew m(w)Nwj(·) + Tα
w(·)k + Wβ (6)
</equation>
<bodyText confidence="0.999819678571429">
If all weights m(w) = 1 this reduces immediately
to the standard LDA formulation in Eq. 1.
The information measure we describe above is
constant for a particular term across the entire cor-
pus, but it is possible to conceive of other, more so-
phisticated weighting schemes as well, for example
those where term weights vary by document. Point-
wise mutual information (PMI) is one such weight-
ing scheme which has a solid basis in information
theory and has been shown to work well in the con-
text of LSI (Chew et al., 2010). According to PMI,
the weight of a given term w in a given document
d is the pointwise mutual information of the term
and document, or − log2 p(w|d) p(w). Extending the LDA
model to accommodate PMI is straightforward. We
replace m(xi) and m(w) in Eq. 4 with m(xi, d) as
follows.
It is possible for PMI of a term within a document
to be negative. When this happens, we clamp the
weight of the offending term to zero in that docu-
ment. In practice, we observe this only with com-
mon words (e.g. ‘and’, ‘in’, ‘of’, ‘that’, ‘the’ and
‘to’ in English) that are assigned very low weight ev-
erywhere else in the corpus. This clamping does not
noticeably affect the results.
In the next sections, we describe tests which have
enabled us to evaluate empirically which of these
formulations works best in practice.
</bodyText>
<equation confidence="0.9973405">
ij + α
M(·)jk
(4)
M(·)j(·) + Tα
m(xi)N−ij
i(·)k + β
p(xi|d)
m(xi, d) = − log2
= −log2 #[tokens of type xi]
p(xi) (7)
</equation>
<bodyText confidence="0.501402">
#[tokens of type xi in d]
</bodyText>
<page confidence="0.996547">
468
</page>
<sectionHeader confidence="0.99548" genericHeader="method">
4 Testing Framework
</sectionHeader>
<bodyText confidence="0.99999264893617">
In this paper, we chose to test our hypotheses with
the same cross-language retrieval task used in a num-
ber of previous studies of LSI (e.g. Chew and Abde-
lali (2007)). Briefly, the task is to train an IR model
on one particular multilingual corpus, then deploy
it on a separate multilingual corpus, using a docu-
ment in one language to retrieve related documents
in other languages. This task is difficult because of
the size of the datasets involved. Its usefulness be-
comes apparent when we consider the following two
use cases: a human wishing (1) to use a search engine
to retrieve relevant documents in many languages re-
gardless of the language in which the query is posed;
or (2) to produce a clustering or visualization of doc-
uments according to their topics even when the doc-
uments are in different languages.
The training corpus consists of the text ofthe Bible
in 31,226 parallel chunks, corresponding generally
to verses, in Arabic, English, French, Russian and
Spanish. These data were obtained from the Un-
bound Bible project (Biola University (2006)). The
test data, obtained from http://www.kuran.gen.
tr/, is the text of the Quran in the same 5 languages,
in 114 parallel chunks corresponding to suras (chap-
ters). The task, in short, is to use the training data
to inform whatever linguistic, semantic, or statistical
model is being tested, and then to infer characteris-
tics of the test data in such a way that the test docu-
ments can automatically be matched with their trans-
lations in other languages. Though the documents
come from a specific domain (scriptural texts), what
is of interest is comparative results using different
weighting schemes, holding the datasets and other
settings constant. The training and test datasets are
large enough to allow statistically significant obser-
vations to be made, and if a significant difference is
observed between experiments using two settings, it
is to be expected that similar basic differences would
be observed with any other set of training and test
data. In any case, it should be noted that the Bible
and Quran were written centuries apart, and in differ-
ent original languages; we believe this contributes
to a clean separation of training and test data, and
makes for a non-trivial retrieval task.
In our framework, a term-by-document matrix is
formed from the Bible as a parallel verse-aligned
corpus. We employed two different approaches
to tokenization, one (word-based tokenization) in
which text was tokenized at every non-word char-
acter, and the other (unsupervised morpheme-based
tokenization) in which after word-based tokeniza-
tion, a further pre-processing step (based on Gold-
smith (2001)) was performed to add extra breaks at
every morpheme. It is shown elsewhere (Chew et al.,
2010) that this step leads to improved performance
with LSI. In each verse, all languages are concate-
nated together, allowing terms (either morphemes or
words) from all languages to be represented in every
verse. Cross-language homographs such as ‘mien’
in English and French are treated as distinct terms
in our framework. Thus, if there are L languages,
D documents (each of which is translated into each
of the L languages), and W distinct linguistic terms
across all languages, then the term-by-document ma-
trix is of dimensions W by D (not W by DxL); with
the Bible as a training corpus, the actual numbers in
our case are 160,345 x 31,226. As described in Sec.
2.2, we use this matrix as the input to a collapsed
Gibbs sampling algorithm to learn the latent assign-
ment of tokens in all five languages to language-
independent topics, as well as the latent assignment
of language-independent topics to the multilingual
(parallel) documents. In general, we specified, arbi-
trarily but consistently across all tests, that the num-
ber of topics to be learned should be 200. Other pa-
rameters for the Gibbs sampler held constant were
the number of iterations for burn-in (200) and the
number of iterations for sampling (1).
To evaluate our different approaches to weighting,
we use classification as described in Sec. 2.3 to ob-
tain, for each document from the Quran test corpus,
a probability distribution across the topics learned
from the Bible. While in training we have D multi-
lingual documents, in testing we have D′ x L docu-
ments, each in a specific language, for which a distri-
bution is computed. For the Quran data, this amounts
to 114 x 5 = 570 documents. This is because our
goal is to match documents with their translations
in other languages using just the probability distri-
butions. For each source-language/target-language
pair L1 and L2, we obtain the similarity of each of
the 114 documents in L1 to each of the 114 doc-
uments in L2. We found that similarity here is
best computed using the Jensen-Shannon divergence
</bodyText>
<page confidence="0.999377">
469
</page>
<table confidence="0.7665142">
Tokenization
Weighting Scheme Word Morpheme
Unweighted 0.505 0.544
logp(w|L) 0.616 0.641
PMI 0.612 0.686
</table>
<tableCaption confidence="0.98917875">
Table 1: Summary of comparison results. This table
shows the average precision at one document (P1) for
each of the tokenization and weighting schemes we eval-
uated. Detailed results are presented in Table 2.
</tableCaption>
<bodyText confidence="0.999745214285714">
(Lin, 2002) and so this measure was used in all
tests. Ultimately, the measure of how well a partic-
ular method performs is average precision at 1 doc-
ument (P1). Among the various measurements for
evaluating the performance of IR systems (Salton
and McGill (1983), van Rijsbergen (1979)), this is
a fairly standard measure. For a particular source-
target pair, this is the percentage (out of 114 cases)
where a document in L1 is most similar to its mate
in L2. With 5 languages, there are 25 source-target
pairs, and we can also calculate average P1 across
all language pairs. Here, we average across 114 x
25 (or 2,850) cases. This is why even small differ-
ences in P1 can be statistically significant.
</bodyText>
<sectionHeader confidence="0.999836" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.99957879710145">
First, we present a summary of our results in Table 1
which clearly demonstrates that it is better in LDA to
use some kind of weighting scheme rather than the
uniform weights in the standard LDA formulation
from Eq. 1. This is true whether tokenization is by
word or by morpheme. All increases from the base-
line precision at 1 document (0.505 and 0.544 re-
spectively), whether under log or PMI weighting, are
highly significant (p &lt; 10−11). Furthermore, all in-
creases in precision when moving from word-based
to morphology-based tokenization are also highly
significant (p &lt; 5 x 10−5 without weighting, p &lt;
5x10−3 with log-weighting, and p &lt; 2x10−15 with
PMI weighting). The best result overall, where P1 is
0.686, is obtained with morphological tokenization
and PMI weighting (parallel to the results in (Chew
et al., 2010) with LSI), and again the difference be-
tween this result and its nearest competitor of 0.641
is highly significant (p &lt; 3 x 10−6). We return to
comment below on lack of an increase in P1 when
moving from log-weighting to PMI-weighting under
word-based tokenization.
These results can also be broken out by language
pair, as shown in Table 2. Here, it is apparent that
Arabic, and to a lesser extent Russian, are harder lan-
guages in the IR problem at hand. Our intuition is
that this is connected with the fact that these two lan-
guages have a more complex morphological struc-
ture: words are formed by a process of agglutination.
A consequence of this is that single Arabic and Rus-
sian tokens can less frequently be mapped to single
tokens in other languages, which appears to “con-
fuse” LDA (and also, as we have found, LSI). The
complex morphology of Russian and Arabic is also
reflected in the type-token ratios for each language:
in our English Bible, there are 12,335 types (unique
words) and 789,744 tokens, a type-token ratio of
0.0156. The ratios for French, Spanish, Russian and
Arabic are 0.0251, 0.0404, 0.0843 and 0.1256 re-
spectively. Though the differences may not be ex-
plicable in purely statistical terms (there may be lin-
guistic factors at play which cannot be reduced to
statistics), it seems plausible that choosing a subop-
timal term-weighting scheme could exacerbate any
intrinsic problems of statistical imbalance. Consid-
ering this, it is interesting to note that the greatest
gains, when moving from unweighted LDA to ei-
ther form of weighted LDA, are often to be found
where Russian and/or Arabic are involved. This, to
us, shows the value of using a multilingual dataset
as a testbed for our different formulations of LDA:
it allows problems which may not be apparent when
working with a monolingual dataset to come more
easily to light.
We have mentioned that the best results are with
PMI and morphological tokenization, and also that
there is an increase in precision for many language of
the pairs when morphological (as opposed to word-
based) tokenization is employed. To us, the results
leave little doubt that both weighting and morpho-
logical tokenization are independently beneficial. It
appears, though, that morphology and weighting are
also complementary and synergistic strategies for
improving the results of LDA: for example, a subop-
timal approach in tokenization may at best place an
upper bound on the overall precision achievable, and
perhaps at worst undo the benefits of a good weight-
ing scheme. This may explain the one apparently
anomalous result, which is the lack of an increase in
</bodyText>
<page confidence="0.993526">
470
</page>
<table confidence="0.99892925">
Original Words Morphological Tokenization
EN ES RU AR FR EN ES RU AR FR
EN 1.000 0.500 0.447 0.132 0.816 1.000 0.500 0.658 0.211 0.640 EN
ES 0.649 1.000 0.307 0.175 0.781 0.605 1.000 0.482 0.175 0.737 ES
RU 0.430 0.316 1.000 0.149 0.430 0.553 0.421 1.000 0.272 0.553 RU
AR 0.070 0.149 0.114 1.000 0.096 0.123 0.105 0.228 1.000 0.114 AR
FR 0.781 0.693 0.421 0.175 1.000 0.693 0.640 0.667 0.211 1.000 FR
EN 1.000 0.518 0.518 0.228 0.658 1.000 0.675 0.561 0.219 0.754 EN
ES 0.558 1.000 0.605 0.254 0.763 0.711 1.000 0.570 0.289 0.860 ES
RU 0.605 0.615 1.000 0.298 0.702 0.684 0.667 1.000 0.289 0.728 RU
AR 0.404 0.430 0.526 1.000 0.439 0.430 0.439 0.535 1.000 0.404 AR
FR 0.667 0.667 0.658 0.281 1.000 0.711 0.667 0.561 0.289 1.000 FR
EN 1.000 0.579 0.658 0.272 0.702 1.000 0.719 0.658 0.342 0.851 EN
ES 0.596 1.000 0.623 0.246 0.693 0.816 1.000 0.675 0.272 0.798 ES
RU 0.649 0.579 1.000 0.307 0.693 0.702 0.693 1.000 0.360 0.772 RU
AR 0.351 0.368 0.421 1.000 0.351 0.456 0.474 0.509 1.000 0.377 AR
FR 0.693 0.667 0.605 0.254 1.000 0.825 0.772 0.719 0.333 1.000 FR
LDA
Log-WLDA
PMI-WLDA
</table>
<tableCaption confidence="0.9904785">
Table 2: Full results for precision at one document for all combinations of LDA, Log-WLDA, PMI-WLDA, word
tokenization and morphological tokenization.
</tableCaption>
<bodyText confidence="0.999754913043478">
precision moving from log-WLDA to PMI-WLDA
under word-based tokenization: if word-based tok-
enization is suboptimal, PMI weighting cannot com-
pensate for that. Effectively, for best results, the
right strategies have to be pursued with respect both
to morphology and to weighting.
Finally, we can illustrate the differences between
weighted and unweighted LDA in another way. As
discussed earlier, each topic in LDA is a probabil-
ity distribution over terms. For each topic, we can
list the most probable terms in decreasing order of
probability; this gives a sense of what each topic
is ‘about’ and whether the groupings of terms ap-
pear reasonable. Since we use 200 topics, an ex-
haustive listing is impractical here, but in Table 3
we present some representative examples from un-
weighted LDA and PMI-WLDA that we judged to
be of interest. It appears to us that the groupings are
not perfect under either LDA or PMI-WLDA; under
both methods, we find examples of rather heteroge-
neous topics, whereas we would like each topic to be
semantically focused. Still, a comparison of the out-
put with LDA and PMI-WLDA sheds some light on
why PMI-WLDA makes it less necessary to remove
stopwords. Note that all words listed for the top two
topics under LDA would commonly be considered
stopwords. This might also be true of the words in
topic 1 for PMI-WLDA, but in the latter case, the
topic is actually one of the most semantically focused
in that the top words have a clear semantic connec-
tion to one another. This cannot be said of topics 1
and 2 in LDA. For one thing, many of the same terms
that appear in topic 1 reappear in topic 2, making the
two topics hard to distinguish from one another. Sec-
ondly, the terms have only a loose semantic connec-
tion to one another: ‘the’, ‘and’, and ‘of’ are all high-
frequency and likely to co-occur, but they are differ-
ent parts of speech and have very different functions
in English. One might say that topics 1 and 2 in LDA
are a rag-bag of high-frequency words, and it is un-
surprising that these topics do little to help charac-
terize documents in our cross-language IR task. The
same cannot be said of any of the top 5 topics in PMI-
WLDA. We believe this illustrates well, and at a fun-
damental level, why weighted forms of LDA work
better in practice than unweighted LDA.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999754166666667">
We have conducted a series of experiments to evalu-
ate the effect of different weighting schemes on La-
tent Dirichlet Allocation. Our results demonstrate,
perhaps contrary to the conventional wisdom that
weighting is unnecessary in LDA, that weighting
schemes (and other pre-processing strategies) simi-
</bodyText>
<page confidence="0.981312">
471
</page>
<bodyText confidence="0.58206875">
Topic 1 2 LDA (no weighting) Weighting Scheme 2 PMI-WLDA 4 5
3 4 5 1 3
the the vanité as cárcel under city coeur sat colère
et de vanidad comme prison sous ville heart assis ira
and et vanity como نجسلا под ciudad corazón vent wrath
los of لطاب как prison تحت ةنيدمل сeрдцe wind anger
Terms и and суeта un тeмницу debajo город сeрдца viento furor
y y aflicción a prisonniers ombre twelve هبلق sentado гнeв
les de poursuite one тeмницы bases douze بلق вeтeр fureur
á и لطابلا امك bound basas doce يبلق حيرلابضغ
de la prédicateur une prisión sombra ةنيد كبلق sitting гнeва
of la ضبقو دحاو prisoners dessous города сeрдцeм сeл contre
</bodyText>
<tableCaption confidence="0.997903">
Table 3: Top 10 terms within top 5 topics for each of LDA and PMI-WLDA. Terms that appear twice within the same
topic (e.g. ‘la’ in LDA topic 2) are words from different languages with the same spelling (here Spanish and French).
</tableCaption>
<bodyText confidence="0.999978379310345">
lar to those commonly employed in other approaches
to IR (such as LSI) can significantly improve the
performance of a system. Our approach also runs
counter to the standard position in LDA that it is
necessary or desirable to remove stopwords as a pre-
processing step, and we have presented an alterna-
tive approach of applying an appropriate weighting
scheme within LDA. This approach is preferable be-
cause it is considerably less ad-hoc than the construc-
tion of stoplists. We have shown mathematically
how alternative weighting schemes can be incorpo-
rated into the Gibbs sampling model. We have also
demonstrated that, far from being arbitrary, the in-
troduction of weighting into the LDA model has a
solid and rational basis in information and probabil-
ity theory, just as the basic LDA model itself has.
In future work, we would like to explore further
enhancements to weighting in LDA. There are many
variants which can be considered: one example is
the incorporation of word order and context through
an n-gram model based on conditional probabilities.
We also aim to evaluate LDA against LSI with a view
to establishing whether one can be said to outperform
the other consistently in terms of precision, with ap-
propriate settings held constant. Finally, we would
like to determine whether other techniques which
have been shown to benefit LSI can also be usefully
brought to bear in LDA, just as we have shown here
in the case of term weighting.
</bodyText>
<sectionHeader confidence="0.999439" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998826">
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal ofMachine
Learning Research 3, pages 993–1022.
Sourav Chatterji and Lior Pachter. 2004. Multiple Or-
ganism Gene Finding by Collapsed Gibbs Sampling.
In RECOMB ’04: Proceedings of the eighth annual in-
ternational conference on Research in computational
molecular biology, pages 187–193, New York, NY,
USA. ACM.
Peter A. Chew and Ahmed Abdelali. 2007. Bene-
fits of the ‘Massively Parallel Rosetta Stone’: Cross-
Language Information Retrieval with Over 30 Lan-
guages. In Association for Computational Linguistics,
editor, Proceedings of the 45th meeting of the Associ-
ation of Computational Linguistics, pages 872–879.
Peter A. Chew, Brett W. Bader, Stephen Helmreich,
Ahmed Abdelali, and Stephen J. Verzi. 2010.
An Information-Theoretic, Vector-Space-Model Ap-
proach to Cross-Language Information Retrieval.
Journal ofNatural Language Engineering. Forthcom-
ing.
Philipp Cimiano, Antje Schultz, Sergej Sizov, Philipp
Sorg, and Steffen Staab. 2009. Explicit Versus
Latent Concept Models for Cross-Language Informa-
tion Retrieval. In Proceedings of the 21st Inter-
national Joint Conference on Artificial Intelligence,
pages 1513–1518.
Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by Latent Semantic Analysis. Jour-
nal of the American Society of Information Science,
41(6):391–407.
Susan T. Dumais. 1991. Improving the Retrieval of In-
formation from External Sources. Behavior Research
Methods, Instruments and Computers, 23(2):229–236.
</reference>
<page confidence="0.98081">
472
</page>
<reference confidence="0.999839485714286">
Stuart Geman, Donald Geman, K. Abend, T. J. Harley,
and L. N. Kanal. 1993. Stochastic Relaxation, Gibbs
Distributions and the Bayesian Restoration of Images*.
Journal ofApplied Statistics, 20(5):25–62.
J. Goldsmith. 2001. Unsupervised Learning of the Mor-
phology of a Natural Language. Computational Lin-
guistics, 27(2):153–198.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing Scientific Topics. In Proceedings of the Na-
tional Academy of Sciences USA, volume 101, pages
5228–5235.
Thomas Hofmann. 1999. Probablistic Latent Semantic
Indexing. In Proceedings of the 22nd Annual Interna-
tional SIGIR Conference, pages 53–57.
Solomon Kullback and Richard A. Leibler. 1951. On
Information and Sufficiency. Annals of Mathematical
Statistics, 22:49–86.
J. Lin. 2002. Divergence Measures based on the Shannon
Entropy. IEEE Transactions on Information Theory,
37(1):145–151, August.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.
2009. Mining Multilingual Topics from Wikipedia. In
18th International World Wide Web Conference, pages
1155–1155, April.
Daniel Ramage, Paul Heymann, Christopher D. Man-
ning, and Hector Garcia-Molina. 2008. Clustering the
Tagged Web. In Second ACM International Confer-
ence on Web Search and Data Mining (WSDM 2009),
November.
G. Salton and M. McGill, editors. 1983. Introduction to
Modern Information Retrieval. McGraw-Hill.
Biola University. 2006. The Unbound Bible.
http://www.unboundbible.com.
C.J. van Rijsbergen. 1979. Information Retrieval.
Butterworth-Heinemann.
</reference>
<page confidence="0.99936">
473
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.222914">
<title confidence="0.999547">Term Weighting Schemes for Latent Dirichlet Allocation</title>
<author confidence="0.998745">T Andrew</author>
<affiliation confidence="0.952007">Sandia National</affiliation>
<address confidence="0.883746">PO Box 5800, MS Albuquerque, NM 87185-1323, USA</address>
<email confidence="0.991652">atwilso@sandia.gov</email>
<affiliation confidence="0.305433">Peter A.</affiliation>
<address confidence="0.938723666666667">Moss Adams 6100 Uptown Blvd. NE, Suite Albuquerque, NM 87110-4489, USA</address>
<email confidence="0.998694">Peter.Chew@MossAdams.com</email>
<abstract confidence="0.9991702">Many implementations of Latent Dirichlet Allocation (LDA), including those described in Blei et al. (2003), rely at some point on the removal of stopwords, words which are assumed to contribute little to the meaning of the text. This step is considered necessary because otherwise high-frequency words tend to end up scattered across many of the latent topics without much rhyme or reason. We show, however, that the ‘problem’ of high-frequency words can be dealt with more elegantly, and in a way that to our knowledge has not been considered in LDA, through the use of appropriate weighting schemes comparable to those sometimes used in Latent Semantic Indexing (LSI). Our proposed weighting methods not only make theoretical sense, but can also be shown to improve precision significantly on a non-trivial cross-language retrieval task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal ofMachine Learning Research</journal>
<volume>3</volume>
<pages>993--1022</pages>
<contexts>
<context position="1208" citStr="Blei et al., 2003" startWordPosition="184" endWordPosition="187">uency words tend to end up scattered across many of the latent topics without much rhyme or reason. We show, however, that the ‘problem’ of high-frequency words can be dealt with more elegantly, and in a way that to our knowledge has not been considered in LDA, through the use of appropriate weighting schemes comparable to those sometimes used in Latent Semantic Indexing (LSI). Our proposed weighting methods not only make theoretical sense, but can also be shown to improve precision significantly on a non-trivial cross-language retrieval task. 1 Introduction Latent Dirichlet Allocation (LDA) (Blei et al., 2003), like its more established competitors Latent Semantic Indexing (LSI) (Deerwester et al., 1990) and Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999), is a model which is applicable to the analysis of text corpora. It is claimed to differ from LSI in that LDA is a generative Bayesian model (Blei et al., 2003), although this may depend upon the manner in which one approaches LSI (see for example Chew et al. (2010)). In LDA as applied to text analysis, each document in the corpus is modeled as a mixture over an underlying set of topics, and each topic is modeled as a probability dis</context>
<context position="2437" citStr="Blei et al. (2003)" startWordPosition="399" endWordPosition="402">ver the terms in the vocabulary. As the newest among the above-mentioned techniques, LDA is still in a relatively early stage of development. It is also sufficiently different from LSI, probably the most popular and well-known compression technique for information retrieval (IR), that many practitioners of LSI may perceive a ‘barrier to entry’ to LDA. This in turn perhaps explains why notions such as term weighting, which have been commonplace in LSI for some time (Dumais, 1991), have not yet found a place in LDA. In fact, it is often assumed that weighting is unnecessary in LDA. For example, Blei et al. (2003) contrast the use of tfidf weighting in both non-reduced space (Salton and McGill, 1983) and LSI on the one hand with PLSI and LDA on the other, where no mention is made of weighting. Ramage et al. (2008) propose a simple term-frequency weighting scheme for tagged documents within the framework of LDA, although term weighting is not their focus and their scheme is intended to incorporate document tags into the same model that represents the documents themselves. In this paper, we produce evidence that term weighting should be given consideration within LDA. First and foremost, this is shown em</context>
<context position="3941" citStr="Blei et al. (2003)" startWordPosition="641" endWordPosition="644">erm weighting, highfrequency words (which might otherwise be eliminated as stopwords) are assigned naturally to topics 465 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 465–473, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics by LDA, rather than dominating and being scattered across many topics as happens with the standard uniform weighting. Our approach belies the usually unstated, but widespread, assumption in papers on LDA that the removal of stopwords is a necessary pre-processing step (see e.g. Blei et al. (2003); Griffiths and Steyvers (2004)). It might seem that to demonstrate this it would be necessary to perform a test that directly compares the results when stoplists are used to those when weighting are used. However, we believe that stopwords are highly ad-hoc to begin with. Assuming a vocabulary of n words and a stoplist of x items, there are (at least in theory) (n) possible stoplists. To be sure x that no stoplist improves on a particular term weighting scheme we would have to test every one of these. In addition, our tests are with a multilingual dataset, which raises the issue that a domain</context>
<context position="5611" citStr="Blei et al. (2003)" startWordPosition="929" endWordPosition="932"> only very recently been applied to cross-language IR. In Section 3, we look at alternatives to the ‘standard’ uniform weighting scheme (i.e., lack of weighting scheme) commonly used in LDA. Section 4 discusses the framework we use for empirical testing of our hypothesis that a weighting scheme would be beneficial. We present the results of this comparison in Section 5 along with an impressionistic comparison of the output of the different alternatives. We conclude in Section 6. 2 Latent Dirichlet Allocation Our IR framework is multilingual Latent Dirichlet Allocation (LDA), first proposed by Blei et al. (2003) as a general Bayesian framework with initial application to topic modeling. It is only very recently that variants of LDA have been applied to crosslanguage IR: examples are Cimiano et al. (2009) and Ni et al. (2009). As an approach to topic modeling, LDA relies on the idea that the tokens in a document are drawn independently from a set of topics where each topic is a distribution over types (words) in the vocabulary. The mixing coefficients for topics within each document and weights for types in each topic can be specified a priori or learned from a training corpus. Blei et al. initially p</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. Journal ofMachine Learning Research 3, pages 993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sourav Chatterji</author>
<author>Lior Pachter</author>
</authors>
<title>Multiple Organism Gene Finding by Collapsed Gibbs Sampling.</title>
<date>2004</date>
<booktitle>In RECOMB ’04: Proceedings of the eighth annual international conference on Research in computational molecular biology,</booktitle>
<pages>187--193</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="8737" citStr="Chatterji and Pachter (2004)" startWordPosition="1464" endWordPosition="1467">do Draw θ — Dirichlet(α) Draw N — Poisson(ξ) for i = 1 to N do Draw z — Multinomial(θ) Draw w — Multinomial(ϕ(z)) end for end for Algorithm 1: Generative algorithm for LDA. This will generate D documents with N tokens each. Each token is drawn from one of K topics. The distributions over topics and terms have Dirichlet hyperparameters α and β respectively. The Poisson distribution over the token count may be replaced with any other convenient distribution. 2.2 Learning Topics via Collapsed Gibbs Sampling Rather than learn 0 and 0 directly, we use collapsed Gibbs sampling (Geman et al. (1993), Chatterji and Pachter (2004)) to learn the latent assignment of tokens to topics z given the observed tokens x. The algorithm operates by repeatedly sampling each zij from a distribution conditioned on the values of all other elements of z. This requires maintaining counts of tokens assigned to topics globally and within each document. We use the following notation for these sums: Nijk: Number of tokens of type wi in document dj assigned to topic k N−st ijk : The sum Nijk with the contribution of token xst excluded We indicate summation over all values of an index with (�). Given the current state of z the conditional pr</context>
</contexts>
<marker>Chatterji, Pachter, 2004</marker>
<rawString>Sourav Chatterji and Lior Pachter. 2004. Multiple Organism Gene Finding by Collapsed Gibbs Sampling. In RECOMB ’04: Proceedings of the eighth annual international conference on Research in computational molecular biology, pages 187–193, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter A Chew</author>
<author>Ahmed Abdelali</author>
</authors>
<title>Benefits of the ‘Massively Parallel Rosetta Stone’: CrossLanguage Information Retrieval with Over 30 Languages.</title>
<date>2007</date>
<booktitle>In Association for Computational Linguistics, editor, Proceedings of the 45th meeting of the Association of Computational Linguistics,</booktitle>
<pages>872--879</pages>
<contexts>
<context position="15621" citStr="Chew and Abdelali (2007)" startWordPosition="2727" endWordPosition="2731">‘that’, ‘the’ and ‘to’ in English) that are assigned very low weight everywhere else in the corpus. This clamping does not noticeably affect the results. In the next sections, we describe tests which have enabled us to evaluate empirically which of these formulations works best in practice. ij + α M(·)jk (4) M(·)j(·) + Tα m(xi)N−ij i(·)k + β p(xi|d) m(xi, d) = − log2 = −log2 #[tokens of type xi] p(xi) (7) #[tokens of type xi in d] 468 4 Testing Framework In this paper, we chose to test our hypotheses with the same cross-language retrieval task used in a number of previous studies of LSI (e.g. Chew and Abdelali (2007)). Briefly, the task is to train an IR model on one particular multilingual corpus, then deploy it on a separate multilingual corpus, using a document in one language to retrieve related documents in other languages. This task is difficult because of the size of the datasets involved. Its usefulness becomes apparent when we consider the following two use cases: a human wishing (1) to use a search engine to retrieve relevant documents in many languages regardless of the language in which the query is posed; or (2) to produce a clustering or visualization of documents according to their topics e</context>
</contexts>
<marker>Chew, Abdelali, 2007</marker>
<rawString>Peter A. Chew and Ahmed Abdelali. 2007. Benefits of the ‘Massively Parallel Rosetta Stone’: CrossLanguage Information Retrieval with Over 30 Languages. In Association for Computational Linguistics, editor, Proceedings of the 45th meeting of the Association of Computational Linguistics, pages 872–879.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter A Chew</author>
<author>Brett W Bader</author>
<author>Stephen Helmreich</author>
<author>Ahmed Abdelali</author>
<author>Stephen J Verzi</author>
</authors>
<date>2010</date>
<contexts>
<context position="1637" citStr="Chew et al. (2010)" startWordPosition="258" endWordPosition="261">oretical sense, but can also be shown to improve precision significantly on a non-trivial cross-language retrieval task. 1 Introduction Latent Dirichlet Allocation (LDA) (Blei et al., 2003), like its more established competitors Latent Semantic Indexing (LSI) (Deerwester et al., 1990) and Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999), is a model which is applicable to the analysis of text corpora. It is claimed to differ from LSI in that LDA is a generative Bayesian model (Blei et al., 2003), although this may depend upon the manner in which one approaches LSI (see for example Chew et al. (2010)). In LDA as applied to text analysis, each document in the corpus is modeled as a mixture over an underlying set of topics, and each topic is modeled as a probability distribution over the terms in the vocabulary. As the newest among the above-mentioned techniques, LDA is still in a relatively early stage of development. It is also sufficiently different from LSI, probably the most popular and well-known compression technique for information retrieval (IR), that many practitioners of LSI may perceive a ‘barrier to entry’ to LDA. This in turn perhaps explains why notions such as term weighting</context>
<context position="14486" citStr="Chew et al., 2010" startWordPosition="2516" endWordPosition="2519">. p(zij = k|z−ij, x, d, m, α, β) = Ew m(w)N−ij wjk+ α E wm(w)N−ij Ew m(w)Nwj(·) + Tα w(·)k + Wβ (6) If all weights m(w) = 1 this reduces immediately to the standard LDA formulation in Eq. 1. The information measure we describe above is constant for a particular term across the entire corpus, but it is possible to conceive of other, more sophisticated weighting schemes as well, for example those where term weights vary by document. Pointwise mutual information (PMI) is one such weighting scheme which has a solid basis in information theory and has been shown to work well in the context of LSI (Chew et al., 2010). According to PMI, the weight of a given term w in a given document d is the pointwise mutual information of the term and document, or − log2 p(w|d) p(w). Extending the LDA model to accommodate PMI is straightforward. We replace m(xi) and m(w) in Eq. 4 with m(xi, d) as follows. It is possible for PMI of a term within a document to be negative. When this happens, we clamp the weight of the offending term to zero in that document. In practice, we observe this only with common words (e.g. ‘and’, ‘in’, ‘of’, ‘that’, ‘the’ and ‘to’ in English) that are assigned very low weight everywhere else in t</context>
<context position="18217" citStr="Chew et al., 2010" startWordPosition="3149" endWordPosition="3152">; we believe this contributes to a clean separation of training and test data, and makes for a non-trivial retrieval task. In our framework, a term-by-document matrix is formed from the Bible as a parallel verse-aligned corpus. We employed two different approaches to tokenization, one (word-based tokenization) in which text was tokenized at every non-word character, and the other (unsupervised morpheme-based tokenization) in which after word-based tokenization, a further pre-processing step (based on Goldsmith (2001)) was performed to add extra breaks at every morpheme. It is shown elsewhere (Chew et al., 2010) that this step leads to improved performance with LSI. In each verse, all languages are concatenated together, allowing terms (either morphemes or words) from all languages to be represented in every verse. Cross-language homographs such as ‘mien’ in English and French are treated as distinct terms in our framework. Thus, if there are L languages, D documents (each of which is translated into each of the L languages), and W distinct linguistic terms across all languages, then the term-by-document matrix is of dimensions W by D (not W by DxL); with the Bible as a training corpus, the actual nu</context>
<context position="22091" citStr="Chew et al., 2010" startWordPosition="3807" endWordPosition="3810">n from Eq. 1. This is true whether tokenization is by word or by morpheme. All increases from the baseline precision at 1 document (0.505 and 0.544 respectively), whether under log or PMI weighting, are highly significant (p &lt; 10−11). Furthermore, all increases in precision when moving from word-based to morphology-based tokenization are also highly significant (p &lt; 5 x 10−5 without weighting, p &lt; 5x10−3 with log-weighting, and p &lt; 2x10−15 with PMI weighting). The best result overall, where P1 is 0.686, is obtained with morphological tokenization and PMI weighting (parallel to the results in (Chew et al., 2010) with LSI), and again the difference between this result and its nearest competitor of 0.641 is highly significant (p &lt; 3 x 10−6). We return to comment below on lack of an increase in P1 when moving from log-weighting to PMI-weighting under word-based tokenization. These results can also be broken out by language pair, as shown in Table 2. Here, it is apparent that Arabic, and to a lesser extent Russian, are harder languages in the IR problem at hand. Our intuition is that this is connected with the fact that these two languages have a more complex morphological structure: words are formed by </context>
</contexts>
<marker>Chew, Bader, Helmreich, Abdelali, Verzi, 2010</marker>
<rawString>Peter A. Chew, Brett W. Bader, Stephen Helmreich, Ahmed Abdelali, and Stephen J. Verzi. 2010.</rawString>
</citation>
<citation valid="false">
<authors>
<author>An Information-Theoretic</author>
</authors>
<title>Vector-Space-Model Approach to Cross-Language Information Retrieval.</title>
<journal>Journal ofNatural Language Engineering. Forthcoming.</journal>
<marker>Information-Theoretic, </marker>
<rawString>An Information-Theoretic, Vector-Space-Model Approach to Cross-Language Information Retrieval. Journal ofNatural Language Engineering. Forthcoming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Cimiano</author>
<author>Antje Schultz</author>
<author>Sergej Sizov</author>
<author>Philipp Sorg</author>
<author>Steffen Staab</author>
</authors>
<title>Explicit Versus Latent Concept Models for Cross-Language Information Retrieval.</title>
<date>2009</date>
<booktitle>In Proceedings of the 21st International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1513--1518</pages>
<contexts>
<context position="5807" citStr="Cimiano et al. (2009)" startWordPosition="962" endWordPosition="965">ction 4 discusses the framework we use for empirical testing of our hypothesis that a weighting scheme would be beneficial. We present the results of this comparison in Section 5 along with an impressionistic comparison of the output of the different alternatives. We conclude in Section 6. 2 Latent Dirichlet Allocation Our IR framework is multilingual Latent Dirichlet Allocation (LDA), first proposed by Blei et al. (2003) as a general Bayesian framework with initial application to topic modeling. It is only very recently that variants of LDA have been applied to crosslanguage IR: examples are Cimiano et al. (2009) and Ni et al. (2009). As an approach to topic modeling, LDA relies on the idea that the tokens in a document are drawn independently from a set of topics where each topic is a distribution over types (words) in the vocabulary. The mixing coefficients for topics within each document and weights for types in each topic can be specified a priori or learned from a training corpus. Blei et al. initially proposed a variational model (2003) for learning topics from data. Griffiths and Steyvers (2004) later developed a Markov chain Monte Carlo approach based on collapsed Gibbs sampling. In this model</context>
</contexts>
<marker>Cimiano, Schultz, Sizov, Sorg, Staab, 2009</marker>
<rawString>Philipp Cimiano, Antje Schultz, Sergej Sizov, Philipp Sorg, and Steffen Staab. 2009. Explicit Versus Latent Concept Models for Cross-Language Information Retrieval. In Proceedings of the 21st International Joint Conference on Artificial Intelligence, pages 1513–1518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by Latent Semantic Analysis.</title>
<date>1990</date>
<journal>Journal of the American Society of Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="1304" citStr="Deerwester et al., 1990" startWordPosition="198" endWordPosition="201">r reason. We show, however, that the ‘problem’ of high-frequency words can be dealt with more elegantly, and in a way that to our knowledge has not been considered in LDA, through the use of appropriate weighting schemes comparable to those sometimes used in Latent Semantic Indexing (LSI). Our proposed weighting methods not only make theoretical sense, but can also be shown to improve precision significantly on a non-trivial cross-language retrieval task. 1 Introduction Latent Dirichlet Allocation (LDA) (Blei et al., 2003), like its more established competitors Latent Semantic Indexing (LSI) (Deerwester et al., 1990) and Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999), is a model which is applicable to the analysis of text corpora. It is claimed to differ from LSI in that LDA is a generative Bayesian model (Blei et al., 2003), although this may depend upon the manner in which one approaches LSI (see for example Chew et al. (2010)). In LDA as applied to text analysis, each document in the corpus is modeled as a mixture over an underlying set of topics, and each topic is modeled as a probability distribution over the terms in the vocabulary. As the newest among the above-mentioned techniques, </context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by Latent Semantic Analysis. Journal of the American Society of Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan T Dumais</author>
</authors>
<title>Improving the Retrieval of Information from External Sources.</title>
<date>1991</date>
<journal>Behavior Research Methods, Instruments and Computers,</journal>
<volume>23</volume>
<issue>2</issue>
<contexts>
<context position="2302" citStr="Dumais, 1991" startWordPosition="374" endWordPosition="375"> in the corpus is modeled as a mixture over an underlying set of topics, and each topic is modeled as a probability distribution over the terms in the vocabulary. As the newest among the above-mentioned techniques, LDA is still in a relatively early stage of development. It is also sufficiently different from LSI, probably the most popular and well-known compression technique for information retrieval (IR), that many practitioners of LSI may perceive a ‘barrier to entry’ to LDA. This in turn perhaps explains why notions such as term weighting, which have been commonplace in LSI for some time (Dumais, 1991), have not yet found a place in LDA. In fact, it is often assumed that weighting is unnecessary in LDA. For example, Blei et al. (2003) contrast the use of tfidf weighting in both non-reduced space (Salton and McGill, 1983) and LSI on the one hand with PLSI and LDA on the other, where no mention is made of weighting. Ramage et al. (2008) propose a simple term-frequency weighting scheme for tagged documents within the framework of LDA, although term weighting is not their focus and their scheme is intended to incorporate document tags into the same model that represents the documents themselves</context>
</contexts>
<marker>Dumais, 1991</marker>
<rawString>Susan T. Dumais. 1991. Improving the Retrieval of Information from External Sources. Behavior Research Methods, Instruments and Computers, 23(2):229–236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Geman</author>
<author>Donald Geman</author>
<author>K Abend</author>
<author>T J Harley</author>
<author>L N Kanal</author>
</authors>
<title>Stochastic Relaxation, Gibbs Distributions and the Bayesian Restoration of Images*.</title>
<date>1993</date>
<journal>Journal ofApplied Statistics,</journal>
<volume>20</volume>
<issue>5</issue>
<contexts>
<context position="8707" citStr="Geman et al. (1993)" startWordPosition="1460" endWordPosition="1463">β) end for ford= 1toDdo Draw θ — Dirichlet(α) Draw N — Poisson(ξ) for i = 1 to N do Draw z — Multinomial(θ) Draw w — Multinomial(ϕ(z)) end for end for Algorithm 1: Generative algorithm for LDA. This will generate D documents with N tokens each. Each token is drawn from one of K topics. The distributions over topics and terms have Dirichlet hyperparameters α and β respectively. The Poisson distribution over the token count may be replaced with any other convenient distribution. 2.2 Learning Topics via Collapsed Gibbs Sampling Rather than learn 0 and 0 directly, we use collapsed Gibbs sampling (Geman et al. (1993), Chatterji and Pachter (2004)) to learn the latent assignment of tokens to topics z given the observed tokens x. The algorithm operates by repeatedly sampling each zij from a distribution conditioned on the values of all other elements of z. This requires maintaining counts of tokens assigned to topics globally and within each document. We use the following notation for these sums: Nijk: Number of tokens of type wi in document dj assigned to topic k N−st ijk : The sum Nijk with the contribution of token xst excluded We indicate summation over all values of an index with (�). Given the current</context>
</contexts>
<marker>Geman, Geman, Abend, Harley, Kanal, 1993</marker>
<rawString>Stuart Geman, Donald Geman, K. Abend, T. J. Harley, and L. N. Kanal. 1993. Stochastic Relaxation, Gibbs Distributions and the Bayesian Restoration of Images*. Journal ofApplied Statistics, 20(5):25–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goldsmith</author>
</authors>
<title>Unsupervised Learning of the Morphology of a Natural Language.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="18121" citStr="Goldsmith (2001)" startWordPosition="3133" endWordPosition="3135">ted that the Bible and Quran were written centuries apart, and in different original languages; we believe this contributes to a clean separation of training and test data, and makes for a non-trivial retrieval task. In our framework, a term-by-document matrix is formed from the Bible as a parallel verse-aligned corpus. We employed two different approaches to tokenization, one (word-based tokenization) in which text was tokenized at every non-word character, and the other (unsupervised morpheme-based tokenization) in which after word-based tokenization, a further pre-processing step (based on Goldsmith (2001)) was performed to add extra breaks at every morpheme. It is shown elsewhere (Chew et al., 2010) that this step leads to improved performance with LSI. In each verse, all languages are concatenated together, allowing terms (either morphemes or words) from all languages to be represented in every verse. Cross-language homographs such as ‘mien’ in English and French are treated as distinct terms in our framework. Thus, if there are L languages, D documents (each of which is translated into each of the L languages), and W distinct linguistic terms across all languages, then the term-by-document m</context>
</contexts>
<marker>Goldsmith, 2001</marker>
<rawString>J. Goldsmith. 2001. Unsupervised Learning of the Morphology of a Natural Language. Computational Linguistics, 27(2):153–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding Scientific Topics.</title>
<date>2004</date>
<booktitle>In Proceedings of the National Academy of Sciences USA,</booktitle>
<volume>101</volume>
<pages>5228--5235</pages>
<contexts>
<context position="3972" citStr="Griffiths and Steyvers (2004)" startWordPosition="645" endWordPosition="649">requency words (which might otherwise be eliminated as stopwords) are assigned naturally to topics 465 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 465–473, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics by LDA, rather than dominating and being scattered across many topics as happens with the standard uniform weighting. Our approach belies the usually unstated, but widespread, assumption in papers on LDA that the removal of stopwords is a necessary pre-processing step (see e.g. Blei et al. (2003); Griffiths and Steyvers (2004)). It might seem that to demonstrate this it would be necessary to perform a test that directly compares the results when stoplists are used to those when weighting are used. However, we believe that stopwords are highly ad-hoc to begin with. Assuming a vocabulary of n words and a stoplist of x items, there are (at least in theory) (n) possible stoplists. To be sure x that no stoplist improves on a particular term weighting scheme we would have to test every one of these. In addition, our tests are with a multilingual dataset, which raises the issue that a domain-appropriate stoplist for a par</context>
<context position="6306" citStr="Griffiths and Steyvers (2004)" startWordPosition="1051" endWordPosition="1054"> modeling. It is only very recently that variants of LDA have been applied to crosslanguage IR: examples are Cimiano et al. (2009) and Ni et al. (2009). As an approach to topic modeling, LDA relies on the idea that the tokens in a document are drawn independently from a set of topics where each topic is a distribution over types (words) in the vocabulary. The mixing coefficients for topics within each document and weights for types in each topic can be specified a priori or learned from a training corpus. Blei et al. initially proposed a variational model (2003) for learning topics from data. Griffiths and Steyvers (2004) later developed a Markov chain Monte Carlo approach based on collapsed Gibbs sampling. In this model, the mixing weights for topics within each document and the multinomial coefficients for terms within each topic are hidden (latent) and must be learned from a training corpus. Blei et al. (2003) proposed LDA as a general Bayesian framework and gave a variational model for learning topics from data. Griffiths and Steyvers (2004) subsequently developed a stochastic learning algorithm based on collapsed Gibbs sampling. In this paper we will focus on the Gibbs sampling approach. 2.1 Generative Do</context>
<context position="9469" citStr="Griffiths and Steyvers (2004)" startWordPosition="1597" endWordPosition="1600">tes by repeatedly sampling each zij from a distribution conditioned on the values of all other elements of z. This requires maintaining counts of tokens assigned to topics globally and within each document. We use the following notation for these sums: Nijk: Number of tokens of type wi in document dj assigned to topic k N−st ijk : The sum Nijk with the contribution of token xst excluded We indicate summation over all values of an index with (�). Given the current state of z the conditional probability of zij is: p(zij = k|z−ij,x,d,α,β) = p(xij|ϕk) p(k|dj) a N−ij (·)jk + α N−ij (·)(·)k + Wβ As Griffiths and Steyvers (2004) point out, this is an intuitive result. The first term, p(xij|ϕk), indicates the importance of term xij in topic k. The second term, p(k|dj), indicates the importance of topic k in document j. The sum of the terms is normalized implicitly to 1 when we draw each new zij. We sample a new value for zij for every token xij during each iteration of Gibbs sampling. We run the sampler for a burn-in period of a few hundred iterations to allow it to reach its converged state and then estimate 0 and 0 from z as follows: (2) N(·)j(·) + Tα Ni(·)k + β (3) 2.3 Classifying New Documents In LSI, new document</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding Scientific Topics. In Proceedings of the National Academy of Sciences USA, volume 101, pages 5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probablistic Latent Semantic Indexing.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd Annual International SIGIR Conference,</booktitle>
<pages>53--57</pages>
<contexts>
<context position="1370" citStr="Hofmann, 1999" startWordPosition="208" endWordPosition="210">e dealt with more elegantly, and in a way that to our knowledge has not been considered in LDA, through the use of appropriate weighting schemes comparable to those sometimes used in Latent Semantic Indexing (LSI). Our proposed weighting methods not only make theoretical sense, but can also be shown to improve precision significantly on a non-trivial cross-language retrieval task. 1 Introduction Latent Dirichlet Allocation (LDA) (Blei et al., 2003), like its more established competitors Latent Semantic Indexing (LSI) (Deerwester et al., 1990) and Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999), is a model which is applicable to the analysis of text corpora. It is claimed to differ from LSI in that LDA is a generative Bayesian model (Blei et al., 2003), although this may depend upon the manner in which one approaches LSI (see for example Chew et al. (2010)). In LDA as applied to text analysis, each document in the corpus is modeled as a mixture over an underlying set of topics, and each topic is modeled as a probability distribution over the terms in the vocabulary. As the newest among the above-mentioned techniques, LDA is still in a relatively early stage of development. It is als</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probablistic Latent Semantic Indexing. In Proceedings of the 22nd Annual International SIGIR Conference, pages 53–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Solomon Kullback</author>
<author>Richard A Leibler</author>
</authors>
<date>1951</date>
<booktitle>On Information and Sufficiency. Annals of Mathematical Statistics,</booktitle>
<pages>22--49</pages>
<contexts>
<context position="11361" citStr="Kullback and Leibler (1951)" startWordPosition="1930" endWordPosition="1933">he vocabulary of the training corpus do not participate in classification. The resulting distribution 0′ essentially encodes how likely each new document is to relate to each of the K topics. We can use this matrix to compute pairwise similarities between any two documents from either corpus (training or newly-classified). Whereas in LSI it may make sense to compute similarity between documents using the cosine metric (since the ‘dimensions’ defining the space are orthogonal), we compute similarities in LDA using either the symmetrized Kullback-Leibler (KL) or Jensen-Shannon (JS) divergences (Kullback and Leibler (1951), Lin (2002)) since these are methods of measuring the similarity between probability distributions. N−ij i(·)k + β (1) N(·)j(·) + Tα θjk = ϕki = N(·)(·)k + Wβ N(·)jk + α 467 3 Term Weighting Schemes and LDA The standard approach presented above assumes, effectively, that each token is equally important in calculating the conditional probabilities. From both an information-theoretic and a linguistic point of view, however, it is clear that this is not the case. In English, a term such as ‘the’ which occurs with high frequency in many documents does not contribute as much to the meaning of each</context>
</contexts>
<marker>Kullback, Leibler, 1951</marker>
<rawString>Solomon Kullback and Richard A. Leibler. 1951. On Information and Sufficiency. Annals of Mathematical Statistics, 22:49–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
</authors>
<title>Divergence Measures based on the Shannon Entropy.</title>
<date>2002</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="11373" citStr="Lin (2002)" startWordPosition="1934" endWordPosition="1935"> corpus do not participate in classification. The resulting distribution 0′ essentially encodes how likely each new document is to relate to each of the K topics. We can use this matrix to compute pairwise similarities between any two documents from either corpus (training or newly-classified). Whereas in LSI it may make sense to compute similarity between documents using the cosine metric (since the ‘dimensions’ defining the space are orthogonal), we compute similarities in LDA using either the symmetrized Kullback-Leibler (KL) or Jensen-Shannon (JS) divergences (Kullback and Leibler (1951), Lin (2002)) since these are methods of measuring the similarity between probability distributions. N−ij i(·)k + β (1) N(·)j(·) + Tα θjk = ϕki = N(·)(·)k + Wβ N(·)jk + α 467 3 Term Weighting Schemes and LDA The standard approach presented above assumes, effectively, that each token is equally important in calculating the conditional probabilities. From both an information-theoretic and a linguistic point of view, however, it is clear that this is not the case. In English, a term such as ‘the’ which occurs with high frequency in many documents does not contribute as much to the meaning of each document as</context>
<context position="20568" citStr="Lin, 2002" startWordPosition="3547" endWordPosition="3548">ges using just the probability distributions. For each source-language/target-language pair L1 and L2, we obtain the similarity of each of the 114 documents in L1 to each of the 114 documents in L2. We found that similarity here is best computed using the Jensen-Shannon divergence 469 Tokenization Weighting Scheme Word Morpheme Unweighted 0.505 0.544 logp(w|L) 0.616 0.641 PMI 0.612 0.686 Table 1: Summary of comparison results. This table shows the average precision at one document (P1) for each of the tokenization and weighting schemes we evaluated. Detailed results are presented in Table 2. (Lin, 2002) and so this measure was used in all tests. Ultimately, the measure of how well a particular method performs is average precision at 1 document (P1). Among the various measurements for evaluating the performance of IR systems (Salton and McGill (1983), van Rijsbergen (1979)), this is a fairly standard measure. For a particular sourcetarget pair, this is the percentage (out of 114 cases) where a document in L1 is most similar to its mate in L2. With 5 languages, there are 25 source-target pairs, and we can also calculate average P1 across all language pairs. Here, we average across 114 x 25 (or</context>
</contexts>
<marker>Lin, 2002</marker>
<rawString>J. Lin. 2002. Divergence Measures based on the Shannon Entropy. IEEE Transactions on Information Theory, 37(1):145–151, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaochuan Ni</author>
<author>Jian-Tao Sun</author>
<author>Jian Hu</author>
<author>Zheng Chen</author>
</authors>
<title>Mining Multilingual Topics from Wikipedia.</title>
<date>2009</date>
<booktitle>In 18th International World Wide Web Conference,</booktitle>
<pages>1155--1155</pages>
<contexts>
<context position="5828" citStr="Ni et al. (2009)" startWordPosition="967" endWordPosition="970">ework we use for empirical testing of our hypothesis that a weighting scheme would be beneficial. We present the results of this comparison in Section 5 along with an impressionistic comparison of the output of the different alternatives. We conclude in Section 6. 2 Latent Dirichlet Allocation Our IR framework is multilingual Latent Dirichlet Allocation (LDA), first proposed by Blei et al. (2003) as a general Bayesian framework with initial application to topic modeling. It is only very recently that variants of LDA have been applied to crosslanguage IR: examples are Cimiano et al. (2009) and Ni et al. (2009). As an approach to topic modeling, LDA relies on the idea that the tokens in a document are drawn independently from a set of topics where each topic is a distribution over types (words) in the vocabulary. The mixing coefficients for topics within each document and weights for types in each topic can be specified a priori or learned from a training corpus. Blei et al. initially proposed a variational model (2003) for learning topics from data. Griffiths and Steyvers (2004) later developed a Markov chain Monte Carlo approach based on collapsed Gibbs sampling. In this model, the mixing weights </context>
</contexts>
<marker>Ni, Sun, Hu, Chen, 2009</marker>
<rawString>Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen. 2009. Mining Multilingual Topics from Wikipedia. In 18th International World Wide Web Conference, pages 1155–1155, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ramage</author>
<author>Paul Heymann</author>
<author>Christopher D Manning</author>
<author>Hector Garcia-Molina</author>
</authors>
<title>Clustering the Tagged Web. In</title>
<date>2008</date>
<booktitle>Second ACM International Conference on Web Search and Data Mining (WSDM</booktitle>
<contexts>
<context position="2641" citStr="Ramage et al. (2008)" startWordPosition="438" endWordPosition="441">t popular and well-known compression technique for information retrieval (IR), that many practitioners of LSI may perceive a ‘barrier to entry’ to LDA. This in turn perhaps explains why notions such as term weighting, which have been commonplace in LSI for some time (Dumais, 1991), have not yet found a place in LDA. In fact, it is often assumed that weighting is unnecessary in LDA. For example, Blei et al. (2003) contrast the use of tfidf weighting in both non-reduced space (Salton and McGill, 1983) and LSI on the one hand with PLSI and LDA on the other, where no mention is made of weighting. Ramage et al. (2008) propose a simple term-frequency weighting scheme for tagged documents within the framework of LDA, although term weighting is not their focus and their scheme is intended to incorporate document tags into the same model that represents the documents themselves. In this paper, we produce evidence that term weighting should be given consideration within LDA. First and foremost, this is shown empirically through a non-trivial multilingual retrieval task which has previously been used as the basis for tests of variants of LSI. We also show that term weighting allows one to avoid maintenance of st</context>
</contexts>
<marker>Ramage, Heymann, Manning, Garcia-Molina, 2008</marker>
<rawString>Daniel Ramage, Paul Heymann, Christopher D. Manning, and Hector Garcia-Molina. 2008. Clustering the Tagged Web. In Second ACM International Conference on Web Search and Data Mining (WSDM 2009), November.</rawString>
</citation>
<citation valid="true">
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<editor>G. Salton and M. McGill, editors.</editor>
<publisher>McGraw-Hill.</publisher>
<contexts>
<context position="20819" citStr="(1983)" startWordPosition="3591" endWordPosition="3591"> Jensen-Shannon divergence 469 Tokenization Weighting Scheme Word Morpheme Unweighted 0.505 0.544 logp(w|L) 0.616 0.641 PMI 0.612 0.686 Table 1: Summary of comparison results. This table shows the average precision at one document (P1) for each of the tokenization and weighting schemes we evaluated. Detailed results are presented in Table 2. (Lin, 2002) and so this measure was used in all tests. Ultimately, the measure of how well a particular method performs is average precision at 1 document (P1). Among the various measurements for evaluating the performance of IR systems (Salton and McGill (1983), van Rijsbergen (1979)), this is a fairly standard measure. For a particular sourcetarget pair, this is the percentage (out of 114 cases) where a document in L1 is most similar to its mate in L2. With 5 languages, there are 25 source-target pairs, and we can also calculate average P1 across all language pairs. Here, we average across 114 x 25 (or 2,850) cases. This is why even small differences in P1 can be statistically significant. 5 Results First, we present a summary of our results in Table 1 which clearly demonstrates that it is better in LDA to use some kind of weighting scheme rather t</context>
</contexts>
<marker>1983</marker>
<rawString>G. Salton and M. McGill, editors. 1983. Introduction to Modern Information Retrieval. McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Biola University</author>
</authors>
<title>The Unbound Bible.</title>
<date>2006</date>
<location>http://www.unboundbible.com.</location>
<contexts>
<context position="16516" citStr="University (2006)" startWordPosition="2879" endWordPosition="2880">volved. Its usefulness becomes apparent when we consider the following two use cases: a human wishing (1) to use a search engine to retrieve relevant documents in many languages regardless of the language in which the query is posed; or (2) to produce a clustering or visualization of documents according to their topics even when the documents are in different languages. The training corpus consists of the text ofthe Bible in 31,226 parallel chunks, corresponding generally to verses, in Arabic, English, French, Russian and Spanish. These data were obtained from the Unbound Bible project (Biola University (2006)). The test data, obtained from http://www.kuran.gen. tr/, is the text of the Quran in the same 5 languages, in 114 parallel chunks corresponding to suras (chapters). The task, in short, is to use the training data to inform whatever linguistic, semantic, or statistical model is being tested, and then to infer characteristics of the test data in such a way that the test documents can automatically be matched with their translations in other languages. Though the documents come from a specific domain (scriptural texts), what is of interest is comparative results using different weighting scheme</context>
</contexts>
<marker>University, 2006</marker>
<rawString>Biola University. 2006. The Unbound Bible. http://www.unboundbible.com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J van Rijsbergen</author>
</authors>
<date>1979</date>
<journal>Information Retrieval. Butterworth-Heinemann.</journal>
<marker>van Rijsbergen, 1979</marker>
<rawString>C.J. van Rijsbergen. 1979. Information Retrieval. Butterworth-Heinemann.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>