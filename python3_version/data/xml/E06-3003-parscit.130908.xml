<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001344">
<title confidence="0.64313775">
An Approach to Summarizing Short Stories
Anna Kazantseva
The School of Information Technology and Engineering
University of Ottawa
</title>
<email confidence="0.977393">
ankazant@site.uottawa.ca
</email>
<sectionHeader confidence="0.99194" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999923">
This paper describes a system that pro-
duces extractive summaries of short
works of literary fiction. The ultimate
purpose of produced summaries is de-
fined as helping a reader to determine
whether she would be interested in read-
ing a particular story. To this end, the
summary aims to provide a reader with
an idea about the settings of a story (such
as characters, time and place) without re-
vealing the plot. The approach presented
here relies heavily on the notion of as-
pect. Preliminary results show an im-
provement over two naïve baselines: a
lead baseline and a more sophisticated
variant of it. Although modest, the results
suggest that using aspectual information
may be of help when summarizing fic-
tion. A more thorough evaluation involv-
ing human judges is under way.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999239375">
In the course of recent years the scientific
community working on the problem of automatic
text summarization has been experiencing an
upsurge. A multitude of different techniques has
been applied to this end, some of the more
remarkable of them being (Marcu, 1997; Mani et
al. 1998; Teufel and Moens, 2002; Elhadad et al.,
2005), to name just a few. These researchers
worked on various text genres: scientific and
popular scientific articles (Marcu, 1997; Mani et
al., 1998), texts in computational linguistics
(Teufel and Moens, 2002), and medical texts
(Elhadad et al., 2002). All these genres are ex-
amples of texts characterized by rigid structure,
relative abundance of surface markers and
straightforwardness. Relatively few attempts
have been made at summarizing less structured
genres, some of them being dialogue and speech
summarization (Zechner, 2002; Koumpis et al.
2001). The issue of summarizing fiction remains
largely untouched, since a few very thorough
earlier works (Charniak, 1972; Lehnert, 1982).
The work presented here seeks to fill in this gap.
The ultimate objective of the project is stated
as follows: to produce indicative summaries of
short works of fiction such that they be helpful to
a potential reader in deciding whether she would
be interested in reading a particular story or not.
To this end, revealing the plot was deemed un-
necessary and even undesirable. Instead, the cur-
rent approach relies on the following assumption:
when a reader is presented with an extracted
summary outlining the general settings of a story
(such as time, place and who it is about), she will
have enough information to decide how inter-
ested she would be in reading a story. For exam-
ple, a fragment of such a summary, produced by
an annotator for the story The Cost of Kindness
by Jerome K. Jerome is presented in Figure 1.
The plot, which is a tale of how one local family
decides to bid a warm farewell to Rev. Crackle-
thorpe and causes the vicar to change his mind
and remain in town, is omitted.
The data used in the experiments consisted of
23 short stories, all written in XIX – early XX
century by main-stream authors such as Kathe-
rine Mansfield, Anton Chekhov, O.Henry, Guy
de Maupassant and others (13 authors in total).
The genre can be vaguely termed social fiction
with the exception of a few fairy-tales. Such
vagueness as far as genre is concerned was de-
liberate, as the author wished to avoid producing
a system relying on cues specific to a particular
genre. Average length of a story in the corpus is
3,333 tokens (approximately 4.5 letter-sized
pages) and the target compression rate is 6%.
In order to separate the background of a story
from events, this project relies heavily on the
notion of aspect (the term is explained in Section
3.1). Each clause of every sentence is described
in terms of aspect-related features. This represen-
tation is then used to select salient descriptive
sentences and to leave out those which describe
events.
</bodyText>
<page confidence="0.998197">
55
</page>
<figureCaption confidence="0.995481">
Figure 1. A fragment of a desired summary for The Cost of Kindness by Jerome K. Jerome.
</figureCaption>
<subsectionHeader confidence="0.834898">
The Cost of Kindness
</subsectionHeader>
<bodyText confidence="0.980070259259259">
Jerome K. Jerome (1859-1927)
Augustus Cracklethorpe would be quitting Wychwood-on-the-Heath the following Monday, never to set
foot--so the Rev. Augustus Cracklethorpe himself and every single member of his congregation hoped sin-
cerely--in the neighbourhood again. [...] The Rev. Augustus Cracklethorpe, M.A., might possibly have been
of service to his Church in, say, some East-end parish of unsavoury reputation, some mission station far
advanced amid the hordes of heathendom. There his inborn instinct of antagonism to everybody and every-
thing surrounding him, his unconquerable disregard for other people&apos;s views and feelings, his inspired con-
viction that everybody but himself was bound to be always wrong about everything, combined with deter-
mination to act and speak fearlessly in such belief, might have found their uses. In picturesque little
Wychwood-on-the-Heath [...] these qualities made only for scandal and disunion.
The organization of the paper follows the
overall architecture of the system. Section 2 pro-
vides a generalized overview of the pre-
processing stage of the project, during which
pronominal and nominal anaphoric references
(the term is explained in Section 2) were re-
solved and main characters were identified. Sec-
tion 3 briefly reviews the concept of aspect,
gives an overview of the system and provides the
linguistic motivation behind it. Section 4 de-
scribes the classification procedures (machine
learning and manual rule creation) used to distin-
guish between descriptive elements of a story
and passages that describe events. It also reports
results. Section 5 draws some conclusions and
outlines possible directions in which this work
may evolve.
</bodyText>
<sectionHeader confidence="0.990446" genericHeader="introduction">
2 Data Pre-Processing
</sectionHeader>
<bodyText confidence="0.999390416666666">
Before working on selecting salient descriptive
sentences, the stories of the training set were ana-
lyzed for presence of surface markers denoting
characters, locations and temporal anchors. To
this end, the GATE Gazetteer (Cunningham et
al., 2002) was used, and only entities recognized
by it automatically were considered.
The findings were as follows. Each story con-
tained multiple mentions of characters (an aver-
age of 64 mentions per story). Yet only 22 loca-
tion markers were found, most of these being
street names. The 22 markers were found in 10
out of 14 stories, leaving 4 stories without any
identifiable location markers. Only 4 temporal
anchors were identified in all 14 stories: 2 abso-
lute (such as years) and 2 relative (names of
holidays). These findings support the intuitive
idea that short stories revolve around their char-
acters, even if the ultimate goal is to show a lar-
ger social phenomenon.
Due to this fact, the data was pre-processed in
such a way as to resolve pronominal and nominal
anaphoric references to animate entities. The
term anaphora can be informally explained as a
way of mentioning a previously encountered en-
tity without naming it explicitly. Consider exam-
ples 1a and 1b from The Gift of the Magi by O.
Henri. 1a is an example of pronominal anaphora,
where the noun phrase (further NP) Della is re-
ferred to as an antecedent and both occurrences
of the pronoun her as anaphoric expressions or
referents. Example 1b illustrates the concept of
nominal anaphora. Here the NP Dell is the ante-
cedent and my girl is the anaphoric expression
(in the context of this story Della and the girl are
the same person).
</bodyText>
<listItem confidence="0.978261166666667">
(1a) Della finished her cry and attended to
her cheeks with the powder rag.
(1b) &amp;quot;Don&apos;t make any mistake, Dell,&amp;quot; he said,
“about me. I don&apos;t think there&apos;s anything
[...] that could make me like my girl any
less.
</listItem>
<bodyText confidence="0.999963666666667">
The author created a system that resolved 1st
and 3rd person singular pronouns (I, me, my, he,
his etc.) and singular nominal anaphoric expres-
sions (e.g. the man, but not men). The system
was implemented in Java, within the GATE
framework, using Connexor Machinese Syntax
parser (Tapanainen and Järvinen, 1997).
A generalized overview of the system is pro-
vided below. During the first step, the docu-
ments were parsed using Connexor Machinese
Syntax parser. The parsed data was then for-
warded to the Gazetteer in GATE, which recog-
nized nouns denoting persons. The original ver-
sion of the Gazetteer recognized only named en-
tities and professions, but the Gazetteer was ex-
tended to include common animate nouns such as
man, woman, etc. As the next step, an imple-
mentation based on a classical pronoun resolu-
tion algorithm (Lappin and Leass, 1994) was ap-
plied to the texts. Subsequently, anaphoric noun
phrases were identified using the rules outlined
</bodyText>
<page confidence="0.994404">
56
</page>
<bodyText confidence="0.99320625">
in (Poesio and Vieira, 2000). Finally, these ana-
phoric noun phrases were resolved using a modi-
fied version of (Lappin and Leass, 1994), ad-
justed to finding antecedents of nouns.
</bodyText>
<tableCaption confidence="0.994318">
Table 1. Results of anaphora resolution.
</tableCaption>
<table confidence="0.9461992">
Type of All Correct Incor- Error
anaphora rect rate, %
Pronominal 597 507 90 15.07
Nominal 152 96 56 36.84
Both 749 603 146 19.49
</table>
<bodyText confidence="0.9924196">
A small-scale evaluation based on 2 short sto-
ries revealed results shown in Table 1. After re-
solving anaphoric expressions, characters that are
central to the story were selected based on nor-
malized frequency counts.
</bodyText>
<sectionHeader confidence="0.997844" genericHeader="method">
3 Selecting Descriptive Sentences Using
Aspectual Information
</sectionHeader>
<subsectionHeader confidence="0.999774">
3.1 Linguistic definition of aspect
</subsectionHeader>
<bodyText confidence="0.972997741935484">
In order to select salient sentences that set out the
background of a story, this project relied on the
notion of aspect. For the purposes of this paper
the author uses the term aspect to denote the
same concept as what (Huddleston and Pullum,
2002) call the situation type. Informally, it can be
explained as a characteristic of a clause that
gives an idea about the temporal flow of an event
or state being described.
A general hierarchy of aspectual classifi-
cation based on (Huddleston and Pullum, 2002)
is shown in Figure 2 with examples for each
type. In addition, aspectual type of a clause may
be altered by multiplicity, e.g. repetitions. Con-
sider examples 2a and 2b.
(2a) She read a book.
(2b) She usually read a book a day. (e.g. She
used to read a book a day).
Example 2b is referred to as serial situation
(Huddleston and Pullum, 2002). It is considered
to be a state, even though a single act of reading
a book would constitute an event.
Intuitively, stative situations (especially serial
ones) are more likely to be associated with de-
scriptions; that is with things that are, or things
that were happening for an extended period of
time (consider He was a tall man. vs. He opened
the window.).The rest of Section 3 describes the
approach used for identifying single and serial
stative clauses and for using them to construct
summaries.
</bodyText>
<subsectionHeader confidence="0.999883">
3.2 Overall system design
</subsectionHeader>
<bodyText confidence="0.9998785">
Selection of the salient background sentences
was conducted in the following manner. Firstly,
the pre-processed data (as outlined in Section 2)
was parsed using Connexor Machinese Syntax
parser. Then, sentences were recursively split
into clauses. For the purposes of this project a
clause is defined as a main verb with all its com-
plements, including subject, modifiers and their
sub-trees.
Subsequently, two different representations
were constructed for each clause: one fine-
grained and one coarse-grained. The main differ-
ence between these two representations was in
the number of attributes and in the cardinality of
the set of possible values, and not in how much
and what kind of information they carried. For
instance, the fine-grained dataset had 3 different
features with 7 possible values to carry tense-
related information: tense, is-progressive and
is-perfect, while the coarse-grained dataset car-
ried only one binary feature,
is-simple-past-or-present.
Two different approaches for selecting de-
scriptive sentences were tested on each of the
representations. The first approach used machine
learning techniques, namely C5.0 (Quinlan,
1992) implementation of decision trees. The sec-
ond approach consisted of applying a set of
manually created rules that guided the classifica-
tion process. Motivation for features used in each
dataset is given in Section 3.3. Both approaches
and preliminary results are discussed in Sections
</bodyText>
<subsectionHeader confidence="0.994148">
4.1 - 4.4.
</subsectionHeader>
<bodyText confidence="0.999055666666667">
The part of the system responsible for select-
ing descriptive sentences was implemented in
Python.
</bodyText>
<subsectionHeader confidence="0.7339405">
3.3 Feature selection: description and moti-
vation
</subsectionHeader>
<figureCaption confidence="0.990359">
Figure 2. Aspectual hierarchy after (Hud-
dleston and Pullum, 2002).
</figureCaption>
<page confidence="0.996265">
57
</page>
<tableCaption confidence="0.972255">
Table 2. Description of the features in both datasets
</tableCaption>
<table confidence="0.997686875">
Fine-grained dataset Coarse-grained dataset
Type of features Number of fea- Number of val- Number of fea- Number of values
tures ues tures
Character-related 9 16 4 6
Aspect-related 12 92 8 16
Location-related 2 4 2 4
Others 4 9 3 4
All 27 121 17 30
</table>
<bodyText confidence="0.98073401754386">
Features for both representations were selected
based on one of the following criteria:
(Criterion 1) a clause should ‘talk’ about im-
portant things, such as characters or locations
(Criterion 2) a clause should contain back-
ground descriptions rather then events
The number of features providing information
towards each criterion, as well as the number of
possible values, is shown in Table 2 for both
representations.
The attributes contributing towards Criterion 1
can be divided into character-related and loca-
tion-related.
Character-related features were designed so as
to help identify sentences that focused on charac-
ters, not just mentioned them in passing. These
attributes described whether a clause contained a
character mention and what its grammatical
function was (subject, object, etc.), whether such
a mention was modified and what was the posi-
tion of a parent sentence relative to the sentence
where this character was first mentioned (intui-
tively, earlier mentions of characters are more
likely to be descriptive).
Location-related features in both datasets de-
scribed whether a clause contained a location
mention and whether it was embedded in a
prepositional phrase (further PP). The rationale
behind these attributes is that location mentions
are more likely to occur in PPs, such as from the
Arc de Triomphe, to the Place de la Concorde.
In order to meet Criterion 2 (that is, to select
descriptive sentences) a number of aspect-related
features were calculated. These features were
selected so as to model characteristics of a clause
that help determine its aspectual class. The char-
acteristics used were default aspect of the main
verb of a clause, tense, temporal expressions,
semantic category of a verb, voice and some
properties of the direct object. Each of these
characteristics is listed below, along with motiva-
tion for it, and information about how it was
calculated.
It must be mentioned that several researchers
looked into determining automatically various
semantic properties of verbs, such as (Siegel,
1998; Merlo et al., 2002). Yet these approaches
dealt with properties of verbs in general and not
with particular usages in the context of concrete
sentences.
Default verbal aspect. A set of verbs, referred
to as stative verbs, tends to produce mostly sta-
tive clauses. Examples of such verbs include be,
like, feel, love, hate and many others. A common
property of such verbs is that they do not readily
yield a progressive form (Vendler, 1967; Dowty,
1979). Consider examples 3a and 3b.
</bodyText>
<listItem confidence="0.990787666666667">
(3a) She is talking. (a dynamic verb talk)
(3b) *She is liking the book. (a stative verb
like)
</listItem>
<bodyText confidence="0.999937055555556">
The default aspectual category of a verb was ap-
proximated using Longman Dictionary of Con-
temporary English (LDOCE). Verbs marked in
LDOCE as not having a progressive form were
considered stative and all others – dynamic. This
information was expressed in both datasets as 1
binary feature.
Grammatical tense. Usually, simple tenses
are more likely to be used in stative or habitual
situations than progressive or perfect tenses. In
fact, it is considered to be a property of stative
clauses that they normally do not occur in pro-
gressive (Vendler, 1967; Huddleston and Pullum,
2002). Perfect tenses are feasible with stative
clauses, yet less frequent. Simple present is only
feasible with states and not with events (Huddle-
ston and Pullum, 2002) (see examples 4a and
4b).
</bodyText>
<listItem confidence="0.905619">
(4a) She likes writing.
(4b) *She writes a book. (e.g. now)
</listItem>
<bodyText confidence="0.998781">
In the fine-grained dataset this information was
expressed using 3 features with 7 possible values
</bodyText>
<page confidence="0.991443">
58
</page>
<bodyText confidence="0.999945642857143">
(whether a clause is in present, past or future
tense, whether it is progressive and whether it is
perfective). In the coarse-grained dataset, this
information was expressed using 1 binary fea-
ture: whether a clause is in simple past or present
tense.
Temporal expressions. Temporal markers
(often referred to as temporal adverbials), such as
usually, never, suddenly, at that moment and
many others are widely employed to mark the
aspectual type of a sentence (Dowty, 1982;
Harkness, 1987; By, 2002). Such markers pro-
vide a wealth of information and often unambi-
guously signal aspectual type. For example:
</bodyText>
<listItem confidence="0.829486666666667">
(5a) She read a lot tonight.
(5b) She always read a lot. (Or She used to
read a lot.)
</listItem>
<bodyText confidence="0.999854136363636">
Yet, such expressions are not easy to capture
automatically. In order to use the information
expressed in temporal adverbials, the author ana-
lyzed the training data for presence of such ex-
pressions and found 295 occurrences in 10 sto-
ries. It appears that this set could be reduced to
95 templates in the following manner. For exam-
ple, the expressions this year, next year, that long
year could all be reduced to a template
&lt;some_expression&gt; year. Each template is char-
acterized by 3 features: type of the temporal ex-
pression (location, duration, frequency, enact-
ment) (Harkness, 1987); magnitude (year, day,
etc.); and plurality (year vs. years). The fine-
grained dataset contained 3 such features with 14
possible values (type of expression, its magni-
tude and plurality). The coarse-grained dataset
contained 1 binary feature (whether there was an
expression of a long period of time).
Verbal semantics. Inherent meaning of a verb
also influences the aspectual type of a given
clause.
</bodyText>
<listItem confidence="0.993294666666667">
(6a) She memorized that book by heart. (an
event)
(6b) She enjoyed that book. (a state)
</listItem>
<bodyText confidence="0.999750105263158">
Not surprisingly, this information is very difficult
to capture automatically. Hoping to leverage it,
the author used semantic categorization of the
3,000 most common English verbs as described
in (Levin, 1993). The fine-grained dataset con-
tained a feature with 49 possible values that cor-
responded to the top-level categories described in
(Levin, 1993). The coarse-grained dataset con-
tained 1 binary feature that carried this informa-
tion. Verbs that belong to more than one category
were manually assigned to a single category that
best captured their literal meaning.
Voice. Usually, clauses in passive voice only
occur with events (Siegel, 1998). Both datasets
contained 1 binary feature to describe this infor-
mation.
Properties of direct object. For some verbs
properties of direct object help determine
whether a given clause is stative or dynamic.
</bodyText>
<listItem confidence="0.642071">
(7a) She wrote a book. (event)
(7b) She wrote books. (state)
</listItem>
<bodyText confidence="0.986018285714286">
The fine-grained dataset contained 2 binary fea-
tures to describe whether direct object is definite
or indefinite and whether it is plural. The coarse-
grained dataset contained no such information
because it appeared that this information was not
crucial.
Several additional features were present in
both datasets that described overall characteris-
tics of a clause and its parent sentence, such as
whether these were affirmative, their index in the
text, etc. The fine-grained dataset contained 4
such features with 9 possible values and the
coarse-grained dataset contained 3 features with
7 values.
</bodyText>
<sectionHeader confidence="0.999842" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998008">
4.1 Experimental setting
</subsectionHeader>
<bodyText confidence="0.999985086956522">
The data used in the experiments consisted of 23
stories split into a training set (14 stories) and a
testing set (9 stories). Each clause of every story
was annotated by the author of this paper as
summary-worthy or not. Therefore, the classifi-
cation process occurred at the clause-level. Yet,
summary construction occurred at the sentence-
level, that is if one clause in a sentence was con-
sidered summary-worthy, the whole sentence
was also considered summary-worthy. Because
of this, results are reported at two levels: clause
and sentence. The results at the clause-level are
more appropriate to judge the accuracy of the
classification process. The results at the sentence
level are better suited for giving an idea about
how close the produced summaries are to their
annotated counterparts.
The training set contained 5,514 clauses and
the testing set contained 4,196 clauses. The target
compression rate was set at 6% expressed in
terms of sentences. This rate was selected be-
cause it approximately corresponds to the aver-
age compression rate achieved by the annotator
</bodyText>
<page confidence="0.997371">
59
</page>
<bodyText confidence="0.999853173913043">
(5.62%). The training set consisted of 310 posi-
tive examples and 5,204 negative examples, and
the testing set included 218 positive and 3,978
negative examples.
Before describing the experiments and dis-
cussing results, it is useful to define baselines.
The author of this paper is not familiar with any
comparable summarization experiments and for
this reason was unable to use existing work for
comparison. Therefore, a baseline needed to be
defined in different terms. To this end, two naïve
baselines were computed.
Intuitively, when a person wishes to decide
whether to read a certain book or not, he opens it
and flips through several pages at the beginning.
Imitating this process, a simple lead baseline
consisting of the first 6% of the sentences in a
story was computed. It is denoted LEAD in Ta-
bles 3 and 4. The second baseline is a slightly
modified version of the lead baseline and it con-
sists of the first 6% of the sentences that contain
at least one mention of one of the important
characters. It is denoted LEAD CHAR in Tables
</bodyText>
<sectionHeader confidence="0.657244" genericHeader="method">
3 and 4.
</sectionHeader>
<subsectionHeader confidence="0.998307">
4.2 Experiments with the rules
</subsectionHeader>
<bodyText confidence="0.999828352941177">
The first classification procedure consisted of
applying a set of manually designed rules to pro-
duce descriptive summaries. The rules were de-
signed using the same features that were used for
machine learning and that are described in Sec-
tion 3.3.
Two sets of rules were created: one for the
fine-grained dataset and another for the coarse-
grained dataset. Due to space restrictions it is not
possible to reproduce the rules in this paper. Yet,
several examples are given in Figure 4. (If a rule
returns True, then a clause is considered to be
summary-worthy.)
The results obtained using these rules are pre-
sented in Table 3. They are discussed along with
the results obtained using machine learning in
Section 4.4.
</bodyText>
<subsectionHeader confidence="0.999554">
4.3 Experiments with machine learning
</subsectionHeader>
<bodyText confidence="0.999979125">
As an alternative to rule construction, the author
used C5.0 (Quilan, 1992) implementation of de-
cision trees to select descriptive sentences. The
algorithm was chosen mainly because of the
readability of its output. Both training and testing
datasets exhibited a 1:18 class imbalance, which,
given a small size of the datasets, needed to be
compensated. Undersampling (randomly remov-
ing instances of the majority class) was applied
to both datasets in order to correct class imbal-
ance.
This yielded altogether 4 different datasets
(see Table 4). For each dataset, the best model
was selected using 10-fold cross-validation on
the training set. The model was then tested on the
testing set and the results are reported in Table 4.
</bodyText>
<figureCaption confidence="0.8466325">
Figure 4. Examples of manually composed
rules.
</figureCaption>
<table confidence="0.91491444">
Rule 1
if a clause contains a character mention as
subject or object and a temporal expression
of type enactment (ever, never, always)
return True
Rule 2
if a clause contains a character mention as
subject or object and a stative verb
return True
Rule 3
if a clause is in progressive tense
return False
Table 3. Results obtained using rules (summary-worthy class)
Dataset Level Preci- Recall, F-score Kappa Overall error rate,%
sion,% % ,% (both classes)
Baseline LEAD Clause 19.92 23.39 21.52 16.85 8.87
Baseline Clause 8.93 25.69 13.25 6.01 17.47
LEAD CHAR
Fine-grained Clause 34.77 40.83 37.55 33.84 17.73
Coarse-grained Clause 32.00 47.71 38.31 34.21 7.98
Baseline LEAD Sent. 23.57 24.18 23.87 19.00 9.24
Baseline Sent. 22.93 23.53 23.23 18.31 9.24
LEAD CHAR
Fine-grained Sent. 41.40 42.48 41.94 38.22 6.99
Coarse-grained Sent. 40.91 41.18 41.04 37.31 7.03
</table>
<page confidence="0.992663">
60
</page>
<sectionHeader confidence="0.712897" genericHeader="evaluation">
4.4 Results
</sectionHeader>
<bodyText confidence="0.999969215686275">
The results displayed in Tables 3 and 4 show
how many clauses (and sentences) selected by
the system corresponded to those chosen by the
annotator. The columns Precision, Recall and F-
score show measures for the minority class (sum-
mary-worthy). The columns Overall error rate
and Kappa show measures for both classes.
Although modest, the results suggest an im-
provement over both baselines. Statistical sig-
nificance of improvements over baselines was
tested for p = 0.001 for each dataset-approach.
The improvements are significant in all cases.
The columns F-score in Tables 3 and 4 show
f-score for the minority class (summary-worthy
sentences), which is a measure combining preci-
sion and recall for this class. Yet, this measure
does not take into account success rate on the
negative class. For this reason, Cohen’s kappa
statistic (Cohen, 1960) was also computed. It
measures the overall agreement between the sys-
tem and the annotator. This measure is shown in
the column named Kappa.
In order to see what features were the most in-
formative in each dataset, a small experiment
was conducted. The author removed one feature
at a time from the training set and used the de-
crease in F-score as a measure of informative-
ness. The experiment revealed that in the coarse-
grained dataset the following features were the
most informative: 1) the position of a sentence
relative to the first mention of a character; 2)
whether a clause contained character mentions;
3) voice and 4) tense. In the fine-grained dataset
the findings were similar: 1) presence of a char-
acter mention; 2) position of a sentence in the
text; 3) voice; and 4) tense were more important
than the other features.
It is not easy to interpret these results in any
conclusive way at this stage. The main weakness,
of course, is that the results are based solely on
the annotations of one person while it is gener-
ally known that human annotators are likely to
exhibit some disagreement. The second issue lies
in the fact that given the compression rate of 6%,
and the objective that the summary be indicative
and not informative, more that one ‘good’ sum-
mary is possible. It would therefore be desirable
that the results be evaluated not based on overlap
with an annotator (or annotators, for that matter),
but on how well they achieve the stated objec-
tive.
</bodyText>
<sectionHeader confidence="0.999717" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999926333333333">
In the immediate future the inconclusiveness of
the results will be addressed by means of asking
human judges to evaluate the produced summa-
ries. During this process the author hopes to find
out how informative the produced summaries are
and how well they achieve the stated objective
(help readers decide whether a story is poten-
tially interesting to them). The judges will also
be asked to annotate their own version of a sum-
mary to explore what inter-judge agreement
means in the context of fiction summarization.
More remote plans include possibly tackling
the problem of summarizing the plot and dealing
more closely with the problem of evaluation in
the context of fiction summarization.
</bodyText>
<tableCaption confidence="0.993885">
Table 4. Results obtained using machine learning (summary-worthy class)
</tableCaption>
<table confidence="0.995381176470588">
Dataset Training data- Level Preci- Recall, F-score, Kap- Overall
set sion, % % % pa error rate,
%
Baseline LEAD Clause 19.92 23.39 21.52 16.85 8.87
Baseline Clause 8.93 25.69 13.25 6.01 17.47
LEAD CHAR
Fine-grained original Clause 28.81 31.19 29.96 25.96 7.58
Fine-grained undersampled Clause 39.06 45.87 42.19 38.76 6.53
Coarse-grained original Clause 34.38 30.28 32.22 28.73 6.63
Coarse-grained undersampled Clause 28.52 33.49 30.80 26.69 7.82
Baseline LEAD Sent. 23.57 24.18 23.87 19.00 9.24
Baseline Sent. 22.93 23.53 23.23 18.31 9.24
LEAD CHAR
Fine-grained original Sent. 38.93 37.91 38.41 34.57 7.22
Fine-grained undersampled Sent. 41.4 42.48 41.94 38.22 6.99
Coarse-grained original Sent. 42.19 35.29 38.43 34.91 6.72
Coarse-grained undersampled Sent. 37.58 38.56 38.06 34.10 7.46
</table>
<page confidence="0.997656">
61
</page>
<sectionHeader confidence="0.996531" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9997624">
The author would like to express her gratitude
to Connexor Oy and especially to Atro Vouti-
lainen for their kind permission to use Connexor
Machinese Syntax parser free of charge for re-
search purposes.
</bodyText>
<sectionHeader confidence="0.999174" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999824606741573">
Tomas By. 2002. Tears in the Rain. Ph.D. thesis, Uni-
versity of Sheffield.
Eugene Charniak. 1972. Toward a Model of Chil-
dren’s Story Comprehension. Ph.D. thesis, Massa-
chusetts Institute of Technology.
Jacob Cohen. 1960. A coefficient of agreement for
nominal scales, Educational and Psychological
Measurement, (20): 37–46.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, Valentin Tablan. 2002. GATE: A
Framework and Graphical Development Environ-
ment for Robust NLP Tools and Applications. Pro-
ceedings of the 40th Anniversary Meeting of the
Association for Computational Linguistics
(ACL&apos;02), Philadelphia.
David Dowty. 1982. Tense, Time Adverbs, and Com-
positional Semantic Theory. Linguistics and Phi-
losophy, (5), p. 23-59.
David Dowty. 1979. Word Meaning and Montague
Grammar. D. Reidel Publishing Company,
Dordrecht.
Noemie Elhadad, Min-Yen Kan, Judith Klavans, and
Kathleen McKeown. 2005. Customization in a uni-
fied framework for summarizing medical literature.
Artificial Intelligence in Medicine 33(2): 179-198.
Janet Harkness. 1987. Time Adverbials in English and
Reference Time. In Alfred Schopf (ed.), Essays on
Tensing in English, Vol. I: Reference Time, Tense
and Adverbs, p. 71-110. Tübingen: Max Niemeyer.
Rodney Huddleston and Geoffrey Pullum. 2002. The
Cambridge Grammar of the English Language Us-
age, p. 74-210. Cambridge University Press.
Konstantinos Koumpis, Steve Renals, and Mahesan
Niranjan. 2001. Extractive summarization of
voicemail using lexical and prosodic feature subset
selection. In Proeedings of Eurospeech, p. 2377–
2380, Aalborg, Denmark.
Herbert Leass and Shalom Lappin. 1994. An algo-
rithm for Pronominal Anaphora Resolution. Com-
putational Linguistics, 20(4): 535-561.
Wendy Lehnert. 1982. Plot Units: A Narrative Sum-
marization Strategy. In W. Lehnert and M. Ringle
(eds.). Strategies for Natural Language Processing.
Erlbaum, Hillsdale, NJ.
Beth Levin. 1993. English Verb Classes and Alterna-
tions. The University of Chicago Press.
Longman Dictionary of Contemporary English. 2002.
Pearson Education.
Inderjeet Mani, Eric Bloedorn and Barbara Gates.
1998. Using Cohesion and Coherence Models for
Text Summarization. In Working Notes of the
Workshop on Intelligent Text Summarization, p.
69-76. Menlo Park, California: American Associa-
tion for Artificial Intelligence Spring Simposium
Series.
Daniel Marcu. 1997. The Rhetorical Parsing, Summa-
rization, and Generation of Natural Language
Texts. PhD Thesis, Department of Computer Sci-
ence, University of Toronto.
Paola Merlo, Suzanne Stevenson, Vivian Tsang and
Gianluca Allaria. 2002. A Multilingual Paradigm
for Automatic Verb Classification. Proceedings of
the 40th Annual Meeting of the Association for
Computational Linguistics.(ACL’02), Philadelphia.
Massimo Poesio and Renata Vieira. 2000 . An Em-
pirically Based System for Processing Definite De-
scriptions. Computational Linguistics, 26(4): 525-
579.
J. Ross Quinlan, 1992: C4.5: Programs for Machine
Learning. Morgan Kaufmann Pub., San Mateo,
CA.
Eric V. Siegel. 1998. Linguistic Indicators for Lan-
guage Understanding: Using machine learning
methods to combine corpus-based indicators for
aspectual classification of clauses. Ph.D. Disserta-
tion. Columbia University.
Pasi Tapanainen and Timo Järvinen. 1997 A non-
projective dependency parser. In Proceedings of
the 5th Conference on Applied Natural Language
Processing, p. 64-71.
Simone Teufel and Marc Moens. 2002. Summarizing
scientific articles-experiments with relevance and
rhetorical status. Computational Linguistics, 28(4):
409–445.
Zeno Vendler. 1967. Linguistics in Philosophy. Cor-
nell University Press, p. 97- 145.
Klaus Zechner. 2002. Automatic Summarization of
Open-Domain Multiparty Dialogues in Diverse
Genres. Computational Linguistics 28(4):447-485.
</reference>
<page confidence="0.999183">
62
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.937680">
<title confidence="0.999614">An Approach to Summarizing Short Stories</title>
<author confidence="0.999885">Anna Kazantseva</author>
<affiliation confidence="0.992901">The School of Information Technology and Engineering University of Ottawa</affiliation>
<email confidence="0.98768">ankazant@site.uottawa.ca</email>
<abstract confidence="0.998245476190476">This paper describes a system that produces extractive summaries of short works of literary fiction. The ultimate purpose of produced summaries is defined as helping a reader to determine whether she would be interested in reading a particular story. To this end, the summary aims to provide a reader with an idea about the settings of a story (such as characters, time and place) without revealing the plot. The approach presented here relies heavily on the notion of aspect. Preliminary results show an improvement over two naïve baselines: a lead baseline and a more sophisticated variant of it. Although modest, the results suggest that using aspectual information may be of help when summarizing fiction. A more thorough evaluation involving human judges is under way.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Tomas By</author>
</authors>
<title>Tears in the Rain.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Sheffield.</institution>
<contexts>
<context position="16616" citStr="By, 2002" startWordPosition="2713" endWordPosition="2714">g. now) In the fine-grained dataset this information was expressed using 3 features with 7 possible values 58 (whether a clause is in present, past or future tense, whether it is progressive and whether it is perfective). In the coarse-grained dataset, this information was expressed using 1 binary feature: whether a clause is in simple past or present tense. Temporal expressions. Temporal markers (often referred to as temporal adverbials), such as usually, never, suddenly, at that moment and many others are widely employed to mark the aspectual type of a sentence (Dowty, 1982; Harkness, 1987; By, 2002). Such markers provide a wealth of information and often unambiguously signal aspectual type. For example: (5a) She read a lot tonight. (5b) She always read a lot. (Or She used to read a lot.) Yet, such expressions are not easy to capture automatically. In order to use the information expressed in temporal adverbials, the author analyzed the training data for presence of such expressions and found 295 occurrences in 10 stories. It appears that this set could be reduced to 95 templates in the following manner. For example, the expressions this year, next year, that long year could all be reduce</context>
</contexts>
<marker>By, 2002</marker>
<rawString>Tomas By. 2002. Tears in the Rain. Ph.D. thesis, University of Sheffield.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Toward a Model of Children’s Story Comprehension.</title>
<date>1972</date>
<tech>Ph.D. thesis,</tech>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="1954" citStr="Charniak, 1972" startWordPosition="306" endWordPosition="307"> text genres: scientific and popular scientific articles (Marcu, 1997; Mani et al., 1998), texts in computational linguistics (Teufel and Moens, 2002), and medical texts (Elhadad et al., 2002). All these genres are examples of texts characterized by rigid structure, relative abundance of surface markers and straightforwardness. Relatively few attempts have been made at summarizing less structured genres, some of them being dialogue and speech summarization (Zechner, 2002; Koumpis et al. 2001). The issue of summarizing fiction remains largely untouched, since a few very thorough earlier works (Charniak, 1972; Lehnert, 1982). The work presented here seeks to fill in this gap. The ultimate objective of the project is stated as follows: to produce indicative summaries of short works of fiction such that they be helpful to a potential reader in deciding whether she would be interested in reading a particular story or not. To this end, revealing the plot was deemed unnecessary and even undesirable. Instead, the current approach relies on the following assumption: when a reader is presented with an extracted summary outlining the general settings of a story (such as time, place and who it is about), sh</context>
</contexts>
<marker>Charniak, 1972</marker>
<rawString>Eugene Charniak. 1972. Toward a Model of Children’s Story Comprehension. Ph.D. thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales,</title>
<date>1960</date>
<journal>Educational and Psychological Measurement,</journal>
<volume>20</volume>
<pages>37--46</pages>
<contexts>
<context position="24849" citStr="Cohen, 1960" startWordPosition="4054" endWordPosition="4055">orthy). The columns Overall error rate and Kappa show measures for both classes. Although modest, the results suggest an improvement over both baselines. Statistical significance of improvements over baselines was tested for p = 0.001 for each dataset-approach. The improvements are significant in all cases. The columns F-score in Tables 3 and 4 show f-score for the minority class (summary-worthy sentences), which is a measure combining precision and recall for this class. Yet, this measure does not take into account success rate on the negative class. For this reason, Cohen’s kappa statistic (Cohen, 1960) was also computed. It measures the overall agreement between the system and the annotator. This measure is shown in the column named Kappa. In order to see what features were the most informative in each dataset, a small experiment was conducted. The author removed one feature at a time from the training set and used the decrease in F-score as a measure of informativeness. The experiment revealed that in the coarsegrained dataset the following features were the most informative: 1) the position of a sentence relative to the first mention of a character; 2) whether a clause contained character</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. A coefficient of agreement for nominal scales, Educational and Psychological Measurement, (20): 37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hamish Cunningham</author>
<author>Diana Maynard</author>
<author>Kalina Bontcheva</author>
<author>Valentin Tablan</author>
</authors>
<title>GATE: A Framework and Graphical Development Environment for Robust NLP Tools and Applications.</title>
<date>2002</date>
<booktitle>Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics (ACL&apos;02),</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="5991" citStr="Cunningham et al., 2002" startWordPosition="965" endWordPosition="968">ovides the linguistic motivation behind it. Section 4 describes the classification procedures (machine learning and manual rule creation) used to distinguish between descriptive elements of a story and passages that describe events. It also reports results. Section 5 draws some conclusions and outlines possible directions in which this work may evolve. 2 Data Pre-Processing Before working on selecting salient descriptive sentences, the stories of the training set were analyzed for presence of surface markers denoting characters, locations and temporal anchors. To this end, the GATE Gazetteer (Cunningham et al., 2002) was used, and only entities recognized by it automatically were considered. The findings were as follows. Each story contained multiple mentions of characters (an average of 64 mentions per story). Yet only 22 location markers were found, most of these being street names. The 22 markers were found in 10 out of 14 stories, leaving 4 stories without any identifiable location markers. Only 4 temporal anchors were identified in all 14 stories: 2 absolute (such as years) and 2 relative (names of holidays). These findings support the intuitive idea that short stories revolve around their characters</context>
</contexts>
<marker>Cunningham, Maynard, Bontcheva, Tablan, 2002</marker>
<rawString>Hamish Cunningham, Diana Maynard, Kalina Bontcheva, Valentin Tablan. 2002. GATE: A Framework and Graphical Development Environment for Robust NLP Tools and Applications. Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics (ACL&apos;02), Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Dowty</author>
</authors>
<title>Tense, Time Adverbs, and Compositional Semantic Theory.</title>
<date>1982</date>
<journal>Linguistics and Philosophy,</journal>
<volume>5</volume>
<pages>23--59</pages>
<contexts>
<context position="16589" citStr="Dowty, 1982" startWordPosition="2709" endWordPosition="2710"> (4b) *She writes a book. (e.g. now) In the fine-grained dataset this information was expressed using 3 features with 7 possible values 58 (whether a clause is in present, past or future tense, whether it is progressive and whether it is perfective). In the coarse-grained dataset, this information was expressed using 1 binary feature: whether a clause is in simple past or present tense. Temporal expressions. Temporal markers (often referred to as temporal adverbials), such as usually, never, suddenly, at that moment and many others are widely employed to mark the aspectual type of a sentence (Dowty, 1982; Harkness, 1987; By, 2002). Such markers provide a wealth of information and often unambiguously signal aspectual type. For example: (5a) She read a lot tonight. (5b) She always read a lot. (Or She used to read a lot.) Yet, such expressions are not easy to capture automatically. In order to use the information expressed in temporal adverbials, the author analyzed the training data for presence of such expressions and found 295 occurrences in 10 stories. It appears that this set could be reduced to 95 templates in the following manner. For example, the expressions this year, next year, that lo</context>
</contexts>
<marker>Dowty, 1982</marker>
<rawString>David Dowty. 1982. Tense, Time Adverbs, and Compositional Semantic Theory. Linguistics and Philosophy, (5), p. 23-59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Dowty</author>
</authors>
<title>Word Meaning and</title>
<date>1979</date>
<publisher>Reidel Publishing Company,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="15056" citStr="Dowty, 1979" startWordPosition="2457" endWordPosition="2458">was calculated. It must be mentioned that several researchers looked into determining automatically various semantic properties of verbs, such as (Siegel, 1998; Merlo et al., 2002). Yet these approaches dealt with properties of verbs in general and not with particular usages in the context of concrete sentences. Default verbal aspect. A set of verbs, referred to as stative verbs, tends to produce mostly stative clauses. Examples of such verbs include be, like, feel, love, hate and many others. A common property of such verbs is that they do not readily yield a progressive form (Vendler, 1967; Dowty, 1979). Consider examples 3a and 3b. (3a) She is talking. (a dynamic verb talk) (3b) *She is liking the book. (a stative verb like) The default aspectual category of a verb was approximated using Longman Dictionary of Contemporary English (LDOCE). Verbs marked in LDOCE as not having a progressive form were considered stative and all others – dynamic. This information was expressed in both datasets as 1 binary feature. Grammatical tense. Usually, simple tenses are more likely to be used in stative or habitual situations than progressive or perfect tenses. In fact, it is considered to be a property of</context>
</contexts>
<marker>Dowty, 1979</marker>
<rawString>David Dowty. 1979. Word Meaning and Montague Grammar. D. Reidel Publishing Company, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noemie Elhadad</author>
<author>Min-Yen Kan</author>
<author>Judith Klavans</author>
<author>Kathleen McKeown</author>
</authors>
<title>Customization in a unified framework for summarizing medical literature.</title>
<date>2005</date>
<journal>Artificial Intelligence in Medicine</journal>
<volume>33</volume>
<issue>2</issue>
<pages>179--198</pages>
<contexts>
<context position="1283" citStr="Elhadad et al., 2005" startWordPosition="205" endWordPosition="208">show an improvement over two naïve baselines: a lead baseline and a more sophisticated variant of it. Although modest, the results suggest that using aspectual information may be of help when summarizing fiction. A more thorough evaluation involving human judges is under way. 1 Introduction In the course of recent years the scientific community working on the problem of automatic text summarization has been experiencing an upsurge. A multitude of different techniques has been applied to this end, some of the more remarkable of them being (Marcu, 1997; Mani et al. 1998; Teufel and Moens, 2002; Elhadad et al., 2005), to name just a few. These researchers worked on various text genres: scientific and popular scientific articles (Marcu, 1997; Mani et al., 1998), texts in computational linguistics (Teufel and Moens, 2002), and medical texts (Elhadad et al., 2002). All these genres are examples of texts characterized by rigid structure, relative abundance of surface markers and straightforwardness. Relatively few attempts have been made at summarizing less structured genres, some of them being dialogue and speech summarization (Zechner, 2002; Koumpis et al. 2001). The issue of summarizing fiction remains lar</context>
</contexts>
<marker>Elhadad, Kan, Klavans, McKeown, 2005</marker>
<rawString>Noemie Elhadad, Min-Yen Kan, Judith Klavans, and Kathleen McKeown. 2005. Customization in a unified framework for summarizing medical literature. Artificial Intelligence in Medicine 33(2): 179-198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janet Harkness</author>
</authors>
<title>Time Adverbials in English and Reference Time.</title>
<date>1987</date>
<booktitle>Essays on Tensing in English, Vol. I: Reference Time, Tense and Adverbs,</booktitle>
<pages>71--110</pages>
<editor>In Alfred Schopf (ed.),</editor>
<location>Tübingen: Max Niemeyer.</location>
<contexts>
<context position="16605" citStr="Harkness, 1987" startWordPosition="2711" endWordPosition="2712">ites a book. (e.g. now) In the fine-grained dataset this information was expressed using 3 features with 7 possible values 58 (whether a clause is in present, past or future tense, whether it is progressive and whether it is perfective). In the coarse-grained dataset, this information was expressed using 1 binary feature: whether a clause is in simple past or present tense. Temporal expressions. Temporal markers (often referred to as temporal adverbials), such as usually, never, suddenly, at that moment and many others are widely employed to mark the aspectual type of a sentence (Dowty, 1982; Harkness, 1987; By, 2002). Such markers provide a wealth of information and often unambiguously signal aspectual type. For example: (5a) She read a lot tonight. (5b) She always read a lot. (Or She used to read a lot.) Yet, such expressions are not easy to capture automatically. In order to use the information expressed in temporal adverbials, the author analyzed the training data for presence of such expressions and found 295 occurrences in 10 stories. It appears that this set could be reduced to 95 templates in the following manner. For example, the expressions this year, next year, that long year could al</context>
</contexts>
<marker>Harkness, 1987</marker>
<rawString>Janet Harkness. 1987. Time Adverbials in English and Reference Time. In Alfred Schopf (ed.), Essays on Tensing in English, Vol. I: Reference Time, Tense and Adverbs, p. 71-110. Tübingen: Max Niemeyer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodney Huddleston</author>
<author>Geoffrey Pullum</author>
</authors>
<title>The Cambridge Grammar of the English Language Usage,</title>
<date>2002</date>
<pages>74--210</pages>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="9465" citStr="Huddleston and Pullum, 2002" startWordPosition="1558" endWordPosition="1561">597 507 90 15.07 Nominal 152 96 56 36.84 Both 749 603 146 19.49 A small-scale evaluation based on 2 short stories revealed results shown in Table 1. After resolving anaphoric expressions, characters that are central to the story were selected based on normalized frequency counts. 3 Selecting Descriptive Sentences Using Aspectual Information 3.1 Linguistic definition of aspect In order to select salient sentences that set out the background of a story, this project relied on the notion of aspect. For the purposes of this paper the author uses the term aspect to denote the same concept as what (Huddleston and Pullum, 2002) call the situation type. Informally, it can be explained as a characteristic of a clause that gives an idea about the temporal flow of an event or state being described. A general hierarchy of aspectual classification based on (Huddleston and Pullum, 2002) is shown in Figure 2 with examples for each type. In addition, aspectual type of a clause may be altered by multiplicity, e.g. repetitions. Consider examples 2a and 2b. (2a) She read a book. (2b) She usually read a book a day. (e.g. She used to read a book a day). Example 2b is referred to as serial situation (Huddleston and Pullum, 2002). </context>
<context position="12252" citStr="Huddleston and Pullum, 2002" startWordPosition="2002" endWordPosition="2006">tested on each of the representations. The first approach used machine learning techniques, namely C5.0 (Quinlan, 1992) implementation of decision trees. The second approach consisted of applying a set of manually created rules that guided the classification process. Motivation for features used in each dataset is given in Section 3.3. Both approaches and preliminary results are discussed in Sections 4.1 - 4.4. The part of the system responsible for selecting descriptive sentences was implemented in Python. 3.3 Feature selection: description and motivation Figure 2. Aspectual hierarchy after (Huddleston and Pullum, 2002). 57 Table 2. Description of the features in both datasets Fine-grained dataset Coarse-grained dataset Type of features Number of fea- Number of val- Number of fea- Number of values tures ues tures Character-related 9 16 4 6 Aspect-related 12 92 8 16 Location-related 2 4 2 4 Others 4 9 3 4 All 27 121 17 30 Features for both representations were selected based on one of the following criteria: (Criterion 1) a clause should ‘talk’ about important things, such as characters or locations (Criterion 2) a clause should contain background descriptions rather then events The number of features providi</context>
<context position="15764" citStr="Huddleston and Pullum, 2002" startWordPosition="2573" endWordPosition="2576">She is liking the book. (a stative verb like) The default aspectual category of a verb was approximated using Longman Dictionary of Contemporary English (LDOCE). Verbs marked in LDOCE as not having a progressive form were considered stative and all others – dynamic. This information was expressed in both datasets as 1 binary feature. Grammatical tense. Usually, simple tenses are more likely to be used in stative or habitual situations than progressive or perfect tenses. In fact, it is considered to be a property of stative clauses that they normally do not occur in progressive (Vendler, 1967; Huddleston and Pullum, 2002). Perfect tenses are feasible with stative clauses, yet less frequent. Simple present is only feasible with states and not with events (Huddleston and Pullum, 2002) (see examples 4a and 4b). (4a) She likes writing. (4b) *She writes a book. (e.g. now) In the fine-grained dataset this information was expressed using 3 features with 7 possible values 58 (whether a clause is in present, past or future tense, whether it is progressive and whether it is perfective). In the coarse-grained dataset, this information was expressed using 1 binary feature: whether a clause is in simple past or present ten</context>
</contexts>
<marker>Huddleston, Pullum, 2002</marker>
<rawString>Rodney Huddleston and Geoffrey Pullum. 2002. The Cambridge Grammar of the English Language Usage, p. 74-210. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Konstantinos Koumpis</author>
<author>Steve Renals</author>
<author>Mahesan Niranjan</author>
</authors>
<title>Extractive summarization of voicemail using lexical and prosodic feature subset selection.</title>
<date>2001</date>
<booktitle>In Proeedings of Eurospeech,</booktitle>
<pages>2377--2380</pages>
<location>Aalborg, Denmark.</location>
<contexts>
<context position="1837" citStr="Koumpis et al. 2001" startWordPosition="287" endWordPosition="290">7; Mani et al. 1998; Teufel and Moens, 2002; Elhadad et al., 2005), to name just a few. These researchers worked on various text genres: scientific and popular scientific articles (Marcu, 1997; Mani et al., 1998), texts in computational linguistics (Teufel and Moens, 2002), and medical texts (Elhadad et al., 2002). All these genres are examples of texts characterized by rigid structure, relative abundance of surface markers and straightforwardness. Relatively few attempts have been made at summarizing less structured genres, some of them being dialogue and speech summarization (Zechner, 2002; Koumpis et al. 2001). The issue of summarizing fiction remains largely untouched, since a few very thorough earlier works (Charniak, 1972; Lehnert, 1982). The work presented here seeks to fill in this gap. The ultimate objective of the project is stated as follows: to produce indicative summaries of short works of fiction such that they be helpful to a potential reader in deciding whether she would be interested in reading a particular story or not. To this end, revealing the plot was deemed unnecessary and even undesirable. Instead, the current approach relies on the following assumption: when a reader is presen</context>
</contexts>
<marker>Koumpis, Renals, Niranjan, 2001</marker>
<rawString>Konstantinos Koumpis, Steve Renals, and Mahesan Niranjan. 2001. Extractive summarization of voicemail using lexical and prosodic feature subset selection. In Proeedings of Eurospeech, p. 2377– 2380, Aalborg, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Leass</author>
<author>Shalom Lappin</author>
</authors>
<title>An algorithm for Pronominal Anaphora Resolution.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>4</issue>
<pages>535--561</pages>
<marker>Leass, Lappin, 1994</marker>
<rawString>Herbert Leass and Shalom Lappin. 1994. An algorithm for Pronominal Anaphora Resolution. Computational Linguistics, 20(4): 535-561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wendy Lehnert</author>
</authors>
<title>Plot Units: A Narrative Summarization Strategy.</title>
<date>1982</date>
<booktitle>Strategies for Natural Language Processing.</booktitle>
<editor>In W. Lehnert and M. Ringle (eds.).</editor>
<publisher>Erlbaum,</publisher>
<location>Hillsdale, NJ.</location>
<contexts>
<context position="1970" citStr="Lehnert, 1982" startWordPosition="308" endWordPosition="309">ientific and popular scientific articles (Marcu, 1997; Mani et al., 1998), texts in computational linguistics (Teufel and Moens, 2002), and medical texts (Elhadad et al., 2002). All these genres are examples of texts characterized by rigid structure, relative abundance of surface markers and straightforwardness. Relatively few attempts have been made at summarizing less structured genres, some of them being dialogue and speech summarization (Zechner, 2002; Koumpis et al. 2001). The issue of summarizing fiction remains largely untouched, since a few very thorough earlier works (Charniak, 1972; Lehnert, 1982). The work presented here seeks to fill in this gap. The ultimate objective of the project is stated as follows: to produce indicative summaries of short works of fiction such that they be helpful to a potential reader in deciding whether she would be interested in reading a particular story or not. To this end, revealing the plot was deemed unnecessary and even undesirable. Instead, the current approach relies on the following assumption: when a reader is presented with an extracted summary outlining the general settings of a story (such as time, place and who it is about), she will have enou</context>
</contexts>
<marker>Lehnert, 1982</marker>
<rawString>Wendy Lehnert. 1982. Plot Units: A Narrative Summarization Strategy. In W. Lehnert and M. Ringle (eds.). Strategies for Natural Language Processing. Erlbaum, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations.</title>
<date>1993</date>
<publisher>The University of Chicago Press.</publisher>
<contexts>
<context position="18092" citStr="Levin, 1993" startWordPosition="2956" endWordPosition="2957">contained 3 such features with 14 possible values (type of expression, its magnitude and plurality). The coarse-grained dataset contained 1 binary feature (whether there was an expression of a long period of time). Verbal semantics. Inherent meaning of a verb also influences the aspectual type of a given clause. (6a) She memorized that book by heart. (an event) (6b) She enjoyed that book. (a state) Not surprisingly, this information is very difficult to capture automatically. Hoping to leverage it, the author used semantic categorization of the 3,000 most common English verbs as described in (Levin, 1993). The fine-grained dataset contained a feature with 49 possible values that corresponded to the top-level categories described in (Levin, 1993). The coarse-grained dataset contained 1 binary feature that carried this information. Verbs that belong to more than one category were manually assigned to a single category that best captured their literal meaning. Voice. Usually, clauses in passive voice only occur with events (Siegel, 1998). Both datasets contained 1 binary feature to describe this information. Properties of direct object. For some verbs properties of direct object help determine wh</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English Verb Classes and Alternations. The University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<title>Longman Dictionary of Contemporary English.</title>
<date>2002</date>
<publisher>Pearson Education.</publisher>
<marker>2002</marker>
<rawString>Longman Dictionary of Contemporary English. 2002. Pearson Education.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>Eric Bloedorn</author>
<author>Barbara Gates</author>
</authors>
<title>Using Cohesion and Coherence Models for Text Summarization.</title>
<date>1998</date>
<booktitle>In Working Notes of the Workshop on Intelligent Text Summarization,</booktitle>
<pages>69--76</pages>
<publisher>American</publisher>
<location>Menlo Park, California:</location>
<contexts>
<context position="1236" citStr="Mani et al. 1998" startWordPosition="197" endWordPosition="200">the notion of aspect. Preliminary results show an improvement over two naïve baselines: a lead baseline and a more sophisticated variant of it. Although modest, the results suggest that using aspectual information may be of help when summarizing fiction. A more thorough evaluation involving human judges is under way. 1 Introduction In the course of recent years the scientific community working on the problem of automatic text summarization has been experiencing an upsurge. A multitude of different techniques has been applied to this end, some of the more remarkable of them being (Marcu, 1997; Mani et al. 1998; Teufel and Moens, 2002; Elhadad et al., 2005), to name just a few. These researchers worked on various text genres: scientific and popular scientific articles (Marcu, 1997; Mani et al., 1998), texts in computational linguistics (Teufel and Moens, 2002), and medical texts (Elhadad et al., 2002). All these genres are examples of texts characterized by rigid structure, relative abundance of surface markers and straightforwardness. Relatively few attempts have been made at summarizing less structured genres, some of them being dialogue and speech summarization (Zechner, 2002; Koumpis et al. 2001</context>
</contexts>
<marker>Mani, Bloedorn, Gates, 1998</marker>
<rawString>Inderjeet Mani, Eric Bloedorn and Barbara Gates. 1998. Using Cohesion and Coherence Models for Text Summarization. In Working Notes of the Workshop on Intelligent Text Summarization, p. 69-76. Menlo Park, California: American Association for Artificial Intelligence Spring Simposium Series.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The Rhetorical Parsing, Summarization, and Generation of Natural Language Texts.</title>
<date>1997</date>
<tech>PhD Thesis,</tech>
<institution>Department of Computer Science, University of Toronto.</institution>
<contexts>
<context position="1218" citStr="Marcu, 1997" startWordPosition="195" endWordPosition="196">s heavily on the notion of aspect. Preliminary results show an improvement over two naïve baselines: a lead baseline and a more sophisticated variant of it. Although modest, the results suggest that using aspectual information may be of help when summarizing fiction. A more thorough evaluation involving human judges is under way. 1 Introduction In the course of recent years the scientific community working on the problem of automatic text summarization has been experiencing an upsurge. A multitude of different techniques has been applied to this end, some of the more remarkable of them being (Marcu, 1997; Mani et al. 1998; Teufel and Moens, 2002; Elhadad et al., 2005), to name just a few. These researchers worked on various text genres: scientific and popular scientific articles (Marcu, 1997; Mani et al., 1998), texts in computational linguistics (Teufel and Moens, 2002), and medical texts (Elhadad et al., 2002). All these genres are examples of texts characterized by rigid structure, relative abundance of surface markers and straightforwardness. Relatively few attempts have been made at summarizing less structured genres, some of them being dialogue and speech summarization (Zechner, 2002; K</context>
</contexts>
<marker>Marcu, 1997</marker>
<rawString>Daniel Marcu. 1997. The Rhetorical Parsing, Summarization, and Generation of Natural Language Texts. PhD Thesis, Department of Computer Science, University of Toronto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Merlo</author>
<author>Suzanne Stevenson</author>
<author>Vivian Tsang</author>
<author>Gianluca Allaria</author>
</authors>
<title>A Multilingual Paradigm for Automatic Verb Classification.</title>
<date>2002</date>
<booktitle>Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.(ACL’02),</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="14624" citStr="Merlo et al., 2002" startWordPosition="2382" endWordPosition="2385">pect-related features were calculated. These features were selected so as to model characteristics of a clause that help determine its aspectual class. The characteristics used were default aspect of the main verb of a clause, tense, temporal expressions, semantic category of a verb, voice and some properties of the direct object. Each of these characteristics is listed below, along with motivation for it, and information about how it was calculated. It must be mentioned that several researchers looked into determining automatically various semantic properties of verbs, such as (Siegel, 1998; Merlo et al., 2002). Yet these approaches dealt with properties of verbs in general and not with particular usages in the context of concrete sentences. Default verbal aspect. A set of verbs, referred to as stative verbs, tends to produce mostly stative clauses. Examples of such verbs include be, like, feel, love, hate and many others. A common property of such verbs is that they do not readily yield a progressive form (Vendler, 1967; Dowty, 1979). Consider examples 3a and 3b. (3a) She is talking. (a dynamic verb talk) (3b) *She is liking the book. (a stative verb like) The default aspectual category of a verb w</context>
</contexts>
<marker>Merlo, Stevenson, Tsang, Allaria, 2002</marker>
<rawString>Paola Merlo, Suzanne Stevenson, Vivian Tsang and Gianluca Allaria. 2002. A Multilingual Paradigm for Automatic Verb Classification. Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.(ACL’02), Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
<author>Renata Vieira</author>
</authors>
<title>An Empirically Based System for Processing Definite Descriptions.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>4</issue>
<pages>525--579</pages>
<contexts>
<context position="8580" citStr="Poesio and Vieira, 2000" startWordPosition="1410" endWordPosition="1413">vided below. During the first step, the documents were parsed using Connexor Machinese Syntax parser. The parsed data was then forwarded to the Gazetteer in GATE, which recognized nouns denoting persons. The original version of the Gazetteer recognized only named entities and professions, but the Gazetteer was extended to include common animate nouns such as man, woman, etc. As the next step, an implementation based on a classical pronoun resolution algorithm (Lappin and Leass, 1994) was applied to the texts. Subsequently, anaphoric noun phrases were identified using the rules outlined 56 in (Poesio and Vieira, 2000). Finally, these anaphoric noun phrases were resolved using a modified version of (Lappin and Leass, 1994), adjusted to finding antecedents of nouns. Table 1. Results of anaphora resolution. Type of All Correct Incor- Error anaphora rect rate, % Pronominal 597 507 90 15.07 Nominal 152 96 56 36.84 Both 749 603 146 19.49 A small-scale evaluation based on 2 short stories revealed results shown in Table 1. After resolving anaphoric expressions, characters that are central to the story were selected based on normalized frequency counts. 3 Selecting Descriptive Sentences Using Aspectual Information </context>
</contexts>
<marker>Poesio, Vieira, 2000</marker>
<rawString>Massimo Poesio and Renata Vieira. 2000 . An Empirically Based System for Processing Definite Descriptions. Computational Linguistics, 26(4): 525-579.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ross Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1992</date>
<publisher>Morgan Kaufmann Pub.,</publisher>
<location>San Mateo, CA.</location>
<contexts>
<context position="11743" citStr="Quinlan, 1992" startWordPosition="1926" endWordPosition="1927"> these two representations was in the number of attributes and in the cardinality of the set of possible values, and not in how much and what kind of information they carried. For instance, the fine-grained dataset had 3 different features with 7 possible values to carry tenserelated information: tense, is-progressive and is-perfect, while the coarse-grained dataset carried only one binary feature, is-simple-past-or-present. Two different approaches for selecting descriptive sentences were tested on each of the representations. The first approach used machine learning techniques, namely C5.0 (Quinlan, 1992) implementation of decision trees. The second approach consisted of applying a set of manually created rules that guided the classification process. Motivation for features used in each dataset is given in Section 3.3. Both approaches and preliminary results are discussed in Sections 4.1 - 4.4. The part of the system responsible for selecting descriptive sentences was implemented in Python. 3.3 Feature selection: description and motivation Figure 2. Aspectual hierarchy after (Huddleston and Pullum, 2002). 57 Table 2. Description of the features in both datasets Fine-grained dataset Coarse-grai</context>
</contexts>
<marker>Quinlan, 1992</marker>
<rawString>J. Ross Quinlan, 1992: C4.5: Programs for Machine Learning. Morgan Kaufmann Pub., San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric V Siegel</author>
</authors>
<title>Linguistic Indicators for Language Understanding: Using machine learning methods to combine corpus-based indicators for aspectual classification of clauses.</title>
<date>1998</date>
<institution>Ph.D. Dissertation. Columbia University.</institution>
<contexts>
<context position="14603" citStr="Siegel, 1998" startWordPosition="2380" endWordPosition="2381">a number of aspect-related features were calculated. These features were selected so as to model characteristics of a clause that help determine its aspectual class. The characteristics used were default aspect of the main verb of a clause, tense, temporal expressions, semantic category of a verb, voice and some properties of the direct object. Each of these characteristics is listed below, along with motivation for it, and information about how it was calculated. It must be mentioned that several researchers looked into determining automatically various semantic properties of verbs, such as (Siegel, 1998; Merlo et al., 2002). Yet these approaches dealt with properties of verbs in general and not with particular usages in the context of concrete sentences. Default verbal aspect. A set of verbs, referred to as stative verbs, tends to produce mostly stative clauses. Examples of such verbs include be, like, feel, love, hate and many others. A common property of such verbs is that they do not readily yield a progressive form (Vendler, 1967; Dowty, 1979). Consider examples 3a and 3b. (3a) She is talking. (a dynamic verb talk) (3b) *She is liking the book. (a stative verb like) The default aspectual</context>
<context position="18530" citStr="Siegel, 1998" startWordPosition="3024" endWordPosition="3025">is very difficult to capture automatically. Hoping to leverage it, the author used semantic categorization of the 3,000 most common English verbs as described in (Levin, 1993). The fine-grained dataset contained a feature with 49 possible values that corresponded to the top-level categories described in (Levin, 1993). The coarse-grained dataset contained 1 binary feature that carried this information. Verbs that belong to more than one category were manually assigned to a single category that best captured their literal meaning. Voice. Usually, clauses in passive voice only occur with events (Siegel, 1998). Both datasets contained 1 binary feature to describe this information. Properties of direct object. For some verbs properties of direct object help determine whether a given clause is stative or dynamic. (7a) She wrote a book. (event) (7b) She wrote books. (state) The fine-grained dataset contained 2 binary features to describe whether direct object is definite or indefinite and whether it is plural. The coarsegrained dataset contained no such information because it appeared that this information was not crucial. Several additional features were present in both datasets that described overal</context>
</contexts>
<marker>Siegel, 1998</marker>
<rawString>Eric V. Siegel. 1998. Linguistic Indicators for Language Understanding: Using machine learning methods to combine corpus-based indicators for aspectual classification of clauses. Ph.D. Dissertation. Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pasi Tapanainen</author>
<author>Timo Järvinen</author>
</authors>
<title>A nonprojective dependency parser.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th Conference on Applied Natural Language Processing,</booktitle>
<pages>64--71</pages>
<contexts>
<context position="7911" citStr="Tapanainen and Järvinen, 1997" startWordPosition="1297" endWordPosition="1300"> my girl is the anaphoric expression (in the context of this story Della and the girl are the same person). (1a) Della finished her cry and attended to her cheeks with the powder rag. (1b) &amp;quot;Don&apos;t make any mistake, Dell,&amp;quot; he said, “about me. I don&apos;t think there&apos;s anything [...] that could make me like my girl any less. The author created a system that resolved 1st and 3rd person singular pronouns (I, me, my, he, his etc.) and singular nominal anaphoric expressions (e.g. the man, but not men). The system was implemented in Java, within the GATE framework, using Connexor Machinese Syntax parser (Tapanainen and Järvinen, 1997). A generalized overview of the system is provided below. During the first step, the documents were parsed using Connexor Machinese Syntax parser. The parsed data was then forwarded to the Gazetteer in GATE, which recognized nouns denoting persons. The original version of the Gazetteer recognized only named entities and professions, but the Gazetteer was extended to include common animate nouns such as man, woman, etc. As the next step, an implementation based on a classical pronoun resolution algorithm (Lappin and Leass, 1994) was applied to the texts. Subsequently, anaphoric noun phrases wer</context>
</contexts>
<marker>Tapanainen, Järvinen, 1997</marker>
<rawString>Pasi Tapanainen and Timo Järvinen. 1997 A nonprojective dependency parser. In Proceedings of the 5th Conference on Applied Natural Language Processing, p. 64-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Marc Moens</author>
</authors>
<title>Summarizing scientific articles-experiments with relevance and rhetorical status.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<pages>409--445</pages>
<contexts>
<context position="1260" citStr="Teufel and Moens, 2002" startWordPosition="201" endWordPosition="204">ct. Preliminary results show an improvement over two naïve baselines: a lead baseline and a more sophisticated variant of it. Although modest, the results suggest that using aspectual information may be of help when summarizing fiction. A more thorough evaluation involving human judges is under way. 1 Introduction In the course of recent years the scientific community working on the problem of automatic text summarization has been experiencing an upsurge. A multitude of different techniques has been applied to this end, some of the more remarkable of them being (Marcu, 1997; Mani et al. 1998; Teufel and Moens, 2002; Elhadad et al., 2005), to name just a few. These researchers worked on various text genres: scientific and popular scientific articles (Marcu, 1997; Mani et al., 1998), texts in computational linguistics (Teufel and Moens, 2002), and medical texts (Elhadad et al., 2002). All these genres are examples of texts characterized by rigid structure, relative abundance of surface markers and straightforwardness. Relatively few attempts have been made at summarizing less structured genres, some of them being dialogue and speech summarization (Zechner, 2002; Koumpis et al. 2001). The issue of summariz</context>
</contexts>
<marker>Teufel, Moens, 2002</marker>
<rawString>Simone Teufel and Marc Moens. 2002. Summarizing scientific articles-experiments with relevance and rhetorical status. Computational Linguistics, 28(4): 409–445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zeno Vendler</author>
</authors>
<title>Linguistics in Philosophy.</title>
<date>1967</date>
<pages>97--145</pages>
<publisher>Cornell University Press,</publisher>
<contexts>
<context position="15042" citStr="Vendler, 1967" startWordPosition="2455" endWordPosition="2456">n about how it was calculated. It must be mentioned that several researchers looked into determining automatically various semantic properties of verbs, such as (Siegel, 1998; Merlo et al., 2002). Yet these approaches dealt with properties of verbs in general and not with particular usages in the context of concrete sentences. Default verbal aspect. A set of verbs, referred to as stative verbs, tends to produce mostly stative clauses. Examples of such verbs include be, like, feel, love, hate and many others. A common property of such verbs is that they do not readily yield a progressive form (Vendler, 1967; Dowty, 1979). Consider examples 3a and 3b. (3a) She is talking. (a dynamic verb talk) (3b) *She is liking the book. (a stative verb like) The default aspectual category of a verb was approximated using Longman Dictionary of Contemporary English (LDOCE). Verbs marked in LDOCE as not having a progressive form were considered stative and all others – dynamic. This information was expressed in both datasets as 1 binary feature. Grammatical tense. Usually, simple tenses are more likely to be used in stative or habitual situations than progressive or perfect tenses. In fact, it is considered to be</context>
</contexts>
<marker>Vendler, 1967</marker>
<rawString>Zeno Vendler. 1967. Linguistics in Philosophy. Cornell University Press, p. 97- 145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Zechner</author>
</authors>
<date>2002</date>
<booktitle>Automatic Summarization of Open-Domain Multiparty Dialogues in Diverse Genres. Computational Linguistics</booktitle>
<pages>28--4</pages>
<contexts>
<context position="1815" citStr="Zechner, 2002" startWordPosition="285" endWordPosition="286">ing (Marcu, 1997; Mani et al. 1998; Teufel and Moens, 2002; Elhadad et al., 2005), to name just a few. These researchers worked on various text genres: scientific and popular scientific articles (Marcu, 1997; Mani et al., 1998), texts in computational linguistics (Teufel and Moens, 2002), and medical texts (Elhadad et al., 2002). All these genres are examples of texts characterized by rigid structure, relative abundance of surface markers and straightforwardness. Relatively few attempts have been made at summarizing less structured genres, some of them being dialogue and speech summarization (Zechner, 2002; Koumpis et al. 2001). The issue of summarizing fiction remains largely untouched, since a few very thorough earlier works (Charniak, 1972; Lehnert, 1982). The work presented here seeks to fill in this gap. The ultimate objective of the project is stated as follows: to produce indicative summaries of short works of fiction such that they be helpful to a potential reader in deciding whether she would be interested in reading a particular story or not. To this end, revealing the plot was deemed unnecessary and even undesirable. Instead, the current approach relies on the following assumption: w</context>
</contexts>
<marker>Zechner, 2002</marker>
<rawString>Klaus Zechner. 2002. Automatic Summarization of Open-Domain Multiparty Dialogues in Diverse Genres. Computational Linguistics 28(4):447-485.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>