<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009375">
<title confidence="0.994746">
Exemplar-Based Models for Word Meaning In Context
</title>
<author confidence="0.981424">
Katrin Erk Sebastian Pad´o
</author>
<affiliation confidence="0.9751075">
Department of Linguistics Institut f¨ur maschinelle Sprachverarbeitung
University of Texas at Austin Stuttgart University
</affiliation>
<email confidence="0.992725">
katrin.erk@mail.utexas.edu pado@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.993739" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999974">
This paper describes ongoing work on dis-
tributional models for word meaning in
context. We abandon the usual one-vector-
per-word paradigm in favor of an exemplar
model that activates only relevant occur-
rences. On a paraphrasing task, we find
that a simple exemplar model outperforms
more complex state-of-the-art models.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999985125">
Distributional models are a popular framework
for representing word meaning. They describe
a lemma through a high-dimensional vector that
records co-occurrence with context features over a
large corpus. Distributional models have been used
in many NLP analysis tasks (Salton et al., 1975;
McCarthy and Carroll, 2003; Salton et al., 1975), as
well as for cognitive modeling (Baroni and Lenci,
2009; Landauer and Dumais, 1997; McDonald and
Ramscar, 2001). Among their attractive properties
are their simplicity and versatility, as well as the
fact that they can be acquired from corpora in an
unsupervised manner.
Distributional models are also attractive as a
model of word meaning in context, since they do
not have to rely on fixed sets of dictionary sense
with their well-known problems (Kilgarriff, 1997;
McCarthy and Navigli, 2009). Also, they can
be used directly for testing paraphrase applicabil-
ity (Szpektor et al., 2008), a task that has recently
become prominent in the context of textual entail-
ment (Bar-Haim et al., 2007). However, polysemy
is a fundamental problem for distributional models.
Typically, distributional models compute a single
“type” vector for a target word, which contains co-
occurrence counts for all the occurrences of the
target in a large corpus. If the target is polyse-
mous, this vector mixes contextual features for all
the senses of the target. For example, among the
top 20 features for coach, we get match and team
(for the “trainer” sense) as well as driver and car
(for the “bus” sense). This problem has typically
been approached by modifying the type vector for
a target to better match a given context (Mitchell
and Lapata, 2008; Erk and Pad´o, 2008; Thater et
al., 2009).
In the terms of research on human concept rep-
resentation, which often employs feature vector
representations, the use of type vectors can be un-
derstood as aprototype-based approach, which uses
a single vector per category. From this angle, com-
puting prototypes throws away much interesting
distributional information. A rival class of mod-
els is that of exemplar models, which memorize
each seen instance of a category and perform cat-
egorization by comparing a new stimulus to each
remembered exemplar vector.
We can address the polysemy issue through an
exemplar model by simply removing all exem-
plars that are “not relevant” for the present con-
text, or conversely activating only the relevant
ones. For the coach example, in the context of
a text about motorways, presumably an instance
like “The coach drove a steady 45 mph” would be
activated, while “The team lost all games since the
new coach arrived” would not.
In this paper, we present an exemplar-based dis-
tributional model for modeling word meaning in
context, applying the model to the task of decid-
ing paraphrase applicability. With a very simple
vector representation and just using activation, we
outperform the state-of-the-art prototype models.
We perform an in-depth error analysis to identify
stable parameters for this class of models.
</bodyText>
<sectionHeader confidence="0.999799" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99845975">
Among distributional models of word, there are
some approaches that address polysemy, either
by inducing a fixed clustering of contexts into
senses (Sch¨utze, 1998) or by dynamically modi-
</bodyText>
<page confidence="0.965184">
92
</page>
<note confidence="0.692349">
Proceedings of the ACL 2010 Conference Short Papers, pages 92–97,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<figure confidence="0.902106571428572">
Sentential context
Paraphrase
After a fire extinguisher is used, it must
always be returned for recharging and
its use recorded.
bring back (3),
take back (2),
send back (1),
give back (1)
We return to the young woman who is
reading the Wrigley’s wrapping paper.
come back (3),
revert (1), revisit
(1), go (1)
</figure>
<bodyText confidence="0.99996011627907">
fying a word’s type vector according to each given
sentence context (Landauer and Dumais, 1997;
Mitchell and Lapata, 2008; Erk and Pad´o, 2008;
Thater et al., 2009). Polysemy-aware approaches
also differ in their notion of context. Some use a
bag-of-words representation of words in the cur-
rent sentence (Sch¨utze, 1998; Landauer and Du-
mais, 1997), some make use of syntactic con-
text (Mitchell and Lapata, 2008; Erk and Pad´o,
2008; Thater et al., 2009). The approach that we
present in the current paper computes a representa-
tion dynamically for each sentence context, using
a simple bag-of-words representation of context.
In cognitive science, prototype models predict
degree of category membership through similar-
ity to a single prototype, while exemplar theory
represents a concept as a collection of all previ-
ously seen exemplars (Murphy, 2002). Griffiths et
al. (2007) found that the benefit of exemplars over
prototypes grows with the number of available ex-
emplars. The problem of representing meaning in
context, which we consider in this paper, is closely
related to the problem of concept combination in
cognitive science, i.e., the derivation of representa-
tions for complex concepts (such as “metal spoon”)
given the representations of base concepts (“metal”
and “spoon”). While most approaches to concept
combination are based on prototype models, Voor-
spoels et al. (2009) show superior results for an
exemplar model based on exemplar activation.
In NLP, exemplar-based (memory-based) mod-
els have been applied to many problems (Daele-
mans et al., 1999). In the current paper, we use an
exemplar model for computing distributional repre-
sentations for word meaning in context, using the
context to activate relevant exemplars. Comparing
representations of context, bag-of-words (BOW)
representations are more informative and noisier,
while syntax-based representations deliver sparser
and less noisy information. Following the hypothe-
sis that richer, topical information is more suitable
for exemplar activation, we use BOW representa-
tions of sentential context in the current paper.
</bodyText>
<sectionHeader confidence="0.998436" genericHeader="method">
3 Exemplar Activation Models
</sectionHeader>
<bodyText confidence="0.999973">
We now present an exemplar-based model for
meaning in context. It assumes that each target
lemma is represented by a set of exemplars, where
an exemplar is a sentence in which the target occurs,
represented as a vector. We use lowercase letters
for individual exemplars (vectors), and uppercase
</bodyText>
<tableCaption confidence="0.996459">
Table 1: The Lexical Substitution (LexSub) dataset.
</tableCaption>
<bodyText confidence="0.988862142857143">
letters for sets of exemplars.
We model polysemy by activating relevant ex-
emplars of a lemma E in a given sentence context
s. (Note that we use E to refer to both a lemma
and its exemplar set, and that s can be viewed as
just another exemplar vector.) In general, we define
activation of a set E by exemplar s as
</bodyText>
<equation confidence="0.594059">
act(E, s) _ {e E E  |sim(e, s) &gt; B(E, s)}
</equation>
<bodyText confidence="0.995338029411765">
where E is an exemplar set, s is the “point of com-
parison”, sim is some similarity measure such as
Cosine or Jaccard, and B(E, s) is a threshold. Ex-
emplars belong to the activated set if their similarity
to s exceeds B(E, s).1 We explore two variants of
activation. In kNN activation, the k most simi-
lar exemplars to s are activated by setting B to the
similarity of the k-th most similar exemplar. In
q-percentage activation, we activate the top q%
of E by setting B to the (100-q)-th percentile of the
sim(e, s) distribution. Note that, while in the kNN
activation scheme the number of activated exem-
plars is the same for every lemma, this is not the
case for percentage activation: There, a more fre-
quent lemma (i.e., a lemma with more exemplars)
will have more exemplars activated.
Exemplar activation for paraphrasing. A para-
phrases is typically only applicable to a particular
sense of a target word. Table 1 illustrates this on
two examples from the Lexical Substitution (Lex-
Sub) dataset (McCarthy and Navigli, 2009), both
featuring the target return. The right column lists
appropriate paraphrases of return in each context
(given by human annotators). 2 We apply the ex-
emplar activation model to the task of predicting
paraphrase felicity: Given a target lemma T in a
particular sentential context s, and given a list of
1In principle, activation could be treated not just as binary
inclusion/exclusion, but also as a graded weighting scheme.
However, weighting schemes introduce a large number of
parameters, which we wanted to avoid.
2Each annotator was allowed to give up to three para-
phrases per target in context. As a consequence, the number
of gold paraphrases per target sentence varies.
</bodyText>
<page confidence="0.989021">
93
</page>
<figure confidence="0.999424909090909">
actT
kNN perc.
36.1 35.5
36.2 35.2
36.1 35.3
36.0 35.3
35.9 35.1
36.0 35.0
35.9 34.8
36.0 34.7
35.9 34.5
</figure>
<bodyText confidence="0.996677166666667">
potential paraphrases of T, the task is to predict
which of the paraphrases are applicable in s.
Previous approaches (Mitchell and Lapata, 2008;
Erk and Pad´o, 2008; Erk and Pad´o, 2009; Thater
et al., 2009) have performed this task by modify-
ing the type vector for T to the context s and then
comparing the resulting vector T0 to the type vec-
tor of a paraphrase candidate P. In our exemplar
setting, we select a contextually adequate subset
of contexts in which T has been observed, using
T0 = act(T, s) as a generalized representation of
meaning of target T in the context of s.
Previous approaches used all of P as a repre-
sentation for a paraphrase candidate P. However,
P includes also irrelevant exemplars, while for a
paraphrase to be judged as good, it is sufficient that
one plausible reading exists. Therefore, we use
P0 = act(P, s) to represent the paraphrase.
</bodyText>
<sectionHeader confidence="0.999148" genericHeader="evaluation">
4 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.977060757575757">
Data. We evaluate our model on predicting para-
phrases from the Lexical Substitution (LexSub)
dataset (McCarthy and Navigli, 2009). This dataset
consists of 2000 instances of 200 target words in
sentential contexts, with paraphrases for each tar-
get word instance generated by up to 6 participants.
Paraphrases are ranked by the number of annota-
tors that chose them (cf. Table 1). Following Erk
and Pad´o (2008), we take the list of paraphrase can-
didates for a target as given (computed by pooling
all paraphrases that LexSub annotators proposed
for the target) and use the models to rank them for
any given sentence context.
As exemplars, we create bag-of-words co-
occurrence vectors from the BNC. These vectors
represent instances of a target word by the other
words in the same sentence, lemmatized and POS-
tagged, minus stop words. E.g., if the lemma
gnurge occurs twice in the BNC, once in the sen-
tence “The dog will gnurge the other dog”, and
once in “The old windows gnurged”, the exemplar
set for gnurge contains the vectors [dog-n: 2, other-
a:1] and [old-a: 1, window-n: 1]. For exemplar
similarity, we use the standard Cosine similarity,
and for the similarity of two exemplar sets, the
Cosine of their centroids.
Evaluation. The model’s prediction for an item
is a list of paraphrases ranked by their predicted
goodness of fit. To evaluate them against a
weighted list of gold paraphrases, we follow Thater
et al. (2009) in using Generalized Average Preci-
para-
meter
</bodyText>
<table confidence="0.992043545454545">
36.5 38.6
36.2 37.9
35.8 37.8
35.8 37.7
35.9 37.5
36.1 37.5
36.1 37.5
36.0 37.4
35.9 37.3
34.6
random BL 28.5
</table>
<tableCaption confidence="0.980304">
Table 2: Activation of T or P individually on the
full LexSub dataset (GAP evaluation)
</tableCaption>
<bodyText confidence="0.999860818181818">
sion (GAP), which interpolates the precision values
of top-n prediction lists for increasing n. Let G =
(q1, ... , qm) be the list of gold paraphrases with
gold weights (y1, ... , ym). Let P = (p1, ... , pn)
be the list of model predictions as ranked by the
model, and let (x1,... , xn) be the gold weights
associated with them (assume xi = 0 if pi E� G),
where G C_ P. Let I(xi) = 1 if pi E G, and zero
otherwise. We write xi = 1i Eik=1 xk for the av-
erage gold weight of the first i model predictions,
and analogously yi. Then
</bodyText>
<equation confidence="0.944628333333333">
GAP(P, G) = m I (xi)xi
�j=1 I(yj)yj i=1
1 n
</equation>
<bodyText confidence="0.9993748">
Since the model may rank multiple paraphrases the
same, we average over 10 random permutations of
equally ranked paraphrases. We report mean GAP
over all items in the dataset.
Results and Discussion. We first computed two
models that activate either the paraphrase or the
target, but not both. Model 1, actT, activates only
the target, using the complete P as paraphrase, and
ranking paraphrases by sim(P, act(T, s)). Model
2, actP, activates only the paraphrase, using s as
the target word, ranking by sim(act(P, s), s).
The results for these models are shown in Ta-
ble 2, with both kNN and percentage activation:
kNN activation with a parameter of 10 means that
the 10 closest neighbors were activated, while per-
centage with a parameter of 10 means that the clos-
est 10% of the exemplars were used. Note first
that we computed a random baseline (last row)
with a GAP of 28.5. The second-to-last row (“no
activation”) shows two more informed baselines.
</bodyText>
<figure confidence="0.987326">
actP
kNN perc.
10
20
30
40
50
60
70
80
90
no act.
35.7
</figure>
<page confidence="0.997479">
94
</page>
<bodyText confidence="0.999995625">
The actT “no act” result (34.6) corresponds to a
prototype-based model that ranks paraphrase can-
didates by the distance between their type vectors
and the target’s type vector. Virtually all exem-
plar models outperform this prototype model. Note
also that both actT and actP show the best results
for small values of the activation parameter. This
indicates paraphrases can be judged on the basis
of a rather small number of exemplars. Neverthe-
less, actT and actP differ with regard to the details
of their optimal activation. For actT, a small ab-
solute number of activated exemplars (here, 20)
works best , while actP yields the best results for
a small percentage of paraphrase exemplars. This
can be explained by the different functions played
by actT and actP (cf. Section 3): Activation of the
paraphrase must allow a guess about whether there
is reasonable interpretation of P in the context s.
This appears to require a reasonably-sized sample
from P. In contrast, target activation merely has to
counteract the sparsity of s, and activation of too
many exemplars from T leads to oversmoothing.
We obtained significances by computing 95%
and 99% confidence intervals with bootstrap re-
sampling. As a rule of thumb, we find that 0.4%
difference in GAP corresponds to a significant dif-
ference at the 95% level, and 0.7% difference in
GAP to significance at the 99% level. The four
activation methods (i.e., columns in Table 2) are
significantly different from each other, with the ex-
ception of the pair actT/kNN and actP/kNN (n.s.),
so that we get the following order:
</bodyText>
<equation confidence="0.504171">
actP/perc &gt; actP/kNN Pt� actT/kNN &gt; actT/perc
</equation>
<bodyText confidence="0.99975">
where &gt; means “significantly outperforms”. In par-
ticular, the best method (actT/kNN) outperforms
all other methods at p&lt;0.01. Here, the best param-
eter setting (10% activation) is also significantly
better than the next-one one (20% activation). With
the exception of actT/perc, all activation methods
significantly outperform the best baseline (actP, no
activation).
Based on these observations, we computed a
third model, actTP, that activates both T (by kNN)
and P (by percentage), ranking paraphrases by
sim(act(P, s), act(T, s)). Table 3 shows the re-
sults. We find the overall best model at a similar
location in parameter space as for actT and actP
(cf. Table 2), namely by setting the activation pa-
rameters to small values. The sensitivity of the
parameters changes considerably, though. When
</bodyText>
<table confidence="0.998987">
P activation (%) ==&gt;. 10 20 30
T activation (kNN) 4
5 38.2 38.1 38.1
10 37.6 37.8 37.7
20 37.3 37.4 37.3
40 37.2 37.2 36.1
</table>
<tableCaption confidence="0.9610555">
Table 3: Joint activation of P and T on the full
LexSub dataset (GAP evaluation)
</tableCaption>
<bodyText confidence="0.998722268292683">
we fix the actP activation level, we find compara-
tively large performance differences between the
T activation settings k=5 and k=10 (highly signif-
icant for 10% actP, and significant for 20% and
30% actP). On the other hand, when we fix the
actT activation level, changes in actP activation
generally have an insignificant impact.
Somewhat disappointingly, we are not able to
surpass the best result for actP alone. This indicates
that – at least in the current vector space – the
sparsity of s is less of a problem than the “dilution”
of s that we face when we representing the target
word by exemplars of T close to s. Note, however,
that the numerically worse performance of the best
actTP model is still not significantly different from
the best actP model.
Influence of POS and frequency. An analysis
of the results by target part-of-speech showed that
the globally optimal parameters also yield the best
results for individual POS, even though there are
substantial differences among POS. For actT, the
best results emerge for all POS with kNN activation
with k between 10 and 30. For k=20, we obtain a
GAP of 35.3 (verbs), 38.2 (nouns), and 35.1 (adjec-
tives). For actP, the best parameter for all POS was
activation of 10%, with GAPs of 36.9 (verbs), 41.4
(nouns), and 37.5 (adjectives). Interestingly, the
results for actTP (verbs: 38.4, nouns: 40.6, adjec-
tives: 36.9) are better than actP for verbs, but worse
for nouns and adjectives, which indicates that the
sparsity problem might be more prominent than for
the other POS. In all three models, we found a clear
effect of target and paraphrase frequency, with de-
teriorating performance for the highest-frequency
targets as well as for the lemmas with the highest
average paraphrase frequency.
Comparison to other models. Many of the
other models are syntax-based and are therefore
only applicable to a subset of the LexSub data.
We have re-evaluated our exemplar models on the
subsets we used in Erk and Pad´o (2008, EP08, 367
</bodyText>
<page confidence="0.997057">
95
</page>
<table confidence="0.995143">
EP08 Models
EP09 TDP09
EP08 dataset 27.4 NA NA
EP09 dataset NA 32.2 36.5
actT actP actTP
EP08 dataset 36.5 38.0 39.9
EP09 dataset 39.1 39.9 39.6
</table>
<tableCaption confidence="0.841372">
Table 4: Comparison to other models on two sub-
sets of LexSub (GAP evaluation)
</tableCaption>
<bodyText confidence="0.999523263157895">
datapoints) and Erk and Pad´o (2009, EP09, 100 dat-
apoints). The second set was also used by Thater et
al. (2009, TDP09). The results in Table 4 compare
these models against our best previous exemplar
models and show that our models outperform these
models across the board. 3 Due to the small sizes
of these datasets, statistical significance is more
difficult to attain. On EP09, the differences among
our models are not significant, but the difference
between them and the original EP09 model is.4 On
EP08, all differences are significant except for actP
vs. actTP.
We note that both the EP08 and the EP09
datasets appear to be simpler to model than the
complete Lexical Substitution dataset, at least by
our exemplar-based models. This underscores an
old insight: namely, that direct syntactic neighbors,
such as arguments and modifiers, provide strong
clues as to word sense.
</bodyText>
<sectionHeader confidence="0.996927" genericHeader="conclusions">
5 Conclusions and Outlook
</sectionHeader>
<bodyText confidence="0.999816833333333">
This paper reports on work in progress on an ex-
emplar activation model as an alternative to one-
vector-per-word approaches to word meaning in
context. Exemplar activation is very effective in
handling polysemy, even with a very simple (and
sparse) bag-of-words vector representation. On
both the EP08 and EP09 datasets, our models sur-
pass more complex prototype-based approaches
(Tab. 4). It is also noteworthy that the exemplar
activation models work best when few exemplars
are used, which bodes well for their efficiency.
We found that the best target representations re-
</bodyText>
<footnote confidence="0.9632855">
3Since our models had the advantage of being tuned on
the dataset, we also report the range of results across the
parameters we tested. On the EP08 dataset, we obtained 33.1–
36.5 for actT; 33.3–38.0 for actP; 37.7-39.9 for actTP. On the
EP09 dataset, the numbers were 35.8–39.1 for actT; 38.1–39.9
for actP; 37.2–39.8 for actTP.
4We did not have access to the TDP09 predictions to do
significance testing.
</footnote>
<bodyText confidence="0.999885305555556">
sult from activating a low absolute number of exem-
plars. Paraphrase representations are best activated
with a percentage-based threshold. Overall, we
found that paraphrase activation had a much larger
impact on performance than target activation, and
that drawing on target exemplars other than s to
represent the target meaning in context improved
over using s itself only for verbs (Tab. 3). This sug-
gests the possibility of considering T’s activated
paraphrase candidates as the representation of T in
the context s, rather than some vector of T itself,
in the spirit of Kintsch (2001).
While it is encouraging that the best parameter
settings involved the activation of only few exem-
plars, computation with exemplar models still re-
quires the management of large numbers of vectors.
The computational overhead can be reduced by us-
ing data structures that cut down on the number
of vector comparisons, or by decreasing vector di-
mensionality (Gorman and Curran, 2006). We will
experiment with those methods to determine the
tradeoff of runtime and accuracy for this task.
Another area of future work is to move beyond
bag-of-words context: It is known from WSD
that syntactic and bag-of-words contexts provide
complementary information (Florian et al., 2002;
Szpektor et al., 2008), and we hope that they can be
integrated in a more sophisticated exemplar model.
Finally, we will to explore task-based evalua-
tions. Relation extraction and textual entailment
in particular are tasks where similar models have
been used before (Szpektor et al., 2008).
Acknowledgements. This work was supported
in part by National Science Foundation grant IIS-
0845925, and by a Morris Memorial Grant from
the New York Community Trust.
</bodyText>
<sectionHeader confidence="0.998624" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999616384615385">
R. Bar-Haim, I. Dagan, I. Greental, and E. Shnarch.
2007. Semantic inference at the lexical-syntactic
level. In Proceedings of AAAI, pages 871–876, Van-
couver, BC.
M. Baroni and A. Lenci. 2009. One distributional
memory, many semantic spaces. In Proceedings of
the EACL Workshop on Geometrical Models of Nat-
ural Language Semantics, Athens, Greece.
W. Daelemans, A. van den Bosch, and J. Zavrel. 1999.
Forgetting exceptions is harmful in language learn-
ing. Machine Learning, 34(1/3):11–43. Special Is-
sue on Natural Language Learning.
K. Erk and S. Pad´o. 2008. A structured vector space
</reference>
<page confidence="0.983481">
96
</page>
<reference confidence="0.984334766666667">
model for word meaning in context. In Proceedings
of EMNLP, pages 897–906, Honolulu, HI.
K. Erk and S. Pad´o. 2009. Paraphrase assessment in
structured vector space: Exploring parameters and
datasets. In Proceedings of the EACL Workshop on
Geometrical Models of Natural Language Seman-
tics, Athens, Greece.
R. Florian, S. Cucerzan, C. Schafer, and D. Yarowsky.
2002. Combining classifiers for word sense disam-
biguation. Journal of Natural Language Engineer-
ing, 8(4):327–341.
J. Gorman and J. R. Curran. 2006. Scaling distribu-
tional similarity to large corpora. In Proceedings of
ACL, pages 361–368, Sydney.
T. Griffiths, K. Canini, A. Sanborn, and D. J. Navarro.
2007. Unifying rational models of categorization
via the hierarchical Dirichlet process. In Proceed-
ings of CogSci, pages 323–328, Nashville, TN.
A. Kilgarriff. 1997. I don’t believe in word senses.
Computers and the Humanities, 31(2):91–113.
W. Kintsch. 2001. Predication. Cognitive Science,
25:173–202.
T. Landauer and S. Dumais. 1997. A solution to Platos
problem: the latent semantic analysis theory of ac-
quisition, induction, and representation of knowl-
edge. Psychological Review, 104(2):211–240.
D. McCarthy and J. Carroll. 2003. Disambiguating
nouns, verbs, and adjectives using automatically ac-
quired selectional preferences. Computational Lin-
guistics, 29(4):639–654.
D. McCarthy and R. Navigli. 2009. The English lexi-
cal substitution task. Language Resources and Eval-
uation, 43(2):139–159. Special Issue on Compu-
tational Semantic Analysis of Language: SemEval-
2007 and Beyond.
S. McDonald and M. Ramscar. 2001. Testing the dis-
tributional hypothesis: The influence of context on
judgements of semantic similarity. In Proceedings
of CogSci, pages 611–616.
J. Mitchell and M. Lapata. 2008. Vector-based models
of semantic composition. In Proceedings of ACL,
pages 236–244, Columbus, OH.
G. L. Murphy. 2002. The Big Book of Concepts. MIT
Press.
G Salton, A Wang, and C Yang. 1975. A vector-
space model for information retrieval. Journal of the
American Society for Information Science, 18:613–
620.
H. Sch¨utze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97–124.
I. Szpektor, I. Dagan, R. Bar-Haim, and J. Goldberger.
2008. Contextual preferences. In Proceedings of
ACL, pages 683–691, Columbus, OH.
S. Thater, G. Dinu, and M. Pinkal. 2009. Ranking
paraphrases in context. In Proceedings of the ACL
Workshop on Applied Textual Inference, pages 44–
47, Singapore.
W. Voorspoels, W. Vanpaemel, and G. Storms. 2009.
The role of extensional information in conceptual
combination. In Proceedings of CogSci.
</reference>
<page confidence="0.999694">
97
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.900848">
<title confidence="0.998059">Exemplar-Based Models for Word Meaning In Context</title>
<author confidence="0.997464">Katrin Erk Sebastian Pad´o</author>
<affiliation confidence="0.9996755">Department of Linguistics Institut f¨ur maschinelle Sprachverarbeitung University of Texas at Austin Stuttgart University</affiliation>
<email confidence="0.912652">katrin.erk@mail.utexas.edupado@ims.uni-stuttgart.de</email>
<abstract confidence="0.999065333333333">This paper describes ongoing work on distributional models for word meaning in context. We abandon the usual one-vectorper-word paradigm in favor of an exemplar model that activates only relevant occurrences. On a paraphrasing task, we find that a simple exemplar model outperforms more complex state-of-the-art models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Bar-Haim</author>
<author>I Dagan</author>
<author>I Greental</author>
<author>E Shnarch</author>
</authors>
<title>Semantic inference at the lexical-syntactic level.</title>
<date>2007</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>871--876</pages>
<location>Vancouver, BC.</location>
<contexts>
<context position="1629" citStr="Bar-Haim et al., 2007" startWordPosition="237" endWordPosition="240">Dumais, 1997; McDonald and Ramscar, 2001). Among their attractive properties are their simplicity and versatility, as well as the fact that they can be acquired from corpora in an unsupervised manner. Distributional models are also attractive as a model of word meaning in context, since they do not have to rely on fixed sets of dictionary sense with their well-known problems (Kilgarriff, 1997; McCarthy and Navigli, 2009). Also, they can be used directly for testing paraphrase applicability (Szpektor et al., 2008), a task that has recently become prominent in the context of textual entailment (Bar-Haim et al., 2007). However, polysemy is a fundamental problem for distributional models. Typically, distributional models compute a single “type” vector for a target word, which contains cooccurrence counts for all the occurrences of the target in a large corpus. If the target is polysemous, this vector mixes contextual features for all the senses of the target. For example, among the top 20 features for coach, we get match and team (for the “trainer” sense) as well as driver and car (for the “bus” sense). This problem has typically been approached by modifying the type vector for a target to better match a gi</context>
</contexts>
<marker>Bar-Haim, Dagan, Greental, Shnarch, 2007</marker>
<rawString>R. Bar-Haim, I. Dagan, I. Greental, and E. Shnarch. 2007. Semantic inference at the lexical-syntactic level. In Proceedings of AAAI, pages 871–876, Vancouver, BC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>A Lenci</author>
</authors>
<title>One distributional memory, many semantic spaces.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL Workshop on Geometrical Models of Natural Language Semantics,</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="992" citStr="Baroni and Lenci, 2009" startWordPosition="135" endWordPosition="138">one-vectorper-word paradigm in favor of an exemplar model that activates only relevant occurrences. On a paraphrasing task, we find that a simple exemplar model outperforms more complex state-of-the-art models. 1 Introduction Distributional models are a popular framework for representing word meaning. They describe a lemma through a high-dimensional vector that records co-occurrence with context features over a large corpus. Distributional models have been used in many NLP analysis tasks (Salton et al., 1975; McCarthy and Carroll, 2003; Salton et al., 1975), as well as for cognitive modeling (Baroni and Lenci, 2009; Landauer and Dumais, 1997; McDonald and Ramscar, 2001). Among their attractive properties are their simplicity and versatility, as well as the fact that they can be acquired from corpora in an unsupervised manner. Distributional models are also attractive as a model of word meaning in context, since they do not have to rely on fixed sets of dictionary sense with their well-known problems (Kilgarriff, 1997; McCarthy and Navigli, 2009). Also, they can be used directly for testing paraphrase applicability (Szpektor et al., 2008), a task that has recently become prominent in the context of textu</context>
</contexts>
<marker>Baroni, Lenci, 2009</marker>
<rawString>M. Baroni and A. Lenci. 2009. One distributional memory, many semantic spaces. In Proceedings of the EACL Workshop on Geometrical Models of Natural Language Semantics, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>A van den Bosch</author>
<author>J Zavrel</author>
</authors>
<title>Forgetting exceptions is harmful in language learning.</title>
<date>1999</date>
<booktitle>Machine Learning, 34(1/3):11–43. Special Issue on Natural Language Learning.</booktitle>
<marker>Daelemans, van den Bosch, Zavrel, 1999</marker>
<rawString>W. Daelemans, A. van den Bosch, and J. Zavrel. 1999. Forgetting exceptions is harmful in language learning. Machine Learning, 34(1/3):11–43. Special Issue on Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Erk</author>
<author>S Pad´o</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>897--906</pages>
<location>Honolulu, HI.</location>
<marker>Erk, Pad´o, 2008</marker>
<rawString>K. Erk and S. Pad´o. 2008. A structured vector space model for word meaning in context. In Proceedings of EMNLP, pages 897–906, Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Erk</author>
<author>S Pad´o</author>
</authors>
<title>Paraphrase assessment in structured vector space: Exploring parameters and datasets.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL Workshop on Geometrical Models of Natural Language Semantics,</booktitle>
<location>Athens, Greece.</location>
<marker>Erk, Pad´o, 2009</marker>
<rawString>K. Erk and S. Pad´o. 2009. Paraphrase assessment in structured vector space: Exploring parameters and datasets. In Proceedings of the EACL Workshop on Geometrical Models of Natural Language Semantics, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian</author>
<author>S Cucerzan</author>
<author>C Schafer</author>
<author>D Yarowsky</author>
</authors>
<title>Combining classifiers for word sense disambiguation.</title>
<date>2002</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>8</volume>
<issue>4</issue>
<marker>Florian, Cucerzan, Schafer, Yarowsky, 2002</marker>
<rawString>R. Florian, S. Cucerzan, C. Schafer, and D. Yarowsky. 2002. Combining classifiers for word sense disambiguation. Journal of Natural Language Engineering, 8(4):327–341.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gorman</author>
<author>J R Curran</author>
</authors>
<title>Scaling distributional similarity to large corpora.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>361--368</pages>
<location>Sydney.</location>
<contexts>
<context position="20753" citStr="Gorman and Curran, 2006" startWordPosition="3444" endWordPosition="3447">oved over using s itself only for verbs (Tab. 3). This suggests the possibility of considering T’s activated paraphrase candidates as the representation of T in the context s, rather than some vector of T itself, in the spirit of Kintsch (2001). While it is encouraging that the best parameter settings involved the activation of only few exemplars, computation with exemplar models still requires the management of large numbers of vectors. The computational overhead can be reduced by using data structures that cut down on the number of vector comparisons, or by decreasing vector dimensionality (Gorman and Curran, 2006). We will experiment with those methods to determine the tradeoff of runtime and accuracy for this task. Another area of future work is to move beyond bag-of-words context: It is known from WSD that syntactic and bag-of-words contexts provide complementary information (Florian et al., 2002; Szpektor et al., 2008), and we hope that they can be integrated in a more sophisticated exemplar model. Finally, we will to explore task-based evaluations. Relation extraction and textual entailment in particular are tasks where similar models have been used before (Szpektor et al., 2008). Acknowledgements.</context>
</contexts>
<marker>Gorman, Curran, 2006</marker>
<rawString>J. Gorman and J. R. Curran. 2006. Scaling distributional similarity to large corpora. In Proceedings of ACL, pages 361–368, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Griffiths</author>
<author>K Canini</author>
<author>A Sanborn</author>
<author>D J Navarro</author>
</authors>
<title>Unifying rational models of categorization via the hierarchical Dirichlet process.</title>
<date>2007</date>
<booktitle>In Proceedings of CogSci,</booktitle>
<pages>323--328</pages>
<location>Nashville, TN.</location>
<contexts>
<context position="5156" citStr="Griffiths et al. (2007)" startWordPosition="804" endWordPosition="807">rds representation of words in the current sentence (Sch¨utze, 1998; Landauer and Dumais, 1997), some make use of syntactic context (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2009). The approach that we present in the current paper computes a representation dynamically for each sentence context, using a simple bag-of-words representation of context. In cognitive science, prototype models predict degree of category membership through similarity to a single prototype, while exemplar theory represents a concept as a collection of all previously seen exemplars (Murphy, 2002). Griffiths et al. (2007) found that the benefit of exemplars over prototypes grows with the number of available exemplars. The problem of representing meaning in context, which we consider in this paper, is closely related to the problem of concept combination in cognitive science, i.e., the derivation of representations for complex concepts (such as “metal spoon”) given the representations of base concepts (“metal” and “spoon”). While most approaches to concept combination are based on prototype models, Voorspoels et al. (2009) show superior results for an exemplar model based on exemplar activation. In NLP, exempla</context>
</contexts>
<marker>Griffiths, Canini, Sanborn, Navarro, 2007</marker>
<rawString>T. Griffiths, K. Canini, A. Sanborn, and D. J. Navarro. 2007. Unifying rational models of categorization via the hierarchical Dirichlet process. In Proceedings of CogSci, pages 323–328, Nashville, TN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
</authors>
<title>I don’t believe in word senses.</title>
<date>1997</date>
<journal>Computers and the Humanities,</journal>
<volume>31</volume>
<issue>2</issue>
<contexts>
<context position="1402" citStr="Kilgarriff, 1997" startWordPosition="202" endWordPosition="203">arge corpus. Distributional models have been used in many NLP analysis tasks (Salton et al., 1975; McCarthy and Carroll, 2003; Salton et al., 1975), as well as for cognitive modeling (Baroni and Lenci, 2009; Landauer and Dumais, 1997; McDonald and Ramscar, 2001). Among their attractive properties are their simplicity and versatility, as well as the fact that they can be acquired from corpora in an unsupervised manner. Distributional models are also attractive as a model of word meaning in context, since they do not have to rely on fixed sets of dictionary sense with their well-known problems (Kilgarriff, 1997; McCarthy and Navigli, 2009). Also, they can be used directly for testing paraphrase applicability (Szpektor et al., 2008), a task that has recently become prominent in the context of textual entailment (Bar-Haim et al., 2007). However, polysemy is a fundamental problem for distributional models. Typically, distributional models compute a single “type” vector for a target word, which contains cooccurrence counts for all the occurrences of the target in a large corpus. If the target is polysemous, this vector mixes contextual features for all the senses of the target. For example, among the to</context>
</contexts>
<marker>Kilgarriff, 1997</marker>
<rawString>A. Kilgarriff. 1997. I don’t believe in word senses. Computers and the Humanities, 31(2):91–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Kintsch</author>
</authors>
<date>2001</date>
<journal>Predication. Cognitive Science,</journal>
<pages>25--173</pages>
<contexts>
<context position="20373" citStr="Kintsch (2001)" startWordPosition="3385" endWordPosition="3386"> do significance testing. sult from activating a low absolute number of exemplars. Paraphrase representations are best activated with a percentage-based threshold. Overall, we found that paraphrase activation had a much larger impact on performance than target activation, and that drawing on target exemplars other than s to represent the target meaning in context improved over using s itself only for verbs (Tab. 3). This suggests the possibility of considering T’s activated paraphrase candidates as the representation of T in the context s, rather than some vector of T itself, in the spirit of Kintsch (2001). While it is encouraging that the best parameter settings involved the activation of only few exemplars, computation with exemplar models still requires the management of large numbers of vectors. The computational overhead can be reduced by using data structures that cut down on the number of vector comparisons, or by decreasing vector dimensionality (Gorman and Curran, 2006). We will experiment with those methods to determine the tradeoff of runtime and accuracy for this task. Another area of future work is to move beyond bag-of-words context: It is known from WSD that syntactic and bag-of-</context>
</contexts>
<marker>Kintsch, 2001</marker>
<rawString>W. Kintsch. 2001. Predication. Cognitive Science, 25:173–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Landauer</author>
<author>S Dumais</author>
</authors>
<title>A solution to Platos problem: the latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="1019" citStr="Landauer and Dumais, 1997" startWordPosition="139" endWordPosition="142">igm in favor of an exemplar model that activates only relevant occurrences. On a paraphrasing task, we find that a simple exemplar model outperforms more complex state-of-the-art models. 1 Introduction Distributional models are a popular framework for representing word meaning. They describe a lemma through a high-dimensional vector that records co-occurrence with context features over a large corpus. Distributional models have been used in many NLP analysis tasks (Salton et al., 1975; McCarthy and Carroll, 2003; Salton et al., 1975), as well as for cognitive modeling (Baroni and Lenci, 2009; Landauer and Dumais, 1997; McDonald and Ramscar, 2001). Among their attractive properties are their simplicity and versatility, as well as the fact that they can be acquired from corpora in an unsupervised manner. Distributional models are also attractive as a model of word meaning in context, since they do not have to rely on fixed sets of dictionary sense with their well-known problems (Kilgarriff, 1997; McCarthy and Navigli, 2009). Also, they can be used directly for testing paraphrase applicability (Szpektor et al., 2008), a task that has recently become prominent in the context of textual entailment (Bar-Haim et </context>
<context position="4375" citStr="Landauer and Dumais, 1997" startWordPosition="682" endWordPosition="685">ering of contexts into senses (Sch¨utze, 1998) or by dynamically modi92 Proceedings of the ACL 2010 Conference Short Papers, pages 92–97, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics Sentential context Paraphrase After a fire extinguisher is used, it must always be returned for recharging and its use recorded. bring back (3), take back (2), send back (1), give back (1) We return to the young woman who is reading the Wrigley’s wrapping paper. come back (3), revert (1), revisit (1), go (1) fying a word’s type vector according to each given sentence context (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2009). Polysemy-aware approaches also differ in their notion of context. Some use a bag-of-words representation of words in the current sentence (Sch¨utze, 1998; Landauer and Dumais, 1997), some make use of syntactic context (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2009). The approach that we present in the current paper computes a representation dynamically for each sentence context, using a simple bag-of-words representation of context. In cognitive science, prototype models predict degree of category mem</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>T. Landauer and S. Dumais. 1997. A solution to Platos problem: the latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
<author>J Carroll</author>
</authors>
<title>Disambiguating nouns, verbs, and adjectives using automatically acquired selectional preferences.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="911" citStr="McCarthy and Carroll, 2003" startWordPosition="121" endWordPosition="124">oing work on distributional models for word meaning in context. We abandon the usual one-vectorper-word paradigm in favor of an exemplar model that activates only relevant occurrences. On a paraphrasing task, we find that a simple exemplar model outperforms more complex state-of-the-art models. 1 Introduction Distributional models are a popular framework for representing word meaning. They describe a lemma through a high-dimensional vector that records co-occurrence with context features over a large corpus. Distributional models have been used in many NLP analysis tasks (Salton et al., 1975; McCarthy and Carroll, 2003; Salton et al., 1975), as well as for cognitive modeling (Baroni and Lenci, 2009; Landauer and Dumais, 1997; McDonald and Ramscar, 2001). Among their attractive properties are their simplicity and versatility, as well as the fact that they can be acquired from corpora in an unsupervised manner. Distributional models are also attractive as a model of word meaning in context, since they do not have to rely on fixed sets of dictionary sense with their well-known problems (Kilgarriff, 1997; McCarthy and Navigli, 2009). Also, they can be used directly for testing paraphrase applicability (Szpektor</context>
</contexts>
<marker>McCarthy, Carroll, 2003</marker>
<rawString>D. McCarthy and J. Carroll. 2003. Disambiguating nouns, verbs, and adjectives using automatically acquired selectional preferences. Computational Linguistics, 29(4):639–654.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
<author>R Navigli</author>
</authors>
<title>The English lexical substitution task.</title>
<date>2009</date>
<journal>Language Resources and Evaluation,</journal>
<booktitle>Special Issue on Computational Semantic Analysis of Language: SemEval2007 and Beyond.</booktitle>
<volume>43</volume>
<issue>2</issue>
<contexts>
<context position="1431" citStr="McCarthy and Navigli, 2009" startWordPosition="204" endWordPosition="207">ibutional models have been used in many NLP analysis tasks (Salton et al., 1975; McCarthy and Carroll, 2003; Salton et al., 1975), as well as for cognitive modeling (Baroni and Lenci, 2009; Landauer and Dumais, 1997; McDonald and Ramscar, 2001). Among their attractive properties are their simplicity and versatility, as well as the fact that they can be acquired from corpora in an unsupervised manner. Distributional models are also attractive as a model of word meaning in context, since they do not have to rely on fixed sets of dictionary sense with their well-known problems (Kilgarriff, 1997; McCarthy and Navigli, 2009). Also, they can be used directly for testing paraphrase applicability (Szpektor et al., 2008), a task that has recently become prominent in the context of textual entailment (Bar-Haim et al., 2007). However, polysemy is a fundamental problem for distributional models. Typically, distributional models compute a single “type” vector for a target word, which contains cooccurrence counts for all the occurrences of the target in a large corpus. If the target is polysemous, this vector mixes contextual features for all the senses of the target. For example, among the top 20 features for coach, we g</context>
<context position="8128" citStr="McCarthy and Navigli, 2009" startWordPosition="1295" endWordPosition="1298">r. In q-percentage activation, we activate the top q% of E by setting B to the (100-q)-th percentile of the sim(e, s) distribution. Note that, while in the kNN activation scheme the number of activated exemplars is the same for every lemma, this is not the case for percentage activation: There, a more frequent lemma (i.e., a lemma with more exemplars) will have more exemplars activated. Exemplar activation for paraphrasing. A paraphrases is typically only applicable to a particular sense of a target word. Table 1 illustrates this on two examples from the Lexical Substitution (LexSub) dataset (McCarthy and Navigli, 2009), both featuring the target return. The right column lists appropriate paraphrases of return in each context (given by human annotators). 2 We apply the exemplar activation model to the task of predicting paraphrase felicity: Given a target lemma T in a particular sentential context s, and given a list of 1In principle, activation could be treated not just as binary inclusion/exclusion, but also as a graded weighting scheme. However, weighting schemes introduce a large number of parameters, which we wanted to avoid. 2Each annotator was allowed to give up to three paraphrases per target in cont</context>
<context position="9944" citStr="McCarthy and Navigli, 2009" startWordPosition="1603" endWordPosition="1606">, we select a contextually adequate subset of contexts in which T has been observed, using T0 = act(T, s) as a generalized representation of meaning of target T in the context of s. Previous approaches used all of P as a representation for a paraphrase candidate P. However, P includes also irrelevant exemplars, while for a paraphrase to be judged as good, it is sufficient that one plausible reading exists. Therefore, we use P0 = act(P, s) to represent the paraphrase. 4 Experimental Evaluation Data. We evaluate our model on predicting paraphrases from the Lexical Substitution (LexSub) dataset (McCarthy and Navigli, 2009). This dataset consists of 2000 instances of 200 target words in sentential contexts, with paraphrases for each target word instance generated by up to 6 participants. Paraphrases are ranked by the number of annotators that chose them (cf. Table 1). Following Erk and Pad´o (2008), we take the list of paraphrase candidates for a target as given (computed by pooling all paraphrases that LexSub annotators proposed for the target) and use the models to rank them for any given sentence context. As exemplars, we create bag-of-words cooccurrence vectors from the BNC. These vectors represent instances</context>
</contexts>
<marker>McCarthy, Navigli, 2009</marker>
<rawString>D. McCarthy and R. Navigli. 2009. The English lexical substitution task. Language Resources and Evaluation, 43(2):139–159. Special Issue on Computational Semantic Analysis of Language: SemEval2007 and Beyond.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S McDonald</author>
<author>M Ramscar</author>
</authors>
<title>Testing the distributional hypothesis: The influence of context on judgements of semantic similarity.</title>
<date>2001</date>
<booktitle>In Proceedings of CogSci,</booktitle>
<pages>611--616</pages>
<contexts>
<context position="1048" citStr="McDonald and Ramscar, 2001" startWordPosition="143" endWordPosition="146"> model that activates only relevant occurrences. On a paraphrasing task, we find that a simple exemplar model outperforms more complex state-of-the-art models. 1 Introduction Distributional models are a popular framework for representing word meaning. They describe a lemma through a high-dimensional vector that records co-occurrence with context features over a large corpus. Distributional models have been used in many NLP analysis tasks (Salton et al., 1975; McCarthy and Carroll, 2003; Salton et al., 1975), as well as for cognitive modeling (Baroni and Lenci, 2009; Landauer and Dumais, 1997; McDonald and Ramscar, 2001). Among their attractive properties are their simplicity and versatility, as well as the fact that they can be acquired from corpora in an unsupervised manner. Distributional models are also attractive as a model of word meaning in context, since they do not have to rely on fixed sets of dictionary sense with their well-known problems (Kilgarriff, 1997; McCarthy and Navigli, 2009). Also, they can be used directly for testing paraphrase applicability (Szpektor et al., 2008), a task that has recently become prominent in the context of textual entailment (Bar-Haim et al., 2007). However, polysemy</context>
</contexts>
<marker>McDonald, Ramscar, 2001</marker>
<rawString>S. McDonald and M. Ramscar. 2001. Testing the distributional hypothesis: The influence of context on judgements of semantic similarity. In Proceedings of CogSci, pages 611–616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mitchell</author>
<author>M Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>236--244</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="2267" citStr="Mitchell and Lapata, 2008" startWordPosition="344" endWordPosition="347">lysemy is a fundamental problem for distributional models. Typically, distributional models compute a single “type” vector for a target word, which contains cooccurrence counts for all the occurrences of the target in a large corpus. If the target is polysemous, this vector mixes contextual features for all the senses of the target. For example, among the top 20 features for coach, we get match and team (for the “trainer” sense) as well as driver and car (for the “bus” sense). This problem has typically been approached by modifying the type vector for a target to better match a given context (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2009). In the terms of research on human concept representation, which often employs feature vector representations, the use of type vectors can be understood as aprototype-based approach, which uses a single vector per category. From this angle, computing prototypes throws away much interesting distributional information. A rival class of models is that of exemplar models, which memorize each seen instance of a category and perform categorization by comparing a new stimulus to each remembered exemplar vector. We can address the polysemy issue through an e</context>
<context position="4402" citStr="Mitchell and Lapata, 2008" startWordPosition="686" endWordPosition="689">es (Sch¨utze, 1998) or by dynamically modi92 Proceedings of the ACL 2010 Conference Short Papers, pages 92–97, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics Sentential context Paraphrase After a fire extinguisher is used, it must always be returned for recharging and its use recorded. bring back (3), take back (2), send back (1), give back (1) We return to the young woman who is reading the Wrigley’s wrapping paper. come back (3), revert (1), revisit (1), go (1) fying a word’s type vector according to each given sentence context (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2009). Polysemy-aware approaches also differ in their notion of context. Some use a bag-of-words representation of words in the current sentence (Sch¨utze, 1998; Landauer and Dumais, 1997), some make use of syntactic context (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2009). The approach that we present in the current paper computes a representation dynamically for each sentence context, using a simple bag-of-words representation of context. In cognitive science, prototype models predict degree of category membership through similarity </context>
<context position="9061" citStr="Mitchell and Lapata, 2008" startWordPosition="1450" endWordPosition="1453">In principle, activation could be treated not just as binary inclusion/exclusion, but also as a graded weighting scheme. However, weighting schemes introduce a large number of parameters, which we wanted to avoid. 2Each annotator was allowed to give up to three paraphrases per target in context. As a consequence, the number of gold paraphrases per target sentence varies. 93 actT kNN perc. 36.1 35.5 36.2 35.2 36.1 35.3 36.0 35.3 35.9 35.1 36.0 35.0 35.9 34.8 36.0 34.7 35.9 34.5 potential paraphrases of T, the task is to predict which of the paraphrases are applicable in s. Previous approaches (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Erk and Pad´o, 2009; Thater et al., 2009) have performed this task by modifying the type vector for T to the context s and then comparing the resulting vector T0 to the type vector of a paraphrase candidate P. In our exemplar setting, we select a contextually adequate subset of contexts in which T has been observed, using T0 = act(T, s) as a generalized representation of meaning of target T in the context of s. Previous approaches used all of P as a representation for a paraphrase candidate P. However, P includes also irrelevant exemplars, while for a paraphrase to be ju</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>J. Mitchell and M. Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL, pages 236–244, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G L Murphy</author>
</authors>
<title>The Big Book of Concepts.</title>
<date>2002</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="5131" citStr="Murphy, 2002" startWordPosition="802" endWordPosition="803">use a bag-of-words representation of words in the current sentence (Sch¨utze, 1998; Landauer and Dumais, 1997), some make use of syntactic context (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2009). The approach that we present in the current paper computes a representation dynamically for each sentence context, using a simple bag-of-words representation of context. In cognitive science, prototype models predict degree of category membership through similarity to a single prototype, while exemplar theory represents a concept as a collection of all previously seen exemplars (Murphy, 2002). Griffiths et al. (2007) found that the benefit of exemplars over prototypes grows with the number of available exemplars. The problem of representing meaning in context, which we consider in this paper, is closely related to the problem of concept combination in cognitive science, i.e., the derivation of representations for complex concepts (such as “metal spoon”) given the representations of base concepts (“metal” and “spoon”). While most approaches to concept combination are based on prototype models, Voorspoels et al. (2009) show superior results for an exemplar model based on exemplar ac</context>
</contexts>
<marker>Murphy, 2002</marker>
<rawString>G. L. Murphy. 2002. The Big Book of Concepts. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Wang</author>
<author>C Yang</author>
</authors>
<title>A vectorspace model for information retrieval.</title>
<date>1975</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>18</volume>
<pages>620</pages>
<contexts>
<context position="883" citStr="Salton et al., 1975" startWordPosition="117" endWordPosition="120">s paper describes ongoing work on distributional models for word meaning in context. We abandon the usual one-vectorper-word paradigm in favor of an exemplar model that activates only relevant occurrences. On a paraphrasing task, we find that a simple exemplar model outperforms more complex state-of-the-art models. 1 Introduction Distributional models are a popular framework for representing word meaning. They describe a lemma through a high-dimensional vector that records co-occurrence with context features over a large corpus. Distributional models have been used in many NLP analysis tasks (Salton et al., 1975; McCarthy and Carroll, 2003; Salton et al., 1975), as well as for cognitive modeling (Baroni and Lenci, 2009; Landauer and Dumais, 1997; McDonald and Ramscar, 2001). Among their attractive properties are their simplicity and versatility, as well as the fact that they can be acquired from corpora in an unsupervised manner. Distributional models are also attractive as a model of word meaning in context, since they do not have to rely on fixed sets of dictionary sense with their well-known problems (Kilgarriff, 1997; McCarthy and Navigli, 2009). Also, they can be used directly for testing paraph</context>
</contexts>
<marker>Salton, Wang, Yang, 1975</marker>
<rawString>G Salton, A Wang, and C Yang. 1975. A vectorspace model for information retrieval. Journal of the American Society for Information Science, 18:613– 620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Sch¨utze, 1998</marker>
<rawString>H. Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Szpektor</author>
<author>I Dagan</author>
<author>R Bar-Haim</author>
<author>J Goldberger</author>
</authors>
<title>Contextual preferences.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>683--691</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="1525" citStr="Szpektor et al., 2008" startWordPosition="219" endWordPosition="222">ll, 2003; Salton et al., 1975), as well as for cognitive modeling (Baroni and Lenci, 2009; Landauer and Dumais, 1997; McDonald and Ramscar, 2001). Among their attractive properties are their simplicity and versatility, as well as the fact that they can be acquired from corpora in an unsupervised manner. Distributional models are also attractive as a model of word meaning in context, since they do not have to rely on fixed sets of dictionary sense with their well-known problems (Kilgarriff, 1997; McCarthy and Navigli, 2009). Also, they can be used directly for testing paraphrase applicability (Szpektor et al., 2008), a task that has recently become prominent in the context of textual entailment (Bar-Haim et al., 2007). However, polysemy is a fundamental problem for distributional models. Typically, distributional models compute a single “type” vector for a target word, which contains cooccurrence counts for all the occurrences of the target in a large corpus. If the target is polysemous, this vector mixes contextual features for all the senses of the target. For example, among the top 20 features for coach, we get match and team (for the “trainer” sense) as well as driver and car (for the “bus” sense). T</context>
</contexts>
<marker>Szpektor, Dagan, Bar-Haim, Goldberger, 2008</marker>
<rawString>I. Szpektor, I. Dagan, R. Bar-Haim, and J. Goldberger. 2008. Contextual preferences. In Proceedings of ACL, pages 683–691, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Thater</author>
<author>G Dinu</author>
<author>M Pinkal</author>
</authors>
<title>Ranking paraphrases in context.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL Workshop on Applied Textual Inference,</booktitle>
<pages>44--47</pages>
<contexts>
<context position="2310" citStr="Thater et al., 2009" startWordPosition="352" endWordPosition="355">al models. Typically, distributional models compute a single “type” vector for a target word, which contains cooccurrence counts for all the occurrences of the target in a large corpus. If the target is polysemous, this vector mixes contextual features for all the senses of the target. For example, among the top 20 features for coach, we get match and team (for the “trainer” sense) as well as driver and car (for the “bus” sense). This problem has typically been approached by modifying the type vector for a target to better match a given context (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2009). In the terms of research on human concept representation, which often employs feature vector representations, the use of type vectors can be understood as aprototype-based approach, which uses a single vector per category. From this angle, computing prototypes throws away much interesting distributional information. A rival class of models is that of exemplar models, which memorize each seen instance of a category and perform categorization by comparing a new stimulus to each remembered exemplar vector. We can address the polysemy issue through an exemplar model by simply removing all exempl</context>
<context position="4445" citStr="Thater et al., 2009" startWordPosition="694" endWordPosition="697">ceedings of the ACL 2010 Conference Short Papers, pages 92–97, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics Sentential context Paraphrase After a fire extinguisher is used, it must always be returned for recharging and its use recorded. bring back (3), take back (2), send back (1), give back (1) We return to the young woman who is reading the Wrigley’s wrapping paper. come back (3), revert (1), revisit (1), go (1) fying a word’s type vector according to each given sentence context (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2009). Polysemy-aware approaches also differ in their notion of context. Some use a bag-of-words representation of words in the current sentence (Sch¨utze, 1998; Landauer and Dumais, 1997), some make use of syntactic context (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2009). The approach that we present in the current paper computes a representation dynamically for each sentence context, using a simple bag-of-words representation of context. In cognitive science, prototype models predict degree of category membership through similarity to a single prototype, while exemplar theor</context>
<context position="9125" citStr="Thater et al., 2009" startWordPosition="1462" endWordPosition="1465">n/exclusion, but also as a graded weighting scheme. However, weighting schemes introduce a large number of parameters, which we wanted to avoid. 2Each annotator was allowed to give up to three paraphrases per target in context. As a consequence, the number of gold paraphrases per target sentence varies. 93 actT kNN perc. 36.1 35.5 36.2 35.2 36.1 35.3 36.0 35.3 35.9 35.1 36.0 35.0 35.9 34.8 36.0 34.7 35.9 34.5 potential paraphrases of T, the task is to predict which of the paraphrases are applicable in s. Previous approaches (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Erk and Pad´o, 2009; Thater et al., 2009) have performed this task by modifying the type vector for T to the context s and then comparing the resulting vector T0 to the type vector of a paraphrase candidate P. In our exemplar setting, we select a contextually adequate subset of contexts in which T has been observed, using T0 = act(T, s) as a generalized representation of meaning of target T in the context of s. Previous approaches used all of P as a representation for a paraphrase candidate P. However, P includes also irrelevant exemplars, while for a paraphrase to be judged as good, it is sufficient that one plausible reading exists</context>
<context position="11241" citStr="Thater et al. (2009)" startWordPosition="1824" endWordPosition="1827">tagged, minus stop words. E.g., if the lemma gnurge occurs twice in the BNC, once in the sentence “The dog will gnurge the other dog”, and once in “The old windows gnurged”, the exemplar set for gnurge contains the vectors [dog-n: 2, othera:1] and [old-a: 1, window-n: 1]. For exemplar similarity, we use the standard Cosine similarity, and for the similarity of two exemplar sets, the Cosine of their centroids. Evaluation. The model’s prediction for an item is a list of paraphrases ranked by their predicted goodness of fit. To evaluate them against a weighted list of gold paraphrases, we follow Thater et al. (2009) in using Generalized Average Preciparameter 36.5 38.6 36.2 37.9 35.8 37.8 35.8 37.7 35.9 37.5 36.1 37.5 36.1 37.5 36.0 37.4 35.9 37.3 34.6 random BL 28.5 Table 2: Activation of T or P individually on the full LexSub dataset (GAP evaluation) sion (GAP), which interpolates the precision values of top-n prediction lists for increasing n. Let G = (q1, ... , qm) be the list of gold paraphrases with gold weights (y1, ... , ym). Let P = (p1, ... , pn) be the list of model predictions as ranked by the model, and let (x1,... , xn) be the gold weights associated with them (assume xi = 0 if pi E� G), wh</context>
<context position="18011" citStr="Thater et al. (2009" startWordPosition="3002" endWordPosition="3005"> with the highest average paraphrase frequency. Comparison to other models. Many of the other models are syntax-based and are therefore only applicable to a subset of the LexSub data. We have re-evaluated our exemplar models on the subsets we used in Erk and Pad´o (2008, EP08, 367 95 EP08 Models EP09 TDP09 EP08 dataset 27.4 NA NA EP09 dataset NA 32.2 36.5 actT actP actTP EP08 dataset 36.5 38.0 39.9 EP09 dataset 39.1 39.9 39.6 Table 4: Comparison to other models on two subsets of LexSub (GAP evaluation) datapoints) and Erk and Pad´o (2009, EP09, 100 datapoints). The second set was also used by Thater et al. (2009, TDP09). The results in Table 4 compare these models against our best previous exemplar models and show that our models outperform these models across the board. 3 Due to the small sizes of these datasets, statistical significance is more difficult to attain. On EP09, the differences among our models are not significant, but the difference between them and the original EP09 model is.4 On EP08, all differences are significant except for actP vs. actTP. We note that both the EP08 and the EP09 datasets appear to be simpler to model than the complete Lexical Substitution dataset, at least by our </context>
</contexts>
<marker>Thater, Dinu, Pinkal, 2009</marker>
<rawString>S. Thater, G. Dinu, and M. Pinkal. 2009. Ranking paraphrases in context. In Proceedings of the ACL Workshop on Applied Textual Inference, pages 44– 47, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Voorspoels</author>
<author>W Vanpaemel</author>
<author>G Storms</author>
</authors>
<title>The role of extensional information in conceptual combination.</title>
<date>2009</date>
<booktitle>In Proceedings of CogSci.</booktitle>
<contexts>
<context position="5666" citStr="Voorspoels et al. (2009)" startWordPosition="882" endWordPosition="886">eory represents a concept as a collection of all previously seen exemplars (Murphy, 2002). Griffiths et al. (2007) found that the benefit of exemplars over prototypes grows with the number of available exemplars. The problem of representing meaning in context, which we consider in this paper, is closely related to the problem of concept combination in cognitive science, i.e., the derivation of representations for complex concepts (such as “metal spoon”) given the representations of base concepts (“metal” and “spoon”). While most approaches to concept combination are based on prototype models, Voorspoels et al. (2009) show superior results for an exemplar model based on exemplar activation. In NLP, exemplar-based (memory-based) models have been applied to many problems (Daelemans et al., 1999). In the current paper, we use an exemplar model for computing distributional representations for word meaning in context, using the context to activate relevant exemplars. Comparing representations of context, bag-of-words (BOW) representations are more informative and noisier, while syntax-based representations deliver sparser and less noisy information. Following the hypothesis that richer, topical information is m</context>
</contexts>
<marker>Voorspoels, Vanpaemel, Storms, 2009</marker>
<rawString>W. Voorspoels, W. Vanpaemel, and G. Storms. 2009. The role of extensional information in conceptual combination. In Proceedings of CogSci.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>