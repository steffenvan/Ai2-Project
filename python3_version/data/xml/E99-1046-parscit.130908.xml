<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012348">
<note confidence="0.849882">
Proceedings of EACL &apos;99
</note>
<title confidence="0.991578">
95% Replicability for Manual Word Sense Tagging
</title>
<author confidence="0.995728">
Adam Kilgarriff
</author>
<affiliation confidence="0.998208">
ITRI, University of Brighton, Lewes Road, Brighton UK
</affiliation>
<email confidence="0.746782">
email: adamOitri.bton.ac.uk
</email>
<bodyText confidence="0.999923568627451">
People have been writing programs for auto-
matic Word Sense Disambiguation (WSD) for
forty years now, yet the validity of the task has
remained in doubt. At a first pass, the task is
simply defined: a word like bank can mean &apos;river
bank&apos; or &apos;money bank&apos; and the task-is to deter-
mine which of these applies in a context in which
the word bank appears. The problems arise be-
cause most sense distinctions are not as clear as
the distinction between &apos;river bank&apos; and &apos;money
bank&apos;, so it is not always straightforward for a
person to say what the correct answer is. Thus
we do not always know what it would mean to
say that a computer program got the right an-
swer. The issue is discussed in detail by (Gale
et al., 1992) who identify the problem as one
of identifying the &apos;upper bound&apos; for the perfor-
mance of a WSD program. If people can only
agree on the correct answer x% of the time, a
claim that a program achieves more than x% ac-
curacy is hard to interpret, and x% is the upper
bound for what the program can (meaningfully)
achieve.
There have been some discussions as to what
this upper bound might be. Gale et al. re-
view a psycholinguistic study (Jorgensen, 1990)
in which the level of agreement averaged 68%.
But an upper bound of 68% is disastrous for
the enterprise, since it implies that the best a
program could possibly do is still not remotely
good enough for any practical purpose.
Even worse news comes from (Ng and Lee,
1996), who re-tagged parts of the manually
tagged SEMCOR corpus (Fellbaum, 1998). The
taggings matched only 57% of the time.
If these represent as high a level of inter-
tagger agreement as one could ever expect,
WSD is a doomed enterprise. However, neither
study set out to identify an upper bound for
WSD and it is far from ideal to use their results
in this way. In this paper we report on a study
which did aim specifically at achieving as high
a level of replicability as possible.
The study took place within the context of
SENSEVAL, an evaluation exercise for WSD
programs.&apos; It was, clearly, critical to the va-
lidity of SENSEVAL as a whole to establish the
integrity of the &apos;gold standard&apos; corpus against
which WSD programs would be judged.
Measures taken to maximise the agreement
level were:
</bodyText>
<listItem confidence="0.849162636363636">
• humans: whereas other tagging exercises
had mostly used students, SENSEVAL
used professional lexicographers
• dictionary: the dictionary that provided
the sense inventory had lengthy entries,
with substantial numbers of examples
• task definition: in cases where none, or
more than one, of the senses applied, the
lexicographer was encouraged to tag the in-
stance as &amp;quot;unassignable&amp;quot; or with multiple
tags2
</listItem>
<footnote confidence="0.509222">
&apos;The exercise is chronicled at
http://www.itri.bton.ac.uk/events/senseval and in
(Kilgarriff and Palmer, Forthcoming), where a fuller ac-
count of all matters covered in the poster can be found.
2The scoring algorithm simply treated &amp;quot;unassignable&amp;quot;
as another tag. (Less than 1% of instnaces were tagged
&amp;quot;unassignable&amp;quot;.) Where there were multiple tags and
a partial match between taggings, partial credit was
assigned.
</footnote>
<page confidence="0.969858">
277
</page>
<table confidence="0.216331">
Proceedings of EACL &apos;99
</table>
<listItem confidence="0.896389">
• arbitration: first, two or three lexicogra-
</listItem>
<bodyText confidence="0.984584652173913">
phers provided taggings. Then, any in-
stances where these taggings were not iden-
tical were forwarded to a third lexicogra-
pher for arbitration.
The data for SENSEVAL comprised around
200 corpus instances for each of 35 words, mak-
ing a total of 8455 instances. A scoring scheme
was developed which assigned partial credit
where more than one sense had been assigned
to an instance. This was developed primarily
for scoring the WSD systems, but was also used
for scoring the lexicographers&apos; taggings.
At the time of the SENSEVAL workshop,
the tagging procedure (including arbitration)
had been undertaken once for each corpus in-
stance. We scored lexicographers&apos; initial pre-
arbitration results against the post-arbitration
results. The scores ranged between 88% to
100%, with just five out of 122 results for
&lt;lexicographer, word&gt; pairs falling below 95%.
To determine the replicability of the whole
process in a thoroughgoing way, we repeated it
for a sample of four of the words. The words
were selected to reflect the spread of difficulty:
we took the word which had given rise to the
lowest inter-tagger agreement in the previous
round, (generous, 6 senses), the word that had
given rise to the highest, (sack, 12 senses), and
two words from the middle of the range (onion,
5, and shake, 36). The 1057 corpus instances
for the four words were tagged by two lexicog-
raphers who had not seen the data before; the
non-identical taggings were forwarded to a third
for arbitration. These taggings were then com-
pared with the ones produced previously.
The table shows, for each word, the number of
corpus instances (Inst), the number of multiply-
tagged instances in each of the two sets of tag-
gings (A and B), and the level of agreement be-
tween the two sets (Agr).
There were 240 partial mismatches, with par-
tial credit assigned, in contrast to just 7 com-
plete mismatches.
A instance on which the taggings disagreed
was:
Give plants generous root space.
</bodyText>
<table confidence="0.9955095">
Word Inst A B Agr %
generous 227 76 68 88.7
onion 214 10 11 98.9
sack 260 0 3 99.4
shake 356 35 49 95.1
ALL 1057 121 131 95.5
</table>
<bodyText confidence="0.999294111111111">
Sense 4 of generous is defined as simply &amp;quot;abun-
dant; copious&amp;quot;, and sense 5 as &amp;quot;(of a room or
building) large in size; spacious&amp;quot;. One tag-
ging selected each. In general, taggings failed
to match where the definitions were vague and
overlapping, and where, as in sense 5, some part
of a defintion matches a corpus instance well
(&amp;quot;spacious&amp;quot;) but another part does not (&amp;quot;of a
room or building&amp;quot;).
</bodyText>
<sectionHeader confidence="0.745511" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.999955714285714">
The upper bound for WSD is around 95%, and
Gale et al.&apos;s worries about the integrity of the
task can be laid to rest. In order for manually
tagged test corpora to achieve 95% replicability,
it is critical to take care over the task definition,
to employ suitably qualified individuals, and to
double-tag and include an arbitration phase.
</bodyText>
<sectionHeader confidence="0.998603" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9992843">
Christiane Fellbaum, editor. 1998. WordNet: An
Electronic Lexical Database. MIT Press, Cam-
bridge, Mass.
William Gale, Kenneth Church, and David
Yarowsky. 1992. Estimating upper and lower
bounds on the performance of word-sense disam-
biguation programs. In Proceedings, 30th ACL,
pages 249-156.
Julia C. Jorgensen. 1990. The psychological reality
of word senses. Journal of Psycholinguistic Re-
search, 19(3):167-190.
Adam Kilgarriff and Martha Palmer. Forthcom-
ing. Guest editors, Special Issue on SENSE-
VAL: Evaluating Word Sense Disambiguation Pro-
grams. Computers and the Humanities.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrat-
ing multiple knowledge sources to disambiguate
word sense: An exemplar-based approach. In
ACL Proceedings, pages 40-47, Technical Univer-
sity, Berlin, Santa Cruz, California.
</reference>
<page confidence="0.996936">
278
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.013482">
<note confidence="0.935018">Proceedings of EACL &apos;99</note>
<title confidence="0.98181">95% Replicability for Manual Word Sense Tagging</title>
<author confidence="0.999075">Adam Kilgarriff</author>
<affiliation confidence="0.966591">ITRI, University of Brighton, Lewes Road, Brighton UK</affiliation>
<email confidence="0.972901">adamOitri.bton.ac.uk</email>
<abstract confidence="0.993718253521127">People have been writing programs for automatic Word Sense Disambiguation (WSD) for forty years now, yet the validity of the task has remained in doubt. At a first pass, the task is defined: a word like mean &apos;river or &apos;money bank&apos; and the to determine which of these applies in a context in which word The problems arise because most sense distinctions are not as clear as the distinction between &apos;river bank&apos; and &apos;money bank&apos;, so it is not always straightforward for a person to say what the correct answer is. Thus we do not always know what it would mean to say that a computer program got the right answer. The issue is discussed in detail by (Gale et al., 1992) who identify the problem as one of identifying the &apos;upper bound&apos; for the performance of a WSD program. If people can only on the correct answer the time, a that a program achieves more than acis hard to interpret, and the upper bound for what the program can (meaningfully) achieve. There have been some discussions as to what this upper bound might be. Gale et al. review a psycholinguistic study (Jorgensen, 1990) in which the level of agreement averaged 68%. an of disastrous for the enterprise, since it implies that the best a program could possibly do is still not remotely good enough for any practical purpose. Even worse news comes from (Ng and Lee, 1996), who re-tagged parts of the manually tagged SEMCOR corpus (Fellbaum, 1998). The taggings matched only 57% of the time. If these represent as high a level of intertagger agreement as one could ever expect, WSD is a doomed enterprise. However, neither study set out to identify an upper bound for WSD and it is far from ideal to use their results in this way. In this paper we report on a study which did aim specifically at achieving as high a level of replicability as possible. The study took place within the context of SENSEVAL, an evaluation exercise for WSD programs.&apos; It was, clearly, critical to the validity of SENSEVAL as a whole to establish the integrity of the &apos;gold standard&apos; corpus against which WSD programs would be judged. Measures taken to maximise the agreement level were: • humans: whereas other tagging exercises had mostly used students, SENSEVAL used professional lexicographers • dictionary: the dictionary that provided the sense inventory had lengthy entries, with substantial numbers of examples • task definition: in cases where none, or more than one, of the senses applied, the lexicographer was encouraged to tag the instance as &amp;quot;unassignable&amp;quot; or with multiple &apos;The exercise is chronicled at in (Kilgarriff and Palmer, Forthcoming), where a fuller account of all matters covered in the poster can be found. scoring algorithm simply treated &amp;quot;unassignable&amp;quot; as another tag. (Less than 1% of instnaces were tagged &amp;quot;unassignable&amp;quot;.) Where there were multiple tags and a partial match between taggings, partial credit was assigned. 277 Proceedings of EACL &apos;99 • arbitration: first, two or three lexicographers provided taggings. Then, any instances where these taggings were not identical were forwarded to a third lexicographer for arbitration. The data for SENSEVAL comprised around 200 corpus instances for each of 35 words, making a total of 8455 instances. A scoring scheme was developed which assigned partial credit where more than one sense had been assigned to an instance. This was developed primarily for scoring the WSD systems, but was also used for scoring the lexicographers&apos; taggings. At the time of the SENSEVAL workshop, the tagging procedure (including arbitration) had been undertaken once for each corpus instance. We scored lexicographers&apos; initial prearbitration results against the post-arbitration results. The scores ranged between 88% to 100%, with just five out of 122 results for &lt;lexicographer, word&gt; pairs falling below 95%. To determine the replicability of the whole process in a thoroughgoing way, we repeated it for a sample of four of the words. The words were selected to reflect the spread of difficulty: we took the word which had given rise to the lowest inter-tagger agreement in the previous senses), the word that had rise to the highest, senses), and words from the middle of the range and The 1057 corpus instances for the four words were tagged by two lexicographers who had not seen the data before; the non-identical taggings were forwarded to a third for arbitration. These taggings were then compared with the ones produced previously. The table shows, for each word, the number of corpus instances (Inst), the number of multiplytagged instances in each of the two sets of taggings (A and B), and the level of agreement between the two sets (Agr). There were 240 partial mismatches, with partial credit assigned, in contrast to just 7 complete mismatches. A instance on which the taggings disagreed was: plants space. Word Inst A B Agr % generous 227 76 68 88.7 onion 214 10 11 98.9 sack 260 0 3 99.4 shake 356 35 49 95.1 ALL 1057 121 131 95.5 4 of defined as simply &amp;quot;abundant; copious&amp;quot;, and sense 5 as &amp;quot;(of a room or building) large in size; spacious&amp;quot;. One tagging selected each. In general, taggings failed to match where the definitions were vague and overlapping, and where, as in sense 5, some part of a defintion matches a corpus instance well (&amp;quot;spacious&amp;quot;) but another part does not (&amp;quot;of a room or building&amp;quot;). Conclusion The upper bound for WSD is around 95%, and Gale et al.&apos;s worries about the integrity of the task can be laid to rest. In order for manually tagged test corpora to achieve 95% replicability, it is critical to take care over the task definition, to employ suitably qualified individuals, and to double-tag and include an arbitration phase.</abstract>
<note confidence="0.776021">References Fellbaum, editor. 1998. An Lexical Database. Press, Cambridge, Mass. William Gale, Kenneth Church, and David</note>
<abstract confidence="0.4839294">Yarowsky. 1992. Estimating upper and lower bounds on the performance of word-sense disamprograms. In 30th ACL, pages 249-156. Julia C. Jorgensen. 1990. The psychological reality word senses. of Psycholinguistic Re- Adam Kilgarriff and Martha Palmer. Forthcoming. Guest editors, Special Issue on SENSE- VAL: Evaluating Word Sense Disambiguation Proand the Humanities.</abstract>
<note confidence="0.792454333333333">Hwee Tou Ng and Hian Beng Lee. 1996. Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach. In Proceedings, 40-47, Technical University, Berlin, Santa Cruz, California. 278</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Gale</author>
<author>Kenneth Church</author>
<author>David Yarowsky</author>
</authors>
<title>Estimating upper and lower bounds on the performance of word-sense disambiguation programs.</title>
<date>1992</date>
<booktitle>In Proceedings, 30th ACL,</booktitle>
<pages>249--156</pages>
<contexts>
<context position="884" citStr="Gale et al., 1992" startWordPosition="153" endWordPosition="156">now, yet the validity of the task has remained in doubt. At a first pass, the task is simply defined: a word like bank can mean &apos;river bank&apos; or &apos;money bank&apos; and the task-is to determine which of these applies in a context in which the word bank appears. The problems arise because most sense distinctions are not as clear as the distinction between &apos;river bank&apos; and &apos;money bank&apos;, so it is not always straightforward for a person to say what the correct answer is. Thus we do not always know what it would mean to say that a computer program got the right answer. The issue is discussed in detail by (Gale et al., 1992) who identify the problem as one of identifying the &apos;upper bound&apos; for the performance of a WSD program. If people can only agree on the correct answer x% of the time, a claim that a program achieves more than x% accuracy is hard to interpret, and x% is the upper bound for what the program can (meaningfully) achieve. There have been some discussions as to what this upper bound might be. Gale et al. review a psycholinguistic study (Jorgensen, 1990) in which the level of agreement averaged 68%. But an upper bound of 68% is disastrous for the enterprise, since it implies that the best a program co</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>William Gale, Kenneth Church, and David Yarowsky. 1992. Estimating upper and lower bounds on the performance of word-sense disambiguation programs. In Proceedings, 30th ACL, pages 249-156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia C Jorgensen</author>
</authors>
<title>The psychological reality of word senses.</title>
<date>1990</date>
<journal>Journal of Psycholinguistic Research,</journal>
<pages>19--3</pages>
<contexts>
<context position="1334" citStr="Jorgensen, 1990" startWordPosition="238" endWordPosition="239"> answer is. Thus we do not always know what it would mean to say that a computer program got the right answer. The issue is discussed in detail by (Gale et al., 1992) who identify the problem as one of identifying the &apos;upper bound&apos; for the performance of a WSD program. If people can only agree on the correct answer x% of the time, a claim that a program achieves more than x% accuracy is hard to interpret, and x% is the upper bound for what the program can (meaningfully) achieve. There have been some discussions as to what this upper bound might be. Gale et al. review a psycholinguistic study (Jorgensen, 1990) in which the level of agreement averaged 68%. But an upper bound of 68% is disastrous for the enterprise, since it implies that the best a program could possibly do is still not remotely good enough for any practical purpose. Even worse news comes from (Ng and Lee, 1996), who re-tagged parts of the manually tagged SEMCOR corpus (Fellbaum, 1998). The taggings matched only 57% of the time. If these represent as high a level of intertagger agreement as one could ever expect, WSD is a doomed enterprise. However, neither study set out to identify an upper bound for WSD and it is far from ideal to </context>
</contexts>
<marker>Jorgensen, 1990</marker>
<rawString>Julia C. Jorgensen. 1990. The psychological reality of word senses. Journal of Psycholinguistic Research, 19(3):167-190.</rawString>
</citation>
<citation valid="false">
<booktitle>Special Issue on SENSEVAL: Evaluating Word Sense Disambiguation Programs. Computers and the Humanities.</booktitle>
<editor>Adam Kilgarriff and Martha Palmer. Forthcoming. Guest editors,</editor>
<marker></marker>
<rawString>Adam Kilgarriff and Martha Palmer. Forthcoming. Guest editors, Special Issue on SENSEVAL: Evaluating Word Sense Disambiguation Programs. Computers and the Humanities.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Hian Beng Lee</author>
</authors>
<title>Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach.</title>
<date>1996</date>
<booktitle>In ACL Proceedings,</booktitle>
<pages>40--47</pages>
<institution>Technical University,</institution>
<location>Berlin, Santa Cruz, California.</location>
<contexts>
<context position="1606" citStr="Ng and Lee, 1996" startWordPosition="285" endWordPosition="288">f people can only agree on the correct answer x% of the time, a claim that a program achieves more than x% accuracy is hard to interpret, and x% is the upper bound for what the program can (meaningfully) achieve. There have been some discussions as to what this upper bound might be. Gale et al. review a psycholinguistic study (Jorgensen, 1990) in which the level of agreement averaged 68%. But an upper bound of 68% is disastrous for the enterprise, since it implies that the best a program could possibly do is still not remotely good enough for any practical purpose. Even worse news comes from (Ng and Lee, 1996), who re-tagged parts of the manually tagged SEMCOR corpus (Fellbaum, 1998). The taggings matched only 57% of the time. If these represent as high a level of intertagger agreement as one could ever expect, WSD is a doomed enterprise. However, neither study set out to identify an upper bound for WSD and it is far from ideal to use their results in this way. In this paper we report on a study which did aim specifically at achieving as high a level of replicability as possible. The study took place within the context of SENSEVAL, an evaluation exercise for WSD programs.&apos; It was, clearly, critical</context>
</contexts>
<marker>Ng, Lee, 1996</marker>
<rawString>Hwee Tou Ng and Hian Beng Lee. 1996. Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach. In ACL Proceedings, pages 40-47, Technical University, Berlin, Santa Cruz, California.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>