<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000772">
<title confidence="0.974875">
Data-Driven Response Generation in Social Media
</title>
<author confidence="0.811055">
Alan Ritter
</author>
<affiliation confidence="0.7817295">
Computer Sci. &amp; Eng.
University of Washington
</affiliation>
<address confidence="0.95668">
Seattle, WA 98195
</address>
<email confidence="0.999308">
aritter@cs.washington.edu
</email>
<author confidence="0.98626">
Colin Cherry
</author>
<affiliation confidence="0.988297">
National Research Council Canada
</affiliation>
<address confidence="0.791052">
Ottawa, Ontario, K1A 0R6
</address>
<email confidence="0.727435">
Colin.Cherry@nrc-cnrc.gc.ca
</email>
<author confidence="0.834431">
William B. Dolan
</author>
<affiliation confidence="0.833248">
Microsoft Research
</affiliation>
<address confidence="0.962774">
Redmond, WA 98052
</address>
<email confidence="0.998532">
billdol@microsoft.com
</email>
<sectionHeader confidence="0.998596" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999861947368421">
We present a data-driven approach to generat-
ing responses to Twitter status posts, based on
phrase-based Statistical Machine Translation.
We find that mapping conversational stimuli
onto responses is more difficult than translat-
ing between languages, due to the wider range
of possible responses, the larger fraction of
unaligned words/phrases, and the presence of
large phrase pairs whose alignment cannot be
further decomposed. After addressing these
challenges, we compare approaches based on
SMT and Information Retrieval in a human
evaluation. We show that SMT outperforms
IR on this task, and its output is preferred over
actual human responses in 15% of cases. As
far as we are aware, this is the first work to
investigate the use of phrase-based SMT to di-
rectly translate a linguistic stimulus into an ap-
propriate response.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999455044444445">
Recently there has been an explosion in the number
of people having informal, public conversations on
social media websites such as Facebook and Twit-
ter. This presents a unique opportunity to build
collections of naturally occurring conversations that
are orders of magnitude larger than those previously
available. These corpora, in turn, present new op-
portunities to apply data-driven techniques to con-
versational tasks.
We investigate the problem of response genera-
tion: given a conversational stimulus, generate an
appropriate response. Specifically, we employ a
large corpus of status-response pairs found on Twit-
ter to create a system that responds to Twitter status
posts. Note that we make no mention of context, in-
tent or dialogue state; our goal is to generate any re-
sponse that fits the provided stimulus; however, we
do so without employing rules or templates, with the
hope of creating a system that is both flexible and
extensible when operating in an open domain.
Success in open domain response generation
could be immediately useful to social media plat-
forms, providing a list of suggested responses to a
target status, or providing conversation-aware auto-
complete for responses in progress. These features
are especially important on hand-held devices (Has-
selgren et al., 2003). Response generation should
also be beneficial in building “chatterbots” (Weizen-
baum, 1966) for entertainment purposes or compan-
ionship (Wilks, 2006). However, we are most ex-
cited by the future potential of data-driven response
generation when used inside larger dialogue sys-
tems, where direct consideration of the user’s utter-
ance could be combined with dialogue state (Wong
and Mooney, 2007; Langner et al., 2010) to generate
locally coherent, purposeful dialogue.
In this work, we investigate statistical machine
translation as an approach for response generation.
We are motivated by the following observation: In
naturally occurring discourse, there is often a strong
structural relationship between adjacent utterances
(Hobbs, 1985). For example, consider the stimulus-
response pair from the data:
Stimulus: I’m slowly making this soup
...... and it smells gorgeous!
</bodyText>
<page confidence="0.985474">
583
</page>
<note confidence="0.90519575">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 583–593,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
Response: I’ll bet it looks delicious too!
Haha
</note>
<bodyText confidence="0.9999595625">
Here “it” in the response refers to “this soup” in
the status by co-reference; however, there is also a
more subtle relationship between the “smells” and
“looks”, as well as “gorgeous” and “delicious”. Par-
allelisms such as these are frequent in naturally oc-
curring conversations, leading us to ask whether it
might be possible to translate a stimulus into an ap-
propriate response. We apply SMT to this problem,
treating Twitter as our parallel corpus, with status
posts as our source language and their responses as
our target language. However, the established SMT
pipeline cannot simply be applied out of the box.
We identify two key challenges in adapting SMT
to the response generation task. First, unlike bilin-
gual text, stimulus-response pairs are not semanti-
cally equivalent, leading to a wider range of possible
responses for a given stimulus phrase. Furthermore,
both sides of our parallel text are written in the same
language. Thus, the most strongly associated word
or phrase pairs found by off-the-shelf word align-
ment and phrase extraction tools are identical pairs.
We address this issue with constraints and features to
limit lexical overlap. Secondly, in stimulus-response
pairs, there are far more unaligned words than in
bilingual pairs; it is often the case that large portions
of the stimulus are not referenced in the response
and vice versa. Also, there are more large phrase-
pairs that cannot be easily decomposed (for example
see figure 2). These difficult cases confuse the IBM
word alignment models. Instead of relying on these
alignments to extract phrase-pairs, we consider all
possible phrase-pairs in our parallel text, and apply
an association-based filter.
We compare our approach to response genera-
tion against two Information Retrieval or nearest
neighbour approaches, which use the input stimu-
lus to select a response directly from the training
data. We analyze the advantages and disadvantages
of each approach, and perform an evaluation using
human annotators from Amazon’s Mechanical Turk.
Along the way, we investigate the utility of SMT’s
BLEU evaluation metric when applied to this do-
main. We show that SMT-based solutions outper-
form IR-based solutions, and are chosen over actual
human responses in our data in 15% of cases. As far
as we are aware, this is the first work to investigate
the feasibility of SMT’s application to generating re-
sponses to open-domain linguistic stimuli.
</bodyText>
<sectionHeader confidence="0.999786" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998388380952381">
There has been a long history of “chatterbots”
(Weizenbaum, 1966; Isbell et al., 2000; Shaikh et
al., 2010), which attempt to engage users, typically
leading the topic of conversation. They usually limit
interactions to a specific scenario (e.g. a Rogerian
psychotherapist), and use a set of template rules for
generating responses. In contrast, we focus on the
simpler task of generating an appropriate response
to a single utterance. We leverage large amounts of
conversational training data to scale to our Social
Media domain, where conversations can be on just
about any topic.
Additionally, there has been work on generat-
ing more natural utterances in goal-directed dia-
logue systems (Ratnaparkhi, 2000; Rambow et al.,
2001). Currently, most dialogue systems rely on ei-
ther canned responses or templates for generation,
which can result in utterances which sound very
unnatural in context (Chambers and Allen, 2004).
Recent work has investigated the use of SMT in
translating internal dialogue state into natural lan-
guage (Langner et al., 2010). In addition to dialogue
state, we believe it may be beneficial to consider
the user’s utterance when generating responses in or-
der to generate locally coherent discourse (Barzilay
and Lapata, 2005). Data-driven generation based on
users’ utterances might also be a useful way to fill in
knowledge gaps in the system (Galley et al., 2001;
Knight and Hatzivassiloglou, 1995).
Statistical machine translation has been applied to
a sm¨org˚asbord of NLP problems, including question
answering (Echihabi and Marcu, 2003), semantic
parsing and generation (Wong and Mooney, 2006;
Wong and Mooney, 2007), summarization (Daum´e
III and Marcu, 2009), generating bid-phrases in on-
line advertising (Ravi et al., 2010), spelling correc-
tion (Sun et al., 2010), paraphrase (Dolan et al.,
2004; Quirk et al., 2004) and query expansion (Rie-
zler et al., 2007). Most relevant to our efforts is the
work by Soricut and Marcu (2006), who applied the
IBM word alignment models to a discourse order-
ing task, exploiting the same intuition investigated
</bodyText>
<page confidence="0.99718">
584
</page>
<bodyText confidence="0.999927714285714">
in this paper: certain words (or phrases) tend to trig-
ger the usage of other words in subsequent discourse
units. As far as we are aware, ours is the first work
to explore the use of phrase-based translation in gen-
erating responses to open-domain linguistic stimuli,
although the analogy between translation and dia-
logue has been drawn (Leuski and Traum, 2010).
</bodyText>
<sectionHeader confidence="0.99665" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999975789473684">
For learning response-generation models, we use
a corpus of roughly 1.3 million conversations
scraped from Twitter (Ritter et al., 2010; Danescu-
Niculescu-Mizil et al., 2011). Twitter conversations
don’t occur in real-time as in IRC; rather as in email,
users typically take turns responding to each other.
Twitter’s 140 character limit, however, keeps con-
versations chat-like. In addition, the Twitter API
maintains a reference from each reply to the post
it responds to, so unlike IRC, there is no need for
conversation disentanglement (Elsner and Charniak,
2008; Wang and Oard, 2009). The first message of a
conversation is typically unique, not directed at any
particular user but instead broadcast to the author’s
followers (a status message). For the purposes of
this paper, we limit the data set to only the first two
utterances from each conversation. As a result of
this constraint, any system trained with this data will
be specialized for responding to Twitter status posts.
</bodyText>
<sectionHeader confidence="0.985029" genericHeader="method">
4 Response Generation as Translation
</sectionHeader>
<bodyText confidence="0.999959529411764">
When applied to conversations, SMT models the
probability of a response r given the input status-
post s using a log-linear combination of feature
functions. Most prominent among these features
are the conditional phrase-translation probabilities
in both directions, P(sIr) and P(rIs), which ensure
r is an appropriate response to s, and the language
model P(r), which ensures r is a well-formed re-
sponse. As in translation, the response models are
estimated from counts of phrase pairs observed in
the training bitext, and the language model is built
using n-gram statistics from a large set of observed
responses. To find the best response to a given input
status-post, we employ the Moses phrase-based de-
coder (Koehn et al., 2007), which conducts a beam
search for the best response given the input, accord-
ing to the log-linear model.
</bodyText>
<equation confidence="0.8563805">
what . . . ■ ■
time . . . ■ ■
u ■ . . . .
get . ■ . . .
out . . ■ . .
? . . . . .
</equation>
<figureCaption confidence="0.988128666666667">
Figure 1: Example from the data where word alignment
is easy. There is a clear correspondence between words
in the status and the response.
</figureCaption>
<subsectionHeader confidence="0.996292">
4.1 Challenge: Lexical Repetition
</subsectionHeader>
<bodyText confidence="0.999730857142857">
When applied directly to conversation data, off-the-
shelf MT systems simply learn to parrot back the
input, sometimes with slight modification. For ex-
ample, directly applying Moses with default settings
to the conversation data produces a system which
yields the following (typical) output on the above
example:
Stimulus: I’m slowly making this soup
...... and it smells gorgeous!
Response: i’m slowly making this soup
...... and you smell gorgeous!
This “paraphrasing” phenomenon occurs because
identical word pairs are frequently observed together
in the training data. Because there is a wide range
of acceptable responses to any status, these identical
pairs have the strongest associations in the data, and
therefore dominate the phrase table. In order to dis-
courage lexically similar translations, we filter out
all phrase-pairs where one phrase is a substring of
the other, and introduce a novel feature to penalize
lexical similarity:
</bodyText>
<equation confidence="0.527228">
Olex(s, t) = J(s, t)
</equation>
<bodyText confidence="0.9962735">
Where J(s, t) is the Jaccard similarity between the
set of words in s and t.
</bodyText>
<subsectionHeader confidence="0.966278">
4.2 Challenge: Word Alignment
</subsectionHeader>
<bodyText confidence="0.9999734">
Alignment is more difficult in conversational data
than bilingual data (Brown et al., 1990), or textual
entailment data (Brockett, 2006; MacCartney et al.,
2008). In conversational data, there are some cases
in which there is a decomposable alignment between
</bodyText>
<figure confidence="0.830477565217391">
at
i
get
off
5
585
if . . . .
anyones . . . .
still . . . .
awake . . . .
lets . . . .
play . . . .
a . . . .
game. . . . .
name ■ ■ ■ .
3 ■ ■ ■ .
kevin ■ ■ ■ .
costner ■ ■ ■ .
movies ■ ■ ■ .
that ■ ■ ■ .
dont ■ ■ ■ .
suck ■ ■ ■ .
. . . .■
</figure>
<figureCaption confidence="0.991117333333333">
Figure 2: Example from the data where word alignment
is difficult (requires alignment between large phrases in
the status and response).
</figureCaption>
<bodyText confidence="0.999592722222222">
words, as seen in figure 1, and some difficult cases
where alignment between large phrases is required,
for example figure 2. These difficult sentence pairs
confuse the IBM word alignment models which have
no way to distinguish between the easy and hard
cases.
We aligned words in our parallel data using the
widely used tool GIZA++ (Och and Ney, 2003);
however, the standard growing heuristic resulted in
very noisy alignments. Precision could be improved
considerably by using the intersection of GIZA++
trained in two directions (s -+ r, and r -+ s), but the
alignment also became extremely sparse. The aver-
age number of alignments-per status/response pair
in our data was only 1.7, as compared to a dataset
of aligned French-English sentence pairs (the WMT
08 news commentary data) where the average num-
ber of intersection alignments is 14.
</bodyText>
<subsectionHeader confidence="0.894918">
Direct Phrase Pair Extraction
</subsectionHeader>
<bodyText confidence="0.99764525">
Because word alignment in status/response pairs is
a difficult problem, instead of relying on local align-
ments for extracting phrase pairs, we exploit infor-
mation from all occurrences of the pair in determin-
</bodyText>
<table confidence="0.702385666666667">
C(s, t) C(s, -,t) C(s)
C(-,s, t) C(-,s, -,t) N − C(s)
C(t) N − C(t) N
</table>
<figureCaption confidence="0.977280666666667">
Figure 3: Contingency table for phrase pair (s,t). Fisher’s
Exact Test estimates the probability of seeing this event,
or one more extreme assuming s and t are independent.
</figureCaption>
<bodyText confidence="0.994628294117647">
ing whether its phrases form a valid mapping.
We consider all possible phrase-pairs in the train-
ing data,1 then use Fisher’s Exact Test to filter out
pairs with low correlation (Johnson et al., 2007).
Given a source and target phrase s and t, we consider
the contingency table illustrated in figure 3, which
includes co-occurrence counts for s and t, the num-
ber of sentence-pairs containing s, but not t and vice
versa, in addition to the number of pairs containing
neither s nor t. Fisher’s Exact Test provides us with
an estimate of the probability of observing this table,
or one more extreme, assuming s and t are indepen-
dent; in other words it gives us a measure of how
strongly associated they are. In contrast to statistical
tests such as χ2, or the G2 Log Likelihood Ratio,
Fisher’s Exact Test produces accurate p-values even
when the expected counts are small (as is extremely
common in our case).
In Fisher’s Exact Test, the hypergeometric proba-
bility distribution is used to compute the exact prob-
ability of a particular joint frequency assuming a
model of independence:
C(s)!C(-,s)!C(t)!C(-,t)!
N!C(s, t)!C(-,s, t)!C(s, -,t)!C(-,s, -,t)!
The statistic is computed by summing the prob-
ability for the joint frequency in Table 3, and ev-
ery more extreme joint frequency consistent with the
marginal frequencies. Moore (2004) illustrates sev-
eral tricks which make this computation feasible in
practice.
We found that this approach generates phrase-
table entries which appear quite reasonable upon
manual inspection. The top 20 phrase-pairs (after fil-
tering out identical source/target phrases, substrings,
</bodyText>
<footnote confidence="0.904132">
1We define a possible phrase-pair as any pair of phrases
found in a sentence-pair from our training corpus, where both
phrases consist of 4 tokens or fewer. The total number of phrase
pairs in a sentence pair (s, r) is O(|s |x |r|).
</footnote>
<bodyText confidence="0.52211175">
easier
question
please
.
</bodyText>
<page confidence="0.838641">
586
</page>
<table confidence="0.999672095238095">
Source Target
rt [retweet] thanks for the
potter harry
ice cream
how are you you ?
good morning
chuck norris
watching movie
i miss miss you too
are you i ’m
my birthday happy birthday
wish me luck good luck
how was it was
miss you i miss
swine flu
i love you love you too
how are are you ?
did you i did
jackson michael
how are you i ’m good
michael mj
</table>
<tableCaption confidence="0.874635">
Table 1: Top 20 Phrase Pairs ranked by the Fisher Exact
Test statistic. Slight variations (substrings or symmetric
pairs) were removed to show more variety. See the sup-
plementary materials for the top 10k (unfiltered) pairs.
</tableCaption>
<bodyText confidence="0.9991992">
and symmetric pairs) are listed in Table 1.2 Our ex-
periments in §6 show that using the phrase table pro-
duced by Fisher’s Exact Test outperforms one gen-
erated based on the poor quality IBM word align-
ments.
</bodyText>
<subsectionHeader confidence="0.998265">
4.3 System Details
</subsectionHeader>
<bodyText confidence="0.947457833333333">
For the phrase-table used in the experiments (§6) we
used the 5M phrases with highest association ac-
cording the Fisher Exact Test statistic.3 To build
the language model, we used all of the 1.3M re-
sponses from the training data, along with roughly
1M replies collected using Twitter’s streaming API.
</bodyText>
<footnote confidence="0.948148125">
2See the supplementary materials for the top 10k (unfiltered)
phrase pairs.
3Note that this includes an arbitrary subset of the (1,1,1)
pairs (phrase pairs where both phrases were only observed once
in the data). Excluding these (1,1,1) pairs yields a rather small
phrase table, 201K phrase-pairs after filtering, while including
all of them led to a table which was too large for the memory of
the machine used to conduct the experiments.
</footnote>
<bodyText confidence="0.9996216">
We do not use any form of SMT reordering
model, as the position of the phrase in the response
does not seem to be very correlated with the corre-
sponding position in the status. Instead we let the
language model drive reordering.
We used the default feature weights provided by
Moses.4 Because automatic evaluation of response
generation is an open problem, we avoided the use of
discriminative training algorithms such as Minimum
Error-Rate Training (Och, 2003).
</bodyText>
<sectionHeader confidence="0.998431" genericHeader="method">
5 Information Retrieval
</sectionHeader>
<bodyText confidence="0.999786777777778">
One straightforward data-driven approach to re-
sponse generation is nearest neighbour, or informa-
tion retrieval. This general approach has been ap-
plied previously by several authors (Isbell et al.,
2000; Swanson and Gordon, 2008; Jafarpour and
Burges, 2010), and is used as a point of compari-
son in our experiments. Given a novel status s and a
training corpus of status/response pairs, two retrieval
strategies can be used to return a best response r&apos;:
IR-STATUS [rargmaxi sim(s,si)] Retrieve the re-
sponse ri whose associated status message si
is most similar to the user’s input s.
IR-RESPONSE [rargmaxi sim(s,ri)] Retrieve the re-
sponse ri which has highest similarity when di-
rectly compared to s.
At first glance, IR-STATUS may appear to be the
most promising option; intuitively, if an input status
is very similar to a training status, we might expect
the corresponding training response to pair well with
the input. However, as we describe in §6, it turns
out that directly retrieving the most similar response
(IR-RESPONSE) tends to return acceptable replies
more reliably, as judged by human annotators. To
implement our two IR response generators, we rely
on the default similarity measure implemented in the
Lucene5 Information Retrieval Library, which is an
IDF-weighted Vector-Space similarity.
</bodyText>
<sectionHeader confidence="0.999826" genericHeader="conclusions">
6 Experiments
</sectionHeader>
<bodyText confidence="0.9917755">
In order to compare various approaches to auto-
mated response generation, we used human evalu-
</bodyText>
<footnote confidence="0.98984675">
4The language model weight was set to 0.5, the translation
model weights in both directions were both set to 0.2, the lexical
similarity weight was set to -0.2.
5http://lucene.apache.org/
</footnote>
<page confidence="0.993627">
587
</page>
<bodyText confidence="0.998858555555556">
ators from Amazon’s Mechanical Turk (Snow et al.,
2008). Human evaluation also provides us with data
for a preliminary investigation into the feasibility
of automatic evaluation metrics. While automated
evaluation has been investigated in the area of spo-
ken dialogue systems (Jung et al., 2009), it is unclear
how well it will correlate with human judgment in
open-domain conversations where the range of pos-
sible responses is very large.
</bodyText>
<subsectionHeader confidence="0.981029">
6.1 Experimental Conditions
</subsectionHeader>
<bodyText confidence="0.999968161290322">
We performed pairwise comparisons of several
response-generation systems. Similar work on eval-
uating MT output (Bloodgood and Callison-Burch,
2010) has asked Turkers to rank more than two
choices, but in order to keep our evaluation as
straightforward as possible, we limited our experi-
ments to pairwise comparisons.
For each experiment comparing 2 systems (a and
b), we built a test set by selecting a random sam-
ple of 200 tweets which had received responses,
and which had a length between 4 and 20 words.
These tweets were selected from conversations col-
lected from a later, non-overlapping time-period
from those used in training. Each experiment used
a different random sample of 200 tweets. For each
of the 200 statuses, we generated a response using
method a and b, then showed the status and both re-
sponses to the Turkers, asking them to choose the
best response. The order of the systems used to
generate a response was randomized, and each of
the 200 HITs was submitted to 3 different Turkers.
Turkers were paid 1¢ per judgment.
The Turkers were instructed that an appropriate
response should be on the same topic as the sta-
tus, and should also “make sense” in response to it.
While this is an inherently subjective task, from in-
specting the results, we found Turkers to be quite
competent in judging between two responses.
The systems used in these pairwise comparisons
are summarized in table 2, and example output gen-
erated by each system is presented in Table 3.
</bodyText>
<subsectionHeader confidence="0.775305">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.99677875">
The results of the experiments are summarized in
Table 4. For each experiment we show the fraction
of HITs where the majority of annotators agreed sys-
tem a was better. We also show the p-value from an
</bodyText>
<table confidence="0.999593055555556">
System Name Description
RND-BASELINE Picks randomly from the set of
responses which are observed at
least twice in the training data.
The assumption is these are
likely very general responses
IR-STATUS rargmaxi sim(s,si) as described
in §5
IR-RESPONSE rargmaxi sim(s,ri) as described
in §5
MT-CHAT Phrase-based translation system
as described in §4
MT-BASELINE Exactly the same as MT-CHAT,
except using a phrase table ex-
tracted based on word align-
ments from GIZA++
HUMAN Actual responses from the test
data.
</table>
<tableCaption confidence="0.999606">
Table 2: Summary of systems compared experimentally
</tableCaption>
<bodyText confidence="0.978574142857143">
exact Binomial significance test; note that all dif-
ferences are significant with above 95% confidence.
Table 4 also shows the 5 coefficient (Bennett et al.,
1954) as a measure of agreement between annotators
for each experiment. 5 is equivalent the commonly
used Kappa, except it assumes the uniform distribu-
tion when computing expected or chance agreement
(Artstein and Poesio, 2008). The uniform distribu-
tion is appropriate in our setup, since annotators are
not told which system generated each output, and the
order of choices is randomized. If annotators were
indeed choosing randomly, they would be expected
to agree in 50% of cases. In most comparisons the
value of 5 is in the range of 0.2 to 0.4, which can
be considered “Fair agreement” (Landis and Koch,
1977).
Note that agreement between annotators is lower
than typically reported in corpus annotation tasks.
When annotating which of two automatically gen-
erated outputs is better, there is not always a clear
answer; both responses might be good or bad. We
can expect strong agreement only in cases where
one response is clearly better. Strong agreement is
not required, however, as we are using many anno-
tations to compare each pair of systems, and the hu-
man judgments are not intended to be used as train-
ing data. Similar agreement was reported in an eval-
uation of automatically generated MT output as part
</bodyText>
<page confidence="0.971517">
588
</page>
<bodyText confidence="0.939803515151515">
Status MT-CHAT MT-BASELINE IR-STATUS HUMAN RND-BASELINE IR-RESPONSE
riding to the good luck on your i ca n’t wait for it grab some re- make sure to ride lmfaoooo ! u sure the cup wo
arc on my bike. bike ride arc ! i ’m arc is a year freshing drinks in the middle of n’t melt ?
Hopefully I won’t . ... and a towel ... the street
melt into the lol ... then eat ice
pavement. cream and suika
later ...
I really hate me too , what me too ! i love :( cheer up ! you Why? no comfy tas fazendo oq ? someone was
watching movies movies are you those ? ) in my need to invest in spots or just too watching in the
in the living doing in room ? mouth. one of those turbo open to public living room kasi e
room. fans .:) i was too lazy
to go up to my
room .:) )
Hello Did I miss hello miss ? * it ’s just like what it did n’t show yet nothing yet...he you wish. are they gon na
David Getting he was getting his you just did the -ha- -lol- presented though show teen choice
his teen choice will probably win exact date hello ? he is so darn cute awards again ? i
award? Or did . missed it
they not show it :(
I can think the im- i think you can i ca n’t wait to i hope you use for sure Eric, and no -user- ? nobody with
possible is possi- do it , anything see if there is no those powers for that makes it not whom i might
ble at any given is given impossi- point in the end evil. i mean good ’impossible’ any apply the blue
moment. ble dreams really but if you do a ! good . that ’s longer. cobalt ’ rabbit ’
specs few hours ... what i meant. at the moment .
given the econ-
omy , perhaps
-url- ( any of em )
I want to enter my me too ! thanks oh man , i do i put troy on the check TLC I’m good job you want to do
parents house into for the win , par- n’t want to come potty everyday at pretty sure it was laundry with me
a home makeover ents are so smart back to my mom lunch and b4 u them who were at my parents
contest. Any help , just got home is getting that know it he was recently posting house after i get
out there? makeover going on his own about looking off ? maybe get a
... only took a for houses to be free meal out of it
week. nominated! ? !
</bodyText>
<tableCaption confidence="0.9996705">
Table 3: Example responses from each system. We tried to pick examples where most (or all) systems generate
reasonable responses for illustration purposes.
</tableCaption>
<table confidence="0.9995665">
System A System B Fraction A p-value Agreement System A System B
(S) BLEU BLEU
MT-CHAT* IR-STATUS 0.645 5.0e-05 0.347 1.15 0.57
MT-CHAT* IR-RESPONSE 0.593 1.0e-02 0.333 0.84 1.53
IR-STATUS IR-RESPONSE* 0.422 3.3e-02 0.330 0.40 1.59
MT-CHAT* MT-BASELINE 0.577 3.8e-02 0.160 1.23 1.14
MT-CHAT HUMAN* 0.145 2.2e-16 0.433 N/A N/A
MT-CHAT* RND-BASELINE 0.880 2.2e-16 0.383 1.17 0.10
</table>
<tableCaption confidence="0.8836465">
Table 4: Results of pairwise comparisons between various response-generation methods. Each row presents a com-
parison between systems a and b on 200 randomly selected tweets. The column Fraction A lists the fraction of HITs
where the majority of annotators agreed System A’s response was better. The winning system is indicated with an
asterisk*. All differences are significant.
</tableCaption>
<page confidence="0.98817">
589
</page>
<bodyText confidence="0.998904661016949">
of the WMT09 shared tasks (Callison-Burch et al., listed in Table 5.
2009).6 We also evaluated the effect of filtering all possi-
The results of the paired evaluations provide a ble phrase pairs using Fisher’s Exact Test, which we
clear ordering on the automatic systems: IR-STATUS did instead of conducting phrase extraction accord-
is outperformed by IR-RESPONSE, which is in turn ing to the very noisy word alignments. We altered
outperformed by MT-CHAT. These results are our MT-CHAT system to use the standard Moses
somewhat surprising. We had expected that match- phrase-extraction pipeline, creating the system de-
ing status to status would create a more natural and noted as MT-BASELINE. We compared this to the
effective IR system, but in practice, it appears that complete MT-CHAT system. Note that both systems
the additional level of indirection employed by IR- account for lexical repetition as described in §4.1.
STATUS created only more opportunity for confu- MT-CHAT’s output is preferred 58% of the time over
sion and error. Also, we did not necessarily expect MT-BASELINE, indicating that direct phrase extrac-
MT-CHAT’s output to be preferred by human anno- tion is useful in this conversational setting.
tators: the SMT system is the only one that generates Finally, as an additional baseline, we compared
a completely novel response, and is therefore the MT-CHAT’s output to random responses selected
system most likely to make fluency errors. We had from those observed 2 or more times in the train-
expected human annotators to pick up on these flu- ing data. One might argue that short, common re-
ency errors, giving the the advantage to the IR sys- sponses are very general, and that a reply like “lol”
tems. However, it appears that MT-CHAT’s ability could be considered a good response to almost any
to tailor its response to the status on a fine-grained status. However, the human evaluation shows a clear
scale overcame the disadvantage of occasionally in- preference for MT-CHAT’s output: raters favour re-
troducing fluency errors.7 sponses that are tailored to the stimulus.
Given MT-CHAT’s success over the IR systems, 6.3 Automatic Evaluation
we conducted further experiments to validate its out- The field of SMT has benefited greatly from
put. In order to test how close MT-CHAT’s responses the existence of an automatic evaluation metric,
come to human-level abilities, we compared its out- BLEU (Papineni et al., 2002), which grades an out-
put to actual human responses from our dataset. In put candidate according to n-gram matches to one or
some cases the human responses change the topic of more reference outputs. To evaluate whether BLEU
conversation, and completely ignore the initial sta- is an appropriate automatic evaluation measure for
tus. For instance, one frequent type of response we response generation, we attempted to measure its
noticed in the data was a greeting: “How have you agreement with the human judgments. We calcu-
been? I haven’t talked to you in a while.” For the late BLEU using a single reference derived from our
purposes of this evaluation, we manually filtered out parallel corpus. We show the smoothed BLEU 1-4
cases where the human response was completely off- scores for each system on each dataset evaluated in
topic from the status, selecting 200 pairs at random Table 4. Although these scores are extremely low,
that met our criteria and using the actual responses the overall BLEU scores agree with overall annota-
as the HUMAN output. tor judgments in all cases except when comparing
When compared to the actual human-generated MT-CHAT and IR-RESPONSE. It would seem that
response, MT-CHAT loses. However, its output is BLEU has some agreement with human judgments
preferred over the human responses 15% of the time, on this task, but perhaps not enough to be immedi-
a fact that is particularly surprising given the very ately useful.
small – by MT standards – amount of data used to 7 Conclusions
train the model. A few examples where MT-CHAT’s
output were selected over the human response are
6See inter annotator agreement in table 4. We have presented several data-driven approaches,
7Also, as one can see from the example exchanges in Ta- based on information retrial and phrase-based SMT,
ble 3, fluency errors are rampant across all systems, including to the task of automatically responding to open-
the gold-standard human responses.
590
Status MT-CHAT HUMAN
i want to go to a bonfire right about that does sound fun bonfire i wan na did my tweet bring that upon you ?
now. go ! hehe
I’m the invisible man. man, you could never be invisible i can still see your face surrounded
in blue !
put ya number in my phone ..and call me when u do that in your phone o well hw kum u dnt eva call or txt
ima call you up ! call :) for what ’s up ? me :) ? ! ? ! lolz jpn hw r ya tho :)
? ! ? !
Gray Maynard with hair? I did not maynard with grey hair ! i take it back, the hair isn’t working
approve this. for maynard.
</bodyText>
<tableCaption confidence="0.993119">
Table 5: Examples where MT-CHAT output was preferred over HUMAN response by Turker annotators
</tableCaption>
<bodyText confidence="0.999113580645161">
domain linguistic stimuli.
Our experiments show that SMT techniques are
better-suited than IR approaches on the task of re-
sponse generation. Our system, MT-CHAT, pro-
duced responses which were preferred by human an-
notators over actual human responses 15% of the
time. Although this is still far from human-level
performance, we believe there is much room for
improvement: from designing appropriate word-
alignment and decoding algorithms that account for
the selective nature of response in dialogue, to sim-
ply adding more training data.
We described the many challenges posed by
adapting phrase-based SMT to dialogue, and pre-
sented initial solutions to several, including direct
phrasal alignment, and phrase-table scores discour-
aging responses that are lexically similar to the sta-
tus. Finally, we have provided results from an initial
experiment to evaluate the BLEU metric when ap-
plied to response generation, showing that though
the metric as is does not work well, there is suffi-
cient correlation to suggest that a similar, dialogue-
focused approach may be feasible.
By generating responses to Tweets out of context,
we have demonstrated that the models underlying
phrase-based SMT are capable of guiding the con-
struction of appropriate responses. In the future, we
are excited about the role these models could po-
tentially play in guiding response construction for
conversationally-aware chat input schemes, as well
as goal-directed dialogue systems.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99978625">
We would like to thank Oren Etzioni, Michael
Gamon, Jerry Hobbs, Dirk Hovy, Yun-Cheng Ju,
Kristina Toutanova, Saif Mohammad, Patrick Pan-
tel, and Luke Zettlemoyer, in addition to the anony-
mous reviewers for helpful discussions and com-
ments on a previous draft. The first author is sup-
pored by a National Defense Science and Engineer-
ing Graduate (NDSEG) Fellowship 32 CFR 168a.
</bodyText>
<sectionHeader confidence="0.998834" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998704333333333">
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Comput. Lin-
guist., 34:555–596, December.
Regina Barzilay and Mirella Lapata. 2005. Modeling
local coherence: an entity-based approach. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, ACL ’05.
E. M. Bennett, R. Alpert, and A. C. Goldstein. 1954.
Communications through limited-response question-
ing. Public Opinion Quarterly, 18(3):303–308.
Michael Bloodgood and Chris Callison-Burch. 2010.
Using mechanical turk to build machine translation
evaluation sets. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon’s Mechanical Turk, CSLDAMT
’10, pages 208–211, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Chris Brockett. 2006. Aligning the rte 2006 corpus. In
Microsoft Research Techincal report MSR-TR-2007-
77.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, John D. Laf-
ferty, Robert L. Mercer, and Paul S. Roossin. 1990. A
statistical approach to machine translation. Comput.
Linguist., 16:79–85, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009 work-
shop on statistical machine translation. In Proceedings
of the Fourth Workshop on Statistical Machine Trans-
lation, StatMT ’09.
</reference>
<page confidence="0.990595">
591
</page>
<reference confidence="0.997486773584906">
Nathanael Chambers and James Allen. 2004. Stochas-
tic language generation in a dialogue system: Toward
a domain independent generator. In Michael Strube
and Candy Sidner, editors, Proceedings of the 5th SIG-
dial Workshop on Discourse and Dialogue, pages 9–
18, Cambridge, Massachusetts, USA, April 30 - May
1. Association for Computational Linguistics.
Cristian Danescu-Niculescu-Mizil, Michael Gamon, and
Susan Dumais. 2011. Mark my words! Linguistic
style accommodation in social media. In Proceedings
of WWW.
Hal Daum´e III and Daniel Marcu. 2009. Induction of
word and phrase alignments for automatic document
summarization. CoRR, abs/0907.0804.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
exploiting massively parallel news sources. In Pro-
ceedings of the 20th international conference on Com-
putational Linguistics, COLING ’04, Morristown, NJ,
USA. Association for Computational Linguistics.
Abdessamad Echihabi and Daniel Marcu. 2003. A
noisy-channel approach to question answering. In
Proceedings of the 41st Annual Meeting on Associa-
tion for Computational Linguistics - Volume 1, ACL
’03, pages 16–23, Morristown, NJ, USA. Association
for Computational Linguistics.
Micha Elsner and Eugene Charniak. 2008. You talking
to me? a corpus and algorithm for conversation disen-
tanglement. In Proceedings of ACL-08: HLT, June.
Michel Galley, Eric Fosler-Lussier, and Alexandros
Potamianos. 2001. Hybrid natural language gener-
ation for spoken dialogue systems. In Proceedings
of the 7th European Conference on Speech Commu-
nication and Technology (EUROSPEECH–01), pages
1735–1738, Aalborg, Denmark, September.
Jon Hasselgren, Erik Montnemery, Pierre Nugues, and
Markus Svensson. 2003. Hms: a predictive text entry
method using bigrams. In Proceedings of the 2003
EACL Workshop on Language Modeling for Text Entry
Methods, TextEntry ’03.
Jerry R. Hobbs. 1985. On the coherence and structure of
discourse.
Charles Lee Isbell, Jr., Michael J. Kearns, Dave Ko-
rmann, Satinder P. Singh, and Peter Stone. 2000.
Cobot in lambdamoo: A social statistics agent. In Pro-
ceedings of the Seventeenth National Conference on
Artificial Intelligence and Twelfth Conference on In-
novative Applications of Artificial Intelligence, pages
36–41. AAAI Press.
Sina Jafarpour and Christopher J. C. Burges. 2010. Fil-
ter, rank, and transfer the knowledge: Learning to chat.
Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving translation quality by dis-
carding most of the phrasetable. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 967–
975, Prague, Czech Republic, June. Association for
Computational Linguistics.
Sangkeun Jung, Cheongjae Lee, Kyungduk Kim, Min-
woo Jeong, and Gary Geunbae Lee. 2009. Data-
driven user simulation for automated evaluation of
spoken dialog systems. Comput. Speech Lang.,
23:479–509, October.
Kevin Knight and Vasileios Hatzivassiloglou. 1995.
Two-level, many-paths generation. In Proceedings of
the 33rd annual meeting on Association for Computa-
tional Linguistics, ACL ’95.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL. The
Association for Computer Linguistics.
J R Landis and G G Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics.
Brian Langner, Stephan Vogel, and Alan W. Black. 2010.
Evaluating a dialog language generation system: com-
paring the mountain system to other nlg approaches.
In INTERSPEECH.
Anton Leuski and David R. Traum. 2010. Practical
language processing for virtual humans. In Twenty-
Second Annual Conference on Innovative Applications
of Artificial Intelligence (IAAI-10).
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model for
natural language inference. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ’08, pages 802–811, Morristown,
NJ, USA. Association for Computational Linguistics.
Robert C. Moore. 2004. On log-likelihood-ratios and the
significance of rare events. In EMNLP.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.
F. J. Och. 2003. Minimum error rate training for statisti-
cal machine translation. In ACL, pages 160–167.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In ACL, pages 311–318.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language Processing,
pages 142–149.
</reference>
<page confidence="0.975025">
592
</page>
<reference confidence="0.99986756">
Owen Rambow, Srinivas Bangalore, and Marilyn Walker.
2001. Natural language generation in dialog systems.
In Proceedings of the first international conference on
Human language technology research, HLT ’01, pages
1–4, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Adwait Ratnaparkhi. 2000. Trainable methods for sur-
face natural language generation. In Proceedings of
the 1st North American chapter of the Association for
Computational Linguistics conference.
Sujith Ravi, Andrei Broder, Evgeniy Gabrilovich, Vanja
Josifovski, Sandeep Pandey, and Bo Pang. 2010. Au-
tomatic generation of bid phrases for online advertis-
ing. In Proceedings of the third ACM international
conference on Web search and data mining, WSDM
’10.
Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-
taridis, Vibhu Mittal, and Yi Liu. 2007. Statistical
machine translation for query expansion in answer re-
trieval. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
464–471, Prague, Czech Republic, June. Association
for Computational Linguistics.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, HLT ’10, pages 172–180,
Morristown, NJ, USA. Association for Computational
Linguistics.
Samira Shaikh, Tomek Strzalkowski, Sarah Taylor, and
Nick Webb. 2010. Vca: an experiment with a mul-
tiparty virtual chat agent. In Proceedings of the 2010
Workshop on Companionable Dialogue Systems.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast—but is it good?:
evaluating non-expert annotations for natural language
tasks. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Radu Soricut and Daniel Marcu. 2006. Discourse gener-
ation using utility-trained coherence models. In Pro-
ceedings of the COLING/ACL on Main conference
poster sessions, COLING-ACL ’06.
Xu Sun, Jianfeng Gao, Daniel Micol, and Chris Quirk.
2010. Learning phrase-based spelling error models
from clickthrough data. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, ACL ’10, pages 266–274, Morristown,
NJ, USA. Association for Computational Linguistics.
Reid Swanson and Andrew S. Gordon. 2008. Say any-
thing: A massively collaborative open domain story
writing companion. In Proceedings of the 1st Joint
International Conference on Interactive Digital Story-
telling: Interactive Storytelling, ICIDS ’08, pages 32–
40, Berlin, Heidelberg. Springer-Verlag.
Lidan Wang and Douglas W. Oard. 2009. Context-based
message expansion for disentanglement of interleaved
text conversations. In HLT-NAACL.
Joseph Weizenbaum. 1966. Eliza: a computer program
for the study of natural language communication be-
tween man and machine. Commun. ACM, 9:36–45,
January.
Yorick Wilks. 2006. Artificial companions as a new kind
of interface to the future internet. In OII Research Re-
port No. 13.
Yuk Wah Wong and Raymond Mooney. 2006. Learning
for semantic parsing with statistical machine transla-
tion. In Proceedings of the Human Language Technol-
ogy Conference of the NAACL, Main Conference.
Yuk Wah Wong and Raymond Mooney. 2007. Gener-
ation by inverting a semantic parser that uses statis-
tical machine translation. In Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics; Proceedings of the Main Conference.
</reference>
<page confidence="0.999047">
593
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.449704">
<title confidence="0.996117">Data-Driven Response Generation in Social Media</title>
<author confidence="0.995937">Alan</author>
<affiliation confidence="0.9930495">Computer Sci. &amp; University of</affiliation>
<address confidence="0.999881">Seattle, WA 98195</address>
<email confidence="0.999392">aritter@cs.washington.edu</email>
<author confidence="0.945894">Colin Cherry</author>
<affiliation confidence="0.998704">National Research Council Canada</affiliation>
<address confidence="0.993019">Ottawa, Ontario, K1A 0R6</address>
<email confidence="0.72071">Colin.Cherry@nrc-cnrc.gc.ca</email>
<author confidence="0.757514">B William</author>
<affiliation confidence="0.946716">Microsoft</affiliation>
<address confidence="0.999808">Redmond, WA 98052</address>
<email confidence="0.999727">billdol@microsoft.com</email>
<abstract confidence="0.9981442">We present a data-driven approach to generating responses to Twitter status posts, based on phrase-based Statistical Machine Translation. We find that mapping conversational stimuli onto responses is more difficult than translating between languages, due to the wider range of possible responses, the larger fraction of unaligned words/phrases, and the presence of large phrase pairs whose alignment cannot be further decomposed. After addressing these challenges, we compare approaches based on SMT and Information Retrieval in a human evaluation. We show that SMT outperforms IR on this task, and its output is preferred over actual human responses in 15% of cases. As far as we are aware, this is the first work to investigate the use of phrase-based SMT to directly translate a linguistic stimulus into an appropriate response.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Inter-coder agreement for computational linguistics.</title>
<date>2008</date>
<journal>Comput. Linguist.,</journal>
<pages>34--555</pages>
<contexts>
<context position="22240" citStr="Artstein and Poesio, 2008" startWordPosition="3637" endWordPosition="3640">tion system as described in §4 MT-BASELINE Exactly the same as MT-CHAT, except using a phrase table extracted based on word alignments from GIZA++ HUMAN Actual responses from the test data. Table 2: Summary of systems compared experimentally exact Binomial significance test; note that all differences are significant with above 95% confidence. Table 4 also shows the 5 coefficient (Bennett et al., 1954) as a measure of agreement between annotators for each experiment. 5 is equivalent the commonly used Kappa, except it assumes the uniform distribution when computing expected or chance agreement (Artstein and Poesio, 2008). The uniform distribution is appropriate in our setup, since annotators are not told which system generated each output, and the order of choices is randomized. If annotators were indeed choosing randomly, they would be expected to agree in 50% of cases. In most comparisons the value of 5 is in the range of 0.2 to 0.4, which can be considered “Fair agreement” (Landis and Koch, 1977). Note that agreement between annotators is lower than typically reported in corpus annotation tasks. When annotating which of two automatically generated outputs is better, there is not always a clear answer; both</context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Comput. Linguist., 34:555–596, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: an entity-based approach.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05.</booktitle>
<contexts>
<context position="7284" citStr="Barzilay and Lapata, 2005" startWordPosition="1119" endWordPosition="1122">nerating more natural utterances in goal-directed dialogue systems (Ratnaparkhi, 2000; Rambow et al., 2001). Currently, most dialogue systems rely on either canned responses or templates for generation, which can result in utterances which sound very unnatural in context (Chambers and Allen, 2004). Recent work has investigated the use of SMT in translating internal dialogue state into natural language (Langner et al., 2010). In addition to dialogue state, we believe it may be beneficial to consider the user’s utterance when generating responses in order to generate locally coherent discourse (Barzilay and Lapata, 2005). Data-driven generation based on users’ utterances might also be a useful way to fill in knowledge gaps in the system (Galley et al., 2001; Knight and Hatzivassiloglou, 1995). Statistical machine translation has been applied to a sm¨org˚asbord of NLP problems, including question answering (Echihabi and Marcu, 2003), semantic parsing and generation (Wong and Mooney, 2006; Wong and Mooney, 2007), summarization (Daum´e III and Marcu, 2009), generating bid-phrases in online advertising (Ravi et al., 2010), spelling correction (Sun et al., 2010), paraphrase (Dolan et al., 2004; Quirk et al., 2004)</context>
</contexts>
<marker>Barzilay, Lapata, 2005</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2005. Modeling local coherence: an entity-based approach. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Bennett</author>
<author>R Alpert</author>
<author>A C Goldstein</author>
</authors>
<title>Communications through limited-response questioning.</title>
<date>1954</date>
<journal>Public Opinion Quarterly,</journal>
<volume>18</volume>
<issue>3</issue>
<contexts>
<context position="22018" citStr="Bennett et al., 1954" startWordPosition="3603" endWordPosition="3606">east twice in the training data. The assumption is these are likely very general responses IR-STATUS rargmaxi sim(s,si) as described in §5 IR-RESPONSE rargmaxi sim(s,ri) as described in §5 MT-CHAT Phrase-based translation system as described in §4 MT-BASELINE Exactly the same as MT-CHAT, except using a phrase table extracted based on word alignments from GIZA++ HUMAN Actual responses from the test data. Table 2: Summary of systems compared experimentally exact Binomial significance test; note that all differences are significant with above 95% confidence. Table 4 also shows the 5 coefficient (Bennett et al., 1954) as a measure of agreement between annotators for each experiment. 5 is equivalent the commonly used Kappa, except it assumes the uniform distribution when computing expected or chance agreement (Artstein and Poesio, 2008). The uniform distribution is appropriate in our setup, since annotators are not told which system generated each output, and the order of choices is randomized. If annotators were indeed choosing randomly, they would be expected to agree in 50% of cases. In most comparisons the value of 5 is in the range of 0.2 to 0.4, which can be considered “Fair agreement” (Landis and Koc</context>
</contexts>
<marker>Bennett, Alpert, Goldstein, 1954</marker>
<rawString>E. M. Bennett, R. Alpert, and A. C. Goldstein. 1954. Communications through limited-response questioning. Public Opinion Quarterly, 18(3):303–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Bloodgood</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Using mechanical turk to build machine translation evaluation sets.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, CSLDAMT ’10,</booktitle>
<pages>208--211</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="19753" citStr="Bloodgood and Callison-Burch, 2010" startWordPosition="3220" endWordPosition="3223">cene.apache.org/ 587 ators from Amazon’s Mechanical Turk (Snow et al., 2008). Human evaluation also provides us with data for a preliminary investigation into the feasibility of automatic evaluation metrics. While automated evaluation has been investigated in the area of spoken dialogue systems (Jung et al., 2009), it is unclear how well it will correlate with human judgment in open-domain conversations where the range of possible responses is very large. 6.1 Experimental Conditions We performed pairwise comparisons of several response-generation systems. Similar work on evaluating MT output (Bloodgood and Callison-Burch, 2010) has asked Turkers to rank more than two choices, but in order to keep our evaluation as straightforward as possible, we limited our experiments to pairwise comparisons. For each experiment comparing 2 systems (a and b), we built a test set by selecting a random sample of 200 tweets which had received responses, and which had a length between 4 and 20 words. These tweets were selected from conversations collected from a later, non-overlapping time-period from those used in training. Each experiment used a different random sample of 200 tweets. For each of the 200 statuses, we generated a respo</context>
</contexts>
<marker>Bloodgood, Callison-Burch, 2010</marker>
<rawString>Michael Bloodgood and Chris Callison-Burch. 2010. Using mechanical turk to build machine translation evaluation sets. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, CSLDAMT ’10, pages 208–211, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Brockett</author>
</authors>
<title>Aligning the rte 2006 corpus.</title>
<date>2006</date>
<journal>In Microsoft Research Techincal</journal>
<volume>report</volume>
<pages>2007--77</pages>
<contexts>
<context position="11806" citStr="Brockett, 2006" startWordPosition="1865" endWordPosition="1866">ide range of acceptable responses to any status, these identical pairs have the strongest associations in the data, and therefore dominate the phrase table. In order to discourage lexically similar translations, we filter out all phrase-pairs where one phrase is a substring of the other, and introduce a novel feature to penalize lexical similarity: Olex(s, t) = J(s, t) Where J(s, t) is the Jaccard similarity between the set of words in s and t. 4.2 Challenge: Word Alignment Alignment is more difficult in conversational data than bilingual data (Brown et al., 1990), or textual entailment data (Brockett, 2006; MacCartney et al., 2008). In conversational data, there are some cases in which there is a decomposable alignment between at i get off 5 585 if . . . . anyones . . . . still . . . . awake . . . . lets . . . . play . . . . a . . . . game. . . . . name ■ ■ ■ . 3 ■ ■ ■ . kevin ■ ■ ■ . costner ■ ■ ■ . movies ■ ■ ■ . that ■ ■ ■ . dont ■ ■ ■ . suck ■ ■ ■ . . . . .■ Figure 2: Example from the data where word alignment is difficult (requires alignment between large phrases in the status and response). words, as seen in figure 1, and some difficult cases where alignment between large phrases is requi</context>
</contexts>
<marker>Brockett, 2006</marker>
<rawString>Chris Brockett. 2006. Aligning the rte 2006 corpus. In Microsoft Research Techincal report MSR-TR-2007-77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>John Cocke</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Fredrick Jelinek</author>
<author>John D Lafferty</author>
<author>Robert L Mercer</author>
<author>Paul S Roossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Comput. Linguist.,</journal>
<pages>16--79</pages>
<contexts>
<context position="11762" citStr="Brown et al., 1990" startWordPosition="1857" endWordPosition="1860">gether in the training data. Because there is a wide range of acceptable responses to any status, these identical pairs have the strongest associations in the data, and therefore dominate the phrase table. In order to discourage lexically similar translations, we filter out all phrase-pairs where one phrase is a substring of the other, and introduce a novel feature to penalize lexical similarity: Olex(s, t) = J(s, t) Where J(s, t) is the Jaccard similarity between the set of words in s and t. 4.2 Challenge: Word Alignment Alignment is more difficult in conversational data than bilingual data (Brown et al., 1990), or textual entailment data (Brockett, 2006; MacCartney et al., 2008). In conversational data, there are some cases in which there is a decomposable alignment between at i get off 5 585 if . . . . anyones . . . . still . . . . awake . . . . lets . . . . play . . . . a . . . . game. . . . . name ■ ■ ■ . 3 ■ ■ ■ . kevin ■ ■ ■ . costner ■ ■ ■ . movies ■ ■ ■ . that ■ ■ ■ . dont ■ ■ ■ . suck ■ ■ ■ . . . . .■ Figure 2: Example from the data where word alignment is difficult (requires alignment between large phrases in the status and response). words, as seen in figure 1, and some difficult cases wh</context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Fredrick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. 1990. A statistical approach to machine translation. Comput. Linguist., 16:79–85, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>Findings of the 2009 workshop on statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation, StatMT ’09.</booktitle>
<marker>Callison-Burch, Koehn, Monz, Schroeder, 2009</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Josh Schroeder. 2009. Findings of the 2009 workshop on statistical machine translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation, StatMT ’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>James Allen</author>
</authors>
<title>Stochastic language generation in a dialogue system: Toward a domain independent generator.</title>
<date>2004</date>
<booktitle>In Michael Strube and Candy Sidner, editors, Proceedings of the 5th SIGdial Workshop on Discourse and Dialogue, pages 9– 18,</booktitle>
<volume>30</volume>
<pages>-</pages>
<location>Cambridge, Massachusetts, USA,</location>
<contexts>
<context position="6956" citStr="Chambers and Allen, 2004" startWordPosition="1067" endWordPosition="1070">plate rules for generating responses. In contrast, we focus on the simpler task of generating an appropriate response to a single utterance. We leverage large amounts of conversational training data to scale to our Social Media domain, where conversations can be on just about any topic. Additionally, there has been work on generating more natural utterances in goal-directed dialogue systems (Ratnaparkhi, 2000; Rambow et al., 2001). Currently, most dialogue systems rely on either canned responses or templates for generation, which can result in utterances which sound very unnatural in context (Chambers and Allen, 2004). Recent work has investigated the use of SMT in translating internal dialogue state into natural language (Langner et al., 2010). In addition to dialogue state, we believe it may be beneficial to consider the user’s utterance when generating responses in order to generate locally coherent discourse (Barzilay and Lapata, 2005). Data-driven generation based on users’ utterances might also be a useful way to fill in knowledge gaps in the system (Galley et al., 2001; Knight and Hatzivassiloglou, 1995). Statistical machine translation has been applied to a sm¨org˚asbord of NLP problems, including </context>
</contexts>
<marker>Chambers, Allen, 2004</marker>
<rawString>Nathanael Chambers and James Allen. 2004. Stochastic language generation in a dialogue system: Toward a domain independent generator. In Michael Strube and Candy Sidner, editors, Proceedings of the 5th SIGdial Workshop on Discourse and Dialogue, pages 9– 18, Cambridge, Massachusetts, USA, April 30 - May 1. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristian Danescu-Niculescu-Mizil</author>
<author>Michael Gamon</author>
<author>Susan Dumais</author>
</authors>
<title>Mark my words! Linguistic style accommodation in social media.</title>
<date>2011</date>
<booktitle>In Proceedings of WWW.</booktitle>
<marker>Danescu-Niculescu-Mizil, Gamon, Dumais, 2011</marker>
<rawString>Cristian Danescu-Niculescu-Mizil, Michael Gamon, and Susan Dumais. 2011. Mark my words! Linguistic style accommodation in social media. In Proceedings of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Induction of word and phrase alignments for automatic document summarization.</title>
<date>2009</date>
<location>CoRR, abs/0907.0804.</location>
<marker>Daum´e, Marcu, 2009</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2009. Induction of word and phrase alignments for automatic document summarization. CoRR, abs/0907.0804.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics, COLING ’04,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="7863" citStr="Dolan et al., 2004" startWordPosition="1206" endWordPosition="1209">t discourse (Barzilay and Lapata, 2005). Data-driven generation based on users’ utterances might also be a useful way to fill in knowledge gaps in the system (Galley et al., 2001; Knight and Hatzivassiloglou, 1995). Statistical machine translation has been applied to a sm¨org˚asbord of NLP problems, including question answering (Echihabi and Marcu, 2003), semantic parsing and generation (Wong and Mooney, 2006; Wong and Mooney, 2007), summarization (Daum´e III and Marcu, 2009), generating bid-phrases in online advertising (Ravi et al., 2010), spelling correction (Sun et al., 2010), paraphrase (Dolan et al., 2004; Quirk et al., 2004) and query expansion (Riezler et al., 2007). Most relevant to our efforts is the work by Soricut and Marcu (2006), who applied the IBM word alignment models to a discourse ordering task, exploiting the same intuition investigated 584 in this paper: certain words (or phrases) tend to trigger the usage of other words in subsequent discourse units. As far as we are aware, ours is the first work to explore the use of phrase-based translation in generating responses to open-domain linguistic stimuli, although the analogy between translation and dialogue has been drawn (Leuski a</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources. In Proceedings of the 20th international conference on Computational Linguistics, COLING ’04, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abdessamad Echihabi</author>
<author>Daniel Marcu</author>
</authors>
<title>A noisy-channel approach to question answering.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03,</booktitle>
<pages>16--23</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="7601" citStr="Echihabi and Marcu, 2003" startWordPosition="1166" endWordPosition="1169">vestigated the use of SMT in translating internal dialogue state into natural language (Langner et al., 2010). In addition to dialogue state, we believe it may be beneficial to consider the user’s utterance when generating responses in order to generate locally coherent discourse (Barzilay and Lapata, 2005). Data-driven generation based on users’ utterances might also be a useful way to fill in knowledge gaps in the system (Galley et al., 2001; Knight and Hatzivassiloglou, 1995). Statistical machine translation has been applied to a sm¨org˚asbord of NLP problems, including question answering (Echihabi and Marcu, 2003), semantic parsing and generation (Wong and Mooney, 2006; Wong and Mooney, 2007), summarization (Daum´e III and Marcu, 2009), generating bid-phrases in online advertising (Ravi et al., 2010), spelling correction (Sun et al., 2010), paraphrase (Dolan et al., 2004; Quirk et al., 2004) and query expansion (Riezler et al., 2007). Most relevant to our efforts is the work by Soricut and Marcu (2006), who applied the IBM word alignment models to a discourse ordering task, exploiting the same intuition investigated 584 in this paper: certain words (or phrases) tend to trigger the usage of other words </context>
</contexts>
<marker>Echihabi, Marcu, 2003</marker>
<rawString>Abdessamad Echihabi and Daniel Marcu. 2003. A noisy-channel approach to question answering. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03, pages 16–23, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Eugene Charniak</author>
</authors>
<title>You talking to me? a corpus and algorithm for conversation disentanglement.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<contexts>
<context position="9050" citStr="Elsner and Charniak, 2008" startWordPosition="1399" endWordPosition="1402"> and dialogue has been drawn (Leuski and Traum, 2010). 3 Data For learning response-generation models, we use a corpus of roughly 1.3 million conversations scraped from Twitter (Ritter et al., 2010; DanescuNiculescu-Mizil et al., 2011). Twitter conversations don’t occur in real-time as in IRC; rather as in email, users typically take turns responding to each other. Twitter’s 140 character limit, however, keeps conversations chat-like. In addition, the Twitter API maintains a reference from each reply to the post it responds to, so unlike IRC, there is no need for conversation disentanglement (Elsner and Charniak, 2008; Wang and Oard, 2009). The first message of a conversation is typically unique, not directed at any particular user but instead broadcast to the author’s followers (a status message). For the purposes of this paper, we limit the data set to only the first two utterances from each conversation. As a result of this constraint, any system trained with this data will be specialized for responding to Twitter status posts. 4 Response Generation as Translation When applied to conversations, SMT models the probability of a response r given the input statuspost s using a log-linear combination of feat</context>
</contexts>
<marker>Elsner, Charniak, 2008</marker>
<rawString>Micha Elsner and Eugene Charniak. 2008. You talking to me? a corpus and algorithm for conversation disentanglement. In Proceedings of ACL-08: HLT, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
</authors>
<title>Eric Fosler-Lussier, and Alexandros Potamianos.</title>
<date>2001</date>
<booktitle>In Proceedings of the 7th European Conference on Speech Communication and Technology (EUROSPEECH–01),</booktitle>
<pages>1735--1738</pages>
<location>Aalborg, Denmark,</location>
<marker>Galley, 2001</marker>
<rawString>Michel Galley, Eric Fosler-Lussier, and Alexandros Potamianos. 2001. Hybrid natural language generation for spoken dialogue systems. In Proceedings of the 7th European Conference on Speech Communication and Technology (EUROSPEECH–01), pages 1735–1738, Aalborg, Denmark, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Hasselgren</author>
<author>Erik Montnemery</author>
<author>Pierre Nugues</author>
<author>Markus Svensson</author>
</authors>
<title>Hms: a predictive text entry method using bigrams.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 EACL Workshop on Language Modeling for Text Entry Methods, TextEntry ’03.</booktitle>
<contexts>
<context position="2476" citStr="Hasselgren et al., 2003" startWordPosition="371" endWordPosition="375">s posts. Note that we make no mention of context, intent or dialogue state; our goal is to generate any response that fits the provided stimulus; however, we do so without employing rules or templates, with the hope of creating a system that is both flexible and extensible when operating in an open domain. Success in open domain response generation could be immediately useful to social media platforms, providing a list of suggested responses to a target status, or providing conversation-aware autocomplete for responses in progress. These features are especially important on hand-held devices (Hasselgren et al., 2003). Response generation should also be beneficial in building “chatterbots” (Weizenbaum, 1966) for entertainment purposes or companionship (Wilks, 2006). However, we are most excited by the future potential of data-driven response generation when used inside larger dialogue systems, where direct consideration of the user’s utterance could be combined with dialogue state (Wong and Mooney, 2007; Langner et al., 2010) to generate locally coherent, purposeful dialogue. In this work, we investigate statistical machine translation as an approach for response generation. We are motivated by the followi</context>
</contexts>
<marker>Hasselgren, Montnemery, Nugues, Svensson, 2003</marker>
<rawString>Jon Hasselgren, Erik Montnemery, Pierre Nugues, and Markus Svensson. 2003. Hms: a predictive text entry method using bigrams. In Proceedings of the 2003 EACL Workshop on Language Modeling for Text Entry Methods, TextEntry ’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
</authors>
<title>On the coherence and structure of discourse.</title>
<date>1985</date>
<contexts>
<context position="3215" citStr="Hobbs, 1985" startWordPosition="480" endWordPosition="481">anionship (Wilks, 2006). However, we are most excited by the future potential of data-driven response generation when used inside larger dialogue systems, where direct consideration of the user’s utterance could be combined with dialogue state (Wong and Mooney, 2007; Langner et al., 2010) to generate locally coherent, purposeful dialogue. In this work, we investigate statistical machine translation as an approach for response generation. We are motivated by the following observation: In naturally occurring discourse, there is often a strong structural relationship between adjacent utterances (Hobbs, 1985). For example, consider the stimulusresponse pair from the data: Stimulus: I’m slowly making this soup ...... and it smells gorgeous! 583 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 583–593, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics Response: I’ll bet it looks delicious too! Haha Here “it” in the response refers to “this soup” in the status by co-reference; however, there is also a more subtle relationship between the “smells” and “looks”, as well as “gorgeous” and “delicious”. Parallelisms such</context>
</contexts>
<marker>Hobbs, 1985</marker>
<rawString>Jerry R. Hobbs. 1985. On the coherence and structure of discourse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Lee Isbell</author>
<author>Michael J Kearns</author>
<author>Dave Kormann</author>
<author>Satinder P Singh</author>
<author>Peter Stone</author>
</authors>
<title>Cobot in lambdamoo: A social statistics agent.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence,</booktitle>
<pages>36--41</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="6121" citStr="Isbell et al., 2000" startWordPosition="938" endWordPosition="941">tages and disadvantages of each approach, and perform an evaluation using human annotators from Amazon’s Mechanical Turk. Along the way, we investigate the utility of SMT’s BLEU evaluation metric when applied to this domain. We show that SMT-based solutions outperform IR-based solutions, and are chosen over actual human responses in our data in 15% of cases. As far as we are aware, this is the first work to investigate the feasibility of SMT’s application to generating responses to open-domain linguistic stimuli. 2 Related Work There has been a long history of “chatterbots” (Weizenbaum, 1966; Isbell et al., 2000; Shaikh et al., 2010), which attempt to engage users, typically leading the topic of conversation. They usually limit interactions to a specific scenario (e.g. a Rogerian psychotherapist), and use a set of template rules for generating responses. In contrast, we focus on the simpler task of generating an appropriate response to a single utterance. We leverage large amounts of conversational training data to scale to our Social Media domain, where conversations can be on just about any topic. Additionally, there has been work on generating more natural utterances in goal-directed dialogue syst</context>
<context position="17736" citStr="Isbell et al., 2000" startWordPosition="2906" endWordPosition="2909">e phrase in the response does not seem to be very correlated with the corresponding position in the status. Instead we let the language model drive reordering. We used the default feature weights provided by Moses.4 Because automatic evaluation of response generation is an open problem, we avoided the use of discriminative training algorithms such as Minimum Error-Rate Training (Och, 2003). 5 Information Retrieval One straightforward data-driven approach to response generation is nearest neighbour, or information retrieval. This general approach has been applied previously by several authors (Isbell et al., 2000; Swanson and Gordon, 2008; Jafarpour and Burges, 2010), and is used as a point of comparison in our experiments. Given a novel status s and a training corpus of status/response pairs, two retrieval strategies can be used to return a best response r&apos;: IR-STATUS [rargmaxi sim(s,si)] Retrieve the response ri whose associated status message si is most similar to the user’s input s. IR-RESPONSE [rargmaxi sim(s,ri)] Retrieve the response ri which has highest similarity when directly compared to s. At first glance, IR-STATUS may appear to be the most promising option; intuitively, if an input status</context>
</contexts>
<marker>Isbell, Kearns, Kormann, Singh, Stone, 2000</marker>
<rawString>Charles Lee Isbell, Jr., Michael J. Kearns, Dave Kormann, Satinder P. Singh, and Peter Stone. 2000. Cobot in lambdamoo: A social statistics agent. In Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence, pages 36–41. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sina Jafarpour</author>
<author>Christopher J C Burges</author>
</authors>
<title>Filter, rank, and transfer the knowledge: Learning to chat.</title>
<date>2010</date>
<contexts>
<context position="17791" citStr="Jafarpour and Burges, 2010" startWordPosition="2914" endWordPosition="2917">ry correlated with the corresponding position in the status. Instead we let the language model drive reordering. We used the default feature weights provided by Moses.4 Because automatic evaluation of response generation is an open problem, we avoided the use of discriminative training algorithms such as Minimum Error-Rate Training (Och, 2003). 5 Information Retrieval One straightforward data-driven approach to response generation is nearest neighbour, or information retrieval. This general approach has been applied previously by several authors (Isbell et al., 2000; Swanson and Gordon, 2008; Jafarpour and Burges, 2010), and is used as a point of comparison in our experiments. Given a novel status s and a training corpus of status/response pairs, two retrieval strategies can be used to return a best response r&apos;: IR-STATUS [rargmaxi sim(s,si)] Retrieve the response ri whose associated status message si is most similar to the user’s input s. IR-RESPONSE [rargmaxi sim(s,ri)] Retrieve the response ri which has highest similarity when directly compared to s. At first glance, IR-STATUS may appear to be the most promising option; intuitively, if an input status is very similar to a training status, we might expect </context>
</contexts>
<marker>Jafarpour, Burges, 2010</marker>
<rawString>Sina Jafarpour and Christopher J. C. Burges. 2010. Filter, rank, and transfer the knowledge: Learning to chat.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Howard Johnson</author>
<author>Joel Martin</author>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Improving translation quality by discarding most of the phrasetable.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>967--975</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="13831" citStr="Johnson et al., 2007" startWordPosition="2248" endWordPosition="2251">atus/response pairs is a difficult problem, instead of relying on local alignments for extracting phrase pairs, we exploit information from all occurrences of the pair in determinC(s, t) C(s, -,t) C(s) C(-,s, t) C(-,s, -,t) N − C(s) C(t) N − C(t) N Figure 3: Contingency table for phrase pair (s,t). Fisher’s Exact Test estimates the probability of seeing this event, or one more extreme assuming s and t are independent. ing whether its phrases form a valid mapping. We consider all possible phrase-pairs in the training data,1 then use Fisher’s Exact Test to filter out pairs with low correlation (Johnson et al., 2007). Given a source and target phrase s and t, we consider the contingency table illustrated in figure 3, which includes co-occurrence counts for s and t, the number of sentence-pairs containing s, but not t and vice versa, in addition to the number of pairs containing neither s nor t. Fisher’s Exact Test provides us with an estimate of the probability of observing this table, or one more extreme, assuming s and t are independent; in other words it gives us a measure of how strongly associated they are. In contrast to statistical tests such as χ2, or the G2 Log Likelihood Ratio, Fisher’s Exact Te</context>
</contexts>
<marker>Johnson, Martin, Foster, Kuhn, 2007</marker>
<rawString>Howard Johnson, Joel Martin, George Foster, and Roland Kuhn. 2007. Improving translation quality by discarding most of the phrasetable. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 967– 975, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sangkeun Jung</author>
<author>Cheongjae Lee</author>
<author>Kyungduk Kim</author>
<author>Minwoo Jeong</author>
<author>Gary Geunbae Lee</author>
</authors>
<title>Datadriven user simulation for automated evaluation of spoken dialog systems.</title>
<date>2009</date>
<journal>Comput. Speech Lang.,</journal>
<pages>23--479</pages>
<contexts>
<context position="19433" citStr="Jung et al., 2009" startWordPosition="3174" endWordPosition="3177"> Vector-Space similarity. 6 Experiments In order to compare various approaches to automated response generation, we used human evalu4The language model weight was set to 0.5, the translation model weights in both directions were both set to 0.2, the lexical similarity weight was set to -0.2. 5http://lucene.apache.org/ 587 ators from Amazon’s Mechanical Turk (Snow et al., 2008). Human evaluation also provides us with data for a preliminary investigation into the feasibility of automatic evaluation metrics. While automated evaluation has been investigated in the area of spoken dialogue systems (Jung et al., 2009), it is unclear how well it will correlate with human judgment in open-domain conversations where the range of possible responses is very large. 6.1 Experimental Conditions We performed pairwise comparisons of several response-generation systems. Similar work on evaluating MT output (Bloodgood and Callison-Burch, 2010) has asked Turkers to rank more than two choices, but in order to keep our evaluation as straightforward as possible, we limited our experiments to pairwise comparisons. For each experiment comparing 2 systems (a and b), we built a test set by selecting a random sample of 200 twe</context>
</contexts>
<marker>Jung, Lee, Kim, Jeong, Lee, 2009</marker>
<rawString>Sangkeun Jung, Cheongjae Lee, Kyungduk Kim, Minwoo Jeong, and Gary Geunbae Lee. 2009. Datadriven user simulation for automated evaluation of spoken dialog systems. Comput. Speech Lang., 23:479–509, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Two-level, many-paths generation.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd annual meeting on Association for Computational Linguistics, ACL ’95.</booktitle>
<contexts>
<context position="7459" citStr="Knight and Hatzivassiloglou, 1995" startWordPosition="1147" endWordPosition="1150">ponses or templates for generation, which can result in utterances which sound very unnatural in context (Chambers and Allen, 2004). Recent work has investigated the use of SMT in translating internal dialogue state into natural language (Langner et al., 2010). In addition to dialogue state, we believe it may be beneficial to consider the user’s utterance when generating responses in order to generate locally coherent discourse (Barzilay and Lapata, 2005). Data-driven generation based on users’ utterances might also be a useful way to fill in knowledge gaps in the system (Galley et al., 2001; Knight and Hatzivassiloglou, 1995). Statistical machine translation has been applied to a sm¨org˚asbord of NLP problems, including question answering (Echihabi and Marcu, 2003), semantic parsing and generation (Wong and Mooney, 2006; Wong and Mooney, 2007), summarization (Daum´e III and Marcu, 2009), generating bid-phrases in online advertising (Ravi et al., 2010), spelling correction (Sun et al., 2010), paraphrase (Dolan et al., 2004; Quirk et al., 2004) and query expansion (Riezler et al., 2007). Most relevant to our efforts is the work by Soricut and Marcu (2006), who applied the IBM word alignment models to a discourse ord</context>
</contexts>
<marker>Knight, Hatzivassiloglou, 1995</marker>
<rawString>Kevin Knight and Vasileios Hatzivassiloglou. 1995. Two-level, many-paths generation. In Proceedings of the 33rd annual meeting on Association for Computational Linguistics, ACL ’95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation. In ACL. The Association for Computer Linguistics.</title>
<date>2007</date>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="10240" citStr="Koehn et al., 2007" startWordPosition="1592" endWordPosition="1595">og-linear combination of feature functions. Most prominent among these features are the conditional phrase-translation probabilities in both directions, P(sIr) and P(rIs), which ensure r is an appropriate response to s, and the language model P(r), which ensures r is a well-formed response. As in translation, the response models are estimated from counts of phrase pairs observed in the training bitext, and the language model is built using n-gram statistics from a large set of observed responses. To find the best response to a given input status-post, we employ the Moses phrase-based decoder (Koehn et al., 2007), which conducts a beam search for the best response given the input, according to the log-linear model. what . . . ■ ■ time . . . ■ ■ u ■ . . . . get . ■ . . . out . . ■ . . ? . . . . . Figure 1: Example from the data where word alignment is easy. There is a clear correspondence between words in the status and the response. 4.1 Challenge: Lexical Repetition When applied directly to conversation data, off-theshelf MT systems simply learn to parrot back the input, sometimes with slight modification. For example, directly applying Moses with default settings to the conversation data produces a s</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Landis</author>
<author>G G Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data.</title>
<date>1977</date>
<journal>Biometrics.</journal>
<contexts>
<context position="22626" citStr="Landis and Koch, 1977" startWordPosition="3704" endWordPosition="3707"> et al., 1954) as a measure of agreement between annotators for each experiment. 5 is equivalent the commonly used Kappa, except it assumes the uniform distribution when computing expected or chance agreement (Artstein and Poesio, 2008). The uniform distribution is appropriate in our setup, since annotators are not told which system generated each output, and the order of choices is randomized. If annotators were indeed choosing randomly, they would be expected to agree in 50% of cases. In most comparisons the value of 5 is in the range of 0.2 to 0.4, which can be considered “Fair agreement” (Landis and Koch, 1977). Note that agreement between annotators is lower than typically reported in corpus annotation tasks. When annotating which of two automatically generated outputs is better, there is not always a clear answer; both responses might be good or bad. We can expect strong agreement only in cases where one response is clearly better. Strong agreement is not required, however, as we are using many annotations to compare each pair of systems, and the human judgments are not intended to be used as training data. Similar agreement was reported in an evaluation of automatically generated MT output as par</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J R Landis and G G Koch. 1977. The measurement of observer agreement for categorical data. Biometrics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Langner</author>
<author>Stephan Vogel</author>
<author>Alan W Black</author>
</authors>
<title>Evaluating a dialog language generation system: comparing the mountain system to other nlg approaches.</title>
<date>2010</date>
<booktitle>In INTERSPEECH.</booktitle>
<contexts>
<context position="2892" citStr="Langner et al., 2010" startWordPosition="435" endWordPosition="438">ist of suggested responses to a target status, or providing conversation-aware autocomplete for responses in progress. These features are especially important on hand-held devices (Hasselgren et al., 2003). Response generation should also be beneficial in building “chatterbots” (Weizenbaum, 1966) for entertainment purposes or companionship (Wilks, 2006). However, we are most excited by the future potential of data-driven response generation when used inside larger dialogue systems, where direct consideration of the user’s utterance could be combined with dialogue state (Wong and Mooney, 2007; Langner et al., 2010) to generate locally coherent, purposeful dialogue. In this work, we investigate statistical machine translation as an approach for response generation. We are motivated by the following observation: In naturally occurring discourse, there is often a strong structural relationship between adjacent utterances (Hobbs, 1985). For example, consider the stimulusresponse pair from the data: Stimulus: I’m slowly making this soup ...... and it smells gorgeous! 583 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 583–593, Edinburgh, Scotland, UK, July 27–31,</context>
<context position="7085" citStr="Langner et al., 2010" startWordPosition="1088" endWordPosition="1091">erance. We leverage large amounts of conversational training data to scale to our Social Media domain, where conversations can be on just about any topic. Additionally, there has been work on generating more natural utterances in goal-directed dialogue systems (Ratnaparkhi, 2000; Rambow et al., 2001). Currently, most dialogue systems rely on either canned responses or templates for generation, which can result in utterances which sound very unnatural in context (Chambers and Allen, 2004). Recent work has investigated the use of SMT in translating internal dialogue state into natural language (Langner et al., 2010). In addition to dialogue state, we believe it may be beneficial to consider the user’s utterance when generating responses in order to generate locally coherent discourse (Barzilay and Lapata, 2005). Data-driven generation based on users’ utterances might also be a useful way to fill in knowledge gaps in the system (Galley et al., 2001; Knight and Hatzivassiloglou, 1995). Statistical machine translation has been applied to a sm¨org˚asbord of NLP problems, including question answering (Echihabi and Marcu, 2003), semantic parsing and generation (Wong and Mooney, 2006; Wong and Mooney, 2007), su</context>
</contexts>
<marker>Langner, Vogel, Black, 2010</marker>
<rawString>Brian Langner, Stephan Vogel, and Alan W. Black. 2010. Evaluating a dialog language generation system: comparing the mountain system to other nlg approaches. In INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anton Leuski</author>
<author>David R Traum</author>
</authors>
<title>Practical language processing for virtual humans.</title>
<date>2010</date>
<booktitle>In TwentySecond Annual Conference on Innovative Applications of Artificial Intelligence (IAAI-10).</booktitle>
<contexts>
<context position="8478" citStr="Leuski and Traum, 2010" startWordPosition="1311" endWordPosition="1314">l., 2004; Quirk et al., 2004) and query expansion (Riezler et al., 2007). Most relevant to our efforts is the work by Soricut and Marcu (2006), who applied the IBM word alignment models to a discourse ordering task, exploiting the same intuition investigated 584 in this paper: certain words (or phrases) tend to trigger the usage of other words in subsequent discourse units. As far as we are aware, ours is the first work to explore the use of phrase-based translation in generating responses to open-domain linguistic stimuli, although the analogy between translation and dialogue has been drawn (Leuski and Traum, 2010). 3 Data For learning response-generation models, we use a corpus of roughly 1.3 million conversations scraped from Twitter (Ritter et al., 2010; DanescuNiculescu-Mizil et al., 2011). Twitter conversations don’t occur in real-time as in IRC; rather as in email, users typically take turns responding to each other. Twitter’s 140 character limit, however, keeps conversations chat-like. In addition, the Twitter API maintains a reference from each reply to the post it responds to, so unlike IRC, there is no need for conversation disentanglement (Elsner and Charniak, 2008; Wang and Oard, 2009). The </context>
</contexts>
<marker>Leuski, Traum, 2010</marker>
<rawString>Anton Leuski and David R. Traum. 2010. Practical language processing for virtual humans. In TwentySecond Annual Conference on Innovative Applications of Artificial Intelligence (IAAI-10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A phrase-based alignment model for natural language inference.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08,</booktitle>
<pages>802--811</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="11832" citStr="MacCartney et al., 2008" startWordPosition="1867" endWordPosition="1870">eptable responses to any status, these identical pairs have the strongest associations in the data, and therefore dominate the phrase table. In order to discourage lexically similar translations, we filter out all phrase-pairs where one phrase is a substring of the other, and introduce a novel feature to penalize lexical similarity: Olex(s, t) = J(s, t) Where J(s, t) is the Jaccard similarity between the set of words in s and t. 4.2 Challenge: Word Alignment Alignment is more difficult in conversational data than bilingual data (Brown et al., 1990), or textual entailment data (Brockett, 2006; MacCartney et al., 2008). In conversational data, there are some cases in which there is a decomposable alignment between at i get off 5 585 if . . . . anyones . . . . still . . . . awake . . . . lets . . . . play . . . . a . . . . game. . . . . name ■ ■ ■ . 3 ■ ■ ■ . kevin ■ ■ ■ . costner ■ ■ ■ . movies ■ ■ ■ . that ■ ■ ■ . dont ■ ■ ■ . suck ■ ■ ■ . . . . .■ Figure 2: Example from the data where word alignment is difficult (requires alignment between large phrases in the status and response). words, as seen in figure 1, and some difficult cases where alignment between large phrases is required, for example figure 2.</context>
</contexts>
<marker>MacCartney, Galley, Manning, 2008</marker>
<rawString>Bill MacCartney, Michel Galley, and Christopher D. Manning. 2008. A phrase-based alignment model for natural language inference. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, pages 802–811, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>On log-likelihood-ratios and the significance of rare events.</title>
<date>2004</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="14964" citStr="Moore (2004)" startWordPosition="2438" endWordPosition="2439">statistical tests such as χ2, or the G2 Log Likelihood Ratio, Fisher’s Exact Test produces accurate p-values even when the expected counts are small (as is extremely common in our case). In Fisher’s Exact Test, the hypergeometric probability distribution is used to compute the exact probability of a particular joint frequency assuming a model of independence: C(s)!C(-,s)!C(t)!C(-,t)! N!C(s, t)!C(-,s, t)!C(s, -,t)!C(-,s, -,t)! The statistic is computed by summing the probability for the joint frequency in Table 3, and every more extreme joint frequency consistent with the marginal frequencies. Moore (2004) illustrates several tricks which make this computation feasible in practice. We found that this approach generates phrasetable entries which appear quite reasonable upon manual inspection. The top 20 phrase-pairs (after filtering out identical source/target phrases, substrings, 1We define a possible phrase-pair as any pair of phrases found in a sentence-pair from our training corpus, where both phrases consist of 4 tokens or fewer. The total number of phrase pairs in a sentence pair (s, r) is O(|s |x |r|). easier question please . 586 Source Target rt [retweet] thanks for the potter harry ice</context>
</contexts>
<marker>Moore, 2004</marker>
<rawString>Robert C. Moore. 2004. On log-likelihood-ratios and the significance of rare events. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="12659" citStr="Och and Ney, 2003" startWordPosition="2053" endWordPosition="2056">ame. . . . . name ■ ■ ■ . 3 ■ ■ ■ . kevin ■ ■ ■ . costner ■ ■ ■ . movies ■ ■ ■ . that ■ ■ ■ . dont ■ ■ ■ . suck ■ ■ ■ . . . . .■ Figure 2: Example from the data where word alignment is difficult (requires alignment between large phrases in the status and response). words, as seen in figure 1, and some difficult cases where alignment between large phrases is required, for example figure 2. These difficult sentence pairs confuse the IBM word alignment models which have no way to distinguish between the easy and hard cases. We aligned words in our parallel data using the widely used tool GIZA++ (Och and Ney, 2003); however, the standard growing heuristic resulted in very noisy alignments. Precision could be improved considerably by using the intersection of GIZA++ trained in two directions (s -+ r, and r -+ s), but the alignment also became extremely sparse. The average number of alignments-per status/response pair in our data was only 1.7, as compared to a dataset of aligned French-English sentence pairs (the WMT 08 news commentary data) where the average number of intersection alignments is 14. Direct Phrase Pair Extraction Because word alignment in status/response pairs is a difficult problem, inste</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="17509" citStr="Och, 2003" startWordPosition="2875" endWordPosition="2876">pairs after filtering, while including all of them led to a table which was too large for the memory of the machine used to conduct the experiments. We do not use any form of SMT reordering model, as the position of the phrase in the response does not seem to be very correlated with the corresponding position in the status. Instead we let the language model drive reordering. We used the default feature weights provided by Moses.4 Because automatic evaluation of response generation is an open problem, we avoided the use of discriminative training algorithms such as Minimum Error-Rate Training (Och, 2003). 5 Information Retrieval One straightforward data-driven approach to response generation is nearest neighbour, or information retrieval. This general approach has been applied previously by several authors (Isbell et al., 2000; Swanson and Gordon, 2008; Jafarpour and Burges, 2010), and is used as a point of comparison in our experiments. Given a novel status s and a training corpus of status/response pairs, two retrieval strategies can be used to return a best response r&apos;: IR-STATUS [rargmaxi sim(s,si)] Retrieve the response ri whose associated status message si is most similar to the user’s </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training for statistical machine translation. In ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="28735" citStr="Papineni et al., 2002" startWordPosition="4809" endWordPosition="4812">ilor its response to the status on a fine-grained status. However, the human evaluation shows a clear scale overcame the disadvantage of occasionally in- preference for MT-CHAT’s output: raters favour retroducing fluency errors.7 sponses that are tailored to the stimulus. Given MT-CHAT’s success over the IR systems, 6.3 Automatic Evaluation we conducted further experiments to validate its out- The field of SMT has benefited greatly from put. In order to test how close MT-CHAT’s responses the existence of an automatic evaluation metric, come to human-level abilities, we compared its out- BLEU (Papineni et al., 2002), which grades an output to actual human responses from our dataset. In put candidate according to n-gram matches to one or some cases the human responses change the topic of more reference outputs. To evaluate whether BLEU conversation, and completely ignore the initial sta- is an appropriate automatic evaluation measure for tus. For instance, one frequent type of response we response generation, we attempted to measure its noticed in the data was a greeting: “How have you agreement with the human judgments. We calcubeen? I haven’t talked to you in a while.” For the late BLEU using a single r</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
<author>William Dolan</author>
</authors>
<title>Monolingual machine translation for paraphrase generation. In</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>142--149</pages>
<contexts>
<context position="7884" citStr="Quirk et al., 2004" startWordPosition="1210" endWordPosition="1213">y and Lapata, 2005). Data-driven generation based on users’ utterances might also be a useful way to fill in knowledge gaps in the system (Galley et al., 2001; Knight and Hatzivassiloglou, 1995). Statistical machine translation has been applied to a sm¨org˚asbord of NLP problems, including question answering (Echihabi and Marcu, 2003), semantic parsing and generation (Wong and Mooney, 2006; Wong and Mooney, 2007), summarization (Daum´e III and Marcu, 2009), generating bid-phrases in online advertising (Ravi et al., 2010), spelling correction (Sun et al., 2010), paraphrase (Dolan et al., 2004; Quirk et al., 2004) and query expansion (Riezler et al., 2007). Most relevant to our efforts is the work by Soricut and Marcu (2006), who applied the IBM word alignment models to a discourse ordering task, exploiting the same intuition investigated 584 in this paper: certain words (or phrases) tend to trigger the usage of other words in subsequent discourse units. As far as we are aware, ours is the first work to explore the use of phrase-based translation in generating responses to open-domain linguistic stimuli, although the analogy between translation and dialogue has been drawn (Leuski and Traum, 2010). 3 Da</context>
</contexts>
<marker>Quirk, Brockett, Dolan, 2004</marker>
<rawString>Chris Quirk, Chris Brockett, and William Dolan. 2004. Monolingual machine translation for paraphrase generation. In In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 142–149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Owen Rambow</author>
<author>Srinivas Bangalore</author>
<author>Marilyn Walker</author>
</authors>
<title>Natural language generation in dialog systems.</title>
<date>2001</date>
<booktitle>In Proceedings of the first international conference on Human language technology research, HLT ’01,</booktitle>
<pages>1--4</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6765" citStr="Rambow et al., 2001" startWordPosition="1038" endWordPosition="1041">which attempt to engage users, typically leading the topic of conversation. They usually limit interactions to a specific scenario (e.g. a Rogerian psychotherapist), and use a set of template rules for generating responses. In contrast, we focus on the simpler task of generating an appropriate response to a single utterance. We leverage large amounts of conversational training data to scale to our Social Media domain, where conversations can be on just about any topic. Additionally, there has been work on generating more natural utterances in goal-directed dialogue systems (Ratnaparkhi, 2000; Rambow et al., 2001). Currently, most dialogue systems rely on either canned responses or templates for generation, which can result in utterances which sound very unnatural in context (Chambers and Allen, 2004). Recent work has investigated the use of SMT in translating internal dialogue state into natural language (Langner et al., 2010). In addition to dialogue state, we believe it may be beneficial to consider the user’s utterance when generating responses in order to generate locally coherent discourse (Barzilay and Lapata, 2005). Data-driven generation based on users’ utterances might also be a useful way to</context>
</contexts>
<marker>Rambow, Bangalore, Walker, 2001</marker>
<rawString>Owen Rambow, Srinivas Bangalore, and Marilyn Walker. 2001. Natural language generation in dialog systems. In Proceedings of the first international conference on Human language technology research, HLT ’01, pages 1–4, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Trainable methods for surface natural language generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference.</booktitle>
<contexts>
<context position="6743" citStr="Ratnaparkhi, 2000" startWordPosition="1036" endWordPosition="1037">ikh et al., 2010), which attempt to engage users, typically leading the topic of conversation. They usually limit interactions to a specific scenario (e.g. a Rogerian psychotherapist), and use a set of template rules for generating responses. In contrast, we focus on the simpler task of generating an appropriate response to a single utterance. We leverage large amounts of conversational training data to scale to our Social Media domain, where conversations can be on just about any topic. Additionally, there has been work on generating more natural utterances in goal-directed dialogue systems (Ratnaparkhi, 2000; Rambow et al., 2001). Currently, most dialogue systems rely on either canned responses or templates for generation, which can result in utterances which sound very unnatural in context (Chambers and Allen, 2004). Recent work has investigated the use of SMT in translating internal dialogue state into natural language (Langner et al., 2010). In addition to dialogue state, we believe it may be beneficial to consider the user’s utterance when generating responses in order to generate locally coherent discourse (Barzilay and Lapata, 2005). Data-driven generation based on users’ utterances might a</context>
</contexts>
<marker>Ratnaparkhi, 2000</marker>
<rawString>Adwait Ratnaparkhi. 2000. Trainable methods for surface natural language generation. In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Andrei Broder</author>
<author>Evgeniy Gabrilovich</author>
<author>Vanja Josifovski</author>
<author>Sandeep Pandey</author>
<author>Bo Pang</author>
</authors>
<title>Automatic generation of bid phrases for online advertising.</title>
<date>2010</date>
<booktitle>In Proceedings of the third ACM international conference on Web search and data mining, WSDM ’10.</booktitle>
<contexts>
<context position="7791" citStr="Ravi et al., 2010" startWordPosition="1194" endWordPosition="1197">utterance when generating responses in order to generate locally coherent discourse (Barzilay and Lapata, 2005). Data-driven generation based on users’ utterances might also be a useful way to fill in knowledge gaps in the system (Galley et al., 2001; Knight and Hatzivassiloglou, 1995). Statistical machine translation has been applied to a sm¨org˚asbord of NLP problems, including question answering (Echihabi and Marcu, 2003), semantic parsing and generation (Wong and Mooney, 2006; Wong and Mooney, 2007), summarization (Daum´e III and Marcu, 2009), generating bid-phrases in online advertising (Ravi et al., 2010), spelling correction (Sun et al., 2010), paraphrase (Dolan et al., 2004; Quirk et al., 2004) and query expansion (Riezler et al., 2007). Most relevant to our efforts is the work by Soricut and Marcu (2006), who applied the IBM word alignment models to a discourse ordering task, exploiting the same intuition investigated 584 in this paper: certain words (or phrases) tend to trigger the usage of other words in subsequent discourse units. As far as we are aware, ours is the first work to explore the use of phrase-based translation in generating responses to open-domain linguistic stimuli, althou</context>
</contexts>
<marker>Ravi, Broder, Gabrilovich, Josifovski, Pandey, Pang, 2010</marker>
<rawString>Sujith Ravi, Andrei Broder, Evgeniy Gabrilovich, Vanja Josifovski, Sandeep Pandey, and Bo Pang. 2010. Automatic generation of bid phrases for online advertising. In Proceedings of the third ACM international conference on Web search and data mining, WSDM ’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Alexander Vasserman</author>
<author>Ioannis Tsochantaridis</author>
<author>Vibhu Mittal</author>
<author>Yi Liu</author>
</authors>
<title>Statistical machine translation for query expansion in answer retrieval.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>464--471</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="7927" citStr="Riezler et al., 2007" startWordPosition="1217" endWordPosition="1221">on based on users’ utterances might also be a useful way to fill in knowledge gaps in the system (Galley et al., 2001; Knight and Hatzivassiloglou, 1995). Statistical machine translation has been applied to a sm¨org˚asbord of NLP problems, including question answering (Echihabi and Marcu, 2003), semantic parsing and generation (Wong and Mooney, 2006; Wong and Mooney, 2007), summarization (Daum´e III and Marcu, 2009), generating bid-phrases in online advertising (Ravi et al., 2010), spelling correction (Sun et al., 2010), paraphrase (Dolan et al., 2004; Quirk et al., 2004) and query expansion (Riezler et al., 2007). Most relevant to our efforts is the work by Soricut and Marcu (2006), who applied the IBM word alignment models to a discourse ordering task, exploiting the same intuition investigated 584 in this paper: certain words (or phrases) tend to trigger the usage of other words in subsequent discourse units. As far as we are aware, ours is the first work to explore the use of phrase-based translation in generating responses to open-domain linguistic stimuli, although the analogy between translation and dialogue has been drawn (Leuski and Traum, 2010). 3 Data For learning response-generation models,</context>
</contexts>
<marker>Riezler, Vasserman, Tsochantaridis, Mittal, Liu, 2007</marker>
<rawString>Stefan Riezler, Alexander Vasserman, Ioannis Tsochantaridis, Vibhu Mittal, and Yi Liu. 2007. Statistical machine translation for query expansion in answer retrieval. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 464–471, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Colin Cherry</author>
<author>Bill Dolan</author>
</authors>
<title>Unsupervised modeling of twitter conversations.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>172--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="8622" citStr="Ritter et al., 2010" startWordPosition="1333" endWordPosition="1336">applied the IBM word alignment models to a discourse ordering task, exploiting the same intuition investigated 584 in this paper: certain words (or phrases) tend to trigger the usage of other words in subsequent discourse units. As far as we are aware, ours is the first work to explore the use of phrase-based translation in generating responses to open-domain linguistic stimuli, although the analogy between translation and dialogue has been drawn (Leuski and Traum, 2010). 3 Data For learning response-generation models, we use a corpus of roughly 1.3 million conversations scraped from Twitter (Ritter et al., 2010; DanescuNiculescu-Mizil et al., 2011). Twitter conversations don’t occur in real-time as in IRC; rather as in email, users typically take turns responding to each other. Twitter’s 140 character limit, however, keeps conversations chat-like. In addition, the Twitter API maintains a reference from each reply to the post it responds to, so unlike IRC, there is no need for conversation disentanglement (Elsner and Charniak, 2008; Wang and Oard, 2009). The first message of a conversation is typically unique, not directed at any particular user but instead broadcast to the author’s followers (a stat</context>
</contexts>
<marker>Ritter, Cherry, Dolan, 2010</marker>
<rawString>Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsupervised modeling of twitter conversations. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 172–180, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samira Shaikh</author>
<author>Tomek Strzalkowski</author>
<author>Sarah Taylor</author>
<author>Nick Webb</author>
</authors>
<title>Vca: an experiment with a multiparty virtual chat agent.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Workshop on Companionable Dialogue Systems.</booktitle>
<contexts>
<context position="6143" citStr="Shaikh et al., 2010" startWordPosition="942" endWordPosition="945">es of each approach, and perform an evaluation using human annotators from Amazon’s Mechanical Turk. Along the way, we investigate the utility of SMT’s BLEU evaluation metric when applied to this domain. We show that SMT-based solutions outperform IR-based solutions, and are chosen over actual human responses in our data in 15% of cases. As far as we are aware, this is the first work to investigate the feasibility of SMT’s application to generating responses to open-domain linguistic stimuli. 2 Related Work There has been a long history of “chatterbots” (Weizenbaum, 1966; Isbell et al., 2000; Shaikh et al., 2010), which attempt to engage users, typically leading the topic of conversation. They usually limit interactions to a specific scenario (e.g. a Rogerian psychotherapist), and use a set of template rules for generating responses. In contrast, we focus on the simpler task of generating an appropriate response to a single utterance. We leverage large amounts of conversational training data to scale to our Social Media domain, where conversations can be on just about any topic. Additionally, there has been work on generating more natural utterances in goal-directed dialogue systems (Ratnaparkhi, 2000</context>
</contexts>
<marker>Shaikh, Strzalkowski, Taylor, Webb, 2010</marker>
<rawString>Samira Shaikh, Tomek Strzalkowski, Sarah Taylor, and Nick Webb. 2010. Vca: an experiment with a multiparty virtual chat agent. In Proceedings of the 2010 Workshop on Companionable Dialogue Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Daniel Marcu</author>
</authors>
<title>Discourse generation using utility-trained coherence models.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Main conference poster sessions, COLING-ACL ’06.</booktitle>
<contexts>
<context position="7997" citStr="Soricut and Marcu (2006)" startWordPosition="1231" endWordPosition="1234"> knowledge gaps in the system (Galley et al., 2001; Knight and Hatzivassiloglou, 1995). Statistical machine translation has been applied to a sm¨org˚asbord of NLP problems, including question answering (Echihabi and Marcu, 2003), semantic parsing and generation (Wong and Mooney, 2006; Wong and Mooney, 2007), summarization (Daum´e III and Marcu, 2009), generating bid-phrases in online advertising (Ravi et al., 2010), spelling correction (Sun et al., 2010), paraphrase (Dolan et al., 2004; Quirk et al., 2004) and query expansion (Riezler et al., 2007). Most relevant to our efforts is the work by Soricut and Marcu (2006), who applied the IBM word alignment models to a discourse ordering task, exploiting the same intuition investigated 584 in this paper: certain words (or phrases) tend to trigger the usage of other words in subsequent discourse units. As far as we are aware, ours is the first work to explore the use of phrase-based translation in generating responses to open-domain linguistic stimuli, although the analogy between translation and dialogue has been drawn (Leuski and Traum, 2010). 3 Data For learning response-generation models, we use a corpus of roughly 1.3 million conversations scraped from Twi</context>
</contexts>
<marker>Soricut, Marcu, 2006</marker>
<rawString>Radu Soricut and Daniel Marcu. 2006. Discourse generation using utility-trained coherence models. In Proceedings of the COLING/ACL on Main conference poster sessions, COLING-ACL ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xu Sun</author>
<author>Jianfeng Gao</author>
<author>Daniel Micol</author>
<author>Chris Quirk</author>
</authors>
<title>Learning phrase-based spelling error models from clickthrough data.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>266--274</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="7831" citStr="Sun et al., 2010" startWordPosition="1201" endWordPosition="1204">der to generate locally coherent discourse (Barzilay and Lapata, 2005). Data-driven generation based on users’ utterances might also be a useful way to fill in knowledge gaps in the system (Galley et al., 2001; Knight and Hatzivassiloglou, 1995). Statistical machine translation has been applied to a sm¨org˚asbord of NLP problems, including question answering (Echihabi and Marcu, 2003), semantic parsing and generation (Wong and Mooney, 2006; Wong and Mooney, 2007), summarization (Daum´e III and Marcu, 2009), generating bid-phrases in online advertising (Ravi et al., 2010), spelling correction (Sun et al., 2010), paraphrase (Dolan et al., 2004; Quirk et al., 2004) and query expansion (Riezler et al., 2007). Most relevant to our efforts is the work by Soricut and Marcu (2006), who applied the IBM word alignment models to a discourse ordering task, exploiting the same intuition investigated 584 in this paper: certain words (or phrases) tend to trigger the usage of other words in subsequent discourse units. As far as we are aware, ours is the first work to explore the use of phrase-based translation in generating responses to open-domain linguistic stimuli, although the analogy between translation and d</context>
</contexts>
<marker>Sun, Gao, Micol, Quirk, 2010</marker>
<rawString>Xu Sun, Jianfeng Gao, Daniel Micol, and Chris Quirk. 2010. Learning phrase-based spelling error models from clickthrough data. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 266–274, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reid Swanson</author>
<author>Andrew S Gordon</author>
</authors>
<title>Say anything: A massively collaborative open domain story writing companion.</title>
<date>2008</date>
<booktitle>In Proceedings of the 1st Joint International Conference on Interactive Digital Storytelling: Interactive Storytelling, ICIDS ’08,</booktitle>
<pages>32--40</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="17762" citStr="Swanson and Gordon, 2008" startWordPosition="2910" endWordPosition="2913">nse does not seem to be very correlated with the corresponding position in the status. Instead we let the language model drive reordering. We used the default feature weights provided by Moses.4 Because automatic evaluation of response generation is an open problem, we avoided the use of discriminative training algorithms such as Minimum Error-Rate Training (Och, 2003). 5 Information Retrieval One straightforward data-driven approach to response generation is nearest neighbour, or information retrieval. This general approach has been applied previously by several authors (Isbell et al., 2000; Swanson and Gordon, 2008; Jafarpour and Burges, 2010), and is used as a point of comparison in our experiments. Given a novel status s and a training corpus of status/response pairs, two retrieval strategies can be used to return a best response r&apos;: IR-STATUS [rargmaxi sim(s,si)] Retrieve the response ri whose associated status message si is most similar to the user’s input s. IR-RESPONSE [rargmaxi sim(s,ri)] Retrieve the response ri which has highest similarity when directly compared to s. At first glance, IR-STATUS may appear to be the most promising option; intuitively, if an input status is very similar to a trai</context>
</contexts>
<marker>Swanson, Gordon, 2008</marker>
<rawString>Reid Swanson and Andrew S. Gordon. 2008. Say anything: A massively collaborative open domain story writing companion. In Proceedings of the 1st Joint International Conference on Interactive Digital Storytelling: Interactive Storytelling, ICIDS ’08, pages 32– 40, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lidan Wang</author>
<author>Douglas W Oard</author>
</authors>
<title>Context-based message expansion for disentanglement of interleaved text conversations.</title>
<date>2009</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="9072" citStr="Wang and Oard, 2009" startWordPosition="1403" endWordPosition="1406">n (Leuski and Traum, 2010). 3 Data For learning response-generation models, we use a corpus of roughly 1.3 million conversations scraped from Twitter (Ritter et al., 2010; DanescuNiculescu-Mizil et al., 2011). Twitter conversations don’t occur in real-time as in IRC; rather as in email, users typically take turns responding to each other. Twitter’s 140 character limit, however, keeps conversations chat-like. In addition, the Twitter API maintains a reference from each reply to the post it responds to, so unlike IRC, there is no need for conversation disentanglement (Elsner and Charniak, 2008; Wang and Oard, 2009). The first message of a conversation is typically unique, not directed at any particular user but instead broadcast to the author’s followers (a status message). For the purposes of this paper, we limit the data set to only the first two utterances from each conversation. As a result of this constraint, any system trained with this data will be specialized for responding to Twitter status posts. 4 Response Generation as Translation When applied to conversations, SMT models the probability of a response r given the input statuspost s using a log-linear combination of feature functions. Most pr</context>
</contexts>
<marker>Wang, Oard, 2009</marker>
<rawString>Lidan Wang and Douglas W. Oard. 2009. Context-based message expansion for disentanglement of interleaved text conversations. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Weizenbaum</author>
</authors>
<title>Eliza: a computer program for the study of natural language communication between man and machine.</title>
<date>1966</date>
<journal>Commun. ACM,</journal>
<pages>9--36</pages>
<contexts>
<context position="2568" citStr="Weizenbaum, 1966" startWordPosition="385" endWordPosition="387"> any response that fits the provided stimulus; however, we do so without employing rules or templates, with the hope of creating a system that is both flexible and extensible when operating in an open domain. Success in open domain response generation could be immediately useful to social media platforms, providing a list of suggested responses to a target status, or providing conversation-aware autocomplete for responses in progress. These features are especially important on hand-held devices (Hasselgren et al., 2003). Response generation should also be beneficial in building “chatterbots” (Weizenbaum, 1966) for entertainment purposes or companionship (Wilks, 2006). However, we are most excited by the future potential of data-driven response generation when used inside larger dialogue systems, where direct consideration of the user’s utterance could be combined with dialogue state (Wong and Mooney, 2007; Langner et al., 2010) to generate locally coherent, purposeful dialogue. In this work, we investigate statistical machine translation as an approach for response generation. We are motivated by the following observation: In naturally occurring discourse, there is often a strong structural relatio</context>
<context position="6100" citStr="Weizenbaum, 1966" startWordPosition="936" endWordPosition="937"> analyze the advantages and disadvantages of each approach, and perform an evaluation using human annotators from Amazon’s Mechanical Turk. Along the way, we investigate the utility of SMT’s BLEU evaluation metric when applied to this domain. We show that SMT-based solutions outperform IR-based solutions, and are chosen over actual human responses in our data in 15% of cases. As far as we are aware, this is the first work to investigate the feasibility of SMT’s application to generating responses to open-domain linguistic stimuli. 2 Related Work There has been a long history of “chatterbots” (Weizenbaum, 1966; Isbell et al., 2000; Shaikh et al., 2010), which attempt to engage users, typically leading the topic of conversation. They usually limit interactions to a specific scenario (e.g. a Rogerian psychotherapist), and use a set of template rules for generating responses. In contrast, we focus on the simpler task of generating an appropriate response to a single utterance. We leverage large amounts of conversational training data to scale to our Social Media domain, where conversations can be on just about any topic. Additionally, there has been work on generating more natural utterances in goal-d</context>
</contexts>
<marker>Weizenbaum, 1966</marker>
<rawString>Joseph Weizenbaum. 1966. Eliza: a computer program for the study of natural language communication between man and machine. Commun. ACM, 9:36–45, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yorick Wilks</author>
</authors>
<title>Artificial companions as a new kind of interface to the future internet.</title>
<date>2006</date>
<journal>In OII Research Report</journal>
<volume>13</volume>
<contexts>
<context position="2626" citStr="Wilks, 2006" startWordPosition="394" endWordPosition="395">o without employing rules or templates, with the hope of creating a system that is both flexible and extensible when operating in an open domain. Success in open domain response generation could be immediately useful to social media platforms, providing a list of suggested responses to a target status, or providing conversation-aware autocomplete for responses in progress. These features are especially important on hand-held devices (Hasselgren et al., 2003). Response generation should also be beneficial in building “chatterbots” (Weizenbaum, 1966) for entertainment purposes or companionship (Wilks, 2006). However, we are most excited by the future potential of data-driven response generation when used inside larger dialogue systems, where direct consideration of the user’s utterance could be combined with dialogue state (Wong and Mooney, 2007; Langner et al., 2010) to generate locally coherent, purposeful dialogue. In this work, we investigate statistical machine translation as an approach for response generation. We are motivated by the following observation: In naturally occurring discourse, there is often a strong structural relationship between adjacent utterances (Hobbs, 1985). For examp</context>
</contexts>
<marker>Wilks, 2006</marker>
<rawString>Yorick Wilks. 2006. Artificial companions as a new kind of interface to the future internet. In OII Research Report No. 13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond Mooney</author>
</authors>
<title>Learning for semantic parsing with statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference.</booktitle>
<contexts>
<context position="7657" citStr="Wong and Mooney, 2006" startWordPosition="1174" endWordPosition="1177">tate into natural language (Langner et al., 2010). In addition to dialogue state, we believe it may be beneficial to consider the user’s utterance when generating responses in order to generate locally coherent discourse (Barzilay and Lapata, 2005). Data-driven generation based on users’ utterances might also be a useful way to fill in knowledge gaps in the system (Galley et al., 2001; Knight and Hatzivassiloglou, 1995). Statistical machine translation has been applied to a sm¨org˚asbord of NLP problems, including question answering (Echihabi and Marcu, 2003), semantic parsing and generation (Wong and Mooney, 2006; Wong and Mooney, 2007), summarization (Daum´e III and Marcu, 2009), generating bid-phrases in online advertising (Ravi et al., 2010), spelling correction (Sun et al., 2010), paraphrase (Dolan et al., 2004; Quirk et al., 2004) and query expansion (Riezler et al., 2007). Most relevant to our efforts is the work by Soricut and Marcu (2006), who applied the IBM word alignment models to a discourse ordering task, exploiting the same intuition investigated 584 in this paper: certain words (or phrases) tend to trigger the usage of other words in subsequent discourse units. As far as we are aware, o</context>
</contexts>
<marker>Wong, Mooney, 2006</marker>
<rawString>Yuk Wah Wong and Raymond Mooney. 2006. Learning for semantic parsing with statistical machine translation. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond Mooney</author>
</authors>
<title>Generation by inverting a semantic parser that uses statistical machine translation.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference.</booktitle>
<contexts>
<context position="2869" citStr="Wong and Mooney, 2007" startWordPosition="431" endWordPosition="434">latforms, providing a list of suggested responses to a target status, or providing conversation-aware autocomplete for responses in progress. These features are especially important on hand-held devices (Hasselgren et al., 2003). Response generation should also be beneficial in building “chatterbots” (Weizenbaum, 1966) for entertainment purposes or companionship (Wilks, 2006). However, we are most excited by the future potential of data-driven response generation when used inside larger dialogue systems, where direct consideration of the user’s utterance could be combined with dialogue state (Wong and Mooney, 2007; Langner et al., 2010) to generate locally coherent, purposeful dialogue. In this work, we investigate statistical machine translation as an approach for response generation. We are motivated by the following observation: In naturally occurring discourse, there is often a strong structural relationship between adjacent utterances (Hobbs, 1985). For example, consider the stimulusresponse pair from the data: Stimulus: I’m slowly making this soup ...... and it smells gorgeous! 583 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 583–593, Edinburgh, Sc</context>
<context position="7681" citStr="Wong and Mooney, 2007" startWordPosition="1178" endWordPosition="1181">age (Langner et al., 2010). In addition to dialogue state, we believe it may be beneficial to consider the user’s utterance when generating responses in order to generate locally coherent discourse (Barzilay and Lapata, 2005). Data-driven generation based on users’ utterances might also be a useful way to fill in knowledge gaps in the system (Galley et al., 2001; Knight and Hatzivassiloglou, 1995). Statistical machine translation has been applied to a sm¨org˚asbord of NLP problems, including question answering (Echihabi and Marcu, 2003), semantic parsing and generation (Wong and Mooney, 2006; Wong and Mooney, 2007), summarization (Daum´e III and Marcu, 2009), generating bid-phrases in online advertising (Ravi et al., 2010), spelling correction (Sun et al., 2010), paraphrase (Dolan et al., 2004; Quirk et al., 2004) and query expansion (Riezler et al., 2007). Most relevant to our efforts is the work by Soricut and Marcu (2006), who applied the IBM word alignment models to a discourse ordering task, exploiting the same intuition investigated 584 in this paper: certain words (or phrases) tend to trigger the usage of other words in subsequent discourse units. As far as we are aware, ours is the first work to</context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Yuk Wah Wong and Raymond Mooney. 2007. Generation by inverting a semantic parser that uses statistical machine translation. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>