<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000027">
<title confidence="0.974484">
Efficient, Feature-based, Conditional Random Field Parsing
</title>
<author confidence="0.998019">
Jenny Rose Finkel, Alex Kleeman, Christopher D. Manning
</author>
<affiliation confidence="0.9848725">
Department of Computer Science
Stanford University
</affiliation>
<address confidence="0.930008">
Stanford, CA 94305
</address>
<email confidence="0.997275">
jrfinkel@cs.stanford.edu, akleeman@stanford.edu, manning@cs.stanford.edu
</email>
<sectionHeader confidence="0.995448" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99694955">
Discriminative feature-based methods are
widely used in natural language processing,
but sentence parsing is still dominated by gen-
erative methods. While prior feature-based
dynamic programming parsers have restricted
training and evaluation to artificially short sen-
tences, we present the first general, feature-
rich discriminative parser, based on a condi-
tional random field model, which has been
successfully scaled to the full WSJ parsing
data. Our efficiency is primarily due to the
use of stochastic optimization techniques, as
well as parallelization and chart prefiltering.
On WSJ15, we attain a state-of-the-artF-score
of 90.9%, a 14% relative reduction in error
over previous models, while being two orders
of magnitude faster. On sentences of length
40, our system achieves an F-score of 89.0%,
a 36% relative reduction in error over a gener-
ative baseline.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999967043478261">
Over the past decade, feature-based discriminative
models have become the tool of choice for many
natural language processing tasks. Although they
take much longer to train than generative models,
they typically produce higher performing systems,
in large part due to the ability to incorporate ar-
bitrary, potentially overlapping features. However,
constituency parsing remains an area dominated by
generative methods, due to the computational com-
plexity of the problem. Previous work on discrim-
inative parsing falls under one of three approaches.
One approach does discriminative reranking of the
n-best list of a generative parser, still usually de-
pending highly on the generative parser score as
a feature (Collins, 2000; Charniak and Johnson,
2005). A second group of papers does parsing by a
sequence of independent, discriminative decisions,
either greedily or with use of a small beam (Ratna-
parkhi, 1997; Henderson, 2004). This paper extends
the third thread of work, where joint inference via
dynamic programming algorithms is used to train
models and to attempt to find the globally best parse.
Work in this context has mainly been limited to use
of artificially short sentences due to exorbitant train-
ing and inference times. One exception is the re-
cent work of Petrov et al. (2007), who discrimina-
tively train a grammar with latent variables and do
not restrict themselves to short sentences. However
their model, like the discriminative parser of John-
son (2001), makes no use of features, and effectively
ignores the largest advantage of discriminative train-
ing. It has been shown on other NLP tasks that mod-
eling improvements, such as the switch from gen-
erative training to discriminative training, usually
provide much smaller performance gains than the
gains possible from good feature engineering. For
example, in (Lafferty et al., 2001), when switching
from a generatively trained hidden Markov model
(HMM) to a discriminatively trained, linear chain,
conditional random field (CRF) for part-of-speech
tagging, their error drops from 5.7% to 5.6%. When
they add in only a small set of orthographic fea-
tures, their CRF error rate drops considerably more
to 4.3%, and their out-of-vocabulary error rate drops
by more than half. This is further supported by John-
son (2001), who saw no parsing gains when switch-
</bodyText>
<page confidence="0.980144">
959
</page>
<note confidence="0.714269">
Proceedings of ACL-08: HLT, pages 959–967,
</note>
<page confidence="0.535531">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.999340971428571">
ing from generative to discriminative training, and
by Petrov et al. (2007) who saw only small gains of
around 0.7% for their final model when switching
training methods.
In this work, we provide just such a framework for
training a feature-rich discriminative parser. Unlike
previous work, we do not restrict ourselves to short
sentences, but we do provide results both for training
and testing on sentences of length &lt; 15 (WSJ15) and
for training and testing on sentences of length &lt; 40,
allowing previous WSJ15 results to be put in context
with respect to most modern parsing literature. Our
model is a conditional random field based model.
For a rule application, we allow arbitrary features
to be defined over the rule categories, span and split
point indices, and the words of the sentence. It is
well known that constituent length influences parse
probability, but PCFGs cannot easily take this infor-
mation into account. Another benefit of our feature
based model is that it effortlessly allows smooth-
ing over previously unseen rules. While the rule
may be novel, it will likely contain features which
are not. Practicality comes from three sources. We
made use of stochastic optimization methods which
allow us to find optimal model parameters with very
few passes through the data. We found no differ-
ence in parser performance between using stochastic
gradient descent (SGD), and the more common, but
significantly slower, L-BFGS. We also used limited
parallelization, and prefiltering of the chart to avoid
scoring rules which cannot tile into complete parses
of the sentence. This speed-up does not come with a
performance cost; we attain an F-score of 90.9%, a
14% relative reduction in errors over previous work
on WSJ15.
</bodyText>
<sectionHeader confidence="0.999044" genericHeader="method">
2 The Model
</sectionHeader>
<subsectionHeader confidence="0.987585">
2.1 A Conditional Random Field Context Free
Grammar (CRF-CFG)
</subsectionHeader>
<bodyText confidence="0.999705475">
Our parsing model is based on a conditional ran-
dom field model, however, unlike previous TreeCRF
work, e.g., (Cohn and Blunsom, 2005; Jousse et al.,
2006), we do not assume a particular tree structure,
and instead find the most likely structure and la-
beling. This is similar to conventional probabilis-
tic context-free grammar (PCFG) parsing, with two
exceptions: (a) we maximize conditional likelihood
of the parse tree, given the sentence, not joint like-
lihood of the tree and sentence; and (b) probabil-
ities are normalized globally instead of locally –
the graphical models depiction of our trees is undi-
rected.
Formally, we have a CFG G, which consists of
(Manning and Sch¨utze, 1999): (i) a set of termi-
nals {wk},k = 1,...,V; (ii) a set of nonterminals
{Nk},k = 1,...,n; (iii) a designated start symbol
ROOT; and (iv) a set of rules, {ρ = Ni —* ζ j}, where
ζ j is a sequence of terminals and nonterminals. A
PCFG additionally assigns probabilities to each rule
ρ such that Vi∑j P(Ni —* ζ j) = 1. Our conditional
random field CFG (CRF-CFG) instead defines local
clique potentials φ(r|s;θ), where s is the sentence,
and r contains a one-level subtree of a tree t, corre-
sponding to a rule ρ, along with relevant information
about the span of words which it encompasses, and,
if applicable, the split position (see Figure 1). These
potentials are relative to the sentence, unlike a PCFG
where rule scores do not have access to words at the
leaves of the tree, or even how many words they
dominate. We then define a conditional probabil-
ity distribution over entire trees, using the standard
CRF distribution, shown in (1). There is, however,
an important subtlety lurking in how we define the
partition function. The partition function Zs, which
makes the probability of all possible parses sum to
unity, is defined over all structures as well as all la-
belings of those structures. We define τ(s) to be the
set of all possible parse trees for the given sentence
licensed by the grammar G.
</bodyText>
<equation confidence="0.9982625">
1
P(t|s;θ) = Zs ∏ rEtφ(r|s;θ) (1)
</equation>
<bodyText confidence="0.568316">
where
</bodyText>
<equation confidence="0.99057">
Zs = ∑ tEτ(s)∏ rEt′ φ(r|s;θ)
</equation>
<bodyText confidence="0.999944727272727">
The above model is not well-defined over all
CFGs. Unary rules of the form Ni —* Nj can form
cycles, leading to infinite unary chains with infinite
mass. However, it is standard in the parsing liter-
ature to transform grammars into a restricted class
of CFGs so as to permit efficient parsing. Binariza-
tion of rules (Earley, 1970) is necessary to obtain
cubic parsing time, and closure of unary chains is re-
quired for finding total probability mass (rather than
just best parses) (Stolcke, 1995). To address this is-
sue, we define our model over a restricted class of
</bodyText>
<page confidence="0.982644">
960
</page>
<figure confidence="0.8172609">
Factory payrolls fell IN NN
in September
NN
NP
NNS
S
VBD
PP
VP
Phrasal rules
</figure>
<equation confidence="0.936499714285714">
r1 = S0,5 → NP0,2 VP2,5  |Factory payrolls fell in September
r3 = VP2,5 → VBD2,3 PP3,5  |Factory payrolls fell in September
. . .
Lexicon rules
r5 = NN0,1 → Factory  |Factory payrolls fell in September
r6 = NNS1,2 → payrolls  |Factory payrolls fell in September
. . .
</equation>
<figure confidence="0.623289">
(a) PCFG Structure (b) Rules r
</figure>
<figureCaption confidence="0.999892">
Figure 1: A parse tree and the corresponding rules over which potentials and features are defined.
</figureCaption>
<bodyText confidence="0.999657875">
CFGs which limits unary chains to not have any re-
peated states. This was done by collapsing all al-
lowed unary chains to single unary rules, and dis-
allowing multiple unary rule applications over the
same span.1 We give the details of our binarization
scheme in Section 5. Note that there exists a gram-
mar in this class which is weakly equivalent with any
arbitrary CFG.
</bodyText>
<subsectionHeader confidence="0.999509">
2.2 Computing the Objective Function
</subsectionHeader>
<bodyText confidence="0.999997333333333">
Our clique potentials take an exponential form. We
have a feature function, represented by f(r,s), which
returns a vector with the value for each feature. We
denote the value of feature fi by fi(r,s) and our
model has a corresponding parameter θi for each
feature. The clique potential function is then:
</bodyText>
<equation confidence="0.992562">
φ(r|s;θ) = exp∑ iθifi(r,s) (2)
</equation>
<bodyText confidence="0.697979">
The log conditional likelihood of the training data
D, with an additional L2 regularization term, is then:
</bodyText>
<equation confidence="0.9977076">
L (D;θ) =
� � !
θ2 i
∑ ∑r∈t∑i θifi(r,s) −Zs +∑i 2σ2 (3)
(t,s)∈D
</equation>
<bodyText confidence="0.999685">
And the partial derivatives of the log likelihood, with
respect to the model weights are, as usual, the dif-
ference between the empirical counts and the model
expectations:
</bodyText>
<equation confidence="0.979223166666667">
(r
s))
∑fi
,
Eθ Vi|s]! + θi (4)
(t,s)∈D r∈t
</equation>
<bodyText confidence="0.987369333333333">
1In our implementation of the inside-outside algorithm, we
then need to keep two inside and outside scores for each span:
one from before and one from after the application of unary
rules.
The partition function Zs and the partial derivatives
can be efficiently computed with the help of the
inside-outside algorithm.2 Zs is equal to the in-
side score of ROOT over the span of the entire sen-
tence. To compute the partial derivatives, we walk
through each rule, and span/split, and add the out-
side log-score of the parent, the inside log-score(s)
of the child(ren), and the log-score for that rule and
span/split. Zs is subtracted from this value to get the
normalized log probability of that rule in that posi-
tion. Using the probabilities of each rule applica-
tion, over each span/split, we can compute the ex-
pected feature values (the second term in Equation
4), by multiplying this probability by the value of
the feature corresponding to the weight for which we
are computing the partial derivative. The process is
analogous to the computation of partial derivatives
in linear chain CRFs. The complexity of the algo-
rithm for a particular sentence is O(n3), where n is
the length of the sentence.
</bodyText>
<subsectionHeader confidence="0.997339">
2.3 Parallelization
</subsectionHeader>
<bodyText confidence="0.942904733333333">
Unlike (Taskar et al., 2004), our algorithm has the
advantage of being easily parallelized (see footnote
7 in their paper). Because the computation of both
the log likelihood and the partial derivatives involves
summing over each tree individually, the compu-
tation can be parallelized by having many clients
which each do the computation for one tree, and one
central server which aggregates the information to
compute the relevant information for a set of trees.
Because we use a stochastic optimization method,
as discussed in Section 3, we compute the objec-
tive for only a small portion of the training data at
a time, typically between 15 and 30 sentences. In
2In our case the values in the chart are the clique potentials
which are non-negative numbers, but not probabilities.
</bodyText>
<equation confidence="0.8616595">
∂L
∂θi (
</equation>
<page confidence="0.973936">
961
</page>
<bodyText confidence="0.999905">
this case the gains from adding additional clients
decrease rapidly, because the computation time is
dominated by the longest sentences in the batch.
</bodyText>
<subsectionHeader confidence="0.996676">
2.4 Chart Prefiltering
</subsectionHeader>
<bodyText confidence="0.999990538461539">
Training is also sped up by prefiltering the chart. On
the inside pass of the algorithm one will see many
rules which cannot actually be tiled into complete
parses. In standard PCFG parsing it is not worth fig-
uring out which rules are viable at a particular chart
position and which are not. In our case however this
can make a big difference.We are not just looking
up a score for the rule, but must compute all the fea-
tures, and dot product them with the feature weights,
which is far more time consuming. We also have to
do an outside pass as well as an inside one, which
is sped up by not considering impossible rule appli-
cations. Lastly, we iterate through the data multi-
ple times, so if we can compute this information just
once, we will save time on all subsequent iterations
on that sentence. We do this by doing an inside-
outside pass that is just boolean valued to determine
which rules are possible at which positions in the
chart. We simultaneously compute the features for
the possible rules and then save the entire data struc-
ture to disk. For all but the shortest of sentences,
the disk I/O is easily worth the time compared to re-
computation. The first time we see a sentence this
method is still about one third faster than if we did
not do the prefiltering, and on subsequent iterations
the improvement is closer to tenfold.
</bodyText>
<sectionHeader confidence="0.996472" genericHeader="method">
3 Stochastic Optimization Methods
</sectionHeader>
<bodyText confidence="0.999811785714286">
Stochastic optimization methods have proven to be
extremely efficient for the training of models involv-
ing computationally expensive objective functions
like those encountered with our task (Vishwanathan
et al., 2006) and, in fact, the on-line backpropagation
learning used in the neural network parser of Hen-
derson (2004) is a form of stochastic gradient de-
scent. Standard deterministic optimization routines
such as L-BFGS (Liu and Nocedal, 1989) make little
progress in the initial iterations, often requiring sev-
eral passes through the data in order to satisfy suffi-
cient descent conditions placed on line searches. In
our experiments SGD converged to a lower objective
function value than L-BFGS, however it required far
</bodyText>
<figure confidence="0.514073">
Passes
</figure>
<figureCaption confidence="0.9860645">
Figure 2: WSJ15 objective value for L-BFGS and SGD
versus passes through the data. SGD ultimately con-
verges to a lower objective value, but does equally well
on test data.
</figureCaption>
<bodyText confidence="0.9974325">
fewer iterations (see Figure 2) and achieved compa-
rable test set performance to L-BFGS in a fraction of
the time. One early experiment on WSJ15 showed a
seven time speed up.
</bodyText>
<subsectionHeader confidence="0.996425">
3.1 Stochastic Function Evaluation
</subsectionHeader>
<bodyText confidence="0.999523090909091">
Utilization of stochastic optimization routines re-
quires the implementation of a stochastic objective
function. This function, Lˆ is designed to approx-
imate the true function L based off a small subset
of the training data represented by Db. Here b, the
batch size, means that Db is created by drawing b
training examples, with replacement, from the train-
ing set D. With this notation we can express the
stochastic evaluation of the function as Lˆ (Db;e).
This stochastic function must be designed to ensure
that:
</bodyText>
<equation confidence="0.996372">
� E � n i Lˆ (D(i)
b ;e) = L (D;e)
</equation>
<bodyText confidence="0.972667857142857">
Note that this property is satisfied, without scaling,
for objective functions that sum over the training
data, as it is in our case, but any priors must be
scaled down by a factor of b/|D|. The stochastic
gradient, �L (D(i)
b ;e), is then simply the derivative
of the stochastic function value.
</bodyText>
<subsectionHeader confidence="0.991105">
3.2 Stochastic Gradient Descent
</subsectionHeader>
<bodyText confidence="0.7809785">
SGD was implemented using the standard update:
ek+1 = ek − gk0L (D(k)
</bodyText>
<figure confidence="0.966348230769231">
b;ek)
0 x 105
−0.5
−1
−1.5
−2
−2.5
−3
−3.5
0 5 10 15 20 25 30 35 40 45 50
SGD
L−BFGS
Log Likelihood
</figure>
<page confidence="0.977581">
962
</page>
<bodyText confidence="0.969183">
And employed a gain schedule in the form
</bodyText>
<equation confidence="0.898954">
τ
ηk = η0
</equation>
<bodyText confidence="0.99993325">
where parameter τ was adjusted such that the gain is
halved after five passes through the data. We found
that an initial gain of η0 = 0.1 and batch size be-
tween 15 and 30 was optimal for this application.
</bodyText>
<sectionHeader confidence="0.999725" genericHeader="method">
4 Features
</sectionHeader>
<bodyText confidence="0.999976545454546">
As discussed in Section 5 we performed experi-
ments on both sentences of length &lt; 15 and length
&lt; 40. All feature development was done on the
length 15 corpus, due to the substantially faster
train and test times. This has the unfortunate effect
that our features are optimized for shorter sentences
and less training data, but we found development
on the longer sentences to be infeasible. Our fea-
tures are divided into two types: lexicon features,
which are over words and tags, and grammar fea-
tures which are over the local subtrees and corre-
sponding span/split (both have access to the entire
sentence). We ran two kinds of experiments: a dis-
criminatively trained model, which used only the
rules and no other grammar features, and a feature-
based model which did make use of grammar fea-
tures. Both models had access to the lexicon fea-
tures. We viewed this as equivalent to the more
elaborate, smoothed unknown word models that are
common in many PCFG parsers, such as (Klein and
Manning, 2003; Petrov et al., 2006).
We preprocessed the words in the sentences to ob-
tain two extra pieces of information. Firstly, each
word is annotated with a distributional similarity tag,
from a distributional similarity model (Clark, 2000)
trained on 100 million words from the British Na-
tional Corpus and English Gigaword corpus. Sec-
ondly, we compute a class for each word based on
the unknown word model of Klein and Manning
(2003); this model takes into account capitaliza-
tion, digits, dashes, and other character-level fea-
tures. The full set of features, along with an expla-
nation of our notation, is listed in Table 1.
</bodyText>
<sectionHeader confidence="0.999291" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9408115">
For all experiments, we trained and tested on the
Penn treebank (PTB) (Marcus et al., 1993). We used
</bodyText>
<table confidence="0.8158126">
Binary Unary
Model States Rules Rules
WSJ15 1,428 5,818 423
WSJ15 relaxed 1,428 22,376 613
WSJ40 7,613 28,240 823
</table>
<tableCaption confidence="0.998915">
Table 2: Grammar size for each of our models.
</tableCaption>
<bodyText confidence="0.999961">
the standard splits, training on sections 2 to 21, test-
ing on section 23 and doing development on section
22. Previous work on (non-reranking) discrimina-
tive parsing has given results on sentences of length
&lt; 15, but most parsing literature gives results on ei-
ther sentences of length &lt; 40, or all sentences. To
properly situate this work with respect to both sets
of literature we trained models on both length &lt;
15 (WSJ15) and length &lt; 40 (WSJ40), and we also
tested on all sentences using the WSJ40 models. Our
results also provide a context for interpreting previ-
ous work which used WSJ15 and not WSJ40.
We used a relatively simple grammar with few ad-
ditional annotations. Starting with the grammar read
off of the training set, we added parent annotations
onto each state, including the POS tags, resulting in
rules such as S-ROOT —* NP-S VP-S. We also added
head tag annotations to VPs, in the same manner as
(Klein and Manning, 2003). Lastly, for the WSJ40
runs we used a simple, right branching binarization
where each active state is annotated with its previous
sibling and first child. This is equivalent to children
of a state being produced by a second order Markov
process. For the WSJ15 runs, each active state was
annotated with only its first child, which is equiva-
lent to a first order Markov process. See Table 5 for
the number of states and rules produced.
</bodyText>
<subsectionHeader confidence="0.844859">
5.1 Experiments
</subsectionHeader>
<bodyText confidence="0.999889909090909">
For both WSJ15 and WSJ40, we trained a genera-
tive model; a discriminative model, which used lexi-
con features, but no grammar features other than the
rules themselves; and a feature-based model which
had access to all features. For the length 15 data we
also did experiments in which we relaxed the gram-
mar. By this we mean that we added (previously un-
seen) rules to the grammar, as a means of smoothing.
We chose which rules to add by taking existing rules
and modifying the parent annotation on the parent
of the rule. We used stochastic gradient descent for
</bodyText>
<equation confidence="0.702282">
τ +k
</equation>
<page confidence="0.99901">
963
</page>
<tableCaption confidence="0.9537045">
Table 1: Lexicon and grammar features. w is the word and t the tag. r represents a particular rule along with span/split
information; ρ is the rule itself, rp is the parent of the rule; wb, ws, and we are the first, first after the split (for binary
rules) and last word that a rule spans in a particular context. All states, including the POS tags, are annotated with
parent information; b(s) represents the base label for a state s and p(s) represents the parent annotation on state s.
ds(w) represents the distributional similarity cluster, and lc(w) the lower cased version of the word, and unk(w) the
unknown word class.
</tableCaption>
<table confidence="0.832255">
Lexicon Features Grammar Features
t ρ Binary-specific features
b(t) (b(p(rp)),ds(ws))
</table>
<listItem confidence="0.876298176470588">
(t,w) (b(p(rp)),ds(we))
(t,lc(w)) unary?
(b(t),w) simplified rule:
(b(t),lc(w)) base labels of states
(t,ds(w)) dist sim bigrams:
(t,ds(w_1)) all dist. sim. bigrams below
(t,ds(w+1)) rule, and base parent state
(b(t),ds(w)) dist sim bigrams:
(b(t),ds(w_1)) same as above, but trigrams
(b(t),ds(w+1)) heavy feature:
(p(t),w) whether the constituent is “big”
(t,unk(w)) as described in (Johnson, 2001)
(b(t),unk(w))
(b(p(rp)),ds(ws_1,dsws))
PP feature:
if right child is a PP then (r,ws)
VP features:
</listItem>
<bodyText confidence="0.924669392857143">
if some child is a verb tag, then rule,
with that child replaced by the word
Unaries which span one word:
(r,w)
(r,ds(w))
(b(p(r)),w)
(b(p(r)),ds(w))
these experiments; the length 15 models had a batch
size of 15 and we allowed twenty passes through
the data.3 The length 40 models had a batch size
of 30 and we allowed ten passes through the data.
We used development data to decide when the mod-
els had converged. Additionally, we provide gener-
ative numbers for training on the entire PTB to give
a sense of how much performance suffered from the
reduced training data (generative-all in Table 4).
The full results for WSJ15 are shown in Table 3
and for WSJ40 are shown in Table 4. The WSJ15
models were each trained on a single Dual-Core
AMD OpteronTM using three gigabytes of RAM and
no parallelization. The discriminatively trained gen-
erative model (discriminative in Table 3) took ap-
proximately 12 minutes per pass through the data,
while the feature-based model (feature-based in Ta-
ble 3) took 35 minutes per pass through the data.
The feature-based model with the relaxed grammar
(relaxed in Table 3) took about four times as long
as the regular feature-based model. The discrimina-
</bodyText>
<footnote confidence="0.9783764">
3Technically we did not make passes through the data, be-
cause we sampled with replacement to get our batches. By this
we mean having seen as many sentences as are in the data, de-
spite having seen some sentences multiple times and some not
at all.
</footnote>
<bodyText confidence="0.999855555555555">
tively trained generative WSJ40 model (discrimina-
tive in Table 4) was trained using two of the same
machines, with 16 gigabytes of RAM each for the
clients.4 It took about one day per pass through
the data. The feature-based WSJ40 model (feature-
based in Table 4) was trained using four of these
machines, also with 16 gigabytes of RAM each for
the clients. It took about three days per pass through
the data.
</bodyText>
<subsectionHeader confidence="0.992464">
5.2 Discussion
</subsectionHeader>
<bodyText confidence="0.999597769230769">
The results clearly show that gains came from both
the switch from generative to discriminative train-
ing, and from the extensive use of features. In Fig-
ure 3 we show for an example from section 22 the
parse trees produced by our generative model and
our feature-based discriminative model, and the cor-
rect parse. The parse from the feature-based model
better exhibits the right branching tendencies of En-
glish. This is likely due to the heavy feature, which
encourages long constituents at the end of the sen-
tence. It is difficult for a standard PCFG to learn this
aspect of the English language, because the score it
assigns to a rule does not take its span into account.
</bodyText>
<footnote confidence="0.976223">
4The server does almost no computation.
</footnote>
<page confidence="0.988584">
964
</page>
<table confidence="0.999904125">
Model P R F1 Exact Avg CB 0 CB P R F1 Exact Avg CB 0 CB
development set – length &lt; 15 test set – length &lt; 15
Taskar 2004 89.7 90.2 90.0 – – – 89.1 89.1 89.1 – – –
Turian 2007 – – – – – – 89.6 89.3 89.4 – – –
generative 86.9 85.8 86.4 46.2 0.34 81.2 87.6 85.8 86.7 49.2 0.33 81.9
discriminative 89.1 88.6 88.9 55.5 0.26 85.5 88.9 88.0 88.5 56.6 0.32 85.0
feature-based 90.4 89.3 89.9 59.5 0.24 88.3 91.1 90.2 90.6 61.3 0.24 86.8
relaxed 91.2 90.3 90.7 62.1 0.24 88.1 91.4 90.4 90.9 62.0 0.22 87.9
</table>
<tableCaption confidence="0.982312">
Table 3: Development and test set results, training and testing on sentences of length &lt; 15 from the Penn treebank.
</tableCaption>
<table confidence="0.999924142857143">
Model P R F1 Exact Avg CB 0 CB P R F1 Exact Avg CB 0 CB
test set – length &lt; 40 test set – all sentences
Petrov 2007 – – 88.8 – – – – – 88.3 – – –
generative 83.5 82.0 82.8 25.5 1.57 53.4 82.8 81.2 82.0 23.8 1.83 50.4
generative-all 83.6 82.1 82.8 25.2 1.56 53.3 – – – – – –
discriminative 85.1 84.5 84.8 29.7 1.41 55.8 84.2 83.7 83.9 27.8 1.67 52.8
feature-based 89.2 88.8 89.0 37.3 0.92 65.1 88.2 87.8 88.0 35.1 1.15 62.3
</table>
<tableCaption confidence="0.9971145">
Table 4: Test set results, training on sentences of length &lt; 40 from the Penn treebank. The generative-all results were
trained on all sentences regardless of length
</tableCaption>
<sectionHeader confidence="0.881565" genericHeader="method">
6 Comparison With Related Work
</sectionHeader>
<bodyText confidence="0.999980018518518">
The most similar related work is (Johnson, 2001),
which did discriminative training of a generative
PCFG. The model was quite similar to ours, except
that it did not incorporate any features and it re-
quired the parameters (which were just scores for
rules) to be locally normalized, as with a genera-
tively trained model. Due to training time, they used
the ATIS treebank corpus , which is much smaller
than even WSJ15, with only 1,088 training sen-
tences, 294 testing sentences, and an average sen-
tence length of around 11. They found no signif-
icant difference in performance between their gen-
eratively and discriminatively trained parsers. There
are two probable reasons for this result. The training
set is very small, and it is a known fact that gener-
ative models tend to work better for small datasets
and discriminative models tend to work better for
larger datasets (Ng and Jordan, 2002). Additionally,
they made no use of features, one of the primary
benefits of discriminative learning.
Taskar et al. (2004) took a large margin approach
to discriminative learning, but achieved only small
gains. We suspect that this is in part due to the gram-
mar that they chose – the grammar of (Klein and
Manning, 2003), which was hand annotated with the
intent of optimizing performance of a PCFG. This
grammar is fairly sparse – for any particular state
there are, on average, only a few rules with that state
as a parent – so the learning algorithm may have suf-
fered because there were few options to discriminate
between. Starting with this grammar we found it dif-
ficult to achieve gains as well. Additionally, their
long training time (several months for WSJ15, ac-
cording to (Turian and Melamed, 2006)) made fea-
ture engineering difficult; they were unable to really
explore the space of possible features.
More recent is the work of (Turian and Melamed,
2006; Turian et al., 2007), which improved both the
training time and accuracy of (Taskar et al., 2004).
They define a simple linear model, use boosted de-
cision trees to select feature conjunctions, and a line
search to optimize the parameters. They use an
agenda parser, and define their atomic features, from
which the decision trees are constructed, over the en-
tire state being considered. While they make exten-
sive use of features, their setup is much more com-
plex than ours and takes substantially longer to train
– up to 5 days on WSJ15 – while achieving only
small gains over (Taskar et al., 2004).
The most recent similar research is (Petrov et al.,
2007). They also do discriminative parsing of length
40 sentences, but with a substantially different setup.
Following up on their previous work (Petrov et al.,
2006) on grammar splitting, they do discriminative
</bodyText>
<page confidence="0.992082">
965
</page>
<figure confidence="0.999778571428572">
S
RB
DT
NP
S
VP
NP
VP
NP
S
VBZ
S
VP
NP
PRP
VBZ
S
NP
PRP
He
VBZ
adds
VP
NP
VP
S
He adds NP VP
PRP
VBZ
This
is
n’t
NP
VP
RB
VBZ
NP
DT
VP
RB
VBZ
NP
DT
He adds CD VBN This is n’t NP VP
This is n’t CD VBN
1987 revisited CD VBN
1987 revisited
1987 revisited
(a) generative output (b) feature-based discriminative output (c) gold parse
</figure>
<figureCaption confidence="0.999954">
Figure 3: Example output from our generative and feature-based discriminative models, along with the correct parse.
</figureCaption>
<bodyText confidence="0.999153444444444">
parsing with latent variables, which requires them
to optimize a non-convex function. Instead of us-
ing a stochastic optimization technique, they use L-
BFGS, but do coarse-to-fine pruning to approximate
their gradients and log likelihood. Because they
were focusing on grammar splitting they, like (John-
son, 2001), did not employ any features, and, like
(Taskar et al., 2004), they saw only small gains from
switching from generative to discriminative training.
</bodyText>
<sectionHeader confidence="0.999209" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999997222222223">
We have presented a new, feature-rich, dynamic pro-
gramming based discriminative parser which is sim-
pler, more effective, and faster to train and test than
previous work, giving us new state-of-the-art per-
formance when training and testing on sentences of
length &lt; 15 and the first results for such a parser
trained and tested on sentences of length &lt; 40. We
also show that the use of SGD for training CRFs per-
forms as well as L-BFGS in a fraction of the time.
Other recent work on discriminative parsing has ne-
glected the use of features, despite their being one of
the main advantages of discriminative training meth-
ods. Looking at how other tasks, such as named
entity recognition and part-of-speech tagging, have
evolved over time, it is clear that greater gains are to
be gotten from developing better features than from
better models. We have provided just such a frame-
work for improving parsing performance.
</bodyText>
<sectionHeader confidence="0.998572" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999923615384615">
Many thanks to Teg Grenager and Paul Heymann
for their advice (and their general awesomeness),
and to our anonymous reviewers for helpful com-
ments.
This paper is based on work funded in part by
the Defense Advanced Research Projects Agency
through IBM, by the Disruptive Technology Office
(DTO) Phase III Program for Advanced Question
Answering for Intelligence (AQUAINT) through
Broad Agency Announcement (BAA) N61339-06-
R-0034, and by a Scottish Enterprise Edinburgh-
Stanford Link grant (R37588), as part of the EASIE
project.
</bodyText>
<sectionHeader confidence="0.999247" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999823727272727">
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In ACL 43, pages 173–180.
Alexander Clark. 2000. Inducing syntactic categories by
context distribution clustering. In Proc. of Conference
on Computational Natural Language Learning, pages
91–94, Lisbon, Portugal.
Trevor Cohn and Philip Blunsom. 2005. Semantic
role labelling with tree conditional random fields. In
CoNLL 2005.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In ICML 17, pages 175–182.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 6(8):451–455.
James Henderson. 2004. Discriminative training of a
neural network statistical parser. In ACL 42, pages 96–
103.
Mark Johnson. 2001. Joint and conditional estimation of
tagging and parsing models. In Meeting of the Associ-
ation for Computational Linguistics, pages 314–321.
Florent Jousse, R´emi Gilleron, Isabelle Tellier, and Marc
Tommasi. 2006. Conditional Random Fields for XML
</reference>
<page confidence="0.985803">
966
</page>
<reference confidence="0.999870407407408">
trees. In ECML Workshop on Mining and Learning in
Graphs.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the Associa-
tion of Computational Linguistics (ACL).
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic mod-
els for segmenting and labeling sequence data. In
ICML 2001, pages 282–289. Morgan Kaufmann, San
Francisco, CA.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Math. Programming, 45(3, (Ser. B)):503–528.
Christopher D. Manning and Hinrich Sch¨utze. 1999.
Foundations of Statistical Natural Language Process-
ing. The MIT Press, Cambridge, Massachusetts.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313–330.
Andrew Ng and Michael Jordan. 2002. On discrimina-
tive vs. generative classifiers: A comparison of logistic
regression and naive bayes. In Advances in Neural In-
formation Processing Systems (NIPS).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In ACL 44/COLING 21,
pages 433–440.
Slav Petrov, Adam Pauls, and Dan Klein. 2007. Dis-
criminative log-linear grammars with latent variables.
In NIPS.
Adwait Ratnaparkhi. 1997. A linear observed time sta-
tistical parser based on maximum entropy models. In
EMNLP 2, pages 1–10.
Andreas Stolcke. 1995. An efficient probabilistic
context-free parsing algorithm that computes prefix
probabilities. Computational Linguistics, 21:165–
202.
Ben Taskar, Dan Klein, Michael Collins, Daphne Koller,
and Christopher D. Manning. 2004. Max-margin
parsing. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).
Joseph Turian and I. Dan Melamed. 2006. Advances in
discriminative parsing. In ACL 44, pages 873–880.
Joseph Turian, Ben Wellington, and I. Dan Melamed.
2007. Scalable discriminative learning for natural lan-
guage parsing and translation. In Advances in Neural
Information Processing Systems 19, pages 1409–1416.
MIT Press.
S. V. N. Vishwanathan, Nichol N. Schraudolph, Mark W.
Schmidt, and Kevin P. Murphy. 2006. Accelerated
training of conditional random fields with stochastic
gradient methods. In ICML 23, pages 969–976.
</reference>
<page confidence="0.997635">
967
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.945401">
<title confidence="0.99816">Efficient, Feature-based, Conditional Random Field Parsing</title>
<author confidence="0.999918">Jenny Rose Finkel</author>
<author confidence="0.999918">Alex Kleeman</author>
<author confidence="0.999918">Christopher D Manning</author>
<affiliation confidence="0.9999185">Department of Computer Science Stanford University</affiliation>
<address confidence="0.999958">Stanford, CA 94305</address>
<email confidence="0.999807">jrfinkel@cs.stanford.edu,akleeman@stanford.edu,manning@cs.stanford.edu</email>
<abstract confidence="0.997492857142857">Discriminative feature-based methods are widely used in natural language processing, but sentence parsing is still dominated by generative methods. While prior feature-based dynamic programming parsers have restricted training and evaluation to artificially short sentences, we present the first general, featurerich discriminative parser, based on a conditional random field model, which has been successfully scaled to the full WSJ parsing data. Our efficiency is primarily due to the use of stochastic optimization techniques, as well as parallelization and chart prefiltering. On WSJ15, we attain a state-of-the-artF-score a 14% relative reduction in error over previous models, while being two orders of magnitude faster. On sentences of length our system achieves an F-score of a 36% relative reduction in error over a generative baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In ACL 43,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="1901" citStr="Charniak and Johnson, 2005" startWordPosition="267" endWordPosition="270"> tasks. Although they take much longer to train than generative models, they typically produce higher performing systems, in large part due to the ability to incorporate arbitrary, potentially overlapping features. However, constituency parsing remains an area dominated by generative methods, due to the computational complexity of the problem. Previous work on discriminative parsing falls under one of three approaches. One approach does discriminative reranking of the n-best list of a generative parser, still usually depending highly on the generative parser score as a feature (Collins, 2000; Charniak and Johnson, 2005). A second group of papers does parsing by a sequence of independent, discriminative decisions, either greedily or with use of a small beam (Ratnaparkhi, 1997; Henderson, 2004). This paper extends the third thread of work, where joint inference via dynamic programming algorithms is used to train models and to attempt to find the globally best parse. Work in this context has mainly been limited to use of artificially short sentences due to exorbitant training and inference times. One exception is the recent work of Petrov et al. (2007), who discriminatively train a grammar with latent variables</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and maxent discriminative reranking. In ACL 43, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Inducing syntactic categories by context distribution clustering.</title>
<date>2000</date>
<booktitle>In Proc. of Conference on Computational Natural Language Learning,</booktitle>
<pages>91--94</pages>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="16855" citStr="Clark, 2000" startWordPosition="2827" endWordPosition="2828">wo kinds of experiments: a discriminatively trained model, which used only the rules and no other grammar features, and a featurebased model which did make use of grammar features. Both models had access to the lexicon features. We viewed this as equivalent to the more elaborate, smoothed unknown word models that are common in many PCFG parsers, such as (Klein and Manning, 2003; Petrov et al., 2006). We preprocessed the words in the sentences to obtain two extra pieces of information. Firstly, each word is annotated with a distributional similarity tag, from a distributional similarity model (Clark, 2000) trained on 100 million words from the British National Corpus and English Gigaword corpus. Secondly, we compute a class for each word based on the unknown word model of Klein and Manning (2003); this model takes into account capitalization, digits, dashes, and other character-level features. The full set of features, along with an explanation of our notation, is listed in Table 1. 5 Experiments For all experiments, we trained and tested on the Penn treebank (PTB) (Marcus et al., 1993). We used Binary Unary Model States Rules Rules WSJ15 1,428 5,818 423 WSJ15 relaxed 1,428 22,376 613 WSJ40 7,6</context>
</contexts>
<marker>Clark, 2000</marker>
<rawString>Alexander Clark. 2000. Inducing syntactic categories by context distribution clustering. In Proc. of Conference on Computational Natural Language Learning, pages 91–94, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Philip Blunsom</author>
</authors>
<title>Semantic role labelling with tree conditional random fields.</title>
<date>2005</date>
<booktitle>In CoNLL</booktitle>
<contexts>
<context position="5542" citStr="Cohn and Blunsom, 2005" startWordPosition="855" endWordPosition="858">rser performance between using stochastic gradient descent (SGD), and the more common, but significantly slower, L-BFGS. We also used limited parallelization, and prefiltering of the chart to avoid scoring rules which cannot tile into complete parses of the sentence. This speed-up does not come with a performance cost; we attain an F-score of 90.9%, a 14% relative reduction in errors over previous work on WSJ15. 2 The Model 2.1 A Conditional Random Field Context Free Grammar (CRF-CFG) Our parsing model is based on a conditional random field model, however, unlike previous TreeCRF work, e.g., (Cohn and Blunsom, 2005; Jousse et al., 2006), we do not assume a particular tree structure, and instead find the most likely structure and labeling. This is similar to conventional probabilistic context-free grammar (PCFG) parsing, with two exceptions: (a) we maximize conditional likelihood of the parse tree, given the sentence, not joint likelihood of the tree and sentence; and (b) probabilities are normalized globally instead of locally – the graphical models depiction of our trees is undirected. Formally, we have a CFG G, which consists of (Manning and Sch¨utze, 1999): (i) a set of terminals {wk},k = 1,...,V; (i</context>
</contexts>
<marker>Cohn, Blunsom, 2005</marker>
<rawString>Trevor Cohn and Philip Blunsom. 2005. Semantic role labelling with tree conditional random fields. In CoNLL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In ICML 17,</booktitle>
<pages>175--182</pages>
<contexts>
<context position="1872" citStr="Collins, 2000" startWordPosition="265" endWordPosition="266">uage processing tasks. Although they take much longer to train than generative models, they typically produce higher performing systems, in large part due to the ability to incorporate arbitrary, potentially overlapping features. However, constituency parsing remains an area dominated by generative methods, due to the computational complexity of the problem. Previous work on discriminative parsing falls under one of three approaches. One approach does discriminative reranking of the n-best list of a generative parser, still usually depending highly on the generative parser score as a feature (Collins, 2000; Charniak and Johnson, 2005). A second group of papers does parsing by a sequence of independent, discriminative decisions, either greedily or with use of a small beam (Ratnaparkhi, 1997; Henderson, 2004). This paper extends the third thread of work, where joint inference via dynamic programming algorithms is used to train models and to attempt to find the globally best parse. Work in this context has mainly been limited to use of artificially short sentences due to exorbitant training and inference times. One exception is the recent work of Petrov et al. (2007), who discriminatively train a </context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language parsing. In ICML 17, pages 175–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<volume>6</volume>
<issue>8</issue>
<contexts>
<context position="7796" citStr="Earley, 1970" startWordPosition="1252" endWordPosition="1253"> possible parses sum to unity, is defined over all structures as well as all labelings of those structures. We define τ(s) to be the set of all possible parse trees for the given sentence licensed by the grammar G. 1 P(t|s;θ) = Zs ∏ rEtφ(r|s;θ) (1) where Zs = ∑ tEτ(s)∏ rEt′ φ(r|s;θ) The above model is not well-defined over all CFGs. Unary rules of the form Ni —* Nj can form cycles, leading to infinite unary chains with infinite mass. However, it is standard in the parsing literature to transform grammars into a restricted class of CFGs so as to permit efficient parsing. Binarization of rules (Earley, 1970) is necessary to obtain cubic parsing time, and closure of unary chains is required for finding total probability mass (rather than just best parses) (Stolcke, 1995). To address this issue, we define our model over a restricted class of 960 Factory payrolls fell IN NN in September NN NP NNS S VBD PP VP Phrasal rules r1 = S0,5 → NP0,2 VP2,5 |Factory payrolls fell in September r3 = VP2,5 → VBD2,3 PP3,5 |Factory payrolls fell in September . . . Lexicon rules r5 = NN0,1 → Factory |Factory payrolls fell in September r6 = NNS1,2 → payrolls |Factory payrolls fell in September . . . (a) PCFG Structure</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Jay Earley. 1970. An efficient context-free parsing algorithm. Communications of the ACM, 6(8):451–455.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
</authors>
<title>Discriminative training of a neural network statistical parser.</title>
<date>2004</date>
<booktitle>In ACL 42,</booktitle>
<pages>96--103</pages>
<contexts>
<context position="2077" citStr="Henderson, 2004" startWordPosition="297" endWordPosition="298">ially overlapping features. However, constituency parsing remains an area dominated by generative methods, due to the computational complexity of the problem. Previous work on discriminative parsing falls under one of three approaches. One approach does discriminative reranking of the n-best list of a generative parser, still usually depending highly on the generative parser score as a feature (Collins, 2000; Charniak and Johnson, 2005). A second group of papers does parsing by a sequence of independent, discriminative decisions, either greedily or with use of a small beam (Ratnaparkhi, 1997; Henderson, 2004). This paper extends the third thread of work, where joint inference via dynamic programming algorithms is used to train models and to attempt to find the globally best parse. Work in this context has mainly been limited to use of artificially short sentences due to exorbitant training and inference times. One exception is the recent work of Petrov et al. (2007), who discriminatively train a grammar with latent variables and do not restrict themselves to short sentences. However their model, like the discriminative parser of Johnson (2001), makes no use of features, and effectively ignores the</context>
<context position="13518" citStr="Henderson (2004)" startWordPosition="2241" endWordPosition="2243">sentences, the disk I/O is easily worth the time compared to recomputation. The first time we see a sentence this method is still about one third faster than if we did not do the prefiltering, and on subsequent iterations the improvement is closer to tenfold. 3 Stochastic Optimization Methods Stochastic optimization methods have proven to be extremely efficient for the training of models involving computationally expensive objective functions like those encountered with our task (Vishwanathan et al., 2006) and, in fact, the on-line backpropagation learning used in the neural network parser of Henderson (2004) is a form of stochastic gradient descent. Standard deterministic optimization routines such as L-BFGS (Liu and Nocedal, 1989) make little progress in the initial iterations, often requiring several passes through the data in order to satisfy sufficient descent conditions placed on line searches. In our experiments SGD converged to a lower objective function value than L-BFGS, however it required far Passes Figure 2: WSJ15 objective value for L-BFGS and SGD versus passes through the data. SGD ultimately converges to a lower objective value, but does equally well on test data. fewer iterations </context>
</contexts>
<marker>Henderson, 2004</marker>
<rawString>James Henderson. 2004. Discriminative training of a neural network statistical parser. In ACL 42, pages 96– 103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Joint and conditional estimation of tagging and parsing models.</title>
<date>2001</date>
<booktitle>In Meeting of the Association for Computational Linguistics,</booktitle>
<pages>314--321</pages>
<contexts>
<context position="2622" citStr="Johnson (2001)" startWordPosition="387" endWordPosition="389">dily or with use of a small beam (Ratnaparkhi, 1997; Henderson, 2004). This paper extends the third thread of work, where joint inference via dynamic programming algorithms is used to train models and to attempt to find the globally best parse. Work in this context has mainly been limited to use of artificially short sentences due to exorbitant training and inference times. One exception is the recent work of Petrov et al. (2007), who discriminatively train a grammar with latent variables and do not restrict themselves to short sentences. However their model, like the discriminative parser of Johnson (2001), makes no use of features, and effectively ignores the largest advantage of discriminative training. It has been shown on other NLP tasks that modeling improvements, such as the switch from generative training to discriminative training, usually provide much smaller performance gains than the gains possible from good feature engineering. For example, in (Lafferty et al., 2001), when switching from a generatively trained hidden Markov model (HMM) to a discriminatively trained, linear chain, conditional random field (CRF) for part-of-speech tagging, their error drops from 5.7% to 5.6%. When the</context>
<context position="20587" citStr="Johnson, 2001" startWordPosition="3461" endWordPosition="3462">resents the distributional similarity cluster, and lc(w) the lower cased version of the word, and unk(w) the unknown word class. Lexicon Features Grammar Features t ρ Binary-specific features b(t) (b(p(rp)),ds(ws)) (t,w) (b(p(rp)),ds(we)) (t,lc(w)) unary? (b(t),w) simplified rule: (b(t),lc(w)) base labels of states (t,ds(w)) dist sim bigrams: (t,ds(w_1)) all dist. sim. bigrams below (t,ds(w+1)) rule, and base parent state (b(t),ds(w)) dist sim bigrams: (b(t),ds(w_1)) same as above, but trigrams (b(t),ds(w+1)) heavy feature: (p(t),w) whether the constituent is “big” (t,unk(w)) as described in (Johnson, 2001) (b(t),unk(w)) (b(p(rp)),ds(ws_1,dsws)) PP feature: if right child is a PP then (r,ws) VP features: if some child is a verb tag, then rule, with that child replaced by the word Unaries which span one word: (r,w) (r,ds(w)) (b(p(r)),w) (b(p(r)),ds(w)) these experiments; the length 15 models had a batch size of 15 and we allowed twenty passes through the data.3 The length 40 models had a batch size of 30 and we allowed ten passes through the data. We used development data to decide when the models had converged. Additionally, we provide generative numbers for training on the entire PTB to give a </context>
<context position="24543" citStr="Johnson, 2001" startWordPosition="4188" endWordPosition="4189">B test set – length &lt; 40 test set – all sentences Petrov 2007 – – 88.8 – – – – – 88.3 – – – generative 83.5 82.0 82.8 25.5 1.57 53.4 82.8 81.2 82.0 23.8 1.83 50.4 generative-all 83.6 82.1 82.8 25.2 1.56 53.3 – – – – – – discriminative 85.1 84.5 84.8 29.7 1.41 55.8 84.2 83.7 83.9 27.8 1.67 52.8 feature-based 89.2 88.8 89.0 37.3 0.92 65.1 88.2 87.8 88.0 35.1 1.15 62.3 Table 4: Test set results, training on sentences of length &lt; 40 from the Penn treebank. The generative-all results were trained on all sentences regardless of length 6 Comparison With Related Work The most similar related work is (Johnson, 2001), which did discriminative training of a generative PCFG. The model was quite similar to ours, except that it did not incorporate any features and it required the parameters (which were just scores for rules) to be locally normalized, as with a generatively trained model. Due to training time, they used the ATIS treebank corpus , which is much smaller than even WSJ15, with only 1,088 training sentences, 294 testing sentences, and an average sentence length of around 11. They found no significant difference in performance between their generatively and discriminatively trained parsers. There ar</context>
<context position="27977" citStr="Johnson, 2001" startWordPosition="4783" endWordPosition="4785">NP DT He adds CD VBN This is n’t NP VP This is n’t CD VBN 1987 revisited CD VBN 1987 revisited 1987 revisited (a) generative output (b) feature-based discriminative output (c) gold parse Figure 3: Example output from our generative and feature-based discriminative models, along with the correct parse. parsing with latent variables, which requires them to optimize a non-convex function. Instead of using a stochastic optimization technique, they use LBFGS, but do coarse-to-fine pruning to approximate their gradients and log likelihood. Because they were focusing on grammar splitting they, like (Johnson, 2001), did not employ any features, and, like (Taskar et al., 2004), they saw only small gains from switching from generative to discriminative training. 7 Conclusions We have presented a new, feature-rich, dynamic programming based discriminative parser which is simpler, more effective, and faster to train and test than previous work, giving us new state-of-the-art performance when training and testing on sentences of length &lt; 15 and the first results for such a parser trained and tested on sentences of length &lt; 40. We also show that the use of SGD for training CRFs performs as well as L-BFGS in a</context>
</contexts>
<marker>Johnson, 2001</marker>
<rawString>Mark Johnson. 2001. Joint and conditional estimation of tagging and parsing models. In Meeting of the Association for Computational Linguistics, pages 314–321.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florent Jousse</author>
<author>R´emi Gilleron</author>
<author>Isabelle Tellier</author>
<author>Marc Tommasi</author>
</authors>
<title>Conditional Random Fields for XML trees.</title>
<date>2006</date>
<booktitle>In ECML Workshop on Mining and Learning in Graphs.</booktitle>
<contexts>
<context position="5564" citStr="Jousse et al., 2006" startWordPosition="859" endWordPosition="862"> using stochastic gradient descent (SGD), and the more common, but significantly slower, L-BFGS. We also used limited parallelization, and prefiltering of the chart to avoid scoring rules which cannot tile into complete parses of the sentence. This speed-up does not come with a performance cost; we attain an F-score of 90.9%, a 14% relative reduction in errors over previous work on WSJ15. 2 The Model 2.1 A Conditional Random Field Context Free Grammar (CRF-CFG) Our parsing model is based on a conditional random field model, however, unlike previous TreeCRF work, e.g., (Cohn and Blunsom, 2005; Jousse et al., 2006), we do not assume a particular tree structure, and instead find the most likely structure and labeling. This is similar to conventional probabilistic context-free grammar (PCFG) parsing, with two exceptions: (a) we maximize conditional likelihood of the parse tree, given the sentence, not joint likelihood of the tree and sentence; and (b) probabilities are normalized globally instead of locally – the graphical models depiction of our trees is undirected. Formally, we have a CFG G, which consists of (Manning and Sch¨utze, 1999): (i) a set of terminals {wk},k = 1,...,V; (ii) a set of nontermina</context>
</contexts>
<marker>Jousse, Gilleron, Tellier, Tommasi, 2006</marker>
<rawString>Florent Jousse, R´emi Gilleron, Isabelle Tellier, and Marc Tommasi. 2006. Conditional Random Fields for XML trees. In ECML Workshop on Mining and Learning in Graphs.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the Association of Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="16623" citStr="Klein and Manning, 2003" startWordPosition="2789" endWordPosition="2792">to be infeasible. Our features are divided into two types: lexicon features, which are over words and tags, and grammar features which are over the local subtrees and corresponding span/split (both have access to the entire sentence). We ran two kinds of experiments: a discriminatively trained model, which used only the rules and no other grammar features, and a featurebased model which did make use of grammar features. Both models had access to the lexicon features. We viewed this as equivalent to the more elaborate, smoothed unknown word models that are common in many PCFG parsers, such as (Klein and Manning, 2003; Petrov et al., 2006). We preprocessed the words in the sentences to obtain two extra pieces of information. Firstly, each word is annotated with a distributional similarity tag, from a distributional similarity model (Clark, 2000) trained on 100 million words from the British National Corpus and English Gigaword corpus. Secondly, we compute a class for each word based on the unknown word model of Klein and Manning (2003); this model takes into account capitalization, digits, dashes, and other character-level features. The full set of features, along with an explanation of our notation, is li</context>
<context position="18455" citStr="Klein and Manning, 2003" startWordPosition="3105" endWordPosition="3108">erly situate this work with respect to both sets of literature we trained models on both length &lt; 15 (WSJ15) and length &lt; 40 (WSJ40), and we also tested on all sentences using the WSJ40 models. Our results also provide a context for interpreting previous work which used WSJ15 and not WSJ40. We used a relatively simple grammar with few additional annotations. Starting with the grammar read off of the training set, we added parent annotations onto each state, including the POS tags, resulting in rules such as S-ROOT —* NP-S VP-S. We also added head tag annotations to VPs, in the same manner as (Klein and Manning, 2003). Lastly, for the WSJ40 runs we used a simple, right branching binarization where each active state is annotated with its previous sibling and first child. This is equivalent to children of a state being produced by a second order Markov process. For the WSJ15 runs, each active state was annotated with only its first child, which is equivalent to a first order Markov process. See Table 5 for the number of states and rules produced. 5.1 Experiments For both WSJ15 and WSJ40, we trained a generative model; a discriminative model, which used lexicon features, but no grammar features other than the</context>
<context position="25707" citStr="Klein and Manning, 2003" startWordPosition="4384" endWordPosition="4387"> generatively and discriminatively trained parsers. There are two probable reasons for this result. The training set is very small, and it is a known fact that generative models tend to work better for small datasets and discriminative models tend to work better for larger datasets (Ng and Jordan, 2002). Additionally, they made no use of features, one of the primary benefits of discriminative learning. Taskar et al. (2004) took a large margin approach to discriminative learning, but achieved only small gains. We suspect that this is in part due to the grammar that they chose – the grammar of (Klein and Manning, 2003), which was hand annotated with the intent of optimizing performance of a PCFG. This grammar is fairly sparse – for any particular state there are, on average, only a few rules with that state as a parent – so the learning algorithm may have suffered because there were few options to discriminate between. Starting with this grammar we found it difficult to achieve gains as well. Additionally, their long training time (several months for WSJ15, according to (Turian and Melamed, 2006)) made feature engineering difficult; they were unable to really explore the space of possible features. More rec</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the Association of Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic models for segmenting and labeling sequence data. In ICML</title>
<date>2001</date>
<pages>282--289</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="3002" citStr="Lafferty et al., 2001" startWordPosition="445" endWordPosition="448">mes. One exception is the recent work of Petrov et al. (2007), who discriminatively train a grammar with latent variables and do not restrict themselves to short sentences. However their model, like the discriminative parser of Johnson (2001), makes no use of features, and effectively ignores the largest advantage of discriminative training. It has been shown on other NLP tasks that modeling improvements, such as the switch from generative training to discriminative training, usually provide much smaller performance gains than the gains possible from good feature engineering. For example, in (Lafferty et al., 2001), when switching from a generatively trained hidden Markov model (HMM) to a discriminatively trained, linear chain, conditional random field (CRF) for part-of-speech tagging, their error drops from 5.7% to 5.6%. When they add in only a small set of orthographic features, their CRF error rate drops considerably more to 4.3%, and their out-of-vocabulary error rate drops by more than half. This is further supported by Johnson (2001), who saw no parsing gains when switch959 Proceedings of ACL-08: HLT, pages 959–967, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics i</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional Random Fields: Probabilistic models for segmenting and labeling sequence data. In ICML 2001, pages 282–289. Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<journal>Math. Programming,</journal>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="13644" citStr="Liu and Nocedal, 1989" startWordPosition="2259" endWordPosition="2262"> is still about one third faster than if we did not do the prefiltering, and on subsequent iterations the improvement is closer to tenfold. 3 Stochastic Optimization Methods Stochastic optimization methods have proven to be extremely efficient for the training of models involving computationally expensive objective functions like those encountered with our task (Vishwanathan et al., 2006) and, in fact, the on-line backpropagation learning used in the neural network parser of Henderson (2004) is a form of stochastic gradient descent. Standard deterministic optimization routines such as L-BFGS (Liu and Nocedal, 1989) make little progress in the initial iterations, often requiring several passes through the data in order to satisfy sufficient descent conditions placed on line searches. In our experiments SGD converged to a lower objective function value than L-BFGS, however it required far Passes Figure 2: WSJ15 objective value for L-BFGS and SGD versus passes through the data. SGD ultimately converges to a lower objective value, but does equally well on test data. fewer iterations (see Figure 2) and achieved comparable test set performance to L-BFGS in a fraction of the time. One early experiment on WSJ15</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Dong C. Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Math. Programming, 45(3, (Ser. B)):503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="17345" citStr="Marcus et al., 1993" startWordPosition="2910" endWordPosition="2913">mation. Firstly, each word is annotated with a distributional similarity tag, from a distributional similarity model (Clark, 2000) trained on 100 million words from the British National Corpus and English Gigaword corpus. Secondly, we compute a class for each word based on the unknown word model of Klein and Manning (2003); this model takes into account capitalization, digits, dashes, and other character-level features. The full set of features, along with an explanation of our notation, is listed in Table 1. 5 Experiments For all experiments, we trained and tested on the Penn treebank (PTB) (Marcus et al., 1993). We used Binary Unary Model States Rules Rules WSJ15 1,428 5,818 423 WSJ15 relaxed 1,428 22,376 613 WSJ40 7,613 28,240 823 Table 2: Grammar size for each of our models. the standard splits, training on sections 2 to 21, testing on section 23 and doing development on section 22. Previous work on (non-reranking) discriminative parsing has given results on sentences of length &lt; 15, but most parsing literature gives results on either sentences of length &lt; 40, or all sentences. To properly situate this work with respect to both sets of literature we trained models on both length &lt; 15 (WSJ15) and l</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Ng</author>
<author>Michael Jordan</author>
</authors>
<title>On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes.</title>
<date>2002</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="25387" citStr="Ng and Jordan, 2002" startWordPosition="4329" endWordPosition="4332">ormalized, as with a generatively trained model. Due to training time, they used the ATIS treebank corpus , which is much smaller than even WSJ15, with only 1,088 training sentences, 294 testing sentences, and an average sentence length of around 11. They found no significant difference in performance between their generatively and discriminatively trained parsers. There are two probable reasons for this result. The training set is very small, and it is a known fact that generative models tend to work better for small datasets and discriminative models tend to work better for larger datasets (Ng and Jordan, 2002). Additionally, they made no use of features, one of the primary benefits of discriminative learning. Taskar et al. (2004) took a large margin approach to discriminative learning, but achieved only small gains. We suspect that this is in part due to the grammar that they chose – the grammar of (Klein and Manning, 2003), which was hand annotated with the intent of optimizing performance of a PCFG. This grammar is fairly sparse – for any particular state there are, on average, only a few rules with that state as a parent – so the learning algorithm may have suffered because there were few option</context>
</contexts>
<marker>Ng, Jordan, 2002</marker>
<rawString>Andrew Ng and Michael Jordan. 2002. On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. In Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In ACL 44/COLING 21,</booktitle>
<pages>433--440</pages>
<contexts>
<context position="16645" citStr="Petrov et al., 2006" startWordPosition="2793" endWordPosition="2796">tures are divided into two types: lexicon features, which are over words and tags, and grammar features which are over the local subtrees and corresponding span/split (both have access to the entire sentence). We ran two kinds of experiments: a discriminatively trained model, which used only the rules and no other grammar features, and a featurebased model which did make use of grammar features. Both models had access to the lexicon features. We viewed this as equivalent to the more elaborate, smoothed unknown word models that are common in many PCFG parsers, such as (Klein and Manning, 2003; Petrov et al., 2006). We preprocessed the words in the sentences to obtain two extra pieces of information. Firstly, each word is annotated with a distributional similarity tag, from a distributional similarity model (Clark, 2000) trained on 100 million words from the British National Corpus and English Gigaword corpus. Secondly, we compute a class for each word based on the unknown word model of Klein and Manning (2003); this model takes into account capitalization, digits, dashes, and other character-level features. The full set of features, along with an explanation of our notation, is listed in Table 1. 5 Exp</context>
<context position="27171" citStr="Petrov et al., 2006" startWordPosition="4634" endWordPosition="4637"> line search to optimize the parameters. They use an agenda parser, and define their atomic features, from which the decision trees are constructed, over the entire state being considered. While they make extensive use of features, their setup is much more complex than ours and takes substantially longer to train – up to 5 days on WSJ15 – while achieving only small gains over (Taskar et al., 2004). The most recent similar research is (Petrov et al., 2007). They also do discriminative parsing of length 40 sentences, but with a substantially different setup. Following up on their previous work (Petrov et al., 2006) on grammar splitting, they do discriminative 965 S RB DT NP S VP NP VP NP S VBZ S VP NP PRP VBZ S NP PRP He VBZ adds VP NP VP S He adds NP VP PRP VBZ This is n’t NP VP RB VBZ NP DT VP RB VBZ NP DT He adds CD VBN This is n’t NP VP This is n’t CD VBN 1987 revisited CD VBN 1987 revisited 1987 revisited (a) generative output (b) feature-based discriminative output (c) gold parse Figure 3: Example output from our generative and feature-based discriminative models, along with the correct parse. parsing with latent variables, which requires them to optimize a non-convex function. Instead of using a </context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In ACL 44/COLING 21, pages 433–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Discriminative log-linear grammars with latent variables.</title>
<date>2007</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="2441" citStr="Petrov et al. (2007)" startWordPosition="358" endWordPosition="361">generative parser score as a feature (Collins, 2000; Charniak and Johnson, 2005). A second group of papers does parsing by a sequence of independent, discriminative decisions, either greedily or with use of a small beam (Ratnaparkhi, 1997; Henderson, 2004). This paper extends the third thread of work, where joint inference via dynamic programming algorithms is used to train models and to attempt to find the globally best parse. Work in this context has mainly been limited to use of artificially short sentences due to exorbitant training and inference times. One exception is the recent work of Petrov et al. (2007), who discriminatively train a grammar with latent variables and do not restrict themselves to short sentences. However their model, like the discriminative parser of Johnson (2001), makes no use of features, and effectively ignores the largest advantage of discriminative training. It has been shown on other NLP tasks that modeling improvements, such as the switch from generative training to discriminative training, usually provide much smaller performance gains than the gains possible from good feature engineering. For example, in (Lafferty et al., 2001), when switching from a generatively tr</context>
<context position="3676" citStr="Petrov et al. (2007)" startWordPosition="550" endWordPosition="553">rkov model (HMM) to a discriminatively trained, linear chain, conditional random field (CRF) for part-of-speech tagging, their error drops from 5.7% to 5.6%. When they add in only a small set of orthographic features, their CRF error rate drops considerably more to 4.3%, and their out-of-vocabulary error rate drops by more than half. This is further supported by Johnson (2001), who saw no parsing gains when switch959 Proceedings of ACL-08: HLT, pages 959–967, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics ing from generative to discriminative training, and by Petrov et al. (2007) who saw only small gains of around 0.7% for their final model when switching training methods. In this work, we provide just such a framework for training a feature-rich discriminative parser. Unlike previous work, we do not restrict ourselves to short sentences, but we do provide results both for training and testing on sentences of length &lt; 15 (WSJ15) and for training and testing on sentences of length &lt; 40, allowing previous WSJ15 results to be put in context with respect to most modern parsing literature. Our model is a conditional random field based model. For a rule application, we allo</context>
<context position="27010" citStr="Petrov et al., 2007" startWordPosition="4609" endWordPosition="4612">both the training time and accuracy of (Taskar et al., 2004). They define a simple linear model, use boosted decision trees to select feature conjunctions, and a line search to optimize the parameters. They use an agenda parser, and define their atomic features, from which the decision trees are constructed, over the entire state being considered. While they make extensive use of features, their setup is much more complex than ours and takes substantially longer to train – up to 5 days on WSJ15 – while achieving only small gains over (Taskar et al., 2004). The most recent similar research is (Petrov et al., 2007). They also do discriminative parsing of length 40 sentences, but with a substantially different setup. Following up on their previous work (Petrov et al., 2006) on grammar splitting, they do discriminative 965 S RB DT NP S VP NP VP NP S VBZ S VP NP PRP VBZ S NP PRP He VBZ adds VP NP VP S He adds NP VP PRP VBZ This is n’t NP VP RB VBZ NP DT VP RB VBZ NP DT He adds CD VBN This is n’t NP VP This is n’t CD VBN 1987 revisited CD VBN 1987 revisited 1987 revisited (a) generative output (b) feature-based discriminative output (c) gold parse Figure 3: Example output from our generative and feature-bas</context>
</contexts>
<marker>Petrov, Pauls, Klein, 2007</marker>
<rawString>Slav Petrov, Adam Pauls, and Dan Klein. 2007. Discriminative log-linear grammars with latent variables. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A linear observed time statistical parser based on maximum entropy models.</title>
<date>1997</date>
<booktitle>In EMNLP 2,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="2059" citStr="Ratnaparkhi, 1997" startWordPosition="294" endWordPosition="296">e arbitrary, potentially overlapping features. However, constituency parsing remains an area dominated by generative methods, due to the computational complexity of the problem. Previous work on discriminative parsing falls under one of three approaches. One approach does discriminative reranking of the n-best list of a generative parser, still usually depending highly on the generative parser score as a feature (Collins, 2000; Charniak and Johnson, 2005). A second group of papers does parsing by a sequence of independent, discriminative decisions, either greedily or with use of a small beam (Ratnaparkhi, 1997; Henderson, 2004). This paper extends the third thread of work, where joint inference via dynamic programming algorithms is used to train models and to attempt to find the globally best parse. Work in this context has mainly been limited to use of artificially short sentences due to exorbitant training and inference times. One exception is the recent work of Petrov et al. (2007), who discriminatively train a grammar with latent variables and do not restrict themselves to short sentences. However their model, like the discriminative parser of Johnson (2001), makes no use of features, and effec</context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Adwait Ratnaparkhi. 1997. A linear observed time statistical parser based on maximum entropy models. In EMNLP 2, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>An efficient probabilistic context-free parsing algorithm that computes prefix probabilities.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<pages>202</pages>
<contexts>
<context position="7961" citStr="Stolcke, 1995" startWordPosition="1279" endWordPosition="1280"> for the given sentence licensed by the grammar G. 1 P(t|s;θ) = Zs ∏ rEtφ(r|s;θ) (1) where Zs = ∑ tEτ(s)∏ rEt′ φ(r|s;θ) The above model is not well-defined over all CFGs. Unary rules of the form Ni —* Nj can form cycles, leading to infinite unary chains with infinite mass. However, it is standard in the parsing literature to transform grammars into a restricted class of CFGs so as to permit efficient parsing. Binarization of rules (Earley, 1970) is necessary to obtain cubic parsing time, and closure of unary chains is required for finding total probability mass (rather than just best parses) (Stolcke, 1995). To address this issue, we define our model over a restricted class of 960 Factory payrolls fell IN NN in September NN NP NNS S VBD PP VP Phrasal rules r1 = S0,5 → NP0,2 VP2,5 |Factory payrolls fell in September r3 = VP2,5 → VBD2,3 PP3,5 |Factory payrolls fell in September . . . Lexicon rules r5 = NN0,1 → Factory |Factory payrolls fell in September r6 = NNS1,2 → payrolls |Factory payrolls fell in September . . . (a) PCFG Structure (b) Rules r Figure 1: A parse tree and the corresponding rules over which potentials and features are defined. CFGs which limits unary chains to not have any repeat</context>
</contexts>
<marker>Stolcke, 1995</marker>
<rawString>Andreas Stolcke. 1995. An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. Computational Linguistics, 21:165– 202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Dan Klein</author>
<author>Michael Collins</author>
<author>Daphne Koller</author>
<author>Christopher D Manning</author>
</authors>
<title>Max-margin parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="10881" citStr="Taskar et al., 2004" startWordPosition="1790" endWordPosition="1793">ted from this value to get the normalized log probability of that rule in that position. Using the probabilities of each rule application, over each span/split, we can compute the expected feature values (the second term in Equation 4), by multiplying this probability by the value of the feature corresponding to the weight for which we are computing the partial derivative. The process is analogous to the computation of partial derivatives in linear chain CRFs. The complexity of the algorithm for a particular sentence is O(n3), where n is the length of the sentence. 2.3 Parallelization Unlike (Taskar et al., 2004), our algorithm has the advantage of being easily parallelized (see footnote 7 in their paper). Because the computation of both the log likelihood and the partial derivatives involves summing over each tree individually, the computation can be parallelized by having many clients which each do the computation for one tree, and one central server which aggregates the information to compute the relevant information for a set of trees. Because we use a stochastic optimization method, as discussed in Section 3, we compute the objective for only a small portion of the training data at a time, typica</context>
<context position="25509" citStr="Taskar et al. (2004)" startWordPosition="4348" endWordPosition="4351">smaller than even WSJ15, with only 1,088 training sentences, 294 testing sentences, and an average sentence length of around 11. They found no significant difference in performance between their generatively and discriminatively trained parsers. There are two probable reasons for this result. The training set is very small, and it is a known fact that generative models tend to work better for small datasets and discriminative models tend to work better for larger datasets (Ng and Jordan, 2002). Additionally, they made no use of features, one of the primary benefits of discriminative learning. Taskar et al. (2004) took a large margin approach to discriminative learning, but achieved only small gains. We suspect that this is in part due to the grammar that they chose – the grammar of (Klein and Manning, 2003), which was hand annotated with the intent of optimizing performance of a PCFG. This grammar is fairly sparse – for any particular state there are, on average, only a few rules with that state as a parent – so the learning algorithm may have suffered because there were few options to discriminate between. Starting with this grammar we found it difficult to achieve gains as well. Additionally, their </context>
<context position="26951" citStr="Taskar et al., 2004" startWordPosition="4599" endWordPosition="4602">an and Melamed, 2006; Turian et al., 2007), which improved both the training time and accuracy of (Taskar et al., 2004). They define a simple linear model, use boosted decision trees to select feature conjunctions, and a line search to optimize the parameters. They use an agenda parser, and define their atomic features, from which the decision trees are constructed, over the entire state being considered. While they make extensive use of features, their setup is much more complex than ours and takes substantially longer to train – up to 5 days on WSJ15 – while achieving only small gains over (Taskar et al., 2004). The most recent similar research is (Petrov et al., 2007). They also do discriminative parsing of length 40 sentences, but with a substantially different setup. Following up on their previous work (Petrov et al., 2006) on grammar splitting, they do discriminative 965 S RB DT NP S VP NP VP NP S VBZ S VP NP PRP VBZ S NP PRP He VBZ adds VP NP VP S He adds NP VP PRP VBZ This is n’t NP VP RB VBZ NP DT VP RB VBZ NP DT He adds CD VBN This is n’t NP VP This is n’t CD VBN 1987 revisited CD VBN 1987 revisited 1987 revisited (a) generative output (b) feature-based discriminative output (c) gold parse F</context>
</contexts>
<marker>Taskar, Klein, Collins, Koller, Manning, 2004</marker>
<rawString>Ben Taskar, Dan Klein, Michael Collins, Daphne Koller, and Christopher D. Manning. 2004. Max-margin parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>I Dan Melamed</author>
</authors>
<title>Advances in discriminative parsing.</title>
<date>2006</date>
<booktitle>In ACL 44,</booktitle>
<pages>873--880</pages>
<contexts>
<context position="26194" citStr="Turian and Melamed, 2006" startWordPosition="4468" endWordPosition="4471">t achieved only small gains. We suspect that this is in part due to the grammar that they chose – the grammar of (Klein and Manning, 2003), which was hand annotated with the intent of optimizing performance of a PCFG. This grammar is fairly sparse – for any particular state there are, on average, only a few rules with that state as a parent – so the learning algorithm may have suffered because there were few options to discriminate between. Starting with this grammar we found it difficult to achieve gains as well. Additionally, their long training time (several months for WSJ15, according to (Turian and Melamed, 2006)) made feature engineering difficult; they were unable to really explore the space of possible features. More recent is the work of (Turian and Melamed, 2006; Turian et al., 2007), which improved both the training time and accuracy of (Taskar et al., 2004). They define a simple linear model, use boosted decision trees to select feature conjunctions, and a line search to optimize the parameters. They use an agenda parser, and define their atomic features, from which the decision trees are constructed, over the entire state being considered. While they make extensive use of features, their setup</context>
</contexts>
<marker>Turian, Melamed, 2006</marker>
<rawString>Joseph Turian and I. Dan Melamed. 2006. Advances in discriminative parsing. In ACL 44, pages 873–880.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Ben Wellington</author>
<author>I Dan Melamed</author>
</authors>
<title>Scalable discriminative learning for natural language parsing and translation.</title>
<date>2007</date>
<booktitle>In Advances in Neural Information Processing Systems 19,</booktitle>
<pages>1409--1416</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="26373" citStr="Turian et al., 2007" startWordPosition="4498" endWordPosition="4501">optimizing performance of a PCFG. This grammar is fairly sparse – for any particular state there are, on average, only a few rules with that state as a parent – so the learning algorithm may have suffered because there were few options to discriminate between. Starting with this grammar we found it difficult to achieve gains as well. Additionally, their long training time (several months for WSJ15, according to (Turian and Melamed, 2006)) made feature engineering difficult; they were unable to really explore the space of possible features. More recent is the work of (Turian and Melamed, 2006; Turian et al., 2007), which improved both the training time and accuracy of (Taskar et al., 2004). They define a simple linear model, use boosted decision trees to select feature conjunctions, and a line search to optimize the parameters. They use an agenda parser, and define their atomic features, from which the decision trees are constructed, over the entire state being considered. While they make extensive use of features, their setup is much more complex than ours and takes substantially longer to train – up to 5 days on WSJ15 – while achieving only small gains over (Taskar et al., 2004). The most recent simi</context>
</contexts>
<marker>Turian, Wellington, Melamed, 2007</marker>
<rawString>Joseph Turian, Ben Wellington, and I. Dan Melamed. 2007. Scalable discriminative learning for natural language parsing and translation. In Advances in Neural Information Processing Systems 19, pages 1409–1416. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S V N Vishwanathan</author>
<author>Nichol N Schraudolph</author>
<author>Mark W Schmidt</author>
<author>Kevin P Murphy</author>
</authors>
<title>Accelerated training of conditional random fields with stochastic gradient methods.</title>
<date>2006</date>
<booktitle>In ICML 23,</booktitle>
<pages>969--976</pages>
<contexts>
<context position="13413" citStr="Vishwanathan et al., 2006" startWordPosition="2223" endWordPosition="2226">e the features for the possible rules and then save the entire data structure to disk. For all but the shortest of sentences, the disk I/O is easily worth the time compared to recomputation. The first time we see a sentence this method is still about one third faster than if we did not do the prefiltering, and on subsequent iterations the improvement is closer to tenfold. 3 Stochastic Optimization Methods Stochastic optimization methods have proven to be extremely efficient for the training of models involving computationally expensive objective functions like those encountered with our task (Vishwanathan et al., 2006) and, in fact, the on-line backpropagation learning used in the neural network parser of Henderson (2004) is a form of stochastic gradient descent. Standard deterministic optimization routines such as L-BFGS (Liu and Nocedal, 1989) make little progress in the initial iterations, often requiring several passes through the data in order to satisfy sufficient descent conditions placed on line searches. In our experiments SGD converged to a lower objective function value than L-BFGS, however it required far Passes Figure 2: WSJ15 objective value for L-BFGS and SGD versus passes through the data. S</context>
</contexts>
<marker>Vishwanathan, Schraudolph, Schmidt, Murphy, 2006</marker>
<rawString>S. V. N. Vishwanathan, Nichol N. Schraudolph, Mark W. Schmidt, and Kevin P. Murphy. 2006. Accelerated training of conditional random fields with stochastic gradient methods. In ICML 23, pages 969–976.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>