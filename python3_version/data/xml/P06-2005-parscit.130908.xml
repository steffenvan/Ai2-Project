<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.966662">
A Phrase-based Statistical Model for SMS Text Normalization
</title>
<author confidence="0.975135">
AiTi Aw, Min Zhang, Juan Xiao, Jian Su
</author>
<affiliation confidence="0.98516">
Institute of Infocomm Research
</affiliation>
<address confidence="0.968301">
21 Heng Mui Keng Terrace
Singapore 119613
</address>
<email confidence="0.975381">
{aaiti,mzhang,stuxj,sujian}@i2r.a-star.edu.sg
</email>
<sectionHeader confidence="0.994059" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999235931034483">
Short Messaging Service (SMS) texts be-
have quite differently from normal written
texts and have some very special phenom-
ena. To translate SMS texts, traditional
approaches model such irregularities di-
rectly in Machine Translation (MT). How-
ever, such approaches suffer from
customization problem as tremendous ef-
fort is required to adapt the language
model of the existing translation system to
handle SMS text style. We offer an alter-
native approach to resolve such irregulari-
ties by normalizing SMS texts before MT.
In this paper, we view the task of SMS
normalization as a translation problem
from the SMS language to the English
language 1 and we propose to adapt a
phrase-based statistical MT model for the
task. Evaluation by 5-fold cross validation
on a parallel SMS normalized corpus of
5000 sentences shows that our method can
achieve 0.80702 in BLEU score against
the baseline BLEU score 0.6958. Another
experiment of translating SMS texts from
English to Chinese on a separate SMS text
corpus shows that, using SMS normaliza-
tion as MT preprocessing can largely
boost SMS translation performance from
0.1926 to 0.3770 in BLEU score.
</bodyText>
<sectionHeader confidence="0.984574" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.999946727272727">
SMS translation is a mobile Machine Translation
(MT) application that translates a message from
one language to another. Though there exists
many commercial MT systems, direct use of
such systems fails to work well due to the special
phenomena in SMS texts, e.g. the unique relaxed
and creative writing style and the frequent use of
unconventional and not yet standardized short-
forms. Direct modeling of these special phenom-
ena in MT requires tremendous effort. Alterna-
tively, we can normalize SMS texts into
</bodyText>
<note confidence="0.61344">
1 This paper only discusses English SMS text normalization.
</note>
<bodyText confidence="0.999923023255814">
grammatical texts before MT. In this way, the
traditional MT is treated as a “black-box” with
little or minimal adaptation. One advantage of
this pre-translation normalization is that the di-
versity in different user groups and domains can
be modeled separately without accessing and
adapting the language model of the MT system
for each SMS application. Another advantage is
that the normalization module can be easily util-
ized by other applications, such as SMS to
voicemail and SMS-based information query.
In this paper, we present a phrase-based statis-
tical model for SMS text normalization. The
normalization is visualized as a translation prob-
lem where messages in the SMS language are to
be translated to normal English using a similar
phrase-based statistical MT method (Koehn et al.,
2003). We use IBM’s BLEU score (Papineni et
al., 2002) to measure the performance of SMS
text normalization. BLEU score computes the
similarity between two sentences using n-gram
statistics, which is widely-used in MT evalua-
tion. A set of parallel SMS messages, consisting
of 5000 raw (un-normalized) SMS messages and
their manually normalized references, is con-
structed for training and testing. Evaluation by 5-
fold cross validation on this corpus shows that
our method can achieve accuracy of 0.80702 in
BLEU score compared to the baseline system of
0.6985. We also study the impact of our SMS
text normalization on the task of SMS transla-
tion. The experiment of translating SMS texts
from English to Chinese on a corpus comprising
402 SMS texts shows that, SMS normalization as
a preprocessing step of MT can boost the transla-
tion performance from 0.1926 to 0.3770 in
BLEU score.
The rest of the paper is organized as follows.
Section 2 reviews the related work. Section 3
summarizes the characteristics of English SMS
texts. Section 4 discusses our method and Sec-
tion 5 reports our experiments. Section 6 con-
cludes the paper.
</bodyText>
<sectionHeader confidence="0.999828" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9879145">
There is little work reported on SMS normaliza-
tion and translation. Bangalore et al. (2002) used
</bodyText>
<page confidence="0.989718">
33
</page>
<note confidence="0.724422">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 33–40,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999759153846154">
a consensus translation technique to bootstrap
parallel data using off-the-shelf translation sys-
tems for training a hierarchical statistical transla-
tion model for general domain instant messaging
used in Internet chat rooms. Their method deals
with the special phenomena of the instant mes-
saging language (rather than the SMS language)
in each individual MT system. Clark (2003)
proposed to unify the process of tokenization,
segmentation and spelling correction for nor-
malization of general noisy text (rather than SMS
or instant messaging texts) based on a noisy
channel model at the character level. However,
results of the normalization are not reported. Aw
et al. (2005) gave a brief description on their in-
put pre-processing work for an English-to-
Chinese SMS translation system using a word-
group model. In addition, in most of the com-
mercial SMS translation applications 2 , SMS
lingo (i.e., SMS short form) dictionary is pro-
vided to replace SMS short-forms with normal
English words. Most of the systems do not han-
dle OOV (out-of-vocabulary) items and ambigu-
ous inputs. Following compares SMS text
normalization with other similar or related appli-
cations.
</bodyText>
<subsectionHeader confidence="0.7487215">
2.1 SMS Normalization versus General
Text Normalization
</subsectionHeader>
<bodyText confidence="0.999970714285714">
General text normalization deals with Non-
Standard Words (NSWs) and has been well-
studied in text-to-speech (Sproat et al., 2001)
while SMS normalization deals with Non-Words
(NSs) or lingoes and has seldom been studied
before. NSWs, such as digit sequences, acronyms,
mixed case words (WinNT, SunOS), abbrevia-
tions and so on, are grammatically correct in lin-
guistics. However lingoes, such as “b4” (before)
and “bf” (boyfriend), which are usually self-
created and only accepted by young SMS users,
are not yet formalized in linguistics. Therefore,
the special phenomena in SMS texts impose a
big challenge to SMS normalization.
</bodyText>
<subsectionHeader confidence="0.996067">
2.2 SMS Normalization versus Spelling
Correction Problem
</subsectionHeader>
<bodyText confidence="0.99998265625">
Intuitively, many would regard SMS normaliza-
tion as a spelling correction problem where the
lingoes are erroneous words or non-words to be
replaced by English words. Researches on spell-
ing correction centralize on typographic and
cognitive/orthographic errors (Kukich, 1992) and
use approaches (M.D. Kernighan, Church and
Gale, 1991) that mostly model the edit operations
using distance measures (Damerau 1964; Leven-
shtein 1966), specific word set confusions (Gold-
ing and Roth, 1999) and pronunciation modeling
(Brill and Moore, 2000; Toutanova and Moore,
2002). These models are mostly character-based
or string-based without considering the context.
In addition, the author might not be aware of the
errors in the word introduced during the edit op-
erations, as most errors are due to mistype of
characters near to each other on the keyboard or
homophones, such as “poor” or “pour”.
In SMS, errors are not isolated within word
and are usually not surrounded by clean context.
Words are altered deliberately to reflect sender’s
distinct creation and idiosyncrasies. A character
can be deleted on purpose, such as “wat” (what)
and “hv” (have). It also consists of short-forms
such as “b4” (before), “bf” (boyfriend). In addi-
tion, normalizing SMS text might require the
context to be spanned over more than one lexical
unit such as “lemme” (let me), “ur” (you are) etc.
Therefore, the models used in spelling correction
are inadequate for providing a complete solution
for SMS normalization.
</bodyText>
<subsectionHeader confidence="0.9934405">
2.3 SMS Normalization versus Text Para-
phrasing Problem
</subsectionHeader>
<bodyText confidence="0.99976508">
Others may regard SMS normalization as a para-
phrasing problem. Broadly speaking, paraphrases
capture core aspects of variability in language,
by representing equivalencies between different
expressions that correspond to the same meaning.
In most of the recent works (Barzilay and
McKeown, 2001; Shimohata, 2002), they are
acquired (semi-) automatically from large com-
parable or parallel corpora using lexical and
morpho-syntactic information.
Text paraphrasing works on clean texts in
which contextual and lexical-syntactic features
can be extracted and used to find “approximate
conceptual equivalence”. In SMS normalization,
we are dealing with non-words and “ungram-
matically” sentences with the purpose to normal-
ize or standardize these words and form better
sentences. The SMS normalization problem is
thus different from text paraphrasing. On the
other hand, it bears some similarities with MT as
we are trying to “convert” text from one lan-
guage to another. However, it is a simpler prob-
lem as most of the time; we can find the same
word in both the source and target text, making
alignment easier.
</bodyText>
<footnote confidence="0.969357">
2 http://www.etranslator.ro and http://www.transl8bit.com
</footnote>
<page confidence="0.999091">
34
</page>
<sectionHeader confidence="0.826198" genericHeader="method">
3 Characteristics of English SMS
</sectionHeader>
<bodyText confidence="0.999974571428571">
Our corpus consists of 55,000 messages collected
from two sources, a SMS chat room and corre-
spondences between university students. The
content is mostly related to football matches,
making friends and casual conversations on
“how, what and where about”. We summarize
the text behaviors into two categories as below.
</bodyText>
<subsectionHeader confidence="0.9981">
3.1 Orthographic Variation
</subsectionHeader>
<bodyText confidence="0.999954541666667">
The most significant orthographic variant in
SMS texts is in the use of non-standard, self-
created short-forms. Usually, sender takes advan-
tage of phonetic spellings, initial letters or num-
ber homophones to mimic spoken conversation
or shorten words or phrases (hw vs. homework or
how, b4 vs. before, cu vs. see you, 2u vs. to you,
oic vs. oh I see, etc.) in the attempt to minimize
key strokes. In addition, senders create a new
form of written representation to express their
oral utterances. Emotions, such as “:(“ symboliz-
ing sad, “:)” symbolizing smiling, “:()” symbol-
izing shocked, are representations of body
language. Verbal effects such as “hehe” for
laughter and emphatic discourse particles such as
“lor”, “lah”, “meh” for colloquial English are
prevalent in the text collection.
The loss of “alpha-case” information posts an-
other challenge in lexical disambiguation and
introduces difficulty in identifying sentence
boundaries, proper nouns, and acronyms. With
the flexible use of punctuation or not using punc-
tuation at all, translation of SMS messages with-
out prior processing is even more difficult.
</bodyText>
<subsectionHeader confidence="0.998289">
3.2 Grammar Variation
</subsectionHeader>
<bodyText confidence="0.9999904">
SMS messages are short, concise and convey
much information within the limited space quota
(160 letters for English), thus they tend to be im-
plicit and influenced by pragmatic and situation
reasons. These inadequacies of language expres-
sion such as deletion of articles and subject pro-
noun, as well as problems in number agreements
or tenses make SMS normalization more chal-
lenging. Table 1 illustrates some orthographic
and grammar variations of SMS texts.
</bodyText>
<subsectionHeader confidence="0.99978">
3.3 Corpus Statistics
</subsectionHeader>
<bodyText confidence="0.997051222222222">
We investigate the corpus to assess the feasibility
of replacing the lingoes with normal English
words and performing limited adjustment to the
text structure. Similarly to Aw et al. (2005), we
focus on the three major cases of transformation
as shown in the corpus: (1) replacement of OOV
words and non-standard SMS lingoes; (2) re-
moval of slang and (3) insertion of auxiliary or
copula verb and subject pronoun.
</bodyText>
<tableCaption confidence="0.998334">
Table 1. Examples of SMS Messages
</tableCaption>
<table confidence="0.99946525">
Transformation Percentage (%)
Insertion 8.09
Deletion 5.48
Substitution 86.43
</table>
<tableCaption confidence="0.974077">
Table 2. Distribution of Insertion, Deletion and
Substitution Transformation.
</tableCaption>
<bodyText confidence="0.852942176470588">
Substitution Deletion Insertion
u -&gt; you m are
2 → to lah am
n → and t is
r → are ah you
ur →your leh to
dun → don’t 1 do
man → manches- huh a
ter
no → number one in
intro → introduce lor yourself
wat → what ahh will
Table 3. Top 10 Most Common Substitu-
tion, Deletion and Insertion
Table 2 shows the statistics of these transfor-
mations based on 700 messages randomly se-
lected, where 621 (88.71%) messages required
</bodyText>
<figure confidence="0.993929702702703">
Phenomena
Messages
1. Dropping ‘?’ at
the end of
question
btw, wat is ur view
(By the way, what is your
view?)
Eh speak english mi malay
not tt good
(Eh, speak English! My Ma-
lay is not that good.)
goooooood Sunday morning
!!!!!!
(Good Sunday morning!)
2. Not using any
punctuation at
all
3. Using spell-
ing/punctuation
for emphasis
6. Introducing
local flavor
yar lor where u go juz now
(yes, where did you go just
now?)
4. Using phonetic
spelling
5. Dropping
vowel
dat iz enuf
(That is enough)
i hv cm to c my luv.
(I have come to see my love.)
7. Dropping verb I hv 2 go. Dinner w parents.
(I have to go. Have dinner
with parents.)
</figure>
<page confidence="0.992121">
35
</page>
<bodyText confidence="0.9999544">
normalization with a total of 2300 transforma-
tions. Substitution accounts for almost 86% of all
transformations. Deletion and substitution make
up the rest. Table 3 shows the top 10 most com-
mon transformations.
</bodyText>
<sectionHeader confidence="0.985741" genericHeader="method">
4 SMS Normalization
</sectionHeader>
<bodyText confidence="0.999949272727273">
We view the SMS language as a variant of Eng-
lish language with some derivations in vocabu-
lary and grammar. Therefore, we can treat SMS
normalization as a MT problem where the SMS
language is to be translated to normal English.
We thus propose to adapt the statistical machine
translation model (Brown et al., 1993; Zens and
Ney, 2004) for SMS text normalization. In this
section, we discuss the three components of our
method: modeling, training and decoding for
SMS text normalization.
</bodyText>
<subsectionHeader confidence="0.996828">
4.1 Basic Word-based Model
</subsectionHeader>
<bodyText confidence="0.8796399">
The SMS normalization model is based on the
source channel model (Shannon, 1948). Assum-
ing that an English sentence e, of length N is
“corrupted” by a noisy channel to produce a
SMS message s, of length M, the English sen-
tence e, could be recovered through a posteriori
distribution for a channel target text given the
source text P s , and a prior distribution for
(  |e)
the channel source text .
</bodyText>
<equation confidence="0.758124">
P(e)
eˆN = arg max { P e s
1 (  |)
eN N M }
1 1 1
argmax { P s e P e
eN (  |) ( )
1 N
M N i }
1 1 1
</equation>
<bodyText confidence="0.992978428571429">
Assuming that one SMS word is mapped ex-
actly to one English word in the channel model
under an
alignment A , we need to con-
P(sm|ea m ) (Brown et al. 1993). The channel
en as in the following equation
If we include the word “null” in the English
vocabulary, the above model can fully address
the deletion and substitution transformations, but
inadequate to address the insertion transforma-
tion. For example, the lingoes “duno”, “ysnite”
have to be normalized using an insertion trans-
formation to become “don’t know” and “yester-
day night”. Moreover, we also want the
normalization to have better lexical affinity and
linguistic equivalent, thus we extend the model
to allow many words to many words alignment,
allowing a sequence of SMS words to be normal-
ized to a sequence of contiguous English words.
We call this updated model a phrase-based nor-
malization model.
</bodyText>
<sectionHeader confidence="0.784979" genericHeader="method">
4.2 Phrase-based Model
</sectionHeader>
<bodyText confidence="0.9827595">
Given an English sentence e and SMS sentence
s , if we assume that e can be decomposed into
K phrases with a segmentation T , such that
each phrase e in can be corresponded with
</bodyText>
<equation confidence="0.98580918">
~k e
s , we have e1N = e1 ... e~k ... e~K
and M 1 1
s = s � ... s � k ... s~ K . The channel model can be
rewritten in equation (3).
P s e
(  |)
M N = ∑ P s T e N
( ,  |)
M
1 1 1 1
T
�
phrase alignment as done in the previous
A
word-based model.
(  |)
� �
K K = ∑ P s A e
( ,  |)
� � �
K K
P s e
1 1 1 1
�
A
P(s  |e)
one phrase s in
~k
∑ (  |) (  |, )
N M N
i
P T e P s T e
1 1 1
N i � �
K K
(  |) (  |)
P T e P s e
1 1 1
T
T
∑
(3)
(1) N
≈ max (  |) (  |)
K K
{ P T e P s e
i � � }
1 1 1
T
</equation>
<bodyText confidence="0.45291275">
m is the position of a word in
san
d its
am
</bodyText>
<equation confidence="0.849958283018868">
K
{ P A e P s A e
K
(  |) (  |, )
K
� � i � � � }
1 1 1
∑
�
A
P s e (  |) M N = ∑ P s A e N ( ,  |) M 1 1 1
A
M N
∑ P A e P s A e
(  |) (  |, )
N i (2)
1 1 1
{ P(m
am)
P(s
ea
M 
≈ 
∑ ∏

|
i
m|
m )} 
A m=1
K
a
=  { P k a P s s e
(  |) (  |, )
∑ ∏
 
� i � � �
k −1 � k }
k k 1 a � 
1
A k
�=1 
K 
≈ 
∑∏
 { P k a P s e
(  |) (  |)
� i � � }
k k a � 
k
A k
�  = 1 
(4)
</equation>
<bodyText confidence="0.652276">
ider only two types of probabilities: the align-
ment probabilities denoted by
</bodyText>
<figure confidence="0.808477777777778">
s
P m and the
(  |am )
lexicon mapping probabilities denoted by
model can be writt
where
alignment in .
e
A
</figure>
<bodyText confidence="0.9409985">
This is the basic function of the channel model
for the phrase-based SMS normalization model,
where we used the maximum approximation for
the sum over all segmentations. Then we further
</bodyText>
<equation confidence="0.93420275">
decompose the probability
( 1  |1 )
P s � e � using a
K K
</equation>
<bodyText confidence="0.780225666666667">
We are now able to model the three tran
sfor-
mations through the normalization pair ( , )
</bodyText>
<equation confidence="0.860268">
s � k � e~ a k ,
</equation>
<page confidence="0.951228">
36
</page>
<bodyText confidence="0.959427">
with the mapping probability P s � k � e~ a k
</bodyText>
<equation confidence="0.587034">
(  |) . The fol-
</equation>
<bodyText confidence="0.955737625">
lowings show the scenarios in which the three
transformations occur.
The statistics in our training corpus shows that
by selecting appropriate phrase segmentation, the
position re-ordering at the phrase level occurs
rarely. It is not surprising since most of the Eng-
lish words or phrases in normal English text are
replaced with lingoes in SMS messages without
position change to make SMS text short and con-
cise and to retain the meaning. Thus we need to
consider only monotone alignment at phrase
level, i.e., k , as in equation (4). In addition,
= � ak
the word-level reordering within phrase is
learned during training. Now we can further de-
rive equation (4) as follows:
</bodyText>
<equation confidence="0.99954025">
K

P s e
(  |)
� �
K K ≈ 
∑ ∏
1 1
A k
�  = 1
� �
P (  |)
s e
k
k=
1
</equation>
<bodyText confidence="0.853287">
The mapping probability P(s�k  |e�k) is esti-
mated via relative frequencies as follows:
</bodyText>
<equation confidence="0.993265615384615">
N s e
( ,
� � )
P s e
(  |)
� � k k (6)
k k N s
( ,
� k � ek )
&apos;
= ∑
� &apos;
sk
</equation>
<bodyText confidence="0.9820666">
Here, N(s�k, e�k) denotes the frequency of the
normalization pair ( s � k , e � k ) .
Using a bigram language model and assuming
Bayes decision rule, we finally obtain the follow-
ing search criterion for equation (1).
</bodyText>
<equation confidence="0.999652071428571">
eN = arg max{ P(eN)iP(sM 1 1
N
N
arg max ( |
∏ P en
e N 
1 n=1
N
i max ( |
P T e ) ( |
i P s � e �
 ∏ k k )
1
T k = 1
</equation>
<bodyText confidence="0.817969">
For the above equation, we assume the seg-
</bodyText>
<equation confidence="0.565994333333333">
mentation probability P(T  |e to be constant.
1 )
N
</equation>
<bodyText confidence="0.9859208">
Finally, the SMS normalization model consists of
two sub-models: a word-based language model
(LM), characterized by P(en  |en−1 ) and a phrase-
based lexical mapping model (channel model),
characterized by P ( s k  |e
</bodyText>
<equation confidence="0.785292">
� � k ) .
</equation>
<subsectionHeader confidence="0.994338">
4.3 Training Issues
</subsectionHeader>
<bodyText confidence="0.962327769230769">
For the phrase-based model training, the sen-
tence-aligned SMS corpus needs to be aligned
first at the phrase level. The maximum likelihood
approach, through EM algorithm and Viterbi
search (Dempster et al., 1977) is employed to
infer such an alignment. Here, we make a rea-
sonable assumption on the alignment unit that a
single SMS word can be mapped to a sequence
of contiguous English words, but not vice verse.
The EM algorithm for phrase alignment is illus-
trated in Figure 1 and is formulated by equation
(8).
The Expectation-Maximization Algorithm
</bodyText>
<listItem confidence="0.996002538461538">
(1) Bootstrap initial alignment using ortho-
graphic similarities
(2) Expectation: Update the joint probabili-
ties P(sk ,
� �ek )
(3) Maximization: Apply the joint probabili-
ties P(s k ,
� ek ) to get new alignment using
Viterbi search algorithm
(4) Repeat (2) to (3) until alignment con-
verges
(5) Derive normalization pairs from final
alignment
</listItem>
<figureCaption confidence="0.996472">
Figure 1. Phrase Alignment Using EM Algorithm
</figureCaption>
<equation confidence="0.9989285">
K
γ &lt; � � &gt; =arg max( ,
∏ � �
P s ek s e
 |, )
M N
ˆ 1 (8)
s e
, k 1 k k ˆ
γ &lt; � � &gt;
s k e k
, k=1
</equation>
<bodyText confidence="0.908469">
The alignment process given in equation (8) is
different from that of normalization given in
equation (7) in that, here we have an aligned in-
put sentence pair, s and . The alignment
</bodyText>
<equation confidence="0.9970695">
M eN
1 1
</equation>
<bodyText confidence="0.985290272727273">
process is just to find the alignment segmentation
between the two sen-
tences that maximizes the joint probability.
Therefore, in step (2) of the EM algorithm given
at Figure 1, only the joint probabilities
P(s�k, e�k ) are involved and updated.
Since EM may fall into local optimization, in
order to speed up convergence and find a nearly
global optimization, a string matching technique
is exploited at the initialization step to identify
the most probable normalization pairs. The or-
</bodyText>
<figure confidence="0.713033340425532">
Deletion
Insertion
ea~k
� = null
sk = ea�k
� �
sk &lt; ea�k
� �
Substitution
arg max ( |
∏ P e e ) (
i � �
 |)
 ∏ P s e
n n − 1 k k
e T
N ,
1 n = 1 k=1

N K
K
≈ ∏
{ P (k  |a k ) P(s k
� i �  |)
e � }
a � k

 
(5)
≈

−
en
 |) }
eN
1
1 )
K
 
 
(7)
ˆ � �
s � k e � k&gt; =&lt; sk e
,
,
γ&lt;
k &gt;k=1, K
</figure>
<page confidence="0.99698">
37
</page>
<bodyText confidence="0.99980625">
thographic similarities captured by edit distance
and a SMS lingo dictionary3 which contains the
commonly used short-forms are first used to es-
tablish phrase mapping boundary candidates.
Heuristics are then exploited to match tokens
within the pairs of boundary candidates by trying
to combine consecutive tokens within the bound-
ary candidates if the numbers of tokens do not
agree.
Finally, a filtering process is carried out to
manually remove the low-frequency noisy
alignment pairs. Table 4 shows some of the ex-
tracted normalization pairs. As can be seen from
the table, our algorithm discovers ambiguous
mappings automatically that are otherwise miss-
ing from most of the lingo dictionary.
</bodyText>
<table confidence="0.998777">
(s�, e�) log P(s�  |e�)
(2, 2) 0
(2, to) -0.579466
(2, too) -0.897016
(2, null) -2.97058
(4, 4) 0
(4, for) -0.431364
(4, null) -3.27161
(w, who are) -0.477121
(w, with) -0.764065
(w, who) -1.83885
(dat, that) -0.726999
(dat, date) -0.845098
(tmr, tomorrow) -0.341514
</table>
<tableCaption confidence="0.999546">
Table 4. Examples of normalization pairs
</tableCaption>
<bodyText confidence="0.987434111111111">
Given the phrase-aligned SMS corpus, the
lexical mapping model, characterized by
P(s�k  |ek) , is easily to be trained using equation
(6). Our n-gram LM P(en  |en−1) is trained on
English Gigaword provided by LDC using
SRILM language modeling toolkit (Stolcke,
2002). Backoff smoothing (Jelinek, 1991) is used
to adjust and assign a non-zero probability to the
unseen words to address data sparseness.
</bodyText>
<subsectionHeader confidence="0.995681">
4.4 Monotone Search
</subsectionHeader>
<bodyText confidence="0.998674">
Given an input , the search, characterized in
</bodyText>
<subsubsectionHeader confidence="0.63877">
s
</subsubsectionHeader>
<footnote confidence="0.91446875">
equation (7), is to find a sentence e that maxi-
3 The entries are collected from various websites such as
http://www.handphones.info/sms-dictionary/sms-lingo.php,
and http://www.funsms.net/sms_dictionary.htm, etc.
</footnote>
<equation confidence="0.227518">
mizes P(s  |e)-P(e) using the normalization
</equation>
<bodyText confidence="0.99984175">
model. In this paper, the maximization problem
in equation (7) is solved using a monotone search,
implemented as a Viterbi search through dy-
namic programming.
</bodyText>
<sectionHeader confidence="0.996846" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9988754375">
The aim of our experiment is to verify the effec-
tiveness of the proposed statistical model for
SMS normalization and the impact of SMS nor-
malization on MT.
A set of 5000 parallel SMS messages, which
consists of raw (un-normalized) SMS messages
and reference messages manually prepared by
two project members with inter-normalization
agreement checked, was prepared for training
and testing. For evaluation, we use IBM’s BLEU
score (Papineni et al., 2002) to measure the per-
formance of the SMS normalization. BLEU score
measures the similarity between two sentences
using n-gram statistics with a penalty for too
short sentences, which is already widely-used in
MT evaluation.
</bodyText>
<table confidence="0.99628625">
Setup BLEU score (3-
gram)
Raw SMS without 0.5784
Normalization
Dictionary Look-up 0.6958
plus Frequency
Bi-gram Language 0.7086
Model Only
</table>
<tableCaption confidence="0.755476333333333">
Table 5. Performance of different set-
ups of the baseline experiments on the
5000 parallel SMS messages
</tableCaption>
<subsectionHeader confidence="0.980015666666667">
5.1 Baseline Experiments: Simple SMS
Lingo Dictionary Look-up and Using
Language Model Only
</subsectionHeader>
<bodyText confidence="0.999898333333333">
The baseline experiment is to moderate the texts
using a lingo dictionary comprises 142 normali-
zation pairs, which is also used in bootstrapping
the phrase alignment learning process.
Table 5 compares the performance of the dif-
ferent setups of the baseline experiments. We
first measure the complexity of the SMS nor-
malization task by directly computing the simi-
larity between the raw SMS text and the
normalized English text. The 1st row of Table 5
reports the similarity as 0.5784 in BLEU score,
which implies that there are quite a number of
English word 3-gram that are common in the raw
and normalized messages. The 2nd experiment is
carried out using only simple dictionary look-up.
</bodyText>
<page confidence="0.996992">
38
</page>
<bodyText confidence="0.9998842">
Lexical ambiguity is addressed by selecting the
highest-frequency normalization candidate, i.e.,
only unigram LM is used. The performance of
the 2nd experiment is 0.6958 in BLEU score. It
suggests that the lingo dictionary plus the uni-
gram LM is very useful for SMS normalization.
Finally we carry out the 3rd experiment using
dictionary look-up plus bi-gram LM. Only a
slight improvement of 0.0128 (0.7086-0.6958) is
obtained. This is largely because the English
words in the lingo dictionary are mostly high-
frequency and commonly-used. Thus bi-gram
does not show much more discriminative ability
than unigram without the help of the phrase-
based lexical mapping model.
</bodyText>
<subsectionHeader confidence="0.998603">
5.2 Using Phrase-based Model
</subsectionHeader>
<bodyText confidence="0.999841176470588">
We then conducted the experiment using the pro-
posed method (Bi-gram LM plus a phrase-based
lexical mapping model) through a five-fold cross
validation on the 5000 parallel SMS messages.
Table 6 shows the results. An average score of
0.8070 is obtained. Compared with the baseline
performance in Table 5, the improvement is very
significant. It suggests that the phrase-based
lexical mapping model is very useful and our
method is effective for SMS text normalization.
Figure 2 is the learning curve. It shows that our
algorithm converges when training data is
increased to 3000 SMS parallel messages. This
suggests that our collected corpus is representa-
tive and enough for training our model. Table 7
illustrates some examples of the normalization
results.
</bodyText>
<table confidence="0.915792">
5-fold cross validation BLEU score (3-gram)
Setup 1 0.8023
Setup 2 0.8236
Setup 3 0.8071
Setup 4 0.8113
Setup 5 0.7908
Ave. 0.8070
</table>
<tableCaption confidence="0.972696">
Table 6. Normalization results for 5-
fold cross validation test
</tableCaption>
<figure confidence="0.99646575">
0.82
0.8
0.78
0.76
0.74
0.72
0.7
1000 2000 3000 4000 5000
</figure>
<figureCaption confidence="0.999987">
Figure 2. Learning Curve
</figureCaption>
<bodyText confidence="0.950718235294118">
Experimental result analysis reveals that the
strength of our model is in its ability to disam-
biguate mapping as in “2” to “two” or “to” and
“w” to “with” or “who”. Error analysis shows
that the challenge of the model lies in the proper
insertion of subject pronoun and auxiliary or
copula verb, which serves to give further seman-
tic information about the main verb, however this
requires significant context understanding. For
example, a message such as “u smart” gives little
clues on whether it should be normalized to “Are
you smart?” or “You are smart.” unless the full
conversation is studied.
Takako w r u?
Takako who are you?
Im in ns, lik soccer, clubbin hangin w frenz!
Wat bout u mee?
I&apos;m in ns, like soccer, clubbing hanging with
friends! What about you?
fancy getting excited w others&apos; boredom
Fancy getting excited with others&apos; boredom
If u ask me b4 he ask me then i&apos;ll go out w u all
lor. N u still can act so real.
If you ask me before he asked me then I&apos;ll go
out with you all. And you still can act so real.
Doing nothing, then u not having dinner w us?
Doing nothing, then you do not having dinner
with us?
Aiyar sorry lor forgot 2 tell u... Mtg at 2 pm.
Sorry forgot to tell you... Meeting at two pm.
tat&apos;s y I said it&apos;s bad dat all e gals know u...
Wat u doing now?
That&apos;s why I said it&apos;s bad that all the girls know
you... What you doing now?
</bodyText>
<tableCaption confidence="0.820973">
Table 7. Examples of Normalization Results
</tableCaption>
<subsectionHeader confidence="0.986535">
5.3 Effect on English-Chinese MT
</subsectionHeader>
<bodyText confidence="0.999989692307692">
An experiment was also conducted to study the
effect of normalization on MT using 402 mes-
sages randomly selected from the text corpus.
We compare three types of SMS message: raw
SMS messages, normalized messages using sim-
ple dictionary look-up and normalized messages
using our method. The messages are passed to
two different English-to-Chinese translation sys-
tems provided by Systran4 and Institute for Info-
comm Research5(I2R) separately to produce three
sets of translation output. The translation quality
is measured using 3-gram cumulative BLEU
score against two reference messages. 3-gram is
</bodyText>
<footnote confidence="0.971137">
4 http://www.systranet.com/systran/net
5 http://nlp.i2r.a-star.edu.sg/techtransfer.html
</footnote>
<sectionHeader confidence="0.297571" genericHeader="method">
BLEU
</sectionHeader>
<page confidence="0.99746">
39
</page>
<bodyText confidence="0.999933777777778">
used as most of the messages are short with aver-
age length of seven words. Table 8 shows the
details of the BLEU scores. We obtain an aver-
age of 0.3770 BLEU score for normalized mes-
sages against 0.1926 for raw messages. The
significant performance improvement suggests
that preprocessing of normalizing SMS text us-
ing our method before MT is an effective way to
adapt a general MT system to SMS domain.
</bodyText>
<table confidence="0.9960405">
I2R Systran Ave.
Raw Message 0.2633 0.1219 0.1926
Dict Lookup 0.3485 0.1690 0.2588
Normalization 0.4423 0.3116 0.3770
</table>
<tableCaption confidence="0.980525">
Table 8. SMS Translation BLEU score with or
without SMS normalization
</tableCaption>
<sectionHeader confidence="0.998724" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999979826086956">
In this paper, we study the differences among
SMS normalization, general text normalization,
spelling check and text paraphrasing, and inves-
tigate the different phenomena of SMS messages.
We propose a phrase-based statistical method to
normalize SMS messages. The method produces
messages that collate well with manually normal-
ized messages, achieving 0.8070 BLEU score
against 0.6958 baseline score. It also signifi-
cantly improves SMS translation accuracy from
0.1926 to 0.3770 in BLEU score without adjust-
ing the MT model.
This experiment results provide us with a good
indication on the feasibility of using this method
in performing the normalization task. We plan to
extend the model to incorporate mechanism to
handle missing punctuation (which potentially
affect MT output and are not being taken care at
the moment), and making use of pronunciation
information to handle OOV caused by the use of
phonetic spelling. A bigger data set will also be
used to test the robustness of the system leading
to a more accurate alignment and normalization.
</bodyText>
<sectionHeader confidence="0.998998" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999675693548387">
A.T. Aw, M. Zhang, Z.Z. Fan, P.K. Yeo and J. Su.
2005. Input Normalization for an English-to-
Chinese SMS Translation System. MT Summit-
2005
S. Bangalore, V. Murdock and G. Riccardi. 2002.
Bootstrapping Bilingual Data using Consensus
Translation for a Multilingual Instant Messaging
System. COLING-2002
R. Barzilay and K. R. McKeown. 2001. Extracting
paraphrases from a parallel corpus. ACL-2001
E. Brill and R. C. Moore. 2000. An Improved Error
Model for Noisy Channel Spelling Correction.
ACL-2000
P. F. Brown, S. D. Pietra, V. D. Pietra and R. Mercer.
1993. The Mathematics of Statistical Machine
Translation: Parameter Estimation. Computational
Linguistics: 19(2)
A. Clark. 2003. Pre-processing very noisy text. In
Proceedings of Workshop on Shallow Processing
of Large Corpora, Lancaster, 2003
F. J. Damerau. 1964. A technique for computer detec-
tion and correction of spelling errors. Communica-
tions ACM 7, 171-176
A.P. Dempster, N.M. Laird and D.B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm, Journal of the Royal Statistical So-
ciety, Series B, Vol. 39, 1-38
A. Golding and D. Roth. 1999. A Winnow-Based Ap-
proach to Spelling Correction. Machine Learning
34: 107-130
F. Jelinek. 1991. Self-organized language modeling
for speech recognition. In A. Waibel and K.F. Lee,
editors, Readings in Speech Recognition, pages
450-506. Morgan Kaufmann, 1991
M. D. Kernighan, K Church and W. Gale. 1990. A
spelling correction program based on a noisy
channel model. COLING-1990
K. Kukich. 1992. Techniques for automatically cor-
recting words in text. ACM Computing Surveys,
24(4):377-439
K. A. Papineni, S. Roukos, T. Ward and W. J. Zhu.
2002. BLEU : a Method for Automatic Evaluation
of Machine Translation. ACL-2002
P. Koehn, F.J. Och and D. Marcu. 2003. Statistical
Phrase-Based Translation. HLT-NAACL-2003
C. Shannon. 1948. A mathematical theory of commu-
nication. Bell System Technical Journal 27(3):
379-423
M. Shimohata and E. Sumita 2002. Automatic Para-
phrasing Based on Parallel Corpus for Normaliza-
tion. LREC-2002
R. Sproat, A. Black, S. Chen, S. Kumar, M. Ostendorf
and C. Richards. 2001. Normalization of Non-
Standard Words. Computer Speech and Language,
15(3):287-333
A. Stolcke. 2002. SRILM – An extensible language
modeling toolkit. ICSLP-2002
K. Toutanova and R. C. Moore. 2002. Pronunciation
Modeling for Improved Spelling Correction. ACL-
2002
R. Zens and H. Ney. 2004. Improvements in Phrase-
Based Statistical MT. HLT-NAALL-2004
</reference>
<page confidence="0.998624">
40
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999928">A Phrase-based Statistical Model for SMS Text Normalization</title>
<author confidence="0.999204">AiTi Aw</author>
<author confidence="0.999204">Min Zhang</author>
<author confidence="0.999204">Juan Xiao</author>
<author confidence="0.999204">Jian Su</author>
<affiliation confidence="0.999973">Institute of Infocomm Research</affiliation>
<address confidence="0.9074525">21 Heng Mui Keng Terrace Singapore 119613</address>
<email confidence="0.890543">aaiti@i2r.a-star.edu.sg</email>
<email confidence="0.890543">mzhang@i2r.a-star.edu.sg</email>
<email confidence="0.890543">stuxj@i2r.a-star.edu.sg</email>
<email confidence="0.890543">sujian@i2r.a-star.edu.sg</email>
<abstract confidence="0.994278035714285">Short Messaging Service (SMS) texts behave quite differently from normal written texts and have some very special phenomena. To translate SMS texts, traditional approaches model such irregularities directly in Machine Translation (MT). However, such approaches suffer from customization problem as tremendous effort is required to adapt the language model of the existing translation system to handle SMS text style. We offer an alternative approach to resolve such irregularities by normalizing SMS texts before MT. In this paper, we view the task of SMS normalization as a translation problem from the SMS language to the English 1and we propose to adapt a phrase-based statistical MT model for the task. Evaluation by 5-fold cross validation on a parallel SMS normalized corpus of 5000 sentences shows that our method can achieve 0.80702 in BLEU score against the baseline BLEU score 0.6958. Another experiment of translating SMS texts from English to Chinese on a separate SMS text corpus shows that, using SMS normalization as MT preprocessing can largely boost SMS translation performance from 0.1926 to 0.3770 in BLEU score. 1 Motivation SMS translation is a mobile Machine Translation (MT) application that translates a message from one language to another. Though there exists many commercial MT systems, direct use of such systems fails to work well due to the special phenomena in SMS texts, e.g. the unique relaxed and creative writing style and the frequent use of unconventional and not yet standardized shortforms. Direct modeling of these special phenomena in MT requires tremendous effort. Alternatively, we can normalize SMS texts into paper only discusses English SMS text normalization. grammatical texts before MT. In this way, the traditional MT is treated as a “black-box” with little or minimal adaptation. One advantage of this pre-translation normalization is that the diversity in different user groups and domains can be modeled separately without accessing and adapting the language model of the MT system for each SMS application. Another advantage is that the normalization module can be easily utilized by other applications, such as SMS to voicemail and SMS-based information query. In this paper, we present a phrase-based statistical model for SMS text normalization. The normalization is visualized as a translation problem where messages in the SMS language are to be translated to normal English using a similar phrase-based statistical MT method (Koehn et al., 2003). We use IBM’s BLEU score (Papineni et al., 2002) to measure the performance of SMS text normalization. BLEU score computes the similarity between two sentences using n-gram statistics, which is widely-used in MT evaluation. A set of parallel SMS messages, consisting of 5000 raw (un-normalized) SMS messages and their manually normalized references, is constructed for training and testing. Evaluation by 5fold cross validation on this corpus shows that our method can achieve accuracy of 0.80702 in BLEU score compared to the baseline system of 0.6985. We also study the impact of our SMS text normalization on the task of SMS translation. The experiment of translating SMS texts from English to Chinese on a corpus comprising 402 SMS texts shows that, SMS normalization as a preprocessing step of MT can boost the translation performance from 0.1926 to 0.3770 in BLEU score. The rest of the paper is organized as follows. Section 2 reviews the related work. Section 3 summarizes the characteristics of English SMS texts. Section 4 discusses our method and Section 5 reports our experiments. Section 6 concludes the paper. 2 Related Work There is little work reported on SMS normalization and translation. Bangalore et al. (2002) used 33 of the COLING/ACL 2006 Main Conference Poster pages 33–40, July 2006. Association for Computational Linguistics a consensus translation technique to bootstrap parallel data using off-the-shelf translation systems for training a hierarchical statistical translation model for general domain instant messaging used in Internet chat rooms. Their method deals with the special phenomena of the instant messaging language (rather than the SMS language) in each individual MT system. Clark (2003) proposed to unify the process of tokenization, segmentation and spelling correction for normalization of general noisy text (rather than SMS or instant messaging texts) based on a noisy channel model at the character level. However, results of the normalization are not reported. Aw et al. (2005) gave a brief description on their input pre-processing work for an English-to- Chinese SMS translation system using a wordgroup model. In addition, in most of the com- SMS translation applications 2, SMS lingo (i.e., SMS short form) dictionary is provided to replace SMS short-forms with normal English words. Most of the systems do not handle OOV (out-of-vocabulary) items and ambiguous inputs. Following compares SMS text normalization with other similar or related applications. 2.1 SMS Normalization versus General Text Normalization General text normalization deals with Non- Standard Words (NSWs) and has been wellstudied in text-to-speech (Sproat et al., 2001) while SMS normalization deals with Non-Words (NSs) or lingoes and has seldom been studied before. NSWs, such as digit sequences, acronyms, mixed case words (WinNT, SunOS), abbreviations and so on, are grammatically correct in lin- However lingoes, such as which are usually selfcreated and only accepted by young SMS users, are not yet formalized in linguistics. Therefore, the special phenomena in SMS texts impose a big challenge to SMS normalization. 2.2 SMS Normalization versus Spelling Correction Problem Intuitively, many would regard SMS normalization as a spelling correction problem where the lingoes are erroneous words or non-words to be replaced by English words. Researches on spelling correction centralize on typographic and cognitive/orthographic errors (Kukich, 1992) and use approaches (M.D. Kernighan, Church and Gale, 1991) that mostly model the edit operations using distance measures (Damerau 1964; Levenshtein 1966), specific word set confusions (Golding and Roth, 1999) and pronunciation modeling (Brill and Moore, 2000; Toutanova and Moore, 2002). These models are mostly character-based or string-based without considering the context. In addition, the author might not be aware of the errors in the word introduced during the edit operations, as most errors are due to mistype of characters near to each other on the keyboard or homophones, such as “poor” or “pour”. In SMS, errors are not isolated within word and are usually not surrounded by clean context. Words are altered deliberately to reflect sender’s distinct creation and idiosyncrasies. A character be deleted on purpose, such as It also consists of short-forms as In addition, normalizing SMS text might require the context to be spanned over more than one lexical such as etc. Therefore, the models used in spelling correction are inadequate for providing a complete solution for SMS normalization. 2.3 SMS Normalization versus Text Paraphrasing Problem Others may regard SMS normalization as a paraphrasing problem. Broadly speaking, paraphrases capture core aspects of variability in language, by representing equivalencies between different expressions that correspond to the same meaning. In most of the recent works (Barzilay and McKeown, 2001; Shimohata, 2002), they are acquired (semi-) automatically from large comparable or parallel corpora using lexical and morpho-syntactic information. Text paraphrasing works on clean texts in which contextual and lexical-syntactic features can be extracted and used to find “approximate conceptual equivalence”. In SMS normalization, we are dealing with non-words and “ungrammatically” sentences with the purpose to normalize or standardize these words and form better sentences. The SMS normalization problem is thus different from text paraphrasing. On the other hand, it bears some similarities with MT as we are trying to “convert” text from one language to another. However, it is a simpler problem as most of the time; we can find the same word in both the source and target text, making alignment easier. and http://www.transl8bit.com 34 3 Characteristics of English SMS Our corpus consists of 55,000 messages collected from two sources, a SMS chat room and correspondences between university students. The content is mostly related to football matches, making friends and casual conversations on “how, what and where about”. We summarize the text behaviors into two categories as below. 3.1 Orthographic Variation The most significant orthographic variant in SMS texts is in the use of non-standard, selfcreated short-forms. Usually, sender takes advantage of phonetic spellings, initial letters or number homophones to mimic spoken conversation shorten words or phrases I etc.) in the attempt to minimize key strokes. In addition, senders create a new form of written representation to express their oral utterances. Emotions, such as “:(“ symbolizing sad, “:)” symbolizing smiling, “:()” symbolizing shocked, are representations of body Verbal effects such as for laughter and emphatic discourse particles such as for colloquial English are prevalent in the text collection. The loss of “alpha-case” information posts another challenge in lexical disambiguation and introduces difficulty in identifying sentence boundaries, proper nouns, and acronyms. With the flexible use of punctuation or not using punctuation at all, translation of SMS messages without prior processing is even more difficult. 3.2 Grammar Variation SMS messages are short, concise and convey much information within the limited space quota (160 letters for English), thus they tend to be implicit and influenced by pragmatic and situation reasons. These inadequacies of language expression such as deletion of articles and subject pronoun, as well as problems in number agreements or tenses make SMS normalization more challenging. Table 1 illustrates some orthographic and grammar variations of SMS texts. 3.3 Corpus Statistics We investigate the corpus to assess the feasibility of replacing the lingoes with normal English words and performing limited adjustment to the text structure. Similarly to Aw et al. (2005), we focus on the three major cases of transformation as shown in the corpus: (1) replacement of OOV words and non-standard SMS lingoes; (2) removal of slang and (3) insertion of auxiliary or copula verb and subject pronoun. Table 1. Examples of SMS Messages Transformation Percentage (%) Insertion 8.09 Deletion 5.48 Substitution 86.43 Table 2. Distribution of Insertion, Deletion and Substitution Transformation. Substitution Deletion Insertion u -&gt; you m are lah am t is ah you leh to 1 do ter huh a one in lor yourself ahh will Table 3. Top 10 Most Common Substitution, Deletion and Insertion Table 2 shows the statistics of these transformations based on 700 messages randomly selected, where 621 (88.71%) messages required Phenomena Messages 1. Dropping ‘?’ at the end of question btw, wat is ur view the way, what is your Eh speak english mi malay not tt good speak English! My Mais not that goooooood Sunday morning !!!!!! Sunday 2. Not using any punctuation at all 3. Using spelling/punctuation for emphasis 6. Introducing local flavor yar lor where u go juz now where did you go just 4. Using phonetic spelling 5. Dropping vowel dat iz enuf is i hv cm to c my luv. have come to see my 7. Dropping verb I hv 2 go. Dinner w parents. have to go. Have dinner 35 normalization with a total of 2300 transformations. Substitution accounts for almost 86% of all transformations. Deletion and substitution make up the rest. Table 3 shows the top 10 most common transformations. 4 SMS Normalization We view the SMS language as a variant of English language with some derivations in vocabulary and grammar. Therefore, we can treat SMS normalization as a MT problem where the SMS language is to be translated to normal English. We thus propose to adapt the statistical machine translation model (Brown et al., 1993; Zens and Ney, 2004) for SMS text normalization. In this section, we discuss the three components of our method: modeling, training and decoding for SMS text normalization. 4.1 Basic Word-based Model The SMS normalization model is based on the source channel model (Shannon, 1948). Assumthat an English sentence of length N is “corrupted” by a noisy channel to produce a message of length M, the English sencould be recovered through a posteriori distribution for a channel target text given the text s and a prior distribution for | the channel source text . = max e s</abstract>
<note confidence="0.803365375">1 (  |) 1 M argmax 1 1 1 s e P e (  |) ( ) N N 1 1 1</note>
<abstract confidence="0.936751327913279">Assuming that one SMS word is mapped exactly to one English word in the channel model under an alignment we need to con- (Brown et al. 1993). The channel en as in the following equation If we include the word “null” in the English vocabulary, the above model can fully address the deletion and substitution transformations, but inadequate to address the insertion transforma- For example, the lingoes have to be normalized using an insertion transto become know” “yester- Moreover, we also want the normalization to have better lexical affinity and linguistic equivalent, thus we extend the model to allow many words to many words alignment, allowing a sequence of SMS words to be normalized to a sequence of contiguous English words. We call this updated model a phrase-based normalization model. 4.2 Phrase-based Model an English sentence SMS sentence , we assume that be decomposed into with a segmentation such that phrase can be corresponded with e we have = ... ... 1 ... The channel model can be rewritten in equation (3). P s e (  |) N s T e N ( ,  |) M 1 1 1 1 T phrase alignment as done in the previous A word-based model. (  |) � � K s A e ( ,  |) � � � K K P s e 1 1 1 1 � A phrase  |) (  |, ) N M N i P T e P s T e 1 1 1 Ni � � K K (  |) (  |) P T e P s e 1 1 1 T T ∑ (3) (  |) (  |) K K T e P s e � � 1 1 1 T the position of a word in san d its am K A e P s A e K (  |) (  |, ) K � � i � � � 1 1 1 ∑ � A s e  |) N s A e N( ,  |) 1 1 A M N A e P s A e (  |) (  |, ) Ni 1 1 1 { P(m am) P(s ea ≈  ∑ ∏ m| m K a  k a P s s e (  |) (  |, ) ∑ ∏   � i � � � k 1 A k K ≈  ∑∏ k a P s e (  |) (  |) i � � k a k A k � (4) only two types of probabilities: the alignment probabilities denoted by P m and the (  |am ) lexicon mapping probabilities denoted by model can be writt where alignment in . e A This is the basic function of the channel model for the phrase-based SMS normalization model, where we used the maximum approximation for the sum over all segmentations. Then we further decompose the probability ( 1  |1 ) P s � e � using a K K We are now able to model the three tran sforthrough the normalization pair , ) 36 the mapping probability s  |) . The followings show the scenarios in which the three transformations occur. The statistics in our training corpus shows that by selecting appropriate phrase segmentation, the position re-ordering at the phrase level occurs rarely. It is not surprising since most of the English words or phrases in normal English text are replaced with lingoes in SMS messages without position change to make SMS text short and concise and to retain the meaning. Thus we need to consider only monotone alignment at phrase i.e., as in equation (4). In addition, the word-level reordering within phrase is learned during training. Now we can further derive equation (4) as follows: K  P s e (  |) � � K  ∑ ∏ 1 1 A k � �  |) s e k 1 mapping probability  |estimated via relative frequencies as follows: Nse ( , � P s e (  |) � � k k(6) k k s ( , ) &apos; the frequency of the pair Using a bigram language model and assuming Bayes decision rule, we finally obtain the following search criterion for equation (1). 1 1 N N arg max ( | N ( | T e ( | s k 1 k the above equation, we assume the segprobability be constant. N Finally, the SMS normalization model consists of sub-models: word-based language model characterized by  |) and phraselexical mapping model model), by � . Issues For the phrase-based model training, the sentence-aligned SMS corpus needs to be aligned first at the phrase level. The maximum likelihood approach, through EM algorithm and Viterbi search (Dempster et al., 1977) is employed to infer such an alignment. Here, we make a reasonable assumption on the alignment unit that a single SMS word can be mapped to a sequence of contiguous English words, but not vice verse. The EM algorithm for phrase alignment is illustrated in Figure 1 and is formulated by equation (8). The Expectation-Maximization Algorithm (1) Bootstrap initial alignment using orthographic similarities (2) Expectation: Update the joint probabili- , ) (3) Maximization: Apply the joint probabilik, ) get new alignment using Viterbi search algorithm (4) Repeat (2) to (3) until alignment converges (5) Derive normalization pairs from final Figure 1. Phrase Alignment Using EM Algorithm K � max( , � s s e  |, ) M N s e k � kek The alignment process given in equation (8) is different from that of normalization given in (7) in that, here we have an aligned input sentence pair, . The alignment process is just to find the alignment segmentation the two sentences that maximizes the joint probability. Therefore, in step (2) of the EM algorithm given at Figure 1, only the joint probabilities ) involved and updated. Since EM may fall into local optimization, in order to speed up convergence and find a nearly global optimization, a string matching technique is exploited at the initialization step to identify most probable normalization pairs. The or- Deletion Insertion null = � � &lt; � � Substitution arg max ( | e e ( i � �  |) s e n k e T  N K K k) i � )    (5) ≈ −  |) } 1 1 ) K   (7) � e , , 37 thographic similarities captured by edit distance a SMS lingo which contains the commonly used short-forms are first used to establish phrase mapping boundary candidates. Heuristics are then exploited to match tokens within the pairs of boundary candidates by trying to combine consecutive tokens within the boundary candidates if the numbers of tokens do not agree. Finally, a filtering process is carried out to manually remove the low-frequency noisy alignment pairs. Table 4 shows some of the extracted normalization pairs. As can be seen from the table, our algorithm discovers ambiguous mappings automatically that are otherwise missing from most of the lingo dictionary. (2, 2) 0 (2, to) -0.579466 (2, too) -0.897016 (2, null) -2.97058 (4, 4) 0 (4, for) -0.431364 (4, null) -3.27161 (w, who are) -0.477121 (w, with) -0.764065 (w, who) -1.83885 (dat, that) -0.726999 (dat, date) -0.845098 (tmr, tomorrow) -0.341514 Table 4. Examples of normalization pairs Given the phrase-aligned SMS corpus, the lexical mapping model, characterized by  |, is easily to be trained using equation Our n-gram LM  |is trained on English Gigaword provided by LDC using SRILM language modeling toolkit (Stolcke, 2002). Backoff smoothing (Jelinek, 1991) is used to adjust and assign a non-zero probability to the unseen words to address data sparseness. 4.4 Monotone Search Given an input , the search, characterized in s (7), is to find a sentence maxientries are collected from various websites such as http://www.handphones.info/sms-dictionary/sms-lingo.php, andhttp://www.funsms.net/sms_dictionary.htm,etc. the normalization model. In this paper, the maximization problem in equation (7) is solved using a monotone search, implemented as a Viterbi search through dynamic programming. 5 Experiments The aim of our experiment is to verify the effectiveness of the proposed statistical model for SMS normalization and the impact of SMS normalization on MT. A set of 5000 parallel SMS messages, which consists of raw (un-normalized) SMS messages and reference messages manually prepared by two project members with inter-normalization agreement checked, was prepared for training and testing. For evaluation, we use IBM’s BLEU score (Papineni et al., 2002) to measure the performance of the SMS normalization. BLEU score measures the similarity between two sentences using n-gram statistics with a penalty for too short sentences, which is already widely-used in MT evaluation.</abstract>
<note confidence="0.6564415">Setup BLEU score (3gram) Raw SMS without Normalization 0.5784 Dictionary Look-up plus Frequency 0.6958 Bi-gram Language Model Only 0.7086</note>
<abstract confidence="0.956910575757576">Table 5. Performance of different setups of the baseline experiments on the 5000 parallel SMS messages 5.1 Baseline Experiments: Simple SMS Lingo Dictionary Look-up and Using Language Model Only The baseline experiment is to moderate the texts using a lingo dictionary comprises 142 normalization pairs, which is also used in bootstrapping the phrase alignment learning process. Table 5 compares the performance of the different setups of the baseline experiments. We first measure the complexity of the SMS normalization task by directly computing the similarity between the raw SMS text and the English text. The row of Table 5 reports the similarity as 0.5784 in BLEU score, which implies that there are quite a number of English word 3-gram that are common in the raw normalized messages. The experiment is carried out using only simple dictionary look-up. 38 Lexical ambiguity is addressed by selecting the highest-frequency normalization candidate, i.e., only unigram LM is used. The performance of experiment is 0.6958 in BLEU score. It suggests that the lingo dictionary plus the unigram LM is very useful for SMS normalization. we carry out the experiment using dictionary look-up plus bi-gram LM. Only a slight improvement of 0.0128 (0.7086-0.6958) is obtained. This is largely because the English words in the lingo dictionary are mostly highfrequency and commonly-used. Thus bi-gram does not show much more discriminative ability than unigram without the help of the phrasebased lexical mapping model. 5.2 Using Phrase-based Model We then conducted the experiment using the proposed method (Bi-gram LM plus a phrase-based lexical mapping model) through a five-fold cross validation on the 5000 parallel SMS messages. Table 6 shows the results. An average score of 0.8070 is obtained. Compared with the baseline performance in Table 5, the improvement is very significant. It suggests that the phrase-based lexical mapping model is very useful and our method is effective for SMS text normalization. Figure 2 is the learning curve. It shows that our algorithm converges when training data is increased to 3000 SMS parallel messages. This suggests that our collected corpus is representative and enough for training our model. Table 7 illustrates some examples of the normalization results. 5-fold cross validation BLEU score (3-gram) Setup 1 0.8023 Setup 2 0.8236 Setup 3 0.8071 Setup 4 0.8113 Setup 5 0.7908 Ave. 0.8070 Table 6. Normalization results for 5fold cross validation test 0.82 0.8 0.78 0.76 0.74 0.72 0.7 1000 2000 3000 4000 5000 Figure 2. Learning Curve Experimental result analysis reveals that the strength of our model is in its ability to disammapping as in to or and to or Error analysis shows that the challenge of the model lies in the proper insertion of subject pronoun and auxiliary or copula verb, which serves to give further semantic information about the main verb, however this requires significant context understanding. For a message such as gives little on whether it should be normalized to or are unless the full conversation is studied. Takako w r u? Takako who are you? Im in ns, lik soccer, clubbin hangin w frenz! Wat bout u mee? I&apos;m in ns, like soccer, clubbing hanging with friends! What about you? fancy getting excited w others&apos; boredom Fancy getting excited with others&apos; boredom If u ask me b4 he ask me then i&apos;ll go out w u all lor. N u still can act so real. If you ask me before he asked me then I&apos;ll go out with you all. And you still can act so real. Doing nothing, then u not having dinner w us? Doing nothing, then you do not having dinner with us? Aiyar sorry lor forgot 2 tell u... Mtg at 2 pm. Sorry forgot to tell you... Meeting at two pm. tat&apos;s y I said it&apos;s bad dat all e gals know u... Wat u doing now? That&apos;s why I said it&apos;s bad that all the girls know you... What you doing now? Table 7. Examples of Normalization Results 5.3 Effect on English-Chinese MT An experiment was also conducted to study the effect of normalization on MT using 402 messages randomly selected from the text corpus. We compare three types of SMS message: raw SMS messages, normalized messages using simple dictionary look-up and normalized messages using our method. The messages are passed to two different English-to-Chinese translation sysprovided by and Institute for Infoseparately to produce three sets of translation output. The translation quality is measured using 3-gram cumulative BLEU score against two reference messages. 3-gram is 4http://www.systranet.com/systran/net 5http://nlp.i2r.a-star.edu.sg/techtransfer.html BLEU 39 used as most of the messages are short with average length of seven words. Table 8 shows the details of the BLEU scores. We obtain an average of 0.3770 BLEU score for normalized messages against 0.1926 for raw messages. The significant performance improvement suggests that preprocessing of normalizing SMS text using our method before MT is an effective way to adapt a general MT system to SMS domain. Systran Ave. Raw Message 0.2633 0.1219 0.1926 Dict Lookup 0.3485 0.1690 0.2588 Normalization 0.4423 0.3116 0.3770 Table 8. SMS Translation BLEU score with or without SMS normalization 6 Conclusion In this paper, we study the differences among SMS normalization, general text normalization, spelling check and text paraphrasing, and investigate the different phenomena of SMS messages. We propose a phrase-based statistical method to normalize SMS messages. The method produces messages that collate well with manually normalized messages, achieving 0.8070 BLEU score against 0.6958 baseline score. It also significantly improves SMS translation accuracy from 0.1926 to 0.3770 in BLEU score without adjusting the MT model. This experiment results provide us with a good indication on the feasibility of using this method in performing the normalization task. We plan to extend the model to incorporate mechanism to handle missing punctuation (which potentially affect MT output and are not being taken care at the moment), and making use of pronunciation information to handle OOV caused by the use of phonetic spelling. A bigger data set will also be used to test the robustness of the system leading to a more accurate alignment and normalization.</abstract>
<note confidence="0.527539">References A.T. Aw, M. Zhang, Z.Z. Fan, P.K. Yeo and J. Su. Normalization for an English-to- SMS Translation System. Summit- 2005 S. Bangalore, V. Murdock and G. Riccardi. 2002.</note>
<title confidence="0.952197">Bootstrapping Bilingual Data using Consensus Translation for a Multilingual Instant Messaging</title>
<note confidence="0.923205526315789">Barzilay and K. R. McKeown. 2001. from a parallel ACL-2001 Brill and R. C. Moore. 2000. Improved Error Model for Noisy Channel Spelling Correction. ACL-2000 P. F. Brown, S. D. Pietra, V. D. Pietra and R. Mercer. Mathematics of Statistical Machine Parameter Computational Linguistics: 19(2) Clark. 2003. very noisy text. Proceedings of Workshop on Shallow Processing of Large Corpora, Lancaster, 2003 J. Damerau. 1964. technique for computer detecand correction of spelling Communications ACM 7, 171-176 A.P. Dempster, N.M. Laird and D.B. Rubin. 1977. Maximum likelihood from incomplete data via the Journal of the Royal Statistical Society, Series B, Vol. 39, 1-38</note>
<title confidence="0.700542">Golding and D. Roth. 1999. Winnow-Based Apto Spelling Machine Learning</title>
<phone confidence="0.782533">34: 107-130</phone>
<abstract confidence="0.887928111111111">Jelinek. 1991. language modeling speech In A. Waibel and K.F. Lee, editors, Readings in Speech Recognition, pages 450-506. Morgan Kaufmann, 1991 D. Kernighan, K Church and W. Gale. 1990. spelling correction program based on a noisy model. Kukich. 1992. for automatically corwords in ACM Computing Surveys,</abstract>
<address confidence="0.641203">24(4):377-439</address>
<author confidence="0.896433">K A Papineni</author>
<author confidence="0.896433">S Roukos</author>
<author confidence="0.896433">T Ward</author>
<author confidence="0.896433">W J Zhu</author>
<affiliation confidence="0.646766">a Method for Automatic Evaluation</affiliation>
<address confidence="0.401837">Machine ACL-2002</address>
<note confidence="0.7013218">Koehn, F.J. Och and D. Marcu. 2003. HLT-NAACL-2003 Shannon. 1948. mathematical theory of commu- Bell System Technical Journal 27(3): 379-423 Shimohata and E. Sumita 2002. Paraphrasing Based on Parallel Corpus for Normaliza- LREC-2002 R. Sproat, A. Black, S. Chen, S. Kumar, M. Ostendorf C. Richards. 2001. of Non- Computer Speech and Language, 15(3):287-333 Stolcke. 2002. SRILM – extensible language ICSLP-2002 Toutanova and R. C. Moore. 2002. for Improved Spelling Correction. ACL- 2002 Zens and H. Ney. 2004. in Phrase- Statistical HLT-NAALL-2004 40</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A T Aw</author>
<author>M Zhang</author>
<author>Z Z Fan</author>
<author>P K Yeo</author>
<author>J Su</author>
</authors>
<title>Input Normalization for an English-toChinese SMS Translation System.</title>
<date>2005</date>
<publisher>MT Summit2005</publisher>
<contexts>
<context position="4815" citStr="Aw et al. (2005)" startWordPosition="759" endWordPosition="762">a using off-the-shelf translation systems for training a hierarchical statistical translation model for general domain instant messaging used in Internet chat rooms. Their method deals with the special phenomena of the instant messaging language (rather than the SMS language) in each individual MT system. Clark (2003) proposed to unify the process of tokenization, segmentation and spelling correction for normalization of general noisy text (rather than SMS or instant messaging texts) based on a noisy channel model at the character level. However, results of the normalization are not reported. Aw et al. (2005) gave a brief description on their input pre-processing work for an English-toChinese SMS translation system using a wordgroup model. In addition, in most of the commercial SMS translation applications 2 , SMS lingo (i.e., SMS short form) dictionary is provided to replace SMS short-forms with normal English words. Most of the systems do not handle OOV (out-of-vocabulary) items and ambiguous inputs. Following compares SMS text normalization with other similar or related applications. 2.1 SMS Normalization versus General Text Normalization General text normalization deals with NonStandard Words </context>
<context position="10936" citStr="Aw et al. (2005)" startWordPosition="1709" endWordPosition="1712">mation within the limited space quota (160 letters for English), thus they tend to be implicit and influenced by pragmatic and situation reasons. These inadequacies of language expression such as deletion of articles and subject pronoun, as well as problems in number agreements or tenses make SMS normalization more challenging. Table 1 illustrates some orthographic and grammar variations of SMS texts. 3.3 Corpus Statistics We investigate the corpus to assess the feasibility of replacing the lingoes with normal English words and performing limited adjustment to the text structure. Similarly to Aw et al. (2005), we focus on the three major cases of transformation as shown in the corpus: (1) replacement of OOV words and non-standard SMS lingoes; (2) removal of slang and (3) insertion of auxiliary or copula verb and subject pronoun. Table 1. Examples of SMS Messages Transformation Percentage (%) Insertion 8.09 Deletion 5.48 Substitution 86.43 Table 2. Distribution of Insertion, Deletion and Substitution Transformation. Substitution Deletion Insertion u -&gt; you m are 2 → to lah am n → and t is r → are ah you ur →your leh to dun → don’t 1 do man → manches- huh a ter no → number one in intro → introduce l</context>
</contexts>
<marker>Aw, Zhang, Fan, Yeo, Su, 2005</marker>
<rawString>A.T. Aw, M. Zhang, Z.Z. Fan, P.K. Yeo and J. Su. 2005. Input Normalization for an English-toChinese SMS Translation System. MT Summit2005</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>V Murdock</author>
<author>G Riccardi</author>
</authors>
<title>Bootstrapping Bilingual Data using Consensus Translation for a Multilingual Instant Messaging System. COLING-2002</title>
<date>2002</date>
<contexts>
<context position="3982" citStr="Bangalore et al. (2002)" startWordPosition="635" endWordPosition="638">n on the task of SMS translation. The experiment of translating SMS texts from English to Chinese on a corpus comprising 402 SMS texts shows that, SMS normalization as a preprocessing step of MT can boost the translation performance from 0.1926 to 0.3770 in BLEU score. The rest of the paper is organized as follows. Section 2 reviews the related work. Section 3 summarizes the characteristics of English SMS texts. Section 4 discusses our method and Section 5 reports our experiments. Section 6 concludes the paper. 2 Related Work There is little work reported on SMS normalization and translation. Bangalore et al. (2002) used 33 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 33–40, Sydney, July 2006. c�2006 Association for Computational Linguistics a consensus translation technique to bootstrap parallel data using off-the-shelf translation systems for training a hierarchical statistical translation model for general domain instant messaging used in Internet chat rooms. Their method deals with the special phenomena of the instant messaging language (rather than the SMS language) in each individual MT system. Clark (2003) proposed to unify the process of tokenization, segmentation and</context>
</contexts>
<marker>Bangalore, Murdock, Riccardi, 2002</marker>
<rawString>S. Bangalore, V. Murdock and G. Riccardi. 2002. Bootstrapping Bilingual Data using Consensus Translation for a Multilingual Instant Messaging System. COLING-2002</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>K R McKeown</author>
</authors>
<title>Extracting paraphrases from a parallel corpus.</title>
<date>2001</date>
<pages>2001</pages>
<contexts>
<context position="7881" citStr="Barzilay and McKeown, 2001" startWordPosition="1234" endWordPosition="1237">). In addition, normalizing SMS text might require the context to be spanned over more than one lexical unit such as “lemme” (let me), “ur” (you are) etc. Therefore, the models used in spelling correction are inadequate for providing a complete solution for SMS normalization. 2.3 SMS Normalization versus Text Paraphrasing Problem Others may regard SMS normalization as a paraphrasing problem. Broadly speaking, paraphrases capture core aspects of variability in language, by representing equivalencies between different expressions that correspond to the same meaning. In most of the recent works (Barzilay and McKeown, 2001; Shimohata, 2002), they are acquired (semi-) automatically from large comparable or parallel corpora using lexical and morpho-syntactic information. Text paraphrasing works on clean texts in which contextual and lexical-syntactic features can be extracted and used to find “approximate conceptual equivalence”. In SMS normalization, we are dealing with non-words and “ungrammatically” sentences with the purpose to normalize or standardize these words and form better sentences. The SMS normalization problem is thus different from text paraphrasing. On the other hand, it bears some similarities wi</context>
</contexts>
<marker>Barzilay, McKeown, 2001</marker>
<rawString>R. Barzilay and K. R. McKeown. 2001. Extracting paraphrases from a parallel corpus. ACL-2001</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>R C Moore</author>
</authors>
<title>An Improved Error Model for Noisy Channel Spelling Correction.</title>
<date>2000</date>
<contexts>
<context position="6575" citStr="Brill and Moore, 2000" startWordPosition="1029" endWordPosition="1032">hallenge to SMS normalization. 2.2 SMS Normalization versus Spelling Correction Problem Intuitively, many would regard SMS normalization as a spelling correction problem where the lingoes are erroneous words or non-words to be replaced by English words. Researches on spelling correction centralize on typographic and cognitive/orthographic errors (Kukich, 1992) and use approaches (M.D. Kernighan, Church and Gale, 1991) that mostly model the edit operations using distance measures (Damerau 1964; Levenshtein 1966), specific word set confusions (Golding and Roth, 1999) and pronunciation modeling (Brill and Moore, 2000; Toutanova and Moore, 2002). These models are mostly character-based or string-based without considering the context. In addition, the author might not be aware of the errors in the word introduced during the edit operations, as most errors are due to mistype of characters near to each other on the keyboard or homophones, such as “poor” or “pour”. In SMS, errors are not isolated within word and are usually not surrounded by clean context. Words are altered deliberately to reflect sender’s distinct creation and idiosyncrasies. A character can be deleted on purpose, such as “wat” (what) and “hv</context>
</contexts>
<marker>Brill, Moore, 2000</marker>
<rawString>E. Brill and R. C. Moore. 2000. An Improved Error Model for Noisy Channel Spelling Correction. ACL-2000</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S D Pietra</author>
<author>V D Pietra</author>
<author>R Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics:</title>
<date>1993</date>
<contexts>
<context position="12945" citStr="Brown et al., 1993" startWordPosition="2072" endWordPosition="2075">I hv 2 go. Dinner w parents. (I have to go. Have dinner with parents.) 35 normalization with a total of 2300 transformations. Substitution accounts for almost 86% of all transformations. Deletion and substitution make up the rest. Table 3 shows the top 10 most common transformations. 4 SMS Normalization We view the SMS language as a variant of English language with some derivations in vocabulary and grammar. Therefore, we can treat SMS normalization as a MT problem where the SMS language is to be translated to normal English. We thus propose to adapt the statistical machine translation model (Brown et al., 1993; Zens and Ney, 2004) for SMS text normalization. In this section, we discuss the three components of our method: modeling, training and decoding for SMS text normalization. 4.1 Basic Word-based Model The SMS normalization model is based on the source channel model (Shannon, 1948). Assuming that an English sentence e, of length N is “corrupted” by a noisy channel to produce a SMS message s, of length M, the English sentence e, could be recovered through a posteriori distribution for a channel target text given the source text P s , and a prior distribution for ( |e) the channel source text . P</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. D. Pietra, V. D. Pietra and R. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics: 19(2)</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Clark</author>
</authors>
<title>Pre-processing very noisy text.</title>
<date>2003</date>
<booktitle>In Proceedings of Workshop on Shallow Processing of Large Corpora,</booktitle>
<location>Lancaster,</location>
<contexts>
<context position="4518" citStr="Clark (2003)" startWordPosition="714" endWordPosition="715"> work reported on SMS normalization and translation. Bangalore et al. (2002) used 33 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 33–40, Sydney, July 2006. c�2006 Association for Computational Linguistics a consensus translation technique to bootstrap parallel data using off-the-shelf translation systems for training a hierarchical statistical translation model for general domain instant messaging used in Internet chat rooms. Their method deals with the special phenomena of the instant messaging language (rather than the SMS language) in each individual MT system. Clark (2003) proposed to unify the process of tokenization, segmentation and spelling correction for normalization of general noisy text (rather than SMS or instant messaging texts) based on a noisy channel model at the character level. However, results of the normalization are not reported. Aw et al. (2005) gave a brief description on their input pre-processing work for an English-toChinese SMS translation system using a wordgroup model. In addition, in most of the commercial SMS translation applications 2 , SMS lingo (i.e., SMS short form) dictionary is provided to replace SMS short-forms with normal En</context>
</contexts>
<marker>Clark, 2003</marker>
<rawString>A. Clark. 2003. Pre-processing very noisy text. In Proceedings of Workshop on Shallow Processing of Large Corpora, Lancaster, 2003</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Damerau</author>
</authors>
<title>A technique for computer detection and correction of spelling errors.</title>
<date>1964</date>
<journal>Communications ACM</journal>
<volume>7</volume>
<pages>171--176</pages>
<contexts>
<context position="6451" citStr="Damerau 1964" startWordPosition="1012" endWordPosition="1013">oung SMS users, are not yet formalized in linguistics. Therefore, the special phenomena in SMS texts impose a big challenge to SMS normalization. 2.2 SMS Normalization versus Spelling Correction Problem Intuitively, many would regard SMS normalization as a spelling correction problem where the lingoes are erroneous words or non-words to be replaced by English words. Researches on spelling correction centralize on typographic and cognitive/orthographic errors (Kukich, 1992) and use approaches (M.D. Kernighan, Church and Gale, 1991) that mostly model the edit operations using distance measures (Damerau 1964; Levenshtein 1966), specific word set confusions (Golding and Roth, 1999) and pronunciation modeling (Brill and Moore, 2000; Toutanova and Moore, 2002). These models are mostly character-based or string-based without considering the context. In addition, the author might not be aware of the errors in the word introduced during the edit operations, as most errors are due to mistype of characters near to each other on the keyboard or homophones, such as “poor” or “pour”. In SMS, errors are not isolated within word and are usually not surrounded by clean context. Words are altered deliberately t</context>
</contexts>
<marker>Damerau, 1964</marker>
<rawString>F. J. Damerau. 1964. A technique for computer detection and correction of spelling errors. Communications ACM 7, 171-176</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm,</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<volume>39</volume>
<pages>1--38</pages>
<contexts>
<context position="17989" citStr="Dempster et al., 1977" startWordPosition="3264" endWordPosition="3267"> en e N  1 n=1 N i max ( | P T e ) ( | i P s � e �  ∏ k k ) 1 T k = 1 For the above equation, we assume the segmentation probability P(T |e to be constant. 1 ) N Finally, the SMS normalization model consists of two sub-models: a word-based language model (LM), characterized by P(en |en−1 ) and a phrasebased lexical mapping model (channel model), characterized by P ( s k |e � � k ) . 4.3 Training Issues For the phrase-based model training, the sentence-aligned SMS corpus needs to be aligned first at the phrase level. The maximum likelihood approach, through EM algorithm and Viterbi search (Dempster et al., 1977) is employed to infer such an alignment. Here, we make a reasonable assumption on the alignment unit that a single SMS word can be mapped to a sequence of contiguous English words, but not vice verse. The EM algorithm for phrase alignment is illustrated in Figure 1 and is formulated by equation (8). The Expectation-Maximization Algorithm (1) Bootstrap initial alignment using orthographic similarities (2) Expectation: Update the joint probabilities P(sk , � �ek ) (3) Maximization: Apply the joint probabilities P(s k , � ek ) to get new alignment using Viterbi search algorithm (4) Repeat (2) to </context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A.P. Dempster, N.M. Laird and D.B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm, Journal of the Royal Statistical Society, Series B, Vol. 39, 1-38</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Golding</author>
<author>D Roth</author>
</authors>
<title>A Winnow-Based Approach to Spelling Correction.</title>
<date>1999</date>
<journal>Machine Learning</journal>
<volume>34</volume>
<pages>107--130</pages>
<contexts>
<context position="6525" citStr="Golding and Roth, 1999" startWordPosition="1021" endWordPosition="1025">e, the special phenomena in SMS texts impose a big challenge to SMS normalization. 2.2 SMS Normalization versus Spelling Correction Problem Intuitively, many would regard SMS normalization as a spelling correction problem where the lingoes are erroneous words or non-words to be replaced by English words. Researches on spelling correction centralize on typographic and cognitive/orthographic errors (Kukich, 1992) and use approaches (M.D. Kernighan, Church and Gale, 1991) that mostly model the edit operations using distance measures (Damerau 1964; Levenshtein 1966), specific word set confusions (Golding and Roth, 1999) and pronunciation modeling (Brill and Moore, 2000; Toutanova and Moore, 2002). These models are mostly character-based or string-based without considering the context. In addition, the author might not be aware of the errors in the word introduced during the edit operations, as most errors are due to mistype of characters near to each other on the keyboard or homophones, such as “poor” or “pour”. In SMS, errors are not isolated within word and are usually not surrounded by clean context. Words are altered deliberately to reflect sender’s distinct creation and idiosyncrasies. A character can b</context>
</contexts>
<marker>Golding, Roth, 1999</marker>
<rawString>A. Golding and D. Roth. 1999. A Winnow-Based Approach to Spelling Correction. Machine Learning 34: 107-130</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>Self-organized language modeling for speech recognition.</title>
<date>1991</date>
<booktitle>Readings in Speech Recognition,</booktitle>
<pages>450--506</pages>
<editor>In A. Waibel and K.F. Lee, editors,</editor>
<publisher>Morgan Kaufmann,</publisher>
<contexts>
<context position="21094" citStr="Jelinek, 1991" startWordPosition="3866" endWordPosition="3867">ionary. (s�, e�) log P(s� |e�) (2, 2) 0 (2, to) -0.579466 (2, too) -0.897016 (2, null) -2.97058 (4, 4) 0 (4, for) -0.431364 (4, null) -3.27161 (w, who are) -0.477121 (w, with) -0.764065 (w, who) -1.83885 (dat, that) -0.726999 (dat, date) -0.845098 (tmr, tomorrow) -0.341514 Table 4. Examples of normalization pairs Given the phrase-aligned SMS corpus, the lexical mapping model, characterized by P(s�k |ek) , is easily to be trained using equation (6). Our n-gram LM P(en |en−1) is trained on English Gigaword provided by LDC using SRILM language modeling toolkit (Stolcke, 2002). Backoff smoothing (Jelinek, 1991) is used to adjust and assign a non-zero probability to the unseen words to address data sparseness. 4.4 Monotone Search Given an input , the search, characterized in s equation (7), is to find a sentence e that maxi3 The entries are collected from various websites such as http://www.handphones.info/sms-dictionary/sms-lingo.php, and http://www.funsms.net/sms_dictionary.htm, etc. mizes P(s |e)-P(e) using the normalization model. In this paper, the maximization problem in equation (7) is solved using a monotone search, implemented as a Viterbi search through dynamic programming. 5 Experiments Th</context>
</contexts>
<marker>Jelinek, 1991</marker>
<rawString>F. Jelinek. 1991. Self-organized language modeling for speech recognition. In A. Waibel and K.F. Lee, editors, Readings in Speech Recognition, pages 450-506. Morgan Kaufmann, 1991</rawString>
</citation>
<citation valid="true">
<authors>
<author>M D Kernighan</author>
<author>K Church</author>
<author>W Gale</author>
</authors>
<title>A spelling correction program based on a noisy channel model.</title>
<date>1990</date>
<pages>1990</pages>
<marker>Kernighan, Church, Gale, 1990</marker>
<rawString>M. D. Kernighan, K Church and W. Gale. 1990. A spelling correction program based on a noisy channel model. COLING-1990</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kukich</author>
</authors>
<title>Techniques for automatically correcting words in text.</title>
<date>1992</date>
<journal>ACM Computing Surveys,</journal>
<pages>24--4</pages>
<contexts>
<context position="6316" citStr="Kukich, 1992" startWordPosition="992" endWordPosition="993">orrect in linguistics. However lingoes, such as “b4” (before) and “bf” (boyfriend), which are usually selfcreated and only accepted by young SMS users, are not yet formalized in linguistics. Therefore, the special phenomena in SMS texts impose a big challenge to SMS normalization. 2.2 SMS Normalization versus Spelling Correction Problem Intuitively, many would regard SMS normalization as a spelling correction problem where the lingoes are erroneous words or non-words to be replaced by English words. Researches on spelling correction centralize on typographic and cognitive/orthographic errors (Kukich, 1992) and use approaches (M.D. Kernighan, Church and Gale, 1991) that mostly model the edit operations using distance measures (Damerau 1964; Levenshtein 1966), specific word set confusions (Golding and Roth, 1999) and pronunciation modeling (Brill and Moore, 2000; Toutanova and Moore, 2002). These models are mostly character-based or string-based without considering the context. In addition, the author might not be aware of the errors in the word introduced during the edit operations, as most errors are due to mistype of characters near to each other on the keyboard or homophones, such as “poor” o</context>
</contexts>
<marker>Kukich, 1992</marker>
<rawString>K. Kukich. 1992. Techniques for automatically correcting words in text. ACM Computing Surveys, 24(4):377-439</rawString>
</citation>
<citation valid="true">
<authors>
<author>K A Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>BLEU : a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<pages>2002</pages>
<contexts>
<context position="2798" citStr="Papineni et al., 2002" startWordPosition="439" endWordPosition="442">d domains can be modeled separately without accessing and adapting the language model of the MT system for each SMS application. Another advantage is that the normalization module can be easily utilized by other applications, such as SMS to voicemail and SMS-based information query. In this paper, we present a phrase-based statistical model for SMS text normalization. The normalization is visualized as a translation problem where messages in the SMS language are to be translated to normal English using a similar phrase-based statistical MT method (Koehn et al., 2003). We use IBM’s BLEU score (Papineni et al., 2002) to measure the performance of SMS text normalization. BLEU score computes the similarity between two sentences using n-gram statistics, which is widely-used in MT evaluation. A set of parallel SMS messages, consisting of 5000 raw (un-normalized) SMS messages and their manually normalized references, is constructed for training and testing. Evaluation by 5- fold cross validation on this corpus shows that our method can achieve accuracy of 0.80702 in BLEU score compared to the baseline system of 0.6985. We also study the impact of our SMS text normalization on the task of SMS translation. The e</context>
<context position="22146" citStr="Papineni et al., 2002" startWordPosition="4022" endWordPosition="4025">. In this paper, the maximization problem in equation (7) is solved using a monotone search, implemented as a Viterbi search through dynamic programming. 5 Experiments The aim of our experiment is to verify the effectiveness of the proposed statistical model for SMS normalization and the impact of SMS normalization on MT. A set of 5000 parallel SMS messages, which consists of raw (un-normalized) SMS messages and reference messages manually prepared by two project members with inter-normalization agreement checked, was prepared for training and testing. For evaluation, we use IBM’s BLEU score (Papineni et al., 2002) to measure the performance of the SMS normalization. BLEU score measures the similarity between two sentences using n-gram statistics with a penalty for too short sentences, which is already widely-used in MT evaluation. Setup BLEU score (3- gram) Raw SMS without 0.5784 Normalization Dictionary Look-up 0.6958 plus Frequency Bi-gram Language 0.7086 Model Only Table 5. Performance of different setups of the baseline experiments on the 5000 parallel SMS messages 5.1 Baseline Experiments: Simple SMS Lingo Dictionary Look-up and Using Language Model Only The baseline experiment is to moderate the </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. A. Papineni, S. Roukos, T. Ward and W. J. Zhu. 2002. BLEU : a Method for Automatic Evaluation of Machine Translation. ACL-2002</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<date>2003</date>
<note>Statistical Phrase-Based Translation. HLT-NAACL-2003</note>
<contexts>
<context position="2749" citStr="Koehn et al., 2003" startWordPosition="430" endWordPosition="433">that the diversity in different user groups and domains can be modeled separately without accessing and adapting the language model of the MT system for each SMS application. Another advantage is that the normalization module can be easily utilized by other applications, such as SMS to voicemail and SMS-based information query. In this paper, we present a phrase-based statistical model for SMS text normalization. The normalization is visualized as a translation problem where messages in the SMS language are to be translated to normal English using a similar phrase-based statistical MT method (Koehn et al., 2003). We use IBM’s BLEU score (Papineni et al., 2002) to measure the performance of SMS text normalization. BLEU score computes the similarity between two sentences using n-gram statistics, which is widely-used in MT evaluation. A set of parallel SMS messages, consisting of 5000 raw (un-normalized) SMS messages and their manually normalized references, is constructed for training and testing. Evaluation by 5- fold cross validation on this corpus shows that our method can achieve accuracy of 0.80702 in BLEU score compared to the baseline system of 0.6985. We also study the impact of our SMS text no</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F.J. Och and D. Marcu. 2003. Statistical Phrase-Based Translation. HLT-NAACL-2003</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Shannon</author>
</authors>
<title>A mathematical theory of communication.</title>
<date>1948</date>
<journal>Bell System Technical Journal</journal>
<volume>27</volume>
<issue>3</issue>
<pages>379--423</pages>
<contexts>
<context position="13226" citStr="Shannon, 1948" startWordPosition="2118" endWordPosition="2119">4 SMS Normalization We view the SMS language as a variant of English language with some derivations in vocabulary and grammar. Therefore, we can treat SMS normalization as a MT problem where the SMS language is to be translated to normal English. We thus propose to adapt the statistical machine translation model (Brown et al., 1993; Zens and Ney, 2004) for SMS text normalization. In this section, we discuss the three components of our method: modeling, training and decoding for SMS text normalization. 4.1 Basic Word-based Model The SMS normalization model is based on the source channel model (Shannon, 1948). Assuming that an English sentence e, of length N is “corrupted” by a noisy channel to produce a SMS message s, of length M, the English sentence e, could be recovered through a posteriori distribution for a channel target text given the source text P s , and a prior distribution for ( |e) the channel source text . P(e) eˆN = arg max { P e s 1 ( |) eN N M } 1 1 1 argmax { P s e P e eN ( |) ( ) 1 N M N i } 1 1 1 Assuming that one SMS word is mapped exactly to one English word in the channel model under an alignment A , we need to conP(sm|ea m ) (Brown et al. 1993). The channel en as in the fol</context>
</contexts>
<marker>Shannon, 1948</marker>
<rawString>C. Shannon. 1948. A mathematical theory of communication. Bell System Technical Journal 27(3): 379-423</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Shimohata</author>
<author>E Sumita</author>
</authors>
<title>Automatic Paraphrasing Based on Parallel Corpus for Normalization.</title>
<date>2002</date>
<pages>2002</pages>
<marker>Shimohata, Sumita, 2002</marker>
<rawString>M. Shimohata and E. Sumita 2002. Automatic Paraphrasing Based on Parallel Corpus for Normalization. LREC-2002</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sproat</author>
<author>A Black</author>
<author>S Chen</author>
<author>S Kumar</author>
<author>M Ostendorf</author>
<author>C Richards</author>
</authors>
<date>2001</date>
<journal>Normalization of NonStandard Words. Computer Speech and Language,</journal>
<pages>15--3</pages>
<contexts>
<context position="5486" citStr="Sproat et al., 2001" startWordPosition="866" endWordPosition="869">sing work for an English-toChinese SMS translation system using a wordgroup model. In addition, in most of the commercial SMS translation applications 2 , SMS lingo (i.e., SMS short form) dictionary is provided to replace SMS short-forms with normal English words. Most of the systems do not handle OOV (out-of-vocabulary) items and ambiguous inputs. Following compares SMS text normalization with other similar or related applications. 2.1 SMS Normalization versus General Text Normalization General text normalization deals with NonStandard Words (NSWs) and has been wellstudied in text-to-speech (Sproat et al., 2001) while SMS normalization deals with Non-Words (NSs) or lingoes and has seldom been studied before. NSWs, such as digit sequences, acronyms, mixed case words (WinNT, SunOS), abbreviations and so on, are grammatically correct in linguistics. However lingoes, such as “b4” (before) and “bf” (boyfriend), which are usually selfcreated and only accepted by young SMS users, are not yet formalized in linguistics. Therefore, the special phenomena in SMS texts impose a big challenge to SMS normalization. 2.2 SMS Normalization versus Spelling Correction Problem Intuitively, many would regard SMS normaliza</context>
</contexts>
<marker>Sproat, Black, Chen, Kumar, Ostendorf, Richards, 2001</marker>
<rawString>R. Sproat, A. Black, S. Chen, S. Kumar, M. Ostendorf and C. Richards. 2001. Normalization of NonStandard Words. Computer Speech and Language, 15(3):287-333</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM – An extensible language modeling toolkit.</title>
<date>2002</date>
<pages>2002</pages>
<contexts>
<context position="21059" citStr="Stolcke, 2002" startWordPosition="3862" endWordPosition="3863">missing from most of the lingo dictionary. (s�, e�) log P(s� |e�) (2, 2) 0 (2, to) -0.579466 (2, too) -0.897016 (2, null) -2.97058 (4, 4) 0 (4, for) -0.431364 (4, null) -3.27161 (w, who are) -0.477121 (w, with) -0.764065 (w, who) -1.83885 (dat, that) -0.726999 (dat, date) -0.845098 (tmr, tomorrow) -0.341514 Table 4. Examples of normalization pairs Given the phrase-aligned SMS corpus, the lexical mapping model, characterized by P(s�k |ek) , is easily to be trained using equation (6). Our n-gram LM P(en |en−1) is trained on English Gigaword provided by LDC using SRILM language modeling toolkit (Stolcke, 2002). Backoff smoothing (Jelinek, 1991) is used to adjust and assign a non-zero probability to the unseen words to address data sparseness. 4.4 Monotone Search Given an input , the search, characterized in s equation (7), is to find a sentence e that maxi3 The entries are collected from various websites such as http://www.handphones.info/sms-dictionary/sms-lingo.php, and http://www.funsms.net/sms_dictionary.htm, etc. mizes P(s |e)-P(e) using the normalization model. In this paper, the maximization problem in equation (7) is solved using a monotone search, implemented as a Viterbi search through dy</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM – An extensible language modeling toolkit. ICSLP-2002</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>R C Moore</author>
</authors>
<title>Pronunciation Modeling for Improved Spelling Correction.</title>
<date>2002</date>
<pages>2002</pages>
<contexts>
<context position="6603" citStr="Toutanova and Moore, 2002" startWordPosition="1033" endWordPosition="1036">zation. 2.2 SMS Normalization versus Spelling Correction Problem Intuitively, many would regard SMS normalization as a spelling correction problem where the lingoes are erroneous words or non-words to be replaced by English words. Researches on spelling correction centralize on typographic and cognitive/orthographic errors (Kukich, 1992) and use approaches (M.D. Kernighan, Church and Gale, 1991) that mostly model the edit operations using distance measures (Damerau 1964; Levenshtein 1966), specific word set confusions (Golding and Roth, 1999) and pronunciation modeling (Brill and Moore, 2000; Toutanova and Moore, 2002). These models are mostly character-based or string-based without considering the context. In addition, the author might not be aware of the errors in the word introduced during the edit operations, as most errors are due to mistype of characters near to each other on the keyboard or homophones, such as “poor” or “pour”. In SMS, errors are not isolated within word and are usually not surrounded by clean context. Words are altered deliberately to reflect sender’s distinct creation and idiosyncrasies. A character can be deleted on purpose, such as “wat” (what) and “hv” (have). It also consists o</context>
</contexts>
<marker>Toutanova, Moore, 2002</marker>
<rawString>K. Toutanova and R. C. Moore. 2002. Pronunciation Modeling for Improved Spelling Correction. ACL2002</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<date>2004</date>
<note>Improvements in PhraseBased Statistical MT. HLT-NAALL-2004</note>
<contexts>
<context position="12966" citStr="Zens and Ney, 2004" startWordPosition="2076" endWordPosition="2079">parents. (I have to go. Have dinner with parents.) 35 normalization with a total of 2300 transformations. Substitution accounts for almost 86% of all transformations. Deletion and substitution make up the rest. Table 3 shows the top 10 most common transformations. 4 SMS Normalization We view the SMS language as a variant of English language with some derivations in vocabulary and grammar. Therefore, we can treat SMS normalization as a MT problem where the SMS language is to be translated to normal English. We thus propose to adapt the statistical machine translation model (Brown et al., 1993; Zens and Ney, 2004) for SMS text normalization. In this section, we discuss the three components of our method: modeling, training and decoding for SMS text normalization. 4.1 Basic Word-based Model The SMS normalization model is based on the source channel model (Shannon, 1948). Assuming that an English sentence e, of length N is “corrupted” by a noisy channel to produce a SMS message s, of length M, the English sentence e, could be recovered through a posteriori distribution for a channel target text given the source text P s , and a prior distribution for ( |e) the channel source text . P(e) eˆN = arg max { P</context>
</contexts>
<marker>Zens, Ney, 2004</marker>
<rawString>R. Zens and H. Ney. 2004. Improvements in PhraseBased Statistical MT. HLT-NAALL-2004</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>