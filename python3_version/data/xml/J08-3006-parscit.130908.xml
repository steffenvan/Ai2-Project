<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002457">
<title confidence="0.8764105">
Book Reviews
Semisupervised Learning for Computational Linguistics
</title>
<author confidence="0.993318">
Steven Abney
</author>
<affiliation confidence="0.984226">
(University of Michigan)
</affiliation>
<footnote confidence="0.38205">
Boca Raton, FL: Chapman &amp; Hall / CRC (Computer science and data analysis series,
edited by David Madigan et al.), 2007, xi+308 pp; hardbound, ISBN 978-1-58488-559-7,
$79.95, £44.99
Reviewed by
Vincent Ng
</footnote>
<subsubsectionHeader confidence="0.546727">
University of Texas at Dallas
</subsubsectionHeader>
<bodyText confidence="0.999888392857143">
Semi-supervised learning is by no means an unfamiliar concept to natural language
processing researchers. Labeled data has been used to improve unsupervised parameter
estimation procedures such as the EM algorithm and its variants since the beginning
of the statistical revolution in NLP (e.g., Pereira and Schabes 1992). Unlabeled data
has also been used to improve supervised learning procedures, the most notable ex-
amples being the successful applications of self-training and co-training to word sense
disambiguation (Yarowsky 1995) and named entity classification (Collins and Singer
1999).
Despite its increasing importance, semi-supervised learning is not a topic that is
typically discussed in introductory machine learning texts (e.g., Mitchell 1997; Alpaydin
2004) or NLP texts (e.g., Manning and Sch¨utze 1999; Jurafsky and Martin 2000).1
Consequently, to learn about semi-supervised learning research, one has to consult the
machine-learning literature. This can be a daunting task for NLP researchers who have
little background in machine learning. Steven Abney’s book Semisupervised Learning for
Computational Linguistics is targeted precisely at such researchers, aiming to provide
them with a “broad and accessible presentation” of topics in semi-supervised learning.
According to the preamble, the reader is assumed to have taken only an introductory
course in NLP “that include[s] statistical methods—concretely the material contained
in Jurafsky and Martin (2000) and Manning and Sch¨utze (1999).” Nonetheless, I agree
with the author that any NLP researcher who has a solid background in machine
learning is ready to “tackle the primary literature on semisupervised learning, and will
probably not find this book particularly useful” (page 11).
As the author promises, the book is self-contained and quite accessible to those
who have little background in machine learning. In particular, of the 12 chapters
in the book, three are devoted to preparatory material, including: a brief introduc-
tion to machine learning, basic unconstrained and constrained optimization tech-
niques (e.g., gradient descent and the method of Lagrange multipliers), and relevant
linear-algebra concepts (e.g., eigenvalues, eigenvectors, matrix and vector norms,
</bodyText>
<footnote confidence="0.684147333333333">
1 Although Manning and Sch¨utze (1999) and Jurafsky and Martin (2000) do discuss self-training, they do
so only in the context of Yarowsky’s word sense disambiguation algorithm.
Computational Linguistics Volume 34, Number 3
</footnote>
<bodyText confidence="0.9593405">
diagonalization). The remaining chapters focus roughly on six types of semi-supervised
learning methods:2
</bodyText>
<listItem confidence="0.980479277777778">
• Self-training. After introducing the self-training algorithm and its variants,
the author discusses its applications in NLP and its relationship to other
semi-supervised learning algorithms.
• Agreement-based methods. The co-training algorithm, along with a
theoretical analysis of its conditional independence assumption and its
applications in NLP, are presented. Additionally, a random field, which
penalizes disagreement among neighboring nodes, is introduced as an
alternative way of enforcing agreement.
• Clustering algorithms. Basic hard clustering algorithms (e.g., k-means,
graph mincuts, hierarchical clustering), EM (as a soft clustering
algorithm), and their role in semi-supervised learning are discussed.
• Boundary-oriented methods. Two discriminative learning algorithms,
boosting and support vector machines, are introduced as a means to
facilitate the discussion of their semi-supervised counterparts: co-boosting
and transductive SVMs.
• Label propagation in graphs. In graph-based approaches to semi-supervised
learning, the labels of the labeled nodes are propagated to the unlabeled
nodes, with the goal of maximizing the agreement of the labels of
</listItem>
<bodyText confidence="0.983837333333333">
proximate nodes. The author shows that this goal is equivalent to finding a
harmonic function given the labeled nodes, and presents several algorithms,
including the method of relaxation, for computing this function.
</bodyText>
<listItem confidence="0.8600228">
• Spectral methods. Spectral methods for semi-supervised learning can be
viewed as interpolation across a partially labeled graph as described
previously using a “standing wave.” The author explains the connection
between such a wave and the spectrum of a matrix, and establishes the
relationship of spectral clustering algorithms to other semi-supervised
</listItem>
<bodyText confidence="0.9695066875">
learners, including graph mincuts, random walks, and label propagation.
The book is rich in theory and algorithms, and although it is targeted at those who
lack relevant mathematical background, each theory and algorithm is presented in a
rigorous manner.
Another nice feature of the book is that it reveals the connection among seemingly
disparate ideas. As mentioned earlier, it shows that many semi-supervised learners
can in fact be viewed as self-training; also, the description of the connection between
spectral clustering and other semi-supervised learners is insightful.
In addition, I like the organization of the book. One reason is the presentation of
co-training: Although the algorithm is presented in Chapter 2, its theoretical underpin-
nings are not described until Chapter 9. This enables the reader to see its applications
in NLP (in Chapter 3) before going through the mathematics, which could be important
for researchers who are linguistically but not mathematically oriented. Another reason
2 The presentation of the methods here does not reflect the order in which they are introduced in the
book; rather, it is motivated by the book’s Section 1.3, which gives an overview of the “leading ideas”
of the book.
</bodyText>
<page confidence="0.985238">
450
</page>
<subsectionHeader confidence="0.814829">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.99999392">
is that the preparatory material is presented on a need-to-know basis. This allows the
discussion of algorithmic ideas as soon as the reader grasps the relevant fundamentals.
For instance, function optimization and basic linear algebra concepts are presented
in separate chapters, with the latter being deferred to Chapter 11, right before the
discussion of spectral clustering in Chapter 12.
Whereas the discussion of self-training and co-training is complemented by their
application to NLP problems, the same is not true for the remaining semi-supervised
learners described in the book. The reader is often left to imagine the potential NLP
applications of these learners, and as a consequence is unable to gain an understanding
of the state of the art of semi-supervised learning for NLP. In fact, given its scarcity of
NLP applications, the book perhaps does not merit its current title. It does have a rich
bibliography on semi-supervised learning for NLP, but most of the references are not
cited in the text.
The book also lacks a discussion of the practical issues in applying the semi-
supervised learners. For instance, the author does not mention that in practice it is
not easy to choose k in k-means clustering, merely describing k as a parameter of the
clustering algorithm. As another example, when introducing the EM algorithm, the
author applies it to a generative model that can be expressed in exponential form,
without acknowledging that one of the most difficult issues surrounding the application
of EM concerns the design of the right generative model given the data. The lack of
NLP applications in the book has unfortunately enabled the author to sidestep these
practical issues. On a related note, one can hardly find any discussions of the strengths
and weaknesses of the semi-supervised learners in the book. This could leave the reader
without the ability to choose the best learner for a given NLP problem, and is probably
another undesirable consequence of the book’s reluctance to discuss NLP applications.
The author’s decision to focus exclusively on semi-supervised classification prob-
lems effectively limits the scope of the book. One consequence of this decision is that the
reader may not be able to apply the EM algorithm to train a hidden Markov model for
solving sequence-learning problems as basic as part-of-speech tagging upon completion
of this book. Given the recent surge of interest in structure prediction in the NLP
community, and the fact that co-training and semi-supervised EM have been applied
to structure-prediction problems such as statistical parsing and part-of-speech tagging,
the book’s sole focus on classification problem is perhaps one of its weaknesses.
There are a few occasions on which the reader might not get a complete picture
of the capability of an algorithm. For instance, the reader might think that spectral
methods can be applied only to binary classification tasks, owing to the book’s exclusive
focus on such tasks in its discussion of spectral clustering. Similarly for the treatment
of support vector machines: The reader may get the impression that SVMs cannot be
used to learn non-linear functions, as the discussion of kernels is deliberately omitted
due to their irrelevance to transductive learning. Although it is important to keep the
presentation focused, I believe that the author could easily have removed potential con-
fusions by explicitly stating the full capability of an algorithm and referring the reader
to the relevant papers for details.
Given the rapid growth of semi-supervised learning research in the past decade,
there is currently a need for a broad and accessible reference to this area of research.
Abney’s book serves this purpose in spite of the aforementioned weaknesses, and I
believe that it is a useful starting point for any non–machine-learning experts who
intend to apply semi-supervised learning techniques to their research. As someone who
has some prior knowledge of semi-supervised learning, I still find this book insightful:
It reveals deep connections among apparently disparate ideas. If I were to teach a course
</bodyText>
<page confidence="0.993794">
451
</page>
<note confidence="0.488328">
Computational Linguistics Volume 34, Number 3
</note>
<bodyText confidence="0.790482">
on semi-supervised learning for NLP, I would undoubtedly use this book as a primary
reference.
</bodyText>
<sectionHeader confidence="0.913399" genericHeader="abstract">
References
</sectionHeader>
<reference confidence="0.9827945">
Alpaydin, Ethem. 2004. Introduction to
Machine Learning. The MIT Press,
Cambridge, MA.
Collins, Michael and Yoram Singer. 1999.
Unsupervised models for named entity
classification. In Proceedings of the 1999
Joint Conference on Empirical Methods in
Natural Language Processing and Very
Large Corpora, pages 100–110, College
Park, MD.
Jurafsky, Daniel and James H. Martin. 2000.
Speech and Language Processing. Prentice
Hall, Upper Saddle River, NJ.
Manning, Christopher D. and Hinrich
Sch¨utze. 1999. Foundations of Statistical
Natural Language Processing. The MIT
Press, Cambridge, MA.
Mitchell, Tom M.˙1997. Machine Learning.
McGraw Hill, Columbus, OH.
Pereira, Fernando and Yves Schabes.1992.
Inside-outside reestimation from partially
bracketed corpora. In Proceedings of the
30th Annual Meeting of the Association for
Computational Linguistics, pages 128–135,
Newark, DE.
Yarowsky, David. 1995. Unsupervised word
sense disambiguation rivaling supervised
methods. In Proceedings of the 33rd
Annual Meeting of the Association for
Computational Linguistics, pages 189–196,
Cambridge, MA.
Vincent Ng is an assistant professor in the Department of Computer Science at the University of
Texas at Dallas. He is also affiliated with the university’s Human Language Technology Research
Institute, where he conducts research on statistical natural language processing and teaches un-
dergraduate and graduate courses in machine learning. Ng’s address is: Department of Computer
Science, University of Texas at Dallas, Richardson, TX 75080-0688; e-mail: vince@hlt.utdallas.edu.
</reference>
<page confidence="0.998421">
452
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.010029">
<title confidence="0.9978355">Book Reviews Semisupervised Learning for Computational Linguistics</title>
<author confidence="0.999932">Steven Abney</author>
<affiliation confidence="0.993782">(University of Michigan)</affiliation>
<address confidence="0.568991666666667">Boca Raton, FL: Chapman &amp; Hall / CRC (Computer science and data analysis series, edited by David Madigan et al.), 2007, xi+308 pp; hardbound, ISBN 978-1-58488-559-7, $79.95, £44.99</address>
<note confidence="0.801588">Reviewed by</note>
<author confidence="0.99839">Vincent Ng</author>
<affiliation confidence="0.99268">University of Texas at Dallas</affiliation>
<abstract confidence="0.997261507692307">Semi-supervised learning is by no means an unfamiliar concept to natural language processing researchers. Labeled data has been used to improve unsupervised parameter estimation procedures such as the EM algorithm and its variants since the beginning of the statistical revolution in NLP (e.g., Pereira and Schabes 1992). Unlabeled data has also been used to improve supervised learning procedures, the most notable examples being the successful applications of self-training and co-training to word sense disambiguation (Yarowsky 1995) and named entity classification (Collins and Singer 1999). Despite its increasing importance, semi-supervised learning is not a topic that is typically discussed in introductory machine learning texts (e.g., Mitchell 1997; Alpaydin or NLP texts (e.g., Manning and Sch¨utze 1999; Jurafsky and Martin Consequently, to learn about semi-supervised learning research, one has to consult the machine-learning literature. This can be a daunting task for NLP researchers who have background in machine learning. Steven Abney’s book Learning for Linguistics targeted precisely at such researchers, aiming to provide them with a “broad and accessible presentation” of topics in semi-supervised learning. According to the preamble, the reader is assumed to have taken only an introductory course in NLP “that include[s] statistical methods—concretely the material contained in Jurafsky and Martin (2000) and Manning and Sch¨utze (1999).” Nonetheless, I agree with the author that any NLP researcher who has a solid background in machine learning is ready to “tackle the primary literature on semisupervised learning, and will probably not find this book particularly useful” (page 11). As the author promises, the book is self-contained and quite accessible to those who have little background in machine learning. In particular, of the 12 chapters in the book, three are devoted to preparatory material, including: a brief introduction to machine learning, basic unconstrained and constrained optimization techniques (e.g., gradient descent and the method of Lagrange multipliers), and relevant linear-algebra concepts (e.g., eigenvalues, eigenvectors, matrix and vector norms, 1 Although Manning and Sch¨utze (1999) and Jurafsky and Martin (2000) do discuss self-training, they do so only in the context of Yarowsky’s word sense disambiguation algorithm. Computational Linguistics Volume 34, Number 3 diagonalization). The remaining chapters focus roughly on six types of semi-supervised Self-training. introducing the self-training algorithm and its variants, the author discusses its applications in NLP and its relationship to other semi-supervised learning algorithms. Agreement-based methods. co-training algorithm, along with a theoretical analysis of its conditional independence assumption and its applications in NLP, are presented. Additionally, a random field, which penalizes disagreement among neighboring nodes, is introduced as an alternative way of enforcing agreement. Clustering algorithms. hard clustering algorithms (e.g., graph mincuts, hierarchical clustering), EM (as a soft clustering algorithm), and their role in semi-supervised learning are discussed. Boundary-oriented methods. discriminative learning algorithms, boosting and support vector machines, are introduced as a means to facilitate the discussion of their semi-supervised counterparts: co-boosting and transductive SVMs. Label propagation in graphs. graph-based approaches to semi-supervised learning, the labels of the labeled nodes are propagated to the unlabeled nodes, with the goal of maximizing the agreement of the labels of proximate nodes. The author shows that this goal is equivalent to finding a function the labeled nodes, and presents several algorithms, including the method of relaxation, for computing this function. Spectral methods. methods for semi-supervised learning can be viewed as interpolation across a partially labeled graph as described previously using a “standing wave.” The author explains the connection between such a wave and the spectrum of a matrix, and establishes the relationship of spectral clustering algorithms to other semi-supervised learners, including graph mincuts, random walks, and label propagation. The book is rich in theory and algorithms, and although it is targeted at those who lack relevant mathematical background, each theory and algorithm is presented in a rigorous manner. Another nice feature of the book is that it reveals the connection among seemingly disparate ideas. As mentioned earlier, it shows that many semi-supervised learners can in fact be viewed as self-training; also, the description of the connection between spectral clustering and other semi-supervised learners is insightful. In addition, I like the organization of the book. One reason is the presentation of co-training: Although the algorithm is presented in Chapter 2, its theoretical underpinnings are not described until Chapter 9. This enables the reader to see its applications in NLP (in Chapter 3) before going through the mathematics, which could be important for researchers who are linguistically but not mathematically oriented. Another reason 2 The presentation of the methods here does not reflect the order in which they are introduced in the book; rather, it is motivated by the book’s Section 1.3, which gives an overview of the “leading ideas” of the book. 450 Book Reviews is that the preparatory material is presented on a need-to-know basis. This allows the discussion of algorithmic ideas as soon as the reader grasps the relevant fundamentals. For instance, function optimization and basic linear algebra concepts are presented in separate chapters, with the latter being deferred to Chapter 11, right before the discussion of spectral clustering in Chapter 12. Whereas the discussion of self-training and co-training is complemented by their application to NLP problems, the same is not true for the remaining semi-supervised learners described in the book. The reader is often left to imagine the potential NLP applications of these learners, and as a consequence is unable to gain an understanding of the state of the art of semi-supervised learning for NLP. In fact, given its scarcity of NLP applications, the book perhaps does not merit its current title. It does have a rich bibliography on semi-supervised learning for NLP, but most of the references are not cited in the text. The book also lacks a discussion of the practical issues in applying the semisupervised learners. For instance, the author does not mention that in practice it is easy to choose clustering, merely describing a parameter of the clustering algorithm. As another example, when introducing the EM algorithm, the author applies it to a generative model that can be expressed in exponential form, without acknowledging that one of the most difficult issues surrounding the application of EM concerns the design of the right generative model given the data. The lack of NLP applications in the book has unfortunately enabled the author to sidestep these practical issues. On a related note, one can hardly find any discussions of the strengths and weaknesses of the semi-supervised learners in the book. This could leave the reader without the ability to choose the best learner for a given NLP problem, and is probably another undesirable consequence of the book’s reluctance to discuss NLP applications. author’s decision to focus exclusively on semi-supervised problems effectively limits the scope of the book. One consequence of this decision is that the reader may not be able to apply the EM algorithm to train a hidden Markov model for as basic as part-of-speech tagging upon completion of this book. Given the recent surge of interest in structure prediction in the NLP community, and the fact that co-training and semi-supervised EM have been applied to structure-prediction problems such as statistical parsing and part-of-speech tagging, the book’s sole focus on classification problem is perhaps one of its weaknesses. There are a few occasions on which the reader might not get a complete picture of the capability of an algorithm. For instance, the reader might think that spectral methods can be applied only to binary classification tasks, owing to the book’s exclusive focus on such tasks in its discussion of spectral clustering. Similarly for the treatment of support vector machines: The reader may get the impression that SVMs cannot be used to learn non-linear functions, as the discussion of kernels is deliberately omitted due to their irrelevance to transductive learning. Although it is important to keep the presentation focused, I believe that the author could easily have removed potential confusions by explicitly stating the full capability of an algorithm and referring the reader to the relevant papers for details. Given the rapid growth of semi-supervised learning research in the past decade, there is currently a need for a broad and accessible reference to this area of research. Abney’s book serves this purpose in spite of the aforementioned weaknesses, and I believe that it is a useful starting point for any non–machine-learning experts who intend to apply semi-supervised learning techniques to their research. As someone who has some prior knowledge of semi-supervised learning, I still find this book insightful: It reveals deep connections among apparently disparate ideas. If I were to teach a course 451 Computational Linguistics Volume 34, Number 3 on semi-supervised learning for NLP, I would undoubtedly use this book as a primary reference.</abstract>
<title confidence="0.763907">References</title>
<author confidence="0.803872">to</author>
<affiliation confidence="0.949686">The MIT Press,</affiliation>
<address confidence="0.954402">Cambridge, MA.</address>
<note confidence="0.820415352941177">Collins, Michael and Yoram Singer. 1999. Unsupervised models for named entity In of the 1999 Joint Conference on Empirical Methods in Natural Language Processing and Very pages 100–110, College Park, MD. Jurafsky, Daniel and James H. Martin. 2000. and Language Prentice Hall, Upper Saddle River, NJ. Manning, Christopher D. and Hinrich 1999. of Statistical Language The MIT Press, Cambridge, MA. Tom M.˙1997. McGraw Hill, Columbus, OH. Pereira, Fernando and Yves Schabes.1992. Inside-outside reestimation from partially corpora. In of the 30th Annual Meeting of the Association for pages 128–135, Newark, DE. Yarowsky, David. 1995. Unsupervised word sense disambiguation rivaling supervised In of the 33rd Annual Meeting of the Association for pages 189–196, Cambridge, MA. Ng an assistant professor in the Department of Computer Science at the University of Texas at Dallas. He is also affiliated with the university’s Human Language Technology Research Institute, where he conducts research on statistical natural language processing and teaches undergraduate and graduate courses in machine learning. Ng’s address is: Department of Computer Science, University of Texas at Dallas, Richardson, TX 75080-0688; e-mail: vince@hlt.utdallas.edu. 452</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ethem Alpaydin</author>
</authors>
<title>Introduction to Machine Learning.</title>
<date>2004</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1113" citStr="Alpaydin 2004" startWordPosition="156" endWordPosition="157">ameter estimation procedures such as the EM algorithm and its variants since the beginning of the statistical revolution in NLP (e.g., Pereira and Schabes 1992). Unlabeled data has also been used to improve supervised learning procedures, the most notable examples being the successful applications of self-training and co-training to word sense disambiguation (Yarowsky 1995) and named entity classification (Collins and Singer 1999). Despite its increasing importance, semi-supervised learning is not a topic that is typically discussed in introductory machine learning texts (e.g., Mitchell 1997; Alpaydin 2004) or NLP texts (e.g., Manning and Sch¨utze 1999; Jurafsky and Martin 2000).1 Consequently, to learn about semi-supervised learning research, one has to consult the machine-learning literature. This can be a daunting task for NLP researchers who have little background in machine learning. Steven Abney’s book Semisupervised Learning for Computational Linguistics is targeted precisely at such researchers, aiming to provide them with a “broad and accessible presentation” of topics in semi-supervised learning. According to the preamble, the reader is assumed to have taken only an introductory course</context>
</contexts>
<marker>Alpaydin, 2004</marker>
<rawString>Alpaydin, Ethem. 2004. Introduction to Machine Learning. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proceedings of the 1999 Joint Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>100--110</pages>
<location>College Park, MD.</location>
<contexts>
<context position="933" citStr="Collins and Singer 1999" startWordPosition="130" endWordPosition="133">University of Texas at Dallas Semi-supervised learning is by no means an unfamiliar concept to natural language processing researchers. Labeled data has been used to improve unsupervised parameter estimation procedures such as the EM algorithm and its variants since the beginning of the statistical revolution in NLP (e.g., Pereira and Schabes 1992). Unlabeled data has also been used to improve supervised learning procedures, the most notable examples being the successful applications of self-training and co-training to word sense disambiguation (Yarowsky 1995) and named entity classification (Collins and Singer 1999). Despite its increasing importance, semi-supervised learning is not a topic that is typically discussed in introductory machine learning texts (e.g., Mitchell 1997; Alpaydin 2004) or NLP texts (e.g., Manning and Sch¨utze 1999; Jurafsky and Martin 2000).1 Consequently, to learn about semi-supervised learning research, one has to consult the machine-learning literature. This can be a daunting task for NLP researchers who have little background in machine learning. Steven Abney’s book Semisupervised Learning for Computational Linguistics is targeted precisely at such researchers, aiming to provi</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>Collins, Michael and Yoram Singer. 1999. Unsupervised models for named entity classification. In Proceedings of the 1999 Joint Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 100–110, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>James H Martin</author>
</authors>
<title>Speech and Language Processing. Prentice Hall, Upper Saddle River,</title>
<date>2000</date>
<location>NJ.</location>
<contexts>
<context position="1186" citStr="Jurafsky and Martin 2000" startWordPosition="166" endWordPosition="169">variants since the beginning of the statistical revolution in NLP (e.g., Pereira and Schabes 1992). Unlabeled data has also been used to improve supervised learning procedures, the most notable examples being the successful applications of self-training and co-training to word sense disambiguation (Yarowsky 1995) and named entity classification (Collins and Singer 1999). Despite its increasing importance, semi-supervised learning is not a topic that is typically discussed in introductory machine learning texts (e.g., Mitchell 1997; Alpaydin 2004) or NLP texts (e.g., Manning and Sch¨utze 1999; Jurafsky and Martin 2000).1 Consequently, to learn about semi-supervised learning research, one has to consult the machine-learning literature. This can be a daunting task for NLP researchers who have little background in machine learning. Steven Abney’s book Semisupervised Learning for Computational Linguistics is targeted precisely at such researchers, aiming to provide them with a “broad and accessible presentation” of topics in semi-supervised learning. According to the preamble, the reader is assumed to have taken only an introductory course in NLP “that include[s] statistical methods—concretely the material cont</context>
<context position="2666" citStr="Jurafsky and Martin (2000)" startWordPosition="379" endWordPosition="382">ill probably not find this book particularly useful” (page 11). As the author promises, the book is self-contained and quite accessible to those who have little background in machine learning. In particular, of the 12 chapters in the book, three are devoted to preparatory material, including: a brief introduction to machine learning, basic unconstrained and constrained optimization techniques (e.g., gradient descent and the method of Lagrange multipliers), and relevant linear-algebra concepts (e.g., eigenvalues, eigenvectors, matrix and vector norms, 1 Although Manning and Sch¨utze (1999) and Jurafsky and Martin (2000) do discuss self-training, they do so only in the context of Yarowsky’s word sense disambiguation algorithm. Computational Linguistics Volume 34, Number 3 diagonalization). The remaining chapters focus roughly on six types of semi-supervised learning methods:2 • Self-training. After introducing the self-training algorithm and its variants, the author discusses its applications in NLP and its relationship to other semi-supervised learning algorithms. • Agreement-based methods. The co-training algorithm, along with a theoretical analysis of its conditional independence assumption and its applica</context>
</contexts>
<marker>Jurafsky, Martin, 2000</marker>
<rawString>Jurafsky, Daniel and James H. Martin. 2000. Speech and Language Processing. Prentice Hall, Upper Saddle River, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Manning, Christopher D. and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Mitchell</author>
</authors>
<title>Tom M.˙1997. Machine Learning.</title>
<publisher>McGraw Hill,</publisher>
<location>Columbus, OH.</location>
<marker>Mitchell, </marker>
<rawString>Mitchell, Tom M.˙1997. Machine Learning. McGraw Hill, Columbus, OH.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Fernando Pereira</author>
<author>Yves Schabes 1992</author>
</authors>
<title>Inside-outside reestimation from partially bracketed corpora.</title>
<booktitle>In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>128--135</pages>
<location>Newark, DE.</location>
<marker>Pereira, 1992, </marker>
<rawString>Pereira, Fernando and Yves Schabes.1992. Inside-outside reestimation from partially bracketed corpora. In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, pages 128–135, Newark, DE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="875" citStr="Yarowsky 1995" startWordPosition="124" endWordPosition="125">88-559-7, $79.95, £44.99 Reviewed by Vincent Ng University of Texas at Dallas Semi-supervised learning is by no means an unfamiliar concept to natural language processing researchers. Labeled data has been used to improve unsupervised parameter estimation procedures such as the EM algorithm and its variants since the beginning of the statistical revolution in NLP (e.g., Pereira and Schabes 1992). Unlabeled data has also been used to improve supervised learning procedures, the most notable examples being the successful applications of self-training and co-training to word sense disambiguation (Yarowsky 1995) and named entity classification (Collins and Singer 1999). Despite its increasing importance, semi-supervised learning is not a topic that is typically discussed in introductory machine learning texts (e.g., Mitchell 1997; Alpaydin 2004) or NLP texts (e.g., Manning and Sch¨utze 1999; Jurafsky and Martin 2000).1 Consequently, to learn about semi-supervised learning research, one has to consult the machine-learning literature. This can be a daunting task for NLP researchers who have little background in machine learning. Steven Abney’s book Semisupervised Learning for Computational Linguistics </context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>Yarowsky, David. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 189–196, Cambridge, MA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Vincent</author>
</authors>
<title>Ng is an assistant professor in the Department of Computer Science at the University of Texas at Dallas. He is also affiliated with the university’s Human Language Technology Research Institute, where he conducts research on statistical natural language processing and teaches undergraduate and graduate courses in machine learning. Ng’s address is:</title>
<institution>Department of Computer Science, University of Texas at Dallas,</institution>
<location>Richardson, TX</location>
<note>75080-0688; e-mail: vince@hlt.utdallas.edu.</note>
<marker>Vincent, </marker>
<rawString>Vincent Ng is an assistant professor in the Department of Computer Science at the University of Texas at Dallas. He is also affiliated with the university’s Human Language Technology Research Institute, where he conducts research on statistical natural language processing and teaches undergraduate and graduate courses in machine learning. Ng’s address is: Department of Computer Science, University of Texas at Dallas, Richardson, TX 75080-0688; e-mail: vince@hlt.utdallas.edu.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>