<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000216">
<title confidence="0.983081">
Unsupervised Learning of Narrative Event Chains
</title>
<author confidence="0.997207">
Nathanael Chambers and Dan Jurafsky
</author>
<affiliation confidence="0.9873105">
Department of Computer Science
Stanford University
</affiliation>
<address confidence="0.931524">
Stanford, CA 94305
</address>
<email confidence="0.999767">
{natec,jurafsky}@stanford.edu
</email>
<sectionHeader confidence="0.998606" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99962575">
Hand-coded scripts were used in the 1970-80s
as knowledge backbones that enabled infer-
ence and other NLP tasks requiring deep se-
mantic knowledge. We propose unsupervised
induction of similar schemata called narrative
event chains from raw newswire text.
A narrative event chain is a partially ordered
set of events related by a common protago-
nist. We describe a three step process to learn-
ing narrative event chains. The first uses unsu-
pervised distributional methods to learn narra-
tive relations between events sharing corefer-
ring arguments. The second applies a tempo-
ral classifier to partially order the connected
events. Finally, the third prunes and clusters
self-contained chains from the space of events.
We introduce two evaluations: the narrative
cloze to evaluate event relatedness, and an or-
der coherence task to evaluate narrative order.
We show a 36% improvement over baseline
for narrative prediction and 25% for temporal
coherence.
tate learning, and thus this paper addresses the three
tasks of chain induction: narrative event induction,
temporal ordering of events and structured selection
(pruning the event space into discrete sets).
Learning these prototypical schematic sequences
of events is important for rich understanding of text.
Scripts were central to natural language understand-
ing research in the 1970s and 1980s for proposed
tasks such as summarization, coreference resolu-
tion and question answering. For example, Schank
and Abelson (1977) proposed that understanding
text about restaurants required knowledge about the
Restaurant Script, including the participants (Cus-
tomer, Waiter, Cook, Tables, etc.), the events consti-
tuting the script (entering, sitting down, asking for
menus, etc.), and the various preconditions, order-
ing, and results of each of the constituent actions.
Consider these two distinct narrative chains.
</bodyText>
<sectionHeader confidence="0.497130666666667" genericHeader="keywords">
accused X
X claimed
X argued
</sectionHeader>
<figure confidence="0.9760778">
W joined
W served
W oversaw
dismissed X
W resigned
</figure>
<sectionHeader confidence="0.99033" genericHeader="method">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999825590909091">
This paper induces a new representation of struc-
tured knowledge called narrative event chains (or
narrative chains). Narrative chains are partially or-
dered sets of events centered around a common pro-
tagonist. They are related to structured sequences of
participants and events that have been called scripts
(Schank and Abelson, 1977) or Fillmorean frames.
These participants and events can be filled in and
instantiated in a particular text situation to draw in-
ferences. Chains focus on a single actor to facili-
It would be useful for question answering or tex-
tual entailment to know that ‘X denied ’ is also a
likely event in the left chain, while ‘ replaces W’
temporally follows the right. Narrative chains (such
as Firing of Employee or Executive Resigns) offer
the structure and power to directly infer these new
subevents by providing critical background knowl-
edge. In part due to its complexity, automatic in-
duction has not been addressed since the early non-
statistical work of Mooney and DeJong (1985).
The first step to narrative induction uses an entity-
based model for learning narrative relations by fol-
</bodyText>
<page confidence="0.980059">
789
</page>
<note confidence="0.714172">
Proceedings of ACL-08: HLT, pages 789–797,
</note>
<page confidence="0.537555">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.99959085">
lowing a protagonist. As a narrative progresses
through a series of events, each event is character-
ized by the grammatical role played by the protag-
onist, and by the protagonist’s shared connection to
surrounding events. Our algorithm is an unsuper-
vised distributional learning approach that uses core-
ferring arguments as evidence of a narrative relation.
We show, using a new evaluation task called narra-
tive cloze, that our protagonist-based method leads
to better induction than a verb-only approach.
The next step is to order events in the same nar-
rative chain. We apply work in the area of temporal
classification to create partial orders of our learned
events. We show, using a coherence-based evalua-
tion of temporal ordering, that our partial orders lead
to better coherence judgements of real narrative in-
stances extracted from documents.
Finally, the space of narrative events and temporal
orders is clustered and pruned to create discrete sets
of narrative chains.
</bodyText>
<sectionHeader confidence="0.996136" genericHeader="method">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.991619847457627">
While previous work hasn’t focused specifically on
learning narratives1, our work draws from two lines
of research in summarization and anaphora resolu-
tion. In summarization, topic signatures are a set
of terms indicative of a topic (Lin and Hovy, 2000).
They are extracted from hand-sorted (by topic) sets
of documents using log-likelihood ratios. These
terms can capture some narrative relations, but the
model requires topic-sorted training data.
Bean and Riloff (2004) proposed the use of
caseframe networks as a kind of contextual role
knoweldge for anaphora resolution. A case-
frame is a verb/event and a semantic role (e.g.
&lt;patient&gt; kidnapped). Caseframe networks are re-
lations between caseframes that may represent syn-
onymy (&lt;patient&gt; kidnapped and &lt;patient&gt; ab-
ducted) or related events (&lt;patient&gt; kidnapped and
&lt;patient&gt; released). Bean and Riloff learn these
networks from two topic-specific texts and apply
them to the problem of anaphora resolution. Our
work can be seen as an attempt to generalize the in-
tuition of caseframes (finding an entire set of events
1We analyzed FrameNet (Baker et al., 1998) for insight, but
found that very few of the frames are event sequences of the
type characterizing narratives and scripts.
rather than just pairs of related frames) and apply it
to a different task (finding a coherent structured nar-
rative in non-topic-specific text).
More recently, Brody (2007) proposed an ap-
proach similar to caseframes that discovers high-
level relatedness between verbs by grouping verbs
that share the same lexical items in subject/object
positions. He calls these shared arguments anchors.
Brody learns pairwise relations between clusters of
related verbs, similar to the results with caseframes.
A human evaluation of these pairs shows an im-
provement over baseline. This and previous case-
frame work lend credence to learning relations from
verbs with common arguments.
We also draw from lexical chains (Morris and
Hirst, 1991), indicators of text coherence from word
overlap/similarity. We use a related notion of protag-
onist overlap to motivate narrative chain learning.
Work on semantic similarity learning such as
Chklovski and Pantel (2004) also automatically
learns relations between verbs. We use similar dis-
tributional scoring metrics, but differ with our use
of a protagonist as the indicator of relatedness. We
also use typed dependencies and the entire space of
events for similarity judgements, rather than only
pairwise lexical decisions.
Finally, Fujiki et al. (2003) investigated script ac-
quisition by extracting the 41 most frequent pairs of
events from the first paragraph of newswire articles,
using the assumption that the paragraph’s textual or-
der follows temporal order. Our model, by contrast,
learns entire event chains, uses more sophisticated
probabilistic measures, and uses temporal ordering
models instead of relying on document order.
</bodyText>
<sectionHeader confidence="0.960304" genericHeader="method">
3 The Narrative Chain Model
</sectionHeader>
<subsectionHeader confidence="0.990664">
3.1 Definition
</subsectionHeader>
<bodyText confidence="0.9998633">
Our model is inspired by Centering (Grosz et al.,
1995) and other entity-based models of coherence
(Barzilay and Lapata, 2005) in which an entity is in
focus through a sequence of sentences. We propose
to use this same intuition to induce narrative chains.
We assume that although a narrative has several
participants, there is a central actor who character-
izes a narrative chain: the protagonist. Narrative
chains are thus structured by the protagonist’s gram-
matical roles in the events. In addition, narrative
</bodyText>
<page confidence="0.991337">
790
</page>
<bodyText confidence="0.999951375">
events are ordered by some theory of time. This pa-
per describes a partial ordering with the before (no
overlap) relation.
Our task, therefore, is to learn events that consti-
tute narrative chains. Formally, a narrative chain
is a partially ordered set of narrative events that
share a common actor. A narrative event is a tu-
ple of an event (most simply a verb) and its par-
ticipants, represented as typed dependencies. Since
we are focusing on a single actor in this study, a
narrative event is thus a tuple of the event and the
typed dependency of the protagonist: (event, depen-
dency). A narrative chain is a set of narrative events
{e1, e2, ..., en}, where n is the size of the chain, and
a relation B(ei, ej) that is true if narrative event ei
occurs strictly before ej in time.
</bodyText>
<subsectionHeader confidence="0.997338">
3.2 The Protagonist
</subsectionHeader>
<bodyText confidence="0.9999374">
The notion of a protagonist motivates our approach
to narrative learning. We make the following as-
sumption of narrative coherence: verbs sharing
coreferring arguments are semantically connected
by virtue of narrative discourse structure. A single
document may contain more than one narrative (or
topic), but the narrative assumption states that a se-
ries of argument-sharing verbs is more likely to par-
ticipate in a narrative chain than those not sharing.
In addition, the narrative approach captures gram-
matical constraints on narrative coherence. Simple
distributional learning might discover that the verb
push is related to the verb fall, but narrative learning
can capture additional facts about the participants,
specifically, that the object or patient of the push is
the subject or agent of the fall.
Each focused protagonist chain offers one per-
spective on a narrative, similar to the multiple per-
spectives on a commercial transaction event offered
by buy and sell.
</bodyText>
<subsectionHeader confidence="0.998994">
3.3 Partial Ordering
</subsectionHeader>
<bodyText confidence="0.999995">
A narrative chain, by definition, includes a partial
ordering of events. Early work on scripts included
ordering constraints with more complex precondi-
tions and side effects on the sequence of events. This
paper presents work toward a partial ordering and
leaves logical constraints as future work. We focus
on the before relation, but the model does not pre-
clude advanced theories of temporal order.
</bodyText>
<sectionHeader confidence="0.94981" genericHeader="method">
4 Learning Narrative Relations
</sectionHeader>
<bodyText confidence="0.9956919375">
Our first model learns basic information about a
narrative chain: the protagonist and the constituent
subevents, although not their ordering. For this we
need a metric for the relation between an event and
a narrative chain.
Pairwise relations between events are first ex-
tracted unsupervised. A distributional score based
on how often two events share grammatical argu-
ments (using pointwise mutual information) is used
to create this pairwise relation. Finally, a global nar-
rative score is built such that all events in the chain
provide feedback on the event in question (whether
for inclusion or for decisions of inference).
Given a list of observed verb/dependency counts,
we approximate the pointwise mutual information
(PMI) by:
</bodyText>
<equation confidence="0.951250333333333">
P(e(w, d), e(v, g))
pmi(e(w, d), e(v, g)) = log (1)
P(e(w, d))P(e(v, g))
</equation>
<bodyText confidence="0.9938895">
where e(w, d) is the verb/dependency pair w and d
(e.g. e(push,subject)). The numerator is defined by:
</bodyText>
<equation confidence="0.819622">
C(e(w, d), e(v, g))
P(e(w, d), e(v, g)) =
E.,, Ed,f C(e(x, d), e(y, f))
(2)
</equation>
<bodyText confidence="0.9994891875">
where C(e(x, d), e(y, f)) is the number of times the
two events e(x, d) and e(y, f) had a coreferring en-
tity filling the values of the dependencies d and f.
We also adopt the ‘discount score’ to penalize low
occuring words (Pantel and Ravichandran, 2004).
Given the debate over appropriate metrics for dis-
tributional learning, we also experimented with the
t-test. Our experiments found that PMI outperforms
the t-test on this task by itself and when interpolated
together using various mixture weights.
Once pairwise relation scores are calculated, a
global narrative score can then be built such that all
events provide feedback on the event in question.
For instance, given all narrative events in a docu-
ment, we can find the next most likely event to occur
by maximizing:
</bodyText>
<equation confidence="0.913068">
pmi(ei, fj) (3)
</equation>
<bodyText confidence="0.9995275">
where n is the number of events in our chain and
ei is the ith event. m is the number of events f in
our training corpus. A ranked list of guesses can be
built from this summation and we hypothesize that
</bodyText>
<equation confidence="0.490652">
E
i=0
max
j:0&lt;j&lt;M
</equation>
<page confidence="0.850237">
791
</page>
<figureCaption confidence="0.997375">
Figure 1: Three narrative events and the six most likely
events to include in the same chain.
</figureCaption>
<bodyText confidence="0.992695">
the more events in our chain, the more informed our
ranked output. An example of a chain with 3 events
and the top 6 ranked guesses is given in figure 1.
</bodyText>
<subsectionHeader confidence="0.975697">
4.1 Evaluation Metric: Narrative Cloze
</subsectionHeader>
<bodyText confidence="0.999892">
The cloze task (Taylor, 1953) is used to evaluate a
system (or human) for language proficiency by re-
moving a random word from a sentence and having
the system attempt to fill in the blank (e.g. I forgot
to the waitress for the good service). Depend-
ing on the type of word removed, the test can evalu-
ate syntactic knowledge as well as semantic. Deyes
(1984) proposed an extended task, discourse cloze,
to evaluate discourse knowledge (removing phrases
that are recoverable from knowledge of discourse re-
lations like contrast and consequence).
We present a new cloze task that requires narra-
tive knowledge to solve, the narrative cloze. The
narrative cloze is a sequence of narrative events in a
document from which one event has been removed.
The task is to predict the missing verb and typed de-
pendency. Take this example text about American
football with McCann as the protagonist:
</bodyText>
<listItem confidence="0.998171333333333">
1. McCann threw two interceptions early.
2. Toledo pulled McCann aside and told him he’d start.
3. McCann quickly completed his first two passes.
</listItem>
<bodyText confidence="0.9998529">
These clauses are represented in the narrative model
as five events: (threw subject), (pulled object),
(told object), (start subject), (completed subject).
These verb/dependency events make up a narrative
cloze model. We could remove (threw subject) and
use the remaining four events to rank this missing
event. Removing a single such pair to be filled in au-
tomatically allows us to evaluate a system’s knowl-
edge of narrative relations and coherence. We do not
claim this cloze task to be solvable even by humans,
</bodyText>
<figure confidence="0.4755472">
New York Times Editorial
occupied subj brought subj rejecting subj
projects subj met subj appeared subj
offered subj voted pp for offer subj
thinks subj
</figure>
<figureCaption confidence="0.9995085">
Figure 2: One of the 69 test documents, containing 10
narrative events. The protagonist is President Bush.
</figureCaption>
<bodyText confidence="0.98689">
but rather assert it as a comparative measure to eval-
uate narrative knowledge.
</bodyText>
<subsectionHeader confidence="0.840531">
4.2 Narrative Cloze Experiment
</subsectionHeader>
<bodyText confidence="0.99975624137931">
We use years 1994-2004 (1,007,227 documents) of
the Gigaword Corpus (Graff, 2002) for training2.
We parse the text into typed dependency graphs
with the Stanford Parser (de Marneffe et al., 2006)3,
recording all verbs with subject, object, or preposi-
tional typed dependencies. We use the OpenNLP4
coreference engine to resolve the entity mentions.
For each document, the verb pairs that share core-
ferring entities are recorded with their dependency
types. Particles are included with the verb.
We used 10 news stories from the 1994 section
of the corpus for development. The stories were
hand chosen to represent a range of topics such as
business, sports, politics, and obituaries. We used
69 news stories from the 2001 (year selected ran-
domly) section of the corpus for testing (also re-
moved from training). The test set documents were
randomly chosen and not preselected for a range of
topics. From each document, the entity involved
in the most events was selected as the protagonist.
For this evaluation, we only look at verbs. All
verb clauses involving the protagonist are manu-
ally extracted and translated into the narrative events
(verb,dependency). Exceptions that are not included
are verbs in headlines, quotations (typically not part
of a narrative), “be” properties (e.g. john is happy),
modifying verbs (e.g. hurried to leave, only leave is
used), and multiple instances of one event.
The original test set included 100 documents, but
</bodyText>
<footnote confidence="0.9886945">
2The document count does not include duplicate news sto-
ries. We found up to 18% of the corpus are duplications, mostly
AP reprints. We automatically found these by matching the first
two paragraphs of each document, removing exact matches.
3http://nlp.stanford.edu/software/lex-parser.shtml
4http://opennlp.sourceforge.net
</footnote>
<table confidence="0.9985535">
Known events:
(pleaded subj), (admits subj), (convicted obj)
Likely Events:
sentenced obj 0.89 indicted obj 0.74
paroled obj 0.76 fined obj 0.73
fired obj 0.75 denied subj 0.73
</table>
<page confidence="0.991057">
792
</page>
<bodyText confidence="0.997068">
those without a narrative chain at least five events in
length were removed, leaving 69 documents. Most
of the removed documents were not stories, but gen-
res such as interviews and cooking recipes. An ex-
ample of an extracted chain is shown in figure 2.
We evalute with Narrative Cloze using leave-one-
out cross validation, removing one event and using
the rest to generate a ranked list of guesses. The test
dataset produces 740 cloze tests (69 narratives with
740 events). After generating our ranked guesses,
the position of the correct event is averaged over all
740 tests for the final score. We penalize unseen
events by setting their ranked position to the length
of the guess list (ranging from 2k to 15k).
Figure 1 is an example of a ranked guess list for a
short chain of three events. If the original document
contained (fired obj), this cloze test would score 3.
</bodyText>
<sectionHeader confidence="0.520928" genericHeader="method">
4.2.1 Baseline
</sectionHeader>
<bodyText confidence="0.999994625">
We want to measure the utility of the protago-
nist and the narrative coherence assumption, so our
baseline learns relatedness strictly based upon verb
co-occurence. The PMI is then defined as between
all occurrences of two verbs in the same document.
This baseline evaluation is verb only, as dependen-
cies require a protagonist to fill them.
After initial evaluations, the baseline was per-
forming very poorly due to the huge amount of data
involved in counting all possible verb pairs (using a
protagonist vastly reduces the number). We exper-
imented with various count cutoffs to remove rare
occurring pairs of verbs. The final results use a base-
line where all pairs occurring less than 10 times in
the training data are removed.
Since the verb-only baseline does not use typed
dependencies, our narrative model cannot directly
compare to this abstracted approach. We thus mod-
ified the narrative model to ignore typed dependen-
cies, but still count events with shared arguments.
Thus, we calculate the PMI across verbs that share
arguments. This approach is called Protagonist.
The full narrative model that includes the grammat-
ical dependencies is called Typed Deps.
</bodyText>
<sectionHeader confidence="0.855354" genericHeader="method">
4.2.2 Results
</sectionHeader>
<bodyText confidence="0.930837">
Experiments with varying sizes of training data
are presented in figure 3. Each ranked list of
candidate verbs for the missing event in Base-
</bodyText>
<figure confidence="0.947316333333333">
Narrative Cloze Test
1995 1996 1997 1998 1999 2000 2001 2002 2003 2004
Training Data from 1994—X
</figure>
<figureCaption confidence="0.997329666666667">
Figure 3: Results with varying sizes of training data. Year
2003 is not explicitly shown because it has an unusually
small number of documents compared to other years.
</figureCaption>
<bodyText confidence="0.986573590909091">
line/Protagonist contained approximately 9 thou-
sand candidates. Of the 740 cloze tests, 714 of the
removed events were present in their respective list
of guesses. This is encouraging as only 3.5% of the
events are unseen (or do not meet cutoff thresholds).
When all training data is used (1994-2004), the
average ranked position is 1826 for Baseline and
1160 for Protagonist (1 being most confident). The
Baseline performs better at first (years 1994-5), but
as more data is seen, the Baseline worsens while
the Protagonist improves. This verb-only narrative
model shows a 36.5% improvement over the base-
line trained on all years. Results from the full Typed
Deps model, not comparable to the baseline, paral-
lel the Protagonist results, improving as more data is
seen (average ranked position of 1908 with all the
training data). We also ran the experiment with-
out OpenNLP coreference, and instead used exact
and substring matching for coreference resolution.
This showed a 5.7% decrease in the verb-only re-
sults. These results show that a protagonist greatly
assists in narrative judgements.
</bodyText>
<sectionHeader confidence="0.986281" genericHeader="method">
5 Ordering Narrative Events
</sectionHeader>
<bodyText confidence="0.9995704">
The model proposed in the previous section is de-
signed to learn the major subevents in a narrative
chain, but not how these events are ordered. In this
section we extend the model to learn a partial tem-
poral ordering of the events.
</bodyText>
<figure confidence="0.999599545454545">
Ranked Position
3000
2500
2000
1500
1000
500
0
Baseline
Protagonist
Typed Deps
</figure>
<page confidence="0.997541">
793
</page>
<bodyText confidence="0.999989230769231">
There are a number of algorithms for determining
the temporal relationship between two events (Mani
et al., 2006; Lapata and Lascarides, 2006; Cham-
bers et al., 2007), many of them trained on the Time-
Bank Corpus (Pustejovsky et al., 2003) which codes
events and their temporal relationships. The cur-
rently highest performing of these on raw data is the
model of temporal labeling described in our previ-
ous work (Chambers et al., 2007). Other approaches
have depended on hand tagged features.
Chambers et al. (2007) shows 59.4% accuracy on
the classification task for six possible relations be-
tween pairs of events: before, immediately-before,
included-by, simultaneous, begins and ends. We fo-
cus on the before relation because the others are
less relevant to our immediate task. We combine
immediately-before with before, and merge the other
four relations into an other category. At the binary
task of determining if one event is before or other,
we achieve 72.1% accuracy on Timebank.
The above approach is a two-stage machine learn-
ing architecture. In the first stage, the model uses
supervised machine learning to label temporal at-
tributes of events, including tense, grammatical as-
pect, and aspectual class. This first stage classi-
fier relies on features such as neighboring part of
speech tags, neighboring auxiliaries and modals, and
WordNet synsets. We use SVMs (Chambers et al.
(2007) uses Naive Bayes) and see minor perfor-
mance boosts on Timebank. These imperfect clas-
sifications, combined with other linguistic features,
are then used in a second stage to classify the tem-
poral relationship between two events. Other fea-
tures include event-event syntactic properties such
as the syntactic dominance relations between the
two events, as well as new bigram features of tense,
aspect and class (e.g. “present past” if the first event
is in the present, and the second past), and whether
the events occur in the same or different sentences.
</bodyText>
<subsectionHeader confidence="0.998323">
5.1 Training a Temporal Classifier
</subsectionHeader>
<bodyText confidence="0.99974825">
We use the entire Timebank Corpus as super-
vised training data, condensing the before and
immediately-before relations into one before rela-
tion. The remaining relations are merged into other.
The vast majority of potential event pairs in Time-
bank are unlabeled. These are often none relations
(events that have no explicit relation) or as is of-
ten the case, overlap relations where the two events
have no Timebank-defined ordering but overlap in
time. Even worse, many events do have an order-
ing, but they were not tagged by the human annota-
tors. This could be due to the overwhelming task of
temporal annotation, or simply because some event
orderings are deemed more important than others in
understanding the document. We consider all un-
tagged relations as other, and experiment with in-
cluding none, half, and all of them in training.
Taking a cue from Mani et al. (2006), we also
increased Timebank’s size by applying transitivity
rules to the hand labeled data. The following is an
example of the applied transitive rule:
if run BEFORE fall and fall BEFORE injured
then run BEFORE injured
This increases the number of relations from 37519
to 45619. Perhaps more importantly for our task,
of all the added relations, the before relation is
added the most. We experimented with original vs.
expanded Timebank and found the expanded per-
formed slightly worse. The decline may be due to
poor transitivity additions, as several Timebank doc-
uments contain inconsistent labelings. All reported
results are from training without transitivity.
</bodyText>
<subsectionHeader confidence="0.999881">
5.2 Temporal Classifier in Narrative Chains
</subsectionHeader>
<bodyText confidence="0.999978">
We classify the Gigaword Corpus in two stages,
once for the temporal features on each event (tense,
grammatical aspect, aspectual class), and once be-
tween all pairs of events that share arguments. This
allows us to classify the before/other relations be-
tween all potential narrative events.
The first stage is trained on Timebank, and the
second is trained using the approach described
above, varying the size of the none training rela-
tions. Each pair of events in a gigaword document
that share a coreferring argument is treated as a sepa-
rate ordering classification task. We count the result-
ing number of labeled before relations between each
verb/dependency pair. Processing the entire corpus
produces a database of event pair counts where con-
fidence of two generic events A and B can be mea-
sured by comparing how many before labels have
been seen versus their inverted order B and A5.
</bodyText>
<footnote confidence="0.854472">
5Note that we train with the before relation, and so transpos-
ing two events is similar to classifying the after relation.
</footnote>
<page confidence="0.992562">
794
</page>
<subsectionHeader confidence="0.994246">
5.3 Temporal Evaluation
</subsectionHeader>
<bodyText confidence="0.99999703125">
We want to evaluate temporal order at the narrative
level, across all events within a chain. We envision
narrative chains being used for tasks of coherence,
among other things, and so it is desired to evaluate
temporal decisions within a coherence framework.
Along these lines, our test set uses actual narrative
chains from documents, hand labeled for a partial
ordering. We evaluate coherence of these true chains
against a random ordering. The task is thus deciding
which of the two chains is most coherent, the orig-
inal or the random (baseline 50%)? We generated
up to 300 random orderings for each test document,
averaging the accuracy across all.
Our evaluation data is the same 69 documents
used in the test set for learning narrative relations.
The chain from each document is hand identified
and labeled for a partial ordering using only the be-
fore relation. Ordering was done by the authors and
all attempts were made to include every before re-
lation that exists in the document, or that could be
deduced through transitivity rules. Figure 4 shows
an example and its full reversal, although the evalu-
ation uses random orderings. Each edge is a distinct
before relation and is used in the judgement score.
The coherence score for a partially ordered nar-
rative chain is the sum of all the relations that our
classified corpus agrees with, weighted by how cer-
tain we are. If the gigaword classifications disagree,
a weighted negative score is given. Confidence is
based on a logarithm scale of the difference between
the counts of before and after classifications. For-
mally, the score is calculated as the following:
</bodyText>
<equation confidence="0.8903795">
log(D(x, y)) if x0y and B(x, y) &gt; B(y, x)
−log(D(x, y)) if x0y and B(y, x) &gt; B(x, y)
−log(D(x, y)) if !x0y &amp; !y0x &amp; D(x, y) &gt; 0
0 otherwise
</equation>
<bodyText confidence="0.994073">
where E is the set of all event pairs, B(i, j) is how
many times we classified events i and j as before in
Gigaword, and D(i, j) _ |B(i, j) − B(j,i)|. The
relation i0j indicates that i is temporally before j.
</bodyText>
<sectionHeader confidence="0.851165" genericHeader="method">
5.4 Results
</sectionHeader>
<bodyText confidence="0.9999536">
Out approach gives higher scores to orders that co-
incide with the pairwise orderings classified in our
gigaword training data. The results are shown in fig-
ure 5. Of the 69 chains, 6 did not have any ordered
events and were removed from the evaluation. We
</bodyText>
<figureCaption confidence="0.997133">
Figure 4: A narrative chain and its reverse order.
</figureCaption>
<table confidence="0.8779035">
All &gt; 6 &gt; 10
correct 8086 75% 7603 78% 6307 89%
incorrect 1738 1493 619
tie 931 627 160
</table>
<figureCaption confidence="0.987747333333333">
Figure 5: Results for choosing the correct ordered chain.
(&gt; 10) means there were at least 10 pairs of ordered
events in the chain.
</figureCaption>
<bodyText confidence="0.999986307692308">
generated (up to) 300 random orderings for each of
the remaining 63. We report 75.2% accuracy, but 22
of the 63 had 5 or fewer pairs of ordered events. Fig-
ure 5 therefore shows results from chains with more
than 5 pairs, and also 10 or more. As we would
hope, the accuracy improves the larger the ordered
narrative chain. We achieve 89.0% accuracy on the
24 documents whose chains most progress through
time, rather than chains that are difficult to order
with just the before relation.
Training without none relations resulted in high
recall for before decisions. Perhaps due to data spar-
sity, this produces our best results as reported above.
</bodyText>
<sectionHeader confidence="0.999169" genericHeader="method">
6 Discrete Narrative Event Chains
</sectionHeader>
<bodyText confidence="0.999951583333333">
Up till this point, we have learned narrative relations
across all possible events, including their temporal
order. However, the discrete lists of events for which
Schank scripts are most famous have not yet been
constructed.
We intentionally did not set out to reproduce ex-
plicit self-contained scripts in the sense that the
‘restaurant script’ is complete and cannot include
other events. The name narrative was chosen to im-
ply a likely order of events that is common in spoken
and written retelling of world events. Discrete sets
have the drawback of shutting out unseen and un-
</bodyText>
<figure confidence="0.842519333333333">
I
E
E:x,y
</figure>
<page confidence="0.838672">
795
</page>
<figureCaption confidence="0.9993195">
Figure 6: An automatically learned Prosecution Chain.
Arrows indicate the before relation.
Figure 7: An Employment Chain. Dotted lines indicate
incorrect before relations.
</figureCaption>
<bodyText confidence="0.999295523809524">
likely events from consideration. It is advantageous
to consider a space of possible narrative events and
the ordering within, not a closed list.
However, it is worthwhile to construct discrete
narrative chains, if only to see whether the combina-
tion of event learning and ordering produce script-
like structures. This is easily achievable by using
the PMI scores from section 4 in an agglomerative
clustering algorithm, and then applying the ordering
relations from section 5 to produce a directed graph.
Figures 6 and 7 show two learned chains after
clustering and ordering. Each arrow indicates a be-
fore relation. Duplicate arrows implied by rules of
transitivity are removed. Figure 6 is remarkably ac-
curate, and figure 7 addresses one of the chains from
our introduction, the employment narrative. The
core employment events are accurate, but cluster-
ing included life events (born, died, graduated) from
obituaries of which some temporal information is in-
correct. The Timebank corpus does not include obit-
uaries, thus we suffer from sparsity in training data.
</bodyText>
<sectionHeader confidence="0.99978" genericHeader="conclusions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999782575757576">
We have shown that it is possible to learn narrative
event chains unsupervised from raw text. Not only
do our narrative relations show improvements over
a baseline, but narrative chains offer hope for many
other areas of NLP. Inference, coherence in summa-
rization and generation, slot filling for question an-
swering, and frame induction are all potential areas.
We learned a new measure of similarity, the nar-
rative relation, using the protagonist as a hook to ex-
tract a list of related events from each document.
The 37% improvement over a verb-only baseline
shows that we may not need presorted topics of doc-
uments to learn inferences. In addition, we applied
state of the art temporal classification to show that
sets of events can be partially ordered. Judgements
of coherence can then be made over chains within
documents. Further work in temporal classification
may increase accuracy even further.
Finally, we showed how the event space of narra-
tive relations can be clustered to create discrete sets.
While it is unclear if these are better than an uncon-
strained distribution of events, they do offer insight
into the quality of narratives.
An important area not discussed in this paper is
the possibility of using narrative chains for semantic
role learning. A narrative chain can be viewed as
defining the semantic roles of an event, constraining
it against roles of the other events in the chain. An
argument’s class can then be defined as the set of
narrative arguments in which it appears.
We believe our model provides an important first
step toward learning the rich causal, temporal and
inferential structure of scripts and frames.
</bodyText>
<reference confidence="0.468488333333333">
Acknowledgment: This work is funded in part
by DARPA through IBM and by the DTO Phase III
Program for AQUAINT through Broad Agency An-
nouncement (BAA) N61339-06-R-0034. Thanks to the
reviewers for helpful comments and the suggestion for a
non-full-coreference baseline.
</reference>
<page confidence="0.998524">
796
</page>
<sectionHeader confidence="0.998339" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999951652173913">
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Christian
Boitet and Pete Whitelock, editors, ACL-98, pages 86–
90, San Francisco, California. Morgan Kaufmann Pub-
lishers.
Regina Barzilay and Mirella Lapata. 2005. Modeling lo-
cal coherence: an entity-based approach. Proceedings
of the 43rd Annual Meeting on Association for Com-
putational Linguistics, pages 141–148.
David Bean and Ellen Riloff. 2004. Unsupervised learn-
ing of contextual role knowledge for coreference reso-
lution. Proc. of HLT/NAACL, pages 297–304.
Samuel Brody. 2007. Clustering Clauses for High-
Level Relation Detection: An Information-theoretic
Approach. Proceedings of the 43rd Annual Meeting
of the Association of Computational Linguistics, pages
448–455.
Nathanael Chambers, Shan Wang, and Dan Jurafsky.
2007. Classifying temporal relations between events.
In Proceedings of ACL-07, Prague, Czech Republic.
Timothy Chklovski and Patrick Pantel. 2004. Verbocean:
Mining the web for fine-grained semantic verb rela-
tions. In Proceedings of EMNLP-04.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC-06, pages 449–454.
Tony Deyes. 1984. Towards an authentic ’discourse
cloze’. Applied Linguistics, 5(2).
Toshiaki Fujiki, Hidetsugu Nanba, and Manabu Oku-
mura. 2003. Automatic acquisition of script knowl-
edge from a text collection. In EACL, pages 91–94.
David Graff. 2002. English Gigaword. Linguistic Data
Consortium.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modelling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2).
Mirella Lapata and Alex Lascarides. 2006. Learning
sentence-internal temporal relations. In Journal of AI
Research, volume 27, pages 85–117.
C.Y. Lin and E. Hovy. 2000. The automated acquisi-
tion of topic signatures for text summarization. Pro-
ceedings of the 17th conference on Computational
linguistics-Volume 1, pages 495–501.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min
Lee, and James Pustejovsky. 2006. Machine learning
of temporal relations. In Proceedings ofACL-06, July.
Raymond Mooney and Gerald DeJong. 1985. Learning
schemata for natural language processing. In Ninth In-
ternational Joint Conference on Artificial Intelligence
(IJCAI), pages 681–687.
Jane Morris and Graeme Hirst. 1991. Lexical cohesion
computed by thesaural relations as an indicator of the
structure of text. Computational Linguistics, 17:21–
43.
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically labeling semantic classes. Proceedings of
HLT/NAACL, 4:321–328.
James Pustejovsky, Patrick Hanks, Roser Sauri, Andrew
See, David Day, Lisa Ferro, Robert Gaizauskas, Mar-
cia Lazo, Andrea Setzer, and Beth Sundheim. 2003.
The timebank corpus. Corpus Linguistics, pages 647–
656.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
plans, goals and understanding. Lawrence Erlbaum.
Wilson L. Taylor. 1953. Cloze procedure: a new tool for
measuring readability. Journalism Quarterly, 30:415–
433.
</reference>
<page confidence="0.997394">
797
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.319043">
<title confidence="0.999426">Unsupervised Learning of Narrative Event Chains</title>
<author confidence="0.996007">Chambers Jurafsky</author>
<affiliation confidence="0.9998785">Department of Computer Science Stanford University</affiliation>
<address confidence="0.999809">Stanford, CA 94305</address>
<abstract confidence="0.998820219512195">used in the 1970-80s as knowledge backbones that enabled inference and other NLP tasks requiring deep semantic knowledge. We propose unsupervised of similar schemata called chains raw newswire text. A narrative event chain is a partially ordered set of events related by a common protagonist. We describe a three step process to learning narrative event chains. The first uses unsupervised distributional methods to learn narrative relations between events sharing coreferring arguments. The second applies a temporal classifier to partially order the connected events. Finally, the third prunes and clusters self-contained chains from the space of events. introduce two evaluations: the evaluate event relatedness, and an orcoherence to evaluate narrative order. show a over baseline narrative prediction and temporal coherence. tate learning, and thus this paper addresses the three of chain induction: event ordering of events selection (pruning the event space into discrete sets). Learning these prototypical schematic sequences of events is important for rich understanding of text. Scripts were central to natural language understanding research in the 1970s and 1980s for proposed tasks such as summarization, coreference resolution and question answering. For example, Schank and Abelson (1977) proposed that understanding text about restaurants required knowledge about the Restaurant Script, including the participants (Customer, Waiter, Cook, Tables, etc.), the events constituting the script (entering, sitting down, asking for menus, etc.), and the various preconditions, ordering, and results of each of the constituent actions. Consider these two distinct narrative chains.</abstract>
<note confidence="0.651609">accused X X claimed X argued W joined W served W oversaw dismissed X W resigned</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>Acknowledgment: This work is funded in part by DARPA through IBM and by the DTO Phase III Program for AQUAINT through Broad Agency Announcement (BAA) N61339-06-R-0034. Thanks to the reviewers for helpful comments and the suggestion for a non-full-coreference baseline.</title>
<marker></marker>
<rawString>Acknowledgment: This work is funded in part by DARPA through IBM and by the DTO Phase III Program for AQUAINT through Broad Agency Announcement (BAA) N61339-06-R-0034. Thanks to the reviewers for helpful comments and the suggestion for a non-full-coreference baseline.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>ACL-98,</booktitle>
<pages>86--90</pages>
<editor>In Christian Boitet and Pete Whitelock, editors,</editor>
<publisher>Morgan Kaufmann Publishers.</publisher>
<location>San Francisco, California.</location>
<contexts>
<context position="5492" citStr="Baker et al., 1998" startWordPosition="844" endWordPosition="847">seframe networks as a kind of contextual role knoweldge for anaphora resolution. A caseframe is a verb/event and a semantic role (e.g. &lt;patient&gt; kidnapped). Caseframe networks are relations between caseframes that may represent synonymy (&lt;patient&gt; kidnapped and &lt;patient&gt; abducted) or related events (&lt;patient&gt; kidnapped and &lt;patient&gt; released). Bean and Riloff learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution. Our work can be seen as an attempt to generalize the intuition of caseframes (finding an entire set of events 1We analyzed FrameNet (Baker et al., 1998) for insight, but found that very few of the frames are event sequences of the type characterizing narratives and scripts. rather than just pairs of related frames) and apply it to a different task (finding a coherent structured narrative in non-topic-specific text). More recently, Brody (2007) proposed an approach similar to caseframes that discovers highlevel relatedness between verbs by grouping verbs that share the same lexical items in subject/object positions. He calls these shared arguments anchors. Brody learns pairwise relations between clusters of related verbs, similar to the result</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet project. In Christian Boitet and Pete Whitelock, editors, ACL-98, pages 86– 90, San Francisco, California. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: an entity-based approach.</title>
<date>2005</date>
<booktitle>Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>141--148</pages>
<contexts>
<context position="7447" citStr="Barzilay and Lapata, 2005" startWordPosition="1142" endWordPosition="1145">rather than only pairwise lexical decisions. Finally, Fujiki et al. (2003) investigated script acquisition by extracting the 41 most frequent pairs of events from the first paragraph of newswire articles, using the assumption that the paragraph’s textual order follows temporal order. Our model, by contrast, learns entire event chains, uses more sophisticated probabilistic measures, and uses temporal ordering models instead of relying on document order. 3 The Narrative Chain Model 3.1 Definition Our model is inspired by Centering (Grosz et al., 1995) and other entity-based models of coherence (Barzilay and Lapata, 2005) in which an entity is in focus through a sequence of sentences. We propose to use this same intuition to induce narrative chains. We assume that although a narrative has several participants, there is a central actor who characterizes a narrative chain: the protagonist. Narrative chains are thus structured by the protagonist’s grammatical roles in the events. In addition, narrative 790 events are ordered by some theory of time. This paper describes a partial ordering with the before (no overlap) relation. Our task, therefore, is to learn events that constitute narrative chains. Formally, a na</context>
</contexts>
<marker>Barzilay, Lapata, 2005</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2005. Modeling local coherence: an entity-based approach. Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 141–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Bean</author>
<author>Ellen Riloff</author>
</authors>
<title>Unsupervised learning of contextual role knowledge for coreference resolution.</title>
<date>2004</date>
<booktitle>Proc. of HLT/NAACL,</booktitle>
<pages>297--304</pages>
<contexts>
<context position="4850" citStr="Bean and Riloff (2004)" startWordPosition="742" endWordPosition="745">ents. Finally, the space of narrative events and temporal orders is clustered and pruned to create discrete sets of narrative chains. 2 Previous Work While previous work hasn’t focused specifically on learning narratives1, our work draws from two lines of research in summarization and anaphora resolution. In summarization, topic signatures are a set of terms indicative of a topic (Lin and Hovy, 2000). They are extracted from hand-sorted (by topic) sets of documents using log-likelihood ratios. These terms can capture some narrative relations, but the model requires topic-sorted training data. Bean and Riloff (2004) proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution. A caseframe is a verb/event and a semantic role (e.g. &lt;patient&gt; kidnapped). Caseframe networks are relations between caseframes that may represent synonymy (&lt;patient&gt; kidnapped and &lt;patient&gt; abducted) or related events (&lt;patient&gt; kidnapped and &lt;patient&gt; released). Bean and Riloff learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution. Our work can be seen as an attempt to generalize the intuition of caseframes (finding an entire set of events </context>
</contexts>
<marker>Bean, Riloff, 2004</marker>
<rawString>David Bean and Ellen Riloff. 2004. Unsupervised learning of contextual role knowledge for coreference resolution. Proc. of HLT/NAACL, pages 297–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
</authors>
<title>Clustering Clauses for HighLevel Relation Detection: An Information-theoretic Approach.</title>
<date>2007</date>
<booktitle>Proceedings of the 43rd Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>448--455</pages>
<contexts>
<context position="5787" citStr="Brody (2007)" startWordPosition="893" endWordPosition="894">patient&gt; kidnapped and &lt;patient&gt; released). Bean and Riloff learn these networks from two topic-specific texts and apply them to the problem of anaphora resolution. Our work can be seen as an attempt to generalize the intuition of caseframes (finding an entire set of events 1We analyzed FrameNet (Baker et al., 1998) for insight, but found that very few of the frames are event sequences of the type characterizing narratives and scripts. rather than just pairs of related frames) and apply it to a different task (finding a coherent structured narrative in non-topic-specific text). More recently, Brody (2007) proposed an approach similar to caseframes that discovers highlevel relatedness between verbs by grouping verbs that share the same lexical items in subject/object positions. He calls these shared arguments anchors. Brody learns pairwise relations between clusters of related verbs, similar to the results with caseframes. A human evaluation of these pairs shows an improvement over baseline. This and previous caseframe work lend credence to learning relations from verbs with common arguments. We also draw from lexical chains (Morris and Hirst, 1991), indicators of text coherence from word overl</context>
</contexts>
<marker>Brody, 2007</marker>
<rawString>Samuel Brody. 2007. Clustering Clauses for HighLevel Relation Detection: An Information-theoretic Approach. Proceedings of the 43rd Annual Meeting of the Association of Computational Linguistics, pages 448–455.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Shan Wang</author>
<author>Dan Jurafsky</author>
</authors>
<title>Classifying temporal relations between events.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL-07,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="20297" citStr="Chambers et al., 2007" startWordPosition="3262" endWordPosition="3266">decrease in the verb-only results. These results show that a protagonist greatly assists in narrative judgements. 5 Ordering Narrative Events The model proposed in the previous section is designed to learn the major subevents in a narrative chain, but not how these events are ordered. In this section we extend the model to learn a partial temporal ordering of the events. Ranked Position 3000 2500 2000 1500 1000 500 0 Baseline Protagonist Typed Deps 793 There are a number of algorithms for determining the temporal relationship between two events (Mani et al., 2006; Lapata and Lascarides, 2006; Chambers et al., 2007), many of them trained on the TimeBank Corpus (Pustejovsky et al., 2003) which codes events and their temporal relationships. The currently highest performing of these on raw data is the model of temporal labeling described in our previous work (Chambers et al., 2007). Other approaches have depended on hand tagged features. Chambers et al. (2007) shows 59.4% accuracy on the classification task for six possible relations between pairs of events: before, immediately-before, included-by, simultaneous, begins and ends. We focus on the before relation because the others are less relevant to our imm</context>
<context position="21524" citStr="Chambers et al. (2007)" startWordPosition="3459" endWordPosition="3462">e task. We combine immediately-before with before, and merge the other four relations into an other category. At the binary task of determining if one event is before or other, we achieve 72.1% accuracy on Timebank. The above approach is a two-stage machine learning architecture. In the first stage, the model uses supervised machine learning to label temporal attributes of events, including tense, grammatical aspect, and aspectual class. This first stage classifier relies on features such as neighboring part of speech tags, neighboring auxiliaries and modals, and WordNet synsets. We use SVMs (Chambers et al. (2007) uses Naive Bayes) and see minor performance boosts on Timebank. These imperfect classifications, combined with other linguistic features, are then used in a second stage to classify the temporal relationship between two events. Other features include event-event syntactic properties such as the syntactic dominance relations between the two events, as well as new bigram features of tense, aspect and class (e.g. “present past” if the first event is in the present, and the second past), and whether the events occur in the same or different sentences. 5.1 Training a Temporal Classifier We use the</context>
</contexts>
<marker>Chambers, Wang, Jurafsky, 2007</marker>
<rawString>Nathanael Chambers, Shan Wang, and Dan Jurafsky. 2007. Classifying temporal relations between events. In Proceedings of ACL-07, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Chklovski</author>
<author>Patrick Pantel</author>
</authors>
<title>Verbocean: Mining the web for fine-grained semantic verb relations.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP-04.</booktitle>
<contexts>
<context position="6559" citStr="Chklovski and Pantel (2004)" startWordPosition="1008" endWordPosition="1011">s in subject/object positions. He calls these shared arguments anchors. Brody learns pairwise relations between clusters of related verbs, similar to the results with caseframes. A human evaluation of these pairs shows an improvement over baseline. This and previous caseframe work lend credence to learning relations from verbs with common arguments. We also draw from lexical chains (Morris and Hirst, 1991), indicators of text coherence from word overlap/similarity. We use a related notion of protagonist overlap to motivate narrative chain learning. Work on semantic similarity learning such as Chklovski and Pantel (2004) also automatically learns relations between verbs. We use similar distributional scoring metrics, but differ with our use of a protagonist as the indicator of relatedness. We also use typed dependencies and the entire space of events for similarity judgements, rather than only pairwise lexical decisions. Finally, Fujiki et al. (2003) investigated script acquisition by extracting the 41 most frequent pairs of events from the first paragraph of newswire articles, using the assumption that the paragraph’s textual order follows temporal order. Our model, by contrast, learns entire event chains, u</context>
</contexts>
<marker>Chklovski, Pantel, 2004</marker>
<rawString>Timothy Chklovski and Patrick Pantel. 2004. Verbocean: Mining the web for fine-grained semantic verb relations. In Proceedings of EMNLP-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC-06,</booktitle>
<pages>449--454</pages>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of LREC-06, pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony Deyes</author>
</authors>
<title>Towards an authentic ’discourse cloze’.</title>
<date>1984</date>
<journal>Applied Linguistics,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="12711" citStr="Deyes (1984)" startWordPosition="2035" endWordPosition="2036">s and the six most likely events to include in the same chain. the more events in our chain, the more informed our ranked output. An example of a chain with 3 events and the top 6 ranked guesses is given in figure 1. 4.1 Evaluation Metric: Narrative Cloze The cloze task (Taylor, 1953) is used to evaluate a system (or human) for language proficiency by removing a random word from a sentence and having the system attempt to fill in the blank (e.g. I forgot to the waitress for the good service). Depending on the type of word removed, the test can evaluate syntactic knowledge as well as semantic. Deyes (1984) proposed an extended task, discourse cloze, to evaluate discourse knowledge (removing phrases that are recoverable from knowledge of discourse relations like contrast and consequence). We present a new cloze task that requires narrative knowledge to solve, the narrative cloze. The narrative cloze is a sequence of narrative events in a document from which one event has been removed. The task is to predict the missing verb and typed dependency. Take this example text about American football with McCann as the protagonist: 1. McCann threw two interceptions early. 2. Toledo pulled McCann aside an</context>
</contexts>
<marker>Deyes, 1984</marker>
<rawString>Tony Deyes. 1984. Towards an authentic ’discourse cloze’. Applied Linguistics, 5(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toshiaki Fujiki</author>
<author>Hidetsugu Nanba</author>
<author>Manabu Okumura</author>
</authors>
<title>Automatic acquisition of script knowledge from a text collection.</title>
<date>2003</date>
<booktitle>In EACL,</booktitle>
<pages>91--94</pages>
<contexts>
<context position="6895" citStr="Fujiki et al. (2003)" startWordPosition="1059" endWordPosition="1062">guments. We also draw from lexical chains (Morris and Hirst, 1991), indicators of text coherence from word overlap/similarity. We use a related notion of protagonist overlap to motivate narrative chain learning. Work on semantic similarity learning such as Chklovski and Pantel (2004) also automatically learns relations between verbs. We use similar distributional scoring metrics, but differ with our use of a protagonist as the indicator of relatedness. We also use typed dependencies and the entire space of events for similarity judgements, rather than only pairwise lexical decisions. Finally, Fujiki et al. (2003) investigated script acquisition by extracting the 41 most frequent pairs of events from the first paragraph of newswire articles, using the assumption that the paragraph’s textual order follows temporal order. Our model, by contrast, learns entire event chains, uses more sophisticated probabilistic measures, and uses temporal ordering models instead of relying on document order. 3 The Narrative Chain Model 3.1 Definition Our model is inspired by Centering (Grosz et al., 1995) and other entity-based models of coherence (Barzilay and Lapata, 2005) in which an entity is in focus through a sequen</context>
</contexts>
<marker>Fujiki, Nanba, Okumura, 2003</marker>
<rawString>Toshiaki Fujiki, Hidetsugu Nanba, and Manabu Okumura. 2003. Automatic acquisition of script knowledge from a text collection. In EACL, pages 91–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
</authors>
<title>English Gigaword. Linguistic Data Consortium.</title>
<date>2002</date>
<contexts>
<context position="14349" citStr="Graff, 2002" startWordPosition="2297" endWordPosition="2298">in automatically allows us to evaluate a system’s knowledge of narrative relations and coherence. We do not claim this cloze task to be solvable even by humans, New York Times Editorial occupied subj brought subj rejecting subj projects subj met subj appeared subj offered subj voted pp for offer subj thinks subj Figure 2: One of the 69 test documents, containing 10 narrative events. The protagonist is President Bush. but rather assert it as a comparative measure to evaluate narrative knowledge. 4.2 Narrative Cloze Experiment We use years 1994-2004 (1,007,227 documents) of the Gigaword Corpus (Graff, 2002) for training2. We parse the text into typed dependency graphs with the Stanford Parser (de Marneffe et al., 2006)3, recording all verbs with subject, object, or prepositional typed dependencies. We use the OpenNLP4 coreference engine to resolve the entity mentions. For each document, the verb pairs that share coreferring entities are recorded with their dependency types. Particles are included with the verb. We used 10 news stories from the 1994 section of the corpus for development. The stories were hand chosen to represent a range of topics such as business, sports, politics, and obituaries</context>
</contexts>
<marker>Graff, 2002</marker>
<rawString>David Graff. 2002. English Gigaword. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Aravind K Joshi</author>
<author>Scott Weinstein</author>
</authors>
<title>Centering: A framework for modelling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="7376" citStr="Grosz et al., 1995" startWordPosition="1132" endWordPosition="1135">ncies and the entire space of events for similarity judgements, rather than only pairwise lexical decisions. Finally, Fujiki et al. (2003) investigated script acquisition by extracting the 41 most frequent pairs of events from the first paragraph of newswire articles, using the assumption that the paragraph’s textual order follows temporal order. Our model, by contrast, learns entire event chains, uses more sophisticated probabilistic measures, and uses temporal ordering models instead of relying on document order. 3 The Narrative Chain Model 3.1 Definition Our model is inspired by Centering (Grosz et al., 1995) and other entity-based models of coherence (Barzilay and Lapata, 2005) in which an entity is in focus through a sequence of sentences. We propose to use this same intuition to induce narrative chains. We assume that although a narrative has several participants, there is a central actor who characterizes a narrative chain: the protagonist. Narrative chains are thus structured by the protagonist’s grammatical roles in the events. In addition, narrative 790 events are ordered by some theory of time. This paper describes a partial ordering with the before (no overlap) relation. Our task, therefo</context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. 1995. Centering: A framework for modelling the local coherence of discourse. Computational Linguistics, 21(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Alex Lascarides</author>
</authors>
<title>Learning sentence-internal temporal relations.</title>
<date>2006</date>
<journal>In Journal of AI Research,</journal>
<volume>27</volume>
<pages>85--117</pages>
<contexts>
<context position="20273" citStr="Lapata and Lascarides, 2006" startWordPosition="3258" endWordPosition="3261">solution. This showed a 5.7% decrease in the verb-only results. These results show that a protagonist greatly assists in narrative judgements. 5 Ordering Narrative Events The model proposed in the previous section is designed to learn the major subevents in a narrative chain, but not how these events are ordered. In this section we extend the model to learn a partial temporal ordering of the events. Ranked Position 3000 2500 2000 1500 1000 500 0 Baseline Protagonist Typed Deps 793 There are a number of algorithms for determining the temporal relationship between two events (Mani et al., 2006; Lapata and Lascarides, 2006; Chambers et al., 2007), many of them trained on the TimeBank Corpus (Pustejovsky et al., 2003) which codes events and their temporal relationships. The currently highest performing of these on raw data is the model of temporal labeling described in our previous work (Chambers et al., 2007). Other approaches have depended on hand tagged features. Chambers et al. (2007) shows 59.4% accuracy on the classification task for six possible relations between pairs of events: before, immediately-before, included-by, simultaneous, begins and ends. We focus on the before relation because the others are </context>
</contexts>
<marker>Lapata, Lascarides, 2006</marker>
<rawString>Mirella Lapata and Alex Lascarides. 2006. Learning sentence-internal temporal relations. In Journal of AI Research, volume 27, pages 85–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
<author>E Hovy</author>
</authors>
<title>The automated acquisition of topic signatures for text summarization.</title>
<date>2000</date>
<booktitle>Proceedings of the 17th conference on Computational linguistics-Volume 1,</booktitle>
<pages>495--501</pages>
<contexts>
<context position="4631" citStr="Lin and Hovy, 2000" startWordPosition="711" endWordPosition="714">ate partial orders of our learned events. We show, using a coherence-based evaluation of temporal ordering, that our partial orders lead to better coherence judgements of real narrative instances extracted from documents. Finally, the space of narrative events and temporal orders is clustered and pruned to create discrete sets of narrative chains. 2 Previous Work While previous work hasn’t focused specifically on learning narratives1, our work draws from two lines of research in summarization and anaphora resolution. In summarization, topic signatures are a set of terms indicative of a topic (Lin and Hovy, 2000). They are extracted from hand-sorted (by topic) sets of documents using log-likelihood ratios. These terms can capture some narrative relations, but the model requires topic-sorted training data. Bean and Riloff (2004) proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution. A caseframe is a verb/event and a semantic role (e.g. &lt;patient&gt; kidnapped). Caseframe networks are relations between caseframes that may represent synonymy (&lt;patient&gt; kidnapped and &lt;patient&gt; abducted) or related events (&lt;patient&gt; kidnapped and &lt;patient&gt; released). Bean and Ril</context>
</contexts>
<marker>Lin, Hovy, 2000</marker>
<rawString>C.Y. Lin and E. Hovy. 2000. The automated acquisition of topic signatures for text summarization. Proceedings of the 17th conference on Computational linguistics-Volume 1, pages 495–501.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>Marc Verhagen</author>
<author>Ben Wellner</author>
<author>Chong Min Lee</author>
<author>James Pustejovsky</author>
</authors>
<title>Machine learning of temporal relations.</title>
<date>2006</date>
<booktitle>In Proceedings ofACL-06,</booktitle>
<contexts>
<context position="20244" citStr="Mani et al., 2006" startWordPosition="3254" endWordPosition="3257"> for coreference resolution. This showed a 5.7% decrease in the verb-only results. These results show that a protagonist greatly assists in narrative judgements. 5 Ordering Narrative Events The model proposed in the previous section is designed to learn the major subevents in a narrative chain, but not how these events are ordered. In this section we extend the model to learn a partial temporal ordering of the events. Ranked Position 3000 2500 2000 1500 1000 500 0 Baseline Protagonist Typed Deps 793 There are a number of algorithms for determining the temporal relationship between two events (Mani et al., 2006; Lapata and Lascarides, 2006; Chambers et al., 2007), many of them trained on the TimeBank Corpus (Pustejovsky et al., 2003) which codes events and their temporal relationships. The currently highest performing of these on raw data is the model of temporal labeling described in our previous work (Chambers et al., 2007). Other approaches have depended on hand tagged features. Chambers et al. (2007) shows 59.4% accuracy on the classification task for six possible relations between pairs of events: before, immediately-before, included-by, simultaneous, begins and ends. We focus on the before rel</context>
<context position="22987" citStr="Mani et al. (2006)" startWordPosition="3702" endWordPosition="3705">eled. These are often none relations (events that have no explicit relation) or as is often the case, overlap relations where the two events have no Timebank-defined ordering but overlap in time. Even worse, many events do have an ordering, but they were not tagged by the human annotators. This could be due to the overwhelming task of temporal annotation, or simply because some event orderings are deemed more important than others in understanding the document. We consider all untagged relations as other, and experiment with including none, half, and all of them in training. Taking a cue from Mani et al. (2006), we also increased Timebank’s size by applying transitivity rules to the hand labeled data. The following is an example of the applied transitive rule: if run BEFORE fall and fall BEFORE injured then run BEFORE injured This increases the number of relations from 37519 to 45619. Perhaps more importantly for our task, of all the added relations, the before relation is added the most. We experimented with original vs. expanded Timebank and found the expanded performed slightly worse. The decline may be due to poor transitivity additions, as several Timebank documents contain inconsistent labelin</context>
</contexts>
<marker>Mani, Verhagen, Wellner, Lee, Pustejovsky, 2006</marker>
<rawString>Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min Lee, and James Pustejovsky. 2006. Machine learning of temporal relations. In Proceedings ofACL-06, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond Mooney</author>
<author>Gerald DeJong</author>
</authors>
<title>Learning schemata for natural language processing.</title>
<date>1985</date>
<booktitle>In Ninth International Joint Conference on Artificial Intelligence (IJCAI),</booktitle>
<pages>681--687</pages>
<contexts>
<context position="3154" citStr="Mooney and DeJong (1985)" startWordPosition="479" endWordPosition="482">s can be filled in and instantiated in a particular text situation to draw inferences. Chains focus on a single actor to faciliIt would be useful for question answering or textual entailment to know that ‘X denied ’ is also a likely event in the left chain, while ‘ replaces W’ temporally follows the right. Narrative chains (such as Firing of Employee or Executive Resigns) offer the structure and power to directly infer these new subevents by providing critical background knowledge. In part due to its complexity, automatic induction has not been addressed since the early nonstatistical work of Mooney and DeJong (1985). The first step to narrative induction uses an entitybased model for learning narrative relations by fol789 Proceedings of ACL-08: HLT, pages 789–797, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics lowing a protagonist. As a narrative progresses through a series of events, each event is characterized by the grammatical role played by the protagonist, and by the protagonist’s shared connection to surrounding events. Our algorithm is an unsupervised distributional learning approach that uses coreferring arguments as evidence of a narrative relation. We show, us</context>
</contexts>
<marker>Mooney, DeJong, 1985</marker>
<rawString>Raymond Mooney and Gerald DeJong. 1985. Learning schemata for natural language processing. In Ninth International Joint Conference on Artificial Intelligence (IJCAI), pages 681–687.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
<author>Graeme Hirst</author>
</authors>
<title>Lexical cohesion computed by thesaural relations as an indicator of the structure of text.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<pages>43</pages>
<contexts>
<context position="6341" citStr="Morris and Hirst, 1991" startWordPosition="976" endWordPosition="979">ed narrative in non-topic-specific text). More recently, Brody (2007) proposed an approach similar to caseframes that discovers highlevel relatedness between verbs by grouping verbs that share the same lexical items in subject/object positions. He calls these shared arguments anchors. Brody learns pairwise relations between clusters of related verbs, similar to the results with caseframes. A human evaluation of these pairs shows an improvement over baseline. This and previous caseframe work lend credence to learning relations from verbs with common arguments. We also draw from lexical chains (Morris and Hirst, 1991), indicators of text coherence from word overlap/similarity. We use a related notion of protagonist overlap to motivate narrative chain learning. Work on semantic similarity learning such as Chklovski and Pantel (2004) also automatically learns relations between verbs. We use similar distributional scoring metrics, but differ with our use of a protagonist as the indicator of relatedness. We also use typed dependencies and the entire space of events for similarity judgements, rather than only pairwise lexical decisions. Finally, Fujiki et al. (2003) investigated script acquisition by extracting</context>
</contexts>
<marker>Morris, Hirst, 1991</marker>
<rawString>Jane Morris and Graeme Hirst. 1991. Lexical cohesion computed by thesaural relations as an indicator of the structure of text. Computational Linguistics, 17:21– 43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Deepak Ravichandran</author>
</authors>
<title>Automatically labeling semantic classes.</title>
<date>2004</date>
<booktitle>Proceedings of HLT/NAACL,</booktitle>
<pages>4--321</pages>
<contexts>
<context position="11304" citStr="Pantel and Ravichandran, 2004" startWordPosition="1780" endWordPosition="1783">s of inference). Given a list of observed verb/dependency counts, we approximate the pointwise mutual information (PMI) by: P(e(w, d), e(v, g)) pmi(e(w, d), e(v, g)) = log (1) P(e(w, d))P(e(v, g)) where e(w, d) is the verb/dependency pair w and d (e.g. e(push,subject)). The numerator is defined by: C(e(w, d), e(v, g)) P(e(w, d), e(v, g)) = E.,, Ed,f C(e(x, d), e(y, f)) (2) where C(e(x, d), e(y, f)) is the number of times the two events e(x, d) and e(y, f) had a coreferring entity filling the values of the dependencies d and f. We also adopt the ‘discount score’ to penalize low occuring words (Pantel and Ravichandran, 2004). Given the debate over appropriate metrics for distributional learning, we also experimented with the t-test. Our experiments found that PMI outperforms the t-test on this task by itself and when interpolated together using various mixture weights. Once pairwise relation scores are calculated, a global narrative score can then be built such that all events provide feedback on the event in question. For instance, given all narrative events in a document, we can find the next most likely event to occur by maximizing: pmi(ei, fj) (3) where n is the number of events in our chain and ei is the ith</context>
</contexts>
<marker>Pantel, Ravichandran, 2004</marker>
<rawString>Patrick Pantel and Deepak Ravichandran. 2004. Automatically labeling semantic classes. Proceedings of HLT/NAACL, 4:321–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Patrick Hanks</author>
<author>Roser Sauri</author>
<author>Andrew See</author>
<author>David Day</author>
<author>Lisa Ferro</author>
<author>Robert Gaizauskas</author>
<author>Marcia Lazo</author>
<author>Andrea Setzer</author>
<author>Beth Sundheim</author>
</authors>
<title>The timebank corpus. Corpus Linguistics,</title>
<date>2003</date>
<pages>647--656</pages>
<contexts>
<context position="20369" citStr="Pustejovsky et al., 2003" startWordPosition="3276" endWordPosition="3279">ist greatly assists in narrative judgements. 5 Ordering Narrative Events The model proposed in the previous section is designed to learn the major subevents in a narrative chain, but not how these events are ordered. In this section we extend the model to learn a partial temporal ordering of the events. Ranked Position 3000 2500 2000 1500 1000 500 0 Baseline Protagonist Typed Deps 793 There are a number of algorithms for determining the temporal relationship between two events (Mani et al., 2006; Lapata and Lascarides, 2006; Chambers et al., 2007), many of them trained on the TimeBank Corpus (Pustejovsky et al., 2003) which codes events and their temporal relationships. The currently highest performing of these on raw data is the model of temporal labeling described in our previous work (Chambers et al., 2007). Other approaches have depended on hand tagged features. Chambers et al. (2007) shows 59.4% accuracy on the classification task for six possible relations between pairs of events: before, immediately-before, included-by, simultaneous, begins and ends. We focus on the before relation because the others are less relevant to our immediate task. We combine immediately-before with before, and merge the ot</context>
</contexts>
<marker>Pustejovsky, Hanks, Sauri, See, Day, Ferro, Gaizauskas, Lazo, Setzer, Sundheim, 2003</marker>
<rawString>James Pustejovsky, Patrick Hanks, Roser Sauri, Andrew See, David Day, Lisa Ferro, Robert Gaizauskas, Marcia Lazo, Andrea Setzer, and Beth Sundheim. 2003. The timebank corpus. Corpus Linguistics, pages 647– 656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger C Schank</author>
<author>Robert P Abelson</author>
</authors>
<title>Scripts, plans, goals and understanding. Lawrence Erlbaum.</title>
<date>1977</date>
<contexts>
<context position="1664" citStr="Schank and Abelson (1977)" startWordPosition="241" endWordPosition="244">er. We show a 36% improvement over baseline for narrative prediction and 25% for temporal coherence. tate learning, and thus this paper addresses the three tasks of chain induction: narrative event induction, temporal ordering of events and structured selection (pruning the event space into discrete sets). Learning these prototypical schematic sequences of events is important for rich understanding of text. Scripts were central to natural language understanding research in the 1970s and 1980s for proposed tasks such as summarization, coreference resolution and question answering. For example, Schank and Abelson (1977) proposed that understanding text about restaurants required knowledge about the Restaurant Script, including the participants (Customer, Waiter, Cook, Tables, etc.), the events constituting the script (entering, sitting down, asking for menus, etc.), and the various preconditions, ordering, and results of each of the constituent actions. Consider these two distinct narrative chains. accused X X claimed X argued W joined W served W oversaw dismissed X W resigned 1 Introduction This paper induces a new representation of structured knowledge called narrative event chains (or narrative chains). N</context>
</contexts>
<marker>Schank, Abelson, 1977</marker>
<rawString>Roger C. Schank and Robert P. Abelson. 1977. Scripts, plans, goals and understanding. Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wilson L Taylor</author>
</authors>
<title>Cloze procedure: a new tool for measuring readability.</title>
<date>1953</date>
<journal>Journalism Quarterly,</journal>
<volume>30</volume>
<pages>433</pages>
<contexts>
<context position="12384" citStr="Taylor, 1953" startWordPosition="1974" endWordPosition="1975">ind the next most likely event to occur by maximizing: pmi(ei, fj) (3) where n is the number of events in our chain and ei is the ith event. m is the number of events f in our training corpus. A ranked list of guesses can be built from this summation and we hypothesize that E i=0 max j:0&lt;j&lt;M 791 Figure 1: Three narrative events and the six most likely events to include in the same chain. the more events in our chain, the more informed our ranked output. An example of a chain with 3 events and the top 6 ranked guesses is given in figure 1. 4.1 Evaluation Metric: Narrative Cloze The cloze task (Taylor, 1953) is used to evaluate a system (or human) for language proficiency by removing a random word from a sentence and having the system attempt to fill in the blank (e.g. I forgot to the waitress for the good service). Depending on the type of word removed, the test can evaluate syntactic knowledge as well as semantic. Deyes (1984) proposed an extended task, discourse cloze, to evaluate discourse knowledge (removing phrases that are recoverable from knowledge of discourse relations like contrast and consequence). We present a new cloze task that requires narrative knowledge to solve, the narrative c</context>
</contexts>
<marker>Taylor, 1953</marker>
<rawString>Wilson L. Taylor. 1953. Cloze procedure: a new tool for measuring readability. Journalism Quarterly, 30:415– 433.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>