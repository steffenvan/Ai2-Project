<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007618">
<title confidence="0.979604">
Language Independent Extractive Summarization
</title>
<author confidence="0.997967">
Rada Mihalcea
</author>
<affiliation confidence="0.999677">
Department of Computer Science and Engineering
University of North Texas
</affiliation>
<email confidence="0.998526">
rada@cs.unt.edu
</email>
<sectionHeader confidence="0.995644" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999758">
We demonstrate TextRank – a system for
unsupervised extractive summarization that
relies on the application of iterative graph-
based ranking algorithms to graphs encod-
ing the cohesive structure of a text. An im-
portant characteristic of the system is that
it does not rely on any language-specific
knowledge resources or any manually con-
structed training data, and thus it is highly
portable to new languages or domains.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999905526315789">
Given the overwhelming amount of information avail-
able today, on the Web and elsewhere, techniques
for efficient automatic text summarization are essen-
tial to improve the access to such information. Al-
gorithms for extractive summarization are typically
based on techniques for sentence extraction, and at-
tempt to identify the set of sentences that are most
important for the understanding of a given document.
Some of the most successful approaches to extractive
summarization consist of supervised algorithms that
attempt to learn what makes a good summary by train-
ing on collections of summaries built for a relatively
large number of training documents, e.g. (Hirao et
al., 2002), (Teufel and Moens, 1997). However, the
price paid for the high performance of such super-
vised algorithms is their inability to easily adapt to
new languages or domains, as new training data are
required for each new type of data. TextRank (Mi-
halcea and Tarau, 2004), (Mihalcea, 2004) is specifi-
</bodyText>
<page confidence="0.99151">
49
</page>
<bodyText confidence="0.999848545454545">
cally designed to address this problem, by using an ex-
tractive summarization technique that does not require
any training data or any language-specific knowledge
sources. TextRank can be effectively applied to the
summarization of documents in different languages
without any modifications of the algorithm and with-
out any requirements for additional data. Moreover,
results from experiments performed on standard data
sets have demonstrated that the performance of Text-
Rank is competitive with that of some of the best sum-
marization systems available today.
</bodyText>
<sectionHeader confidence="0.990656" genericHeader="method">
2 Extractive Summarization
</sectionHeader>
<bodyText confidence="0.999754">
Ranking algorithms, such as Kleinberg’s HITS al-
gorithm (Kleinberg, 1999) or Google’s PageRank
(Brin and Page, 1998) have been traditionally and suc-
cessfully used in Web-link analysis, social networks,
and more recently in text processing applications. In
short, a graph-based ranking algorithm is a way of de-
ciding on the importance of a vertex within a graph,
by taking into account global information recursively
computed from the entire graph, rather than relying
only on local vertex-specific information. The basic
idea implemented by the ranking model is that of vot-
ing or recommendation. When one vertex links to an-
other one, it is basically casting a vote for that other
vertex. The higher the number of votes that are cast
for a vertex, the higher the importance of the vertex.
These graph ranking algorithms are based on a
random walk model, where a walker takes random
steps on the graph, with the walk being modeled as a
Markov process – that is, the decision on what edge to
follow is solely based on the vertex where the walker
is currently located. Under certain conditions, this
</bodyText>
<subsubsectionHeader confidence="0.360947">
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
</subsubsectionHeader>
<bodyText confidence="0.979186913043478">
pages 49–52, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
model converges to a stationary distribution of prob-
abilities associated with vertices in the graph, repre-
senting the probability of finding the walker at a cer-
tain vertex in the graph. Based on the Ergodic theorem
for Markov chains (Grimmett and Stirzaker, 1989),
the algorithms are guaranteed to converge if the graph
is both aperiodic and irreducible. The first condition
is achieved for any graph that is a non-bipartite graph,
while the second condition holds for any strongly con-
nected graph. Both these conditions are achieved in
the graphs constructed for the extractive summariza-
tion application implemented in TextRank.
While there are several graph-based ranking algo-
rithms previously proposed in the literature, we fo-
cus on two algorithms, namely PageRank (Brin and
Page, 1998) and HITS (Kleinberg, 1999).
Let G = (V, E) be a directed graph with the set of
vertices V and set of edges E, where E is a subset
of V x V . For a given vertex VZ, let In(Vi) be the
set of vertices that point to it (predecessors), and let
Out(Vi) be the set of vertices that vertex V� points to
(successors).
</bodyText>
<subsectionHeader confidence="0.963843">
2.1 PageRank
</subsectionHeader>
<bodyText confidence="0.999959">
PageRank (Brin and Page, 1998) is perhaps one
of the most popular ranking algorithms, and was
designed as a method for Web link analysis. Un-
like other graph ranking algorithms, PageRank inte-
grates the impact of both incoming and outgoing links
into one single model, and therefore it produces only
one set of scores:
</bodyText>
<equation confidence="0.997939">
� PR(Vj) (1)
PR(V�) = (1 − d) + d * Out(Vj)
VjEIn(Vi)
</equation>
<bodyText confidence="0.999894666666667">
where d is a parameter that is set between 0 and 1,
and has the role of integrating random jumps into the
random walking model.
</bodyText>
<subsectionHeader confidence="0.896577">
2.2 HITS
</subsectionHeader>
<bodyText confidence="0.999257333333333">
HITS (Hyperlinked Induced Topic Search) (Klein-
berg, 1999) is an iterative algorithm that was designed
for ranking Web pages according to their degree of
“authority”. The HITS algorithm makes a distinc-
tion between “authorities” (pages with a large num-
ber of incoming links) and “hubs” (pages with a large
number of outgoing links). For each vertex, HITS
produces two sets of scores – an “authority” score, and
a “hub” score:
</bodyText>
<equation confidence="0.99937825">
HITSA(Vi) = � HITSH(Vj) (2)
VjEIn(Vi)
HITSH(Vi) = � HITSA(Vj) (3)
VjEOut(Vi)
</equation>
<bodyText confidence="0.975934352941176">
Starting from arbitrary values assigned to each node
in the graph, the ranking algorithm iterates until con-
vergence below a given threshold is achieved. After
running the algorithm, a score is associated with each
vertex, which represents the importance of that ver-
tex within the graph. Note that the final values are
not affected by the choice of the initial value, only the
number of iterations to convergence may be different.
When the graphs are built starting with natural lan-
guage texts, it may be useful to integrate into the graph
model the strength of the connection between two ver-
tices V� and Vj, indicated as a weight wzj added to
the corresponding edge. Consequently, the ranking
algorithm is adapted to include edge weights, e.g. for
PageRank the score is determined using the follow-
ing formula (a similar change can be applied to the
HITS algorithm):
</bodyText>
<equation confidence="0.8944255">
�
PRW (V�) = (1−d)+d*
VjEIn(Vi)
(4)
</equation>
<bodyText confidence="0.999556277777778">
While the final vertex scores (and therefore rank-
ings) for weighted graphs differ significantly as com-
pared to their unweighted alternatives, the number of
iterations to convergence and the shape of the con-
vergence curves is almost identical for weighted and
unweighted graphs.
For the task of single-document extractive summa-
rization, the goal is to rank the sentences in a given
text with respect to their importance for the overall
understanding of the text. A graph is therefore con-
structed by adding a vertex for each sentence in the
text, and edges between vertices are established us-
ing sentence inter-connections, defined using a simple
similarity relation measured as a function of content
overlap. Such a relation between two sentences can be
seen as a process of recommendation: a sentence that
addresses certain concepts in a text gives the reader
a recommendation to refer to other sentences in the
</bodyText>
<figure confidence="0.678532">
PRW (Vj)
E wkj
VkEOut(Vj)
wj-
</figure>
<page confidence="0.966973">
50
</page>
<bodyText confidence="0.998085384615385">
text that address the same concepts, and therefore a
link can be drawn between any two such sentences
that share common content.
The overlap of two sentences can be determined
simply as the number of common tokens between the
lexical representations of the two sentences, or it can
be run through filters that e.g. eliminate stopwords,
count only words of a certain category, etc. Moreover,
to avoid promoting long sentences, we use a normal-
ization factor and divide the content overlap of two
sentences with the length of each sentence.
The resulting graph is highly connected, with a
weight associated with each edge, indicating the
strength of the connections between various sentence
pairs in the text. The graph can be represented as: (a)
simple undirected graph; (b) directed weighted graph
with the orientation of edges set from a sentence to
sentences that follow in the text (directed forward);
or (c) directed weighted graph with the orientation of
edges set from a sentence to previous sentences in the
text (directed backward).
After the ranking algorithm is run on the graph,
sentences are sorted in reversed order of their score,
and the top ranked sentences are selected for inclu-
sion in the summary. Figure 1 shows an example of a
weighted graph built for a short sample text.
</bodyText>
<listItem confidence="0.92802075">
[1] Watching the new movie, “Imagine: John Lennon,” was very
painful for the late Beatle’s wife, Yoko Ono.
[2] “The only reason why I did watch it to the end is because I’m
responsible for it, even though somebody else made it,” she said.
[3] Cassettes, film footage and other elements of the acclaimed
movie were collected by Ono.
[4] She also took cassettes of interviews by Lennon, which were
edited in such a way that he narrates the picture.
[5] Andrew Solt (“This Is Elvis”) directed, Solt and David L.
Wolper produced and Solt and Sam Egan wrote it.
[6] “I think this is really the definitive documentary of John
Lennon’s life,” Ono said in an interview.
</listItem>
<sectionHeader confidence="0.997149" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999067555555555">
English document summarization experiments are run
using the summarization test collection provided in
the framework of the Document Understanding Con-
ference (DUC). In particular, we use the data set of
567 news articles made available during the DUC
2002 evaluations (DUC, 2002), and the correspond-
ing 100-word summaries generated for each of these
documents. This is the single document summariza-
tion task undertaken by other systems participating in
</bodyText>
<figureCaption confidence="0.904274666666667">
Figure 1: Graph of sentence similarities built on a
sample text. Scores reflecting sentence importance are
shown in brackets next to each sentence.
</figureCaption>
<bodyText confidence="0.996436911764706">
the DUC 2002 document summarization evaluations.
To test the language independence aspect of the al-
gorithm, in addition to the English test collection, we
also use a Brazilian Portuguese data set consisting of
100 news articles and their corresponding manually
produced summaries. We use the TeM´ario test col-
lection (Pardo and Rino, 2003), containing newspa-
per articles from online Brazilian newswire: 40 docu-
ments from Jornal de Brasil and 60 documents from
Folha de S˜ao Paulo. The documents were selected to
cover a variety of domains (e.g. world, politics, for-
eign affairs, editorials), and manual summaries were
produced by an expert in Brazilian Portuguese. Unlike
the summaries produced for the English DUC docu-
ments – which had a length requirement of approxi-
mately 100 words, the length of the summaries in the
TeM´ario data set is constrained relative to the length
of the corresponding documents, i.e. a summary has
to account for about 25-30% of the original document.
Consequently, the automatic summaries generated for
the documents in this collection are not restricted to
100 words, as in the English experiments, but are re-
quired to have a length comparable to the correspond-
ing manual summaries, to ensure a fair evaluation.
For evaluation, we are using the ROUGE evaluation
toolkit&apos;, which is a method based on Ngram statistics,
found to be highly correlated with human evaluations
(Lin and Hovy, 2003). The evaluation is done using
the Ngram(1,1) setting of ROUGE, which was found
to have the highest correlation with human judgments,
at a confidence level of 95%.
Table 2 shows the results obtained on these two data
sets for different graph settings. The table also lists
baseline results, obtained on summaries generated by
</bodyText>
<footnote confidence="0.461215">
&apos;ROUGE is available at http://www.isi.edu/˜cyl/ROUGE/.
</footnote>
<figure confidence="0.99687875">
1
3
0.15
0.15
0.29
6
0.16
0.15
4
[0.70]
0.32
2
0.30
[0.91]
5
[1.34]
0.46
[1.75]
[0.74]
[0.52]
</figure>
<page confidence="0.990726">
51
</page>
<table confidence="0.999215">
Algorithm Graph
Undirected Forward Backward
HITSW 0.4912 0.4584 0.5023
A 0.4912 0.5023 0.4584
HITSW 0.4904 0.4202 0.5008
H
PageRankW
Baseline 0.4799
</table>
<tableCaption confidence="0.995663">
Table 1: English single-document summarization.
</tableCaption>
<table confidence="0.999783375">
Algorithm Graph
Undirected Forward Backward
HITSW 0.4814 0.4834 0.5002
A 0.4814 0.5002 0.4834
HITSW 0.4939 0.4574 0.5121
H
PageRankW
Baseline 0.4963
</table>
<tableCaption confidence="0.999198">
Table 2: Portuguese single-document summarization.
</tableCaption>
<bodyText confidence="0.999723052631579">
taking the first sentences in each document. By ways
of comparison, the best participating system in DUC
2002 was a supervised system that led to a ROUGE
score of 0.5011.
For both data sets, TextRank applied on a directed
backward graph structure exceeds the performance
achieved through a simple (but powerful) baseline.
These results prove that graph-based ranking algo-
rithms, previously found successful in Web link anal-
ysis and social networks, can be turned into a state-
of-the-art tool for extractive summarization when ap-
plied to graphs extracted from texts. Moreover, due
to its unsupervised nature, the algorithm was also
shown to be language independent, leading to similar
results and similar improvements over baseline tech-
niques when applied on documents in different lan-
guages. More extensive experimental results with the
TextRank system are reported in (Mihalcea and Tarau,
2004), (Mihalcea, 2004).
</bodyText>
<sectionHeader confidence="0.999535" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9999463">
Intuitively, iterative graph-based ranking algorithms
work well on the task of extractive summarization be-
cause they do not only rely on the local context of a
text unit (vertex), but they also take into account infor-
mation recursively drawn from the entire text (graph).
Through the graphs it builds on texts, a graph-based
ranking algorithm identifies connections between var-
ious entities in a text, and implements the concept of
recommendation. In the process of identifying impor-
tant sentences in a text, a sentence recommends other
sentences that address similar concepts as being use-
ful for the overall understanding of the text. Sentences
that are highly recommended by other sentences are
likely to be more informative for the given text, and
will be therefore given a higher score.
An important aspect of the graph-based extractive
summarization method is that it does not require deep
linguistic knowledge, nor domain or language specific
annotated corpora, which makes it highly portable to
other domains, genres, or languages.
</bodyText>
<sectionHeader confidence="0.998929" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999922">
We are grateful to Lucia Helena Machado Rino for
making available the TeM´ario summarization test col-
lection and for her help with this data set.
</bodyText>
<sectionHeader confidence="0.999441" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999892484848485">
S. Brin and L. Page. 1998. The anatomy of a large-scale
hypertextual Web search engine. Computer Networks
and ISDN Systems, 30(1–7).
DUC. 2002. Document understanding conference 2002.
http://www-nlpir.nist.gov/projects/duc/.
G. Grimmett and D. Stirzaker. 1989. Probability and Ran-
dom Processes. Oxford University Press.
T. Hirao, Y. Sasaki, H. Isozaki, and E. Maeda. 2002. Ntt’s
text summarization system for duc-2002. In Proceed-
ings of the Document Understanding Conference 2002.
J.M. Kleinberg. 1999. Authoritative sources in a hyper-
linked environment. Journal of the ACM, 46(5):604–
632.
C.Y. Lin and E.H. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
Proceedings of Human Language Technology Confer-
ence (HLT-NAACL 2003), Edmonton, Canada, May.
R. Mihalcea and P. Tarau. 2004. TextRank – bringing order
into texts. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing (EMNLP
2004), Barcelona, Spain.
R. Mihalcea. 2004. Graph-based ranking algorithms for
sentence extraction, applied to text summarization. In
Proceedings of the 42nd Annual Meeting of the Associ-
ation for Computational Lingusitics (ACL 2004) (com-
panion volume), Barcelona, Spain.
T.A.S. Pardo and L.H.M. Rino. 2003. TeMario: a cor-
pus for automatic text summarization. Technical report,
NILC-TR-03-09.
S. Teufel and M. Moens. 1997. Sentence extraction as a
classification task. In ACL/EACL workshop on ”Intel-
ligent and scalable Text summarization”, pages 58–65,
Madrid, Spain.
</reference>
<page confidence="0.998858">
52
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.898350">
<title confidence="0.999963">Language Independent Extractive Summarization</title>
<author confidence="0.999515">Rada Mihalcea</author>
<affiliation confidence="0.995413">Department of Computer Science and Engineering University of North Texas</affiliation>
<email confidence="0.999815">rada@cs.unt.edu</email>
<abstract confidence="0.989506454545454">demonstrate a system for unsupervised extractive summarization that relies on the application of iterative graphbased ranking algorithms to graphs encoding the cohesive structure of a text. An important characteristic of the system is that it does not rely on any language-specific knowledge resources or any manually constructed training data, and thus it is highly portable to new languages or domains.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Brin</author>
<author>L Page</author>
</authors>
<title>The anatomy of a large-scale hypertextual Web search engine.</title>
<date>1998</date>
<journal>Computer Networks and ISDN Systems,</journal>
<pages>30--1</pages>
<contexts>
<context position="2274" citStr="Brin and Page, 1998" startWordPosition="343" endWordPosition="346">nique that does not require any training data or any language-specific knowledge sources. TextRank can be effectively applied to the summarization of documents in different languages without any modifications of the algorithm and without any requirements for additional data. Moreover, results from experiments performed on standard data sets have demonstrated that the performance of TextRank is competitive with that of some of the best summarization systems available today. 2 Extractive Summarization Ranking algorithms, such as Kleinberg’s HITS algorithm (Kleinberg, 1999) or Google’s PageRank (Brin and Page, 1998) have been traditionally and successfully used in Web-link analysis, social networks, and more recently in text processing applications. In short, a graph-based ranking algorithm is a way of deciding on the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph, rather than relying only on local vertex-specific information. The basic idea implemented by the ranking model is that of voting or recommendation. When one vertex links to another one, it is basically casting a vote for that other vertex. The higher the number of vot</context>
<context position="4197" citStr="Brin and Page, 1998" startWordPosition="654" endWordPosition="657">ex in the graph. Based on the Ergodic theorem for Markov chains (Grimmett and Stirzaker, 1989), the algorithms are guaranteed to converge if the graph is both aperiodic and irreducible. The first condition is achieved for any graph that is a non-bipartite graph, while the second condition holds for any strongly connected graph. Both these conditions are achieved in the graphs constructed for the extractive summarization application implemented in TextRank. While there are several graph-based ranking algorithms previously proposed in the literature, we focus on two algorithms, namely PageRank (Brin and Page, 1998) and HITS (Kleinberg, 1999). Let G = (V, E) be a directed graph with the set of vertices V and set of edges E, where E is a subset of V x V . For a given vertex VZ, let In(Vi) be the set of vertices that point to it (predecessors), and let Out(Vi) be the set of vertices that vertex V� points to (successors). 2.1 PageRank PageRank (Brin and Page, 1998) is perhaps one of the most popular ranking algorithms, and was designed as a method for Web link analysis. Unlike other graph ranking algorithms, PageRank integrates the impact of both incoming and outgoing links into one single model, and theref</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>S. Brin and L. Page. 1998. The anatomy of a large-scale hypertextual Web search engine. Computer Networks and ISDN Systems, 30(1–7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>DUC</author>
</authors>
<title>Document understanding conference</title>
<date>2002</date>
<note>http://www-nlpir.nist.gov/projects/duc/.</note>
<contexts>
<context position="9597" citStr="DUC 2002" startWordPosition="1582" endWordPosition="1583">d by Ono. [4] She also took cassettes of interviews by Lennon, which were edited in such a way that he narrates the picture. [5] Andrew Solt (“This Is Elvis”) directed, Solt and David L. Wolper produced and Solt and Sam Egan wrote it. [6] “I think this is really the definitive documentary of John Lennon’s life,” Ono said in an interview. 3 Evaluation English document summarization experiments are run using the summarization test collection provided in the framework of the Document Understanding Conference (DUC). In particular, we use the data set of 567 news articles made available during the DUC 2002 evaluations (DUC, 2002), and the corresponding 100-word summaries generated for each of these documents. This is the single document summarization task undertaken by other systems participating in Figure 1: Graph of sentence similarities built on a sample text. Scores reflecting sentence importance are shown in brackets next to each sentence. the DUC 2002 document summarization evaluations. To test the language independence aspect of the algorithm, in addition to the English test collection, we also use a Brazilian Portuguese data set consisting of 100 news articles and their corresponding ma</context>
<context position="12349" citStr="DUC 2002" startWordPosition="2012" endWordPosition="2013">GE/. 1 3 0.15 0.15 0.29 6 0.16 0.15 4 [0.70] 0.32 2 0.30 [0.91] 5 [1.34] 0.46 [1.75] [0.74] [0.52] 51 Algorithm Graph Undirected Forward Backward HITSW 0.4912 0.4584 0.5023 A 0.4912 0.5023 0.4584 HITSW 0.4904 0.4202 0.5008 H PageRankW Baseline 0.4799 Table 1: English single-document summarization. Algorithm Graph Undirected Forward Backward HITSW 0.4814 0.4834 0.5002 A 0.4814 0.5002 0.4834 HITSW 0.4939 0.4574 0.5121 H PageRankW Baseline 0.4963 Table 2: Portuguese single-document summarization. taking the first sentences in each document. By ways of comparison, the best participating system in DUC 2002 was a supervised system that led to a ROUGE score of 0.5011. For both data sets, TextRank applied on a directed backward graph structure exceeds the performance achieved through a simple (but powerful) baseline. These results prove that graph-based ranking algorithms, previously found successful in Web link analysis and social networks, can be turned into a stateof-the-art tool for extractive summarization when applied to graphs extracted from texts. Moreover, due to its unsupervised nature, the algorithm was also shown to be language independent, leading to similar results and similar improv</context>
</contexts>
<marker>DUC, 2002</marker>
<rawString>DUC. 2002. Document understanding conference 2002. http://www-nlpir.nist.gov/projects/duc/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grimmett</author>
<author>D Stirzaker</author>
</authors>
<title>Probability and Random Processes.</title>
<date>1989</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="3671" citStr="Grimmett and Stirzaker, 1989" startWordPosition="573" endWordPosition="576">ps on the graph, with the walk being modeled as a Markov process – that is, the decision on what edge to follow is solely based on the vertex where the walker is currently located. Under certain conditions, this Proceedings of the ACL Interactive Poster and Demonstration Sessions, pages 49–52, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics model converges to a stationary distribution of probabilities associated with vertices in the graph, representing the probability of finding the walker at a certain vertex in the graph. Based on the Ergodic theorem for Markov chains (Grimmett and Stirzaker, 1989), the algorithms are guaranteed to converge if the graph is both aperiodic and irreducible. The first condition is achieved for any graph that is a non-bipartite graph, while the second condition holds for any strongly connected graph. Both these conditions are achieved in the graphs constructed for the extractive summarization application implemented in TextRank. While there are several graph-based ranking algorithms previously proposed in the literature, we focus on two algorithms, namely PageRank (Brin and Page, 1998) and HITS (Kleinberg, 1999). Let G = (V, E) be a directed graph with the s</context>
</contexts>
<marker>Grimmett, Stirzaker, 1989</marker>
<rawString>G. Grimmett and D. Stirzaker. 1989. Probability and Random Processes. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hirao</author>
<author>Y Sasaki</author>
<author>H Isozaki</author>
<author>E Maeda</author>
</authors>
<title>Ntt’s text summarization system for duc-2002.</title>
<date>2002</date>
<booktitle>In Proceedings of the Document Understanding Conference</booktitle>
<contexts>
<context position="1274" citStr="Hirao et al., 2002" startWordPosition="189" endWordPosition="192">n the Web and elsewhere, techniques for efficient automatic text summarization are essential to improve the access to such information. Algorithms for extractive summarization are typically based on techniques for sentence extraction, and attempt to identify the set of sentences that are most important for the understanding of a given document. Some of the most successful approaches to extractive summarization consist of supervised algorithms that attempt to learn what makes a good summary by training on collections of summaries built for a relatively large number of training documents, e.g. (Hirao et al., 2002), (Teufel and Moens, 1997). However, the price paid for the high performance of such supervised algorithms is their inability to easily adapt to new languages or domains, as new training data are required for each new type of data. TextRank (Mihalcea and Tarau, 2004), (Mihalcea, 2004) is specifi49 cally designed to address this problem, by using an extractive summarization technique that does not require any training data or any language-specific knowledge sources. TextRank can be effectively applied to the summarization of documents in different languages without any modifications of the algo</context>
</contexts>
<marker>Hirao, Sasaki, Isozaki, Maeda, 2002</marker>
<rawString>T. Hirao, Y. Sasaki, H. Isozaki, and E. Maeda. 2002. Ntt’s text summarization system for duc-2002. In Proceedings of the Document Understanding Conference 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Kleinberg</author>
</authors>
<title>Authoritative sources in a hyperlinked environment.</title>
<date>1999</date>
<journal>Journal of the ACM,</journal>
<volume>46</volume>
<issue>5</issue>
<pages>632</pages>
<contexts>
<context position="2231" citStr="Kleinberg, 1999" startWordPosition="338" endWordPosition="339"> using an extractive summarization technique that does not require any training data or any language-specific knowledge sources. TextRank can be effectively applied to the summarization of documents in different languages without any modifications of the algorithm and without any requirements for additional data. Moreover, results from experiments performed on standard data sets have demonstrated that the performance of TextRank is competitive with that of some of the best summarization systems available today. 2 Extractive Summarization Ranking algorithms, such as Kleinberg’s HITS algorithm (Kleinberg, 1999) or Google’s PageRank (Brin and Page, 1998) have been traditionally and successfully used in Web-link analysis, social networks, and more recently in text processing applications. In short, a graph-based ranking algorithm is a way of deciding on the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph, rather than relying only on local vertex-specific information. The basic idea implemented by the ranking model is that of voting or recommendation. When one vertex links to another one, it is basically casting a vote for that</context>
<context position="4224" citStr="Kleinberg, 1999" startWordPosition="660" endWordPosition="661">rgodic theorem for Markov chains (Grimmett and Stirzaker, 1989), the algorithms are guaranteed to converge if the graph is both aperiodic and irreducible. The first condition is achieved for any graph that is a non-bipartite graph, while the second condition holds for any strongly connected graph. Both these conditions are achieved in the graphs constructed for the extractive summarization application implemented in TextRank. While there are several graph-based ranking algorithms previously proposed in the literature, we focus on two algorithms, namely PageRank (Brin and Page, 1998) and HITS (Kleinberg, 1999). Let G = (V, E) be a directed graph with the set of vertices V and set of edges E, where E is a subset of V x V . For a given vertex VZ, let In(Vi) be the set of vertices that point to it (predecessors), and let Out(Vi) be the set of vertices that vertex V� points to (successors). 2.1 PageRank PageRank (Brin and Page, 1998) is perhaps one of the most popular ranking algorithms, and was designed as a method for Web link analysis. Unlike other graph ranking algorithms, PageRank integrates the impact of both incoming and outgoing links into one single model, and therefore it produces only one se</context>
</contexts>
<marker>Kleinberg, 1999</marker>
<rawString>J.M. Kleinberg. 1999. Authoritative sources in a hyperlinked environment. Journal of the ACM, 46(5):604– 632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
<author>E H Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of Human Language Technology Conference (HLT-NAACL 2003),</booktitle>
<location>Edmonton, Canada,</location>
<contexts>
<context position="11365" citStr="Lin and Hovy, 2003" startWordPosition="1861" endWordPosition="1864"> summaries in the TeM´ario data set is constrained relative to the length of the corresponding documents, i.e. a summary has to account for about 25-30% of the original document. Consequently, the automatic summaries generated for the documents in this collection are not restricted to 100 words, as in the English experiments, but are required to have a length comparable to the corresponding manual summaries, to ensure a fair evaluation. For evaluation, we are using the ROUGE evaluation toolkit&apos;, which is a method based on Ngram statistics, found to be highly correlated with human evaluations (Lin and Hovy, 2003). The evaluation is done using the Ngram(1,1) setting of ROUGE, which was found to have the highest correlation with human judgments, at a confidence level of 95%. Table 2 shows the results obtained on these two data sets for different graph settings. The table also lists baseline results, obtained on summaries generated by &apos;ROUGE is available at http://www.isi.edu/˜cyl/ROUGE/. 1 3 0.15 0.15 0.29 6 0.16 0.15 4 [0.70] 0.32 2 0.30 [0.91] 5 [1.34] 0.46 [1.75] [0.74] [0.52] 51 Algorithm Graph Undirected Forward Backward HITSW 0.4912 0.4584 0.5023 A 0.4912 0.5023 0.4584 HITSW 0.4904 0.4202 0.5008 H</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>C.Y. Lin and E.H. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of Human Language Technology Conference (HLT-NAACL 2003), Edmonton, Canada, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>P Tarau</author>
</authors>
<title>TextRank – bringing order into texts.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2004),</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="1541" citStr="Mihalcea and Tarau, 2004" startWordPosition="234" endWordPosition="238">e set of sentences that are most important for the understanding of a given document. Some of the most successful approaches to extractive summarization consist of supervised algorithms that attempt to learn what makes a good summary by training on collections of summaries built for a relatively large number of training documents, e.g. (Hirao et al., 2002), (Teufel and Moens, 1997). However, the price paid for the high performance of such supervised algorithms is their inability to easily adapt to new languages or domains, as new training data are required for each new type of data. TextRank (Mihalcea and Tarau, 2004), (Mihalcea, 2004) is specifi49 cally designed to address this problem, by using an extractive summarization technique that does not require any training data or any language-specific knowledge sources. TextRank can be effectively applied to the summarization of documents in different languages without any modifications of the algorithm and without any requirements for additional data. Moreover, results from experiments performed on standard data sets have demonstrated that the performance of TextRank is competitive with that of some of the best summarization systems available today. 2 Extract</context>
<context position="13134" citStr="Mihalcea and Tarau, 2004" startWordPosition="2131" endWordPosition="2134">e achieved through a simple (but powerful) baseline. These results prove that graph-based ranking algorithms, previously found successful in Web link analysis and social networks, can be turned into a stateof-the-art tool for extractive summarization when applied to graphs extracted from texts. Moreover, due to its unsupervised nature, the algorithm was also shown to be language independent, leading to similar results and similar improvements over baseline techniques when applied on documents in different languages. More extensive experimental results with the TextRank system are reported in (Mihalcea and Tarau, 2004), (Mihalcea, 2004). 4 Conclusion Intuitively, iterative graph-based ranking algorithms work well on the task of extractive summarization because they do not only rely on the local context of a text unit (vertex), but they also take into account information recursively drawn from the entire text (graph). Through the graphs it builds on texts, a graph-based ranking algorithm identifies connections between various entities in a text, and implements the concept of recommendation. In the process of identifying important sentences in a text, a sentence recommends other sentences that address similar</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>R. Mihalcea and P. Tarau. 2004. TextRank – bringing order into texts. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2004), Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
</authors>
<title>Graph-based ranking algorithms for sentence extraction, applied to text summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Lingusitics (ACL</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="1559" citStr="Mihalcea, 2004" startWordPosition="239" endWordPosition="240">most important for the understanding of a given document. Some of the most successful approaches to extractive summarization consist of supervised algorithms that attempt to learn what makes a good summary by training on collections of summaries built for a relatively large number of training documents, e.g. (Hirao et al., 2002), (Teufel and Moens, 1997). However, the price paid for the high performance of such supervised algorithms is their inability to easily adapt to new languages or domains, as new training data are required for each new type of data. TextRank (Mihalcea and Tarau, 2004), (Mihalcea, 2004) is specifi49 cally designed to address this problem, by using an extractive summarization technique that does not require any training data or any language-specific knowledge sources. TextRank can be effectively applied to the summarization of documents in different languages without any modifications of the algorithm and without any requirements for additional data. Moreover, results from experiments performed on standard data sets have demonstrated that the performance of TextRank is competitive with that of some of the best summarization systems available today. 2 Extractive Summarization </context>
<context position="13152" citStr="Mihalcea, 2004" startWordPosition="2135" endWordPosition="2136">(but powerful) baseline. These results prove that graph-based ranking algorithms, previously found successful in Web link analysis and social networks, can be turned into a stateof-the-art tool for extractive summarization when applied to graphs extracted from texts. Moreover, due to its unsupervised nature, the algorithm was also shown to be language independent, leading to similar results and similar improvements over baseline techniques when applied on documents in different languages. More extensive experimental results with the TextRank system are reported in (Mihalcea and Tarau, 2004), (Mihalcea, 2004). 4 Conclusion Intuitively, iterative graph-based ranking algorithms work well on the task of extractive summarization because they do not only rely on the local context of a text unit (vertex), but they also take into account information recursively drawn from the entire text (graph). Through the graphs it builds on texts, a graph-based ranking algorithm identifies connections between various entities in a text, and implements the concept of recommendation. In the process of identifying important sentences in a text, a sentence recommends other sentences that address similar concepts as being</context>
</contexts>
<marker>Mihalcea, 2004</marker>
<rawString>R. Mihalcea. 2004. Graph-based ranking algorithms for sentence extraction, applied to text summarization. In Proceedings of the 42nd Annual Meeting of the Association for Computational Lingusitics (ACL 2004) (companion volume), Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T A S Pardo</author>
<author>L H M Rino</author>
</authors>
<title>TeMario: a corpus for automatic text summarization.</title>
<date>2003</date>
<tech>Technical report,</tech>
<pages>03--09</pages>
<contexts>
<context position="10282" citStr="Pardo and Rino, 2003" startWordPosition="1685" endWordPosition="1688">es generated for each of these documents. This is the single document summarization task undertaken by other systems participating in Figure 1: Graph of sentence similarities built on a sample text. Scores reflecting sentence importance are shown in brackets next to each sentence. the DUC 2002 document summarization evaluations. To test the language independence aspect of the algorithm, in addition to the English test collection, we also use a Brazilian Portuguese data set consisting of 100 news articles and their corresponding manually produced summaries. We use the TeM´ario test collection (Pardo and Rino, 2003), containing newspaper articles from online Brazilian newswire: 40 documents from Jornal de Brasil and 60 documents from Folha de S˜ao Paulo. The documents were selected to cover a variety of domains (e.g. world, politics, foreign affairs, editorials), and manual summaries were produced by an expert in Brazilian Portuguese. Unlike the summaries produced for the English DUC documents – which had a length requirement of approximately 100 words, the length of the summaries in the TeM´ario data set is constrained relative to the length of the corresponding documents, i.e. a summary has to account </context>
</contexts>
<marker>Pardo, Rino, 2003</marker>
<rawString>T.A.S. Pardo and L.H.M. Rino. 2003. TeMario: a corpus for automatic text summarization. Technical report, NILC-TR-03-09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Teufel</author>
<author>M Moens</author>
</authors>
<title>Sentence extraction as a classification task.</title>
<date>1997</date>
<booktitle>In ACL/EACL workshop on ”Intelligent and scalable Text summarization”,</booktitle>
<pages>58--65</pages>
<location>Madrid,</location>
<contexts>
<context position="1300" citStr="Teufel and Moens, 1997" startWordPosition="193" endWordPosition="196">e, techniques for efficient automatic text summarization are essential to improve the access to such information. Algorithms for extractive summarization are typically based on techniques for sentence extraction, and attempt to identify the set of sentences that are most important for the understanding of a given document. Some of the most successful approaches to extractive summarization consist of supervised algorithms that attempt to learn what makes a good summary by training on collections of summaries built for a relatively large number of training documents, e.g. (Hirao et al., 2002), (Teufel and Moens, 1997). However, the price paid for the high performance of such supervised algorithms is their inability to easily adapt to new languages or domains, as new training data are required for each new type of data. TextRank (Mihalcea and Tarau, 2004), (Mihalcea, 2004) is specifi49 cally designed to address this problem, by using an extractive summarization technique that does not require any training data or any language-specific knowledge sources. TextRank can be effectively applied to the summarization of documents in different languages without any modifications of the algorithm and without any requ</context>
</contexts>
<marker>Teufel, Moens, 1997</marker>
<rawString>S. Teufel and M. Moens. 1997. Sentence extraction as a classification task. In ACL/EACL workshop on ”Intelligent and scalable Text summarization”, pages 58–65, Madrid, Spain.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>