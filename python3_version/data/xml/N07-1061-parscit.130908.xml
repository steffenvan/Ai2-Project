<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000125">
<title confidence="0.9988285">
A Comparison of Pivot Methods for Phrase-based Statistical Machine
Translation
</title>
<author confidence="0.982037">
Masao Utiyama and Hitoshi Isahara
</author>
<affiliation confidence="0.994959">
National Institute of Information and Communications Technology
</affiliation>
<address confidence="0.986389">
3-5 Hikari-dai, Soraku-gun, Kyoto 619-0289 Japan
</address>
<email confidence="0.999425">
{mutiyama,isahara}@nict.go.jp
</email>
<sectionHeader confidence="0.998603" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999957464285714">
We compare two pivot strategies for
phrase-based statistical machine transla-
tion (SMT), namely phrase translation
and sentence translation. The phrase
translation strategy means that we di-
rectly construct a phrase translation ta-
ble (phrase-table) of the source and tar-
get language pair from two phrase-tables;
one constructed from the source language
and English and one constructed from En-
glish and the target language. We then use
that phrase-table in a phrase-based SMT
system. The sentence translation strat-
egy means that we first translate a source
language sentence into n English sen-
tences and then translate these n sentences
into target language sentences separately.
Then, we select the highest scoring sen-
tence from these target sentences. We con-
ducted controlled experiments using the
Europarl corpus to evaluate the perfor-
mance of these pivot strategies as com-
pared to directly trained SMT systems.
The phrase translation strategy signifi-
cantly outperformed the sentence transla-
tion strategy. Its relative performance was
0.92 to 0.97 compared to directly trained
SMT systems.
</bodyText>
<sectionHeader confidence="0.99951" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999949361111111">
The rapid and steady progress in corpus-based ma-
chine translation (Nagao, 1981; Brown et al., 1993)
has been supported by large parallel corpora such
as the Arabic-English and Chinese-English paral-
lel corpora distributed by the Linguistic Data Con-
sortium and the Europarl corpus (Koehn, 2005),
which consists of 11 European languages. How-
ever, large parallel corpora do not exist for many
language pairs. For example, there are no pub-
licly available Arabic-Chinese large-scale parallel
corpora even though there are Arabic-English and
Chinese-English parallel corpora.
Much work has been done to overcome the lack
of parallel corpora. For example, Resnik and Smith
(2003) propose mining the web to collect parallel
corpora for low-density language pairs. Utiyama
and Isahara (2003) extract Japanese-English parallel
sentences from a noisy-parallel corpus. Munteanu
and Marcu (2005) extract parallel sentences from
large Chinese, Arabic, and English non-parallel
newspaper corpora.
Researchers can also make the best use of exist-
ing (small) parallel corpora. For example, Nießen
and Ney (2004) use morpho-syntactic information to
take into account the interdependencies of inflected
forms of the same lemma in order to reduce the
amount of bilingual data necessary to sufficiently
cover the vocabulary in translation. Callison-Burch
et al. (2006a) use paraphrases to deal with unknown
source language phrases to improve coverage and
translation quality.
In this paper, we focus on situations where no par-
allel corpus is available (except a few hundred paral-
lel sentences for tuning parameters). To tackle these
extremely scarce training data situations, we pro-
pose using a pivot language (English) to bridge the
</bodyText>
<page confidence="0.986271">
484
</page>
<note confidence="0.797876">
Proceedings of NAACL HLT 2007, pages 484–491,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999531681818182">
source and target languages in translation. We first
translate source language sentences or phrases into
English and then translate those English sentences
or phrases into the target language, as described in
Section 3. We thus assume that there is a parallel
corpus consisting of the source language and En-
glish as well as one consisting of English and the tar-
get language. Selecting English as a pivot language
is a reasonable pragmatic choice because English is
included in parallel corpora more often than other
languages are, though any language can be used as a
pivot language.
In Section 2, we describe a phrase-based statisti-
cal machine translation (SMT) system that was used
to develop the pivot methods described in Section
3. This is the shared task baseline system for the
2006 NAACL/HLT workshop on statistical machine
translation (Koehn and Monz, 2006) and consists of
the Pharaoh decoder (Koehn, 2004), SRILM (Stol-
cke, 2002), GIZA++ (Och and Ney, 2003), mkcls
(Och, 1999), Carmel,1 and a phrase model training
code.
</bodyText>
<sectionHeader confidence="0.997079" genericHeader="method">
2 Phrase-based SMT
</sectionHeader>
<bodyText confidence="0.999131583333333">
We use a phrase-based SMT system, Pharaoh,
(Koehn et al., 2003; Koehn, 2004), which is based
on a log-linear formulation (Och and Ney, 2002). It
is a state-of-the-art SMT system with freely avail-
able software, as described in the introduction.
The system segments the source sentence into so-
called phrases (a number of sequences of consecu-
tive words). Each phrase is translated into a target
language phrase. Phrases may be reordered.
Let f be a source sentence (e.g, French) and e be a
target sentence (e.g., English), the SMT system out-
puts an e� that satisfies
</bodyText>
<equation confidence="0.997657333333333">
e� = argmaxPr(elf) (1)
e
amhm(e, f) (2)
</equation>
<bodyText confidence="0.99974">
where hm(e, f) is a feature function and am is a
weight. The system uses a total of eight feature
functions: a trigram language model probability of
the target language, two phrase translation probabil-
ities (both directions), two lexical translation prob-
</bodyText>
<footnote confidence="0.940791">
1http://www.isi.edu/licensed-sw/carmel/
</footnote>
<bodyText confidence="0.99933">
abilities (both directions), a word penalty, a phrase
penalty, and a linear reordering penalty. For details
on these feature functions, please refer to (Koehn et
al., 2003; Koehn, 2004; Koehn et al., 2005). To set
the weights, am, we carried out minimum error rate
training (Och, 2003) using BLEU (Papineni et al.,
2002) as the objective function.
</bodyText>
<sectionHeader confidence="0.999842" genericHeader="method">
3 Pivot methods
</sectionHeader>
<bodyText confidence="0.999976173913043">
We use the phrase-based SMT system described in
the previous section to develop pivot methods. We
use English e as the pivot language. We use French
f and German g as examples of the source and target
languages in this section.
We describe two types of pivot strategies, namely
phrase translation and sentence translation.
The phrase translation strategy means that we di-
rectly construct a French-German phrase translation
table (phrase-table for short) from a French-English
phrase-table and an English-German phrase-table.
We assume that these French-English and English-
German tables are built using the phrase model train-
ing code in the baseline system described in the
introduction. That is, phrases are heuristically ex-
tracted from word-level alignments produced by do-
ing GIZA++ training on the corresponding parallel
corpora (Koehn et al., 2003).
The sentence translation strategy means that we
first translate a French sentence into n English sen-
tences and translate these n sentences into German
separately. Then, we select the highest scoring sen-
tence from the German sentences.
</bodyText>
<subsectionHeader confidence="0.998017">
3.1 Phrase translation strategy
</subsectionHeader>
<bodyText confidence="0.999983923076923">
The phrase translation strategy is based on the fact
that the phrase-based SMT system needs a phrase-
table and a language model for translation. Usually,
we have the language model of a target language.
Consequently, we only need to construct a phrase-
table to train the phrase-based SMT system.
We assume that we have a French-English phrase-
table TFE and an English-German phrase-table
TEG. From these tables, we construct a French-
German phrase-table TFG, which requires estimat-
ing four feature functions; phrase translation prob-
abilities for both directions, 0(fIg) and 0(gI f) and
lexical translation probabilities for both directions,
</bodyText>
<equation confidence="0.9415684">
= arg max
e
M
E
m=1
</equation>
<page confidence="0.993654">
485
</page>
<bodyText confidence="0.999159">
pw( f|¯g) and pw(¯g |f), where f and g¯ are French and
German phrases that are parts of phrase translation
pairs in TFE and TEG, respectively.2
We estimate these probabilities using the proba-
bilities available in TFE and TEG as follows.3
</bodyText>
<equation confidence="0.9984992">
φ( ¯f|¯g) = φ(
X f|¯e)φ(¯e|¯g) (3)
¯eETFEnTEG
φ(¯g |¯f) = X φ(¯g|¯e)φ(¯e |f) (4)
¯eETFEnTEG
pw( ¯f|¯g) = pw(
X f|¯e)pw(¯e|¯g) (5)
¯eETFEnTEG
pw(¯g |¯f) = X pw(¯g|¯e)pw(¯e |f) (6)
¯eETFEnTEG
</equation>
<bodyText confidence="0.999945333333333">
where e¯ ∈ TFE ∩TEG means that the English phrase
e¯ is included in both TFE and TEG as part of phrase
translation pairs. φ(¯f|¯e) and φ(¯e |¯f) are phrase
translation probabilities for TFE and φ(¯e|¯g) and
φ(¯g|¯e) are those for TEG. pw(¯f|¯e) and pw(¯e |¯f) are
lexical translation probabilities for TFE and pw(¯e|¯g)
and pw(¯g|¯e) are those for TEG.
The definitions of the phrase and lexical transla-
tion probabilities are as follows (Koehn et al., 2003).
</bodyText>
<equation confidence="0.958737">
¯f|¯e) =count( f, ¯e)
P ¯f, count( ¯f&apos;, (7)
¯e)
</equation>
<bodyText confidence="0.999684166666667">
where count( f, ¯e) gives the total number of times
the phrase f¯ is aligned with the phrase e¯ in the par-
allel corpus. Eq. 7 means that φ(¯f|¯e) is calculated
using maximum likelihood estimation.
The definition of the lexical translation probabil-
ity is
</bodyText>
<equation confidence="0.999432333333333">
pw( f|¯e, a) (8)
Ew(fi|¯e, a) (9)
1 X
Ew(fi|¯e, a) = w(fi|ej)
|{j|(i, j) ∈ a}|b(i,j)Ea
(10)
</equation>
<bodyText confidence="0.9110534">
2Feature functions scores are calculated using these proba-
bilities. For example, for a translation probability of a French
sentence f = fl ... �fK and a German sentence g = gl ... gK,
h(g, f) = log QK iK1 O(�fi|gi), where K is the number of
phrases.
3Wang et al. (2006) use essentially the same definition to
induce the translation probability of the source and target lan-
guage word alignment that is bridged by an intermediate lan-
guage. Callison-Burch et al. (2006a) use a similar definition for
a paraphrase probability.
</bodyText>
<equation confidence="0.964088333333333">
wY
(f |e) = P f, count(f&apos;, e) 11
count(f, e) ( )
</equation>
<bodyText confidence="0.999942685714286">
where count(f, e) gives the total number of times
the word f is aligned with the word e in the par-
allel corpus. Thus, w(f|e) is the maximum likeli-
hood estimation of the word translation probability
of f given e. Ew(fi|¯e, a) is calculated from a word
f = f1f2 ...fn
alignment a between a phrase pair
and e¯ = e1e2 ... em where fi is connected to several
(|{j|(i, j) ∈ a}|) English words. Thus, Ew(fi|¯e, a)
is the average (or mixture) of w(fi|ej). This means
that Ew(fi|¯e, a) is an estimation of the probabil-
ity of fi in a. Consequently, pw(¯f|¯e, a) estimates
the probability of f¯ given e¯ and a using the prod-
uct of the probabilities Ew(fi|¯e, a). This assumes
that the probability of fi is independent given e¯ and
a. pw(¯f|¯e) takes the highest pw(¯f|¯e, a) if there
are multiple alignments a. This discussion, which
is partly based on Section 4.1.2 of (Och and Ney,
2004), means that the lexical translation probability
pw(¯f|¯e) is another probability estimated using the
word translation probability w(f|e).
The justification of Eqs. 3–6 is straightforward.
From the discussion above, we know that the prob-
abilities, φ( f|¯e), φ(¯e |f), φ(¯g|¯e), φ(¯e|¯g), pw( f|¯e),
pw(¯e |f), pw(¯g|¯e), and pw(¯e|¯g) are probabilities in
the ordinary sense. Thus, we can derive φ( f|¯g),
φ(¯g |f), pw( f|¯g), and pw(¯g |f) by assuming that
these probabilities are independent given an English
phrase e¯ (e.g., φ(
We construct a TFG that consists of all French-
German phrases whose phrase and lexical transla-
tion probabilities as defined in Eqs. 3–6 are greater
than 0. We use the term PhraseTrans to denote SMT
systems that use the phrase translation strategy de-
scribed above.
</bodyText>
<subsectionHeader confidence="0.999341">
3.2 Sentence translation strategy
</subsectionHeader>
<bodyText confidence="0.999858">
The sentence translation strategy uses two inde-
pendently trained SMT systems. We first trans-
late a French sentence f into n English sentences
e1, e2,..., en using a French-English SMT system.
Each ei (i = 1... n) has the eight scores calcu-
lated from the eight feature functions described in
Section 2. We denote these scores hei1, hei2, ... hei8.
Second, we translate each ei into n German sen-
tences gi1, gi2, . . . , gin using an English-German
</bodyText>
<equation confidence="0.983725">
φ(
pw( f|¯e) = max
a
pw( f|¯e, a) = Yn
i=1
f|¯g, ¯e) = φ(
f|¯e)).
</equation>
<page confidence="0.984907">
486
</page>
<bodyText confidence="0.987814666666667">
SMT system. Each gij (j = 1... n) has the eight
scores, which are denoted as hgij1, hgij2, ... , hgij8.
This situation is depicted as
f —* ei (he i1,he i2,...,he i8)
� gij (hg ij1,hg ij2,...,hg ij8)
We define the score of gij, S(gij), as
</bodyText>
<equation confidence="0.982698666666667">
8
S(gij) = E (λemheim + λgmhgijm) (12)
m=1
</equation>
<bodyText confidence="0.998432666666667">
where λe m and λgm are weights set by performing
minimum error rate training4 as described in Section
2. We select the highest scoring German sentence
</bodyText>
<equation confidence="0.97875">
g� = arg max S(gij) (13)
9ij
</equation>
<bodyText confidence="0.999699833333333">
as the translation of the French sentence f.
A drawback of this strategy is that translation
speed is about O(n) times slower than those of the
component SMT systems. This is because we have
to run the English-German SMT system n times for
a French sentence. Consequently, we cannot set n
very high. When we used n = 15 in the experi-
ments described in Section 4, it took more than two
days to translate 3064 test sentences on a 3.06GHz
LINUX machine.
Note that when n = 1, the above strategy pro-
duces the same translation with the simple sequen-
tial method that we first translate a French sentence
into an English sentence and then translate that sen-
tence into a German sentence.
We use the terms SntTrans15 and SntTrans1 to de-
note SMT systems that use the sentence translation
strategy with n = 15 and n = 1, respectively.
</bodyText>
<sectionHeader confidence="0.999819" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.995789954545454">
We conducted controlled experiments using the
Europarl corpus. For each language pair de-
scribed below, the Europarl corpus provides three
4We use a reranking strategy for the sentence translation
strategy. We first obtain n2 German sentences for each French
sentence by applying two independently trained French-English
and English-German SMT systems. Each of the translated Ger-
man sentences has the sixteen scores as described above. The
weights in Eq. 12 are tuned against reference German sentences
by performing minimum error rate training. These weights are
in general different from those of the original French-English
and English-German SMT systems.
types of parallel corpora; the source language–
English, English–the target language, and the source
language–the target language. This means that we
can directly train an SMT system using the source
and target language parallel corpus as well as pivot
SMT systems using English as the pivot language.
We use the term Direct to denote directly trained
SMT systems. For each language pair, we com-
pare four SMT systems; Direct, PhraseTrans, Snt-
Trans15, and SntTrans1.5
</bodyText>
<subsectionHeader confidence="0.997004">
4.1 Training, tuning and testing SMT systems
</subsectionHeader>
<bodyText confidence="0.999985846153846">
We used the training data for the shared task of
the SMT workshop (Koehn and Monz, 2006) to
train our SMT systems. It consists of three paral-
lel corpora: French-English, Spanish-English, and
German-English.
We used these three corpora to extract a set of
sentences that were aligned to each other across all
four languages. For that purpose, we used English
as the pivot. For each distinct English sentence, we
extracted the corresponding French, Spanish, and
German sentences. When an English sentence oc-
curred multiple times, we extracted the most fre-
quent translation. For example, because “Resump-
tion of the session” was translated into “Wiederauf-
nahme der Sitzungsperiode” 120 times and “Wieder-
aufnahme der Sitzung” once, we extracted “Wieder-
aufnahme der Sitzungsperiode” as its translation.
Consequently, we extracted 585,830 sentences for
each language. From these corpora, we constructed
the training parallel corpora for all language pairs.
We followed the instruction of the shared task
baseline system to train our SMT systems.6 We
used the trigram language models provided with the
shared task. We did minimum error rate training on
the first 500 sentences in the shared task develop-
ment data to tune our SMT systems and used the
</bodyText>
<footnote confidence="0.763776333333333">
5As discussed in the introduction, we intend to use the pivot
strategies in a situation where a very limited amount of parallel
text is available. The use of the Europarl corpus is not an accu-
rate simulation of the intended situation because it enables us to
use a relatively large parallel corpus for direct training. How-
ever, it is necessary to evaluate the performance of the pivot
strategies against that of Direct SMT systems under controlled
experiments in order to determine how much the pivot strate-
gies can be improved. This is a first step toward the use of pivot
methods in situations where training data is extremely scarce.
6The parameters for the Pharaoh decoder were “-dl 4 -b 0.03
-s 100”. The maximum phrase length was 7.
</footnote>
<page confidence="0.998105">
487
</page>
<bodyText confidence="0.9993622">
3064 test sentences for each language as our test set.
Our evaluation metric was %BLEU scores, as cal-
culated by the script provided along with the shared
task.7 We lowercased the training, development and
test sentences.
</bodyText>
<sectionHeader confidence="0.601959" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.999833066666667">
Table 1 compares the BLEU scores of the four SMT
systems; Direct, PhraseTrans, SntTrans15, and Snt-
Trans1 for each language pair. The columns SE and
ET list the BLEU scores of the Direct SMT sys-
tems trained on the source language–English and
English–the target language parallel corpora. The
numbers in the parentheses are the relative scores
of the pivot SMT systems, which were obtained
by dividing their BLEU scores by that of the cor-
responding Direct system. For example, for the
Spanish–French language pair, the BLEU score of
the Direct SMT system was 35.78, that of the
PhraseTrans SMT system was 32.90, and the rela-
tive performance was 0.92 = (32.90/35.78). For
the SntTrans15 SMT system, the BLEU score was
29.49 and the relative performance was 0.82 =
(29.49/35.78).
The BLEU scores of the Direct SMT systems
were higher than those of the PhraseTrans SMT sys-
tems for all six source-target language pairs. The
PhraseTrans SMT systems performed better than
the SntTrans15 SMT systems for all pairs. The
SntTrans15 SMT systems were better than the Snt-
Trans1 SMT systems for four pairs. According
to the sign test, under the null hypothesis that the
BLEU scores of two systems are equivalent, finding
one system obtaining better BLEU scores on all six
language pairs is statistically significant at the 5 %
level. Obtaining four better scores is not statistically
significant. Thus, Table 1 indicates
</bodyText>
<subsubsectionHeader confidence="0.60155">
Direct &gt; PhraseTrans &gt; SntTrans15 — SntTrans1
</subsubsectionHeader>
<bodyText confidence="0.999947">
where “&gt;” and “—” means that the differences of
the BLEU scores of the corresponding SMT systems
are statistically significant and insignificant, respec-
tively.
</bodyText>
<footnote confidence="0.612811">
7Callison-Burch et al. (2006b) show that in general a higher
BLEU score is not necessarily indicative of better translation
quality. However, they also suggest that the use of BLEU is
appropriate for comparing systems that use similar translation
strategies, which is the case with our experiments.
</footnote>
<bodyText confidence="0.999855466666667">
As expected, the Direct SMT systems outper-
formed the other systems. We regard the BLEU
scores of the Direct systems as the upperbound. The
SntTrans15 SMT systems did not significantly out-
perform the SntTrans1 SMT systems. We think that
this is because n = 15 was not large enough to cover
good translation candidates.8 Selecting the highest
scoring translation from a small pool did not always
lead to better performance. To improve the perfor-
mance of the sentence translation strategy, we need
to use a large n. However, this is not practical be-
cause of the slow translation speed, as discussed in
Section 3.2.
The PhraseTrans SMT systems significantly out-
performed the SntTrans15 and SntTrans1 systems.
That is, the phrase translation strategy is better
than the sentence translation strategy. Since the
phrase-tables constructed using the phrase transla-
tion strategy can be integrated into the Pharaoh de-
coder as well as the directly extracted phrase-tables,
the PhraseTrans SMT systems can fully exploit the
power of the decoder. This led to better performance
even when the induced phrase-tables were noisy, as
described below.
The relative performance of the PhraseTrans
SMT systems compared to the Direct SMT systems
was 0.92 to 0.97. These are very promising re-
sults. To show how these systems translated the
test sentences, we translated some outputs of the
Spanish-French Direct and PhraseTrans SMT sys-
tems into English using the French-English Direct
system. These are shown in Table 3 with the refer-
ence English sentences.
The relative performance seems to be related to
the BLEU scores for the Direct SMT systems. It
was relatively high (0.95 to 0.97) for the difficult (in
terms of BLEU) language pairs but relatively low
(0.92) for the easy language pairs; Spanish–French
and French–Spanish. There is a lot of room for
improvement for the relatively easy language pairs.
This relationship is stronger than the relationship be-
tween the BLEU scores for SE/ET and those for the
PhraseTrans systems, where no clear trend exists.
Table 2 shows the number of phrases stored in the
phrase-tables. The Direct SMT systems had 7.3 to
</bodyText>
<footnote confidence="0.9751">
8A typical reranking approach to SMT (Och et al., 2004)
uses a 1000–best list.
</footnote>
<page confidence="0.98885">
488
</page>
<table confidence="0.999842">
Source–Target Direct PhraseTrans SntTrans15 SntTrans1 SE ET
Spanish–French 35.78 &gt; 32.90 (0.92) &gt; 29.49 (0.82) &gt; 29.16 (0.81) 29.31 28.80
French–Spanish 34.16 &gt; 31.49 (0.92) &gt; 28.41 (0.83) &gt; 27.99 (0.82) 27.59 29.07
German–French 23.37 &gt; 22.47 (0.96) &gt; 22.03 (0.94) &gt; 21.64 (0.93) 22.40 28.80
French–German 15.27 &gt; 14.51 (0.95) &gt; 14.03 (0.92) &lt; 14.21 (0.93) 27.59 15.81
German–Spanish 22.34 &gt; 21.76 (0.97) &gt; 21.36 (0.96) &gt; 20.97 (0.94) 22.40 29.07
Spanish–German 15.50 &gt; 15.11 (0.97) &gt; 14.46 (0.93) &lt; 14.61 (0.94) 29.31 15.81
</table>
<tableCaption confidence="0.982306">
Table 1: BLEU scores and relative performance
</tableCaption>
<table confidence="0.999761875">
No. of phrases (“M” means 106) R P
Direct PhraseTrans common
S–F 18.2M 190.8M 6.3M 34.7 3.3
F–S 18.2M 186.8M 6.3M 34.7 3.4
G–F 7.3M 174.9M 3.1M 43.2 1.8
F–G 7.3M 168.2M 3.1M 43.2 1.9
G–S 7.5M 179.6M 3.3M 44.1 1.9
S–G 7.6M 176.6M 3.3M 44.1 1.9
</table>
<tableCaption confidence="0.65252925">
“S”, “F”, and “G” are the acronyms of Spanish, French, and
German, respectively. “X–Y” means that “X” is the source lan-
guage and “Y” is the target language.
Table 2: Statistics for the phrase-tables
</tableCaption>
<bodyText confidence="0.955993086956522">
18.2 million phrases, and the PhraseTrans systems
had 168.2 to 190.8 million phrases. The numbers of
phrases stored in the PhraseTrans systems were very
large compared to those of Direct systems.9 How-
ever, this does not cause a computational problem in
decoding because those phrases that do not appear in
source sentences are filtered so that only the relevant
phrases are used during decoding.
The figures in the common column are the number
of phrases common to the Direct and PhraseTrans
systems. R (recall) and P (precision) are defined as
follows.
R = No. of common phrases X 100
No. of phrases in Direct system
9In Table 2, the PhraseTrans systems have more than 10x
as many phrases as the Direct systems. This can be explained
as follows. Let fi be the fanout of an English phrase i, i.e.,
fi is the number of phrase pairs containing the English phrase
i in a phrase-table, then the size of the phrase-table is s1 =
Ei= 1 fi, where n is the number of distinct English phrases.
When we combine two phrase-tables, the size of the combined
phrase table is roughly s2 = E7 1 f2i . Thus, the relative size
of the combined phrase table is roughly r = s2 = E(f2)
</bodyText>
<equation confidence="0.38519">
s1 E(f) ,
</equation>
<bodyText confidence="0.970869666666667">
where E(f) = s1 nand E(f2) = s2n are the averages over
fi and f2i , respectively. As an example, we calculated these
averages for the German-English phrase table. E(f) was 1.5,
E(f2) was 43.7, and r was 28.9. This shows that even if an
average fanout is small, the size of a combined phrase table can
be very large.
</bodyText>
<equation confidence="0.871949">
P = No. of common phrases X 100
</equation>
<bodyText confidence="0.966131083333333">
No. of phrases in PhraseTrans system
Recall was reasonably high. However, the upper
bound of recall was 100 percent because we used
a multilingual corpus whose sentences were aligned
to each other across all four languages, as described
in Section 4.1. Thus, there is a lot of room for im-
provement with respect to recall. Precision, on the
other hand, was very low. However, translation per-
formance was not significantly affected by this low
precision, as is shown in Table 1. This indicates that
recall is more important than precision in building
phrase-tables.
</bodyText>
<sectionHeader confidence="0.999936" genericHeader="evaluation">
5 Related work
</sectionHeader>
<bodyText confidence="0.999933916666667">
Pivot languages have been used in rule-based ma-
chine translation systems. Boitet (1988) discusses
the pros and cons of the pivot approaches in multi-
lingual machine translation. Schubert (1988) argues
that a pivot language needs to be a natural language,
due to the inherent lack of expressiveness of artifi-
cial languages.
Pivot-based methods have also been used in other
related areas, such as translation lexicon induc-
tion (Schafer and Yarowsky, 2002), word alignment
(Wang et al., 2006), and cross language information
retrieval (Gollins and Sanderson, 2001). The trans-
lation disambiguation techniques used in these stud-
ies could be used for improving the quality of phrase
translation tables.
In contrast to these, very little work has been
done on pivot-based methods for SMT. Kauers et
al. (2002) used an artificial interlingua for spoken
language translation. Gispert and Mari˜no (2006)
created an English-Catalan parallel corpus by auto-
matically translating the Spanish part of an English-
Spanish parallel corpus into Catalan with a Spanish-
Catalan SMT system. They then directly trained an
SMT system on the English-Catalan corpus. They
</bodyText>
<page confidence="0.997482">
489
</page>
<bodyText confidence="0.99992225">
showed that this direct training method is superior
to the sentence translation strategy (SntTrans1) in
translating Catalan into English but is inferior to
it in the opposite translation direction (in terms of
the BLEU score). In contrast, we have shown that
the phrase translation strategy consistently outper-
formed the sentence translation strategy in the con-
trolled experiments.
</bodyText>
<sectionHeader confidence="0.999342" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.967810631578947">
We have compared two types of pivot strategies,
namely phrase translation and sentence translation.
The phrase translation strategy directly constructs a
phrase translation table from a source language and
English phrase-table and a target language and En-
glish phrase-table. It then uses this phrase table in
a phrase-based SMT system. The sentence transla-
tion strategy first translates a source language sen-
tence into n English sentences and translates these n
sentences into target language sentences separately.
Then, it selects the highest scoring sentence from the
target language sentences.
We conducted controlled experiments using the
Europarl corpus to compare the performance of
these two strategies to that of directly trained SMT
systems. The experiments showed that the perfor-
mance of the phrase translation strategy was statis-
tically significantly better than that of the sentence
translation strategy and that its relative performance
compared to the directly trained SMT systems was
0.92 to 0.97. These are very promising results.
Although we used the Europarl corpus for con-
trolled experiments, we intend to use the pivot strate-
gies in situations where very limited amount of par-
allel corpora are available for a source and target lan-
guage but where relatively large parallel corpora are
available for the source language–English and the
target language–English. In future work, we will
further investigate the pivot strategies described in
this paper to confirm that the phrase translation strat-
egy is better than the sentence translation strategy in
the intended situation as well as with the Europarl
corpus.10
10As a first step towards real situations, we conducted addi-
tional experiments. We divided the training corpora in Section
4 into two halves. We used the first 292915 sentences to train
source-English SMT systems and the remaining 292915 ones
to train English-target SMT systems. Based on these source-
</bodyText>
<sectionHeader confidence="0.990119" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997152162162162">
Christian Boitet. 1988. Pros and cons of the pivot and
transfer approaches in multilingual machine transla-
tion. In Dan Maxwell, Klaus Schubert, and Toon
Witkam, editors, New Directions in Machine Trans-
lation. Foris. (appeared in Sergei Nirenburg, Harold
Somers and Yorick Wilks (eds.) Readings in Machine
Translation published by the MIT Press in 2003).
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263–311.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006a. Improved statistical machine transla-
tion using paraphrases. In NAACL.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006b. Re-evaluating the role of BLEU in
machine translation research. In EACL.
Adri´a de Gispert and Jos´e B. Mari no. 2006. Catalan-
English statistical machine translation without parallel
corpus: Bridging through Spanish. In Proc. of LREC
5th Workshop on Strategies for developing Machine
Translation for Minority Languages.
Tim Gollins and Mark Sanderson. 2001. Improving
cross language information retrieval with triangulated
translation. In SIGIR.
Manuel Kauers, Stephan Vogel, Christian F¨ugen, and
Alex Waibel. 2002. Interlingua based statistical ma-
chine translation. In ICSLP.
Philipp Koehn and Christof Monz. 2006. Manual and au-
tomatic evaluation of machine translation between eu-
ropean languages. In Proceedings on the Workshop on
Statistical Machine Translation, pages 102–121, New
York City, June. Association for Computational Lin-
guistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In HLT-
NAACL.
</reference>
<bodyText confidence="0.694033833333333">
English and English-target SMT systems, we trained Phrase-
Trans and SntTrans1 SMT systems. Other experimental condi-
tions were the same as those described in Section 4. The table
below shows the BLUE scores of these SMT systems. It indi-
cates that the PhraseTrans systems consistently outperformed
the SntTrans1 systems.
</bodyText>
<reference confidence="0.831323142857143">
Source-Target PhraseTrans SntTrans1
Spanish-French 31.57 28.36
French-Spanish 30.18 27.75
German-French 20.48 19.83
French-German 14.38 14.11
German-Spanish 19.58 18.67
Spanish-German 14.80 14.46
</reference>
<page confidence="0.901619">
490
</page>
<table confidence="0.7695958">
Ref i hope with all my heart, and i must say this quite emphatically, that an opportunity will arise when this
document can be incorporated into the treaties at some point in the future .
Dir i hope with conviction , and put great emphasis , that again is a serious possibility of including this in the treaties .
PT i hope with conviction , and i very much , insisted that never be a serious possibility of including this in the
treaties .
Ref should this fail to materialise , we should not be surprised if public opinion proves sceptical about europe , or even
rejects it .
Dir otherwise , we must not be surprised by the scepticism , even the rejection of europe in the public .
PT otherwise , we must not be surprised by the scepticism , and even the rejection of europe in the public .
Ref the intergovernmental conference - to address a third subject - on the reform of the european institutions is also of
decisive significance for us in parliament.
Dir the intergovernmental conference - and this i turn to the third issue on the reform of the european institutions is of
enormous importance for the european parliament.
PT the intergovernmental conference - and this brings me to the third issue - on the reform of the european institutions
has enormous importance for the european parliament.
</table>
<tableCaption confidence="0.7982145">
Table 3: Reference sentences (Ref) and the English translations (by the French-English Direct system) of
the outputs of the Spanish-French Direct and PhraseTrans SMT systems (Dir and PT).
</tableCaption>
<reference confidence="0.99920125">
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation. In
IWSLT.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In AMTA.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31(4):477–504.
Makoto Nagao. 1981. A framework of a mechani-
cal translation between Japanese and English by anal-
ogy principle. In the International NATO Symposium
on Artificial and Human Intelligence. (appeared in
Sergei Nirenburg, Harold Somers and Yorick Wilks
(eds.) Readings in Machine Translation published by
the MIT Press in 2003).
Sonja Nießen and Hermann Ney. 2004. Statistical ma-
chine translation with scarce resources using morpho-
syntactic information. Computational Linguistics,
30(2):181–204.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In ACL.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417–449.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine trans-
lation. In HLT-NAACL.
Franz Josef Och. 1999. An efficient method for deter-
mining bilingual word classes. In EACL.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL.
Philip Resnik and Noah A. Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29(3):349–380.
Charles Schafer and David Yarowsky. 2002. Induc-
ing translation lexicons via diverse similarity measures
and bridge languages. In CoNLL.
Klaus Schubert. 1988. Implicitness as a guiding princi-
ple in machine translation. In COLING.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In ICSLP.
Masao Utiyama and Hitoshi Isahara. 2003. Reliable
measures for aligning Japanese-English news articles
and sentences. In ACL, pages 72–79.
Haifeng Wang, Hua Wu, and Zhanyi Liu. 2006. Word
alignment for languages with scarce resources using
bilingual corpora of other language pairs. In COL-
ING/ACL 2006 Main Conference Poster Sessions.
</reference>
<page confidence="0.998612">
491
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.483912">
<title confidence="0.99556">A Comparison of Pivot Methods for Phrase-based Statistical Machine Translation</title>
<author confidence="0.602381">Utiyama</author>
<affiliation confidence="0.874463">National Institute of Information and Communications</affiliation>
<address confidence="0.783881">3-5 Hikari-dai, Soraku-gun, Kyoto 619-0289</address>
<abstract confidence="0.997398206896552">We compare two pivot strategies for phrase-based statistical machine transla- (SMT), namely translation The phrase translation strategy means that we directly construct a phrase translation table (phrase-table) of the source and target language pair from two phrase-tables; one constructed from the source language and English and one constructed from English and the target language. We then use that phrase-table in a phrase-based SMT system. The sentence translation strategy means that we first translate a source sentence into senand then translate these into target language sentences separately. Then, we select the highest scoring sentence from these target sentences. We conducted controlled experiments using the Europarl corpus to evaluate the performance of these pivot strategies as compared to directly trained SMT systems. The phrase translation strategy significantly outperformed the sentence translation strategy. Its relative performance was 0.92 to 0.97 compared to directly trained SMT systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Christian Boitet</author>
</authors>
<title>Pros and cons of the pivot and transfer approaches in multilingual machine translation.</title>
<date>1988</date>
<booktitle>New Directions in Machine Translation. Foris. (appeared in Sergei Nirenburg, Harold Somers and Yorick Wilks (eds.) Readings in Machine Translation published by the</booktitle>
<editor>In Dan Maxwell, Klaus Schubert, and Toon Witkam, editors,</editor>
<publisher>MIT Press</publisher>
<contexts>
<context position="23396" citStr="Boitet (1988)" startWordPosition="3857" endWordPosition="3858">nably high. However, the upper bound of recall was 100 percent because we used a multilingual corpus whose sentences were aligned to each other across all four languages, as described in Section 4.1. Thus, there is a lot of room for improvement with respect to recall. Precision, on the other hand, was very low. However, translation performance was not significantly affected by this low precision, as is shown in Table 1. This indicates that recall is more important than precision in building phrase-tables. 5 Related work Pivot languages have been used in rule-based machine translation systems. Boitet (1988) discusses the pros and cons of the pivot approaches in multilingual machine translation. Schubert (1988) argues that a pivot language needs to be a natural language, due to the inherent lack of expressiveness of artificial languages. Pivot-based methods have also been used in other related areas, such as translation lexicon induction (Schafer and Yarowsky, 2002), word alignment (Wang et al., 2006), and cross language information retrieval (Gollins and Sanderson, 2001). The translation disambiguation techniques used in these studies could be used for improving the quality of phrase translation</context>
</contexts>
<marker>Boitet, 1988</marker>
<rawString>Christian Boitet. 1988. Pros and cons of the pivot and transfer approaches in multilingual machine translation. In Dan Maxwell, Klaus Schubert, and Toon Witkam, editors, New Directions in Machine Translation. Foris. (appeared in Sergei Nirenburg, Harold Somers and Yorick Wilks (eds.) Readings in Machine Translation published by the MIT Press in 2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1469" citStr="Brown et al., 1993" startWordPosition="210" endWordPosition="213">h sentences and then translate these n sentences into target language sentences separately. Then, we select the highest scoring sentence from these target sentences. We conducted controlled experiments using the Europarl corpus to evaluate the performance of these pivot strategies as compared to directly trained SMT systems. The phrase translation strategy significantly outperformed the sentence translation strategy. Its relative performance was 0.92 to 0.97 compared to directly trained SMT systems. 1 Introduction The rapid and steady progress in corpus-based machine translation (Nagao, 1981; Brown et al., 1993) has been supported by large parallel corpora such as the Arabic-English and Chinese-English parallel corpora distributed by the Linguistic Data Consortium and the Europarl corpus (Koehn, 2005), which consists of 11 European languages. However, large parallel corpora do not exist for many language pairs. For example, there are no publicly available Arabic-Chinese large-scale parallel corpora even though there are Arabic-English and Chinese-English parallel corpora. Much work has been done to overcome the lack of parallel corpora. For example, Resnik and Smith (2003) propose mining the web to c</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Miles Osborne</author>
</authors>
<title>Improved statistical machine translation using paraphrases.</title>
<date>2006</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="2714" citStr="Callison-Burch et al. (2006" startWordPosition="392" endWordPosition="395">rpora for low-density language pairs. Utiyama and Isahara (2003) extract Japanese-English parallel sentences from a noisy-parallel corpus. Munteanu and Marcu (2005) extract parallel sentences from large Chinese, Arabic, and English non-parallel newspaper corpora. Researchers can also make the best use of existing (small) parallel corpora. For example, Nießen and Ney (2004) use morpho-syntactic information to take into account the interdependencies of inflected forms of the same lemma in order to reduce the amount of bilingual data necessary to sufficiently cover the vocabulary in translation. Callison-Burch et al. (2006a) use paraphrases to deal with unknown source language phrases to improve coverage and translation quality. In this paper, we focus on situations where no parallel corpus is available (except a few hundred parallel sentences for tuning parameters). To tackle these extremely scarce training data situations, we propose using a pivot language (English) to bridge the 484 Proceedings of NAACL HLT 2007, pages 484–491, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics source and target languages in translation. We first translate source language sentences or phrases into En</context>
<context position="9045" citStr="Callison-Burch et al. (2006" startWordPosition="1427" endWordPosition="1430"> estimation. The definition of the lexical translation probability is pw( f|¯e, a) (8) Ew(fi|¯e, a) (9) 1 X Ew(fi|¯e, a) = w(fi|ej) |{j|(i, j) ∈ a}|b(i,j)Ea (10) 2Feature functions scores are calculated using these probabilities. For example, for a translation probability of a French sentence f = fl ... �fK and a German sentence g = gl ... gK, h(g, f) = log QK iK1 O(�fi|gi), where K is the number of phrases. 3Wang et al. (2006) use essentially the same definition to induce the translation probability of the source and target language word alignment that is bridged by an intermediate language. Callison-Burch et al. (2006a) use a similar definition for a paraphrase probability. wY (f |e) = P f, count(f&apos;, e) 11 count(f, e) ( ) where count(f, e) gives the total number of times the word f is aligned with the word e in the parallel corpus. Thus, w(f|e) is the maximum likelihood estimation of the word translation probability of f given e. Ew(fi|¯e, a) is calculated from a word f = f1f2 ...fn alignment a between a phrase pair and e¯ = e1e2 ... em where fi is connected to several (|{j|(i, j) ∈ a}|) English words. Thus, Ew(fi|¯e, a) is the average (or mixture) of w(fi|ej). This means that Ew(fi|¯e, a) is an estimation</context>
<context position="17701" citStr="Callison-Burch et al. (2006" startWordPosition="2887" endWordPosition="2890">Trans15 SMT systems were better than the SntTrans1 SMT systems for four pairs. According to the sign test, under the null hypothesis that the BLEU scores of two systems are equivalent, finding one system obtaining better BLEU scores on all six language pairs is statistically significant at the 5 % level. Obtaining four better scores is not statistically significant. Thus, Table 1 indicates Direct &gt; PhraseTrans &gt; SntTrans15 — SntTrans1 where “&gt;” and “—” means that the differences of the BLEU scores of the corresponding SMT systems are statistically significant and insignificant, respectively. 7Callison-Burch et al. (2006b) show that in general a higher BLEU score is not necessarily indicative of better translation quality. However, they also suggest that the use of BLEU is appropriate for comparing systems that use similar translation strategies, which is the case with our experiments. As expected, the Direct SMT systems outperformed the other systems. We regard the BLEU scores of the Direct systems as the upperbound. The SntTrans15 SMT systems did not significantly outperform the SntTrans1 SMT systems. We think that this is because n = 15 was not large enough to cover good translation candidates.8 Selecting </context>
</contexts>
<marker>Callison-Burch, Koehn, Osborne, 2006</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, and Miles Osborne. 2006a. Improved statistical machine translation using paraphrases. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>Re-evaluating the role of BLEU in machine translation research.</title>
<date>2006</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="2714" citStr="Callison-Burch et al. (2006" startWordPosition="392" endWordPosition="395">rpora for low-density language pairs. Utiyama and Isahara (2003) extract Japanese-English parallel sentences from a noisy-parallel corpus. Munteanu and Marcu (2005) extract parallel sentences from large Chinese, Arabic, and English non-parallel newspaper corpora. Researchers can also make the best use of existing (small) parallel corpora. For example, Nießen and Ney (2004) use morpho-syntactic information to take into account the interdependencies of inflected forms of the same lemma in order to reduce the amount of bilingual data necessary to sufficiently cover the vocabulary in translation. Callison-Burch et al. (2006a) use paraphrases to deal with unknown source language phrases to improve coverage and translation quality. In this paper, we focus on situations where no parallel corpus is available (except a few hundred parallel sentences for tuning parameters). To tackle these extremely scarce training data situations, we propose using a pivot language (English) to bridge the 484 Proceedings of NAACL HLT 2007, pages 484–491, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics source and target languages in translation. We first translate source language sentences or phrases into En</context>
<context position="9045" citStr="Callison-Burch et al. (2006" startWordPosition="1427" endWordPosition="1430"> estimation. The definition of the lexical translation probability is pw( f|¯e, a) (8) Ew(fi|¯e, a) (9) 1 X Ew(fi|¯e, a) = w(fi|ej) |{j|(i, j) ∈ a}|b(i,j)Ea (10) 2Feature functions scores are calculated using these probabilities. For example, for a translation probability of a French sentence f = fl ... �fK and a German sentence g = gl ... gK, h(g, f) = log QK iK1 O(�fi|gi), where K is the number of phrases. 3Wang et al. (2006) use essentially the same definition to induce the translation probability of the source and target language word alignment that is bridged by an intermediate language. Callison-Burch et al. (2006a) use a similar definition for a paraphrase probability. wY (f |e) = P f, count(f&apos;, e) 11 count(f, e) ( ) where count(f, e) gives the total number of times the word f is aligned with the word e in the parallel corpus. Thus, w(f|e) is the maximum likelihood estimation of the word translation probability of f given e. Ew(fi|¯e, a) is calculated from a word f = f1f2 ...fn alignment a between a phrase pair and e¯ = e1e2 ... em where fi is connected to several (|{j|(i, j) ∈ a}|) English words. Thus, Ew(fi|¯e, a) is the average (or mixture) of w(fi|ej). This means that Ew(fi|¯e, a) is an estimation</context>
<context position="17701" citStr="Callison-Burch et al. (2006" startWordPosition="2887" endWordPosition="2890">Trans15 SMT systems were better than the SntTrans1 SMT systems for four pairs. According to the sign test, under the null hypothesis that the BLEU scores of two systems are equivalent, finding one system obtaining better BLEU scores on all six language pairs is statistically significant at the 5 % level. Obtaining four better scores is not statistically significant. Thus, Table 1 indicates Direct &gt; PhraseTrans &gt; SntTrans15 — SntTrans1 where “&gt;” and “—” means that the differences of the BLEU scores of the corresponding SMT systems are statistically significant and insignificant, respectively. 7Callison-Burch et al. (2006b) show that in general a higher BLEU score is not necessarily indicative of better translation quality. However, they also suggest that the use of BLEU is appropriate for comparing systems that use similar translation strategies, which is the case with our experiments. As expected, the Direct SMT systems outperformed the other systems. We regard the BLEU scores of the Direct systems as the upperbound. The SntTrans15 SMT systems did not significantly outperform the SntTrans1 SMT systems. We think that this is because n = 15 was not large enough to cover good translation candidates.8 Selecting </context>
</contexts>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>Chris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006b. Re-evaluating the role of BLEU in machine translation research. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adri´a de Gispert</author>
<author>Jos´e B Mari no</author>
</authors>
<title>CatalanEnglish statistical machine translation without parallel corpus: Bridging through Spanish.</title>
<date>2006</date>
<booktitle>In Proc. of LREC 5th Workshop on Strategies for developing Machine Translation for Minority Languages.</booktitle>
<marker>de Gispert, no, 2006</marker>
<rawString>Adri´a de Gispert and Jos´e B. Mari no. 2006. CatalanEnglish statistical machine translation without parallel corpus: Bridging through Spanish. In Proc. of LREC 5th Workshop on Strategies for developing Machine Translation for Minority Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Gollins</author>
<author>Mark Sanderson</author>
</authors>
<title>Improving cross language information retrieval with triangulated translation.</title>
<date>2001</date>
<booktitle>In SIGIR.</booktitle>
<contexts>
<context position="23869" citStr="Gollins and Sanderson, 2001" startWordPosition="3928" endWordPosition="3931"> important than precision in building phrase-tables. 5 Related work Pivot languages have been used in rule-based machine translation systems. Boitet (1988) discusses the pros and cons of the pivot approaches in multilingual machine translation. Schubert (1988) argues that a pivot language needs to be a natural language, due to the inherent lack of expressiveness of artificial languages. Pivot-based methods have also been used in other related areas, such as translation lexicon induction (Schafer and Yarowsky, 2002), word alignment (Wang et al., 2006), and cross language information retrieval (Gollins and Sanderson, 2001). The translation disambiguation techniques used in these studies could be used for improving the quality of phrase translation tables. In contrast to these, very little work has been done on pivot-based methods for SMT. Kauers et al. (2002) used an artificial interlingua for spoken language translation. Gispert and Mari˜no (2006) created an English-Catalan parallel corpus by automatically translating the Spanish part of an EnglishSpanish parallel corpus into Catalan with a SpanishCatalan SMT system. They then directly trained an SMT system on the English-Catalan corpus. They 489 showed that t</context>
</contexts>
<marker>Gollins, Sanderson, 2001</marker>
<rawString>Tim Gollins and Mark Sanderson. 2001. Improving cross language information retrieval with triangulated translation. In SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manuel Kauers</author>
<author>Stephan Vogel</author>
<author>Christian F¨ugen</author>
<author>Alex Waibel</author>
</authors>
<title>Interlingua based statistical machine translation.</title>
<date>2002</date>
<booktitle>In ICSLP.</booktitle>
<marker>Kauers, Vogel, F¨ugen, Waibel, 2002</marker>
<rawString>Manuel Kauers, Stephan Vogel, Christian F¨ugen, and Alex Waibel. 2002. Interlingua based statistical machine translation. In ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
</authors>
<title>Manual and automatic evaluation of machine translation between european languages.</title>
<date>2006</date>
<booktitle>In Proceedings on the Workshop on Statistical Machine Translation,</booktitle>
<pages>102--121</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City,</location>
<contexts>
<context position="4073" citStr="Koehn and Monz, 2006" startWordPosition="608" endWordPosition="611"> is a parallel corpus consisting of the source language and English as well as one consisting of English and the target language. Selecting English as a pivot language is a reasonable pragmatic choice because English is included in parallel corpora more often than other languages are, though any language can be used as a pivot language. In Section 2, we describe a phrase-based statistical machine translation (SMT) system that was used to develop the pivot methods described in Section 3. This is the shared task baseline system for the 2006 NAACL/HLT workshop on statistical machine translation (Koehn and Monz, 2006) and consists of the Pharaoh decoder (Koehn, 2004), SRILM (Stolcke, 2002), GIZA++ (Och and Ney, 2003), mkcls (Och, 1999), Carmel,1 and a phrase model training code. 2 Phrase-based SMT We use a phrase-based SMT system, Pharaoh, (Koehn et al., 2003; Koehn, 2004), which is based on a log-linear formulation (Och and Ney, 2002). It is a state-of-the-art SMT system with freely available software, as described in the introduction. The system segments the source sentence into socalled phrases (a number of sequences of consecutive words). Each phrase is translated into a target language phrase. Phrases</context>
<context position="13931" citStr="Koehn and Monz, 2006" startWordPosition="2273" endWordPosition="2276">-German SMT systems. types of parallel corpora; the source language– English, English–the target language, and the source language–the target language. This means that we can directly train an SMT system using the source and target language parallel corpus as well as pivot SMT systems using English as the pivot language. We use the term Direct to denote directly trained SMT systems. For each language pair, we compare four SMT systems; Direct, PhraseTrans, SntTrans15, and SntTrans1.5 4.1 Training, tuning and testing SMT systems We used the training data for the shared task of the SMT workshop (Koehn and Monz, 2006) to train our SMT systems. It consists of three parallel corpora: French-English, Spanish-English, and German-English. We used these three corpora to extract a set of sentences that were aligned to each other across all four languages. For that purpose, we used English as the pivot. For each distinct English sentence, we extracted the corresponding French, Spanish, and German sentences. When an English sentence occurred multiple times, we extracted the most frequent translation. For example, because “Resumption of the session” was translated into “Wiederaufnahme der Sitzungsperiode” 120 times </context>
</contexts>
<marker>Koehn, Monz, 2006</marker>
<rawString>Philipp Koehn and Christof Monz. 2006. Manual and automatic evaluation of machine translation between european languages. In Proceedings on the Workshop on Statistical Machine Translation, pages 102–121, New York City, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In HLTNAACL.</booktitle>
<contexts>
<context position="4319" citStr="Koehn et al., 2003" startWordPosition="649" endWordPosition="652">more often than other languages are, though any language can be used as a pivot language. In Section 2, we describe a phrase-based statistical machine translation (SMT) system that was used to develop the pivot methods described in Section 3. This is the shared task baseline system for the 2006 NAACL/HLT workshop on statistical machine translation (Koehn and Monz, 2006) and consists of the Pharaoh decoder (Koehn, 2004), SRILM (Stolcke, 2002), GIZA++ (Och and Ney, 2003), mkcls (Och, 1999), Carmel,1 and a phrase model training code. 2 Phrase-based SMT We use a phrase-based SMT system, Pharaoh, (Koehn et al., 2003; Koehn, 2004), which is based on a log-linear formulation (Och and Ney, 2002). It is a state-of-the-art SMT system with freely available software, as described in the introduction. The system segments the source sentence into socalled phrases (a number of sequences of consecutive words). Each phrase is translated into a target language phrase. Phrases may be reordered. Let f be a source sentence (e.g, French) and e be a target sentence (e.g., English), the SMT system outputs an e� that satisfies e� = argmaxPr(elf) (1) e amhm(e, f) (2) where hm(e, f) is a feature function and am is a weight. T</context>
<context position="6370" citStr="Koehn et al., 2003" startWordPosition="975" endWordPosition="978">escribe two types of pivot strategies, namely phrase translation and sentence translation. The phrase translation strategy means that we directly construct a French-German phrase translation table (phrase-table for short) from a French-English phrase-table and an English-German phrase-table. We assume that these French-English and EnglishGerman tables are built using the phrase model training code in the baseline system described in the introduction. That is, phrases are heuristically extracted from word-level alignments produced by doing GIZA++ training on the corresponding parallel corpora (Koehn et al., 2003). The sentence translation strategy means that we first translate a French sentence into n English sentences and translate these n sentences into German separately. Then, we select the highest scoring sentence from the German sentences. 3.1 Phrase translation strategy The phrase translation strategy is based on the fact that the phrase-based SMT system needs a phrasetable and a language model for translation. Usually, we have the language model of a target language. Consequently, we only need to construct a phrasetable to train the phrase-based SMT system. We assume that we have a French-Engli</context>
<context position="8184" citStr="Koehn et al., 2003" startWordPosition="1273" endWordPosition="1276">¯g) (3) ¯eETFEnTEG φ(¯g |¯f) = X φ(¯g|¯e)φ(¯e |f) (4) ¯eETFEnTEG pw( ¯f|¯g) = pw( X f|¯e)pw(¯e|¯g) (5) ¯eETFEnTEG pw(¯g |¯f) = X pw(¯g|¯e)pw(¯e |f) (6) ¯eETFEnTEG where e¯ ∈ TFE ∩TEG means that the English phrase e¯ is included in both TFE and TEG as part of phrase translation pairs. φ(¯f|¯e) and φ(¯e |¯f) are phrase translation probabilities for TFE and φ(¯e|¯g) and φ(¯g|¯e) are those for TEG. pw(¯f|¯e) and pw(¯e |¯f) are lexical translation probabilities for TFE and pw(¯e|¯g) and pw(¯g|¯e) are those for TEG. The definitions of the phrase and lexical translation probabilities are as follows (Koehn et al., 2003). ¯f|¯e) =count( f, ¯e) P ¯f, count( ¯f&apos;, (7) ¯e) where count( f, ¯e) gives the total number of times the phrase f¯ is aligned with the phrase e¯ in the parallel corpus. Eq. 7 means that φ(¯f|¯e) is calculated using maximum likelihood estimation. The definition of the lexical translation probability is pw( f|¯e, a) (8) Ew(fi|¯e, a) (9) 1 X Ew(fi|¯e, a) = w(fi|ej) |{j|(i, j) ∈ a}|b(i,j)Ea (10) 2Feature functions scores are calculated using these probabilities. For example, for a translation probability of a French sentence f = fl ... �fK and a German sentence g = gl ... gK, h(g, f) = log QK iK1</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>Edinburgh system description for the 2005 IWSLT speech translation evaluation.</title>
<date>2005</date>
<booktitle>In IWSLT.</booktitle>
<contexts>
<context position="5360" citStr="Koehn et al., 2005" startWordPosition="817" endWordPosition="820"> a target sentence (e.g., English), the SMT system outputs an e� that satisfies e� = argmaxPr(elf) (1) e amhm(e, f) (2) where hm(e, f) is a feature function and am is a weight. The system uses a total of eight feature functions: a trigram language model probability of the target language, two phrase translation probabilities (both directions), two lexical translation prob1http://www.isi.edu/licensed-sw/carmel/ abilities (both directions), a word penalty, a phrase penalty, and a linear reordering penalty. For details on these feature functions, please refer to (Koehn et al., 2003; Koehn, 2004; Koehn et al., 2005). To set the weights, am, we carried out minimum error rate training (Och, 2003) using BLEU (Papineni et al., 2002) as the objective function. 3 Pivot methods We use the phrase-based SMT system described in the previous section to develop pivot methods. We use English e as the pivot language. We use French f and German g as examples of the source and target languages in this section. We describe two types of pivot strategies, namely phrase translation and sentence translation. The phrase translation strategy means that we directly construct a French-German phrase translation table (phrase-tabl</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh system description for the 2005 IWSLT speech translation evaluation. In IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In AMTA.</booktitle>
<contexts>
<context position="4123" citStr="Koehn, 2004" startWordPosition="618" endWordPosition="619"> English as well as one consisting of English and the target language. Selecting English as a pivot language is a reasonable pragmatic choice because English is included in parallel corpora more often than other languages are, though any language can be used as a pivot language. In Section 2, we describe a phrase-based statistical machine translation (SMT) system that was used to develop the pivot methods described in Section 3. This is the shared task baseline system for the 2006 NAACL/HLT workshop on statistical machine translation (Koehn and Monz, 2006) and consists of the Pharaoh decoder (Koehn, 2004), SRILM (Stolcke, 2002), GIZA++ (Och and Ney, 2003), mkcls (Och, 1999), Carmel,1 and a phrase model training code. 2 Phrase-based SMT We use a phrase-based SMT system, Pharaoh, (Koehn et al., 2003; Koehn, 2004), which is based on a log-linear formulation (Och and Ney, 2002). It is a state-of-the-art SMT system with freely available software, as described in the introduction. The system segments the source sentence into socalled phrases (a number of sequences of consecutive words). Each phrase is translated into a target language phrase. Phrases may be reordered. Let f be a source sentence (e.g</context>
<context position="5339" citStr="Koehn, 2004" startWordPosition="815" endWordPosition="816">nch) and e be a target sentence (e.g., English), the SMT system outputs an e� that satisfies e� = argmaxPr(elf) (1) e amhm(e, f) (2) where hm(e, f) is a feature function and am is a weight. The system uses a total of eight feature functions: a trigram language model probability of the target language, two phrase translation probabilities (both directions), two lexical translation prob1http://www.isi.edu/licensed-sw/carmel/ abilities (both directions), a word penalty, a phrase penalty, and a linear reordering penalty. For details on these feature functions, please refer to (Koehn et al., 2003; Koehn, 2004; Koehn et al., 2005). To set the weights, am, we carried out minimum error rate training (Och, 2003) using BLEU (Papineni et al., 2002) as the objective function. 3 Pivot methods We use the phrase-based SMT system described in the previous section to develop pivot methods. We use English e as the pivot language. We use French f and German g as examples of the source and target languages in this section. We describe two types of pivot strategies, namely phrase translation and sentence translation. The phrase translation strategy means that we directly construct a French-German phrase translati</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. In AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT Summit.</booktitle>
<contexts>
<context position="1662" citStr="Koehn, 2005" startWordPosition="241" endWordPosition="242">s using the Europarl corpus to evaluate the performance of these pivot strategies as compared to directly trained SMT systems. The phrase translation strategy significantly outperformed the sentence translation strategy. Its relative performance was 0.92 to 0.97 compared to directly trained SMT systems. 1 Introduction The rapid and steady progress in corpus-based machine translation (Nagao, 1981; Brown et al., 1993) has been supported by large parallel corpora such as the Arabic-English and Chinese-English parallel corpora distributed by the Linguistic Data Consortium and the Europarl corpus (Koehn, 2005), which consists of 11 European languages. However, large parallel corpora do not exist for many language pairs. For example, there are no publicly available Arabic-Chinese large-scale parallel corpora even though there are Arabic-English and Chinese-English parallel corpora. Much work has been done to overcome the lack of parallel corpora. For example, Resnik and Smith (2003) propose mining the web to collect parallel corpora for low-density language pairs. Utiyama and Isahara (2003) extract Japanese-English parallel sentences from a noisy-parallel corpus. Munteanu and Marcu (2005) extract pa</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragos Stefan Munteanu</author>
<author>Daniel Marcu</author>
</authors>
<title>Improving machine translation performance by exploiting non-parallel corpora.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>4</issue>
<contexts>
<context position="2251" citStr="Munteanu and Marcu (2005)" startWordPosition="324" endWordPosition="327">nd the Europarl corpus (Koehn, 2005), which consists of 11 European languages. However, large parallel corpora do not exist for many language pairs. For example, there are no publicly available Arabic-Chinese large-scale parallel corpora even though there are Arabic-English and Chinese-English parallel corpora. Much work has been done to overcome the lack of parallel corpora. For example, Resnik and Smith (2003) propose mining the web to collect parallel corpora for low-density language pairs. Utiyama and Isahara (2003) extract Japanese-English parallel sentences from a noisy-parallel corpus. Munteanu and Marcu (2005) extract parallel sentences from large Chinese, Arabic, and English non-parallel newspaper corpora. Researchers can also make the best use of existing (small) parallel corpora. For example, Nießen and Ney (2004) use morpho-syntactic information to take into account the interdependencies of inflected forms of the same lemma in order to reduce the amount of bilingual data necessary to sufficiently cover the vocabulary in translation. Callison-Burch et al. (2006a) use paraphrases to deal with unknown source language phrases to improve coverage and translation quality. In this paper, we focus on s</context>
</contexts>
<marker>Munteanu, Marcu, 2005</marker>
<rawString>Dragos Stefan Munteanu and Daniel Marcu. 2005. Improving machine translation performance by exploiting non-parallel corpora. Computational Linguistics, 31(4):477–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Nagao</author>
</authors>
<title>A framework of a mechanical translation between Japanese and English by analogy principle.</title>
<date>1981</date>
<booktitle>In the International NATO Symposium on Artificial and Human Intelligence. (appeared in Sergei Nirenburg, Harold Somers and Yorick Wilks (eds.) Readings in Machine Translation published by the</booktitle>
<publisher>MIT Press</publisher>
<contexts>
<context position="1448" citStr="Nagao, 1981" startWordPosition="208" endWordPosition="209">into n English sentences and then translate these n sentences into target language sentences separately. Then, we select the highest scoring sentence from these target sentences. We conducted controlled experiments using the Europarl corpus to evaluate the performance of these pivot strategies as compared to directly trained SMT systems. The phrase translation strategy significantly outperformed the sentence translation strategy. Its relative performance was 0.92 to 0.97 compared to directly trained SMT systems. 1 Introduction The rapid and steady progress in corpus-based machine translation (Nagao, 1981; Brown et al., 1993) has been supported by large parallel corpora such as the Arabic-English and Chinese-English parallel corpora distributed by the Linguistic Data Consortium and the Europarl corpus (Koehn, 2005), which consists of 11 European languages. However, large parallel corpora do not exist for many language pairs. For example, there are no publicly available Arabic-Chinese large-scale parallel corpora even though there are Arabic-English and Chinese-English parallel corpora. Much work has been done to overcome the lack of parallel corpora. For example, Resnik and Smith (2003) propos</context>
</contexts>
<marker>Nagao, 1981</marker>
<rawString>Makoto Nagao. 1981. A framework of a mechanical translation between Japanese and English by analogy principle. In the International NATO Symposium on Artificial and Human Intelligence. (appeared in Sergei Nirenburg, Harold Somers and Yorick Wilks (eds.) Readings in Machine Translation published by the MIT Press in 2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Nießen</author>
<author>Hermann Ney</author>
</authors>
<title>Statistical machine translation with scarce resources using morphosyntactic information.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>2</issue>
<contexts>
<context position="2462" citStr="Nießen and Ney (2004)" startWordPosition="355" endWordPosition="358">scale parallel corpora even though there are Arabic-English and Chinese-English parallel corpora. Much work has been done to overcome the lack of parallel corpora. For example, Resnik and Smith (2003) propose mining the web to collect parallel corpora for low-density language pairs. Utiyama and Isahara (2003) extract Japanese-English parallel sentences from a noisy-parallel corpus. Munteanu and Marcu (2005) extract parallel sentences from large Chinese, Arabic, and English non-parallel newspaper corpora. Researchers can also make the best use of existing (small) parallel corpora. For example, Nießen and Ney (2004) use morpho-syntactic information to take into account the interdependencies of inflected forms of the same lemma in order to reduce the amount of bilingual data necessary to sufficiently cover the vocabulary in translation. Callison-Burch et al. (2006a) use paraphrases to deal with unknown source language phrases to improve coverage and translation quality. In this paper, we focus on situations where no parallel corpus is available (except a few hundred parallel sentences for tuning parameters). To tackle these extremely scarce training data situations, we propose using a pivot language (Engl</context>
</contexts>
<marker>Nießen, Ney, 2004</marker>
<rawString>Sonja Nießen and Hermann Ney. 2004. Statistical machine translation with scarce resources using morphosyntactic information. Computational Linguistics, 30(2):181–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="4397" citStr="Och and Ney, 2002" startWordPosition="662" endWordPosition="665">t language. In Section 2, we describe a phrase-based statistical machine translation (SMT) system that was used to develop the pivot methods described in Section 3. This is the shared task baseline system for the 2006 NAACL/HLT workshop on statistical machine translation (Koehn and Monz, 2006) and consists of the Pharaoh decoder (Koehn, 2004), SRILM (Stolcke, 2002), GIZA++ (Och and Ney, 2003), mkcls (Och, 1999), Carmel,1 and a phrase model training code. 2 Phrase-based SMT We use a phrase-based SMT system, Pharaoh, (Koehn et al., 2003; Koehn, 2004), which is based on a log-linear formulation (Och and Ney, 2002). It is a state-of-the-art SMT system with freely available software, as described in the introduction. The system segments the source sentence into socalled phrases (a number of sequences of consecutive words). Each phrase is translated into a target language phrase. Phrases may be reordered. Let f be a source sentence (e.g, French) and e be a target sentence (e.g., English), the SMT system outputs an e� that satisfies e� = argmaxPr(elf) (1) e amhm(e, f) (2) where hm(e, f) is a feature function and am is a weight. The system uses a total of eight feature functions: a trigram language model pr</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="4174" citStr="Och and Ney, 2003" startWordPosition="625" endWordPosition="628"> and the target language. Selecting English as a pivot language is a reasonable pragmatic choice because English is included in parallel corpora more often than other languages are, though any language can be used as a pivot language. In Section 2, we describe a phrase-based statistical machine translation (SMT) system that was used to develop the pivot methods described in Section 3. This is the shared task baseline system for the 2006 NAACL/HLT workshop on statistical machine translation (Koehn and Monz, 2006) and consists of the Pharaoh decoder (Koehn, 2004), SRILM (Stolcke, 2002), GIZA++ (Och and Ney, 2003), mkcls (Och, 1999), Carmel,1 and a phrase model training code. 2 Phrase-based SMT We use a phrase-based SMT system, Pharaoh, (Koehn et al., 2003; Koehn, 2004), which is based on a log-linear formulation (Och and Ney, 2002). It is a state-of-the-art SMT system with freely available software, as described in the introduction. The system segments the source sentence into socalled phrases (a number of sequences of consecutive words). Each phrase is translated into a target language phrase. Phrases may be reordered. Let f be a source sentence (e.g, French) and e be a target sentence (e.g., English</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="10030" citStr="Och and Ney, 2004" startWordPosition="1609" endWordPosition="1612">...fn alignment a between a phrase pair and e¯ = e1e2 ... em where fi is connected to several (|{j|(i, j) ∈ a}|) English words. Thus, Ew(fi|¯e, a) is the average (or mixture) of w(fi|ej). This means that Ew(fi|¯e, a) is an estimation of the probability of fi in a. Consequently, pw(¯f|¯e, a) estimates the probability of f¯ given e¯ and a using the product of the probabilities Ew(fi|¯e, a). This assumes that the probability of fi is independent given e¯ and a. pw(¯f|¯e) takes the highest pw(¯f|¯e, a) if there are multiple alignments a. This discussion, which is partly based on Section 4.1.2 of (Och and Ney, 2004), means that the lexical translation probability pw(¯f|¯e) is another probability estimated using the word translation probability w(f|e). The justification of Eqs. 3–6 is straightforward. From the discussion above, we know that the probabilities, φ( f|¯e), φ(¯e |f), φ(¯g|¯e), φ(¯e|¯g), pw( f|¯e), pw(¯e |f), pw(¯g|¯e), and pw(¯e|¯g) are probabilities in the ordinary sense. Thus, we can derive φ( f|¯g), φ(¯g |f), pw( f|¯g), and pw(¯g |f) by assuming that these probabilities are independent given an English phrase e¯ (e.g., φ( We construct a TFG that consists of all FrenchGerman phrases whose ph</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Daniel Gildea</author>
</authors>
<title>Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar,</title>
<date>2004</date>
<booktitle>In HLT-NAACL.</booktitle>
<location>Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen</location>
<marker>Och, Gildea, 2004</marker>
<rawString>Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A smorgasbord of features for statistical machine translation. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>An efficient method for determining bilingual word classes.</title>
<date>1999</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="4193" citStr="Och, 1999" startWordPosition="630" endWordPosition="631">electing English as a pivot language is a reasonable pragmatic choice because English is included in parallel corpora more often than other languages are, though any language can be used as a pivot language. In Section 2, we describe a phrase-based statistical machine translation (SMT) system that was used to develop the pivot methods described in Section 3. This is the shared task baseline system for the 2006 NAACL/HLT workshop on statistical machine translation (Koehn and Monz, 2006) and consists of the Pharaoh decoder (Koehn, 2004), SRILM (Stolcke, 2002), GIZA++ (Och and Ney, 2003), mkcls (Och, 1999), Carmel,1 and a phrase model training code. 2 Phrase-based SMT We use a phrase-based SMT system, Pharaoh, (Koehn et al., 2003; Koehn, 2004), which is based on a log-linear formulation (Och and Ney, 2002). It is a state-of-the-art SMT system with freely available software, as described in the introduction. The system segments the source sentence into socalled phrases (a number of sequences of consecutive words). Each phrase is translated into a target language phrase. Phrases may be reordered. Let f be a source sentence (e.g, French) and e be a target sentence (e.g., English), the SMT system o</context>
</contexts>
<marker>Och, 1999</marker>
<rawString>Franz Josef Och. 1999. An efficient method for determining bilingual word classes. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="5440" citStr="Och, 2003" startWordPosition="833" endWordPosition="834">axPr(elf) (1) e amhm(e, f) (2) where hm(e, f) is a feature function and am is a weight. The system uses a total of eight feature functions: a trigram language model probability of the target language, two phrase translation probabilities (both directions), two lexical translation prob1http://www.isi.edu/licensed-sw/carmel/ abilities (both directions), a word penalty, a phrase penalty, and a linear reordering penalty. For details on these feature functions, please refer to (Koehn et al., 2003; Koehn, 2004; Koehn et al., 2005). To set the weights, am, we carried out minimum error rate training (Och, 2003) using BLEU (Papineni et al., 2002) as the objective function. 3 Pivot methods We use the phrase-based SMT system described in the previous section to develop pivot methods. We use English e as the pivot language. We use French f and German g as examples of the source and target languages in this section. We describe two types of pivot strategies, namely phrase translation and sentence translation. The phrase translation strategy means that we directly construct a French-German phrase translation table (phrase-table for short) from a French-English phrase-table and an English-German phrase-tab</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="5475" citStr="Papineni et al., 2002" startWordPosition="837" endWordPosition="840"> f) (2) where hm(e, f) is a feature function and am is a weight. The system uses a total of eight feature functions: a trigram language model probability of the target language, two phrase translation probabilities (both directions), two lexical translation prob1http://www.isi.edu/licensed-sw/carmel/ abilities (both directions), a word penalty, a phrase penalty, and a linear reordering penalty. For details on these feature functions, please refer to (Koehn et al., 2003; Koehn, 2004; Koehn et al., 2005). To set the weights, am, we carried out minimum error rate training (Och, 2003) using BLEU (Papineni et al., 2002) as the objective function. 3 Pivot methods We use the phrase-based SMT system described in the previous section to develop pivot methods. We use English e as the pivot language. We use French f and German g as examples of the source and target languages in this section. We describe two types of pivot strategies, namely phrase translation and sentence translation. The phrase translation strategy means that we directly construct a French-German phrase translation table (phrase-table for short) from a French-English phrase-table and an English-German phrase-table. We assume that these French-Eng</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>Noah A Smith</author>
</authors>
<title>The web as a parallel corpus.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="2041" citStr="Resnik and Smith (2003)" startWordPosition="296" endWordPosition="299">chine translation (Nagao, 1981; Brown et al., 1993) has been supported by large parallel corpora such as the Arabic-English and Chinese-English parallel corpora distributed by the Linguistic Data Consortium and the Europarl corpus (Koehn, 2005), which consists of 11 European languages. However, large parallel corpora do not exist for many language pairs. For example, there are no publicly available Arabic-Chinese large-scale parallel corpora even though there are Arabic-English and Chinese-English parallel corpora. Much work has been done to overcome the lack of parallel corpora. For example, Resnik and Smith (2003) propose mining the web to collect parallel corpora for low-density language pairs. Utiyama and Isahara (2003) extract Japanese-English parallel sentences from a noisy-parallel corpus. Munteanu and Marcu (2005) extract parallel sentences from large Chinese, Arabic, and English non-parallel newspaper corpora. Researchers can also make the best use of existing (small) parallel corpora. For example, Nießen and Ney (2004) use morpho-syntactic information to take into account the interdependencies of inflected forms of the same lemma in order to reduce the amount of bilingual data necessary to suff</context>
</contexts>
<marker>Resnik, Smith, 2003</marker>
<rawString>Philip Resnik and Noah A. Smith. 2003. The web as a parallel corpus. Computational Linguistics, 29(3):349–380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Schafer</author>
<author>David Yarowsky</author>
</authors>
<title>Inducing translation lexicons via diverse similarity measures and bridge languages. In CoNLL.</title>
<date>2002</date>
<contexts>
<context position="23761" citStr="Schafer and Yarowsky, 2002" startWordPosition="3913" endWordPosition="3916">ot significantly affected by this low precision, as is shown in Table 1. This indicates that recall is more important than precision in building phrase-tables. 5 Related work Pivot languages have been used in rule-based machine translation systems. Boitet (1988) discusses the pros and cons of the pivot approaches in multilingual machine translation. Schubert (1988) argues that a pivot language needs to be a natural language, due to the inherent lack of expressiveness of artificial languages. Pivot-based methods have also been used in other related areas, such as translation lexicon induction (Schafer and Yarowsky, 2002), word alignment (Wang et al., 2006), and cross language information retrieval (Gollins and Sanderson, 2001). The translation disambiguation techniques used in these studies could be used for improving the quality of phrase translation tables. In contrast to these, very little work has been done on pivot-based methods for SMT. Kauers et al. (2002) used an artificial interlingua for spoken language translation. Gispert and Mari˜no (2006) created an English-Catalan parallel corpus by automatically translating the Spanish part of an EnglishSpanish parallel corpus into Catalan with a SpanishCatala</context>
</contexts>
<marker>Schafer, Yarowsky, 2002</marker>
<rawString>Charles Schafer and David Yarowsky. 2002. Inducing translation lexicons via diverse similarity measures and bridge languages. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Schubert</author>
</authors>
<title>Implicitness as a guiding principle in machine translation.</title>
<date>1988</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="23501" citStr="Schubert (1988)" startWordPosition="3873" endWordPosition="3874">ose sentences were aligned to each other across all four languages, as described in Section 4.1. Thus, there is a lot of room for improvement with respect to recall. Precision, on the other hand, was very low. However, translation performance was not significantly affected by this low precision, as is shown in Table 1. This indicates that recall is more important than precision in building phrase-tables. 5 Related work Pivot languages have been used in rule-based machine translation systems. Boitet (1988) discusses the pros and cons of the pivot approaches in multilingual machine translation. Schubert (1988) argues that a pivot language needs to be a natural language, due to the inherent lack of expressiveness of artificial languages. Pivot-based methods have also been used in other related areas, such as translation lexicon induction (Schafer and Yarowsky, 2002), word alignment (Wang et al., 2006), and cross language information retrieval (Gollins and Sanderson, 2001). The translation disambiguation techniques used in these studies could be used for improving the quality of phrase translation tables. In contrast to these, very little work has been done on pivot-based methods for SMT. Kauers et a</context>
</contexts>
<marker>Schubert, 1988</marker>
<rawString>Klaus Schubert. 1988. Implicitness as a guiding principle in machine translation. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In ICSLP.</booktitle>
<contexts>
<context position="4146" citStr="Stolcke, 2002" startWordPosition="621" endWordPosition="623">ne consisting of English and the target language. Selecting English as a pivot language is a reasonable pragmatic choice because English is included in parallel corpora more often than other languages are, though any language can be used as a pivot language. In Section 2, we describe a phrase-based statistical machine translation (SMT) system that was used to develop the pivot methods described in Section 3. This is the shared task baseline system for the 2006 NAACL/HLT workshop on statistical machine translation (Koehn and Monz, 2006) and consists of the Pharaoh decoder (Koehn, 2004), SRILM (Stolcke, 2002), GIZA++ (Och and Ney, 2003), mkcls (Och, 1999), Carmel,1 and a phrase model training code. 2 Phrase-based SMT We use a phrase-based SMT system, Pharaoh, (Koehn et al., 2003; Koehn, 2004), which is based on a log-linear formulation (Och and Ney, 2002). It is a state-of-the-art SMT system with freely available software, as described in the introduction. The system segments the source sentence into socalled phrases (a number of sequences of consecutive words). Each phrase is translated into a target language phrase. Phrases may be reordered. Let f be a source sentence (e.g, French) and e be a ta</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masao Utiyama</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Reliable measures for aligning Japanese-English news articles and sentences.</title>
<date>2003</date>
<booktitle>In ACL,</booktitle>
<pages>72--79</pages>
<contexts>
<context position="2151" citStr="Utiyama and Isahara (2003)" startWordPosition="312" endWordPosition="315">e Arabic-English and Chinese-English parallel corpora distributed by the Linguistic Data Consortium and the Europarl corpus (Koehn, 2005), which consists of 11 European languages. However, large parallel corpora do not exist for many language pairs. For example, there are no publicly available Arabic-Chinese large-scale parallel corpora even though there are Arabic-English and Chinese-English parallel corpora. Much work has been done to overcome the lack of parallel corpora. For example, Resnik and Smith (2003) propose mining the web to collect parallel corpora for low-density language pairs. Utiyama and Isahara (2003) extract Japanese-English parallel sentences from a noisy-parallel corpus. Munteanu and Marcu (2005) extract parallel sentences from large Chinese, Arabic, and English non-parallel newspaper corpora. Researchers can also make the best use of existing (small) parallel corpora. For example, Nießen and Ney (2004) use morpho-syntactic information to take into account the interdependencies of inflected forms of the same lemma in order to reduce the amount of bilingual data necessary to sufficiently cover the vocabulary in translation. Callison-Burch et al. (2006a) use paraphrases to deal with unkno</context>
</contexts>
<marker>Utiyama, Isahara, 2003</marker>
<rawString>Masao Utiyama and Hitoshi Isahara. 2003. Reliable measures for aligning Japanese-English news articles and sentences. In ACL, pages 72–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haifeng Wang</author>
<author>Hua Wu</author>
<author>Zhanyi Liu</author>
</authors>
<title>Word alignment for languages with scarce resources using bilingual corpora of other language pairs.</title>
<date>2006</date>
<booktitle>In COLING/ACL 2006 Main Conference Poster Sessions.</booktitle>
<contexts>
<context position="8849" citStr="Wang et al. (2006)" startWordPosition="1396" endWordPosition="1399">e) where count( f, ¯e) gives the total number of times the phrase f¯ is aligned with the phrase e¯ in the parallel corpus. Eq. 7 means that φ(¯f|¯e) is calculated using maximum likelihood estimation. The definition of the lexical translation probability is pw( f|¯e, a) (8) Ew(fi|¯e, a) (9) 1 X Ew(fi|¯e, a) = w(fi|ej) |{j|(i, j) ∈ a}|b(i,j)Ea (10) 2Feature functions scores are calculated using these probabilities. For example, for a translation probability of a French sentence f = fl ... �fK and a German sentence g = gl ... gK, h(g, f) = log QK iK1 O(�fi|gi), where K is the number of phrases. 3Wang et al. (2006) use essentially the same definition to induce the translation probability of the source and target language word alignment that is bridged by an intermediate language. Callison-Burch et al. (2006a) use a similar definition for a paraphrase probability. wY (f |e) = P f, count(f&apos;, e) 11 count(f, e) ( ) where count(f, e) gives the total number of times the word f is aligned with the word e in the parallel corpus. Thus, w(f|e) is the maximum likelihood estimation of the word translation probability of f given e. Ew(fi|¯e, a) is calculated from a word f = f1f2 ...fn alignment a between a phrase pa</context>
<context position="23797" citStr="Wang et al., 2006" startWordPosition="3919" endWordPosition="3922">on, as is shown in Table 1. This indicates that recall is more important than precision in building phrase-tables. 5 Related work Pivot languages have been used in rule-based machine translation systems. Boitet (1988) discusses the pros and cons of the pivot approaches in multilingual machine translation. Schubert (1988) argues that a pivot language needs to be a natural language, due to the inherent lack of expressiveness of artificial languages. Pivot-based methods have also been used in other related areas, such as translation lexicon induction (Schafer and Yarowsky, 2002), word alignment (Wang et al., 2006), and cross language information retrieval (Gollins and Sanderson, 2001). The translation disambiguation techniques used in these studies could be used for improving the quality of phrase translation tables. In contrast to these, very little work has been done on pivot-based methods for SMT. Kauers et al. (2002) used an artificial interlingua for spoken language translation. Gispert and Mari˜no (2006) created an English-Catalan parallel corpus by automatically translating the Spanish part of an EnglishSpanish parallel corpus into Catalan with a SpanishCatalan SMT system. They then directly tra</context>
</contexts>
<marker>Wang, Wu, Liu, 2006</marker>
<rawString>Haifeng Wang, Hua Wu, and Zhanyi Liu. 2006. Word alignment for languages with scarce resources using bilingual corpora of other language pairs. In COLING/ACL 2006 Main Conference Poster Sessions.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>