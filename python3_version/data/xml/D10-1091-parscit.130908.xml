<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.992333">
Assessing Phrase-Based Translation Models with Oracle Decoding
</title>
<author confidence="0.877915">
Guillaume Wisniewski and Alexandre Allauzen and Fran¸cois Yvon
</author>
<affiliation confidence="0.688775">
Univ. Paris Sud; LIMSI—CNRS
</affiliation>
<address confidence="0.6259975">
91403 ORSAY CEDEX
France
</address>
<email confidence="0.997557">
{wisniews,allauzen,yvon}@limsi.fr
</email>
<sectionHeader confidence="0.9975" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.989041285714286">
Extant Statistical Machine Translation (SMT) sys-
tems are very complex softwares, which embed mul-
tiple layers of heuristics and embark very large num-
bers of numerical parameters. As a result, it is diffi-
cult to analyze output translations and there is a real
need for tools that could help developers to better
understand the various causes of errors.
In this study, we make a step in that direction and
present an attempt to evaluate the quality of the
phrase-based translation model. In order to identify
those translation errors that stem from deficiencies
in the phrase table (PT), we propose to compute the
oracle BLEU-4 score, that is the best score that a
system based on this PT can achieve on a reference
corpus. By casting the computation of the oracle
BLEU-1 as an Integer Linear Programming (ILP)
problem, we show that it is possible to efficiently
compute accurate lower-bounds of this score, and re-
port measures performed on several standard bench-
marks. Various other applications of these oracle de-
coding techniques are also reported and discussed.
</bodyText>
<sectionHeader confidence="0.969469" genericHeader="method">
1 Phrase-Based Machine Translation
</sectionHeader>
<subsectionHeader confidence="0.963639">
1.1 Principle
</subsectionHeader>
<bodyText confidence="0.99981875">
A Phrase-Based Translation System (PBTS) consists of a
ruleset and a scoring function (Lopez, 2009). The ruleset,
represented in the phrase table, is a set of phrase1pairs
{(f, e)}, each pair expressing that the source phrase f
can be rewritten (translated) into a target phrase e. Trans-
lation hypotheses are generated by iteratively rewriting
portions of the source sentence as prescribed by the rule-
set, until each source word has been consumed by exactly
one rule. The order of target words in an hypothesis is
uniquely determined by the order in which the rewrite op-
eration are performed. The search space of the translation
model corresponds to the set of all possible sequences of
</bodyText>
<footnote confidence="0.8733085">
1Following the usage in statistical machine translation literature, we
use “phrase” to denote a subsequence of consecutive words.
</footnote>
<bodyText confidence="0.998850111111111">
rules applications. The scoring function aims to rank all
possible translation hypotheses in such a way that the best
one has the highest score.
A PBTS is learned from a parallel corpus in two inde-
pendent steps. In a first step, the corpus is aligned at the
word level, by using alignment tools such as Giza++
(Och and Ney, 2003) and some symmetrisation heuris-
tics; phrases are then extracted by other heuristics (Koehn
et al., 2003) and assigned numerical weights. In the
second step, the parameters of the scoring function are
estimated, typically through Minimum Error Rate train-
ing (Och, 2003).
Translating a sentence amounts to finding the best scor-
ing translation hypothesis in the search space. Because
of the combinatorial nature of this problem, translation
has to rely on heuristic search techniques such as greedy
hill-climbing (Germann, 2003) or variants of best-first
search like multi-stack decoding (Koehn, 2004). More-
over, to reduce the overall complexity of decoding, the
search space is typically pruned using simple heuristics.
For instance, the state-of-the-art phrase-based decoder
Moses (Koehn et al., 2007) considers only a restricted
number of translations for each source sequence2 and en-
forces a distortion limit3 over which phrases can be re-
ordered. As a consequence, the best translation hypothe-
sis returned by the decoder is not always the one with the
highest score.
</bodyText>
<subsectionHeader confidence="0.993588">
1.2 Typology of PBTS Errors
</subsectionHeader>
<bodyText confidence="0.999947444444444">
Analyzing the errors of a SMT system is not an easy task,
because of the number of models that are combined, the
size of these models, and the high complexity of the vari-
ous decision making processes. For a SMT system, three
different kinds of errors can be distinguished (Germann
et al., 2004; Auli et al., 2009): search errors, induction
errors and model errors. The former corresponds to cases
where the hypothesis with the best score is missed by
the search procedure, either because of the use of an ap-
</bodyText>
<footnote confidence="0.998311">
2the ttl option of Moses, defaulting to 20.
3the dl option of Moses, whose default value is 7.
</footnote>
<page confidence="0.924575">
933
</page>
<note confidence="0.819117">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 933–943,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999896451612903">
proximate search method or because of the restrictions of
the search space. Induction errors correspond to cases
where, given the model, the search space does not contain
the reference. Finally, model errors correspond to cases
where the hypothesis with the highest score is not the best
translation according to the evaluation metric.
Model errors encompass several types of errors that oc-
cur during learning (Bottou and Bousquet, 2008)4. Ap-
proximation errors are errors caused by the use of a re-
stricted and oversimplistic class of functions (here, finite-
state transducers to model the generation of hypotheses
and a linear scoring function to discriminate them) to
model the translation process. Estimation errors corre-
spond to the use of sub-optimal values for both the phrase
pairs weights and the parameters of the scoring function.
The reasons behind these errors are twofold: first, train-
ing only considers a finite sample of data; second, it re-
lies on error prone alignments. As a result, some “good”
phrases are extracted with a small weight, or, in the limit,
are not extracted at all; and conversely that some “poor”
phrases are inserted into the phrase table, sometimes with
a really optimistic score.
Sorting out and assessing the impact of these various
causes of errors is of primary interest for SMT system
developers: for lack of such diagnoses, it is difficult to
figure out which components of the system require the
most urgent attention. Diagnoses are however, given the
tight intertwining among the various component of a sys-
tem, very difficult to obtain: most evaluations are limited
to the computation of global scores and usually do not
imply any kind of failure analysis.
</bodyText>
<subsectionHeader confidence="0.981578">
1.3 Contribution and organization
</subsectionHeader>
<bodyText confidence="0.999950157894737">
To systematically assess the impact of the multiple
heuristic decisions made during training and decoding,
we propose, following (Dreyer et al., 2007; Auli et al.,
2009), to work out oracle scores, that is to evaluate the
best achievable performances of a PBTS. We aim at both
studying the expressive power of PBTS and at providing
tools for identifying and quantifying causes of failure.
Under standard metrics such as BLEU (Papineni et al.,
2002), oracle scores are difficult (if not impossible) to
compute, but, by casting the computation of the oracle
unigram recall and precision as an Integer Linear Pro-
gramming (ILP) problem, we show that it is possible to
efficiently compute accurate lower-bounds of the oracle
BLEU-4 scores and report measurements performed on
several standard benchmarks.
The main contributions of this paper are twofold. We
first introduce an ILP program able to efficiently find
the best hypothesis a PBTS can achieve. This program
can be easily extended to test various improvements to
</bodyText>
<footnote confidence="0.654584">
4We omit here optimization errors.
</footnote>
<bodyText confidence="0.999948269230769">
phrase-base systems or to evaluate the impact of differ-
ent parameter settings. Second, we present a number of
complementary results illustrating the usage of our or-
acle decoder for identifying and analyzing PBTS errors.
Our experimental results confirm the main conclusions of
(Turchi et al., 2008), showing that extant PBTs have the
potential to generate hypotheses having very high BLEU-
4 score and that their main bottleneck is their scoring
function.
The rest of this paper is organized as follows: in Sec-
tion 2, we introduce and formalize the oracle decoding
problem, and present a series of ILP problems of increas-
ing complexity designed so as to deliver accurate lower-
bounds of oracle score. This section closes with various
extensions allowing to model supplementary constraints,
most notably reordering constraints (Section 2.5). Our
experiments are reported in Section 3, where we first in-
troduce the training and test corpora, along with a de-
scription of our system building pipeline (Section 3.1).
We then discuss the baseline oracle BLEU scores (Sec-
tion 3.2), analyze the non-reachable parts of the reference
translations, and comment several complementary results
which allow to identify causes of failures. Section 4 dis-
cuss our approach and findings with respect to the exist-
ing literature on error analysis and oracle decoding. We
conclude and discuss further prospects in Section 5.
</bodyText>
<sectionHeader confidence="0.993883" genericHeader="method">
2 Oracle Decoder
</sectionHeader>
<subsectionHeader confidence="0.998421">
2.1 The Oracle Decoding Problem
</subsectionHeader>
<bodyText confidence="0.999812523809524">
Definition To get some insights on the errors of phrase-
based systems and better understand their limits, we pro-
pose to consider the oracle decoding problem defined as
follows: given a source sentence, its reference transla-
tion5 and a phrase table, what is the “best” translation
hypothesis a system can generate? As usual, the quality
of an hypothesis is evaluated by the similarity between
the reference and the hypothesis. Note that in the ora-
cle decoding problem, we are only assessing the ability
of PBT systems to generate good candidate translations,
irrespective of their ability to score them properly.
We believe that studying this problem is interesting for
various reasons. First, as described in Section 3.4, com-
paring the best hypothesis a system could have gener-
ated and the hypothesis it actually generates allows us to
carry on both quantitative and qualitative failure analysis.
The oracle decoding problem can also be used to assess
the expressive power of phrase-based systems (Auli et
al., 2009). Other applications include computing accept-
able pseudo-references for discriminative training (Till-
mann and Zhang, 2006; Liang et al., 2006; Arun and
</bodyText>
<footnote confidence="0.993998333333333">
5The oracle decoding problem can be extended to the case of multi-
ple references. For the sake of simplicity, we only describe the case of
a single reference.
</footnote>
<page confidence="0.998132">
934
</page>
<bodyText confidence="0.999917333333333">
Koehn, 2007) or combining machine translation systems
in a multi-source setting (Li and Khudanpur, 2009). We
have also used oracle decoding to identify erroneous or
difficult to translate references (Section 3.3).
Evaluation Measure To fully define the oracle de-
coding problem, a measure of the similarity between a
translation hypothesis and its reference translation has
to be chosen. The most obvious choice is the BLEU-4
score (Papineni et al., 2002) used in most machine trans-
lation evaluations.
However, using this metric in the oracle decoding
problem raises several issues. First, BLEU-4 is a met-
ric defined at the corpus level and is hard to interpret at
the sentence level. More importantly, BLEU-4 is not de-
composable6: as it relies on 4-grams statistics, the con-
tribution of each phrase pair to the global score depends
on the translation of the previous and following phrases
and can not be evaluated in isolation. Because of its non-
decomposability, maximizing BLEU-4 is hard; in partic-
ular, the phrase-level decomposability of the evaluation
metric is necessary in our approach.
To circumvent this difficulty, we propose to evaluate
the similarity between a translation hypothesis and a ref-
erence by the number of their common words. This
amounts to evaluating translation quality in terms of un-
igram precision and recall, which are highly correlated
with human judgements (Lavie et al., ). This measure
is closely related to the BLEU-1 evaluation metric and
the Meteor (Banerjee and Lavie, 2005) metric (when it is
evaluated without considering near-matches and the dis-
tortion penalty). We also believe that hypotheses that
maximize the unigram precision and recall at the sen-
tence level yield corpus level BLEU-4 scores close the
maximal achievable. Indeed, in the setting we will intro-
duce in the next section, BLEU-1 and BLEU-4 are highly
correlated: as all correct words of the hypothesis will be
compelled to be at their correct position, any hypothesis
with a high 1-gram precision is also bound to have a high
2-gram precision, etc.
</bodyText>
<subsectionHeader confidence="0.999237">
2.2 Formalizing the Oracle Decoding Problem
</subsectionHeader>
<bodyText confidence="0.999944777777778">
The oracle decoding problem has already been consid-
ered in the case of word-based models, in which all trans-
lation units are bound to contain only one word. The
problem can then be solved by a bipartite graph matching
algorithm (Leusch et al., 2008): given a nxm binary ma-
trix describing possible translation links between source
words and target words7, this algorithm finds the subset
of links maximizing the number of words of the reference
that have been translated, while ensuring that each word
</bodyText>
<footnote confidence="0.991515">
6Neither at the sentence (Chiang et al., 2008), nor at the phrase level.
7The (i, j) entry of the matrix is 1 if the ith word of the source can
be translated by the jth word of the reference, 0 otherwise.
</footnote>
<bodyText confidence="0.993068225806452">
is translated only once.
Generalizing this approach to phrase-based systems
amounts to solving the following problem: given a set
of possible translation links between potential phrases of
the source and of the target, find the subset of links so that
the unigram precision and recall are the highest possible.
The corresponding oracle hypothesis can then be easily
generated by selecting the target phrases that are aligned
with one source phrase, disregarding the others. In ad-
dition, to mimic the way OOVs are usually handled, we
match identical OOV tokens appearing both in the source
and target sentences. In this approach, the unigram pre-
cision is always one (every word generated in the oracle
hypothesis matches exactly one word in the reference).
As a consequence, to find the oracle hypothesis, we just
have to maximize the recall, that is the number of words
appearing both in the hypothesis and in the reference.
Considering phrases instead of isolated words has a
major impact on the computational complexity: in this
new setting, the optimal segmentations in phrases of both
the source and of the target have to be worked out in ad-
dition to links selection. Moreover, constraints have to
be taken into account so as to enforce a proper segmenta-
tion of the source and target sentences. These constraints
make it impossible to use the approach of (Leusch et al.,
2008) and concur in making the oracle decoding prob-
lem for phrase-based models more complex than it is for
word-based models: it can be proven, using arguments
borrowed from (De Nero and Klein, 2008), that this prob-
lem is NP-hard even for the simple unigram precision
measure.
</bodyText>
<subsectionHeader confidence="0.999651">
2.3 An Integer Program for Oracle Decoding
</subsectionHeader>
<bodyText confidence="0.999666117647059">
To solve the combinatorial problem introduced in the pre-
vious section, we propose to cast it into an Integer Lin-
ear Programming (ILP) problem, for which many generic
solvers exist. ILP has already been used in SMT to find
the optimal translation for word-based (Germann et al.,
2001) and to study the complexity of learning phrase
alignments (De Nero and Klein, 2008) models. Follow-
ing the latter reference, we introduce the following vari-
ables: fi,j (resp. ek,l) is a binary indicator variable that
is true when the phrase contains all spans from between-
word position i to j (resp. k to l) of the source (resp.
target) sentence. We also introduce a binary variable, de-
noted ai,j,k,l, to describe a possible link between source
phrase fi,j and target phrase ek,l. These variables are
built from the entries of the phrase table according to se-
lection strategies introduced in Section 2.4. In the fol-
lowing, index variables are so that:
</bodyText>
<equation confidence="0.558527">
0 &lt; i &lt; j &lt; n, in the source sentence and
0 &lt; k &lt; l &lt; m, in the target sentence,
</equation>
<page confidence="0.980032">
935
</page>
<bodyText confidence="0.99996175">
where n (resp. m) is the length of the source (resp. target)
sentence.
Solving the oracle decoding problem then amounts to
optimizing the following objective function:
</bodyText>
<equation confidence="0.985917333333334">
ai,j,k,l · (l − k) , (1)
dx E Q1, m� : X ek,l &lt; 1 (2)
k,l s.t. k&lt;x&lt;l
dy E Q1, n� : X fi,j = 1 (3)
i,j s.t. i&lt;y&lt;j
dk, l : X ai,j,k,l = fk,l (4)
i,j
di, j : X ai,j,k,l = ei,j (5)
k,l
</equation>
<bodyText confidence="0.967178038461538">
The objective function (1) corresponds to the number
of target words that are generated. The first set of con-
straints (2) ensures that each word in the reference a ap-
pears in no more than one phrase. Maximizing the objec-
tive under these constraints amounts to maximizing the
unigram recall. The second set of constraints (3) ensures
that each word in the source f is translated exactly once,
which guarantees that the search space of the ILP prob-
lem is the same as the search space of a phrase-based sys-
tem. Constraints (4) bind the fk,l and ai,j,k,l variables,
ensuring that whenever a link ai,j,k,l is active, the corre-
sponding phrase fk,l is also active. Constraints (5) play a
similar role for the reference.
The Relaxed Problem Even though it accurately
models the search space of a phrase-based decoder,
this programs is not really useful as is: due to out-of-
vocabulary words or missing entries in the phrase table,
the constraint that all source words should be translated
yields infeasible problems8. We propose to relax this
problem and allow some source words to remain untrans-
lated. This is done by replacing constraints (3) by:
dy E Q1, n� : X fi,j &lt; 1
i,j s.t. i&lt;y&lt;j
To better reflect the behavior of phrase-based decoders,
which attempt to translate all source words, we also need
to modify the objective function as follows:
</bodyText>
<equation confidence="0.8996975">
X Xai,j,k,l · (l − k) + fi,j · (j − i) (6)
i,j,k,l i,j
</equation>
<bodyText confidence="0.971614866666667">
The second term in this new objective ensures that opti-
mal solutions translate as many source words as possible.
8An ILP problem is said to be infeasible when every possible solu-
tion violates at least one constraint.
The Relaxed-Distortion Problem A last caveat
with the Relaxed optimization program is caused by
frequently occurring source tokens, such as function
words or punctuation signs, which can often align with
more than one target word. For lack of taking distor-
tion information into account in our objective function,
all these alignments are deemed equivalent, even if some
of them are clearly more satisfactory than others. This
situation is illustrated on Figure 1.
le chat et le chien
the cat and the dog
</bodyText>
<figureCaption confidence="0.995364">
Figure 1: Equivalent alignments between “le” and “the”. The
dashed lines corresponds to a less interpretable solution.
</figureCaption>
<bodyText confidence="0.99832">
To overcome this difficulty, we propose a last change
to the objective function:
</bodyText>
<equation confidence="0.95625475">
X Xai,j,k,l · (l − k) + fi,j · (j − i)
i,j,k,l i,j
X−α ai,j,k,l|k − i |(7)
i,j,k,l
</equation>
<bodyText confidence="0.9999949">
Compared to the objective function of the relaxed prob-
lem (6), we introduce here a supplementary penalty factor
which favors monotonous alignments. For each phrase
pair, the higher the difference between source and target
positions, the higher this penalty. If α is small enough,
this extra term allows us to select, among all the opti-
mal alignments of the relaxed problem, the one with
the lowest distortion. In our experiments, we set α to
min in, m} to ensure that the penalty factor is always
smaller than the reward for aligning two single words.
</bodyText>
<subsectionHeader confidence="0.999582">
2.4 Selecting Indicator Variables
</subsectionHeader>
<bodyText confidence="0.999873117647059">
In the approach introduced in the previous sections, the
oracle decoding problem is solved by selecting, among
a set of possible translation links, the ones that yield the
solution with the highest unigram recall.
We propose two strategies to build this set of possible
translation links. In the first one, denoted exact match,
an indicator ai,j,k,l is created if there is an entry (f, e) so
that f spans from word position i to j in the source and
e from word position k to l in the target. In this strat-
egy, the ILP program considers exactly the same ruleset
as conventional phrase-based decoders.
We also consider an alternative strategy, which could
help us to identify errors made during the phrase extrac-
tion process. In this strategy, denoted inside match, an
indicator ai,j,k,l is created when the following three cri-
teria are met: i) f spans from position i to j of the source;
ii) a substring of e, denoted ¯e, spans from position k to l
</bodyText>
<figure confidence="0.74398075">
Xmax
i,j,k,l
i,j,k,l
under the constraints:
</figure>
<page confidence="0.990459">
936
</page>
<bodyText confidence="0.9999211">
of the reference; iii) (f, e) is not an entry of the phrase ta-
ble. The resulting set of indicator variables thus contains,
at least, all the variables used in the exact match strategy.
In addition, we license here the use of phrases containing
words that do not occur in the reference. In fact, using
such solutions can yield higher BLEU scores when the
reward for additional correct matches exceeds the cost
incurred by wrong predictions. These cases are symp-
toms of situations where the extraction heuristic failed to
extract potentially useful subphrases.
</bodyText>
<subsectionHeader confidence="0.946746">
2.5 Oracle Decoding with Reordering Constraints
</subsectionHeader>
<bodyText confidence="0.998634285714286">
The ILP problem introduced in the previous section can
be extended in several ways to describe and test various
improvements to phrase-based systems or to evaluate the
impact of different parameter settings. This flexibility
mainly stems from the possibility offered by our frame-
work to express arbitrary constraints over variables. In
this section, we illustrate these possibilities by describing
how reordering constraints can easily be considered.
As a first example, the Moses decoder uses a distortion
limit to constrain the set of possible reorderings. This
constraint “enforces (...) that the last word of a phrase
chosen for translation cannot be more than d9 words from
the leftmost untranslated word in the source” (Lopez,
2009) and is expressed as:
</bodyText>
<equation confidence="0.914928">
∀aijkl, ai0j0k0l0 s.t. k &gt; k0,
aijkl · ai0j0k0l0 · |j − i0 + 1 |≤ d,
</equation>
<bodyText confidence="0.999573666666667">
The maximum distortion limit strategy (Lopez, 2009) is
also easily expressed and take the following form (assum-
ing this constraint is parameterized by d):
</bodyText>
<equation confidence="0.9948355">
∀l &lt; m − 1,
ai,j,k,l·ai0,j0,l+1,l0 · |i0 − j − 1 |&lt; d
</equation>
<bodyText confidence="0.999750333333333">
Implementing the “local” or MJ-d (Kumar and Byrne,
2005) reordering strategy is also straightforward, and im-
plies using the following constraints:
</bodyText>
<equation confidence="0.9707015">
Xai0,j0,k0,l0 −
k0≤k
</equation>
<bodyText confidence="0.994200333333333">
Similarly, It is possible to simulate decoding under the
so-called IBM(d) reordering constraints10 by considering
the following constraints:
</bodyText>
<footnote confidence="0.9736955">
9This corresponds to the dl parameter of Moses
10Under IBM(d) constraints, the translation is done, phrase by phrase,
from the beginning of the sentence until the end and only one of the first
d untranslated phrase can be selected for translation.
</footnote>
<bodyText confidence="0.999682666666667">
In these constraints, the first factor corresponds to the
rightmost translated word of the source and the second
one to the number of translated source words. The con-
straints simply enforce that, at each step of the decoding,
there are no more than d source words that were skipped.
Note that the constraints introduced above are not all
linear in the problem variables; however they can eas-
ily be linearized using standard ILP techniques (Roth and
Yih, 2005).
</bodyText>
<sectionHeader confidence="0.983" genericHeader="method">
3 Oracle Decoding for Failure Analysis
</sectionHeader>
<subsectionHeader confidence="0.988975">
3.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.999982157894737">
We propose to use our oracle decoder to study the ability
of a PBTS to translate from English to French and from
German to English. These two languages pairs present
different challenges: English to French translation is con-
sidered a relatively easy pair, notwithstanding the diffi-
culties of generating the right inflection marks in French.
Translating from German into English is more difficult,
notably due to the productivity of inflectional and com-
pounding processes in German, and also to significant
differences in word ordering between these languages.
Our experiments are based on the corpora distributed
for the WMT’09 constrained tasks (Callison-Burch et
al., 2009). All data are tokenized, cleaned and con-
verted to lowercase letters using the tools provided
by the organizers. We then used a standard training
pipeline to construct the translation model: the bitexts
were aligned using Giza++11, symmetrized using the
grow-diag-final-and heuristic; the phrase table
was extracted and scored using the tools distributed with
Moses.12 Finally, baseline systems were optimized using
WMT’08 test set as development using MERT. Note that,
for all these steps, we used the default value of the var-
ious parameters. The extracted phrase table is then used
to find the oracle alignment on the task test set. Recall
that oracle decoding do not use the scores estimated by
Moses in any way.
In the experiments reported below, two settings are
considered. In the first one, denoted NEWSCO, Moses
was trained only on a small data set taken from the News
Commentary corpus. Using a small sized corpus reduces
both training time and decoding time, which allows us to
quickly test different configurations of the decoder. In a
second setting, denoted EUROPARL, Moses was trained
on a larger corpora containing the entirety of the Europarl
Corpus, but no in-domain data, to provide results on more
realistic conditions. Statistics regarding the different cor-
pora used are reported in Table 1. These statistics are
computed on the lowercase cleaned corpora.
</bodyText>
<footnote confidence="0.9852915">
11http://www.fjoch.com/GIZA++.html
12http://statmt.org/moses
</footnote>
<equation confidence="0.727582375">
������
∀i, k,
X
i0≤i
ai0,j0,k0,l0 ������ ≤ d
∀µ ≤ m, max Xai,j,k,l · j − ai,j,k,l · (j − i) ≤ d
i,k,l i,j,k,l
j≤1Z
</equation>
<page confidence="0.487726">
937
</page>
<table confidence="0.999886428571429">
en → fr de → en
NEWSCO EUROPARL NEWSCO EUROPARL
#words 1,023,401 21, 616,114 1,530,693 22, 898, 644
#sentences 51,375 1,050,398 71,691 1,118,399
#vocabulary 31,416 78,071 78,140 242,219
#phrase table 3,061,701 46,003, 525 4,133,190 44, 402, 367
% OOV 5.3% 3.1% 8.0% 5.2%
</table>
<tableCaption confidence="0.9967885">
Table 1: Statistics regarding the training corpora: number of words, number of sentences, vocabulary and phrase table size and
percentage of test words not appearing in the train set (OOV).
</tableCaption>
<bodyText confidence="0.9997358">
Finding the oracle alignment amounts to solving the
ILP problems introduced above. Even though ILP prob-
lems are NP-hard in general, there exist several off-the-
shelf ILP solvers able to efficiently find an optimal solu-
tion or decide that the problem is infeasible. In our exper-
iments, we used the free solver SCIP (Achterberg, 2007).
An optimal solution was found for all problems we con-
sidered. Decoding the 3,027 sentences of WMT’09 test
set takes about 10 minutes (wall time) for the NEWSCO
setting, and several hours for the EUROPARL setting13.
</bodyText>
<subsectionHeader confidence="0.994168">
3.2 Oracle BLEU Score
</subsectionHeader>
<bodyText confidence="0.999245678571429">
Table 2 reports, for all considered settings, the BLEU-4
scores14 achieved by our oracle decoder, as well as the
number of source words used to generate the oracle hy-
pothesis and the number of target words that are reach-
able. In these experiments, two objective functions were
considered: first, we only consider the objective function
corresponding to the relaxed problem defined by Eq. (6);
second, we introduced an extra term in the objective to
penalize distortion, as described by Eq. (7). Unless ex-
plicitly stated otherwise, we always used the exact match
strategy.
The main result in 2 is that, for the two language pairs
considered, the expressive power of PBTS is not the lim-
iting factor to achieve high translation performance. In
fact, for most sentences in the test set, excellent oracle
hypotheses, which contain a very high proportion of ref-
erence words, are found. This remains true even when the
phrase table is extracted from a small corpus. Given that
the best BLEU-4 scores achieved during the WMT’09
evaluation are about 28 for the English to French task
and 24 for the German to English task ((Callison-Burch
et al., 2009), Tables 26 and 25), these results strongly
suggest that the main bottleneck of current phrase-based
translation systems is their scoring function rather than
their expressive power. As we will discuss in Section 4,
similar conclusions were drawn by (Auli et al., 2009) and
(Turchi et al., 2008).
Several additional comments on these numbers are in
</bodyText>
<footnote confidence="0.546570666666667">
13All our experiments are run on a 8 cores computer, each core being
a 2.2GHz Intel Processor; the decoder is multi-threaded.
14These are computed on lowercase with the default tokenization.
</footnote>
<bodyText confidence="0.999865108108108">
order. Despite these very high BLEU scores, in most
cases, the reference is only partly translated. In the most
favorable case, for the English to French EUROPARL set-
ting, only 26% of the references could be fully gener-
ated15. These numbers are consistent with the results re-
ported in (Auli et al., 2009). Similarly, only about 31%
of the source sentences are completely translated by the
oracle decoder, which supports our choice to consider a
relaxed version of the ILP problem. Finally, Table 2 also
shows that introducing the distortion penalty does not af-
fect the oracle performance of the decoder.
Considering the inside match strategy improves the
performance of the oracle decoder: for instance, for the
English to French NEWSCO setting, oracle decoder with
the inside match strategy achieves a BLEU-4 score of
70.15 (a 2.5 points improvement over the baseline). To
achieve this score, 21.45% of the phrases used during de-
coding were phrases that are not considered by the exact
match strategy. Similar results can be observed for other
settings, which highlights the significance of one kind of
failure of the extraction heuristic: useful “subphrases” of
actual phrase pairs are not always extracted.
The numbers in Table 2, no matter how good they may
look, should be considered with caution: they only imply
that, for most test sentences, all the information necessary
to produce a good translation is available in the phrase ta-
ble. However, the alignment decisions underlying these
oracle hypotheses are sometimes hard to justify, and one
has to accept that part of these good hypotheses transla-
tions are due to a series of lucky alignment errors. This
is illustrated on Figure 2, which displays one such lucky
oracle alignment based on the misalignment, during train-
ing, of the French preposition “des” (of the) with the En-
glish noun “stock”. Such lucky errors are naturally also
observed in the outputs of conventional decoders, even
though phrase table filtering heuristics probably makes
them somewhat more rare.
</bodyText>
<subsectionHeader confidence="0.999788">
3.3 Analyzing Non-Reachable Parts of a Reference
</subsectionHeader>
<bodyText confidence="0.9559665">
Table 3 contains typical examples of sentence pairs that
could not be fully generated by our oracle decoder. They
15Similar numbers were obtained, albeit much more slowly, with the
--constraint option of Moses.
</bodyText>
<page confidence="0.99612">
938
</page>
<table confidence="0.9783506">
training set objective function % source translated % target generated 4-BLEU
en → fr NEWSCO RELAXED 86.04% 84.74% 67.65
de → en RELAXED-DISTORTION 85.99% 84.77% 67.77
RELAXED
RELAXED-DISTORTION
RELAXED
RELAXED-DISTORTION
RELAXED
RELAXED-DISTORTION
EUROPARL 93.66% 93.06% 85.05
NEWSCO 93.65% 93.06% 85.08
EUROPARL 82.57% 82.33% 64.60
82.59% 82.30% 64.65
90.34% 91.16% 81.77
90.36% 91.12% 81.77
</table>
<tableCaption confidence="0.999779">
Table 2: Translation score of the ILP oracle decoder for the various settings described in Section 3.1
</tableCaption>
<bodyText confidence="0.5576545">
stock fall in asia
chute des actions en asie
</bodyText>
<figureCaption confidence="0.999461">
Figure 2: Example of alignment obtained by our oracle decoder
</figureCaption>
<bodyText confidence="0.779458333333333">
illustrate the three main reasons which cause some parts
of the reference to remain unreachable:
• phrases are missing from the phrase table, either
because they do not occur in the training corpus
(OOVs) or because they failed to be extracted. In
Table 3, OOV errors are mainly due to past tense
forms translated into verbs conjugated in pass´e sim-
ple (“rejeta”, “rencontr`erent”, “renoua”) a French
literary tense, mostly used in formal writings.
</bodyText>
<listItem confidence="0.898647444444444">
• obvious errors (misspelled words, misinterpretation
or mistranslation, ...) in the reference. The refer-
ence of the fifth example contains one such error:
the state name “Nevada” is translated to “n’´evadiez”
(literally “have not escaped”), yielding a very poor
reference sentence.
• parts of the reference have no translation equiva-
lence in the source. This can be either because ref-
erences are produced in “context” and some pieces
</listItem>
<bodyText confidence="0.995311076923077">
of information are moved across sentence bound-
aries or because these references are non-literal. The
fourth example, which seems to be the translation of
a title, falls into this category: the French part con-
tains a reference to the context (“les SA” is referring
to the bacteria the text is talking about) which is not
in the source text. Non-literal translation are illus-
trated by the third example, where English “Mon-
day” is translated into French “la veille” (the day
before).
While the first kind of errors is inherent to the use of
a statistical approach, the last two kinds result from the
quality of the data used in the evaluation and directly im-
pact both training and evaluation of automatic translation
systems: if they should not distort too much comparisons
of MT systems, these errors prevent us from assessing
the “global” quality of automatic translation and, if sim-
ilar errors are found in the train set, they make learning
harder as some probability mass is wasted to model them.
To provide a more quantitative analysis, we manually
looked at all the non-aligned parts of some WMT’09 ref-
erences and found that out of 800 references, more than
133 contain either an obvious translation error or can not
be achieved by a PBTS16. Note that, while identifying
these errors could be done in many ways, our oracle de-
coder makes it far easier.
</bodyText>
<subsectionHeader confidence="0.990554">
3.4 Identifying Causes of Failure
</subsectionHeader>
<bodyText confidence="0.999445565217391">
By comparing the hypotheses found by the oracle de-
coder and the ones found by the phrase-based decoder,
causes of failure can be easily identified. In this section,
we will present several measures that allow us to identify
and quantify several causes of failure.
Errors Caused by Search Space Pruning Recall from
Section 1.1 that Moses uses several heuristics to prune the
search space. In particular, there is a distortion limit and
a limit on the number of target phrases considered for one
source phrase. In this paragraph, we evaluate the impact
of these two heuristics on translation quality.
Table 4 presents the average distortion computed on
the oracle hypotheses, as well as the percentage of
phrases used that have a distortion strictly greater than
6 (the default distortion limit of Moses). All these num-
bers are obtained by solving the RELAXED-DISTORTION
problem. Surprisingly enough, the average distortion of
oracle hypotheses is quite small, even for the German to
English task, and the distortion constraint seems to be vi-
olated only in a few cases. It also appears that the distor-
tion of the hypotheses generated in the NEWSCO setting
is significantly larger than in the EUROPARL setting. This
can be explained by the extra degrees of freedom in the
</bodyText>
<footnote confidence="0.999584666666667">
16Annotation at a finer level is an on-going effort; the annotated
corpus is available from http://www.limsi.fr/Individu/
wisniews/oracle decoding.
</footnote>
<page confidence="0.997263">
939
</page>
<bodyText confidence="0.9786408">
0 – On Monday the American House of Representatives rejected the plan to support the financial
system, into which up to 700 billion dollars (nearly 12 billion Czech crowns) was to be invested.
– Lundi, la chambre des repr´esentants am´ericaine rejeta le projet de soutient du syst`eme financier,
auquel elle aurait dˆu consacrer jusqu’`a 700 milliards de dollars (pr`es de 12 bilions de kˇc).
� – Representatives of the legislators met with American Finance Minister Henry Paulson Saturday
night in order to give the government fund a final form.
– Dans la nuit de samedi a` dimanche, des repr´esentants des l´egislateurs rencontr`erent le ministre
des finances am´ericain Henry Paulson, afin de donner au fond du gouvernement une forme finale.
OO – The Prague Stock Market immediately continued its fall from Monday at the beginning of
Tuesday’s trading , when it dropped by nearly six percent.
</bodyText>
<listItem confidence="0.358009">
– Mardi, d`es le d´ebut des ´echanges, la bourse de prague renoua avec sa chute de la veille,
lorsqu’elle perdait presque six pour cent.
� – Antibiotic Resistance
– Les SA r´esistent aux antibiotiques.
(a – According to Nevada Democratic senator Harry Reid, that is how that legislators are trying to
have Congress to reach a definitive agreement as early as on Sunday.
– D’apr`es le s´enateur d`emocrate n’´evadiez Harry Reid, les l´egislateurs font de sorte que le Congr`es
aboutisse a` un accord d´efinitif d`es dimanche.
</listItem>
<tableCaption confidence="0.9933885">
Table 3: Output examples of our oracle decoder on the English to French task. Words in bold are non-aligned words and words in
italic are non-aligned out-of-vocabulary words. For clarity the examples have been detokenized and recased.
</tableCaption>
<figure confidence="0.908314142857143">
avg. %phrases
distortion with a dist.
&gt; 6
4.57 22.02%
3.21 13.32%
5.16 25.37%
3.81 17.21%
</figure>
<tableCaption confidence="0.9628855">
Table 4: Average distortion and percentage of phrases with a
distortion greater that Moses default distortion limit.
</tableCaption>
<bodyText confidence="0.999353235294117">
alignment decisions enabled by the use of larger training
corpora and phrase table.
To evaluate the impact of the second heuristic, we com-
puted the number of phrases discarded by Moses (be-
cause of the default ttl limit) but used in the oracle hy-
potheses. In the English to French NEWSCO setting,
they account for 34.11% of the total number of phrases
used in the oracle hypotheses. When the oracle decoder
is constrained to use the same phrase table as Moses, its
BLEU-4 score drops to 42.78. This shows that filtering
the phrase table prior to decoding discards many useful
phrase pairs and is seriously limiting the best achievable
performance, a conclusion shared with (Auli et al., 2009).
Search Errors Search errors can be identified by com-
paring the score of the best hypothesis found by Moses
and the score of the oracle hypothesis. If the score of the
oracle hypothesis is higher, then there has been a search
error; on the contrary, there has been an estimation error
when the score of the oracle hypothesis is lower than the
score of the best hypothesis found by Moses.
Based on the comparison of the score of Moses hy-
potheses and of oracle hypotheses for the English to
French NEWSCO setting, our preliminary conclusion is
that the number of search errors is quite limited: only
about 5% of the hypotheses of our oracle decoder are ac-
tually getting a better score than Moses solutions. Again,
this shows that the scoring function (model error) is
one of the main bottleneck of current PBTS. Compar-
ing these hypotheses is nonetheless quite revealing: while
Moses mostly selects phrase pairs with high translation
scores and generates monotonous alignments, our ILP de-
coder uses larger reorderings and less probable phrases
to achieve better solutions: on average, the reordering
score of oracle solutions is −5.74, compared to −76.78
for Moses outputs. Given the weight assigned through
MERT training to the distortion score, no wonder that
these hypotheses are severely penalized.
The Impact of Phrase Length The observed outputs
do not only depend on decisions made during the search,
but also on decisions made during training. One such
decision is the specification of maximal length for the
source and target phrases. In our framework, evaluating
the impact of this decision is simple: it suffices to change
the definition of indicator variables so as to consider only
alignments between phrases of a given length.
In the English-French NEWSCO setting, the most re-
strictive choice, when only alignments between single
words are authorized, yields an oracle BLEU-4 of 48.68;
however, authorizing phrases up to length 2 allows to
achieve an oracle value of 66.57, very close to the score
achieved when considering all extracted phrases (67.77).
</bodyText>
<figure confidence="0.782105">
en → fr training set
NEWSCO
EUROPARL
de → en NEWSCO
EUROPARL
</figure>
<page confidence="0.98918">
940
</page>
<bodyText confidence="0.999936375">
This is corroborated with a further analysis of our ora-
cle alignments, which use phrases whose average source
length is 1.21 words (respectively 1.31 for target words).
If many studies have already acknowledged the predomi-
nance of “small” phrases in actual translations, our oracle
scores suggest that, for this language pair, increasing the
phrase length limit beyond 2 or 3 might be a waste of
computational resources.
</bodyText>
<sectionHeader confidence="0.999986" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999970220588235">
To the best of our knowledge, there are only a few works
that try to study the expressive power of phrase-based ma-
chine translation systems or to provide tools for analyzing
potential causes of failure.
The approach described in (Auli et al., 2009) is very
similar to ours: in this study, the authors propose to find
and analyze the limits of machine translation systems by
studying the reference reachability. A reference is reach-
able for a given system if it can be exactly generated
by this system. Reference reachability is assessed using
Moses in forced decoding mode: during search, all hy-
potheses that deviate from the reference are simply dis-
carded. Even though the main goal of this study was to
compare the search space of phrase-based and hierarchi-
cal systems, it also provides some insights on the impact
of various search parameters in Moses, delivering con-
clusions that are consistent with our main results. As de-
scribed in Section 1.2, these authors also propose a typol-
ogy of the errors of a statistical translation systems, but
do not attempt to provide methods for identifying them.
The authors of (Turchi et al., 2008) study the learn-
ing capabilities of Moses by extensively analyzing learn-
ing curves representing the translation performances as a
function of the number of examples, and by corrupting
the model parameters. Even though their focus is more
on assessing the scoring function, they reach conclusions
similar to ours: the current bottleneck of translation per-
formances is not the representation power of the PBTS
but rather in their scoring functions.
Oracle decoding is useful to compute reachable
pseudo-references in the context of discriminative train-
ing. This is the main motivation of (Tillmann and Zhang,
2006), where the authors compute high BLEU hypothe-
ses by running a conventional decoder so as to maximize
a per-sentence approximation of BLEU-4, under a simple
(local) reordering model.
Oracle decoding has also been used to assess the
limitations induced by various reordering constraints in
(Dreyer et al., 2007). To this end, the authors propose
to use a beam-search based oracle decoder, which com-
putes lower bounds of the best achievable BLEU-4 us-
ing dynamic programming techniques over finite-state
(for so-called local and IBM constraints) or hierarchically
structured (for ITG constraints) sets of hypotheses. Even
though the numbers reported in this study are not directly
comparable with ours17, it seems that our decoder is not
only conceptually much simpler, but also achieves much
more optimistic lower-bounds of the oracle BLEU score.
The approach described in (Li and Khudanpur, 2009) em-
ploys a similar technique, which is to guide a heuristic
search in an hypergraph representing possible translation
hypotheses with n-gram counts matches, which amounts
to decoding with a n-gram model trained on the sole ref-
erence translation. Additional tricks are presented in this
article to speed-up decoding.
Computing oracle BLEU scores is also the subject of
(Zens and Ney, 2005; Leusch et al., 2008), yet with a
different emphasis. These studies are concerned with
finding the best hypotheses in a word graph or in a con-
sensus network, a problem that has various implications
for multi-pass decoding and/or system combination tech-
niques. The former reference describes an exponential
approximate algorithm, while the latter proves the NP-
completeness of this problem and discuss various heuris-
tic approaches. Our problem is somewhat more complex
and using their techniques would require us to built word
graphs containing all the translations induced by arbitrary
segmentations and permutations of the source sentence.
</bodyText>
<sectionHeader confidence="0.999575" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.99998784">
In this paper, we have presented a methodology for ana-
lyzing the errors of PBTS, based on the computation of
an approximation of the BLEU-4 oracle score. We have
shown that this approximation could be computed fairly
accurately and efficiently using Integer Linear Program-
ming techniques. Our main result is a confirmation of
the fact that extant PBTS systems are expressive enough
to achieve very high translation performance with respect
to conventional quality measurements. The main efforts
should therefore strive to improve on the way phrases and
hypotheses are scored during training. This gives further
support to attempts aimed at designing context-dependent
scoring functions as in (Stroppa et al., 2007; Gimpel and
Smith, 2008), or at attempts to perform discriminative
training of feature-rich models. (Bangalore et al., 2007).
We have shown that the examination of difficult-to-
translate sentences was an effective way to detect errors
or inconsistencies in the reference translations, making
our approach a potential aid for controlling the quality or
assessing the difficulty of test data. Our experiments have
also highlighted the impact of various parameters.
Various extensions of the baseline ILP program have
been suggested and/or evaluated. In particular, the ILP
formalism lends itself well to expressing various con-
straints that are typically used in conventional PBTS. In
</bodyText>
<footnote confidence="0.611073333333333">
17The best BLEU-4 oracle they achieve on Europarl German to En-
glish is approximately 48; but they considered a smaller version of the
training corpus and the WMT’06 test set.
</footnote>
<page confidence="0.995278">
941
</page>
<bodyText confidence="0.999914777777778">
our future work, we aim at using this ILP framework to
systematically assess various search configurations. We
plan to explore how replacing non-reachable references
with high-score pseudo-references can improve discrim-
inative training of PBTS. We are also concerned by de-
termining how tight is our approximation of the BLEU-
4 score is: to this end, we intend to compute the best
BLEU-4 score within the n-best solutions of the oracle
decoding problem.
</bodyText>
<sectionHeader confidence="0.998135" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.98728625">
Warm thanks to Houda Bouamor for helping us with the
annotation tool. This work has been partly financed by
OSEO, the French State Agency for Innovation, under
the Quaero program.
</bodyText>
<sectionHeader confidence="0.999164" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999732292929293">
Tobias Achterberg. 2007. Constraint Integer Program-
ming. Ph.D. thesis, Technische Universit¨at Berlin.
http://opus.kobv.de/tuberlin/volltexte/
2007/1611/.
Abhishek Arun and Philipp Koehn. 2007. Online learning
methods for discriminative training of phrase based statis-
tical machine translation. In Proc. of MT Summit XI, Copen-
hagen, Denmark.
Michael Auli, Adam Lopez, Hieu Hoang, and Philipp Koehn.
2009. A systematic analysis of translation model search
spaces. In Proc. of WMT, pages 224–232, Athens, Greece.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An
automatic metric for MT evaluation with improved correla-
tion with human judgments. In Proc. of the ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization, pages 65–72, Ann Arbor,
Michigan.
Srinivas Bangalore, Patrick Haffner, and Stephan Kanthak.
2007. Statistical machine translation through global lexi-
cal selection and sentence reconstruction. In Proc. of ACL,
pages 152–159, Prague, Czech Republic.
L´eon Bottou and Olivier Bousquet. 2008. The tradeoffs of large
scale learning. In Proc. of NIPS, pages 161–168, Vancouver,
B.C., Canada.
Chris Callison-Burch, Philipp Koehn, Christof Monz, and Josh
Schroeder. 2009. Findings of the 2009 Workshop on Sta-
tistical Machine Translation. In Proc. of WMT, pages 1–28,
Athens, Greece.
David Chiang, Steve DeNeefe, Yee Seng Chan, and Hwee Tou
Ng. 2008. Decomposability of translation metrics for
improved evaluation and efficient algorithms. In Proc. of
ECML, pages 610–619, Honolulu, Hawaii.
John De Nero and Dan Klein. 2008. The complexity of phrase
alignment problems. In Proc. of ACL: HLT, Short Papers,
pages 25–28, Columbus, Ohio.
Markus Dreyer, Keith B. Hall, and Sanjeev P. Khudanpur. 2007.
Comparing reordering constraints for smt using efficient bleu
oracle computation. In NAACL-HLT/AMTA Workshop on
Syntax and Structure in Statistical Translation, pages 103–
110, Rochester, New York.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu,
and Kenji Yamada. 2001. Fast decoding and optimal decod-
ing for machine translation. In Proc. ofACL, pages 228–235,
Toulouse, France.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu,
and Kenji Yamada. 2004. Fast and optimal decoding for
machine translation. Artificial Intelligence, 154(1-2):127–
143.
Ulrich Germann. 2003. Greedy decoding for statistical ma-
chine translation in almost linear time. In Proc. of NAACL,
pages 1–8, Edmonton, Canada.
Kevin Gimpel and Noah A. Smith. 2008. Rich source-side
context for statistical machine translation. In Proc. of WMT,
pages 9–17, Columbus, Ohio.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Sta-
tistical phrase-based translation. In Proc. of NAACL, pages
48–54, Edmonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-
Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan,
Wade Shen, Christine Moran, Richard Zens, Chris Dyer, On-
drej Bojar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine transla-
tion. In Proc. of ACL, demonstration session.
Philipp Koehn. 2004. Pharaoh: A beam search decoder for
phrase-based statistical machine translation models. In Proc.
of AMTA, pages 115–124, Washington DC.
Shankar Kumar and William Byrne. 2005. Local phrase re-
ordering models for statistical machine translation. In Proc.
of HLT, pages 161–168, Vancouver, Canada.
Alon Lavie, Kenji Sagae, and Shyamsundar Jayaraman. The
significance of recall in automatic metrics for MT evaluation.
In In Proc. of AMTA, pages 134–143, Washington DC.
Gregor Leusch, Evgeny Matusov, and Hermann Ney. 2008.
Complexity of finding the BLEU-optimal hypothesis in a
confusion network. In Proc. of EMNLP, pages 839–847,
Honolulu, Hawaii.
Zhifei Li and Sanjeev Khudanpur. 2009. Efficient extraction
of oracle-best translations from hypergraphs. In Proc. of
NAACL, pages 9–12, Boulder, Colorado.
Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and Ben
Taskar. 2006. An end-to-end discriminative approach to ma-
chine translation. In Proc. of ACL, pages 761–768, Sydney,
Australia.
Adam Lopez. 2009. Translation as weighted deduction. In
Proc. of EACL, pages 532–540, Athens, Greece.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Comput.
Linguist., 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proc. of ACL, pages 160–167,
Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-jing Zhu.
2002. Bleu: A method for automatic evaluation of machine
translation. Technical report, Philadelphia, Pennsylvania.
D. Roth and W. Yih. 2005. Integer linear programming infer-
ence for conditional random fields. In Proc. of ICML, pages
737–744, Bonn, Germany.
Nicolas Stroppa, Antal van den Bosch, and Andy Way. 2007.
Exploiting source similarity for smt using context-informed
</reference>
<page confidence="0.977376">
942
</page>
<reference confidence="0.999442153846154">
features. In Andy Way and Barbara Gawronska, editors,
Proc. of TMI, pages 231–240, Sk¨ovde, Sweden.
Christoph Tillmann and Tong Zhang. 2006. A discriminative
global training algorithm for statistical mt. In Proc. of ACL,
pages 721–728, Sydney, Australia.
Marco Turchi, Tijl De Bie, and Nello Cristianini. 2008. Learn-
ing performance of a machine translation system: a statistical
and computational analysis. In Proc. of WMT, pages 35–43,
Columbus, Ohio.
Richard Zens and Hermann Ney. 2005. Word graphs for sta-
tistical machine translation. In Proc. of the ACL Workshop
on Building and Using Parallel Texts, pages 191–198, Ann
Arbor, Michigan.
</reference>
<page confidence="0.999167">
943
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.999958">Assessing Phrase-Based Translation Models with Oracle Decoding</title>
<author confidence="0.978636">Wisniewski Allauzen</author>
<affiliation confidence="0.8794">Univ. Paris Sud;</affiliation>
<address confidence="0.996232">91403 ORSAY</address>
<abstract confidence="0.992007078947368">Extant Statistical Machine Translation (SMT) systems are very complex softwares, which embed multiple layers of heuristics and embark very large numbers of numerical parameters. As a result, it is difficult to analyze output translations and there is a real need for tools that could help developers to better understand the various causes of errors. In this study, we make a step in that direction and present an attempt to evaluate the quality of the phrase-based translation model. In order to identify those translation errors that stem from deficiencies in the phrase table (PT), we propose to compute the BLEU-4 that is the best score that a system based on this PT can achieve on a reference corpus. By casting the computation of the oracle BLEU-1 as an Integer Linear Programming (ILP) problem, we show that it is possible to efficiently compute accurate lower-bounds of this score, and report measures performed on several standard benchmarks. Various other applications of these oracle decoding techniques are also reported and discussed. 1 Phrase-Based Machine Translation 1.1 Principle A Phrase-Based Translation System (PBTS) consists of a ruleset and a scoring function (Lopez, 2009). The ruleset, in the phrase table, is a set of each pair expressing that the source phrase be rewritten (translated) into a target phrase Transhypotheses generated by iteratively rewriting portions of the source sentence as prescribed by the ruleset, until each source word has been consumed by exactly one rule. The order of target words in an hypothesis is uniquely determined by the order in which the rewrite opare performed. The space the translation model corresponds to the set of all possible sequences of the usage in statistical machine translation literature, we use “phrase” to denote a subsequence of consecutive words. rules applications. The scoring function aims to rank all possible translation hypotheses in such a way that the best one has the highest score. A PBTS is learned from a parallel corpus in two independent steps. In a first step, the corpus is aligned at the level, by using alignment tools such as (Och and Ney, 2003) and some symmetrisation heuristics; phrases are then extracted by other heuristics (Koehn et al., 2003) and assigned numerical weights. In the second step, the parameters of the scoring function are estimated, typically through Minimum Error Rate training (Och, 2003). Translating a sentence amounts to finding the best scoring translation hypothesis in the search space. Because of the combinatorial nature of this problem, translation has to rely on heuristic search techniques such as greedy hill-climbing (Germann, 2003) or variants of best-first search like multi-stack decoding (Koehn, 2004). Moreover, to reduce the overall complexity of decoding, the search space is typically pruned using simple heuristics. For instance, the state-of-the-art phrase-based decoder Moses (Koehn et al., 2007) considers only a restricted of translations for each source and ena distortion over which phrases can be reordered. As a consequence, the best translation hypothesis returned by the decoder is not always the one with the highest score. 1.2 Typology of PBTS Errors Analyzing the errors of a SMT system is not an easy task, because of the number of models that are combined, the size of these models, and the high complexity of the various decision making processes. For a SMT system, three different kinds of errors can be distinguished (Germann al., 2004; Auli et al., 2009): The former corresponds to cases where the hypothesis with the best score is missed by search procedure, either because of the use of an apof Moses, defaulting to 20.</abstract>
<note confidence="0.690557">of Moses, whose default value is 7. 933 of the 2010 Conference on Empirical Methods in Natural Language pages 933–943, Massachusetts, USA, 9-11 October 2010. Association for Computational Linguistics</note>
<abstract confidence="0.998229517150397">proximate search method or because of the restrictions of search space. errors to cases where, given the model, the search space does not contain reference. Finally, errors to cases where the hypothesis with the highest score is not the best translation according to the evaluation metric. Model errors encompass several types of errors that ocduring learning (Bottou and Bousquet, Aperrors errors caused by the use of a restricted and oversimplistic class of functions (here, finitestate transducers to model the generation of hypotheses and a linear scoring function to discriminate them) to the translation process. errors correspond to the use of sub-optimal values for both the phrase pairs weights and the parameters of the scoring function. The reasons behind these errors are twofold: first, training only considers a finite sample of data; second, it relies on error prone alignments. As a result, some “good” phrases are extracted with a small weight, or, in the limit, are not extracted at all; and conversely that some “poor” phrases are inserted into the phrase table, sometimes with a really optimistic score. Sorting out and assessing the impact of these various causes of errors is of primary interest for SMT system developers: for lack of such diagnoses, it is difficult to figure out which components of the system require the most urgent attention. Diagnoses are however, given the tight intertwining among the various component of a system, very difficult to obtain: most evaluations are limited to the computation of global scores and usually do not imply any kind of failure analysis. 1.3 Contribution and organization To systematically assess the impact of the multiple heuristic decisions made during training and decoding, we propose, following (Dreyer et al., 2007; Auli et al., to work out that is to evaluate the best achievable performances of a PBTS. We aim at both studying the expressive power of PBTS and at providing tools for identifying and quantifying causes of failure. Under standard metrics such as BLEU (Papineni et al., 2002), oracle scores are difficult (if not impossible) to compute, but, by casting the computation of the oracle unigram recall and precision as an Integer Linear Programming (ILP) problem, we show that it is possible to efficiently compute accurate lower-bounds of the oracle BLEU-4 scores and report measurements performed on several standard benchmarks. The main contributions of this paper are twofold. We first introduce an ILP program able to efficiently find the best hypothesis a PBTS can achieve. This program can be easily extended to test various improvements to omit here phrase-base systems or to evaluate the impact of different parameter settings. Second, we present a number of complementary results illustrating the usage of our oracle decoder for identifying and analyzing PBTS errors. Our experimental results confirm the main conclusions of (Turchi et al., 2008), showing that extant PBTs have the potential to generate hypotheses having very high BLEU- 4 score and that their main bottleneck is their scoring function. The rest of this paper is organized as follows: in Section 2, we introduce and formalize the oracle decoding problem, and present a series of ILP problems of increasing complexity designed so as to deliver accurate lowerbounds of oracle score. This section closes with various extensions allowing to model supplementary constraints, most notably reordering constraints (Section 2.5). Our experiments are reported in Section 3, where we first introduce the training and test corpora, along with a description of our system building pipeline (Section 3.1). We then discuss the baseline oracle BLEU scores (Section 3.2), analyze the non-reachable parts of the reference translations, and comment several complementary results which allow to identify causes of failures. Section 4 discuss our approach and findings with respect to the existing literature on error analysis and oracle decoding. We conclude and discuss further prospects in Section 5. 2 Oracle Decoder 2.1 The Oracle Decoding Problem get some insights on the errors of phrasebased systems and better understand their limits, we proto consider the decoding problem as follows: given a source sentence, its reference translaand a phrase table, what is the “best” translation hypothesis a system can generate? As usual, the quality of an hypothesis is evaluated by the similarity between the reference and the hypothesis. Note that in the oracle decoding problem, we are only assessing the ability of PBT systems to generate good candidate translations, irrespective of their ability to score them properly. We believe that studying this problem is interesting for various reasons. First, as described in Section 3.4, comparing the best hypothesis a system could have generated and the hypothesis it actually generates allows us to carry on both quantitative and qualitative failure analysis. The oracle decoding problem can also be used to assess power phrase-based systems (Auli et al., 2009). Other applications include computing acceptable pseudo-references for discriminative training (Tillmann and Zhang, 2006; Liang et al., 2006; Arun and oracle decoding problem can be extended to the case of multiple references. For the sake of simplicity, we only describe the case of a single reference. 934 Koehn, 2007) or combining machine translation systems in a multi-source setting (Li and Khudanpur, 2009). We have also used oracle decoding to identify erroneous or difficult to translate references (Section 3.3). Measure fully define the oracle decoding problem, a measure of the similarity between a translation hypothesis and its reference translation has to be chosen. The most obvious choice is the BLEU-4 score (Papineni et al., 2002) used in most machine translation evaluations. However, using this metric in the oracle decoding problem raises several issues. First, BLEU-4 is a metric defined at the corpus level and is hard to interpret at the sentence level. More importantly, BLEU-4 is not deas it relies on 4-grams statistics, the contribution of each phrase pair to the global score depends on the translation of the previous and following phrases and can not be evaluated in isolation. Because of its nondecomposability, maximizing BLEU-4 is hard; in particular, the phrase-level decomposability of the evaluation metric is necessary in our approach. To circumvent this difficulty, we propose to evaluate the similarity between a translation hypothesis and a reference by the number of their common words. This amounts to evaluating translation quality in terms of unigram precision and recall, which are highly correlated with human judgements (Lavie et al., ). This measure is closely related to the BLEU-1 evaluation metric and the Meteor (Banerjee and Lavie, 2005) metric (when it is evaluated without considering near-matches and the distortion penalty). We also believe that hypotheses that maximize the unigram precision and recall at the sentence level yield corpus level BLEU-4 scores close the maximal achievable. Indeed, in the setting we will introduce in the next section, BLEU-1 and BLEU-4 are highly correlated: as all correct words of the hypothesis will be compelled to be at their correct position, any hypothesis with a high 1-gram precision is also bound to have a high 2-gram precision, etc. 2.2 Formalizing the Oracle Decoding Problem The oracle decoding problem has already been considered in the case of word-based models, in which all translation units are bound to contain only one word. The can then be solved by a graph matching (Leusch et al., 2008): given a matrix describing possible translation links between source and target this algorithm finds the subset of links maximizing the number of words of the reference that have been translated, while ensuring that each word at the sentence (Chiang et al., 2008), nor at the phrase level. of the matrix is the word of the source can translated by the word of the reference, 0 otherwise. is translated only once. Generalizing this approach to phrase-based systems amounts to solving the following problem: given a set of possible translation links between potential phrases of the source and of the target, find the subset of links so that the unigram precision and recall are the highest possible. The corresponding oracle hypothesis can then be easily generated by selecting the target phrases that are aligned with one source phrase, disregarding the others. In addition, to mimic the way OOVs are usually handled, we match identical OOV tokens appearing both in the source and target sentences. In this approach, the unigram precision is always one (every word generated in the oracle hypothesis matches exactly one word in the reference). As a consequence, to find the oracle hypothesis, we just have to maximize the recall, that is the number of words appearing both in the hypothesis and in the reference. Considering phrases instead of isolated words has a major impact on the computational complexity: in this new setting, the optimal segmentations in phrases of both the source and of the target have to be worked out in addition to links selection. Moreover, constraints have to be taken into account so as to enforce a proper segmentation of the source and target sentences. These constraints make it impossible to use the approach of (Leusch et al., 2008) and concur in making the oracle decoding problem for phrase-based models more complex than it is for word-based models: it can be proven, using arguments borrowed from (De Nero and Klein, 2008), that this problem is NP-hard even for the simple unigram precision measure. 2.3 An Integer Program for Oracle Decoding To solve the combinatorial problem introduced in the previous section, we propose to cast it into an Integer Linear Programming (ILP) problem, for which many generic solvers exist. ILP has already been used in SMT to find the optimal translation for word-based (Germann et al., 2001) and to study the complexity of learning phrase alignments (De Nero and Klein, 2008) models. Following the latter reference, we introduce the following vari- (resp.is a binary indicator variable that is true when the phrase contains all spans from betweenposition of the source (resp. target) sentence. We also introduce a binary variable, deto describe a possible link between source target phrase These variables are from the entries of the phrase table according to sestrategies in Section 2.4. In the following, index variables are so that: &lt; j the source sentence and &lt; l the target 935 is the length of the source (resp. target) sentence. Solving the oracle decoding problem then amounts to optimizing the following objective function: X X 1 l X i,j j X k,l The objective function (1) corresponds to the number of target words that are generated. The first set of con- (2) ensures that each word in the reference appears in no more than one phrase. Maximizing the objective under these constraints amounts to maximizing the unigram recall. The second set of constraints (3) ensures each word in the source translated exactly once, which guarantees that the search space of the ILP problem is the same as the search space of a phrase-based sys- Constraints (4) bind the and that whenever a link active, the correphrase also active. Constraints (5) play a similar role for the reference. though it accurately models the search space of a phrase-based decoder, this programs is not really useful as is: due to out-ofvocabulary words or missing entries in the phrase table, the constraint that all source words should be translated infeasible We propose to relax this problem and allow some source words to remain untranslated. This is done by replacing constraints (3) by: X To better reflect the behavior of phrase-based decoders, which attempt to translate all source words, we also need to modify the objective function as follows: X + i,j,k,l i,j The second term in this new objective ensures that optimal solutions translate as many source words as possible. ILP problem is said to be every possible solution violates at least one constraint. last caveat the program is caused by frequently occurring source tokens, such as function words or punctuation signs, which can often align with more than one target word. For lack of taking distortion information into account in our objective function, all these alignments are deemed equivalent, even if some of them are clearly more satisfactory than others. This situation is illustrated on Figure 1. le chat et le the cat and the dog Figure 1: Equivalent alignments between “le” and “the”. The dashed lines corresponds to a less interpretable solution. To overcome this difficulty, we propose a last change to the objective function: X + i,j,k,l i,j i,j,k,l Compared to the objective function of the relaxed problem (6), we introduce here a supplementary penalty factor which favors monotonous alignments. For each phrase pair, the higher the difference between source and target the higher this penalty. If small enough, this extra term allows us to select, among all the optialignments of the the one with lowest distortion. In our experiments, we set ensure that the penalty factor is always smaller than the reward for aligning two single words. 2.4 Selecting Indicator Variables In the approach introduced in the previous sections, the oracle decoding problem is solved by selecting, among a set of possible translation links, the ones that yield the solution with the highest unigram recall. We propose two strategies to build this set of possible links. In the first one, denoted indicator created if there is an entry from word position the source and word position the target. In this strategy, the ILP program considers exactly the same ruleset as conventional phrase-based decoders. We also consider an alternative strategy, which could help us to identify errors made during the phrase extracprocess. In this strategy, denoted an created when the following three criare met: from position the source; denoted spans from position i,j,k,l i,j,k,l under the constraints: 936 the reference; not an entry of the phrase table. The resulting set of indicator variables thus contains, at least, all the variables used in the exact match strategy. In addition, we license here the use of phrases containing words that do not occur in the reference. In fact, using such solutions can yield higher BLEU scores when the reward for additional correct matches exceeds the cost incurred by wrong predictions. These cases are symptoms of situations where the extraction heuristic failed to extract potentially useful subphrases. 2.5 Oracle Decoding with Reordering Constraints The ILP problem introduced in the previous section can be extended in several ways to describe and test various improvements to phrase-based systems or to evaluate the impact of different parameter settings. This flexibility mainly stems from the possibility offered by our framework to express arbitrary constraints over variables. In this section, we illustrate these possibilities by describing how reordering constraints can easily be considered. As a first example, the Moses decoder uses a distortion limit to constrain the set of possible reorderings. This constraint “enforces (...) that the last word of a phrase for translation cannot be more than words from the leftmost untranslated word in the source” (Lopez, 2009) and is expressed as: s.t. &gt; · + ≤ The maximum distortion limit strategy (Lopez, 2009) is also easily expressed and take the following form (assumthis constraint is parameterized by &lt; m · − d Implementing the “local” or MJ-d (Kumar and Byrne, 2005) reordering strategy is also straightforward, and implies using the following constraints: − Similarly, It is possible to simulate decoding under the IBM(d) reordering by considering the following constraints: corresponds to the of Moses IBM(d) constraints, the translation is done, phrase by phrase, from the beginning of the sentence until the end and only one of the first phrase can be selected for translation. In these constraints, the first factor corresponds to the rightmost translated word of the source and the second one to the number of translated source words. The constraints simply enforce that, at each step of the decoding, are no more than words that were skipped. Note that the constraints introduced above are not all linear in the problem variables; however they can easily be linearized using standard ILP techniques (Roth and Yih, 2005). 3 Oracle Decoding for Failure Analysis 3.1 Experimental Setting We propose to use our oracle decoder to study the ability of a PBTS to translate from English to French and from German to English. These two languages pairs present different challenges: English to French translation is considered a relatively easy pair, notwithstanding the difficulties of generating the right inflection marks in French. Translating from German into English is more difficult, notably due to the productivity of inflectional and compounding processes in German, and also to significant differences in word ordering between these languages. Our experiments are based on the corpora distributed for the WMT’09 constrained tasks (Callison-Burch et al., 2009). All data are tokenized, cleaned and converted to lowercase letters using the tools provided by the organizers. We then used a standard training pipeline to construct the translation model: the bitexts aligned using symmetrized using the the phrase table was extracted and scored using the tools distributed with Finally, baseline systems were optimized using WMT’08 test set as development using MERT. Note that, for all these steps, we used the default value of the various parameters. The extracted phrase table is then used to find the oracle alignment on the task test set. Recall that oracle decoding do not use the scores estimated by Moses in any way. In the experiments reported below, two settings are In the first one, denoted Moses was trained only on a small data set taken from the News Commentary corpus. Using a small sized corpus reduces both training time and decoding time, which allows us to quickly test different configurations of the decoder. In a setting, denoted Moses was trained on a larger corpora containing the entirety of the Europarl Corpus, but no in-domain data, to provide results on more realistic conditions. Statistics regarding the different corpora used are reported in Table 1. These statistics are computed on the lowercase cleaned corpora. ������ k, X 937 % OOV Table 1: Statistics regarding the training corpora: number of words, number of sentences, vocabulary and phrase table size and percentage of test words not appearing in the train set (OOV). Finding the oracle alignment amounts to solving the ILP problems introduced above. Even though ILP problems are NP-hard in general, there exist several off-theshelf ILP solvers able to efficiently find an optimal solution or decide that the problem is infeasible. In our experwe used the free solver 2007). An optimal solution was found for all problems we con- Decoding the of WMT’09 test takes about 10 minutes (wall time) for the and several hours for the 3.2 Oracle BLEU Score Table 2 reports, for all considered settings, the BLEU-4 achieved by our oracle decoder, as well as the number of source words used to generate the oracle hypothesis and the number of target words that are reachable. In these experiments, two objective functions were considered: first, we only consider the objective function corresponding to the relaxed problem defined by Eq. (6); second, we introduced an extra term in the objective to penalize distortion, as described by Eq. (7). Unless explicitly stated otherwise, we always used the exact match strategy. The main result in 2 is that, for the two language pairs considered, the expressive power of PBTS is not the limiting factor to achieve high translation performance. In fact, for most sentences in the test set, excellent oracle hypotheses, which contain a very high proportion of reference words, are found. This remains true even when the phrase table is extracted from a small corpus. Given that the best BLEU-4 scores achieved during the WMT’09 are about the English to French task the German to English task ((Callison-Burch et al., 2009), Tables 26 and 25), these results strongly suggest that the main bottleneck of current phrase-based translation systems is their scoring function rather than their expressive power. As we will discuss in Section 4, similar conclusions were drawn by (Auli et al., 2009) and (Turchi et al., 2008). Several additional comments on these numbers are in our experiments are run on a 8 cores computer, each core being a 2.2GHz Intel Processor; the decoder is multi-threaded. are computed on lowercase with the default tokenization. order. Despite these very high BLEU scores, in most cases, the reference is only partly translated. In the most case, for the English to French setonly the references could be fully gener- These numbers are consistent with the results rein (Auli et al., 2009). Similarly, only about of the source sentences are completely translated by the oracle decoder, which supports our choice to consider a relaxed version of the ILP problem. Finally, Table 2 also shows that introducing the distortion penalty does not affect the oracle performance of the decoder. Considering the inside match strategy improves the performance of the oracle decoder: for instance, for the to French oracle decoder with the inside match strategy achieves a BLEU-4 score of improvement over the baseline). To this score, the phrases used during decoding were phrases that are not considered by the exact match strategy. Similar results can be observed for other settings, which highlights the significance of one kind of failure of the extraction heuristic: useful “subphrases” of actual phrase pairs are not always extracted. The numbers in Table 2, no matter how good they may look, should be considered with caution: they only imply that, for most test sentences, all the information necessary to produce a good translation is available in the phrase table. However, the alignment decisions underlying these oracle hypotheses are sometimes hard to justify, and one has to accept that part of these good hypotheses translations are due to a series of lucky alignment errors. This is illustrated on Figure 2, which displays one such lucky oracle alignment based on the misalignment, during trainof the French preposition “des” with the English noun “stock”. Such lucky errors are naturally also observed in the outputs of conventional decoders, even though phrase table filtering heuristics probably makes them somewhat more rare. 3.3 Analyzing Non-Reachable Parts of a Reference Table 3 contains typical examples of sentence pairs that could not be fully generated by our oracle decoder. They numbers were obtained, albeit much more slowly, with the of Moses. 938 training set objective function % source translated % target generated 4-BLEU Table 2: Translation score of the ILP oracle decoder for the various settings described in Section 3.1 stock fall in chute des actions en asie Figure 2: Example of alignment obtained by our oracle decoder illustrate the three main reasons which cause some parts the reference to remain • phrases are missing from the phrase table, either because they do not occur in the training corpus (OOVs) or because they failed to be extracted. In Table 3, OOV errors are mainly due to past tense translated into verbs conjugated in sim- “rencontr`erent”, “renoua”) a French literary tense, mostly used in formal writings. • obvious errors (misspelled words, misinterpretation or mistranslation, ...) in the reference. The reference of the fifth example contains one such error: the state name “Nevada” is translated to “n’´evadiez” (literally “have not escaped”), yielding a very poor reference sentence. • parts of the reference have no translation equivalence in the source. This can be either because references are produced in “context” and some pieces of information are moved across sentence boundaries or because these references are non-literal. The fourth example, which seems to be the translation of a title, falls into this category: the French part contains a reference to the context (“les SA” is referring to the bacteria the text is talking about) which is not in the source text. Non-literal translation are illustrated by the third example, where English “Monis translated into French “la veille” day before). While the first kind of errors is inherent to the use of a statistical approach, the last two kinds result from the quality of the data used in the evaluation and directly impact both training and evaluation of automatic translation systems: if they should not distort too much comparisons of MT systems, these errors prevent us from assessing the “global” quality of automatic translation and, if similar errors are found in the train set, they make learning harder as some probability mass is wasted to model them. To provide a more quantitative analysis, we manually looked at all the non-aligned parts of some WMT’09 references and found that out of 800 references, more than 133 contain either an obvious translation error or can not achieved by a Note that, while identifying these errors could be done in many ways, our oracle decoder makes it far easier. 3.4 Identifying Causes of Failure By comparing the hypotheses found by the oracle decoder and the ones found by the phrase-based decoder, causes of failure can be easily identified. In this section, we will present several measures that allow us to identify and quantify several causes of failure. Caused by Search Space Pruning from Section 1.1 that Moses uses several heuristics to prune the search space. In particular, there is a distortion limit and a limit on the number of target phrases considered for one source phrase. In this paragraph, we evaluate the impact of these two heuristics on translation quality. Table 4 presents the average distortion computed on the oracle hypotheses, as well as the percentage of phrases used that have a distortion strictly greater than 6 (the default distortion limit of Moses). All these numare obtained by solving the problem. Surprisingly enough, the average distortion of oracle hypotheses is quite small, even for the German to English task, and the distortion constraint seems to be violated only in a few cases. It also appears that the distorof the hypotheses generated in the significantly larger than in the This can be explained by the extra degrees of freedom in the at a finer level is an on-going effort; the annotated is available from 939 On Monday the American House of Representatives plan to support the financial into which up to 700 billion dollars (nearly 12 Czech was to be invested. Lundi, la chambre des repr´esentants am´ericaine projet de soutient du syst`eme financier, elle aurait dˆu consacrer jusqu’`a 700 milliards de dollars (pr`es de 12 Representatives of the legislators met with American Finance Minister Henry Paulson Saturday night in order to give the government fund a final form. Dans la nuit de samedi a` des repr´esentants des l´egislateurs ministre des finances am´ericain Henry Paulson, afin de donner au fond du gouvernement une forme finale. The Prague Stock Market immediately continued its fall from the beginning of Tuesday’s trading , when it dropped by nearly six percent. Mardi, d`es le d´ebut des ´echanges, la bourse de prague sa chute de la six pour Resistance SA r´esistent aux antibiotiques. According to senator Harry Reid, that is how that legislators are trying to have Congress to reach a definitive agreement as early as on Sunday. D’apr`es le s´enateur d`emocrate Reid, les l´egislateurs font de sorte que le Congr`es un accord d´efinitif d`es dimanche. Table 3: Output examples of our oracle decoder on the English to French task. Words in bold are non-aligned words and words in italic are non-aligned out-of-vocabulary words. For clarity the examples have been detokenized and recased. avg. %phrases distortion with a dist. Table 4: Average distortion and percentage of phrases with a distortion greater that Moses default distortion limit. alignment decisions enabled by the use of larger training corpora and phrase table. To evaluate the impact of the second heuristic, we computed the number of phrases discarded by Moses (beof the default but used in the oracle hy- In the English to French account for the total number of phrases used in the oracle hypotheses. When the oracle decoder is constrained to use the same phrase table as Moses, its score drops to This shows that filtering the phrase table prior to decoding discards many useful phrase pairs and is seriously limiting the best achievable performance, a conclusion shared with (Auli et al., 2009). Errors errors can be identified by comparing the score of the best hypothesis found by Moses and the score of the oracle hypothesis. If the score of the oracle hypothesis is higher, then there has been a search error; on the contrary, there has been an estimation error when the score of the oracle hypothesis is lower than the score of the best hypothesis found by Moses. Based on the comparison of the score of Moses hypotheses and of oracle hypotheses for the English to our preliminary conclusion is that the number of search errors is quite limited: only about 5% of the hypotheses of our oracle decoder are actually getting a better score than Moses solutions. Again, this shows that the scoring function (model error) is one of the main bottleneck of current PBTS. Comparing these hypotheses is nonetheless quite revealing: while Moses mostly selects phrase pairs with high translation scores and generates monotonous alignments, our ILP decoder uses larger reorderings and less probable phrases to achieve better solutions: on average, the reordering of oracle solutions is compared to for Moses outputs. Given the weight assigned through MERT training to the distortion score, no wonder that these hypotheses are severely penalized. Impact of Phrase Length observed outputs do not only depend on decisions made during the search, but also on decisions made during training. One such decision is the specification of maximal length for the source and target phrases. In our framework, evaluating the impact of this decision is simple: it suffices to change the definition of indicator variables so as to consider only alignments between phrases of a given length. the English-French the most restrictive choice, when only alignments between single are authorized, yields an oracle BLEU-4 of however, authorizing phrases up to length 2 allows to an oracle value of very close to the score when considering all extracted phrases training set 940 This is corroborated with a further analysis of our oracle alignments, which use phrases whose average source is (respectively target words). If many studies have already acknowledged the predominance of “small” phrases in actual translations, our oracle scores suggest that, for this language pair, increasing the length limit beyond be a waste of computational resources. 4 Related Work To the best of our knowledge, there are only a few works that try to study the expressive power of phrase-based machine translation systems or to provide tools for analyzing potential causes of failure. The approach described in (Auli et al., 2009) is very similar to ours: in this study, the authors propose to find and analyze the limits of machine translation systems by the A reference is reachable for a given system if it can be exactly generated by this system. Reference reachability is assessed using Moses in forced decoding mode: during search, all hypotheses that deviate from the reference are simply discarded. Even though the main goal of this study was to compare the search space of phrase-based and hierarchical systems, it also provides some insights on the impact of various search parameters in Moses, delivering conclusions that are consistent with our main results. As described in Section 1.2, these authors also propose a typology of the errors of a statistical translation systems, but do not attempt to provide methods for identifying them. The authors of (Turchi et al., 2008) study the learning capabilities of Moses by extensively analyzing learning curves representing the translation performances as a function of the number of examples, and by corrupting the model parameters. Even though their focus is more on assessing the scoring function, they reach conclusions similar to ours: the current bottleneck of translation performances is not the representation power of the PBTS but rather in their scoring functions. Oracle decoding is useful to compute reachable pseudo-references in the context of discriminative training. This is the main motivation of (Tillmann and Zhang, 2006), where the authors compute high BLEU hypotheses by running a conventional decoder so as to maximize a per-sentence approximation of BLEU-4, under a simple (local) reordering model. Oracle decoding has also been used to assess the limitations induced by various reordering constraints in (Dreyer et al., 2007). To this end, the authors propose to use a beam-search based oracle decoder, which computes lower bounds of the best achievable BLEU-4 using dynamic programming techniques over finite-state (for so-called local and IBM constraints) or hierarchically structured (for ITG constraints) sets of hypotheses. Even though the numbers reported in this study are not directly with it seems that our decoder is not only conceptually much simpler, but also achieves much more optimistic lower-bounds of the oracle BLEU score. The approach described in (Li and Khudanpur, 2009) employs a similar technique, which is to guide a heuristic search in an hypergraph representing possible translation hypotheses with n-gram counts matches, which amounts to decoding with a n-gram model trained on the sole reference translation. Additional tricks are presented in this article to speed-up decoding. Computing oracle BLEU scores is also the subject of (Zens and Ney, 2005; Leusch et al., 2008), yet with a different emphasis. These studies are concerned with finding the best hypotheses in a word graph or in a consensus network, a problem that has various implications for multi-pass decoding and/or system combination techniques. The former reference describes an exponential approximate algorithm, while the latter proves the NPcompleteness of this problem and discuss various heuristic approaches. Our problem is somewhat more complex and using their techniques would require us to built word graphs containing all the translations induced by arbitrary segmentations and permutations of the source sentence. 5 Conclusions In this paper, we have presented a methodology for analyzing the errors of PBTS, based on the computation of an approximation of the BLEU-4 oracle score. We have shown that this approximation could be computed fairly accurately and efficiently using Integer Linear Programming techniques. Our main result is a confirmation of the fact that extant PBTS systems are expressive enough to achieve very high translation performance with respect to conventional quality measurements. The main efforts should therefore strive to improve on the way phrases and hypotheses are scored during training. This gives further support to attempts aimed at designing context-dependent scoring functions as in (Stroppa et al., 2007; Gimpel and Smith, 2008), or at attempts to perform discriminative training of feature-rich models. (Bangalore et al., 2007). We have shown that the examination of difficult-totranslate sentences was an effective way to detect errors or inconsistencies in the reference translations, making our approach a potential aid for controlling the quality or assessing the difficulty of test data. Our experiments have also highlighted the impact of various parameters. Various extensions of the baseline ILP program have been suggested and/or evaluated. In particular, the ILP formalism lends itself well to expressing various constraints that are typically used in conventional PBTS. In best BLEU-4 oracle they achieve on Europarl German to Enis approximately but they considered a smaller version of the training corpus and the WMT’06 test set. 941 our future work, we aim at using this ILP framework to systematically assess various search configurations. We plan to explore how replacing non-reachable references with high-score pseudo-references can improve discriminative training of PBTS. We are also concerned by determining how tight is our approximation of the BLEU- 4 score is: to this end, we intend to compute the best score within the solutions of the oracle decoding problem. Acknowledgments Warm thanks to Houda Bouamor for helping us with the annotation tool. This work has been partly financed by OSEO, the French State Agency for Innovation, under the Quaero program.</abstract>
<note confidence="0.948197">References Achterberg. 2007. Integer Program- Ph.D. thesis, Technische Universit¨at</note>
<web confidence="0.997502">http://opus.kobv.de/tuberlin/volltexte/</web>
<abstract confidence="0.949402454545455">Abhishek Arun and Philipp Koehn. 2007. Online learning methods for discriminative training of phrase based statismachine translation. In of MT Summit Copenhagen, Denmark. Michael Auli, Adam Lopez, Hieu Hoang, and Philipp Koehn. 2009. A systematic analysis of translation model search In of pages 224–232, Athens, Greece. Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlawith human judgments. In of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine</abstract>
<note confidence="0.680949543478261">and/or pages 65–72, Ann Arbor, Michigan. Srinivas Bangalore, Patrick Haffner, and Stephan Kanthak. 2007. Statistical machine translation through global lexiselection and sentence reconstruction. In of pages 152–159, Prague, Czech Republic. L´eon Bottou and Olivier Bousquet. 2008. The tradeoffs of large learning. In of pages 161–168, Vancouver, B.C., Canada. Chris Callison-Burch, Philipp Koehn, Christof Monz, and Josh Schroeder. 2009. Findings of the 2009 Workshop on Sta- Machine Translation. In of pages 1–28, Athens, Greece. David Chiang, Steve DeNeefe, Yee Seng Chan, and Hwee Tou Ng. 2008. Decomposability of translation metrics for evaluation and efficient algorithms. In of pages 610–619, Honolulu, Hawaii. John De Nero and Dan Klein. 2008. The complexity of phrase problems. In of ACL: HLT, Short pages 25–28, Columbus, Ohio. Markus Dreyer, Keith B. Hall, and Sanjeev P. Khudanpur. 2007. Comparing reordering constraints for smt using efficient bleu computation. In Workshop on and Structure in Statistical pages 103– 110, Rochester, New York. Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu, and Kenji Yamada. 2001. Fast decoding and optimal decodfor machine translation. In pages 228–235, Toulouse, France. Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu, and Kenji Yamada. 2004. Fast and optimal decoding for translation. 154(1-2):127– 143. Ulrich Germann. 2003. Greedy decoding for statistical matranslation in almost linear time. In of pages 1–8, Edmonton, Canada. Kevin Gimpel and Noah A. Smith. 2008. Rich source-side for statistical machine translation. In of pages 9–17, Columbus, Ohio. Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Staphrase-based translation. In of pages 48–54, Edmonton, Canada. Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison- Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007.</note>
<title confidence="0.8559475">Moses: Open source toolkit for statistical machine transla- In of ACL, demonstration</title>
<author confidence="0.950383">Pharaoh A beam search decoder for</author>
<abstract confidence="0.56427375">statistical machine translation models. In pages 115–124, Washington DC. Shankar Kumar and William Byrne. 2005. Local phrase remodels for statistical machine translation. In</abstract>
<note confidence="0.902777175">pages 161–168, Vancouver, Canada. Alon Lavie, Kenji Sagae, and Shyamsundar Jayaraman. The significance of recall in automatic metrics for MT evaluation. Proc. of pages 134–143, Washington DC. Gregor Leusch, Evgeny Matusov, and Hermann Ney. 2008. Complexity of finding the BLEU-optimal hypothesis in a network. In of pages 839–847, Honolulu, Hawaii. Zhifei Li and Sanjeev Khudanpur. 2009. Efficient extraction oracle-best translations from hypergraphs. In of pages 9–12, Boulder, Colorado. Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and Ben Taskar. 2006. An end-to-end discriminative approach to matranslation. In of pages 761–768, Sydney, Australia. Adam Lopez. 2009. Translation as weighted deduction. In of pages 532–540, Athens, Greece. Franz Josef Och and Hermann Ney. 2003. A systematic comof various statistical alignment models. 29(1):19–51. Franz Josef Och. 2003. Minimum error rate training in statismachine translation. In of pages 160–167, Sapporo, Japan. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-jing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. Technical report, Philadelphia, Pennsylvania. D. Roth and W. Yih. 2005. Integer linear programming inferfor conditional random fields. In of pages 737–744, Bonn, Germany. Nicolas Stroppa, Antal van den Bosch, and Andy Way. 2007. Exploiting source similarity for smt using context-informed 942 features. In Andy Way and Barbara Gawronska, editors, of pages 231–240, Sk¨ovde, Sweden. Christoph Tillmann and Tong Zhang. 2006. A discriminative training algorithm for statistical mt. In of pages 721–728, Sydney, Australia. Marco Turchi, Tijl De Bie, and Nello Cristianini. 2008. Learning performance of a machine translation system: a statistical computational analysis. In of pages 35–43,</note>
<address confidence="0.921973">Columbus, Ohio.</address>
<author confidence="0.846991">Word graphs for sta-</author>
<affiliation confidence="0.301142">machine translation. In of the ACL Workshop</affiliation>
<address confidence="0.699147666666667">Building and Using Parallel pages 191–198, Ann Arbor, Michigan. 943</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Tobias Achterberg</author>
</authors>
<title>Constraint Integer Programming.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<pages>2007--1611</pages>
<institution>Technische Universit¨at Berlin.</institution>
<note>http://opus.kobv.de/tuberlin/volltexte/</note>
<contexts>
<context position="25541" citStr="Achterberg, 2007" startWordPosition="4205" endWordPosition="4206">40 242,219 #phrase table 3,061,701 46,003, 525 4,133,190 44, 402, 367 % OOV 5.3% 3.1% 8.0% 5.2% Table 1: Statistics regarding the training corpora: number of words, number of sentences, vocabulary and phrase table size and percentage of test words not appearing in the train set (OOV). Finding the oracle alignment amounts to solving the ILP problems introduced above. Even though ILP problems are NP-hard in general, there exist several off-theshelf ILP solvers able to efficiently find an optimal solution or decide that the problem is infeasible. In our experiments, we used the free solver SCIP (Achterberg, 2007). An optimal solution was found for all problems we considered. Decoding the 3,027 sentences of WMT’09 test set takes about 10 minutes (wall time) for the NEWSCO setting, and several hours for the EUROPARL setting13. 3.2 Oracle BLEU Score Table 2 reports, for all considered settings, the BLEU-4 scores14 achieved by our oracle decoder, as well as the number of source words used to generate the oracle hypothesis and the number of target words that are reachable. In these experiments, two objective functions were considered: first, we only consider the objective function corresponding to the rela</context>
</contexts>
<marker>Achterberg, 2007</marker>
<rawString>Tobias Achterberg. 2007. Constraint Integer Programming. Ph.D. thesis, Technische Universit¨at Berlin. http://opus.kobv.de/tuberlin/volltexte/ 2007/1611/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abhishek Arun</author>
<author>Philipp Koehn</author>
</authors>
<title>Online learning methods for discriminative training of phrase based statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of MT</booktitle>
<location>Summit XI, Copenhagen, Denmark.</location>
<marker>Arun, Koehn, 2007</marker>
<rawString>Abhishek Arun and Philipp Koehn. 2007. Online learning methods for discriminative training of phrase based statistical machine translation. In Proc. of MT Summit XI, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Adam Lopez</author>
<author>Hieu Hoang</author>
<author>Philipp Koehn</author>
</authors>
<title>A systematic analysis of translation model search spaces.</title>
<date>2009</date>
<booktitle>In Proc. of WMT,</booktitle>
<pages>224--232</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="3889" citStr="Auli et al., 2009" startWordPosition="620" endWordPosition="623">l., 2007) considers only a restricted number of translations for each source sequence2 and enforces a distortion limit3 over which phrases can be reordered. As a consequence, the best translation hypothesis returned by the decoder is not always the one with the highest score. 1.2 Typology of PBTS Errors Analyzing the errors of a SMT system is not an easy task, because of the number of models that are combined, the size of these models, and the high complexity of the various decision making processes. For a SMT system, three different kinds of errors can be distinguished (Germann et al., 2004; Auli et al., 2009): search errors, induction errors and model errors. The former corresponds to cases where the hypothesis with the best score is missed by the search procedure, either because of the use of an ap2the ttl option of Moses, defaulting to 20. 3the dl option of Moses, whose default value is 7. 933 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 933–943, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics proximate search method or because of the restrictions of the search space. Induction errors correspond to case</context>
<context position="6280" citStr="Auli et al., 2009" startWordPosition="1003" endWordPosition="1006">s is of primary interest for SMT system developers: for lack of such diagnoses, it is difficult to figure out which components of the system require the most urgent attention. Diagnoses are however, given the tight intertwining among the various component of a system, very difficult to obtain: most evaluations are limited to the computation of global scores and usually do not imply any kind of failure analysis. 1.3 Contribution and organization To systematically assess the impact of the multiple heuristic decisions made during training and decoding, we propose, following (Dreyer et al., 2007; Auli et al., 2009), to work out oracle scores, that is to evaluate the best achievable performances of a PBTS. We aim at both studying the expressive power of PBTS and at providing tools for identifying and quantifying causes of failure. Under standard metrics such as BLEU (Papineni et al., 2002), oracle scores are difficult (if not impossible) to compute, but, by casting the computation of the oracle unigram recall and precision as an Integer Linear Programming (ILP) problem, we show that it is possible to efficiently compute accurate lower-bounds of the oracle BLEU-4 scores and report measurements performed o</context>
<context position="9627" citStr="Auli et al., 2009" startWordPosition="1537" endWordPosition="1540">ference and the hypothesis. Note that in the oracle decoding problem, we are only assessing the ability of PBT systems to generate good candidate translations, irrespective of their ability to score them properly. We believe that studying this problem is interesting for various reasons. First, as described in Section 3.4, comparing the best hypothesis a system could have generated and the hypothesis it actually generates allows us to carry on both quantitative and qualitative failure analysis. The oracle decoding problem can also be used to assess the expressive power of phrase-based systems (Auli et al., 2009). Other applications include computing acceptable pseudo-references for discriminative training (Tillmann and Zhang, 2006; Liang et al., 2006; Arun and 5The oracle decoding problem can be extended to the case of multiple references. For the sake of simplicity, we only describe the case of a single reference. 934 Koehn, 2007) or combining machine translation systems in a multi-source setting (Li and Khudanpur, 2009). We have also used oracle decoding to identify erroneous or difficult to translate references (Section 3.3). Evaluation Measure To fully define the oracle decoding problem, a measur</context>
<context position="27192" citStr="Auli et al., 2009" startWordPosition="4477" endWordPosition="4480">racle hypotheses, which contain a very high proportion of reference words, are found. This remains true even when the phrase table is extracted from a small corpus. Given that the best BLEU-4 scores achieved during the WMT’09 evaluation are about 28 for the English to French task and 24 for the German to English task ((Callison-Burch et al., 2009), Tables 26 and 25), these results strongly suggest that the main bottleneck of current phrase-based translation systems is their scoring function rather than their expressive power. As we will discuss in Section 4, similar conclusions were drawn by (Auli et al., 2009) and (Turchi et al., 2008). Several additional comments on these numbers are in 13All our experiments are run on a 8 cores computer, each core being a 2.2GHz Intel Processor; the decoder is multi-threaded. 14These are computed on lowercase with the default tokenization. order. Despite these very high BLEU scores, in most cases, the reference is only partly translated. In the most favorable case, for the English to French EUROPARL setting, only 26% of the references could be fully generated15. These numbers are consistent with the results reported in (Auli et al., 2009). Similarly, only about 3</context>
<context position="36608" citStr="Auli et al., 2009" startWordPosition="6007" endWordPosition="6010">ase table. To evaluate the impact of the second heuristic, we computed the number of phrases discarded by Moses (because of the default ttl limit) but used in the oracle hypotheses. In the English to French NEWSCO setting, they account for 34.11% of the total number of phrases used in the oracle hypotheses. When the oracle decoder is constrained to use the same phrase table as Moses, its BLEU-4 score drops to 42.78. This shows that filtering the phrase table prior to decoding discards many useful phrase pairs and is seriously limiting the best achievable performance, a conclusion shared with (Auli et al., 2009). Search Errors Search errors can be identified by comparing the score of the best hypothesis found by Moses and the score of the oracle hypothesis. If the score of the oracle hypothesis is higher, then there has been a search error; on the contrary, there has been an estimation error when the score of the oracle hypothesis is lower than the score of the best hypothesis found by Moses. Based on the comparison of the score of Moses hypotheses and of oracle hypotheses for the English to French NEWSCO setting, our preliminary conclusion is that the number of search errors is quite limited: only a</context>
<context position="39424" citStr="Auli et al., 2009" startWordPosition="6471" endWordPosition="6474">nts, which use phrases whose average source length is 1.21 words (respectively 1.31 for target words). If many studies have already acknowledged the predominance of “small” phrases in actual translations, our oracle scores suggest that, for this language pair, increasing the phrase length limit beyond 2 or 3 might be a waste of computational resources. 4 Related Work To the best of our knowledge, there are only a few works that try to study the expressive power of phrase-based machine translation systems or to provide tools for analyzing potential causes of failure. The approach described in (Auli et al., 2009) is very similar to ours: in this study, the authors propose to find and analyze the limits of machine translation systems by studying the reference reachability. A reference is reachable for a given system if it can be exactly generated by this system. Reference reachability is assessed using Moses in forced decoding mode: during search, all hypotheses that deviate from the reference are simply discarded. Even though the main goal of this study was to compare the search space of phrase-based and hierarchical systems, it also provides some insights on the impact of various search parameters in</context>
</contexts>
<marker>Auli, Lopez, Hoang, Koehn, 2009</marker>
<rawString>Michael Auli, Adam Lopez, Hieu Hoang, and Philipp Koehn. 2009. A systematic analysis of translation model search spaces. In Proc. of WMT, pages 224–232, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>65--72</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="11451" citStr="Banerjee and Lavie, 2005" startWordPosition="1828" endWordPosition="1831">ses and can not be evaluated in isolation. Because of its nondecomposability, maximizing BLEU-4 is hard; in particular, the phrase-level decomposability of the evaluation metric is necessary in our approach. To circumvent this difficulty, we propose to evaluate the similarity between a translation hypothesis and a reference by the number of their common words. This amounts to evaluating translation quality in terms of unigram precision and recall, which are highly correlated with human judgements (Lavie et al., ). This measure is closely related to the BLEU-1 evaluation metric and the Meteor (Banerjee and Lavie, 2005) metric (when it is evaluated without considering near-matches and the distortion penalty). We also believe that hypotheses that maximize the unigram precision and recall at the sentence level yield corpus level BLEU-4 scores close the maximal achievable. Indeed, in the setting we will introduce in the next section, BLEU-1 and BLEU-4 are highly correlated: as all correct words of the hypothesis will be compelled to be at their correct position, any hypothesis with a high 1-gram precision is also bound to have a high 2-gram precision, etc. 2.2 Formalizing the Oracle Decoding Problem The oracle </context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proc. of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65–72, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Patrick Haffner</author>
<author>Stephan Kanthak</author>
</authors>
<title>Statistical machine translation through global lexical selection and sentence reconstruction.</title>
<date>2007</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>152--159</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="43699" citStr="Bangalore et al., 2007" startWordPosition="7141" endWordPosition="7144">y accurately and efficiently using Integer Linear Programming techniques. Our main result is a confirmation of the fact that extant PBTS systems are expressive enough to achieve very high translation performance with respect to conventional quality measurements. The main efforts should therefore strive to improve on the way phrases and hypotheses are scored during training. This gives further support to attempts aimed at designing context-dependent scoring functions as in (Stroppa et al., 2007; Gimpel and Smith, 2008), or at attempts to perform discriminative training of feature-rich models. (Bangalore et al., 2007). We have shown that the examination of difficult-totranslate sentences was an effective way to detect errors or inconsistencies in the reference translations, making our approach a potential aid for controlling the quality or assessing the difficulty of test data. Our experiments have also highlighted the impact of various parameters. Various extensions of the baseline ILP program have been suggested and/or evaluated. In particular, the ILP formalism lends itself well to expressing various constraints that are typically used in conventional PBTS. In 17The best BLEU-4 oracle they achieve on Eu</context>
</contexts>
<marker>Bangalore, Haffner, Kanthak, 2007</marker>
<rawString>Srinivas Bangalore, Patrick Haffner, and Stephan Kanthak. 2007. Statistical machine translation through global lexical selection and sentence reconstruction. In Proc. of ACL, pages 152–159, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L´eon Bottou</author>
<author>Olivier Bousquet</author>
</authors>
<title>The tradeoffs of large scale learning.</title>
<date>2008</date>
<booktitle>In Proc. of NIPS,</booktitle>
<pages>161--168</pages>
<location>Vancouver, B.C.,</location>
<contexts>
<context position="4815" citStr="Bottou and Bousquet, 2008" startWordPosition="765" endWordPosition="768">ings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 933–943, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics proximate search method or because of the restrictions of the search space. Induction errors correspond to cases where, given the model, the search space does not contain the reference. Finally, model errors correspond to cases where the hypothesis with the highest score is not the best translation according to the evaluation metric. Model errors encompass several types of errors that occur during learning (Bottou and Bousquet, 2008)4. Approximation errors are errors caused by the use of a restricted and oversimplistic class of functions (here, finitestate transducers to model the generation of hypotheses and a linear scoring function to discriminate them) to model the translation process. Estimation errors correspond to the use of sub-optimal values for both the phrase pairs weights and the parameters of the scoring function. The reasons behind these errors are twofold: first, training only considers a finite sample of data; second, it relies on error prone alignments. As a result, some “good” phrases are extracted with </context>
</contexts>
<marker>Bottou, Bousquet, 2008</marker>
<rawString>L´eon Bottou and Olivier Bousquet. 2008. The tradeoffs of large scale learning. In Proc. of NIPS, pages 161–168, Vancouver, B.C., Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<date>2009</date>
<booktitle>Findings of the 2009 Workshop on Statistical Machine Translation. In Proc. of WMT,</booktitle>
<pages>1--28</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="23193" citStr="Callison-Burch et al., 2009" startWordPosition="3825" endWordPosition="3828">he ability of a PBTS to translate from English to French and from German to English. These two languages pairs present different challenges: English to French translation is considered a relatively easy pair, notwithstanding the difficulties of generating the right inflection marks in French. Translating from German into English is more difficult, notably due to the productivity of inflectional and compounding processes in German, and also to significant differences in word ordering between these languages. Our experiments are based on the corpora distributed for the WMT’09 constrained tasks (Callison-Burch et al., 2009). All data are tokenized, cleaned and converted to lowercase letters using the tools provided by the organizers. We then used a standard training pipeline to construct the translation model: the bitexts were aligned using Giza++11, symmetrized using the grow-diag-final-and heuristic; the phrase table was extracted and scored using the tools distributed with Moses.12 Finally, baseline systems were optimized using WMT’08 test set as development using MERT. Note that, for all these steps, we used the default value of the various parameters. The extracted phrase table is then used to find the orac</context>
<context position="26923" citStr="Callison-Burch et al., 2009" startWordPosition="4435" endWordPosition="4438">ated otherwise, we always used the exact match strategy. The main result in 2 is that, for the two language pairs considered, the expressive power of PBTS is not the limiting factor to achieve high translation performance. In fact, for most sentences in the test set, excellent oracle hypotheses, which contain a very high proportion of reference words, are found. This remains true even when the phrase table is extracted from a small corpus. Given that the best BLEU-4 scores achieved during the WMT’09 evaluation are about 28 for the English to French task and 24 for the German to English task ((Callison-Burch et al., 2009), Tables 26 and 25), these results strongly suggest that the main bottleneck of current phrase-based translation systems is their scoring function rather than their expressive power. As we will discuss in Section 4, similar conclusions were drawn by (Auli et al., 2009) and (Turchi et al., 2008). Several additional comments on these numbers are in 13All our experiments are run on a 8 cores computer, each core being a 2.2GHz Intel Processor; the decoder is multi-threaded. 14These are computed on lowercase with the default tokenization. order. Despite these very high BLEU scores, in most cases, t</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Schroeder, 2009</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Josh Schroeder. 2009. Findings of the 2009 Workshop on Statistical Machine Translation. In Proc. of WMT, pages 1–28, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Steve DeNeefe</author>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Decomposability of translation metrics for improved evaluation and efficient algorithms.</title>
<date>2008</date>
<booktitle>In Proc. of ECML,</booktitle>
<pages>610--619</pages>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="12587" citStr="Chiang et al., 2008" startWordPosition="2016" endWordPosition="2019"> high 2-gram precision, etc. 2.2 Formalizing the Oracle Decoding Problem The oracle decoding problem has already been considered in the case of word-based models, in which all translation units are bound to contain only one word. The problem can then be solved by a bipartite graph matching algorithm (Leusch et al., 2008): given a nxm binary matrix describing possible translation links between source words and target words7, this algorithm finds the subset of links maximizing the number of words of the reference that have been translated, while ensuring that each word 6Neither at the sentence (Chiang et al., 2008), nor at the phrase level. 7The (i, j) entry of the matrix is 1 if the ith word of the source can be translated by the jth word of the reference, 0 otherwise. is translated only once. Generalizing this approach to phrase-based systems amounts to solving the following problem: given a set of possible translation links between potential phrases of the source and of the target, find the subset of links so that the unigram precision and recall are the highest possible. The corresponding oracle hypothesis can then be easily generated by selecting the target phrases that are aligned with one source </context>
</contexts>
<marker>Chiang, DeNeefe, Chan, Ng, 2008</marker>
<rawString>David Chiang, Steve DeNeefe, Yee Seng Chan, and Hwee Tou Ng. 2008. Decomposability of translation metrics for improved evaluation and efficient algorithms. In Proc. of ECML, pages 610–619, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John De Nero</author>
<author>Dan Klein</author>
</authors>
<title>The complexity of phrase alignment problems.</title>
<date>2008</date>
<booktitle>In Proc. of ACL: HLT, Short Papers,</booktitle>
<pages>25--28</pages>
<location>Columbus, Ohio.</location>
<marker>De Nero, Klein, 2008</marker>
<rawString>John De Nero and Dan Klein. 2008. The complexity of phrase alignment problems. In Proc. of ACL: HLT, Short Papers, pages 25–28, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Keith B Hall</author>
<author>Sanjeev P Khudanpur</author>
</authors>
<title>Comparing reordering constraints for smt using efficient bleu oracle computation.</title>
<date>2007</date>
<booktitle>In NAACL-HLT/AMTA Workshop on Syntax and Structure in Statistical Translation,</booktitle>
<pages>103--110</pages>
<location>Rochester, New York.</location>
<contexts>
<context position="6260" citStr="Dreyer et al., 2007" startWordPosition="999" endWordPosition="1002">rious causes of errors is of primary interest for SMT system developers: for lack of such diagnoses, it is difficult to figure out which components of the system require the most urgent attention. Diagnoses are however, given the tight intertwining among the various component of a system, very difficult to obtain: most evaluations are limited to the computation of global scores and usually do not imply any kind of failure analysis. 1.3 Contribution and organization To systematically assess the impact of the multiple heuristic decisions made during training and decoding, we propose, following (Dreyer et al., 2007; Auli et al., 2009), to work out oracle scores, that is to evaluate the best achievable performances of a PBTS. We aim at both studying the expressive power of PBTS and at providing tools for identifying and quantifying causes of failure. Under standard metrics such as BLEU (Papineni et al., 2002), oracle scores are difficult (if not impossible) to compute, but, by casting the computation of the oracle unigram recall and precision as an Integer Linear Programming (ILP) problem, we show that it is possible to efficiently compute accurate lower-bounds of the oracle BLEU-4 scores and report meas</context>
<context position="41234" citStr="Dreyer et al., 2007" startWordPosition="6763" endWordPosition="6766">ar to ours: the current bottleneck of translation performances is not the representation power of the PBTS but rather in their scoring functions. Oracle decoding is useful to compute reachable pseudo-references in the context of discriminative training. This is the main motivation of (Tillmann and Zhang, 2006), where the authors compute high BLEU hypotheses by running a conventional decoder so as to maximize a per-sentence approximation of BLEU-4, under a simple (local) reordering model. Oracle decoding has also been used to assess the limitations induced by various reordering constraints in (Dreyer et al., 2007). To this end, the authors propose to use a beam-search based oracle decoder, which computes lower bounds of the best achievable BLEU-4 using dynamic programming techniques over finite-state (for so-called local and IBM constraints) or hierarchically structured (for ITG constraints) sets of hypotheses. Even though the numbers reported in this study are not directly comparable with ours17, it seems that our decoder is not only conceptually much simpler, but also achieves much more optimistic lower-bounds of the oracle BLEU score. The approach described in (Li and Khudanpur, 2009) employs a simi</context>
</contexts>
<marker>Dreyer, Hall, Khudanpur, 2007</marker>
<rawString>Markus Dreyer, Keith B. Hall, and Sanjeev P. Khudanpur. 2007. Comparing reordering constraints for smt using efficient bleu oracle computation. In NAACL-HLT/AMTA Workshop on Syntax and Structure in Statistical Translation, pages 103– 110, Rochester, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Kenji Yamada</author>
</authors>
<title>Fast decoding and optimal decoding for machine translation.</title>
<date>2001</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>228--235</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="14724" citStr="Germann et al., 2001" startWordPosition="2379" endWordPosition="2382">f (Leusch et al., 2008) and concur in making the oracle decoding problem for phrase-based models more complex than it is for word-based models: it can be proven, using arguments borrowed from (De Nero and Klein, 2008), that this problem is NP-hard even for the simple unigram precision measure. 2.3 An Integer Program for Oracle Decoding To solve the combinatorial problem introduced in the previous section, we propose to cast it into an Integer Linear Programming (ILP) problem, for which many generic solvers exist. ILP has already been used in SMT to find the optimal translation for word-based (Germann et al., 2001) and to study the complexity of learning phrase alignments (De Nero and Klein, 2008) models. Following the latter reference, we introduce the following variables: fi,j (resp. ek,l) is a binary indicator variable that is true when the phrase contains all spans from betweenword position i to j (resp. k to l) of the source (resp. target) sentence. We also introduce a binary variable, denoted ai,j,k,l, to describe a possible link between source phrase fi,j and target phrase ek,l. These variables are built from the entries of the phrase table according to selection strategies introduced in Section </context>
</contexts>
<marker>Germann, Jahr, Knight, Marcu, Yamada, 2001</marker>
<rawString>Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu, and Kenji Yamada. 2001. Fast decoding and optimal decoding for machine translation. In Proc. ofACL, pages 228–235, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Kenji Yamada</author>
</authors>
<title>Fast and optimal decoding for machine translation.</title>
<date>2004</date>
<journal>Artificial Intelligence,</journal>
<pages>154--1</pages>
<contexts>
<context position="3869" citStr="Germann et al., 2004" startWordPosition="616" endWordPosition="619">oder Moses (Koehn et al., 2007) considers only a restricted number of translations for each source sequence2 and enforces a distortion limit3 over which phrases can be reordered. As a consequence, the best translation hypothesis returned by the decoder is not always the one with the highest score. 1.2 Typology of PBTS Errors Analyzing the errors of a SMT system is not an easy task, because of the number of models that are combined, the size of these models, and the high complexity of the various decision making processes. For a SMT system, three different kinds of errors can be distinguished (Germann et al., 2004; Auli et al., 2009): search errors, induction errors and model errors. The former corresponds to cases where the hypothesis with the best score is missed by the search procedure, either because of the use of an ap2the ttl option of Moses, defaulting to 20. 3the dl option of Moses, whose default value is 7. 933 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 933–943, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics proximate search method or because of the restrictions of the search space. Induction error</context>
</contexts>
<marker>Germann, Jahr, Knight, Marcu, Yamada, 2004</marker>
<rawString>Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu, and Kenji Yamada. 2004. Fast and optimal decoding for machine translation. Artificial Intelligence, 154(1-2):127– 143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
</authors>
<title>Greedy decoding for statistical machine translation in almost linear time.</title>
<date>2003</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>1--8</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="3005" citStr="Germann, 2003" startWordPosition="476" endWordPosition="477">is aligned at the word level, by using alignment tools such as Giza++ (Och and Ney, 2003) and some symmetrisation heuristics; phrases are then extracted by other heuristics (Koehn et al., 2003) and assigned numerical weights. In the second step, the parameters of the scoring function are estimated, typically through Minimum Error Rate training (Och, 2003). Translating a sentence amounts to finding the best scoring translation hypothesis in the search space. Because of the combinatorial nature of this problem, translation has to rely on heuristic search techniques such as greedy hill-climbing (Germann, 2003) or variants of best-first search like multi-stack decoding (Koehn, 2004). Moreover, to reduce the overall complexity of decoding, the search space is typically pruned using simple heuristics. For instance, the state-of-the-art phrase-based decoder Moses (Koehn et al., 2007) considers only a restricted number of translations for each source sequence2 and enforces a distortion limit3 over which phrases can be reordered. As a consequence, the best translation hypothesis returned by the decoder is not always the one with the highest score. 1.2 Typology of PBTS Errors Analyzing the errors of a SMT</context>
</contexts>
<marker>Germann, 2003</marker>
<rawString>Ulrich Germann. 2003. Greedy decoding for statistical machine translation in almost linear time. In Proc. of NAACL, pages 1–8, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Rich source-side context for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of WMT,</booktitle>
<pages>9--17</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="43599" citStr="Gimpel and Smith, 2008" startWordPosition="7127" endWordPosition="7130">roximation of the BLEU-4 oracle score. We have shown that this approximation could be computed fairly accurately and efficiently using Integer Linear Programming techniques. Our main result is a confirmation of the fact that extant PBTS systems are expressive enough to achieve very high translation performance with respect to conventional quality measurements. The main efforts should therefore strive to improve on the way phrases and hypotheses are scored during training. This gives further support to attempts aimed at designing context-dependent scoring functions as in (Stroppa et al., 2007; Gimpel and Smith, 2008), or at attempts to perform discriminative training of feature-rich models. (Bangalore et al., 2007). We have shown that the examination of difficult-totranslate sentences was an effective way to detect errors or inconsistencies in the reference translations, making our approach a potential aid for controlling the quality or assessing the difficulty of test data. Our experiments have also highlighted the impact of various parameters. Various extensions of the baseline ILP program have been suggested and/or evaluated. In particular, the ILP formalism lends itself well to expressing various cons</context>
</contexts>
<marker>Gimpel, Smith, 2008</marker>
<rawString>Kevin Gimpel and Noah A. Smith. 2008. Rich source-side context for statistical machine translation. In Proc. of WMT, pages 9–17, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>48--54</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="2584" citStr="Koehn et al., 2003" startWordPosition="411" endWordPosition="414">del corresponds to the set of all possible sequences of 1Following the usage in statistical machine translation literature, we use “phrase” to denote a subsequence of consecutive words. rules applications. The scoring function aims to rank all possible translation hypotheses in such a way that the best one has the highest score. A PBTS is learned from a parallel corpus in two independent steps. In a first step, the corpus is aligned at the word level, by using alignment tools such as Giza++ (Och and Ney, 2003) and some symmetrisation heuristics; phrases are then extracted by other heuristics (Koehn et al., 2003) and assigned numerical weights. In the second step, the parameters of the scoring function are estimated, typically through Minimum Error Rate training (Och, 2003). Translating a sentence amounts to finding the best scoring translation hypothesis in the search space. Because of the combinatorial nature of this problem, translation has to rely on heuristic search techniques such as greedy hill-climbing (Germann, 2003) or variants of best-first search like multi-stack decoding (Koehn, 2004). Moreover, to reduce the overall complexity of decoding, the search space is typically pruned using simpl</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. of NAACL, pages 48–54, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris CallisonBurch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL, demonstration session.</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="3280" citStr="Koehn et al., 2007" startWordPosition="513" endWordPosition="516">oring function are estimated, typically through Minimum Error Rate training (Och, 2003). Translating a sentence amounts to finding the best scoring translation hypothesis in the search space. Because of the combinatorial nature of this problem, translation has to rely on heuristic search techniques such as greedy hill-climbing (Germann, 2003) or variants of best-first search like multi-stack decoding (Koehn, 2004). Moreover, to reduce the overall complexity of decoding, the search space is typically pruned using simple heuristics. For instance, the state-of-the-art phrase-based decoder Moses (Koehn et al., 2007) considers only a restricted number of translations for each source sequence2 and enforces a distortion limit3 over which phrases can be reordered. As a consequence, the best translation hypothesis returned by the decoder is not always the one with the highest score. 1.2 Typology of PBTS Errors Analyzing the errors of a SMT system is not an easy task, because of the number of models that are combined, the size of these models, and the high complexity of the various decision making processes. For a SMT system, three different kinds of errors can be distinguished (Germann et al., 2004; Auli et a</context>
</contexts>
<marker>Koehn, Hoang, Birch, CallisonBurch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris CallisonBurch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of ACL, demonstration session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: A beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proc. of AMTA,</booktitle>
<pages>115--124</pages>
<location>Washington DC.</location>
<contexts>
<context position="3078" citStr="Koehn, 2004" startWordPosition="486" endWordPosition="487">and Ney, 2003) and some symmetrisation heuristics; phrases are then extracted by other heuristics (Koehn et al., 2003) and assigned numerical weights. In the second step, the parameters of the scoring function are estimated, typically through Minimum Error Rate training (Och, 2003). Translating a sentence amounts to finding the best scoring translation hypothesis in the search space. Because of the combinatorial nature of this problem, translation has to rely on heuristic search techniques such as greedy hill-climbing (Germann, 2003) or variants of best-first search like multi-stack decoding (Koehn, 2004). Moreover, to reduce the overall complexity of decoding, the search space is typically pruned using simple heuristics. For instance, the state-of-the-art phrase-based decoder Moses (Koehn et al., 2007) considers only a restricted number of translations for each source sequence2 and enforces a distortion limit3 over which phrases can be reordered. As a consequence, the best translation hypothesis returned by the decoder is not always the one with the highest score. 1.2 Typology of PBTS Errors Analyzing the errors of a SMT system is not an easy task, because of the number of models that are com</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: A beam search decoder for phrase-based statistical machine translation models. In Proc. of AMTA, pages 115–124, Washington DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Local phrase reordering models for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of HLT,</booktitle>
<pages>161--168</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="21492" citStr="Kumar and Byrne, 2005" startWordPosition="3560" endWordPosition="3563">he Moses decoder uses a distortion limit to constrain the set of possible reorderings. This constraint “enforces (...) that the last word of a phrase chosen for translation cannot be more than d9 words from the leftmost untranslated word in the source” (Lopez, 2009) and is expressed as: ∀aijkl, ai0j0k0l0 s.t. k &gt; k0, aijkl · ai0j0k0l0 · |j − i0 + 1 |≤ d, The maximum distortion limit strategy (Lopez, 2009) is also easily expressed and take the following form (assuming this constraint is parameterized by d): ∀l &lt; m − 1, ai,j,k,l·ai0,j0,l+1,l0 · |i0 − j − 1 |&lt; d Implementing the “local” or MJ-d (Kumar and Byrne, 2005) reordering strategy is also straightforward, and implies using the following constraints: Xai0,j0,k0,l0 − k0≤k Similarly, It is possible to simulate decoding under the so-called IBM(d) reordering constraints10 by considering the following constraints: 9This corresponds to the dl parameter of Moses 10Under IBM(d) constraints, the translation is done, phrase by phrase, from the beginning of the sentence until the end and only one of the first d untranslated phrase can be selected for translation. In these constraints, the first factor corresponds to the rightmost translated word of the source a</context>
</contexts>
<marker>Kumar, Byrne, 2005</marker>
<rawString>Shankar Kumar and William Byrne. 2005. Local phrase reordering models for statistical machine translation. In Proc. of HLT, pages 161–168, Vancouver, Canada.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Alon Lavie</author>
<author>Kenji Sagae</author>
<author>Shyamsundar Jayaraman</author>
</authors>
<title>The significance of recall in automatic metrics for MT evaluation. In</title>
<booktitle>In Proc. of AMTA,</booktitle>
<pages>134--143</pages>
<location>Washington DC.</location>
<marker>Lavie, Sagae, Jayaraman, </marker>
<rawString>Alon Lavie, Kenji Sagae, and Shyamsundar Jayaraman. The significance of recall in automatic metrics for MT evaluation. In In Proc. of AMTA, pages 134–143, Washington DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor Leusch</author>
<author>Evgeny Matusov</author>
<author>Hermann Ney</author>
</authors>
<title>Complexity of finding the BLEU-optimal hypothesis in a confusion network. In</title>
<date>2008</date>
<booktitle>Proc. of EMNLP,</booktitle>
<pages>839--847</pages>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="12289" citStr="Leusch et al., 2008" startWordPosition="1968" endWordPosition="1971"> scores close the maximal achievable. Indeed, in the setting we will introduce in the next section, BLEU-1 and BLEU-4 are highly correlated: as all correct words of the hypothesis will be compelled to be at their correct position, any hypothesis with a high 1-gram precision is also bound to have a high 2-gram precision, etc. 2.2 Formalizing the Oracle Decoding Problem The oracle decoding problem has already been considered in the case of word-based models, in which all translation units are bound to contain only one word. The problem can then be solved by a bipartite graph matching algorithm (Leusch et al., 2008): given a nxm binary matrix describing possible translation links between source words and target words7, this algorithm finds the subset of links maximizing the number of words of the reference that have been translated, while ensuring that each word 6Neither at the sentence (Chiang et al., 2008), nor at the phrase level. 7The (i, j) entry of the matrix is 1 if the ith word of the source can be translated by the jth word of the reference, 0 otherwise. is translated only once. Generalizing this approach to phrase-based systems amounts to solving the following problem: given a set of possible t</context>
<context position="14126" citStr="Leusch et al., 2008" startWordPosition="2278" endWordPosition="2281"> consequence, to find the oracle hypothesis, we just have to maximize the recall, that is the number of words appearing both in the hypothesis and in the reference. Considering phrases instead of isolated words has a major impact on the computational complexity: in this new setting, the optimal segmentations in phrases of both the source and of the target have to be worked out in addition to links selection. Moreover, constraints have to be taken into account so as to enforce a proper segmentation of the source and target sentences. These constraints make it impossible to use the approach of (Leusch et al., 2008) and concur in making the oracle decoding problem for phrase-based models more complex than it is for word-based models: it can be proven, using arguments borrowed from (De Nero and Klein, 2008), that this problem is NP-hard even for the simple unigram precision measure. 2.3 An Integer Program for Oracle Decoding To solve the combinatorial problem introduced in the previous section, we propose to cast it into an Integer Linear Programming (ILP) problem, for which many generic solvers exist. ILP has already been used in SMT to find the optimal translation for word-based (Germann et al., 2001) a</context>
<context position="42228" citStr="Leusch et al., 2008" startWordPosition="6918" endWordPosition="6921">able with ours17, it seems that our decoder is not only conceptually much simpler, but also achieves much more optimistic lower-bounds of the oracle BLEU score. The approach described in (Li and Khudanpur, 2009) employs a similar technique, which is to guide a heuristic search in an hypergraph representing possible translation hypotheses with n-gram counts matches, which amounts to decoding with a n-gram model trained on the sole reference translation. Additional tricks are presented in this article to speed-up decoding. Computing oracle BLEU scores is also the subject of (Zens and Ney, 2005; Leusch et al., 2008), yet with a different emphasis. These studies are concerned with finding the best hypotheses in a word graph or in a consensus network, a problem that has various implications for multi-pass decoding and/or system combination techniques. The former reference describes an exponential approximate algorithm, while the latter proves the NPcompleteness of this problem and discuss various heuristic approaches. Our problem is somewhat more complex and using their techniques would require us to built word graphs containing all the translations induced by arbitrary segmentations and permutations of th</context>
</contexts>
<marker>Leusch, Matusov, Ney, 2008</marker>
<rawString>Gregor Leusch, Evgeny Matusov, and Hermann Ney. 2008. Complexity of finding the BLEU-optimal hypothesis in a confusion network. In Proc. of EMNLP, pages 839–847, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Efficient extraction of oracle-best translations from hypergraphs.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>9--12</pages>
<location>Boulder, Colorado.</location>
<contexts>
<context position="10045" citStr="Li and Khudanpur, 2009" startWordPosition="1602" endWordPosition="1605">y generates allows us to carry on both quantitative and qualitative failure analysis. The oracle decoding problem can also be used to assess the expressive power of phrase-based systems (Auli et al., 2009). Other applications include computing acceptable pseudo-references for discriminative training (Tillmann and Zhang, 2006; Liang et al., 2006; Arun and 5The oracle decoding problem can be extended to the case of multiple references. For the sake of simplicity, we only describe the case of a single reference. 934 Koehn, 2007) or combining machine translation systems in a multi-source setting (Li and Khudanpur, 2009). We have also used oracle decoding to identify erroneous or difficult to translate references (Section 3.3). Evaluation Measure To fully define the oracle decoding problem, a measure of the similarity between a translation hypothesis and its reference translation has to be chosen. The most obvious choice is the BLEU-4 score (Papineni et al., 2002) used in most machine translation evaluations. However, using this metric in the oracle decoding problem raises several issues. First, BLEU-4 is a metric defined at the corpus level and is hard to interpret at the sentence level. More importantly, BL</context>
<context position="41819" citStr="Li and Khudanpur, 2009" startWordPosition="6853" endWordPosition="6856">g constraints in (Dreyer et al., 2007). To this end, the authors propose to use a beam-search based oracle decoder, which computes lower bounds of the best achievable BLEU-4 using dynamic programming techniques over finite-state (for so-called local and IBM constraints) or hierarchically structured (for ITG constraints) sets of hypotheses. Even though the numbers reported in this study are not directly comparable with ours17, it seems that our decoder is not only conceptually much simpler, but also achieves much more optimistic lower-bounds of the oracle BLEU score. The approach described in (Li and Khudanpur, 2009) employs a similar technique, which is to guide a heuristic search in an hypergraph representing possible translation hypotheses with n-gram counts matches, which amounts to decoding with a n-gram model trained on the sole reference translation. Additional tricks are presented in this article to speed-up decoding. Computing oracle BLEU scores is also the subject of (Zens and Ney, 2005; Leusch et al., 2008), yet with a different emphasis. These studies are concerned with finding the best hypotheses in a word graph or in a consensus network, a problem that has various implications for multi-pass</context>
</contexts>
<marker>Li, Khudanpur, 2009</marker>
<rawString>Zhifei Li and Sanjeev Khudanpur. 2009. Efficient extraction of oracle-best translations from hypergraphs. In Proc. of NAACL, pages 9–12, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
<author>Ben Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>761--768</pages>
<location>Sydney, Australia.</location>
<marker>Liang, Bouchard-Cˆot´e, Klein, Taskar, 2006</marker>
<rawString>Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and Ben Taskar. 2006. An end-to-end discriminative approach to machine translation. In Proc. of ACL, pages 761–768, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Translation as weighted deduction.</title>
<date>2009</date>
<booktitle>In Proc. of EACL,</booktitle>
<pages>532--540</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="1433" citStr="Lopez, 2009" startWordPosition="220" endWordPosition="221">compute the oracle BLEU-4 score, that is the best score that a system based on this PT can achieve on a reference corpus. By casting the computation of the oracle BLEU-1 as an Integer Linear Programming (ILP) problem, we show that it is possible to efficiently compute accurate lower-bounds of this score, and report measures performed on several standard benchmarks. Various other applications of these oracle decoding techniques are also reported and discussed. 1 Phrase-Based Machine Translation 1.1 Principle A Phrase-Based Translation System (PBTS) consists of a ruleset and a scoring function (Lopez, 2009). The ruleset, represented in the phrase table, is a set of phrase1pairs {(f, e)}, each pair expressing that the source phrase f can be rewritten (translated) into a target phrase e. Translation hypotheses are generated by iteratively rewriting portions of the source sentence as prescribed by the ruleset, until each source word has been consumed by exactly one rule. The order of target words in an hypothesis is uniquely determined by the order in which the rewrite operation are performed. The search space of the translation model corresponds to the set of all possible sequences of 1Following t</context>
<context position="21136" citStr="Lopez, 2009" startWordPosition="3494" endWordPosition="3495">o phrase-based systems or to evaluate the impact of different parameter settings. This flexibility mainly stems from the possibility offered by our framework to express arbitrary constraints over variables. In this section, we illustrate these possibilities by describing how reordering constraints can easily be considered. As a first example, the Moses decoder uses a distortion limit to constrain the set of possible reorderings. This constraint “enforces (...) that the last word of a phrase chosen for translation cannot be more than d9 words from the leftmost untranslated word in the source” (Lopez, 2009) and is expressed as: ∀aijkl, ai0j0k0l0 s.t. k &gt; k0, aijkl · ai0j0k0l0 · |j − i0 + 1 |≤ d, The maximum distortion limit strategy (Lopez, 2009) is also easily expressed and take the following form (assuming this constraint is parameterized by d): ∀l &lt; m − 1, ai,j,k,l·ai0,j0,l+1,l0 · |i0 − j − 1 |&lt; d Implementing the “local” or MJ-d (Kumar and Byrne, 2005) reordering strategy is also straightforward, and implies using the following constraints: Xai0,j0,k0,l0 − k0≤k Similarly, It is possible to simulate decoding under the so-called IBM(d) reordering constraints10 by considering the following cons</context>
</contexts>
<marker>Lopez, 2009</marker>
<rawString>Adam Lopez. 2009. Translation as weighted deduction. In Proc. of EACL, pages 532–540, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Comput. Linguist.,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="2480" citStr="Och and Ney, 2003" startWordPosition="395" endWordPosition="398">mined by the order in which the rewrite operation are performed. The search space of the translation model corresponds to the set of all possible sequences of 1Following the usage in statistical machine translation literature, we use “phrase” to denote a subsequence of consecutive words. rules applications. The scoring function aims to rank all possible translation hypotheses in such a way that the best one has the highest score. A PBTS is learned from a parallel corpus in two independent steps. In a first step, the corpus is aligned at the word level, by using alignment tools such as Giza++ (Och and Ney, 2003) and some symmetrisation heuristics; phrases are then extracted by other heuristics (Koehn et al., 2003) and assigned numerical weights. In the second step, the parameters of the scoring function are estimated, typically through Minimum Error Rate training (Och, 2003). Translating a sentence amounts to finding the best scoring translation hypothesis in the search space. Because of the combinatorial nature of this problem, translation has to rely on heuristic search techniques such as greedy hill-climbing (Germann, 2003) or variants of best-first search like multi-stack decoding (Koehn, 2004). </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Comput. Linguist., 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="2748" citStr="Och, 2003" startWordPosition="438" endWordPosition="439">utive words. rules applications. The scoring function aims to rank all possible translation hypotheses in such a way that the best one has the highest score. A PBTS is learned from a parallel corpus in two independent steps. In a first step, the corpus is aligned at the word level, by using alignment tools such as Giza++ (Och and Ney, 2003) and some symmetrisation heuristics; phrases are then extracted by other heuristics (Koehn et al., 2003) and assigned numerical weights. In the second step, the parameters of the scoring function are estimated, typically through Minimum Error Rate training (Och, 2003). Translating a sentence amounts to finding the best scoring translation hypothesis in the search space. Because of the combinatorial nature of this problem, translation has to rely on heuristic search techniques such as greedy hill-climbing (Germann, 2003) or variants of best-first search like multi-stack decoding (Koehn, 2004). Moreover, to reduce the overall complexity of decoding, the search space is typically pruned using simple heuristics. For instance, the state-of-the-art phrase-based decoder Moses (Koehn et al., 2007) considers only a restricted number of translations for each source </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of ACL, pages 160–167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-jing Zhu</author>
</authors>
<title>Bleu: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<tech>Technical report,</tech>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="6559" citStr="Papineni et al., 2002" startWordPosition="1050" endWordPosition="1053">ifficult to obtain: most evaluations are limited to the computation of global scores and usually do not imply any kind of failure analysis. 1.3 Contribution and organization To systematically assess the impact of the multiple heuristic decisions made during training and decoding, we propose, following (Dreyer et al., 2007; Auli et al., 2009), to work out oracle scores, that is to evaluate the best achievable performances of a PBTS. We aim at both studying the expressive power of PBTS and at providing tools for identifying and quantifying causes of failure. Under standard metrics such as BLEU (Papineni et al., 2002), oracle scores are difficult (if not impossible) to compute, but, by casting the computation of the oracle unigram recall and precision as an Integer Linear Programming (ILP) problem, we show that it is possible to efficiently compute accurate lower-bounds of the oracle BLEU-4 scores and report measurements performed on several standard benchmarks. The main contributions of this paper are twofold. We first introduce an ILP program able to efficiently find the best hypothesis a PBTS can achieve. This program can be easily extended to test various improvements to 4We omit here optimization erro</context>
<context position="10395" citStr="Papineni et al., 2002" startWordPosition="1657" endWordPosition="1660">n and 5The oracle decoding problem can be extended to the case of multiple references. For the sake of simplicity, we only describe the case of a single reference. 934 Koehn, 2007) or combining machine translation systems in a multi-source setting (Li and Khudanpur, 2009). We have also used oracle decoding to identify erroneous or difficult to translate references (Section 3.3). Evaluation Measure To fully define the oracle decoding problem, a measure of the similarity between a translation hypothesis and its reference translation has to be chosen. The most obvious choice is the BLEU-4 score (Papineni et al., 2002) used in most machine translation evaluations. However, using this metric in the oracle decoding problem raises several issues. First, BLEU-4 is a metric defined at the corpus level and is hard to interpret at the sentence level. More importantly, BLEU-4 is not decomposable6: as it relies on 4-grams statistics, the contribution of each phrase pair to the global score depends on the translation of the previous and following phrases and can not be evaluated in isolation. Because of its nondecomposability, maximizing BLEU-4 is hard; in particular, the phrase-level decomposability of the evaluatio</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-jing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. Technical report, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>Integer linear programming inference for conditional random fields.</title>
<date>2005</date>
<booktitle>In Proc. of ICML,</booktitle>
<pages>737--744</pages>
<location>Bonn, Germany.</location>
<contexts>
<context position="22452" citStr="Roth and Yih, 2005" startWordPosition="3712" endWordPosition="3715">on is done, phrase by phrase, from the beginning of the sentence until the end and only one of the first d untranslated phrase can be selected for translation. In these constraints, the first factor corresponds to the rightmost translated word of the source and the second one to the number of translated source words. The constraints simply enforce that, at each step of the decoding, there are no more than d source words that were skipped. Note that the constraints introduced above are not all linear in the problem variables; however they can easily be linearized using standard ILP techniques (Roth and Yih, 2005). 3 Oracle Decoding for Failure Analysis 3.1 Experimental Setting We propose to use our oracle decoder to study the ability of a PBTS to translate from English to French and from German to English. These two languages pairs present different challenges: English to French translation is considered a relatively easy pair, notwithstanding the difficulties of generating the right inflection marks in French. Translating from German into English is more difficult, notably due to the productivity of inflectional and compounding processes in German, and also to significant differences in word ordering</context>
</contexts>
<marker>Roth, Yih, 2005</marker>
<rawString>D. Roth and W. Yih. 2005. Integer linear programming inference for conditional random fields. In Proc. of ICML, pages 737–744, Bonn, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicolas Stroppa</author>
<author>Antal van den Bosch</author>
<author>Andy Way</author>
</authors>
<title>Exploiting source similarity for smt using context-informed features.</title>
<date>2007</date>
<booktitle>Proc. of TMI,</booktitle>
<pages>231--240</pages>
<editor>In Andy Way and Barbara Gawronska, editors,</editor>
<location>Sk¨ovde, Sweden.</location>
<marker>Stroppa, van den Bosch, Way, 2007</marker>
<rawString>Nicolas Stroppa, Antal van den Bosch, and Andy Way. 2007. Exploiting source similarity for smt using context-informed features. In Andy Way and Barbara Gawronska, editors, Proc. of TMI, pages 231–240, Sk¨ovde, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Tong Zhang</author>
</authors>
<title>A discriminative global training algorithm for statistical mt.</title>
<date>2006</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>721--728</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="9748" citStr="Tillmann and Zhang, 2006" startWordPosition="1551" endWordPosition="1555">ems to generate good candidate translations, irrespective of their ability to score them properly. We believe that studying this problem is interesting for various reasons. First, as described in Section 3.4, comparing the best hypothesis a system could have generated and the hypothesis it actually generates allows us to carry on both quantitative and qualitative failure analysis. The oracle decoding problem can also be used to assess the expressive power of phrase-based systems (Auli et al., 2009). Other applications include computing acceptable pseudo-references for discriminative training (Tillmann and Zhang, 2006; Liang et al., 2006; Arun and 5The oracle decoding problem can be extended to the case of multiple references. For the sake of simplicity, we only describe the case of a single reference. 934 Koehn, 2007) or combining machine translation systems in a multi-source setting (Li and Khudanpur, 2009). We have also used oracle decoding to identify erroneous or difficult to translate references (Section 3.3). Evaluation Measure To fully define the oracle decoding problem, a measure of the similarity between a translation hypothesis and its reference translation has to be chosen. The most obvious cho</context>
<context position="40925" citStr="Tillmann and Zhang, 2006" startWordPosition="6715" endWordPosition="6718">et al., 2008) study the learning capabilities of Moses by extensively analyzing learning curves representing the translation performances as a function of the number of examples, and by corrupting the model parameters. Even though their focus is more on assessing the scoring function, they reach conclusions similar to ours: the current bottleneck of translation performances is not the representation power of the PBTS but rather in their scoring functions. Oracle decoding is useful to compute reachable pseudo-references in the context of discriminative training. This is the main motivation of (Tillmann and Zhang, 2006), where the authors compute high BLEU hypotheses by running a conventional decoder so as to maximize a per-sentence approximation of BLEU-4, under a simple (local) reordering model. Oracle decoding has also been used to assess the limitations induced by various reordering constraints in (Dreyer et al., 2007). To this end, the authors propose to use a beam-search based oracle decoder, which computes lower bounds of the best achievable BLEU-4 using dynamic programming techniques over finite-state (for so-called local and IBM constraints) or hierarchically structured (for ITG constraints) sets of</context>
</contexts>
<marker>Tillmann, Zhang, 2006</marker>
<rawString>Christoph Tillmann and Tong Zhang. 2006. A discriminative global training algorithm for statistical mt. In Proc. of ACL, pages 721–728, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Turchi</author>
<author>Tijl De Bie</author>
<author>Nello Cristianini</author>
</authors>
<title>Learning performance of a machine translation system: a statistical and computational analysis.</title>
<date>2008</date>
<booktitle>In Proc. of WMT,</booktitle>
<pages>35--43</pages>
<location>Columbus, Ohio.</location>
<marker>Turchi, De Bie, Cristianini, 2008</marker>
<rawString>Marco Turchi, Tijl De Bie, and Nello Cristianini. 2008. Learning performance of a machine translation system: a statistical and computational analysis. In Proc. of WMT, pages 35–43, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Word graphs for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL Workshop on Building and Using Parallel Texts,</booktitle>
<pages>191--198</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="42206" citStr="Zens and Ney, 2005" startWordPosition="6914" endWordPosition="6917"> not directly comparable with ours17, it seems that our decoder is not only conceptually much simpler, but also achieves much more optimistic lower-bounds of the oracle BLEU score. The approach described in (Li and Khudanpur, 2009) employs a similar technique, which is to guide a heuristic search in an hypergraph representing possible translation hypotheses with n-gram counts matches, which amounts to decoding with a n-gram model trained on the sole reference translation. Additional tricks are presented in this article to speed-up decoding. Computing oracle BLEU scores is also the subject of (Zens and Ney, 2005; Leusch et al., 2008), yet with a different emphasis. These studies are concerned with finding the best hypotheses in a word graph or in a consensus network, a problem that has various implications for multi-pass decoding and/or system combination techniques. The former reference describes an exponential approximate algorithm, while the latter proves the NPcompleteness of this problem and discuss various heuristic approaches. Our problem is somewhat more complex and using their techniques would require us to built word graphs containing all the translations induced by arbitrary segmentations </context>
</contexts>
<marker>Zens, Ney, 2005</marker>
<rawString>Richard Zens and Hermann Ney. 2005. Word graphs for statistical machine translation. In Proc. of the ACL Workshop on Building and Using Parallel Texts, pages 191–198, Ann Arbor, Michigan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>