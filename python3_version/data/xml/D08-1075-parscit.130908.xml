<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002655">
<title confidence="0.990282">
Learning the Scope of Negation in Biomedical Texts
</title>
<author confidence="0.989573">
Roser Morante†, Anthony Liekens$, Walter Daelemans†
</author>
<affiliation confidence="0.979128">
CNTS - Language Technology Group†, Applied Molecular Genomics Group$
University of Antwerp
</affiliation>
<address confidence="0.968604">
Prinsstraat 13, B-2000 Antwerpen, Belgium
</address>
<email confidence="0.998">
{Roser.Morante,Anthony.Liekens,Walter.Daelemans}@ua.ac.be
</email>
<sectionHeader confidence="0.995796" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9941755625">
In this paper we present a machine learning
system that finds the scope of negation in
biomedical texts. The system consists of two
memory-based engines, one that decides if the
tokens in a sentence are negation signals, and
another that finds the full scope of these nega-
tion signals. Our approach to negation detec-
tion differs in two main aspects from existing
research on negation. First, we focus on find-
ing the scope of negation signals, instead of
determining whether a term is negated or not.
Second, we apply supervised machine learn-
ing techniques, whereas most existing systems
apply rule-based algorithms. As far as we
know, this way of approaching the negation
scope finding task is novel.
</bodyText>
<sectionHeader confidence="0.998999" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999720918367347">
In this paper we present a machine learning system
that finds the scope of negation in biomedical texts.
The system consists of two classifiers, one that de-
cides if the tokens in a sentence are negation sig-
nals (i.e., words indicating negation), and another
that finds the full scope of these negation signals.
Finding the scope of a negation signal means deter-
mining at sentence level which words in the sentence
are affected by the negation. Our approach differs in
two main aspects from existing research. First, we
focus on finding the scope of negation signals, in-
stead of determining whether a term is negated or
not. Second, we apply supervised machine learn-
ing techniques, whereas most existing systems apply
rule-based algorithms.
Predicting the scope of negation is important in
information extraction from text for obvious rea-
sons; instead of simply flagging the sentences con-
taining negation as not suited for extraction (which
is currently the best that can be done), correct se-
mantic relations can be extracted when the scope of
negation is known, providing a better recall.
Not being able to recognize negation can also
hinder automated indexing systems (Mutalik et al.,
2001; Rokach et al., 2008). As Mutalik et al. (2001)
put it, “to increase the utility of concept indexing of
medical documents, it is necessary to record whether
the concept has been negated or not”. They highlight
the need to detect negations in examples like “no ev-
idence of fracture”, so that an information retrieval
system does not return irrelevant reports.
Szarvas et al. (2008) report that 13.45% of the
sentences in the abstracts section of the BioScope
corpus and 13.76% of the sentences in the full papers
section contain negations. A system that does not
deal with negation would treat these cases as false
positives.
The goals of this research are to model the scope
finding task as a classification task similar to the se-
mantic role labeling task, and to test the performance
of a memory–based system that finds the scope of
negation signals. Memory-based language process-
ing (Daelemans and van den Bosch, 2005) is based
on the idea that NLP problems can be solved by
reuse of solved examples of the problem in mem-
ory, applying similarity-based reasoning on these
examples in order to solve new problems. As lan-
guage processing tasks typically involve many sub-
regularities and (pockets of) exceptions, it has been
</bodyText>
<page confidence="0.976577">
715
</page>
<note confidence="0.962067">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 715–724,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999764823529412">
argued that lazy learning is at an advantage in solv-
ing these highly disjunctive learning problems com-
pared to eager learning, as the latter eliminates not
only noise but also potentially useful exceptions
(Daelemans et al., 1999). Memory-based algorithms
have been successfully applied in language process-
ing to a wide range of linguistic tasks, from phonol-
ogy to semantic analysis, such as semantic role la-
beling (Morante et al., 2008).
The paper is organised as follows. In Section 2,
we summarise related work. In Section 3, we de-
scribe the corpus with which the system has been
trained. In Section 4, we introduce the task to be
performed by the system, which is described in Sec-
tion 5. The results are presented and discussed in
Section 6. Finally, Section 7 puts forward some con-
clusions.
</bodyText>
<sectionHeader confidence="0.99966" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999903868421053">
Negation has been a neglected area in open-domain
natural language processing. Most research has been
performed in the biomedical domain and has fo-
cused on detecting if a medical term is negated or
not, whereas in this paper we focus on detecting the
full scope of negation signals.
Chapman et al. (2001) developed NegEx, a reg-
ular expression based algorithm for determining
whether a finding or disease mentioned within nar-
rative medical reports is present or absent. The re-
ported results are 94.51 precision and 77.84 recall.
Mutalik et al. (2001) developed Negfinder, a rule-
based system that recognises negated patterns in
medical documents. It consists of two tools: a lexi-
cal scanner called lever that uses regular expressions
to generate a finite state machine, and a parser. The
reported results are 95.70 recall and 91.80 precision.
Sanchez-Graillet and Poesio (2007) present an
analysis of negated interactions in biological texts
and a heuristics-based system that extracts such in-
formation. They treat all types of negation: (i) Af-
fixal negation, which is expressed by an affix. (ii)
Noun phrase or emphatic negation, expressed syn-
tactically by using a negative determiner (e.g. no,
nothing). (iii) Inherent negation, expressed by words
with an inherently negative meaning (e.g. absent).
(iv) Negation with explicit negative particles (e.g.
no, not). The texts are 50 journal articles. The pre-
liminary results reported range from 54.32 F-score
to 76.68, depending on the method applied.
Elkin et al. (2005) describe a rule-based system
that assigns to concepts a level of certainty as part of
the generation of a dyadic parse tree in two phases:
First a preprocessor breaks each sentence into text
and operators. Then, a rule based system is used to
decide if a concept has been positively, negatively,
or uncertainly asserted. The system achieves 97.20
recall and 98.80 precision.
The systems mentioned above are essentially
based on lexical information. Huang and
Lowe (2007) propose a classification scheme of
negations based on syntactic categories and patterns
in order to locate negated concepts, regardless of
their distance from the negation signal. Their hybrid
system that combines regular expression matching
with grammatical parsing achieves 92.60 recall and
99.80 precision.
Additionally, Boytcheva et al. (2005) incorporate
the treatment of negation in a system, MEHR, that
extracts from electronic health records all the in-
formation required to generate automatically patient
chronicles. According to the authors “the nega-
tion treatment module inserts markers in the text for
negated phrases and determines scope of negation by
using negation rules”. However, in the paper there
is no description of the rules that are used and it is
not explained how the results presented for negation
recognition (57% of negations correctly recognised)
are evaluated.
The above-mentioned research applies rule-based
algorithms to negation finding. Machine learning
techniques have been used in some cases. Averbuch
et al. (2004) developed an algorithm that uses infor-
mation gain to learn negative context patterns.
Golding and Chapman (2003) experiment with
machine learning techniques to distinguish whether
a medical observation is negated by the word not.
Their corpus contains 207 selected sentences from
hospital reports, in which a negation appears. They
use Naive Bayes and Decision Trees and achieve a
maximum of 90 F-score. According to the authors,
their main finding is that “when negation of a UMLS
term is triggered with the negation phrase not, if the
term is preceded by the then do not negate”.
Goryachev et al. (2006) compare the perfor-
mance of four different methods of negation de-
</bodyText>
<page confidence="0.997078">
716
</page>
<bodyText confidence="0.9999783">
tection, two regular expression-based methods and
two classification-based methods trained on 1745
discharge reports. They show that the regular
expression-based methods have better agreement
with humans and better accuracy than the classifica-
tion methods. Like in most of the mentioned work,
the task consists in determining if a medical term is
negated.
Rokach et al. (2008) present a new pattern-based
algorithm for identifying context in free-text med-
ical narratives.The originality of the algorithm lies
in that it automatically learns patterns similar to the
manually written patterns for negation detection.
Apart from work on determining whether a term is
negated or not, we are not aware of research that has
focused on learning the full scope of negation sig-
nals inside or outside biomedical natural language
processing. The research presented in this paper pro-
vides a new approach to the treatment of negation
scope in natural language processing.
</bodyText>
<sectionHeader confidence="0.99435" genericHeader="method">
3 Corpus
</sectionHeader>
<bodyText confidence="0.998823166666667">
The corpus used is a part of the BioScope cor-
pus (Szarvas et al., 2008)1, a freely available re-
source that consists of medical and biological texts.
Every sentence is annotated with information about
negation and speculation that indicates the bound-
aries of the scope and the keywords, as shown in (1).
</bodyText>
<listItem confidence="0.96118875">
(1) PMA treatment, and &lt;xcope id=“X1.4.1”&gt;&lt;cue
type=“negation” ref=”X1.4.1”&gt;not&lt;cue&gt; retinoic
acid treatment of the U937 cells&lt;/xcope&gt; acts in
inducing NF-KB expression in the nuclei.
</listItem>
<bodyText confidence="0.999692181818182">
A first characteristic of the annotation of scope in
the BioScope corpus is that all sentences that assert
the non-existence or uncertainty of something are
annotated, in contrast to other corpora where only
sentences of interest in the domain are annotated.
A second characteristic is that the annotation is ex-
tended to the biggest syntactic unit possible so that
scopes have the maximal length. In (2) below, nega-
tion signal no scopes over primary impairment of
glucocorticoid metabolism instead of scoping only
over primary.
</bodyText>
<listItem confidence="0.5043895">
(2) There is [no] primary impairment of glucocorticoid
metabolism in the asthmatics.
</listItem>
<bodyText confidence="0.984590666666667">
The part used in our experiments are the biologi-
cal paper abstracts from the GENIA corpus (Collier
et al., 1999). This part consists of 11,872 sentences
in 1,273 abstracts. We automatically discarded five
sentences due to annotation errors. The total num-
ber of words used is 313,222, 1,739 of which are
negation signals that belong to the different types
described in (Sanchez-Graillet and Poesio, 2007).
We processed the texts with the GENIA tag-
ger (Tsuruoka and Tsujii, 2005; Tsuruoka et al.,
2005), a bidirectional inference based tagger that an-
alyzes English sentences and outputs the base forms,
part-of-speech tags, chunk tags, and named entity
tags in a tab-separated format2. Additionally, we
converted the annotation about scope of negation
into a token-per-token representation.
Table 1 shows an example sentence of the corpus
that results from converting and processing the Bio-
Scope representation. Following the standard for-
mat of the CoNLL Shared Task 2006 (Buchholz and
Marsi, 2006), sentences are separated by a blank line
and fields are separated by a single tab character. A
sentence consists of tokens, each one starting on a
new line. A token consists of the following 10 fields:
</bodyText>
<listItem confidence="0.9989345625">
1. ABSTRACT ID: number of the GENIA ab-
stract.
2. SENTENCE ID: sentence counter starting at 1
for each new abstract.
3. TOKEN ID: token counter, starting at 1 for
each new sentence.
4. FORM: word form or punctuation symbol.
5. LEMMA: lemma of word form.
6. POS TAG: Penn Treebank part-of-speech tags
described in (Santorini, 1990).
7. CHUNK TAG: IOB (Inside, Outside, Begin)
tags produced by the GENIA tagger that indi-
cate if a token is inside a certain chunk, outside,
or at the beginning.
8. NE TAG: IOB named entity tags produced by
the GENIA tagger that indicate if a token is in-
</listItem>
<footnote confidence="0.983047666666667">
1Web page: www.inf.u-szeged.hu/rgai/bioscope.
2The accuracy of the tagger might be inflated due to the fact
that it was trained on the GENIA corpus.
</footnote>
<page confidence="0.982497">
717
</page>
<table confidence="0.9997305">
ABSTR SNT TOK FORM LEMMA POS CHUNK NE NEG NEG SCOPE
ID ID ID TAG TAG TAG SGN
10415075 07 1 NF-kappa NF-kappa NN B-NP B-protein I-NEG O-NEG
10415075 07 2 B B NN I-NP I-protein I-NEG O-NEG
10415075 07 3 binding binding NN I-NP O I-NEG O-NEG
10415075 07 4 activity activity NN I-NP O I-NEG O-NEG
10415075 07 5 was be VBD B-VP O I-NEG O-NEG
10415075 07 6 absent absent JJ B-ADJP O NEG I-NEG O-NEG
10415075 07 7 in in IN B-PP O I-NEG O-NEG
10415075 07 8 several several JJ B-NP O I-NEG O-NEG
10415075 07 9 SLE SLE NN I-NP O I-NEG O-NEG
10415075 07 10 patients patient NNS I-NP O I-NEG O-NEG
10415075 07 11 who who WP B-NP O I-NEG O-NEG
10415075 07 12 were be VBD B-VP O I-NEG O-NEG
10415075 07 13 not not RB I-VP O NEG I-NEG I-NEG
10415075 07 14 receiving receive VBG I-VP O I-NEG I-NEG
10415075 07 15 any any DT B-NP O I-NEG I-NEG
10415075 07 16 medication medication NN I-NP O I-NEG I-NEG
10415075 07 17 , , , O O I-NEG I-NEG
10415075 07 18 including include VBG B-PP O I-NEG I-NEG
10415075 07 19 corticosteroidscorticosteroid NNS B-NP O I-NEG I-NEG
10415075 07 20 . . . O O O-NEG O-NEG
</table>
<tableCaption confidence="0.999932">
Table 1: Example sentence of the BioScope corpus converted into columns format.
</tableCaption>
<bodyText confidence="0.903568">
side a certain named entity, outside, or at the
beginning.
</bodyText>
<listItem confidence="0.954327785714286">
9. NEG SIGNAL: tokens that are negation signals
are marked as NEG. Negation signals in the
BioScope corpus are not always single words,
like the signal could not. After the tagging pro-
cess the signal cannot becomes also multiword
because the tagger splits it in two words. In
these cases we assign the NEG mark to not.
10. NEG SCOPE: IO tags that indicate if a token
is inside the negation scope (I-NEG), or out-
side (O-NEG). These tags have been obtained
by converting the xml files of BioScope. Each
token can have one or more NEG SCOPE tags,
depending on the number of negation signals in
the sentence.
</listItem>
<sectionHeader confidence="0.944265" genericHeader="method">
4 Task description
</sectionHeader>
<bodyText confidence="0.999896925925926">
We approach the scope finding task as a classifica-
tion task that consists of classifying the tokens of
a sentence as being a negation signal or not, and
as being inside or outside the scope of the negation
signal(s). This happens as many times as there are
negation signals in the sentence. Our conception of
the task is inspired by Ramshaw and Marcus’ rep-
resentation of text chunking as a tagging problem
(Ramshaw and Marcus, 1995) .
The information that can be used to train the sys-
tem appears in columns 1 to 8 of Table 1. The infor-
mation to be predicted by the system is contained in
columns 9 and 10.
As far as we know, approaching the negation
scope finding task as a token per token classifica-
tion task is novel, whereas at the same time it con-
forms to the well established standards of the re-
cent CoNLL Shared Tasks3 on dependency parsing
(Buchholz and Marsi, 2006; Nivre et al., 2007) and
semantic role labeling (Surdeanu et al., 2008). By
setting up the task in this way we show that the nega-
tion scope finding task can be modelled in a way
similar to semantic role labeling, and by conform-
ing to existing standards we show that learning the
scope of negation can be integrated in a joint learn-
ing task with dependency parsing and semantic role
labeling.
</bodyText>
<footnote confidence="0.7779155">
3Web page of CoNLL:
http://www.ifarm.nl/signll/conll/.
</footnote>
<page confidence="0.992382">
718
</page>
<sectionHeader confidence="0.959023" genericHeader="method">
5 System description
</sectionHeader>
<bodyText confidence="0.999983818181818">
In order to solve the task, we apply supervised ma-
chine learning techniques. We build a memory-
based scope finder, that tackles the task in two
phases. In the first phase a classifier predicts if a to-
ken is a negation signal, and in the second phase an-
other classifier predicts if a token is inside the scope
of each of the negation signals. Additionally, the
output of the second classifier is postprocessed with
an algorithm that converts non-consecutive blocks of
scope into consecutive, as explained in Section 5.3.
As for the first and second phases, we use a
memory–based classifier as implemented in TiMBL
(version 6.1.2) (Daelemans et al., 2007), a super-
vised inductive algorithm for learning classification
tasks based on the k-nearest neighbor classification
rule (Cover and Hart, 1967). Similarity is defined by
computing (weighted) overlap of the feature values
of a test instance and training instances. The metric
combines a per-feature value distance metric (Cost
and Salzberg, 1993) with gain ratio (Quinlan, 1993)
based global feature weights that account for relative
differences in discriminative power of the features.
</bodyText>
<subsectionHeader confidence="0.984087">
5.1 Negation signal finding
</subsectionHeader>
<bodyText confidence="0.99448925">
In this phase, a classifier predicts whether a token is
a negation signal or not. The memory-based classi-
fier was parameterised by using overlap as the sim-
ilarity metric, gain ratio for feature weighting, and
using 7 k-nearest neighbors. All neighbors have
equal weight when voting for a class. The instances
represent all tokens in the corpus and they have the
following features:
</bodyText>
<listItem confidence="0.98939475">
• Of the token: Form, lemma, part of speech, and
chunk IOB tag.
• Of the token context: Form, POS, and IOB tag
of the three previous and three next tokens.
</listItem>
<subsectionHeader confidence="0.996663">
5.2 Scope finding
</subsectionHeader>
<bodyText confidence="0.989676333333333">
In the first step of this phase, a classifier predicts
whether a token is in the scope of each of the nega-
tion signals of a sentence. A pair of a negation signal
and a token from the sentence represents an instance.
This means that all tokens in a sentence are paired
with all negation signals that occur in the sentence.
For example, token NF-kappa in Table 1 will be rep-
resented in two instances as shown in (3). An in-
stance represents the pair [NF–KAPPA, absent] and
another one represents the pair [NF–KAPPA, not].
(3) NF-kappa absent [features] I-NEG
NF-kappa not [features] O-NEG
Negation signals are those that have been classi-
fied as such in the previous phase. Only sentences
that have negation signals are selected for this phase.
The memory–based algorithm was parameterised
in this case by using overlap as the similarity metric,
gain ratio for feature weighting, using 7 k-nearest
neighbors, and weighting the class vote of neighbors
as a function of their inverse linear distance.
The features of the scope finding classifier are:
</bodyText>
<listItem confidence="0.9951905">
• Of the negation signal: Form, POS, chunk IOB
tag, type of chunk (NP, VP, ...), and form, POS,
chunk IOB tag, type of chunk, and named en-
tity of the 3 previous and 3 next tokens.
• Of the paired token: form, POS, chunk IOB
tag, type of chunk, named entity, and form,
POS, chunk IOB tag, type of chunk, and named
entity type of the 3 previous and 3 next tokens.
• Of the tokens between the negation signal and
the token in focus: Chain of POS types, dis-
tance in number of tokens, and chain of chunk
IOB tags.
• Others: A binary feature indicating whether the
token and the negation signal are in the same
chunk, and location of the token relative to the
negation signal (pre, post, same).
</listItem>
<subsectionHeader confidence="0.998782">
5.3 Post-processing
</subsectionHeader>
<bodyText confidence="0.997494363636363">
Negation signals in the BioScope corpus always
have one consecutive block of scope tokens, includ-
ing the signal token itself. However, the scope find-
ing classifier can make predictions that result in non-
consecutive blocks of scope tokens: we observed
that 54% of scope blocks predicted by the sys-
tem given gold standard negation signals are non–
consecutive. This is why in the second step of the
scope finding phase, we apply a post-processing al-
gorithm in order to increase the number of fully cor-
rect scopes. A scope is fully correct if all tokens in a
</bodyText>
<page confidence="0.992774">
719
</page>
<bodyText confidence="0.992610444444444">
sentence have been assigned their correct class label
for a given negation signal. Post-processing ensures
that the resulting scope is one consecutive block of
tokens.
In the BioScope corpus negation signals are inside
of their scope. The post-processing algorithm that
we apply first checks if the negation signal is in its
scope. If the signal is out, the algorithm overwrites
the predicted scope in order to include the signal in
its scope.
Given the position of the signal in the sentence,
the algorithm locates the starting and ending tokens
of the consecutive block of predicted scope tokens
that surrounds the signal. Other blocks of predicted
scope tokens may have been predicted outside of this
block, but they are separated from the current block,
which contains the signal, by tokens that have been
predicted not to be in the scope of the negation, as in
</bodyText>
<figureCaption confidence="0.985691666666667">
Figure 1.
Figure 1: Non-consecutive blocks of scope tokens. For
a signal, two blocks of k = 6 and m = 3 tokens are
predicted to be the scope of the signal token, but they are
separated by l = 2 tokens that are predicted to be out of
scope.
</figureCaption>
<bodyText confidence="0.999112488888889">
The post-processing algorithm decides whether
the detached blocks should be connected as one con-
secutive block of scope tokens, or whether the de-
tached block of scope tokens should be discarded
from the scope. Dependent on this decision, ei-
ther the classification of the separated blocks, or the
separating non-scope tokens are considered noisy,
and their classification is updated to produce one
consecutive block of scope tokens for each signal.
This check is performed iteratively for all detached
blocks of scope tokens.
As in Figure 1, consider a sentence where the
negation signal is in one block K of predicted scope
of length k tokens and another block M of m con-
secutive tokens that is predicted as scope but is sep-
arated from the latter scope block by l out-of-scope
tokens.
If non-consecutive blocks are near each other, i.e.,
if l is sufficiently small in comparison with k and
m, then the intermediate tokens that have been pre-
dicted out of scope could be considered as noise and
converted into scope tokens. In contrast, if there are
too many intermediate tokens that separate the two
blocks of scope tokens, then the additional block of
scope is probably wrongly classified.
Following this logic, if l &lt; α(k + m), with a
specifically chosen α, the intermediate out-of-scope
tokens are re-classified as scope tokens, and the
separated blocks are connected to form one bigger
block containing the negation signal. Otherwise,
the loose block of scope is re-classified to be out of
scope. When the main scope is extended, and more
blocks are found that are separated from the main
scope block, the algorithm reiterates this procedure
until one consecutive block of scope tokens has been
found.
Our implementation first looks for separated
blocks from right to left, and then from left to right.
Dependent on whether blocks need to be added be-
fore or after the main scope block, we have observed
in preliminary tests that α = 0.2 for extending the
main scope block from right to left, and α = 0.3 for
extending the block from left to right into the sen-
tence provide the best results. Algorithm 1 details
the above procedure in pseudo code.
</bodyText>
<equation confidence="0.7556334">
Algorithm 1 Post-processing
K ← scope block that contains signal
while M ← nearest separated scope block do
L ← non-scope block between K and M
if |L |&lt; α(|K |+ |M|) then
include L in scope
else
exclude M from scope
end if
K ← scope block that contains signal
</equation>
<bodyText confidence="0.647434">
end while
</bodyText>
<sectionHeader confidence="0.99945" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.9998354">
The results have been obtained by performing 10-
fold cross validation experiments. The evaluation
is made using the precision and recall measures
(Van Rijsbergen, 1979), and their harmonic mean,
F-Measure. We calculate micro F1.
</bodyText>
<equation confidence="0.7345555">
signal
k
l
m
</equation>
<page confidence="0.97047">
720
</page>
<bodyText confidence="0.998273933333333">
In the negation finding task, a negation token is
correctly classified if it has been assigned a NEG
class. In the scope finding task, a token is correctly
classified if all the IO tag(s) that it has been assigned
are correct. This means that when there is more than
one negation signal in the sentence, the token has to
be correctly assigned an IO tag for as many negation
signals as there are. For example, token NF-kappa
from Table 1 reproduced in (4) will not be correct
if it is assigned classes I-NEG I-NEG or O-NEG I-
NEG.
(4) 10415075 07 1 NF-kappa NF-kappa NN B-NP
B-protein I-NEG O-NEG
Additionally, we evaluated the percentage of fully
correct scopes (PCS).
</bodyText>
<subsectionHeader confidence="0.995282">
6.1 Negation signal finding
</subsectionHeader>
<bodyText confidence="0.999982">
We calculate two baselines for negation signal find-
ing. Baseline 1 (B1) is calculated by assigning the
NEG class to all the tokens that had no or not as
lemma, which account for 72.80% of the negation
signals. The F1 of the baseline is 80.66. Baseline
2 (B2) is calculated by assigning the NEG class to
all the tokens that had no, not, lack, neither, unable,
without, fail, absence, or nor as lemma. These lem-
mas account for 85.85 % of the negation signals.
</bodyText>
<table confidence="0.998643">
Baseline Total Prec. Recall F1
B1 1739 90.42 72.80 80.66
B2 1739 89.77 93.38 91.54
</table>
<tableCaption confidence="0.999229">
Table 2: Baselines of the negation finding system.
</tableCaption>
<bodyText confidence="0.9972708">
Table 3 shows the overall results of the negation
signal finding system and the results per negation
signal. With F1 94.40, it outperforms Baseline 2
by 2.86 points. Precision and recall are very simi-
lar. Scores show a clear unbalance between different
negation signals. Those with the lowest frequencies
get lower scores than those with the highest frequen-
cies. Probably, this could be avoided by training the
system with a bigger corpus.
However, a bigger corpus would not help solve all
the errors because some of them are caused by in-
consistency in the annotation. For example, absence
is annotated as a negation signal in 57 cases, whereas
in 22 cases it is not annotated as such, although in all
cases it is used as a negation signal. Example 5 (a)
</bodyText>
<table confidence="0.999864551724138">
Neg signals Total Prec. Recall F1
lack (v) 55 100.00 100.00 100.00
neither (con) 34 100.00 100.00 100.00
lack (n) 33 100.00 100.00 100.00
unable 30 100.00 100.00 100.00
neither (det) 8 100.00 100.00 100.00
no (adv) 5 100.00 100.00 100.00
without 83 100.00 98.79 99.39
nor 44 100.00 100.00 98.89
rather 19 95.00 100.00 97.43
not 1057 96.15 96.97 96.56
no (det) 204 95.63 96.56 96.09
none 7 85.71 85.71 85.71
fail 57 79.36 87.71 83.33
miss 2 66.66 100.00 80.00
absence 57 67.64 80.70 73.60
failure 8 45.54 62.50 52.63
could 6 66.66 33.33 44.44
absent 13 42.85 23.07 30.00
with 6 0.00 0.00 0.00
either 2 0.00 0.00 0.00
instead 2 0.00 0.00 0.00
never 2 0.00 0.00 0.00
impossible 1 0.00 0.00 0.00
lacking 1 0.00 0.00 0.00
loss 1 0.00 0.00 0.00
negative 1 0.00 0.00 0.00
or 1 0.00 0.00 0.00
Overall 1739 94.21 94.59 94.40
</table>
<tableCaption confidence="0.999318">
Table 3: F scores of the negation finding classifier.
</tableCaption>
<bodyText confidence="0.945146">
shows one of the 22 cases of absence that has not
been annotated, and Example 5 (b) shows one of the
57 cases of absence annotated as a negation signal.
Also fail is not annotated as a negation signal in 13
cases where it should.
</bodyText>
<listItem confidence="0.89284975">
(5) (a) Retroviral induction of TIMP-1 not only
resulted in cell survival but also in continued DNA
synthesis for up to 5 d in the absence of serum,
while controls underwent apoptosis.
(b) A significant proportion of transcripts appear to
terminate prematurely in the &lt;xcope id= X654.8.1
&gt;&lt;cue type= negation ref= X654.8.1 &gt; absence
&lt;/cue&gt; of transactivators &lt;/xcope&gt;.
</listItem>
<bodyText confidence="0.998584666666667">
Other negation signals are arbitrarily annotated.
Failure is annotated as a negation signal in 8 cases
where it is followed by a preposition, like in Exam-
ple 6 (a), and it is not annotated as such in 26 cases,
like Example 6 (b), where it is modified by an adjec-
tive.
</bodyText>
<page confidence="0.988903">
721
</page>
<listItem confidence="0.93765">
(6) (a) ... the &lt;xcope id= X970.8.2&gt; &lt;cue type=
</listItem>
<bodyText confidence="0.968218421052632">
negation ref= X970.8.2&gt;failure&lt;/cue&gt; of eTh1
cells to produce IL-4 in response to an antigen
&lt;/xcope&gt; is due, at least partially, to a &lt;xcope id=
X970.8.1&gt; &lt; cue type= negation ref= X970.8.1&gt;
failure&lt;/cue&gt; to induce high-level transcription
of the IL-4 gene by NFAT &lt;/xcope&gt;&lt;/xcope&gt;.
(b) Positive-pressure mechanical ventilation
supports gas exchange in patients with respiratory
failure but is also responsible for significant lung
injury.
The errors in detecting with as a negation signal
are caused by the fact that it is embedded in the ex-
pression with the exception of, which occurs 6 times
in contrast with the 5265 occurrences of with. Could
appears as a negation signal because the tagger does
not assign to it the lemma can, but could, causing
the wrong assignment of the tag NEG to not, instead
of could when the negation cue in BioScope is could
not.
</bodyText>
<subsectionHeader confidence="0.999679">
6.2 Scope finding
</subsectionHeader>
<bodyText confidence="0.9999">
We provide the results of the classifier and the re-
sults of applying the postprocessing algorithm to the
output of the classifier.
Table 4 shows results for two versions of the
scope finding classifier, one based on gold standard
negation signals (GS NEG), and another (PR NEG)
based on negation signals predicted by the classifier
described in the previous section.
</bodyText>
<table confidence="0.999727">
Prec. Recall F1 PCS
GS NEG 86.03 85.53 85.78 39.39
PR NEG 79.83 77.42 78.60 36.31
</table>
<tableCaption confidence="0.985434666666667">
Table 4: Results of the scope finding classifier with gold-
standard (GS NEG) and with predicted negation signals
(PR NEG).
</tableCaption>
<bodyText confidence="0.999380411764706">
The F1 of PR NEG is 7.18 points lower than the
F1 of GS NEG, which is an expected effect due to
the performance of classifier that finds negation sig-
nals. Precision and recall of GS NEG are very bal-
anced, whereas PR NEG has a lower recall than pre-
cision. These measures are the result of a token per
token evaluation, which does not guarantee that the
complete sequence of scope is correct. This is re-
flected in the low percentage of fully correct scopes
of both versions of the classifier.
In Table 5, we present the results of the system af-
ter applying the postprocessing algorithm. The most
remarkable result is the 29.60 and 21.58 error reduc-
tion in the percentage of fully correct scopes of GS
NEG and PR NEG respectively, which shows that
the algorithm is efficient. Also interesting is the in-
crease in F1 of GS NEG and PR NEG.
</bodyText>
<table confidence="0.999832">
Prec. Recall F1 PCS
GS NEG 88.63 88.17 88.40 57.33
PR NEG 80.70 81.29 80.99 50.05
</table>
<tableCaption confidence="0.676852222222222">
Table 5: Results of the system with gold-standard (GS
NEG) and with predicted negation signals (PR NEG) af-
ter applying the postprocessing algorithm.
Table 6 shows detailed results of the system based
on predicted negation signals after applying the
postprocessing algorithm. Classes O-NEG and I-
NEG are among the most frequent and get high
scores. Classes composed only of O-NEG tags are
easier to predict.
</tableCaption>
<table confidence="0.999469214285714">
Scope tags Total Prec. Recall F1
O-NEG 29590 86.78 84.75 85.75
O-NEG O-NEG O-NEG 46 100.00 63.04 77.33
I-NEG 12990 73.41 80.72 76.89
O-NEG O-NEG 2848 84.11 68.43 75.46
I-NEG I-NEG O-NEG 69 62.92 81.15 70.88
I-NEG I-NEG 684 57.30 65.93 61.31
I-NEG O-NEG O-NEG 20 50.00 75.00 60.00
O-NEG I-NEG 791 72.13 50.06 59.10
I-NEGO-NEG 992 45.32 67.94 54.37
O-NEG I-NEG I-NEG 39 100.00 20.51 34.04
I-NEG I-NEG I-NEG 22 26.66 36.36 30.76
O-NEG O-NEG I-NEG 14 0.00 0.00 0.00
Overall 48105 80.70 81.29 80.99
</table>
<tableCaption confidence="0.852630666666667">
Table 6: F scores of the system per scope class after ap-
plying the postprocessing algorithm.
Table 7 shows information about the percentage
</tableCaption>
<bodyText confidence="0.9985598">
of correct scopes per negation signal after applying
the algorithm to PR-NEG. A clear example of an
incorrect prediction is the occurrence of box in the
list. The signal with the highest percentage of PCS
is without, followed by no (determiner), rather and
not, which are above 50%. It would be interesting to
investigate how the syntactic properties of the nega-
tion signals are related to the percentage of correct
scopes, and how does the algorithm perform depend-
ing on the type of signal.
</bodyText>
<page confidence="0.988362">
722
</page>
<table confidence="0.99995747826087">
Neg signals Total Correct PCS
without 82 56 68.29
no (det) 206 133 64.56
rather 20 11 55.00
not 1066 556 52.15
neither (det) 8 4 50.00
none 7 3 42.85
neither (conj) 34 16 47.05
no (adv) 5 2 40.00
fail 63 23 36.50
missing 3 1 33.33
absence 68 22 32.35
lack (v.) 54 17 31.48
absent 7 2 28.57
lack (n.) 33 9 27.27
nor 43 11 25.58
unable 30 8 26.66
failure 11 0 0.00
could 3 0 0.00
negative 1 0 0.00
never 1 0 0.00
box 1 0 0.00
Overall 1746 874 50.05
</table>
<tableCaption confidence="0.991927">
Table 7: Information about Percentage of Correct Scopes
(PCS) per negation signal in PR-NEG.
</tableCaption>
<sectionHeader confidence="0.997878" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999989096153846">
Given the fact that a significant portion of biomed-
ical text is negated, recognising negated instances
is important in NLP applications. In this paper we
have presented a machine learning system that finds
the scope of negation in biomedical texts. The sys-
tem consists of two memory-based classifiers, one
that decides if the tokens in a sentence are negation
signals, and another that finds the full scope of the
negation signals.
The first classifier achieves 94.40 F1, and the sec-
ond 80.99. However, the evaluation in terms of cor-
rect scopes shows the weakness of the system. This
is why a postprocessing algorithm is applied. The
algorithm achieves an error reduction of 21.58, with
50.05 % of fully correct scopes in the system based
on predicted negation signals.
These results suggest that unsupervised machine
learning algorithms are suited for tackling the task,
as it was expected from results obtained in other
natural language processing tasks. However, results
also suggest that there is room for improvement. A
first improvement would consist in predicting the
scope chunk per chunk instead of token per token,
because most negation scope boundaries coincide
with boundaries of chunks.
We have highlighted the fact that our approach
to negation detection focuses on finding the scope
of negation signals, instead of determining whether
a term is negated or not, and on applying super-
vised machine learning techniques. As far as we
know, this approach is novel. Unfortunately, there
are no previous comparable approaches to measure
the quality of our results.
Additionally, we have shown that negation find-
ing can be modelled as a classification task in a way
similar to other linguistic tasks like semantic role la-
beling. In our model, tokens of a sentence are clas-
sified as being a negation signal or not, and as being
inside or outside the scope of the negation signal(s).
This representation would allow to integrate the task
with other semantic tasks and exploring the interac-
tion between different types of knowledge in a joint
learning setting.
Further research is possible in several directions.
In the first place, other machine learning algorithms
could be integrated in the system in order to opti-
mise performance. Secondly, the system should be
tested in different types of biomedical texts, like full
papers or medical reports to check its robustness.
Finally, the postprocessing algorithm could be im-
proved by using more sophisticated sequence classi-
fication techniques (Dietterich, 2002) .
</bodyText>
<sectionHeader confidence="0.997486" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999248">
Our work was made possible through financial sup-
port from the University of Antwerp (GOA project
BIOGRAPH). We are thankful to three anonymous
reviewers for their valuable comments and sugges-
tions.
</bodyText>
<sectionHeader confidence="0.999004" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9994746">
M. Averbuch, T. Karson, B. Ben-Ami, O. Maimon, and
L. Rokach. 2004. Context-sensitive medical informa-
tion retrieval. In Proc. of the 11th World Congress
on Medical Informatics (MEDINFO-2004), pages 1–
8, San Francisco, CA. IOS Press.
</reference>
<page confidence="0.985065">
723
</page>
<reference confidence="0.999790169811321">
S. Boytcheva, A. Strupchanska, E. Paskaleva, and
D. Tcharaktchiev. 2005. Some aspects of negation
processing in electronic health records. In Proc. of
International Workshop Language and Speech Infras-
tructure for Information Access in the Balkan Coun-
tries, pages 1–8, Borovets, Bulgaria.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In Proc. of the X
CoNLL Shared Task, New York. SIGNLL.
W. W. Chapman, W. Bridewell, P. Hanbury, G. F. Cooper,
and B.G. Buchanan. 2001. A simple algorithm for
identifying negated findings and diseases in discharge
summaries. JBiomed Inform, 34:301–310.
N. Collier, H.S. Park, N. Ogata, Y. Tateisi, C. Nobata,
T. Sekimizu, H. Imai, and J. Tsujii. 1999. The GE-
NIA project: corpus-based knowledge acquisition and
information extraction from genome research papers.
In Proceedings of EACL-99.
S. Cost and S. Salzberg. 1993. A weighted nearest neigh-
bour algorithm for learning with symbolic features.
Machine learning, 10:57–78.
T. M. Cover and P. E. Hart. 1967. Nearest neighbor
pattern classification. Institute of Electrical and Elec-
tronics Engineers Transactions on Information The-
ory, 13:21–27.
W. Daelemans and A. van den Bosch. 2005. Memory-
based language processing. Cambridge University
Press, Cambridge, UK.
W. Daelemans, A. Van den Bosch, and J. Zavrel. 1999.
Forgetting exceptions is harmful in language learning.
Machine Learning, Special issue on Natural Language
Learning, 34:11–41.
W. Daelemans, J. Zavrel, K. Van der Sloot, and A. Van
den Bosch. 2007. TiMBL: Tilburg memory based
learner, version 6.1, reference guide. Technical Report
Series 07-07, ILK, Tilburg, The Netherlands.
T. G. Dietterich. 2002. Machine learning for sequential
data: A review. In Lecture Notes in Computer Science
2396, pages 15–30, London. Springer Verlag.
P. L. Elkin, S. H. Brown, B. A. Bauer, C.S. Husser,
W. Carruth, L.R. Bergstrom, and D. L. Wahner-
Roedler. 2005. A controlled trial of automated classi-
fication of negation from clinical notes. BMC Medical
Informatics and Decision Making, 5(13).
l. M. Goldin and W.W. Chapman. 2003. Learning to
detect negation with ‘Not’ in medical texts. In Pro-
ceedings ofACM-SIGIR 2003.
S. Goryachev, M. Sordo, Q.T. Zeng, and L. Ngo. 2006.
Implementation and evaluation of four different meth-
ods of negation detection. Technical report, DSG.
Y. Huang and H.J. Lowe. 2007. A novel hybrid approach
to automated negation detection in clinical radiology
reports. JAm Med Inform Assoc, 14(3):304–311.
R. Morante, W. Daelemans, and V. Van Asch. 2008. A
combined memory-based semantic role labeler of en-
glish. In Proc. of the CoNLL 2008, pages 208–212,
Manchester, UK.
A.G. Mutalik, A. Deshpande, and P.M. Nadkarni. 2001.
Use of general-purpose negation detection to augment
concept indexing of medical documents. a quantita-
tive study using the UMLS. JAm Med Inform Assoc,
8(6):598–609.
J. Nivre, J. Hall, S. K¨ubler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL-2007
shared task on dependency parsing. In Proc. of the
CoNLL Shared Task Session ofEMNLP-CoNLL 2007,
pages 915–932, Prague.
J.R. Quinlan. 1993. C4.5: Programs for Machine Learn-
ing. Morgan Kaufmann, San Mateo, CA.
L. Ramshaw and M. Marcus. 1995. Text chunking using
transformation-based learning. In Proc. ofACL Third
Workshop on Very Large Corpora, pages 82–94, Cam-
bridge, MA. ACL.
L. Rokach, R.Romano, and O. Maimon. 2008. Negation
recognition in medical narrative reports. Information
Retrieval Online.
O. Sanchez-Graillet and M. Poesio. 2007. Negation of
protein-protein interactions: analysis and extraction.
Bioinformatics, 23(13):424–432.
B. Santorini. 1990. Part-of-speech tagging guidelines
for the penn treebank project. Technical report MS-
CIS-90-47, Department of Computer and Information
Science, University of Pennsylvania.
M. Surdeanu, R. Johansson, A. Meyers, Ll. M`arquez,
and J. Nivre. 2008. The CoNLL-2008 shared task on
joint parsing of syntactic and semantic dependencies.
In Proc. of CoNLL 2008, pages 159–177, Manchester,
UK.
G. Szarvas, V. Vincze, R. Farkas, and J. Csirik. 2008.
The BioScope corpus: annotation for negation, uncer-
tainty and their scopein biomedical texts. In Proc. of
BioNLP 2008, pages 38–45, Columbus, Ohio, USA.
ACL.
Y. Tsuruoka and J. Tsujii. 2005. Bidirectional in-
ference with the easiest-first strategy for tagging se-
quence data. In Proc. of HLT/EMNLP 2005, pages
467–474.
Y. Tsuruoka, Y. Tateishi, J. Kim, T. Ohta, J. McNaught,
S. Ananiadou, and J. Tsujii, 2005. Advances in Infor-
matics - 10th Panhellenic Conference on Informatics,
volume 3746 of Lecture Notes in Computer Science,
chapter Part-of-Speech Tagger for Biomedical Text,
Advances in Informatics, pages 382–392. Springer,
Berlin/Heidelberg.
C.J. Van Rijsbergen. 1979. Information Retrieval. But-
terworths, London.
</reference>
<page confidence="0.998234">
724
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.593773">
<title confidence="0.999973">Learning the Scope of Negation in Biomedical Texts</title>
<author confidence="0.999841">Anthony Walter</author>
<affiliation confidence="0.9922955">Language Technology Applied Molecular Genomics University of</affiliation>
<address confidence="0.610121">Prinsstraat 13, B-2000 Antwerpen,</address>
<abstract confidence="0.998855294117647">In this paper we present a machine learning system that finds the scope of negation in biomedical texts. The system consists of two memory-based engines, one that decides if the tokens in a sentence are negation signals, and another that finds the full scope of these negation signals. Our approach to negation detection differs in two main aspects from existing research on negation. First, we focus on finding the scope of negation signals, instead of determining whether a term is negated or not. Second, we apply supervised machine learning techniques, whereas most existing systems apply rule-based algorithms. As far as we know, this way of approaching the negation scope finding task is novel.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Averbuch</author>
<author>T Karson</author>
<author>B Ben-Ami</author>
<author>O Maimon</author>
<author>L Rokach</author>
</authors>
<title>Context-sensitive medical information retrieval.</title>
<date>2004</date>
<booktitle>In Proc. of the 11th World Congress on Medical Informatics (MEDINFO-2004),</booktitle>
<volume>1</volume>
<pages>pages</pages>
<publisher>IOS Press.</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="7463" citStr="Averbuch et al. (2004)" startWordPosition="1179" endWordPosition="1182">ronic health records all the information required to generate automatically patient chronicles. According to the authors “the negation treatment module inserts markers in the text for negated phrases and determines scope of negation by using negation rules”. However, in the paper there is no description of the rules that are used and it is not explained how the results presented for negation recognition (57% of negations correctly recognised) are evaluated. The above-mentioned research applies rule-based algorithms to negation finding. Machine learning techniques have been used in some cases. Averbuch et al. (2004) developed an algorithm that uses information gain to learn negative context patterns. Golding and Chapman (2003) experiment with machine learning techniques to distinguish whether a medical observation is negated by the word not. Their corpus contains 207 selected sentences from hospital reports, in which a negation appears. They use Naive Bayes and Decision Trees and achieve a maximum of 90 F-score. According to the authors, their main finding is that “when negation of a UMLS term is triggered with the negation phrase not, if the term is preceded by the then do not negate”. Goryachev et al. </context>
</contexts>
<marker>Averbuch, Karson, Ben-Ami, Maimon, Rokach, 2004</marker>
<rawString>M. Averbuch, T. Karson, B. Ben-Ami, O. Maimon, and L. Rokach. 2004. Context-sensitive medical information retrieval. In Proc. of the 11th World Congress on Medical Informatics (MEDINFO-2004), pages 1– 8, San Francisco, CA. IOS Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Boytcheva</author>
<author>A Strupchanska</author>
<author>E Paskaleva</author>
<author>D Tcharaktchiev</author>
</authors>
<title>Some aspects of negation processing in electronic health records.</title>
<date>2005</date>
<booktitle>In Proc. of International Workshop Language and Speech Infrastructure for Information Access in the Balkan Countries,</booktitle>
<pages>1--8</pages>
<location>Borovets, Bulgaria.</location>
<contexts>
<context position="6759" citStr="Boytcheva et al. (2005)" startWordPosition="1073" endWordPosition="1076">operators. Then, a rule based system is used to decide if a concept has been positively, negatively, or uncertainly asserted. The system achieves 97.20 recall and 98.80 precision. The systems mentioned above are essentially based on lexical information. Huang and Lowe (2007) propose a classification scheme of negations based on syntactic categories and patterns in order to locate negated concepts, regardless of their distance from the negation signal. Their hybrid system that combines regular expression matching with grammatical parsing achieves 92.60 recall and 99.80 precision. Additionally, Boytcheva et al. (2005) incorporate the treatment of negation in a system, MEHR, that extracts from electronic health records all the information required to generate automatically patient chronicles. According to the authors “the negation treatment module inserts markers in the text for negated phrases and determines scope of negation by using negation rules”. However, in the paper there is no description of the rules that are used and it is not explained how the results presented for negation recognition (57% of negations correctly recognised) are evaluated. The above-mentioned research applies rule-based algorith</context>
</contexts>
<marker>Boytcheva, Strupchanska, Paskaleva, Tcharaktchiev, 2005</marker>
<rawString>S. Boytcheva, A. Strupchanska, E. Paskaleva, and D. Tcharaktchiev. 2005. Some aspects of negation processing in electronic health records. In Proc. of International Workshop Language and Speech Infrastructure for Information Access in the Balkan Countries, pages 1–8, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>E Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proc. of the X CoNLL Shared Task,</booktitle>
<publisher>SIGNLL.</publisher>
<location>New York.</location>
<contexts>
<context position="11202" citStr="Buchholz and Marsi, 2006" startWordPosition="1766" endWordPosition="1769">nchez-Graillet and Poesio, 2007). We processed the texts with the GENIA tagger (Tsuruoka and Tsujii, 2005; Tsuruoka et al., 2005), a bidirectional inference based tagger that analyzes English sentences and outputs the base forms, part-of-speech tags, chunk tags, and named entity tags in a tab-separated format2. Additionally, we converted the annotation about scope of negation into a token-per-token representation. Table 1 shows an example sentence of the corpus that results from converting and processing the BioScope representation. Following the standard format of the CoNLL Shared Task 2006 (Buchholz and Marsi, 2006), sentences are separated by a blank line and fields are separated by a single tab character. A sentence consists of tokens, each one starting on a new line. A token consists of the following 10 fields: 1. ABSTRACT ID: number of the GENIA abstract. 2. SENTENCE ID: sentence counter starting at 1 for each new abstract. 3. TOKEN ID: token counter, starting at 1 for each new sentence. 4. FORM: word form or punctuation symbol. 5. LEMMA: lemma of word form. 6. POS TAG: Penn Treebank part-of-speech tags described in (Santorini, 1990). 7. CHUNK TAG: IOB (Inside, Outside, Begin) tags produced by the GE</context>
<context position="14859" citStr="Buchholz and Marsi, 2006" startWordPosition="2445" endWordPosition="2448">here are negation signals in the sentence. Our conception of the task is inspired by Ramshaw and Marcus’ representation of text chunking as a tagging problem (Ramshaw and Marcus, 1995) . The information that can be used to train the system appears in columns 1 to 8 of Table 1. The information to be predicted by the system is contained in columns 9 and 10. As far as we know, approaching the negation scope finding task as a token per token classification task is novel, whereas at the same time it conforms to the well established standards of the recent CoNLL Shared Tasks3 on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007) and semantic role labeling (Surdeanu et al., 2008). By setting up the task in this way we show that the negation scope finding task can be modelled in a way similar to semantic role labeling, and by conforming to existing standards we show that learning the scope of negation can be integrated in a joint learning task with dependency parsing and semantic role labeling. 3Web page of CoNLL: http://www.ifarm.nl/signll/conll/. 718 5 System description In order to solve the task, we apply supervised machine learning techniques. We build a memorybased scope finder, that tackles </context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proc. of the X CoNLL Shared Task, New York. SIGNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W W Chapman</author>
<author>W Bridewell</author>
<author>P Hanbury</author>
<author>G F Cooper</author>
<author>B G Buchanan</author>
</authors>
<title>A simple algorithm for identifying negated findings and diseases in discharge summaries. JBiomed Inform,</title>
<date>2001</date>
<pages>34--301</pages>
<contexts>
<context position="4727" citStr="Chapman et al. (2001)" startWordPosition="761" endWordPosition="764">marise related work. In Section 3, we describe the corpus with which the system has been trained. In Section 4, we introduce the task to be performed by the system, which is described in Section 5. The results are presented and discussed in Section 6. Finally, Section 7 puts forward some conclusions. 2 Related work Negation has been a neglected area in open-domain natural language processing. Most research has been performed in the biomedical domain and has focused on detecting if a medical term is negated or not, whereas in this paper we focus on detecting the full scope of negation signals. Chapman et al. (2001) developed NegEx, a regular expression based algorithm for determining whether a finding or disease mentioned within narrative medical reports is present or absent. The reported results are 94.51 precision and 77.84 recall. Mutalik et al. (2001) developed Negfinder, a rulebased system that recognises negated patterns in medical documents. It consists of two tools: a lexical scanner called lever that uses regular expressions to generate a finite state machine, and a parser. The reported results are 95.70 recall and 91.80 precision. Sanchez-Graillet and Poesio (2007) present an analysis of negat</context>
</contexts>
<marker>Chapman, Bridewell, Hanbury, Cooper, Buchanan, 2001</marker>
<rawString>W. W. Chapman, W. Bridewell, P. Hanbury, G. F. Cooper, and B.G. Buchanan. 2001. A simple algorithm for identifying negated findings and diseases in discharge summaries. JBiomed Inform, 34:301–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Collier</author>
<author>H S Park</author>
<author>N Ogata</author>
<author>Y Tateisi</author>
<author>C Nobata</author>
<author>T Sekimizu</author>
<author>H Imai</author>
<author>J Tsujii</author>
</authors>
<title>The GENIA project: corpus-based knowledge acquisition and information extraction from genome research papers.</title>
<date>1999</date>
<booktitle>In Proceedings of EACL-99.</booktitle>
<contexts>
<context position="10318" citStr="Collier et al., 1999" startWordPosition="1630" endWordPosition="1633">rt the non-existence or uncertainty of something are annotated, in contrast to other corpora where only sentences of interest in the domain are annotated. A second characteristic is that the annotation is extended to the biggest syntactic unit possible so that scopes have the maximal length. In (2) below, negation signal no scopes over primary impairment of glucocorticoid metabolism instead of scoping only over primary. (2) There is [no] primary impairment of glucocorticoid metabolism in the asthmatics. The part used in our experiments are the biological paper abstracts from the GENIA corpus (Collier et al., 1999). This part consists of 11,872 sentences in 1,273 abstracts. We automatically discarded five sentences due to annotation errors. The total number of words used is 313,222, 1,739 of which are negation signals that belong to the different types described in (Sanchez-Graillet and Poesio, 2007). We processed the texts with the GENIA tagger (Tsuruoka and Tsujii, 2005; Tsuruoka et al., 2005), a bidirectional inference based tagger that analyzes English sentences and outputs the base forms, part-of-speech tags, chunk tags, and named entity tags in a tab-separated format2. Additionally, we converted t</context>
</contexts>
<marker>Collier, Park, Ogata, Tateisi, Nobata, Sekimizu, Imai, Tsujii, 1999</marker>
<rawString>N. Collier, H.S. Park, N. Ogata, Y. Tateisi, C. Nobata, T. Sekimizu, H. Imai, and J. Tsujii. 1999. The GENIA project: corpus-based knowledge acquisition and information extraction from genome research papers. In Proceedings of EACL-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cost</author>
<author>S Salzberg</author>
</authors>
<title>A weighted nearest neighbour algorithm for learning with symbolic features.</title>
<date>1993</date>
<booktitle>Machine learning,</booktitle>
<pages>10--57</pages>
<contexts>
<context position="16329" citStr="Cost and Salzberg, 1993" startWordPosition="2686" endWordPosition="2689">second classifier is postprocessed with an algorithm that converts non-consecutive blocks of scope into consecutive, as explained in Section 5.3. As for the first and second phases, we use a memory–based classifier as implemented in TiMBL (version 6.1.2) (Daelemans et al., 2007), a supervised inductive algorithm for learning classification tasks based on the k-nearest neighbor classification rule (Cover and Hart, 1967). Similarity is defined by computing (weighted) overlap of the feature values of a test instance and training instances. The metric combines a per-feature value distance metric (Cost and Salzberg, 1993) with gain ratio (Quinlan, 1993) based global feature weights that account for relative differences in discriminative power of the features. 5.1 Negation signal finding In this phase, a classifier predicts whether a token is a negation signal or not. The memory-based classifier was parameterised by using overlap as the similarity metric, gain ratio for feature weighting, and using 7 k-nearest neighbors. All neighbors have equal weight when voting for a class. The instances represent all tokens in the corpus and they have the following features: • Of the token: Form, lemma, part of speech, and </context>
</contexts>
<marker>Cost, Salzberg, 1993</marker>
<rawString>S. Cost and S. Salzberg. 1993. A weighted nearest neighbour algorithm for learning with symbolic features. Machine learning, 10:57–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M Cover</author>
<author>P E Hart</author>
</authors>
<title>Nearest neighbor pattern classification.</title>
<date>1967</date>
<booktitle>Institute of Electrical and Electronics Engineers Transactions on Information Theory,</booktitle>
<pages>13--21</pages>
<contexts>
<context position="16127" citStr="Cover and Hart, 1967" startWordPosition="2656" endWordPosition="2659">assifier predicts if a token is a negation signal, and in the second phase another classifier predicts if a token is inside the scope of each of the negation signals. Additionally, the output of the second classifier is postprocessed with an algorithm that converts non-consecutive blocks of scope into consecutive, as explained in Section 5.3. As for the first and second phases, we use a memory–based classifier as implemented in TiMBL (version 6.1.2) (Daelemans et al., 2007), a supervised inductive algorithm for learning classification tasks based on the k-nearest neighbor classification rule (Cover and Hart, 1967). Similarity is defined by computing (weighted) overlap of the feature values of a test instance and training instances. The metric combines a per-feature value distance metric (Cost and Salzberg, 1993) with gain ratio (Quinlan, 1993) based global feature weights that account for relative differences in discriminative power of the features. 5.1 Negation signal finding In this phase, a classifier predicts whether a token is a negation signal or not. The memory-based classifier was parameterised by using overlap as the similarity metric, gain ratio for feature weighting, and using 7 k-nearest ne</context>
</contexts>
<marker>Cover, Hart, 1967</marker>
<rawString>T. M. Cover and P. E. Hart. 1967. Nearest neighbor pattern classification. Institute of Electrical and Electronics Engineers Transactions on Information Theory, 13:21–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>A van den Bosch</author>
</authors>
<title>Memorybased language processing.</title>
<date>2005</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<marker>Daelemans, van den Bosch, 2005</marker>
<rawString>W. Daelemans and A. van den Bosch. 2005. Memorybased language processing. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>A Van den Bosch</author>
<author>J Zavrel</author>
</authors>
<title>Forgetting exceptions is harmful in language learning.</title>
<date>1999</date>
<booktitle>Machine Learning, Special issue on Natural Language Learning,</booktitle>
<pages>34--11</pages>
<marker>Daelemans, Van den Bosch, Zavrel, 1999</marker>
<rawString>W. Daelemans, A. Van den Bosch, and J. Zavrel. 1999. Forgetting exceptions is harmful in language learning. Machine Learning, Special issue on Natural Language Learning, 34:11–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>K Van der Sloot</author>
<author>A Van den Bosch</author>
</authors>
<title>TiMBL: Tilburg memory based learner, version 6.1, reference guide.</title>
<date>2007</date>
<tech>Technical Report Series 07-07,</tech>
<location>ILK, Tilburg, The Netherlands.</location>
<marker>Daelemans, Zavrel, Van der Sloot, Van den Bosch, 2007</marker>
<rawString>W. Daelemans, J. Zavrel, K. Van der Sloot, and A. Van den Bosch. 2007. TiMBL: Tilburg memory based learner, version 6.1, reference guide. Technical Report Series 07-07, ILK, Tilburg, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T G Dietterich</author>
</authors>
<title>Machine learning for sequential data: A review.</title>
<date>2002</date>
<booktitle>In Lecture Notes in Computer Science 2396,</booktitle>
<pages>15--30</pages>
<publisher>Springer Verlag.</publisher>
<location>London.</location>
<marker>Dietterich, 2002</marker>
<rawString>T. G. Dietterich. 2002. Machine learning for sequential data: A review. In Lecture Notes in Computer Science 2396, pages 15–30, London. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P L Elkin</author>
<author>S H Brown</author>
<author>B A Bauer</author>
<author>C S Husser</author>
<author>W Carruth</author>
<author>L R Bergstrom</author>
<author>D L WahnerRoedler</author>
</authors>
<title>A controlled trial of automated classification of negation from clinical notes.</title>
<date>2005</date>
<journal>BMC Medical Informatics and Decision Making,</journal>
<volume>5</volume>
<issue>13</issue>
<contexts>
<context position="5940" citStr="Elkin et al. (2005)" startWordPosition="949" endWordPosition="952"> negated interactions in biological texts and a heuristics-based system that extracts such information. They treat all types of negation: (i) Affixal negation, which is expressed by an affix. (ii) Noun phrase or emphatic negation, expressed syntactically by using a negative determiner (e.g. no, nothing). (iii) Inherent negation, expressed by words with an inherently negative meaning (e.g. absent). (iv) Negation with explicit negative particles (e.g. no, not). The texts are 50 journal articles. The preliminary results reported range from 54.32 F-score to 76.68, depending on the method applied. Elkin et al. (2005) describe a rule-based system that assigns to concepts a level of certainty as part of the generation of a dyadic parse tree in two phases: First a preprocessor breaks each sentence into text and operators. Then, a rule based system is used to decide if a concept has been positively, negatively, or uncertainly asserted. The system achieves 97.20 recall and 98.80 precision. The systems mentioned above are essentially based on lexical information. Huang and Lowe (2007) propose a classification scheme of negations based on syntactic categories and patterns in order to locate negated concepts, reg</context>
</contexts>
<marker>Elkin, Brown, Bauer, Husser, Carruth, Bergstrom, WahnerRoedler, 2005</marker>
<rawString>P. L. Elkin, S. H. Brown, B. A. Bauer, C.S. Husser, W. Carruth, L.R. Bergstrom, and D. L. WahnerRoedler. 2005. A controlled trial of automated classification of negation from clinical notes. BMC Medical Informatics and Decision Making, 5(13).</rawString>
</citation>
<citation valid="true">
<authors>
<author>l M Goldin</author>
<author>W W Chapman</author>
</authors>
<title>Learning to detect negation with ‘Not’ in medical texts.</title>
<date>2003</date>
<booktitle>In Proceedings ofACM-SIGIR</booktitle>
<marker>Goldin, Chapman, 2003</marker>
<rawString>l. M. Goldin and W.W. Chapman. 2003. Learning to detect negation with ‘Not’ in medical texts. In Proceedings ofACM-SIGIR 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goryachev</author>
<author>M Sordo</author>
<author>Q T Zeng</author>
<author>L Ngo</author>
</authors>
<title>Implementation and evaluation of four different methods of negation detection.</title>
<date>2006</date>
<tech>Technical report, DSG.</tech>
<contexts>
<context position="8069" citStr="Goryachev et al. (2006)" startWordPosition="1278" endWordPosition="1281">uch et al. (2004) developed an algorithm that uses information gain to learn negative context patterns. Golding and Chapman (2003) experiment with machine learning techniques to distinguish whether a medical observation is negated by the word not. Their corpus contains 207 selected sentences from hospital reports, in which a negation appears. They use Naive Bayes and Decision Trees and achieve a maximum of 90 F-score. According to the authors, their main finding is that “when negation of a UMLS term is triggered with the negation phrase not, if the term is preceded by the then do not negate”. Goryachev et al. (2006) compare the performance of four different methods of negation de716 tection, two regular expression-based methods and two classification-based methods trained on 1745 discharge reports. They show that the regular expression-based methods have better agreement with humans and better accuracy than the classification methods. Like in most of the mentioned work, the task consists in determining if a medical term is negated. Rokach et al. (2008) present a new pattern-based algorithm for identifying context in free-text medical narratives.The originality of the algorithm lies in that it automatical</context>
</contexts>
<marker>Goryachev, Sordo, Zeng, Ngo, 2006</marker>
<rawString>S. Goryachev, M. Sordo, Q.T. Zeng, and L. Ngo. 2006. Implementation and evaluation of four different methods of negation detection. Technical report, DSG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Huang</author>
<author>H J Lowe</author>
</authors>
<title>A novel hybrid approach to automated negation detection in clinical radiology reports. JAm Med Inform Assoc,</title>
<date>2007</date>
<contexts>
<context position="6411" citStr="Huang and Lowe (2007)" startWordPosition="1025" endWordPosition="1028">exts are 50 journal articles. The preliminary results reported range from 54.32 F-score to 76.68, depending on the method applied. Elkin et al. (2005) describe a rule-based system that assigns to concepts a level of certainty as part of the generation of a dyadic parse tree in two phases: First a preprocessor breaks each sentence into text and operators. Then, a rule based system is used to decide if a concept has been positively, negatively, or uncertainly asserted. The system achieves 97.20 recall and 98.80 precision. The systems mentioned above are essentially based on lexical information. Huang and Lowe (2007) propose a classification scheme of negations based on syntactic categories and patterns in order to locate negated concepts, regardless of their distance from the negation signal. Their hybrid system that combines regular expression matching with grammatical parsing achieves 92.60 recall and 99.80 precision. Additionally, Boytcheva et al. (2005) incorporate the treatment of negation in a system, MEHR, that extracts from electronic health records all the information required to generate automatically patient chronicles. According to the authors “the negation treatment module inserts markers in</context>
</contexts>
<marker>Huang, Lowe, 2007</marker>
<rawString>Y. Huang and H.J. Lowe. 2007. A novel hybrid approach to automated negation detection in clinical radiology reports. JAm Med Inform Assoc, 14(3):304–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Morante</author>
<author>W Daelemans</author>
<author>V Van Asch</author>
</authors>
<title>A combined memory-based semantic role labeler of english.</title>
<date>2008</date>
<booktitle>In Proc. of the CoNLL</booktitle>
<pages>208--212</pages>
<location>Manchester, UK.</location>
<marker>Morante, Daelemans, Van Asch, 2008</marker>
<rawString>R. Morante, W. Daelemans, and V. Van Asch. 2008. A combined memory-based semantic role labeler of english. In Proc. of the CoNLL 2008, pages 208–212, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A G Mutalik</author>
<author>A Deshpande</author>
<author>P M Nadkarni</author>
</authors>
<title>Use of general-purpose negation detection to augment concept indexing of medical documents. a quantitative study using the UMLS. JAm Med Inform Assoc,</title>
<date>2001</date>
<contexts>
<context position="2209" citStr="Mutalik et al., 2001" startWordPosition="343" endWordPosition="346">tead of determining whether a term is negated or not. Second, we apply supervised machine learning techniques, whereas most existing systems apply rule-based algorithms. Predicting the scope of negation is important in information extraction from text for obvious reasons; instead of simply flagging the sentences containing negation as not suited for extraction (which is currently the best that can be done), correct semantic relations can be extracted when the scope of negation is known, providing a better recall. Not being able to recognize negation can also hinder automated indexing systems (Mutalik et al., 2001; Rokach et al., 2008). As Mutalik et al. (2001) put it, “to increase the utility of concept indexing of medical documents, it is necessary to record whether the concept has been negated or not”. They highlight the need to detect negations in examples like “no evidence of fracture”, so that an information retrieval system does not return irrelevant reports. Szarvas et al. (2008) report that 13.45% of the sentences in the abstracts section of the BioScope corpus and 13.76% of the sentences in the full papers section contain negations. A system that does not deal with negation would treat these </context>
<context position="4972" citStr="Mutalik et al. (2001)" startWordPosition="800" endWordPosition="803">ion 6. Finally, Section 7 puts forward some conclusions. 2 Related work Negation has been a neglected area in open-domain natural language processing. Most research has been performed in the biomedical domain and has focused on detecting if a medical term is negated or not, whereas in this paper we focus on detecting the full scope of negation signals. Chapman et al. (2001) developed NegEx, a regular expression based algorithm for determining whether a finding or disease mentioned within narrative medical reports is present or absent. The reported results are 94.51 precision and 77.84 recall. Mutalik et al. (2001) developed Negfinder, a rulebased system that recognises negated patterns in medical documents. It consists of two tools: a lexical scanner called lever that uses regular expressions to generate a finite state machine, and a parser. The reported results are 95.70 recall and 91.80 precision. Sanchez-Graillet and Poesio (2007) present an analysis of negated interactions in biological texts and a heuristics-based system that extracts such information. They treat all types of negation: (i) Affixal negation, which is expressed by an affix. (ii) Noun phrase or emphatic negation, expressed syntactica</context>
</contexts>
<marker>Mutalik, Deshpande, Nadkarni, 2001</marker>
<rawString>A.G. Mutalik, A. Deshpande, and P.M. Nadkarni. 2001. Use of general-purpose negation detection to augment concept indexing of medical documents. a quantitative study using the UMLS. JAm Med Inform Assoc, 8(6):598–609.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>S K¨ubler</author>
<author>R McDonald</author>
<author>J Nilsson</author>
<author>S Riedel</author>
<author>D Yuret</author>
</authors>
<title>The CoNLL-2007 shared task on dependency parsing.</title>
<date>2007</date>
<booktitle>In Proc. of the CoNLL Shared Task Session ofEMNLP-CoNLL</booktitle>
<pages>915--932</pages>
<location>Prague.</location>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>J. Nivre, J. Hall, S. K¨ubler, R. McDonald, J. Nilsson, S. Riedel, and D. Yuret. 2007. The CoNLL-2007 shared task on dependency parsing. In Proc. of the CoNLL Shared Task Session ofEMNLP-CoNLL 2007, pages 915–932, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo, CA.</location>
<contexts>
<context position="16361" citStr="Quinlan, 1993" startWordPosition="2693" endWordPosition="2694"> algorithm that converts non-consecutive blocks of scope into consecutive, as explained in Section 5.3. As for the first and second phases, we use a memory–based classifier as implemented in TiMBL (version 6.1.2) (Daelemans et al., 2007), a supervised inductive algorithm for learning classification tasks based on the k-nearest neighbor classification rule (Cover and Hart, 1967). Similarity is defined by computing (weighted) overlap of the feature values of a test instance and training instances. The metric combines a per-feature value distance metric (Cost and Salzberg, 1993) with gain ratio (Quinlan, 1993) based global feature weights that account for relative differences in discriminative power of the features. 5.1 Negation signal finding In this phase, a classifier predicts whether a token is a negation signal or not. The memory-based classifier was parameterised by using overlap as the similarity metric, gain ratio for feature weighting, and using 7 k-nearest neighbors. All neighbors have equal weight when voting for a class. The instances represent all tokens in the corpus and they have the following features: • Of the token: Form, lemma, part of speech, and chunk IOB tag. • Of the token co</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J.R. Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann, San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ramshaw</author>
<author>M Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proc. ofACL Third Workshop on Very Large Corpora,</booktitle>
<pages>82--94</pages>
<publisher>ACL.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="14419" citStr="Ramshaw and Marcus, 1995" startWordPosition="2360" endWordPosition="2363">tags have been obtained by converting the xml files of BioScope. Each token can have one or more NEG SCOPE tags, depending on the number of negation signals in the sentence. 4 Task description We approach the scope finding task as a classification task that consists of classifying the tokens of a sentence as being a negation signal or not, and as being inside or outside the scope of the negation signal(s). This happens as many times as there are negation signals in the sentence. Our conception of the task is inspired by Ramshaw and Marcus’ representation of text chunking as a tagging problem (Ramshaw and Marcus, 1995) . The information that can be used to train the system appears in columns 1 to 8 of Table 1. The information to be predicted by the system is contained in columns 9 and 10. As far as we know, approaching the negation scope finding task as a token per token classification task is novel, whereas at the same time it conforms to the well established standards of the recent CoNLL Shared Tasks3 on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007) and semantic role labeling (Surdeanu et al., 2008). By setting up the task in this way we show that the negation scope finding task can be</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>L. Ramshaw and M. Marcus. 1995. Text chunking using transformation-based learning. In Proc. ofACL Third Workshop on Very Large Corpora, pages 82–94, Cambridge, MA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Rokach</author>
<author>R Romano</author>
<author>O Maimon</author>
</authors>
<title>Negation recognition in medical narrative reports. Information Retrieval Online.</title>
<date>2008</date>
<contexts>
<context position="2231" citStr="Rokach et al., 2008" startWordPosition="347" endWordPosition="350">ether a term is negated or not. Second, we apply supervised machine learning techniques, whereas most existing systems apply rule-based algorithms. Predicting the scope of negation is important in information extraction from text for obvious reasons; instead of simply flagging the sentences containing negation as not suited for extraction (which is currently the best that can be done), correct semantic relations can be extracted when the scope of negation is known, providing a better recall. Not being able to recognize negation can also hinder automated indexing systems (Mutalik et al., 2001; Rokach et al., 2008). As Mutalik et al. (2001) put it, “to increase the utility of concept indexing of medical documents, it is necessary to record whether the concept has been negated or not”. They highlight the need to detect negations in examples like “no evidence of fracture”, so that an information retrieval system does not return irrelevant reports. Szarvas et al. (2008) report that 13.45% of the sentences in the abstracts section of the BioScope corpus and 13.76% of the sentences in the full papers section contain negations. A system that does not deal with negation would treat these cases as false positiv</context>
<context position="8514" citStr="Rokach et al. (2008)" startWordPosition="1346" endWordPosition="1349">their main finding is that “when negation of a UMLS term is triggered with the negation phrase not, if the term is preceded by the then do not negate”. Goryachev et al. (2006) compare the performance of four different methods of negation de716 tection, two regular expression-based methods and two classification-based methods trained on 1745 discharge reports. They show that the regular expression-based methods have better agreement with humans and better accuracy than the classification methods. Like in most of the mentioned work, the task consists in determining if a medical term is negated. Rokach et al. (2008) present a new pattern-based algorithm for identifying context in free-text medical narratives.The originality of the algorithm lies in that it automatically learns patterns similar to the manually written patterns for negation detection. Apart from work on determining whether a term is negated or not, we are not aware of research that has focused on learning the full scope of negation signals inside or outside biomedical natural language processing. The research presented in this paper provides a new approach to the treatment of negation scope in natural language processing. 3 Corpus The corp</context>
</contexts>
<marker>Rokach, Romano, Maimon, 2008</marker>
<rawString>L. Rokach, R.Romano, and O. Maimon. 2008. Negation recognition in medical narrative reports. Information Retrieval Online.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Sanchez-Graillet</author>
<author>M Poesio</author>
</authors>
<title>Negation of protein-protein interactions: analysis and extraction.</title>
<date>2007</date>
<journal>Bioinformatics,</journal>
<volume>23</volume>
<issue>13</issue>
<contexts>
<context position="5298" citStr="Sanchez-Graillet and Poesio (2007)" startWordPosition="850" endWordPosition="853">cting the full scope of negation signals. Chapman et al. (2001) developed NegEx, a regular expression based algorithm for determining whether a finding or disease mentioned within narrative medical reports is present or absent. The reported results are 94.51 precision and 77.84 recall. Mutalik et al. (2001) developed Negfinder, a rulebased system that recognises negated patterns in medical documents. It consists of two tools: a lexical scanner called lever that uses regular expressions to generate a finite state machine, and a parser. The reported results are 95.70 recall and 91.80 precision. Sanchez-Graillet and Poesio (2007) present an analysis of negated interactions in biological texts and a heuristics-based system that extracts such information. They treat all types of negation: (i) Affixal negation, which is expressed by an affix. (ii) Noun phrase or emphatic negation, expressed syntactically by using a negative determiner (e.g. no, nothing). (iii) Inherent negation, expressed by words with an inherently negative meaning (e.g. absent). (iv) Negation with explicit negative particles (e.g. no, not). The texts are 50 journal articles. The preliminary results reported range from 54.32 F-score to 76.68, depending </context>
<context position="10609" citStr="Sanchez-Graillet and Poesio, 2007" startWordPosition="1675" endWordPosition="1678">aximal length. In (2) below, negation signal no scopes over primary impairment of glucocorticoid metabolism instead of scoping only over primary. (2) There is [no] primary impairment of glucocorticoid metabolism in the asthmatics. The part used in our experiments are the biological paper abstracts from the GENIA corpus (Collier et al., 1999). This part consists of 11,872 sentences in 1,273 abstracts. We automatically discarded five sentences due to annotation errors. The total number of words used is 313,222, 1,739 of which are negation signals that belong to the different types described in (Sanchez-Graillet and Poesio, 2007). We processed the texts with the GENIA tagger (Tsuruoka and Tsujii, 2005; Tsuruoka et al., 2005), a bidirectional inference based tagger that analyzes English sentences and outputs the base forms, part-of-speech tags, chunk tags, and named entity tags in a tab-separated format2. Additionally, we converted the annotation about scope of negation into a token-per-token representation. Table 1 shows an example sentence of the corpus that results from converting and processing the BioScope representation. Following the standard format of the CoNLL Shared Task 2006 (Buchholz and Marsi, 2006), sente</context>
</contexts>
<marker>Sanchez-Graillet, Poesio, 2007</marker>
<rawString>O. Sanchez-Graillet and M. Poesio. 2007. Negation of protein-protein interactions: analysis and extraction. Bioinformatics, 23(13):424–432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Santorini</author>
</authors>
<title>Part-of-speech tagging guidelines for the penn treebank project.</title>
<date>1990</date>
<tech>Technical report MSCIS-90-47,</tech>
<institution>Department of Computer and Information Science, University of Pennsylvania.</institution>
<contexts>
<context position="11734" citStr="Santorini, 1990" startWordPosition="1861" endWordPosition="1862">llowing the standard format of the CoNLL Shared Task 2006 (Buchholz and Marsi, 2006), sentences are separated by a blank line and fields are separated by a single tab character. A sentence consists of tokens, each one starting on a new line. A token consists of the following 10 fields: 1. ABSTRACT ID: number of the GENIA abstract. 2. SENTENCE ID: sentence counter starting at 1 for each new abstract. 3. TOKEN ID: token counter, starting at 1 for each new sentence. 4. FORM: word form or punctuation symbol. 5. LEMMA: lemma of word form. 6. POS TAG: Penn Treebank part-of-speech tags described in (Santorini, 1990). 7. CHUNK TAG: IOB (Inside, Outside, Begin) tags produced by the GENIA tagger that indicate if a token is inside a certain chunk, outside, or at the beginning. 8. NE TAG: IOB named entity tags produced by the GENIA tagger that indicate if a token is in1Web page: www.inf.u-szeged.hu/rgai/bioscope. 2The accuracy of the tagger might be inflated due to the fact that it was trained on the GENIA corpus. 717 ABSTR SNT TOK FORM LEMMA POS CHUNK NE NEG NEG SCOPE ID ID ID TAG TAG TAG SGN 10415075 07 1 NF-kappa NF-kappa NN B-NP B-protein I-NEG O-NEG 10415075 07 2 B B NN I-NP I-protein I-NEG O-NEG 1041507</context>
</contexts>
<marker>Santorini, 1990</marker>
<rawString>B. Santorini. 1990. Part-of-speech tagging guidelines for the penn treebank project. Technical report MSCIS-90-47, Department of Computer and Information Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M`arquez</author>
<author>J Nivre</author>
</authors>
<title>The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proc. of CoNLL</booktitle>
<pages>159--177</pages>
<location>Manchester, UK.</location>
<marker>M`arquez, Nivre, 2008</marker>
<rawString>M. Surdeanu, R. Johansson, A. Meyers, Ll. M`arquez, and J. Nivre. 2008. The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies. In Proc. of CoNLL 2008, pages 159–177, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Szarvas</author>
<author>V Vincze</author>
<author>R Farkas</author>
<author>J Csirik</author>
</authors>
<title>The BioScope corpus: annotation for negation, uncertainty and their scopein biomedical texts.</title>
<date>2008</date>
<booktitle>In Proc. of BioNLP</booktitle>
<pages>38--45</pages>
<publisher>ACL.</publisher>
<location>Columbus, Ohio, USA.</location>
<contexts>
<context position="2590" citStr="Szarvas et al. (2008)" startWordPosition="407" endWordPosition="410">ly the best that can be done), correct semantic relations can be extracted when the scope of negation is known, providing a better recall. Not being able to recognize negation can also hinder automated indexing systems (Mutalik et al., 2001; Rokach et al., 2008). As Mutalik et al. (2001) put it, “to increase the utility of concept indexing of medical documents, it is necessary to record whether the concept has been negated or not”. They highlight the need to detect negations in examples like “no evidence of fracture”, so that an information retrieval system does not return irrelevant reports. Szarvas et al. (2008) report that 13.45% of the sentences in the abstracts section of the BioScope corpus and 13.76% of the sentences in the full papers section contain negations. A system that does not deal with negation would treat these cases as false positives. The goals of this research are to model the scope finding task as a classification task similar to the semantic role labeling task, and to test the performance of a memory–based system that finds the scope of negation signals. Memory-based language processing (Daelemans and van den Bosch, 2005) is based on the idea that NLP problems can be solved by reu</context>
<context position="9177" citStr="Szarvas et al., 2008" startWordPosition="1454" endWordPosition="1457"> identifying context in free-text medical narratives.The originality of the algorithm lies in that it automatically learns patterns similar to the manually written patterns for negation detection. Apart from work on determining whether a term is negated or not, we are not aware of research that has focused on learning the full scope of negation signals inside or outside biomedical natural language processing. The research presented in this paper provides a new approach to the treatment of negation scope in natural language processing. 3 Corpus The corpus used is a part of the BioScope corpus (Szarvas et al., 2008)1, a freely available resource that consists of medical and biological texts. Every sentence is annotated with information about negation and speculation that indicates the boundaries of the scope and the keywords, as shown in (1). (1) PMA treatment, and &lt;xcope id=“X1.4.1”&gt;&lt;cue type=“negation” ref=”X1.4.1”&gt;not&lt;cue&gt; retinoic acid treatment of the U937 cells&lt;/xcope&gt; acts in inducing NF-KB expression in the nuclei. A first characteristic of the annotation of scope in the BioScope corpus is that all sentences that assert the non-existence or uncertainty of something are annotated, in contrast to o</context>
</contexts>
<marker>Szarvas, Vincze, Farkas, Csirik, 2008</marker>
<rawString>G. Szarvas, V. Vincze, R. Farkas, and J. Csirik. 2008. The BioScope corpus: annotation for negation, uncertainty and their scopein biomedical texts. In Proc. of BioNLP 2008, pages 38–45, Columbus, Ohio, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Tsuruoka</author>
<author>J Tsujii</author>
</authors>
<title>Bidirectional inference with the easiest-first strategy for tagging sequence data.</title>
<date>2005</date>
<booktitle>In Proc. of HLT/EMNLP</booktitle>
<pages>467--474</pages>
<contexts>
<context position="10682" citStr="Tsuruoka and Tsujii, 2005" startWordPosition="1688" endWordPosition="1691">glucocorticoid metabolism instead of scoping only over primary. (2) There is [no] primary impairment of glucocorticoid metabolism in the asthmatics. The part used in our experiments are the biological paper abstracts from the GENIA corpus (Collier et al., 1999). This part consists of 11,872 sentences in 1,273 abstracts. We automatically discarded five sentences due to annotation errors. The total number of words used is 313,222, 1,739 of which are negation signals that belong to the different types described in (Sanchez-Graillet and Poesio, 2007). We processed the texts with the GENIA tagger (Tsuruoka and Tsujii, 2005; Tsuruoka et al., 2005), a bidirectional inference based tagger that analyzes English sentences and outputs the base forms, part-of-speech tags, chunk tags, and named entity tags in a tab-separated format2. Additionally, we converted the annotation about scope of negation into a token-per-token representation. Table 1 shows an example sentence of the corpus that results from converting and processing the BioScope representation. Following the standard format of the CoNLL Shared Task 2006 (Buchholz and Marsi, 2006), sentences are separated by a blank line and fields are separated by a single t</context>
</contexts>
<marker>Tsuruoka, Tsujii, 2005</marker>
<rawString>Y. Tsuruoka and J. Tsujii. 2005. Bidirectional inference with the easiest-first strategy for tagging sequence data. In Proc. of HLT/EMNLP 2005, pages 467–474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Tsuruoka</author>
<author>Y Tateishi</author>
<author>J Kim</author>
<author>T Ohta</author>
<author>J McNaught</author>
<author>S Ananiadou</author>
<author>J Tsujii</author>
</authors>
<date>2005</date>
<booktitle>Advances in Informatics - 10th Panhellenic Conference on Informatics,</booktitle>
<volume>3746</volume>
<pages>382--392</pages>
<publisher>Springer,</publisher>
<location>Berlin/Heidelberg.</location>
<contexts>
<context position="10706" citStr="Tsuruoka et al., 2005" startWordPosition="1692" endWordPosition="1695">nstead of scoping only over primary. (2) There is [no] primary impairment of glucocorticoid metabolism in the asthmatics. The part used in our experiments are the biological paper abstracts from the GENIA corpus (Collier et al., 1999). This part consists of 11,872 sentences in 1,273 abstracts. We automatically discarded five sentences due to annotation errors. The total number of words used is 313,222, 1,739 of which are negation signals that belong to the different types described in (Sanchez-Graillet and Poesio, 2007). We processed the texts with the GENIA tagger (Tsuruoka and Tsujii, 2005; Tsuruoka et al., 2005), a bidirectional inference based tagger that analyzes English sentences and outputs the base forms, part-of-speech tags, chunk tags, and named entity tags in a tab-separated format2. Additionally, we converted the annotation about scope of negation into a token-per-token representation. Table 1 shows an example sentence of the corpus that results from converting and processing the BioScope representation. Following the standard format of the CoNLL Shared Task 2006 (Buchholz and Marsi, 2006), sentences are separated by a blank line and fields are separated by a single tab character. A sentence</context>
</contexts>
<marker>Tsuruoka, Tateishi, Kim, Ohta, McNaught, Ananiadou, Tsujii, 2005</marker>
<rawString>Y. Tsuruoka, Y. Tateishi, J. Kim, T. Ohta, J. McNaught, S. Ananiadou, and J. Tsujii, 2005. Advances in Informatics - 10th Panhellenic Conference on Informatics, volume 3746 of Lecture Notes in Computer Science, chapter Part-of-Speech Tagger for Biomedical Text, Advances in Informatics, pages 382–392. Springer, Berlin/Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Van Rijsbergen</author>
</authors>
<title>Information Retrieval.</title>
<date>1979</date>
<publisher>Butterworths,</publisher>
<location>London.</location>
<marker>Van Rijsbergen, 1979</marker>
<rawString>C.J. Van Rijsbergen. 1979. Information Retrieval. Butterworths, London.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>