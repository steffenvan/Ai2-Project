<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011176">
<title confidence="0.967316">
N-Best Rescoring Based on Pitch-accent Patterns
</title>
<author confidence="0.999112">
Je Hun Jeon&apos; Wen Wang2 Yang Liu&apos;
</author>
<affiliation confidence="0.9817355">
&apos;Department of Computer Science, The University of Texas at Dallas, USA
2Speech Technology and Research Laboratory, SRI International, USA
</affiliation>
<email confidence="0.998671">
{jhjeon,yangl}@hlt.utdallas.edu, wwang@speech.sri.com
</email>
<sectionHeader confidence="0.995649" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999894434782609">
In this paper, we adopt an n-best rescoring
scheme using pitch-accent patterns to improve
automatic speech recognition (ASR) perfor-
mance. The pitch-accent model is decoupled
from the main ASR system, thus allowing us
to develop it independently. N-best hypothe-
ses from recognizers are rescored by addi-
tional scores that measure the correlation of
the pitch-accent patterns between the acoustic
signal and lexical cues. To test the robustness
of our algorithm, we use two different data
sets and recognition setups: the first one is En-
glish radio news data that has pitch accent la-
bels, but the recognizer is trained from a small
amount of data and has high error rate; the sec-
ond one is English broadcast news data using
a state-of-the-art SRI recognizer. Our experi-
mental results demonstrate that our approach
is able to reduce word error rate relatively by
about 3%. This gain is consistent across the
two different tests, showing promising future
directions of incorporating prosodic informa-
tion to improve speech recognition.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971790697674">
Prosody refers to the suprasegmental features of nat-
ural speech, such as rhythm and intonation, since
it normally extends over more than one phoneme
segment. Speakers use prosody to convey paralin-
guistic information such as emphasis, intention, atti-
tude, and emotion. Humans listening to speech with
natural prosody are able to understand the content
with low cognitive load and high accuracy. How-
ever, most modern ASR systems only use an acous-
tic model and a language model. Acoustic informa-
tion in ASR is represented by spectral features that
are usually extracted over a window length of a few
tens of milliseconds. They miss useful information
contained in the prosody of the speech that may help
recognition.
Recently a lot of research has been done in au-
tomatic annotation of prosodic events (Wightman
and Ostendorf, 1994; Sridhar et al., 2008; Anan-
thakrishnan and Narayanan, 2008; Jeon and Liu,
2009). They used acoustic and lexical-syntactic
cues to annotate prosodic events with a variety of
machine learning approaches and achieved good
performance. There are also many studies us-
ing prosodic information for various spoken lan-
guage understanding tasks. However, research using
prosodic knowledge for speech recognition is still
quite limited. In this study, we investigate leverag-
ing prosodic information for recognition in an n-best
rescoring framework.
Previous studies showed that prosodic events,
such as pitch-accent, are closely related with acous-
tic prosodic cues and lexical structure of utterance.
The pitch-accent pattern given acoustic signal is
strongly correlated with lexical items, such as syl-
lable identity and canonical stress pattern. There-
fore as a first study, we focus on pitch-accent in this
paper. We develop two separate pitch-accent de-
tection models, using acoustic (observation model)
and lexical information (expectation model) respec-
tively, and propose a scoring method for the cor-
relation of pitch-accent patterns between the two
models for recognition hypotheses. The n-best list
is rescored using the pitch-accent matching scores
</bodyText>
<page confidence="0.94">
732
</page>
<note confidence="0.9796835">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 732–741,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999843388888889">
combined with the other scores from the ASR sys-
tem (acoustic and language model scores). We show
that our method yields a word error rate (WER) re-
duction of about 3.64% and 2.07% relatively on two
baseline ASR systems, one being a state-of-the-art
recognizer for the broadcast news domain. The fact
that it holds across different baseline systems sug-
gests the possibility that prosody can be used to help
improve speech recognition performance.
The remainder of this paper is organized as fol-
lows. In the next section, we review previous work
briefly. Section 3 explains the models and features
for pitch-accent detection. We provide details of our
n-best rescoring approach in Section 4. Section 5
describes our corpus and baseline ASR setup. Sec-
tion 6 presents our experiments and results. The last
section gives a brief summary along with future di-
rections.
</bodyText>
<sectionHeader confidence="0.997211" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999981483870968">
Prosody is of interest to speech researchers be-
cause it plays an important role in comprehension
of spoken language by human listeners. The use
of prosody in speech understanding applications has
been quite extensive. A variety of applications
have been explored, such as sentence and topic seg-
mentation (Shriberg et al., 2000; Rosenberg and
Hirschberg, 2006), word error detection (Litman et
al., 2000), dialog act detection (Sridhar et al., 2009),
speaker recognition (Shriberg et al., 2005), and emo-
tion recognition (Benus et al., 2007), just to name a
few.
Incorporating prosodic knowledge is expected
to improve the performance of speech recogni-
tion. However, how to effectively integrate prosody
within the traditional ASR framework is a difficult
problem, since prosodic features are not well de-
fined and they come from a longer region, which is
different from spectral features used in current ASR
systems. Various research has been conducted try-
ing to incorporate prosodic information in ASR. One
way is to directly integrate prosodic features into
the ASR framework (Vergyri et al., 2003; Ostendorf
et al., 2003; Chen and Hasegawa-Johnson, 2006).
Such efforts include prosody dependent acoustic and
pronunciation model (allophones were distinguished
according to different prosodic phenomenon), lan-
guage model (words were augmented by prosody
events), and duration modeling (different prosodic
events were modeled separately and combined with
conventional HMM). This kind of integration has
advantages in that spectral and prosodic features are
more tightly coupled and jointly modeled. Alterna-
tively, prosody was modeled independently from the
acoustic and language models of ASR and used to
rescore recognition hypotheses in the second pass.
This approach makes it possible to independently
model and optimize the prosodic knowledge and to
combine with ASR hypotheses without any modi-
fication of the conventional ASR modules. In or-
der to improve the rescoring performance, various
prosodic knowledge was studied. (Ananthakrishnan
and Narayanan, 2007) used acoustic pitch-accent
pattern and its sequential information given lexi-
cal cues to rescore n-best hypotheses. (Kalinli and
Narayanan, 2009) used acoustic prosodic cues such
as pitch and duration along with other knowledge
to choose a proper word among several candidates
in confusion networks. Prosodic boundaries based
on acoustic cues were used in (Szaszak and Vicsi,
2007).
We take a similar approach in this study as the
second approach above in that we develop prosodic
models separately and use them in a rescoring
framework. Our proposed method differs from pre-
vious work in the way that the prosody model is used
to help ASR. In our approach, we explicitly model
the symbolic prosodic events based on acoustic and
lexical information. We then capture the correla-
tion of pitch-accent patterns between the two differ-
ent cues, and use that to improve recognition perfor-
mance in an n-best rescoring paradigm.
</bodyText>
<sectionHeader confidence="0.998961" genericHeader="method">
3 Prosodic Model
</sectionHeader>
<bodyText confidence="0.9998675">
Among all the prosodic events, we use only pitch-
accent pattern in this study, because previous stud-
ies have shown that acoustic pitch-accent is strongly
correlated with lexical items, such as canonical
stress pattern and syllable identity that can be eas-
ily acquired from the output of conventional ASR
and pronunciation dictionary. We treat pitch-accent
detection as a binary classification task, that is, a
classifier is used to determine whether the base unit
is prominent or not. Since pitch-accent is usually
</bodyText>
<page confidence="0.997562">
733
</page>
<bodyText confidence="0.966565">
carried by syllables, we use syllables as our units,
and the syllable definition of each word is based
on CMU pronunciation dictionary which has lexi-
cal stress and syllable boundary marks (Bartlett et
al., 2009). We separately develop acoustic-prosodic
and lexical-prosodic models and use the correlation
between the two models for each syllable to rescore
the n-best hypotheses of baseline ASR systems.
</bodyText>
<subsectionHeader confidence="0.997739">
3.1 Acoustic-prosodic Features
</subsectionHeader>
<bodyText confidence="0.999567727272727">
Similar to most previous work, the prosodic features
we use include pitch, energy, and duration. We also
add delta features of pitch and energy. Duration in-
formation for syllables is derived from the speech
waveform and phone-level forced alignment of the
transcriptions. In order to reduce the effect by both
inter-speaker and intra-speaker variation, both pitch
and energy values are normalized (z-value) with ut-
terance specific means and variances. For pitch, en-
ergy, and their delta values, we apply several cate-
gories of 12 functions to generate derived features.
</bodyText>
<listItem confidence="0.984555857142857">
• Statistics (7): minimum, maximum, range,
mean, standard deviation, skewness and kurto-
sis value. These are used widely in prosodic
event detection and emotion detection.
• Contour (5): This is approximated by taking
5 leading terms in the Legendre polynomial
expansion. The approximation of the contour
using the Legendre polynomial expansion has
been successfully applied in quantitative pho-
netics (Grabe et al., 2003) and in engineering
applications (Dehak et al., 2007). Each term
models a particular aspect of the contour, such
as the slope, and information about the curva-
ture.
</listItem>
<bodyText confidence="0.999892375">
We use 6 duration features, that is, raw, normal-
ized, and relative durations (ms) of the syllable and
vowel. Normalization (z-value) is performed based
on statistics for each syllable and vowel. The rela-
tive value is the difference between the normalized
current duration and the following one.
In the above description, we assumed that the
event of a syllable is only dependent on its observa-
tions, and did not consider contextual effect. To al-
leviate this restriction, we expand the features by in-
corporating information about the neighboring sylla-
bles. Based on the study in (Jeon and Liu, 2010) that
evaluated using left and right contexts, we choose to
use one previous and one following context in the
features. The total number of features used in this
study is 162.
</bodyText>
<subsectionHeader confidence="0.998792">
3.2 Lexical-prosodic Features
</subsectionHeader>
<bodyText confidence="0.999955333333333">
There is a very strong correlation between pitch-
accent in an utterance and its lexical information.
Previous studies have shown that the lexical fea-
tures perform well for pitch-accent prediction. The
detailed features for training the lexical-prosodic
model are as follows.
</bodyText>
<listItem confidence="0.996534">
• Syllable identity: We kept syllables that appear
more than 5 times in the training corpus. The
other syllables that occur less are collapsed into
one syllable representation.
• Vowel phone identity: We used vowel phone
identity as a feature.
• Lexical stress: This is a binary feature to rep-
resent if the syllable corresponds to a lexical
stress based on the pronunciation dictionary.
• Boundary information: This is a binary feature
to indicate if there is a word boundary before
the syllable.
</listItem>
<bodyText confidence="0.797870666666667">
For lexical features, based on the study in (Jeon
and Liu, 2010), we added two previous and two fol-
lowing contexts in the final features.
</bodyText>
<subsectionHeader confidence="0.975841">
3.3 Prosodic Model Training
</subsectionHeader>
<bodyText confidence="0.999428384615385">
We choose to use a support vector machine (SVM)
classifier&apos; for the prosodic model based on previous
work on prosody labeling study in (Jeon and Liu,
2010). We use RBF kernel for the acoustic model,
and 3-order polynomial kernel for the lexical model.
In our experiments, we investigate two kinds
of training methods for prosodic modeling. The
first one is a supervised method where models are
trained using all the labeled data. The second is
a semi-supervised method using co-training algo-
rithm (Blum and Mitchell, 1998), described in Algo-
rithm 1. Given a set L of labeled data and a set U of
unlabeled data with two views, it then iterates in the
</bodyText>
<footnote confidence="0.9992425">
1LIBSVM – A Library for Support Vector Machines, loca-
tion: http://www.csie.ntu.edu.tw/˜cjlin/libsvm/
</footnote>
<page confidence="0.99371">
734
</page>
<figure confidence="0.3083034">
Algorithm 1 Co-training algorithm.
Given:
- L: labeled examples; U: unlabeled examples
- there are two views V1 and V2 on an example x
Initialize:
</figure>
<bodyText confidence="0.93652">
- L1=L, samples used to train classifiers h1
- L2=L, samples used to train classifiers h2
</bodyText>
<subsectionHeader confidence="0.63754">
Loop for k iterations
</subsectionHeader>
<bodyText confidence="0.995869404761905">
- create a small pool U´ choosing from U
- use V1(L1) to train classifier h1
and V2(L2) to train classifier h2
- let h1 label/select examples Dh1 from U´
- let h2 label/select examples Dh2 from U´
- add self-labeled examples Dh1 to L2
and Dh2 to L1
- remove Dh1 and Dh2 from U
following procedure. The algorithm first creates a
smaller pool U′ containing unlabeled data from U. It
uses Li (i = 1, 2) to train two distinct classifiers: the
acoustic classifier h1, and the lexical classifier h2.
We use function U (i = 1, 2) to represent that only
a single view is used for training h1 or h2. These two
classifiers are used to make predictions for the unla-
beled set U′, and only when they agree on the predic-
tion for a sample, their predicted class is used as the
label for this sample. Then among these self-labeled
samples, the most confident ones by one classifier
are added to the data set Li for training the other
classifier. This iteration continues until reaching the
defined number of iterations. In our experiment, the
size of the pool U´ is 5 times of the size of training
data Li, and the size of the added self-labeled ex-
ample set, Dhz, is 5% of Li. For the newly selected
Dhz, the distribution of the positive and negative ex-
amples is the same as that of the training data Li.
This co-training method is expected to cope with
two problems in prosodic model training. The first
problem is the different decision patterns between
the two classifiers: the acoustic model has relatively
higher precision, while the lexical model has rela-
tively higher recall. The goal of the co-training al-
gorithm is to learn from the difference of each clas-
sifier, thus it can improve the performance as well
as reduce the mismatch of two classifiers. The sec-
ond problem is the mismatch of data used for model
training and testing, which often results in system
performance degradation. Using co-training, we can
use the unlabeled data from the domain that matches
the test data, adapting the model towards test do-
main.
</bodyText>
<sectionHeader confidence="0.902255" genericHeader="method">
4 N-Best Rescoring Scheme
</sectionHeader>
<bodyText confidence="0.99993425">
In order to leverage prosodic information for bet-
ter speech recognition performance, we augment the
standard ASR equation to include prosodic informa-
tion as following:
</bodyText>
<equation confidence="0.999058">
W� = arg W p(W  |As, Ap)
= arg Wp(As,Ap|W)p(W) (1)
</equation>
<bodyText confidence="0.9999906">
where As and Ap represent acoustic-spectral fea-
tures and acoustic-prosodic features. We can further
assume that spectral and prosodic features are con-
ditionally independent given a word sequence W,
therefore, Equation 1 can be rewritten as following:
</bodyText>
<equation confidence="0.997065">
W� ≈ arg max p(As|W)p(W)p(Ap|W) (2)
W
</equation>
<bodyText confidence="0.999919958333333">
The first two terms stand for the acoustic and lan-
guage models in the original ASR system, and the
last term means the prosody model we introduce. In-
stead of using the prosodic model in the first pass de-
coding, we use it to rescore n-best candidates from
a speech recognizer. This allows us to train the
prosody models independently and better optimize
the models.
For p(Ap|W), the prosody score for a word se-
quence W, in this work we propose a method to es-
timate it, also represented as ScoreW−prosody(W).
The idea of scoring the prosody patterns is that there
is some expectation of pitch-accent patterns given
the lexical sequence (W), and the acoustic pitch-
accent should match with this expectation. For in-
stance, in the case of a prominent syllable, both
acoustic and lexical evidence show pitch-accent, and
vice versa. In order to maximize the agreement be-
tween the two sources, we measure how good the
acoustic pitch-accent in speech signal matches the
given lexical cues. For each syllable Si in the n-best
list, we use acoustic-prosodic cues (ai) to estimate
the posterior probability that the syllable is promi-
nent (P), p(P|ai). Similarly, we use lexical cues (li)
</bodyText>
<page confidence="0.992486">
735
</page>
<bodyText confidence="0.999866">
to determine the syllable’s pitch-accent probability
p(P|li). Then the prosody score for a syllable Si is
estimated by the match of the pitch-accent patterns
between acoustic and lexical information using the
difference of the posteriors from the two models:
</bodyText>
<equation confidence="0.994948">
scoreS−prosody(Si) pz� 1−  |p(P|ai) − p(P|li)  |(3)
</equation>
<bodyText confidence="0.999863714285714">
Furthermore, we take into account the effect due
to varying durations for different syllables. We no-
tice that syllables without pitch-accent have much
shorter duration than the prominent ones, and the
prosody scores for the short syllables tend to be
high. This means that if a syllable is split into two
consecutive non-prominent syllables, the agreement
score may be higher than a long prominent syllable.
Therefore, we introduce a weighting factor based on
syllable duration (dur(i)). For a candidate word se-
quence (W) consisting of n syllables, its prosodic
score is the sum of the prosodic scores for all the
syllables in it weighted by their duration (measured
using milliseconds), that is:
</bodyText>
<equation confidence="0.999727">
scoreW−prosody(W) �
∑n log(scoreS−prosody(Si)) · dur(i) (4)
i=1
</equation>
<bodyText confidence="0.9999448">
We then combine this prosody score with the
original acoustic and language model likelihood
(P(As|W) and P(W) in Equation 2). In practice,
we need to weight them differently, therefore, the
combined score for a hypothesis W is:
</bodyText>
<equation confidence="0.9998325">
Score(W) = λ · scoreW−prosody(W)
+ scoreASR(W) (5)
</equation>
<bodyText confidence="0.999937333333333">
where scoreASR(W) is generated by ASR systems
(composed of acoustic and language model scores)
and λ is optimized using held out data.
</bodyText>
<sectionHeader confidence="0.980237" genericHeader="method">
5 Data and Baseline Systems
</sectionHeader>
<bodyText confidence="0.999937557692308">
Our experiments are carried out using two different
data sets and two different recognition systems as
well in order to test the robustness of our proposed
method.
The first data set is the Boston University Radio
News Corpus (BU) (Ostendorf et al., 1995), which
consists of broadcast news style read speech. The
BU corpus has about 3 hours of read speech from
7 speakers (3 female, 4 male). Part of the data has
been labeled with ToBI-style prosodic annotations.
In fact, the reason that we use this corpus, instead of
other corpora typically used for ASR experiments,
is because of its prosodic labels. We divided the
entire data corpus into a training set and a test set.
There was no speaker overlap between training and
test sets. The training set has 2 female speakers (f2
and f3) and 3 male ones (m2, m3, m4). The test set is
from the other two speakers (f1 and m1). We use 200
utterances for the recognition experiments. Each ut-
terance in BU corpus consists of more than one sen-
tences, so we segmented each utterance based on
pause, resulting in a total number of 713 segments
for testing. We divided the test set roughly equally
into two sets, and used one for parameter tuning and
the other for rescoring test. The recognizer used for
this data set was based on Sphinx-32. The context-
dependent triphone acoustic models with 32 Gaus-
sian mixtures were trained using the training par-
tition of the BU corpus described above, together
with the broadcast new data. A standard back-off tri-
gram language model with Kneser-Ney smoothing
was trained using the combined text from the train-
ing partition of the BU, Wall Street Journal data, and
part of Gigaword corpus. The vocabulary size was
about 10K words and the out-of-vocabulary (OOV)
rate on the test set was 2.1%.
The second data set is from broadcast news (BN)
speech used in the GALE program. The recognition
test set contains 1,001 utterances. The n-best hy-
potheses for this data set are generated by a state-of-
the-art SRI speech recognizer, developed for broad-
cast news speech (Stolcke et al., 2006; Zheng et
al., 2007). This system yields much better perfor-
mance than the first one. We also divided the test
set roughly equally into two sets for parameter tun-
ing and testing. From the data used for training the
speech recognizer, we randomly selected 5.7 hours
of speech (4,234 utterances) for the co-training al-
gorithm for the prosodic models.
For prosodic models, we used a simple binary
representation of pitch-accent in the form of pres-
ence versus absence. The reference labels are de-
</bodyText>
<footnote confidence="0.991869">
2CMU Sphinx - Speech Recognition Toolkit, location:
http://www.speech.cs.cmu.edu/sphinx/tutorial.html
</footnote>
<page confidence="0.998101">
736
</page>
<bodyText confidence="0.999898333333333">
rived from the ToBI annotation in the BU corpus,
and the ratio of pitch-accented syllables is about
34%. Acoustic-prosodic and lexical-prosodic mod-
els were separately developed using the features de-
scribed in Section 3. Feature extraction was per-
formed at the syllable level from force-aligned data.
For the supervised approach, we used those utter-
ances in the training data partition with ToBI labels
in the BU corpus (245 utterances, 14,767 syllables).
For co-training, the labeled data from BU corpus is
used as initial training, and the other unlabeled data
from BU and BN are used as unlabeled data.
</bodyText>
<sectionHeader confidence="0.994102" genericHeader="evaluation">
6 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.963732">
6.1 Pitch-accent Detection
</subsectionHeader>
<bodyText confidence="0.999996928571428">
First we evaluate the performance of our acoustic-
prosodic and lexical-prosodic models for pitch-
accent detection. For rescoring, not only the ac-
curacies of the two individual prosodic models are
important, but also the pitch-accent agreement score
between the two models (as shown in Equation 3)
is critical, therefore, we present results using these
two metrics. Table 1 shows the accuracy of each
model for pitch-accent detection, and also the av-
erage prosody score of the two models (i.e., Equa-
tion 3) for positive and negative classes (using ref-
erence labels). These results are based on the BU
labeled data in the test set. To compare our pitch ac-
cent detection performance with previous work, we
include the result of (Jeon and Liu, 2009) as a ref-
erence. Compared to previous work, the acoustic
model achieved similar performance, while the per-
formance of lexical model is a bit lower. The lower
performance of lexical model is mainly because we
do not use part-of-speech (POS) information in the
features, since we want to only use the word output
from the ASR system (without additional POS tag-
ging).
As shown in Table 1, when using the co-training
algorithm, as described in Section 3.3, the over-
all accuracies improve slightly and therefore the
prosody score is also increased. We expect this im-
proved model will be more beneficial for rescoring.
</bodyText>
<subsectionHeader confidence="0.998295">
6.2 N-Best Rescoring
</subsectionHeader>
<bodyText confidence="0.999166">
For the rescoring experiment, we use 100-best hy-
potheses from the two different ASR systems, as de-
</bodyText>
<table confidence="0.9969524">
Accuracy(%) Prosody score
Acoustic Lexical Pos Neg
Supervised 83.97 84.48 0.747 0.852
Co-training 84.54 84.99 0.771 0.867
Reference 83.53 87.92 - -
</table>
<tableCaption confidence="0.710883">
Table 1: Pitch accent detection results: performance of
individual acoustic and lexical models, and the agreement
between the two models (i.e., prosody score for a syllable,
Equation 3) for positive and negative classes. Also shown
is the reference result for pitch accent detection from Jeon
and Liu (2009).
</tableCaption>
<bodyText confidence="0.999948085714286">
scribed in Section 5. We apply the acoustic and lex-
ical prosodic models to each hypothesis to obtain its
prosody score, and combine it with ASR scores to
find the top hypothesis. The weights were optimized
using one test set and applied to the other. We report
the average result of the two testings.
Table 2 shows the rescoring results using the first
recognition system on BU data, which was trained
with a relatively small amount of data. The 1-
best baseline uses the first hypothesis that has the
best ASR score. The oracle result is from the best
hypothesis that gives the lowest WER by compar-
ing all the candidates to the reference transcript.
We used two prosodic models as described in Sec-
tion 3.3. The first one is the base prosodic model us-
ing supervised training (S-model). The second is the
prosodic model with the co-training algorithm (C-
model). For these rescoring experiments, we tuned
A (in Equation 5) when combining the ASR acous-
tic and language model scores with the additional
prosody score. The value in parenthesis in Table 2
means the relative WER reduction when compared
to the baseline result. We show the WER results for
both the development and the test set.
As shown in Table 2, we observe performance
improvement using our rescoring method. Using
the base S-model yields reasonable improvement,
and C-model further reduces WER. Even though the
prosodic event detection performance of these two
prosodic models is similar, the improved prosody
score between the acoustic and lexical prosodic
models using co-training helps rescoring. After
rescoring using prosodic knowledge, the WER is re-
duced by 0.82% (3.64% relative). Furthermore, we
notice that the difference between development and
</bodyText>
<page confidence="0.991503">
737
</page>
<table confidence="0.999552">
WER (%)
1-best baseline 22.64
S Dev 21.93 (3.11%)
-model
Test 22.10 (2.39%)
C-model Dev 21.76 (3.88%)
Test 21.81 (3.64%)
Oracle 15.58
</table>
<tableCaption confidence="0.992829333333333">
Table 2: WER of the baseline system and after rescoring
using prosodic models. Results are based on the first ASR
system.
</tableCaption>
<bodyText confidence="0.999694">
test data is smaller when using the C-model than S-
model, which means that the prosodic model with
co-training is more stable. In fact, we found that
the optimal value of A is 94 and 57 for the two
folds using S-model, and is 99 and 110 for the C-
model. These verify again that the prosodic scores
contribute more in the combination with ASR likeli-
hood scores when using the C-model, and are more
robust across different tuning sets. Ananthakrish-
nan and Narayanan (2007) also used acoustic/lexical
prosodic models to estimate a prosody score and re-
ported 0.3% recognition error reduction on BU data
when rescoring 100-best list (their baseline WER is
22.8%). Although there is some difference in experi-
mental setup (data, classifier, features) between ours
and theirs, our S-model showed comparable perfor-
mance gain and the result of C-model is significantly
better than theirs.
Next we test our n-best rescoring approach using a
state-of-the-art SRI speech recognizer on BN data to
verify if our approach can generalize to better ASR
n-best lists. This is often the concern that improve-
ments observed on a poor ASR system do not hold
for better ASR systems. The rescoring results are
shown in Table 3. We can see that the baseline per-
formance of this recognizer is much better than that
of the first ASR system (even though the recogni-
tion task is also harder). Our rescoring approach
still yields performance gain even using this state-
of-the-art system. The WER is reduced by 0.29%
(2.07% relative). This error reduction is lower than
that in the first ASR system. There are several pos-
sible reasons. First, the baseline ASR performance
is higher, making further improvement hard; sec-
ond, and more importantly, the prosody models do
not match well to the test domain. We trained the
prosody model using the BU data. Even though co-
training is used to leverage unlabeled BN data to re-
duce data mismatch, it is still not as good as using
labeled in-domain data for model training.
</bodyText>
<table confidence="0.999474625">
WER (%)
1-best baseline 13.77
S Dev 13.53 (1.78 %)
-model
Test 13.55 (1.63 %)
C-model Dev 13.48 (2.16 %)
Test 13.49 (2.07 %)
Oracle 9.23
</table>
<tableCaption confidence="0.99264">
Table 3: WER of the baseline system and after rescoring
using prosodic models. Results are based on the second
ASR system.
</tableCaption>
<subsectionHeader confidence="0.999009">
6.3 Analysis and Discussion
</subsectionHeader>
<bodyText confidence="0.9999947">
We also analyze what kinds of errors are reduced
using our rescoring approach. Most of the error re-
duction came from substitution and insertion errors.
Deletion error rate did not change much or some-
times even increased. For a better understanding of
the improvement using the prosody model, we ana-
lyzed the pattern of corrections (the new hypothesis
after rescoring is correct while the original 1-best is
wrong) and errors. Table 4 shows some positive and
negative examples from rescoring results using the
first ASR system. In this table, each word is asso-
ciated with some binary expressions inside a paren-
thesis, which stand for pitch-accent markers. Two
bits are used for each syllable: the first one is for
the acoustic-prosodic model and the second one is
for the lexical-prosodic model. For both bits, 1 rep-
resents pitch-accent, and 0 indicates none. These
hard decisions are obtained by setting a threshold of
0.5 for the posterior probabilities from the acoustic
or lexical models. For example, when the acoustic
classifier predicts a syllable as pitch-accented and
the lexical one as not accented, ‘10’ marker is as-
signed to the syllable. The number of such pairs of
pitch-accent markers is the same as the number of
syllables in a word. The bold words indicate correct
words and italic means errors. As shown in the pos-
itive example of Table 4, we find that our prosodic
model is effective at identifying an erroneous word
when it is split into two words, resulting in dif-
ferent pitch-accent patterns. Language models are
</bodyText>
<page confidence="0.994127">
738
</page>
<table confidence="0.988281125">
1-best : most of the massachusetts
Positive example (11 ) (10) (00) (11 00 01 00)
rescored : most other massachusetts
(11 ) (11 00) (11 00 01 00)
1-best : robbery and on a theft
Negative example (11 00 00) (00) (10) (00) (11)
rescored : robbery and lot of theft
(11 00 00) (00) (11) (00) (11)
</table>
<tableCaption confidence="0.9957455">
Table 4: Examples of rescoring results. Binary expressions inside the parenthesis below a word represent pitch-accent
markers for the syllables in the word.
</tableCaption>
<bodyText confidence="0.999958">
not good at correcting this kind of errors since both
word sequences are plausible. Our model also intro-
duces some errors, as shown in the negative exam-
ple, which is mainly due to the inaccurate prosody
model.
We conducted more prosody rescoring experi-
ments in order to understand the model behavior.
These analyses are based on the n-best list from the
first ASR system for the entire test set. In the first
experiment, among the 100 hypotheses in n-best list,
we gave a prosody score of 0 to the 100th hypothe-
sis, and used automatically obtained prosodic scores
for the other hypotheses. A zero prosody score
means the perfect agreement given acoustic and lex-
ical cues. The original scores from the recognizer
were combined with the prosodic scores for rescor-
ing. This was to verify that the range of the weight-
ing factor A estimated on the development data (us-
ing the original, not the modified prosody scores for
all candidates) was reasonable to choose proper hy-
pothesis among all the candidates. We noticed that
27% of the times the last hypothesis on the list was
selected as the best hypothesis. This hypothesis has
the highest prosodic scores, but lowest ASR score.
This result showed that if the prosodic models were
accurate enough, the correct candidate could be cho-
sen using our rescoring framework.
In the second experiment, we put the reference
text together with the other candidates. We use the
same ASR scores for all candidates, and generated
prosodic scores using our prosody model. This was
to test that our model could pick up correct candi-
date using only the prosodic score. We found that
for 26% of the utterances, the reference transcript
was chosen as the best one. This was significantly
better than random selection (i.e., 1/100), suggest-
ing the benefit of the prosody model; however, this
percentage is not very high, implying the limitation
of prosodic information for ASR or the current im-
perfect prosodic models.
In the third experiment, we replaced the 100th
candidate with the reference transcript and kept its
ASR score. When using our prosody rescoring ap-
proach, we obtained a relative error rate reduction
of 6.27%. This demonstrates again that our rescor-
ing method works well – if the correct hypothesis is
on the list, even though with a low ASR score, us-
ing prosodic information can help identify the cor-
rect candidate.
Overall the performance improvement we ob-
tained from rescoring by incorporating prosodic in-
formation is very promising. Our evaluation using
two different ASR systems shows that the improve-
ment holds even when we use a state-of-the-art rec-
ognizer and the training data for the prosody model
does not come from the same corpus. We believe
the consistent improvements we observed for differ-
ent conditions show that this is a direction worthy of
further investigation.
</bodyText>
<sectionHeader confidence="0.998485" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999942">
In this paper, we attempt to integrate prosodic infor-
mation for ASR using an n-best rescoring scheme.
This approach decouples the prosodic model from
the main ASR system, thus the prosodic model can
be built independently. The prosodic scores that we
use for n-best rescoring are based on the matching
of pitch-accent patterns by acoustic and lexical fea-
tures. Our rescoring method achieved a WER reduc-
tion of 3.64% and 2.07% relatively using two differ-
ent ASR systems. The fact that the gain holds across
different baseline systems (including a state-of-the-
</bodyText>
<page confidence="0.994586">
739
</page>
<bodyText confidence="0.999937666666667">
art speech recognizer) suggests the possibility that
prosody can be used to improve speech recognition
performance.
As suggested by our experiments, better prosodic
models can result in more WER reduction. The per-
formance of our prosodic model was improved with
co-training, but there are still problems, such as the
imbalance of the two classifiers’ prediction, as well
as for the two events. In order to address these prob-
lems, we plan to improve the labeling and selec-
tion method in the co-training algorithm, and also
explore other training algorithms to reduce domain
mismatch. Furthermore, we are also interested in
evaluating our approach on the spontaneous speech
domain, which is quite different from the data we
used in this study.
In this study, we used n-best rather than lattice
rescoring. Since the prosodic features we use in-
clude cross-word contextual information, it is not
straightforward to apply it directly to lattices. In
our future work, we will develop models with only
within-word context, and thus allowing us to explore
lattice rescoring, which we expect will yield more
performance gain.
</bodyText>
<sectionHeader confidence="0.998152" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999792973333334">
Sankaranarayanan Ananthakrishnan and Shrikanth
Narayanan. 2007. Improved speech recognition using
acoustic and lexical correlated of pitch accent in a
n-best rescoring framework. Proc. of ICASSP, pages
65–68.
Sankaranarayanan Ananthakrishnan and Shrikanth
Narayanan. 2008. Automatic prosodic event detec-
tion using acoustic, lexical and syntactic evidence.
IEEE Transactions on Audio, Speech, and Language
Processing, 16(1):216–228.
Susan Bartlett, Grzegorz Kondrak, and Colin Cherry.
2009. On the syllabification of phonemes. Proc. of
NAACL-HLT, pages 308–316.
Stefan Benus, Agustin Gravano, and Julia Hirschberg.
2007. Prosody, emotions, and whatever. Proc. of In-
terspeech, pages 2629–2632.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. Proc. of the
Workshop on Computational Learning Theory, pages
92–100.
Ken Chen and Mark Hasegawa-Johnson. 2006. Prosody
dependent speech recognition on radio news corpus
of American English. IEEE Transactions on Audio,
Speech, and Language Processing, 14(1):232– 245.
Najim Dehak, Pierre Dumouchel, and Patrick Kenny.
2007. Modeling prosodic features with joint fac-
tor analysis for speaker verification. IEEE Transac-
tions on Audio, Speech, and Language Processing,
15(7):2095–2103.
Esther Grabe, Greg Kochanski, and John Coleman. 2003.
Quantitative modelling of intonational variation. Proc.
of SASRTLM, pages 45–57.
Je Hun Jeon and Yang Liu. 2009. Automatic prosodic
events detection suing syllable-based acoustic and syn-
tactic features. Proc. of ICASSP, pages 4565–4568.
Je Hun Jeon and Yang Liu. 2010. Syllable-level promi-
nence detection with acoustic evidence. Proc. of Inter-
speech, pages 1772–1775.
Ozlem Kalinli and Shrikanth Narayanan. 2009. Contin-
uous speech recognition using attention shift decoding
with soft decision. Proc. of Interspeech, pages 1927–
1930.
Diane J. Litman, Julia B. Hirschberg, and Marc Swerts.
2000. Predicting automatic speech recognition perfor-
mance using prosodic cues. Proc. of NAACL, pages
218–225.
Mari Ostendorf, Patti Price, and Stefanie Shattuck-
Hufnagel. 1995. The Boston University radio news
corpus. Linguistic Data Consortium.
Mari Ostendorf, Izhak Shafran, and Rebecca Bates.
2003. Prosody models for conversational speech
recognition. Proc. of the 2nd Plenary Meeting and
Symposium on Prosody and Speech Processing, pages
147–154.
Andrew Rosenberg and Julia Hirschberg. 2006. Story
segmentation of broadcast news in English, Mandarin
and Arabic. Proc. of HLT-NAACL, pages 125–128.
Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-T¨ur,
and G¨okhan T¨ur. 2000. Prosody-based automatic seg-
mentation of speech into sentences and topics. Speech
Communication, 32(1-2):127–154.
Elizabeth Shriberg, Luciana Ferrer, Sachin S. Kajarekar,
Anand Venkataraman, and Andreas Stolcke. 2005.
Modeling prosodic feature sequences for speaker
recognition. Speech Communication, 46(3-4):455–
472.
Vivek Kumar Rangarajan Sridhar, Srinivas Bangalore,
and Shrikanth S. Narayanan. 2008. Exploiting acous-
tic and syntactic features for automatic prosody label-
ing in a maximum entropy framework. IEEE Trans-
actions on Audio, Speech, and Language Processing,
16(4):797–811.
Vivek Kumar Rangarajan Sridhar, Srinivas Bangalore,
and Shrikanth Narayanan. 2009. Combining lexi-
cal, syntactic and prosodic cues for improved online
</reference>
<page confidence="0.960934">
740
</page>
<reference confidence="0.998588629629629">
dialog act tagging. Computer Speech and Language,
23(4):407–422.
Andreas Stolcke, Barry Chen, Horacio Franco, Venkata
Ramana Rao Gadde, Martin Graciarena, Mei-Yuh
Hwang, Katrin Kirchhoff, Arindam Mandal, Nelson
Morgan, Xin Lin, Tim Ng, Mari Ostendorf, Kemal
S¨onmez, Anand Venkataraman, Dimitra Vergyri, Wen
Wang, Jing Zheng, and Qifeng Zhu. 2006. Recent in-
novations in speech-to-text transcription at SRI-ICSI-
UW. IEEE Transactions on Audio, Speech and Lan-
guage Processing, 14(5):1729–1744. Special Issue on
Progress in Rich Transcription.
Gyorgy Szaszak and Klara Vicsi. 2007. Speech recogni-
tion supported by prosodic information for fixed stress
languages. Proc. of TSD Conference, pages 262–269.
Dimitra Vergyri, Andreas Stolcke, Venkata R. R. Gadde,
Luciana Ferrer, and Elizabeth Shriberg. 2003.
Prosodic knowledge sources for automatic speech
recognition. Proc. of ICASSP, pages 208–211.
Colin W. Wightman and Mari Ostendorf. 1994. Auto-
matic labeling of prosodic patterns. IEEE Transaction
on Speech and Auido Processing, 2(4):469–481.
Jing Zheng, Ozgur Cetin, Mei-Yuh Hwang, Xin Lei, An-
dreas Stolcke, and Nelson Morgan. 2007. Combin-
ing discriminative feature, transform, and model train-
ing for large vocabulary speech recognition. Proc. of
ICASSP, pages 633–636.
</reference>
<page confidence="0.997855">
741
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.335548">
<title confidence="0.999799">N-Best Rescoring Based on Pitch-accent Patterns</title>
<author confidence="0.999415">Hun Yang</author>
<address confidence="0.553213">of Computer Science, The University of Texas at Dallas, USA Technology and Research Laboratory, SRI International, USA</address>
<email confidence="0.999598">wwang@speech.sri.com</email>
<abstract confidence="0.998136791666667">In this paper, we adopt an n-best rescoring scheme using pitch-accent patterns to improve automatic speech recognition (ASR) performance. The pitch-accent model is decoupled from the main ASR system, thus allowing us to develop it independently. N-best hypotheses from recognizers are rescored by additional scores that measure the correlation of the pitch-accent patterns between the acoustic signal and lexical cues. To test the robustness of our algorithm, we use two different data sets and recognition setups: the first one is English radio news data that has pitch accent labels, but the recognizer is trained from a small amount of data and has high error rate; the second one is English broadcast news data using a state-of-the-art SRI recognizer. Our experimental results demonstrate that our approach is able to reduce word error rate relatively by about 3%. This gain is consistent across the two different tests, showing promising future directions of incorporating prosodic information to improve speech recognition.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sankaranarayanan Ananthakrishnan</author>
<author>Shrikanth Narayanan</author>
</authors>
<title>Improved speech recognition using acoustic and lexical correlated of pitch accent in a n-best rescoring framework.</title>
<date>2007</date>
<booktitle>Proc. of ICASSP,</booktitle>
<pages>65--68</pages>
<contexts>
<context position="6532" citStr="Ananthakrishnan and Narayanan, 2007" startWordPosition="993" endWordPosition="996">parately and combined with conventional HMM). This kind of integration has advantages in that spectral and prosodic features are more tightly coupled and jointly modeled. Alternatively, prosody was modeled independently from the acoustic and language models of ASR and used to rescore recognition hypotheses in the second pass. This approach makes it possible to independently model and optimize the prosodic knowledge and to combine with ASR hypotheses without any modification of the conventional ASR modules. In order to improve the rescoring performance, various prosodic knowledge was studied. (Ananthakrishnan and Narayanan, 2007) used acoustic pitch-accent pattern and its sequential information given lexical cues to rescore n-best hypotheses. (Kalinli and Narayanan, 2009) used acoustic prosodic cues such as pitch and duration along with other knowledge to choose a proper word among several candidates in confusion networks. Prosodic boundaries based on acoustic cues were used in (Szaszak and Vicsi, 2007). We take a similar approach in this study as the second approach above in that we develop prosodic models separately and use them in a rescoring framework. Our proposed method differs from previous work in the way that</context>
<context position="25255" citStr="Ananthakrishnan and Narayanan (2007)" startWordPosition="4094" endWordPosition="4098">21.76 (3.88%) Test 21.81 (3.64%) Oracle 15.58 Table 2: WER of the baseline system and after rescoring using prosodic models. Results are based on the first ASR system. test data is smaller when using the C-model than Smodel, which means that the prosodic model with co-training is more stable. In fact, we found that the optimal value of A is 94 and 57 for the two folds using S-model, and is 99 and 110 for the Cmodel. These verify again that the prosodic scores contribute more in the combination with ASR likelihood scores when using the C-model, and are more robust across different tuning sets. Ananthakrishnan and Narayanan (2007) also used acoustic/lexical prosodic models to estimate a prosody score and reported 0.3% recognition error reduction on BU data when rescoring 100-best list (their baseline WER is 22.8%). Although there is some difference in experimental setup (data, classifier, features) between ours and theirs, our S-model showed comparable performance gain and the result of C-model is significantly better than theirs. Next we test our n-best rescoring approach using a state-of-the-art SRI speech recognizer on BN data to verify if our approach can generalize to better ASR n-best lists. This is often the con</context>
</contexts>
<marker>Ananthakrishnan, Narayanan, 2007</marker>
<rawString>Sankaranarayanan Ananthakrishnan and Shrikanth Narayanan. 2007. Improved speech recognition using acoustic and lexical correlated of pitch accent in a n-best rescoring framework. Proc. of ICASSP, pages 65–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sankaranarayanan Ananthakrishnan</author>
<author>Shrikanth Narayanan</author>
</authors>
<title>Automatic prosodic event detection using acoustic, lexical and syntactic evidence.</title>
<date>2008</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="2214" citStr="Ananthakrishnan and Narayanan, 2008" startWordPosition="341" endWordPosition="345">de, and emotion. Humans listening to speech with natural prosody are able to understand the content with low cognitive load and high accuracy. However, most modern ASR systems only use an acoustic model and a language model. Acoustic information in ASR is represented by spectral features that are usually extracted over a window length of a few tens of milliseconds. They miss useful information contained in the prosody of the speech that may help recognition. Recently a lot of research has been done in automatic annotation of prosodic events (Wightman and Ostendorf, 1994; Sridhar et al., 2008; Ananthakrishnan and Narayanan, 2008; Jeon and Liu, 2009). They used acoustic and lexical-syntactic cues to annotate prosodic events with a variety of machine learning approaches and achieved good performance. There are also many studies using prosodic information for various spoken language understanding tasks. However, research using prosodic knowledge for speech recognition is still quite limited. In this study, we investigate leveraging prosodic information for recognition in an n-best rescoring framework. Previous studies showed that prosodic events, such as pitch-accent, are closely related with acoustic prosodic cues and </context>
</contexts>
<marker>Ananthakrishnan, Narayanan, 2008</marker>
<rawString>Sankaranarayanan Ananthakrishnan and Shrikanth Narayanan. 2008. Automatic prosodic event detection using acoustic, lexical and syntactic evidence. IEEE Transactions on Audio, Speech, and Language Processing, 16(1):216–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Bartlett</author>
<author>Grzegorz Kondrak</author>
<author>Colin Cherry</author>
</authors>
<title>On the syllabification of phonemes.</title>
<date>2009</date>
<booktitle>Proc. of NAACL-HLT,</booktitle>
<pages>308--316</pages>
<contexts>
<context position="8197" citStr="Bartlett et al., 2009" startWordPosition="1261" endWordPosition="1264"> shown that acoustic pitch-accent is strongly correlated with lexical items, such as canonical stress pattern and syllable identity that can be easily acquired from the output of conventional ASR and pronunciation dictionary. We treat pitch-accent detection as a binary classification task, that is, a classifier is used to determine whether the base unit is prominent or not. Since pitch-accent is usually 733 carried by syllables, we use syllables as our units, and the syllable definition of each word is based on CMU pronunciation dictionary which has lexical stress and syllable boundary marks (Bartlett et al., 2009). We separately develop acoustic-prosodic and lexical-prosodic models and use the correlation between the two models for each syllable to rescore the n-best hypotheses of baseline ASR systems. 3.1 Acoustic-prosodic Features Similar to most previous work, the prosodic features we use include pitch, energy, and duration. We also add delta features of pitch and energy. Duration information for syllables is derived from the speech waveform and phone-level forced alignment of the transcriptions. In order to reduce the effect by both inter-speaker and intra-speaker variation, both pitch and energy v</context>
</contexts>
<marker>Bartlett, Kondrak, Cherry, 2009</marker>
<rawString>Susan Bartlett, Grzegorz Kondrak, and Colin Cherry. 2009. On the syllabification of phonemes. Proc. of NAACL-HLT, pages 308–316.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Benus</author>
<author>Agustin Gravano</author>
<author>Julia Hirschberg</author>
</authors>
<title>Prosody, emotions, and whatever.</title>
<date>2007</date>
<booktitle>Proc. of Interspeech,</booktitle>
<pages>2629--2632</pages>
<contexts>
<context position="5009" citStr="Benus et al., 2007" startWordPosition="769" endWordPosition="772">ast section gives a brief summary along with future directions. 2 Previous Work Prosody is of interest to speech researchers because it plays an important role in comprehension of spoken language by human listeners. The use of prosody in speech understanding applications has been quite extensive. A variety of applications have been explored, such as sentence and topic segmentation (Shriberg et al., 2000; Rosenberg and Hirschberg, 2006), word error detection (Litman et al., 2000), dialog act detection (Sridhar et al., 2009), speaker recognition (Shriberg et al., 2005), and emotion recognition (Benus et al., 2007), just to name a few. Incorporating prosodic knowledge is expected to improve the performance of speech recognition. However, how to effectively integrate prosody within the traditional ASR framework is a difficult problem, since prosodic features are not well defined and they come from a longer region, which is different from spectral features used in current ASR systems. Various research has been conducted trying to incorporate prosodic information in ASR. One way is to directly integrate prosodic features into the ASR framework (Vergyri et al., 2003; Ostendorf et al., 2003; Chen and Hasegaw</context>
</contexts>
<marker>Benus, Gravano, Hirschberg, 2007</marker>
<rawString>Stefan Benus, Agustin Gravano, and Julia Hirschberg. 2007. Prosody, emotions, and whatever. Proc. of Interspeech, pages 2629–2632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>Proc. of the Workshop on Computational Learning Theory,</booktitle>
<pages>92--100</pages>
<contexts>
<context position="11837" citStr="Blum and Mitchell, 1998" startWordPosition="1842" endWordPosition="1845">, we added two previous and two following contexts in the final features. 3.3 Prosodic Model Training We choose to use a support vector machine (SVM) classifier&apos; for the prosodic model based on previous work on prosody labeling study in (Jeon and Liu, 2010). We use RBF kernel for the acoustic model, and 3-order polynomial kernel for the lexical model. In our experiments, we investigate two kinds of training methods for prosodic modeling. The first one is a supervised method where models are trained using all the labeled data. The second is a semi-supervised method using co-training algorithm (Blum and Mitchell, 1998), described in Algorithm 1. Given a set L of labeled data and a set U of unlabeled data with two views, it then iterates in the 1LIBSVM – A Library for Support Vector Machines, location: http://www.csie.ntu.edu.tw/˜cjlin/libsvm/ 734 Algorithm 1 Co-training algorithm. Given: - L: labeled examples; U: unlabeled examples - there are two views V1 and V2 on an example x Initialize: - L1=L, samples used to train classifiers h1 - L2=L, samples used to train classifiers h2 Loop for k iterations - create a small pool U´ choosing from U - use V1(L1) to train classifier h1 and V2(L2) to train classifier </context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. Proc. of the Workshop on Computational Learning Theory, pages 92–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Chen</author>
<author>Mark Hasegawa-Johnson</author>
</authors>
<title>Prosody dependent speech recognition on radio news corpus of American English.</title>
<date>2006</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>14</volume>
<issue>1</issue>
<pages>245</pages>
<contexts>
<context position="5625" citStr="Chen and Hasegawa-Johnson, 2006" startWordPosition="865" endWordPosition="868">us et al., 2007), just to name a few. Incorporating prosodic knowledge is expected to improve the performance of speech recognition. However, how to effectively integrate prosody within the traditional ASR framework is a difficult problem, since prosodic features are not well defined and they come from a longer region, which is different from spectral features used in current ASR systems. Various research has been conducted trying to incorporate prosodic information in ASR. One way is to directly integrate prosodic features into the ASR framework (Vergyri et al., 2003; Ostendorf et al., 2003; Chen and Hasegawa-Johnson, 2006). Such efforts include prosody dependent acoustic and pronunciation model (allophones were distinguished according to different prosodic phenomenon), language model (words were augmented by prosody events), and duration modeling (different prosodic events were modeled separately and combined with conventional HMM). This kind of integration has advantages in that spectral and prosodic features are more tightly coupled and jointly modeled. Alternatively, prosody was modeled independently from the acoustic and language models of ASR and used to rescore recognition hypotheses in the second pass. T</context>
</contexts>
<marker>Chen, Hasegawa-Johnson, 2006</marker>
<rawString>Ken Chen and Mark Hasegawa-Johnson. 2006. Prosody dependent speech recognition on radio news corpus of American English. IEEE Transactions on Audio, Speech, and Language Processing, 14(1):232– 245.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Najim Dehak</author>
<author>Pierre Dumouchel</author>
<author>Patrick Kenny</author>
</authors>
<title>Modeling prosodic features with joint factor analysis for speaker verification.</title>
<date>2007</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>15</volume>
<issue>7</issue>
<contexts>
<context position="9463" citStr="Dehak et al., 2007" startWordPosition="1451" endWordPosition="1454">pecific means and variances. For pitch, energy, and their delta values, we apply several categories of 12 functions to generate derived features. • Statistics (7): minimum, maximum, range, mean, standard deviation, skewness and kurtosis value. These are used widely in prosodic event detection and emotion detection. • Contour (5): This is approximated by taking 5 leading terms in the Legendre polynomial expansion. The approximation of the contour using the Legendre polynomial expansion has been successfully applied in quantitative phonetics (Grabe et al., 2003) and in engineering applications (Dehak et al., 2007). Each term models a particular aspect of the contour, such as the slope, and information about the curvature. We use 6 duration features, that is, raw, normalized, and relative durations (ms) of the syllable and vowel. Normalization (z-value) is performed based on statistics for each syllable and vowel. The relative value is the difference between the normalized current duration and the following one. In the above description, we assumed that the event of a syllable is only dependent on its observations, and did not consider contextual effect. To alleviate this restriction, we expand the feat</context>
</contexts>
<marker>Dehak, Dumouchel, Kenny, 2007</marker>
<rawString>Najim Dehak, Pierre Dumouchel, and Patrick Kenny. 2007. Modeling prosodic features with joint factor analysis for speaker verification. IEEE Transactions on Audio, Speech, and Language Processing, 15(7):2095–2103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Esther Grabe</author>
<author>Greg Kochanski</author>
<author>John Coleman</author>
</authors>
<title>Quantitative modelling of intonational variation.</title>
<date>2003</date>
<booktitle>Proc. of SASRTLM,</booktitle>
<pages>45--57</pages>
<contexts>
<context position="9410" citStr="Grabe et al., 2003" startWordPosition="1443" endWordPosition="1446">ergy values are normalized (z-value) with utterance specific means and variances. For pitch, energy, and their delta values, we apply several categories of 12 functions to generate derived features. • Statistics (7): minimum, maximum, range, mean, standard deviation, skewness and kurtosis value. These are used widely in prosodic event detection and emotion detection. • Contour (5): This is approximated by taking 5 leading terms in the Legendre polynomial expansion. The approximation of the contour using the Legendre polynomial expansion has been successfully applied in quantitative phonetics (Grabe et al., 2003) and in engineering applications (Dehak et al., 2007). Each term models a particular aspect of the contour, such as the slope, and information about the curvature. We use 6 duration features, that is, raw, normalized, and relative durations (ms) of the syllable and vowel. Normalization (z-value) is performed based on statistics for each syllable and vowel. The relative value is the difference between the normalized current duration and the following one. In the above description, we assumed that the event of a syllable is only dependent on its observations, and did not consider contextual effe</context>
</contexts>
<marker>Grabe, Kochanski, Coleman, 2003</marker>
<rawString>Esther Grabe, Greg Kochanski, and John Coleman. 2003. Quantitative modelling of intonational variation. Proc. of SASRTLM, pages 45–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Je Hun Jeon</author>
<author>Yang Liu</author>
</authors>
<title>Automatic prosodic events detection suing syllable-based acoustic and syntactic features.</title>
<date>2009</date>
<booktitle>Proc. of ICASSP,</booktitle>
<pages>4565--4568</pages>
<contexts>
<context position="2235" citStr="Jeon and Liu, 2009" startWordPosition="346" endWordPosition="349">speech with natural prosody are able to understand the content with low cognitive load and high accuracy. However, most modern ASR systems only use an acoustic model and a language model. Acoustic information in ASR is represented by spectral features that are usually extracted over a window length of a few tens of milliseconds. They miss useful information contained in the prosody of the speech that may help recognition. Recently a lot of research has been done in automatic annotation of prosodic events (Wightman and Ostendorf, 1994; Sridhar et al., 2008; Ananthakrishnan and Narayanan, 2008; Jeon and Liu, 2009). They used acoustic and lexical-syntactic cues to annotate prosodic events with a variety of machine learning approaches and achieved good performance. There are also many studies using prosodic information for various spoken language understanding tasks. However, research using prosodic knowledge for speech recognition is still quite limited. In this study, we investigate leveraging prosodic information for recognition in an n-best rescoring framework. Previous studies showed that prosodic events, such as pitch-accent, are closely related with acoustic prosodic cues and lexical structure of </context>
<context position="21620" citStr="Jeon and Liu, 2009" startWordPosition="3489" endWordPosition="3492">ng, not only the accuracies of the two individual prosodic models are important, but also the pitch-accent agreement score between the two models (as shown in Equation 3) is critical, therefore, we present results using these two metrics. Table 1 shows the accuracy of each model for pitch-accent detection, and also the average prosody score of the two models (i.e., Equation 3) for positive and negative classes (using reference labels). These results are based on the BU labeled data in the test set. To compare our pitch accent detection performance with previous work, we include the result of (Jeon and Liu, 2009) as a reference. Compared to previous work, the acoustic model achieved similar performance, while the performance of lexical model is a bit lower. The lower performance of lexical model is mainly because we do not use part-of-speech (POS) information in the features, since we want to only use the word output from the ASR system (without additional POS tagging). As shown in Table 1, when using the co-training algorithm, as described in Section 3.3, the overall accuracies improve slightly and therefore the prosody score is also increased. We expect this improved model will be more beneficial fo</context>
</contexts>
<marker>Jeon, Liu, 2009</marker>
<rawString>Je Hun Jeon and Yang Liu. 2009. Automatic prosodic events detection suing syllable-based acoustic and syntactic features. Proc. of ICASSP, pages 4565–4568.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Je Hun Jeon</author>
<author>Yang Liu</author>
</authors>
<title>Syllable-level prominence detection with acoustic evidence.</title>
<date>2010</date>
<booktitle>Proc. of Interspeech,</booktitle>
<pages>1772--1775</pages>
<contexts>
<context position="10172" citStr="Jeon and Liu, 2010" startWordPosition="1569" endWordPosition="1572">about the curvature. We use 6 duration features, that is, raw, normalized, and relative durations (ms) of the syllable and vowel. Normalization (z-value) is performed based on statistics for each syllable and vowel. The relative value is the difference between the normalized current duration and the following one. In the above description, we assumed that the event of a syllable is only dependent on its observations, and did not consider contextual effect. To alleviate this restriction, we expand the features by incorporating information about the neighboring syllables. Based on the study in (Jeon and Liu, 2010) that evaluated using left and right contexts, we choose to use one previous and one following context in the features. The total number of features used in this study is 162. 3.2 Lexical-prosodic Features There is a very strong correlation between pitchaccent in an utterance and its lexical information. Previous studies have shown that the lexical features perform well for pitch-accent prediction. The detailed features for training the lexical-prosodic model are as follows. • Syllable identity: We kept syllables that appear more than 5 times in the training corpus. The other syllables that oc</context>
<context position="11470" citStr="Jeon and Liu, 2010" startWordPosition="1783" endWordPosition="1786">: We used vowel phone identity as a feature. • Lexical stress: This is a binary feature to represent if the syllable corresponds to a lexical stress based on the pronunciation dictionary. • Boundary information: This is a binary feature to indicate if there is a word boundary before the syllable. For lexical features, based on the study in (Jeon and Liu, 2010), we added two previous and two following contexts in the final features. 3.3 Prosodic Model Training We choose to use a support vector machine (SVM) classifier&apos; for the prosodic model based on previous work on prosody labeling study in (Jeon and Liu, 2010). We use RBF kernel for the acoustic model, and 3-order polynomial kernel for the lexical model. In our experiments, we investigate two kinds of training methods for prosodic modeling. The first one is a supervised method where models are trained using all the labeled data. The second is a semi-supervised method using co-training algorithm (Blum and Mitchell, 1998), described in Algorithm 1. Given a set L of labeled data and a set U of unlabeled data with two views, it then iterates in the 1LIBSVM – A Library for Support Vector Machines, location: http://www.csie.ntu.edu.tw/˜cjlin/libsvm/ 734 </context>
</contexts>
<marker>Jeon, Liu, 2010</marker>
<rawString>Je Hun Jeon and Yang Liu. 2010. Syllable-level prominence detection with acoustic evidence. Proc. of Interspeech, pages 1772–1775.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ozlem Kalinli</author>
<author>Shrikanth Narayanan</author>
</authors>
<title>Continuous speech recognition using attention shift decoding with soft decision.</title>
<date>2009</date>
<booktitle>Proc. of Interspeech,</booktitle>
<pages>pages</pages>
<contexts>
<context position="6677" citStr="Kalinli and Narayanan, 2009" startWordPosition="1013" endWordPosition="1016"> jointly modeled. Alternatively, prosody was modeled independently from the acoustic and language models of ASR and used to rescore recognition hypotheses in the second pass. This approach makes it possible to independently model and optimize the prosodic knowledge and to combine with ASR hypotheses without any modification of the conventional ASR modules. In order to improve the rescoring performance, various prosodic knowledge was studied. (Ananthakrishnan and Narayanan, 2007) used acoustic pitch-accent pattern and its sequential information given lexical cues to rescore n-best hypotheses. (Kalinli and Narayanan, 2009) used acoustic prosodic cues such as pitch and duration along with other knowledge to choose a proper word among several candidates in confusion networks. Prosodic boundaries based on acoustic cues were used in (Szaszak and Vicsi, 2007). We take a similar approach in this study as the second approach above in that we develop prosodic models separately and use them in a rescoring framework. Our proposed method differs from previous work in the way that the prosody model is used to help ASR. In our approach, we explicitly model the symbolic prosodic events based on acoustic and lexical informati</context>
</contexts>
<marker>Kalinli, Narayanan, 2009</marker>
<rawString>Ozlem Kalinli and Shrikanth Narayanan. 2009. Continuous speech recognition using attention shift decoding with soft decision. Proc. of Interspeech, pages 1927– 1930.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane J Litman</author>
<author>Julia B Hirschberg</author>
<author>Marc Swerts</author>
</authors>
<title>Predicting automatic speech recognition performance using prosodic cues.</title>
<date>2000</date>
<booktitle>Proc. of NAACL,</booktitle>
<pages>218--225</pages>
<contexts>
<context position="4873" citStr="Litman et al., 2000" startWordPosition="748" endWordPosition="751">oring approach in Section 4. Section 5 describes our corpus and baseline ASR setup. Section 6 presents our experiments and results. The last section gives a brief summary along with future directions. 2 Previous Work Prosody is of interest to speech researchers because it plays an important role in comprehension of spoken language by human listeners. The use of prosody in speech understanding applications has been quite extensive. A variety of applications have been explored, such as sentence and topic segmentation (Shriberg et al., 2000; Rosenberg and Hirschberg, 2006), word error detection (Litman et al., 2000), dialog act detection (Sridhar et al., 2009), speaker recognition (Shriberg et al., 2005), and emotion recognition (Benus et al., 2007), just to name a few. Incorporating prosodic knowledge is expected to improve the performance of speech recognition. However, how to effectively integrate prosody within the traditional ASR framework is a difficult problem, since prosodic features are not well defined and they come from a longer region, which is different from spectral features used in current ASR systems. Various research has been conducted trying to incorporate prosodic information in ASR. O</context>
</contexts>
<marker>Litman, Hirschberg, Swerts, 2000</marker>
<rawString>Diane J. Litman, Julia B. Hirschberg, and Marc Swerts. 2000. Predicting automatic speech recognition performance using prosodic cues. Proc. of NAACL, pages 218–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mari Ostendorf</author>
<author>Patti Price</author>
<author>Stefanie ShattuckHufnagel</author>
</authors>
<title>The Boston University radio news corpus. Linguistic Data Consortium.</title>
<date>1995</date>
<contexts>
<context position="17820" citStr="Ostendorf et al., 1995" startWordPosition="2854" endWordPosition="2857"> model likelihood (P(As|W) and P(W) in Equation 2). In practice, we need to weight them differently, therefore, the combined score for a hypothesis W is: Score(W) = λ · scoreW−prosody(W) + scoreASR(W) (5) where scoreASR(W) is generated by ASR systems (composed of acoustic and language model scores) and λ is optimized using held out data. 5 Data and Baseline Systems Our experiments are carried out using two different data sets and two different recognition systems as well in order to test the robustness of our proposed method. The first data set is the Boston University Radio News Corpus (BU) (Ostendorf et al., 1995), which consists of broadcast news style read speech. The BU corpus has about 3 hours of read speech from 7 speakers (3 female, 4 male). Part of the data has been labeled with ToBI-style prosodic annotations. In fact, the reason that we use this corpus, instead of other corpora typically used for ASR experiments, is because of its prosodic labels. We divided the entire data corpus into a training set and a test set. There was no speaker overlap between training and test sets. The training set has 2 female speakers (f2 and f3) and 3 male ones (m2, m3, m4). The test set is from the other two spe</context>
</contexts>
<marker>Ostendorf, Price, ShattuckHufnagel, 1995</marker>
<rawString>Mari Ostendorf, Patti Price, and Stefanie ShattuckHufnagel. 1995. The Boston University radio news corpus. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mari Ostendorf</author>
<author>Izhak Shafran</author>
<author>Rebecca Bates</author>
</authors>
<title>Prosody models for conversational speech recognition.</title>
<date>2003</date>
<booktitle>Proc. of the 2nd Plenary Meeting and Symposium on Prosody and Speech Processing,</booktitle>
<pages>147--154</pages>
<contexts>
<context position="5591" citStr="Ostendorf et al., 2003" startWordPosition="861" endWordPosition="864">emotion recognition (Benus et al., 2007), just to name a few. Incorporating prosodic knowledge is expected to improve the performance of speech recognition. However, how to effectively integrate prosody within the traditional ASR framework is a difficult problem, since prosodic features are not well defined and they come from a longer region, which is different from spectral features used in current ASR systems. Various research has been conducted trying to incorporate prosodic information in ASR. One way is to directly integrate prosodic features into the ASR framework (Vergyri et al., 2003; Ostendorf et al., 2003; Chen and Hasegawa-Johnson, 2006). Such efforts include prosody dependent acoustic and pronunciation model (allophones were distinguished according to different prosodic phenomenon), language model (words were augmented by prosody events), and duration modeling (different prosodic events were modeled separately and combined with conventional HMM). This kind of integration has advantages in that spectral and prosodic features are more tightly coupled and jointly modeled. Alternatively, prosody was modeled independently from the acoustic and language models of ASR and used to rescore recognitio</context>
</contexts>
<marker>Ostendorf, Shafran, Bates, 2003</marker>
<rawString>Mari Ostendorf, Izhak Shafran, and Rebecca Bates. 2003. Prosody models for conversational speech recognition. Proc. of the 2nd Plenary Meeting and Symposium on Prosody and Speech Processing, pages 147–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Rosenberg</author>
<author>Julia Hirschberg</author>
</authors>
<title>Story segmentation of broadcast news</title>
<date>2006</date>
<booktitle>in English, Mandarin and Arabic. Proc. of HLT-NAACL,</booktitle>
<pages>125--128</pages>
<contexts>
<context position="4829" citStr="Rosenberg and Hirschberg, 2006" startWordPosition="741" endWordPosition="744">accent detection. We provide details of our n-best rescoring approach in Section 4. Section 5 describes our corpus and baseline ASR setup. Section 6 presents our experiments and results. The last section gives a brief summary along with future directions. 2 Previous Work Prosody is of interest to speech researchers because it plays an important role in comprehension of spoken language by human listeners. The use of prosody in speech understanding applications has been quite extensive. A variety of applications have been explored, such as sentence and topic segmentation (Shriberg et al., 2000; Rosenberg and Hirschberg, 2006), word error detection (Litman et al., 2000), dialog act detection (Sridhar et al., 2009), speaker recognition (Shriberg et al., 2005), and emotion recognition (Benus et al., 2007), just to name a few. Incorporating prosodic knowledge is expected to improve the performance of speech recognition. However, how to effectively integrate prosody within the traditional ASR framework is a difficult problem, since prosodic features are not well defined and they come from a longer region, which is different from spectral features used in current ASR systems. Various research has been conducted trying t</context>
</contexts>
<marker>Rosenberg, Hirschberg, 2006</marker>
<rawString>Andrew Rosenberg and Julia Hirschberg. 2006. Story segmentation of broadcast news in English, Mandarin and Arabic. Proc. of HLT-NAACL, pages 125–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Shriberg</author>
<author>Andreas Stolcke</author>
<author>Dilek Hakkani-T¨ur</author>
<author>G¨okhan T¨ur</author>
</authors>
<title>Prosody-based automatic segmentation of speech into sentences and topics.</title>
<date>2000</date>
<journal>Speech Communication,</journal>
<pages>32--1</pages>
<marker>Shriberg, Stolcke, Hakkani-T¨ur, T¨ur, 2000</marker>
<rawString>Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-T¨ur, and G¨okhan T¨ur. 2000. Prosody-based automatic segmentation of speech into sentences and topics. Speech Communication, 32(1-2):127–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Shriberg</author>
<author>Luciana Ferrer</author>
<author>Sachin S Kajarekar</author>
<author>Anand Venkataraman</author>
<author>Andreas Stolcke</author>
</authors>
<title>Modeling prosodic feature sequences for speaker recognition. Speech Communication,</title>
<date>2005</date>
<pages>46--3</pages>
<contexts>
<context position="4963" citStr="Shriberg et al., 2005" startWordPosition="761" endWordPosition="764">ion 6 presents our experiments and results. The last section gives a brief summary along with future directions. 2 Previous Work Prosody is of interest to speech researchers because it plays an important role in comprehension of spoken language by human listeners. The use of prosody in speech understanding applications has been quite extensive. A variety of applications have been explored, such as sentence and topic segmentation (Shriberg et al., 2000; Rosenberg and Hirschberg, 2006), word error detection (Litman et al., 2000), dialog act detection (Sridhar et al., 2009), speaker recognition (Shriberg et al., 2005), and emotion recognition (Benus et al., 2007), just to name a few. Incorporating prosodic knowledge is expected to improve the performance of speech recognition. However, how to effectively integrate prosody within the traditional ASR framework is a difficult problem, since prosodic features are not well defined and they come from a longer region, which is different from spectral features used in current ASR systems. Various research has been conducted trying to incorporate prosodic information in ASR. One way is to directly integrate prosodic features into the ASR framework (Vergyri et al., </context>
</contexts>
<marker>Shriberg, Ferrer, Kajarekar, Venkataraman, Stolcke, 2005</marker>
<rawString>Elizabeth Shriberg, Luciana Ferrer, Sachin S. Kajarekar, Anand Venkataraman, and Andreas Stolcke. 2005. Modeling prosodic feature sequences for speaker recognition. Speech Communication, 46(3-4):455– 472.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivek Kumar Rangarajan Sridhar</author>
<author>Srinivas Bangalore</author>
<author>Shrikanth S Narayanan</author>
</authors>
<title>Exploiting acoustic and syntactic features for automatic prosody labeling in a maximum entropy framework.</title>
<date>2008</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>16</volume>
<issue>4</issue>
<contexts>
<context position="2177" citStr="Sridhar et al., 2008" startWordPosition="337" endWordPosition="340">sis, intention, attitude, and emotion. Humans listening to speech with natural prosody are able to understand the content with low cognitive load and high accuracy. However, most modern ASR systems only use an acoustic model and a language model. Acoustic information in ASR is represented by spectral features that are usually extracted over a window length of a few tens of milliseconds. They miss useful information contained in the prosody of the speech that may help recognition. Recently a lot of research has been done in automatic annotation of prosodic events (Wightman and Ostendorf, 1994; Sridhar et al., 2008; Ananthakrishnan and Narayanan, 2008; Jeon and Liu, 2009). They used acoustic and lexical-syntactic cues to annotate prosodic events with a variety of machine learning approaches and achieved good performance. There are also many studies using prosodic information for various spoken language understanding tasks. However, research using prosodic knowledge for speech recognition is still quite limited. In this study, we investigate leveraging prosodic information for recognition in an n-best rescoring framework. Previous studies showed that prosodic events, such as pitch-accent, are closely rel</context>
</contexts>
<marker>Sridhar, Bangalore, Narayanan, 2008</marker>
<rawString>Vivek Kumar Rangarajan Sridhar, Srinivas Bangalore, and Shrikanth S. Narayanan. 2008. Exploiting acoustic and syntactic features for automatic prosody labeling in a maximum entropy framework. IEEE Transactions on Audio, Speech, and Language Processing, 16(4):797–811.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivek Kumar Rangarajan Sridhar</author>
<author>Srinivas Bangalore</author>
<author>Shrikanth Narayanan</author>
</authors>
<title>Combining lexical, syntactic and prosodic cues for improved online dialog act tagging.</title>
<date>2009</date>
<journal>Computer Speech and Language,</journal>
<volume>23</volume>
<issue>4</issue>
<contexts>
<context position="4918" citStr="Sridhar et al., 2009" startWordPosition="755" endWordPosition="758">ibes our corpus and baseline ASR setup. Section 6 presents our experiments and results. The last section gives a brief summary along with future directions. 2 Previous Work Prosody is of interest to speech researchers because it plays an important role in comprehension of spoken language by human listeners. The use of prosody in speech understanding applications has been quite extensive. A variety of applications have been explored, such as sentence and topic segmentation (Shriberg et al., 2000; Rosenberg and Hirschberg, 2006), word error detection (Litman et al., 2000), dialog act detection (Sridhar et al., 2009), speaker recognition (Shriberg et al., 2005), and emotion recognition (Benus et al., 2007), just to name a few. Incorporating prosodic knowledge is expected to improve the performance of speech recognition. However, how to effectively integrate prosody within the traditional ASR framework is a difficult problem, since prosodic features are not well defined and they come from a longer region, which is different from spectral features used in current ASR systems. Various research has been conducted trying to incorporate prosodic information in ASR. One way is to directly integrate prosodic feat</context>
</contexts>
<marker>Sridhar, Bangalore, Narayanan, 2009</marker>
<rawString>Vivek Kumar Rangarajan Sridhar, Srinivas Bangalore, and Shrikanth Narayanan. 2009. Combining lexical, syntactic and prosodic cues for improved online dialog act tagging. Computer Speech and Language, 23(4):407–422.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Andreas Stolcke</author>
<author>Barry Chen</author>
<author>Horacio Franco</author>
</authors>
<title>Venkata Ramana Rao Gadde, Martin Graciarena, Mei-Yuh Hwang, Katrin Kirchhoff,</title>
<date>2006</date>
<journal>IEEE Transactions on Audio, Speech and Language Processing,</journal>
<volume>14</volume>
<issue>5</issue>
<location>Arindam Mandal, Nelson Morgan, Xin Lin, Tim Ng, Mari Ostendorf, Kemal S¨onmez, Anand Venkataraman, Dimitra Vergyri, Wen</location>
<note>Special Issue on Progress in Rich Transcription.</note>
<contexts>
<context position="19624" citStr="Stolcke et al., 2006" startWordPosition="3169" endWordPosition="3172"> with the broadcast new data. A standard back-off trigram language model with Kneser-Ney smoothing was trained using the combined text from the training partition of the BU, Wall Street Journal data, and part of Gigaword corpus. The vocabulary size was about 10K words and the out-of-vocabulary (OOV) rate on the test set was 2.1%. The second data set is from broadcast news (BN) speech used in the GALE program. The recognition test set contains 1,001 utterances. The n-best hypotheses for this data set are generated by a state-ofthe-art SRI speech recognizer, developed for broadcast news speech (Stolcke et al., 2006; Zheng et al., 2007). This system yields much better performance than the first one. We also divided the test set roughly equally into two sets for parameter tuning and testing. From the data used for training the speech recognizer, we randomly selected 5.7 hours of speech (4,234 utterances) for the co-training algorithm for the prosodic models. For prosodic models, we used a simple binary representation of pitch-accent in the form of presence versus absence. The reference labels are de2CMU Sphinx - Speech Recognition Toolkit, location: http://www.speech.cs.cmu.edu/sphinx/tutorial.html 736 ri</context>
</contexts>
<marker>Stolcke, Chen, Franco, 2006</marker>
<rawString>Andreas Stolcke, Barry Chen, Horacio Franco, Venkata Ramana Rao Gadde, Martin Graciarena, Mei-Yuh Hwang, Katrin Kirchhoff, Arindam Mandal, Nelson Morgan, Xin Lin, Tim Ng, Mari Ostendorf, Kemal S¨onmez, Anand Venkataraman, Dimitra Vergyri, Wen Wang, Jing Zheng, and Qifeng Zhu. 2006. Recent innovations in speech-to-text transcription at SRI-ICSIUW. IEEE Transactions on Audio, Speech and Language Processing, 14(5):1729–1744. Special Issue on Progress in Rich Transcription.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gyorgy Szaszak</author>
<author>Klara Vicsi</author>
</authors>
<title>Speech recognition supported by prosodic information for fixed stress languages.</title>
<date>2007</date>
<booktitle>Proc. of TSD Conference,</booktitle>
<pages>262--269</pages>
<contexts>
<context position="6913" citStr="Szaszak and Vicsi, 2007" startWordPosition="1050" endWordPosition="1053">ze the prosodic knowledge and to combine with ASR hypotheses without any modification of the conventional ASR modules. In order to improve the rescoring performance, various prosodic knowledge was studied. (Ananthakrishnan and Narayanan, 2007) used acoustic pitch-accent pattern and its sequential information given lexical cues to rescore n-best hypotheses. (Kalinli and Narayanan, 2009) used acoustic prosodic cues such as pitch and duration along with other knowledge to choose a proper word among several candidates in confusion networks. Prosodic boundaries based on acoustic cues were used in (Szaszak and Vicsi, 2007). We take a similar approach in this study as the second approach above in that we develop prosodic models separately and use them in a rescoring framework. Our proposed method differs from previous work in the way that the prosody model is used to help ASR. In our approach, we explicitly model the symbolic prosodic events based on acoustic and lexical information. We then capture the correlation of pitch-accent patterns between the two different cues, and use that to improve recognition performance in an n-best rescoring paradigm. 3 Prosodic Model Among all the prosodic events, we use only pi</context>
</contexts>
<marker>Szaszak, Vicsi, 2007</marker>
<rawString>Gyorgy Szaszak and Klara Vicsi. 2007. Speech recognition supported by prosodic information for fixed stress languages. Proc. of TSD Conference, pages 262–269.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Dimitra Vergyri</author>
<author>Andreas Stolcke</author>
<author>Venkata R R Gadde</author>
</authors>
<marker>Vergyri, Stolcke, Gadde, </marker>
<rawString>Dimitra Vergyri, Andreas Stolcke, Venkata R. R. Gadde,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luciana Ferrer</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Prosodic knowledge sources for automatic speech recognition.</title>
<date>2003</date>
<booktitle>Proc. of ICASSP,</booktitle>
<pages>208--211</pages>
<marker>Ferrer, Shriberg, 2003</marker>
<rawString>Luciana Ferrer, and Elizabeth Shriberg. 2003. Prosodic knowledge sources for automatic speech recognition. Proc. of ICASSP, pages 208–211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin W Wightman</author>
<author>Mari Ostendorf</author>
</authors>
<title>Automatic labeling of prosodic patterns.</title>
<date>1994</date>
<booktitle>IEEE Transaction on Speech and Auido Processing,</booktitle>
<pages>2--4</pages>
<contexts>
<context position="2155" citStr="Wightman and Ostendorf, 1994" startWordPosition="333" endWordPosition="336">stic information such as emphasis, intention, attitude, and emotion. Humans listening to speech with natural prosody are able to understand the content with low cognitive load and high accuracy. However, most modern ASR systems only use an acoustic model and a language model. Acoustic information in ASR is represented by spectral features that are usually extracted over a window length of a few tens of milliseconds. They miss useful information contained in the prosody of the speech that may help recognition. Recently a lot of research has been done in automatic annotation of prosodic events (Wightman and Ostendorf, 1994; Sridhar et al., 2008; Ananthakrishnan and Narayanan, 2008; Jeon and Liu, 2009). They used acoustic and lexical-syntactic cues to annotate prosodic events with a variety of machine learning approaches and achieved good performance. There are also many studies using prosodic information for various spoken language understanding tasks. However, research using prosodic knowledge for speech recognition is still quite limited. In this study, we investigate leveraging prosodic information for recognition in an n-best rescoring framework. Previous studies showed that prosodic events, such as pitch-a</context>
</contexts>
<marker>Wightman, Ostendorf, 1994</marker>
<rawString>Colin W. Wightman and Mari Ostendorf. 1994. Automatic labeling of prosodic patterns. IEEE Transaction on Speech and Auido Processing, 2(4):469–481.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Zheng</author>
<author>Ozgur Cetin</author>
<author>Mei-Yuh Hwang</author>
<author>Xin Lei</author>
<author>Andreas Stolcke</author>
<author>Nelson Morgan</author>
</authors>
<title>Combining discriminative feature, transform, and model training for large vocabulary speech recognition.</title>
<date>2007</date>
<booktitle>Proc. of ICASSP,</booktitle>
<pages>633--636</pages>
<contexts>
<context position="19645" citStr="Zheng et al., 2007" startWordPosition="3173" endWordPosition="3176">w data. A standard back-off trigram language model with Kneser-Ney smoothing was trained using the combined text from the training partition of the BU, Wall Street Journal data, and part of Gigaword corpus. The vocabulary size was about 10K words and the out-of-vocabulary (OOV) rate on the test set was 2.1%. The second data set is from broadcast news (BN) speech used in the GALE program. The recognition test set contains 1,001 utterances. The n-best hypotheses for this data set are generated by a state-ofthe-art SRI speech recognizer, developed for broadcast news speech (Stolcke et al., 2006; Zheng et al., 2007). This system yields much better performance than the first one. We also divided the test set roughly equally into two sets for parameter tuning and testing. From the data used for training the speech recognizer, we randomly selected 5.7 hours of speech (4,234 utterances) for the co-training algorithm for the prosodic models. For prosodic models, we used a simple binary representation of pitch-accent in the form of presence versus absence. The reference labels are de2CMU Sphinx - Speech Recognition Toolkit, location: http://www.speech.cs.cmu.edu/sphinx/tutorial.html 736 rived from the ToBI ann</context>
</contexts>
<marker>Zheng, Cetin, Hwang, Lei, Stolcke, Morgan, 2007</marker>
<rawString>Jing Zheng, Ozgur Cetin, Mei-Yuh Hwang, Xin Lei, Andreas Stolcke, and Nelson Morgan. 2007. Combining discriminative feature, transform, and model training for large vocabulary speech recognition. Proc. of ICASSP, pages 633–636.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>