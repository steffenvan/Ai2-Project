<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000022">
<title confidence="0.471878">
Unsupervised Learning on an Approximate Corpus*
</title>
<author confidence="0.616608">
Jason Smith and Jason Eisner
</author>
<affiliation confidence="0.5121">
Center for Language and Speech Processing
Johns Hopkins University
3400 N. Charles St., Baltimore, MD 21218, USA
</affiliation>
<email confidence="0.921189">
{jsmith,jason}@cs.jhu.edu
</email>
<bodyText confidence="0.991486875">
Unsupervised learning techniques can take advan-
tage of large amounts of unannotated text, but the
largest text corpus (the Web) is not easy to use in
its full form. Instead, we have statistics about this
corpus in the form of n-gram counts (Brants and
Franz, 2006). While n-gram counts do not directly
provide sentences, a distribution over sentences can
be estimated from them in the same way that n-
gram language models are estimated. We treat this
distribution over sentences as an approximate cor-
pus and show how unsupervised learning can be
performed on such a corpus using variational infer-
ence. We compare hidden Markov model (HMM)
training on exact and approximate corpora of vari-
ous sizes, measuring speed and accuracy on unsu-
pervised part-of-speech tagging.
</bodyText>
<sectionHeader confidence="0.998492" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.991029291666667">
We consider the problem of training generative mod-
els on very large datasets in sublinear time. It is well
known how to train an HMM to maximize the like-
lihood of a corpus of sentences. Here we show how
to train faster on a distribution over sentences that
compactly approximates the corpus. The distribu-
tion is given by an 5-gram backoff language model
that has been estimated from statistics of the corpus.
In this paper, we demonstrate our approach on
a traditional testbed for new structured-prediction
learning algorithms, namely HMMs. We focus on
unsupervised learning. This serves to elucidate the
structure of our variational training approach, which
stitches overlapping n-grams together rather than
treating them in isolation. It also confirms that at
least in this case, accuracy is not harmed by the
key approximations made by our method. In future,
we hope to scale up to the Google n-gram corpus
(Brants and Franz, 2006) and learn a more detailed,
explanatory joint model of tags, syntactic dependen-
cies, and topics. Our intuition here is that web-scale
data may be needed to learn the large number of lex-
ically and contextually specific parameters.
&amp;quot;Work was supported in part by NSF grant No. 0347822.
</bodyText>
<subsectionHeader confidence="0.955932">
1.1 Formulation
</subsectionHeader>
<bodyText confidence="0.998894125">
Let w (“words”) denote an observation sequence,
and let t (“tags”) denote a hidden HMM state se-
quence that may explain w. This terminology is
taken from the literature on inducing part-of-speech
(POS) taggers using a first-order HMM (Merialdo,
1994), which we use as our experimental setting.
Maximum a posteriori (MAP) training of an
HMM pg seeks parameters 0 to maximize
</bodyText>
<equation confidence="0.992086">
p0(w, t) + log Pr prior(0) (1)
</equation>
<bodyText confidence="0.999990666666667">
where c is an empirical distribution that assigns
probability 1/N to each of the N sentences in a
training corpus. Our technical challenge is to gen-
eralize this MAP criterion to other, structured dis-
tributions c that compactly approximate the corpus.
Specifically, we address the case where c is given
by any probabilistic FSA, such as a backoff lan-
guage model—that is, a variable-order Markov
model estimated from corpus statistics. Similar sen-
tences w share subpaths in the FSA and cannot eas-
ily be disentangled. The support of c is typically infi-
nite (for a cyclic FSA) or at least exponential. Hence
it is no longer practical to compute the tagging distri-
bution p(t  |w) for each sentence w separately, as in
traditional MAP-EM or gradient ascent approaches.
We will maximize our exact objective, or a cheaper
variational approximation to it, in a way that cru-
cially allows us to retain the structure-sharing.
</bodyText>
<subsectionHeader confidence="0.939687">
1.2 Motivations
</subsectionHeader>
<bodyText confidence="0.9999795">
Why train from a distribution rather than a corpus?
First, the foundation of statistical NLP is distribu-
tions over strings that are specified by weighted au-
tomata and grammars. We regard parameter estima-
tion from such a distribution c (rather than from a
sample) as a natural question. Previous work on
modeling c with a distribution from another fam-
ily was motivated by approximating a grammar or
</bodyText>
<figure confidence="0.842627666666667">
� �c(w) log
N · t
w
</figure>
<page confidence="0.651270666666667">
131
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 131–141,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.999405112676057">
model rather than generalizing from a dataset, and
hence removed latent variables while adding param-
eters (Nederhof, 2000; Mohri and Nederhof, 2001;
Liang et al., 2008), whereas we do the reverse.
Second, in practice, one may want to incorporate
massive amounts of (possibly out-of-domain) data
in order to get better coverage of phenomena. Mas-
sive datasets usually require a simple model (given a
time budget). We propose that it may be possible to
use a lot of data and a good model by reducing the
accuracy of the data representation instead. While
training will become more complicated, it can still
result in an overall speedup, because a frequent 5-
gram collapses into a single parameter of the esti-
mated distribution that only needs to be processed
once per training iteration. By pruning low-count
n-grams or reducing the maximum n below 5, one
can further increase data volume for the fixed time
budget at the expense of approximation quality.
Third, one may not have access to the original
corpus. If one lacks the resources to harvest the
web, the Google n-gram corpus was derived from
over a trillion words of English web text. Privacy
or copyright issues may prevent access, but one may
still be able to work with n-gram statistics: Michel
et al. (2010) used such statistics from 5 million
scanned books. Several systems use n-gram counts
(Bergsma et al., 2009; Lin et al., 2009) or other
web statistics (Lapata and Keller, 2005) as features
within a classifier. A large language model from n-
gram counts yields an effective prior over hypothe-
ses in tasks like machine translation (Brants et al.,
2007). We similarly construct an n-gram model, but
treat it as the primary training data whose structure
is to be explained by the generative HMM. Thus our
criterion does not explain the n-grams in isolation,
but rather tries to explain the likely full sentences
w that the model reconstructed from overlapping n-
grams. This is something like shotgun sequencing,
in which likely DNA strings are reconstructed from
overlapping short reads (Staden, 1979); however, we
train an HMM on the resulting distribution rather
than merely trying to find its mode.
Finally, unsupervised HMM training discovers la-
tent structure by approximating an empirical distri-
bution c (the corpus) with a latent-variable distribu-
tion p (the trained HMM) that has fewer parameters.
We show how to do the same where the distribution
c is not a corpus but a finite-state distribution. In
general, this finite-state c could represent some so-
phisticated estimate of the population distribution,
using shrinkage, word classes, neural-net predictors,
etc. to generalize in some way beyond the training
sample before fitting p. For the sake of speed and
clear comparison, however, our present experiments
take c to be a compact approximation to the sample
distribution, requiring only n-grams.
Spectral learning of HMMs (Hsu et al., 2009)
also learns from a collection of n-grams. It has the
striking advantage of converging globally to the true
HMM parameters (under a certain reparameteriza-
tion), with enough data and under certain assump-
tions. However, it does not exploit context beyond
a trigram (it will not maximize, even locally, the
likelihood of a finite sample of sentences), and can-
not exploit priors or structure—e.g., that the emis-
sions are consistent with a tag dictionary or that the
transitions encode a higher-order or factorial HMM.
Our more general technique extends to other latent-
variable models, although it suffers from variational
EM’s usual local optima and approximation errors.
</bodyText>
<sectionHeader confidence="0.878416" genericHeader="method">
2 A variational lower bound
</sectionHeader>
<bodyText confidence="0.99947025">
Our starting point is the variational EM algorithm
(Jordan et al., 1999). Recall that this maximizes a
lower bound on the MAP criterion of equation 1, by
bounding the log-likelihood subterm as follows:
</bodyText>
<equation confidence="0.99949925">
log Et po(w, t) (2)
= log Et q(t)(po(w, t)/q(t))
&gt; Et q(t) log(po(w, t)/q(t))
= E,(t)[log po(w, t) − log q(t) (3)
</equation>
<bodyText confidence="0.999207076923077">
This use of Jensen’s inequality is valid for any distri-
bution q. As Neal and Hinton (1998) show, the EM
algorithm (Dempster et al., 1977) can be regarded
as locally maximizing the resulting lower bound by
alternating optimization, where q is a free parame-
ter. The E-step optimizes q for fixed 0, and the M-
step optimizes 0 for fixed q. These computations are
tractable for HMMs, since the distribution q(t) =
po(t  |w) that is optimal at the E-step (which makes
the inequality tight) can be represented as a lattice
(a certain kind of weighted DFA), and this makes
the M-step tractable via the forward-backward algo-
rithm. However, there are many extensions such as
</bodyText>
<page confidence="0.996793">
132
</page>
<bodyText confidence="0.99918772">
factorial HMMs and Bayesian HMMs in which an
expectation under pθ(t  |w) involves an intractable
sum. In this setting, one may use variational EM, in
which q is restricted to some parametric family qφ
that will permit a tractable M-step. In this case the
E-step chooses the optimal values of the variational
parameters 0; the inequality is no longer tight.
There are two equivalent views of how this pro-
cedure is applied to a training corpus. One view is
that the corpus log-likelihood is just as in (2), where
w is taken to be the concatenation of all training
sentences. The other view is that the corpus log-
likelihood is a sum over many terms of the form (2),
one for each training sentence w, and we bound each
summand individually using a different qφ.
However, neither view leads to a practical imple-
mentation in our setting. We can neither concatenate
all the relevant w nor loop over them, since we want
the expectation of (2) under some distribution c(w)
such that {w : c(w) &gt; 01 is very large or infinite.
Our move is to make q be a conditional distribution
q(t  |w) that applies to all w at once. The follow-
ing holds by applying Jensen’s inequality separately
to each w in the expectation (this is valid since for
each w, q(t  |w) is a distribution):
</bodyText>
<equation confidence="0.997573">
Ec(w) log Et pθ(w, t) (4)
= Ec(w) log Et q(t  |w)(pθ(w, t)/q(t  |w))
E
&gt; Ec(w) t q(t  |w) log(pθ(w, t)/q(t  |w))
= Ecq(w,t)[log pθ(w, t) − log q(t  |w)] (5)
</equation>
<bodyText confidence="0.999930375">
where we use cq(w, t) to denote the joint distribu-
tion c(w) · q(t  |w). Thus, just as c is our approx-
imate corpus, cq is our approximate tagged corpus.
Our variational parameters 0 will be used to param-
eterize cq directly. To ensure that cqφ can indeed
be expressed as c(w) · q(t  |w), making the above
bound valid, it suffices to guarantee that our varia-
tional family preserves the marginals:
</bodyText>
<equation confidence="0.835996">
(bw) Et cqφ(w, t) = c(w)
</equation>
<sectionHeader confidence="0.920923" genericHeader="method">
3 Finite-state encodings and algorithms
</sectionHeader>
<bodyText confidence="0.993342857142857">
In the following, we will show how to maximize
(5) for particular families of p, c, and cq that can
be expressed using finite-state machines (FSMs)—
that is, finite-state acceptors (FSAs) and transducers
(FSTs). This general presentation of our method en-
ables variations using other FSMs.
A path in an FSA accepts a string. In an FST,
each arc is labeled with a “word : tag” pair, so that a
path accepts a string pair (w, t) obtained by respec-
tively concatenating the words and the tags encoun-
tered along the path. Our FSMs are weighted in the
(+, x) semiring: the weight of any path is the prod-
uct (x) of its arc weights, while the weight assigned
to a string or string pair is the total weight (+) of all
its accepting paths. An FSM is unambiguous if each
string or string pair has at most one accepting path.
Figure 1 reviews how to represent an HMM POS
tagger as an FST (b), and how composing this with
an FSA that accepts a single sentence gives us the
familiar HMM tagging lattice as an FST (c). The
forward-backward algorithm sums over paths in the
lattice via dynamic programming (Rabiner, 1989).
In section 3.1, we replace the straight-line FSA
of Figure 1a with an FSA that defines a more gen-
eral distribution c(w) over many sentences. Note
that we cannot simply use this as a drop-in replace-
ment in the construction of Figure 1. That would
correspond to running EM on a single but uncer-
tain sentence (distributed as c(w)) rather than a col-
lection of observed sentences. For example, in the
case of an ordinary training corpus of N sentences,
the new FSA would be a parallel union (sum) of
N straight-line paths—rather than a serial concate-
nation (product) of those paths as in ordinary EM
(see above). Running the forward algorithm on the
</bodyText>
<equation confidence="0.580451">
E
</equation>
<bodyText confidence="0.843629">
resulting lattice would compute Ec(w) t p(w, t),
</bodyText>
<equation confidence="0.9515525">
E
whose log is log Ec(w) t p(w, t) rather than our
</equation>
<bodyText confidence="0.98033275">
desired Ec(w) log Et p(w, t). Instead, we use c in
section 3.2 to construct a variational family cqφ. We
then show in sections 3.3–3.5 how to compute and
locally maximize the variational lower bound (5).
</bodyText>
<subsectionHeader confidence="0.999973">
3.1 Modeling a corpus with n-gram counts
</subsectionHeader>
<bodyText confidence="0.997517272727273">
n-gram backoff language models have been used for
decades in automatic speech recognition and statis-
tical machine translation. We follow the usual FSA
construction (Allauzen et al., 2003). The state of a 5-
gram FSA model c(w) must remember the previous
4-gram. For example, it would include an arc from
state defg (the previous 4-gram) to state efgh with
label h and weight c(h  |defg). Then, with appro-
priate handling of boundary conditions, a sentence
w = ... defghi ... is accepted along a single path of
weight c(w) = · · · c(h  |defg) · c(i  |efgh) · · · . Arcs
</bodyText>
<page confidence="0.998422">
133
</page>
<figureCaption confidence="0.926261333333333">
Figure 1: Ordinary HMM tagging with finite-state machines. An arc’s label may have up to three components:
“word:tag / weight.” (Weights are suppressed for space. State labels are not part of the machine but suggest the history
recorded by each state.) (a) w is an FSA that generates the sentence “Time flies like an arrow”; all arcs have probability
</figureCaption>
<figure confidence="0.953437696969697">
1. (b) p(w, t) is an FST representing an HMM (many arcs are not shown and words are abbreviated as “w”). Each arc
w : t is weighted by the product of transition and emission probabilities, p(t  |previous t) · p(w  |t). Composing (a)
with (b) yields (c), an FST that encodes the joint probabilities p(w, t) of all possible taggings of the sentence w.
w:V
DT
(a) w
Time flies like an arrow
w:N
N
Stop
w:V
(b) p(w,t) Start V
w:V
w:DT
flies : V
like : Prep
(c) w o p(w,t) Start
Time : V
Time : N
V like : V
like : Prep
N like : V
V
N
flies : N
flies : V
flies : N
Prep
V
an : DT
arrow : N
DT N
an : DT
</figure>
<bodyText confidence="0.998420057692308">
of weight 0 can be omitted from the FSA.1
To estimate a conditional probability like c(h |
defg) above, we simply take an unsmoothed ratio of
two n-gram counts. This ML estimation means that
c will approximate as closely as possible the train-
ing sample from which the counts were drawn. That
gives a fair comparison with ordinary EM, which
trains directly on that sample. (See discussion at the
end of section 1.2 for alternatives.)
Yet we decline to construct a full 5-gram model,
which would not be as compact as desired. A col-
lection of all web 5-grams would be nearly as large
as the web itself (by Zipf’s Law). We may not have
such a collection. For example, the Google n-gram
corpus version 2 contains counts only for 1-grams
that appear at least 40 times and 2-, 3-, 4-, and 5-
grams that appear at least 10 times (Lin et al., 2009).
1The FSA’s initial state is the unigram history #, and its final
states (which have no outgoing arcs) are the other states whose
n-gram labels end in #. Here # is a boundary symbol that falls
between sentences. To compute the weighted transitions, sen-
tence boundaries must be manually or automatically annotated,
either on the training corpus as in our present experiments, or
directly on the training n-grams if we have only those.
To automatically find boundaries in an n-gram collection,
one could apply a local classifier to each n-gram. But in princi-
ple, one could exploit more context and get a globally consistent
annotation by stitching the n-grams together and applying the
methods of this paper—replacing po with an existing CRF sen-
tence boundary detector, replacing c with a document-level (not
sentence-level) language model, and optimizing cqy to be a ver-
sion of c that is probabilistically annotated with sentence bound-
aries, which yields our desired distribution over sentences.
Instead, we construct a backoff language model.
This FSA has one arc for each n-gram in the col-
lection. Our algorithm’s runtime (per iteration) will
be linear in the number of arcs. If the 5-gram defgh
is not in our collection, then there can be no h arc
leaving defg. When encountering h in state defg, the
automaton will instead take a failure arc (Allauzen
et al., 2003) to the “backoff state” efg. It may be
able to consume the h from that state, on an arc with
weight c(h  |efg); or it may have to back off further
to fg. Each state’s failure arc is weighted such that
the state’s outgoing arcs sum to 1. It is labeled with
the special symbol 4b, which does not contribute to
the word string accepted along a path.
We take care never to allow backoff to the empty
state E,2 since we find that c(w) is otherwise too
coarse an approximation to English: sampled sen-
tences tend to be disjointed, with some words gener-
ated in complete ignorance of their left context.
</bodyText>
<subsectionHeader confidence="0.999416">
3.2 The variational distribution cq(w, t)
</subsectionHeader>
<bodyText confidence="0.9987202">
The “variational gap” between (4) and (5) is
E&apos; c(w)KL(q(t  |w)  ||po(t  |w)). That is, the bound
is good if q does a good job of approximating po’s
tagging distribution on a randomly drawn sentence.
Note that n−1 is the order of our n-gram Markov
2To prevent such backoff, it suffices to include all 2-grams
with count &gt; 0. But where the full collection of 2-grams is
unavailable or too large, one can remove the empty state (and
recursively remove all states that transition only to removed
states), and then renormalize the model locally or globally.
</bodyText>
<page confidence="0.994513">
134
</page>
<bodyText confidence="0.999899571428571">
model c(w) (i.e., each word is chosen given the pre-
vious n − 1 words). Let np − 1 be the order of the
HMM pθ(w, t) that we are training: i.e., each tag is
chosen given the previous np − 1 tags. Our experi-
ments take np = 2 (a bigram HMM) as in Figure 1.
We will take qφ(t  |w) to be a conditional Markov
model of order nq − 1.3 It will predict the tag at po-
sition i using a multinomial conditioned on the pre-
ceding nq −1 tags and on the word n-gram ending at
position i (where n is as large as possible such that
this n-gram is in our training collection). 0 is the
collection of all multinomial parameters.
If nq = np, then our variational gap can be made 0
as in ordinary non-variational EM (see section 3.5).
In our experiments, however, we save memory by
choosing nq = 1. Thus, our variational gap is tight
to the extent that a word’s POS tag under the model
pθ is conditionally independent of previous tags and
the rest of the sentence, given an n-word window.4
This is the assumption made by local classification
models (Punyakanok et al., 2005; Toutanova and
Johnson, 2007). Note that it is milder than the “one
tagging per n-gram” hypothesis (Dawborn and Cur-
ran, 2009; Lin et al., 2009), which claims that each
5-gram (and therefore each sentence!) is unambigu-
ous as to its full tagging. In contrast, we allow that
a tag may be ambiguous even given an n-word win-
dow; we merely suppose that there is no further dis-
ambiguating information accessible to pθ.5
We can encode the resulting cq(w, t) as an FST.
With nq = 1, the states of cq are isomorphic to the
states of c. However, an arc in c from defg with
label h and weight 0.2 is replaced in cq by several
arcs—one per tag t—with label h : t and weight
0.2 · qφ(t  |defgh).6 We remark that an encoding of
</bodyText>
<footnote confidence="0.854294333333333">
3A conditional Markov model is a simple case of a
maximum-entropy Markov model (McCallum et al., 2000).
4At present, the word being tagged is the last word in the
window. We do have an efficient modification in which the win-
dow is centered on the word, by using an FST cq that delays the
emission of a tag until up to 2 subsequent words have been seen.
5With difficulty, one can construct English examples that
violate our assumption. (1) “Some monitor lizards from
Africa... ” versus “Some monitor lizards from a distance... ”:
there are words far away from “monitor” that help disambiguate
whether “monitor” is a noun or a verb. (“Monitor lizards” are
a species, but some people like to monitor lizards.) (2) “Time
flies”: “flies” is more likely to be a noun if “time” is a verb.
6In the case n9 &gt; 1, the states of c would need to be split
in order to remember n9 − 1 tags of history. For example, if
</footnote>
<bodyText confidence="0.999605857142857">
q(t  |w) as an FST would be identical except for
dropping the c factor (e.g., 0.2) from each weight.
Composing c o q would then recover cq.
This construction associates one variational pa-
rameter in 0 with each arc in cq—that is, with each
pair (arc in c, tag t), if nq = 1. There would be lit-
tle point in sharing these parameters across arcs of
cq, as that would reduce the expressiveness of the
variational distribution without reducing runtime.7
Notice that maximizing equation (5) jointly learns
not only a compact slow HMM tagger pθ, but also a
large fast tagger qφ that simply memorizes the likely
tags in each n-gram context. This is reminiscent of
structure compilation (Liang et al., 2008).
</bodyText>
<subsectionHeader confidence="0.998276">
3.3 Computing the variational objective
</subsectionHeader>
<bodyText confidence="0.999636176470589">
The expectation in equation (5) can now be com-
puted efficiently and elegantly by dynamic program-
ming over the FSMs, for a given 0 and 0.
We exploit our representation of cqφ as an FSM
over the (+, x) semiring. The path weights repre-
sent a probability distribution over the paths. In gen-
eral, it is efficient to compute the expected value of
a random FSM path, for any definition of value that
decomposes additively over the path’s arcs. The ap-
proach is to apply the forward algorithm to a version
of cqφ where we now regard each arc as weighted
by an ordered pair of real numbers. The (+, x) op-
erations for combining weights (section 3) are re-
placed with the operations of an “expectation semir-
ing” whose elements are such pairs (Eisner, 2002).
Suppose we want to find Ecq, (w t) log qφ (t  |w).
To reduce this to an expected value problem, we
must assign a value to each arc of cqφ such that the
c is Figure 1a, splitting its states with n9 = 2 would yield a
cq with a topology like Figure 1c, but with each arc having an
independent variational parameter.
7One could increase the number of arcs and hence varia-
tional parameters by splitting the states of cq to remember more
history. In particular, one could increase the width n9 of the tag
window, or one could increase the width of the word window by
splitting states of c (without changing the distribution c(w)).
Conversely, one could reduce the number of variational pa-
rameters by further restricting the variational family. For exam-
ple, requiring q(t  |w) to have entropy 0 (analogous to “hard
EM” or “Viterbi EM”) would associate a single deterministic
tag with each arc of c. This is fast, makes cq as compact as c,
and is still milder than “one tagging per n-gram.” More gener-
ously, one could allow up to 2 tags per arc of c, or use a low-
dimensional representation of the arc’s distribution over tags.
</bodyText>
<page confidence="0.995264">
135
</page>
<bodyText confidence="0.972702545454545">
total value of a path accepting (w, t) is log qφ(t |
w). Thus, let the value of each arc in cqφ be the log
of its weight in the isomorphic FST qφ(t  |w).8
We introduce some notation to make this precise.
A state of cqφ is a pair of the form [hc, hq], where hc
is a state of c (e.g., an (n − 1)-word history) and hq
is an (nq − 1)-tag history. We saw in the previous
section that an arc a leaving this state, and labeled
with w : t where w is a word and t is a tag, will
def
have a weight of the form ka = c(w  |hc)Oa where
</bodyText>
<equation confidence="0.9154152">
def def
�a = qφ(t  |hcw, hq). We now let the value va =
log Oa.9 Then, just as the weight of a path accepting
(w, t) is Ha ka = cqφ(w, t), the value of that path
is Ea va = log qφ(t  |w), as desired.
</equation>
<bodyText confidence="0.979308195652174">
To compute the expected value r over all paths,
we follow a generalized forward-backward recipe
(Li and Eisner, 2009, section 4.2). First, run the for-
ward and backward algorithms over cqφ.10 Now the
expected value is a sum over all arcs of cqφ, namely
r = Ea αakavaQa, where αa denotes the forward
probability of arc a’s source state and Qa denotes
the backward probability of arc a’s target state.
Now, in fact, the expectation we need to compute
is not Ecqφ(w,t) log qφ(t  |w) but rather equation (5).
So the value va of arc a should not actually be
def
log �a but rather log �a − log �a where �a = pθ(t |
8The total value is then the sum of the logs, i.e., the log
of the product. This works because qφ is unambiguous, i.e., it
computes qφ(t  |w) as a product along a single accepting path,
rather than summing over multiple paths.
9The special case of a failure arc a goes from [hc, hq] to
[h0c, hq], where h0 c is a backed-off version of hc. It is labeled
with 4) : E, which does not contribute to the word string or
tag string accepted along a path. Its weight ka is the weight
c(d)  |hc) of the corresponding failure arc in c from hc to h0c.
def
We define va = 0, so it does not contribute to the total value.
10Recall that the forward probability of each state is defined
recursively from the forward probabilities of the states that have
arcs leading to it. As our FST is cyclic, it is not possible to visit
the states in topologically sorted order. We instead solve these
simultaneous equations by a relaxation algorithm (Eisner, 2002,
section 5): repeatedly sweep through all states, updating their
forward probability, until the total forward probability of all fi-
nal states is close to the correct total of 1 = Ew,t cqφ(w, t)
(showing that we have covered all high-prob paths). A corre-
sponding backward relaxation is actually not needed yet (we do
need it for , in section 3.4): backward probabilities are just 1,
since cqφ is constructed with locally normalized probabilities.
When we rerun the forward-backward algorithm after a pa-
rameter update, we use the previous solution as a starting point
for the relaxation algorithm. This greatly speeds convergence.
hp) · pθ(w  |t). This is a minor change—except that
va now depends on hp, which is the history of np −1
previous tags. If np &gt; nq, then a’s start state does
not store such a long history. Thus, the value of a
actually depends on how one reaches a! It is prop-
erly written as vza, where za is a path ending with
a and z is sufficiently long to determine hp.11
Formally, let Za be a “partitioning” set of paths to
a, such that any path in cqφ from an initial state to
the start state of a must have exactly one z E Za as
a suffix, and each z E Za is sufficiently long so that
vza is well-defined. We can now find the expected
value as r = Ea EzEZa αz MzEz kz) kavzaQa.
The above method permits pθ to score the tag se-
quences of length np that are hypothesized by cqφ.
One can regard it as implicitly running the general-
ized forward-backward algorithm over a larger FST
that marries the structure of cqφ with the np-gram
HMM structure,12 so that each value is again local to
a single arc za. However, it saves space by working
directly on cqφ (which has manageable size because
we deliberately kept nq small), rather than material-
izing the larger FST (as bad as increasing nq to np).
The Za trick uses O(CTnq) rather than O(CTnp)
space to store the FST, where C is the number of
arcs in c (= number of training n-grams) and T is
the number of tag types. With or without the trick,
runtime is O(CTnp+BCTnq), where B is the num-
11By concatenating z’s start state’s hq with the tags along z.
Typically z has length np − nq (and Za consists of the paths
of that length to a’s start state). However, z may be longer if it
contains 4) arcs, or shorter if it begins with an initial state.
12Constructed by lazy finite-state intersection of cqφ and pθ
(Mohri et al., 2000). These do not have to be n-gram taggers,
but must be same-length FSTs (these are closed under inter-
section) and unambiguous. Define arc values in both FSTs such
that for any (w, t), cqφ and pθ accept (w, t) along unique paths
of total values v = − log qφ(t  |w) and v0 = log pθ(w, t), re-
spectively. We now lift the weights into the expectation semir-
ing (Eisner, 2002) as follows. In cqφ, replace arc a’s weight
ka with the semiring weight (ka, kava). In pθ, replace arc a0’s
weight with (1, v0a,). Then if k = cqφ(w, t), the intersected
FST accepts (w, t) with weight (k, k(v + v0)). The Li //expecta-
tion of v + v0 over all paths is then a sum za αzarza)3za over
arcs za of the intersected FST—we are using za to denote the
arc in the intersected FST that corresponds to “a in cqφ when
reached via path z,” and rza to denote the second component
of its semiring weight. Here αza and Nza denote the forward
and backward probabilities in the intersected FST, defined from
the first components of the semiring weights. We can get them
more efficiently from the results of running forward-backward
�
on the smaller cqφ: αza = αz z∈z kz and ,Qza = 3a = 1.
</bodyText>
<page confidence="0.997667">
136
</page>
<bodyText confidence="0.913764">
ber of forward-backward sweeps (footnote 10). The
ordinary forward algorithm requires nq = np and
takes O(CT np) time and space on a length-C string.
</bodyText>
<subsectionHeader confidence="0.997555">
3.4 Computing the gradient as well
</subsectionHeader>
<bodyText confidence="0.997626412698413">
To maximize our objective (5), we compute its gra-
dient with respect to θ and φ. We follow an efficient
recipe from Li and Eisner (2009, section 5, case 3).
The runtime and space match those of section 3.3,
except that the runtime rises to O(BCT np).13
First suppose that each va is local to a single arc.
We replace each weight ka with ka = (ka, kava)
in the so-called expectation semiring, whose sum
and product operations can be found in Li and Eis-
ner (2009, Table 1). Using these in the forward-
backward algorithm yields quantities &amp;a and I
that also fall in the expectation semiring.14 (Their
first components are the old αa and βa.) The
desired gradient15 (V�k, Vf) is Pa &amp;a(Vka)�βa,16
where Vka = (Vka, V(kava)) = (Vka, (Vka)va +
ka(Vva)). Here V gives the vector of partial deriva-
tives with respect to all φ and θ parameters. Yet each
Vka is sparse, with only 3 nonzero components, be-
cause ka depends on only one φ parameter (φa) and
two θ parameters (via θa as defined in section 3.3).
When np &gt; nq, we sum not over arcs a of cqφ but
over arcs za of the larger FST (footnote 12). Again
we can do this implicitly, by using the short path za
in cqφ in place of the arc za. Each state of cqφ must
then store α� and β� values for each of the Tnp−nq
states of the larger FST that it corresponds to. (In the
case np − nq = 1, as in our experiments, this fortu-
nately does not increase the total asymptotic space,
13An alternative would be to apply back-propagation
(reverse-mode automatic differentiation) to section 3.3’s com-
putation of the objective. This would achieve the same runtime
as in section 3.3, but would need as much space as time.
14This also computes our objective r: summing the &amp;’s of the
final states of cqφ gives (k, r) where k = 1 is the total probabil-
ity of all paths. This alternative computation of the expectation
r, using the forward algorithm (instead of forward-backward)
but over the expectation semiring, was given by Eisner (2002).
15We are interested in Or. Ok is just a byproduct. We re-
mark that Ok =� 0, even though k = 1 for any valid parameter
vector φ (footnote 14), as increasing φ invalidly can increase k.
16By a product of pairs we always mean (k, r)(s, t) def =
(ks, kt + rs), just as in the expectation semiring, even though
the pair Oka is not in that semiring (its components are vectors
rather than scalars). See (Li and Eisner, 2009, section 4.3). We
also define scalar-by-pair products as k(s, t) def = (ks, kt).
since each state of cqφ already has to store T arcs.)
With more cleverness, one can eliminate this
extra storage while preserving asymptotic runtime
(still using sparse vectors). Find (Vk, (Vr)(1)) =
Pa �αa(Vka, 0) 1. Also find (f, (Vr)(2)) =
Pa PzEZaαz MzEz (kz, V kz)) (kavza, V (kavza))
βa. Now our desired gradient Vr emerges as
(Vr)(1) + (Vr)(2). The computation of (Vr)(1)
uses modified definitions of (aa and �βa that depend
only on (respectively) the source and target states of
a—not za.17 To compute them, initialize α� (respec-
tively �β) at each state to (1, 0) or (0, 0) according to
whether the state is initial (respectively final). Now
iterate repeatedly (footnote 10) over all arcs a: Add
αa (ka, 0) + PzEZa αz (ILEz kz) (0, kavza) to the
α� at a’s target state. Conversely, add (ka, 0)�βa to
the β� at a’s source state, and for each z E ia, add
(QzEz kz) (0, kavza)βa to the β� at z’s source state.
</bodyText>
<subsectionHeader confidence="0.968495">
3.5 Locally optimizing the objective
</subsectionHeader>
<bodyText confidence="0.975912739130435">
Recall that cqφ associates with each [hc, hq, w] a
block of φ parameters that must be &gt; 0 and sum to
1. Our optimization method must enforce these con-
straints. A standard approach is to use a projected
gradient method, where after each gradient step on
φ, the parameters are projected back onto the prob-
ability simplex. We implemented another standard
approach: reexpress each block of parameters 1φa �
def
a E Al as φa = exp ηa/ PbEA exp ηb, as is possi-
ble iff the φa parameters satisfy the constraints. We
then follow the gradient of r� with respect to the new
η parameters, given by ∂r/∂ηa = φa(∂�r/∂φa−EA)
where EA = Pb φb(∂0∂φb).
Another common approach is block coordinate
ascent on θ and φ—this is “variational EM.” M-
step: Given φ, we can easily find optimal esti-
mates of the emission and transition probabilities θ.
They are respectively proportional to the posterior
expected counts of arcs a and paths za under cqφ,
namely N · αakaβa and N · αz REz kz) kaβa.
E-step: Given θ, we cannot easily find the opti-
mal φ (even if nq = np).18 This was the rea-
</bodyText>
<footnote confidence="0.815668666666667">
17First components αa and βa remain as in cqφ. &amp;a sums
paths to a. (Oka, 0)&amp; can’t quite sum over paths starting with
a (their early weights depend on z), but (Or)(2) corrects this.
18Recall that cqφ must have locally normalized probabilities
(to ensure that its marginal is c). If nq = np, the optimal φ
is as follows: we can reduce the variational gap to 0 by setting
</footnote>
<page confidence="0.996097">
137
</page>
<bodyText confidence="0.989209142857143">
son for gradient ascent. However, for any single
sum-to-1 block of parameters tφa : a E Al, it
is easy to find the optimal values if the others are
held fixed. We maximize LA def r + λA EaEA φa,
where λA is a Lagrange multiplier chosen so that
the sum is 1. The partial derivative ∂r/∂φa can be
found using methods of section 3.4, restricting the
sums to za for the given a. For example, follow-
def
ing paragraphs 2–3 of section 3.4, let (αa, ra) =
&amp;EZa (αza, rza) where (αza, rza) def = &amp;za �βza.19
Setting ∂LA/∂φa = 0 implies that φa is propor-
tional to exp((ra + EzEZa αza log θza)/αa).20
Rather than doing block coordinate ascent by up-
dating one φ block at a time (and then recomputing
ra values for all blocks, which is slow), one can take
an approximate step by updating all blocks in paral-
lel. We find that replacing the E-step with a single
parallel step still tends to improve the objective, and
that this approximate variational EM is faster than
gradient ascent with comparable results.21
</bodyText>
<sectionHeader confidence="0.999676" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999635">
4.1 Constrained unsupervised HMM learning
</subsectionHeader>
<bodyText confidence="0.989496026666667">
We follow the unsupervised POS tagging setup of
Merialdo (1994) and many others (Smith and Eis-
ner, 2005; Haghighi and Klein, 2006; Toutanova and
Johnson, 2007; Goldwater and Griffiths, 2007; John-
son, 2007). Given a corpus of sentences, one seeks
the maximum-likelihood or MAP parameters of a bi-
gram HMM (np = 2). The observed sentences, for
qy(t  |h.w, h9) to the probability that t begins with t if we
randomly draw a suffix w — c(·  |hew) and randomly tag ww
with t — pe(·  |ww, h9). This is equivalent to using pe with the
backward algorithm to conditionally tag each possible suffix.
19The first component of &amp;za�βza is αzaβza = αza · 1.
20If a is an arc of cqy then ∂�r/∂φa is the second component
of Ez∈Z, &amp;za(∂�kza/∂φa)�βza. Then ∂LA/∂φa works out to
Ez∈Z, ca(rza+αza(log θza—log φa—1))+λA. Set to 0 and
solve for φa, noting that ca, αa, λA are constant over a E A.
21In retrospect, an even faster strategy might be to do a series
of block φ and β� updates, updating β� at a state (footnote 10) im-
mediately after updating φ on the arcs leading from that state,
which allows a better block update at predecessor states. On an
acyclic machine, a single backward pass of this sort will reduce
the variational gap to 0 if n9 = n, (footnote 18). This is be-
cause, thanks to the up-to-date �β, each block of arcs gets new φ
weights in proportion to relative suffix path probabilities under
the new θ. After this backward pass, a single forward pass can
update the α values and collect expected counts for the M-step
that will update θ. Standard EM is a special case of this strategy.
us, are replaced by the faux sentences extrapolated
from observed n-grams via the language model c.
The states of the HMM correspond to POS tags as
in Figure 1. All transitions are allowed, but not all
emissions. If a word is listed in a provided “dictio-
nary” with its possible tags, then other tags are given
0 probability of emitting that word. The EM algo-
rithm uses the corpus to learn transition and emis-
sion probabilities that explain the data under this
constraint. The constraint ensures that the learned
states have something to do with true POS tags.
Merialdo (1994) spawned a long line of work
on this task. Ideas have included Bayesian learn-
ing methods (MacKay, 1997; Goldwater and Grif-
fiths, 2007; Johnson, 2007), better initial parame-
ters (Goldberg et al., 2008), and learning how to
constrain the possible parts of speech for a word
(Ravi and Knight, 2008), as well as non-HMM se-
quence models (Smith and Eisner, 2005; Haghighi
and Klein, 2006; Toutanova and Johnson, 2007).
Most of this work has used the Penn Treebank
(Marcus et al., 1993) as a dataset. While this
million-word Wall Street Journal (WSJ) corpus is
one of the largest that is manually annotated with
parts of speech, unsupervised learning methods
could take advantage of vast amounts of unannotated
text. In practice, runtime concerns have sometimes
led researchers to use small subsets of the Penn Tree-
bank (Goldwater and Griffiths, 2007; Smith and Eis-
ner, 2005; Haghighi and Klein, 2006). Our goal is
to point the way to using even larger datasets.
The reason for all this past research is that (Meri-
aldo, 1994) was a negative result: while EM is
guaranteed to improve the model’s likelihood, it de-
grades the match between the latent states and true
parts of speech (if the starting point is a good one
obtained with some supervision). Thus, for the task
of POS induction, there must be something wrong
with the HMM model, the likelihood objective, or
the search procedure. It is clear that the model is far
too weak: there are many latent variables in natural
language, so the HMM may be picking up on some-
thing other than POS tags. Ultimately, fixing this
will require richer models with many more param-
eters. But learning these (lexically specific) param-
eters will require large training datasets—hence our
present methodological exploration on whether it is
possible to scale up the original setting.
</bodyText>
<page confidence="0.993792">
138
</page>
<subsectionHeader confidence="0.915318">
4.2 Setup
</subsectionHeader>
<bodyText confidence="0.999966">
We investigate how much performance degrades
when we approximate the corpus and train approx-
imately with nQ = 1. We examine two measures:
likelihood on a held-out corpus and accuracy in POS
tagging. We train on corpora of three different sizes:
</bodyText>
<listItem confidence="0.874926285714286">
• WSJ-big (910k words → 441k n-grams @ cutoff 3),
• Giga-20 (20M words → 2.9M n-grams @ cutoff 10),
• Giga-200 (200M wds → 14.4M n-grams @ cutoff 20).
These were drawn from the Penn Treebank (sections
2–23) and the English Gigaword corpus (Parker et
al., 2009). For held-out evaluation, we use WSJ-
small (Penn Treebank section 0) or WSJ-big.
</listItem>
<bodyText confidence="0.999987666666667">
We estimate backoff language models for these
corpora based on collections of n-grams with n &lt; 5.
In this work, we select the n-grams by simple count
cutoffs as shown above,22 taking care to keep all 2-
grams as mentioned in footnote 2.
Similar to Merialdo (1994), we use a tag dictio-
nary which limits the possible tags of a word to those
it was observed with in the WSJ, provided that the
word was observed at least 5 times in the WSJ. We
used the reduced tagset of Smith and Eisner (2005),
which collapses the original 45 fine-grained part-of-
speech tags into just 17 coarser tags.
</bodyText>
<subsectionHeader confidence="0.785863">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.97002480952381">
In all experiments, our method achieves similar ac-
curacy though slightly worse likelihood. Although
this method is meant to be a fast approximation of
EM, standard EM is faster on the smallest dataset
(WSJ-big). This is because this corpus is not much
bigger than the 5-gram language model built from it
(at our current pruning level), and so the overhead
of the more complex n-gram EM method is a net
disadvantage. However, when moving to larger cor-
pora, the iterations of n-gram EM become as fast as
standard EM and then faster. We expect this trend
to continue as one moves to much larger datasets, as
the compression ratio of the pruned language model
relative to the original corpus will only improve.
The Google n-gram corpus is based on 50x more
data than our largest but could be handled in RAM.
22Entropy-based pruning (Stolcke, 2000) may be a better se-
lection method when one is in a position to choose. However,
count cutoffs were already used in the creation of the Google
n-gram corpus, and more complex methods of pruning may not
be practical for very large datasets.
</bodyText>
<figure confidence="0.9864715">
Time
Time
</figure>
<figureCaption confidence="0.915605333333333">
Figure 2: POS-tagging accuracy and log-likelihood af-
ter each iteration, measured on WSJ-big when training
on the Gigaword datasets, else on WSJ-small. Runtime
and log-likelihood are scaled differently for each dataset.
Replacing EM with our method changes runtime per it-
eration from 1.4s → 3.5s, 48s → 47s, and 506s → 321s.
</figureCaption>
<sectionHeader confidence="0.998966" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999963947368421">
We presented a general approach to training genera-
tive models on a distribution rather than on a training
sample. We gave several motivations for this novel
problem. We formulated an objective function simi-
lar to MAP, and presented a variational lower bound.
Algorithmically, we gave nontrivial general meth-
ods for computing and optimizing our variational
lower bound for arbitrary finite-state data distribu-
tions c, generative models p, and variational fami-
lies q, provided that p and q are unambiguous same-
length FSTs. We also gave details for specific useful
families for c, p, and q.
As proof of principle, we used a traditional HMM
POS tagging task to demonstrate that we can train
a model from n-grams almost as accurately as from
full sentences, and do so faster to the extent that the
n-gram dataset is smaller. More generally, we offer
our approach as an intriguing new tool to help semi-
supervised learning benefit from very large datasets.
</bodyText>
<figure confidence="0.994229454545454">
86
84
82
80
78
76
74
72
EM (WSJ-big)
N-gram EM (WSJ-big)
EM (Giga-20)
N-gram EM (Giga-20)
EM (Giga-200)
N-gram EM (Giga-200)
Likelihood
EM (WSJ-big)
N-gram EM (WSJ-big)
EM (Giga-20)
N-gram EM (Giga-20)
EM (Giga-200)
N-gram EM (Giga-200)
Accuracy
</figure>
<page confidence="0.99259">
139
</page>
<sectionHeader confidence="0.993077" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999879304761905">
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003.
Generalized algorithms for constructing statistical lan-
guage models. In Proc. ofACL, pages 40–47.
Shane Bergsma, Dekang Lin, and Randy Goebel. 2009.
Web-scale n-gram models for lexical disambiguation.
In Proc. of IJCAI.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
version 1. Linguistic Data Consortium, Philadelphia.
LDC2006T13.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proc. of EMNLP.
Tim Dawborn and James R. Curran. 2009. CCG
parsing with one syntactic structure per n-gram. In
Australasian Language Technology Association Work-
shop, pages 71–79.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society. Series B (Methodological), 39(1):1–38.
Jason Eisner. 2002. Parameter estimation for probabilis-
tic finite-state transducers. In Proc. ofACL, pages 1–8.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proc. ofACL, pages 746–754.
Sharon Goldwater and Thomas Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech tag-
ging. In Proc. of ACL, pages 744–751.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proc. of NAACL,
pages 320–327.
Daniel Hsu, Sham M. Kakade, and Tong Zhang. 2009. A
spectral algorithm for learning hidden Markov models.
In Proc. of COLT.
Mark Johnson. 2007. Why doesn’t EM find good HMM
POS-taggers? In Proc. of EMNLP-CoNLL, pages
296–305.
M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K.
Saul. 1999. An introduction to variational methods
for graphical models. In M. I. Jordan, editor, Learning
in Graphical Models. Kluwer.
Mirella Lapata and Frank Keller. 2005. Web-based mod-
els for natural language processing. ACM Transac-
tions on Speech and Language Processing.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proc. of
EMNLP, pages 40–51.
Percy Liang, Hal Daum´e III, and Dan Klein. 2008.
Structure compilation: Trading structure for features.
In International Conference on Machine Learning
(ICML), Helsinki, Finland.
D. Lin, K. Church, H. Ji, S. Sekine, D. Yarowsky,
S. Bergsma, K. Patil, E. Pitler, R. Lathbury, V. Rao,
K. Dalwani, and S. Narsale. 2009. Unsupervised ac-
quisition of lexical knowledge from n-grams. Sum-
mer workshop technical report, Center for Language
and Speech Processing, Johns Hopkins University.
David J. C. MacKay. 1997. Ensemble learning for hid-
den Markov models. http://www.inference.
phy.cam.ac.uk/mackay/abstracts/
ensemblePaper.html.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy Markov models for
information extraction and segmentation. In Proc. of
ICML, pages 591–598.
B. Merialdo. 1994. Tagging English text with a proba-
bilistic model. Computational Linguistics, 20(2):155–
171.
J.-B. Michel, Y. K. Shen, A. P. Aiden, A. Veres, M. K.
Gray, W. Brockman, The Google Books Team, J. P.
Pickett, D. Hoiberg, D. Clancy, P. Norvig, J. Orwant,
S. Pinker, M. A. Nowak, and E. L. Aiden. 2010.
Quantitative analysis of culture using millions of digi-
tized books. Science, 331(6014):176–182.
Mehryar Mohri and Mark-Jan Nederhof. 2001. Regu-
lar approximation of context-free grammars through
transformation. In Jean-Claude Junqua and Gert-
jan van Noord, editors, Robustness in Language and
Speech Technology, chapter 9, pages 153–163. Kluwer
Academic Publishers, The Netherlands, February.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
2000. The design principles of a weighted finite-
state transducer library. Theoretical Computer Sci-
ence, 231(1):17–32, January.
Radford M. Neal and Geoffrey E. Hinton. 1998. A view
of the EM algorithm that justifies incremental, sparse,
and other variants. In M.I. Jordan, editor, Learning in
Graphical Models, pages 355–368. Kluwer.
Mark-Jan Nederhof. 2000. Practical experiments
with regular approximation of context-free languages.
Computational Linguistics, 26(1).
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English Gigaword fourth
edition. Linguistic Data Consortium, Philadelphia.
LDC2009T13.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2005.
Learning and inference over constrained output. In
Proc. of IJCAI, pages 1124–1129.
Lawrence R. Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
</reference>
<page confidence="0.972289">
140
</page>
<reference confidence="0.999082333333333">
recognition. Proc. of the IEEE, 77(2):257–286, Febru-
ary.
Sujith Ravi and Kevin Knight. 2008. Minimized models
for unsupervised part-of-speech tagging. In Proc. of
ACL, pages 504–512.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proc. of ACL, pages 354–362.
R. Staden. 1979. A strategy of DNA sequencing em-
ploying computer programs. Nucleic Acids Research,
6(7):2601–2610, June.
Andreas Stolcke. 2000. Entropy-based pruning of back-
off language models. In DARPA Broadcast News
Transcription and Understanding Workshop, pages
270–274.
Kristina Toutanova and Mark Johnson. 2007. A
Bayesian LDA-based model for semi-supervised part-
of-speech tagging. In Proc. of NIPS, volume 20.
</reference>
<page confidence="0.998263">
141
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.656421">
<title confidence="0.99993">Learning on an Approximate</title>
<author confidence="0.992507">Smith</author>
<affiliation confidence="0.997372">Center for Language and Speech</affiliation>
<address confidence="0.8653065">Johns Hopkins 3400 N. Charles St., Baltimore, MD 21218,</address>
<abstract confidence="0.99359675">Unsupervised learning techniques can take advantage of large amounts of unannotated text, but the largest text corpus (the Web) is not easy to use in its full form. Instead, we have statistics about this corpus in the form of n-gram counts (Brants and Franz, 2006). While n-gram counts do not directly provide sentences, a distribution over sentences can be estimated from them in the same way that ngram language models are estimated. We treat this over sentences as an corshow how unsupervised learning can be performed on such a corpus using variational inference. We compare hidden Markov model (HMM) training on exact and approximate corpora of various sizes, measuring speed and accuracy on unsupervised part-of-speech tagging.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Mehryar Mohri</author>
<author>Brian Roark</author>
</authors>
<title>Generalized algorithms for constructing statistical language models.</title>
<date>2003</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>40--47</pages>
<contexts>
<context position="12956" citStr="Allauzen et al., 2003" startWordPosition="2192" endWordPosition="2195">uct) of those paths as in ordinary EM (see above). Running the forward algorithm on the E resulting lattice would compute Ec(w) t p(w, t), E whose log is log Ec(w) t p(w, t) rather than our desired Ec(w) log Et p(w, t). Instead, we use c in section 3.2 to construct a variational family cqφ. We then show in sections 3.3–3.5 how to compute and locally maximize the variational lower bound (5). 3.1 Modeling a corpus with n-gram counts n-gram backoff language models have been used for decades in automatic speech recognition and statistical machine translation. We follow the usual FSA construction (Allauzen et al., 2003). The state of a 5- gram FSA model c(w) must remember the previous 4-gram. For example, it would include an arc from state defg (the previous 4-gram) to state efgh with label h and weight c(h |defg). Then, with appropriate handling of boundary conditions, a sentence w = ... defghi ... is accepted along a single path of weight c(w) = · · · c(h |defg) · c(i |efgh) · · · . Arcs 133 Figure 1: Ordinary HMM tagging with finite-state machines. An arc’s label may have up to three components: “word:tag / weight.” (Weights are suppressed for space. State labels are not part of the machine but suggest th</context>
<context position="16493" citStr="Allauzen et al., 2003" startWordPosition="2845" endWordPosition="2848">ce boundary detector, replacing c with a document-level (not sentence-level) language model, and optimizing cqy to be a version of c that is probabilistically annotated with sentence boundaries, which yields our desired distribution over sentences. Instead, we construct a backoff language model. This FSA has one arc for each n-gram in the collection. Our algorithm’s runtime (per iteration) will be linear in the number of arcs. If the 5-gram defgh is not in our collection, then there can be no h arc leaving defg. When encountering h in state defg, the automaton will instead take a failure arc (Allauzen et al., 2003) to the “backoff state” efg. It may be able to consume the h from that state, on an arc with weight c(h |efg); or it may have to back off further to fg. Each state’s failure arc is weighted such that the state’s outgoing arcs sum to 1. It is labeled with the special symbol 4b, which does not contribute to the word string accepted along a path. We take care never to allow backoff to the empty state E,2 since we find that c(w) is otherwise too coarse an approximation to English: sampled sentences tend to be disjointed, with some words generated in complete ignorance of their left context. 3.2 Th</context>
</contexts>
<marker>Allauzen, Mohri, Roark, 2003</marker>
<rawString>Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003. Generalized algorithms for constructing statistical language models. In Proc. ofACL, pages 40–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Dekang Lin</author>
<author>Randy Goebel</author>
</authors>
<title>Web-scale n-gram models for lexical disambiguation.</title>
<date>2009</date>
<booktitle>In Proc. of IJCAI.</booktitle>
<contexts>
<context position="5585" citStr="Bergsma et al., 2009" startWordPosition="913" endWordPosition="916">ed once per training iteration. By pruning low-count n-grams or reducing the maximum n below 5, one can further increase data volume for the fixed time budget at the expense of approximation quality. Third, one may not have access to the original corpus. If one lacks the resources to harvest the web, the Google n-gram corpus was derived from over a trillion words of English web text. Privacy or copyright issues may prevent access, but one may still be able to work with n-gram statistics: Michel et al. (2010) used such statistics from 5 million scanned books. Several systems use n-gram counts (Bergsma et al., 2009; Lin et al., 2009) or other web statistics (Lapata and Keller, 2005) as features within a classifier. A large language model from ngram counts yields an effective prior over hypotheses in tasks like machine translation (Brants et al., 2007). We similarly construct an n-gram model, but treat it as the primary training data whose structure is to be explained by the generative HMM. Thus our criterion does not explain the n-grams in isolation, but rather tries to explain the likely full sentences w that the model reconstructed from overlapping ngrams. This is something like shotgun sequencing, in</context>
</contexts>
<marker>Bergsma, Lin, Goebel, 2009</marker>
<rawString>Shane Bergsma, Dekang Lin, and Randy Goebel. 2009. Web-scale n-gram models for lexical disambiguation. In Proc. of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1T 5-gram version 1. Linguistic Data Consortium,</title>
<date>2006</date>
<location>Philadelphia. LDC2006T13.</location>
<contexts>
<context position="1932" citStr="Brants and Franz, 2006" startWordPosition="307" endWordPosition="310"> given by an 5-gram backoff language model that has been estimated from statistics of the corpus. In this paper, we demonstrate our approach on a traditional testbed for new structured-prediction learning algorithms, namely HMMs. We focus on unsupervised learning. This serves to elucidate the structure of our variational training approach, which stitches overlapping n-grams together rather than treating them in isolation. It also confirms that at least in this case, accuracy is not harmed by the key approximations made by our method. In future, we hope to scale up to the Google n-gram corpus (Brants and Franz, 2006) and learn a more detailed, explanatory joint model of tags, syntactic dependencies, and topics. Our intuition here is that web-scale data may be needed to learn the large number of lexically and contextually specific parameters. &amp;quot;Work was supported in part by NSF grant No. 0347822. 1.1 Formulation Let w (“words”) denote an observation sequence, and let t (“tags”) denote a hidden HMM state sequence that may explain w. This terminology is taken from the literature on inducing part-of-speech (POS) taggers using a first-order HMM (Merialdo, 1994), which we use as our experimental setting. Maximum</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram version 1. Linguistic Data Consortium, Philadelphia. LDC2006T13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="5826" citStr="Brants et al., 2007" startWordPosition="954" endWordPosition="957">ginal corpus. If one lacks the resources to harvest the web, the Google n-gram corpus was derived from over a trillion words of English web text. Privacy or copyright issues may prevent access, but one may still be able to work with n-gram statistics: Michel et al. (2010) used such statistics from 5 million scanned books. Several systems use n-gram counts (Bergsma et al., 2009; Lin et al., 2009) or other web statistics (Lapata and Keller, 2005) as features within a classifier. A large language model from ngram counts yields an effective prior over hypotheses in tasks like machine translation (Brants et al., 2007). We similarly construct an n-gram model, but treat it as the primary training data whose structure is to be explained by the generative HMM. Thus our criterion does not explain the n-grams in isolation, but rather tries to explain the likely full sentences w that the model reconstructed from overlapping ngrams. This is something like shotgun sequencing, in which likely DNA strings are reconstructed from overlapping short reads (Staden, 1979); however, we train an HMM on the resulting distribution rather than merely trying to find its mode. Finally, unsupervised HMM training discovers latent s</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Dawborn</author>
<author>James R Curran</author>
</authors>
<title>CCG parsing with one syntactic structure per n-gram.</title>
<date>2009</date>
<booktitle>In Australasian Language Technology Association Workshop,</booktitle>
<pages>71--79</pages>
<contexts>
<context position="18857" citStr="Dawborn and Curran, 2009" startWordPosition="3285" endWordPosition="3289">ion). 0 is the collection of all multinomial parameters. If nq = np, then our variational gap can be made 0 as in ordinary non-variational EM (see section 3.5). In our experiments, however, we save memory by choosing nq = 1. Thus, our variational gap is tight to the extent that a word’s POS tag under the model pθ is conditionally independent of previous tags and the rest of the sentence, given an n-word window.4 This is the assumption made by local classification models (Punyakanok et al., 2005; Toutanova and Johnson, 2007). Note that it is milder than the “one tagging per n-gram” hypothesis (Dawborn and Curran, 2009; Lin et al., 2009), which claims that each 5-gram (and therefore each sentence!) is unambiguous as to its full tagging. In contrast, we allow that a tag may be ambiguous even given an n-word window; we merely suppose that there is no further disambiguating information accessible to pθ.5 We can encode the resulting cq(w, t) as an FST. With nq = 1, the states of cq are isomorphic to the states of c. However, an arc in c from defg with label h and weight 0.2 is replaced in cq by several arcs—one per tag t—with label h : t and weight 0.2 · qφ(t |defgh).6 We remark that an encoding of 3A condition</context>
</contexts>
<marker>Dawborn, Curran, 2009</marker>
<rawString>Tim Dawborn and James R. Curran. 2009. CCG parsing with one syntactic structure per n-gram. In Australasian Language Technology Association Workshop, pages 71–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur P Dempster</author>
<author>Nan M Laird</author>
<author>Donald B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society. Series B (Methodological),</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="8274" citStr="Dempster et al., 1977" startWordPosition="1350" endWordPosition="1353">ral technique extends to other latentvariable models, although it suffers from variational EM’s usual local optima and approximation errors. 2 A variational lower bound Our starting point is the variational EM algorithm (Jordan et al., 1999). Recall that this maximizes a lower bound on the MAP criterion of equation 1, by bounding the log-likelihood subterm as follows: log Et po(w, t) (2) = log Et q(t)(po(w, t)/q(t)) &gt; Et q(t) log(po(w, t)/q(t)) = E,(t)[log po(w, t) − log q(t) (3) This use of Jensen’s inequality is valid for any distribution q. As Neal and Hinton (1998) show, the EM algorithm (Dempster et al., 1977) can be regarded as locally maximizing the resulting lower bound by alternating optimization, where q is a free parameter. The E-step optimizes q for fixed 0, and the Mstep optimizes 0 for fixed q. These computations are tractable for HMMs, since the distribution q(t) = po(t |w) that is optimal at the E-step (which makes the inequality tight) can be represented as a lattice (a certain kind of weighted DFA), and this makes the M-step tractable via the forward-backward algorithm. However, there are many extensions such as 132 factorial HMMs and Bayesian HMMs in which an expectation under pθ(t |w</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Parameter estimation for probabilistic finite-state transducers.</title>
<date>2002</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="21829" citStr="Eisner, 2002" startWordPosition="3834" endWordPosition="3835">and 0. We exploit our representation of cqφ as an FSM over the (+, x) semiring. The path weights represent a probability distribution over the paths. In general, it is efficient to compute the expected value of a random FSM path, for any definition of value that decomposes additively over the path’s arcs. The approach is to apply the forward algorithm to a version of cqφ where we now regard each arc as weighted by an ordered pair of real numbers. The (+, x) operations for combining weights (section 3) are replaced with the operations of an “expectation semiring” whose elements are such pairs (Eisner, 2002). Suppose we want to find Ecq, (w t) log qφ (t |w). To reduce this to an expected value problem, we must assign a value to each arc of cqφ such that the c is Figure 1a, splitting its states with n9 = 2 would yield a cq with a topology like Figure 1c, but with each arc having an independent variational parameter. 7One could increase the number of arcs and hence variational parameters by splitting the states of cq to remember more history. In particular, one could increase the width n9 of the tag window, or one could increase the width of the word window by splitting states of c (without changin</context>
<context position="25212" citStr="Eisner, 2002" startWordPosition="4493" endWordPosition="4494"> version of hc. It is labeled with 4) : E, which does not contribute to the word string or tag string accepted along a path. Its weight ka is the weight c(d) |hc) of the corresponding failure arc in c from hc to h0c. def We define va = 0, so it does not contribute to the total value. 10Recall that the forward probability of each state is defined recursively from the forward probabilities of the states that have arcs leading to it. As our FST is cyclic, it is not possible to visit the states in topologically sorted order. We instead solve these simultaneous equations by a relaxation algorithm (Eisner, 2002, section 5): repeatedly sweep through all states, updating their forward probability, until the total forward probability of all final states is close to the correct total of 1 = Ew,t cqφ(w, t) (showing that we have covered all high-prob paths). A corresponding backward relaxation is actually not needed yet (we do need it for , in section 3.4): backward probabilities are just 1, since cqφ is constructed with locally normalized probabilities. When we rerun the forward-backward algorithm after a parameter update, we use the previous solution as a starting point for the relaxation algorithm. Thi</context>
<context position="27966" citStr="Eisner, 2002" startWordPosition="5007" endWordPosition="5008">q (and Za consists of the paths of that length to a’s start state). However, z may be longer if it contains 4) arcs, or shorter if it begins with an initial state. 12Constructed by lazy finite-state intersection of cqφ and pθ (Mohri et al., 2000). These do not have to be n-gram taggers, but must be same-length FSTs (these are closed under intersection) and unambiguous. Define arc values in both FSTs such that for any (w, t), cqφ and pθ accept (w, t) along unique paths of total values v = − log qφ(t |w) and v0 = log pθ(w, t), respectively. We now lift the weights into the expectation semiring (Eisner, 2002) as follows. In cqφ, replace arc a’s weight ka with the semiring weight (ka, kava). In pθ, replace arc a0’s weight with (1, v0a,). Then if k = cqφ(w, t), the intersected FST accepts (w, t) with weight (k, k(v + v0)). The Li //expectation of v + v0 over all paths is then a sum za αzarza)3za over arcs za of the intersected FST—we are using za to denote the arc in the intersected FST that corresponds to “a in cqφ when reached via path z,” and rza to denote the second component of its semiring weight. Here αza and Nza denote the forward and backward probabilities in the intersected FST, defined fr</context>
<context position="30900" citStr="Eisner (2002)" startWordPosition="5551" endWordPosition="5552">, this fortunately does not increase the total asymptotic space, 13An alternative would be to apply back-propagation (reverse-mode automatic differentiation) to section 3.3’s computation of the objective. This would achieve the same runtime as in section 3.3, but would need as much space as time. 14This also computes our objective r: summing the &amp;’s of the final states of cqφ gives (k, r) where k = 1 is the total probability of all paths. This alternative computation of the expectation r, using the forward algorithm (instead of forward-backward) but over the expectation semiring, was given by Eisner (2002). 15We are interested in Or. Ok is just a byproduct. We remark that Ok =� 0, even though k = 1 for any valid parameter vector φ (footnote 14), as increasing φ invalidly can increase k. 16By a product of pairs we always mean (k, r)(s, t) def = (ks, kt + rs), just as in the expectation semiring, even though the pair Oka is not in that semiring (its components are vectors rather than scalars). See (Li and Eisner, 2009, section 4.3). We also define scalar-by-pair products as k(s, t) def = (ks, kt). since each state of cqφ already has to store T arcs.) With more cleverness, one can eliminate this e</context>
</contexts>
<marker>Eisner, 2002</marker>
<rawString>Jason Eisner. 2002. Parameter estimation for probabilistic finite-state transducers. In Proc. ofACL, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Meni Adler</author>
<author>Michael Elhadad</author>
</authors>
<title>EM can find pretty good HMM POS-taggers (when given a good start).</title>
<date>2008</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>746--754</pages>
<contexts>
<context position="37195" citStr="Goldberg et al., 2008" startWordPosition="6698" endWordPosition="6701">gure 1. All transitions are allowed, but not all emissions. If a word is listed in a provided “dictionary” with its possible tags, then other tags are given 0 probability of emitting that word. The EM algorithm uses the corpus to learn transition and emission probabilities that explain the data under this constraint. The constraint ensures that the learned states have something to do with true POS tags. Merialdo (1994) spawned a long line of work on this task. Ideas have included Bayesian learning methods (MacKay, 1997; Goldwater and Griffiths, 2007; Johnson, 2007), better initial parameters (Goldberg et al., 2008), and learning how to constrain the possible parts of speech for a word (Ravi and Knight, 2008), as well as non-HMM sequence models (Smith and Eisner, 2005; Haghighi and Klein, 2006; Toutanova and Johnson, 2007). Most of this work has used the Penn Treebank (Marcus et al., 1993) as a dataset. While this million-word Wall Street Journal (WSJ) corpus is one of the largest that is manually annotated with parts of speech, unsupervised learning methods could take advantage of vast amounts of unannotated text. In practice, runtime concerns have sometimes led researchers to use small subsets of the P</context>
</contexts>
<marker>Goldberg, Adler, Elhadad, 2008</marker>
<rawString>Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008. EM can find pretty good HMM POS-taggers (when given a good start). In Proc. ofACL, pages 746–754.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas Griffiths</author>
</authors>
<title>A fully Bayesian approach to unsupervised part-of-speech tagging.</title>
<date>2007</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>744--751</pages>
<contexts>
<context position="35028" citStr="Goldwater and Griffiths, 2007" startWordPosition="6306" endWordPosition="6309">coordinate ascent by updating one φ block at a time (and then recomputing ra values for all blocks, which is slow), one can take an approximate step by updating all blocks in parallel. We find that replacing the E-step with a single parallel step still tends to improve the objective, and that this approximate variational EM is faster than gradient ascent with comparable results.21 4 Experiments 4.1 Constrained unsupervised HMM learning We follow the unsupervised POS tagging setup of Merialdo (1994) and many others (Smith and Eisner, 2005; Haghighi and Klein, 2006; Toutanova and Johnson, 2007; Goldwater and Griffiths, 2007; Johnson, 2007). Given a corpus of sentences, one seeks the maximum-likelihood or MAP parameters of a bigram HMM (np = 2). The observed sentences, for qy(t |h.w, h9) to the probability that t begins with t if we randomly draw a suffix w — c(· |hew) and randomly tag ww with t — pe(· |ww, h9). This is equivalent to using pe with the backward algorithm to conditionally tag each possible suffix. 19The first component of &amp;za�βza is αzaβza = αza · 1. 20If a is an arc of cqy then ∂�r/∂φa is the second component of Ez∈Z, &amp;za(∂�kza/∂φa)�βza. Then ∂LA/∂φa works out to Ez∈Z, ca(rza+αza(log θza—log φa—1)</context>
<context position="37128" citStr="Goldwater and Griffiths, 2007" startWordPosition="6687" endWordPosition="6691">he language model c. The states of the HMM correspond to POS tags as in Figure 1. All transitions are allowed, but not all emissions. If a word is listed in a provided “dictionary” with its possible tags, then other tags are given 0 probability of emitting that word. The EM algorithm uses the corpus to learn transition and emission probabilities that explain the data under this constraint. The constraint ensures that the learned states have something to do with true POS tags. Merialdo (1994) spawned a long line of work on this task. Ideas have included Bayesian learning methods (MacKay, 1997; Goldwater and Griffiths, 2007; Johnson, 2007), better initial parameters (Goldberg et al., 2008), and learning how to constrain the possible parts of speech for a word (Ravi and Knight, 2008), as well as non-HMM sequence models (Smith and Eisner, 2005; Haghighi and Klein, 2006; Toutanova and Johnson, 2007). Most of this work has used the Penn Treebank (Marcus et al., 1993) as a dataset. While this million-word Wall Street Journal (WSJ) corpus is one of the largest that is manually annotated with parts of speech, unsupervised learning methods could take advantage of vast amounts of unannotated text. In practice, runtime co</context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Sharon Goldwater and Thomas Griffiths. 2007. A fully Bayesian approach to unsupervised part-of-speech tagging. In Proc. of ACL, pages 744–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Prototype-driven learning for sequence models.</title>
<date>2006</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>320--327</pages>
<contexts>
<context position="34968" citStr="Haghighi and Klein, 2006" startWordPosition="6298" endWordPosition="6301">ra + EzEZa αza log θza)/αa).20 Rather than doing block coordinate ascent by updating one φ block at a time (and then recomputing ra values for all blocks, which is slow), one can take an approximate step by updating all blocks in parallel. We find that replacing the E-step with a single parallel step still tends to improve the objective, and that this approximate variational EM is faster than gradient ascent with comparable results.21 4 Experiments 4.1 Constrained unsupervised HMM learning We follow the unsupervised POS tagging setup of Merialdo (1994) and many others (Smith and Eisner, 2005; Haghighi and Klein, 2006; Toutanova and Johnson, 2007; Goldwater and Griffiths, 2007; Johnson, 2007). Given a corpus of sentences, one seeks the maximum-likelihood or MAP parameters of a bigram HMM (np = 2). The observed sentences, for qy(t |h.w, h9) to the probability that t begins with t if we randomly draw a suffix w — c(· |hew) and randomly tag ww with t — pe(· |ww, h9). This is equivalent to using pe with the backward algorithm to conditionally tag each possible suffix. 19The first component of &amp;za�βza is αzaβza = αza · 1. 20If a is an arc of cqy then ∂�r/∂φa is the second component of Ez∈Z, &amp;za(∂�kza/∂φa)�βza. </context>
<context position="37376" citStr="Haghighi and Klein, 2006" startWordPosition="6730" endWordPosition="6733">ing that word. The EM algorithm uses the corpus to learn transition and emission probabilities that explain the data under this constraint. The constraint ensures that the learned states have something to do with true POS tags. Merialdo (1994) spawned a long line of work on this task. Ideas have included Bayesian learning methods (MacKay, 1997; Goldwater and Griffiths, 2007; Johnson, 2007), better initial parameters (Goldberg et al., 2008), and learning how to constrain the possible parts of speech for a word (Ravi and Knight, 2008), as well as non-HMM sequence models (Smith and Eisner, 2005; Haghighi and Klein, 2006; Toutanova and Johnson, 2007). Most of this work has used the Penn Treebank (Marcus et al., 1993) as a dataset. While this million-word Wall Street Journal (WSJ) corpus is one of the largest that is manually annotated with parts of speech, unsupervised learning methods could take advantage of vast amounts of unannotated text. In practice, runtime concerns have sometimes led researchers to use small subsets of the Penn Treebank (Goldwater and Griffiths, 2007; Smith and Eisner, 2005; Haghighi and Klein, 2006). Our goal is to point the way to using even larger datasets. The reason for all this p</context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>Aria Haghighi and Dan Klein. 2006. Prototype-driven learning for sequence models. In Proc. of NAACL, pages 320–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Hsu</author>
<author>Sham M Kakade</author>
<author>Tong Zhang</author>
</authors>
<title>A spectral algorithm for learning hidden Markov models.</title>
<date>2009</date>
<booktitle>In Proc. of COLT.</booktitle>
<contexts>
<context position="7125" citStr="Hsu et al., 2009" startWordPosition="1159" endWordPosition="1162">ariable distribution p (the trained HMM) that has fewer parameters. We show how to do the same where the distribution c is not a corpus but a finite-state distribution. In general, this finite-state c could represent some sophisticated estimate of the population distribution, using shrinkage, word classes, neural-net predictors, etc. to generalize in some way beyond the training sample before fitting p. For the sake of speed and clear comparison, however, our present experiments take c to be a compact approximation to the sample distribution, requiring only n-grams. Spectral learning of HMMs (Hsu et al., 2009) also learns from a collection of n-grams. It has the striking advantage of converging globally to the true HMM parameters (under a certain reparameterization), with enough data and under certain assumptions. However, it does not exploit context beyond a trigram (it will not maximize, even locally, the likelihood of a finite sample of sentences), and cannot exploit priors or structure—e.g., that the emissions are consistent with a tag dictionary or that the transitions encode a higher-order or factorial HMM. Our more general technique extends to other latentvariable models, although it suffers</context>
</contexts>
<marker>Hsu, Kakade, Zhang, 2009</marker>
<rawString>Daniel Hsu, Sham M. Kakade, and Tong Zhang. 2009. A spectral algorithm for learning hidden Markov models. In Proc. of COLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Why doesn’t EM find good HMM POS-taggers?</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL,</booktitle>
<pages>296--305</pages>
<contexts>
<context position="18762" citStr="Johnson, 2007" startWordPosition="3271" endWordPosition="3272">n i (where n is as large as possible such that this n-gram is in our training collection). 0 is the collection of all multinomial parameters. If nq = np, then our variational gap can be made 0 as in ordinary non-variational EM (see section 3.5). In our experiments, however, we save memory by choosing nq = 1. Thus, our variational gap is tight to the extent that a word’s POS tag under the model pθ is conditionally independent of previous tags and the rest of the sentence, given an n-word window.4 This is the assumption made by local classification models (Punyakanok et al., 2005; Toutanova and Johnson, 2007). Note that it is milder than the “one tagging per n-gram” hypothesis (Dawborn and Curran, 2009; Lin et al., 2009), which claims that each 5-gram (and therefore each sentence!) is unambiguous as to its full tagging. In contrast, we allow that a tag may be ambiguous even given an n-word window; we merely suppose that there is no further disambiguating information accessible to pθ.5 We can encode the resulting cq(w, t) as an FST. With nq = 1, the states of cq are isomorphic to the states of c. However, an arc in c from defg with label h and weight 0.2 is replaced in cq by several arcs—one per ta</context>
<context position="34997" citStr="Johnson, 2007" startWordPosition="6304" endWordPosition="6305">an doing block coordinate ascent by updating one φ block at a time (and then recomputing ra values for all blocks, which is slow), one can take an approximate step by updating all blocks in parallel. We find that replacing the E-step with a single parallel step still tends to improve the objective, and that this approximate variational EM is faster than gradient ascent with comparable results.21 4 Experiments 4.1 Constrained unsupervised HMM learning We follow the unsupervised POS tagging setup of Merialdo (1994) and many others (Smith and Eisner, 2005; Haghighi and Klein, 2006; Toutanova and Johnson, 2007; Goldwater and Griffiths, 2007; Johnson, 2007). Given a corpus of sentences, one seeks the maximum-likelihood or MAP parameters of a bigram HMM (np = 2). The observed sentences, for qy(t |h.w, h9) to the probability that t begins with t if we randomly draw a suffix w — c(· |hew) and randomly tag ww with t — pe(· |ww, h9). This is equivalent to using pe with the backward algorithm to conditionally tag each possible suffix. 19The first component of &amp;za�βza is αzaβza = αza · 1. 20If a is an arc of cqy then ∂�r/∂φa is the second component of Ez∈Z, &amp;za(∂�kza/∂φa)�βza. Then ∂LA/∂φa works out to Ez∈</context>
<context position="37144" citStr="Johnson, 2007" startWordPosition="6692" endWordPosition="6693"> of the HMM correspond to POS tags as in Figure 1. All transitions are allowed, but not all emissions. If a word is listed in a provided “dictionary” with its possible tags, then other tags are given 0 probability of emitting that word. The EM algorithm uses the corpus to learn transition and emission probabilities that explain the data under this constraint. The constraint ensures that the learned states have something to do with true POS tags. Merialdo (1994) spawned a long line of work on this task. Ideas have included Bayesian learning methods (MacKay, 1997; Goldwater and Griffiths, 2007; Johnson, 2007), better initial parameters (Goldberg et al., 2008), and learning how to constrain the possible parts of speech for a word (Ravi and Knight, 2008), as well as non-HMM sequence models (Smith and Eisner, 2005; Haghighi and Klein, 2006; Toutanova and Johnson, 2007). Most of this work has used the Penn Treebank (Marcus et al., 1993) as a dataset. While this million-word Wall Street Journal (WSJ) corpus is one of the largest that is manually annotated with parts of speech, unsupervised learning methods could take advantage of vast amounts of unannotated text. In practice, runtime concerns have some</context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>Mark Johnson. 2007. Why doesn’t EM find good HMM POS-taggers? In Proc. of EMNLP-CoNLL, pages 296–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M I Jordan</author>
<author>Z Ghahramani</author>
<author>T S Jaakkola</author>
<author>L K Saul</author>
</authors>
<title>An introduction to variational methods for graphical models.</title>
<date>1999</date>
<booktitle>Learning in Graphical Models.</booktitle>
<editor>In M. I. Jordan, editor,</editor>
<publisher>Kluwer.</publisher>
<contexts>
<context position="7893" citStr="Jordan et al., 1999" startWordPosition="1281" endWordPosition="1284">terization), with enough data and under certain assumptions. However, it does not exploit context beyond a trigram (it will not maximize, even locally, the likelihood of a finite sample of sentences), and cannot exploit priors or structure—e.g., that the emissions are consistent with a tag dictionary or that the transitions encode a higher-order or factorial HMM. Our more general technique extends to other latentvariable models, although it suffers from variational EM’s usual local optima and approximation errors. 2 A variational lower bound Our starting point is the variational EM algorithm (Jordan et al., 1999). Recall that this maximizes a lower bound on the MAP criterion of equation 1, by bounding the log-likelihood subterm as follows: log Et po(w, t) (2) = log Et q(t)(po(w, t)/q(t)) &gt; Et q(t) log(po(w, t)/q(t)) = E,(t)[log po(w, t) − log q(t) (3) This use of Jensen’s inequality is valid for any distribution q. As Neal and Hinton (1998) show, the EM algorithm (Dempster et al., 1977) can be regarded as locally maximizing the resulting lower bound by alternating optimization, where q is a free parameter. The E-step optimizes q for fixed 0, and the Mstep optimizes 0 for fixed q. These computations ar</context>
</contexts>
<marker>Jordan, Ghahramani, Jaakkola, Saul, 1999</marker>
<rawString>M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. 1999. An introduction to variational methods for graphical models. In M. I. Jordan, editor, Learning in Graphical Models. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Frank Keller</author>
</authors>
<title>Web-based models for natural language processing.</title>
<date>2005</date>
<journal>ACM Transactions on Speech and Language Processing.</journal>
<contexts>
<context position="5654" citStr="Lapata and Keller, 2005" startWordPosition="925" endWordPosition="928">educing the maximum n below 5, one can further increase data volume for the fixed time budget at the expense of approximation quality. Third, one may not have access to the original corpus. If one lacks the resources to harvest the web, the Google n-gram corpus was derived from over a trillion words of English web text. Privacy or copyright issues may prevent access, but one may still be able to work with n-gram statistics: Michel et al. (2010) used such statistics from 5 million scanned books. Several systems use n-gram counts (Bergsma et al., 2009; Lin et al., 2009) or other web statistics (Lapata and Keller, 2005) as features within a classifier. A large language model from ngram counts yields an effective prior over hypotheses in tasks like machine translation (Brants et al., 2007). We similarly construct an n-gram model, but treat it as the primary training data whose structure is to be explained by the generative HMM. Thus our criterion does not explain the n-grams in isolation, but rather tries to explain the likely full sentences w that the model reconstructed from overlapping ngrams. This is something like shotgun sequencing, in which likely DNA strings are reconstructed from overlapping short re</context>
</contexts>
<marker>Lapata, Keller, 2005</marker>
<rawString>Mirella Lapata and Frank Keller. 2005. Web-based models for natural language processing. ACM Transactions on Speech and Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Jason Eisner</author>
</authors>
<title>First- and second-order expectation semirings with applications to minimumrisk training on translation forests.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>40--51</pages>
<contexts>
<context position="23787" citStr="Li and Eisner, 2009" startWordPosition="4221" endWordPosition="4224">φ is a pair of the form [hc, hq], where hc is a state of c (e.g., an (n − 1)-word history) and hq is an (nq − 1)-tag history. We saw in the previous section that an arc a leaving this state, and labeled with w : t where w is a word and t is a tag, will def have a weight of the form ka = c(w |hc)Oa where def def �a = qφ(t |hcw, hq). We now let the value va = log Oa.9 Then, just as the weight of a path accepting (w, t) is Ha ka = cqφ(w, t), the value of that path is Ea va = log qφ(t |w), as desired. To compute the expected value r over all paths, we follow a generalized forward-backward recipe (Li and Eisner, 2009, section 4.2). First, run the forward and backward algorithms over cqφ.10 Now the expected value is a sum over all arcs of cqφ, namely r = Ea αakavaQa, where αa denotes the forward probability of arc a’s source state and Qa denotes the backward probability of arc a’s target state. Now, in fact, the expectation we need to compute is not Ecqφ(w,t) log qφ(t |w) but rather equation (5). So the value va of arc a should not actually be def log �a but rather log �a − log �a where �a = pθ(t | 8The total value is then the sum of the logs, i.e., the log of the product. This works because qφ is unambigu</context>
<context position="29073" citStr="Li and Eisner (2009" startWordPosition="5213" endWordPosition="5216">miring weight. Here αza and Nza denote the forward and backward probabilities in the intersected FST, defined from the first components of the semiring weights. We can get them more efficiently from the results of running forward-backward � on the smaller cqφ: αza = αz z∈z kz and ,Qza = 3a = 1. 136 ber of forward-backward sweeps (footnote 10). The ordinary forward algorithm requires nq = np and takes O(CT np) time and space on a length-C string. 3.4 Computing the gradient as well To maximize our objective (5), we compute its gradient with respect to θ and φ. We follow an efficient recipe from Li and Eisner (2009, section 5, case 3). The runtime and space match those of section 3.3, except that the runtime rises to O(BCT np).13 First suppose that each va is local to a single arc. We replace each weight ka with ka = (ka, kava) in the so-called expectation semiring, whose sum and product operations can be found in Li and Eisner (2009, Table 1). Using these in the forwardbackward algorithm yields quantities &amp;a and I that also fall in the expectation semiring.14 (Their first components are the old αa and βa.) The desired gradient15 (V�k, Vf) is Pa &amp;a(Vka)�βa,16 where Vka = (Vka, V(kava)) = (Vka, (Vka)va +</context>
<context position="31318" citStr="Li and Eisner, 2009" startWordPosition="5631" endWordPosition="5634">otal probability of all paths. This alternative computation of the expectation r, using the forward algorithm (instead of forward-backward) but over the expectation semiring, was given by Eisner (2002). 15We are interested in Or. Ok is just a byproduct. We remark that Ok =� 0, even though k = 1 for any valid parameter vector φ (footnote 14), as increasing φ invalidly can increase k. 16By a product of pairs we always mean (k, r)(s, t) def = (ks, kt + rs), just as in the expectation semiring, even though the pair Oka is not in that semiring (its components are vectors rather than scalars). See (Li and Eisner, 2009, section 4.3). We also define scalar-by-pair products as k(s, t) def = (ks, kt). since each state of cqφ already has to store T arcs.) With more cleverness, one can eliminate this extra storage while preserving asymptotic runtime (still using sparse vectors). Find (Vk, (Vr)(1)) = Pa �αa(Vka, 0) 1. Also find (f, (Vr)(2)) = Pa PzEZaαz MzEz (kz, V kz)) (kavza, V (kavza)) βa. Now our desired gradient Vr emerges as (Vr)(1) + (Vr)(2). The computation of (Vr)(1) uses modified definitions of (aa and �βa that depend only on (respectively) the source and target states of a—not za.17 To compute them, in</context>
</contexts>
<marker>Li, Eisner, 2009</marker>
<rawString>Zhifei Li and Jason Eisner. 2009. First- and second-order expectation semirings with applications to minimumrisk training on translation forests. In Proc. of EMNLP, pages 40–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Hal Daum´e</author>
<author>Dan Klein</author>
</authors>
<title>Structure compilation: Trading structure for features.</title>
<date>2008</date>
<booktitle>In International Conference on Machine Learning (ICML),</booktitle>
<location>Helsinki, Finland.</location>
<marker>Liang, Daum´e, Klein, 2008</marker>
<rawString>Percy Liang, Hal Daum´e III, and Dan Klein. 2008. Structure compilation: Trading structure for features. In International Conference on Machine Learning (ICML), Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
<author>K Church</author>
<author>H Ji</author>
<author>S Sekine</author>
<author>D Yarowsky</author>
<author>S Bergsma</author>
<author>K Patil</author>
<author>E Pitler</author>
<author>R Lathbury</author>
<author>V Rao</author>
<author>K Dalwani</author>
<author>S Narsale</author>
</authors>
<title>Unsupervised acquisition of lexical knowledge from n-grams. Summer workshop technical report,</title>
<date>2009</date>
<institution>Center for Language and Speech Processing, Johns Hopkins University.</institution>
<contexts>
<context position="5604" citStr="Lin et al., 2009" startWordPosition="917" endWordPosition="920">teration. By pruning low-count n-grams or reducing the maximum n below 5, one can further increase data volume for the fixed time budget at the expense of approximation quality. Third, one may not have access to the original corpus. If one lacks the resources to harvest the web, the Google n-gram corpus was derived from over a trillion words of English web text. Privacy or copyright issues may prevent access, but one may still be able to work with n-gram statistics: Michel et al. (2010) used such statistics from 5 million scanned books. Several systems use n-gram counts (Bergsma et al., 2009; Lin et al., 2009) or other web statistics (Lapata and Keller, 2005) as features within a classifier. A large language model from ngram counts yields an effective prior over hypotheses in tasks like machine translation (Brants et al., 2007). We similarly construct an n-gram model, but treat it as the primary training data whose structure is to be explained by the generative HMM. Thus our criterion does not explain the n-grams in isolation, but rather tries to explain the likely full sentences w that the model reconstructed from overlapping ngrams. This is something like shotgun sequencing, in which likely DNA s</context>
<context position="15124" citStr="Lin et al., 2009" startWordPosition="2617" endWordPosition="2620">s possible the training sample from which the counts were drawn. That gives a fair comparison with ordinary EM, which trains directly on that sample. (See discussion at the end of section 1.2 for alternatives.) Yet we decline to construct a full 5-gram model, which would not be as compact as desired. A collection of all web 5-grams would be nearly as large as the web itself (by Zipf’s Law). We may not have such a collection. For example, the Google n-gram corpus version 2 contains counts only for 1-grams that appear at least 40 times and 2-, 3-, 4-, and 5- grams that appear at least 10 times (Lin et al., 2009). 1The FSA’s initial state is the unigram history #, and its final states (which have no outgoing arcs) are the other states whose n-gram labels end in #. Here # is a boundary symbol that falls between sentences. To compute the weighted transitions, sentence boundaries must be manually or automatically annotated, either on the training corpus as in our present experiments, or directly on the training n-grams if we have only those. To automatically find boundaries in an n-gram collection, one could apply a local classifier to each n-gram. But in principle, one could exploit more context and get</context>
<context position="18876" citStr="Lin et al., 2009" startWordPosition="3290" endWordPosition="3293">of all multinomial parameters. If nq = np, then our variational gap can be made 0 as in ordinary non-variational EM (see section 3.5). In our experiments, however, we save memory by choosing nq = 1. Thus, our variational gap is tight to the extent that a word’s POS tag under the model pθ is conditionally independent of previous tags and the rest of the sentence, given an n-word window.4 This is the assumption made by local classification models (Punyakanok et al., 2005; Toutanova and Johnson, 2007). Note that it is milder than the “one tagging per n-gram” hypothesis (Dawborn and Curran, 2009; Lin et al., 2009), which claims that each 5-gram (and therefore each sentence!) is unambiguous as to its full tagging. In contrast, we allow that a tag may be ambiguous even given an n-word window; we merely suppose that there is no further disambiguating information accessible to pθ.5 We can encode the resulting cq(w, t) as an FST. With nq = 1, the states of cq are isomorphic to the states of c. However, an arc in c from defg with label h and weight 0.2 is replaced in cq by several arcs—one per tag t—with label h : t and weight 0.2 · qφ(t |defgh).6 We remark that an encoding of 3A conditional Markov model is </context>
</contexts>
<marker>Lin, Church, Ji, Sekine, Yarowsky, Bergsma, Patil, Pitler, Lathbury, Rao, Dalwani, Narsale, 2009</marker>
<rawString>D. Lin, K. Church, H. Ji, S. Sekine, D. Yarowsky, S. Bergsma, K. Patil, E. Pitler, R. Lathbury, V. Rao, K. Dalwani, and S. Narsale. 2009. Unsupervised acquisition of lexical knowledge from n-grams. Summer workshop technical report, Center for Language and Speech Processing, Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David J C MacKay</author>
</authors>
<title>Ensemble learning for hidden Markov models.</title>
<date>1997</date>
<note>http://www.inference. phy.cam.ac.uk/mackay/abstracts/ ensemblePaper.html.</note>
<contexts>
<context position="37097" citStr="MacKay, 1997" startWordPosition="6685" endWordPosition="6686"> n-grams via the language model c. The states of the HMM correspond to POS tags as in Figure 1. All transitions are allowed, but not all emissions. If a word is listed in a provided “dictionary” with its possible tags, then other tags are given 0 probability of emitting that word. The EM algorithm uses the corpus to learn transition and emission probabilities that explain the data under this constraint. The constraint ensures that the learned states have something to do with true POS tags. Merialdo (1994) spawned a long line of work on this task. Ideas have included Bayesian learning methods (MacKay, 1997; Goldwater and Griffiths, 2007; Johnson, 2007), better initial parameters (Goldberg et al., 2008), and learning how to constrain the possible parts of speech for a word (Ravi and Knight, 2008), as well as non-HMM sequence models (Smith and Eisner, 2005; Haghighi and Klein, 2006; Toutanova and Johnson, 2007). Most of this work has used the Penn Treebank (Marcus et al., 1993) as a dataset. While this million-word Wall Street Journal (WSJ) corpus is one of the largest that is manually annotated with parts of speech, unsupervised learning methods could take advantage of vast amounts of unannotate</context>
</contexts>
<marker>MacKay, 1997</marker>
<rawString>David J. C. MacKay. 1997. Ensemble learning for hidden Markov models. http://www.inference. phy.cam.ac.uk/mackay/abstracts/ ensemblePaper.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics.</title>
<date>1993</date>
<contexts>
<context position="37474" citStr="Marcus et al., 1993" startWordPosition="6747" endWordPosition="6750">plain the data under this constraint. The constraint ensures that the learned states have something to do with true POS tags. Merialdo (1994) spawned a long line of work on this task. Ideas have included Bayesian learning methods (MacKay, 1997; Goldwater and Griffiths, 2007; Johnson, 2007), better initial parameters (Goldberg et al., 2008), and learning how to constrain the possible parts of speech for a word (Ravi and Knight, 2008), as well as non-HMM sequence models (Smith and Eisner, 2005; Haghighi and Klein, 2006; Toutanova and Johnson, 2007). Most of this work has used the Penn Treebank (Marcus et al., 1993) as a dataset. While this million-word Wall Street Journal (WSJ) corpus is one of the largest that is manually annotated with parts of speech, unsupervised learning methods could take advantage of vast amounts of unannotated text. In practice, runtime concerns have sometimes led researchers to use small subsets of the Penn Treebank (Goldwater and Griffiths, 2007; Smith and Eisner, 2005; Haghighi and Klein, 2006). Our goal is to point the way to using even larger datasets. The reason for all this past research is that (Merialdo, 1994) was a negative result: while EM is guaranteed to improve the</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Dayne Freitag</author>
<author>Fernando Pereira</author>
</authors>
<title>Maximum entropy Markov models for information extraction and segmentation.</title>
<date>2000</date>
<booktitle>In Proc. of ICML,</booktitle>
<pages>591--598</pages>
<contexts>
<context position="19547" citStr="McCallum et al., 2000" startWordPosition="3418" endWordPosition="3421">ch sentence!) is unambiguous as to its full tagging. In contrast, we allow that a tag may be ambiguous even given an n-word window; we merely suppose that there is no further disambiguating information accessible to pθ.5 We can encode the resulting cq(w, t) as an FST. With nq = 1, the states of cq are isomorphic to the states of c. However, an arc in c from defg with label h and weight 0.2 is replaced in cq by several arcs—one per tag t—with label h : t and weight 0.2 · qφ(t |defgh).6 We remark that an encoding of 3A conditional Markov model is a simple case of a maximum-entropy Markov model (McCallum et al., 2000). 4At present, the word being tagged is the last word in the window. We do have an efficient modification in which the window is centered on the word, by using an FST cq that delays the emission of a tag until up to 2 subsequent words have been seen. 5With difficulty, one can construct English examples that violate our assumption. (1) “Some monitor lizards from Africa... ” versus “Some monitor lizards from a distance... ”: there are words far away from “monitor” that help disambiguate whether “monitor” is a noun or a verb. (“Monitor lizards” are a species, but some people like to monitor lizar</context>
</contexts>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>Andrew McCallum, Dayne Freitag, and Fernando Pereira. 2000. Maximum entropy Markov models for information extraction and segmentation. In Proc. of ICML, pages 591–598.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Merialdo</author>
</authors>
<title>Tagging English text with a probabilistic model.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<pages>171</pages>
<contexts>
<context position="2481" citStr="Merialdo, 1994" startWordPosition="398" endWordPosition="399">e to scale up to the Google n-gram corpus (Brants and Franz, 2006) and learn a more detailed, explanatory joint model of tags, syntactic dependencies, and topics. Our intuition here is that web-scale data may be needed to learn the large number of lexically and contextually specific parameters. &amp;quot;Work was supported in part by NSF grant No. 0347822. 1.1 Formulation Let w (“words”) denote an observation sequence, and let t (“tags”) denote a hidden HMM state sequence that may explain w. This terminology is taken from the literature on inducing part-of-speech (POS) taggers using a first-order HMM (Merialdo, 1994), which we use as our experimental setting. Maximum a posteriori (MAP) training of an HMM pg seeks parameters 0 to maximize p0(w, t) + log Pr prior(0) (1) where c is an empirical distribution that assigns probability 1/N to each of the N sentences in a training corpus. Our technical challenge is to generalize this MAP criterion to other, structured distributions c that compactly approximate the corpus. Specifically, we address the case where c is given by any probabilistic FSA, such as a backoff language model—that is, a variable-order Markov model estimated from corpus statistics. Similar sen</context>
<context position="34902" citStr="Merialdo (1994)" startWordPosition="6288" endWordPosition="6289">ting ∂LA/∂φa = 0 implies that φa is proportional to exp((ra + EzEZa αza log θza)/αa).20 Rather than doing block coordinate ascent by updating one φ block at a time (and then recomputing ra values for all blocks, which is slow), one can take an approximate step by updating all blocks in parallel. We find that replacing the E-step with a single parallel step still tends to improve the objective, and that this approximate variational EM is faster than gradient ascent with comparable results.21 4 Experiments 4.1 Constrained unsupervised HMM learning We follow the unsupervised POS tagging setup of Merialdo (1994) and many others (Smith and Eisner, 2005; Haghighi and Klein, 2006; Toutanova and Johnson, 2007; Goldwater and Griffiths, 2007; Johnson, 2007). Given a corpus of sentences, one seeks the maximum-likelihood or MAP parameters of a bigram HMM (np = 2). The observed sentences, for qy(t |h.w, h9) to the probability that t begins with t if we randomly draw a suffix w — c(· |hew) and randomly tag ww with t — pe(· |ww, h9). This is equivalent to using pe with the backward algorithm to conditionally tag each possible suffix. 19The first component of &amp;za�βza is αzaβza = αza · 1. 20If a is an arc of cqy </context>
<context position="36995" citStr="Merialdo (1994)" startWordPosition="6667" endWordPosition="6668"> EM is a special case of this strategy. us, are replaced by the faux sentences extrapolated from observed n-grams via the language model c. The states of the HMM correspond to POS tags as in Figure 1. All transitions are allowed, but not all emissions. If a word is listed in a provided “dictionary” with its possible tags, then other tags are given 0 probability of emitting that word. The EM algorithm uses the corpus to learn transition and emission probabilities that explain the data under this constraint. The constraint ensures that the learned states have something to do with true POS tags. Merialdo (1994) spawned a long line of work on this task. Ideas have included Bayesian learning methods (MacKay, 1997; Goldwater and Griffiths, 2007; Johnson, 2007), better initial parameters (Goldberg et al., 2008), and learning how to constrain the possible parts of speech for a word (Ravi and Knight, 2008), as well as non-HMM sequence models (Smith and Eisner, 2005; Haghighi and Klein, 2006; Toutanova and Johnson, 2007). Most of this work has used the Penn Treebank (Marcus et al., 1993) as a dataset. While this million-word Wall Street Journal (WSJ) corpus is one of the largest that is manually annotated </context>
<context position="39670" citStr="Merialdo (1994)" startWordPosition="7124" endWordPosition="7125">ent sizes: • WSJ-big (910k words → 441k n-grams @ cutoff 3), • Giga-20 (20M words → 2.9M n-grams @ cutoff 10), • Giga-200 (200M wds → 14.4M n-grams @ cutoff 20). These were drawn from the Penn Treebank (sections 2–23) and the English Gigaword corpus (Parker et al., 2009). For held-out evaluation, we use WSJsmall (Penn Treebank section 0) or WSJ-big. We estimate backoff language models for these corpora based on collections of n-grams with n &lt; 5. In this work, we select the n-grams by simple count cutoffs as shown above,22 taking care to keep all 2- grams as mentioned in footnote 2. Similar to Merialdo (1994), we use a tag dictionary which limits the possible tags of a word to those it was observed with in the WSJ, provided that the word was observed at least 5 times in the WSJ. We used the reduced tagset of Smith and Eisner (2005), which collapses the original 45 fine-grained part-ofspeech tags into just 17 coarser tags. 4.3 Results In all experiments, our method achieves similar accuracy though slightly worse likelihood. Although this method is meant to be a fast approximation of EM, standard EM is faster on the smallest dataset (WSJ-big). This is because this corpus is not much bigger than the </context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>B. Merialdo. 1994. Tagging English text with a probabilistic model. Computational Linguistics, 20(2):155– 171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-B Michel</author>
<author>Y K Shen</author>
<author>A P Aiden</author>
<author>A Veres</author>
<author>M K Gray</author>
<author>W Brockman</author>
</authors>
<title>The Google Books Team,</title>
<date>2010</date>
<journal>Science,</journal>
<volume>331</volume>
<issue>6014</issue>
<contexts>
<context position="5478" citStr="Michel et al. (2010)" startWordPosition="896" endWordPosition="899">quent 5- gram collapses into a single parameter of the estimated distribution that only needs to be processed once per training iteration. By pruning low-count n-grams or reducing the maximum n below 5, one can further increase data volume for the fixed time budget at the expense of approximation quality. Third, one may not have access to the original corpus. If one lacks the resources to harvest the web, the Google n-gram corpus was derived from over a trillion words of English web text. Privacy or copyright issues may prevent access, but one may still be able to work with n-gram statistics: Michel et al. (2010) used such statistics from 5 million scanned books. Several systems use n-gram counts (Bergsma et al., 2009; Lin et al., 2009) or other web statistics (Lapata and Keller, 2005) as features within a classifier. A large language model from ngram counts yields an effective prior over hypotheses in tasks like machine translation (Brants et al., 2007). We similarly construct an n-gram model, but treat it as the primary training data whose structure is to be explained by the generative HMM. Thus our criterion does not explain the n-grams in isolation, but rather tries to explain the likely full sent</context>
</contexts>
<marker>Michel, Shen, Aiden, Veres, Gray, Brockman, 2010</marker>
<rawString>J.-B. Michel, Y. K. Shen, A. P. Aiden, A. Veres, M. K. Gray, W. Brockman, The Google Books Team, J. P. Pickett, D. Hoiberg, D. Clancy, P. Norvig, J. Orwant, S. Pinker, M. A. Nowak, and E. L. Aiden. 2010. Quantitative analysis of culture using millions of digitized books. Science, 331(6014):176–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Mark-Jan Nederhof</author>
</authors>
<title>Regular approximation of context-free grammars through transformation.</title>
<date>2001</date>
<booktitle>In Jean-Claude Junqua and Gertjan</booktitle>
<pages>153--163</pages>
<editor>van Noord, editors,</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<location>The Netherlands,</location>
<contexts>
<context position="4358" citStr="Mohri and Nederhof, 2001" startWordPosition="704" endWordPosition="707">ammars. We regard parameter estimation from such a distribution c (rather than from a sample) as a natural question. Previous work on modeling c with a distribution from another family was motivated by approximating a grammar or � �c(w) log N · t w 131 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 131–141, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics model rather than generalizing from a dataset, and hence removed latent variables while adding parameters (Nederhof, 2000; Mohri and Nederhof, 2001; Liang et al., 2008), whereas we do the reverse. Second, in practice, one may want to incorporate massive amounts of (possibly out-of-domain) data in order to get better coverage of phenomena. Massive datasets usually require a simple model (given a time budget). We propose that it may be possible to use a lot of data and a good model by reducing the accuracy of the data representation instead. While training will become more complicated, it can still result in an overall speedup, because a frequent 5- gram collapses into a single parameter of the estimated distribution that only needs to be </context>
</contexts>
<marker>Mohri, Nederhof, 2001</marker>
<rawString>Mehryar Mohri and Mark-Jan Nederhof. 2001. Regular approximation of context-free grammars through transformation. In Jean-Claude Junqua and Gertjan van Noord, editors, Robustness in Language and Speech Technology, chapter 9, pages 153–163. Kluwer Academic Publishers, The Netherlands, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Fernando Pereira</author>
<author>Michael Riley</author>
</authors>
<title>The design principles of a weighted finitestate transducer library.</title>
<date>2000</date>
<journal>Theoretical Computer Science,</journal>
<volume>231</volume>
<issue>1</issue>
<contexts>
<context position="27599" citStr="Mohri et al., 2000" startWordPosition="4934" endWordPosition="4937">er FST (as bad as increasing nq to np). The Za trick uses O(CTnq) rather than O(CTnp) space to store the FST, where C is the number of arcs in c (= number of training n-grams) and T is the number of tag types. With or without the trick, runtime is O(CTnp+BCTnq), where B is the num11By concatenating z’s start state’s hq with the tags along z. Typically z has length np − nq (and Za consists of the paths of that length to a’s start state). However, z may be longer if it contains 4) arcs, or shorter if it begins with an initial state. 12Constructed by lazy finite-state intersection of cqφ and pθ (Mohri et al., 2000). These do not have to be n-gram taggers, but must be same-length FSTs (these are closed under intersection) and unambiguous. Define arc values in both FSTs such that for any (w, t), cqφ and pθ accept (w, t) along unique paths of total values v = − log qφ(t |w) and v0 = log pθ(w, t), respectively. We now lift the weights into the expectation semiring (Eisner, 2002) as follows. In cqφ, replace arc a’s weight ka with the semiring weight (ka, kava). In pθ, replace arc a0’s weight with (1, v0a,). Then if k = cqφ(w, t), the intersected FST accepts (w, t) with weight (k, k(v + v0)). The Li //expecta</context>
</contexts>
<marker>Mohri, Pereira, Riley, 2000</marker>
<rawString>Mehryar Mohri, Fernando Pereira, and Michael Riley. 2000. The design principles of a weighted finitestate transducer library. Theoretical Computer Science, 231(1):17–32, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford M Neal</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>A view of the EM algorithm that justifies incremental, sparse, and other variants.</title>
<date>1998</date>
<booktitle>Learning in Graphical Models,</booktitle>
<pages>355--368</pages>
<editor>In M.I. Jordan, editor,</editor>
<publisher>Kluwer.</publisher>
<contexts>
<context position="8227" citStr="Neal and Hinton (1998)" startWordPosition="1342" endWordPosition="1345"> a higher-order or factorial HMM. Our more general technique extends to other latentvariable models, although it suffers from variational EM’s usual local optima and approximation errors. 2 A variational lower bound Our starting point is the variational EM algorithm (Jordan et al., 1999). Recall that this maximizes a lower bound on the MAP criterion of equation 1, by bounding the log-likelihood subterm as follows: log Et po(w, t) (2) = log Et q(t)(po(w, t)/q(t)) &gt; Et q(t) log(po(w, t)/q(t)) = E,(t)[log po(w, t) − log q(t) (3) This use of Jensen’s inequality is valid for any distribution q. As Neal and Hinton (1998) show, the EM algorithm (Dempster et al., 1977) can be regarded as locally maximizing the resulting lower bound by alternating optimization, where q is a free parameter. The E-step optimizes q for fixed 0, and the Mstep optimizes 0 for fixed q. These computations are tractable for HMMs, since the distribution q(t) = po(t |w) that is optimal at the E-step (which makes the inequality tight) can be represented as a lattice (a certain kind of weighted DFA), and this makes the M-step tractable via the forward-backward algorithm. However, there are many extensions such as 132 factorial HMMs and Baye</context>
</contexts>
<marker>Neal, Hinton, 1998</marker>
<rawString>Radford M. Neal and Geoffrey E. Hinton. 1998. A view of the EM algorithm that justifies incremental, sparse, and other variants. In M.I. Jordan, editor, Learning in Graphical Models, pages 355–368. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
</authors>
<title>Practical experiments with regular approximation of context-free languages.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="4332" citStr="Nederhof, 2000" startWordPosition="702" endWordPosition="703"> automata and grammars. We regard parameter estimation from such a distribution c (rather than from a sample) as a natural question. Previous work on modeling c with a distribution from another family was motivated by approximating a grammar or � �c(w) log N · t w 131 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 131–141, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics model rather than generalizing from a dataset, and hence removed latent variables while adding parameters (Nederhof, 2000; Mohri and Nederhof, 2001; Liang et al., 2008), whereas we do the reverse. Second, in practice, one may want to incorporate massive amounts of (possibly out-of-domain) data in order to get better coverage of phenomena. Massive datasets usually require a simple model (given a time budget). We propose that it may be possible to use a lot of data and a good model by reducing the accuracy of the data representation instead. While training will become more complicated, it can still result in an overall speedup, because a frequent 5- gram collapses into a single parameter of the estimated distribut</context>
</contexts>
<marker>Nederhof, 2000</marker>
<rawString>Mark-Jan Nederhof. 2000. Practical experiments with regular approximation of context-free languages. Computational Linguistics, 26(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Parker</author>
<author>David Graff</author>
<author>Junbo Kong</author>
<author>Ke Chen</author>
<author>Kazuaki Maeda</author>
</authors>
<title>English Gigaword fourth edition. Linguistic Data Consortium,</title>
<date>2009</date>
<location>Philadelphia. LDC2009T13.</location>
<contexts>
<context position="39326" citStr="Parker et al., 2009" startWordPosition="7062" endWordPosition="7065">e our present methodological exploration on whether it is possible to scale up the original setting. 138 4.2 Setup We investigate how much performance degrades when we approximate the corpus and train approximately with nQ = 1. We examine two measures: likelihood on a held-out corpus and accuracy in POS tagging. We train on corpora of three different sizes: • WSJ-big (910k words → 441k n-grams @ cutoff 3), • Giga-20 (20M words → 2.9M n-grams @ cutoff 10), • Giga-200 (200M wds → 14.4M n-grams @ cutoff 20). These were drawn from the Penn Treebank (sections 2–23) and the English Gigaword corpus (Parker et al., 2009). For held-out evaluation, we use WSJsmall (Penn Treebank section 0) or WSJ-big. We estimate backoff language models for these corpora based on collections of n-grams with n &lt; 5. In this work, we select the n-grams by simple count cutoffs as shown above,22 taking care to keep all 2- grams as mentioned in footnote 2. Similar to Merialdo (1994), we use a tag dictionary which limits the possible tags of a word to those it was observed with in the WSJ, provided that the word was observed at least 5 times in the WSJ. We used the reduced tagset of Smith and Eisner (2005), which collapses the origina</context>
</contexts>
<marker>Parker, Graff, Kong, Chen, Maeda, 2009</marker>
<rawString>Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2009. English Gigaword fourth edition. Linguistic Data Consortium, Philadelphia. LDC2009T13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
<author>D Zimak</author>
</authors>
<title>Learning and inference over constrained output.</title>
<date>2005</date>
<booktitle>In Proc. of IJCAI,</booktitle>
<pages>1124--1129</pages>
<contexts>
<context position="18732" citStr="Punyakanok et al., 2005" startWordPosition="3265" endWordPosition="3268">nd on the word n-gram ending at position i (where n is as large as possible such that this n-gram is in our training collection). 0 is the collection of all multinomial parameters. If nq = np, then our variational gap can be made 0 as in ordinary non-variational EM (see section 3.5). In our experiments, however, we save memory by choosing nq = 1. Thus, our variational gap is tight to the extent that a word’s POS tag under the model pθ is conditionally independent of previous tags and the rest of the sentence, given an n-word window.4 This is the assumption made by local classification models (Punyakanok et al., 2005; Toutanova and Johnson, 2007). Note that it is milder than the “one tagging per n-gram” hypothesis (Dawborn and Curran, 2009; Lin et al., 2009), which claims that each 5-gram (and therefore each sentence!) is unambiguous as to its full tagging. In contrast, we allow that a tag may be ambiguous even given an n-word window; we merely suppose that there is no further disambiguating information accessible to pθ.5 We can encode the resulting cq(w, t) as an FST. With nq = 1, the states of cq are isomorphic to the states of c. However, an arc in c from defg with label h and weight 0.2 is replaced in</context>
</contexts>
<marker>Punyakanok, Roth, Yih, Zimak, 2005</marker>
<rawString>V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2005. Learning and inference over constrained output. In Proc. of IJCAI, pages 1124–1129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence R Rabiner</author>
</authors>
<title>A tutorial on hidden Markov models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>Proc. of the IEEE,</booktitle>
<volume>77</volume>
<issue>2</issue>
<contexts>
<context position="11775" citStr="Rabiner, 1989" startWordPosition="1988" endWordPosition="1989">countered along the path. Our FSMs are weighted in the (+, x) semiring: the weight of any path is the product (x) of its arc weights, while the weight assigned to a string or string pair is the total weight (+) of all its accepting paths. An FSM is unambiguous if each string or string pair has at most one accepting path. Figure 1 reviews how to represent an HMM POS tagger as an FST (b), and how composing this with an FSA that accepts a single sentence gives us the familiar HMM tagging lattice as an FST (c). The forward-backward algorithm sums over paths in the lattice via dynamic programming (Rabiner, 1989). In section 3.1, we replace the straight-line FSA of Figure 1a with an FSA that defines a more general distribution c(w) over many sentences. Note that we cannot simply use this as a drop-in replacement in the construction of Figure 1. That would correspond to running EM on a single but uncertain sentence (distributed as c(w)) rather than a collection of observed sentences. For example, in the case of an ordinary training corpus of N sentences, the new FSA would be a parallel union (sum) of N straight-line paths—rather than a serial concatenation (product) of those paths as in ordinary EM (se</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>Lawrence R. Rabiner. 1989. A tutorial on hidden Markov models and selected applications in speech recognition. Proc. of the IEEE, 77(2):257–286, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Minimized models for unsupervised part-of-speech tagging.</title>
<date>2008</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>504--512</pages>
<contexts>
<context position="37290" citStr="Ravi and Knight, 2008" startWordPosition="6715" endWordPosition="6718">dictionary” with its possible tags, then other tags are given 0 probability of emitting that word. The EM algorithm uses the corpus to learn transition and emission probabilities that explain the data under this constraint. The constraint ensures that the learned states have something to do with true POS tags. Merialdo (1994) spawned a long line of work on this task. Ideas have included Bayesian learning methods (MacKay, 1997; Goldwater and Griffiths, 2007; Johnson, 2007), better initial parameters (Goldberg et al., 2008), and learning how to constrain the possible parts of speech for a word (Ravi and Knight, 2008), as well as non-HMM sequence models (Smith and Eisner, 2005; Haghighi and Klein, 2006; Toutanova and Johnson, 2007). Most of this work has used the Penn Treebank (Marcus et al., 1993) as a dataset. While this million-word Wall Street Journal (WSJ) corpus is one of the largest that is manually annotated with parts of speech, unsupervised learning methods could take advantage of vast amounts of unannotated text. In practice, runtime concerns have sometimes led researchers to use small subsets of the Penn Treebank (Goldwater and Griffiths, 2007; Smith and Eisner, 2005; Haghighi and Klein, 2006).</context>
</contexts>
<marker>Ravi, Knight, 2008</marker>
<rawString>Sujith Ravi and Kevin Knight. 2008. Minimized models for unsupervised part-of-speech tagging. In Proc. of ACL, pages 504–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>354--362</pages>
<contexts>
<context position="34942" citStr="Smith and Eisner, 2005" startWordPosition="6293" endWordPosition="6297">is proportional to exp((ra + EzEZa αza log θza)/αa).20 Rather than doing block coordinate ascent by updating one φ block at a time (and then recomputing ra values for all blocks, which is slow), one can take an approximate step by updating all blocks in parallel. We find that replacing the E-step with a single parallel step still tends to improve the objective, and that this approximate variational EM is faster than gradient ascent with comparable results.21 4 Experiments 4.1 Constrained unsupervised HMM learning We follow the unsupervised POS tagging setup of Merialdo (1994) and many others (Smith and Eisner, 2005; Haghighi and Klein, 2006; Toutanova and Johnson, 2007; Goldwater and Griffiths, 2007; Johnson, 2007). Given a corpus of sentences, one seeks the maximum-likelihood or MAP parameters of a bigram HMM (np = 2). The observed sentences, for qy(t |h.w, h9) to the probability that t begins with t if we randomly draw a suffix w — c(· |hew) and randomly tag ww with t — pe(· |ww, h9). This is equivalent to using pe with the backward algorithm to conditionally tag each possible suffix. 19The first component of &amp;za�βza is αzaβza = αza · 1. 20If a is an arc of cqy then ∂�r/∂φa is the second component of </context>
<context position="37350" citStr="Smith and Eisner, 2005" startWordPosition="6726" endWordPosition="6729">n 0 probability of emitting that word. The EM algorithm uses the corpus to learn transition and emission probabilities that explain the data under this constraint. The constraint ensures that the learned states have something to do with true POS tags. Merialdo (1994) spawned a long line of work on this task. Ideas have included Bayesian learning methods (MacKay, 1997; Goldwater and Griffiths, 2007; Johnson, 2007), better initial parameters (Goldberg et al., 2008), and learning how to constrain the possible parts of speech for a word (Ravi and Knight, 2008), as well as non-HMM sequence models (Smith and Eisner, 2005; Haghighi and Klein, 2006; Toutanova and Johnson, 2007). Most of this work has used the Penn Treebank (Marcus et al., 1993) as a dataset. While this million-word Wall Street Journal (WSJ) corpus is one of the largest that is manually annotated with parts of speech, unsupervised learning methods could take advantage of vast amounts of unannotated text. In practice, runtime concerns have sometimes led researchers to use small subsets of the Penn Treebank (Goldwater and Griffiths, 2007; Smith and Eisner, 2005; Haghighi and Klein, 2006). Our goal is to point the way to using even larger datasets.</context>
<context position="39897" citStr="Smith and Eisner (2005)" startWordPosition="7168" endWordPosition="7171">nd the English Gigaword corpus (Parker et al., 2009). For held-out evaluation, we use WSJsmall (Penn Treebank section 0) or WSJ-big. We estimate backoff language models for these corpora based on collections of n-grams with n &lt; 5. In this work, we select the n-grams by simple count cutoffs as shown above,22 taking care to keep all 2- grams as mentioned in footnote 2. Similar to Merialdo (1994), we use a tag dictionary which limits the possible tags of a word to those it was observed with in the WSJ, provided that the word was observed at least 5 times in the WSJ. We used the reduced tagset of Smith and Eisner (2005), which collapses the original 45 fine-grained part-ofspeech tags into just 17 coarser tags. 4.3 Results In all experiments, our method achieves similar accuracy though slightly worse likelihood. Although this method is meant to be a fast approximation of EM, standard EM is faster on the smallest dataset (WSJ-big). This is because this corpus is not much bigger than the 5-gram language model built from it (at our current pruning level), and so the overhead of the more complex n-gram EM method is a net disadvantage. However, when moving to larger corpora, the iterations of n-gram EM become as f</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A. Smith and Jason Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Proc. of ACL, pages 354–362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Staden</author>
</authors>
<title>A strategy of DNA sequencing employing computer programs.</title>
<date>1979</date>
<journal>Nucleic Acids Research,</journal>
<volume>6</volume>
<issue>7</issue>
<contexts>
<context position="6272" citStr="Staden, 1979" startWordPosition="1027" endWordPosition="1028">eatures within a classifier. A large language model from ngram counts yields an effective prior over hypotheses in tasks like machine translation (Brants et al., 2007). We similarly construct an n-gram model, but treat it as the primary training data whose structure is to be explained by the generative HMM. Thus our criterion does not explain the n-grams in isolation, but rather tries to explain the likely full sentences w that the model reconstructed from overlapping ngrams. This is something like shotgun sequencing, in which likely DNA strings are reconstructed from overlapping short reads (Staden, 1979); however, we train an HMM on the resulting distribution rather than merely trying to find its mode. Finally, unsupervised HMM training discovers latent structure by approximating an empirical distribution c (the corpus) with a latent-variable distribution p (the trained HMM) that has fewer parameters. We show how to do the same where the distribution c is not a corpus but a finite-state distribution. In general, this finite-state c could represent some sophisticated estimate of the population distribution, using shrinkage, word classes, neural-net predictors, etc. to generalize in some way be</context>
</contexts>
<marker>Staden, 1979</marker>
<rawString>R. Staden. 1979. A strategy of DNA sequencing employing computer programs. Nucleic Acids Research, 6(7):2601–2610, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Entropy-based pruning of backoff language models.</title>
<date>2000</date>
<booktitle>In DARPA Broadcast News Transcription and Understanding Workshop,</booktitle>
<pages>270--274</pages>
<contexts>
<context position="40845" citStr="Stolcke, 2000" startWordPosition="7331" endWordPosition="7332">his corpus is not much bigger than the 5-gram language model built from it (at our current pruning level), and so the overhead of the more complex n-gram EM method is a net disadvantage. However, when moving to larger corpora, the iterations of n-gram EM become as fast as standard EM and then faster. We expect this trend to continue as one moves to much larger datasets, as the compression ratio of the pruned language model relative to the original corpus will only improve. The Google n-gram corpus is based on 50x more data than our largest but could be handled in RAM. 22Entropy-based pruning (Stolcke, 2000) may be a better selection method when one is in a position to choose. However, count cutoffs were already used in the creation of the Google n-gram corpus, and more complex methods of pruning may not be practical for very large datasets. Time Time Figure 2: POS-tagging accuracy and log-likelihood after each iteration, measured on WSJ-big when training on the Gigaword datasets, else on WSJ-small. Runtime and log-likelihood are scaled differently for each dataset. Replacing EM with our method changes runtime per iteration from 1.4s → 3.5s, 48s → 47s, and 506s → 321s. 5 Conclusions We presented </context>
</contexts>
<marker>Stolcke, 2000</marker>
<rawString>Andreas Stolcke. 2000. Entropy-based pruning of backoff language models. In DARPA Broadcast News Transcription and Understanding Workshop, pages 270–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Mark Johnson</author>
</authors>
<title>A Bayesian LDA-based model for semi-supervised partof-speech tagging.</title>
<date>2007</date>
<booktitle>In Proc. of NIPS,</booktitle>
<volume>20</volume>
<contexts>
<context position="18762" citStr="Toutanova and Johnson, 2007" startWordPosition="3269" endWordPosition="3272">ing at position i (where n is as large as possible such that this n-gram is in our training collection). 0 is the collection of all multinomial parameters. If nq = np, then our variational gap can be made 0 as in ordinary non-variational EM (see section 3.5). In our experiments, however, we save memory by choosing nq = 1. Thus, our variational gap is tight to the extent that a word’s POS tag under the model pθ is conditionally independent of previous tags and the rest of the sentence, given an n-word window.4 This is the assumption made by local classification models (Punyakanok et al., 2005; Toutanova and Johnson, 2007). Note that it is milder than the “one tagging per n-gram” hypothesis (Dawborn and Curran, 2009; Lin et al., 2009), which claims that each 5-gram (and therefore each sentence!) is unambiguous as to its full tagging. In contrast, we allow that a tag may be ambiguous even given an n-word window; we merely suppose that there is no further disambiguating information accessible to pθ.5 We can encode the resulting cq(w, t) as an FST. With nq = 1, the states of cq are isomorphic to the states of c. However, an arc in c from defg with label h and weight 0.2 is replaced in cq by several arcs—one per ta</context>
<context position="34997" citStr="Toutanova and Johnson, 2007" startWordPosition="6302" endWordPosition="6305">).20 Rather than doing block coordinate ascent by updating one φ block at a time (and then recomputing ra values for all blocks, which is slow), one can take an approximate step by updating all blocks in parallel. We find that replacing the E-step with a single parallel step still tends to improve the objective, and that this approximate variational EM is faster than gradient ascent with comparable results.21 4 Experiments 4.1 Constrained unsupervised HMM learning We follow the unsupervised POS tagging setup of Merialdo (1994) and many others (Smith and Eisner, 2005; Haghighi and Klein, 2006; Toutanova and Johnson, 2007; Goldwater and Griffiths, 2007; Johnson, 2007). Given a corpus of sentences, one seeks the maximum-likelihood or MAP parameters of a bigram HMM (np = 2). The observed sentences, for qy(t |h.w, h9) to the probability that t begins with t if we randomly draw a suffix w — c(· |hew) and randomly tag ww with t — pe(· |ww, h9). This is equivalent to using pe with the backward algorithm to conditionally tag each possible suffix. 19The first component of &amp;za�βza is αzaβza = αza · 1. 20If a is an arc of cqy then ∂�r/∂φa is the second component of Ez∈Z, &amp;za(∂�kza/∂φa)�βza. Then ∂LA/∂φa works out to Ez∈</context>
<context position="37406" citStr="Toutanova and Johnson, 2007" startWordPosition="6734" endWordPosition="6737">rithm uses the corpus to learn transition and emission probabilities that explain the data under this constraint. The constraint ensures that the learned states have something to do with true POS tags. Merialdo (1994) spawned a long line of work on this task. Ideas have included Bayesian learning methods (MacKay, 1997; Goldwater and Griffiths, 2007; Johnson, 2007), better initial parameters (Goldberg et al., 2008), and learning how to constrain the possible parts of speech for a word (Ravi and Knight, 2008), as well as non-HMM sequence models (Smith and Eisner, 2005; Haghighi and Klein, 2006; Toutanova and Johnson, 2007). Most of this work has used the Penn Treebank (Marcus et al., 1993) as a dataset. While this million-word Wall Street Journal (WSJ) corpus is one of the largest that is manually annotated with parts of speech, unsupervised learning methods could take advantage of vast amounts of unannotated text. In practice, runtime concerns have sometimes led researchers to use small subsets of the Penn Treebank (Goldwater and Griffiths, 2007; Smith and Eisner, 2005; Haghighi and Klein, 2006). Our goal is to point the way to using even larger datasets. The reason for all this past research is that (Merialdo</context>
</contexts>
<marker>Toutanova, Johnson, 2007</marker>
<rawString>Kristina Toutanova and Mark Johnson. 2007. A Bayesian LDA-based model for semi-supervised partof-speech tagging. In Proc. of NIPS, volume 20.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>