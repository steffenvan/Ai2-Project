<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002142">
<title confidence="0.9987715">
UBA: Using Automatic Translation and Wikipedia for
Cross-Lingual Lexical Substitution
</title>
<author confidence="0.98666">
Pierpaolo Basile
</author>
<affiliation confidence="0.9977395">
Dept. of Computer Science
University of Bari “Aldo Moro”
</affiliation>
<address confidence="0.982924">
Via E. Orabona, 4
70125 Bari (ITALY)
</address>
<email confidence="0.998855">
basilepp@di.uniba.it
</email>
<sectionHeader confidence="0.997381" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999897766666667">
This paper presents the participation of the
University of Bari (UBA) at the SemEval-
2010 Cross-Lingual Lexical Substitution
Task. The goal of the task is to substi-
tute a word in a language L3, which oc-
curs in a particular context, by provid-
ing the best synonyms in a different lan-
guage Lt which fit in that context. This
task has a strict relation with the task of
automatic machine translation, but there
are some differences: Cross-lingual lexi-
cal substitution targets one word at a time
and the main goal is to find as many good
translations as possible for the given tar-
get word. Moreover, there are some con-
nections with Word Sense Disambiguation
(WSD) algorithms. Indeed, understand-
ing the meaning of the target word is nec-
essary to find the best substitutions. An
important aspect of this kind of task is
the possibility of finding synonyms with-
out using a particular sense inventory or a
specific parallel corpus, thus allowing the
participation of unsupervised approaches.
UBA proposes two systems: the former is
based on an automatic translation system
which exploits Google Translator, the lat-
ter is based on a parallel corpus approach
which relies on Wikipedia in order to find
the best substitutions.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.981384428571428">
The goal of the Cross-Lingual Lexical Substitu-
tion (CLLS) task is to substitute a word in a lan-
guage L3, which occurs in a particular context,
by providing the best substitutions in a different
language Lt. In SemEval-2010 the source lan-
guage L3 is English, while the target language Lt
is Spanish. Clearly, this task is related to Lexical
</bodyText>
<author confidence="0.848594">
Giovanni Semeraro
</author>
<affiliation confidence="0.997215">
Dept. of Computer Science
University of Bari “Aldo Moro”
</affiliation>
<address confidence="0.965607">
Via E. Orabona, 4
70125 Bari (ITALY)
</address>
<email confidence="0.99226">
semeraro@di.uniba.it
</email>
<bodyText confidence="0.996800948717949">
Substitution (LS) (McCarthy and Navigli, 2007)
which consists in selecting an alternative word for
a given one in a particular context by preserving
its meaning. The main difference between the LS
task and the CLLS one is that in LS source and
target languages are the same. CLLS is not a easy
task since neither a list of candidate words nor a
specific parallel corpus are supplied by the orga-
nizers. However, this opens the possibility of us-
ing several knowledge sources, instead of a single
one fixed by the task organizers. Therefore, the
system must identify a set of candidate words in
Lt and then select only those words which fit the
context. From another point of view, the cross-
lingual nature of the task allows to exploit auto-
matic machine translation methods, hence the goal
is to find as many good translations as possible for
the given target word. A thorough description of
the task can be found in (Mihalcea et al., 2010;
Sinha et al., 2009).
To easily understand the task, an example fol-
lows. Consider the sentence:
During the siege, George Robertson had
appointed Shuja-ul-Mulk , who was a bright
boy only 12 years old and the youngest
surviving son of Aman-ul-Mulk, as the ruler
of Chitral.
In the previous sentence the target word is
“bright”. Taking into account the meaning of
the word “bright” in this particular context, the
best substitutions in Spanish are: “inteligente”,
“brillante” and “listo”.
We propose two systems to tackle the problem
of CLLS: the first is based on an automatic trans-
lation system which exploits the API of Google
Translator1, the second is based on a parallel cor-
pus approach which relies on Wikipedia. In par-
ticular, in the second approach we use a struc-
tured version of Wikipedia called DBpedia (Bizer
</bodyText>
<footnote confidence="0.992887">
1http://code.google.com/p/google-api-translate-java/
</footnote>
<page confidence="0.95523">
242
</page>
<bodyText confidence="0.942167">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 242–247,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
et al., 2009). Both systems adopt several lexical
resources to select the list of possible substitutions
for a given word. Specifically, we use three differ-
ent dictionaries: Google Dictionary, Babylon Dic-
tionary and Spanishdict. Then, we combine the
dictionaries into a single one, as described in Sec-
tion 2.1.
The paper is organized as follows: Section 2 de-
scribes the strategy we adopted to tackle the CLLS
task, while results of an experimental session we
carried out in order to evaluate the proposed ap-
proaches are presented in Section 3. Conclusions
are discussed in Section 4.
</bodyText>
<sectionHeader confidence="0.997613" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.987132666666667">
Generally speaking, the problem of CLLS can be
coped with a strategy which consists of two steps,
as suggested in (Sinha et al., 2009):
</bodyText>
<listItem confidence="0.9922666">
• candidate collection: in this step several re-
sources are queried to retrieve a list of po-
tential translation candidates for each target
word and part of speech;
• candidate selection: this step concerns the
</listItem>
<bodyText confidence="0.9774159">
ranking of potential candidates, which are the
most suitable ones for each instance, by using
information about the context.
Regarding the candidate collection, we exploit
three dictionaries: Google Dictionary, Babylon
Dictionary and Spanishdict. Each dictionary is
modeled using a strategy described in Section 2.1.
We use the same approach to model each dictio-
nary in order to make easy both the inclusion of fu-
ture dictionaries and the integration with the can-
didate selection step.
Candidate selection is performed in two dif-
ferent ways. The first one relies on the auto-
matic translation of the sentence in which the tar-
get word occurs, in order to find the best substitu-
tions. The second method uses a parallel corpus
built on DBpedia to discover the number of doc-
uments in which the target word is translated by
one of the potential translation candidates. Details
about both methods are reported in Section 2.2
</bodyText>
<subsectionHeader confidence="0.996384">
2.1 Candidate collection
</subsectionHeader>
<bodyText confidence="0.998615">
This section describes the method adopted to re-
trieve the list of potential translation candidates for
each target word and part of speech.
Our strategy combines several bi-lingual dictio-
naries and builds a single list of candidates for
each target word. The involved dictionaries meet
the following requirements:
</bodyText>
<listItem confidence="0.9461864">
1. the source language Ls must be English and
the target one Lt must be Spanish;
2. each dictionary must provide information
about the part of speech;
3. the dictionary must be freely available.
</listItem>
<bodyText confidence="0.999910875">
Moreover, each candidate has a score sij com-
puted by taking into account its rank in the list
of possible translations supplied by the i − th
dictionary. Formally, let us denote by D =
{d1, d2,.. . , dn} the set of n dictionaries and by
Li = {c1, c2, ... , cmi} the list of potential candi-
dates provided by di. The score sij is computed
by the following equation:
</bodyText>
<equation confidence="0.776526">
j E {1, 2, . . . ,mi} (1)
</equation>
<bodyText confidence="0.993674714285714">
Since each list Li has a different size, we adopt
a score normalization strategy based on Z-score
to merge the lists in a unique one. Z-score nor-
malizes the scores according to the average µ and
standard deviation σ. Given the list of scores
L = {s1, s2, ... , sn}, µ and σ are computed on
L and the normalized score is defined as:
</bodyText>
<equation confidence="0.748997">
si = si − µ (2)
σ
</equation>
<bodyText confidence="0.999982416666667">
Then, all the lists Li are merged in a single list
M. The list M contains all the potential candi-
dates belonging to all the dictionaries with the re-
lated score. If a candidate occurs in more than one
dictionary, only the occurrence with the maximum
score is chosen.
At the end of the candidate collection step the
list M of potential translation candidates for each
target word is computed. It is important to point
out that the list M is sorted and supplies an initial
rank, which can be then modified by the candidate
selection step.
</bodyText>
<subsectionHeader confidence="0.999438">
2.2 Candidate selection
</subsectionHeader>
<bodyText confidence="0.99996625">
While the candidate collection step is common to
the two proposed systems, the problem of candi-
date selection is faced by using different strategies
in the two systems.
</bodyText>
<equation confidence="0.914231666666667">
j
sij = 1 −
mi
</equation>
<page confidence="0.983638">
243
</page>
<bodyText confidence="0.994286">
The first system, called unibaTranslate,
uses a method based on google-api-translate-java2.
The main idea behind unibaTranslate is to
look for a potential candidate in the translation of
the target sentence. Sometimes, no potential can-
didates occur into the translation. When this hap-
pens the system uses some heuristics to discover a
possible translation.
For example, given the target word “raw” and
the potential candidates M ={puro, crudo, sin re-
finar, de baja calidad, agrietado, al natural, bozal,
asado, frito and bruto}, the two possible scenarios
are:
</bodyText>
<listItem confidence="0.993407473684211">
1. a potential candidate occurs into the transla-
tion:
• Sen: The raw honesty of that basic
crudeness makes you feel stronger in a
way.
• Ses: La cruda honestidad de esa
crudeza de base que te hace sentir mas
fuerte en un camino.
2. no potential candidates occur into the trans-
lation, but a correct translation of the target
word is provided:
• Sen: Many institutional investors are
now deciding that they are getting a raw
deal from the company boards of Aus-
tralia.
• Ses: Muchos inversores institucionales
estan ahora decidiendo que estan recibi-
endo un trato injusto de los directorios
de las empresas de Australia.
</listItem>
<bodyText confidence="0.56015">
In detail, the strategy can be split in several
steps:
</bodyText>
<listItem confidence="0.971311666666666">
1. Retrieve the list M of potential translation
candidates using the method described in
Section 2.1.
2. Translate the target sentence Sen from En-
glish to Spanish, using the google-api-
translate-java, which results into the sentence
Ses.
3. Enrich M by adding multiword expressions.
To implement this step, the two bigrams
which contain the target word and the only
trigram in which the target word is the 2nd
term are taken into to account.
</listItem>
<footnote confidence="0.82426">
2http://code.google.com/p/google-api-translate-java/
</footnote>
<bodyText confidence="0.881432">
Coming back to the first sentence in the previ-
ous example, the following n-grams are built:
“the raw”, “raw honesty” and “the raw hon-
esty”. For each n-gram, candidate transla-
tions are looked for using Google Dictionary.
If translations are found, they are added to M
with an initial score equal to 0.
4. Fix a window W3 of n words to the right and
to the left of the target word, and perform the
following steps:
</bodyText>
<listItem confidence="0.948399285714286">
(a) for each candidate ck in M, try to find
ck in W. If ck occurs in W, then add 2
to the score of ck in M;
(b) if no exact match is found in the previ-
ous step, perform a new search by com-
paring ck with the words in W using
the Levenshtein distance4(Levenshtein,
1966). If the Levenshtein distance is
greater than 0.8, then add 2 to the score
of ck in M.
5. If no exact/partial match is found in the pre-
vious steps, probably the target word is trans-
lated with a word which does not belong to
M. To overcome this problem, we implement
a strategy able to discover a possible transla-
tion in Ses which is not in M. This approach
involves three steps:
(a) for each word wi in Sen, a list of poten-
tial translations Pi is retrieved;
(b) if a word in Pi is found in Ses, the word
is removed from Ses5;
(c) at this point, Ses contains a list R of
words with no candidate translations. A
score is assigned to those words by tak-
ing into account their position in Ses
with respect to the position of the target
word in Sen, using the following equa-
tion:
</listItem>
<equation confidence="0.952546">
1 − |posc − post |(3)
Lmax
</equation>
<bodyText confidence="0.9988896">
where posc is the translation candidate
position in Ses, post is the target word
position in Sen and Lmax is the maxi-
mum length between the length of Sen
and Ses.
</bodyText>
<footnote confidence="0.994487">
3The window W is the same for both Sen and Ses.
4A normalized Levenshtein distance is adopted to obtain
a value in [0, 1].
5A partial match based on normalized Levenshtein dis-
tance is implemented.
</footnote>
<page confidence="0.996166">
244
</page>
<bodyText confidence="0.994162042553192">
Moreover, the words not semanti-
cally related to the potential candidates
(found using Spanish WordNet6) are re-
moved from R. In detail, for each candi-
date in M a list of semantically related
words in Spanish WordNet7 is retrieved
which results in a set WN of related
words. Words in R but not in WN are
removed from R. In the final step, the
list R is sorted and the first word in R is
added to M assigning a score equal to 2.
6. In the last step, the list M is sorted. The out-
put of this process is the ranked list of poten-
tial candidates.
It is important to underline that both Sen and
Ses are tokenized, part-of-speech tagged and lem-
matized. Lemmatization plays a key role in the
matching step, while part-of-speech tagging is
needed to query both the dictionaries and the
Spanish WordNet. We adopt META (Basile et al.,
2008) and FreeLing (Atserias et al., 2006) to per-
form text processing for English and Spanish re-
spectively.
The second proposed system, called
unibaWiki, is based on the idea of automati-
cally building a parallel corpus from Wikipedia.
We use a structured version of Wikipedia called
DBpedia (Bizer et al., 2009). The main idea be-
hind DBpedia is to extract structured information
from Wikipedia and then to make this information
available. The main goal is to have access easily
to the large amount of information in Wikipedia.
DBpedia opens new and interesting ways to use
Wikipedia in NLP applications.
In CLLS task, we use the extended abstracts of
English and Spanish provided by DBpedia. For
each extended abstract in Spanish which has the
corresponding extended abstract in English, we
build a document composed by two fields: the for-
mer contains the English text (texten) and the lat-
ter contains the Spanish text (textes). We adopt
Lucene8 as storage and retrieval engine to make
the documents access fast and easy.
The idea behind unibaWiki is to count, for
each potential candidate, the number of docu-
ments in which the target word occurs in texten
and the potential candidate occurs in textes. A
</bodyText>
<footnote confidence="0.99414625">
6http://www.lsi.upc.edu/∼nlp/projectes/ewn.html
7The semantic relations of hyperonymy, hyponymy and
“similar to” are exploited.
8http://lucene.apache.org/
</footnote>
<bodyText confidence="0.9650998">
score equal to the number of retrieved documents
is assigned, then the candidates are sorted accord-
ing to that score.
Given the list M of potential candidates and the
target word t, for each ck ∈ M we perform the
following query:
texten : t AND textes : ck
where the field name is followed by a colon and
by the term you are looking for.
It is important to underline here that multiword
expressions require a specific kind of query. For
each multiword expression we adopt the Phrase-
Query which is able to retrieve documents that
contain a specific sequence of words instead of a
single keyword.
</bodyText>
<subsectionHeader confidence="0.99221">
2.3 Implementation
</subsectionHeader>
<bodyText confidence="0.9976455">
To implement the candidate collection step we de-
veloped a Java application able to retrieve infor-
mation from dictionaries. For each dictionary, a
different strategy has been adopted. In particular:
</bodyText>
<listItem confidence="0.983566588235294">
1. Google Dictionary: Google Dictionary web-
site is queried by using the HTTP protocol
and the answer page is parsed;
2. Spanishdict: the same strategy adopted for
Google Dictionary is used for the Spanishdict
website9;
3. Babylon Dictionary: the original file avail-
able from the Babylon website10 is converted
to obtain a plain text file by using the Unix
utility dictconv. After that, an application
queries the text file in an efficient way by
means of a hash map.
Both candidate selection systems are developed
in Java. Regarding the unibaWiki system, we
adopt Lucene to index DBpedia abstracts. The
output of Lucene is an index of about 680 Mbytes,
277,685 documents and about 1,500,000 terms.
</listItem>
<sectionHeader confidence="0.996505" genericHeader="method">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999543">
The goal of the evaluation is to measure the sys-
tems’ ability to find correct Spanish substitutions
for a given word. The dataset supplied by the or-
ganizers contains 1,000 instances in XML format.
</bodyText>
<footnote confidence="0.999924">
9http://www.spanishdict.com/
10www.babylon.com
</footnote>
<page confidence="0.998072">
245
</page>
<bodyText confidence="0.9998842">
Moreover, the organizers provide trial data com-
posed by 300 instances to help the participants
during the development of their systems.
The systems are evaluated using two scoring
types: best scores the best guessed substitution,
while out-of-ten (oot) scores the best 10 guessed
substitutions. For each scoring type, precision (P)
and recall (R) are computed. Mode precision (P-
mode) and mode recall (R-mode) calculate preci-
sion and recall against the substitution chosen by
the majority of the annotators (if there is a ma-
jority), respectively. Details about evaluation and
scoring types are provided in the task guidelines
(McCarthy et al., 2009).
Results of the evaluation using trial data are
reported in Table 1 and Table 2. Our systems
are tagged as UBA-T and UBA-W, which de-
note unibaTranslate and unibaWiki, re-
spectively. Systems marked as BL-1 and BL-2
are the two baselines provided by the organiz-
ers. The baselines use Spanishdict dictionary to
retrieve candidates. The system BL-1 ranks the
candidates according to the order returned on the
online query page, while the BL-2 rank is based on
candidate frequencies in the Spanish Wikipedia.
</bodyText>
<tableCaption confidence="0.996059">
Table 1: best results (trial data)
</tableCaption>
<table confidence="0.9999576">
System P R P-mode R-Mode
BL-1 24.50 24.50 51.80 51.80
BL-2 14.10 14.10 28.38 28.38
UBA-T 26.39 26.39 59.01 59.01
UBA-W 22.18 22.18 48.65 48.65
</table>
<tableCaption confidence="0.974458">
Table 2: oot results (trial data)
</tableCaption>
<table confidence="0.9992462">
System P R P-mode R-Mode
BL-1 38.58 38.58 71.62 71.62
BL-2 37.83 37.83 68.02 68.02
UBA-T 44.16 44.16 78.38 78.38
UBA-W 45.15 45.15 72.52 72.52
</table>
<bodyText confidence="0.995410448275862">
Results obtained using trial data show that our
systems are able to overcome the baselines. Only
the best score achieved by UBA-W is below BL-1.
Moreover, our strategy based on Wikipedia (UBA-
W) works better than the one proposed by the or-
ganizers (BL-2).
Results of the evaluation using test data are re-
ported in Table 3 and Table 4, which include all the
participants. Results show that UBA-T obtains the
highest recall using best scoring strategy. More-
over, both systems UBA-T and UBA-W achieve
the highest R-mode and P-mode using oot scoring
strategy. It is worthwhile to point out that the pres-
ence of duplicates affect recall (R) and precision
(P), but not R-mode and P-mode. For this reason
some systems, such as SWAT-E, obtain very high
recall (R) and low R-mode using oot scoring. Du-
plicates are not produced by our systems, but we
performed an a posteriori experiment in which du-
plicates are allowed. In that experiment, the first
candidate provided by UBA-T has been duplicated
ten times in the results. Using that strategy, UBA-T
achieves a recall (and precision) equal to 271.51.
This experiment proves that also our system is able
to obtain the highest recall when duplicates are al-
lowed into the results. Moreover, it is important to
underline here that we do not know how other par-
ticipants generate duplicates in their results. We
adopted a trivial strategy to introduce duplicates.
</bodyText>
<tableCaption confidence="0.994814">
Table 3: best results (test data)
</tableCaption>
<table confidence="0.999983411764706">
System P R P-mode R-Mode
BL-1 24.34 24.34 50.34 50.34
BL-2 15.09 15.09 29.22 29.22
UBA-T 27.15 27.15 57.20 57.20
UBA-W 19.68 19.68 39.09 39.09
USPWLV 26.81 26.81 58.85 58.85
Colslm 27.59 25.99 59.16 56.24
WLVUSP 25.27 25.27 52.81 52.81
SWAT-E 21.46 21.46 43.21 43.21
UvT-v 21.09 21.09 43.76 43.76
CU-SMT 21.62 20.56 45.01 44.58
UvT-g 19.59 19.59 41.02 41.02
SWAT-S 18.87 18.87 36.63 36.63
ColEur 19.47 18.15 40.03 37.72
IRST-1 22.16 15.38 45.95 33.47
IRSTbs 22.51 13.21 45.27 28.26
TYO 8.62 8.39 15.31 14.95
</table>
<tableCaption confidence="0.991969">
Table 4: oot results (test data)
</tableCaption>
<table confidence="0.998848882352941">
System P R P-mode R-Mode
BL-1 44.04 44.04 73.53 73.53
BL-2 42.65 42.65 71.60 71.60
UBA-T 47.99 47.99 81.07 81.07
UBA-W 52.75 52.75 83.54 83.54
USPWLV 47.60 47.60 79.84 79.84
Colslm 46.61 43.91 69.41 65.98
WLVUSP 48.48 48.48 77.91 77.91
SWAT-E 174.59 174.59 66.94 66.94
UvT-v 58.91 58.91 62.96 62.96
UvT-g 55.29 55.29 73.94 73.94
SWAT-S 97.98 97.98 79.01 79.01
ColEur 44.77 41.72 71.47 67.35
IRST-1 33.14 31.48 58.30 55.42
IRSTbs 29.74 8.33 64.44 19.89
TYO 35.46 34.54 59.16 58.02
FCC-LS 23.90 23.90 31.96 31.96
</table>
<bodyText confidence="0.9297375">
Finally, Table 5 reports some statistics about
UBA-T and the number of times (N) the candi-
</bodyText>
<page confidence="0.994214">
246
</page>
<bodyText confidence="0.9999585">
date translation is taken from Spanish WordNet
(Spanish WN) or multiword expressions (Multi-
word exp.). The number of instances in which
the candidate is a correct substitution is reported
in column C. Analyzing the results we note that
most errors are due to part-of-speech tagging. For
example, given the following sentence:
Sen: You will still be responsible for the shipping
and handling fees, and for the cost of return-
ing the merchandise.
Sea: Usted seguira siendo responsable de los gas-
tos de envio y manipulacion y, para los gastos
de devolucion de la mercancia.
where the target word is the verb return. In this
case the verb is used as noun and the algorithm
suggests correctly devolucion (noun) as substitu-
tion instead of devolver (verb). The gold standard
provided by the organizers contains devolver as
substitution and there is no match between devolu-
cion and devolver during the scoring.
</bodyText>
<tableCaption confidence="0.996716">
Table 5: UBA-T statistics.
</tableCaption>
<table confidence="0.966067666666667">
Strategy N C
Spanish WN 34 11
Multiword exp. 21 11
</table>
<sectionHeader confidence="0.999713" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.99994975">
We described our participation at SemEval-2
Cross-Lingual Lexical Substitution Task, propos-
ing two systems called UBA-T and UBA-W. The
first relies on Google Translator, the second
is based on DBpedia, a structured version of
Wikipedia. Moreover, we exploited several dictio-
naries to retrieve the list of candidate substitutions.
UBA-T achieves the highest recall among all
the participants to the task. Moreover, the results
proved that the method based on Google Transla-
tor is more effective than the one based on DBpe-
dia.
</bodyText>
<sectionHeader confidence="0.999365" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.924902833333333">
This research was partially funded by Regione
Puglia under the contract POR PUGLIA 2007-
2013 - Asse I Linea 1.1 Azione 1.1.2 - Bando
“Aiuti agli Investimenti in Ricerca per le PMI” -
Fondo per le Agevolazioni alla Ricerca, project ti-
tle: “Natural Browsing”.
</bodyText>
<sectionHeader confidence="0.989307" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9995278">
Jordi Atserias, Bernardino Casas, Elisabet Comelles,
Meritxell Gonz´alez, Llu´ıs Padr´o, and Muntsa Padr´o.
2006. FreeLing 1.3: Syntactic and semantic services
in an open-source NLP library. In Proceedings of
the 5th International Conference on Language Re-
sources and Evaluation (LREC06), pages 48–55.
Pierpaolo Basile, Marco de Gemmis, Anna Lisa Gen-
tile, Leo Iaquinta, Pasquale Lops, and Giovanni Se-
meraro. 2008. META - MultilanguagE Text Ana-
lyzer. In Proceedings of the Language and Speech
Technnology Conference - LangTech 2008, Rome,
Italy, February 28-29, pages 137–140.
Christian Bizer, Jens Lehmann, Georgi Kobilarov,
S¨oren Auer, Christian Becker, Richard Cyganiak,
and Sebastian Hellmann. 2009. DBpedia-A crys-
tallization point for the Web of Data. Web Seman-
tics: Science, Services and Agents on the World Wide
Web, Issue 7:154–165.
Vladimir Iosifovich Levenshtein. 1966. Binary codes
capable of correcting deletions, insertions, and re-
versals. In Soviet Physics-Doklady, volume 10.
Diana McCarthy and Roberto Navigli. 2007.
SemEval-2007 Task 10: English Lexical Sub-
stitution Task. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 48–53, Prague, Czech Re-
public, June. Association for Computational Lin-
guistics.
Diana McCarthy, Rada Sinha, and Ravi Mihal-
cea. 2009. Cross Lingual Lexical Sub-
stitution. http://lit.csci.unt.edu/DOCS/task2clls-
documentation.pdf.
Rada Mihalcea, Ravi Sinha, and Diana McCarthy.
2010. SemEval-2010 Task 2: Cross-Lingual
Lexical Substitution. In Proceedings of the 5th
International Workshop on Semantic Evaluations
(SemEval-2010). Association for Computational
Linguistics.
Ravi Sinha, Diana McCarthy, and Rada Mihalcea.
2009. SemEval-2010 Task 2: cross-lingual lexical
substitution. In SEW ’09: Proceedings of the Work-
shop on Semantic Evaluations: Recent Achieve-
ments and Future Directions, pages 76–81, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.99797">
247
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.472852">
<title confidence="0.998075">UBA: Using Automatic Translation and Wikipedia for Cross-Lingual Lexical Substitution</title>
<author confidence="0.998354">Pierpaolo Basile</author>
<affiliation confidence="0.999589">Dept. of Computer Science University of Bari “Aldo Moro”</affiliation>
<address confidence="0.994956">Via E. Orabona, 4 70125 Bari (ITALY)</address>
<email confidence="0.99861">basilepp@di.uniba.it</email>
<abstract confidence="0.96709870967742">This paper presents the participation of the University of Bari (UBA) at the SemEval- 2010 Cross-Lingual Lexical Substitution Task. The goal of the task is to substia word in a language which occurs in a particular context, by providing the best synonyms in a different lanfit in that context. This task has a strict relation with the task of automatic machine translation, but there are some differences: Cross-lingual lexical substitution targets one word at a time and the main goal is to find as many good translations as possible for the given target word. Moreover, there are some connections with Word Sense Disambiguation (WSD) algorithms. Indeed, understanding the meaning of the target word is necessary to find the best substitutions. An important aspect of this kind of task is the possibility of finding synonyms without using a particular sense inventory or a specific parallel corpus, thus allowing the participation of unsupervised approaches. UBA proposes two systems: the former is based on an automatic translation system which exploits Google Translator, the latter is based on a parallel corpus approach which relies on Wikipedia in order to find the best substitutions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jordi Atserias</author>
<author>Bernardino Casas</author>
<author>Elisabet Comelles</author>
<author>Meritxell Gonz´alez</author>
<author>Llu´ıs Padr´o</author>
<author>Muntsa Padr´o</author>
</authors>
<title>FreeLing 1.3: Syntactic and semantic services in an open-source NLP library.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC06),</booktitle>
<pages>48--55</pages>
<marker>Atserias, Casas, Comelles, Gonz´alez, Padr´o, Padr´o, 2006</marker>
<rawString>Jordi Atserias, Bernardino Casas, Elisabet Comelles, Meritxell Gonz´alez, Llu´ıs Padr´o, and Muntsa Padr´o. 2006. FreeLing 1.3: Syntactic and semantic services in an open-source NLP library. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC06), pages 48–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierpaolo Basile</author>
<author>Marco de Gemmis</author>
<author>Anna Lisa Gentile</author>
<author>Leo Iaquinta</author>
<author>Pasquale Lops</author>
<author>Giovanni Semeraro</author>
</authors>
<title>META - MultilanguagE Text Analyzer.</title>
<date>2008</date>
<booktitle>In Proceedings of the Language and Speech Technnology Conference - LangTech</booktitle>
<pages>137--140</pages>
<location>Rome, Italy,</location>
<marker>Basile, de Gemmis, Gentile, Iaquinta, Lops, Semeraro, 2008</marker>
<rawString>Pierpaolo Basile, Marco de Gemmis, Anna Lisa Gentile, Leo Iaquinta, Pasquale Lops, and Giovanni Semeraro. 2008. META - MultilanguagE Text Analyzer. In Proceedings of the Language and Speech Technnology Conference - LangTech 2008, Rome, Italy, February 28-29, pages 137–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Bizer</author>
<author>Jens Lehmann</author>
<author>Georgi Kobilarov</author>
<author>S¨oren Auer</author>
<author>Christian Becker</author>
<author>Richard Cyganiak</author>
<author>Sebastian Hellmann</author>
</authors>
<title>DBpedia-A crystallization point for the Web of Data. Web Semantics: Science, Services and Agents on the World Wide Web, Issue 7:154–165.</title>
<date>2009</date>
<contexts>
<context position="12509" citStr="Bizer et al., 2009" startWordPosition="2164" endWordPosition="2167"> of potential candidates. It is important to underline that both Sen and Ses are tokenized, part-of-speech tagged and lemmatized. Lemmatization plays a key role in the matching step, while part-of-speech tagging is needed to query both the dictionaries and the Spanish WordNet. We adopt META (Basile et al., 2008) and FreeLing (Atserias et al., 2006) to perform text processing for English and Spanish respectively. The second proposed system, called unibaWiki, is based on the idea of automatically building a parallel corpus from Wikipedia. We use a structured version of Wikipedia called DBpedia (Bizer et al., 2009). The main idea behind DBpedia is to extract structured information from Wikipedia and then to make this information available. The main goal is to have access easily to the large amount of information in Wikipedia. DBpedia opens new and interesting ways to use Wikipedia in NLP applications. In CLLS task, we use the extended abstracts of English and Spanish provided by DBpedia. For each extended abstract in Spanish which has the corresponding extended abstract in English, we build a document composed by two fields: the former contains the English text (texten) and the latter contains the Spani</context>
</contexts>
<marker>Bizer, Lehmann, Kobilarov, Auer, Becker, Cyganiak, Hellmann, 2009</marker>
<rawString>Christian Bizer, Jens Lehmann, Georgi Kobilarov, S¨oren Auer, Christian Becker, Richard Cyganiak, and Sebastian Hellmann. 2009. DBpedia-A crystallization point for the Web of Data. Web Semantics: Science, Services and Agents on the World Wide Web, Issue 7:154–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Iosifovich Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions, and reversals.</title>
<date>1966</date>
<booktitle>In Soviet Physics-Doklady,</booktitle>
<volume>10</volume>
<contexts>
<context position="10211" citStr="Levenshtein, 1966" startWordPosition="1735" endWordPosition="1736">following n-grams are built: “the raw”, “raw honesty” and “the raw honesty”. For each n-gram, candidate translations are looked for using Google Dictionary. If translations are found, they are added to M with an initial score equal to 0. 4. Fix a window W3 of n words to the right and to the left of the target word, and perform the following steps: (a) for each candidate ck in M, try to find ck in W. If ck occurs in W, then add 2 to the score of ck in M; (b) if no exact match is found in the previous step, perform a new search by comparing ck with the words in W using the Levenshtein distance4(Levenshtein, 1966). If the Levenshtein distance is greater than 0.8, then add 2 to the score of ck in M. 5. If no exact/partial match is found in the previous steps, probably the target word is translated with a word which does not belong to M. To overcome this problem, we implement a strategy able to discover a possible translation in Ses which is not in M. This approach involves three steps: (a) for each word wi in Sen, a list of potential translations Pi is retrieved; (b) if a word in Pi is found in Ses, the word is removed from Ses5; (c) at this point, Ses contains a list R of words with no candidate transl</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Vladimir Iosifovich Levenshtein. 1966. Binary codes capable of correcting deletions, insertions, and reversals. In Soviet Physics-Doklady, volume 10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Roberto Navigli</author>
</authors>
<title>SemEval-2007 Task 10: English Lexical Substitution Task.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>48--53</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1975" citStr="McCarthy and Navigli, 2007" startWordPosition="317" endWordPosition="320"> parallel corpus approach which relies on Wikipedia in order to find the best substitutions. 1 Introduction The goal of the Cross-Lingual Lexical Substitution (CLLS) task is to substitute a word in a language L3, which occurs in a particular context, by providing the best substitutions in a different language Lt. In SemEval-2010 the source language L3 is English, while the target language Lt is Spanish. Clearly, this task is related to Lexical Giovanni Semeraro Dept. of Computer Science University of Bari “Aldo Moro” Via E. Orabona, 4 70125 Bari (ITALY) semeraro@di.uniba.it Substitution (LS) (McCarthy and Navigli, 2007) which consists in selecting an alternative word for a given one in a particular context by preserving its meaning. The main difference between the LS task and the CLLS one is that in LS source and target languages are the same. CLLS is not a easy task since neither a list of candidate words nor a specific parallel corpus are supplied by the organizers. However, this opens the possibility of using several knowledge sources, instead of a single one fixed by the task organizers. Therefore, the system must identify a set of candidate words in Lt and then select only those words which fit the cont</context>
</contexts>
<marker>McCarthy, Navigli, 2007</marker>
<rawString>Diana McCarthy and Roberto Navigli. 2007. SemEval-2007 Task 10: English Lexical Substitution Task. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 48–53, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rada Sinha</author>
<author>Ravi Mihalcea</author>
</authors>
<title>Cross Lingual Lexical Substitution.</title>
<date>2009</date>
<note>http://lit.csci.unt.edu/DOCS/task2cllsdocumentation.pdf.</note>
<contexts>
<context position="15977" citStr="McCarthy et al., 2009" startWordPosition="2722" endWordPosition="2725">rovide trial data composed by 300 instances to help the participants during the development of their systems. The systems are evaluated using two scoring types: best scores the best guessed substitution, while out-of-ten (oot) scores the best 10 guessed substitutions. For each scoring type, precision (P) and recall (R) are computed. Mode precision (Pmode) and mode recall (R-mode) calculate precision and recall against the substitution chosen by the majority of the annotators (if there is a majority), respectively. Details about evaluation and scoring types are provided in the task guidelines (McCarthy et al., 2009). Results of the evaluation using trial data are reported in Table 1 and Table 2. Our systems are tagged as UBA-T and UBA-W, which denote unibaTranslate and unibaWiki, respectively. Systems marked as BL-1 and BL-2 are the two baselines provided by the organizers. The baselines use Spanishdict dictionary to retrieve candidates. The system BL-1 ranks the candidates according to the order returned on the online query page, while the BL-2 rank is based on candidate frequencies in the Spanish Wikipedia. Table 1: best results (trial data) System P R P-mode R-Mode BL-1 24.50 24.50 51.80 51.80 BL-2 14</context>
</contexts>
<marker>McCarthy, Sinha, Mihalcea, 2009</marker>
<rawString>Diana McCarthy, Rada Sinha, and Ravi Mihalcea. 2009. Cross Lingual Lexical Substitution. http://lit.csci.unt.edu/DOCS/task2cllsdocumentation.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Ravi Sinha</author>
<author>Diana McCarthy</author>
</authors>
<title>SemEval-2010 Task 2: Cross-Lingual Lexical Substitution.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluations (SemEval-2010). Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2865" citStr="Mihalcea et al., 2010" startWordPosition="476" endWordPosition="479">list of candidate words nor a specific parallel corpus are supplied by the organizers. However, this opens the possibility of using several knowledge sources, instead of a single one fixed by the task organizers. Therefore, the system must identify a set of candidate words in Lt and then select only those words which fit the context. From another point of view, the crosslingual nature of the task allows to exploit automatic machine translation methods, hence the goal is to find as many good translations as possible for the given target word. A thorough description of the task can be found in (Mihalcea et al., 2010; Sinha et al., 2009). To easily understand the task, an example follows. Consider the sentence: During the siege, George Robertson had appointed Shuja-ul-Mulk , who was a bright boy only 12 years old and the youngest surviving son of Aman-ul-Mulk, as the ruler of Chitral. In the previous sentence the target word is “bright”. Taking into account the meaning of the word “bright” in this particular context, the best substitutions in Spanish are: “inteligente”, “brillante” and “listo”. We propose two systems to tackle the problem of CLLS: the first is based on an automatic translation system whic</context>
</contexts>
<marker>Mihalcea, Sinha, McCarthy, 2010</marker>
<rawString>Rada Mihalcea, Ravi Sinha, and Diana McCarthy. 2010. SemEval-2010 Task 2: Cross-Lingual Lexical Substitution. In Proceedings of the 5th International Workshop on Semantic Evaluations (SemEval-2010). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ravi Sinha</author>
<author>Diana McCarthy</author>
<author>Rada Mihalcea</author>
</authors>
<title>SemEval-2010 Task 2: cross-lingual lexical substitution.</title>
<date>2009</date>
<booktitle>In SEW ’09: Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions,</booktitle>
<pages>76--81</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2886" citStr="Sinha et al., 2009" startWordPosition="480" endWordPosition="483"> nor a specific parallel corpus are supplied by the organizers. However, this opens the possibility of using several knowledge sources, instead of a single one fixed by the task organizers. Therefore, the system must identify a set of candidate words in Lt and then select only those words which fit the context. From another point of view, the crosslingual nature of the task allows to exploit automatic machine translation methods, hence the goal is to find as many good translations as possible for the given target word. A thorough description of the task can be found in (Mihalcea et al., 2010; Sinha et al., 2009). To easily understand the task, an example follows. Consider the sentence: During the siege, George Robertson had appointed Shuja-ul-Mulk , who was a bright boy only 12 years old and the youngest surviving son of Aman-ul-Mulk, as the ruler of Chitral. In the previous sentence the target word is “bright”. Taking into account the meaning of the word “bright” in this particular context, the best substitutions in Spanish are: “inteligente”, “brillante” and “listo”. We propose two systems to tackle the problem of CLLS: the first is based on an automatic translation system which exploits the API of</context>
<context position="4652" citStr="Sinha et al., 2009" startWordPosition="760" endWordPosition="763">ven word. Specifically, we use three different dictionaries: Google Dictionary, Babylon Dictionary and Spanishdict. Then, we combine the dictionaries into a single one, as described in Section 2.1. The paper is organized as follows: Section 2 describes the strategy we adopted to tackle the CLLS task, while results of an experimental session we carried out in order to evaluate the proposed approaches are presented in Section 3. Conclusions are discussed in Section 4. 2 Methodology Generally speaking, the problem of CLLS can be coped with a strategy which consists of two steps, as suggested in (Sinha et al., 2009): • candidate collection: in this step several resources are queried to retrieve a list of potential translation candidates for each target word and part of speech; • candidate selection: this step concerns the ranking of potential candidates, which are the most suitable ones for each instance, by using information about the context. Regarding the candidate collection, we exploit three dictionaries: Google Dictionary, Babylon Dictionary and Spanishdict. Each dictionary is modeled using a strategy described in Section 2.1. We use the same approach to model each dictionary in order to make easy </context>
</contexts>
<marker>Sinha, McCarthy, Mihalcea, 2009</marker>
<rawString>Ravi Sinha, Diana McCarthy, and Rada Mihalcea. 2009. SemEval-2010 Task 2: cross-lingual lexical substitution. In SEW ’09: Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 76–81, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>