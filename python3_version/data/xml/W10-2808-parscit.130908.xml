<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002718">
<title confidence="0.996828">
Sketch Techniques for Scaling Distributional Similarity to the Web
</title>
<author confidence="0.997105">
Amit Goyal, Jagadeesh Jagarlamudi, Hal Daum´e III, and Suresh Venkatasubramanian
</author>
<affiliation confidence="0.86544">
School of Computing
University of Utah
Salt Lake City, UT 84112
</affiliation>
<email confidence="0.999511">
{amitg,jags,hal,suresh}@cs.utah.edu
</email>
<sectionHeader confidence="0.997399" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999982555555556">
In this paper, we propose a memory, space,
and time efficient framework to scale dis-
tributional similarity to the web. We
exploit sketch techniques, especially the
Count-Min sketch, which approximates
the frequency of an item in the corpus
without explicitly storing the item itself.
These methods use hashing to deal with
massive amounts of the streaming text. We
store all item counts computed from 90
GB of web data in just 2 billion coun-
ters (8 GB main memory) of CM sketch.
Our method returns semantic similarity
between word pairs in O(K) time and
can compute similarity between any word
pairs that are stored in the sketch. In our
experiments, we show that our framework
is as effective as using the exact counts.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999179375">
In many NLP problems, researchers (Brants et al.,
2007; Turney, 2008) have shown that having large
amounts of data is beneficial. It has also been
shown that (Agirre et al., 2009; Pantel et al., 2009;
Ravichandran et al., 2005) having large amounts
of data helps capturing the semantic similarity be-
tween pairs of words. However, computing distri-
butional similarity (Sec. 2.1) between word pairs
from large text collections is a computationally ex-
pensive task. In this work, we consider scaling dis-
tributional similarity methods for computing se-
mantic similarity between words to Web-scale.
The major difficulty in computing pairwise sim-
ilarities stems from the rapid increase in the num-
ber of unique word-context pairs with the size of
text corpus (number of tokens). Fig. 1 shows that
</bodyText>
<note confidence="0.440197">
Log2 of # of words
</note>
<figureCaption confidence="0.999608">
Figure 1: Token Type Curve
</figureCaption>
<bodyText confidence="0.999948571428571">
the number of unique word-context pairs increase
rapidly compared to the number words when plot-
ted against the number of tokens1. For example,
a 57 million word corpus2 generates 224 thousand
unique words and 15 million unique word-context
pairs. As a result, it is computationally hard to
compute counts of all word-context pairs with a gi-
ant corpora using conventional machines (say with
main memory of 8 GB). To overcome this, Agirre
et al. (2009) used MapReduce infrastructure (with
2, 000 cores) to compute pairwise similarities of
words on a corpus of roughly 1.6 Terawords.
In a different direction, our earlier work (Goyal
et al., 2010) developed techniques to make the
computations feasible on a conventional machines
by willing to accept some error in the counts. Sim-
ilar to that work, this work exploits the idea of
Count-Min (CM) sketch (Cormode and Muthukr-
ishnan, 2004) to approximate the frequency of
word pairs in the corpus without explicitly stor-
ing the word pairs themselves. In their, we stored
</bodyText>
<footnote confidence="0.8023285">
1Note that the plot is in log-log scale.
2‘Subset’ column of Table 1 in Section 5.1
</footnote>
<figure confidence="0.99754175">
5 10 15 20 25
Log2 of # of unique Items
20
15
10
5
word−context pairs
words
</figure>
<page confidence="0.983949">
51
</page>
<note confidence="0.631274">
Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 51–56,
Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999462657894737">
counts of all words/word pairs in fixed amount of
main memory. We used conservative update with
CM sketch (referred as CU sketch) and showed
that it reduces the average relative error of its ap-
proximate counts by a factor of two. The approx-
imate counts returned by CU Sketch were used
to compute approximate PMI between word pairs.
We found their that the approximate PMI values
are as useful as the exact PMI values for com-
puting semantic orientation (Turney and Littman,
2002) of words. In addition, our intrinsic evalua-
tions in their showed that the quality of approxi-
mate counts and approximate PMI is good.
In this work, we use CU-sketch to store counts
of items (words, contexts, and word-context pairs)
using fixed amount of memory of 8 GB by using
only 2B counters. These approximate counts re-
turned by CU Sketch are converted into approx-
imate PMI between word-context pairs. The top
K contexts (based on PMI score) for each word
are used to construct distributional profile (DP) for
each word. The similarity between a pair of words
is computed based on the cosine similarity of their
respective DPs.
The above framework of using CU sketch to
compute semantic similarity between words has
five good properties. First, this framework can re-
turn semantic similarity between any word pairs
that are stored in the CU sketch. Second, it can
return the similarity between word pairs in time
O(K). Third, because we do not store items ex-
plicitly, the overall space required is significantly
smaller. Fourth, the additive property of CU
sketch (Sec. 3.2) enables us to parallelize most
of the steps in the algorithm. Thus it can be easily
extended to very large amounts of text data. Fifth,
this easily generalizes to any kind of association
measure and semantic similarity measure.
</bodyText>
<sectionHeader confidence="0.998083" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.999081">
2.1 Distributional Similarity
</subsectionHeader>
<bodyText confidence="0.999184125">
Distributional Similarity is based on the distribu-
tional hypothesis (Firth, 1968; Harris, 1985) that
words occur in similar contexts tend to be sim-
ilar. The context of a word is represented by
the distributional profile (DP), which contains the
strength of association between the word and each
of the lexical, syntactic, semantic, and/or depen-
dency units that co-occur with it3. The association
</bodyText>
<footnote confidence="0.928053">
3In this work, we only consider lexical units as context.
</footnote>
<bodyText confidence="0.99799125">
is commonly measured using conditional proba-
bility, pointwise mutual information (PMI) or log
likelihood ratios. Then the semantic similarity be-
tween two words, given their DPs, is calculated
using similarity measures such as Cosine, α-skew
divergence, and Jensen-Shannon divergence. In
our work, we use PMI as association measure and
cosine similarity to compute pairwise similarities.
</bodyText>
<subsectionHeader confidence="0.999442">
2.2 Large Scale NLP problems
</subsectionHeader>
<bodyText confidence="0.999987333333333">
Pantel et al. (2009) computed similarity between
500 million word pairs using the MapReduce
framework from a 200 billion word corpus using
200 quad-core nodes. The inaccessibility of clus-
ters for every one has attracted NLP community to
use streaming, and randomized algorithms to han-
dle large amounts of data.
Ravichandran et al. (2005) used locality sensi-
tive hash functions for computing word-pair simi-
larities from large text collections. Their approach
stores a enormous matrix of all unique words and
their contexts in main memory which makes it
hard for larger data sets. In our work, we store
all unique word-context pairs in CU sketch with a
pre-defined size4.
Recently, the streaming algorithm paradigm has
been used to provide memory and time-efficient
platform to deal with terabytes of data. For
example, we (Goyal et al., 2009); Levenberg
and Osborne (2009) build approximate language
models and show their effectiveness in SMT. In
(Van Durme and Lall, 2009b), a TOMB Counter
(Van Durme and Lall, 2009a) was used to find the
top-K verbs “y” with the highest PMI for a given
verb “x”. The idea of TOMB is similar to CU
Sketch. However, we use CU Sketch because of
its simplicity and attractive properties (see Sec. 3).
In this work, we go one step further, and compute
semantic similarity between word-pairs using ap-
proximate PMI scores from CU sketch.
</bodyText>
<subsectionHeader confidence="0.999631">
2.3 Sketch Techniques
</subsectionHeader>
<bodyText confidence="0.999949857142857">
Sketch techniques use a sketch vector as a data
structure to store the streaming data compactly in
a small-memory footprint. These techniques use
hashing to map items in the streaming data onto a
small sketch vector that can be easily updated and
queried. These techniques generally process the
input stream in one direction, say from left to right,
</bodyText>
<footnote confidence="0.5215175">
4We use only 2 billion counters which takes up to 8 GB
of main memory.
</footnote>
<page confidence="0.994934">
52
</page>
<bodyText confidence="0.999914666666667">
without re-processing previous input. The main
advantage of using these techniques is that they
require a storage which is significantly smaller
than the input stream length. A survey by (Rusu
and Dobra, 2007; Cormode and Hadjieleftheriou,
2008) comprehensively reviews the literature.
</bodyText>
<sectionHeader confidence="0.991413" genericHeader="method">
3 Count-Min Sketch
</sectionHeader>
<bodyText confidence="0.99831">
The Count-Min Sketch (Cormode and Muthukr-
ishnan, 2004) is a compact summary data struc-
ture used to store the frequencies of all items in
the input stream.
Given an input stream of items of length N
and user chosen parameters S and ǫ, the algorithm
stores the frequencies of all the items with the fol-
lowing guarantees:
</bodyText>
<listItem confidence="0.9983338">
• All reported frequencies are within ǫN of
true frequencies with probability of atleast S.
• Space used by the algorithm is O(1ǫ log δ1 ).
• Constant time of O(log(1δ )) per each update
and query operation.
</listItem>
<subsectionHeader confidence="0.991572">
3.1 CM Data Structure
</subsectionHeader>
<bodyText confidence="0.998842285714286">
A Count-Min Sketch with parameters (ǫ,S) is rep-
resented by a two-dimensional array with width w
and depth d :
Among the user chosen parameters, ǫ controls the
amount of tolerable error in the returned count and
S controls the probability with which the returned
count is not within this acceptable error. These
values of ǫ and S determine the width and depth
of the two-dimensional array respectively. To
achieve the guarantees mentioned in the previous
section, we set w=2 ǫ and d=log(1δ). The depth d
denotes the number of pairwise-independent hash
functions employed by the algorithm and there
exists an one-to-one correspondence between the
rows and the set of hash functions. Each of these
hash functions hk:{x1 ... xN} —* {1... w}, 1 &lt;
k &lt; d takes an item from the input stream and
maps it into a counter indexed by the correspond-
ing hash function. For example, h2(x) = 10 indi-
cates that the item “x” is mapped to the 10th posi-
tion in the second row of the sketch array. These
d hash functions are chosen uniformly at random
from a pairwise-independent family.
Initialize the entire sketch array with zeros.
Update Procedure: When a new item “x” with
count c arrives5, one counter in each row, as de-
cided by its corresponding hash function, is up-
dated by c. Formally, V1 &lt; k &lt; d
</bodyText>
<equation confidence="0.929333">
sketch[k,hk(x)] &lt;-- sketch[k,hk(x)] + c
</equation>
<bodyText confidence="0.997838333333333">
Query Procedure: Since multiple items can be
hashed to the same counter, the frequency stored
by each counter is an overestimate of the true
count. Thus, to answer the point query, we con-
sider all the positions indexed by the hash func-
tions for the given item and return the minimum
of all these values. The answer to Query(x) is:
c� = mink sketch[k, hk(x)].
Both update and query procedures involve eval-
uating d hash functions. Hence, both these proce-
dures are linear in the number of hash functions. In
our experiments (see Section5), we use d=3 simi-
lar to our earlier work (Goyal et al., 2010). Hence,
the update and query operations take only constant
time.
</bodyText>
<subsectionHeader confidence="0.989852">
3.2 Properties
</subsectionHeader>
<bodyText confidence="0.9622544">
Apart from the advantages of being space efficient
and having constant update and querying time, the
CM sketch has other advantages that makes it at-
tractive for scaling distributional similarity to the
web:
</bodyText>
<listItem confidence="0.7422744">
1. Linearity: given two sketches s1 and s2 com-
puted (using the same parameters w and d)
over different input streams, the sketch of the
combined data stream can be easily obtained
by adding the individual sketches.
2. The linearity allows the individual sketches
to be computed independent of each other.
This means that it is easy to implement it in
distributed setting, where each machine com-
putes the sketch over a subset of the corpus.
</listItem>
<subsectionHeader confidence="0.996199">
3.3 Conservative Update
</subsectionHeader>
<bodyText confidence="0.998936333333333">
Estan and Varghese introduce the idea of conserva-
tive update (Estan and Varghese, 2002) in the con-
text of networking. This can easily be used with
CM Sketch (CU Sketch) to further improve the es-
timate of a point query. To update an item, w with
frequency c, we first compute the frequency c� of
</bodyText>
<footnote confidence="0.497811">
5In our setting, c is always 1.
</footnote>
<figure confidence="0.6484">
sketch[1, 1] • • • sketch[1, w]
..
.. ..
sketch[d, 1] • • • sketch[d, w]
�
� �
1
...
</figure>
<page confidence="0.993017">
53
</page>
<bodyText confidence="0.9835425">
this item from the existing data structure and the
counts are updated according to: b1 &lt; k &lt; d
</bodyText>
<equation confidence="0.870674">
sketch[k,hkW] &lt;-- max{sketch[k,hkW], c� + c}
</equation>
<bodyText confidence="0.996986166666667">
The intuition is that, since the point query returns
the minimum of all the d values, we will update a
counter only if it is necessary as indicated by the
above equation. This heuristic avoids the unneces-
sary updating of counter values and thus reduces
the error.
</bodyText>
<sectionHeader confidence="0.993625" genericHeader="method">
4 Efficient Distributional Similarity
</sectionHeader>
<bodyText confidence="0.990539">
To compute distributional similarity efficiently, we
store counts in CU sketch. Our algorithm has three
main steps:
</bodyText>
<listItem confidence="0.922849166666667">
1. Store approximate counts of all words, con-
texts, and word-context pairs in CU-sketch
using fixed amount of counters.
2. Convert these counts into approximate PMI
scores between word-context pairs. Use these
PMI scores to store top K contexts for a word
on the disk. Store these top K context vectors
for every word stored in the sketch.
3. Use cosine similarity to compute the similar-
ity between word pairs using these approxi-
mate top K context vectors constructed using
CU sketch.
</listItem>
<sectionHeader confidence="0.870064" genericHeader="method">
5 Word pair Ranking Evaluations
</sectionHeader>
<bodyText confidence="0.9998688">
As discussed earlier, the DPs of words are used to
compute similarity between a pair of words. We
used the following four test sets and their corre-
sponding human judgements to evaluate the word
pair rankings.
</bodyText>
<listItem confidence="0.9977675">
1. WS-353: WordSimilarity-3536 (Finkelstein
et al., 2002) is a set of 353 word pairs.
2. WS-203: A subset of WS-353 containing 203
word pairs marked according to similarity7
(Agirre et al., 2009).
3. RG-65: (Rubenstein and Goodenough, 1965)
is set of 65 word pairs.
4. MC-30: A smaller subset of the RG-65
dataset containing 30 word pairs (Miller and
Charles, 1991).
</listItem>
<footnote confidence="0.992471666666667">
6http://www.cs.technion.ac.il/ gabr/resources/data/word-
sim353/wordsim353.html
7http://alfonseca.org/pubs/ws353simrel.tar.gz
</footnote>
<bodyText confidence="0.999828">
Each of these data sets come with human ranking
of the word pairs. We rank the word pairs based
on the similarity computed using DPs and evalu-
ate this ranking against the human ranking. We
report the spearman’s rank correlation coefficient
(p) between these two rankings.
</bodyText>
<subsectionHeader confidence="0.998183">
5.1 Corpus Statistics
</subsectionHeader>
<bodyText confidence="0.999539888888889">
The Gigaword corpus (Graff, 2003) and a copy of
the web crawled by (Ravichandran et al., 2005)
are used to compute counts of all items (Table. 1).
For both the corpora, we split the text into sen-
tences, tokenize, convert into lower-case, remove
punctuations, and collapse each digit to a sym-
bol “0” (e.g. “1996” gets collapsed to “0000”).
We store the counts of all words (excluding num-
bers, and stop words), their contexts, and counts
of word-context pairs in the CU sketch. We de-
fine the context for a given word “x” as the sur-
rounding words appearing in a window of 2 words
to the left and 2 words to the right. The context
words are concatenated along with their positions
-2, -1, +1, and +2. We evaluate ranking of word
pairs on three different sized corpora: Gigaword
(GW), GigaWord + 50% of web data (GW-WB1),
and GigaWord + 100% of web data (GW-WB2).
</bodyText>
<table confidence="0.99853775">
Corpus Sub GW GW- GW-
set WB1 WB2
Size .32 9.8 49 90
(GB)
# ofsentences 2.00 56.78 462.60 866.02
(Million)
Stream Size .25 7.65 37.93 69.41
(Billion)
</table>
<tableCaption confidence="0.998709">
Table 1: Corpus Description
</tableCaption>
<subsectionHeader confidence="0.905409">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999744933333333">
We compare our system with two baselines: Ex-
act and Exact1000 which use exact counts. Since
computing the exact counts of all word-context
pairs on these corpora is not possible using main
memory of only 8 GB , we generate context vec-
tors for only those words which appear in the test
set. The former baseline uses all possible contexts
which appear with a test word, while the latter
baseline uses only the top 1000 contexts (based on
PMI value) for each word. In each case, we use
a cutoff (of 10, 60 and 120) on the frequency of
word-context pairs. These cut-offs were selected
based on the intuition that, with more data, you
get more noise, and not considering word-context
pairs with frequency less than 120 might be a bet-
</bodyText>
<page confidence="0.997262">
54
</page>
<table confidence="0.9999829">
Data GW GW-WB1 GW-WB2
Model Frequency cutoff Frequency cutoff Frequency cutoff
10 60 120 10 60 120 10 60 120
P P P
WS-353
Exact .25 .25 .22 .29 .28 .28 .30 .28 .28
Exact1000 .36 .28 .22 .46 .43 .37 .47 .44 .41
Our Model .39 .28 .22 -0.09 .48 .40 -0.03 .04 .47
WS-203
Exact .35 .36 .33 .38 .38 .37 .40 .38 .38
Exact1000 .49 .40 .35 .57 .55 .47 .56 .56 .52
Our Model .49 .39 .35 -0.08 .58 .47 -0.06 .03 .55
RG-65
Exact .21 .12 .08 .42 .28 .22 .39 .31 .23
Exact1000 .14 .09 .08 .45 .16 .13 .47 .26 .12
Our Model .13 .10 .09 -0.06 .32 .18 -0.05 .08 .31
MC-30
Exact .26 .23 .21 .45 .33 .31 .46 .39 .29
Exact1000 .27 .18 .21 .63 .42 .32 .59 .47 .36
Our Model .36 .20 .21 -0.08 .52 .39 -0.27 -0.29 .52
</table>
<tableCaption confidence="0.999907">
Table 2: Evaluating word pairs ranking with Exact and CU counts. Scores are evaluated using P metric.
</tableCaption>
<bodyText confidence="0.999577806451613">
ter choice than a cutoff of 10. The results are
shown in Table 2
From the above baseline results, first we learn
that using more data helps in better capturing
the semantic similarity between words. Second,
it shows that using top (K) 1000 contexts for
each target word captures better semantic similar-
ity than using all possible contexts for that word.
Third, using a cutoff of 10 is optimal for all differ-
ent sized corpora on all test-sets.
We use approximate counts from CU sketch
with depth=3 and 2 billion (2B) counters (‘Our
Model’)8. Based on previous observation, we re-
strict the number of contexts for a target word to
1000. Table 2 shows that using CU counts makes
the algorithm sensitive to frequency cutoff. How-
ever, with appropriate frequency cutoff for each
corpus, approximate counts are nearly as effective
as exact counts. For GW, GW-WB1, and GW-
WB2, the frequency cutoffs of 10, 60, and 120 re-
spectively performed the best. The reason for de-
pendence on frequency cutoffs is due to the over-
estimation of low-frequent items. This is more
pronounced with bigger corpus (GW-WB2) as the
size of CU sketch is fixed to 2B counters and
stream size is much bigger (69.41 billion) com-
pared to GW where the stream size is 7.65 billion.
The advantages of using our model is that the
sketch contains counts for all words, contexts, and
word-context pairs stored in fixed memory of 8
GB by using only 2B counters. Note that it is not
</bodyText>
<footnote confidence="0.948147666666667">
8Our goal is not to build the best distributional similarity
method. It is to show that our framework scales easily to large
corpus and it is as effective as exact method.
</footnote>
<bodyText confidence="0.999651083333333">
feasible to keep track of exact counts of all word-
context pairs since their number increases rapidly
with increase in data (see Fig. 1). We can use our
model to create context vectors of size K for all
possible words stored in the Sketch and computes
semantic similarity between two words in O(K)
time. In addition, the linearity of sketch allows
us to include new incoming data into the sketch
without building the sketch from scratch. Also,
it allows for parallelization using the MapReduce
framework. We can generalize our framework to
any kind of association and similarity measure.
</bodyText>
<sectionHeader confidence="0.999541" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999925545454545">
We proposed a framework which uses CU Sketch
to scale distributional similarity to the web. It can
compute similarity between any word pairs that
are stored in the sketch and returns similarity be-
tween them in O(K) time. In our experiments, we
show that our framework is as effective as using
the exact counts, however it is sensitive to the fre-
quency cutoffs. In future, we will explore ways to
make this framework robust to the frequency cut-
offs. In addition, we are interested in exploring
this framework for entity set expansion problem.
</bodyText>
<sectionHeader confidence="0.998811" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99806475">
We thank the anonymous reviewers for helpful
comments. This work is partially funded by NSF
grant IIS-0712764 and Google Research Grant
Grant for Large-Data NLP.
</bodyText>
<page confidence="0.998158">
55
</page>
<sectionHeader confidence="0.995998" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9995882">
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distri-
butional and wordnet-based approaches. In NAACL
’09: Proceedings ofHLT-NAACL.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).
Graham Cormode and Marios Hadjieleftheriou. 2008.
Finding frequent items in data streams. In VLDB.
Graham Cormode and S. Muthukrishnan. 2004. An
improved data stream summary: The count-min
sketch and its applications. J. Algorithms.
Cristian Estan and George Varghese. 2002. New direc-
tions in traffic measurement and accounting. SIG-
COMM Comput. Commun. Rev., 32(4).
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing search in context: The concept revisited. In
ACM Transactions on Information Systems.
J. Firth. 1968. A synopsis of linguistic theory 1930-
1955. In F. Palmer, editor, Selected Papers of J. R.
Firth. Longman.
Amit Goyal, Hal Daum´e III, and Suresh Venkatasub-
ramanian. 2009. Streaming for large scale NLP:
Language modeling. In North American Chap-
ter of the Association for Computational Linguistics
(NAACL).
Amit Goyal, Jagadeesh Jagarlamudi, Hal Daum´e III,
and Suresh Venkatasubramanian. 2010. Sketching
techniques for Large Scale NLP. In 6th Web as Cor-
pus Workshop in conjunction with NAACL-HLT.
D. Graff. 2003. English Gigaword. Linguistic Data
Consortium, Philadelphia, PA, January.
Z. Harris. 1985. Distributional structure. In J. J. Katz,
editor, The Philosophy of Linguistics, pages 26–47.
Oxford University Press, New York.
Abby Levenberg and Miles Osborne. 2009. Stream-
based randomised language models for SMT. In
Proceedings ofEMNLP, August.
G.A. Miller and W.G. Charles. 1991. Contextual cor-
relates of semantic similarity. Language and Cogni-
tive Processes, 6(1):1–28.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings ofEMNLP.
Deepak Ravichandran, Patrick Pantel, and Eduard
Hovy. 2005. Randomized algorithms and nlp: using
locality sensitive hash function for high speed noun
clustering. In ACL ’05: Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics.
H. Rubenstein and J.B. Goodenough. 1965. Contex-
tual correlates of synonymy. Computational Lin-
guistics, 8:627–633.
Florin Rusu and Alin Dobra. 2007. Statistical analysis
of sketch estimators. In SIGMOD ’07. ACM.
P.D. Turney and M.L. Littman. 2002. Unsupervised
learning of semantic orientation from a hundred-
billion-word corpus.
Peter D. Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In Pro-
ceedings of COLING 2008.
Benjamin Van Durme and Ashwin Lall. 2009a. Prob-
abilistic counting with randomized storage. In IJ-
CAI’09: Proceedings of the 21st international jont
conference on Artifical intelligence.
Benjamin Van Durme and Ashwin Lall. 2009b.
Streaming pointwise mutual information. In Ad-
vances in Neural Information Processing Systems
22.
</reference>
<page confidence="0.998272">
56
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.737820">
<title confidence="0.997686">Sketch Techniques for Scaling Distributional Similarity to the Web</title>
<author confidence="0.990488">Amit Goyal</author>
<author confidence="0.990488">Jagadeesh Jagarlamudi</author>
<author confidence="0.990488">Hal Daum´e</author>
<author confidence="0.990488">Suresh</author>
<affiliation confidence="0.998741">School of University of</affiliation>
<address confidence="0.806593">Salt Lake City, UT</address>
<abstract confidence="0.996078578947368">In this paper, we propose a memory, space, and time efficient framework to scale distributional similarity to the web. We exploit sketch techniques, especially the Count-Min sketch, which approximates the frequency of an item in the corpus without explicitly storing the item itself. These methods use hashing to deal with massive amounts of the streaming text. We all item counts computed from of web data in just counmain memory) of CM sketch. Our method returns semantic similarity word pairs in time and can compute similarity between any word pairs that are stored in the sketch. In our experiments, we show that our framework is as effective as using the exact counts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Enrique Alfonseca</author>
<author>Keith Hall</author>
<author>Jana Kravalova</author>
<author>Marius Pas¸ca</author>
<author>Aitor Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and wordnet-based approaches.</title>
<date>2009</date>
<booktitle>In NAACL ’09: Proceedings ofHLT-NAACL.</booktitle>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pas¸ca, Soroa, 2009</marker>
<rawString>Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and wordnet-based approaches. In NAACL ’09: Proceedings ofHLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).</booktitle>
<contexts>
<context position="1047" citStr="Brants et al., 2007" startWordPosition="162" endWordPosition="165">in sketch, which approximates the frequency of an item in the corpus without explicitly storing the item itself. These methods use hashing to deal with massive amounts of the streaming text. We store all item counts computed from 90 GB of web data in just 2 billion counters (8 GB main memory) of CM sketch. Our method returns semantic similarity between word pairs in O(K) time and can compute similarity between any word pairs that are stored in the sketch. In our experiments, we show that our framework is as effective as using the exact counts. 1 Introduction In many NLP problems, researchers (Brants et al., 2007; Turney, 2008) have shown that having large amounts of data is beneficial. It has also been shown that (Agirre et al., 2009; Pantel et al., 2009; Ravichandran et al., 2005) having large amounts of data helps capturing the semantic similarity between pairs of words. However, computing distributional similarity (Sec. 2.1) between word pairs from large text collections is a computationally expensive task. In this work, we consider scaling distributional similarity methods for computing semantic similarity between words to Web-scale. The major difficulty in computing pairwise similarities stems f</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Cormode</author>
<author>Marios Hadjieleftheriou</author>
</authors>
<title>Finding frequent items in data streams.</title>
<date>2008</date>
<booktitle>In VLDB.</booktitle>
<contexts>
<context position="7946" citStr="Cormode and Hadjieleftheriou, 2008" startWordPosition="1298" endWordPosition="1301">tor as a data structure to store the streaming data compactly in a small-memory footprint. These techniques use hashing to map items in the streaming data onto a small sketch vector that can be easily updated and queried. These techniques generally process the input stream in one direction, say from left to right, 4We use only 2 billion counters which takes up to 8 GB of main memory. 52 without re-processing previous input. The main advantage of using these techniques is that they require a storage which is significantly smaller than the input stream length. A survey by (Rusu and Dobra, 2007; Cormode and Hadjieleftheriou, 2008) comprehensively reviews the literature. 3 Count-Min Sketch The Count-Min Sketch (Cormode and Muthukrishnan, 2004) is a compact summary data structure used to store the frequencies of all items in the input stream. Given an input stream of items of length N and user chosen parameters S and ǫ, the algorithm stores the frequencies of all the items with the following guarantees: • All reported frequencies are within ǫN of true frequencies with probability of atleast S. • Space used by the algorithm is O(1ǫ log δ1 ). • Constant time of O(log(1δ )) per each update and query operation. 3.1 CM Data S</context>
</contexts>
<marker>Cormode, Hadjieleftheriou, 2008</marker>
<rawString>Graham Cormode and Marios Hadjieleftheriou. 2008. Finding frequent items in data streams. In VLDB.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Cormode</author>
<author>S Muthukrishnan</author>
</authors>
<title>An improved data stream summary: The count-min sketch and its applications.</title>
<date>2004</date>
<journal>J. Algorithms.</journal>
<contexts>
<context position="2708" citStr="Cormode and Muthukrishnan, 2004" startWordPosition="434" endWordPosition="438">a result, it is computationally hard to compute counts of all word-context pairs with a giant corpora using conventional machines (say with main memory of 8 GB). To overcome this, Agirre et al. (2009) used MapReduce infrastructure (with 2, 000 cores) to compute pairwise similarities of words on a corpus of roughly 1.6 Terawords. In a different direction, our earlier work (Goyal et al., 2010) developed techniques to make the computations feasible on a conventional machines by willing to accept some error in the counts. Similar to that work, this work exploits the idea of Count-Min (CM) sketch (Cormode and Muthukrishnan, 2004) to approximate the frequency of word pairs in the corpus without explicitly storing the word pairs themselves. In their, we stored 1Note that the plot is in log-log scale. 2‘Subset’ column of Table 1 in Section 5.1 5 10 15 20 25 Log2 of # of unique Items 20 15 10 5 word−context pairs words 51 Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 51–56, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics counts of all words/word pairs in fixed amount of main memory. We used conservative update with CM sketch (referred </context>
<context position="8060" citStr="Cormode and Muthukrishnan, 2004" startWordPosition="1312" endWordPosition="1316">ng to map items in the streaming data onto a small sketch vector that can be easily updated and queried. These techniques generally process the input stream in one direction, say from left to right, 4We use only 2 billion counters which takes up to 8 GB of main memory. 52 without re-processing previous input. The main advantage of using these techniques is that they require a storage which is significantly smaller than the input stream length. A survey by (Rusu and Dobra, 2007; Cormode and Hadjieleftheriou, 2008) comprehensively reviews the literature. 3 Count-Min Sketch The Count-Min Sketch (Cormode and Muthukrishnan, 2004) is a compact summary data structure used to store the frequencies of all items in the input stream. Given an input stream of items of length N and user chosen parameters S and ǫ, the algorithm stores the frequencies of all the items with the following guarantees: • All reported frequencies are within ǫN of true frequencies with probability of atleast S. • Space used by the algorithm is O(1ǫ log δ1 ). • Constant time of O(log(1δ )) per each update and query operation. 3.1 CM Data Structure A Count-Min Sketch with parameters (ǫ,S) is represented by a two-dimensional array with width w and depth</context>
</contexts>
<marker>Cormode, Muthukrishnan, 2004</marker>
<rawString>Graham Cormode and S. Muthukrishnan. 2004. An improved data stream summary: The count-min sketch and its applications. J. Algorithms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristian Estan</author>
<author>George Varghese</author>
</authors>
<title>New directions in traffic measurement and accounting.</title>
<date>2002</date>
<journal>SIGCOMM Comput. Commun. Rev.,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="11315" citStr="Estan and Varghese, 2002" startWordPosition="1883" endWordPosition="1886">hat makes it attractive for scaling distributional similarity to the web: 1. Linearity: given two sketches s1 and s2 computed (using the same parameters w and d) over different input streams, the sketch of the combined data stream can be easily obtained by adding the individual sketches. 2. The linearity allows the individual sketches to be computed independent of each other. This means that it is easy to implement it in distributed setting, where each machine computes the sketch over a subset of the corpus. 3.3 Conservative Update Estan and Varghese introduce the idea of conservative update (Estan and Varghese, 2002) in the context of networking. This can easily be used with CM Sketch (CU Sketch) to further improve the estimate of a point query. To update an item, w with frequency c, we first compute the frequency c� of 5In our setting, c is always 1. sketch[1, 1] • • • sketch[1, w] .. .. .. sketch[d, 1] • • • sketch[d, w] � � � 1 ... 53 this item from the existing data structure and the counts are updated according to: b1 &lt; k &lt; d sketch[k,hkW] &lt;-- max{sketch[k,hkW], c� + c} The intuition is that, since the point query returns the minimum of all the d values, we will update a counter only if it is necessa</context>
</contexts>
<marker>Estan, Varghese, 2002</marker>
<rawString>Cristian Estan and George Varghese. 2002. New directions in traffic measurement and accounting. SIGCOMM Comput. Commun. Rev., 32(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Finkelstein</author>
<author>E Gabrilovich</author>
<author>Y Matias</author>
<author>E Rivlin</author>
<author>Z Solan</author>
<author>G Wolfman</author>
<author>E Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited. In</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems.</journal>
<contexts>
<context position="12984" citStr="Finkelstein et al., 2002" startWordPosition="2177" endWordPosition="2180">te PMI scores between word-context pairs. Use these PMI scores to store top K contexts for a word on the disk. Store these top K context vectors for every word stored in the sketch. 3. Use cosine similarity to compute the similarity between word pairs using these approximate top K context vectors constructed using CU sketch. 5 Word pair Ranking Evaluations As discussed earlier, the DPs of words are used to compute similarity between a pair of words. We used the following four test sets and their corresponding human judgements to evaluate the word pair rankings. 1. WS-353: WordSimilarity-3536 (Finkelstein et al., 2002) is a set of 353 word pairs. 2. WS-203: A subset of WS-353 containing 203 word pairs marked according to similarity7 (Agirre et al., 2009). 3. RG-65: (Rubenstein and Goodenough, 1965) is set of 65 word pairs. 4. MC-30: A smaller subset of the RG-65 dataset containing 30 word pairs (Miller and Charles, 1991). 6http://www.cs.technion.ac.il/ gabr/resources/data/wordsim353/wordsim353.html 7http://alfonseca.org/pubs/ws353simrel.tar.gz Each of these data sets come with human ranking of the word pairs. We rank the word pairs based on the similarity computed using DPs and evaluate this ranking against</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. 2002. Placing search in context: The concept revisited. In ACM Transactions on Information Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Firth</author>
</authors>
<title>A synopsis of linguistic theory 1930-1955.</title>
<date>1968</date>
<editor>In F. Palmer, editor, Selected Papers of J. R. Firth.</editor>
<publisher>Longman.</publisher>
<contexts>
<context position="5099" citStr="Firth, 1968" startWordPosition="838" endWordPosition="839">that are stored in the CU sketch. Second, it can return the similarity between word pairs in time O(K). Third, because we do not store items explicitly, the overall space required is significantly smaller. Fourth, the additive property of CU sketch (Sec. 3.2) enables us to parallelize most of the steps in the algorithm. Thus it can be easily extended to very large amounts of text data. Fifth, this easily generalizes to any kind of association measure and semantic similarity measure. 2 Background 2.1 Distributional Similarity Distributional Similarity is based on the distributional hypothesis (Firth, 1968; Harris, 1985) that words occur in similar contexts tend to be similar. The context of a word is represented by the distributional profile (DP), which contains the strength of association between the word and each of the lexical, syntactic, semantic, and/or dependency units that co-occur with it3. The association 3In this work, we only consider lexical units as context. is commonly measured using conditional probability, pointwise mutual information (PMI) or log likelihood ratios. Then the semantic similarity between two words, given their DPs, is calculated using similarity measures such as </context>
</contexts>
<marker>Firth, 1968</marker>
<rawString>J. Firth. 1968. A synopsis of linguistic theory 1930-1955. In F. Palmer, editor, Selected Papers of J. R. Firth. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Goyal</author>
<author>Hal Daum´e</author>
<author>Suresh Venkatasubramanian</author>
</authors>
<title>Streaming for large scale NLP: Language modeling.</title>
<date>2009</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics (NAACL).</booktitle>
<marker>Goyal, Daum´e, Venkatasubramanian, 2009</marker>
<rawString>Amit Goyal, Hal Daum´e III, and Suresh Venkatasubramanian. 2009. Streaming for large scale NLP: Language modeling. In North American Chapter of the Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Goyal</author>
<author>Jagadeesh Jagarlamudi</author>
<author>Hal Daum´e</author>
<author>Suresh Venkatasubramanian</author>
</authors>
<title>Sketching techniques for Large Scale NLP.</title>
<date>2010</date>
<booktitle>In 6th Web as Corpus Workshop in conjunction with NAACL-HLT.</booktitle>
<marker>Goyal, Jagarlamudi, Daum´e, Venkatasubramanian, 2010</marker>
<rawString>Amit Goyal, Jagadeesh Jagarlamudi, Hal Daum´e III, and Suresh Venkatasubramanian. 2010. Sketching techniques for Large Scale NLP. In 6th Web as Corpus Workshop in conjunction with NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Graff</author>
</authors>
<title>English Gigaword. Linguistic Data Consortium,</title>
<date>2003</date>
<location>Philadelphia, PA,</location>
<contexts>
<context position="13745" citStr="Graff, 2003" startWordPosition="2291" endWordPosition="2292"> (Rubenstein and Goodenough, 1965) is set of 65 word pairs. 4. MC-30: A smaller subset of the RG-65 dataset containing 30 word pairs (Miller and Charles, 1991). 6http://www.cs.technion.ac.il/ gabr/resources/data/wordsim353/wordsim353.html 7http://alfonseca.org/pubs/ws353simrel.tar.gz Each of these data sets come with human ranking of the word pairs. We rank the word pairs based on the similarity computed using DPs and evaluate this ranking against the human ranking. We report the spearman’s rank correlation coefficient (p) between these two rankings. 5.1 Corpus Statistics The Gigaword corpus (Graff, 2003) and a copy of the web crawled by (Ravichandran et al., 2005) are used to compute counts of all items (Table. 1). For both the corpora, we split the text into sentences, tokenize, convert into lower-case, remove punctuations, and collapse each digit to a symbol “0” (e.g. “1996” gets collapsed to “0000”). We store the counts of all words (excluding numbers, and stop words), their contexts, and counts of word-context pairs in the CU sketch. We define the context for a given word “x” as the surrounding words appearing in a window of 2 words to the left and 2 words to the right. The context words </context>
</contexts>
<marker>Graff, 2003</marker>
<rawString>D. Graff. 2003. English Gigaword. Linguistic Data Consortium, Philadelphia, PA, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Harris</author>
</authors>
<title>Distributional structure. In</title>
<date>1985</date>
<booktitle>The Philosophy of Linguistics,</booktitle>
<pages>26--47</pages>
<editor>J. J. Katz, editor,</editor>
<publisher>Oxford University Press,</publisher>
<location>New York.</location>
<contexts>
<context position="5114" citStr="Harris, 1985" startWordPosition="840" endWordPosition="841">ed in the CU sketch. Second, it can return the similarity between word pairs in time O(K). Third, because we do not store items explicitly, the overall space required is significantly smaller. Fourth, the additive property of CU sketch (Sec. 3.2) enables us to parallelize most of the steps in the algorithm. Thus it can be easily extended to very large amounts of text data. Fifth, this easily generalizes to any kind of association measure and semantic similarity measure. 2 Background 2.1 Distributional Similarity Distributional Similarity is based on the distributional hypothesis (Firth, 1968; Harris, 1985) that words occur in similar contexts tend to be similar. The context of a word is represented by the distributional profile (DP), which contains the strength of association between the word and each of the lexical, syntactic, semantic, and/or dependency units that co-occur with it3. The association 3In this work, we only consider lexical units as context. is commonly measured using conditional probability, pointwise mutual information (PMI) or log likelihood ratios. Then the semantic similarity between two words, given their DPs, is calculated using similarity measures such as Cosine, α-skew </context>
</contexts>
<marker>Harris, 1985</marker>
<rawString>Z. Harris. 1985. Distributional structure. In J. J. Katz, editor, The Philosophy of Linguistics, pages 26–47. Oxford University Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abby Levenberg</author>
<author>Miles Osborne</author>
</authors>
<title>Streambased randomised language models for SMT.</title>
<date>2009</date>
<booktitle>In Proceedings ofEMNLP,</booktitle>
<contexts>
<context position="6760" citStr="Levenberg and Osborne (2009)" startWordPosition="1097" endWordPosition="1100">reaming, and randomized algorithms to handle large amounts of data. Ravichandran et al. (2005) used locality sensitive hash functions for computing word-pair similarities from large text collections. Their approach stores a enormous matrix of all unique words and their contexts in main memory which makes it hard for larger data sets. In our work, we store all unique word-context pairs in CU sketch with a pre-defined size4. Recently, the streaming algorithm paradigm has been used to provide memory and time-efficient platform to deal with terabytes of data. For example, we (Goyal et al., 2009); Levenberg and Osborne (2009) build approximate language models and show their effectiveness in SMT. In (Van Durme and Lall, 2009b), a TOMB Counter (Van Durme and Lall, 2009a) was used to find the top-K verbs “y” with the highest PMI for a given verb “x”. The idea of TOMB is similar to CU Sketch. However, we use CU Sketch because of its simplicity and attractive properties (see Sec. 3). In this work, we go one step further, and compute semantic similarity between word-pairs using approximate PMI scores from CU sketch. 2.3 Sketch Techniques Sketch techniques use a sketch vector as a data structure to store the streaming da</context>
</contexts>
<marker>Levenberg, Osborne, 2009</marker>
<rawString>Abby Levenberg and Miles Osborne. 2009. Streambased randomised language models for SMT. In Proceedings ofEMNLP, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>W G Charles</author>
</authors>
<title>Contextual correlates of semantic similarity.</title>
<date>1991</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>6--1</pages>
<contexts>
<context position="13292" citStr="Miller and Charles, 1991" startWordPosition="2231" endWordPosition="2234">d using CU sketch. 5 Word pair Ranking Evaluations As discussed earlier, the DPs of words are used to compute similarity between a pair of words. We used the following four test sets and their corresponding human judgements to evaluate the word pair rankings. 1. WS-353: WordSimilarity-3536 (Finkelstein et al., 2002) is a set of 353 word pairs. 2. WS-203: A subset of WS-353 containing 203 word pairs marked according to similarity7 (Agirre et al., 2009). 3. RG-65: (Rubenstein and Goodenough, 1965) is set of 65 word pairs. 4. MC-30: A smaller subset of the RG-65 dataset containing 30 word pairs (Miller and Charles, 1991). 6http://www.cs.technion.ac.il/ gabr/resources/data/wordsim353/wordsim353.html 7http://alfonseca.org/pubs/ws353simrel.tar.gz Each of these data sets come with human ranking of the word pairs. We rank the word pairs based on the similarity computed using DPs and evaluate this ranking against the human ranking. We report the spearman’s rank correlation coefficient (p) between these two rankings. 5.1 Corpus Statistics The Gigaword corpus (Graff, 2003) and a copy of the web crawled by (Ravichandran et al., 2005) are used to compute counts of all items (Table. 1). For both the corpora, we split th</context>
</contexts>
<marker>Miller, Charles, 1991</marker>
<rawString>G.A. Miller and W.G. Charles. 1991. Contextual correlates of semantic similarity. Language and Cognitive Processes, 6(1):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
</authors>
<title>Eric Crestan, Arkady Borkovsky, AnaMaria Popescu, and Vishnu Vyas.</title>
<date>2009</date>
<booktitle>In Proceedings ofEMNLP.</booktitle>
<marker>Pantel, 2009</marker>
<rawString>Patrick Pantel, Eric Crestan, Arkady Borkovsky, AnaMaria Popescu, and Vishnu Vyas. 2009. Web-scale distributional similarity and entity set expansion. In Proceedings ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Patrick Pantel</author>
<author>Eduard Hovy</author>
</authors>
<title>Randomized algorithms and nlp: using locality sensitive hash function for high speed noun clustering.</title>
<date>2005</date>
<booktitle>In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1220" citStr="Ravichandran et al., 2005" startWordPosition="192" endWordPosition="195">s of the streaming text. We store all item counts computed from 90 GB of web data in just 2 billion counters (8 GB main memory) of CM sketch. Our method returns semantic similarity between word pairs in O(K) time and can compute similarity between any word pairs that are stored in the sketch. In our experiments, we show that our framework is as effective as using the exact counts. 1 Introduction In many NLP problems, researchers (Brants et al., 2007; Turney, 2008) have shown that having large amounts of data is beneficial. It has also been shown that (Agirre et al., 2009; Pantel et al., 2009; Ravichandran et al., 2005) having large amounts of data helps capturing the semantic similarity between pairs of words. However, computing distributional similarity (Sec. 2.1) between word pairs from large text collections is a computationally expensive task. In this work, we consider scaling distributional similarity methods for computing semantic similarity between words to Web-scale. The major difficulty in computing pairwise similarities stems from the rapid increase in the number of unique word-context pairs with the size of text corpus (number of tokens). Fig. 1 shows that Log2 of # of words Figure 1: Token Type </context>
<context position="6226" citStr="Ravichandran et al. (2005)" startWordPosition="1011" endWordPosition="1014">tic similarity between two words, given their DPs, is calculated using similarity measures such as Cosine, α-skew divergence, and Jensen-Shannon divergence. In our work, we use PMI as association measure and cosine similarity to compute pairwise similarities. 2.2 Large Scale NLP problems Pantel et al. (2009) computed similarity between 500 million word pairs using the MapReduce framework from a 200 billion word corpus using 200 quad-core nodes. The inaccessibility of clusters for every one has attracted NLP community to use streaming, and randomized algorithms to handle large amounts of data. Ravichandran et al. (2005) used locality sensitive hash functions for computing word-pair similarities from large text collections. Their approach stores a enormous matrix of all unique words and their contexts in main memory which makes it hard for larger data sets. In our work, we store all unique word-context pairs in CU sketch with a pre-defined size4. Recently, the streaming algorithm paradigm has been used to provide memory and time-efficient platform to deal with terabytes of data. For example, we (Goyal et al., 2009); Levenberg and Osborne (2009) build approximate language models and show their effectiveness in</context>
<context position="13806" citStr="Ravichandran et al., 2005" startWordPosition="2301" endWordPosition="2304"> word pairs. 4. MC-30: A smaller subset of the RG-65 dataset containing 30 word pairs (Miller and Charles, 1991). 6http://www.cs.technion.ac.il/ gabr/resources/data/wordsim353/wordsim353.html 7http://alfonseca.org/pubs/ws353simrel.tar.gz Each of these data sets come with human ranking of the word pairs. We rank the word pairs based on the similarity computed using DPs and evaluate this ranking against the human ranking. We report the spearman’s rank correlation coefficient (p) between these two rankings. 5.1 Corpus Statistics The Gigaword corpus (Graff, 2003) and a copy of the web crawled by (Ravichandran et al., 2005) are used to compute counts of all items (Table. 1). For both the corpora, we split the text into sentences, tokenize, convert into lower-case, remove punctuations, and collapse each digit to a symbol “0” (e.g. “1996” gets collapsed to “0000”). We store the counts of all words (excluding numbers, and stop words), their contexts, and counts of word-context pairs in the CU sketch. We define the context for a given word “x” as the surrounding words appearing in a window of 2 words to the left and 2 words to the right. The context words are concatenated along with their positions -2, -1, +1, and +</context>
</contexts>
<marker>Ravichandran, Pantel, Hovy, 2005</marker>
<rawString>Deepak Ravichandran, Patrick Pantel, and Eduard Hovy. 2005. Randomized algorithms and nlp: using locality sensitive hash function for high speed noun clustering. In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Rubenstein</author>
<author>J B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Computational Linguistics,</journal>
<pages>8--627</pages>
<contexts>
<context position="13167" citStr="Rubenstein and Goodenough, 1965" startWordPosition="2208" endWordPosition="2211">etch. 3. Use cosine similarity to compute the similarity between word pairs using these approximate top K context vectors constructed using CU sketch. 5 Word pair Ranking Evaluations As discussed earlier, the DPs of words are used to compute similarity between a pair of words. We used the following four test sets and their corresponding human judgements to evaluate the word pair rankings. 1. WS-353: WordSimilarity-3536 (Finkelstein et al., 2002) is a set of 353 word pairs. 2. WS-203: A subset of WS-353 containing 203 word pairs marked according to similarity7 (Agirre et al., 2009). 3. RG-65: (Rubenstein and Goodenough, 1965) is set of 65 word pairs. 4. MC-30: A smaller subset of the RG-65 dataset containing 30 word pairs (Miller and Charles, 1991). 6http://www.cs.technion.ac.il/ gabr/resources/data/wordsim353/wordsim353.html 7http://alfonseca.org/pubs/ws353simrel.tar.gz Each of these data sets come with human ranking of the word pairs. We rank the word pairs based on the similarity computed using DPs and evaluate this ranking against the human ranking. We report the spearman’s rank correlation coefficient (p) between these two rankings. 5.1 Corpus Statistics The Gigaword corpus (Graff, 2003) and a copy of the web</context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>H. Rubenstein and J.B. Goodenough. 1965. Contextual correlates of synonymy. Computational Linguistics, 8:627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florin Rusu</author>
<author>Alin Dobra</author>
</authors>
<title>Statistical analysis of sketch estimators.</title>
<date>2007</date>
<booktitle>In SIGMOD ’07.</booktitle>
<publisher>ACM.</publisher>
<contexts>
<context position="7909" citStr="Rusu and Dobra, 2007" startWordPosition="1294" endWordPosition="1297">iques use a sketch vector as a data structure to store the streaming data compactly in a small-memory footprint. These techniques use hashing to map items in the streaming data onto a small sketch vector that can be easily updated and queried. These techniques generally process the input stream in one direction, say from left to right, 4We use only 2 billion counters which takes up to 8 GB of main memory. 52 without re-processing previous input. The main advantage of using these techniques is that they require a storage which is significantly smaller than the input stream length. A survey by (Rusu and Dobra, 2007; Cormode and Hadjieleftheriou, 2008) comprehensively reviews the literature. 3 Count-Min Sketch The Count-Min Sketch (Cormode and Muthukrishnan, 2004) is a compact summary data structure used to store the frequencies of all items in the input stream. Given an input stream of items of length N and user chosen parameters S and ǫ, the algorithm stores the frequencies of all the items with the following guarantees: • All reported frequencies are within ǫN of true frequencies with probability of atleast S. • Space used by the algorithm is O(1ǫ log δ1 ). • Constant time of O(log(1δ )) per each upda</context>
</contexts>
<marker>Rusu, Dobra, 2007</marker>
<rawString>Florin Rusu and Alin Dobra. 2007. Statistical analysis of sketch estimators. In SIGMOD ’07. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
<author>M L Littman</author>
</authors>
<title>Unsupervised learning of semantic orientation from a hundredbillion-word corpus.</title>
<date>2002</date>
<contexts>
<context position="3670" citStr="Turney and Littman, 2002" startWordPosition="601" endWordPosition="604">rical Models of Natural Language Semantics, ACL 2010, pages 51–56, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics counts of all words/word pairs in fixed amount of main memory. We used conservative update with CM sketch (referred as CU sketch) and showed that it reduces the average relative error of its approximate counts by a factor of two. The approximate counts returned by CU Sketch were used to compute approximate PMI between word pairs. We found their that the approximate PMI values are as useful as the exact PMI values for computing semantic orientation (Turney and Littman, 2002) of words. In addition, our intrinsic evaluations in their showed that the quality of approximate counts and approximate PMI is good. In this work, we use CU-sketch to store counts of items (words, contexts, and word-context pairs) using fixed amount of memory of 8 GB by using only 2B counters. These approximate counts returned by CU Sketch are converted into approximate PMI between word-context pairs. The top K contexts (based on PMI score) for each word are used to construct distributional profile (DP) for each word. The similarity between a pair of words is computed based on the cosine simi</context>
</contexts>
<marker>Turney, Littman, 2002</marker>
<rawString>P.D. Turney and M.L. Littman. 2002. Unsupervised learning of semantic orientation from a hundredbillion-word corpus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>A uniform approach to analogies, synonyms, antonyms, and associations.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="1062" citStr="Turney, 2008" startWordPosition="166" endWordPosition="167">oximates the frequency of an item in the corpus without explicitly storing the item itself. These methods use hashing to deal with massive amounts of the streaming text. We store all item counts computed from 90 GB of web data in just 2 billion counters (8 GB main memory) of CM sketch. Our method returns semantic similarity between word pairs in O(K) time and can compute similarity between any word pairs that are stored in the sketch. In our experiments, we show that our framework is as effective as using the exact counts. 1 Introduction In many NLP problems, researchers (Brants et al., 2007; Turney, 2008) have shown that having large amounts of data is beneficial. It has also been shown that (Agirre et al., 2009; Pantel et al., 2009; Ravichandran et al., 2005) having large amounts of data helps capturing the semantic similarity between pairs of words. However, computing distributional similarity (Sec. 2.1) between word pairs from large text collections is a computationally expensive task. In this work, we consider scaling distributional similarity methods for computing semantic similarity between words to Web-scale. The major difficulty in computing pairwise similarities stems from the rapid i</context>
</contexts>
<marker>Turney, 2008</marker>
<rawString>Peter D. Turney. 2008. A uniform approach to analogies, synonyms, antonyms, and associations. In Proceedings of COLING 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Ashwin Lall</author>
</authors>
<title>Probabilistic counting with randomized storage.</title>
<date>2009</date>
<booktitle>In IJCAI’09: Proceedings of the 21st international jont conference on Artifical intelligence.</booktitle>
<marker>Van Durme, Lall, 2009</marker>
<rawString>Benjamin Van Durme and Ashwin Lall. 2009a. Probabilistic counting with randomized storage. In IJCAI’09: Proceedings of the 21st international jont conference on Artifical intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Ashwin Lall</author>
</authors>
<title>Streaming pointwise mutual information.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems 22.</booktitle>
<marker>Van Durme, Lall, 2009</marker>
<rawString>Benjamin Van Durme and Ashwin Lall. 2009b. Streaming pointwise mutual information. In Advances in Neural Information Processing Systems 22.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>