<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007139">
<title confidence="0.989098">
Learning Hierarchical Translation Structure with Linguistic Annotations
</title>
<author confidence="0.963493">
Markos Mylonakis Khalil Sima’an
</author>
<affiliation confidence="0.938643">
ILLC ILLC
University of Amsterdam University of Amsterdam
</affiliation>
<email confidence="0.996107">
m.mylonakis@uva.nl k.simaan@uva.nl
</email>
<sectionHeader confidence="0.997312" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999196958333333">
While it is generally accepted that many trans-
lation phenomena are correlated with linguis-
tic structures, employing linguistic syntax for
translation has proven a highly non-trivial
task. The key assumption behind many ap-
proaches is that translation is guided by the
source and/or target language parse, employ-
ing rules extracted from the parse tree or
performing tree transformations. These ap-
proaches enforce strict constraints and might
overlook important translation phenomena
that cross linguistic constituents. We propose
a novel flexible modelling approach to intro-
duce linguistic information of varying gran-
ularity from the source side. Our method
induces joint probability synchronous gram-
mars and estimates their parameters, by select-
ing and weighing together linguistically moti-
vated rules according to an objective function
directly targeting generalisation over future
data. We obtain statistically significant im-
provements across 4 different language pairs
with English as source, mounting up to +1.92
BLEU for Chinese as target.
</bodyText>
<sectionHeader confidence="0.999466" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.977556659574468">
Recent advances in Statistical Machine Translation
(SMT) are widely centred around two concepts:
(a) hierarchical translation processes, frequently
employing Synchronous Context Free Grammars
(SCFGs) and (b) transduction or synchronous
rewrite processes over a linguistic syntactic tree.
SCFGs in the form of the Inversion-Transduction
Grammar (ITG) were first introduced by (Wu, 1997)
as a formalism to recursively describe the trans-
lation process. The Hiero system (Chiang, 2005)
642
utilised an ITG-flavour which focused on hierarchi-
cal phrase-pairs to capture context-driven translation
and reordering patterns with ‘gaps’, offering com-
petitive performance particularly for language pairs
with extensive reordering. As Hiero uses a single
non-terminal and concentrates on overcoming trans-
lation lexicon sparsity, it barely explores the recur-
sive nature of translation past the lexical level. Nev-
ertheless, the successful employment of SCFGs for
phrase-based SMT brought translation models as-
suming latent syntactic structure to the spotlight.
Simultaneously, mounting efforts have been di-
rected towards SMT models employing linguistic
syntax on the source side (Yamada and Knight,
2001; Quirk et al., 2005; Liu et al., 2006), target
side (Galley et al., 2004; Galley et al., 2006) or both
(Zhang et al., 2008; Liu et al., 2009; Chiang, 2010).
Hierarchical translation was combined with target
side linguistic annotation in (Zollmann and Venu-
gopal, 2006). Interestingly, early on (Koehn et al.,
2003) exemplified the difficulties of integrating lin-
guistic information in translation systems. Syntax-
based MT often suffers from inadequate constraints
in the translation rules extracted, or from striving to
combine these rules together towards a full deriva-
tion. Recent research tries to address these issues,
by re-structuring training data parse trees to bet-
ter suit syntax-based SMT training (Wang et al.,
2010), or by moving from linguistically motivated
synchronous grammars to systems where linguistic
plausibility of the translation is assessed through ad-
ditional features in a phrase-based system (Venu-
gopal et al., 2009; Chiang et al., 2009), obscuring
the impact of higher level syntactic processes.
While it is assumed that linguistic structure does
correlate with some translation phenomena, in this
</bodyText>
<note confidence="0.9703975">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 642–652,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999976837209302">
work we do not employ it as the backbone of trans-
lation. In place of linguistically constrained trans-
lation imposing syntactic parse structure, we opt for
linguistically motivated translation. We learn latent
hierarchical structure, taking advantage of linguistic
annotations but shaped and trained for translation.
We start by labelling each phrase-pair span in the
word-aligned training data with multiple linguisti-
cally motivated categories, offering multi-grained
abstractions from its lexical content. These phrase-
pair label charts are the input of our learning al-
gorithm, which extracts the linguistically motivated
rules and estimates the probabilities for a stochastic
SCFG, without arbitrary constraints such as phrase
or span sizes. Estimating such grammars under
a Maximum Likelihood criterion is known to be
plagued by strong overfitting leading to degener-
ate estimates (DeNero et al., 2006). In contrast,
our learning objective not only avoids overfitting
the training data but, most importantly, learns joint
stochastic synchronous grammars which directly
aim at generalisation towards yet unseen instances.
By advancing from structures which mimic lin-
guistic syntax, to learning linguistically aware latent
recursive structures targeting translation, we achieve
significant improvements in translation quality for 4
different language pairs in comparison with a strong
hierarchical translation baseline.
Our key contributions are presented in the fol-
lowing sections. Section 2 discusses the weak in-
dependence assumptions of SCFGs and introduces
a joint translation model which addresses these is-
sues and separates hierarchical translation structure
from phrase-pair emission. In section 3 we consider
a chart over phrase-pair spans filled with source-
language linguistically motivated labels. We show
how we can employ this crucial input to extract and
train a hierarchical translation structure model with
millions of rules. Section 4 demonstrates decoding
with the model by constraining derivations to lin-
guistic hints of the source sentence and presents our
empirical results. We close with a discussion of re-
lated work and our conclusions.
</bodyText>
<sectionHeader confidence="0.981126" genericHeader="method">
2 Joint Translation Model
</sectionHeader>
<bodyText confidence="0.5764405">
Our model is based on a probabilistic Synchronous
CFG (Wu, 1997; Chiang, 2005). SCFGs define a
</bodyText>
<figureCaption confidence="0.963949">
Figure 1: English-German SCFG rules for the relative
</figureCaption>
<equation confidence="0.853151333333333">
clause(s) ‘which is the solution (to the problem) / der die
L¨osung (f¨ur das Problem) ist’, [ ] signify monotone trans-
lation, ( ) a swap reordering.
</equation>
<bodyText confidence="0.999943210526316">
language over string pairs, which are generated be-
ginning from a start symbol 5 and recursively ex-
panding pairs of linked non-terminals across the two
strings using the grammar’s rule set. By crossing the
links between the non-terminals of the two sides re-
ordering phenomena are captured. We employ bi-
nary SCFGs, i.e. grammars with a maximum of two
non-terminals on the right-hand side. Also, for this
work we only used grammars with either purely lexi-
cal or purely abstract rules involving one or two non-
terminal pairs. An example can be seen in Figure 1,
using an ITG-style notation and assuming the same
non-terminal labels for both sides.
We utilise probabilistic SCFGs, where each rule
is assigned a conditional probability of expanding
the left-hand side symbol with the rule’s right-hand
side. Phrase-pairs are emitted jointly and the over-
all probabilistic SCFG is a joint model over parallel
strings.
</bodyText>
<subsectionHeader confidence="0.990369">
2.1 SCFG Reordering Weaknesses
</subsectionHeader>
<bodyText confidence="0.994966333333333">
An interesting feature of all probabilistic SCFGs
(i.e. not only binary ones), which has received sur-
prisingly little attention, is that the reordering pat-
</bodyText>
<figure confidence="0.989092333333333">
SBAR [WHNP SBAR\WHNP] (a)
SBAR\WHNP (VP/NPL NPR) (b)
NPR [NP PP] (c)
WHNP WHNPP (d)
WHNPP which / der (e)
VP/NPL VP/NPL (f)
P
VP/NPLP is / ist (g)
NPR NPR (h)
P
NPRP the solution / die L¨osung (i)
NP NPP (j)
NPP the solution / die L¨osung (k)
PP PPP (l)
PPP to the problem / f¨ur das Problem (m)
</figure>
<page confidence="0.998417">
643
</page>
<bodyText confidence="0.999941107142857">
tern between the non-terminal pairs (or in the case
of ITGs the choice between monotone and swap ex-
pansion) are not conditioned on any other part of a
derivation. The result is that, the reordering pattern
with the highest probability will always be preferred
(e.g. in the Viterbi derivation) over the rest, irre-
spective of lexical or abstract context. As an ex-
ample, a probabilistic SCFG will always assign a
higher probability to derivations swapping or mono-
tonically translating nouns and adjectives between
English and French, only depending on which of the
two rules NP —* [NN JJ], NP —* (NN JJ)
has a higher probability. The rest of the (sometimes
thousands of) rule-specific features usually added to
SCFG translation models do not directly help either,
leaving reordering decisions disconnected from the
rest of the derivation.
While in a decoder this is somehow mitigated by
the use of a language model, we believe that the
weakness of straightforward applications of SCFGs
to model reordering structure at the sentence level
misses a chance to learn this crucial part of the
translation process during grammar induction. As
(Mylonakis and Sima’an, 2010) note, ‘plain’ SCFGs
seem to perform worse than the grammars described
next, mainly due to wrong long-range reordering de-
cisions for which the language model can hardly
help.
</bodyText>
<subsectionHeader confidence="0.999325">
2.2 Hierarchical Reordering SCFG
</subsectionHeader>
<bodyText confidence="0.999921222222222">
We address the weaknesses mentioned above by re-
lying on an SCFG grammar design that is similar to
the ‘Lexicalised Reordering’ grammar of (Mylon-
akis and Sima’an, 2010). As in the rules of Fig-
ure 1, we separate non-terminals according to the
reordering patterns in which they participate. Non-
terminals such as BL, CR take part only in swap-
ping right-hand sides (BL CR) (with BL swap-
ping from the source side’s left to the target side’s
right, CR swapping in the opposite direction), while
non-terminals such as B, C take part solely in mono-
tone right-hand side expansions [B C]. These non-
terminal categories can appear also on the left-hand
side of a rule, as in rule (c) of Figure 1.
In contrast with (Mylonakis and Sima’an, 2010),
monotone and swapping non-terminals do not emit
phrase-pairs themselves. Rather, each non-terminal
NT is expanded to a dedicated phrase-pair emit-
</bodyText>
<equation confidence="0.994438">
A —* [B C] A —* (BL CR)
AL —* [B C] AL —* (BL CR)
AR —* [B C] AR —* (BL CR)
A —* AP AP —* α / 0
AL —* AL AL P —* α / 0
P
AR —* AR AR P —* α / 0
P
</equation>
<figureCaption confidence="0.996891666666667">
Figure 2: Recursive Reordering Grammar rule cate-
gories; A, B, C non-terminals; α, 0 source and target
strings respectively.
</figureCaption>
<bodyText confidence="0.9995152">
ting non-terminal NTP, which generates all phrase-
pairs for it and nothing more. In this way, the pref-
erence of non-terminals to either expand towards
a (long) phrase-pair or be further analysed recur-
sively is explicitly modelled. Furthermore, this set
of pre-terminals allows us to separate the higher or-
der translation structure from the process that emits
phrase-pairs, a feature we employ next.
In (Mylonakis and Sima’an, 2010) this grammar
design mainly contributed to model lexical reorder-
ing preferences. While we retain this function, for
the rich linguistically-motivated grammars used in
this work this design effectively propagates reorder-
ing preferences above and below the current rule ap-
plication (e.g. Figure 1, rules (a)-(c)), allowing to
learn and apply complex reordering patterns.
The different types of grammar rules are sum-
marised in abstract form in Figure 2. We will subse-
quently refer to this grammar structure as Hierarchi-
cal Reordering SCFG (HR-SCFG).
</bodyText>
<subsectionHeader confidence="0.877439">
2.3 Generative Model
</subsectionHeader>
<bodyText confidence="0.999927818181818">
We arrive at a probabilistic SCFG model which
jointly generates source e and target f strings, by
augmenting each grammar rule with a probability,
summing up to one for every left-hand side. The
probability of a derivation D of tuple (e, f) begin-
ning from start symbol 5 is equal to the product of
the probabilities of the rules used to recursively gen-
erate it.
We separate the structural part of the derivation
D, down to the pre-terminals NTP, from the phrase-
emission part. The grammar rules pertaining to the
</bodyText>
<page confidence="0.996837">
644
</page>
<table confidence="0.970134222222222">
X, SBAR, WHNP+VP, WHNP+VBZ+NP
X, VBZ+NP, VP, SBAR\WHNP
X, SBAR/NN, WHNP+VBZ+DT
X, VBZ+DT, VP/NN
X, WHNP+VBZ, X, NP,
SBAR/NP VP\VBZ
X, WHNP, X, VBZ, X, DT, X, NN,
SBAR/VP VP/NP NP/NN NP\DT
which is the problem
</table>
<figureCaption confidence="0.997492">
Figure 3: The label chart for the source fragment ‘which
is the problem’. Only a sample of the entries is listed.
</figureCaption>
<bodyText confidence="0.999640727272727">
structural part and their associated probabilities de-
fine a model p(a) over the latent variable a de-
termining the recursive, reordering and phrase-pair
segmenting structure of translation, as in Figure 4.
Given a, the phrase-pair emission part merely gener-
ates the phrase-pairs utilising distributions from ev-
ery NTP to the phrase-pairs that it covers, thereby
defining a model over all sentence-pairs generated
given each translation structure. The probabilities of
a derivation and of a sentence-pair are then as fol-
lows:
</bodyText>
<equation confidence="0.996761">
p(D) =p(�)p(e, f|u) (1)
p(e,f) = � p(D) (2)
D:D=*&gt;(e,f)
</equation>
<bodyText confidence="0.999789">
By splitting the joint model in a hierarchical struc-
ture model and a lexical emission one we facilitate
estimating the two models separately. The following
section discusses this.
</bodyText>
<sectionHeader confidence="0.936625" genericHeader="method">
3 Learning Translation Structure
</sectionHeader>
<subsectionHeader confidence="0.999396">
3.1 Phrase-Pair Label Chart
</subsectionHeader>
<bodyText confidence="0.994543861111111">
The input to our learning algorithm is a word-
aligned parallel corpus. We consider as phrase-
pair spans those that obey the word-alignment con-
straints of (Koehn et al., 2003). For every train-
ing sentence-pair, we also input a chart containing
one or more labels for every synchronous span, such
as that of Figure 3. Each label describes differ-
ent properties of the phrase pair (syntactic, semantic
etc.), possibly in relation to its context, or supply-
ing varying levels of abstraction (phrase-pair, deter-
miner with noun, noun-phrase, sentence etc.). We
aim to induce a recursive translation structure ex-
plaining the joint generation of the source and target
sentence taking advantage of these phrase-pair span
labels.
For this work we employ the linguistically mo-
tivated labels of (Zollmann and Venugopal, 2006),
albeit for the source language. Given a parse of the
source sentence, each span is assigned the following
kind of labels:
Phrase-Pair All phrase-pairs are assigned the X
label
Constituent Source phrase is a constituent A
Concatenation of Constituents Source phrase la-
belled A+B as a concatenation of constituents A and
B, similarly for 3 constituents.
Partial Constituents Categorial grammar (Bar-
Hillel, 1953) inspired labels A/B, A\B, indicating
a partial constituent A missing constituent B right or
left respectively.
An important point is that we assign all applica-
ble labels to every span. In this way, each label set
captures the features of the source side’s parse-tree
without being bounded by the actual parse structure,
as well as provides a coarse to fine-grained view of
the source phrase.
</bodyText>
<subsectionHeader confidence="0.999488">
3.2 Grammar Extraction
</subsectionHeader>
<bodyText confidence="0.99997605">
From every word-aligned sentence-pair and its la-
bel chart, we extract SCFG rules as those of Figure
2. Binary rules are extracted from adjoining syn-
chronous spans up to the whole sentence-pair level,
with the non-terminals of both left and right-hand
side derived from the label names plus their reorder-
ing function (monotone, left/right swapping) in the
span examined. A single unary rule per non-terminal
NT generates the phrase-pair emitting NTP. Unary
rules NTP —* a / Q generating the phrase-pair are
created for all the labels covering it.
While we label the phrase-pairs similarly to (Zoll-
mann and Venugopal, 2006), the extracted grammar
is rather different. We do not employ rules that are
grounded to lexical context (‘gap’ rules), relying in-
stead on the reordering-aware non-terminal set and
related unary and binary rules. The result is a gram-
mar which can both capture a rich array of trans-
lation phenomena based on linguistic and lexical
grounds and explicitly model the balance between
</bodyText>
<page confidence="0.989615">
645
</page>
<figure confidence="0.97675775">
SBAR
PPP
to the problem
f¨ur das Problem
</figure>
<figureCaption confidence="0.994872">
Figure 4: A derivation of a sentence fragment with the
grammar of Figure 1.
</figureCaption>
<bodyText confidence="0.999833666666667">
memorising long phrase-pairs and generalising over
yet unseen ones, as shown in the next example.
The derivation in Figure 4 illustrates some of the
formalism’s features. A preference to reorder based
on lexical content is applied for is / ist. Noun phrase
NPR is recursively constructed with a preference to
constitute the right branch of an order swapping non-
terminal expansion. This is matched with VP/NPL
which reorders in the opposite direction. The labels
VP/NP and SBAR\WHNP allow linguistic syntax
context to influence the lexical and reordering trans-
lation choices. Crucially, all these lexical, attach-
ment and reordering preferences (as encoded in the
model’s rules and probabilities) must be matched to-
gether to arrive at the analysis in Figure 4.
</bodyText>
<subsectionHeader confidence="0.995721">
3.3 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.99997675">
We estimate the parameters for the phrase-emission
model p(e, f|a) using Relative Frequency Estima-
tion (RFE) on the label charts induced for the train-
ing sentence-pairs, after the labels have been aug-
mented by the reordering indications. In the RFE
estimate, every rule NTP —* a / Q receives a prob-
ability in proportion with the times that a / Q was
covered by the NT label.
On the other hand, estimating the parameters un-
der Maximum-Likelihood Estimation (MLE) for the
latent translation structure model p(a) is bound to
overfit towards memorising whole sentence-pairs as
discussed in (Mylonakis and Sima’an, 2010), with
the resulting grammar estimate not being able to
generalise past the training data. However, apart
from overfitting towards long phrase-pairs, a gram-
mar with millions of structural rules is also liable to
overfit towards degenerate latent structures which,
while fitting the training data well, have limited ap-
plicability to unseen sentences.
We avoid both pitfalls by estimating the grammar
probabilities with the Cross-Validating Expectation-
Maximization algorithm (CV-EM) (Mylonakis and
Sima’an, 2008; Mylonakis and Sima’an, 2010). CV-
EM is a cross-validating instance of the well known
EM algorithm (Dempster et al., 1977). It works it-
eratively on a partition of the training data, climb-
ing the likelihood of the training data while cross-
validating the latent variable values, considering for
every training data point only those which can be
produced by models built from the rest of the data
excluding the current part. As a result, the estima-
tion process simulates maximising future data likeli-
hood, using the training data to directly aim towards
strong generalisation of the estimate.
For our probabilistic SCFG-based translation
structure variable a, implementing CV-EM boils
down to a synchronous version of the Inside-Outside
algorithm, modified to enforce the CV criterion. In
this way we arrive at cross-validated ML estimate of
the a parameters while keeping the phrase-emission
parameters of p(e, f|a) fixed. The CV-criterion,
apart from avoiding overfitting, results in discarding
the structural rules which are only found in a single
part of the training corpus, leading to a more com-
pact grammar while still retaining millions of struc-
tural rules that are more hopeful to generalise.
Unravelling the joint generative process, by mod-
elling latent hierarchical structure separately from
phrase-pair emission, allows us to concentrate our
inference efforts towards the hidden, higher-level
translation mechanism.
</bodyText>
<sectionHeader confidence="0.999827" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.84264">
4.1 Decoding Model
</subsectionHeader>
<bodyText confidence="0.999876166666667">
The induced joint translation model can be used
to recover arg maxe p(e|f), as it is equal to
arg maxe p(e, f). We employ the induced proba-
bilistic HR-SCFG G as the backbone of a log-linear,
feature based translation model, with the derivation
probability p(D) under the grammar estimate being
</bodyText>
<figure confidence="0.994434866666667">
NP
PP
WHNP
WHNPP
which
der
VP/NPL
VP/NPL P
is
ist
&lt; SBAR\WHNP &gt;
NPR
NPP
the solution
die L¨osung
</figure>
<page confidence="0.994317">
646
</page>
<bodyText confidence="0.999598217391304">
one of the features. This is augmented with a small
number n of additional smoothing features Oi for
derivation rules r: (a) conditional phrase translation
probabilities, (b) lexical phrase translation probabil-
ities, (c) word generation penalty, and (d) a count
of swapping reordering operations. Features (a), (b)
and (c) are applicable to phrase-pair emission rules
and features for both translation directions are used,
while (d) is only triggered by structural rules.
These extra features assess translation quality past
the synchronous grammar derivation and learning
general reordering or word emission preferences
for the language pair. As an example, while our
probabilistic HR-SCFG maintains a separate joint
phrase-pair emission distribution per non-terminal,
the smoothing features (a) above assess the condi-
tional translation of surface phrases irrespective of
any notion of recursive translation structure.
The final feature is the language model score
for the target sentence, mounting up to the follow-
ing model used at decoding time, with the feature
weights A trained by Minimum Error Rate Training
(MERT) (Och, 2003) on a development corpus.
</bodyText>
<equation confidence="0.736269">
p(D � � (e, f)) a p(e)λ&amp;quot; pG(D)λc
</equation>
<subsectionHeader confidence="0.972488">
4.2 Decoding Modifications
</subsectionHeader>
<bodyText confidence="0.999919">
We use a customised version of the Joshua SCFG
decoder (Li et al., 2009) to translate, with the fol-
lowing modifications:
Source Labels Constraints As for this work the
phrase-pair labels used to extract the grammar are
based on the linguistic analysis of the source side,
we can construct the label chart for every input sen-
tence from its parse. We subsequently use it to con-
sider only derivations with synchronous spans which
are covered by non-terminals matching one of the
labels for those spans. This applies both for the non-
terminals covering phrase-pairs as well as the higher
level parts of the derivation.
In this manner we not only constrain the trans-
lation hypotheses resulting in faster decoding time,
but, more importantly, we may ground the hypothe-
ses more closely to the available linguistic informa-
tion of the source sentence. This is of particular
interest as we move up the derivation tree, where
an initial wrong choice below could propagate to-
wards hypotheses wildly diverging from the input
sentence’s linguistic annotation.
Per Non-Terminal Pruning The decoder uses a
combination of beam and cube-pruning (Huang and
Chiang, 2007). As our grammar uses non-terminals
in the hundreds of thousands, it is important not
to prune away prematurely non-terminals covering
smaller spans and to leave more options to be con-
sidered as we move up the derivation tree.
For this, for every cell in the decoder’s chart, we
keep a separate bin per non-terminal and prune to-
gether hypotheses leading to the same non-terminal
covering a cell. This allows full derivations to be
found for all input sentences, as well as avoids ag-
gressive pruning at an early stage. Given the source
label constraint discussed above, this does not in-
crease running times or memory demands consid-
erably as we allow only up to a few tens of non-
terminals per span.
Expected Counts Rule Pruning To compact the
hierarchical structure part of the grammar prior to
decoding, we prune rules that fail to accumulate
10−8 expected counts during the last CV-EM iter-
ation. For English to German, this brings the struc-
tural rules from 15M down to 1.2M. Note that we
do not prune the phrase-pair emitting rules. Over-
all, we consider this a much more informed pruning
criterion than those based on probability values (that
are not comparable across left-hand sides) or right-
hand side counts (frequent symbols need many more
expansions than a highly specialised one).
</bodyText>
<subsectionHeader confidence="0.995021">
4.3 Experimental Setting &amp; Baseline
</subsectionHeader>
<bodyText confidence="0.999987461538462">
We evaluate our method on four different lan-
guage pairs with English as the source language
and French, German, Dutch and Chinese as tar-
get. The data for the first three language pairs are
derived from parliament proceedings sourced from
the Europarl corpus (Koehn, 2005), with WMT-
07 development and test data for French and Ger-
man. The data for the English to Chinese task is
composed of parliament proceedings and news arti-
cles. For all language pairs we employ 200K and
400K sentence pairs for training, 2K for develop-
ment and 2K for testing (single reference per source
sentence). Both the baseline and our method decode
</bodyText>
<equation confidence="0.933652">
n
i=1
Oi(r)λi
H
rED
</equation>
<page confidence="0.995645">
647
</page>
<table confidence="0.993764571428572">
Training English to French German Dutch Chinese
set size
BLEU NIST BLEU NIST BLEU NIST BLEU NIST
200K josh-base 29.20 7.2123 18.65 5.8047 21.97 6.2469 22.34 6.5540
lts 29.43 7.2611** 19.10** 5.8714** 22.31* 6.2903* 23.67** 6.6595**
400K josh-base 29.58 7.3033 18.86 5.8818 22.25 6.2949 23.24 6.7402
lts 29.83 7.4000** 19.49** 5.9374** 22.92** 6.3727** 25.16** 6.9005**
</table>
<tableCaption confidence="0.976997">
Table 1: Experimental results for training sets of 200K and 400K sentence pairs. Statistically significant score im-
provements from the baseline at the 95% confidence level are labelled with a single star, at the 99% level with two.
</tableCaption>
<bodyText confidence="0.999989095238096">
with a 3-gram language model smoothed with modi-
fied Knesser-Ney discounting (Chen and Goodman,
1998), trained on around 1M sentences per target
language. The parses of the source sentences em-
ployed by our system during training and decod-
ing are created with the Charniak parser (Charniak,
2000).
We compare against a state-of-the-art hierarchi-
cal translation (Chiang, 2005) baseline, based on the
Joshua translation system under the default training
and decoding settings (josh-base). Apart of eval-
uating against a state-of-the-art system, especially
on the English-Chinese language pair, the compar-
ison has an added interesting aspect. The heuristi-
cally trained baseline takes advantage of ‘gap rules’
to reorder based on lexical context cues, but makes
very limited use of the hierarchical structure above
the lexical surface. In contrast, our method induces
a grammar with no such rules, relying on lexical
content and the strength of a higher level translation
structure instead.
</bodyText>
<subsectionHeader confidence="0.993335">
4.4 Training &amp; Decoding Details
</subsectionHeader>
<bodyText confidence="0.999873">
To train our Latent Translation Structure (LTS) sys-
tem, we used the following settings. CV-EM cross-
validated on a 10-part partition of the training data
and performed 10 iterations. The structural rule
probabilities were initialised to uniform per left-
hand side.
The decoder does not employ any ‘glue grammar’
as is usual with hierarchical translation systems to
limit reordering up to a certain cut-off length. In-
stead, we rely on our LTS grammar to reorder and
construct the translation output up to the full sen-
tence length.
In summary, our system’s experimental pipeline is
as follows. All input sentences are parsed and label
charts are created from these parses. The Hierarchi-
cal Reordering SCFG is extracted and its parame-
ters are estimated employing CV-EM. The structural
rules of the estimate are pruned according to their
expected counts and smoothing features are added to
all rules. We train the feature weights under MERT
and decode with the resulting log-linear model.
The overall training and decoding setup is appeal-
ing also regarding computational demands. On an
8-core 2.3GHz system, training on 200K sentence-
pairs demands 4.5 hours while decoding runs on 25
sentences per minute.
</bodyText>
<subsectionHeader confidence="0.638536">
4.5 Results
</subsectionHeader>
<bodyText confidence="0.99996096">
Table 1 presents the results for the baseline and our
method for the 4 language pairs, for training sets of
both 200K and 400K sentence pairs. Our system
(lts) outperforms the baseline for all 4 language
pairs for both BLEU and NIST scores, by a margin
which scales up to +1.92 BLEU points for English to
Chinese translation when training on the 400K set.
In addition, increasing the size of the training data
from 200K to 400K sentence pairs widens the per-
formance margin between the baseline and our sys-
tem, in some cases considerably. All but one of the
performance improvements are found to be statis-
tically significant (Koehn, 2004) at the 95% confi-
dence level, most of them also at the 99% level.
We selected an array of target languages of
increasing reordering complexity with English as
source. Examining the results across the target lan-
guages, LTS performance gains increase the more
challenging the sentence structure of the target lan-
guage is in relation to the source’s, highlighted when
translating to Chinese. Even for Dutch and German,
which pose additional challenges such as compound
words and morphology which we do not explicitly
treat in the current system, LTS still delivers signif-
icant improvements in performance. Additionally,
</bodyText>
<page confidence="0.995354">
648
</page>
<table confidence="0.975769">
System 200K 400K
lts-nolabels 22.50 24.24
lts 23.67** 25.16**
josh-base-lm4 23.81 24.77
lts-lm4 24.48** 26.35**
</table>
<tableCaption confidence="0.996302571428571">
Table 2: Additional experiments for English to Chi-
nese translation examining (a) the impact of the linguis-
tic annotations in the LTS system (lts), when com-
pared with an instance not employing such annotations
(lts-nolabels) and (b) decoding with a 4th-order
language model (-lm4). BLEU scores for 200K and
400K training sentence pairs.
</tableCaption>
<bodyText confidence="0.999856034482759">
the robustness of our system is exemplified by deliv-
ering significant performance increases for all lan-
guage pairs.
For the English to Chinese translation task, we
performed further experiments along two axes. We
first investigate the contribution of the linguistic
annotations, by comparing our complete system
(lts) with an otherwise identical implementation
(lts-nolabels) which does not employ any lin-
guistically motivated labels. The latter system then
uses a labels chart as that of Figure 3, which however
labels all phrase-pair spans solely with the generic
X label. The results in Table 2(a) indicate that a
large part of the performance improvement can be
attributed to the use of the linguistic annotations ex-
tracted from the source parse trees, indicating the
potential of the LTS system to take advantage of
such additional annotations to deliver better trans-
lations.
The second additional experiment relates to the
impact of employing a stronger language model dur-
ing decoding, which may increase performance but
slows down decoding speed. Notably, as can be seen
in Table 2(b), switching to a 4-gram LM results in
performance gains for both the baseline and our sys-
tem and while the margin between the two systems
decreases, our system continues to deliver a con-
siderable and significant improvement in translation
BLEU scores.
</bodyText>
<sectionHeader confidence="0.999946" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999904568627451">
In this work, we focus on the combination of
learning latent structure with syntax and linguistic
annotations, exploring the crossroads of machine
learning, linguistic syntax and machine translation.
Training a joint probability model was first dis-
cussed in (Marcu and Wong, 2002). We show that
a translation system based on such a joint model
can perform competitively in comparison with con-
ditional probability models, when it is augmented
with a rich latent hierarchical structure trained ade-
quately to avoid overfitting.
Earlier approaches for linguistic syntax-based
translation such as (Yamada and Knight, 2001; Gal-
ley et al., 2006; Huang et al., 2006; Liu et al., 2006)
focus on memorising and reusing parts of the struc-
ture of the source and/or target parse trees and con-
straining decoding by the input parse tree. In con-
trast to this approach, we choose to employ lin-
guistic annotations in the form of unambiguous syn-
chronous span labels, while discovering ambiguous
translation structure taking advantage of them.
Later work (Marton and Resnik, 2008; Venugopal
et al., 2009; Chiang et al., 2009) takes a more flex-
ible approach, influencing translation output using
linguistically motivated features, or features based
on source-side linguistically-guided latent syntactic
categories (Huang et al., 2010). A feature-based ap-
proach and ours are not mutually exclusive, as we
also employ a limited set of features next to our
trained model during decoding. We find augment-
ing our system with a more extensive feature set an
interesting research direction for the future.
An array of recent work (Chiang, 2010; Zhang et
al., 2008; Liu et al., 2009) sets off to utilise source
and target syntax for translation. While for this work
we constrain ourselves to source language syntax
annotations, our method can be directly applied to
employ labels taking advantage of linguistic annota-
tions from both sides of translation. The decoding
constraints of section 4.2 can then still be applied on
the source part of hybrid source-target labels.
For the experiments in this paper we employ a la-
bel set similar to the non-terminals set of (Zollmann
and Venugopal, 2006). However, the synchronous
grammars we learn share few similarities with those
that they heuristically extract. The HR-SCFG we
adopt allows capturing more complex reordering
phenomena and, in contrast to both (Chiang, 2005;
Zollmann and Venugopal, 2006), is not exposed to
the issues highlighted in section 2.1. Nevertheless,
our results underline the capacity of linguistic anno-
</bodyText>
<page confidence="0.980617">
649
</page>
<bodyText confidence="0.99500885106383">
tations similar to those of (Zollmann and Venugopal, Instead of employing hierarchical phrase-pairs, we
2006) as part of latent translation variables. invest in learning the higher-order hierarchical syn-
Most of the aforementioned work does concen- chronous structure behind translation, up to the full
trate on learning hierarchical, linguistically moti- sentence length. While these choices and the related
vated translation models. Cohn and Blunsom (2009) results challenge current MT research trends, they
sample rules of the form proposed in (Galley et al., are not mutually exclusive with them. Future work
2004) from a Bayesian model, employing Dirich- directions include investigating the impact of hierar-
let Process priors favouring smaller rules to avoid chical phrases for our models as well as any gains
overfitting. Their grammar is however also based from additional features in the log-linear decoding
on the target parse-tree structure, with their system model.
surpassing a weak baseline by a small margin. In Smoothing the HR-SCFG grammar estimates
contrast to the Bayesian approach which imposes could prove a possible source of further perfor-
external priors to lead estimation away from degen- mance improvements. Learning translation and re-
erate solutions, we take a data-driven approach to ordering behaviour with respect to linguistic cues
arrive to estimates which generalise well. The rich is facilitated in our approach by keeping separate
linguistically motivated latent variable learnt by our phrase-pair emission distributions per emitting non-
method delivers translation performance that com- terminal and reordering pattern, while the employ-
pares favourably to a state-of-the-art system. ment of the generic X non-terminals already allows
Mylonakis and Sima’an (2010) also employ the backing off to more coarse-grained rules. Neverthe-
CV-EM algorithm to estimate the parameters of an less, we still believe that further smoothing of these
SCFG, albeit a much simpler one based on a hand- sparse distributions, e.g. by interpolating them with
ful of non-terminals. In this work we employ some less sparse ones, could in the future lead to an addi-
of their grammar design principles for an immensely tional increase in translation quality.
more complex grammar with millions of hierarchi- Finally, we discuss in this work how our method
cal latent structure rules and show how such gram- can already utilise hundreds of thousands of phrase-
mar can be learnt and applied taking advantage of pair labels and millions of structural rules. A fur-
source language linguistic annotations. ther promising direction is broadening this set with
6 Conclusions labels taking advantage of both source and target-
In this work we contribute a method to learn and language linguistic annotation or categories explor-
apply latent hierarchical translation structure. To ing additional phrase-pair properties past the parse
this end, we take advantage of source-language lin- trees such as semantic annotations.
guistic annotations to motivate instead of constrain Acknowledgments
the translation process. An input chart over phrase- Both authors are supported by a VIDI grant (nr.
pair spans, with each cell filled with multiple lin- 639.022.604) from The Netherlands Organization
guistically motivated labels, is coupled with the HR- for Scientific Research (NWO). The authors would
SCFG design to arrive at a rich synchronous gram- like to thank Maxim Khalilov for helping with
mar with millions of structural rules and the capacity experimental data and Andreas Zollmann and the
to capture complex linguistically conditioned trans- anonymous reviewers for their valuable comments.
lation phenomena. We address overfitting issues by References
cross-validating climbing the likelihood of the train- Yehoshua Bar-Hillel. 1953. A quasi-arithmetical nota-
ing data and propose solutions to increase the effi- tion for syntactic description. Language, 29(1):47–58.
ciency and accuracy of decoding. Eugene Charniak. 2000. A maximum-entropy-inspired
An interesting aspect of our work is delivering parser. In Proceedings of the North American Asso-
competitive performance for difficult language pairs ciation for Computational Linguistics (HLT/NAACL),
such as English-Chinese with a joint probability Seattle, Washington, USA, April.
generative model and an SCFG without ‘gap rules’.
650
</bodyText>
<reference confidence="0.99592512264151">
Stanley Chen and Joshua Goodman. 1998. An empirical
study of smoothing techniques for language modeling.
Technical Report TR-10-98, Harvard University, Au-
gust.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In Proceedings of Human Language Technolo-
gies: The 2009Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 218–226, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL 2005, pages 263–270.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1443–1452, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Trevor Cohn and Phil Blunsom. 2009. A Bayesian model
of syntax-directed tree to string grammar induction.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
352–361, Singapore, August. Association for Compu-
tational Linguistics.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Max-
imum likelihood from incomplete data via the em al-
gorithm. Journal of the Royal Statistical Society, Se-
ries B, 39(1):1–38.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
surface heuristics. In Proceedings on the Workshop
on Statistical Machine Translation, pages 31–38, New
York City. Association for Computational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In
Daniel Marcu Susan Dumais and Salim Roukos, ed-
itors, HLT-NAACL 2004: Main Proceedings, pages
273–280, Boston, Massachusetts, USA, May. Associ-
ation for Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 961–
968, Sydney, Australia, July. Association for Compu-
tational Linguistics.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 144–151,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the 7th Biennial
Conference of the Association for Machine Translation
in the Americas (AMTA), Boston, MA, USA.
Zhongqiang Huang, Martin Cmejrek, and Bowen Zhou.
2010. Soft syntactic constraints for hierarchical
phrase-based translation using latent syntactic distri-
butions. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 138–147, Cambridge, MA, October. Associa-
tion for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In HLT-
NAACL 2003.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388–395, Barcelona, Spain, July. Association
for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In MT Summit 2005.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Sanjeev
Khudanpur, Lane Schwartz, Wren Thornton, Jonathan
Weese, and Omar Zaidan. 2009. Joshua: An open
source toolkit for parsing-based machine translation.
In Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 135–139, Athens, Greece,
March. Association for Computational Linguistics.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 609–616, Sydney, Australia, July. Associa-
tion for Computational Linguistics.
Yang Liu, Yajuan L¨u, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 558–566, Suntec, Singapore, August.
Association for Computational Linguistics.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings of Empirical methods in natural
language processing, pages 133–139. Association for
Computational Linguistics.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proceedings of ACL-08: HLT, pages 1003–1011,
</reference>
<page confidence="0.984598">
651
</page>
<reference confidence="0.991673218181819">
Translation, pages 138–141, New York City, June. As-
sociation for Computational Linguistics.
Columbus, Ohio, June. Association for Computational
Linguistics.
Markos Mylonakis and Khalil Sima’an. 2008. Phrase
translation probabilities with ITG priors and smooth-
ing as learning objective. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 630–639, Honolulu, USA,
October.
Markos Mylonakis and Khalil Sima’an. 2010. Learn-
ing probabilistic synchronous CFGs for phrase-based
translation. In Fourteenth Conference on Computa-
tional Natural Language Learning, Uppsala, Sweden,
July.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 160–167, Sapporo, Japan,
July. Association for Computational Linguistics.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In Proceedings of 43rd Annual Meeting
of the Association for Computational Linguistics, Ann
Arbor, Michigan, USA, June.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference grammars: Soft-
ening syntactic constraints to improve statistical ma-
chine translation. In Proceedings of Human Language
Technologies: The 2009 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 236–244, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
Wei Wang, Jonathan May, Kevin Knight, and Daniel
Marcu. 2010. Re-structuring, re-labeling, and re-
aligning for syntax-based machine translation. Com-
putational Linguistics, 36(2):247–277.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of 39th
Annual Meeting of the Association for Computational
Linguistics, pages 523–530, Toulouse, France, July.
Association for Computational Linguistics.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree sequence
alignment-based tree-to-tree translation model. In
Proceedings ofACL-08: HLT, pages 559–567, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings on the Workshop on Statistical Machine
</reference>
<page confidence="0.998113">
652
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.795212">
<title confidence="0.999993">Learning Hierarchical Translation Structure with Linguistic Annotations</title>
<author confidence="0.999917">Markos Mylonakis Khalil Sima’an</author>
<affiliation confidence="0.999228">ILLC ILLC University of Amsterdam University of Amsterdam</affiliation>
<email confidence="0.965264">m.mylonakis@uva.nlk.simaan@uva.nl</email>
<abstract confidence="0.98822196">While it is generally accepted that many translation phenomena are correlated with linguistic structures, employing linguistic syntax for translation has proven a highly non-trivial task. The key assumption behind many approaches is that translation is guided by the source and/or target language parse, employing rules extracted from the parse tree or performing tree transformations. These approaches enforce strict constraints and might overlook important translation phenomena that cross linguistic constituents. We propose a novel flexible modelling approach to introduce linguistic information of varying granularity from the source side. Our method induces joint probability synchronous grammars and estimates their parameters, by selecting and weighing together linguistically motivated rules according to an objective function directly targeting generalisation over future data. We obtain statistically significant improvements across 4 different language pairs with English as source, mounting up to +1.92 BLEU for Chinese as target.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stanley Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Harvard University,</institution>
<contexts>
<context position="24380" citStr="Chen and Goodman, 1998" startWordPosition="3859" endWordPosition="3862">LEU NIST BLEU NIST 200K josh-base 29.20 7.2123 18.65 5.8047 21.97 6.2469 22.34 6.5540 lts 29.43 7.2611** 19.10** 5.8714** 22.31* 6.2903* 23.67** 6.6595** 400K josh-base 29.58 7.3033 18.86 5.8818 22.25 6.2949 23.24 6.7402 lts 29.83 7.4000** 19.49** 5.9374** 22.92** 6.3727** 25.16** 6.9005** Table 1: Experimental results for training sets of 200K and 400K sentence pairs. Statistically significant score improvements from the baseline at the 95% confidence level are labelled with a single star, at the 99% level with two. with a 3-gram language model smoothed with modified Knesser-Ney discounting (Chen and Goodman, 1998), trained on around 1M sentences per target language. The parses of the source sentences employed by our system during training and decoding are created with the Charniak parser (Charniak, 2000). We compare against a state-of-the-art hierarchical translation (Chiang, 2005) baseline, based on the Joshua translation system under the default training and decoding settings (josh-base). Apart of evaluating against a state-of-the-art system, especially on the English-Chinese language pair, the comparison has an added interesting aspect. The heuristically trained baseline takes advantage of ‘gap rule</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Harvard University, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>218--226</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="3416" citStr="Chiang et al., 2009" startWordPosition="484" endWordPosition="487">s of integrating linguistic information in translation systems. Syntaxbased MT often suffers from inadequate constraints in the translation rules extracted, or from striving to combine these rules together towards a full derivation. Recent research tries to address these issues, by re-structuring training data parse trees to better suit syntax-based SMT training (Wang et al., 2010), or by moving from linguistically motivated synchronous grammars to systems where linguistic plausibility of the translation is assessed through additional features in a phrase-based system (Venugopal et al., 2009; Chiang et al., 2009), obscuring the impact of higher level syntactic processes. While it is assumed that linguistic structure does correlate with some translation phenomena, in this Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 642–652, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics work we do not employ it as the backbone of translation. In place of linguistically constrained translation imposing syntactic parse structure, we opt for linguistically motivated translation. We learn latent hierarchical structure, taking advantage</context>
<context position="30676" citStr="Chiang et al., 2009" startWordPosition="4861" endWordPosition="4864">rained adequately to avoid overfitting. Earlier approaches for linguistic syntax-based translation such as (Yamada and Knight, 2001; Galley et al., 2006; Huang et al., 2006; Liu et al., 2006) focus on memorising and reusing parts of the structure of the source and/or target parse trees and constraining decoding by the input parse tree. In contrast to this approach, we choose to employ linguistic annotations in the form of unambiguous synchronous span labels, while discovering ambiguous translation structure taking advantage of them. Later work (Marton and Resnik, 2008; Venugopal et al., 2009; Chiang et al., 2009) takes a more flexible approach, influencing translation output using linguistically motivated features, or features based on source-side linguistically-guided latent syntactic categories (Huang et al., 2010). A feature-based approach and ours are not mutually exclusive, as we also employ a limited set of features next to our trained model during decoding. We find augmenting our system with a more extensive feature set an interesting research direction for the future. An array of recent work (Chiang, 2010; Zhang et al., 2008; Liu et al., 2009) sets off to utilise source and target syntax for t</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 new features for statistical machine translation. In Proceedings of Human Language Technologies: The 2009Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 218–226, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>263--270</pages>
<contexts>
<context position="1746" citStr="Chiang, 2005" startWordPosition="239" endWordPosition="240">rovements across 4 different language pairs with English as source, mounting up to +1.92 BLEU for Chinese as target. 1 Introduction Recent advances in Statistical Machine Translation (SMT) are widely centred around two concepts: (a) hierarchical translation processes, frequently employing Synchronous Context Free Grammars (SCFGs) and (b) transduction or synchronous rewrite processes over a linguistic syntactic tree. SCFGs in the form of the Inversion-Transduction Grammar (ITG) were first introduced by (Wu, 1997) as a formalism to recursively describe the translation process. The Hiero system (Chiang, 2005) 642 utilised an ITG-flavour which focused on hierarchical phrase-pairs to capture context-driven translation and reordering patterns with ‘gaps’, offering competitive performance particularly for language pairs with extensive reordering. As Hiero uses a single non-terminal and concentrates on overcoming translation lexicon sparsity, it barely explores the recursive nature of translation past the lexical level. Nevertheless, the successful employment of SCFGs for phrase-based SMT brought translation models assuming latent syntactic structure to the spotlight. Simultaneously, mounting efforts h</context>
<context position="6029" citStr="Chiang, 2005" startWordPosition="861" endWordPosition="862">tion structure from phrase-pair emission. In section 3 we consider a chart over phrase-pair spans filled with sourcelanguage linguistically motivated labels. We show how we can employ this crucial input to extract and train a hierarchical translation structure model with millions of rules. Section 4 demonstrates decoding with the model by constraining derivations to linguistic hints of the source sentence and presents our empirical results. We close with a discussion of related work and our conclusions. 2 Joint Translation Model Our model is based on a probabilistic Synchronous CFG (Wu, 1997; Chiang, 2005). SCFGs define a Figure 1: English-German SCFG rules for the relative clause(s) ‘which is the solution (to the problem) / der die L¨osung (f¨ur das Problem) ist’, [ ] signify monotone translation, ( ) a swap reordering. language over string pairs, which are generated beginning from a start symbol 5 and recursively expanding pairs of linked non-terminals across the two strings using the grammar’s rule set. By crossing the links between the non-terminals of the two sides reordering phenomena are captured. We employ binary SCFGs, i.e. grammars with a maximum of two non-terminals on the right-hand</context>
<context position="24653" citStr="Chiang, 2005" startWordPosition="3903" endWordPosition="3904">.9005** Table 1: Experimental results for training sets of 200K and 400K sentence pairs. Statistically significant score improvements from the baseline at the 95% confidence level are labelled with a single star, at the 99% level with two. with a 3-gram language model smoothed with modified Knesser-Ney discounting (Chen and Goodman, 1998), trained on around 1M sentences per target language. The parses of the source sentences employed by our system during training and decoding are created with the Charniak parser (Charniak, 2000). We compare against a state-of-the-art hierarchical translation (Chiang, 2005) baseline, based on the Joshua translation system under the default training and decoding settings (josh-base). Apart of evaluating against a state-of-the-art system, especially on the English-Chinese language pair, the comparison has an added interesting aspect. The heuristically trained baseline takes advantage of ‘gap rules’ to reorder based on lexical context cues, but makes very limited use of the hierarchical structure above the lexical surface. In contrast, our method induces a grammar with no such rules, relying on lexical content and the strength of a higher level translation structur</context>
<context position="31958" citStr="Chiang, 2005" startWordPosition="5064" endWordPosition="5065">age syntax annotations, our method can be directly applied to employ labels taking advantage of linguistic annotations from both sides of translation. The decoding constraints of section 4.2 can then still be applied on the source part of hybrid source-target labels. For the experiments in this paper we employ a label set similar to the non-terminals set of (Zollmann and Venugopal, 2006). However, the synchronous grammars we learn share few similarities with those that they heuristically extract. The HR-SCFG we adopt allows capturing more complex reordering phenomena and, in contrast to both (Chiang, 2005; Zollmann and Venugopal, 2006), is not exposed to the issues highlighted in section 2.1. Nevertheless, our results underline the capacity of linguistic anno649 tations similar to those of (Zollmann and Venugopal, Instead of employing hierarchical phrase-pairs, we 2006) as part of latent translation variables. invest in learning the higher-order hierarchical synMost of the aforementioned work does concen- chronous structure behind translation, up to the full trate on learning hierarchical, linguistically moti- sentence length. While these choices and the related vated translation models. Cohn </context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of ACL 2005, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Learning to translate with source and target syntax.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1443--1452</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="2610" citStr="Chiang, 2010" startWordPosition="368" endWordPosition="369">s a single non-terminal and concentrates on overcoming translation lexicon sparsity, it barely explores the recursive nature of translation past the lexical level. Nevertheless, the successful employment of SCFGs for phrase-based SMT brought translation models assuming latent syntactic structure to the spotlight. Simultaneously, mounting efforts have been directed towards SMT models employing linguistic syntax on the source side (Yamada and Knight, 2001; Quirk et al., 2005; Liu et al., 2006), target side (Galley et al., 2004; Galley et al., 2006) or both (Zhang et al., 2008; Liu et al., 2009; Chiang, 2010). Hierarchical translation was combined with target side linguistic annotation in (Zollmann and Venugopal, 2006). Interestingly, early on (Koehn et al., 2003) exemplified the difficulties of integrating linguistic information in translation systems. Syntaxbased MT often suffers from inadequate constraints in the translation rules extracted, or from striving to combine these rules together towards a full derivation. Recent research tries to address these issues, by re-structuring training data parse trees to better suit syntax-based SMT training (Wang et al., 2010), or by moving from linguistic</context>
<context position="31186" citStr="Chiang, 2010" startWordPosition="4941" endWordPosition="4942">g advantage of them. Later work (Marton and Resnik, 2008; Venugopal et al., 2009; Chiang et al., 2009) takes a more flexible approach, influencing translation output using linguistically motivated features, or features based on source-side linguistically-guided latent syntactic categories (Huang et al., 2010). A feature-based approach and ours are not mutually exclusive, as we also employ a limited set of features next to our trained model during decoding. We find augmenting our system with a more extensive feature set an interesting research direction for the future. An array of recent work (Chiang, 2010; Zhang et al., 2008; Liu et al., 2009) sets off to utilise source and target syntax for translation. While for this work we constrain ourselves to source language syntax annotations, our method can be directly applied to employ labels taking advantage of linguistic annotations from both sides of translation. The decoding constraints of section 4.2 can then still be applied on the source part of hybrid source-target labels. For the experiments in this paper we employ a label set similar to the non-terminals set of (Zollmann and Venugopal, 2006). However, the synchronous grammars we learn share</context>
</contexts>
<marker>Chiang, 2010</marker>
<rawString>David Chiang. 2010. Learning to translate with source and target syntax. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1443–1452, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Phil Blunsom</author>
</authors>
<title>A Bayesian model of syntax-directed tree to string grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>352--361</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="32576" citStr="Cohn and Blunsom (2009)" startWordPosition="5150" endWordPosition="5153"> 2005; Zollmann and Venugopal, 2006), is not exposed to the issues highlighted in section 2.1. Nevertheless, our results underline the capacity of linguistic anno649 tations similar to those of (Zollmann and Venugopal, Instead of employing hierarchical phrase-pairs, we 2006) as part of latent translation variables. invest in learning the higher-order hierarchical synMost of the aforementioned work does concen- chronous structure behind translation, up to the full trate on learning hierarchical, linguistically moti- sentence length. While these choices and the related vated translation models. Cohn and Blunsom (2009) results challenge current MT research trends, they sample rules of the form proposed in (Galley et al., are not mutually exclusive with them. Future work 2004) from a Bayesian model, employing Dirich- directions include investigating the impact of hierarlet Process priors favouring smaller rules to avoid chical phrases for our models as well as any gains overfitting. Their grammar is however also based from additional features in the log-linear decoding on the target parse-tree structure, with their system model. surpassing a weak baseline by a small margin. In Smoothing the HR-SCFG grammar e</context>
</contexts>
<marker>Cohn, Blunsom, 2009</marker>
<rawString>Trevor Cohn and Phil Blunsom. 2009. A Bayesian model of syntax-directed tree to string grammar induction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 352–361, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the em algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="17607" citStr="Dempster et al., 1977" startWordPosition="2769" endWordPosition="2772">e resulting grammar estimate not being able to generalise past the training data. However, apart from overfitting towards long phrase-pairs, a grammar with millions of structural rules is also liable to overfit towards degenerate latent structures which, while fitting the training data well, have limited applicability to unseen sentences. We avoid both pitfalls by estimating the grammar probabilities with the Cross-Validating ExpectationMaximization algorithm (CV-EM) (Mylonakis and Sima’an, 2008; Mylonakis and Sima’an, 2010). CVEM is a cross-validating instance of the well known EM algorithm (Dempster et al., 1977). It works iteratively on a partition of the training data, climbing the likelihood of the training data while crossvalidating the latent variable values, considering for every training data point only those which can be produced by models built from the rest of the data excluding the current part. As a result, the estimation process simulates maximising future data likelihood, using the training data to directly aim towards strong generalisation of the estimate. For our probabilistic SCFG-based translation structure variable a, implementing CV-EM boils down to a synchronous version of the Ins</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society, Series B, 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Gillick</author>
<author>James Zhang</author>
<author>Dan Klein</author>
</authors>
<title>Why generative phrase models underperform surface heuristics.</title>
<date>2006</date>
<booktitle>In Proceedings on the Workshop on Statistical Machine Translation,</booktitle>
<pages>31--38</pages>
<institution>City. Association for Computational Linguistics.</institution>
<location>New York</location>
<contexts>
<context position="4670" citStr="DeNero et al., 2006" startWordPosition="662" endWordPosition="665">ped and trained for translation. We start by labelling each phrase-pair span in the word-aligned training data with multiple linguistically motivated categories, offering multi-grained abstractions from its lexical content. These phrasepair label charts are the input of our learning algorithm, which extracts the linguistically motivated rules and estimates the probabilities for a stochastic SCFG, without arbitrary constraints such as phrase or span sizes. Estimating such grammars under a Maximum Likelihood criterion is known to be plagued by strong overfitting leading to degenerate estimates (DeNero et al., 2006). In contrast, our learning objective not only avoids overfitting the training data but, most importantly, learns joint stochastic synchronous grammars which directly aim at generalisation towards yet unseen instances. By advancing from structures which mimic linguistic syntax, to learning linguistically aware latent recursive structures targeting translation, we achieve significant improvements in translation quality for 4 different language pairs in comparison with a strong hierarchical translation baseline. Our key contributions are presented in the following sections. Section 2 discusses t</context>
</contexts>
<marker>DeNero, Gillick, Zhang, Klein, 2006</marker>
<rawString>John DeNero, Dan Gillick, James Zhang, and Dan Klein. 2006. Why generative phrase models underperform surface heuristics. In Proceedings on the Workshop on Statistical Machine Translation, pages 31–38, New York City. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings,</booktitle>
<pages>273--280</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="2527" citStr="Galley et al., 2004" startWordPosition="350" endWordPosition="353">itive performance particularly for language pairs with extensive reordering. As Hiero uses a single non-terminal and concentrates on overcoming translation lexicon sparsity, it barely explores the recursive nature of translation past the lexical level. Nevertheless, the successful employment of SCFGs for phrase-based SMT brought translation models assuming latent syntactic structure to the spotlight. Simultaneously, mounting efforts have been directed towards SMT models employing linguistic syntax on the source side (Yamada and Knight, 2001; Quirk et al., 2005; Liu et al., 2006), target side (Galley et al., 2004; Galley et al., 2006) or both (Zhang et al., 2008; Liu et al., 2009; Chiang, 2010). Hierarchical translation was combined with target side linguistic annotation in (Zollmann and Venugopal, 2006). Interestingly, early on (Koehn et al., 2003) exemplified the difficulties of integrating linguistic information in translation systems. Syntaxbased MT often suffers from inadequate constraints in the translation rules extracted, or from striving to combine these rules together towards a full derivation. Recent research tries to address these issues, by re-structuring training data parse trees to bett</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings, pages 273–280, Boston, Massachusetts, USA, May. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>961--968</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="2549" citStr="Galley et al., 2006" startWordPosition="354" endWordPosition="357">ticularly for language pairs with extensive reordering. As Hiero uses a single non-terminal and concentrates on overcoming translation lexicon sparsity, it barely explores the recursive nature of translation past the lexical level. Nevertheless, the successful employment of SCFGs for phrase-based SMT brought translation models assuming latent syntactic structure to the spotlight. Simultaneously, mounting efforts have been directed towards SMT models employing linguistic syntax on the source side (Yamada and Knight, 2001; Quirk et al., 2005; Liu et al., 2006), target side (Galley et al., 2004; Galley et al., 2006) or both (Zhang et al., 2008; Liu et al., 2009; Chiang, 2010). Hierarchical translation was combined with target side linguistic annotation in (Zollmann and Venugopal, 2006). Interestingly, early on (Koehn et al., 2003) exemplified the difficulties of integrating linguistic information in translation systems. Syntaxbased MT often suffers from inadequate constraints in the translation rules extracted, or from striving to combine these rules together towards a full derivation. Recent research tries to address these issues, by re-structuring training data parse trees to better suit syntax-based S</context>
<context position="30208" citStr="Galley et al., 2006" startWordPosition="4781" endWordPosition="4785">ocus on the combination of learning latent structure with syntax and linguistic annotations, exploring the crossroads of machine learning, linguistic syntax and machine translation. Training a joint probability model was first discussed in (Marcu and Wong, 2002). We show that a translation system based on such a joint model can perform competitively in comparison with conditional probability models, when it is augmented with a rich latent hierarchical structure trained adequately to avoid overfitting. Earlier approaches for linguistic syntax-based translation such as (Yamada and Knight, 2001; Galley et al., 2006; Huang et al., 2006; Liu et al., 2006) focus on memorising and reusing parts of the structure of the source and/or target parse trees and constraining decoding by the input parse tree. In contrast to this approach, we choose to employ linguistic annotations in the form of unambiguous synchronous span labels, while discovering ambiguous translation structure taking advantage of them. Later work (Marton and Resnik, 2008; Venugopal et al., 2009; Chiang et al., 2009) takes a more flexible approach, influencing translation output using linguistically motivated features, or features based on source</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 961– 968, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>144--151</pages>
<contexts>
<context position="21707" citStr="Huang and Chiang, 2007" startWordPosition="3414" endWordPosition="3417">onterminals covering phrase-pairs as well as the higher level parts of the derivation. In this manner we not only constrain the translation hypotheses resulting in faster decoding time, but, more importantly, we may ground the hypotheses more closely to the available linguistic information of the source sentence. This is of particular interest as we move up the derivation tree, where an initial wrong choice below could propagate towards hypotheses wildly diverging from the input sentence’s linguistic annotation. Per Non-Terminal Pruning The decoder uses a combination of beam and cube-pruning (Huang and Chiang, 2007). As our grammar uses non-terminals in the hundreds of thousands, it is important not to prune away prematurely non-terminals covering smaller spans and to leave more options to be considered as we move up the derivation tree. For this, for every cell in the decoder’s chart, we keep a separate bin per non-terminal and prune together hypotheses leading to the same non-terminal covering a cell. This allows full derivations to be found for all input sentences, as well as avoids aggressive pruning at an early stage. Given the source label constraint discussed above, this does not increase running </context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144–151,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Czech Republic Prague</author>
</authors>
<date></date>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Prague, </marker>
<rawString>Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Statistical syntax-directed translation with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Biennial Conference of the Association for Machine Translation in the Americas (AMTA),</booktitle>
<location>Boston, MA, USA.</location>
<contexts>
<context position="30228" citStr="Huang et al., 2006" startWordPosition="4786" endWordPosition="4789">on of learning latent structure with syntax and linguistic annotations, exploring the crossroads of machine learning, linguistic syntax and machine translation. Training a joint probability model was first discussed in (Marcu and Wong, 2002). We show that a translation system based on such a joint model can perform competitively in comparison with conditional probability models, when it is augmented with a rich latent hierarchical structure trained adequately to avoid overfitting. Earlier approaches for linguistic syntax-based translation such as (Yamada and Knight, 2001; Galley et al., 2006; Huang et al., 2006; Liu et al., 2006) focus on memorising and reusing parts of the structure of the source and/or target parse trees and constraining decoding by the input parse tree. In contrast to this approach, we choose to employ linguistic annotations in the form of unambiguous synchronous span labels, while discovering ambiguous translation structure taking advantage of them. Later work (Marton and Resnik, 2008; Venugopal et al., 2009; Chiang et al., 2009) takes a more flexible approach, influencing translation output using linguistically motivated features, or features based on source-side linguistically</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proceedings of the 7th Biennial Conference of the Association for Machine Translation in the Americas (AMTA), Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Martin Cmejrek</author>
<author>Bowen Zhou</author>
</authors>
<title>Soft syntactic constraints for hierarchical phrase-based translation using latent syntactic distributions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>138--147</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="30884" citStr="Huang et al., 2010" startWordPosition="4887" endWordPosition="4890">ng and reusing parts of the structure of the source and/or target parse trees and constraining decoding by the input parse tree. In contrast to this approach, we choose to employ linguistic annotations in the form of unambiguous synchronous span labels, while discovering ambiguous translation structure taking advantage of them. Later work (Marton and Resnik, 2008; Venugopal et al., 2009; Chiang et al., 2009) takes a more flexible approach, influencing translation output using linguistically motivated features, or features based on source-side linguistically-guided latent syntactic categories (Huang et al., 2010). A feature-based approach and ours are not mutually exclusive, as we also employ a limited set of features next to our trained model during decoding. We find augmenting our system with a more extensive feature set an interesting research direction for the future. An array of recent work (Chiang, 2010; Zhang et al., 2008; Liu et al., 2009) sets off to utilise source and target syntax for translation. While for this work we constrain ourselves to source language syntax annotations, our method can be directly applied to employ labels taking advantage of linguistic annotations from both sides of </context>
</contexts>
<marker>Huang, Cmejrek, Zhou, 2010</marker>
<rawString>Zhongqiang Huang, Martin Cmejrek, and Bowen Zhou. 2010. Soft syntactic constraints for hierarchical phrase-based translation using latent syntactic distributions. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 138–147, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In HLTNAACL</booktitle>
<contexts>
<context position="2768" citStr="Koehn et al., 2003" startWordPosition="388" endWordPosition="391">ical level. Nevertheless, the successful employment of SCFGs for phrase-based SMT brought translation models assuming latent syntactic structure to the spotlight. Simultaneously, mounting efforts have been directed towards SMT models employing linguistic syntax on the source side (Yamada and Knight, 2001; Quirk et al., 2005; Liu et al., 2006), target side (Galley et al., 2004; Galley et al., 2006) or both (Zhang et al., 2008; Liu et al., 2009; Chiang, 2010). Hierarchical translation was combined with target side linguistic annotation in (Zollmann and Venugopal, 2006). Interestingly, early on (Koehn et al., 2003) exemplified the difficulties of integrating linguistic information in translation systems. Syntaxbased MT often suffers from inadequate constraints in the translation rules extracted, or from striving to combine these rules together towards a full derivation. Recent research tries to address these issues, by re-structuring training data parse trees to better suit syntax-based SMT training (Wang et al., 2010), or by moving from linguistically motivated synchronous grammars to systems where linguistic plausibility of the translation is assessed through additional features in a phrase-based syst</context>
<context position="12992" citStr="Koehn et al., 2003" startWordPosition="2036" endWordPosition="2039">del over all sentence-pairs generated given each translation structure. The probabilities of a derivation and of a sentence-pair are then as follows: p(D) =p(�)p(e, f|u) (1) p(e,f) = � p(D) (2) D:D=*&gt;(e,f) By splitting the joint model in a hierarchical structure model and a lexical emission one we facilitate estimating the two models separately. The following section discusses this. 3 Learning Translation Structure 3.1 Phrase-Pair Label Chart The input to our learning algorithm is a wordaligned parallel corpus. We consider as phrasepair spans those that obey the word-alignment constraints of (Koehn et al., 2003). For every training sentence-pair, we also input a chart containing one or more labels for every synchronous span, such as that of Figure 3. Each label describes different properties of the phrase pair (syntactic, semantic etc.), possibly in relation to its context, or supplying varying levels of abstraction (phrase-pair, determiner with noun, noun-phrase, sentence etc.). We aim to induce a recursive translation structure explaining the joint generation of the source and target sentence taking advantage of these phrase-pair span labels. For this work we employ the linguistically motivated lab</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In HLTNAACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>388--395</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="27144" citStr="Koehn, 2004" startWordPosition="4306" endWordPosition="4307">lts for the baseline and our method for the 4 language pairs, for training sets of both 200K and 400K sentence pairs. Our system (lts) outperforms the baseline for all 4 language pairs for both BLEU and NIST scores, by a margin which scales up to +1.92 BLEU points for English to Chinese translation when training on the 400K set. In addition, increasing the size of the training data from 200K to 400K sentence pairs widens the performance margin between the baseline and our system, in some cases considerably. All but one of the performance improvements are found to be statistically significant (Koehn, 2004) at the 95% confidence level, most of them also at the 99% level. We selected an array of target languages of increasing reordering complexity with English as source. Examining the results across the target languages, LTS performance gains increase the more challenging the sentence structure of the target language is in relation to the source’s, highlighted when translating to Chinese. Even for Dutch and German, which pose additional challenges such as compound words and morphology which we do not explicitly treat in the current system, LTS still delivers significant improvements in performanc</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 388–395, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In MT Summit</booktitle>
<contexts>
<context position="23301" citStr="Koehn, 2005" startWordPosition="3685" endWordPosition="3686">at we do not prune the phrase-pair emitting rules. Overall, we consider this a much more informed pruning criterion than those based on probability values (that are not comparable across left-hand sides) or righthand side counts (frequent symbols need many more expansions than a highly specialised one). 4.3 Experimental Setting &amp; Baseline We evaluate our method on four different language pairs with English as the source language and French, German, Dutch and Chinese as target. The data for the first three language pairs are derived from parliament proceedings sourced from the Europarl corpus (Koehn, 2005), with WMT07 development and test data for French and German. The data for the English to Chinese task is composed of parliament proceedings and news articles. For all language pairs we employ 200K and 400K sentence pairs for training, 2K for development and 2K for testing (single reference per source sentence). Both the baseline and our method decode n i=1 Oi(r)λi H rED 647 Training English to French German Dutch Chinese set size BLEU NIST BLEU NIST BLEU NIST BLEU NIST 200K josh-base 29.20 7.2123 18.65 5.8047 21.97 6.2469 22.34 6.5540 lts 29.43 7.2611** 19.10** 5.8714** 22.31* 6.2903* 23.67**</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In MT Summit 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Chris Callison-Burch</author>
<author>Chris Dyer</author>
<author>Sanjeev Khudanpur</author>
<author>Lane Schwartz</author>
<author>Wren Thornton</author>
<author>Jonathan Weese</author>
<author>Omar Zaidan</author>
</authors>
<title>Joshua: An open source toolkit for parsing-based machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>135--139</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="20629" citStr="Li et al., 2009" startWordPosition="3240" endWordPosition="3243">ilistic HR-SCFG maintains a separate joint phrase-pair emission distribution per non-terminal, the smoothing features (a) above assess the conditional translation of surface phrases irrespective of any notion of recursive translation structure. The final feature is the language model score for the target sentence, mounting up to the following model used at decoding time, with the feature weights A trained by Minimum Error Rate Training (MERT) (Och, 2003) on a development corpus. p(D � � (e, f)) a p(e)λ&amp;quot; pG(D)λc 4.2 Decoding Modifications We use a customised version of the Joshua SCFG decoder (Li et al., 2009) to translate, with the following modifications: Source Labels Constraints As for this work the phrase-pair labels used to extract the grammar are based on the linguistic analysis of the source side, we can construct the label chart for every input sentence from its parse. We subsequently use it to consider only derivations with synchronous spans which are covered by non-terminals matching one of the labels for those spans. This applies both for the nonterminals covering phrase-pairs as well as the higher level parts of the derivation. In this manner we not only constrain the translation hypot</context>
</contexts>
<marker>Li, Callison-Burch, Dyer, Khudanpur, Schwartz, Thornton, Weese, Zaidan, 2009</marker>
<rawString>Zhifei Li, Chris Callison-Burch, Chris Dyer, Sanjeev Khudanpur, Lane Schwartz, Wren Thornton, Jonathan Weese, and Omar Zaidan. 2009. Joshua: An open source toolkit for parsing-based machine translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 135–139, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-tostring alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>609--616</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="2493" citStr="Liu et al., 2006" startWordPosition="344" endWordPosition="347">rns with ‘gaps’, offering competitive performance particularly for language pairs with extensive reordering. As Hiero uses a single non-terminal and concentrates on overcoming translation lexicon sparsity, it barely explores the recursive nature of translation past the lexical level. Nevertheless, the successful employment of SCFGs for phrase-based SMT brought translation models assuming latent syntactic structure to the spotlight. Simultaneously, mounting efforts have been directed towards SMT models employing linguistic syntax on the source side (Yamada and Knight, 2001; Quirk et al., 2005; Liu et al., 2006), target side (Galley et al., 2004; Galley et al., 2006) or both (Zhang et al., 2008; Liu et al., 2009; Chiang, 2010). Hierarchical translation was combined with target side linguistic annotation in (Zollmann and Venugopal, 2006). Interestingly, early on (Koehn et al., 2003) exemplified the difficulties of integrating linguistic information in translation systems. Syntaxbased MT often suffers from inadequate constraints in the translation rules extracted, or from striving to combine these rules together towards a full derivation. Recent research tries to address these issues, by re-structuring</context>
<context position="30247" citStr="Liu et al., 2006" startWordPosition="4790" endWordPosition="4793">t structure with syntax and linguistic annotations, exploring the crossroads of machine learning, linguistic syntax and machine translation. Training a joint probability model was first discussed in (Marcu and Wong, 2002). We show that a translation system based on such a joint model can perform competitively in comparison with conditional probability models, when it is augmented with a rich latent hierarchical structure trained adequately to avoid overfitting. Earlier approaches for linguistic syntax-based translation such as (Yamada and Knight, 2001; Galley et al., 2006; Huang et al., 2006; Liu et al., 2006) focus on memorising and reusing parts of the structure of the source and/or target parse trees and constraining decoding by the input parse tree. In contrast to this approach, we choose to employ linguistic annotations in the form of unambiguous synchronous span labels, while discovering ambiguous translation structure taking advantage of them. Later work (Marton and Resnik, 2008; Venugopal et al., 2009; Chiang et al., 2009) takes a more flexible approach, influencing translation output using linguistically motivated features, or features based on source-side linguistically-guided latent synt</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-tostring alignment template for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 609–616, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yajuan L¨u</author>
<author>Qun Liu</author>
</authors>
<title>Improving tree-to-tree translation with packed forests.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>558--566</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<marker>Liu, L¨u, Liu, 2009</marker>
<rawString>Yang Liu, Yajuan L¨u, and Qun Liu. 2009. Improving tree-to-tree translation with packed forests. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 558–566, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>William Wong</author>
</authors>
<title>A phrase-based, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of Empirical methods in natural language processing,</booktitle>
<pages>133--139</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="29851" citStr="Marcu and Wong, 2002" startWordPosition="4727" endWordPosition="4730">mance but slows down decoding speed. Notably, as can be seen in Table 2(b), switching to a 4-gram LM results in performance gains for both the baseline and our system and while the margin between the two systems decreases, our system continues to deliver a considerable and significant improvement in translation BLEU scores. 5 Related Work In this work, we focus on the combination of learning latent structure with syntax and linguistic annotations, exploring the crossroads of machine learning, linguistic syntax and machine translation. Training a joint probability model was first discussed in (Marcu and Wong, 2002). We show that a translation system based on such a joint model can perform competitively in comparison with conditional probability models, when it is augmented with a rich latent hierarchical structure trained adequately to avoid overfitting. Earlier approaches for linguistic syntax-based translation such as (Yamada and Knight, 2001; Galley et al., 2006; Huang et al., 2006; Liu et al., 2006) focus on memorising and reusing parts of the structure of the source and/or target parse trees and constraining decoding by the input parse tree. In contrast to this approach, we choose to employ linguis</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Daniel Marcu and William Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In Proceedings of Empirical methods in natural language processing, pages 133–139. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Soft syntactic constraints for hierarchical phrased-based translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>1003--1011</pages>
<contexts>
<context position="30630" citStr="Marton and Resnik, 2008" startWordPosition="4853" endWordPosition="4856">ented with a rich latent hierarchical structure trained adequately to avoid overfitting. Earlier approaches for linguistic syntax-based translation such as (Yamada and Knight, 2001; Galley et al., 2006; Huang et al., 2006; Liu et al., 2006) focus on memorising and reusing parts of the structure of the source and/or target parse trees and constraining decoding by the input parse tree. In contrast to this approach, we choose to employ linguistic annotations in the form of unambiguous synchronous span labels, while discovering ambiguous translation structure taking advantage of them. Later work (Marton and Resnik, 2008; Venugopal et al., 2009; Chiang et al., 2009) takes a more flexible approach, influencing translation output using linguistically motivated features, or features based on source-side linguistically-guided latent syntactic categories (Huang et al., 2010). A feature-based approach and ours are not mutually exclusive, as we also employ a limited set of features next to our trained model during decoding. We find augmenting our system with a more extensive feature set an interesting research direction for the future. An array of recent work (Chiang, 2010; Zhang et al., 2008; Liu et al., 2009) sets</context>
</contexts>
<marker>Marton, Resnik, 2008</marker>
<rawString>Yuval Marton and Philip Resnik. 2008. Soft syntactic constraints for hierarchical phrased-based translation. In Proceedings of ACL-08: HLT, pages 1003–1011,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Translation</author>
</authors>
<title>Association for Computational Linguistics.</title>
<date></date>
<pages>138--141</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City,</location>
<marker>Translation, </marker>
<rawString>Translation, pages 138–141, New York City, June. Association for Computational Linguistics. Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markos Mylonakis</author>
<author>Khalil Sima’an</author>
</authors>
<title>Phrase translation probabilities with ITG priors and smoothing as learning objective.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>630--639</pages>
<location>Honolulu, USA,</location>
<marker>Mylonakis, Sima’an, 2008</marker>
<rawString>Markos Mylonakis and Khalil Sima’an. 2008. Phrase translation probabilities with ITG priors and smoothing as learning objective. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 630–639, Honolulu, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markos Mylonakis</author>
<author>Khalil Sima’an</author>
</authors>
<title>Learning probabilistic synchronous CFGs for phrase-based translation.</title>
<date>2010</date>
<booktitle>In Fourteenth Conference on Computational Natural Language Learning,</booktitle>
<location>Uppsala, Sweden,</location>
<marker>Mylonakis, Sima’an, 2010</marker>
<rawString>Markos Mylonakis and Khalil Sima’an. 2010. Learning probabilistic synchronous CFGs for phrase-based translation. In Fourteenth Conference on Computational Natural Language Learning, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan,</location>
<contexts>
<context position="20471" citStr="Och, 2003" startWordPosition="3213" endWordPosition="3214">t the synchronous grammar derivation and learning general reordering or word emission preferences for the language pair. As an example, while our probabilistic HR-SCFG maintains a separate joint phrase-pair emission distribution per non-terminal, the smoothing features (a) above assess the conditional translation of surface phrases irrespective of any notion of recursive translation structure. The final feature is the language model score for the target sentence, mounting up to the following model used at decoding time, with the feature weights A trained by Minimum Error Rate Training (MERT) (Och, 2003) on a development corpus. p(D � � (e, f)) a p(e)λ&amp;quot; pG(D)λc 4.2 Decoding Modifications We use a customised version of the Joshua SCFG decoder (Li et al., 2009) to translate, with the following modifications: Source Labels Constraints As for this work the phrase-pair labels used to extract the grammar are based on the linguistic analysis of the source side, we can construct the label chart for every input sentence from its parse. We subsequently use it to consider only derivations with synchronous spans which are covered by non-terminals matching one of the labels for those spans. This applies b</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal smt.</title>
<date>2005</date>
<booktitle>In Proceedings of 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Ann Arbor, Michigan, USA,</location>
<contexts>
<context position="2474" citStr="Quirk et al., 2005" startWordPosition="340" endWordPosition="343">and reordering patterns with ‘gaps’, offering competitive performance particularly for language pairs with extensive reordering. As Hiero uses a single non-terminal and concentrates on overcoming translation lexicon sparsity, it barely explores the recursive nature of translation past the lexical level. Nevertheless, the successful employment of SCFGs for phrase-based SMT brought translation models assuming latent syntactic structure to the spotlight. Simultaneously, mounting efforts have been directed towards SMT models employing linguistic syntax on the source side (Yamada and Knight, 2001; Quirk et al., 2005; Liu et al., 2006), target side (Galley et al., 2004; Galley et al., 2006) or both (Zhang et al., 2008; Liu et al., 2009; Chiang, 2010). Hierarchical translation was combined with target side linguistic annotation in (Zollmann and Venugopal, 2006). Interestingly, early on (Koehn et al., 2003) exemplified the difficulties of integrating linguistic information in translation systems. Syntaxbased MT often suffers from inadequate constraints in the translation rules extracted, or from striving to combine these rules together towards a full derivation. Recent research tries to address these issues</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal smt. In Proceedings of 43rd Annual Meeting of the Association for Computational Linguistics, Ann Arbor, Michigan, USA, June.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ashish Venugopal</author>
<author>Andreas Zollmann</author>
<author>Noah A Smith</author>
<author>Stephan Vogel</author>
</authors>
<title>Preference grammars: Softening syntactic constraints to improve statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>236--244</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="3394" citStr="Venugopal et al., 2009" startWordPosition="479" endWordPosition="483">mplified the difficulties of integrating linguistic information in translation systems. Syntaxbased MT often suffers from inadequate constraints in the translation rules extracted, or from striving to combine these rules together towards a full derivation. Recent research tries to address these issues, by re-structuring training data parse trees to better suit syntax-based SMT training (Wang et al., 2010), or by moving from linguistically motivated synchronous grammars to systems where linguistic plausibility of the translation is assessed through additional features in a phrase-based system (Venugopal et al., 2009; Chiang et al., 2009), obscuring the impact of higher level syntactic processes. While it is assumed that linguistic structure does correlate with some translation phenomena, in this Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 642–652, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics work we do not employ it as the backbone of translation. In place of linguistically constrained translation imposing syntactic parse structure, we opt for linguistically motivated translation. We learn latent hierarchical struc</context>
<context position="30654" citStr="Venugopal et al., 2009" startWordPosition="4857" endWordPosition="4860">hierarchical structure trained adequately to avoid overfitting. Earlier approaches for linguistic syntax-based translation such as (Yamada and Knight, 2001; Galley et al., 2006; Huang et al., 2006; Liu et al., 2006) focus on memorising and reusing parts of the structure of the source and/or target parse trees and constraining decoding by the input parse tree. In contrast to this approach, we choose to employ linguistic annotations in the form of unambiguous synchronous span labels, while discovering ambiguous translation structure taking advantage of them. Later work (Marton and Resnik, 2008; Venugopal et al., 2009; Chiang et al., 2009) takes a more flexible approach, influencing translation output using linguistically motivated features, or features based on source-side linguistically-guided latent syntactic categories (Huang et al., 2010). A feature-based approach and ours are not mutually exclusive, as we also employ a limited set of features next to our trained model during decoding. We find augmenting our system with a more extensive feature set an interesting research direction for the future. An array of recent work (Chiang, 2010; Zhang et al., 2008; Liu et al., 2009) sets off to utilise source a</context>
</contexts>
<marker>Venugopal, Zollmann, Smith, Vogel, 2009</marker>
<rawString>Ashish Venugopal, Andreas Zollmann, Noah A. Smith, and Stephan Vogel. 2009. Preference grammars: Softening syntactic constraints to improve statistical machine translation. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 236–244, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Wang</author>
<author>Jonathan May</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Re-structuring, re-labeling, and realigning for syntax-based machine translation.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>2</issue>
<contexts>
<context position="3180" citStr="Wang et al., 2010" startWordPosition="449" endWordPosition="452">ng et al., 2008; Liu et al., 2009; Chiang, 2010). Hierarchical translation was combined with target side linguistic annotation in (Zollmann and Venugopal, 2006). Interestingly, early on (Koehn et al., 2003) exemplified the difficulties of integrating linguistic information in translation systems. Syntaxbased MT often suffers from inadequate constraints in the translation rules extracted, or from striving to combine these rules together towards a full derivation. Recent research tries to address these issues, by re-structuring training data parse trees to better suit syntax-based SMT training (Wang et al., 2010), or by moving from linguistically motivated synchronous grammars to systems where linguistic plausibility of the translation is assessed through additional features in a phrase-based system (Venugopal et al., 2009; Chiang et al., 2009), obscuring the impact of higher level syntactic processes. While it is assumed that linguistic structure does correlate with some translation phenomena, in this Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 642–652, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics work we do no</context>
</contexts>
<marker>Wang, May, Knight, Marcu, 2010</marker>
<rawString>Wei Wang, Jonathan May, Kevin Knight, and Daniel Marcu. 2010. Re-structuring, re-labeling, and realigning for syntax-based machine translation. Computational Linguistics, 36(2):247–277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="1650" citStr="Wu, 1997" startWordPosition="224" endWordPosition="225"> directly targeting generalisation over future data. We obtain statistically significant improvements across 4 different language pairs with English as source, mounting up to +1.92 BLEU for Chinese as target. 1 Introduction Recent advances in Statistical Machine Translation (SMT) are widely centred around two concepts: (a) hierarchical translation processes, frequently employing Synchronous Context Free Grammars (SCFGs) and (b) transduction or synchronous rewrite processes over a linguistic syntactic tree. SCFGs in the form of the Inversion-Transduction Grammar (ITG) were first introduced by (Wu, 1997) as a formalism to recursively describe the translation process. The Hiero system (Chiang, 2005) 642 utilised an ITG-flavour which focused on hierarchical phrase-pairs to capture context-driven translation and reordering patterns with ‘gaps’, offering competitive performance particularly for language pairs with extensive reordering. As Hiero uses a single non-terminal and concentrates on overcoming translation lexicon sparsity, it barely explores the recursive nature of translation past the lexical level. Nevertheless, the successful employment of SCFGs for phrase-based SMT brought translation</context>
<context position="6014" citStr="Wu, 1997" startWordPosition="859" endWordPosition="860">al translation structure from phrase-pair emission. In section 3 we consider a chart over phrase-pair spans filled with sourcelanguage linguistically motivated labels. We show how we can employ this crucial input to extract and train a hierarchical translation structure model with millions of rules. Section 4 demonstrates decoding with the model by constraining derivations to linguistic hints of the source sentence and presents our empirical results. We close with a discussion of related work and our conclusions. 2 Joint Translation Model Our model is based on a probabilistic Synchronous CFG (Wu, 1997; Chiang, 2005). SCFGs define a Figure 1: English-German SCFG rules for the relative clause(s) ‘which is the solution (to the problem) / der die L¨osung (f¨ur das Problem) ist’, [ ] signify monotone translation, ( ) a swap reordering. language over string pairs, which are generated beginning from a start symbol 5 and recursively expanding pairs of linked non-terminals across the two strings using the grammar’s rule set. By crossing the links between the non-terminals of the two sides reordering phenomena are captured. We employ binary SCFGs, i.e. grammars with a maximum of two non-terminals on</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>523--530</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Toulouse, France,</location>
<contexts>
<context position="2454" citStr="Yamada and Knight, 2001" startWordPosition="336" endWordPosition="339">ntext-driven translation and reordering patterns with ‘gaps’, offering competitive performance particularly for language pairs with extensive reordering. As Hiero uses a single non-terminal and concentrates on overcoming translation lexicon sparsity, it barely explores the recursive nature of translation past the lexical level. Nevertheless, the successful employment of SCFGs for phrase-based SMT brought translation models assuming latent syntactic structure to the spotlight. Simultaneously, mounting efforts have been directed towards SMT models employing linguistic syntax on the source side (Yamada and Knight, 2001; Quirk et al., 2005; Liu et al., 2006), target side (Galley et al., 2004; Galley et al., 2006) or both (Zhang et al., 2008; Liu et al., 2009; Chiang, 2010). Hierarchical translation was combined with target side linguistic annotation in (Zollmann and Venugopal, 2006). Interestingly, early on (Koehn et al., 2003) exemplified the difficulties of integrating linguistic information in translation systems. Syntaxbased MT often suffers from inadequate constraints in the translation rules extracted, or from striving to combine these rules together towards a full derivation. Recent research tries to </context>
<context position="30187" citStr="Yamada and Knight, 2001" startWordPosition="4777" endWordPosition="4780">d Work In this work, we focus on the combination of learning latent structure with syntax and linguistic annotations, exploring the crossroads of machine learning, linguistic syntax and machine translation. Training a joint probability model was first discussed in (Marcu and Wong, 2002). We show that a translation system based on such a joint model can perform competitively in comparison with conditional probability models, when it is augmented with a rich latent hierarchical structure trained adequately to avoid overfitting. Earlier approaches for linguistic syntax-based translation such as (Yamada and Knight, 2001; Galley et al., 2006; Huang et al., 2006; Liu et al., 2006) focus on memorising and reusing parts of the structure of the source and/or target parse trees and constraining decoding by the input parse tree. In contrast to this approach, we choose to employ linguistic annotations in the form of unambiguous synchronous span labels, while discovering ambiguous translation structure taking advantage of them. Later work (Marton and Resnik, 2008; Venugopal et al., 2009; Chiang et al., 2009) takes a more flexible approach, influencing translation output using linguistically motivated features, or fea</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. In Proceedings of 39th Annual Meeting of the Association for Computational Linguistics, pages 523–530, Toulouse, France, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
<author>Aiti Aw</author>
<author>Haizhou Li</author>
<author>Chew Lim Tan</author>
<author>Sheng Li</author>
</authors>
<title>A tree sequence alignment-based tree-to-tree translation model.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>559--567</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="2577" citStr="Zhang et al., 2008" startWordPosition="360" endWordPosition="363">ith extensive reordering. As Hiero uses a single non-terminal and concentrates on overcoming translation lexicon sparsity, it barely explores the recursive nature of translation past the lexical level. Nevertheless, the successful employment of SCFGs for phrase-based SMT brought translation models assuming latent syntactic structure to the spotlight. Simultaneously, mounting efforts have been directed towards SMT models employing linguistic syntax on the source side (Yamada and Knight, 2001; Quirk et al., 2005; Liu et al., 2006), target side (Galley et al., 2004; Galley et al., 2006) or both (Zhang et al., 2008; Liu et al., 2009; Chiang, 2010). Hierarchical translation was combined with target side linguistic annotation in (Zollmann and Venugopal, 2006). Interestingly, early on (Koehn et al., 2003) exemplified the difficulties of integrating linguistic information in translation systems. Syntaxbased MT often suffers from inadequate constraints in the translation rules extracted, or from striving to combine these rules together towards a full derivation. Recent research tries to address these issues, by re-structuring training data parse trees to better suit syntax-based SMT training (Wang et al., 20</context>
<context position="31206" citStr="Zhang et al., 2008" startWordPosition="4943" endWordPosition="4946"> them. Later work (Marton and Resnik, 2008; Venugopal et al., 2009; Chiang et al., 2009) takes a more flexible approach, influencing translation output using linguistically motivated features, or features based on source-side linguistically-guided latent syntactic categories (Huang et al., 2010). A feature-based approach and ours are not mutually exclusive, as we also employ a limited set of features next to our trained model during decoding. We find augmenting our system with a more extensive feature set an interesting research direction for the future. An array of recent work (Chiang, 2010; Zhang et al., 2008; Liu et al., 2009) sets off to utilise source and target syntax for translation. While for this work we constrain ourselves to source language syntax annotations, our method can be directly applied to employ labels taking advantage of linguistic annotations from both sides of translation. The decoding constraints of section 4.2 can then still be applied on the source part of hybrid source-target labels. For the experiments in this paper we employ a label set similar to the non-terminals set of (Zollmann and Venugopal, 2006). However, the synchronous grammars we learn share few similarities wi</context>
</contexts>
<marker>Zhang, Jiang, Aw, Li, Tan, Li, 2008</marker>
<rawString>Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan, and Sheng Li. 2008. A tree sequence alignment-based tree-to-tree translation model. In Proceedings ofACL-08: HLT, pages 559–567, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings on the Workshop on Statistical Machine</booktitle>
<contexts>
<context position="2722" citStr="Zollmann and Venugopal, 2006" startWordPosition="380" endWordPosition="384">xplores the recursive nature of translation past the lexical level. Nevertheless, the successful employment of SCFGs for phrase-based SMT brought translation models assuming latent syntactic structure to the spotlight. Simultaneously, mounting efforts have been directed towards SMT models employing linguistic syntax on the source side (Yamada and Knight, 2001; Quirk et al., 2005; Liu et al., 2006), target side (Galley et al., 2004; Galley et al., 2006) or both (Zhang et al., 2008; Liu et al., 2009; Chiang, 2010). Hierarchical translation was combined with target side linguistic annotation in (Zollmann and Venugopal, 2006). Interestingly, early on (Koehn et al., 2003) exemplified the difficulties of integrating linguistic information in translation systems. Syntaxbased MT often suffers from inadequate constraints in the translation rules extracted, or from striving to combine these rules together towards a full derivation. Recent research tries to address these issues, by re-structuring training data parse trees to better suit syntax-based SMT training (Wang et al., 2010), or by moving from linguistically motivated synchronous grammars to systems where linguistic plausibility of the translation is assessed thro</context>
<context position="13629" citStr="Zollmann and Venugopal, 2006" startWordPosition="2136" endWordPosition="2139">ery training sentence-pair, we also input a chart containing one or more labels for every synchronous span, such as that of Figure 3. Each label describes different properties of the phrase pair (syntactic, semantic etc.), possibly in relation to its context, or supplying varying levels of abstraction (phrase-pair, determiner with noun, noun-phrase, sentence etc.). We aim to induce a recursive translation structure explaining the joint generation of the source and target sentence taking advantage of these phrase-pair span labels. For this work we employ the linguistically motivated labels of (Zollmann and Venugopal, 2006), albeit for the source language. Given a parse of the source sentence, each span is assigned the following kind of labels: Phrase-Pair All phrase-pairs are assigned the X label Constituent Source phrase is a constituent A Concatenation of Constituents Source phrase labelled A+B as a concatenation of constituents A and B, similarly for 3 constituents. Partial Constituents Categorial grammar (BarHillel, 1953) inspired labels A/B, A\B, indicating a partial constituent A missing constituent B right or left respectively. An important point is that we assign all applicable labels to every span. In </context>
<context position="15077" citStr="Zollmann and Venugopal, 2006" startWordPosition="2367" endWordPosition="2371">raction From every word-aligned sentence-pair and its label chart, we extract SCFG rules as those of Figure 2. Binary rules are extracted from adjoining synchronous spans up to the whole sentence-pair level, with the non-terminals of both left and right-hand side derived from the label names plus their reordering function (monotone, left/right swapping) in the span examined. A single unary rule per non-terminal NT generates the phrase-pair emitting NTP. Unary rules NTP —* a / Q generating the phrase-pair are created for all the labels covering it. While we label the phrase-pairs similarly to (Zollmann and Venugopal, 2006), the extracted grammar is rather different. We do not employ rules that are grounded to lexical context (‘gap’ rules), relying instead on the reordering-aware non-terminal set and related unary and binary rules. The result is a grammar which can both capture a rich array of translation phenomena based on linguistic and lexical grounds and explicitly model the balance between 645 SBAR PPP to the problem f¨ur das Problem Figure 4: A derivation of a sentence fragment with the grammar of Figure 1. memorising long phrase-pairs and generalising over yet unseen ones, as shown in the next example. Th</context>
<context position="31736" citStr="Zollmann and Venugopal, 2006" startWordPosition="5030" endWordPosition="5033">esting research direction for the future. An array of recent work (Chiang, 2010; Zhang et al., 2008; Liu et al., 2009) sets off to utilise source and target syntax for translation. While for this work we constrain ourselves to source language syntax annotations, our method can be directly applied to employ labels taking advantage of linguistic annotations from both sides of translation. The decoding constraints of section 4.2 can then still be applied on the source part of hybrid source-target labels. For the experiments in this paper we employ a label set similar to the non-terminals set of (Zollmann and Venugopal, 2006). However, the synchronous grammars we learn share few similarities with those that they heuristically extract. The HR-SCFG we adopt allows capturing more complex reordering phenomena and, in contrast to both (Chiang, 2005; Zollmann and Venugopal, 2006), is not exposed to the issues highlighted in section 2.1. Nevertheless, our results underline the capacity of linguistic anno649 tations similar to those of (Zollmann and Venugopal, Instead of employing hierarchical phrase-pairs, we 2006) as part of latent translation variables. invest in learning the higher-order hierarchical synMost of the af</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings on the Workshop on Statistical Machine</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>