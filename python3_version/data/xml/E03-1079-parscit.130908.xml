<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009616">
<title confidence="0.9989145">
Generalised PP-Attachment Disambiguation
using Corpus-based Linguistic Diagnostics
</title>
<author confidence="0.957733">
Paola Merlo
</author>
<affiliation confidence="0.954891">
Linguistics Department
University of Geneva
</affiliation>
<address confidence="0.911586">
2 rue de Candolle
1211 Geneva 4, Switzerland
</address>
<email confidence="0.99788">
merlo@lettres.unige.ch
</email>
<sectionHeader confidence="0.99737" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999881071428571">
We propose a new formulation of the
PP attachment problem as a 4-way
classification which takes into account
the argument or adjunct status of the
PP. Based on linguistic diagnostics, we
train a 4-way classifier that reaches an
average accuracy of 73.9% (baseline
66.2%). Compared to a sequence of
binary classifiers, the 4-way classifier
reaches better performance and individ-
uates a verb&apos;s arguments more accu-
rately, thus improving the acquisition of
a crucial piece of information for many
NLP applications.
</bodyText>
<sectionHeader confidence="0.993112" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.99860670909091">
Incorrect attachment of prepositional phrases of-
ten constitutes the main source of errors in current
parsing systems. Correct attachment of PPs is nec-
essary to construct a parse tree which will support
the proper interpretation of constituents in the sen-
tence. Consider the time-worn example
I saw the man with the telescope
It is important to determine if the PP with the
telescope is to be attached as a sister to the noun
the man, restricting its interpretation, or if it is to
be attached to the verb, thereby indicating the in-
strument of the main action described by the sen-
tence. Based on examples of this sort, recent ap-
proaches have formalised the problem of disam-
biguating PP attachments as a binary choice, dis-
tinguishing between attachment of a PP to a given
verb or to the verb&apos;s direct object (Ratnaparkhi et
al., 1994; Collins and Brooks, 1995).
This is, however, a simplification of the prob-
lem, which does not take the nature of the attach-
ment into account. Precisely, it does not distin-
guish PP arguments from PP adjuncts. Consider
the following example, which contains two PPs,
both modifying the verb.
Put the block on the table in the morning
The first PP is a locative PP required by the sub-
categorisation frame of the verb put, while in the
morning is an optional descriptor of the time at
which the action was performed. Though both at-
tached to the verb, the two PPs entertain different
relationships with the verb — the first is an argu-
ment while the latter is an adjunct. Analogous ex-
amples could be built for attachments to the noun.
Is it important to model not just the site but
also the nature of the attachment of a PP into the
tree structure? We would like to claim that it is.
Distinguishing arguments from adjuncts is key to
identifying the semantic kernel of a sentence. Ex-
tracting the core meaning of a sentence or phrase,
in turn, is necessary for automatic acquisition of
important lexical knowledge, such as subcategori-
sation frames and argument structures, which is
used in several NLP tasks and applications, such
as parsing or machine translation (Srinivas and
Joshi, 1999; Don, 1997). Moreover, from a quan-
titative point of view, arguments and adjuncts have
different statistical properties, requiring different
statistical techniques. For example, (Hindle and
Rooth, 1993) clearly indicate that their lexical as-
sociation technique performs much better for argu-
ments than for adjuncts, whether the attachment is
to the verb or to the noun.
Researchers have abstracted away from this dis-
tinction, because identifying arguments and ad-
juncts is a notoriously difficult task, taxing many
</bodyText>
<page confidence="0.994867">
251
</page>
<bodyText confidence="0.999972837837838">
native speakers&apos; intuitions and requiring complex
world knowledge. The usual expectation has
been that this discrimination is not amenable to
a corpus-based treatment. In recent work, how-
ever, we succeed in distinguishing arguments from
adjuncts using evidence extracted from a parsed
corpus (Merlo and Leybold, 2001). Our method
develops corpus-based statistical correlates for the
diagnostics used in linguistics to decide whether
a PP is an argument or an adjunct. A numerical
vectorial representation of the notion of argument-
hood is provided, which supports automatic clas-
sification, reaching 86% accuracy. In the current
paper, we extend this work and propose a new for-
mulation of the PP attachment problem. We treat
PP attachment as a 4-way classification of PPs into
noun argument PPs, noun adjunct PPs, verb argu-
ment PPs, and verb adjunct PPs.
We show that it is possible to build a classifier
that solves this problem using corpus evidence,
with good accuracy (74%). This classifier solves
the classification problem directly, in one step. In-
terestingly, we show that a 4-way classifier reaches
better accuracy than a two-step sequence of binary
classifiers, which first solve the noun-verb attach-
ment problem and then refine the attachment deci-
sion into argument or adjunct. This result indicates
that the current formulation of the PP attachment
problem cannot be considered a first step in the
solution of the final 4-way discrimination task. Fi-
nally, we note that the improvement is especially
due to a better recognition of verbs&apos; arguments,
thus providing more accurate information to many
NLP tasks and applications.
Solving this novel 4-way classification task cru-
cially relies on the ability to distinguish arguments
from adjuncts using corpus counts.
</bodyText>
<sectionHeader confidence="0.9909635" genericHeader="introduction">
2 A Novel Method to Distinguish
Arguments from Adjuncts
</sectionHeader>
<bodyText confidence="0.999820695652174">
Few attempts have been made to distinguish ar-
guments from adjuncts automatically (Buchholz,
1999; Merlo and Leybold, 2001; Villavicencio,
2002; Aldezabal et al., 2002). The core difficulty
in this enterprise is to define the notion of argu-
ment precisely. There is a consensus in linguis-
tics that arguments and adjuncts are different both
with respect to their function in the sentence and in
the way they themselves are interpreted (Jackend-
off, 1977; Marantz, 1984; Pollard and Sag, 1987;
Grimshaw, 1990). With respect to their function,
an argument fills a role in the relation described by
its associated head, while an adjunct predicates a
separate property of its associate head or phrase.
With respect to their interpretation, a complement
is an argument if its interpretation depends ex-
clusively on the head with which it is associated,
while it is an adjunct if its interpretation remains
relatively constant when associating with different
heads (Grimshaw, 1990, 108). Restricting the dis-
cussion to PPs, these differences are illustrated in
the following examples (PP-argument in bold), see
also (Schiitze, 1995, 100).
</bodyText>
<listItem confidence="0.4369495">
a) Kim camps/jogs/meditates on Sunday
b) Kim depended on Sandy
</listItem>
<bodyText confidence="0.9997896875">
In example a) the PP on Sunday can be con-
strued without any reference to the preceding part
of the sentence, and it preserves its meaning even
when combining with different heads. This is,
however, not the case for b). Here, the PP can
only be properly understood in connection with
the rest of the sentence: Sandy is the person on
whom someone depends.
These semantic distinctions surface in observ-
able syntactic differences, giving rise to a set of
linguistic diagnostics to determine whether a PP is
an adjunct or an argument. We illustrate here those
countable diagnostics that can be approximated
statistically and estimated using corpus counts,
thus combining linguistic insight with the robust-
ness of corpus-based methods.
</bodyText>
<sectionHeader confidence="0.981406" genericHeader="method">
3 The Linguistic Diagnostics
</sectionHeader>
<bodyText confidence="0.999841916666667">
Many diagnostics for argumenthood have been
proposed in the literature (Schiitze, 1995). Some
of them require complex syntactic manipulation of
the sentence, such as wh-extraction, and are there-
fore too difficult to apply automatically. We ex-
tend our previous work (Merlo and Leybold, 2001)
and choose six diagnostics that can be captured
by simple corpus counts: head dependence, op-
tionality, iterativity, ordering, copular paraphrase,
and deverbal nominalisation. These diagnostics
tap into the deeper semantic properties that distin-
guish arguments from adjuncts.
</bodyText>
<page confidence="0.994619">
252
</page>
<bodyText confidence="0.999384555555556">
Head Dependence Arguments depend on their
lexical heads, because they form an integral part
of the phrase. Adjuncts do not. Consequently, PP-
arguments can only appear with the specific ver-
bal or nominal head by which they are lexically
selected, while PP-adjuncts can co-occur with a
far greater range of different heads than arguments
(Pollard and Sag, 1987, 136), as illustrated in the
example sentences below (PP-argument in bold).
</bodyText>
<listItem confidence="0.9090415">
a) a man/woman/scarecrow with gray hair
b) a student/*punk/*watermelon of physics
</listItem>
<bodyText confidence="0.9992311">
We capture this insight by measuring the disper-
sion of the distribution over the different verbs or
nouns that co-occur with a given PP in a corpus.
We expect adjunct PPs to have higher dispersion
than argument PPs. Differently from our previ-
ous simpler implementation (Merlo and Leybold,
2001), we use entropy as a measure of the disper-
sion of the distribution, as indicated in (1) (h in-
dicates the noun or verb head to which the PP is
attached).
</bodyText>
<equation confidence="0.999094">
H(PP) = —E,p(h,)log2p(h,) (1)
</equation>
<bodyText confidence="0.999369333333333">
Optionality In most cases of verb attachment,&apos;
PP-arguments are obligatory elements of a given
sentence whose absence leads to ungrammatical-
ity, while adjuncts do not contribute to the seman-
tics of any particular verb, hence they are optional,
as illustrated in the following examples: 2
</bodyText>
<listItem confidence="0.98100075">
a) John put the book in the room
b) *John put the book
c) John saw/read the book in the room
d) John saw/read the book
</listItem>
<bodyText confidence="0.968044777777778">
The notion of optionality can be captured by the
conditional probability of a PP given a particular
verbal head, P(PP1v).
&apos;We do not compute this measure for nominal heads as
PP are always optional when governed by a nominal head.
2Notice that this diagnostics can only be interpreted as
a statistical tendency, and not as a strict test, because not
all arguments are obligatory (but all adjuncts are indeed op-
tional). The best known descriptive exceptions to the crite-
rion of optionality are the class of so-called object-drop verbs
(Levin, 1993) and, arguably, instrumental verbs (Schatze,
1995). While keeping these exceptions in mind, we maintain
optionality as a valid diagnostic here.
Iterativity and Ordering Arguments cannot be
iterated and they must be adjacent to the select-
ing lexical head. Neither of these two restrictions
apply to adjuncts, as illustrated in the examples
below.
</bodyText>
<listItem confidence="0.808408666666667">
a) *Chris rented the gazebo to girls, to boys
b) Kim met Sandy in Baltimore in the hotel
lobby in a corner
</listItem>
<bodyText confidence="0.99657675">
Thus, the probability of a PP being an adjunct
can be approximated as the probability of its oc-
currence in second position in a sequence of PPs,
as indicated in (2).
</bodyText>
<equation confidence="0.994444">
P(ADJ1(PP)1);----- P(PP)2 (2)
</equation>
<bodyText confidence="0.9984645">
Copular Paraphrase The diagnostic of copular
paraphrase is specific to the distinction of NPs ar-
guments and adjuncts (Schiitze, 1995, 103). NP
arguments cannot be paraphrased by a copular rel-
ative clause (examples b and b&apos;), while adjuncts
can (examples a and a&apos;).
</bodyText>
<listItem confidence="0.675438">
a) a man from Paris
a&apos;) a man who was from Paris
</listItem>
<bodyText confidence="0.894288285714286">
b) the weight of the cow
b&apos;) *the weight that was of the cow
Thus, the probability that a PP is an adjunct can
be approximated by the probability of its occur-
rence following a copular verb, such as be, be-
come, appear, seem, remain (Quirk et al., 1985),
as indicated in (3).
</bodyText>
<equation confidence="0.958648">
P(ADJ1(PP)) P(copulary &lt; PP) (3)
</equation>
<bodyText confidence="0.9998036">
Deverbal Nouns This diagnostic is based on the
observation that PPs following a deverbal noun are
likely to be arguments. This diagnostic can be cap-
tured by a probability indicators function, that as-
signs probability 1 of being an argument to PPs
following a deverbal noun and 0 otherwise.
In conclusion, the different properties of argu-
ments and adjuncts can be reduced to surface in-
dicators, which can be estimated by appropriate
corpus counts.
</bodyText>
<sectionHeader confidence="0.998662" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999429">
The success and generality of corpus-based clas-
sifier induction rests in large part on the accurate
</bodyText>
<page confidence="0.990023">
253
</page>
<bodyText confidence="0.9999565">
estimation of the feature vectors used for training.
We explain the details of our methodology below.3
</bodyText>
<subsectionHeader confidence="0.996683">
4.1 The Materials
</subsectionHeader>
<bodyText confidence="0.999444806451613">
Corpora We construct two corpora comprising
examples of PP sequences. A PP is a preposi-
tion and head noun sequence. One corpus contains
data encoding information for attachment of single
PPs in the form of four head words (verb, object
noun, preposition and PP-internal noun) for each
instance of PP attachments found in the corpus.
We also create an auxiliary corpus of sequences of
two PPs, where each data item consists of verb,
direct object and the two following PPs. This cor-
pus is only used to estimate the feature Iterativ-
ity. All the data was newly extracted from the
Penn Tree-bank. Our goal was to create a more
comprehensive and possibly more accurate corpus
than existing ones (Merlo et al., 1997; Collins and
Brooks, 1995). To improve coverage, we extracted
all cases of PPs following transitive, and intran-
sitive verbs and following nominal phrases. We
include also passive sentences and sentences con-
taining a sentential object. To improve accuracy,
attention was paid not to extract overlapping data,
contrary to counts in previous corpora, where mul-
tiple PP sequences were counted more than once,
each time as part of a different structural configu-
ration.
The Counts The linguistic diagnostics illus-
trated in the previous section are approximated by
corpus counts based on the extracted tuples.
Head dependence is approximated by the en-
tropy of the distribution of the verb or noun heads
for each PP, as indicated in (4).
</bodyText>
<equation confidence="0.996879">
Oki) tog C(h) (4)
H(PP) E,C(h) 2C(h)
</equation>
<bodyText confidence="0.99610975">
We implement also some more general variants,
where PP-internal nouns and head nouns are re-
placed by their WordNet 1.7 class (Miller et al.,
1990). Polysemous nouns are disambiguated by
selecting the most frequent WordNet sense.
Optionality is captured by the conditional prob-
ability of a PP given a particular verbal head, as
indicated in (5).
</bodyText>
<footnote confidence="0.9393945">
3For more detail on the features and experiments, please
see (Esteve-Ferrer and Merlo, 2002).
</footnote>
<equation confidence="0.996831">
C (v , PP)
P(PP1v) (v) (5)
</equation>
<bodyText confidence="0.9998701">
Analogously to the measure of head depen-
dence, optionality is also measured in general vari-
ants that rely on verb and noun classes, based on
WordNet 1.7.
Iterativity and ordering are approximated by
collecting counts indicating the proportion of
cases in which a given PP in first position had been
found in second position in a sequence of multiple
PPs over the total of PPs in second position, as in-
dicated in (6).
</bodyText>
<equation confidence="0.995529333333333">
C (PP)2
P(ADJI(PP)i) (6)
E,C(PP,)2
</equation>
<bodyText confidence="0.999183777777778">
The problem of sparse data here is especially se-
rious, because of the small frequencies of multiple
PPs. We estimate this measure in a single variant
using a backed-off estimation (Katz, 1987), where
we replace lexical items by their WordNet classes.
Copular paraphrase is captured by calculating
the proportion of times a given PP follows a copu-
lar verb over all the times it appears following any
verb.
</bodyText>
<equation confidence="0.985985">
P(ADJI(PP)) C (copular, &lt; PP) (7)
EiC(vi &lt; PP)
</equation>
<bodyText confidence="0.9998767">
This measure is an approximation, since we
don&apos;t know whether the copular verb is indeed in
a relative clause or not. Here again, we back-off
to the noun classes of the PP-internal noun, to ad-
dress the problem of sparse data.
The diagnostic of deverbal nouns is imple-
mented as a binary feature that simply indicates
if the PP follows a deverbal noun or not. Dever-
bal nouns are identified by inspecting their mor-
phology (Quirk et al., 1985). The suffixes that
can combine with verb bases to form deverbal
nouns are -ant (inhabitant), -ee (appointee), -er, or
(singer), -age (breakage), -al (refusal), -ion (ex-
ploration), -sion (invasion), -ing (building), -ment
(arrangement).
In conclusion, the linguistic diagnostics can be
approximated by simple statistical indicators es-
timated in a sufficiently large corpus. Once the
counts are collected they constitute the input to an
automatic classifier.
</bodyText>
<page confidence="0.993205">
254
</page>
<table confidence="0.984495333333333">
-CLR dative object if dative shift not possible(e.g. do-
nate); phrasal verbs; predication adjuncts
-DTV dative object if dative shift possible (e.g. give)
-BNF benefactive (dative object offor)
-PRD non VP predicates
-PUT locative complement of put
-LGS logical subjects in passives
-DIR direction and trajectory
-LOC location
-MNR manner
-PRP purpose and reason
-TMP temporal phrases
</table>
<figureCaption confidence="0.9901895">
Figure 1: Grammatical function and semantic tags
that involve PP constituents in the PTB
</figureCaption>
<bodyText confidence="0.999741842105264">
The Target Attribute Since we are planning to
use a supervised learning method, we need to la-
bel each example with a four-valued target at-
tribute (the values are Narg, Nadj, Varg, Vadj).
The Penn Treebank annotation does not explic-
itly make the distinction between arguments and
adjuncts. Information about this difference then
must be gleaned from the semantic and function
tags that have been assigned to the nodes (Bies et
al., 1995). Figure 1 illustrates the tags that involve
PP constituents. Based on the guidelines (Marcus
et al., 1994, 4),(Bies et al., 1995, 12), inspection
of the actual annotation in the Tree bank, and dis-
cussions in the literature (Quirk et al. 1985, sec-
tions 8.27-35, 15.22, 16-48), we mapped PPs into
arguments and adjuncts as follows: Adjuncts: All
PPs tagged with a semantic tag (DIR, LOC, MNR,
PRP, TMP). Arguments: All untagged PPs or PPs
tagged with CLR, PUT, DTV, BNF, PRD or LGS.
</bodyText>
<subsectionHeader confidence="0.987075">
4.2 The Method
</subsectionHeader>
<bodyText confidence="0.973212475409836">
The Input Data Each input vector represents an
instance of a PP attachment, which could be both
noun or verb attached, either as an argument or as
an adj unct.4 Each vector contains 20 training fea-
tures. They comprise the four lexical heads and
their WordNet classes, all the different variants of
the implementation of the diagnostics, and one 4-
valued target feature, indicating the type of attach-
ment.
4Notice however that the learning features were calculated
on all the instances described above, so in practice we use
both the unambiguous cases and ambiguous cases in the esti-
mation of the features of the ambiguous cases.
Experimental Settings We use the C5.0 Deci-
sion Tree Induction Algorithm (Quinlan, 1992),
applied to a training and testing corpus contain-
ing 13906 exemplars, of which we used 90% for
training and 10% for testing. The test sets were se-
lected by stratified sampling. (Nine samples were
created.)
Clearly, to classify PPs into four classes, we
have two options: we can construct a single four-
class classifier or we can build a sequence of bi-
nary classifiers. The discrimination between noun
and verb attachment can be performed first, and
then further refined into attachment as argument
or adjunct, performing the 4-way classification in
two steps. The two-step approach would be the
natural way of extending current PP attachment
disambiguation methods to the more specific 4-
way attachment we propose here. However, based
on exploratory data analysis and general wisdom
in machine learning, there is reason to believe that
it is better to solve the 4-way classification prob-
lem directly rather than first solving a more gen-
eral problem and then specialise the classification.
To test these expectations, we performed both
kinds of experiments: a direct 4-way classifica-
tion experiment, and a two-step classification ex-
periment, to investigate which of the two meth-
ods is better. The direct four-way classification
uses the attributes described above to build a sin-
gle classifier. For comparability, we created a two-
step experimental setup as follows. We created
three binary classifiers. The first one performs the
noun-verb attachment classification. Its learning
features comprise the four lexical heads and their
WordNet classes. We also train two classifiers
that learn to distinguish arguments from adjuncts.
One classifier is trained only on verb-attachment
exemplars and uses only the verb attachment re-
lated features. The third classifier is trained only
on noun-attachment exemplars, and utilises only
the noun attachment related features The test data
is first given to the noun-verb attachment classi-
fier. Then, the test examples classified as verbs
are given to the verb argument-adjunct classifier,
and the test examples classified as nouns are given
to the noun argument-adjunct classifier. Thus, this
cascade of classifiers performs the same task as the
4-way classifier, but it does so in two passes.
</bodyText>
<page confidence="0.997483">
255
</page>
<table confidence="0.99174525">
% Accuracy (% Error reduction)
Task Base All fts Bst+w Bst+w+c
2+2 65.3 70.9(16) 69.9(13) 71.1(17)
4way 66.2 72.7(19) 73.9(23) 73.9(23)
</table>
<tableCaption confidence="0.906593666666667">
Table 1: Percent accuracy (percent error reduc-
tion) using combination of features in the two dif-
ferent experimental settings
</tableCaption>
<bodyText confidence="0.998023769230769">
Each binary classifier reaches good or even the
state of the art accuracy for these tasks. Specifi-
cally, the classifier disambiguating the noun-verb
attachment reaches an accuracy of 80.2% (base-
line 71.6%. using only the preposition) using in-
formation about argumenthood and 77.2% if the
decision tree induction is performed exclusively
on the basis of words and classes. The classifier
that distinguishes verb arguments from verb ad-
juncts performs at an accuracy of 81.1% (baseline
72.3%), while the discrimination of noun argu-
ments from noun adjuncts reaches an accuracy of
89.8% (the baseline is already very high, 88.4%.)
</bodyText>
<sectionHeader confidence="0.994823" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999971888888889">
Tables 1 reports the classification accuracy of the
two comparative experiments performed. All re-
ported numbers are averages of accuracies on 9
different data samples. The first column reports
the baseline accuracy for both tasks, calculated by
performing the classification using only the fea-
ture preposition. The second column shows the
results of an experiment where all the features de-
scribed above were used for the classification. The
third column reports the results in which only the
lexical heads and most effective features — deter-
mined experimentally — were used. The fourth col-
umn reports the results in which the lexical heads
and their classes together with the most effective
features were used. For the two-step classification
experiment, we determined the best feature for
each classifier independently. We observe that the
4-way classification is better in both cases, even if
the two-step method decomposes the problem in
simpler tasks.
The degradation in performance of the two-step
approach compared to the 4-way classification is
not unexpected and is likely due to the inappro-
priate underlying assumptions about the distribu-
tion of the data. A two-step approach assumes that
the preferred attachment in the first step subsumes
the preferred attachment in the second step. For
example, it assumes that a given PP can first be
classified as attached to the verb and then refined
as a verb argument. Numerically, this assumption
is only verified if the largest proportion of cases
over the four possible attachments is a subset of
the larger proportion of cases in the disambigua-
tion between noun and verb attachment. In gen-
eral, this assumption holds if the distribution of
cases is very skewed. But in some case it does not
hold. For example, consider a hypothetical am-
biguous PP sequence, distributed across the four
possible outcomes with the following proportions:
N-arg .25; N-adj .35; V-arg .4; V-adj 0. As can be
easily seen, a two-step approach relying directly
on the proportion of cases in the data would clas-
sify this sequence as an NP attachment at first (.25
+ .35 = .6&gt; .4), assigning it to the N-adj class in a
second step. However, the most likely assignment
in a 4-way classification, and the correct one, is
that the PP is an argument of the verb.
Detailed data analysis of our corpus indicates
that some n-way ambiguities exist and that in these
cases even distributions of attachments, though
rare, do arise in practice. Multiple ambiguities
arise, not at the level of word sequences, but at
the level of the abstract representations used to
cope with sparse data. For example, they arise for
PPs represented as a preposition and the semantic
class of the PP-internal noun. For such abstract
representations, one can find a few instances of
the unskewed distribution illustrated above. For
these cases, a two-step approach would favour the
wrong solution. Consider the case of the PP con-
sisting of the preposition at and the WordNet class
14 (group), whose relative frequencies in the Penn
Treebank are N-arg .12, N-adj .41, V-arg 0, V-
adj .47. A two-step approach would first choose
a noun attachment, and further refine the choice to
adjunct, while the most frequent case is the verb
adjunct attachment case. Another similar exam-
ple, is the case of the PP consisting of the prepo-
sition from and the WordNet class 4 (artifact). In
this instance, a two-step approach would favour a
verb attachment at first, further refined into argu-
ment attachment, while the preferred attachment is
</bodyText>
<page confidence="0.996103">
256
</page>
<table confidence="0.984078">
F-scores (%)
Task Nadj Narg Vadj Varg
4-way 47 86 63 56
2+2 51 86 54 47
</table>
<tableCaption confidence="0.8525615">
Table 2: Percent F-scores using best features in the
two different experimental settings
</tableCaption>
<bodyText confidence="0.999878488372093">
as noun argument. (Relative frequencies are N-arg
.47, N-adj 0, V-arg .44, V-adj .09.)
A more refined analysis of the errors of our
classifiers confirms this interpretation. Table 2
shows the F-scores for a sample used in the ex-
periment reported in the fourth column of Table 1.
As can be seen, the 4-way classification is a little
worse for noun attachments (due to worse recall
of noun adjuncts) but better for the verb attach-
ments. The distribution of the errors reveals that
improvements can be observed in distinguishing
noun arguments and verb arguments — clearly an
instance in which the actual attachment site does
not subsume the assigned attachment. This result
confirms that it is misleading to formalise PP at-
tachment as a binary problem, assuming that a dis-
tinction between arguments and adjuncts can be
performed later, if necessary, without loss of accu-
racy.
The 4-way classification also reveals a little im-
provements in the discrimination of verb argu-
ments from verb adjuncts (especially fewer verb
arguments misclassified as adjuncts). Improve-
ments in the precision of verb arguments is par-
ticularly relevant, as this class of attachments is
crucial in supporting further language processing
tasks, which usually require precise knowledge of
a verb&apos;s subcategorization frame.
In conclusion, these results show that good ac-
curacy on a fine-grained and informative classifi-
cation of PPs can be achieved using corpus counts.
Moreover, classification of PPs performed with a
single classifier is more accurate than sequencing
binary classifiers: by tackling the 4-way discrimi-
nation problem directly, the approach does not rely
on any assumption on the distribution of the data,
and thereby reaches better accuracy. Finally, a re-
sult of particular interest is the improved discrimi-
nation of verb arguments from verb adjuncts com-
pared to a two-step approach, a promising result
for all the numerous NLP tasks and applications
that rely on the correct identification of a verb&apos;s
subcategorization frame.
</bodyText>
<sectionHeader confidence="0.999967" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999952833333334">
As far as we are aware, this is the first attempt
to integrate the notion of argumenthood in a more
comprehensive formulation of the problem of dis-
ambiguating the attachment of PPs. Hindle and
Rooth (1993) mention the interaction between the
structural and the semantic factors in the disam-
biguation of a PP, indicating that verb adjuncts are
the most difficult. We confirm their finding that
noun arguments are more easily identified, while
verb complements (either arguments or adjuncts)
are more difficult.
Few previous pieces of work attempt to dis-
tinguish arguments from adjuncts automatically
(Buchholz, 1999; Merlo and Leybold, 2001). We
extend here (Merlo and Leybold, 2001) by elab-
orating more learning features, refining all the
counting methods and extending the method to
noun attachment, which had not been consid-
ered before, thus validating and extending the ap-
proach. The current work on binary argument-
adjunct classifiers compares favourably to the only
other comparable study on this topic (Buchholz,
1999). Buchholz reports an accuracy of 77%
for the argument-adjunct distinction of PPs, to be
compared to our 81% and 89% for verb and noun
attachments respectively. Buchholz considers all
types of attachment sites, not just verbs and nouns.
Recently (Villavicencio, 2002) has explored the
performance of an argument identifier, developed
in the framework of a model of child language
learning. The approach is not directly compara-
ble, as it is not entirely corpus-based (the input
to the algorithm is an impoverished logical form),
and the evaluation is on a smaller scale than the
present work. Other pieces of work address the
current problem in the larger perspective of dis-
tinguishing arguments from adjuncts for subcate-
gorization acquisition (Korhonen, 2002; Aldeza-
bal et al., 2002). Our work confirms the results
reported in (Korhonen, 2002), which indicate that
using word classes improves the extraction of sub-
categorisation frames.
</bodyText>
<page confidence="0.994119">
257
</page>
<sectionHeader confidence="0.999543" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999967466666667">
We have proposed a reformulation of the problem
of PP attachment as a 4-way disambiguation prob-
lem, arguing that what is needed in interpreting
prepositional phrases is knowledge about both the
structural attachment site - the traditional noun-
verb attachment distinction - and the nature of the
attachment - the distinction of arguments from ad-
juncts. Practically, we have shown that a 4-way
classifier which solves the complete problem di-
rectly performs better than solving a sequence of
binary decisions. Future work lies in further inves-
tigating the difference between arguments and ad-
juncts to achieve even finer-grained classifications
and to model more precisely the semantic core of
a sentence.
</bodyText>
<sectionHeader confidence="0.999506" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998730333333333">
This research was made possible by Swiss NSF
grant no. 11-65328.01. I would like to thank Eva
Esteve Ferrer for her collaboration.
</bodyText>
<sectionHeader confidence="0.999414" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999911320512821">
Izaskun Aldezabal, Maxux Aranzabe, Koldo Gojenola, Kepa
Sarasota, and Aitziber Atutxa. 2002. Learning argu-
ment/adjunct dictinction for Basque. In Procs of the
SIGLEX Workshop on Unsupervised Lexical Acquisition,
pages 42-50, Philadelphia,PA, July.
Ann Bies, M. Ferguson, K.Katz, and Robert MacIntyre.
1995. Bracketing guidelines for Treebank II style. Tech-
nical report, University of Pennsylvania.
Sabine Buchholz. 1999. Distinguishing complements from
adjuncts using memory-based learning. ILK, Computa-
tional Linguistics, Tilburg University.
Michael Collins and James Brooks. 1995. Prepositional
Phrase Attachment through a Backed-Off Model. In Procs
of the Third Workshop on Very Large Corpora, pages 27-
38, Cambridge, MA.
Bonnie Don. 1997. Large-scale dictionary construction for
foreign language tutoring and interlingual machine trans-
lation. Machine Translation, 12(4):1-55.
Eva Esteve-Ferrer and Paola Merlo. 2002. Automatic dis-
tinction of PP arguments and adjuncts. Technical report,
MALA Project 1, University of Geneva.
Jane Grimshaw. 1990. Argument Structure. MIT Press.
Donald Hindle and Mats Rooth. 1993. Structural ambi-
guity and lexical relations. Computational Linguistics,
19(1):103-120.
Ray Jackendoff. 1977. X&apos; Syntax: A Study of Phrase Struc-
ture. MIT Press, Cambridge, MA.
S.M. Katz. 1987. Estimation of probabilities from sparse
data for the language model component of a speech recog-
nizer. IEEE Transactions on Acoustic, Speech and Signal
Processing, 35(3):400-401.
Anna Korhonen. 2002. Semantically motivated subcatego-
rization acquisition. In Procs of the SIGLEX Workshop on
Unsupervised Lexical Acquisition, pages 51-58, Philadel-
phia,PA, July.
Beth Levin. 1993. English Verb Classes and Alternations.
University of Chicago Press, Chicago, IL.
A. Marantz. 1984. On the Nature of Grammatical Relations.
MIT Press, Cambridge, MA.
M. Marcus, G. Kim, A. Marcinkiewicz, R.Macintyre,
A. Bies, M. Ferguson, K.Katz, and B.Schasberger. 1994.
The Penn Treebank: Annotating argument structure.
Technical report, University of Pennsylvania.
Paola Merlo and Matthias Leybold. 2001. Automatic dis-
tinction of arguments and modifiers: the case of preposi-
tional phrases. In Procs of the Fifth Computational Nat-
ural Language Learning Workshop (C0NLL-2001 ), pages
121-128, Toulouse, France.
Paola Merlo, Matt Crocker, and Cathy Berthouzoz. 1997.
Attaching multiple prepositional phrases: Generalized
backed-off estimation. In Procs of the Second Conference
on Empirical Methods in Natural Language Processing,
pages 145-154, Providence, RI.
George Miller, R. Beckwith, C. Fellbaum, D. Gross, and
K. Miller. 1990. Five papers on Wordnet. Technical re-
port, Cognitive Science Lab, Princeton University.
Carl Pollard and Ivan Sag. 1987. An Information-based Syn-
tax and Semantics, volume 13. CSLI lecture Notes, Stan-
ford University.
J. Ross Quinlan. 1992. C4.5: Programs for Machine Learn-
ing. Series in Machine Learning. Morgan Kaufmann, San
Mateo, CA.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech, and
Jan Svartvik. 1985. A Comprehensive Grammar of the
English Language. Longman, London.
Adwait Ratnaparkhi, Jeffrey Reynar, and Salim Roukos.
1994. A Maximum Entropy Model for Prepositional
Phrase Attachment. In Procs of the ARPA Workshop on
Human Language Technology, pages 250-255.
Carson T. Schiitze. 1995. PP Attachment and Argument-
hood. MIT Working Papers in Linguistics, 26:95-151.
Bangalore Srinivas and Aravind K. Joshi. 1999. Supertag-
ging: An approach to almost parsing. Computational Lin-
guistics, 25(2):237-265.
Aline Villavicencio. 2002. Learning to distinguish PP argu-
ments from adjuncts. In Procs of the 6th Conference on
Natural Language Learning (CoNLL-2002), pages 84-90,
Taipei,Taiwan.
</reference>
<page confidence="0.996135">
258
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000277">
<title confidence="0.9989255">Generalised PP-Attachment Disambiguation using Corpus-based Linguistic Diagnostics</title>
<author confidence="0.999497">Paola Merlo</author>
<affiliation confidence="0.965199666666667">Linguistics Department University of Geneva 2 rue de Candolle</affiliation>
<address confidence="0.995123">1211 Geneva 4, Switzerland</address>
<email confidence="0.981903">merlo@lettres.unige.ch</email>
<abstract confidence="0.992667822429908">We propose a new formulation of the PP attachment problem as a 4-way classification which takes into account the argument or adjunct status of the PP. Based on linguistic diagnostics, we train a 4-way classifier that reaches an average accuracy of 73.9% (baseline 66.2%). Compared to a sequence of binary classifiers, the 4-way classifier reaches better performance and individuates a verb&apos;s arguments more accurately, thus improving the acquisition of a crucial piece of information for many NLP applications. 1 Motivation Incorrect attachment of prepositional phrases often constitutes the main source of errors in current parsing systems. Correct attachment of PPs is necessary to construct a parse tree which will support the proper interpretation of constituents in the sentence. Consider the time-worn example I saw the man with the telescope is important to determine if the PP the to be attached as a sister to the noun man, its interpretation, or if it is to be attached to the verb, thereby indicating the instrument of the main action described by the sentence. Based on examples of this sort, recent approaches have formalised the problem of disambiguating PP attachments as a binary choice, distinguishing between attachment of a PP to a given verb or to the verb&apos;s direct object (Ratnaparkhi et al., 1994; Collins and Brooks, 1995). This is, however, a simplification of the problem, which does not take the nature of the attachment into account. Precisely, it does not distinguish PP arguments from PP adjuncts. Consider the following example, which contains two PPs, both modifying the verb. Put the block on the table in the morning The first PP is a locative PP required by the subframe of the verb the an optional descriptor of the time at which the action was performed. Though both attached to the verb, the two PPs entertain different relationships with the verb — the first is an argument while the latter is an adjunct. Analogous examples could be built for attachments to the noun. Is it important to model not just the site but also the nature of the attachment of a PP into the tree structure? We would like to claim that it is. Distinguishing arguments from adjuncts is key to identifying the semantic kernel of a sentence. Extracting the core meaning of a sentence or phrase, in turn, is necessary for automatic acquisition of important lexical knowledge, such as subcategorisation frames and argument structures, which is used in several NLP tasks and applications, such as parsing or machine translation (Srinivas and Joshi, 1999; Don, 1997). Moreover, from a quantitative point of view, arguments and adjuncts have different statistical properties, requiring different statistical techniques. For example, (Hindle and Rooth, 1993) clearly indicate that their lexical association technique performs much better for arguments than for adjuncts, whether the attachment is to the verb or to the noun. Researchers have abstracted away from this distinction, because identifying arguments and adjuncts is a notoriously difficult task, taxing many 251 native speakers&apos; intuitions and requiring complex world knowledge. The usual expectation has been that this discrimination is not amenable to a corpus-based treatment. In recent work, however, we succeed in distinguishing arguments from adjuncts using evidence extracted from a parsed corpus (Merlo and Leybold, 2001). Our method develops corpus-based statistical correlates for the diagnostics used in linguistics to decide whether a PP is an argument or an adjunct. A numerical vectorial representation of the notion of argumenthood is provided, which supports automatic classification, reaching 86% accuracy. In the current paper, we extend this work and propose a new formulation of the PP attachment problem. We treat PP attachment as a 4-way classification of PPs into noun argument PPs, noun adjunct PPs, verb argument PPs, and verb adjunct PPs. We show that it is possible to build a classifier that solves this problem using corpus evidence, with good accuracy (74%). This classifier solves the classification problem directly, in one step. Interestingly, we show that a 4-way classifier reaches better accuracy than a two-step sequence of binary classifiers, which first solve the noun-verb attachment problem and then refine the attachment decision into argument or adjunct. This result indicates that the current formulation of the PP attachment problem cannot be considered a first step in the solution of the final 4-way discrimination task. Finally, we note that the improvement is especially due to a better recognition of verbs&apos; arguments, thus providing more accurate information to many NLP tasks and applications. Solving this novel 4-way classification task crucially relies on the ability to distinguish arguments from adjuncts using corpus counts. 2 A Novel Method to Distinguish Arguments from Adjuncts Few attempts have been made to distinguish arguments from adjuncts automatically (Buchholz, 1999; Merlo and Leybold, 2001; Villavicencio, 2002; Aldezabal et al., 2002). The core difficulty in this enterprise is to define the notion of argument precisely. There is a consensus in linguistics that arguments and adjuncts are different both with respect to their function in the sentence and in the way they themselves are interpreted (Jackendoff, 1977; Marantz, 1984; Pollard and Sag, 1987; Grimshaw, 1990). With respect to their function, an argument fills a role in the relation described by its associated head, while an adjunct predicates a separate property of its associate head or phrase. With respect to their interpretation, a complement is an argument if its interpretation depends exclusively on the head with which it is associated, while it is an adjunct if its interpretation remains relatively constant when associating with different heads (Grimshaw, 1990, 108). Restricting the discussion to PPs, these differences are illustrated in the following examples (PP-argument in bold), see also (Schiitze, 1995, 100). a) Kim camps/jogs/meditates on Sunday b) Kim depended on Sandy example a) the PP Sunday be construed without any reference to the preceding part of the sentence, and it preserves its meaning even when combining with different heads. This is, however, not the case for b). Here, the PP can only be properly understood in connection with the rest of the sentence: Sandy is the person on whom someone depends. These semantic distinctions surface in observable syntactic differences, giving rise to a set of linguistic diagnostics to determine whether a PP is an adjunct or an argument. We illustrate here those countable diagnostics that can be approximated statistically and estimated using corpus counts, thus combining linguistic insight with the robustness of corpus-based methods. 3 The Linguistic Diagnostics Many diagnostics for argumenthood have been proposed in the literature (Schiitze, 1995). Some of them require complex syntactic manipulation of the sentence, such as wh-extraction, and are therefore too difficult to apply automatically. We extend our previous work (Merlo and Leybold, 2001) and choose six diagnostics that can be captured by simple corpus counts: head dependence, optionality, iterativity, ordering, copular paraphrase, and deverbal nominalisation. These diagnostics tap into the deeper semantic properties that distinguish arguments from adjuncts. 252 Head Dependence Arguments depend on their lexical heads, because they form an integral part of the phrase. Adjuncts do not. Consequently, PParguments can only appear with the specific verbal or nominal head by which they are lexically selected, while PP-adjuncts can co-occur with a far greater range of different heads than arguments (Pollard and Sag, 1987, 136), as illustrated in the example sentences below (PP-argument in bold). a) a man/woman/scarecrow with gray hair b) a student/*punk/*watermelon of physics We capture this insight by measuring the dispersion of the distribution over the different verbs or nouns that co-occur with a given PP in a corpus. We expect adjunct PPs to have higher dispersion than argument PPs. Differently from our previous simpler implementation (Merlo and Leybold, 2001), we use entropy as a measure of the disperof the distribution, as indicated in (1) indicates the noun or verb head to which the PP is attached). = Optionality In most cases of verb attachment,&apos; PP-arguments are obligatory elements of a given sentence whose absence leads to ungrammaticality, while adjuncts do not contribute to the semantics of any particular verb, hence they are optional, illustrated in the following examples: 2 a) John put the book in the room b) *John put the book c) John saw/read the book in the room d) John saw/read the book The notion of optionality can be captured by the conditional probability of a PP given a particular head, &apos;We do not compute this measure for nominal heads as PP are always optional when governed by a nominal head. that this diagnostics can only be interpreted as a statistical tendency, and not as a strict test, because not all arguments are obligatory (but all adjuncts are indeed optional). The best known descriptive exceptions to the criterion of optionality are the class of so-called object-drop verbs (Levin, 1993) and, arguably, instrumental verbs (Schatze, 1995). While keeping these exceptions in mind, we maintain optionality as a valid diagnostic here. Iterativity and Ordering Arguments cannot be iterated and they must be adjacent to the selecting lexical head. Neither of these two restrictions apply to adjuncts, as illustrated in the examples below. a) *Chris rented the gazebo to girls, to boys b) Kim met Sandy in Baltimore in the hotel lobby in a corner Thus, the probability of a PP being an adjunct can be approximated as the probability of its occurrence in second position in a sequence of PPs, as indicated in (2). (2) Copular Paraphrase The diagnostic of copular paraphrase is specific to the distinction of NPs arguments and adjuncts (Schiitze, 1995, 103). NP arguments cannot be paraphrased by a copular relative clause (examples b and b&apos;), while adjuncts can (examples a and a&apos;). a) a man from Paris a&apos;) a man who was from Paris b) the weight of the cow b&apos;) *the weight that was of the cow Thus, the probability that a PP is an adjunct can be approximated by the probability of its occurfollowing a copular verb, such as beappear, seem, remain et al., 1985), as indicated in (3). P(copulary &lt; PP) Deverbal Nouns This diagnostic is based on the observation that PPs following a deverbal noun are likely to be arguments. This diagnostic can be captured by a probability indicators function, that assigns probability 1 of being an argument to PPs following a deverbal noun and 0 otherwise. In conclusion, the different properties of arguments and adjuncts can be reduced to surface indicators, which can be estimated by appropriate corpus counts. 4 Experiments The success and generality of corpus-based classifier induction rests in large part on the accurate 253 estimation of the feature vectors used for training. explain the details of our methodology 4.1 The Materials construct two corpora comprising examples of PP sequences. A PP is a preposition and head noun sequence. One corpus contains data encoding information for attachment of single PPs in the form of four head words (verb, object noun, preposition and PP-internal noun) for each instance of PP attachments found in the corpus. We also create an auxiliary corpus of sequences of two PPs, where each data item consists of verb, direct object and the two following PPs. This corpus is only used to estimate the feature Iterativity. All the data was newly extracted from the Penn Tree-bank. Our goal was to create a more comprehensive and possibly more accurate corpus than existing ones (Merlo et al., 1997; Collins and Brooks, 1995). To improve coverage, we extracted all cases of PPs following transitive, and intransitive verbs and following nominal phrases. We include also passive sentences and sentences containing a sentential object. To improve accuracy, attention was paid not to extract overlapping data, contrary to counts in previous corpora, where multiple PP sequences were counted more than once, each time as part of a different structural configuration. Counts linguistic diagnostics illustrated in the previous section are approximated by corpus counts based on the extracted tuples. Head dependence is approximated by the entropy of the distribution of the verb or noun heads for each PP, as indicated in (4). C(h)(4) H(PP)E,C(h) We implement also some more general variants, where PP-internal nouns and head nouns are replaced by their WordNet 1.7 class (Miller et al., 1990). Polysemous nouns are disambiguated by selecting the most frequent WordNet sense. Optionality is captured by the conditional probability of a PP given a particular verbal head, as indicated in (5). more detail on the features and experiments, please see (Esteve-Ferrer and Merlo, 2002). C (v , PP) (v)(5) Analogously to the measure of head dependence, optionality is also measured in general variants that rely on verb and noun classes, based on WordNet 1.7. Iterativity and ordering are approximated by collecting counts indicating the proportion of cases in which a given PP in first position had been found in second position in a sequence of multiple PPs over the total of PPs in second position, as indicated in (6). E,C(PP,)2 The problem of sparse data here is especially serious, because of the small frequencies of multiple PPs. We estimate this measure in a single variant using a backed-off estimation (Katz, 1987), where we replace lexical items by their WordNet classes. Copular paraphrase is captured by calculating the proportion of times a given PP follows a copular verb over all the times it appears following any verb. P(ADJI(PP)) (copular, &lt; PP)(7) &lt; This measure is an approximation, since we don&apos;t know whether the copular verb is indeed in a relative clause or not. Here again, we back-off to the noun classes of the PP-internal noun, to address the problem of sparse data. The diagnostic of deverbal nouns is implemented as a binary feature that simply indicates if the PP follows a deverbal noun or not. Deverbal nouns are identified by inspecting their morphology (Quirk et al., 1985). The suffixes that can combine with verb bases to form deverbal are or (ex- (arrangement). In conclusion, the linguistic diagnostics can be approximated by simple statistical indicators estimated in a sufficiently large corpus. Once the counts are collected they constitute the input to an automatic classifier. 254 CLR dative object if dative shift not possible(e.g. verbs; predication adjuncts dative object if dative shift possible (e.g. BNF benefactive (dative object offor) -PRD non VP predicates locative complement of -LGS logical subjects in passives -DIR direction and trajectory -LOC location -MNR manner -PRP purpose and reason -TMP temporal phrases Figure 1: Grammatical function and semantic tags that involve PP constituents in the PTB Target Attribute we are planning to use a supervised learning method, we need to label each example with a four-valued target attribute (the values are Narg, Nadj, Varg, Vadj). The Penn Treebank annotation does not explicitly make the distinction between arguments and adjuncts. Information about this difference then must be gleaned from the semantic and function tags that have been assigned to the nodes (Bies et al., 1995). Figure 1 illustrates the tags that involve PP constituents. Based on the guidelines (Marcus et al., 1994, 4),(Bies et al., 1995, 12), inspection of the actual annotation in the Tree bank, and discussions in the literature (Quirk et al. 1985, sections 8.27-35, 15.22, 16-48), we mapped PPs into arguments and adjuncts as follows: Adjuncts: All tagged with a semantic tag MNR, PRP, TMP). Arguments: All untagged PPs or PPs with CLR, PUT, DTV, BNF, LGS. 4.2 The Method Input Data input vector represents an instance of a PP attachment, which could be both noun or verb attached, either as an argument or as adj Each vector contains 20 training features. They comprise the four lexical heads and their WordNet classes, all the different variants of the implementation of the diagnostics, and one 4valued target feature, indicating the type of attachment. however that the learning features were calculated on all the instances described above, so in practice we use both the unambiguous cases and ambiguous cases in the estimation of the features of the ambiguous cases. Settings use the C5.0 Decision Tree Induction Algorithm (Quinlan, 1992), applied to a training and testing corpus containing 13906 exemplars, of which we used 90% for training and 10% for testing. The test sets were selected by stratified sampling. (Nine samples were created.) Clearly, to classify PPs into four classes, we have two options: we can construct a single fourclass classifier or we can build a sequence of binary classifiers. The discrimination between noun and verb attachment can be performed first, and then further refined into attachment as argument or adjunct, performing the 4-way classification in two steps. The two-step approach would be the natural way of extending current PP attachment disambiguation methods to the more specific 4way attachment we propose here. However, based on exploratory data analysis and general wisdom in machine learning, there is reason to believe that it is better to solve the 4-way classification problem directly rather than first solving a more general problem and then specialise the classification. To test these expectations, we performed both kinds of experiments: a direct 4-way classification experiment, and a two-step classification experiment, to investigate which of the two methods is better. The direct four-way classification uses the attributes described above to build a single classifier. For comparability, we created a twostep experimental setup as follows. We created three binary classifiers. The first one performs the noun-verb attachment classification. Its learning features comprise the four lexical heads and their WordNet classes. We also train two classifiers that learn to distinguish arguments from adjuncts. One classifier is trained only on verb-attachment exemplars and uses only the verb attachment related features. The third classifier is trained only on noun-attachment exemplars, and utilises only the noun attachment related features The test data is first given to the noun-verb attachment classifier. Then, the test examples classified as verbs are given to the verb argument-adjunct classifier, and the test examples classified as nouns are given to the noun argument-adjunct classifier. Thus, this cascade of classifiers performs the same task as the 4-way classifier, but it does so in two passes. 255 % Accuracy (% Error reduction) Task Base All fts Bst+w Bst+w+c 2+2 65.3 70.9(16) 69.9(13) 71.1(17) 4way 66.2 72.7(19) 73.9(23) 73.9(23) Table 1: Percent accuracy (percent error reduction) using combination of features in the two different experimental settings Each binary classifier reaches good or even the state of the art accuracy for these tasks. Specifically, the classifier disambiguating the noun-verb attachment reaches an accuracy of 80.2% (baseline 71.6%. using only the preposition) using information about argumenthood and 77.2% if the decision tree induction is performed exclusively on the basis of words and classes. The classifier that distinguishes verb arguments from verb adjuncts performs at an accuracy of 81.1% (baseline 72.3%), while the discrimination of noun arguments from noun adjuncts reaches an accuracy of 89.8% (the baseline is already very high, 88.4%.) 5 Results and Discussion Tables 1 reports the classification accuracy of the two comparative experiments performed. All reported numbers are averages of accuracies on 9 different data samples. The first column reports the baseline accuracy for both tasks, calculated by performing the classification using only the feasecond column shows the results of an experiment where all the features described above were used for the classification. The third column reports the results in which only the lexical heads and most effective features — determined experimentally — were used. The fourth column reports the results in which the lexical heads and their classes together with the most effective features were used. For the two-step classification experiment, we determined the best feature for each classifier independently. We observe that the 4-way classification is better in both cases, even if the two-step method decomposes the problem in simpler tasks. The degradation in performance of the two-step approach compared to the 4-way classification is not unexpected and is likely due to the inappropriate underlying assumptions about the distribution of the data. A two-step approach assumes that the preferred attachment in the first step subsumes the preferred attachment in the second step. For example, it assumes that a given PP can first be classified as attached to the verb and then refined as a verb argument. Numerically, this assumption is only verified if the largest proportion of cases over the four possible attachments is a subset of the larger proportion of cases in the disambiguation between noun and verb attachment. In general, this assumption holds if the distribution of cases is very skewed. But in some case it does not hold. For example, consider a hypothetical ambiguous PP sequence, distributed across the four possible outcomes with the following proportions: N-arg .25; N-adj .35; V-arg .4; V-adj 0. As can be easily seen, a two-step approach relying directly on the proportion of cases in the data would classify this sequence as an NP attachment at first (.25 + .35 = .6&gt; .4), assigning it to the N-adj class in a second step. However, the most likely assignment in a 4-way classification, and the correct one, is that the PP is an argument of the verb. Detailed data analysis of our corpus indicates that some n-way ambiguities exist and that in these cases even distributions of attachments, though rare, do arise in practice. Multiple ambiguities arise, not at the level of word sequences, but at the level of the abstract representations used to cope with sparse data. For example, they arise for PPs represented as a preposition and the semantic class of the PP-internal noun. For such abstract representations, one can find a few instances of the unskewed distribution illustrated above. For these cases, a two-step approach would favour the wrong solution. Consider the case of the PP conof the preposition the WordNet class 14 (group), whose relative frequencies in the Penn Treebank are N-arg .12, N-adj .41, V-arg 0, Vadj .47. A two-step approach would first choose a noun attachment, and further refine the choice to adjunct, while the most frequent case is the verb adjunct attachment case. Another similar example, is the case of the PP consisting of the prepothe WordNet class 4 (artifact). In this instance, a two-step approach would favour a verb attachment at first, further refined into argument attachment, while the preferred attachment is 256 F-scores (%) Task Nadj Narg Vadj Varg 4-way 47 86 63 56 2+2 51 86 54 47 Table 2: Percent F-scores using best features in the two different experimental settings as noun argument. (Relative frequencies are N-arg .47, N-adj 0, V-arg .44, V-adj .09.) A more refined analysis of the errors of our classifiers confirms this interpretation. Table 2 shows the F-scores for a sample used in the experiment reported in the fourth column of Table 1. As can be seen, the 4-way classification is a little worse for noun attachments (due to worse recall of noun adjuncts) but better for the verb attachments. The distribution of the errors reveals that improvements can be observed in distinguishing noun arguments and verb arguments — clearly an instance in which the actual attachment site does not subsume the assigned attachment. This result confirms that it is misleading to formalise PP attachment as a binary problem, assuming that a distinction between arguments and adjuncts can be performed later, if necessary, without loss of accuracy. The 4-way classification also reveals a little improvements in the discrimination of verb arguments from verb adjuncts (especially fewer verb arguments misclassified as adjuncts). Improvements in the precision of verb arguments is particularly relevant, as this class of attachments is crucial in supporting further language processing tasks, which usually require precise knowledge of a verb&apos;s subcategorization frame. In conclusion, these results show that good accuracy on a fine-grained and informative classification of PPs can be achieved using corpus counts. Moreover, classification of PPs performed with a single classifier is more accurate than sequencing binary classifiers: by tackling the 4-way discrimination problem directly, the approach does not rely on any assumption on the distribution of the data, and thereby reaches better accuracy. Finally, a result of particular interest is the improved discrimination of verb arguments from verb adjuncts compared to a two-step approach, a promising result for all the numerous NLP tasks and applications that rely on the correct identification of a verb&apos;s subcategorization frame. 6 Related Work As far as we are aware, this is the first attempt to integrate the notion of argumenthood in a more comprehensive formulation of the problem of disambiguating the attachment of PPs. Hindle and Rooth (1993) mention the interaction between the structural and the semantic factors in the disambiguation of a PP, indicating that verb adjuncts are the most difficult. We confirm their finding that noun arguments are more easily identified, while verb complements (either arguments or adjuncts) are more difficult. Few previous pieces of work attempt to distinguish arguments from adjuncts automatically (Buchholz, 1999; Merlo and Leybold, 2001). We extend here (Merlo and Leybold, 2001) by elaborating more learning features, refining all the counting methods and extending the method to noun attachment, which had not been considered before, thus validating and extending the approach. The current work on binary argumentadjunct classifiers compares favourably to the only other comparable study on this topic (Buchholz, 1999). Buchholz reports an accuracy of 77% for the argument-adjunct distinction of PPs, to be compared to our 81% and 89% for verb and noun attachments respectively. Buchholz considers all types of attachment sites, not just verbs and nouns. Recently (Villavicencio, 2002) has explored the performance of an argument identifier, developed in the framework of a model of child language learning. The approach is not directly comparable, as it is not entirely corpus-based (the input to the algorithm is an impoverished logical form), and the evaluation is on a smaller scale than the present work. Other pieces of work address the current problem in the larger perspective of distinguishing arguments from adjuncts for subcategorization acquisition (Korhonen, 2002; Aldezabal et al., 2002). Our work confirms the results reported in (Korhonen, 2002), which indicate that using word classes improves the extraction of subcategorisation frames. 257 7 Conclusions We have proposed a reformulation of the problem as a 4-way disambiguation problem, arguing that what is needed in interpreting prepositional phrases is knowledge about both the structural attachment site the traditional nounverb attachment distinction and the nature of the attachment the distinction of arguments from adjuncts. Practically, we have shown that a 4-way classifier which solves the complete problem directly performs better than solving a sequence of binary decisions. Future work lies in further investigating the difference between arguments and adjuncts to achieve even finer-grained classifications and to model more precisely the semantic core of a sentence.</abstract>
<note confidence="0.834671146341464">Acknowledgments This research was made possible by Swiss NSF no. 11-65328.01. like to thank Eva Esteve Ferrer for her collaboration. References Izaskun Aldezabal, Maxux Aranzabe, Koldo Gojenola, Kepa Sarasota, and Aitziber Atutxa. 2002. Learning argudictinction for Basque. In of the SIGLEX Workshop on Unsupervised Lexical Acquisition, pages 42-50, Philadelphia,PA, July. Ann Bies, M. Ferguson, K.Katz, and Robert MacIntyre. 1995. Bracketing guidelines for Treebank II style. Technical report, University of Pennsylvania. Sabine Buchholz. 1999. Distinguishing complements from adjuncts using memory-based learning. ILK, Computational Linguistics, Tilburg University. Michael Collins and James Brooks. 1995. Prepositional Attachment through a Backed-Off Model. In the Third Workshop on Very Large Corpora, 27- 38, Cambridge, MA. Bonnie Don. 1997. Large-scale dictionary construction for foreign language tutoring and interlingual machine trans- Translation, Eva Esteve-Ferrer and Paola Merlo. 2002. Automatic distinction of PP arguments and adjuncts. Technical report, MALA Project 1, University of Geneva. Grimshaw. 1990. Structure. Press. Donald Hindle and Mats Rooth. 1993. Structural ambiand lexical relations. Linguistics, 19(1):103-120. Jackendoff. 1977. Syntax: A Study of Phrase Struc- Press, Cambridge, MA. S.M. Katz. 1987. Estimation of probabilities from sparse data for the language model component of a speech recog- Transactions on Acoustic, Speech and Signal Anna Korhonen. 2002. Semantically motivated subcategoacquisition. In of the SIGLEX Workshop on Lexical Acquisition, 51-58, Philadelphia,PA, July. Levin. 1993. Verb Classes and Alternations. University of Chicago Press, Chicago, IL. Marantz. 1984. the Nature of Grammatical Relations. MIT Press, Cambridge, MA. M. Marcus, G. Kim, A. Marcinkiewicz, R.Macintyre, A. Bies, M. Ferguson, K.Katz, and B.Schasberger. 1994. The Penn Treebank: Annotating argument structure. Technical report, University of Pennsylvania. Paola Merlo and Matthias Leybold. 2001. Automatic distinction of arguments and modifiers: the case of preposiphrases. In of the Fifth Computational Nat- Language Learning Workshop (C0NLL-2001 ), 121-128, Toulouse, France. Paola Merlo, Matt Crocker, and Cathy Berthouzoz. 1997. Attaching multiple prepositional phrases: Generalized estimation. In of the Second Conference on Empirical Methods in Natural Language Processing, pages 145-154, Providence, RI. George Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. Miller. 1990. Five papers on Wordnet. Technical report, Cognitive Science Lab, Princeton University. Pollard and Ivan Sag. 1987. Information-based Synand Semantics, 13. CSLI lecture Notes, Stanford University. Ross Quinlan. 1992. Programs for Machine Learnin Machine Learning. Morgan Kaufmann, San Mateo, CA. Randolph Quirk, Sidney Greenbaum, Geoffrey Leech, and Svartvik. 1985. Comprehensive Grammar of the Language. London. Adwait Ratnaparkhi, Jeffrey Reynar, and Salim Roukos. 1994. A Maximum Entropy Model for Prepositional Attachment. In of the ARPA Workshop on Language Technology, 250-255. Carson T. Schiitze. 1995. PP Attachment and Argument- Working Papers in Linguistics, Bangalore Srinivas and Aravind K. Joshi. 1999. Supertag- An approach to almost parsing. Lin- Aline Villavicencio. 2002. Learning to distinguish PP argufrom adjuncts. In of the 6th Conference on Language Learning (CoNLL-2002), 84-90, Taipei,Taiwan. 258</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Izaskun Aldezabal</author>
</authors>
<title>Maxux Aranzabe, Koldo Gojenola, Kepa Sarasota, and Aitziber Atutxa.</title>
<date>2002</date>
<booktitle>In Procs of the SIGLEX Workshop on Unsupervised Lexical Acquisition,</booktitle>
<pages>42--50</pages>
<location>Philadelphia,PA,</location>
<marker>Aldezabal, 2002</marker>
<rawString>Izaskun Aldezabal, Maxux Aranzabe, Koldo Gojenola, Kepa Sarasota, and Aitziber Atutxa. 2002. Learning argument/adjunct dictinction for Basque. In Procs of the SIGLEX Workshop on Unsupervised Lexical Acquisition, pages 42-50, Philadelphia,PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Bies</author>
<author>M Ferguson</author>
<author>K Katz</author>
<author>Robert MacIntyre</author>
</authors>
<title>Bracketing guidelines for Treebank II style.</title>
<date>1995</date>
<tech>Technical report,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="16277" citStr="Bies et al., 1995" startWordPosition="2650" endWordPosition="2653">ction and trajectory -LOC location -MNR manner -PRP purpose and reason -TMP temporal phrases Figure 1: Grammatical function and semantic tags that involve PP constituents in the PTB The Target Attribute Since we are planning to use a supervised learning method, we need to label each example with a four-valued target attribute (the values are Narg, Nadj, Varg, Vadj). The Penn Treebank annotation does not explicitly make the distinction between arguments and adjuncts. Information about this difference then must be gleaned from the semantic and function tags that have been assigned to the nodes (Bies et al., 1995). Figure 1 illustrates the tags that involve PP constituents. Based on the guidelines (Marcus et al., 1994, 4),(Bies et al., 1995, 12), inspection of the actual annotation in the Tree bank, and discussions in the literature (Quirk et al. 1985, sections 8.27-35, 15.22, 16-48), we mapped PPs into arguments and adjuncts as follows: Adjuncts: All PPs tagged with a semantic tag (DIR, LOC, MNR, PRP, TMP). Arguments: All untagged PPs or PPs tagged with CLR, PUT, DTV, BNF, PRD or LGS. 4.2 The Method The Input Data Each input vector represents an instance of a PP attachment, which could be both noun or</context>
</contexts>
<marker>Bies, Ferguson, Katz, MacIntyre, 1995</marker>
<rawString>Ann Bies, M. Ferguson, K.Katz, and Robert MacIntyre. 1995. Bracketing guidelines for Treebank II style. Technical report, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
</authors>
<title>Distinguishing complements from adjuncts using memory-based learning.</title>
<date>1999</date>
<journal>ILK, Computational</journal>
<institution>Linguistics, Tilburg University.</institution>
<contexts>
<context position="5293" citStr="Buchholz, 1999" startWordPosition="849" endWordPosition="850">hat the current formulation of the PP attachment problem cannot be considered a first step in the solution of the final 4-way discrimination task. Finally, we note that the improvement is especially due to a better recognition of verbs&apos; arguments, thus providing more accurate information to many NLP tasks and applications. Solving this novel 4-way classification task crucially relies on the ability to distinguish arguments from adjuncts using corpus counts. 2 A Novel Method to Distinguish Arguments from Adjuncts Few attempts have been made to distinguish arguments from adjuncts automatically (Buchholz, 1999; Merlo and Leybold, 2001; Villavicencio, 2002; Aldezabal et al., 2002). The core difficulty in this enterprise is to define the notion of argument precisely. There is a consensus in linguistics that arguments and adjuncts are different both with respect to their function in the sentence and in the way they themselves are interpreted (Jackendoff, 1977; Marantz, 1984; Pollard and Sag, 1987; Grimshaw, 1990). With respect to their function, an argument fills a role in the relation described by its associated head, while an adjunct predicates a separate property of its associate head or phrase. Wi</context>
<context position="26972" citStr="Buchholz, 1999" startWordPosition="4382" endWordPosition="4383">As far as we are aware, this is the first attempt to integrate the notion of argumenthood in a more comprehensive formulation of the problem of disambiguating the attachment of PPs. Hindle and Rooth (1993) mention the interaction between the structural and the semantic factors in the disambiguation of a PP, indicating that verb adjuncts are the most difficult. We confirm their finding that noun arguments are more easily identified, while verb complements (either arguments or adjuncts) are more difficult. Few previous pieces of work attempt to distinguish arguments from adjuncts automatically (Buchholz, 1999; Merlo and Leybold, 2001). We extend here (Merlo and Leybold, 2001) by elaborating more learning features, refining all the counting methods and extending the method to noun attachment, which had not been considered before, thus validating and extending the approach. The current work on binary argumentadjunct classifiers compares favourably to the only other comparable study on this topic (Buchholz, 1999). Buchholz reports an accuracy of 77% for the argument-adjunct distinction of PPs, to be compared to our 81% and 89% for verb and noun attachments respectively. Buchholz considers all types o</context>
</contexts>
<marker>Buchholz, 1999</marker>
<rawString>Sabine Buchholz. 1999. Distinguishing complements from adjuncts using memory-based learning. ILK, Computational Linguistics, Tilburg University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>James Brooks</author>
</authors>
<title>Prepositional Phrase Attachment through a Backed-Off Model.</title>
<date>1995</date>
<booktitle>In Procs of the Third Workshop on Very Large Corpora,</booktitle>
<pages>27--38</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="1598" citStr="Collins and Brooks, 1995" startWordPosition="251" endWordPosition="254">nstituents in the sentence. Consider the time-worn example I saw the man with the telescope It is important to determine if the PP with the telescope is to be attached as a sister to the noun the man, restricting its interpretation, or if it is to be attached to the verb, thereby indicating the instrument of the main action described by the sentence. Based on examples of this sort, recent approaches have formalised the problem of disambiguating PP attachments as a binary choice, distinguishing between attachment of a PP to a given verb or to the verb&apos;s direct object (Ratnaparkhi et al., 1994; Collins and Brooks, 1995). This is, however, a simplification of the problem, which does not take the nature of the attachment into account. Precisely, it does not distinguish PP arguments from PP adjuncts. Consider the following example, which contains two PPs, both modifying the verb. Put the block on the table in the morning The first PP is a locative PP required by the subcategorisation frame of the verb put, while in the morning is an optional descriptor of the time at which the action was performed. Though both attached to the verb, the two PPs entertain different relationships with the verb — the first is an ar</context>
<context position="12356" citStr="Collins and Brooks, 1995" startWordPosition="2009" endWordPosition="2012">nce. One corpus contains data encoding information for attachment of single PPs in the form of four head words (verb, object noun, preposition and PP-internal noun) for each instance of PP attachments found in the corpus. We also create an auxiliary corpus of sequences of two PPs, where each data item consists of verb, direct object and the two following PPs. This corpus is only used to estimate the feature Iterativity. All the data was newly extracted from the Penn Tree-bank. Our goal was to create a more comprehensive and possibly more accurate corpus than existing ones (Merlo et al., 1997; Collins and Brooks, 1995). To improve coverage, we extracted all cases of PPs following transitive, and intransitive verbs and following nominal phrases. We include also passive sentences and sentences containing a sentential object. To improve accuracy, attention was paid not to extract overlapping data, contrary to counts in previous corpora, where multiple PP sequences were counted more than once, each time as part of a different structural configuration. The Counts The linguistic diagnostics illustrated in the previous section are approximated by corpus counts based on the extracted tuples. Head dependence is appr</context>
</contexts>
<marker>Collins, Brooks, 1995</marker>
<rawString>Michael Collins and James Brooks. 1995. Prepositional Phrase Attachment through a Backed-Off Model. In Procs of the Third Workshop on Very Large Corpora, pages 27-38, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Don</author>
</authors>
<title>Large-scale dictionary construction for foreign language tutoring and interlingual machine translation.</title>
<date>1997</date>
<journal>Machine Translation,</journal>
<pages>12--4</pages>
<contexts>
<context position="2865" citStr="Don, 1997" startWordPosition="475" endWordPosition="476"> could be built for attachments to the noun. Is it important to model not just the site but also the nature of the attachment of a PP into the tree structure? We would like to claim that it is. Distinguishing arguments from adjuncts is key to identifying the semantic kernel of a sentence. Extracting the core meaning of a sentence or phrase, in turn, is necessary for automatic acquisition of important lexical knowledge, such as subcategorisation frames and argument structures, which is used in several NLP tasks and applications, such as parsing or machine translation (Srinivas and Joshi, 1999; Don, 1997). Moreover, from a quantitative point of view, arguments and adjuncts have different statistical properties, requiring different statistical techniques. For example, (Hindle and Rooth, 1993) clearly indicate that their lexical association technique performs much better for arguments than for adjuncts, whether the attachment is to the verb or to the noun. Researchers have abstracted away from this distinction, because identifying arguments and adjuncts is a notoriously difficult task, taxing many 251 native speakers&apos; intuitions and requiring complex world knowledge. The usual expectation has be</context>
</contexts>
<marker>Don, 1997</marker>
<rawString>Bonnie Don. 1997. Large-scale dictionary construction for foreign language tutoring and interlingual machine translation. Machine Translation, 12(4):1-55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Esteve-Ferrer</author>
<author>Paola Merlo</author>
</authors>
<title>Automatic distinction of PP arguments and adjuncts.</title>
<date>2002</date>
<tech>Technical report, MALA Project 1,</tech>
<institution>University of Geneva.</institution>
<contexts>
<context position="13534" citStr="Esteve-Ferrer and Merlo, 2002" startWordPosition="2199" endWordPosition="2202">sed on the extracted tuples. Head dependence is approximated by the entropy of the distribution of the verb or noun heads for each PP, as indicated in (4). Oki) tog C(h) (4) H(PP) E,C(h) 2C(h) We implement also some more general variants, where PP-internal nouns and head nouns are replaced by their WordNet 1.7 class (Miller et al., 1990). Polysemous nouns are disambiguated by selecting the most frequent WordNet sense. Optionality is captured by the conditional probability of a PP given a particular verbal head, as indicated in (5). 3For more detail on the features and experiments, please see (Esteve-Ferrer and Merlo, 2002). C (v , PP) P(PP1v) (v) (5) Analogously to the measure of head dependence, optionality is also measured in general variants that rely on verb and noun classes, based on WordNet 1.7. Iterativity and ordering are approximated by collecting counts indicating the proportion of cases in which a given PP in first position had been found in second position in a sequence of multiple PPs over the total of PPs in second position, as indicated in (6). C (PP)2 P(ADJI(PP)i) (6) E,C(PP,)2 The problem of sparse data here is especially serious, because of the small frequencies of multiple PPs. We estimate th</context>
</contexts>
<marker>Esteve-Ferrer, Merlo, 2002</marker>
<rawString>Eva Esteve-Ferrer and Paola Merlo. 2002. Automatic distinction of PP arguments and adjuncts. Technical report, MALA Project 1, University of Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Grimshaw</author>
</authors>
<title>Argument Structure.</title>
<date>1990</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="5701" citStr="Grimshaw, 1990" startWordPosition="915" endWordPosition="916">stinguish arguments from adjuncts using corpus counts. 2 A Novel Method to Distinguish Arguments from Adjuncts Few attempts have been made to distinguish arguments from adjuncts automatically (Buchholz, 1999; Merlo and Leybold, 2001; Villavicencio, 2002; Aldezabal et al., 2002). The core difficulty in this enterprise is to define the notion of argument precisely. There is a consensus in linguistics that arguments and adjuncts are different both with respect to their function in the sentence and in the way they themselves are interpreted (Jackendoff, 1977; Marantz, 1984; Pollard and Sag, 1987; Grimshaw, 1990). With respect to their function, an argument fills a role in the relation described by its associated head, while an adjunct predicates a separate property of its associate head or phrase. With respect to their interpretation, a complement is an argument if its interpretation depends exclusively on the head with which it is associated, while it is an adjunct if its interpretation remains relatively constant when associating with different heads (Grimshaw, 1990, 108). Restricting the discussion to PPs, these differences are illustrated in the following examples (PP-argument in bold), see also </context>
</contexts>
<marker>Grimshaw, 1990</marker>
<rawString>Jane Grimshaw. 1990. Argument Structure. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
<author>Mats Rooth</author>
</authors>
<title>Structural ambiguity and lexical relations.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--1</pages>
<contexts>
<context position="3055" citStr="Hindle and Rooth, 1993" startWordPosition="498" endWordPosition="501">laim that it is. Distinguishing arguments from adjuncts is key to identifying the semantic kernel of a sentence. Extracting the core meaning of a sentence or phrase, in turn, is necessary for automatic acquisition of important lexical knowledge, such as subcategorisation frames and argument structures, which is used in several NLP tasks and applications, such as parsing or machine translation (Srinivas and Joshi, 1999; Don, 1997). Moreover, from a quantitative point of view, arguments and adjuncts have different statistical properties, requiring different statistical techniques. For example, (Hindle and Rooth, 1993) clearly indicate that their lexical association technique performs much better for arguments than for adjuncts, whether the attachment is to the verb or to the noun. Researchers have abstracted away from this distinction, because identifying arguments and adjuncts is a notoriously difficult task, taxing many 251 native speakers&apos; intuitions and requiring complex world knowledge. The usual expectation has been that this discrimination is not amenable to a corpus-based treatment. In recent work, however, we succeed in distinguishing arguments from adjuncts using evidence extracted from a parsed </context>
<context position="26563" citStr="Hindle and Rooth (1993)" startWordPosition="4319" endWordPosition="4322">, the approach does not rely on any assumption on the distribution of the data, and thereby reaches better accuracy. Finally, a result of particular interest is the improved discrimination of verb arguments from verb adjuncts compared to a two-step approach, a promising result for all the numerous NLP tasks and applications that rely on the correct identification of a verb&apos;s subcategorization frame. 6 Related Work As far as we are aware, this is the first attempt to integrate the notion of argumenthood in a more comprehensive formulation of the problem of disambiguating the attachment of PPs. Hindle and Rooth (1993) mention the interaction between the structural and the semantic factors in the disambiguation of a PP, indicating that verb adjuncts are the most difficult. We confirm their finding that noun arguments are more easily identified, while verb complements (either arguments or adjuncts) are more difficult. Few previous pieces of work attempt to distinguish arguments from adjuncts automatically (Buchholz, 1999; Merlo and Leybold, 2001). We extend here (Merlo and Leybold, 2001) by elaborating more learning features, refining all the counting methods and extending the method to noun attachment, whic</context>
</contexts>
<marker>Hindle, Rooth, 1993</marker>
<rawString>Donald Hindle and Mats Rooth. 1993. Structural ambiguity and lexical relations. Computational Linguistics, 19(1):103-120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Jackendoff</author>
</authors>
<title>X&apos; Syntax: A Study of Phrase Structure.</title>
<date>1977</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="5646" citStr="Jackendoff, 1977" startWordPosition="906" endWordPosition="908">lassification task crucially relies on the ability to distinguish arguments from adjuncts using corpus counts. 2 A Novel Method to Distinguish Arguments from Adjuncts Few attempts have been made to distinguish arguments from adjuncts automatically (Buchholz, 1999; Merlo and Leybold, 2001; Villavicencio, 2002; Aldezabal et al., 2002). The core difficulty in this enterprise is to define the notion of argument precisely. There is a consensus in linguistics that arguments and adjuncts are different both with respect to their function in the sentence and in the way they themselves are interpreted (Jackendoff, 1977; Marantz, 1984; Pollard and Sag, 1987; Grimshaw, 1990). With respect to their function, an argument fills a role in the relation described by its associated head, while an adjunct predicates a separate property of its associate head or phrase. With respect to their interpretation, a complement is an argument if its interpretation depends exclusively on the head with which it is associated, while it is an adjunct if its interpretation remains relatively constant when associating with different heads (Grimshaw, 1990, 108). Restricting the discussion to PPs, these differences are illustrated in </context>
</contexts>
<marker>Jackendoff, 1977</marker>
<rawString>Ray Jackendoff. 1977. X&apos; Syntax: A Study of Phrase Structure. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recognizer.</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustic, Speech and Signal Processing,</journal>
<pages>35--3</pages>
<contexts>
<context position="14207" citStr="Katz, 1987" startWordPosition="2318" endWordPosition="2319">d dependence, optionality is also measured in general variants that rely on verb and noun classes, based on WordNet 1.7. Iterativity and ordering are approximated by collecting counts indicating the proportion of cases in which a given PP in first position had been found in second position in a sequence of multiple PPs over the total of PPs in second position, as indicated in (6). C (PP)2 P(ADJI(PP)i) (6) E,C(PP,)2 The problem of sparse data here is especially serious, because of the small frequencies of multiple PPs. We estimate this measure in a single variant using a backed-off estimation (Katz, 1987), where we replace lexical items by their WordNet classes. Copular paraphrase is captured by calculating the proportion of times a given PP follows a copular verb over all the times it appears following any verb. P(ADJI(PP)) C (copular, &lt; PP) (7) EiC(vi &lt; PP) This measure is an approximation, since we don&apos;t know whether the copular verb is indeed in a relative clause or not. Here again, we back-off to the noun classes of the PP-internal noun, to address the problem of sparse data. The diagnostic of deverbal nouns is implemented as a binary feature that simply indicates if the PP follows a deve</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>S.M. Katz. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE Transactions on Acoustic, Speech and Signal Processing, 35(3):400-401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Korhonen</author>
</authors>
<title>Semantically motivated subcategorization acquisition.</title>
<date>2002</date>
<booktitle>In Procs of the SIGLEX Workshop on Unsupervised Lexical Acquisition,</booktitle>
<pages>51--58</pages>
<location>Philadelphia,PA,</location>
<contexts>
<context position="28139" citStr="Korhonen, 2002" startWordPosition="4565" endWordPosition="4566">ts respectively. Buchholz considers all types of attachment sites, not just verbs and nouns. Recently (Villavicencio, 2002) has explored the performance of an argument identifier, developed in the framework of a model of child language learning. The approach is not directly comparable, as it is not entirely corpus-based (the input to the algorithm is an impoverished logical form), and the evaluation is on a smaller scale than the present work. Other pieces of work address the current problem in the larger perspective of distinguishing arguments from adjuncts for subcategorization acquisition (Korhonen, 2002; Aldezabal et al., 2002). Our work confirms the results reported in (Korhonen, 2002), which indicate that using word classes improves the extraction of subcategorisation frames. 257 7 Conclusions We have proposed a reformulation of the problem of PP attachment as a 4-way disambiguation problem, arguing that what is needed in interpreting prepositional phrases is knowledge about both the structural attachment site - the traditional nounverb attachment distinction - and the nature of the attachment - the distinction of arguments from adjuncts. Practically, we have shown that a 4-way classifier </context>
</contexts>
<marker>Korhonen, 2002</marker>
<rawString>Anna Korhonen. 2002. Semantically motivated subcategorization acquisition. In Procs of the SIGLEX Workshop on Unsupervised Lexical Acquisition, pages 51-58, Philadelphia,PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations.</title>
<date>1993</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago, IL.</location>
<contexts>
<context position="9664" citStr="Levin, 1993" startWordPosition="1552" endWordPosition="1553"> c) John saw/read the book in the room d) John saw/read the book The notion of optionality can be captured by the conditional probability of a PP given a particular verbal head, P(PP1v). &apos;We do not compute this measure for nominal heads as PP are always optional when governed by a nominal head. 2Notice that this diagnostics can only be interpreted as a statistical tendency, and not as a strict test, because not all arguments are obligatory (but all adjuncts are indeed optional). The best known descriptive exceptions to the criterion of optionality are the class of so-called object-drop verbs (Levin, 1993) and, arguably, instrumental verbs (Schatze, 1995). While keeping these exceptions in mind, we maintain optionality as a valid diagnostic here. Iterativity and Ordering Arguments cannot be iterated and they must be adjacent to the selecting lexical head. Neither of these two restrictions apply to adjuncts, as illustrated in the examples below. a) *Chris rented the gazebo to girls, to boys b) Kim met Sandy in Baltimore in the hotel lobby in a corner Thus, the probability of a PP being an adjunct can be approximated as the probability of its occurrence in second position in a sequence of PPs, as</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English Verb Classes and Alternations. University of Chicago Press, Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Marantz</author>
</authors>
<title>On the Nature of Grammatical Relations.</title>
<date>1984</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="5661" citStr="Marantz, 1984" startWordPosition="909" endWordPosition="910"> crucially relies on the ability to distinguish arguments from adjuncts using corpus counts. 2 A Novel Method to Distinguish Arguments from Adjuncts Few attempts have been made to distinguish arguments from adjuncts automatically (Buchholz, 1999; Merlo and Leybold, 2001; Villavicencio, 2002; Aldezabal et al., 2002). The core difficulty in this enterprise is to define the notion of argument precisely. There is a consensus in linguistics that arguments and adjuncts are different both with respect to their function in the sentence and in the way they themselves are interpreted (Jackendoff, 1977; Marantz, 1984; Pollard and Sag, 1987; Grimshaw, 1990). With respect to their function, an argument fills a role in the relation described by its associated head, while an adjunct predicates a separate property of its associate head or phrase. With respect to their interpretation, a complement is an argument if its interpretation depends exclusively on the head with which it is associated, while it is an adjunct if its interpretation remains relatively constant when associating with different heads (Grimshaw, 1990, 108). Restricting the discussion to PPs, these differences are illustrated in the following e</context>
</contexts>
<marker>Marantz, 1984</marker>
<rawString>A. Marantz. 1984. On the Nature of Grammatical Relations. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>G Kim</author>
<author>A Marcinkiewicz</author>
<author>A Bies R Macintyre</author>
<author>M Ferguson</author>
<author>K Katz</author>
<author>B Schasberger</author>
</authors>
<title>The Penn Treebank: Annotating argument structure.</title>
<date>1994</date>
<tech>Technical report,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="16383" citStr="Marcus et al., 1994" startWordPosition="2667" endWordPosition="2670">ammatical function and semantic tags that involve PP constituents in the PTB The Target Attribute Since we are planning to use a supervised learning method, we need to label each example with a four-valued target attribute (the values are Narg, Nadj, Varg, Vadj). The Penn Treebank annotation does not explicitly make the distinction between arguments and adjuncts. Information about this difference then must be gleaned from the semantic and function tags that have been assigned to the nodes (Bies et al., 1995). Figure 1 illustrates the tags that involve PP constituents. Based on the guidelines (Marcus et al., 1994, 4),(Bies et al., 1995, 12), inspection of the actual annotation in the Tree bank, and discussions in the literature (Quirk et al. 1985, sections 8.27-35, 15.22, 16-48), we mapped PPs into arguments and adjuncts as follows: Adjuncts: All PPs tagged with a semantic tag (DIR, LOC, MNR, PRP, TMP). Arguments: All untagged PPs or PPs tagged with CLR, PUT, DTV, BNF, PRD or LGS. 4.2 The Method The Input Data Each input vector represents an instance of a PP attachment, which could be both noun or verb attached, either as an argument or as an adj unct.4 Each vector contains 20 training features. They </context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, Macintyre, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>M. Marcus, G. Kim, A. Marcinkiewicz, R.Macintyre, A. Bies, M. Ferguson, K.Katz, and B.Schasberger. 1994. The Penn Treebank: Annotating argument structure. Technical report, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Merlo</author>
<author>Matthias Leybold</author>
</authors>
<title>Automatic distinction of arguments and modifiers: the case of prepositional phrases.</title>
<date>2001</date>
<booktitle>In Procs of the Fifth Computational Natural Language Learning Workshop (C0NLL-2001 ),</booktitle>
<pages>121--128</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="3687" citStr="Merlo and Leybold, 2001" startWordPosition="594" endWordPosition="597"> indicate that their lexical association technique performs much better for arguments than for adjuncts, whether the attachment is to the verb or to the noun. Researchers have abstracted away from this distinction, because identifying arguments and adjuncts is a notoriously difficult task, taxing many 251 native speakers&apos; intuitions and requiring complex world knowledge. The usual expectation has been that this discrimination is not amenable to a corpus-based treatment. In recent work, however, we succeed in distinguishing arguments from adjuncts using evidence extracted from a parsed corpus (Merlo and Leybold, 2001). Our method develops corpus-based statistical correlates for the diagnostics used in linguistics to decide whether a PP is an argument or an adjunct. A numerical vectorial representation of the notion of argumenthood is provided, which supports automatic classification, reaching 86% accuracy. In the current paper, we extend this work and propose a new formulation of the PP attachment problem. We treat PP attachment as a 4-way classification of PPs into noun argument PPs, noun adjunct PPs, verb argument PPs, and verb adjunct PPs. We show that it is possible to build a classifier that solves th</context>
<context position="5318" citStr="Merlo and Leybold, 2001" startWordPosition="851" endWordPosition="854">formulation of the PP attachment problem cannot be considered a first step in the solution of the final 4-way discrimination task. Finally, we note that the improvement is especially due to a better recognition of verbs&apos; arguments, thus providing more accurate information to many NLP tasks and applications. Solving this novel 4-way classification task crucially relies on the ability to distinguish arguments from adjuncts using corpus counts. 2 A Novel Method to Distinguish Arguments from Adjuncts Few attempts have been made to distinguish arguments from adjuncts automatically (Buchholz, 1999; Merlo and Leybold, 2001; Villavicencio, 2002; Aldezabal et al., 2002). The core difficulty in this enterprise is to define the notion of argument precisely. There is a consensus in linguistics that arguments and adjuncts are different both with respect to their function in the sentence and in the way they themselves are interpreted (Jackendoff, 1977; Marantz, 1984; Pollard and Sag, 1987; Grimshaw, 1990). With respect to their function, an argument fills a role in the relation described by its associated head, while an adjunct predicates a separate property of its associate head or phrase. With respect to their inter</context>
<context position="7436" citStr="Merlo and Leybold, 2001" startWordPosition="1186" endWordPosition="1189">, giving rise to a set of linguistic diagnostics to determine whether a PP is an adjunct or an argument. We illustrate here those countable diagnostics that can be approximated statistically and estimated using corpus counts, thus combining linguistic insight with the robustness of corpus-based methods. 3 The Linguistic Diagnostics Many diagnostics for argumenthood have been proposed in the literature (Schiitze, 1995). Some of them require complex syntactic manipulation of the sentence, such as wh-extraction, and are therefore too difficult to apply automatically. We extend our previous work (Merlo and Leybold, 2001) and choose six diagnostics that can be captured by simple corpus counts: head dependence, optionality, iterativity, ordering, copular paraphrase, and deverbal nominalisation. These diagnostics tap into the deeper semantic properties that distinguish arguments from adjuncts. 252 Head Dependence Arguments depend on their lexical heads, because they form an integral part of the phrase. Adjuncts do not. Consequently, PParguments can only appear with the specific verbal or nominal head by which they are lexically selected, while PP-adjuncts can co-occur with a far greater range of different heads </context>
<context position="26998" citStr="Merlo and Leybold, 2001" startWordPosition="4384" endWordPosition="4387"> aware, this is the first attempt to integrate the notion of argumenthood in a more comprehensive formulation of the problem of disambiguating the attachment of PPs. Hindle and Rooth (1993) mention the interaction between the structural and the semantic factors in the disambiguation of a PP, indicating that verb adjuncts are the most difficult. We confirm their finding that noun arguments are more easily identified, while verb complements (either arguments or adjuncts) are more difficult. Few previous pieces of work attempt to distinguish arguments from adjuncts automatically (Buchholz, 1999; Merlo and Leybold, 2001). We extend here (Merlo and Leybold, 2001) by elaborating more learning features, refining all the counting methods and extending the method to noun attachment, which had not been considered before, thus validating and extending the approach. The current work on binary argumentadjunct classifiers compares favourably to the only other comparable study on this topic (Buchholz, 1999). Buchholz reports an accuracy of 77% for the argument-adjunct distinction of PPs, to be compared to our 81% and 89% for verb and noun attachments respectively. Buchholz considers all types of attachment sites, not ju</context>
</contexts>
<marker>Merlo, Leybold, 2001</marker>
<rawString>Paola Merlo and Matthias Leybold. 2001. Automatic distinction of arguments and modifiers: the case of prepositional phrases. In Procs of the Fifth Computational Natural Language Learning Workshop (C0NLL-2001 ), pages 121-128, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Merlo</author>
<author>Matt Crocker</author>
<author>Cathy Berthouzoz</author>
</authors>
<title>Attaching multiple prepositional phrases: Generalized backed-off estimation.</title>
<date>1997</date>
<booktitle>In Procs of the Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>145--154</pages>
<location>Providence, RI.</location>
<contexts>
<context position="12329" citStr="Merlo et al., 1997" startWordPosition="2005" endWordPosition="2008"> and head noun sequence. One corpus contains data encoding information for attachment of single PPs in the form of four head words (verb, object noun, preposition and PP-internal noun) for each instance of PP attachments found in the corpus. We also create an auxiliary corpus of sequences of two PPs, where each data item consists of verb, direct object and the two following PPs. This corpus is only used to estimate the feature Iterativity. All the data was newly extracted from the Penn Tree-bank. Our goal was to create a more comprehensive and possibly more accurate corpus than existing ones (Merlo et al., 1997; Collins and Brooks, 1995). To improve coverage, we extracted all cases of PPs following transitive, and intransitive verbs and following nominal phrases. We include also passive sentences and sentences containing a sentential object. To improve accuracy, attention was paid not to extract overlapping data, contrary to counts in previous corpora, where multiple PP sequences were counted more than once, each time as part of a different structural configuration. The Counts The linguistic diagnostics illustrated in the previous section are approximated by corpus counts based on the extracted tupl</context>
</contexts>
<marker>Merlo, Crocker, Berthouzoz, 1997</marker>
<rawString>Paola Merlo, Matt Crocker, and Cathy Berthouzoz. 1997. Attaching multiple prepositional phrases: Generalized backed-off estimation. In Procs of the Second Conference on Empirical Methods in Natural Language Processing, pages 145-154, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
<author>R Beckwith</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K Miller</author>
</authors>
<title>Five papers on Wordnet.</title>
<date>1990</date>
<tech>Technical report,</tech>
<institution>Cognitive Science Lab, Princeton University.</institution>
<contexts>
<context position="13243" citStr="Miller et al., 1990" startWordPosition="2154" endWordPosition="2157">rlapping data, contrary to counts in previous corpora, where multiple PP sequences were counted more than once, each time as part of a different structural configuration. The Counts The linguistic diagnostics illustrated in the previous section are approximated by corpus counts based on the extracted tuples. Head dependence is approximated by the entropy of the distribution of the verb or noun heads for each PP, as indicated in (4). Oki) tog C(h) (4) H(PP) E,C(h) 2C(h) We implement also some more general variants, where PP-internal nouns and head nouns are replaced by their WordNet 1.7 class (Miller et al., 1990). Polysemous nouns are disambiguated by selecting the most frequent WordNet sense. Optionality is captured by the conditional probability of a PP given a particular verbal head, as indicated in (5). 3For more detail on the features and experiments, please see (Esteve-Ferrer and Merlo, 2002). C (v , PP) P(PP1v) (v) (5) Analogously to the measure of head dependence, optionality is also measured in general variants that rely on verb and noun classes, based on WordNet 1.7. Iterativity and ordering are approximated by collecting counts indicating the proportion of cases in which a given PP in first</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>George Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. Miller. 1990. Five papers on Wordnet. Technical report, Cognitive Science Lab, Princeton University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan Sag</author>
</authors>
<title>An Information-based Syntax and Semantics, volume 13. CSLI lecture Notes,</title>
<date>1987</date>
<institution>Stanford University.</institution>
<contexts>
<context position="5684" citStr="Pollard and Sag, 1987" startWordPosition="911" endWordPosition="914">es on the ability to distinguish arguments from adjuncts using corpus counts. 2 A Novel Method to Distinguish Arguments from Adjuncts Few attempts have been made to distinguish arguments from adjuncts automatically (Buchholz, 1999; Merlo and Leybold, 2001; Villavicencio, 2002; Aldezabal et al., 2002). The core difficulty in this enterprise is to define the notion of argument precisely. There is a consensus in linguistics that arguments and adjuncts are different both with respect to their function in the sentence and in the way they themselves are interpreted (Jackendoff, 1977; Marantz, 1984; Pollard and Sag, 1987; Grimshaw, 1990). With respect to their function, an argument fills a role in the relation described by its associated head, while an adjunct predicates a separate property of its associate head or phrase. With respect to their interpretation, a complement is an argument if its interpretation depends exclusively on the head with which it is associated, while it is an adjunct if its interpretation remains relatively constant when associating with different heads (Grimshaw, 1990, 108). Restricting the discussion to PPs, these differences are illustrated in the following examples (PP-argument in</context>
<context position="8073" citStr="Pollard and Sag, 1987" startWordPosition="1282" endWordPosition="1285">diagnostics that can be captured by simple corpus counts: head dependence, optionality, iterativity, ordering, copular paraphrase, and deverbal nominalisation. These diagnostics tap into the deeper semantic properties that distinguish arguments from adjuncts. 252 Head Dependence Arguments depend on their lexical heads, because they form an integral part of the phrase. Adjuncts do not. Consequently, PParguments can only appear with the specific verbal or nominal head by which they are lexically selected, while PP-adjuncts can co-occur with a far greater range of different heads than arguments (Pollard and Sag, 1987, 136), as illustrated in the example sentences below (PP-argument in bold). a) a man/woman/scarecrow with gray hair b) a student/*punk/*watermelon of physics We capture this insight by measuring the dispersion of the distribution over the different verbs or nouns that co-occur with a given PP in a corpus. We expect adjunct PPs to have higher dispersion than argument PPs. Differently from our previous simpler implementation (Merlo and Leybold, 2001), we use entropy as a measure of the dispersion of the distribution, as indicated in (1) (h indicates the noun or verb head to which the PP is atta</context>
</contexts>
<marker>Pollard, Sag, 1987</marker>
<rawString>Carl Pollard and Ivan Sag. 1987. An Information-based Syntax and Semantics, volume 13. CSLI lecture Notes, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ross Quinlan</author>
</authors>
<date>1992</date>
<booktitle>C4.5: Programs for Machine Learning. Series in Machine Learning.</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo, CA.</location>
<contexts>
<context position="17491" citStr="Quinlan, 1992" startWordPosition="2857" endWordPosition="2858"> verb attached, either as an argument or as an adj unct.4 Each vector contains 20 training features. They comprise the four lexical heads and their WordNet classes, all the different variants of the implementation of the diagnostics, and one 4- valued target feature, indicating the type of attachment. 4Notice however that the learning features were calculated on all the instances described above, so in practice we use both the unambiguous cases and ambiguous cases in the estimation of the features of the ambiguous cases. Experimental Settings We use the C5.0 Decision Tree Induction Algorithm (Quinlan, 1992), applied to a training and testing corpus containing 13906 exemplars, of which we used 90% for training and 10% for testing. The test sets were selected by stratified sampling. (Nine samples were created.) Clearly, to classify PPs into four classes, we have two options: we can construct a single fourclass classifier or we can build a sequence of binary classifiers. The discrimination between noun and verb attachment can be performed first, and then further refined into attachment as argument or adjunct, performing the 4-way classification in two steps. The two-step approach would be the natur</context>
</contexts>
<marker>Quinlan, 1992</marker>
<rawString>J. Ross Quinlan. 1992. C4.5: Programs for Machine Learning. Series in Machine Learning. Morgan Kaufmann, San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Randolph Quirk</author>
<author>Sidney Greenbaum</author>
<author>Geoffrey Leech</author>
<author>Jan Svartvik</author>
</authors>
<title>A Comprehensive Grammar of the English Language.</title>
<date>1985</date>
<location>Longman, London.</location>
<contexts>
<context position="10878" citStr="Quirk et al., 1985" startWordPosition="1762" endWordPosition="1765">s, as indicated in (2). P(ADJ1(PP)1);----- P(PP)2 (2) Copular Paraphrase The diagnostic of copular paraphrase is specific to the distinction of NPs arguments and adjuncts (Schiitze, 1995, 103). NP arguments cannot be paraphrased by a copular relative clause (examples b and b&apos;), while adjuncts can (examples a and a&apos;). a) a man from Paris a&apos;) a man who was from Paris b) the weight of the cow b&apos;) *the weight that was of the cow Thus, the probability that a PP is an adjunct can be approximated by the probability of its occurrence following a copular verb, such as be, become, appear, seem, remain (Quirk et al., 1985), as indicated in (3). P(ADJ1(PP)) P(copulary &lt; PP) (3) Deverbal Nouns This diagnostic is based on the observation that PPs following a deverbal noun are likely to be arguments. This diagnostic can be captured by a probability indicators function, that assigns probability 1 of being an argument to PPs following a deverbal noun and 0 otherwise. In conclusion, the different properties of arguments and adjuncts can be reduced to surface indicators, which can be estimated by appropriate corpus counts. 4 Experiments The success and generality of corpus-based classifier induction rests in large part</context>
<context position="14906" citStr="Quirk et al., 1985" startWordPosition="2440" endWordPosition="2443"> captured by calculating the proportion of times a given PP follows a copular verb over all the times it appears following any verb. P(ADJI(PP)) C (copular, &lt; PP) (7) EiC(vi &lt; PP) This measure is an approximation, since we don&apos;t know whether the copular verb is indeed in a relative clause or not. Here again, we back-off to the noun classes of the PP-internal noun, to address the problem of sparse data. The diagnostic of deverbal nouns is implemented as a binary feature that simply indicates if the PP follows a deverbal noun or not. Deverbal nouns are identified by inspecting their morphology (Quirk et al., 1985). The suffixes that can combine with verb bases to form deverbal nouns are -ant (inhabitant), -ee (appointee), -er, or (singer), -age (breakage), -al (refusal), -ion (exploration), -sion (invasion), -ing (building), -ment (arrangement). In conclusion, the linguistic diagnostics can be approximated by simple statistical indicators estimated in a sufficiently large corpus. Once the counts are collected they constitute the input to an automatic classifier. 254 -CLR dative object if dative shift not possible(e.g. donate); phrasal verbs; predication adjuncts -DTV dative object if dative shift possi</context>
<context position="16519" citStr="Quirk et al. 1985" startWordPosition="2691" endWordPosition="2694">d learning method, we need to label each example with a four-valued target attribute (the values are Narg, Nadj, Varg, Vadj). The Penn Treebank annotation does not explicitly make the distinction between arguments and adjuncts. Information about this difference then must be gleaned from the semantic and function tags that have been assigned to the nodes (Bies et al., 1995). Figure 1 illustrates the tags that involve PP constituents. Based on the guidelines (Marcus et al., 1994, 4),(Bies et al., 1995, 12), inspection of the actual annotation in the Tree bank, and discussions in the literature (Quirk et al. 1985, sections 8.27-35, 15.22, 16-48), we mapped PPs into arguments and adjuncts as follows: Adjuncts: All PPs tagged with a semantic tag (DIR, LOC, MNR, PRP, TMP). Arguments: All untagged PPs or PPs tagged with CLR, PUT, DTV, BNF, PRD or LGS. 4.2 The Method The Input Data Each input vector represents an instance of a PP attachment, which could be both noun or verb attached, either as an argument or as an adj unct.4 Each vector contains 20 training features. They comprise the four lexical heads and their WordNet classes, all the different variants of the implementation of the diagnostics, and one </context>
</contexts>
<marker>Quirk, Greenbaum, Leech, Svartvik, 1985</marker>
<rawString>Randolph Quirk, Sidney Greenbaum, Geoffrey Leech, and Jan Svartvik. 1985. A Comprehensive Grammar of the English Language. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
<author>Jeffrey Reynar</author>
<author>Salim Roukos</author>
</authors>
<title>A Maximum Entropy Model for Prepositional Phrase Attachment.</title>
<date>1994</date>
<booktitle>In Procs of the ARPA Workshop on Human Language Technology,</booktitle>
<pages>250--255</pages>
<contexts>
<context position="1571" citStr="Ratnaparkhi et al., 1994" startWordPosition="247" endWordPosition="250">roper interpretation of constituents in the sentence. Consider the time-worn example I saw the man with the telescope It is important to determine if the PP with the telescope is to be attached as a sister to the noun the man, restricting its interpretation, or if it is to be attached to the verb, thereby indicating the instrument of the main action described by the sentence. Based on examples of this sort, recent approaches have formalised the problem of disambiguating PP attachments as a binary choice, distinguishing between attachment of a PP to a given verb or to the verb&apos;s direct object (Ratnaparkhi et al., 1994; Collins and Brooks, 1995). This is, however, a simplification of the problem, which does not take the nature of the attachment into account. Precisely, it does not distinguish PP arguments from PP adjuncts. Consider the following example, which contains two PPs, both modifying the verb. Put the block on the table in the morning The first PP is a locative PP required by the subcategorisation frame of the verb put, while in the morning is an optional descriptor of the time at which the action was performed. Though both attached to the verb, the two PPs entertain different relationships with th</context>
</contexts>
<marker>Ratnaparkhi, Reynar, Roukos, 1994</marker>
<rawString>Adwait Ratnaparkhi, Jeffrey Reynar, and Salim Roukos. 1994. A Maximum Entropy Model for Prepositional Phrase Attachment. In Procs of the ARPA Workshop on Human Language Technology, pages 250-255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carson T Schiitze</author>
</authors>
<date>1995</date>
<booktitle>PP Attachment and Argumenthood. MIT Working Papers in Linguistics,</booktitle>
<pages>26--95</pages>
<contexts>
<context position="6316" citStr="Schiitze, 1995" startWordPosition="1010" endWordPosition="1011"> With respect to their function, an argument fills a role in the relation described by its associated head, while an adjunct predicates a separate property of its associate head or phrase. With respect to their interpretation, a complement is an argument if its interpretation depends exclusively on the head with which it is associated, while it is an adjunct if its interpretation remains relatively constant when associating with different heads (Grimshaw, 1990, 108). Restricting the discussion to PPs, these differences are illustrated in the following examples (PP-argument in bold), see also (Schiitze, 1995, 100). a) Kim camps/jogs/meditates on Sunday b) Kim depended on Sandy In example a) the PP on Sunday can be construed without any reference to the preceding part of the sentence, and it preserves its meaning even when combining with different heads. This is, however, not the case for b). Here, the PP can only be properly understood in connection with the rest of the sentence: Sandy is the person on whom someone depends. These semantic distinctions surface in observable syntactic differences, giving rise to a set of linguistic diagnostics to determine whether a PP is an adjunct or an argument.</context>
<context position="10445" citStr="Schiitze, 1995" startWordPosition="1680" endWordPosition="1681">g Arguments cannot be iterated and they must be adjacent to the selecting lexical head. Neither of these two restrictions apply to adjuncts, as illustrated in the examples below. a) *Chris rented the gazebo to girls, to boys b) Kim met Sandy in Baltimore in the hotel lobby in a corner Thus, the probability of a PP being an adjunct can be approximated as the probability of its occurrence in second position in a sequence of PPs, as indicated in (2). P(ADJ1(PP)1);----- P(PP)2 (2) Copular Paraphrase The diagnostic of copular paraphrase is specific to the distinction of NPs arguments and adjuncts (Schiitze, 1995, 103). NP arguments cannot be paraphrased by a copular relative clause (examples b and b&apos;), while adjuncts can (examples a and a&apos;). a) a man from Paris a&apos;) a man who was from Paris b) the weight of the cow b&apos;) *the weight that was of the cow Thus, the probability that a PP is an adjunct can be approximated by the probability of its occurrence following a copular verb, such as be, become, appear, seem, remain (Quirk et al., 1985), as indicated in (3). P(ADJ1(PP)) P(copulary &lt; PP) (3) Deverbal Nouns This diagnostic is based on the observation that PPs following a deverbal noun are likely to be </context>
</contexts>
<marker>Schiitze, 1995</marker>
<rawString>Carson T. Schiitze. 1995. PP Attachment and Argumenthood. MIT Working Papers in Linguistics, 26:95-151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bangalore Srinivas</author>
<author>Aravind K Joshi</author>
</authors>
<title>Supertagging: An approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<pages>25--2</pages>
<contexts>
<context position="2853" citStr="Srinivas and Joshi, 1999" startWordPosition="471" endWordPosition="474">djunct. Analogous examples could be built for attachments to the noun. Is it important to model not just the site but also the nature of the attachment of a PP into the tree structure? We would like to claim that it is. Distinguishing arguments from adjuncts is key to identifying the semantic kernel of a sentence. Extracting the core meaning of a sentence or phrase, in turn, is necessary for automatic acquisition of important lexical knowledge, such as subcategorisation frames and argument structures, which is used in several NLP tasks and applications, such as parsing or machine translation (Srinivas and Joshi, 1999; Don, 1997). Moreover, from a quantitative point of view, arguments and adjuncts have different statistical properties, requiring different statistical techniques. For example, (Hindle and Rooth, 1993) clearly indicate that their lexical association technique performs much better for arguments than for adjuncts, whether the attachment is to the verb or to the noun. Researchers have abstracted away from this distinction, because identifying arguments and adjuncts is a notoriously difficult task, taxing many 251 native speakers&apos; intuitions and requiring complex world knowledge. The usual expect</context>
</contexts>
<marker>Srinivas, Joshi, 1999</marker>
<rawString>Bangalore Srinivas and Aravind K. Joshi. 1999. Supertagging: An approach to almost parsing. Computational Linguistics, 25(2):237-265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aline Villavicencio</author>
</authors>
<title>Learning to distinguish PP arguments from adjuncts.</title>
<date>2002</date>
<booktitle>In Procs of the 6th Conference on Natural Language Learning (CoNLL-2002),</booktitle>
<pages>84--90</pages>
<location>Taipei,Taiwan.</location>
<contexts>
<context position="5339" citStr="Villavicencio, 2002" startWordPosition="855" endWordPosition="856">achment problem cannot be considered a first step in the solution of the final 4-way discrimination task. Finally, we note that the improvement is especially due to a better recognition of verbs&apos; arguments, thus providing more accurate information to many NLP tasks and applications. Solving this novel 4-way classification task crucially relies on the ability to distinguish arguments from adjuncts using corpus counts. 2 A Novel Method to Distinguish Arguments from Adjuncts Few attempts have been made to distinguish arguments from adjuncts automatically (Buchholz, 1999; Merlo and Leybold, 2001; Villavicencio, 2002; Aldezabal et al., 2002). The core difficulty in this enterprise is to define the notion of argument precisely. There is a consensus in linguistics that arguments and adjuncts are different both with respect to their function in the sentence and in the way they themselves are interpreted (Jackendoff, 1977; Marantz, 1984; Pollard and Sag, 1987; Grimshaw, 1990). With respect to their function, an argument fills a role in the relation described by its associated head, while an adjunct predicates a separate property of its associate head or phrase. With respect to their interpretation, a compleme</context>
<context position="27648" citStr="Villavicencio, 2002" startWordPosition="4487" endWordPosition="4488">ybold, 2001) by elaborating more learning features, refining all the counting methods and extending the method to noun attachment, which had not been considered before, thus validating and extending the approach. The current work on binary argumentadjunct classifiers compares favourably to the only other comparable study on this topic (Buchholz, 1999). Buchholz reports an accuracy of 77% for the argument-adjunct distinction of PPs, to be compared to our 81% and 89% for verb and noun attachments respectively. Buchholz considers all types of attachment sites, not just verbs and nouns. Recently (Villavicencio, 2002) has explored the performance of an argument identifier, developed in the framework of a model of child language learning. The approach is not directly comparable, as it is not entirely corpus-based (the input to the algorithm is an impoverished logical form), and the evaluation is on a smaller scale than the present work. Other pieces of work address the current problem in the larger perspective of distinguishing arguments from adjuncts for subcategorization acquisition (Korhonen, 2002; Aldezabal et al., 2002). Our work confirms the results reported in (Korhonen, 2002), which indicate that us</context>
</contexts>
<marker>Villavicencio, 2002</marker>
<rawString>Aline Villavicencio. 2002. Learning to distinguish PP arguments from adjuncts. In Procs of the 6th Conference on Natural Language Learning (CoNLL-2002), pages 84-90, Taipei,Taiwan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>