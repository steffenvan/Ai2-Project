<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000502">
<title confidence="0.982234">
Randomized Language Models via Perfect Hash Functions
</title>
<author confidence="0.985722">
David Talbot*
</author>
<affiliation confidence="0.922362333333333">
School of Informatics
University of Edinburgh
2 Buccleuch Place, Edinburgh, UK
</affiliation>
<email confidence="0.996515">
d.r.talbot@sms.ed.ac.uk
</email>
<author confidence="0.970103">
Thorsten Brants
</author>
<affiliation confidence="0.942083">
Google Inc.
</affiliation>
<address confidence="0.7364015">
1600 Amphitheatre Parkway
Mountain View, CA 94303, USA
</address>
<email confidence="0.998888">
brants@google.com
</email>
<sectionHeader confidence="0.998599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998440818181818">
We propose a succinct randomized language
model which employs a perfect hash func-
tion to encode fingerprints of n-grams and
their associated probabilities, backoff weights,
or other parameters. The scheme can repre-
sent any standard n-gram model and is easily
combined with existing model reduction tech-
niques such as entropy-pruning. We demon-
strate the space-savings of the scheme via ma-
chine translation experiments within a dis-
tributed language modeling framework.
</bodyText>
<sectionHeader confidence="0.999513" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999395075471698">
Language models (LMs) are a core component in
statistical machine translation, speech recognition,
optical character recognition and many other areas.
They distinguish plausible word sequences from a
set of candidates. LMs are usually implemented
as n-gram models parameterized for each distinct
sequence of up to n words observed in the train-
ing corpus. Using higher-order models and larger
amounts of training data can significantly improve
performance in applications, however the size of the
resulting LM can become prohibitive.
With large monolingual corpora available in ma-
jor languages, making use of all the available data
is now a fundamental challenge in language mod-
eling. Efficiency is paramount in applications such
as machine translation which make huge numbers
of LM requests per sentence. To scale LMs to larger
corpora with higher-order dependencies, researchers
*Work completed while this author was at Google Inc.
have considered alternative parameterizations such
as class-based models (Brown et al., 1992), model
reduction techniques such as entropy-based pruning
(Stolcke, 1998), novel represention schemes such as
suffix arrays (Emami et al., 2007), Golomb Coding
(Church et al., 2007) and distributed language mod-
els that scale more readily (Brants et al., 2007).
In this paper we propose a novel randomized lan-
guage model. Recent work (Talbot and Osborne,
2007b) has demonstrated that randomized encod-
ings can be used to represent n-gram counts for
LMs with signficant space-savings, circumventing
information-theoretic constraints on lossless data
structures by allowing errors with some small prob-
ability. In contrast the representation scheme used
by our model encodes parameters directly. It can
be combined with any n-gram parameter estimation
method and existing model reduction techniques
such as entropy-based pruning. Parameters that are
stored in the model are retrieved without error; how-
ever, false positives may occur whereby n-grams not
in the model are incorrectly ‘found’ when requested.
The false positive rate is determined by the space us-
age of the model.
Our randomized language model is based on the
Bloomier filter (Chazelle et al., 2004). We encode
fingerprints (random hashes) of n-grams together
with their associated probabilities using a perfect
hash function generated at random (Majewski et al.,
1996). Lookup is very efficient: the values of 3 cells
in a large array are combined with the fingerprint of
an n-gram. This paper focuses on machine transla-
tion. However, many of our findings should transfer
to other applications of language modeling.
</bodyText>
<page confidence="0.978801">
505
</page>
<note confidence="0.702082">
Proceedings of ACL-08: HLT, pages 505–513,
</note>
<page confidence="0.408734">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<sectionHeader confidence="0.958603" genericHeader="method">
2 Scaling Language Models
</sectionHeader>
<bodyText confidence="0.999974583333333">
In statistical machine translation (SMT), LMs are
used to score candidate translations in the target lan-
guage. These are typically n-gram models that ap-
proximate the probability of a word sequence by as-
suming each token to be independent of all but n−1
preceding tokens. Parameters are estimated from
monolingual corpora with parameters for each dis-
tinct word sequence of length l E [n] observed in
the corpus. Since the number of parameters grows
somewhat exponentially with n and linearly with the
size of the training corpus, the resulting models can
be unwieldy even for relatively small corpora.
</bodyText>
<subsectionHeader confidence="0.992148">
2.1 Scaling Strategies
</subsectionHeader>
<bodyText confidence="0.999966545454545">
Various strategies have been proposed to scale LMs
to larger corpora and higher-order dependencies.
Model-based techniques seek to parameterize the
model more efficiently (e.g. latent variable models,
neural networks) or to reduce the model size directly
by pruning uninformative parameters, e.g. (Stolcke,
1998), (Goodman and Gao, 2000). Representation-
based techniques attempt to reduce space require-
ments by representing the model more efficiently or
in a form that scales more readily, e.g. (Emami et al.,
2007), (Brants et al., 2007), (Church et al., 2007).
</bodyText>
<subsectionHeader confidence="0.999269">
2.2 Lossy Randomized Encodings
</subsectionHeader>
<bodyText confidence="0.999982153846154">
A fundamental result in information theory (Carter
et al., 1978) states that a random set of objects can-
not be stored using constant space per object as the
universe from which the objects are drawn grows
in size: the space required to uniquely identify an
object increases as the set of possible objects from
which it must be distinguished grows. In language
modeling the universe under consideration is the
set of all possible n-grams of length n for given
vocabulary. Although n-grams observed in natu-
ral language corpora are not randomly distributed
within this universe no lossless data structure that we
are aware of can circumvent this space-dependency
on both the n-gram order and the vocabulary size.
Hence as the training corpus and vocabulary grow, a
model will require more space per parameter.
However, if we are willing to accept that occa-
sionally our model will be unable to distinguish be-
tween distinct n-grams, then it is possible to store
each parameter in constant space independent of
both n and the vocabulary size (Carter et al., 1978),
(Talbot and Osborne, 2007a). The space required in
such a lossy encoding depends only on the range of
values associated with the n-grams and the desired
error rate, i.e. the probability with which two dis-
tinct n-grams are assigned the same fingerprint.
</bodyText>
<subsectionHeader confidence="0.999663">
2.3 Previous Randomized LMs
</subsectionHeader>
<bodyText confidence="0.999926">
Recent work (Talbot and Osborne, 2007b) has used
lossy encodings based on Bloom filters (Bloom,
1970) to represent logarithmically quantized cor-
pus statistics for language modeling. While the ap-
proach results in significant space savings, working
with corpus statistics, rather than n-gram probabil-
ities directly, is computationally less efficient (par-
ticularly in a distributed setting) and introduces a
dependency on the smoothing scheme used. It also
makes it difficult to leverage existing model reduc-
tion strategies such as entropy-based pruning that
are applied to final parameter estimates.
In the next section we describe our randomized
LM scheme based on perfect hash functions. This
scheme can be used to encode any standard n-gram
model which may first be processed using any con-
ventional model reduction technique.
</bodyText>
<sectionHeader confidence="0.978864" genericHeader="method">
3 Perfect Hash-based Language Models
</sectionHeader>
<bodyText confidence="0.999939705882353">
Our randomized LM is based on the Bloomier filter
(Chazelle et al., 2004). We assume the n-grams and
their associated parameter values have been precom-
puted and stored on disk. We then encode the model
in an array such that each n-gram’s value can be re-
trieved. Storage for this array is the model’s only
significant space requirement once constructed.1
The model uses randomization to map n-grams
to fingerprints and to generate a perfect hash func-
tion that associates n-grams with their values. The
model can erroneously return a value for an n-gram
that was never actually stored, but will always return
the correct value for an n-gram that is in the model.
We will describe the randomized algorithm used to
encode n-gram parameters in the model, analyze the
probability of a false positive, and explain how we
construct and query the model in practice.
</bodyText>
<footnote confidence="0.7664525">
1Note that we do not store the n-grams explicitly and there-
fore that the model’s parameter set cannot easily be enumerated.
</footnote>
<page confidence="0.995307">
506
</page>
<subsectionHeader confidence="0.990867">
3.1 N-gram Fingerprints
</subsectionHeader>
<bodyText confidence="0.999545">
We wish to encode a set of n-gram/value pairs
</bodyText>
<equation confidence="0.987337">
S = I(x1, v(x1)), (x2, v(x2)), ... , (xN, v(xN))I
</equation>
<bodyText confidence="0.999631">
using an array A of size M and a perfect hash func-
tion. Each n-gram xi is drawn from some set of pos-
sible n-grams U and its associated value v(xi) from
a corresponding set of possible values V.
We do not store the n-grams and their proba-
bilities directly but rather encode a fingerprint of
each n-gram f(xi) together with its associated value
v(xi) in such a way that the value can be retrieved
when the model is queried with the n-gram xi.
A fingerprint hash function f : U —* [0, B − 1]
maps n-grams to integers between 0 and B − 1.2
The array A in which we encode n-gram/value pairs
has addresses of size 1­lo92 B] hence B will deter-
mine the amount of space used per n-gram. There
is a trade-off between space and error rate since the
larger B is, the lower the probability of a false pos-
itive. This is analyzed in detail below. For now we
assume only that B is at least as large as the range
of values stored in the model, i.e. B &gt; IVI.
</bodyText>
<subsectionHeader confidence="0.999589">
3.2 Composite Perfect Hash Functions
</subsectionHeader>
<bodyText confidence="0.991616625">
The function used to associate n-grams with their
values (Eq. (1)) combines a composite perfect hash
function (Majewski et al., 1996) with the finger-
print function. An example is shown in Fig. 1.
The composite hash function is made up of k in-
dependent hash functions h1, h2, ... , hk where each
hi : U —* [0, M − 1] maps n-grams to locations in
the array A. The lookup function is then defined as
</bodyText>
<equation confidence="0.999452">
g : U —* [0,B − 1] by3
�A[hi(xi)] (1)
</equation>
<bodyText confidence="0.999978333333333">
where f(xi) is the fingerprint of n-gram xi and
A[hi(xi)] is the value stored in location hi(xi) of the
array A. Eq. (1) is evaluated to retrieve an n-gram’s
parameter during decoding. To encode our model
correctly we must ensure that g(xi) = v(xi) for all
n-grams in our set S. Generating A to encode this
</bodyText>
<footnote confidence="0.999112">
2The analysis assumes that all hash functions are random.
3We use ® to denote the exclusive bitwise OR operator.
</footnote>
<figureCaption confidence="0.999704">
Figure 1: Encoding an n-gram’s value in the array.
</figureCaption>
<bodyText confidence="0.9995885">
function for a given set of n-grams is a significant
challenge described in the following sections.
</bodyText>
<subsectionHeader confidence="0.999875">
3.3 Encoding n-grams in the model
</subsectionHeader>
<bodyText confidence="0.999760590909091">
All addresses in A are initialized to zero. The proce-
dure we use to ensure g(xi) = v(xi) for all xi E S
updates a single, unique location in A for each n-
gram xi. This location is chosen from among the k
locations given by hj(xi), j E [k]. Since the com-
posite function g(xi) depends on the values stored at
all k locations A[h1(xi)], A[h2(xi)], ... , A[hk(xi)]
in A, we must also ensure that once an n-gram xi
has been encoded in the model, these k locations
are not subsequently changed since this would inval-
idate the encoding; however, n-grams encoded later
may reference earlier entries and therefore locations
in A can effectively be ‘shared’ among parameters.
In the following section we describe a randomized
algorithm to find a suitable order in which to enter
n-grams in the model and, for each n-gram xi, de-
termine which of the k hash functions, say hj, can
be used to update A without invalidating previous
entries. Given this ordering of the n-grams and the
choice of hash function hj for each xi E S, it is clear
that the following update rule will encode xi in the
array A so that g(xi) will return v(xi) (cf. Eq.(1))
</bodyText>
<equation confidence="0.950567">
k
A[hj(xi)] = v(xi) ® f(xi) � A[hi(xi)]. (2)
i=1ni7�j
</equation>
<subsectionHeader confidence="0.995157">
3.4 Finding an Ordered Matching
</subsectionHeader>
<bodyText confidence="0.996006">
We now describe an algorithm (Algorithm 1; (Ma-
jewski et al., 1996)) that selects one of the k hash
</bodyText>
<equation confidence="0.993597333333333">
k
g(xi) = f(xi) �
i=1
</equation>
<page confidence="0.904767">
507
</page>
<bodyText confidence="0.998004447368421">
functions hj, j E [k] for each n-gram xi E S and
an order in which to apply the update rule Eq. (2) so
that g(xi) maps xi to v(xi) for all n-grams in S.
This problem is equivalent to finding an ordered
matching in a bipartite graph whose LHS nodes cor-
respond to n-grams in S and RHS nodes correspond
to locations in A. The graph initially contains edges
from each n-gram to each of the k locations in A
given by h1(xi), h2(xi), ... , hk(xi) (see Fig. (2)).
The algorithm uses the fact that any RHS node that
has degree one (i.e. a single edge) can be safely
matched with its associated LHS node since no re-
maining LHS nodes can be dependent on it.
We first create the graph using the k hash func-
tions hj, j E [k] and store a list (degree one)
of those RHS nodes (locations) with degree one.
The algorithm proceeds by removing nodes from
degree one in turn, pairing each RHS node with
the unique LHS node to which it is connected. We
then remove both nodes from the graph and push the
pair (xi, hj(xi)) onto a stack (matched). We also
remove any other edges from the matched LHS node
and add any RHS nodes that now have degree one
to degree one. The algorithm succeeds if, while
there are still n-grams left to match, degree one
is never empty. We then encode n-grams in the order
given by the stack (i.e., first-in-last-out).
Since we remove each location in A (RHS node)
from the graph as it is matched to an n-gram (LHS
node), each location will be associated with at most
one n-gram for updating. Moreover, since we match
an n-gram to a location only once the location has
degree one, we are guaranteed that any other n-
grams that depend on this location are already on
the stack and will therefore only be encoded once
we have updated this location. Hence dependencies
in g are respected and g(xi) = v(xi) will remain
true following the update in Eq. (2) for each xi E S.
</bodyText>
<subsectionHeader confidence="0.986058">
3.5 Choosing Random Hash Functions
</subsectionHeader>
<bodyText confidence="0.999059571428571">
The algorithm described above is not guaranteed to
succeed. Its success depends on the size of the array
M, the number of n-grams stored |S |and the choice
of random hash functions hj, j E [k]. Clearly we
require M &gt; |S|; in fact, an argument from Majew-
ski et al. (1996) implies that if M &gt; 1.23|S |and
k = 3, the algorithm succeeds with high probabil-
</bodyText>
<figureCaption confidence="0.935633">
Figure 2: The ordered matching algorithm: matched =
[(a,1), (b, 2), (d, 4), (c, 5)]
</figureCaption>
<bodyText confidence="0.965416">
ity. We use 2-universal hash functions (L. Carter
and M. Wegman, 1979) defined for a range of size
M via a prime P &gt; M and two random numbers
</bodyText>
<equation confidence="0.9508645">
1 &lt; aj &lt; P and 0 &lt; bj &lt; P for j E [k] as
hj(x) - ajx + bj mod P
</equation>
<bodyText confidence="0.9997425">
taken modulo M. We generate a set of k hash
functions by sampling k pairs of random numbers
(aj, bj), j E [k]. If the algorithm does not find
a matching with the current set of hash functions,
we re-sample these parameters and re-start the algo-
rithm. Since the probability of failure on a single
attempt is low when M &gt; 1.23|S|, the probability
of failing multiple times is very small.
</bodyText>
<subsectionHeader confidence="0.992171">
3.6 Querying the Model and False Positives
</subsectionHeader>
<bodyText confidence="0.9997967">
The construction we have described above ensures
that for any n-gram xi E S we have g(xi) = v(xi),
i.e., we retrieve the correct value. To retrieve a value
given an n-gram xi we simply compute the finger-
print f(xi), the hash functions hj(xi), j E [k] and
then return g(xi) using Eq. (1). Note that unlike the
constructions in (Talbot and Osborne, 2007b) and
(Church et al., 2007) no errors are possible for n-
grams stored in the model. Hence we will not make
errors for common n-grams that are typically in S.
</bodyText>
<page confidence="0.986679">
508
</page>
<table confidence="0.949708709677419">
Algorithm 1 Ordered Matching
Input : Set of n-grams S; k hash functions hj, j E [k];
number of available locations M.
Output: Ordered matching matched or FAIL.
matched &lt;-- [ ]
for all i E [0, M − 1] do
r2li &lt;-- 0
end for
for all xi E S do
l2ri &lt;-- 0
for all j E [k] do
l2ri &lt;-- l2ri U hj(xi)
r2lhj(xi) &lt;-- r2lhj(xi) U xi
end for
end for
degree one &lt;-- {i E [0, M − 1]  ||r2li |= 11
while |degree one |&gt; 1 do
rhs &lt;-- POP degree one
lhs &lt;-- POP r2l,.h3
PUSH (lhs, rhs) onto matched
for all rhs&apos; E l2rlh3 do
POP r2l,.h3�
if |r2lrhe = 1 then
degree one &lt;-- degree one U rhs&apos;
end if
end for
end while
if |matched |= |S |then
return matched
else
return FAIL
</table>
<subsectionHeader confidence="0.446417">
end if
</subsectionHeader>
<bodyText confidence="0.99747325">
On the other hand, querying the model with an n-
gram that was not stored, i.e. with xi E U \ S we
may erroneously return a value v E V.
Since the fingerprint f(xi) is assumed to be dis-
tributed uniformly at random (u.a.r.) in [0, B − 1],
g(xi) is also u.a.r. in [0, B−1] for xi E U\S. Hence
with |V |values stored in the model, the probability
that xi E U \ S is assigned a value in v E V is
</bodyText>
<equation confidence="0.668773">
Pr{g(xi) E V|xi E U \ S} = |V|/B.
</equation>
<bodyText confidence="0.9665175">
We refer to this event as a false positive. If V is fixed,
we can obtain a false positive rate c by setting B as
</bodyText>
<equation confidence="0.694126">
B = |V|/c.
</equation>
<bodyText confidence="0.998925857142857">
For example, if |V |is 128 then taking B = 1024
gives an error rate of c = 128/1024 = 0.125 with
each entry in A using �lo92 1024] = 10 bits. Clearly
B must be at least |V |in order to distinguish each
value. We refer to the additional bits allocated to
each location (i.e. lo92 B] − lo92 |V |or 3 in our
example) as error bits in our experiments below.
</bodyText>
<subsectionHeader confidence="0.99519">
3.7 Constructing the Full Model
</subsectionHeader>
<bodyText confidence="0.999972333333333">
When encoding a large set of n-gram/value pairs S,
Algorithm 1 will only be practical if the raw data
and graph can be held in memory as the perfect hash
function is generated. This makes it difficult to en-
code an extremely large set S into a single array A.
The solution we adopt is to split S into t smaller
sets S&apos;i, i E [t] that are arranged in lexicographic or-
der.4 We can then encode each subset in a separate
array A&apos;i, i E [t] in turn in memory. Querying each
of these arrays for each n-gram requested would be
inefficient and inflate the error rate since a false posi-
tive could occur on each individual array. Instead we
store an index of the final n-gram encoded in each
array and given a request for an n-gram’s value, per-
form a binary search for the appropriate array.
</bodyText>
<subsectionHeader confidence="0.989663">
3.8 Sanity Checks
</subsectionHeader>
<bodyText confidence="0.986568583333333">
Our models are consistent in the following sense
(w1,w2,...,wn) E S ==�&apos; (w2,...,wn) E S.
Hence we can infer that an n-gram can not be
present in the model, if the n −1-gram consisting of
the final n − 1 words has already tested false. Fol-
lowing (Talbot and Osborne, 2007a) we can avoid
unnecessary false positives by not querying for the
longer n-gram in such cases.
Backoff smoothing algorithms typically request
the longest n-gram supported by the model first, re-
questing shorter n-grams only if this is not found. In
our case, however, if a query is issued for the 5-gram
(w1, w2, w3, w4, w5) when only the unigram (w5) is
present in the model, the probability of a false posi-
tive using such a backoff procedure would not be E as
stated above, but rather the probability that we fail to
avoid an error on any of the four queries performed
prior to requesting the unigram, i.e. 1−(1−E)4 pz� 4E.
We therefore query the model first with the unigram
working up to the full n-gram requested by the de-
coder only if the preceding queries test positive. The
probability of returning a false positive for any n-
gram requested by the decoder (but not in the model)
will then be at most c.
</bodyText>
<footnote confidence="0.988755">
4In our system we use subsets of 5 million n-grams which
can easily be encoded using less than 2GB of working space.
</footnote>
<page confidence="0.998366">
509
</page>
<sectionHeader confidence="0.999879" genericHeader="method">
4 Experimental Set-up
</sectionHeader>
<subsectionHeader confidence="0.995525">
4.1 Distributed LM Framework
</subsectionHeader>
<bodyText confidence="0.999993928571429">
We deploy the randomized LM in a distributed
framework which allows it to scale more easily
by distributing it across multiple language model
servers. We encode the model stored on each lan-
guagage model server using the randomized scheme.
The proposed randomized LM can encode param-
eters estimated using any smoothing scheme (e.g.
Kneser-Ney, Katz etc.). Here we choose to work
with stupid backoff smoothing (Brants et al., 2007)
since this is significantly more efficient to train and
deploy in a distributed framework than a context-
dependent smoothing scheme such as Kneser-Ney.
Previous work (Brants et al., 2007) has shown it to
be appropriate to large-scale language modeling.
</bodyText>
<subsectionHeader confidence="0.97579">
4.2 LM Data Sets
</subsectionHeader>
<bodyText confidence="0.994629928571428">
The language model is trained on four data sets:
target: The English side of Arabic-English parallel
data provided by LDC (132 million tokens).
gigaword: The English Gigaword dataset provided
by LDC (3.7 billion tokens).
webnews: Data collected over several years, up to
January 2006 (34 billion tokens).
web: The Web 1T 5-gram Version 1 corpus provided
by LDC (1 trillion tokens).5
An initial experiment will use the Web 1T 5-gram
corpus only; all other experiments will use a log-
linear combination of models trained on each cor-
pus. The combined model is pre-compiled with
weights trained on development data by our system.
</bodyText>
<subsectionHeader confidence="0.996945">
4.3 Machine Translation
</subsectionHeader>
<bodyText confidence="0.998482875">
The SMT system used is based on the framework
proposed in (Och and Ney, 2004) where translation
is treated as the following optimization problem
Ai-bi(e, f). (3)
Here f is the source sentence that we wish to trans-
late, e is a translation in the target language, 4bi, i E
[M] are feature functions and Ai, i E [M] are
weights. (Some features may not depend on f.)
</bodyText>
<footnote confidence="0.74813">
5N-grams with count &lt; 40 are not included in this data set.
</footnote>
<table confidence="0.999751">
Full Set Entropy-Pruned
# 1-grams 13,588,391 13,588,391
# 2-grams 314,843,401 184,541,402
# 3-grams 977,069,902 439,430,328
# 4-grams 1,313,818,354 407,613,274
# 5-grams 1,176,470,663 238,348,867
Total 3,795,790,711 1,283,522,262
</table>
<tableCaption confidence="0.999849">
Table 1: Num. of n-grams in the Web 1T 5-gram corpus.
</tableCaption>
<sectionHeader confidence="0.99875" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999991538461539">
This section describes three sets of experiments:
first, we encode the Web 1T 5-gram corpus as a ran-
domized language model and compare the result-
ing size with other representations; then we mea-
sure false positive rates when requesting n-grams
for a held-out data set; finally we compare transla-
tion quality when using conventional (lossless) lan-
guages models and our randomized language model.
Note that the standard practice of measuring per-
plexity is not meaningful here since (1) for efficient
computation, the language model is not normalized;
and (2) even if this were not the case, quantization
and false positives would render it unnormalized.
</bodyText>
<subsectionHeader confidence="0.993788">
5.1 Encoding the Web 1T 5-gram corpus
</subsectionHeader>
<bodyText confidence="0.999872476190476">
We build a language model from the Web 1T 5-gram
corpus. Parameters, corresponding to negative loga-
rithms of relative frequencies, are quantized to 8-bits
using a uniform quantizer. More sophisticated quan-
tizers (e.g. (S. Lloyd, 1982)) may yield better results
but are beyond the scope of this paper.
Table 1 provides some statistics about the corpus.
We first encode the full set of n-grams, and then a
version that is reduced to approx. 1/3 of its original
size using entropy pruning (Stolcke, 1998).
Table 2 shows the total space and number of bytes
required per n-gram to encode the model under dif-
ferent schemes: “LDC gzip’d” is the size of the files
as delivered by LDC; “Trie” uses a compact trie rep-
resentation (e.g., (Clarkson et al., 1997; Church et
al., 2007)) with 3 byte word ids, 1 byte values, and 3
byte indices; “Block encoding” is the encoding used
in (Brants et al., 2007); and “randomized” uses our
novel randomized scheme with 12 error bits. The
latter requires around 60% of the space of the next
best representation and less than half of the com-
</bodyText>
<equation confidence="0.99084425">
e� = arg max
e
M
i=1
</equation>
<page confidence="0.991185">
510
</page>
<table confidence="0.9997647">
size (GB) bytes/n-gram
Full Set
LDC gzip’d 24.68 6.98
Trie 21.46 6.07
Block Encoding 18.00 5.14
Randomized 10.87 3.08
Entropy Pruned
Trie 7.70 6.44
Block Encoding 6.20 5.08
Randomized 3.68 3.08
</table>
<tableCaption confidence="0.9891135">
Table 2: Web 1T 5-gram language model sizes with dif-
ferent encodings. “Randomized” uses 12 error bits.
</tableCaption>
<bodyText confidence="0.989689">
monly used trie encoding. Our method is the only
one to use the same amount of space per parameter
for both full and entropy-pruned models.
</bodyText>
<subsectionHeader confidence="0.999349">
5.2 False Positive Rates
</subsectionHeader>
<bodyText confidence="0.999916931034483">
All n-grams explicitly inserted into our randomized
language model are retrieved without error; how-
ever, n-grams not stored may be incorrectly assigned
a value resulting in a false positive. Section (3) an-
alyzed the theoretical error rate; here, we measure
error rates in practice when retrieving n-grams for
approx. 11 million tokens of previously unseen text
(news articles published after the training data had
been collected). We measure this separately for all
n-grams of order 2 to 5 from the same text.
The language model is trained on the four data
sources listed above and contains 24 billion n-
grams. With 8-bit parameter values, the model
requires 55.2/69.0/82.7 GB storage when using
8/12/16 error bits respectively (this corresponds to
2.46/3.08/3.69 bytes/n-gram).
Using such a large language model results in a
large fraction of known n-grams in new text. Table
3 shows, e.g., that almost half of all 5-grams from
the new text were seen in the training data.
Column (1) in Table 4 shows the number of false
positives that occurred for this test data. Column
(2) shows this as a fraction of the number of unseen
n-grams in the data. This number should be close to
2−b where b is the number of error bits (i.e. 0.003906
for 8 bits and 0.000244 for 12 bits). The error rates
for bigrams are close to their expected values. The
numbers are much lower for higher n-gram orders
due to the use of sanity checks (see Section 3.8).
</bodyText>
<table confidence="0.998919">
total seen unseen
2gms 11,093,093 98.98% 1.02%
3gms 10,652,693 91.08% 8.92%
4gms 10,212,293 68.39% 31.61%
5gms 9,781,777 45.51% 54.49%
</table>
<tableCaption confidence="0.9928745">
Table 3: Number of n-grams in test set and percentages
of n-grams that were seen/unseen in the training data.
</tableCaption>
<table confidence="0.9987574">
(1) (2) (3)
false pos. false pos false pos
unseen total
8 error bits
2gms 376 0.003339 0.000034
3gms 2839 0.002988 0.000267
4gms 6659 0.002063 0.000652
5gms 6356 0.001192 0.000650
total 16230 0.001687 0.000388
12 error bits
2gms 25 0.000222 0.000002
3gms 182 0.000192 0.000017
4gms 416 0.000129 0.000041
5gms 407 0.000076 0.000042
total 1030 0.000107 0.000025
</table>
<tableCaption confidence="0.999787">
Table 4: False positive rates with 8 and 12 error bits.
</tableCaption>
<bodyText confidence="0.999928142857143">
The overall fraction of n-grams requested for
which an error occurs is of most interest in applica-
tions. This is shown in Column (3) and is around a
factor of 4 smaller than the values in Column (2). On
average, we expect to see 1 error in around 2,500 re-
quests when using 8 error bits, and 1 error in 40,000
requests with 12 error bits (see “total” row).
</bodyText>
<subsectionHeader confidence="0.98386">
5.3 Machine Translation
</subsectionHeader>
<bodyText confidence="0.999956818181818">
We run an improved version of our 2006 NIST MT
Evaluation entry for the Arabic-English “Unlimited”
data track.6 The language model is the same one as
in the previous section.
Table 5 shows baseline translation BLEU scores
for a lossless (non-randomized) language model
with parameter values quantized into 5 to 8 bits. We
use MT04 data for system development, with MT05
data and MT06 (“NIST” subset) data for blind test-
ing. As expected, results improve when using more
bits. There seems to be little benefit in going beyond
</bodyText>
<footnote confidence="0.997456">
6See http://www.nist.gov/speech/tests/mt/2006/doc/
</footnote>
<page confidence="0.990821">
511
</page>
<figure confidence="0.986069">
bits dev test test
MT04 MT05 MT06
5 0.5237 0.5608 0.4636
6 0.5280 0.5671 0.4649
7 0.5299 0.5691 0.4672
8 0.5304 0.5697 0.4663
8 9 10 11 12 13 14 15 16
Number of Error Bits
</figure>
<figureCaption confidence="0.947232">
Figure 3: BLEU scores on the MT05 data set.
</figureCaption>
<figure confidence="0.9887745">
8 9 10 11 12 13 14 15 16
Number of Error Bits
</figure>
<figureCaption confidence="0.992528">
Figure 4: BLEU scores on MT06 data (“NIST” subset).
</figureCaption>
<table confidence="0.995431833333333">
size dev test test
LM GB MT04 MT05 MT06
unpruned block 116 0.5304 0.5697 0.4663
unpruned rand 69 0.5299 0.5692 0.4659
pruned block 42 0.5294 0.5683 0.4665
pruned rand 27 0.5289 0.5679 0.4656
</table>
<tableCaption confidence="0.972846">
Table 6: Combining randomization and entropy pruning.
All models use 8-bit values; “rand” uses 12 error bits.
</tableCaption>
<figure confidence="0.958136535714286">
0.468
MT06 (NIST) BLEU 0.466
0.464
0.462
0.46
0.458
0.456
0.454
8 bit values
7 bit values
6 bit values
5 bit values
Table 5: Baseline BLEU scores with lossless n-gram
model and different quantization levels (bits).
MT05 BLEU
0.568
0.566
0.564
0.562
0.558
0.556
0.554
0.57
0.56
8 bit values
7 bit values
6 bit values
5 bit values
</figure>
<bodyText confidence="0.991888913043478">
8 bits. Overall, our baseline results compare favor-
ably to those reported on the NIST MT06 web site.
We now replace the language model with a ran-
domized version. Fig. 3 shows BLEU scores for the
MT05 evaluation set with parameter values quan-
tized into 5 to 8 bits and 8 to 16 additional ‘er-
ror’ bits. Figure 4 shows a similar graph for MT06
data. We again see improvements as quantization
uses more bits. There is a large drop in performance
when reducing the number of error bits from 10 to
8, while increasing it beyond 12 bits offers almost
no further gains with scores that are almost identi-
cal to the lossless model. Using 8-bit quantization
and 12 error bits results in an overall requirement of
(8+12)x1.23 = 24.6 bits = 3.08 bytes per n-gram.
All runs use the sanity checks described in Sec-
tion 3.8. Without sanity checks, scores drop, e.g. by
0.002 for 8-bit quantization and 12 error bits.
Randomization and entropy pruning can be com-
bined to achieve further space savings with minimal
loss in quality as shown in Table (6). The BLEU
score drops by between 0.0007 to 0.0018 while the
model is reduced to approx. 1/4 of its original size.
</bodyText>
<sectionHeader confidence="0.99991" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9999812">
We have presented a novel randomized language
model based on perfect hashing. It can associate
arbitrary parameter types with n-grams. Values ex-
plicitly inserted into the model are retrieved without
error; false positives may occur but are controlled
by the number of bits used per n-gram. The amount
of storage needed is independent of the size of the
vocabulary and the n-gram order. Lookup is very
efficient: the values of 3 cells in a large array are
combined with the fingerprint of an n-gram.
Experiments have shown that this randomized
language model can be combined with entropy prun-
ing to achieve further memory reductions; that error
rates occurring in practice are much lower than those
predicted by theoretical analysis due to the use of
runtime sanity checks; and that the same translation
quality as a lossless language model representation
can be achieved when using 12 ‘error’ bits, resulting
in approx. 3 bytes per n-gram (this includes one byte
to store parameter values).
</bodyText>
<page confidence="0.995207">
512
</page>
<sectionHeader confidence="0.998338" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998191819672131">
B. Bloom. 1970. Space/time tradeoffs in hash coding
with allowable errors. CACM, 13:422–426.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proceedings of EMNLP-
CoNLL 2007, Prague.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,
Jennifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467–479.
Peter Brown, Stephen Della Pietra, Vincent Della Pietra,
and Robert Mercer. 1993. The mathematics of ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263–311.
Larry Carter, Robert W. Floyd, John Gill, George
Markowsky, and Mark N. Wegman. 1978. Exact and
approximate membership testers. In STOC, pages 59–
65.
L. Carter and M. Wegman. 1979. Universal classes of
hash functions. Journal of Computer and System Sci-
ence, 18:143–154.
Bernard Chazelle, Joe Kilian, Ronitt Rubinfeld, and
Ayellet Tal. 2004. The Bloomier Filter: an efficient
data structure for static support lookup tables. In Proc.
15th ACM-SIAM Symposium on Discrete Algoritms,
pages 30–39.
Kenneth Church, Ted Hart, and Jianfeng Gao. 2007.
Compressing trigram language models with golomb
coding. In Proceedings of EMNLP-CoNLL 2007,
Prague, Czech Republic, June.
P. Clarkson and R. Rosenfeld. 1997. Statistical language
modeling using the CMU-Cambridge toolkit. In Pro-
ceedings of EUROSPEECH, vol. 1, pages 2707–2710,
Rhodes, Greece.
Ahmad Emami, Kishore Papineni, and Jeffrey Sorensen.
2007. Large-scale distributed language modeling. In
Proceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP)
2007, Hawaii, USA.
J. Goodman and J. Gao. 2000. Language model size re-
duction by pruning and clustering. In ICSLP’00, Bei-
jing, China.
S. Lloyd. 1982. Least squares quantization in PCM.
IEEE Transactions on Information Theory, 28(2):129–
137.
B.S. Majewski, N.C. Wormald, G. Havas, and Z.J. Czech.
1996. A family of perfect hashing methods. British
Computer Journal, 39(6):547–554.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417–449.
Andreas Stolcke. 1998. Entropy-based pruning of back-
off language models. In Proc. DARPA Broadcast News
Transcription and Understanding Workshop, pages
270–274.
D. Talbot and M. Osborne. 2007a. Randomised language
modelling for statistical machine translation. In 45th
Annual Meeting of the ACL 2007, Prague.
D. Talbot and M. Osborne. 2007b. Smoothed Bloom
filter language models: Tera-scale LMs on the cheap.
In EMNLP/CoNLL 2007, Prague.
</reference>
<page confidence="0.998827">
513
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.900354">
<title confidence="0.960083">Randomized Language Models via Perfect Hash Functions</title>
<affiliation confidence="0.998262">School of Informatics University of Edinburgh</affiliation>
<address confidence="0.991269">2 Buccleuch Place, Edinburgh, UK</address>
<email confidence="0.995544">d.r.talbot@sms.ed.ac.uk</email>
<author confidence="0.998143">Thorsten Brants</author>
<affiliation confidence="0.999054">Google Inc.</affiliation>
<address confidence="0.9993285">1600 Amphitheatre Parkway Mountain View, CA 94303, USA</address>
<email confidence="0.999886">brants@google.com</email>
<abstract confidence="0.9964325">We propose a succinct randomized language which employs a hash funcencode and their associated probabilities, backoff weights, or other parameters. The scheme can repreany standard model and is easily combined with existing model reduction techniques such as entropy-pruning. We demonstrate the space-savings of the scheme via machine translation experiments within a distributed language modeling framework.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Bloom</author>
</authors>
<title>Space/time tradeoffs in hash coding with allowable errors.</title>
<date>1970</date>
<journal>CACM,</journal>
<pages>13--422</pages>
<contexts>
<context position="6178" citStr="Bloom, 1970" startWordPosition="946" endWordPosition="947">ling to accept that occasionally our model will be unable to distinguish between distinct n-grams, then it is possible to store each parameter in constant space independent of both n and the vocabulary size (Carter et al., 1978), (Talbot and Osborne, 2007a). The space required in such a lossy encoding depends only on the range of values associated with the n-grams and the desired error rate, i.e. the probability with which two distinct n-grams are assigned the same fingerprint. 2.3 Previous Randomized LMs Recent work (Talbot and Osborne, 2007b) has used lossy encodings based on Bloom filters (Bloom, 1970) to represent logarithmically quantized corpus statistics for language modeling. While the approach results in significant space savings, working with corpus statistics, rather than n-gram probabilities directly, is computationally less efficient (particularly in a distributed setting) and introduces a dependency on the smoothing scheme used. It also makes it difficult to leverage existing model reduction strategies such as entropy-based pruning that are applied to final parameter estimates. In the next section we describe our randomized LM scheme based on perfect hash functions. This scheme c</context>
</contexts>
<marker>Bloom, 1970</marker>
<rawString>B. Bloom. 1970. Space/time tradeoffs in hash coding with allowable errors. CACM, 13:422–426.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLPCoNLL 2007,</booktitle>
<location>Prague.</location>
<contexts>
<context position="2049" citStr="Brants et al., 2007" startWordPosition="294" endWordPosition="297">anguage modeling. Efficiency is paramount in applications such as machine translation which make huge numbers of LM requests per sentence. To scale LMs to larger corpora with higher-order dependencies, researchers *Work completed while this author was at Google Inc. have considered alternative parameterizations such as class-based models (Brown et al., 1992), model reduction techniques such as entropy-based pruning (Stolcke, 1998), novel represention schemes such as suffix arrays (Emami et al., 2007), Golomb Coding (Church et al., 2007) and distributed language models that scale more readily (Brants et al., 2007). In this paper we propose a novel randomized language model. Recent work (Talbot and Osborne, 2007b) has demonstrated that randomized encodings can be used to represent n-gram counts for LMs with signficant space-savings, circumventing information-theoretic constraints on lossless data structures by allowing errors with some small probability. In contrast the representation scheme used by our model encodes parameters directly. It can be combined with any n-gram parameter estimation method and existing model reduction techniques such as entropy-based pruning. Parameters that are stored in the </context>
<context position="4681" citStr="Brants et al., 2007" startWordPosition="697" endWordPosition="700">g models can be unwieldy even for relatively small corpora. 2.1 Scaling Strategies Various strategies have been proposed to scale LMs to larger corpora and higher-order dependencies. Model-based techniques seek to parameterize the model more efficiently (e.g. latent variable models, neural networks) or to reduce the model size directly by pruning uninformative parameters, e.g. (Stolcke, 1998), (Goodman and Gao, 2000). Representationbased techniques attempt to reduce space requirements by representing the model more efficiently or in a form that scales more readily, e.g. (Emami et al., 2007), (Brants et al., 2007), (Church et al., 2007). 2.2 Lossy Randomized Encodings A fundamental result in information theory (Carter et al., 1978) states that a random set of objects cannot be stored using constant space per object as the universe from which the objects are drawn grows in size: the space required to uniquely identify an object increases as the set of possible objects from which it must be distinguished grows. In language modeling the universe under consideration is the set of all possible n-grams of length n for given vocabulary. Although n-grams observed in natural language corpora are not randomly di</context>
<context position="19073" citStr="Brants et al., 2007" startWordPosition="3363" endWordPosition="3366">then be at most c. 4In our system we use subsets of 5 million n-grams which can easily be encoded using less than 2GB of working space. 509 4 Experimental Set-up 4.1 Distributed LM Framework We deploy the randomized LM in a distributed framework which allows it to scale more easily by distributing it across multiple language model servers. We encode the model stored on each languagage model server using the randomized scheme. The proposed randomized LM can encode parameters estimated using any smoothing scheme (e.g. Kneser-Ney, Katz etc.). Here we choose to work with stupid backoff smoothing (Brants et al., 2007) since this is significantly more efficient to train and deploy in a distributed framework than a contextdependent smoothing scheme such as Kneser-Ney. Previous work (Brants et al., 2007) has shown it to be appropriate to large-scale language modeling. 4.2 LM Data Sets The language model is trained on four data sets: target: The English side of Arabic-English parallel data provided by LDC (132 million tokens). gigaword: The English Gigaword dataset provided by LDC (3.7 billion tokens). webnews: Data collected over several years, up to January 2006 (34 billion tokens). web: The Web 1T 5-gram Ve</context>
<context position="22293" citStr="Brants et al., 2007" startWordPosition="3896" endWordPosition="3899">yond the scope of this paper. Table 1 provides some statistics about the corpus. We first encode the full set of n-grams, and then a version that is reduced to approx. 1/3 of its original size using entropy pruning (Stolcke, 1998). Table 2 shows the total space and number of bytes required per n-gram to encode the model under different schemes: “LDC gzip’d” is the size of the files as delivered by LDC; “Trie” uses a compact trie representation (e.g., (Clarkson et al., 1997; Church et al., 2007)) with 3 byte word ids, 1 byte values, and 3 byte indices; “Block encoding” is the encoding used in (Brants et al., 2007); and “randomized” uses our novel randomized scheme with 12 error bits. The latter requires around 60% of the space of the next best representation and less than half of the come� = arg max e M i=1 510 size (GB) bytes/n-gram Full Set LDC gzip’d 24.68 6.98 Trie 21.46 6.07 Block Encoding 18.00 5.14 Randomized 10.87 3.08 Entropy Pruned Trie 7.70 6.44 Block Encoding 6.20 5.08 Randomized 3.68 3.08 Table 2: Web 1T 5-gram language model sizes with different encodings. “Randomized” uses 12 error bits. monly used trie encoding. Our method is the only one to use the same amount of space per parameter fo</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings of EMNLPCoNLL 2007, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V deSouza</author>
<author>Jennifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Classbased n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="1789" citStr="Brown et al., 1992" startWordPosition="255" endWordPosition="258">ning data can significantly improve performance in applications, however the size of the resulting LM can become prohibitive. With large monolingual corpora available in major languages, making use of all the available data is now a fundamental challenge in language modeling. Efficiency is paramount in applications such as machine translation which make huge numbers of LM requests per sentence. To scale LMs to larger corpora with higher-order dependencies, researchers *Work completed while this author was at Google Inc. have considered alternative parameterizations such as class-based models (Brown et al., 1992), model reduction techniques such as entropy-based pruning (Stolcke, 1998), novel represention schemes such as suffix arrays (Emami et al., 2007), Golomb Coding (Church et al., 2007) and distributed language models that scale more readily (Brants et al., 2007). In this paper we propose a novel randomized language model. Recent work (Talbot and Osborne, 2007b) has demonstrated that randomized encodings can be used to represent n-gram counts for LMs with signficant space-savings, circumventing information-theoretic constraints on lossless data structures by allowing errors with some small probab</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jennifer C. Lai, and Robert L. Mercer. 1992. Classbased n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Brown</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>Robert Mercer</author>
</authors>
<title>The mathematics of machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter Brown, Stephen Della Pietra, Vincent Della Pietra, and Robert Mercer. 1993. The mathematics of machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Larry Carter</author>
<author>Robert W Floyd</author>
<author>John Gill</author>
<author>George Markowsky</author>
<author>Mark N Wegman</author>
</authors>
<title>Exact and approximate membership testers.</title>
<date>1978</date>
<booktitle>In STOC,</booktitle>
<pages>59--65</pages>
<contexts>
<context position="4801" citStr="Carter et al., 1978" startWordPosition="715" endWordPosition="718"> to scale LMs to larger corpora and higher-order dependencies. Model-based techniques seek to parameterize the model more efficiently (e.g. latent variable models, neural networks) or to reduce the model size directly by pruning uninformative parameters, e.g. (Stolcke, 1998), (Goodman and Gao, 2000). Representationbased techniques attempt to reduce space requirements by representing the model more efficiently or in a form that scales more readily, e.g. (Emami et al., 2007), (Brants et al., 2007), (Church et al., 2007). 2.2 Lossy Randomized Encodings A fundamental result in information theory (Carter et al., 1978) states that a random set of objects cannot be stored using constant space per object as the universe from which the objects are drawn grows in size: the space required to uniquely identify an object increases as the set of possible objects from which it must be distinguished grows. In language modeling the universe under consideration is the set of all possible n-grams of length n for given vocabulary. Although n-grams observed in natural language corpora are not randomly distributed within this universe no lossless data structure that we are aware of can circumvent this space-dependency on b</context>
</contexts>
<marker>Carter, Floyd, Gill, Markowsky, Wegman, 1978</marker>
<rawString>Larry Carter, Robert W. Floyd, John Gill, George Markowsky, and Mark N. Wegman. 1978. Exact and approximate membership testers. In STOC, pages 59– 65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Carter</author>
<author>M Wegman</author>
</authors>
<title>Universal classes of hash functions.</title>
<date>1979</date>
<journal>Journal of Computer and System Science,</journal>
<pages>18--143</pages>
<marker>Carter, Wegman, 1979</marker>
<rawString>L. Carter and M. Wegman. 1979. Universal classes of hash functions. Journal of Computer and System Science, 18:143–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Chazelle</author>
<author>Joe Kilian</author>
<author>Ronitt Rubinfeld</author>
<author>Ayellet Tal</author>
</authors>
<title>The Bloomier Filter: an efficient data structure for static support lookup tables.</title>
<date>2004</date>
<booktitle>In Proc. 15th ACM-SIAM Symposium on Discrete Algoritms,</booktitle>
<pages>30--39</pages>
<contexts>
<context position="2948" citStr="Chazelle et al., 2004" startWordPosition="431" endWordPosition="434">sless data structures by allowing errors with some small probability. In contrast the representation scheme used by our model encodes parameters directly. It can be combined with any n-gram parameter estimation method and existing model reduction techniques such as entropy-based pruning. Parameters that are stored in the model are retrieved without error; however, false positives may occur whereby n-grams not in the model are incorrectly ‘found’ when requested. The false positive rate is determined by the space usage of the model. Our randomized language model is based on the Bloomier filter (Chazelle et al., 2004). We encode fingerprints (random hashes) of n-grams together with their associated probabilities using a perfect hash function generated at random (Majewski et al., 1996). Lookup is very efficient: the values of 3 cells in a large array are combined with the fingerprint of an n-gram. This paper focuses on machine translation. However, many of our findings should transfer to other applications of language modeling. 505 Proceedings of ACL-08: HLT, pages 505–513, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics 2 Scaling Language Models In statistical machine trans</context>
<context position="7014" citStr="Chazelle et al., 2004" startWordPosition="1071" endWordPosition="1074">s computationally less efficient (particularly in a distributed setting) and introduces a dependency on the smoothing scheme used. It also makes it difficult to leverage existing model reduction strategies such as entropy-based pruning that are applied to final parameter estimates. In the next section we describe our randomized LM scheme based on perfect hash functions. This scheme can be used to encode any standard n-gram model which may first be processed using any conventional model reduction technique. 3 Perfect Hash-based Language Models Our randomized LM is based on the Bloomier filter (Chazelle et al., 2004). We assume the n-grams and their associated parameter values have been precomputed and stored on disk. We then encode the model in an array such that each n-gram’s value can be retrieved. Storage for this array is the model’s only significant space requirement once constructed.1 The model uses randomization to map n-grams to fingerprints and to generate a perfect hash function that associates n-grams with their values. The model can erroneously return a value for an n-gram that was never actually stored, but will always return the correct value for an n-gram that is in the model. We will desc</context>
</contexts>
<marker>Chazelle, Kilian, Rubinfeld, Tal, 2004</marker>
<rawString>Bernard Chazelle, Joe Kilian, Ronitt Rubinfeld, and Ayellet Tal. 2004. The Bloomier Filter: an efficient data structure for static support lookup tables. In Proc. 15th ACM-SIAM Symposium on Discrete Algoritms, pages 30–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>Ted Hart</author>
<author>Jianfeng Gao</author>
</authors>
<title>Compressing trigram language models with golomb coding.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL 2007,</booktitle>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1971" citStr="Church et al., 2007" startWordPosition="281" endWordPosition="284">ages, making use of all the available data is now a fundamental challenge in language modeling. Efficiency is paramount in applications such as machine translation which make huge numbers of LM requests per sentence. To scale LMs to larger corpora with higher-order dependencies, researchers *Work completed while this author was at Google Inc. have considered alternative parameterizations such as class-based models (Brown et al., 1992), model reduction techniques such as entropy-based pruning (Stolcke, 1998), novel represention schemes such as suffix arrays (Emami et al., 2007), Golomb Coding (Church et al., 2007) and distributed language models that scale more readily (Brants et al., 2007). In this paper we propose a novel randomized language model. Recent work (Talbot and Osborne, 2007b) has demonstrated that randomized encodings can be used to represent n-gram counts for LMs with signficant space-savings, circumventing information-theoretic constraints on lossless data structures by allowing errors with some small probability. In contrast the representation scheme used by our model encodes parameters directly. It can be combined with any n-gram parameter estimation method and existing model reductio</context>
<context position="4704" citStr="Church et al., 2007" startWordPosition="701" endWordPosition="704">y even for relatively small corpora. 2.1 Scaling Strategies Various strategies have been proposed to scale LMs to larger corpora and higher-order dependencies. Model-based techniques seek to parameterize the model more efficiently (e.g. latent variable models, neural networks) or to reduce the model size directly by pruning uninformative parameters, e.g. (Stolcke, 1998), (Goodman and Gao, 2000). Representationbased techniques attempt to reduce space requirements by representing the model more efficiently or in a form that scales more readily, e.g. (Emami et al., 2007), (Brants et al., 2007), (Church et al., 2007). 2.2 Lossy Randomized Encodings A fundamental result in information theory (Carter et al., 1978) states that a random set of objects cannot be stored using constant space per object as the universe from which the objects are drawn grows in size: the space required to uniquely identify an object increases as the set of possible objects from which it must be distinguished grows. In language modeling the universe under consideration is the set of all possible n-grams of length n for given vocabulary. Although n-grams observed in natural language corpora are not randomly distributed within this u</context>
<context position="14765" citStr="Church et al., 2007" startWordPosition="2524" endWordPosition="2527">ons, we re-sample these parameters and re-start the algorithm. Since the probability of failure on a single attempt is low when M &gt; 1.23|S|, the probability of failing multiple times is very small. 3.6 Querying the Model and False Positives The construction we have described above ensures that for any n-gram xi E S we have g(xi) = v(xi), i.e., we retrieve the correct value. To retrieve a value given an n-gram xi we simply compute the fingerprint f(xi), the hash functions hj(xi), j E [k] and then return g(xi) using Eq. (1). Note that unlike the constructions in (Talbot and Osborne, 2007b) and (Church et al., 2007) no errors are possible for ngrams stored in the model. Hence we will not make errors for common n-grams that are typically in S. 508 Algorithm 1 Ordered Matching Input : Set of n-grams S; k hash functions hj, j E [k]; number of available locations M. Output: Ordered matching matched or FAIL. matched &lt;-- [ ] for all i E [0, M − 1] do r2li &lt;-- 0 end for for all xi E S do l2ri &lt;-- 0 for all j E [k] do l2ri &lt;-- l2ri U hj(xi) r2lhj(xi) &lt;-- r2lhj(xi) U xi end for end for degree one &lt;-- {i E [0, M − 1] ||r2li |= 11 while |degree one |&gt; 1 do rhs &lt;-- POP degree one lhs &lt;-- POP r2l,.h3 PUSH (lhs, rhs) </context>
<context position="22172" citStr="Church et al., 2007" startWordPosition="3873" endWordPosition="3876">bits using a uniform quantizer. More sophisticated quantizers (e.g. (S. Lloyd, 1982)) may yield better results but are beyond the scope of this paper. Table 1 provides some statistics about the corpus. We first encode the full set of n-grams, and then a version that is reduced to approx. 1/3 of its original size using entropy pruning (Stolcke, 1998). Table 2 shows the total space and number of bytes required per n-gram to encode the model under different schemes: “LDC gzip’d” is the size of the files as delivered by LDC; “Trie” uses a compact trie representation (e.g., (Clarkson et al., 1997; Church et al., 2007)) with 3 byte word ids, 1 byte values, and 3 byte indices; “Block encoding” is the encoding used in (Brants et al., 2007); and “randomized” uses our novel randomized scheme with 12 error bits. The latter requires around 60% of the space of the next best representation and less than half of the come� = arg max e M i=1 510 size (GB) bytes/n-gram Full Set LDC gzip’d 24.68 6.98 Trie 21.46 6.07 Block Encoding 18.00 5.14 Randomized 10.87 3.08 Entropy Pruned Trie 7.70 6.44 Block Encoding 6.20 5.08 Randomized 3.68 3.08 Table 2: Web 1T 5-gram language model sizes with different encodings. “Randomized” </context>
</contexts>
<marker>Church, Hart, Gao, 2007</marker>
<rawString>Kenneth Church, Ted Hart, and Jianfeng Gao. 2007. Compressing trigram language models with golomb coding. In Proceedings of EMNLP-CoNLL 2007, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Clarkson</author>
<author>R Rosenfeld</author>
</authors>
<title>Statistical language modeling using the CMU-Cambridge toolkit.</title>
<date>1997</date>
<booktitle>In Proceedings of EUROSPEECH,</booktitle>
<volume>1</volume>
<pages>2707--2710</pages>
<location>Rhodes, Greece.</location>
<marker>Clarkson, Rosenfeld, 1997</marker>
<rawString>P. Clarkson and R. Rosenfeld. 1997. Statistical language modeling using the CMU-Cambridge toolkit. In Proceedings of EUROSPEECH, vol. 1, pages 2707–2710, Rhodes, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmad Emami</author>
<author>Kishore Papineni</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Large-scale distributed language modeling.</title>
<date>2007</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</booktitle>
<location>Hawaii, USA.</location>
<contexts>
<context position="1934" citStr="Emami et al., 2007" startWordPosition="275" endWordPosition="278">ual corpora available in major languages, making use of all the available data is now a fundamental challenge in language modeling. Efficiency is paramount in applications such as machine translation which make huge numbers of LM requests per sentence. To scale LMs to larger corpora with higher-order dependencies, researchers *Work completed while this author was at Google Inc. have considered alternative parameterizations such as class-based models (Brown et al., 1992), model reduction techniques such as entropy-based pruning (Stolcke, 1998), novel represention schemes such as suffix arrays (Emami et al., 2007), Golomb Coding (Church et al., 2007) and distributed language models that scale more readily (Brants et al., 2007). In this paper we propose a novel randomized language model. Recent work (Talbot and Osborne, 2007b) has demonstrated that randomized encodings can be used to represent n-gram counts for LMs with signficant space-savings, circumventing information-theoretic constraints on lossless data structures by allowing errors with some small probability. In contrast the representation scheme used by our model encodes parameters directly. It can be combined with any n-gram parameter estimati</context>
<context position="4658" citStr="Emami et al., 2007" startWordPosition="693" endWordPosition="696">g corpus, the resulting models can be unwieldy even for relatively small corpora. 2.1 Scaling Strategies Various strategies have been proposed to scale LMs to larger corpora and higher-order dependencies. Model-based techniques seek to parameterize the model more efficiently (e.g. latent variable models, neural networks) or to reduce the model size directly by pruning uninformative parameters, e.g. (Stolcke, 1998), (Goodman and Gao, 2000). Representationbased techniques attempt to reduce space requirements by representing the model more efficiently or in a form that scales more readily, e.g. (Emami et al., 2007), (Brants et al., 2007), (Church et al., 2007). 2.2 Lossy Randomized Encodings A fundamental result in information theory (Carter et al., 1978) states that a random set of objects cannot be stored using constant space per object as the universe from which the objects are drawn grows in size: the space required to uniquely identify an object increases as the set of possible objects from which it must be distinguished grows. In language modeling the universe under consideration is the set of all possible n-grams of length n for given vocabulary. Although n-grams observed in natural language corp</context>
</contexts>
<marker>Emami, Papineni, Sorensen, 2007</marker>
<rawString>Ahmad Emami, Kishore Papineni, and Jeffrey Sorensen. 2007. Large-scale distributed language modeling. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2007, Hawaii, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
<author>J Gao</author>
</authors>
<title>Language model size reduction by pruning and clustering.</title>
<date>2000</date>
<booktitle>In ICSLP’00,</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="4481" citStr="Goodman and Gao, 2000" startWordPosition="665" endWordPosition="668">r each distinct word sequence of length l E [n] observed in the corpus. Since the number of parameters grows somewhat exponentially with n and linearly with the size of the training corpus, the resulting models can be unwieldy even for relatively small corpora. 2.1 Scaling Strategies Various strategies have been proposed to scale LMs to larger corpora and higher-order dependencies. Model-based techniques seek to parameterize the model more efficiently (e.g. latent variable models, neural networks) or to reduce the model size directly by pruning uninformative parameters, e.g. (Stolcke, 1998), (Goodman and Gao, 2000). Representationbased techniques attempt to reduce space requirements by representing the model more efficiently or in a form that scales more readily, e.g. (Emami et al., 2007), (Brants et al., 2007), (Church et al., 2007). 2.2 Lossy Randomized Encodings A fundamental result in information theory (Carter et al., 1978) states that a random set of objects cannot be stored using constant space per object as the universe from which the objects are drawn grows in size: the space required to uniquely identify an object increases as the set of possible objects from which it must be distinguished gro</context>
</contexts>
<marker>Goodman, Gao, 2000</marker>
<rawString>J. Goodman and J. Gao. 2000. Language model size reduction by pruning and clustering. In ICSLP’00, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lloyd</author>
</authors>
<title>Least squares quantization in PCM.</title>
<date>1982</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>28</volume>
<issue>2</issue>
<pages>137</pages>
<contexts>
<context position="21636" citStr="Lloyd, 1982" startWordPosition="3779" endWordPosition="3780"> using conventional (lossless) languages models and our randomized language model. Note that the standard practice of measuring perplexity is not meaningful here since (1) for efficient computation, the language model is not normalized; and (2) even if this were not the case, quantization and false positives would render it unnormalized. 5.1 Encoding the Web 1T 5-gram corpus We build a language model from the Web 1T 5-gram corpus. Parameters, corresponding to negative logarithms of relative frequencies, are quantized to 8-bits using a uniform quantizer. More sophisticated quantizers (e.g. (S. Lloyd, 1982)) may yield better results but are beyond the scope of this paper. Table 1 provides some statistics about the corpus. We first encode the full set of n-grams, and then a version that is reduced to approx. 1/3 of its original size using entropy pruning (Stolcke, 1998). Table 2 shows the total space and number of bytes required per n-gram to encode the model under different schemes: “LDC gzip’d” is the size of the files as delivered by LDC; “Trie” uses a compact trie representation (e.g., (Clarkson et al., 1997; Church et al., 2007)) with 3 byte word ids, 1 byte values, and 3 byte indices; “Bloc</context>
</contexts>
<marker>Lloyd, 1982</marker>
<rawString>S. Lloyd. 1982. Least squares quantization in PCM. IEEE Transactions on Information Theory, 28(2):129– 137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B S Majewski</author>
<author>N C Wormald</author>
<author>G Havas</author>
<author>Z J Czech</author>
</authors>
<title>A family of perfect hashing methods.</title>
<date>1996</date>
<journal>British Computer Journal,</journal>
<volume>39</volume>
<issue>6</issue>
<contexts>
<context position="3118" citStr="Majewski et al., 1996" startWordPosition="455" endWordPosition="458">ned with any n-gram parameter estimation method and existing model reduction techniques such as entropy-based pruning. Parameters that are stored in the model are retrieved without error; however, false positives may occur whereby n-grams not in the model are incorrectly ‘found’ when requested. The false positive rate is determined by the space usage of the model. Our randomized language model is based on the Bloomier filter (Chazelle et al., 2004). We encode fingerprints (random hashes) of n-grams together with their associated probabilities using a perfect hash function generated at random (Majewski et al., 1996). Lookup is very efficient: the values of 3 cells in a large array are combined with the fingerprint of an n-gram. This paper focuses on machine translation. However, many of our findings should transfer to other applications of language modeling. 505 Proceedings of ACL-08: HLT, pages 505–513, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics 2 Scaling Language Models In statistical machine translation (SMT), LMs are used to score candidate translations in the target language. These are typically n-gram models that approximate the probability of a word sequence b</context>
<context position="9157" citStr="Majewski et al., 1996" startWordPosition="1460" endWordPosition="1463">maps n-grams to integers between 0 and B − 1.2 The array A in which we encode n-gram/value pairs has addresses of size 1lo92 B] hence B will determine the amount of space used per n-gram. There is a trade-off between space and error rate since the larger B is, the lower the probability of a false positive. This is analyzed in detail below. For now we assume only that B is at least as large as the range of values stored in the model, i.e. B &gt; IVI. 3.2 Composite Perfect Hash Functions The function used to associate n-grams with their values (Eq. (1)) combines a composite perfect hash function (Majewski et al., 1996) with the fingerprint function. An example is shown in Fig. 1. The composite hash function is made up of k independent hash functions h1, h2, ... , hk where each hi : U —* [0, M − 1] maps n-grams to locations in the array A. The lookup function is then defined as g : U —* [0,B − 1] by3 �A[hi(xi)] (1) where f(xi) is the fingerprint of n-gram xi and A[hi(xi)] is the value stored in location hi(xi) of the array A. Eq. (1) is evaluated to retrieve an n-gram’s parameter during decoding. To encode our model correctly we must ensure that g(xi) = v(xi) for all n-grams in our set S. Generating A to enc</context>
<context position="11347" citStr="Majewski et al., 1996" startWordPosition="1865" endWordPosition="1869">rs. In the following section we describe a randomized algorithm to find a suitable order in which to enter n-grams in the model and, for each n-gram xi, determine which of the k hash functions, say hj, can be used to update A without invalidating previous entries. Given this ordering of the n-grams and the choice of hash function hj for each xi E S, it is clear that the following update rule will encode xi in the array A so that g(xi) will return v(xi) (cf. Eq.(1)) k A[hj(xi)] = v(xi) ® f(xi) � A[hi(xi)]. (2) i=1ni7�j 3.4 Finding an Ordered Matching We now describe an algorithm (Algorithm 1; (Majewski et al., 1996)) that selects one of the k hash k g(xi) = f(xi) � i=1 507 functions hj, j E [k] for each n-gram xi E S and an order in which to apply the update rule Eq. (2) so that g(xi) maps xi to v(xi) for all n-grams in S. This problem is equivalent to finding an ordered matching in a bipartite graph whose LHS nodes correspond to n-grams in S and RHS nodes correspond to locations in A. The graph initially contains edges from each n-gram to each of the k locations in A given by h1(xi), h2(xi), ... , hk(xi) (see Fig. (2)). The algorithm uses the fact that any RHS node that has degree one (i.e. a single edg</context>
<context position="13586" citStr="Majewski et al. (1996)" startWordPosition="2293" endWordPosition="2297"> the location has degree one, we are guaranteed that any other ngrams that depend on this location are already on the stack and will therefore only be encoded once we have updated this location. Hence dependencies in g are respected and g(xi) = v(xi) will remain true following the update in Eq. (2) for each xi E S. 3.5 Choosing Random Hash Functions The algorithm described above is not guaranteed to succeed. Its success depends on the size of the array M, the number of n-grams stored |S |and the choice of random hash functions hj, j E [k]. Clearly we require M &gt; |S|; in fact, an argument from Majewski et al. (1996) implies that if M &gt; 1.23|S |and k = 3, the algorithm succeeds with high probabilFigure 2: The ordered matching algorithm: matched = [(a,1), (b, 2), (d, 4), (c, 5)] ity. We use 2-universal hash functions (L. Carter and M. Wegman, 1979) defined for a range of size M via a prime P &gt; M and two random numbers 1 &lt; aj &lt; P and 0 &lt; bj &lt; P for j E [k] as hj(x) - ajx + bj mod P taken modulo M. We generate a set of k hash functions by sampling k pairs of random numbers (aj, bj), j E [k]. If the algorithm does not find a matching with the current set of hash functions, we re-sample these parameters and re</context>
</contexts>
<marker>Majewski, Wormald, Havas, Czech, 1996</marker>
<rawString>B.S. Majewski, N.C. Wormald, G. Havas, and Z.J. Czech. 1996. A family of perfect hashing methods. British Computer Journal, 39(6):547–554.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="20069" citStr="Och and Ney, 2004" startWordPosition="3526" endWordPosition="3529">ovided by LDC (132 million tokens). gigaword: The English Gigaword dataset provided by LDC (3.7 billion tokens). webnews: Data collected over several years, up to January 2006 (34 billion tokens). web: The Web 1T 5-gram Version 1 corpus provided by LDC (1 trillion tokens).5 An initial experiment will use the Web 1T 5-gram corpus only; all other experiments will use a loglinear combination of models trained on each corpus. The combined model is pre-compiled with weights trained on development data by our system. 4.3 Machine Translation The SMT system used is based on the framework proposed in (Och and Ney, 2004) where translation is treated as the following optimization problem Ai-bi(e, f). (3) Here f is the source sentence that we wish to translate, e is a translation in the target language, 4bi, i E [M] are feature functions and Ai, i E [M] are weights. (Some features may not depend on f.) 5N-grams with count &lt; 40 are not included in this data set. Full Set Entropy-Pruned # 1-grams 13,588,391 13,588,391 # 2-grams 314,843,401 184,541,402 # 3-grams 977,069,902 439,430,328 # 4-grams 1,313,818,354 407,613,274 # 5-grams 1,176,470,663 238,348,867 Total 3,795,790,711 1,283,522,262 Table 1: Num. of n-grams</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz J. Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Entropy-based pruning of backoff language models.</title>
<date>1998</date>
<booktitle>In Proc. DARPA Broadcast News Transcription and Understanding Workshop,</booktitle>
<pages>270--274</pages>
<contexts>
<context position="1863" citStr="Stolcke, 1998" startWordPosition="266" endWordPosition="267">ze of the resulting LM can become prohibitive. With large monolingual corpora available in major languages, making use of all the available data is now a fundamental challenge in language modeling. Efficiency is paramount in applications such as machine translation which make huge numbers of LM requests per sentence. To scale LMs to larger corpora with higher-order dependencies, researchers *Work completed while this author was at Google Inc. have considered alternative parameterizations such as class-based models (Brown et al., 1992), model reduction techniques such as entropy-based pruning (Stolcke, 1998), novel represention schemes such as suffix arrays (Emami et al., 2007), Golomb Coding (Church et al., 2007) and distributed language models that scale more readily (Brants et al., 2007). In this paper we propose a novel randomized language model. Recent work (Talbot and Osborne, 2007b) has demonstrated that randomized encodings can be used to represent n-gram counts for LMs with signficant space-savings, circumventing information-theoretic constraints on lossless data structures by allowing errors with some small probability. In contrast the representation scheme used by our model encodes par</context>
<context position="4456" citStr="Stolcke, 1998" startWordPosition="663" endWordPosition="664">ith parameters for each distinct word sequence of length l E [n] observed in the corpus. Since the number of parameters grows somewhat exponentially with n and linearly with the size of the training corpus, the resulting models can be unwieldy even for relatively small corpora. 2.1 Scaling Strategies Various strategies have been proposed to scale LMs to larger corpora and higher-order dependencies. Model-based techniques seek to parameterize the model more efficiently (e.g. latent variable models, neural networks) or to reduce the model size directly by pruning uninformative parameters, e.g. (Stolcke, 1998), (Goodman and Gao, 2000). Representationbased techniques attempt to reduce space requirements by representing the model more efficiently or in a form that scales more readily, e.g. (Emami et al., 2007), (Brants et al., 2007), (Church et al., 2007). 2.2 Lossy Randomized Encodings A fundamental result in information theory (Carter et al., 1978) states that a random set of objects cannot be stored using constant space per object as the universe from which the objects are drawn grows in size: the space required to uniquely identify an object increases as the set of possible objects from which it </context>
<context position="21903" citStr="Stolcke, 1998" startWordPosition="3826" endWordPosition="3827">t the case, quantization and false positives would render it unnormalized. 5.1 Encoding the Web 1T 5-gram corpus We build a language model from the Web 1T 5-gram corpus. Parameters, corresponding to negative logarithms of relative frequencies, are quantized to 8-bits using a uniform quantizer. More sophisticated quantizers (e.g. (S. Lloyd, 1982)) may yield better results but are beyond the scope of this paper. Table 1 provides some statistics about the corpus. We first encode the full set of n-grams, and then a version that is reduced to approx. 1/3 of its original size using entropy pruning (Stolcke, 1998). Table 2 shows the total space and number of bytes required per n-gram to encode the model under different schemes: “LDC gzip’d” is the size of the files as delivered by LDC; “Trie” uses a compact trie representation (e.g., (Clarkson et al., 1997; Church et al., 2007)) with 3 byte word ids, 1 byte values, and 3 byte indices; “Block encoding” is the encoding used in (Brants et al., 2007); and “randomized” uses our novel randomized scheme with 12 error bits. The latter requires around 60% of the space of the next best representation and less than half of the come� = arg max e M i=1 510 size (GB</context>
</contexts>
<marker>Stolcke, 1998</marker>
<rawString>Andreas Stolcke. 1998. Entropy-based pruning of backoff language models. In Proc. DARPA Broadcast News Transcription and Understanding Workshop, pages 270–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Talbot</author>
<author>M Osborne</author>
</authors>
<title>Randomised language modelling for statistical machine translation.</title>
<date>2007</date>
<booktitle>In 45th Annual Meeting of the ACL 2007,</booktitle>
<location>Prague.</location>
<contexts>
<context position="2148" citStr="Talbot and Osborne, 2007" startWordPosition="311" endWordPosition="314">e huge numbers of LM requests per sentence. To scale LMs to larger corpora with higher-order dependencies, researchers *Work completed while this author was at Google Inc. have considered alternative parameterizations such as class-based models (Brown et al., 1992), model reduction techniques such as entropy-based pruning (Stolcke, 1998), novel represention schemes such as suffix arrays (Emami et al., 2007), Golomb Coding (Church et al., 2007) and distributed language models that scale more readily (Brants et al., 2007). In this paper we propose a novel randomized language model. Recent work (Talbot and Osborne, 2007b) has demonstrated that randomized encodings can be used to represent n-gram counts for LMs with signficant space-savings, circumventing information-theoretic constraints on lossless data structures by allowing errors with some small probability. In contrast the representation scheme used by our model encodes parameters directly. It can be combined with any n-gram parameter estimation method and existing model reduction techniques such as entropy-based pruning. Parameters that are stored in the model are retrieved without error; however, false positives may occur whereby n-grams not in the mo</context>
<context position="5821" citStr="Talbot and Osborne, 2007" startWordPosition="886" endWordPosition="889">ocabulary. Although n-grams observed in natural language corpora are not randomly distributed within this universe no lossless data structure that we are aware of can circumvent this space-dependency on both the n-gram order and the vocabulary size. Hence as the training corpus and vocabulary grow, a model will require more space per parameter. However, if we are willing to accept that occasionally our model will be unable to distinguish between distinct n-grams, then it is possible to store each parameter in constant space independent of both n and the vocabulary size (Carter et al., 1978), (Talbot and Osborne, 2007a). The space required in such a lossy encoding depends only on the range of values associated with the n-grams and the desired error rate, i.e. the probability with which two distinct n-grams are assigned the same fingerprint. 2.3 Previous Randomized LMs Recent work (Talbot and Osborne, 2007b) has used lossy encodings based on Bloom filters (Bloom, 1970) to represent logarithmically quantized corpus statistics for language modeling. While the approach results in significant space savings, working with corpus statistics, rather than n-gram probabilities directly, is computationally less effici</context>
<context position="14737" citStr="Talbot and Osborne, 2007" startWordPosition="2519" endWordPosition="2522">h the current set of hash functions, we re-sample these parameters and re-start the algorithm. Since the probability of failure on a single attempt is low when M &gt; 1.23|S|, the probability of failing multiple times is very small. 3.6 Querying the Model and False Positives The construction we have described above ensures that for any n-gram xi E S we have g(xi) = v(xi), i.e., we retrieve the correct value. To retrieve a value given an n-gram xi we simply compute the fingerprint f(xi), the hash functions hj(xi), j E [k] and then return g(xi) using Eq. (1). Note that unlike the constructions in (Talbot and Osborne, 2007b) and (Church et al., 2007) no errors are possible for ngrams stored in the model. Hence we will not make errors for common n-grams that are typically in S. 508 Algorithm 1 Ordered Matching Input : Set of n-grams S; k hash functions hj, j E [k]; number of available locations M. Output: Ordered matching matched or FAIL. matched &lt;-- [ ] for all i E [0, M − 1] do r2li &lt;-- 0 end for for all xi E S do l2ri &lt;-- 0 for all j E [k] do l2ri &lt;-- l2ri U hj(xi) r2lhj(xi) &lt;-- r2lhj(xi) U xi end for end for degree one &lt;-- {i E [0, M − 1] ||r2li |= 11 while |degree one |&gt; 1 do rhs &lt;-- POP degree one lhs &lt;-- </context>
<context position="17561" citStr="Talbot and Osborne, 2007" startWordPosition="3099" endWordPosition="3102">memory. Querying each of these arrays for each n-gram requested would be inefficient and inflate the error rate since a false positive could occur on each individual array. Instead we store an index of the final n-gram encoded in each array and given a request for an n-gram’s value, perform a binary search for the appropriate array. 3.8 Sanity Checks Our models are consistent in the following sense (w1,w2,...,wn) E S ==�&apos; (w2,...,wn) E S. Hence we can infer that an n-gram can not be present in the model, if the n −1-gram consisting of the final n − 1 words has already tested false. Following (Talbot and Osborne, 2007a) we can avoid unnecessary false positives by not querying for the longer n-gram in such cases. Backoff smoothing algorithms typically request the longest n-gram supported by the model first, requesting shorter n-grams only if this is not found. In our case, however, if a query is issued for the 5-gram (w1, w2, w3, w4, w5) when only the unigram (w5) is present in the model, the probability of a false positive using such a backoff procedure would not be E as stated above, but rather the probability that we fail to avoid an error on any of the four queries performed prior to requesting the unig</context>
</contexts>
<marker>Talbot, Osborne, 2007</marker>
<rawString>D. Talbot and M. Osborne. 2007a. Randomised language modelling for statistical machine translation. In 45th Annual Meeting of the ACL 2007, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Talbot</author>
<author>M Osborne</author>
</authors>
<title>Smoothed Bloom filter language models: Tera-scale LMs on the cheap.</title>
<date>2007</date>
<booktitle>In EMNLP/CoNLL 2007,</booktitle>
<location>Prague.</location>
<contexts>
<context position="2148" citStr="Talbot and Osborne, 2007" startWordPosition="311" endWordPosition="314">e huge numbers of LM requests per sentence. To scale LMs to larger corpora with higher-order dependencies, researchers *Work completed while this author was at Google Inc. have considered alternative parameterizations such as class-based models (Brown et al., 1992), model reduction techniques such as entropy-based pruning (Stolcke, 1998), novel represention schemes such as suffix arrays (Emami et al., 2007), Golomb Coding (Church et al., 2007) and distributed language models that scale more readily (Brants et al., 2007). In this paper we propose a novel randomized language model. Recent work (Talbot and Osborne, 2007b) has demonstrated that randomized encodings can be used to represent n-gram counts for LMs with signficant space-savings, circumventing information-theoretic constraints on lossless data structures by allowing errors with some small probability. In contrast the representation scheme used by our model encodes parameters directly. It can be combined with any n-gram parameter estimation method and existing model reduction techniques such as entropy-based pruning. Parameters that are stored in the model are retrieved without error; however, false positives may occur whereby n-grams not in the mo</context>
<context position="5821" citStr="Talbot and Osborne, 2007" startWordPosition="886" endWordPosition="889">ocabulary. Although n-grams observed in natural language corpora are not randomly distributed within this universe no lossless data structure that we are aware of can circumvent this space-dependency on both the n-gram order and the vocabulary size. Hence as the training corpus and vocabulary grow, a model will require more space per parameter. However, if we are willing to accept that occasionally our model will be unable to distinguish between distinct n-grams, then it is possible to store each parameter in constant space independent of both n and the vocabulary size (Carter et al., 1978), (Talbot and Osborne, 2007a). The space required in such a lossy encoding depends only on the range of values associated with the n-grams and the desired error rate, i.e. the probability with which two distinct n-grams are assigned the same fingerprint. 2.3 Previous Randomized LMs Recent work (Talbot and Osborne, 2007b) has used lossy encodings based on Bloom filters (Bloom, 1970) to represent logarithmically quantized corpus statistics for language modeling. While the approach results in significant space savings, working with corpus statistics, rather than n-gram probabilities directly, is computationally less effici</context>
<context position="14737" citStr="Talbot and Osborne, 2007" startWordPosition="2519" endWordPosition="2522">h the current set of hash functions, we re-sample these parameters and re-start the algorithm. Since the probability of failure on a single attempt is low when M &gt; 1.23|S|, the probability of failing multiple times is very small. 3.6 Querying the Model and False Positives The construction we have described above ensures that for any n-gram xi E S we have g(xi) = v(xi), i.e., we retrieve the correct value. To retrieve a value given an n-gram xi we simply compute the fingerprint f(xi), the hash functions hj(xi), j E [k] and then return g(xi) using Eq. (1). Note that unlike the constructions in (Talbot and Osborne, 2007b) and (Church et al., 2007) no errors are possible for ngrams stored in the model. Hence we will not make errors for common n-grams that are typically in S. 508 Algorithm 1 Ordered Matching Input : Set of n-grams S; k hash functions hj, j E [k]; number of available locations M. Output: Ordered matching matched or FAIL. matched &lt;-- [ ] for all i E [0, M − 1] do r2li &lt;-- 0 end for for all xi E S do l2ri &lt;-- 0 for all j E [k] do l2ri &lt;-- l2ri U hj(xi) r2lhj(xi) &lt;-- r2lhj(xi) U xi end for end for degree one &lt;-- {i E [0, M − 1] ||r2li |= 11 while |degree one |&gt; 1 do rhs &lt;-- POP degree one lhs &lt;-- </context>
<context position="17561" citStr="Talbot and Osborne, 2007" startWordPosition="3099" endWordPosition="3102">memory. Querying each of these arrays for each n-gram requested would be inefficient and inflate the error rate since a false positive could occur on each individual array. Instead we store an index of the final n-gram encoded in each array and given a request for an n-gram’s value, perform a binary search for the appropriate array. 3.8 Sanity Checks Our models are consistent in the following sense (w1,w2,...,wn) E S ==�&apos; (w2,...,wn) E S. Hence we can infer that an n-gram can not be present in the model, if the n −1-gram consisting of the final n − 1 words has already tested false. Following (Talbot and Osborne, 2007a) we can avoid unnecessary false positives by not querying for the longer n-gram in such cases. Backoff smoothing algorithms typically request the longest n-gram supported by the model first, requesting shorter n-grams only if this is not found. In our case, however, if a query is issued for the 5-gram (w1, w2, w3, w4, w5) when only the unigram (w5) is present in the model, the probability of a false positive using such a backoff procedure would not be E as stated above, but rather the probability that we fail to avoid an error on any of the four queries performed prior to requesting the unig</context>
</contexts>
<marker>Talbot, Osborne, 2007</marker>
<rawString>D. Talbot and M. Osborne. 2007b. Smoothed Bloom filter language models: Tera-scale LMs on the cheap. In EMNLP/CoNLL 2007, Prague.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>