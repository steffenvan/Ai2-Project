<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000014">
<title confidence="0.9991235">
Keyword Extraction using Term-Domain Interdependence for
Dictation of Radio News
</title>
<author confidence="0.998338">
Yoshimi Suzuki Fumiyo Fukumoto Yoshihiro Sekiguchi
</author>
<affiliation confidence="0.998508">
Dept. of Computer Science and Media Engineering
Yamanashi University
</affiliation>
<address confidence="0.981139">
4-3-11 Takeda, Kofu 400 Japan
</address>
<email confidence="0.982259">
{ysuzukiesuwa,fukumotalskye,sekigutiesaiko}.esi.yamanashi.ac.jp
</email>
<sectionHeader confidence="0.992789" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999941166666667">
In this paper, we propose keyword extraction
method for dictation of radio news which con-
sists of several domains. In our method, news-
paper articles which are automatically classified
into suitable domains are used in order to calcu-
late feature vectors. The feature vectors shows
term-domain interdependence and are used for
selecting a suitable domain of each part of ra-
dio news. Keywords are extracted by using the
selected domain. The results of keyword extrac-
tion experiments showed that our methods are
robust and effective for dictation of radio news.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.991919729166666">
Recently, many speech recognition systems
are designed for various tasks. However, most
of them are restricted to certain tasks, for ex-
ample, a tourist information and a hamburger
shop. Speech recognition systems for the task
which consists of various domains seems to be
required for some tasks, e.g. a closed caption
system for TV and a transcription system of
public proceedings. In order to recognize spoken
discourse which has several domains, the speech
recognition system has to have large vocabu-
lary. Therefore, it is necessary to limit word
search space using linguistic restricts, e.g. do-
main identification.
There have been many studies of do-
main identification which used term weight-
ing (J.McDonough et al., 1994; Yokoi et al.,
1997). McDonough proposed a topic identifi-
cation method on switch board corpus. He re-
ported that the result was best when the num-
ber of words in keyword dictionary was about
800. In his method, duration of discourses of
switch board corpora is rather long and there
are many keywords in the discourse. However,
for a short discourse, there are few keywords
in a short discourse. Yokoi also proposed a
topic identification method using co-occurrence
of words for topic identification (Yokoi et al.,
1997). He classified each dictated sentence of
news into 8 topics. In TV or Radio news, how-
ever, it is difficult to segment each sentence au-
tomatically. Sekine proposed a method for se-
lecting a suitable sentence from sentences which
were extracted by a speech recognition system
using statistical language model (Sekine, 1996).
However, if the statistical model is used for ex-
traction of sentence candidates, we will obtain
higher recognition accuracy.
Some initial studies of transcription of broad-
cast news proceed (Bakis et al., 1997). However
there are some remaining problems, e.g. speak-
ing styles and domain identification.
We conducted domain identification and key-
word extraction experiment (Suzuki et al.,
1997) for radio news. In the experiment,
we classified radio news into 5 domains (i.e.
accident, economy, international, politics and
sports). The problems which we faced with are;
</bodyText>
<listItem confidence="0.985249">
1. Classification of newspaper articles into
suitable domains could not be performed
automatically.
2. Many incorrect keywords are extracted, be-
cause the number of domains was few.
</listItem>
<bodyText confidence="0.9994864">
In this paper, we propose a method for key-
word extraction using term-domain interdepen-
dence in order to cope with these two problems.
The results of the experiments demonstrated
the effectiveness of our method.
</bodyText>
<sectionHeader confidence="0.555023" genericHeader="method">
2 An overview of our method
</sectionHeader>
<bodyText confidence="0.9910915">
Figure 1 shows an overview of our method.
Our method consists of two procedures. In the
procedure of term-domain interdependence cal-
culation, the system calculates feature vectors
</bodyText>
<page confidence="0.98724">
1272
</page>
<bodyText confidence="0.999767583333333">
of term-domain interdependence using an ency-
clopedia of current term and newspaper articles.
In the procedure of keyword extraction in radio
news, firstly, the system divides radio news into
segments according to the length of pauses. We
call the segments units. The domain which has
the largest similarity between the unit of news
and the feature vector of each domain is selected
as domain of the unit. Finally, the system ex-
tracts keywords in each unit using the feature
vector of selected domain which is selected by
domain identification.
</bodyText>
<subsectionHeader confidence="0.527059">
&apos;Explanations of &apos;Newspaper
</subsectionHeader>
<figure confidence="0.936426666666667">
,an en clopedia, articles
Feature vectors •
(FeaVa)
</figure>
<figureCaption confidence="0.999835">
Figure 1: An overview of our method
</figureCaption>
<sectionHeader confidence="0.884321" genericHeader="method">
3 Calculating feature vectors
</sectionHeader>
<bodyText confidence="0.9999173125">
In the procedure of term-domain interdepen-
dence calculation, We calculate likelihood of ap-
pearance of each noun in each domain. Figure 2
shows how to calculate feature vectors of term-
domain interdependence.
In our previous experiments, we used 5 do-
mains which were sorted manually and calcu-
lated 5 feature vectors for classifying domains of
each unit of radio news and for extracting key-
words. Our previous system could not extract
some keywords because of many noisy keywords.
In our method, newspaper articles and units of
radio news are classified into many domains. At
each domain, a feature vector is calculated by
an encyclopedia of current terms and newspaper
articles.
</bodyText>
<subsectionHeader confidence="0.9937855">
3.1: Sorting newspaper articles
according to their domains
</subsectionHeader>
<bodyText confidence="0.997472">
Firstly, all sentences in the encyclopedia are
analyzed morpheme by Chasen (Matsumoto et
</bodyText>
<figure confidence="0.919194125">
Calculating frequency vectors (FreqVe
Calculating similarity
between FeaVe and FreqVa
.C7
(141 feature vectors (FeaVeD---
Calculating x- values of
each noun on domains
(141 feature vectors (FeaVa))
</figure>
<figureCaption confidence="0.998922">
Figure 2: Calculating feature vectors
</figureCaption>
<bodyText confidence="0.999070153846154">
al., 1997) and nouns which frequently appear
are extracted. A feature vector is calculated by
frequency of each noun at each domain. We
call the feature vector FeaVe. Each element
of FeaVe is a X2 value (Suzuki et al., 1997).
Then, nouns are extracted from newspaper ar-
ticles by a morphological analysis system (Mat-
sumoto et al., 1997), and frequency of each noun
are counted. Next, similarity between FeaVe of
each domain and each newspaper article are cal-
culated by using formula (1). Finally, a suitable
domain of each newspaper article are selected by
using formula (2).
</bodyText>
<equation confidence="0.992976">
Sim(i, j) = FeaVei • FreqVa i (1)
. .
Domains = arg max Sim(z, 2) (2)
1&lt;j&lt;N
</equation>
<bodyText confidence="0.999808">
where i means a newspaper article and j means
a domain. (.) means operation of inner vector.
</bodyText>
<subsectionHeader confidence="0.9373415">
3.2 Term-domain interdependence
represented by feature vectors
</subsectionHeader>
<bodyText confidence="0.999975444444444">
Firstly, at each newspaper articles, less than
5 domains whose similarities between each ar-
ticle and each domain are large are selected.
Then, at each selected domain, the frequency
vector is modified according to similarity value
and frequency of each noun in the article. For
example, If an article whose selected domains
are &amp;quot;political party&amp;quot; and &amp;quot;election&amp;quot;, and simi-
larity between the article and &amp;quot;political party&amp;quot;
</bodyText>
<figure confidence="0.954281444444444">
Domain Identification
D7
E&gt; Keyword Extraction
D1
• . •
D7
D141
President
Democratic party
I I
Calculation of term-domain
interdependence
Keyword extraction
&apos;An encyclopedia of current term&apos;
141 domains 10,236 explanations
[Sorting explanations I
r Newspaper articles
Labout 110,000 articles )
.C7
[Separatin
articles
Calculating frequency vectors (FreqVa)
Extractina nouns
Extracting nouns
Radio News
Feature vectors
(FeaVe)
D1
• • •
D7
D141
C&gt;
Calculating X values of
each noun on domains
Sorting articles into domains
according to similarity
</figure>
<page confidence="0.952326">
1273
</page>
<bodyText confidence="0.998495">
and similarity between the article and &amp;quot;elec-
tion&amp;quot; are 100 and 60 respectively, each fre-
quency vector is calculated by formula (3) and
formula (4).
</bodyText>
<equation confidence="0.58283525">
100
FreqVpr, FreqVppi FreqVai x 1-To- (3)
60
FreqVei = FreqVe&apos;t FreqVai x —100 (4)
</equation>
<bodyText confidence="0.999606">
where i means a newspaper article.
Then, we calculate feature vectors FeaV a us-
ing FreqV using the method mentioned in our
previous paper (Suzuki et al., 1997). Each el-
ement of feature vectors shows x2 value of the
domain and wordk. All wordk (1 &lt; k &lt; M :M
means the number of elements of a feature vec-
tor) are put into the keyword dictionary.
</bodyText>
<sectionHeader confidence="0.995537" genericHeader="method">
4 Keyword extraction
</sectionHeader>
<bodyText confidence="0.999664555555556">
Input news stories are represented by
phoneme lattice. There are no marks for word
boundaries in input news stories. Phoneme lat-
tices are segmented by pauses which are longer
than 0.5 second in recorded radio news. The
system selects a domain of each unit which is
a segmented phoneme lattice. At each frame of
phoneme lattice, the system selects maximum
20 words from keyword dictionary.
</bodyText>
<subsectionHeader confidence="0.783387">
4.1 Similarity between a domain and
an unit
</subsectionHeader>
<bodyText confidence="0.9999085">
We define the words whose x2 values in
the feature vector of domainj are large as key-
words of the domainj. In an unit of radio
news about &amp;quot;political party&amp;quot;, there are many
keywords of &amp;quot;political party&amp;quot; and the x2 value
of keywords in the feature vector of &amp;quot;political
party&amp;quot; is large. Therefore, sum of Xw2 ,politicalparty
tends to be large (w: a word in the unit). In our
method, the system selects a word path whose
sum of x12,,i is maximized in the word lattice
at domainj. The similarity between unit, and
domainj is calculated by formula (5).
</bodyText>
<equation confidence="0.688347666666667">
Sim(i, j) = max Sim&apos;(i, j)
all paths
max E np(wordk) x xl(5)
</equation>
<subsectionHeader confidence="0.40404">
all paths
</subsectionHeader>
<bodyText confidence="0.995605416666667">
In formula (5), wordk is a word in the
word lattice, and each selected word does not
share any frames with any other selected words.
np(wordk) is the number of phonemes of wordk.
xi2e,j is x2value of wordk for domainj.
The system selects a word path whose
Sim&apos;(i, j) is the largest among all word paths
for domainj.
Figure 3 shows the method of calculating sim-
ilarity between unit, and domainm. The sys-
tem selects a word path whose Sinil(uniti, D1)
is larger than those of any other word paths.
</bodyText>
<figureCaption confidence="0.984871">
Figure 3: Calculating similarity between unit,
and D1
</figureCaption>
<subsectionHeader confidence="0.604704">
4.2 Domain identification and keyword
extraction
</subsectionHeader>
<bodyText confidence="0.9999483">
In the domain identification process, the sys-
tem identifies each unit to a domain by formula
(5). If Sim(i, j) is larger than similarities be-
tween an unit and any other domains, domainj
seems to be the domain of unit,. The system se-
lects the domain which is the largest of all sim-
ilarities in N of domains as the domain of the
unit (formula (6)) . The words in the selected
word path for selected domain are selected as
keywords of the unit.
</bodyText>
<equation confidence="0.935324">
Domain, = arg max Sim(i, j) (6)
1&lt;j&lt;N
</equation>
<sectionHeader confidence="0.997433" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998843">
5.1 Test data
</subsectionHeader>
<bodyText confidence="0.9999695">
The test data we have used is a radio news
which is selected from NHK 6 o&apos;clock radio news
in August and September of 1995. Some news
stories are hard to be classified into one do-
main in radio news by human. For evalua-
tion of domain identification experiments, we
</bodyText>
<figure confidence="0.990559272727273">
phoneme lattice of uni4
DOEll IDEICIDOM
:::::EHREE0RI I
word candidates
Lii ir
Sin(unib, D1 )=max(3.2x3+ 0.5x6,3.2x3+ 4.3x4+ 0.7x 2,
3.2x3+4.3x4+4.3x3,
1.2 x 3+ 0.3 x
I I
I I
EDH
</figure>
<page confidence="0.992392">
1274
</page>
<bodyText confidence="0.8854084">
selected news stories which two persons classi-
fied into the same domains are selected. The
units which were used as test data are seg-
mented by pauses which are longer than 0.5
second. We selected 50 units of radio news for
the experiments. The 50 units consisted of 10
units of each domain. We used two kinds of test
data. One is described with correct phoneme
sequence. The other is written in phoneme lat-
tice which is obtained by a phoneme recognition
system (Suzuki et al., 1993). In each frame of
phoneme lattice, the number of phoneme candi-
dates did not exceed 3. The following equations
show the results of phoneme recognition.
the number of correct phonemes in
phoneme lattice
the number of uttered phonemes
the number of correct phonemes in
phoneme lattice
phoneme segments in phoneme lattice
</bodyText>
<subsectionHeader confidence="0.999647">
5.2 Training data
</subsectionHeader>
<bodyText confidence="0.99999665">
In order to classify newspaper articles into
small domain, we used an encyclopedia of cur-
rent terms &amp;quot;Chiezo&amp;quot;(Yamamoto, 1995). In the
encyclopedia, there are 141 domains in 9 large
domains. There are 10,236 head-words and
those explanations in the encyclopedia. In or-
der to calculate feature vectors of domains, all
explanations in the encyclopedia are performed
morphological analysis by Chasen (Matsumoto
et al., 1997). 9,805 nouns which appeared more
than 5 times in the same domains were selected
and a feature vector of each domain was cal-
culated. Using 141 feature vectors which were
calculated in the encyclopedia, we identified do-
mains of newspaper articles. We identified do-
mains of 110,000 articles of newspaper for cal-
culating feature vectors automatically. We se-
lected 61,727 nouns which appeared at least 5
times in the newspaper articles of same domains
and calculated 141 feature vectors.
</bodyText>
<subsectionHeader confidence="0.991901">
5.3 Domain identification experiment
</subsectionHeader>
<bodyText confidence="0.999910307692308">
The system selects suitable domain of each
unit for keyword extraction. Table 1 shows
the results of domain identification. We con-
ducted domain identification experiments using
two kinds of input data, i.e. correct phoneme
sequence and phoneme lattice and two kinds of
domains, i.e. 141 domains and 9 large domains.
We also compared the results and the result us-
ing previous method (Suzuki et al., 1997). For
comparison, we selected 5 domains which are
used by previous method in our method. In
previous method, we used a keyword dictionary
which has 4,212 words.
</bodyText>
<tableCaption confidence="0.7814485">
Table 1: The result of domain identification
method number of Correct Phoneme
</tableCaption>
<table confidence="0.984755166666667">
domains phoneme lattice
Our 141 62% 40%
method 9 78% 54%
5 90% 82%
previous 5 86% 78%
method
</table>
<subsectionHeader confidence="0.931694">
5.4 Keyword extraction experiment
</subsectionHeader>
<bodyText confidence="0.766731444444444">
We have conducted keyword extraction ex-
periment using the method with 141 feature
vectors (our method), 5 feature vectors (pre-
vious method) and without domain identifica-
tion. Table 2 shows recall and precision which
are shown in formula (7), and formula (8), re-
spectively, when the input data was phoneme
lattice.
the number of correct words in
recall = MSKP
the number of selected words in (7)
MSKP
the number of correct words
precision = in MSKP (8)
the number of correct nouns
in the unit
MSKP : the most suitable keyword path for se-
lected domain
</bodyText>
<sectionHeader confidence="0.999916" genericHeader="method">
6 Discussion
</sectionHeader>
<subsectionHeader confidence="0.9982505">
6.1 Sorting newspaper articles
according to their domains
</subsectionHeader>
<bodyText confidence="0.9999688">
For using X2 values in feature vectors, we
have good result of domain identification of
newspaper articles. Even if the newspaper ar-
ticles which are classified into several domains,
the suitable domains are selected correctly.
</bodyText>
<subsectionHeader confidence="0.996027">
6.2 Domain identification of radio news
</subsectionHeader>
<bodyText confidence="0.8023142">
Table 1 shows that when we used 141 kinds of
domains and phoneme lattice, 40% of units were
identified as the most suitable domains by our
= 95.6%
81.2%
</bodyText>
<page confidence="0.987649">
1275
</page>
<tableCaption confidence="0.82564">
Table 2: Recall and precision of keyword extrac-
tion
</tableCaption>
<table confidence="0.943184666666667">
Method R/P Correct Phoneme
phoneme lattice
our method R 88.5% 48.9%
(141 domains) P 69.0% 38.1%
previous method R 80.0% 24.0%
(5 domains) P 63.1% 33.0%
without DI R 77.0% 12.2%
(1 domain) P 60.1% 9.5%
R: recall P: precision DI: domain identification
</table>
<bodyText confidence="0.999873466666667">
method and shows that when we used 9 kinds
of domains and phoneme lattice, 54% of units
are identified as the most suitable domains by
our method. When the number of domains was
5, the results using our method are better than
our previous experiment. The reason is that we
use small domains. Using small domains, the
number of words whose X2 values of a certain
domain are high is smaller than when large do-
mains are used.
For further improvement of domain identifi-
cation, it is necessary to use larger newspaper
corpus in order to calculate feature vectors pre-
cisely and have to improve phoneme recogni-
tion.
</bodyText>
<subsectionHeader confidence="0.98844">
6.3 Keyword extraction of radio news
</subsectionHeader>
<bodyText confidence="0.999962266666667">
When we used our method to phoneme lat-
tice, recall was 48.9% and precision was 38.1%.
We compared the result with the result of our
previous experiment (Suzuki et al., 1997). The
result of our method is better than the our pre-
vious result. The reason is that we used do-
mains which are precisely classified, and we can
limit keyword search space. However recall was
48.9% using our method. It shows that about
50% of selected keywords were incorrect words,
because the system tries to find keywords for
all parts of the units. In order to raise recall
value, the system has to use co-occurrence be-
tween keywords in the most suitable keyword
path.
</bodyText>
<sectionHeader confidence="0.999671" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999922909090909">
In this paper, we proposed keyword extrac-
tion in radio news using term-domain interde-
pendence. In our method, we could obtain
sorted large corpus according to domains for
keyword extraction automatically. Using our
method, the number of incorrect keywords in
extracted words was smaller than the previous
method.
In future, we will study how to select correct
words from extracted keywords in order to ap-
ply our method for dictation of radio news.
</bodyText>
<sectionHeader confidence="0.998786" genericHeader="acknowledgments">
8 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999967888888889">
The authors would like to thank Mainichi
Shimbun for permission to use newspaper arti-
cles on CD-Mainichi Shimbun 1994 and 1995,
Asahi Shimbun for permission to use the data
of the encyclopedia of current terms &amp;quot;Chiezo
1996&amp;quot; and Japan Broadcasting Corporation
(NHK) for permission to use radio news. The
authors would also like to thank the anonymous
reviewers for their valuable comments.
</bodyText>
<sectionHeader confidence="0.999436" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999858393939394">
Baimo Bakis, Scott Chen, Ponani Gopalakrishnan,
Ramesh Gopinath, Stephane Maes, and Lazaros
Pllymenakos. 1997. Transcription of broadcast
news - system robustness issues and adaptation
techniques. In Proc. ICASSP&apos;97, pages 711-714.
J.McDonough, K.Ng, P.Jeanrenaud, H.Gish, and
J.R.Rohlicek. 1994. Approaches to topic identifi-
cation on the switchboard corpus. In Proc. IEEE
ICASSP&apos;94, volume 1, pages 385-388.
Yuji Matsumoto, Akira Kitauchi, Tatuo Yamashita,
Osamu Imaichi, and Tomoaki Imamura, 1997.
Japanese Morphological Analysis System ChaSen
Manual. Matsumoto Lab. Nara Institute of Sci-
ence and Technology.
Satoshi. Sekine. 1996. Modeling topic coherence for
speech recognition. In Proc. COLING 96, pages
913-918.
Yoshimi Suzuki, Chieko Furuichi, and Satoshi Imai.
1993. Spoken japanese sentence recognition us-
ing dependency relationship with systematical
semantic category. Trans. of IEICE J76 D-II,
11:2264-2273. (in Japanese).
Yoshimi Suzuki, Fumiyo Fukumoto, and Yoshihiro
Sekiguchi. 1997. Keyword extraction of radio
news using term weighting for speech recognition.
In NLPRS97, pages 301-306.
Shin Yamamoto, editor. 1995. The Asahi Encyclo-
pedia of Current Terms &apos;Chiezo&apos;. Asahi Shimbun.
Kentaro Yokoi, Tatsuya Kawahara, and Shuji
Doshita. 1997. Topic identification of news
speech using word cooccurrence statistics. In
Technical Report of IEICE SP96-105, pages 71-
78. (in Japanese).
</reference>
<page confidence="0.991132">
1276
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.961444">
<title confidence="0.99944">Keyword Extraction using Term-Domain Interdependence for Dictation of Radio News</title>
<author confidence="0.998451">Yoshimi Suzuki Fumiyo Fukumoto Yoshihiro Sekiguchi</author>
<affiliation confidence="0.9995865">Dept. of Computer Science and Media Engineering Yamanashi University</affiliation>
<address confidence="0.990397">4-3-11 Takeda, Kofu 400 Japan</address>
<email confidence="0.98473">ysuzukiesuwa.esi.yamanashi.ac.jp</email>
<email confidence="0.98473">fukumotalskye.esi.yamanashi.ac.jp</email>
<email confidence="0.98473">sekigutiesaiko.esi.yamanashi.ac.jp</email>
<abstract confidence="0.999115769230769">paper, we propose keyword extraction method for dictation of radio news which consists of several domains. In our method, newspaper articles which are automatically classified into suitable domains are used in order to calculate feature vectors. The feature vectors shows term-domain interdependence and are used for selecting a suitable domain of each part of radio news. Keywords are extracted by using the selected domain. The results of keyword extraction experiments showed that our methods are robust and effective for dictation of radio news.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Baimo Bakis</author>
<author>Scott Chen</author>
<author>Ponani Gopalakrishnan</author>
<author>Ramesh Gopinath</author>
<author>Stephane Maes</author>
<author>Lazaros Pllymenakos</author>
</authors>
<title>Transcription of broadcast news - system robustness issues and adaptation techniques.</title>
<date>1997</date>
<booktitle>In Proc. ICASSP&apos;97,</booktitle>
<pages>711--714</pages>
<contexts>
<context position="2652" citStr="Bakis et al., 1997" startWordPosition="407" endWordPosition="410">ation method using co-occurrence of words for topic identification (Yokoi et al., 1997). He classified each dictated sentence of news into 8 topics. In TV or Radio news, however, it is difficult to segment each sentence automatically. Sekine proposed a method for selecting a suitable sentence from sentences which were extracted by a speech recognition system using statistical language model (Sekine, 1996). However, if the statistical model is used for extraction of sentence candidates, we will obtain higher recognition accuracy. Some initial studies of transcription of broadcast news proceed (Bakis et al., 1997). However there are some remaining problems, e.g. speaking styles and domain identification. We conducted domain identification and keyword extraction experiment (Suzuki et al., 1997) for radio news. In the experiment, we classified radio news into 5 domains (i.e. accident, economy, international, politics and sports). The problems which we faced with are; 1. Classification of newspaper articles into suitable domains could not be performed automatically. 2. Many incorrect keywords are extracted, because the number of domains was few. In this paper, we propose a method for keyword extraction us</context>
</contexts>
<marker>Bakis, Chen, Gopalakrishnan, Gopinath, Maes, Pllymenakos, 1997</marker>
<rawString>Baimo Bakis, Scott Chen, Ponani Gopalakrishnan, Ramesh Gopinath, Stephane Maes, and Lazaros Pllymenakos. 1997. Transcription of broadcast news - system robustness issues and adaptation techniques. In Proc. ICASSP&apos;97, pages 711-714.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ng J McDonough</author>
<author>H Gish P Jeanrenaud</author>
<author>J R Rohlicek</author>
</authors>
<title>Approaches to topic identification on the switchboard corpus.</title>
<date>1994</date>
<booktitle>In Proc. IEEE ICASSP&apos;94,</booktitle>
<volume>1</volume>
<pages>385--388</pages>
<contexts>
<context position="1601" citStr="McDonough et al., 1994" startWordPosition="235" endWordPosition="238">stricted to certain tasks, for example, a tourist information and a hamburger shop. Speech recognition systems for the task which consists of various domains seems to be required for some tasks, e.g. a closed caption system for TV and a transcription system of public proceedings. In order to recognize spoken discourse which has several domains, the speech recognition system has to have large vocabulary. Therefore, it is necessary to limit word search space using linguistic restricts, e.g. domain identification. There have been many studies of domain identification which used term weighting (J.McDonough et al., 1994; Yokoi et al., 1997). McDonough proposed a topic identification method on switch board corpus. He reported that the result was best when the number of words in keyword dictionary was about 800. In his method, duration of discourses of switch board corpora is rather long and there are many keywords in the discourse. However, for a short discourse, there are few keywords in a short discourse. Yokoi also proposed a topic identification method using co-occurrence of words for topic identification (Yokoi et al., 1997). He classified each dictated sentence of news into 8 topics. In TV or Radio news</context>
</contexts>
<marker>McDonough, Jeanrenaud, Rohlicek, 1994</marker>
<rawString>J.McDonough, K.Ng, P.Jeanrenaud, H.Gish, and J.R.Rohlicek. 1994. Approaches to topic identification on the switchboard corpus. In Proc. IEEE ICASSP&apos;94, volume 1, pages 385-388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuji Matsumoto</author>
</authors>
<title>Akira Kitauchi, Tatuo Yamashita, Osamu Imaichi, and Tomoaki Imamura,</title>
<date>1997</date>
<institution>Japanese Morphological Analysis System ChaSen Manual. Matsumoto Lab. Nara Institute of Science and Technology.</institution>
<marker>Matsumoto, 1997</marker>
<rawString>Yuji Matsumoto, Akira Kitauchi, Tatuo Yamashita, Osamu Imaichi, and Tomoaki Imamura, 1997. Japanese Morphological Analysis System ChaSen Manual. Matsumoto Lab. Nara Institute of Science and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sekine</author>
</authors>
<title>Modeling topic coherence for speech recognition.</title>
<date>1996</date>
<booktitle>In Proc. COLING 96,</booktitle>
<pages>913--918</pages>
<contexts>
<context position="2441" citStr="Sekine, 1996" startWordPosition="376" endWordPosition="377">urses of switch board corpora is rather long and there are many keywords in the discourse. However, for a short discourse, there are few keywords in a short discourse. Yokoi also proposed a topic identification method using co-occurrence of words for topic identification (Yokoi et al., 1997). He classified each dictated sentence of news into 8 topics. In TV or Radio news, however, it is difficult to segment each sentence automatically. Sekine proposed a method for selecting a suitable sentence from sentences which were extracted by a speech recognition system using statistical language model (Sekine, 1996). However, if the statistical model is used for extraction of sentence candidates, we will obtain higher recognition accuracy. Some initial studies of transcription of broadcast news proceed (Bakis et al., 1997). However there are some remaining problems, e.g. speaking styles and domain identification. We conducted domain identification and keyword extraction experiment (Suzuki et al., 1997) for radio news. In the experiment, we classified radio news into 5 domains (i.e. accident, economy, international, politics and sports). The problems which we faced with are; 1. Classification of newspaper</context>
</contexts>
<marker>Sekine, 1996</marker>
<rawString>Satoshi. Sekine. 1996. Modeling topic coherence for speech recognition. In Proc. COLING 96, pages 913-918.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimi Suzuki</author>
<author>Chieko Furuichi</author>
<author>Satoshi Imai</author>
</authors>
<title>Spoken japanese sentence recognition using dependency relationship with systematical semantic category.</title>
<date>1993</date>
<journal>Trans. of IEICE J76 D-II,</journal>
<pages>11--2264</pages>
<note>(in Japanese).</note>
<contexts>
<context position="10825" citStr="Suzuki et al., 1993" startWordPosition="1780" endWordPosition="1783">HREE0RI I word candidates Lii ir Sin(unib, D1 )=max(3.2x3+ 0.5x6,3.2x3+ 4.3x4+ 0.7x 2, 3.2x3+4.3x4+4.3x3, 1.2 x 3+ 0.3 x I I I I EDH 1274 selected news stories which two persons classified into the same domains are selected. The units which were used as test data are segmented by pauses which are longer than 0.5 second. We selected 50 units of radio news for the experiments. The 50 units consisted of 10 units of each domain. We used two kinds of test data. One is described with correct phoneme sequence. The other is written in phoneme lattice which is obtained by a phoneme recognition system (Suzuki et al., 1993). In each frame of phoneme lattice, the number of phoneme candidates did not exceed 3. The following equations show the results of phoneme recognition. the number of correct phonemes in phoneme lattice the number of uttered phonemes the number of correct phonemes in phoneme lattice phoneme segments in phoneme lattice 5.2 Training data In order to classify newspaper articles into small domain, we used an encyclopedia of current terms &amp;quot;Chiezo&amp;quot;(Yamamoto, 1995). In the encyclopedia, there are 141 domains in 9 large domains. There are 10,236 head-words and those explanations in the encyclopedia. In</context>
</contexts>
<marker>Suzuki, Furuichi, Imai, 1993</marker>
<rawString>Yoshimi Suzuki, Chieko Furuichi, and Satoshi Imai. 1993. Spoken japanese sentence recognition using dependency relationship with systematical semantic category. Trans. of IEICE J76 D-II, 11:2264-2273. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimi Suzuki</author>
<author>Fumiyo Fukumoto</author>
<author>Yoshihiro Sekiguchi</author>
</authors>
<title>Keyword extraction of radio news using term weighting for speech recognition. In</title>
<date>1997</date>
<booktitle>NLPRS97,</booktitle>
<pages>301--306</pages>
<contexts>
<context position="2835" citStr="Suzuki et al., 1997" startWordPosition="433" endWordPosition="436">is difficult to segment each sentence automatically. Sekine proposed a method for selecting a suitable sentence from sentences which were extracted by a speech recognition system using statistical language model (Sekine, 1996). However, if the statistical model is used for extraction of sentence candidates, we will obtain higher recognition accuracy. Some initial studies of transcription of broadcast news proceed (Bakis et al., 1997). However there are some remaining problems, e.g. speaking styles and domain identification. We conducted domain identification and keyword extraction experiment (Suzuki et al., 1997) for radio news. In the experiment, we classified radio news into 5 domains (i.e. accident, economy, international, politics and sports). The problems which we faced with are; 1. Classification of newspaper articles into suitable domains could not be performed automatically. 2. Many incorrect keywords are extracted, because the number of domains was few. In this paper, we propose a method for keyword extraction using term-domain interdependence in order to cope with these two problems. The results of the experiments demonstrated the effectiveness of our method. 2 An overview of our method Figu</context>
<context position="5591" citStr="Suzuki et al., 1997" startWordPosition="869" endWordPosition="872">r articles. 3.1: Sorting newspaper articles according to their domains Firstly, all sentences in the encyclopedia are analyzed morpheme by Chasen (Matsumoto et Calculating frequency vectors (FreqVe Calculating similarity between FeaVe and FreqVa .C7 (141 feature vectors (FeaVeD--- Calculating x- values of each noun on domains (141 feature vectors (FeaVa)) Figure 2: Calculating feature vectors al., 1997) and nouns which frequently appear are extracted. A feature vector is calculated by frequency of each noun at each domain. We call the feature vector FeaVe. Each element of FeaVe is a X2 value (Suzuki et al., 1997). Then, nouns are extracted from newspaper articles by a morphological analysis system (Matsumoto et al., 1997), and frequency of each noun are counted. Next, similarity between FeaVe of each domain and each newspaper article are calculated by using formula (1). Finally, a suitable domain of each newspaper article are selected by using formula (2). Sim(i, j) = FeaVei • FreqVa i (1) . . Domains = arg max Sim(z, 2) (2) 1&lt;j&lt;N where i means a newspaper article and j means a domain. (.) means operation of inner vector. 3.2 Term-domain interdependence represented by feature vectors Firstly, at each </context>
<context position="7532" citStr="Suzuki et al., 1997" startWordPosition="1183" endWordPosition="1186">alculating frequency vectors (FreqVa) Extractina nouns Extracting nouns Radio News Feature vectors (FeaVe) D1 • • • D7 D141 C&gt; Calculating X values of each noun on domains Sorting articles into domains according to similarity 1273 and similarity between the article and &amp;quot;election&amp;quot; are 100 and 60 respectively, each frequency vector is calculated by formula (3) and formula (4). 100 FreqVpr, FreqVppi FreqVai x 1-To- (3) 60 FreqVei = FreqVe&apos;t FreqVai x —100 (4) where i means a newspaper article. Then, we calculate feature vectors FeaV a using FreqV using the method mentioned in our previous paper (Suzuki et al., 1997). Each element of feature vectors shows x2 value of the domain and wordk. All wordk (1 &lt; k &lt; M :M means the number of elements of a feature vector) are put into the keyword dictionary. 4 Keyword extraction Input news stories are represented by phoneme lattice. There are no marks for word boundaries in input news stories. Phoneme lattices are segmented by pauses which are longer than 0.5 second in recorded radio news. The system selects a domain of each unit which is a segmented phoneme lattice. At each frame of phoneme lattice, the system selects maximum 20 words from keyword dictionary. 4.1 S</context>
<context position="12505" citStr="Suzuki et al., 1997" startWordPosition="2047" endWordPosition="2050">r calculating feature vectors automatically. We selected 61,727 nouns which appeared at least 5 times in the newspaper articles of same domains and calculated 141 feature vectors. 5.3 Domain identification experiment The system selects suitable domain of each unit for keyword extraction. Table 1 shows the results of domain identification. We conducted domain identification experiments using two kinds of input data, i.e. correct phoneme sequence and phoneme lattice and two kinds of domains, i.e. 141 domains and 9 large domains. We also compared the results and the result using previous method (Suzuki et al., 1997). For comparison, we selected 5 domains which are used by previous method in our method. In previous method, we used a keyword dictionary which has 4,212 words. Table 1: The result of domain identification method number of Correct Phoneme domains phoneme lattice Our 141 62% 40% method 9 78% 54% 5 90% 82% previous 5 86% 78% method 5.4 Keyword extraction experiment We have conducted keyword extraction experiment using the method with 141 feature vectors (our method), 5 feature vectors (previous method) and without domain identification. Table 2 shows recall and precision which are shown in formu</context>
<context position="15038" citStr="Suzuki et al., 1997" startWordPosition="2484" endWordPosition="2487">our method are better than our previous experiment. The reason is that we use small domains. Using small domains, the number of words whose X2 values of a certain domain are high is smaller than when large domains are used. For further improvement of domain identification, it is necessary to use larger newspaper corpus in order to calculate feature vectors precisely and have to improve phoneme recognition. 6.3 Keyword extraction of radio news When we used our method to phoneme lattice, recall was 48.9% and precision was 38.1%. We compared the result with the result of our previous experiment (Suzuki et al., 1997). The result of our method is better than the our previous result. The reason is that we used domains which are precisely classified, and we can limit keyword search space. However recall was 48.9% using our method. It shows that about 50% of selected keywords were incorrect words, because the system tries to find keywords for all parts of the units. In order to raise recall value, the system has to use co-occurrence between keywords in the most suitable keyword path. 7 Conclusions In this paper, we proposed keyword extraction in radio news using term-domain interdependence. In our method, we </context>
</contexts>
<marker>Suzuki, Fukumoto, Sekiguchi, 1997</marker>
<rawString>Yoshimi Suzuki, Fumiyo Fukumoto, and Yoshihiro Sekiguchi. 1997. Keyword extraction of radio news using term weighting for speech recognition. In NLPRS97, pages 301-306.</rawString>
</citation>
<citation valid="true">
<date>1995</date>
<booktitle>The Asahi Encyclopedia of Current Terms &apos;Chiezo&apos;. Asahi Shimbun.</booktitle>
<editor>Shin Yamamoto, editor.</editor>
<marker>1995</marker>
<rawString>Shin Yamamoto, editor. 1995. The Asahi Encyclopedia of Current Terms &apos;Chiezo&apos;. Asahi Shimbun.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kentaro Yokoi</author>
<author>Tatsuya Kawahara</author>
<author>Shuji Doshita</author>
</authors>
<title>Topic identification of news speech using word cooccurrence statistics.</title>
<date>1997</date>
<booktitle>In Technical Report of IEICE SP96-105,</booktitle>
<pages>71--78</pages>
<note>(in Japanese).</note>
<contexts>
<context position="1622" citStr="Yokoi et al., 1997" startWordPosition="239" endWordPosition="242">s, for example, a tourist information and a hamburger shop. Speech recognition systems for the task which consists of various domains seems to be required for some tasks, e.g. a closed caption system for TV and a transcription system of public proceedings. In order to recognize spoken discourse which has several domains, the speech recognition system has to have large vocabulary. Therefore, it is necessary to limit word search space using linguistic restricts, e.g. domain identification. There have been many studies of domain identification which used term weighting (J.McDonough et al., 1994; Yokoi et al., 1997). McDonough proposed a topic identification method on switch board corpus. He reported that the result was best when the number of words in keyword dictionary was about 800. In his method, duration of discourses of switch board corpora is rather long and there are many keywords in the discourse. However, for a short discourse, there are few keywords in a short discourse. Yokoi also proposed a topic identification method using co-occurrence of words for topic identification (Yokoi et al., 1997). He classified each dictated sentence of news into 8 topics. In TV or Radio news, however, it is diff</context>
</contexts>
<marker>Yokoi, Kawahara, Doshita, 1997</marker>
<rawString>Kentaro Yokoi, Tatsuya Kawahara, and Shuji Doshita. 1997. Topic identification of news speech using word cooccurrence statistics. In Technical Report of IEICE SP96-105, pages 71-78. (in Japanese).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>