<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001320">
<title confidence="0.9979445">
Automated Vocabulary Acquisition and Interpretation
in Multimodal Conversational Systems
</title>
<author confidence="0.999535">
Yi Liu Joyce Y. Chai Rong Jin
</author>
<affiliation confidence="0.995922">
Department of Computer Science and Engineering
Michigan State University
</affiliation>
<address confidence="0.935498">
East Lansing, MI 48824, USA
</address>
<email confidence="0.996409">
{liuyi3, jchai, rongjin}@cse.msu.edu
</email>
<sectionHeader confidence="0.995585" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999715384615384">
Motivated by psycholinguistic findings that
eye gaze is tightly linked to human lan-
guage production, we developed an unsuper-
vised approach based on translation models
to automatically learn the mappings between
words and objects on a graphic display dur-
ing human machine conversation. The ex-
perimental results indicate that user eye gaze
can provide useful information to establish
such mappings, which have important impli-
cations in automatically acquiring and inter-
preting user vocabularies for conversational
systems.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999963411764706">
To facilitate effective human machine conversation,
it is important for a conversational system to have
knowledge about user vocabularies and understand
how these vocabularies are mapped to the internal
entities for which the system has representations.
For example, in a multimodal conversational system
that allows users to converse with a graphic inter-
face, the system needs to know what vocabularies
users tend to use to describe objects on the graphic
display and what (type of) object(s) a user is attend-
ing to when a particular word is expressed. Here,
we use acquisition to refer to the process of acquir-
ing relevant vocabularies describing internal entities,
and interpretation to refer to the process of automat-
ically identifying internal entities given a particular
word. Both acquisition and interpretation have been
traditionally approached by either knowledge engi-
neering (e.g., manually created lexicons) or super-
vised learning from annotated data. In this paper,
we describe an unsupervised approach that relies
on naturally co-occurred eye gaze and spoken utter-
ances during human machine conversation to auto-
matically acquire and interpret vocabularies.
Motivated by psycholinguistic studies (Just and
Carpenter, 1976; Griffin and Bock, 2000; Tenenhaus
et al., 1995) and recent investigations on computa-
tional models for language acquisition and ground-
ing (Siskind, 1995; Roy and Pentland, 2002; Yu
and Ballard, 2004), we are particularly interested in
two unique questions related to multimodal conver-
sational systems: (1) In a multimodal conversation
that involves more complex tasks (e.g., both user
initiated tasks and system initiated tasks), is there
a reliable temporal alignment between eye gaze and
spoken references so that the coupled inputs can be
used for automated vocabulary acquisition and inter-
pretation? (2) If such an alignment exists, how can
we model this alignment and automatically acquire
and interpret the vocabularies?
To address the first question, we conducted an
empirical study to examine the temporal relation-
ships between eye fixations and their correspond-
ing spoken references. As shown later in section 4,
although a larger variance (compared to the find-
ings from psycholinguistic studies) exists in terms of
how eye gaze is linked to speech production during
human machine conversation, eye fixations and the
corresponding spoken references still occur in a very
close vicinity to each other. This natural coupling
between eye gaze and speech provides an opportu-
nity to automatically learn the mappings between
</bodyText>
<page confidence="0.976359">
368
</page>
<note confidence="0.9257755">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 368–375,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.986220571428572">
words and objects without any human supervision. Studies have also shown that eye gaze has a poten-
Because of the larger variance, it is difficult to tial to improve resolution of underspecified referring
apply rule-based approaches to quantify this align- expressions in spoken dialog systems (Campana et
ment. Therefore, to address the second question, al., 2001) and to disambiguate speech input (Tanaka,
we developed an approach based on statistical trans- 1999). In contrast to these earlier studies, our work
lation models to explore the co-occurrence patterns focuses on a different goal of using eye gaze for au-
between eye fixated objects and spoken references. tomated vocabulary acquisition and interpretation.
Our preliminary experiment results indicate that the The third area of research that influenced our
translation model can reliably capture the mappings work is computational modeling of language acqui-
between the eye fixated objects and the correspond- sition and grounding. Recent studies have shown
ing spoken references. Given an object, this model that multisensory information (e.g., through vision
can provide possible words describing this object, and language processing) can be combined to effec-
which represents the acquisition process; given a tively acquire words to their perceptually grounded
word, this model can also provide possible objects objects in the environment (Siskind, 1995; Roy and
that are likely to be described, which represents the Pentland, 2002; Yu and Ballard, 2004). Especially in
interpretation process. (Yu and Ballard, 2004), an unsupervised approach
In the following sections, we first review some re- based on a generative correspondence model was
lated work and introduce the procedures used to col- developed to capture the mapping between spoken
lect eye gaze and speech data during human machine words and the occurring perceptual features of ob-
conversation. We then describe our empirical study jects. This approach is most similar to the transla-
and the unsupervised approach based on translation tion model used in our work. However, compared
models. Finally, we present experiment results and to this work where multisensory information comes
discuss their implications in natural language pro- from vision and language processing, our work fo-
cessing applications. cuses on a different aspect. Here, instead of applying
2 Related Work vision processing on objects, we are interested in eye
Our work is motivated by previous work in the fol- gaze behavior when users interact with a graphic dis-
lowing three areas: psycholinguistics studies, multi- play. Eye gaze is an implicit and subconscious input
modal interactive systems, and computational mod- modality during human machine interaction. Eye
eling of language acquisition and grounding. gaze data inevitably contain a significant amount of
Previous psycholinguistics studies have shown noise. Therefore, it is the goal of this paper to exam-
that the direction of gaze carries information about ine whether this modality can be utilized for vocab-
the focus of the user’s attention (Just and Carpenter, ulary acquisition for conversational systems.
1976). Specifically, in human language processing 3 Data Collection
tasks, eye gaze is tightly linked to language produc- We used a simplified multimodal conversational sys-
tion. The perceived visual context influences spo- tem to collect synchronized speech and eye gaze
ken word recognition and mediates syntactic pro- data. A room interior scene was displayed on a com-
cessing (Tenenhaus et al., 1995). Additionally, be- puter screen, as shown in Figure 1. While watching
fore speaking a word, the eyes usually move to the the graphical display, users were asked to communi-
objects to be mentioned (Griffin and Bock, 2000). cate with the system on topics about the room dec-
These psycholinguistics findings have provided a orations. A total of 28 objects (e.g., multiple lamps
foundation for our investigation. and picture frames, a bed, two chairs, a candle, a
In research on multimodal interactive systems, re- dresser, etc., as marked in Figure 1) are explicitly
cent work indicates that the speech and gaze inte- modeled in this scene. The system is simplified in
gration patterns can be modeled reliably for indi- the sense that it only supports 14 tasks during human
vidual users and therefore be used to improve mul- machine interaction. These tasks are designed to
timodal system performances (Kaur et al., 2003). cover both open-ended utterances (e.g., the system
369
measure the length of time gap between a user’s eye
fixation falling on an object and the corresponding
spoken reference being uttered (which we refer to
as “length of time gap” for brevity). Also, we can
count the number of times that user fixations hap-
pen to change their target objects during this time
gap (which we refer to as “number of fixated object
changes” for brevity). The nine most frequently oc-
curred spoken references in utterances from all users
(as shown in Table 1) are chosen for this empirical
study. For each of those spoken references, we use
human judgment to decide which object is referred
to. Then, from both before and after the onset of
the spoken reference, we find the closest occurrence
of the fixation falling on that particular object. Al-
together we have 96 such speech-gaze pairs. In 54
pairs, the eye gaze fixation occurred before the cor-
responding speech reference was uttered; and in the
other 42 pairs, the eye fixation occurred after the
corresponding speech reference was uttered. This
observation suggests that in human machine conver-
sation, eye fixation on an object does not necessarily
always proceed the utterance of the corresponding
speech reference.
Further, we computed the average absolute length
of the time gap and the average number of fixated
object changes, as well as their variances for each of
5 selected users2 as shown in Table 1. From Table 1,
it is easy to observe that: (I) A spoken reference al-
ways appears within a short period of time (usually
1-2 seconds) before or after the corresponding eye
gaze fixation. But, the exact length of the period is
far from constant. (II) It is not necessary for a user
to utter the corresponding spoken reference imme-
diately before or after the eye gaze fixation falls on
that particular object. Eye gaze fixations may move
back and forth. Between the time an object is fixated
and the corresponding spoken reference is uttered, a
user’s eye gaze may fixate on a few other objects
(reflected by the average number of eye fixated ob-
ject changes shown in the table). (III) There is a
large variance in both the length of time gap and the
number of fixated object changes in terms of 1) the
same user and the same spoken reference at differ-
ent time-stamps, 2) the same user but different spo-
Figure 1: The room interior scene for user studies.
For easy reference, we give each object an ID. These
IDs are hidden from the system users.
asks users to describe the room) and more restricted
utterances (e.g., the system asks the user whether
he/she likes the bed) that are commonly supported in
conversational systems. Seven human subjects par-
ticipated in our study.
User speech inputs were recorded using the Au-
dacity software1, with each utterance time-stamped.
Eye movements were recorded using an EyeLink II
eye tracker sampled at 250Hz. The eye tracker au-
tomatically saved two-dimensional coordinates of a
user’s eye fixations as well as the time-stamps when
the fixations occurred.
The collected raw gaze data is extremely noisy.
To refine the gaze data, we further eliminated in-
valid and saccadic gaze points (known as “saccadic
suppression” in vision studies). Since eyes do not
stay still but rather make small, frequent jerky move-
ments, we also smoothed the data by averaging
nearby gaze locations to identify fixations.
4 Empirical Study on Speech-Gaze
Alignment
Based on the data collected, we investigated the tem-
poral alignment between co-occurred eye gaze and
spoken utterances. In particular, we examined the
temporal alignment between eye gaze fixations and
the corresponding spoken references (i.e., the spo-
ken words that are used to refer to the objects on the
graphic display).
According to the time-stamp information, we can
2The other two users are not selected because the nine se-
lected words do not appear frequently in their utterances.
</bodyText>
<figure confidence="0.89641684">
1http://audacity.sourceforge.net/
370
Spoken Average Absolute Length of Time Gap (in seconds) Average Number of Eye Fixated Object Changes
Reference User 1 User 2 User 3 User 4 User 5 User 1 User 2 User 3 User 4 User 5
bed 1.27 ± 1.40 1.02 ± 0.65 0.32 ± 0.21 0.59 ± 0.77 2.57 ± 3.25 2.1 ± 3.2 2.1 ± 2.2 0.4 ± 0.5 1.4 ± 2.2 5.3 ± 7.9
tree
window
mirror
candle
- 0.24 ± 0.24 - - -
- 0.67 ± 0.74 - - 1.95 ± 3.20
- 1.04 ± 1.36 - - -
- - 3.64 ± 0.59 - -
- 0.0 ± 0.0 - - -
- 0.0 ± 0.0 - - 3.3 ± 5.9
- 1.0 ± 1.4 - - -
- - 8.5 ± 2.1 - -
5.5 ± 4.9 - - - -
0.2 ± 0.4 - - - -
1.3 ± 1.3 1.8 ± 1.5 0.3 ± 0.6 4.8 ± 4.3 2.7 ± 2.2
5.0 ± 2.6 - - 6.7 ± 5.5 13.3 ± 6.7
waterfall 1.80 ± 1.12 - - - -
painting 0.10 ± 0.10 - - - -
lamp 0.74 ± 0.54 1.70 ± 0.99 0.26 ± 0.35 1.98 ± 1.72 2.84 ± 2.42
door 2.47 ± 0.84 - - 2.49 ± 1.90 6.36 ± 2.29
</figure>
<tableCaption confidence="0.803705">
Table 1: The average absolute length of time and the number of eye fixated object changes within the time
</tableCaption>
<bodyText confidence="0.993634564102564">
gap of eye gaze and corresponding spoken references. Variances are also listed. Some of the entries are not
available because the spoken references were never or rarely used by the corresponding users.
ken references, and 3) the same spoken reference but
different users. We believe this is due to the different
dialog scenarios and user language habits.
To summarize our empirical study, we find that
in human machine conversation, there still exists a
natural temporal coupling between user speech and
eye gaze, i.e. the spoken reference and the corre-
sponding eye fixation happen within a close vicinity
of each other. However, a large variance is also ob-
served in terms of these temporal vicinities, which
indicates an intrinsically more complex gaze-speech
pattern. Therefore, it is hard to directly quantify
the temporal or ordering relationship between spo-
ken references and corresponding eye fixated objects
(for example, through rules).
To better handle the complexity in the gaze-
speech pattern, we propose to use statistical transla-
tion models. Given a time window of enough length,
a speech input that contains a list of spoken refer-
ences (e.g., definite noun phrases) is always accom-
panied by a list of naturally occurred eye fixations
and therefore a list of objects receiving those fixa-
tions. All those pairs of speech references and cor-
responding fixated objects could be viewed as paral-
lel, i.e. they co-occur within the time window. This
situation is very similar to the training process of
translation models in statistical machine translation
(Brown et al., 1993), where parallel corpus is used to
find the mappings between words from different lan-
guages by exploiting their co-occurrence patterns.
The same idea can be borrowed here: by exploring
the co-occurrence statistics, we hope to uncover the
exact mapping between those eye fixated objects and
spoken references. The intuition is that, the more of-
ten a fixation is found to exclusively co-occur with a
spoken reference, the more likely a mapping should
be established between them.
</bodyText>
<sectionHeader confidence="0.999296" genericHeader="method">
5 Translation Models for Vocabulary
Acquisition and Interpretation
</sectionHeader>
<bodyText confidence="0.991829285714286">
Formally, we denote the set of observations by
D = {wi, oi}N i�1 where wi and oi refers to
the i-th speech utterance (i.e., a list of words
of spoken references) and the i-th corresponding
eye gaze pattern (i.e., a list of eye fixated ob-
jects) respectively. When we study the prob-
lem of mapping given objects to words (for vo-
cabulary acquisition), the parameter space O =
{Pr(wj|ok),1 ≤ j ≤ m&apos;,1 ≤ k ≤ m°} consists of
the mapping probabilities of an arbitrary word wj
to an arbitrary object ok, where m&apos; and m° repre-
sent the total number of unique words and objects
respectively. Those mapping probabilities are sub-
ject to constraints ���
</bodyText>
<equation confidence="0.834056">
j�1 Pr(wj|ok) = 1. Note that
Pr(wj|ok) = 0 if the corresponding word wj and ok
never co-occur in any observed list pair (wi, oi).
</equation>
<bodyText confidence="0.999745071428571">
Let lF and l°i denote the length of lists wi and
oi respectively. To distinguish with the notations
wj and ok whose subscripts are indices for unique
words and objects respectively, we use wi,j to de-
note the word in the j-th position of the list wi and
5i,k to denote the object in the k-th position of the
list oi. In translation models, we assume that any
word in the list wi is mapped to an object in the cor-
responding list oi or a null object (we reserve the
position 0 for it in every object list). To denote all
the word-object mappings in the i-th list pair, we in-
troduce an alignment vector ai, whose element ai,j
takes the value k if the word -wi,j is mapped to �oi,k.
Then, the likelihood of the observations given the
</bodyText>
<page confidence="0.989779">
371
</page>
<bodyText confidence="0.916385">
Pr(wi, ai|oi)
where the right-hand side is actually the expansion
</bodyText>
<equation confidence="0.579781">
w
</equation>
<bodyText confidence="0.511037">
of Pai Qji Pr(wi j |5ai ). Therefore, the likelihood
can be simplified as
</bodyText>
<equation confidence="0.937774">
Pr(D; O) = YN Pr(liw |oi) lw i loi Pr( �wi,j|5i,k)
i=1 (lo i + 1)lw i Y X
j=1 k=0
</equation>
<bodyText confidence="0.9971714">
Switching to the notations wj and ok, we have
where δwi,j = 1 if wi,j E wi and δwi,j = 0 otherwise,
and δoi,k = 1 if 5i,k E oi and δoi,k = 0 otherwise.
Finally, the translation model can be formalized
as the following optimization problem
</bodyText>
<equation confidence="0.903766">
arg maxp log Pr(D; O)
mw
s.t. X Pr(wj|ok) = 1, bk
j=1
</equation>
<bodyText confidence="0.999560357142857">
This optimization problem can be solved by the EM
algorithm (Brown et al., 1993).
The above model is developed in the con-
text of mapping given objects to words, i.e., its
solution yields a set of conditional probabilities
{Pr(wj|ok), bj} for each object ok, indicating how
likely every word is mapped to it. Similarly, we
can develop the model in the context of mapping
given words to objects (for vocabulary interpreta-
tion), whose solution leads to another set of prob-
abilities {Pr(ok|wj), bk} for each word wj indicat-
ing how likely every object is mapped to it. In our
experiments, both models are implemented and we
will present the results later.
</bodyText>
<sectionHeader confidence="0.999311" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999982">
We experimented our proposed statistical translation
model on the collected data mentioned in Section 3.
</bodyText>
<subsectionHeader confidence="0.995278">
6.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999976823529412">
The main purpose of preprocessing is to create a
“parallel corpus” for training a translation model.
Here, the “parallel corpus” refers to a series of
speech-gaze pairs, each of them consisting of a list
of words from the spoken references in the user ut-
terances and a list of objects that are fixated upon
within the same time window.
Specifically, we first transcribed the user speech
into scripts by automatic speech recognition soft-
ware and then refined them manually. A time-stamp
was associated with each word in the speech script.
Further, we detected long pauses in the speech script
as splitting points to create time windows, since a
long pause usually marks the start of a sentence
that indicates a user’s attention shift. In our exper-
iment, we set the threshold of judging a long pause
to be 1 second. From all the data gathered from 7
users, we get 357 such time windows (which typi-
cally contain 10-20 spoken words and 5-10 fixated
object changes).
Given a time window, we then found the objects
being fixated upon by eye gaze (represented by their
IDs as shown in Figure 1). Considering that eye gaze
fixation could occur during the pauses in speech, we
expanded each time window by a fixed length at both
its start and end to find the fixations. In our experi-
ments, the expansion length is set to 0.5 seconds.
Finally, we applied a part-of-speech tagger to
each sentence in the user script and only singled out
nouns as potential spoken references in the word list.
The Porter stemming algorithm was also used to get
the normalized forms of those nouns.
The translation model was trained based on this
preprocessed parallel data.
</bodyText>
<subsectionHeader confidence="0.99282">
6.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999952666666667">
As described in Section 5, by using a statistical
translation model we can get a set of translation
probabilities, either from any given spoken word to
all the objects, or from any given object to all the
spoken words. To evaluate the two sets of trans-
lation probabilities, we use precision and recall as
</bodyText>
<equation confidence="0.999638125">
Pr(lwi |oi)
(loi + 1)lw i
Pr(lwi |oi) X
(loi + 1)lw i ai
Pr(�wi,j|�oai,j)
Pr( ibi,j|�oai,j)
= YN
i=1
</equation>
<bodyText confidence="0.715046">
Note that the following equation holds:
=
</bodyText>
<equation confidence="0.998261340909091">
YN
i=1
X
ai
lw
i
Y
j=1
lw
i
Y
j=1
lw i
Y
ai,1=1 ai lw=1
i j=1
Pr( wi,j|5ai,j)
lo i
lw i
Y
j=1
X
k=0
Pr( wi,j|5i,k) =
� � �
loXi
loXi
YN
i=1
Pr(lwi |oi)
(loi + 1)lwi
Pr(D; O)=
&amp;quot; mo #δw
mw Y X i,j
Pr(wj|ok)δo i,k
j=1k=0
parameters can be computed as follows
YN Pr(wi|oi) =
i=1
Pr(D; O) =
YN
i=1
X
ai
</equation>
<page confidence="0.995796">
372
</page>
<table confidence="0.999286333333333">
#Rank Precision Recall #Rank Precision Recall
1 0.6667 0.2593 6 0.2302 0.5370
2 0.4524 0.3519 7 0.2041 0.5556
3 0.3810 0.4444 8 0.1905 0.5926
4 0.3095 0.4815 9 0.1799 0.6296
5 0.2667 0.5185 10 0.1619 0.6296
</table>
<tableCaption confidence="0.9371085">
Table 2: Average precision/recall of mapping given
objects to words (i.e., acquisition)
</tableCaption>
<table confidence="0.999722333333333">
#Rank Precision Recall #Rank Precision Recall
1 0.7826 0.3214 6 0.3043 0.7500
2 0.5870 0.4821 7 0.2671 0.7679
3 0.4638 0.5714 8 0.2446 0.8036
4 0.3804 0.6250 9 0.2293 0.8393
5 0.3478 0.7143 10 0.2124 0.8571
</table>
<tableCaption confidence="0.883131">
Table 3: Average precision/recall of mapping given
words to objects.(i.e., interpretation)
evaluation metrics.
</tableCaption>
<bodyText confidence="0.998794038461538">
Specifically, for a given object ok the trans-
lation model will yield a set of probabilities
{Pr(wj|ok), bj}. We can sort the probabilities and
get a ranked list. Let us assume that we have the
ground truth about all the spoken words to which
the given object should be mapped. Then, at a given
number n of top ranked words, the precision of map-
ping the given object ok to words is defined as
# words that ok is correctly mapped to
# words that ok is mapped to
and the recall is defined as
# words that ok is correctly mapped to
# words that ok should be mapped to
All the counting above is done within the top n rank.
Therefore, we can get different precision/recall at
different ranks. At each rank, the overall perfor-
mance can be evaluated by averaging the preci-
sion/recall for all the given objects. Human judg-
ment is used to decide whether an object-word map-
ping is correct or not, as ground truth for evaluation.
Similarly, based on the set of probabilities of map-
ping a given object with spoken words, we can
find a ranked list of objects for a given word, i.e.
{Pr(ok|wj), bk}. Thus, at a given rank the preci-
sion and recall of mapping a given word wj to ob-
jects can be measured.
</bodyText>
<subsectionHeader confidence="0.998617">
6.3 Experiment Results
</subsectionHeader>
<bodyText confidence="0.99957616">
Vocabulary acquisition is the process of finding
the appropriate word(s) for any given object. For
the sake of statistical significance, our evaluation is
done on 21 objects that were mentioned at least 3
times by the users.
Table 2 gives the average precision/recall evalu-
ated at the top 10 ranks. As we can see, if we use
the most probable word acquired for each object,
about 66.67% of them are appropriate. With the
rank increasing, more and more appropriate words
can be acquired. About 62.96% of all the appropri-
ate words are included within the top 10 probable
words found. The results indicate that by using a
translation model, we can obtain the words that are
used by the users to describe the objects with rea-
sonable accuracy.
Table 4 presents the top 3 most probable words
found for each object. It shows that although there
may be more than one word appropriate to describe
a given object, those words with highest probabil-
ities always suggest the most popular way of de-
scribing the corresponding object among the users.
For example, for the object with ID 26, the word
candle gets a higher probability than the word
candlestick, which is in accordance with our
observation that in our user study, on most occasions
users tend to use the word candle rather than the
word candlestick.
Vocabulary interpretation is the process of find-
ing the appropriate object(s) for any given spoken
word. Out of 176 nouns in the user vocabulary,
we only evaluate those used at least three times for
statistical significance concerns. Further, abstract
words (such as reason, position) and general
words (such as room, furniture) are not eval-
uated since they do not refer to any particular objects
in the scene. Finally, 23 nouns remain for evalua-
tion.
We manually enumerated all the object(s) that
those 23 nouns refer to as the ground truth in our
evaluation. Note that a given noun can possibly
be used to refer to multiple objects, such as lamp,
since we have several lamps (with object ID 3, 8, 17,
and 23) in the experiment setting, and bed, since
bed frame, bed spread, and pillows (with object ID
19, 21, and 20 respectively) are all part of a bed.
Also, an object can be referred to by multiple nouns.
For example, the words painting, picture,
or waterfall can all be used to refer to the ob-
ject with ID 15.
</bodyText>
<page confidence="0.998196">
373
</page>
<table confidence="0.974191363636363">
Rank 1 Rank 2 Rank 3
paint (0.254) * wall (0.191) left (0.150)
pictur (0.305) * girl (0.122) niagara (0.095) *
wall (0.109) lamp (0.093) * floor (0.084)
upsid (0.174) * left (0.151) * paint (0.149) *
5 pictur (0.172) window (0.157) * wall (0.116)
6 window (0.287) * curtain (0.115) pictur (0.076)
chair (0.287) * tabl (0.088) bird (0.083)
mirror (0.161) * dresser (0.137) bird (0.098) *
room (0.131) lamp (0.127) left (0.069)
hang (0.104) favourit (0.085) natur (0.064)
thing (0.066) size (0.059) queen (0.057)
paint (0.211) * pictur (0.116) * forest (0.076) *
17 lamp (0.354) * end (0.154) tabl (0.097)
18 bedroom (0.158) side (0.128) bed (0.104)
bed (0.576) * room (0.059) candl (0.049)
bed (0.396) * queen (0.211) * size (0.176)
bed (0.180) * chair (0.097) orang (0.078)
bed (0.282) door (0.235) * chair (0.128)
chair (0.215) * bed (0.162) candlestick (0.124)
candl (0.145) * chair (0.114) candlestick (0.092) *
tree (0.246) * chair (0.107) floor (0.096)
</table>
<tableCaption confidence="0.988789">
Table 4: Words found for given objects. Each row
</tableCaption>
<bodyText confidence="0.954046875">
lists the top 3 most probable spoken words (being
stemmed) for the corresponding given object, with
the mapping probabilities in parentheses. Asterisks
indicate correctly identified spoken words. Note
that some objects are heavily overlapped, so the cor-
responding words are considered correct for all the
overlapping objects, such as bed being considered
correct for objects with ID 19, 20, and 21.
</bodyText>
<table confidence="0.921440916666666">
Word Rank 1 Rank 2 Rank 3 Rank 4
curtain 6 (0.305) * 5 (0.305) * 7 (0.133) 1 (0.121)
candlestick 25 (0.147) * 28 (0.135) 24 (0.131) 22 (0.117)
lamp 22 (0.126) 12 (0.094) 17 (0.093) * 25 (0.093)
dresser 12 (0.298) * 9 (0.294) * 13 (0.173) * 7 (0.104)
queen 20 (0.187) * 21 (0.182) * 22 (0.136) 19 (0.136) *
door 22 (0.200) * 27 (0.124) 25 (0.108) 24 (0.106)
tabl 9 (0.152) * 12 (0.125) * 13 (0.112) * 22 (0.107)
mirror 9 (0.251) * 12 (0.238) 8 (0.109) 13 (0.081)
girl 2 (0.173) 22 (0.128) 16 (0.099) 10 (0.074)
chair 22 (0.132) 25 (0.099) * 28 (0.085) 24 (0.082)
waterfal 6 (0.226) 5 (0.215) 1 (0.118) 9 (0.083)
candl 19 (0.156) 22 (0.139) 28 (0.134) 24 (0.131)
niagara 4 (0.359) * 2 (0.262) * 1 (0.226) 7 (0.045)
plant 27 (0.230) * 22 (0.181) 23 (0.131) 28 (0.117)
tree 27 (0.352) * 22 (0.218) 26 (0.100) 13 (0.062)
upsid 4 (0.204) * 12 (0.188) 9 (0.153) 1 (0.104) *
bird 9 (0.142) * 10 (0.138) 12 (0.131) 7 (0.121)
desk 12 (0.170) * 9 (0.141) * 19 (0.118) 8 (0.118)
bed 19 (0.207) * 22 (0.141) 20 (0.111) * 28 (0.090)
upsidedown 4 (0.243) * 3 (0.219) 6 (0.203) 5 (0.188)
paint 4 (0.188) * 16 (0.148) * 1 (0.137) * 15 (0.118) *
window 6 (0.305) * 5 (0.290) * 3 (0.085) 22 (0.065)
lampshad 3 (0.223) * 7 (0.137) 11 (0.137) 10 (0.137)
</table>
<tableCaption confidence="0.943055">
Table 5: Objects found for given words. Each row
</tableCaption>
<bodyText confidence="0.9941615">
lists the 4 most probable object IDs for the corre-
sponding given words (being stemmed), with the
mapping probabilities in parentheses. Asterisks in-
dicate correctly identified objects. Note that some
objects are heavily overlapped, such as the candle
(with object ID 26) and the chair (with object ID
25), and both were considered correct for the re-
spective spoken words.
</bodyText>
<figure confidence="0.971378722222222">
Object
1
2
3
4
7
9
12
14
15
16
19
20
21
22
25
26
27
</figure>
<bodyText confidence="0.999825419354839">
Table 3 gives the average precision/recall evalu-
ated at the top 10 ranks. As we can see, if we use the
most probable object found for each speech word,
about 78.26% of them are appropriate. With the rank
increasing, more and more appropriate objects can
be found. About 85.71% of all the appropriate ob-
jects are included within the top 10 probable objects
found. The results indicate that by using a trans-
lation model, we can predict the objects from user
spoken words with reasonable accuracy.
Table 5 lists the top 4 probable objects found for
each spoken word being evaluated. A close look re-
veals that in general, the top ranked objects tend to
gather around the correct object for a given spoken
word. This is consistent with the fact that eye gaze
tends to move back and forth. It also indicates that
the mappings established by the translation model
can effectively find the approximate area of the cor-
responding fixated object, even if it cannot find the
object due to the noisy and jerky nature of eye gaze.
The precision/recall in vocabulary acquisition is
not as high as that in vocabulary interpretation, par-
tially due to the relatively small scale of our exper-
iment data. For example, with only 7 users’ speech
data on 14 conversational tasks, some words were
only spoken a few times to refer to an object, which
prevented them from getting a significant portion of
probability mass among all the words in the vocab-
ulary. This degrades both precision and recall. We
believe that in large scale experiments or real-world
applications, the performance will be improved.
</bodyText>
<sectionHeader confidence="0.996067" genericHeader="evaluation">
7 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.999977320754717">
Previous psycholinguistic findings have shown that
eye gaze is tightly linked with human language pro-
duction. During human machine conversation, our
study shows that although a larger variance is ob-
served on how eye fixations are exactly linked with
corresponding spoken references (compared to the
psycholinguistic findings), eye gaze in general is
closely coupled with corresponding referring ex-
pressions in the utterances. This close coupling na-
ture between eye gaze and speech utterances pro-
vides an opportunity for the system to automatically
acquire different words related to different objects
without any human supervision. To further explore
this idea, we developed a novel unsupervised ap-
proach using statistical translation models.
Our experimental results have shown that this ap-
proach can reasonably uncover the mappings be-
tween words and objects on the graphical display.
The main advantages of this approach include: 1) It
is an unsupervised approach with minimum human
inference; 2) It does not need any prior knowledge to
train a statistical translation model; 3) It yields prob-
abilities that indicate the reliability of the mappings.
Certainly, our current approach is built upon sim-
plified assumptions. It is quite challenging to in-
corporate eye gaze information since it is extremely
noisy with large variances. Recent work has shown
that the effect of eye gaze in facilitating spoken lan-
guage processing varies among different users (Qu
and Chai, 2007). In addition, visual properties of
the interface also affect user gaze behavior and thus
influence the predication of attention (Prasov et al.,
2007) based on eye gaze. Our future work will de-
velop models to address these variations.
Nevertheless, the results from our current work
have several important implications in building ro-
bust conversational interfaces. First of all, most
conversational systems are built with static knowl-
edge space (e.g., vocabularies) and can only be up-
dated by the system developers. Our approach can
potentially allow the system to automatically ac-
quire knowledge and vocabularies based on the nat-
ural interactions with the users without human in-
tervention. Furthermore, the automatically acquired
mappings between words and objects can also help
language interpretation tasks such as reference res-
olution. Given the recent advances in eye track-
ing technology (Duchowski, 2002), integrating non-
intrusive and high performance eye trackers with
conversational interfaces becomes feasible. The
work reported here can potentially be integrated in
practical systems to improve the overall robustness
of human machine conversation.
</bodyText>
<sectionHeader confidence="0.976625" genericHeader="conclusions">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.9999178">
This work was supported by funding from National
Science Foundation (IIS-0347548, IIS-0535112,
and IIS-0643494) and Disruptive Technology Of-
fice. The authors would like to thank Zahar Prasov
for his contribution to data collection.
</bodyText>
<sectionHeader confidence="0.996465" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999633977272727">
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263–311.
E. Campana, J. Baldridge, J. Dowding, B. A. Hockey,
R. Remington, and L. S. Stone. 2001. Using eye
movements to determine referents in a spoken dialog
system. In Proceedings of PUI’01.
A. T. Duchowski. 2002. A breath-first survey of eye
tracking applications. Behavior Research methods, In-
struments, and Computers, 33(4).
Z. M. Griffin and K. Bock. 2000. What the eyes say
about speaking. Psychological Science, 11:274–279.
M. A. Just and P. A. Carpenter. 1976. Eye fixations and
cognitive processes. Cognitive Psychology, 8:441–
480.
M. Kaur, M. Tremaine, N. Huang, J. Wilder, Z. Gacovski,
F. Flippo, and C. S. Mantravadi. 2003. Where is “it”?
Event synchronization in gaze-speech input systems.
In Proceedings ofICMI’03, pages 151–157.
Z. Prasov, J. Y. Chai, and H. Jeong. 2007. Eye gaze
for attention prediction in multimodal human-machine
conversation. In 2007 Spring Symposium on Inter-
action Challenges for Arti�cial Assistants, Palo Alto,
California, March.
S. Qu and J. Y. Chai. 2007. An exploration of eye gaze
in spoken language processing for multimodal con-
versational interfaces. In NAACL’07, pages 284–291,
Rochester, New York, April.
D. Roy and A. Pentland. 2002. Learning words from
sights and sounds, a computational model. Cognitive
Science, 26(1):113–1146.
J. M. Siskind. 1995. Grounding language in perception.
Arti�cial Intelligence Review, 8:371–391.
K. Tanaka. 1999. A robust selection system using real-
time multi-modal user-agent interactions. In Proceed-
ings ofIUI’99, pages 105–108.
M. K. Tenenhaus, M. Sivey-Knowlton, E. Eberhard, and
J. Sedivy. 1995. Integration of visual and linguistic
information during spoken language comprehension.
Science, 268:1632–1634.
C. Yu and D. H. Ballard. 2004. On the integration of
grounding language and learning objects. Proceedings
ofAAAI’04.
</reference>
<page confidence="0.999106">
375
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.969059">
<title confidence="0.999862">Automated Vocabulary Acquisition and Interpretation in Multimodal Conversational Systems</title>
<author confidence="0.999967">Yi Liu Joyce Y Chai Rong Jin</author>
<affiliation confidence="0.99922">Department of Computer Science and Engineering Michigan State University</affiliation>
<address confidence="0.99994">East Lansing, MI 48824, USA</address>
<email confidence="0.993761">jchai,</email>
<abstract confidence="0.998173428571429">Motivated by psycholinguistic findings that eye gaze is tightly linked to human language production, we developed an unsupervised approach based on translation models to automatically learn the mappings between words and objects on a graphic display during human machine conversation. The experimental results indicate that user eye gaze can provide useful information to establish such mappings, which have important implications in automatically acquiring and interpreting user vocabularies for conversational systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="14459" citStr="Brown et al., 1993" startWordPosition="2358" endWordPosition="2361">the complexity in the gazespeech pattern, we propose to use statistical translation models. Given a time window of enough length, a speech input that contains a list of spoken references (e.g., definite noun phrases) is always accompanied by a list of naturally occurred eye fixations and therefore a list of objects receiving those fixations. All those pairs of speech references and corresponding fixated objects could be viewed as parallel, i.e. they co-occur within the time window. This situation is very similar to the training process of translation models in statistical machine translation (Brown et al., 1993), where parallel corpus is used to find the mappings between words from different languages by exploiting their co-occurrence patterns. The same idea can be borrowed here: by exploring the co-occurrence statistics, we hope to uncover the exact mapping between those eye fixated objects and spoken references. The intuition is that, the more often a fixation is found to exclusively co-occur with a spoken reference, the more likely a mapping should be established between them. 5 Translation Models for Vocabulary Acquisition and Interpretation Formally, we denote the set of observations by D = {wi,</context>
<context position="17103" citStr="Brown et al., 1993" startWordPosition="2844" endWordPosition="2847">vations given the 371 Pr(wi, ai|oi) where the right-hand side is actually the expansion w of Pai Qji Pr(wi j |5ai ). Therefore, the likelihood can be simplified as Pr(D; O) = YN Pr(liw |oi) lw i loi Pr( �wi,j|5i,k) i=1 (lo i + 1)lw i Y X j=1 k=0 Switching to the notations wj and ok, we have where δwi,j = 1 if wi,j E wi and δwi,j = 0 otherwise, and δoi,k = 1 if 5i,k E oi and δoi,k = 0 otherwise. Finally, the translation model can be formalized as the following optimization problem arg maxp log Pr(D; O) mw s.t. X Pr(wj|ok) = 1, bk j=1 This optimization problem can be solved by the EM algorithm (Brown et al., 1993). The above model is developed in the context of mapping given objects to words, i.e., its solution yields a set of conditional probabilities {Pr(wj|ok), bj} for each object ok, indicating how likely every word is mapped to it. Similarly, we can develop the model in the context of mapping given words to objects (for vocabulary interpretation), whose solution leads to another set of probabilities {Pr(ok|wj), bk} for each word wj indicating how likely every object is mapped to it. In our experiments, both models are implemented and we will present the results later. 6 Experiments We experimented</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Campana</author>
<author>J Baldridge</author>
<author>J Dowding</author>
<author>B A Hockey</author>
<author>R Remington</author>
<author>L S Stone</author>
</authors>
<title>Using eye movements to determine referents in a spoken dialog system.</title>
<date>2001</date>
<booktitle>In Proceedings of PUI’01.</booktitle>
<marker>Campana, Baldridge, Dowding, Hockey, Remington, Stone, 2001</marker>
<rawString>E. Campana, J. Baldridge, J. Dowding, B. A. Hockey, R. Remington, and L. S. Stone. 2001. Using eye movements to determine referents in a spoken dialog system. In Proceedings of PUI’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A T Duchowski</author>
</authors>
<title>A breath-first survey of eye tracking applications.</title>
<date>2002</date>
<journal>Behavior Research methods, Instruments, and Computers,</journal>
<volume>33</volume>
<issue>4</issue>
<marker>Duchowski, 2002</marker>
<rawString>A. T. Duchowski. 2002. A breath-first survey of eye tracking applications. Behavior Research methods, Instruments, and Computers, 33(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z M Griffin</author>
<author>K Bock</author>
</authors>
<title>What the eyes say about speaking.</title>
<date>2000</date>
<journal>Psychological Science,</journal>
<pages>11--274</pages>
<contexts>
<context position="2060" citStr="Griffin and Bock, 2000" startWordPosition="296" endWordPosition="299">s describing internal entities, and interpretation to refer to the process of automatically identifying internal entities given a particular word. Both acquisition and interpretation have been traditionally approached by either knowledge engineering (e.g., manually created lexicons) or supervised learning from annotated data. In this paper, we describe an unsupervised approach that relies on naturally co-occurred eye gaze and spoken utterances during human machine conversation to automatically acquire and interpret vocabularies. Motivated by psycholinguistic studies (Just and Carpenter, 1976; Griffin and Bock, 2000; Tenenhaus et al., 1995) and recent investigations on computational models for language acquisition and grounding (Siskind, 1995; Roy and Pentland, 2002; Yu and Ballard, 2004), we are particularly interested in two unique questions related to multimodal conversational systems: (1) In a multimodal conversation that involves more complex tasks (e.g., both user initiated tasks and system initiated tasks), is there a reliable temporal alignment between eye gaze and spoken references so that the coupled inputs can be used for automated vocabulary acquisition and interpretation? (2) If such an alig</context>
<context position="7343" citStr="Griffin and Bock, 2000" startWordPosition="1097" endWordPosition="1100"> 1976). Specifically, in human language processing 3 Data Collection tasks, eye gaze is tightly linked to language produc- We used a simplified multimodal conversational systion. The perceived visual context influences spo- tem to collect synchronized speech and eye gaze ken word recognition and mediates syntactic pro- data. A room interior scene was displayed on a comcessing (Tenenhaus et al., 1995). Additionally, be- puter screen, as shown in Figure 1. While watching fore speaking a word, the eyes usually move to the the graphical display, users were asked to communiobjects to be mentioned (Griffin and Bock, 2000). cate with the system on topics about the room decThese psycholinguistics findings have provided a orations. A total of 28 objects (e.g., multiple lamps foundation for our investigation. and picture frames, a bed, two chairs, a candle, a In research on multimodal interactive systems, re- dresser, etc., as marked in Figure 1) are explicitly cent work indicates that the speech and gaze inte- modeled in this scene. The system is simplified in gration patterns can be modeled reliably for indi- the sense that it only supports 14 tasks during human vidual users and therefore be used to improve mul-</context>
</contexts>
<marker>Griffin, Bock, 2000</marker>
<rawString>Z. M. Griffin and K. Bock. 2000. What the eyes say about speaking. Psychological Science, 11:274–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Just</author>
<author>P A Carpenter</author>
</authors>
<title>Eye fixations and cognitive processes.</title>
<date>1976</date>
<journal>Cognitive Psychology,</journal>
<volume>8</volume>
<pages>480</pages>
<contexts>
<context position="2036" citStr="Just and Carpenter, 1976" startWordPosition="292" endWordPosition="295">iring relevant vocabularies describing internal entities, and interpretation to refer to the process of automatically identifying internal entities given a particular word. Both acquisition and interpretation have been traditionally approached by either knowledge engineering (e.g., manually created lexicons) or supervised learning from annotated data. In this paper, we describe an unsupervised approach that relies on naturally co-occurred eye gaze and spoken utterances during human machine conversation to automatically acquire and interpret vocabularies. Motivated by psycholinguistic studies (Just and Carpenter, 1976; Griffin and Bock, 2000; Tenenhaus et al., 1995) and recent investigations on computational models for language acquisition and grounding (Siskind, 1995; Roy and Pentland, 2002; Yu and Ballard, 2004), we are particularly interested in two unique questions related to multimodal conversational systems: (1) In a multimodal conversation that involves more complex tasks (e.g., both user initiated tasks and system initiated tasks), is there a reliable temporal alignment between eye gaze and spoken references so that the coupled inputs can be used for automated vocabulary acquisition and interpretat</context>
</contexts>
<marker>Just, Carpenter, 1976</marker>
<rawString>M. A. Just and P. A. Carpenter. 1976. Eye fixations and cognitive processes. Cognitive Psychology, 8:441– 480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kaur</author>
<author>M Tremaine</author>
<author>N Huang</author>
<author>J Wilder</author>
<author>Z Gacovski</author>
<author>F Flippo</author>
<author>C S Mantravadi</author>
</authors>
<title>Where is “it”? Event synchronization in gaze-speech input systems.</title>
<date>2003</date>
<booktitle>In Proceedings ofICMI’03,</booktitle>
<pages>151--157</pages>
<contexts>
<context position="8040" citStr="Kaur et al., 2003" startWordPosition="1211" endWordPosition="1214">ings have provided a orations. A total of 28 objects (e.g., multiple lamps foundation for our investigation. and picture frames, a bed, two chairs, a candle, a In research on multimodal interactive systems, re- dresser, etc., as marked in Figure 1) are explicitly cent work indicates that the speech and gaze inte- modeled in this scene. The system is simplified in gration patterns can be modeled reliably for indi- the sense that it only supports 14 tasks during human vidual users and therefore be used to improve mul- machine interaction. These tasks are designed to timodal system performances (Kaur et al., 2003). cover both open-ended utterances (e.g., the system 369 measure the length of time gap between a user’s eye fixation falling on an object and the corresponding spoken reference being uttered (which we refer to as “length of time gap” for brevity). Also, we can count the number of times that user fixations happen to change their target objects during this time gap (which we refer to as “number of fixated object changes” for brevity). The nine most frequently occurred spoken references in utterances from all users (as shown in Table 1) are chosen for this empirical study. For each of those spok</context>
</contexts>
<marker>Kaur, Tremaine, Huang, Wilder, Gacovski, Flippo, Mantravadi, 2003</marker>
<rawString>M. Kaur, M. Tremaine, N. Huang, J. Wilder, Z. Gacovski, F. Flippo, and C. S. Mantravadi. 2003. Where is “it”? Event synchronization in gaze-speech input systems. In Proceedings ofICMI’03, pages 151–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Prasov</author>
<author>J Y Chai</author>
<author>H Jeong</author>
</authors>
<title>Eye gaze for attention prediction in multimodal human-machine conversation.</title>
<date>2007</date>
<booktitle>In 2007 Spring Symposium on Interaction Challenges for Arti�cial Assistants,</booktitle>
<location>Palo Alto, California,</location>
<contexts>
<context position="30712" citStr="Prasov et al., 2007" startWordPosition="5231" endWordPosition="5234">oes not need any prior knowledge to train a statistical translation model; 3) It yields probabilities that indicate the reliability of the mappings. Certainly, our current approach is built upon simplified assumptions. It is quite challenging to incorporate eye gaze information since it is extremely noisy with large variances. Recent work has shown that the effect of eye gaze in facilitating spoken language processing varies among different users (Qu and Chai, 2007). In addition, visual properties of the interface also affect user gaze behavior and thus influence the predication of attention (Prasov et al., 2007) based on eye gaze. Our future work will develop models to address these variations. Nevertheless, the results from our current work have several important implications in building robust conversational interfaces. First of all, most conversational systems are built with static knowledge space (e.g., vocabularies) and can only be updated by the system developers. Our approach can potentially allow the system to automatically acquire knowledge and vocabularies based on the natural interactions with the users without human intervention. Furthermore, the automatically acquired mappings between wo</context>
</contexts>
<marker>Prasov, Chai, Jeong, 2007</marker>
<rawString>Z. Prasov, J. Y. Chai, and H. Jeong. 2007. Eye gaze for attention prediction in multimodal human-machine conversation. In 2007 Spring Symposium on Interaction Challenges for Arti�cial Assistants, Palo Alto, California, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Qu</author>
<author>J Y Chai</author>
</authors>
<title>An exploration of eye gaze in spoken language processing for multimodal conversational interfaces.</title>
<date>2007</date>
<booktitle>In NAACL’07,</booktitle>
<pages>284--291</pages>
<location>Rochester, New York,</location>
<contexts>
<context position="30562" citStr="Qu and Chai, 2007" startWordPosition="5208" endWordPosition="5211">ects on the graphical display. The main advantages of this approach include: 1) It is an unsupervised approach with minimum human inference; 2) It does not need any prior knowledge to train a statistical translation model; 3) It yields probabilities that indicate the reliability of the mappings. Certainly, our current approach is built upon simplified assumptions. It is quite challenging to incorporate eye gaze information since it is extremely noisy with large variances. Recent work has shown that the effect of eye gaze in facilitating spoken language processing varies among different users (Qu and Chai, 2007). In addition, visual properties of the interface also affect user gaze behavior and thus influence the predication of attention (Prasov et al., 2007) based on eye gaze. Our future work will develop models to address these variations. Nevertheless, the results from our current work have several important implications in building robust conversational interfaces. First of all, most conversational systems are built with static knowledge space (e.g., vocabularies) and can only be updated by the system developers. Our approach can potentially allow the system to automatically acquire knowledge and</context>
</contexts>
<marker>Qu, Chai, 2007</marker>
<rawString>S. Qu and J. Y. Chai. 2007. An exploration of eye gaze in spoken language processing for multimodal conversational interfaces. In NAACL’07, pages 284–291, Rochester, New York, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roy</author>
<author>A Pentland</author>
</authors>
<title>Learning words from sights and sounds, a computational model.</title>
<date>2002</date>
<journal>Cognitive Science,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="2213" citStr="Roy and Pentland, 2002" startWordPosition="319" endWordPosition="322">cquisition and interpretation have been traditionally approached by either knowledge engineering (e.g., manually created lexicons) or supervised learning from annotated data. In this paper, we describe an unsupervised approach that relies on naturally co-occurred eye gaze and spoken utterances during human machine conversation to automatically acquire and interpret vocabularies. Motivated by psycholinguistic studies (Just and Carpenter, 1976; Griffin and Bock, 2000; Tenenhaus et al., 1995) and recent investigations on computational models for language acquisition and grounding (Siskind, 1995; Roy and Pentland, 2002; Yu and Ballard, 2004), we are particularly interested in two unique questions related to multimodal conversational systems: (1) In a multimodal conversation that involves more complex tasks (e.g., both user initiated tasks and system initiated tasks), is there a reliable temporal alignment between eye gaze and spoken references so that the coupled inputs can be used for automated vocabulary acquisition and interpretation? (2) If such an alignment exists, how can we model this alignment and automatically acquire and interpret the vocabularies? To address the first question, we conducted an em</context>
</contexts>
<marker>Roy, Pentland, 2002</marker>
<rawString>D. Roy and A. Pentland. 2002. Learning words from sights and sounds, a computational model. Cognitive Science, 26(1):113–1146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Siskind</author>
</authors>
<title>Grounding language in perception.</title>
<date>1995</date>
<journal>Arti�cial Intelligence Review,</journal>
<pages>8--371</pages>
<contexts>
<context position="2189" citStr="Siskind, 1995" startWordPosition="317" endWordPosition="318">ar word. Both acquisition and interpretation have been traditionally approached by either knowledge engineering (e.g., manually created lexicons) or supervised learning from annotated data. In this paper, we describe an unsupervised approach that relies on naturally co-occurred eye gaze and spoken utterances during human machine conversation to automatically acquire and interpret vocabularies. Motivated by psycholinguistic studies (Just and Carpenter, 1976; Griffin and Bock, 2000; Tenenhaus et al., 1995) and recent investigations on computational models for language acquisition and grounding (Siskind, 1995; Roy and Pentland, 2002; Yu and Ballard, 2004), we are particularly interested in two unique questions related to multimodal conversational systems: (1) In a multimodal conversation that involves more complex tasks (e.g., both user initiated tasks and system initiated tasks), is there a reliable temporal alignment between eye gaze and spoken references so that the coupled inputs can be used for automated vocabulary acquisition and interpretation? (2) If such an alignment exists, how can we model this alignment and automatically acquire and interpret the vocabularies? To address the first ques</context>
<context position="4973" citStr="Siskind, 1995" startWordPosition="733" endWordPosition="734"> influenced our translation model can reliably capture the mappings work is computational modeling of language acquibetween the eye fixated objects and the correspond- sition and grounding. Recent studies have shown ing spoken references. Given an object, this model that multisensory information (e.g., through vision can provide possible words describing this object, and language processing) can be combined to effecwhich represents the acquisition process; given a tively acquire words to their perceptually grounded word, this model can also provide possible objects objects in the environment (Siskind, 1995; Roy and that are likely to be described, which represents the Pentland, 2002; Yu and Ballard, 2004). Especially in interpretation process. (Yu and Ballard, 2004), an unsupervised approach In the following sections, we first review some re- based on a generative correspondence model was lated work and introduce the procedures used to col- developed to capture the mapping between spoken lect eye gaze and speech data during human machine words and the occurring perceptual features of obconversation. We then describe our empirical study jects. This approach is most similar to the translaand the </context>
</contexts>
<marker>Siskind, 1995</marker>
<rawString>J. M. Siskind. 1995. Grounding language in perception. Arti�cial Intelligence Review, 8:371–391.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Tanaka</author>
</authors>
<title>A robust selection system using realtime multi-modal user-agent interactions.</title>
<date>1999</date>
<booktitle>In Proceedings ofIUI’99,</booktitle>
<pages>105--108</pages>
<marker>Tanaka, 1999</marker>
<rawString>K. Tanaka. 1999. A robust selection system using realtime multi-modal user-agent interactions. In Proceedings ofIUI’99, pages 105–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M K Tenenhaus</author>
<author>M Sivey-Knowlton</author>
<author>E Eberhard</author>
<author>J Sedivy</author>
</authors>
<title>Integration of visual and linguistic information during spoken language comprehension.</title>
<date>1995</date>
<journal>Science,</journal>
<pages>268--1632</pages>
<contexts>
<context position="2085" citStr="Tenenhaus et al., 1995" startWordPosition="300" endWordPosition="303">tities, and interpretation to refer to the process of automatically identifying internal entities given a particular word. Both acquisition and interpretation have been traditionally approached by either knowledge engineering (e.g., manually created lexicons) or supervised learning from annotated data. In this paper, we describe an unsupervised approach that relies on naturally co-occurred eye gaze and spoken utterances during human machine conversation to automatically acquire and interpret vocabularies. Motivated by psycholinguistic studies (Just and Carpenter, 1976; Griffin and Bock, 2000; Tenenhaus et al., 1995) and recent investigations on computational models for language acquisition and grounding (Siskind, 1995; Roy and Pentland, 2002; Yu and Ballard, 2004), we are particularly interested in two unique questions related to multimodal conversational systems: (1) In a multimodal conversation that involves more complex tasks (e.g., both user initiated tasks and system initiated tasks), is there a reliable temporal alignment between eye gaze and spoken references so that the coupled inputs can be used for automated vocabulary acquisition and interpretation? (2) If such an alignment exists, how can we </context>
<context position="7123" citStr="Tenenhaus et al., 1995" startWordPosition="1060" endWordPosition="1063">is paper to examthat the direction of gaze carries information about ine whether this modality can be utilized for vocabthe focus of the user’s attention (Just and Carpenter, ulary acquisition for conversational systems. 1976). Specifically, in human language processing 3 Data Collection tasks, eye gaze is tightly linked to language produc- We used a simplified multimodal conversational systion. The perceived visual context influences spo- tem to collect synchronized speech and eye gaze ken word recognition and mediates syntactic pro- data. A room interior scene was displayed on a comcessing (Tenenhaus et al., 1995). Additionally, be- puter screen, as shown in Figure 1. While watching fore speaking a word, the eyes usually move to the the graphical display, users were asked to communiobjects to be mentioned (Griffin and Bock, 2000). cate with the system on topics about the room decThese psycholinguistics findings have provided a orations. A total of 28 objects (e.g., multiple lamps foundation for our investigation. and picture frames, a bed, two chairs, a candle, a In research on multimodal interactive systems, re- dresser, etc., as marked in Figure 1) are explicitly cent work indicates that the speech a</context>
</contexts>
<marker>Tenenhaus, Sivey-Knowlton, Eberhard, Sedivy, 1995</marker>
<rawString>M. K. Tenenhaus, M. Sivey-Knowlton, E. Eberhard, and J. Sedivy. 1995. Integration of visual and linguistic information during spoken language comprehension. Science, 268:1632–1634.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Yu</author>
<author>D H Ballard</author>
</authors>
<title>On the integration of grounding language and learning objects.</title>
<date>2004</date>
<booktitle>Proceedings ofAAAI’04.</booktitle>
<contexts>
<context position="2236" citStr="Yu and Ballard, 2004" startWordPosition="323" endWordPosition="326">ation have been traditionally approached by either knowledge engineering (e.g., manually created lexicons) or supervised learning from annotated data. In this paper, we describe an unsupervised approach that relies on naturally co-occurred eye gaze and spoken utterances during human machine conversation to automatically acquire and interpret vocabularies. Motivated by psycholinguistic studies (Just and Carpenter, 1976; Griffin and Bock, 2000; Tenenhaus et al., 1995) and recent investigations on computational models for language acquisition and grounding (Siskind, 1995; Roy and Pentland, 2002; Yu and Ballard, 2004), we are particularly interested in two unique questions related to multimodal conversational systems: (1) In a multimodal conversation that involves more complex tasks (e.g., both user initiated tasks and system initiated tasks), is there a reliable temporal alignment between eye gaze and spoken references so that the coupled inputs can be used for automated vocabulary acquisition and interpretation? (2) If such an alignment exists, how can we model this alignment and automatically acquire and interpret the vocabularies? To address the first question, we conducted an empirical study to examin</context>
<context position="5074" citStr="Yu and Ballard, 2004" startWordPosition="748" endWordPosition="751">eling of language acquibetween the eye fixated objects and the correspond- sition and grounding. Recent studies have shown ing spoken references. Given an object, this model that multisensory information (e.g., through vision can provide possible words describing this object, and language processing) can be combined to effecwhich represents the acquisition process; given a tively acquire words to their perceptually grounded word, this model can also provide possible objects objects in the environment (Siskind, 1995; Roy and that are likely to be described, which represents the Pentland, 2002; Yu and Ballard, 2004). Especially in interpretation process. (Yu and Ballard, 2004), an unsupervised approach In the following sections, we first review some re- based on a generative correspondence model was lated work and introduce the procedures used to col- developed to capture the mapping between spoken lect eye gaze and speech data during human machine words and the occurring perceptual features of obconversation. We then describe our empirical study jects. This approach is most similar to the translaand the unsupervised approach based on translation tion model used in our work. However, compared models. Fin</context>
</contexts>
<marker>Yu, Ballard, 2004</marker>
<rawString>C. Yu and D. H. Ballard. 2004. On the integration of grounding language and learning objects. Proceedings ofAAAI’04.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>