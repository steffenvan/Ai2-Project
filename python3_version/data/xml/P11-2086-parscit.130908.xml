<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007460">
<title confidence="0.989218">
Automatic Assessment of Coverage Quality in Intelligence
Reports
</title>
<author confidence="0.997774">
Samuel Brody
</author>
<affiliation confidence="0.953542666666667">
School of Communication
and Information
Rutgers University
</affiliation>
<email confidence="0.995381">
sdbrody@gmail.com
</email>
<author confidence="0.996196">
Paul Kantor
</author>
<affiliation confidence="0.952930666666667">
School of Communication
and Information
Rutgers University
</affiliation>
<email confidence="0.997996">
paul.kantor@rutgers.edu
</email>
<sectionHeader confidence="0.980045" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9994205625">
Common approaches to assessing docu-
ment quality look at shallow aspects, such
as grammar and vocabulary. For many
real-world applications, deeper notions of
quality are needed. This work represents
a first step in a project aimed at devel-
oping computational methods for deep as-
sessment of quality in the domain of intel-
ligence reports. We present an automated
system for ranking intelligence reports
with regard to coverage of relevant mate-
rial. The system employs methodologies
from the field of automatic summarization,
and achieves performance on a par with
human judges, even in the absence of the
underlying information sources.
</bodyText>
<sectionHeader confidence="0.996308" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.988611864864865">
Distinguishing between high- and low-quality
documents is an important skill for humans, and
a challenging task for machines. The majority of
previous research on the subject has focused on
low-level measures of quality, such as spelling,
vocabulary and grammar. However, in many
real-world situations, it is necessary to employ
deeper criteria, which look at the content of the
document and the structure of argumentation.
One example where such criteria are essential
is decision-making in the intelligence commu-
nity. This is also a domain where computational
methods can play an important role. In a typi-
cal situation, an intelligence officer faced with an
important decision receives reports from a team
of analysts on a specific topic of interest. Each
decision may involve several areas of interest,
resulting in several collections of reports. Addi-
491
tionally, the officer may be engaged in many de-
cision processes within a small window of time.
Given the nature of the task, it is vital that
the limited time be used effectively, i.e., that
the highest-quality information be handled first.
Our project aims to provide a system that will
assist intelligence officers in the decision making
process by quickly and accurately ranking re-
ports according to the most important criteria
for the task.
In this paper, as a first step in the project,
we focus on content-related criteria. In particu-
lar, we chose to start with the aspect of “cover-
age&amp;quot;. Coverage is perhaps the most important
element in a time-sensitive scenario, where an
intelligence officer may need to choose among
several reports while ensuring no relevant and
important topics are overlooked.
</bodyText>
<sectionHeader confidence="0.998525" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.977151609756098">
Much of the work on automatic assessment of
document quality has focused on student essays
(e.g., Larkey 1998; Shermis and Burstein 2002;
Burstein et al. 2004), for the purpose of grad-
ing or assisting the writers (e.g., ESL students).
This research looks primarily at issues of gram-
mar, lexical selection, etc. For the purpose of
judging the quality of intelligence reports, these
aspects are relatively peripheral, and relevant
mostly through their effect on the overall read-
ability of the document. The criteria judged
most important for determining the quality of
an intelligence report (see Sec. 2.1) are more
complex and deal with a deeper level of repre-
sentation.
In this work, we chose to start with crite-
ria related to content choice. For this task,
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 491–495,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
we propose that the most closely related prior
research is that on automatic summarization,
specifically multi-document extractive summa-
rization. Extractive summarization works along
the following lines (Goldstein et al., 2000): (1)
analyze the input document(s) for important
themes; (2) select the best sentences to include
in the summary, taking into account the sum-
marization aspects (coverage, relevance, redun-
dancy) and generation aspects (grammaticality,
sentence flow, etc.). Since we are interested in
content choice, we focus on the summarization
aspects, starting with coverage. Effective ways
of representing content and ensuring coverage
are the subject of ongoing research in the field
(e.g., Gillick et al. 2009, Haghighi and Vander-
wende 2009). In our work, we draw on ele-
ments from this research. However, they must
be adapted to our task of quality assessment and
must take into account the specific characteris-
tics of our domain of intelligence reports. More
detail is provided in Sec. 3.1.
</bodyText>
<subsectionHeader confidence="0.957047">
2.1 The ARDA Challenge Workshop
</subsectionHeader>
<bodyText confidence="0.897275666666667">
Given the nature of our domain, real-world data
and gold standard evaluations are difficult to ob-
tain. We were fortunate to gain access to the
reports and evaluations from the ARDA work-
shop (Morse et al., 2004), which was conducted
by NIST in 2004. The workshop was designed to
demonstrate the feasibility of assessing the effec-
tiveness of information retrieval systems. Dur-
ing the workshop, seven intelligence analysts
were each asked to use one of several IR sys-
tems to obtain information about eight different
scenarios and write a report about each. This
resulted in 56 individual reports.
The same seven analysts were then asked to
judge each of the 56 reports (including their
own) on several criteria on a scale of 0 (worst)
to 5 (best). These criteria, listed in Table 1,
were chosen by the researchers as desirable in
a “high-quality” intelligence report. From an
NLP perspective they can be divided into three
broad categories: content selection, structure,
and readability. The written reports, along with
their associated human quality judgments, form
the dataset used in our experiments. As men-
tioned, this work focuses on coverage. When as-
Content
COVER covers the material relevant to the query
NO-IRR avoids irrelevant material
NO-RED avoids redundancy
Structure
ORG organized presentation of material
Readability
CLEAR clear and easy to read and understand
</bodyText>
<tableCaption confidence="0.9927615">
Table 1: Quality criteria used in the ARDA work-
shop, divided into broad categories.
</tableCaption>
<bodyText confidence="0.999795">
sessing coverage, it is only meaningful to com-
pare reports on the same scenario. Therefore,
we regard our dataset as 8 collections (Scenario
A to Scenario H), each containing 7 reports.
</bodyText>
<sectionHeader confidence="0.998664" genericHeader="conclusions">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.958182">
3.1 Methodology
</subsectionHeader>
<bodyText confidence="0.999955655172414">
In the ARDA workshop, the analysts were
tasked to extract and present the information
which was relevant to the query subject. This
can be viewed as a summarization task. In fact,
a high quality report shares many of the charac-
teristics of a good document summary. In par-
ticular, it seeks to cover as much of the impor-
tant information as possible, while avoiding re-
dundancy and irrelevant information.
When seeking to assess these qualities, we
can treat the analysts’ reports as output from
(human) summarization systems, and employ
methods from automatic summarization to eval-
uate how well they did.
One challenge to our analysis is that we do
not have access to the information sources used
by the analysts. This limitation is inherent to
the domain, and will necessarily impact the as-
sessment of coverage, since we have no means of
determining whether an analyst has included all
the relevant information to which she, in partic-
ular, had access. We can only assess coverage
with respect to what was included in the other
analysts’ reports. For our task, however, this
is sufficient, since our purpose is to identify, for
the person who must choose among them, the
report which is most comprehensive in its cover-
age, or indicate a subset of reports which cover
all topics discussed in the collection as a wholes.
</bodyText>
<footnote confidence="0.871975666666667">
&apos;The absence of the sources also means the system
is only able to compare reports on the same subject, as
opposed to humans, who might rank the coverage quality
</footnote>
<page confidence="0.995369">
492
</page>
<bodyText confidence="0.999953576923077">
As a first step in modeling relevant concepts
we employ a word-gram representation, and use
frequency as a measure of relevance. Exam-
ination of high-quality human summaries has
shown that frequency is an important factor
(Nenkova et al., 2006), and word-gram repre-
sentations are employed in many summariza-
tion systems (e.g., Radev et al. 2004, Gillick and
Favre 2009). Following Gillick and Favre (2009),
we use a bigram representation of concepts2. For
each document collection D, we calculate the av-
erage prevalence of every bigram concept in the
collection:
Where r labels a report in the collection, and
Countr(c) is the number of times the concept c
appears in report r.
This scoring function gives higher weight to
concepts which many reports mentioned many
times. These are, presumably, the terms consid-
ered important to the subject of interest. We
ignore concepts (bigrams) composed entirely of
stop words. To model the coverage of a report,
we calculate a weighted sum of the concepts it
mentions (multiple mentions do not increase this
score), using the prevalence score as the weight,
as shown in Equation 2.
</bodyText>
<equation confidence="0.9907545">
Cover5core(r E D) = � prevD(c)
cEConcepts(r)
</equation>
<bodyText confidence="0.976452166666667">
(2)
Here, Concepts(r) is the set of concepts ap-
pearing at least once in report r. The system
produces a ranking of the reports in order of
their coverage score (where highest is considered
best).
</bodyText>
<subsectionHeader confidence="0.985794">
3.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.998130821428571">
As a gold standard, we use the average of
the scores given to each report by the human
of two reports on completely different subjects, based on
external knowledge. For our usage scenario, this is not
an issue.
2 W also experimented with unigram and trigram rep-
resentations, which did not do as well as the bigram rep-
resentation (as suggested by Gillick and Favre 2009).
judges3. Since we are interested in ranking re-
ports by coverage, we convert the scores from
the original numerical scale to a ranked list.
We evaluate the performance of the algorithms
(and of the individual judges) using Kendall&apos;s
Tau to measure concordance with the gold stan-
dard. Kendall&apos;s Tau coefficient (Tk) is com-
monly used (e.g., Jijkoun and Hofmann 2009)
to compare rankings, and looks at the number
of pairs of ranked items that agree or disagree
with the ordering in the gold standard. Let
T = {(ai, aj) : ai �g aj} denote the set of pairs
ordered in the gold standard (ai precedes aj).
Let R = {(al, am) : al �r am} denote the set of
pairs ordered by a ranking algorithm. C = T flR
is the set of concordant pairs, i.e., pairs ordered
the same way in the gold standard and in the
ranking, and D = T fl R is the set of discordant
pairs. Kendall&apos;s rank correlation coefficient Tk is
defined as follows:
</bodyText>
<equation confidence="0.9794275">
(3)
|T|
</equation>
<bodyText confidence="0.999979923076923">
The value of Tk ranges from -1 (reversed rank-
ing) to 1 (perfect agreement), with 0 being
equivalent to a random ranking (50% agree-
ment). As a simple baseline system, we rank the
reports according to their length in words, which
asserts that a longer document has “more cov-
erage&amp;quot;. For comparison, we also examine agree-
ment between individual human judges and the
gold standard. In each scenario, we calculate
the average agreement (Tau value) between an
individual judge and the gold standard, and also
look at the highest and lowest Tau value from
among the individual judges.
</bodyText>
<subsectionHeader confidence="0.884651">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.97660825">
Figure 1 presents the results of our ranking ex-
periments on each of the eight scenarios.
Human Performance There is a relatively
wide range of performance among the human
</bodyText>
<footnote confidence="0.982516">
3Since the judges in the NIST experiment were also
the writers of the documents, and the workshop report
(Morse et al., 2004) identified a bias of the individual
judges when evaluating their own reports, we did not
include the score given by the report’s author in this
average. I.e, the gold standard score was the average of
the scores given by the 6 judges who were not the author.
</footnote>
<equation confidence="0.998108142857143">
1 �
prevD(c) =
|D|
rED
Countr(c) (1)
Tk =
|C |− |D|
</equation>
<page confidence="0.999513">
493
</page>
<figure confidence="0.99968275">
A
B
C
D
E
F
G
H
Agreement
-0.2
0.8
0.6
0.4
0.2
0
1
Num. Words
Judges
Concepts
Scenario
</figure>
<figureCaption confidence="0.998113666666667">
Figure 1: Agreement scores (Kendall&apos;s Tau) for the word-count baseline (Num. Words), the concept-based
algorithm (Concepts). Scores for the individual human judges (Judges) are given as a range from lowest to
highest individual agreement score, with `x&apos; indicating the average.
</figureCaption>
<bodyText confidence="0.988456111111111">
judges. This is indicative of the cognitive com-
plexity of the notion of coverage. We can see
that some human judges are better than oth-
ers at assessing this quality (as represented by
the gold standard). It is interesting to note that
there was not a single individual judge who was
worst or best across all cases. A system that out-
performs some individual human judge on this
task can be considered successful, and one that
surpasses the average individual agreement even
more so.
Baseline The experiments bear out the intu-
ition that led to our choice of baseline. The num-
ber of words in a document is significantly corre-
lated with its gold-standard coverage rank. This
simple baseline is surprisingly effective, outper-
forming the worst human judge in seven out of
eight scenarios, and doing better than the aver-
age individual in two of them.
System Performance Our concept-based
ranking system exhibits very strong perfor-
mance4. It is as good or better than the
baseline in all scenarios. It outperforms the
worst individual human judge in seven of the
eight cases, and does better than the average
individual agreement in four. This is in spite of
the fact that the system had no access to the
</bodyText>
<footnote confidence="0.986618">
4Our conclusions are based on the observed differences
in performance, although statistical significance is diffi-
cult to assess, due to the small sample size.
</footnote>
<bodyText confidence="0.998529272727273">
sources of information available to the writers
(and judges) of the reports.
When calculating the overall agreement with
the gold-standard over all the scenarios, our
concept-based system came in second, outper-
forming all but one of the human judges. The
word-count baseline was in the last place, close
behind a human judge. A unigram-based sys-
tem (which was our first attempt at modeling
concepts) tied for third place with two human
judges.
</bodyText>
<subsectionHeader confidence="0.565238">
3.4 Discussion and Future Work
</subsectionHeader>
<bodyText confidence="0.999984375">
We have presented a system for assessing the
relative quality of intelligence reports with re-
gard to their coverage. Our method makes use
of ideas from the summarization literature de-
signed to capture the notion of content units and
relevance. Our system is as accurate as individ-
ual human judges for this concept.
The bigram representation we employ is only
a rough approximation of actual concepts or
themes. We are in the process of obtaining more
documents in the domain, which will allow the
use of more complex models and more sophis-
ticated representations. In particular, we are
considering clusters of terms and probabilistic
topic models such as LDA (Blei et al., 2003).
However, the limitations of our domain, primar-
</bodyText>
<page confidence="0.966961">
494
</page>
<bodyText confidence="0.993175538461538">
ily the small amount of relatively short docu- Goldstein, Jade, Vibhu Mittal, Jaime Carbonell,
ments, may restrict their applicability, and ad- and Mark Kantrowitz. 2000. Multi-document
vocate instead the use of semantic knowledge summarization by sentence extraction. In
and resources. Proc. of the 2000 NAACL-ANLP Work-
This work represents a first step in the com- shop on Automatic summarization - Volume
plex task of assessing the quality of intelligence 4. Association for Computational Linguis-
reports. In this paper we focused on coverage - tics, Stroudsburg, PA, USA, NAACL-ANLP-
perhaps the most important aspect in determin- AutoSum &apos;00, pages 40–48.
ing which single report to read among several. Haghighi, Aria and Lucy Vanderwende. 2009.
There are many other important factors in as- Exploring content models for multi-document
sessing quality, as described in Section 2.1. We summarization. In Proc. of Human Language
will address these in future stages of the quality Technologies: The 2009 Annual Conference
assessment project. of the North American Chapter of the Asso-
4 ACKNOWLEDGMENTS ciation for Computational Linguistics. ACL,
The authors were funded by an IC Postdoc Boulder, Colorado, pages 362–370.
Grant (HM 1582-09-01-0022). The second Jijkoun, Valentin and Katja Hofmann. 2009.
author also acknowledges the support of the Generating a non-english subjectivity lexicon:
AQUAINT program, and the KDD program un- Relations that matter. In Proc. of the 12th
der NSF Grants SES 05-18543 and CCR 00- Conference of the European Chapter of the
87022. We would like to thank Dr. Emile ACL (EACL 2009). ACL, Athens, Greece,
Morse of NIST for her generosity in providing pages 398–405.
the documents and set of judgments from the Larkey, Leah S. 1998. Automatic essay grad-
ARDA Challenge Workshop project, and Prof. ing using text categorization techniques. In
Dragomir Radev for his assistance and advice. SIGIR &apos;98: Proceedings of the 21st annual
We would also like to thank the anonymous re- international ACM SIGIR conference on Re-
viewers for their helpful comments. search and development in information re-
</bodyText>
<reference confidence="0.962392476190476">
References trieval. ACM, New York, NY, USA, pages 90–
Blei, David M., Andrew Y. Ng, and Michael I. 95.
Jordan. 2003. Latent dirichlet allocation. Morse, Emile L., Jean Scholtz, Paul Kantor, Di-
Journal of Machine Learning Research 3:993– ane Kelly, and Ying Sun. 2004. An investi-
1022. gation of evaluation metrics for analytic ques-
Burstein, Jill, Martin Chodorow, and Claudia tion answering. Available by request from the
Leacock. 2004. Automated essay evaluation: first author.
the criterion online writing service. AI Mag. Nenkova, Ani, Lucy Vanderwende, and Kath-
25:27–36. leen McKeown. 2006. A compositional context
Gillick, Dan and Benoit Favre. 2009. A scal- sensitive multi-document summarizer: ex-
able global model for summarization. In Proc. ploring the factors that influence summariza-
of the Workshop on Integer Linear Program- tion. In SIGIR. ACM, pages 573–580.
ming for Natural Language Processing. ACL, Radev, Dragomir R., Hongyan Jing, Ma lgorzata
Stroudsburg, PA, USA, ILP &apos;09, pages 10–18. Sty´s, and Daniel Tam. 2004. Centroid-based
Gillick, Daniel, Benoit Favre, Dilek Hakkani- summarization of multiple documents. Inf.
Tur, Berndt Bohnet, Yang Liu, and Shasha Process. Manage. 40:919–938.
Xie. 2009. The ICSI/UTD Summarization Shermis, Mark D. and Jill C. Burstein, editors.
System at TAC 2009. In Proc. of the Text 2002. Automated Essay Scoring: A Cross-
Analysis Conference workshop, Gaithersburg, disciplinary Perspective. Routledge, 1 edition.
MD (USA).
495
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.580701">
<title confidence="0.994016">Automatic Assessment of Coverage Quality in Intelligence Reports</title>
<author confidence="0.996778">Samuel</author>
<affiliation confidence="0.911002333333333">School of and Rutgers</affiliation>
<email confidence="0.996687">sdbrody@gmail.com</email>
<author confidence="0.99296">Paul</author>
<affiliation confidence="0.917327333333333">School of and Rutgers</affiliation>
<email confidence="0.994462">paul.kantor@rutgers.edu</email>
<abstract confidence="0.999845470588235">Common approaches to assessing document quality look at shallow aspects, such as grammar and vocabulary. For many real-world applications, deeper notions of quality are needed. This work represents a first step in a project aimed at developing computational methods for deep assessment of quality in the domain of intelligence reports. We present an automated system for ranking intelligence reports with regard to coverage of relevant material. The system employs methodologies from the field of automatic summarization, and achieves performance on a par with human judges, even in the absence of the underlying information sources.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>