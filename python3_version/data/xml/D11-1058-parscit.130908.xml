<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.716788">
Multilayer Sequence Labeling
</title>
<author confidence="0.965411">
Ai Azuma Yuji Matsumoto
</author>
<affiliation confidence="0.929434333333333">
Graduate School of Information Science
Nara Institute of Science and Technology
Ikoma, Nara 630-0192, Japan
</affiliation>
<email confidence="0.998535">
{ai-a,matsu}@is.naist.jp
</email>
<sectionHeader confidence="0.996658" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999913076923077">
In this paper, we describe a novel approach to
cascaded learning and inference on sequences.
We propose a weakly joint learning model
on cascaded inference on sequences, called
multilayer sequence labeling. In this model,
inference on sequences is modeled as cas-
caded decision. However, the decision on a
sequence labeling sequel to other decisions
utilizes the features on the preceding results
as marginalized by the probabilistic models
on them. It is not novel itself, but our idea
central to this paper is that the probabilis-
tic models on succeeding labeling are viewed
as indirectly depending on the probabilistic
models on preceding analyses. We also pro-
pose two types of efficient dynamic program-
ming which are required in the gradient-based
optimization of an objective function. One
of the dynamic programming algorithms re-
sembles back propagation algorithm for mul-
tilayer feed-forward neural networks. The
other is a generalized version of the forward-
backward algorithm. We also report experi-
ments of cascaded part-of-speech tagging and
chunking of English sentences and show ef-
fectiveness of the proposed method.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999958829268292">
Machine learning approach is widely used to clas-
sify instances into discrete categories. In many
tasks, however, some set of inter-related labels
should be decided simultaneously. Such tasks are
called structured prediction. Sequence labeling is
the simplest subclass of structured prediction prob-
lems. In sequence labeling, the most likely one
among all the possible label sequences is predicted
for a given input. Although sequence labeling is
the simplest subclass, a lot of real-world tasks are
modeled as problems of this simplest subclass. In
addition, it might offer valuable insight and a toe-
hold for more general and complex structured pre-
diction problems. Many models have been proposed
for sequence labeling tasks, such as Hidden Markov
Models (HMM), Conditional Random Fields (CRF)
(Lafferty et al., 2001), Max-Margin Markov Net-
works (Taskar et al., 2003) and others. These models
have been applied to lots of practical tasks in natural
language processing (NLP), bioinformatics, speech
recognition, and so on. And they have shown great
success in recent years.
In real-world tasks, it is often needed to cascade
multiple predictions. A cascade of predictions here
means the situation in which some of predictions are
made based upon the results of other predictions.
Sequence labeling is not an exception. For exam-
ple, in NLP, we perform named entity recognition or
base-phrase chunking for given sentences based on
part-of-speech (POS) labels predicted by another se-
quence labeler. Natural languages are especially in-
terpreted to have a hierarchy of sequential structures
on different levels of abstraction. Therefore, many
tasks in NLP are modeled as a cascade of sequence
predictions.
If a prediction is based upon the result of another
prediction, we call the former upper stage and the
latter lower stage.
Methods pursued for a cascade of predictions –
including sequence predictions, of course–, are de-
sired to perform certain types of capability. One de-
</bodyText>
<page confidence="0.968758">
628
</page>
<note confidence="0.958068">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 628–637,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.99989609375">
sired capability is rich forward information propa-
gation, that is, the learning and estimation on each
stage of predictions should utilize rich informa-
tion of the results of lower stages whenever pos-
sible. “Rich information” here includes next bests
and confidence information of the results of lower
stages. Another is backward information propaga-
tion, that is, the rich annotated data on an upper stage
should affect the models on lower stages retroac-
tively.
Many current systems for a cascade of sequence
predictions adopt a simple 1-best feed-forward ap-
proach. They simply take the most likely output at
each prediction stage and transfer it to the next upper
stage. Such a framework can maximize reusability
of existing sequence labeling systems. On the other
hand, it exhibits a strong tendency to propagate er-
rors to upper labelers.
Typical improvement on the 1-best approach is
to keep k-best results in the cascade of predictions.
However, the larger k becomes, the more difficult it
is to enumerate and maintain the k-best results. It is
particularly prominent in sequence labeling.
The essence of this orientation is that the labeler
on an upper stage utilizes the information of all the
possible output candidates on lower stages. How-
ever, the size of the output space can become quite
large in sequence labeling. It effectively forbids ex-
plicit enumeration of all possible outputs, so it is
required to represent all the labeling possibilities
compactly or employ some approximation schemes.
Several studies are in this direction. In the method
proposed in Finkel et al. (2006), a cascades of se-
quence predictions is viewed as a Bayesian network,
and sample sequences are drawn at each stage ac-
cording to the output distribution. The samples are
then used to estimate the entire distribution of the
cascade. In the method proposed in Bunescu (2008),
an upper labeler uses the probabilities marginalized
on the parts of the output sequences on lower stages
as weights for the features. The weighted features
are integrated in the model of the labeler on the
upper stage. A k-best approach (e.g., (Collins and
Duffy, 2002)) and the methods mentioned above are
effective to improve the forward information propa-
gation. However, they can never contribute on back-
ward information propagation.
To improve the both directions of information
propagation, Some studies propose the joint learning
of multiple sequence labelers. Sutton et al. (2007)
proposes the joint learning method in case where
multiple labels are assigned to each time slice of
the input sequences. It enables simultaneous learn-
ing and estimation of multiple sequence labelings
on the same input sequences, where time slices of
the outputs of all the out sequences are regularly
aligned. However, it puts the distribution of states
into Bayesian networks with cyclic dependencies,
and exact inference is not tractable in such a model
in general. Therefore, it requires some approxi-
mate inference algorithms in learning or predictions.
Moreover, it only considers the cases where labels of
an input sequence and all output sequences are reg-
ularly aligned. It is not clear how to build a joint
labeling model which handles irregular output label
sequences like semi-Markov models (Sarawagi and
Cohen, 2005).
In this paper, we propose a middle ground for
a cascade of sequence predictions. The proposed
method adopts the basic idea of Bunescu (2008). We
first assume that the model on all the sequence la-
beling stages is probabilistic one. In modeling of an
upper stage, a feature is weighted by the marginal
probability of the fragment of the outputs from a
lower stage. However, this is not novel itself be-
cause it is just a paraphrase of Bunescu’s core idea.
Our intuition behind the proposed method is as fol-
lows. Features integrated in the model on each stage
are weighted by the marginal probabilities of the
fragments of the outputs on lower stages. So, if
the output distributions on lower stages change, the
marginal probabilities of any fragments also change,
and this in turn can change the value of the features
on the upper stage. In other words, the features on
an upper stage indirectly depend on the models on
the lower stages. Based on this intuition, the learn-
ing procedure of the model on an upper stage can
affect not only direct model parameters, but also the
weights of the features by changing the model on
the lower stages. Supervised learning based on an-
notated data on an upper stage may affect the model
or model parameters on the lower stages. It could
be said that the information of annotation data on
an upper stage is propagated back to the model on
lower stages.
In the next section, we describe the formal nota-
</bodyText>
<page confidence="0.998574">
629
</page>
<bodyText confidence="0.9999155">
tion of our model. In Section 3, we propose an opti-
mization procedure according to the intuition noted
above. In Section 4, we report an experimental result
of our method. The proposed method shows some
improvements on a real-world task in comparison
with ordinary methods.
</bodyText>
<sectionHeader confidence="0.991367" genericHeader="introduction">
2 Formalization
</sectionHeader>
<bodyText confidence="0.993895261904762">
In this section, we introduce the formal notation of
our model. Hereafter, for the sake of simplicity, we
only describe the simplest case in which there are
just two stages, one lower stage of sequence labeling
named L1 and one upper stage of sequence labeling
named L2. In L1, the most likely one among a set
of possible sequences is predicted for a given input
x. L2 is also a sequence labeling stage for the same
input x and the output of L1. No assumption is made
on the structure of x. The information of x is totally
encoded in feature functions. It is only assumed that
the output spaces of both L1 and L2 are conditioned
on the initial input x.
First of all, we describe the formalization of the
probabilistic model for L1. The model for L1 per
se is the same as ordinary ones for sequence label-
ing. For a given input x, consider a directed acyclic
graph (DAG) G1 = (V1, E1). A source of a DAG G
is a node whose in-degree is equal to zero. A sink
of a DAG G is nodes whose out-degree is equal to
zero. Let src(G), snk(G) denote the set of source
and sink nodes in G, respectively. A successful path
of a DAG G is defined as a directed path on G whose
starting node is a source and end node is a sink. If y
denotes a path on a DAG, let y also denote the set of
all the arcs appearing on y for the sake of shorthand.
We denote the set of all the possible successful paths
on G1 by Y1. The space of the output candidates for
L1 is exactly equal to Y1. For the modeling of L1, it
is assumed that features of the form f(1,k1,e1,x) ∈ R
(k1 ∈ K1, e1 ∈ E1) are allowed to be used. Here,
K1 is the index set of the feature types for L1. Such
a feature can capture an aspect of the correlation be-
tween adjacent nodes. We call this kind of features
input features for L1. This naming is used to distin-
guish them from another kind of features defined on
L1, which comes later. Although features on V1 can
be also defined, they are totally omitted in this paper
for brevity. Hereafter, if a symbol has subscripts,
then missing subscript indicates a set that range over
def
the omitted subscript. For example, f(1,e1,x)
</bodyText>
<equation confidence="0.9415584">
≡
def
f l
lf(1,k1,e1,x) } k1E)C1, f(1,k1,x) ≡ {f(1,k1,e1,x) 1e1EE1,
def f
</equation>
<bodyText confidence="0.811428666666667">
f(1,x) ≡ lf(1,k1,e1,x)}k1E)C1,e1EE1, and so on.
The probabilistic model on L1 forms the log-linear
model, that is,
</bodyText>
<equation confidence="0.96922025">
def 1
P1(y1|x; 91) ≡ Z1 (x; 91) exp(91 · F(1,y1,x))
(y1 ∈ Y1) ,
(1)
</equation>
<bodyText confidence="0.992196">
where θ(1 k1) ∈ R (k1 ∈ K1) is the weight for the
feature of the same index k1, and the k1-th element
</bodyText>
<equation confidence="0.897926666666667">
def
≡ ∑
of F(1,y1,x), F(1,k1,y1,x) e1Ey1 f(1,k1,e1,x). Dot
</equation>
<bodyText confidence="0.872676666666667">
operator (·) denotes the inner product with respect to
the subscripts commonly missing in both operands.
Z1 is the partition function for P1, defined as
</bodyText>
<equation confidence="0.98418">
exp(91 · F(1,y1,x)) . (2)
</equation>
<bodyText confidence="0.998245666666667">
It is worth noting that this formalization subsumes
both directed and undirected linear-chain graphical
models, which are the most typical models for se-
quence labeling, including HMM and CRF. That is,
if the elements of V1 are aligned into regular time
slices, and the nodes in each time slice are associated
with possible assignments of labels for that time, we
obtain the representation equivalent to the ordinary
linear-chain graphical models, in which all possible
label assignments for each state are expanded. In
such configuration, all the possible successful paths
defined in our notation have strict one-to-one corre-
spondence to all the possible joint assignments of
labels in linear-chain graphical models. We pur-
posely employ this DAG-based notation because; it
is convenient to describe the models and algorithms
for our purpose, it allows for labels to stay in arbi-
trary time as in semi-Markov models, and it is easily
extended to models for a set of trees instead of se-
quences by replacing the graph-based notation with
hypergraph-based notation.
Next, we formalize the probabilistic model on the
upper stage L2. Like L1, consider a DAG G2 =
(V2, E2) conditioned on the input x, and the set of
all the possible successful paths on G2, denoted Y2.
The space of the output candidates for L2 becomes
Y2.
</bodyText>
<equation confidence="0.956635">
∑
def
Z1 (x; 91) ≡
y1EY1
</equation>
<page confidence="0.920252">
630
</page>
<bodyText confidence="0.999738277777778">
The form of the features available in designing the
probabilistic model for L2, denoted by P2, is the key
of this paper. A feature on an arc e2 ∈ E2 can ac-
cess local characteristics of the confidence-rated su-
perposition of the L1’s outputs, in addition to the
information of the input x. To formulate local char-
acteristics of the superposition of the L1’s outputs,
we first define output features of L1, denoted by
h(1,kl,e1) ∈ R (k�1 ∈ Ki, e1 ∈ E1). Here, Ki is
the index set of the output feature types of L1. Be-
fore the output features are integrated into the model
for L2, they all are confidence-rated with respect to
P1, that is, each output feature h(1,k1,e1) is numer-
ically rated by the estimated probabilities summed
over the sequences emitting that feature. More for-
mally, all the L1’s output features are integrated in
features for P2 in the form of the marginalized out-
put features, which are defined as follows;
</bodyText>
<equation confidence="0.978815333333333">
def
h(1,ki,e1)(θ1) ≡ h(1,ki,e1)P1(e1|x;θ1) (3)
(k�1 ∈ K1, e1 ∈ E1 ) ,
</equation>
<bodyText confidence="0.719536">
where
</bodyText>
<equation confidence="0.999644">
∑P1(e1|x; θ1) def ≡ P1(y1|x;θ1)
y1—e1
∑= δe1Ey1P1(y1|x; θ1) (4)
y1EY1
(e1 ∈ E1) .
</equation>
<bodyText confidence="0.9977094">
Here, the notation ∑y1—e1 represents the sum-
mation over sequences consistent with an arc
e1 ∈ E1, that is, the summation over the set
{y1 ∈ Y1  |e1 ∈ y1}. δP denotes the indicator
function for a predicate P. The input features for P2
on an arc e2 ∈ E2 are permitted to arbitrarily com-
bine the information of x and the L1’s marginalized
output features h1, in addition to the local charac-
teristics of the arc at hand e2. In summary, an input
feature for L2 on an arc e2 ∈ E2 is of the form
</bodyText>
<equation confidence="0.996904">
f(2,k2,e2,x) (�h1(θ1)) ∈ R (k2 ∈ K2) , (5)
</equation>
<bodyText confidence="0.9356735">
where K2 is the index set of the input feature types
for L2. To make the optimization procedure feasible,
smoothness condition on any L2’s input feature is
assumed with respect to all the L1’s output features,
that is af(2,k2,e2,X) is always guaranteed to exist for
, a�h(1,ki,e1)
dki, e1, k2, e2. For example, additions and mul-
tiplications between some elements of �h1(θ1) can
appear in the definition of L2’s input features. For
given input features f(2,x) (�h1(θ1)) and parameters
</bodyText>
<equation confidence="0.946884071428571">
θ(2,k2) ∈ R (k2 ∈ K2), the probabilistic model for
L2 is defined as follows;
P2(y2|x; θ1, θ2)
Z2(x; θ1, θ2)
1 exp(θ2 · F(2,y2,x) (�h1(θ1)))
(y2 ∈ Y2) ,
where F(2,k2,y2,x)(�h1(θ1))
def
≡
∑e2Ey2 f(2,k2,e2,x) (h1 (θ1)) and Z2 is the par-
tition function of P2, defined by
Z2(x; θ1, θ2)
∑def ≡ exp(θ2 · F(2,y2,x) (�h1(θ1))) .
y2EY2
</equation>
<bodyText confidence="0.999883384615385">
The definition of P2 (6) reveals one of the most im-
portant points in this paper. P2 is viewed not only
as the function of the ordinary direct parameters θ2
but also as the function of θ1, which represents the
parameters for the L1’s model, through the interme-
diate variables h1. So optimization procedure on P2
may affect the determination of the values not only
of the direct parameters θ2 but also of the indirect
ones θ1.
If the result of L1 is reduced to the single golden
output k1, i.e. P1(y1|x) = δy1=y1, the definitions
above boil down to the formulation of the simple 1-
best feed forward architecture.
</bodyText>
<sectionHeader confidence="0.994009" genericHeader="method">
3 Optimization Algorithm
</sectionHeader>
<bodyText confidence="0.999923555555556">
In this section, we describe optimization procedure
for the model formulated in the previous section.
Let D = {⟨X, ⟨G1, Y1⟩, ⟨G2, Y2⟩⟩7n}7n=1,2,···,M de-
note annotated data for the supervised learning of
the model. Here, ⟨G1, k1⟩ is a pair of a DAG and
correctly annotated successful sequence for L1. The
same holds for ⟨G2, k2⟩. For given D, we can define
the conditional log-likelihood function on L1 and L2
respectively, that is,
</bodyText>
<equation confidence="0.9958004">
L1 (θ1; D)
∑def ≡
(X,Y1)ED log (P1 (k1 |X; θ1)) −2σ12
|θ1 |(8)
def ≡
</equation>
<page confidence="0.98756">
631
</page>
<figureCaption confidence="0.997864">
Figure 1: Computation Graph of the Proposed Model
</figureCaption>
<equation confidence="0.87553375">
|θ2|
_
log (P2 (�y2|k; θ1, θ2)) 2σ22 .
(9)
</equation>
<bodyText confidence="0.9947035">
Here, σ12, σ22 are the variances of the prior distribu-
tions of the parameters. For the sake of simplicity,
we set the prior distribution as the zero-mean uni-
variance Gaussian. To optimize the both probabilis-
tic models P1 and P2 jointly, we also define the joint
conditional log-likelihood function
</bodyText>
<equation confidence="0.988993">
L (θ1, θ2; D) def ≡ L1 + L2 . (10)
</equation>
<bodyText confidence="0.999927892857143">
The parameter values to be learned are the ones that
(possibly locally) maximize this objective function.
Note that this objective function is not guaranteed to
be globally convex.
We employ gradient-based parameter optimiza-
tion here. Optimization procedure repeatedly
searches a direction in the parameter space which
is ascendent with respect to the objective function,
and updates the parameter values into that direction
by small advances. Many existing optimization rou-
tines like steepest descent or conjugation gradient do
that job only by giving the objective value and gra-
dients on parameter values to be updated. So, the
optimization problem here boils down to the calcu-
lation of the objective value and gradients on given
parameter values.
Before entering the detailed description of the al-
gorithm for calculating the objective function and
gradients, we note the functional relations among
the objective function and previously defined vari-
ables. The diagram shown in Figure 1 illustrates
the functional relations among the parameters, input
and output feature functions, models, and objective
function. The variables at the head of a directed ar-
row in the figure is directly defined in terms of the
ones at the tail of the same arrow. The value of the
objective function on given parameter values can be
calculated in order of the arrows shown in the di-
agram. On the other hand, the parameter gradients
are calculated step-by-step in reverse order of the ar-
rows. The functional relations illustrated in the Fig-
ure 1 ensure some forms of the chain rule of dif-
ferentiation among the variables. The chain rule is
iteratively used to decompose the calculation of the
gradients into a divide-and-conquer fashion. These
two directions of stepwise computation are analo-
gous to the forward and back propagation for multi-
layer feedforward neural networks, respectively.
Algorithm 1 shows the whole picture of the
gradient-based optimization procedure for our
model. We first describe the flow to calculate the
objective value for a given parameters θ1 and θ2,
which is shown from line 2 through 4 in Algo-
rithm 1. The values of marginalized output features
�h(1,x) can be calculated by (3). Because they are the
simple marginals of features, the ordinary forward-
backward algorithm (hereafter, abbreviated as “F-
B”) on G1 offers an efficient way to calculate their
values. Although nothing definite about the forms
of the input features for L2 is presented in this pa-
per, f(2,x) can be calculated once the values of �h(1,x)
have been obtained. Finally, L1, L2 and then L are
easy to calculate because they are no different from
the ordinary log-likelihood computation.
Now we describe the algorithm to calculate the
parameter gradients,
</bodyText>
<equation confidence="0.931074">
∂L2 .(11)
∂θ2
</equation>
<bodyText confidence="0.877470875">
Line 5 through line 7 in Algorithm 1 describe the
gradient computation. The terms ∂�1
∂θ1 and ∂�2
∂θ2 in
(11) become the same forms that appear in the ordi-
nary CRF optimization, i.e., the difference between
the empirical frequencies of the features and the
model expectations of them,
</bodyText>
<equation confidence="0.906816">
101
E[F(1,y1,x)] − EP1 [F(1,y1,x) 1� — 101 ,
_ |θ2|
E[F(2,y2,x)] − EP2 [F(2,y2,x)] σ22 .
(12)
</equation>
<bodyText confidence="0.97381">
These calculations are performed by the ordinary F-
B on G1 and G2, respectively. Using the chain rule
of differentiation derived from the functional rela-
tions illustrated in Figure 1, the remaining term ∂L2
</bodyText>
<page confidence="0.810746">
∂θ1
</page>
<figure confidence="0.772763678571428">
91
0,
P, Y1IX
� � ���
�
P2 Y21X
Ll
L2 L
and
L2 (θ1, θ2; D)
∑def ≡
(z,y2)ED
∂L ∂L1 ∂L2 ∂L
= + =
∂θ1 ∂θ1 ∂θ2
,
∂θ1
∂L1
=
∂θ1
∂L2
=
∂θ2
632
Algorithm 1 Gradient-based optimization of the model parameters
Input: θ1, θ2
Output: arg max L
⟨θ1, θ2⟩
</figure>
<listItem confidence="0.98381325">
1: while θ1 or θ2 changes significantly do
2: calculate Z1 by (2), h1 by (3) with the F-B on G1, and then L1 by (8)
3: calculate f⟨2,x⟩ according to their definitions
4: calculate Z2 by (7) with the F-B on G2, and then L2 by (9) and L by (10)
5: calculate ∂L1
∂θ1 and ∂L2
∂θ2 by (12) with the F-B on G1 and G2, respectively
6: calculate ∂L
</listItem>
<equation confidence="0.843563090909091">
∂f⟨1,x⟩ by (16) with the F-B on G2, ∂f⟨1,x⟩
∂�h1 , and them ∂L2 ∂�h1= ∂L
∂f⟨1,x⟩ ¢∂f⟨1,x⟩
∂�h1
7: calculate ∂L2 ∂θ1by (18) with Algorithm 2
8: hθ1, θ2i Ã update-parameters (θ1, θ2, L, ae , a 2 )
9: end while
in (11) can be decomposed as follows;
∂ h1
¢ .
∂θ1
</equation>
<bodyText confidence="0.990205666666667">
Note that Leibniz’s notation here denotes a Jacobian
with the index sets omitted in the numerator and the
denominator, for example,
</bodyText>
<equation confidence="0.819139">
def f ∂f⟨2,k2,e2,x⟩
∂h⟨1,k′1,e1⟩
1k2∈K2,e2∈E2,k′1∈K′1,e1∈E1
</equation>
<bodyText confidence="0.994138">
And also recall that dot operators here stand for the
inner product with respect to the index sets com-
monly omitted in both operands, for example,
</bodyText>
<equation confidence="0.9901646">
∂L2 ∂f2
¢
∂f2 ∂�h1
�= ∂L2 ¢ ∂f⟨2,k2,e2,x⟩ .
k2∈K2,e2∈E2 ∂f⟨2,k2,e2,x⟩ ∂�h1
</equation>
<bodyText confidence="0.859617">
We describe the manipulation of each factor in
the right side of (13) in turn. Noting ∂f⟨2,k2,e2,x⟩∂f⟨2 `k2,`e2,x⟩
δk2=k2δe2=e2, each element of the first factor of (13)
</bodyText>
<equation confidence="0.9597452">
∂L2 can be transformed as follows;
∂f⟨2,x⟩
∂L2
∂f⟨2,k2,e2,x⟩ �
= θ⟨2,k2⟩
⟨z,y2⟩∈D
P2(e2|x; θ1, θ2), the marginal probability on e2, can
be obtained as a by-product of the F-B for (12).
As described in the previous section, it is assumed
∂f⟨2,x⟩
</equation>
<bodyText confidence="0.977844285714286">
that the values of the second factor ∂�h1 is guaran-
teed to exists for any given θ1, and the procedure for
calculating them is fixed in advance. The procedure
for some of concrete features is exemplified in the
previous section.
From the definition of h1 (3), each element of the
third factor of (13) ∂�h1
</bodyText>
<equation confidence="0.909993833333333">
∂θ1 becomes
∂ h⟨1,k′1,e1⟩
∂θ⟨1,k1⟩
� .
= h⟨1,k′ 1,e1⟩���P1�y1|x� �δe1∈y1, F⟨1,k1,y1,x⟩
(17)
</equation>
<bodyText confidence="0.999886333333333">
There exists efficient dynamic programming to cal-
culate the covariance value (17) (without goint into
that detail because it is very similar to the one shown
later in this paper), and of course we can run such
dynamic programming for ∀k′1 E K′1, e1 E E1.
However, the size of the Jacobian
</bodyText>
<equation confidence="0.757538">
∂�h1
∂θ1
</equation>
<bodyText confidence="0.9960955">
is equal to
|K′1|£|E1|£|K1|. Since it is too large in many tasks
likely to arise in practice, we should avoid to calcu-
late all the elements of this Jacobian in a straight-
forward way. Instead of such naive computation, if
the values of ∂L2 and ∂f⟨2,x⟩ are obtained, then we
</bodyText>
<equation confidence="0.975571111111111">
∂f⟨2,X) ah1
can compute ahi 49L2a
= �2 af�2,x⟩ a • ah x⟩1 , and from (13)
∂L2 ∂L2 ¢ ∂f⟨2,x⟩ ∂L2 ¢ ∂f⟨2,x⟩
= ∂f⟨2,x⟩ ∂θ1 ∂f⟨2,x⟩ ∂�h1
∂θ1
∂f⟨2,x⟩
∂�h1
(δe2∈k2 − P2(e2|�x; θ1, θ2)) .
</equation>
<page confidence="0.912894">
633
</page>
<bodyText confidence="0.3375705">
and (17),
aL2
</bodyText>
<equation confidence="0.968788230769231">
=
a01
[ ]
= EP1(y1|x) H′ ⟨1,y1⟩F⟨1,y1,x⟩
− EP1(y1|x)[H⟨1,y1⟩] EP1(y1|x) [F⟨1,y1,x⟩]
(18)
defh In other
where Hs1 y1⟩ ≡ Ee1∈y1 ∂¯h(1e1) ⟨1&apos;e1⟩
words ∂L2 becomes the covariance between the
, ∂θ(1,k1)
k1-th input feature for L1 and the hypothetical fea-
ture h′ def ≡ ∂L2 h
⟨1,e1⟩ ∂¯h(1 e1) ⟨1,e1⟩.
</equation>
<bodyText confidence="0.941188909090909">
The final problem is to derive an efficient way to
compute the first term of (18). The second term of
(18) can be calculated by the ordinary F-B because
it consists of the marginals of arc features. There are
two derivations of the algorithm for calculating the
first term. We describe briefly the both derivations.
One is a variant of the F-B on the expectation
semi-ring proposed in Li and Eisner (2009). First,
the F-B is generalized to the expectation semi-ring
with respect to the hypothetical feature h′⟨1,e1⟩, and
by summing up the marginals of the feature vectors
f⟨1,e1,x⟩ on all the arcs under the distribution of the
semi-ring, then we obtain the expectation of the fea-
ture vector f⟨1,e1,x⟩ on the semi-ring potential. This
expectation is equal to the first term of (18). 1
Another derivation is to apply the automatic dif-
ferentiation (AD)(Wengert, 1964; Corliss et al.,
]. It
2002) on the F-B calculating EP1 [F⟨1,y1,x⟩
exploits the fact that ∂∂λ EPi [F⟨1 y1 x⟩]� is
λ=0
equal to the first term of (18), where A ∈
</bodyText>
<equation confidence="0.9839374">
R is a dummy parameter, and P (y1 Ix� def
i
Z1
1exp (01 · F⟨1,y1,x⟩ + AH′ . It is easy
⟨1,y1⟩
</equation>
<bodyText confidence="0.794424">
to derive the F-B for calculating the value
EP� [F⟨1,y1,x⟩] ���λ=0. AD transforms this F-B into
</bodyText>
<equation confidence="0.687122">
1
</equation>
<bodyText confidence="0.998796285714286">
another algorithm for calculating the differentiation
w.r.t. A evaluated at the point A = 0. This trans-
formation is achieved in an automatic manner, by
replacing all appearances of A in the F-B with a dual
number A + ε. The dual number is a variant of the
complex number, with a kind of the imaginary unit
ε with the property ε2 = 0. Like the usual complex
</bodyText>
<footnote confidence="0.979433">
1For the detailed description, see Li and Eisner (2009) and
its references.
</footnote>
<bodyText confidence="0.999937952380953">
numbers, the arithmetic operations and the exponen-
tial function are generalized to the dual numbers,
and the ordinary F-B is also generalized to the dual
numbers. The imaginary part of the resulting values
is equal to the needed differentiation. 2 Anyway,
these two derivations lead to the same algorithm, and
the resulting algorithm is shown as Algorithm 2.
The final line in the loop of Algorithm 1 can be
implemented by various optimization routines and
line search algorithms.
The time and space complexity to compute the ob-
jective and gradient values for given parameter vec-
tors 01, 02 is the same as that for that for Bunescu
(2008), up to a constant factor. Because the calcula-
tion of the objective function is essentially the same
as that for Bunescu (2008), and in gradient com-
putation, the time complexity of Algorithm 1 is the
same as that for the ordinary F-B (up to a constant
factor), and the proposed optimization procedure is
only required to store additional scalar values h′⟨1,e1⟩
on each G1’s arc.
</bodyText>
<sectionHeader confidence="0.999397" genericHeader="method">
4 Experiment
</sectionHeader>
<bodyText confidence="0.999977590909091">
We examined effectiveness of the method proposed
in this paper on a real task. The task is to annotate
the POS tags and to perform base-phrase chunking
on English sentences.
Base-phrase chunking is a task to classify con-
tinuous subsequences of words into syntactic cat-
egories. This task is performed by annotating a
chunking label on each word (Ramshaw and Mar-
cus, 1995). The types of chunking label consist of
“Begin-Category”, which represents the beginning
of a chunk, “Inside-Category”, which represents the
inside of a chunk, and “Other.” Usually, POS la-
beling runs first before base-phrase chunking is per-
formed. Therefore, this task is a typical interesting
case where a sequence labeling depends on the out-
put from other sequence labelers.
The data used for our experiment consist of En-
glish sentences from the Penn Treebank project
(Marcus et al., 1993) consisting of 10948 sentences
and 259104 words. We divided them into two
groups, training data consisting of 8936 sentences
and 211727 words and test data consisting of 2012
</bodyText>
<footnote confidence="0.992082">
2For example, Berz (1992) gives a detailed description of
the reason why the dual number is used for this purpose.
</footnote>
<note confidence="0.957173333333333">
aL2 · a h1
ah1
a01
</note>
<page confidence="0.95708">
634
</page>
<figure confidence="0.989555230769231">
Algorithm 2 Forward-backward Algorithm for Calculating Feature Covariances
def
def
Input: f⟨1,x⟩, ϕe1 ≡ exp(�1 · f⟨1,e1,x⟩), h′ ≡ ∂L2
e1 ∂�h⟨1��1⟩ ·h⟨1,e1⟩
Output: qk1 = CovP(y1|x) I
H′ F⟨1,k1,y1,x⟩1 (∀k1 ∈ K1)
1: for ∀v1 ∈ src(G1), αv1 ← 1, α′v1 ← 1
2: for all v1 ∈ V1 in a topological order do
3: prev ← {x ∈ V1  |(x, v1) ∈ E1}
4: αv1 ← E ϕ(x,v1)αx, α′v1 ← E (ϕ(x,v1) h′ (x,v1)α ′
x∈prev x∈prev x + αx)
5: end for
6: Z1 ← E αx
x∈snk(G1)
7: for ∀v1 ∈ snk(G1), βv1 ← 1, β′v1 ← 1
8: for all v1 ∈ V1 in a reverse topological order do
9: next ← {x ∈ V1  |(v1, x) ∈ E1}
10: βv1 ← E ϕ(v1,x)βx, β′v1 ← E ( )
x∈next x∈next ϕ(v1,x) h′(v1,x)βx + β′ x
11: end for
12: for ∀k1 ∈ K1, qk1 ← 0
13: for all (u1, v1) ∈ E1 do
) /Z1
14: p ← ϕ(u1,v1) (αu1β′ v1 + α′ u1βv1
15: for ∀k1 ∈ K1, qk1 ← qk1 + pf⟨1,k1,e1,x⟩
</figure>
<sectionHeader confidence="0.46384" genericHeader="method">
16: end for
</sectionHeader>
<bodyText confidence="0.999110782608696">
sentences and 47377 words. The number of the POS
label types is equal to 45. The number of the label
types used in base-phrase chunking is equal to 23.
We compare the proposed method to two exist-
ing sequence labeling methods as baselines. The
POS labeler is the same in all the three methods
used in this experiment. This labeler is a simple
CRF and learned by ordinary optimization proce-
dure. One baseline method is the 1-best pipeline
method. A simple CRF model is learned for the
chunking labeling, on the input sentences and the
most likely POS label sequences predicted by the
already learned POS labeler. We call this method
“CRF + CRF.” The other baseline method has a
CRF model for the chunking labeling, which uses
the marginalized features offered by the POS la-
beler. However, the parameters of the POS labeler
are fixed in the training of the chunking model.
This method corresponds to the method proposed
in Bunescu (2008). We call this baseline “CRF +
CRF-MF” (“MF” for “marginalized features”). The
proposed method is the same as “CRF + CRF-MF”,
except that the both labelers are jointly trained by the
</bodyText>
<table confidence="0.990494">
CRF CRF CRF
+ CRF + CRF-MF +CRF-BP
POS labeling 95.6 (95.6) 95.8
Base-phrase 92.1 92.7 93.1
chunking
</table>
<tableCaption confidence="0.999317">
Table 2: Experimental result (F-measure)
</tableCaption>
<bodyText confidence="0.9991154">
procedure described in Section 3. We call this pro-
posed method “CRF + CRF-BP” (“BP” for “back
propagation”).
In “CRF + CRF-BP,” the objective function for
joint learning (10) is not guaranteed to be convex, so
optimization procedure is sensible to the initial con-
figuration of the model parameters. In this experi-
ment, we set the parameter values learned by “CRF
+ CRF-MF” as the initial values for the training of
the “CRF + CRF-BP” method. Feature templates
used in this experiment are listed in Table 1. Al-
though we only described the formalization and op-
timization procedure of the models with arc features,
We use node features in the experiment.
Table 2 shows the result of the methods we men-
</bodyText>
<page confidence="0.995792">
635
</page>
<table confidence="0.990957714285714">
=== Node feature templates ===
Node is source
Node is sink
Input word on the same time slice
Suffix of input word on the same time slice, n characters (n E [1, 2, 3])
Initial word character is capitalizedf
All word characters are capitalizedf
Input word included in the vocabulary of POS Tf (T E {(All possible POS labels)})
Input word contains numbersf
POS label$
=== Arc feature templates ===
Tail node is source
Head node is sink
Corresponding ordered pair of POS labels$
</table>
<tableCaption confidence="0.9848105">
Table 1: List of feature templates. All node features are combined with the corresponding node label (POS or chunking
label) feature. All arc features are combined with the feature of the corresponding arc label pair. t features are
instantiated on each time slice in five character window. t features are not used in POS labeler, and marginalized as
output features for “CRF + CRF-MF” and “CRF + CRF-BP.”
</tableCaption>
<bodyText confidence="0.996851">
tioned. In Table 2, bold numbers indicate significant
improvement over the baseline models with α =
0.05. From Table 2, the proposed method signifi-
cantly outperforms two baseline methods on chunk-
ing performance. Although the improvement on
POS labeling performance by the proposed method
“CRF + CRF-BP” is not significant, it might show
that optimization procedure provides some form of
backward information propagation in comparison to
“CRF + CRF-MF.”
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999961222222222">
In this paper, we adopt the method to weight features
on an upper sequence labeling stage by the marginal-
ized probabilities estimated by the model on lower
stages. We also point out that the model on an upper
stage is considered to depend on the model on lower
stages indirectly. In addition, we propose optimiza-
tion procedure that enables the joint optimization of
the multiple models on the different level of stages.
We perform an experiment on a real-world task, and
our method significantly outperforms existing meth-
ods.
We examined the effectiveness of the proposed
method only on one task in comparison to just a few
existing methods. In the future, we hope to compare
our method to other competing methods like joint
learning approaches in terms of both accuracy and
computational efficiency, and perform extensive ex-
periments on various tasks.
</bodyText>
<sectionHeader confidence="0.998748" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999746083333334">
M. Berz. 1992. Automatic differentiation as nonar-
chimedean analysis. In Computer Arithmetic and En-
closure, pages 439–450.
R.C. Bunescu. 2008. Learning with probabilistic fea-
tures for improved pipeline models. In Proceedings of
the 2008 Conference on Empirical Methods in Natural
Language Processing, pages 670–679.
M. Collins and N. Duffy. 2002. New ranking algorithms
for parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In Proceedings of
the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 263–270. Association for
Computational Linguistics.
G.F. Corliss, C. Faure, and A. Griewank. 2002. Auto-
matic differentiation of algorithms: from simulation to
optimization. Springer Verlag.
J.R. Finkel, C.D. Manning, and A.Y. Ng. 2006. Solv-
ing the problem of cascading errors: Approximate
bayesian inference for linguistic annotation pipelines.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 618–
626.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
</reference>
<page confidence="0.986292">
636
</page>
<reference confidence="0.9984588125">
menting and labeling sequence data. In Proceedings of
the Eighteenth International Conference on Machine
Learning, pages 282–289.
Z. Li and J. Eisner. 2009. First-and second-order ex-
pectation semirings with applications to minimum-risk
training on translation forests. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing: Volume 1-Volume 1, pages 40–
51.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational linguistics,
19(2):330.
L.A. Ramshaw and M.P. Marcus. 1995. Text chunking
using transformation-based learning. In Proceedings
of the Third ACL Workshop on Very Large Corpora,
pages 82–94. Cambridge MA, USA.
S. Sarawagi and W.W. Cohen. 2005. Semi-markov
conditional random fields for information extraction.
Advances in Neural Information Processing Systems,
17:1185–1192.
C. Sutton, A. McCallum, and K. Rohanimanesh. 2007.
Dynamic conditional random fields: Factorized proba-
bilistic models for labeling and segmenting sequence
data. The Journal of Machine Learning Research,
8:693–723.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In Advances in Neural Information
Processing Systems 16.
RE Wengert. 1964. A simple automatic derivative
evaluation program. Communications of the ACM,
7(8):464.
</reference>
<page confidence="0.997825">
637
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.780250">
<title confidence="0.999218">Multilayer Sequence Labeling</title>
<author confidence="0.990039">Ai Azuma Yuji Matsumoto</author>
<affiliation confidence="0.9995205">Graduate School of Information Nara Institute of Science and</affiliation>
<address confidence="0.812636">Ikoma, Nara 630-0192,</address>
<abstract confidence="0.99889137037037">In this paper, we describe a novel approach to cascaded learning and inference on sequences. We propose a weakly joint learning model on cascaded inference on sequences, called multilayer sequence labeling. In this model, inference on sequences is modeled as cascaded decision. However, the decision on a sequence labeling sequel to other decisions utilizes the features on the preceding results as marginalized by the probabilistic models on them. It is not novel itself, but our idea central to this paper is that the probabilistic models on succeeding labeling are viewed as indirectly depending on the probabilistic models on preceding analyses. We also propose two types of efficient dynamic programming which are required in the gradient-based optimization of an objective function. One of the dynamic programming algorithms resembles back propagation algorithm for multilayer feed-forward neural networks. The other is a generalized version of the forwardbackward algorithm. We also report experiments of cascaded part-of-speech tagging and chunking of English sentences and show effectiveness of the proposed method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Berz</author>
</authors>
<title>Automatic differentiation as nonarchimedean analysis.</title>
<date>1992</date>
<booktitle>In Computer Arithmetic and Enclosure,</booktitle>
<pages>439--450</pages>
<contexts>
<context position="26777" citStr="Berz (1992)" startWordPosition="4595" endWordPosition="4596">eginning of a chunk, “Inside-Category”, which represents the inside of a chunk, and “Other.” Usually, POS labeling runs first before base-phrase chunking is performed. Therefore, this task is a typical interesting case where a sequence labeling depends on the output from other sequence labelers. The data used for our experiment consist of English sentences from the Penn Treebank project (Marcus et al., 1993) consisting of 10948 sentences and 259104 words. We divided them into two groups, training data consisting of 8936 sentences and 211727 words and test data consisting of 2012 2For example, Berz (1992) gives a detailed description of the reason why the dual number is used for this purpose. aL2 · a h1 ah1 a01 634 Algorithm 2 Forward-backward Algorithm for Calculating Feature Covariances def def Input: f⟨1,x⟩, ϕe1 ≡ exp(�1 · f⟨1,e1,x⟩), h′ ≡ ∂L2 e1 ∂�h⟨1��1⟩ ·h⟨1,e1⟩ Output: qk1 = CovP(y1|x) I H′ F⟨1,k1,y1,x⟩1 (∀k1 ∈ K1) 1: for ∀v1 ∈ src(G1), αv1 ← 1, α′v1 ← 1 2: for all v1 ∈ V1 in a topological order do 3: prev ← {x ∈ V1 |(x, v1) ∈ E1} 4: αv1 ← E ϕ(x,v1)αx, α′v1 ← E (ϕ(x,v1) h′ (x,v1)α ′ x∈prev x∈prev x + αx) 5: end for 6: Z1 ← E αx x∈snk(G1) 7: for ∀v1 ∈ snk(G1), βv1 ← 1, β′v1 ← 1 8: for al</context>
</contexts>
<marker>Berz, 1992</marker>
<rawString>M. Berz. 1992. Automatic differentiation as nonarchimedean analysis. In Computer Arithmetic and Enclosure, pages 439–450.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Bunescu</author>
</authors>
<title>Learning with probabilistic features for improved pipeline models.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>670--679</pages>
<contexts>
<context position="5368" citStr="Bunescu (2008)" startWordPosition="834" endWordPosition="835">ver, the size of the output space can become quite large in sequence labeling. It effectively forbids explicit enumeration of all possible outputs, so it is required to represent all the labeling possibilities compactly or employ some approximation schemes. Several studies are in this direction. In the method proposed in Finkel et al. (2006), a cascades of sequence predictions is viewed as a Bayesian network, and sample sequences are drawn at each stage according to the output distribution. The samples are then used to estimate the entire distribution of the cascade. In the method proposed in Bunescu (2008), an upper labeler uses the probabilities marginalized on the parts of the output sequences on lower stages as weights for the features. The weighted features are integrated in the model of the labeler on the upper stage. A k-best approach (e.g., (Collins and Duffy, 2002)) and the methods mentioned above are effective to improve the forward information propagation. However, they can never contribute on backward information propagation. To improve the both directions of information propagation, Some studies propose the joint learning of multiple sequence labelers. Sutton et al. (2007) proposes </context>
<context position="6927" citStr="Bunescu (2008)" startWordPosition="1080" endWordPosition="1081">into Bayesian networks with cyclic dependencies, and exact inference is not tractable in such a model in general. Therefore, it requires some approximate inference algorithms in learning or predictions. Moreover, it only considers the cases where labels of an input sequence and all output sequences are regularly aligned. It is not clear how to build a joint labeling model which handles irregular output label sequences like semi-Markov models (Sarawagi and Cohen, 2005). In this paper, we propose a middle ground for a cascade of sequence predictions. The proposed method adopts the basic idea of Bunescu (2008). We first assume that the model on all the sequence labeling stages is probabilistic one. In modeling of an upper stage, a feature is weighted by the marginal probability of the fragment of the outputs from a lower stage. However, this is not novel itself because it is just a paraphrase of Bunescu’s core idea. Our intuition behind the proposed method is as follows. Features integrated in the model on each stage are weighted by the marginal probabilities of the fragments of the outputs on lower stages. So, if the output distributions on lower stages change, the marginal probabilities of any fr</context>
<context position="25323" citStr="Bunescu (2008)" startWordPosition="4355" endWordPosition="4356">operations and the exponential function are generalized to the dual numbers, and the ordinary F-B is also generalized to the dual numbers. The imaginary part of the resulting values is equal to the needed differentiation. 2 Anyway, these two derivations lead to the same algorithm, and the resulting algorithm is shown as Algorithm 2. The final line in the loop of Algorithm 1 can be implemented by various optimization routines and line search algorithms. The time and space complexity to compute the objective and gradient values for given parameter vectors 01, 02 is the same as that for that for Bunescu (2008), up to a constant factor. Because the calculation of the objective function is essentially the same as that for Bunescu (2008), and in gradient computation, the time complexity of Algorithm 1 is the same as that for the ordinary F-B (up to a constant factor), and the proposed optimization procedure is only required to store additional scalar values h′⟨1,e1⟩ on each G1’s arc. 4 Experiment We examined effectiveness of the method proposed in this paper on a real task. The task is to annotate the POS tags and to perform base-phrase chunking on English sentences. Base-phrase chunking is a task to </context>
<context position="28631" citStr="Bunescu (2008)" startWordPosition="4965" endWordPosition="4966">s labeler is a simple CRF and learned by ordinary optimization procedure. One baseline method is the 1-best pipeline method. A simple CRF model is learned for the chunking labeling, on the input sentences and the most likely POS label sequences predicted by the already learned POS labeler. We call this method “CRF + CRF.” The other baseline method has a CRF model for the chunking labeling, which uses the marginalized features offered by the POS labeler. However, the parameters of the POS labeler are fixed in the training of the chunking model. This method corresponds to the method proposed in Bunescu (2008). We call this baseline “CRF + CRF-MF” (“MF” for “marginalized features”). The proposed method is the same as “CRF + CRF-MF”, except that the both labelers are jointly trained by the CRF CRF CRF + CRF + CRF-MF +CRF-BP POS labeling 95.6 (95.6) 95.8 Base-phrase 92.1 92.7 93.1 chunking Table 2: Experimental result (F-measure) procedure described in Section 3. We call this proposed method “CRF + CRF-BP” (“BP” for “back propagation”). In “CRF + CRF-BP,” the objective function for joint learning (10) is not guaranteed to be convex, so optimization procedure is sensible to the initial configuration o</context>
</contexts>
<marker>Bunescu, 2008</marker>
<rawString>R.C. Bunescu. 2008. Learning with probabilistic features for improved pipeline models. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 670–679.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>263--270</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5640" citStr="Collins and Duffy, 2002" startWordPosition="877" endWordPosition="880">ral studies are in this direction. In the method proposed in Finkel et al. (2006), a cascades of sequence predictions is viewed as a Bayesian network, and sample sequences are drawn at each stage according to the output distribution. The samples are then used to estimate the entire distribution of the cascade. In the method proposed in Bunescu (2008), an upper labeler uses the probabilities marginalized on the parts of the output sequences on lower stages as weights for the features. The weighted features are integrated in the model of the labeler on the upper stage. A k-best approach (e.g., (Collins and Duffy, 2002)) and the methods mentioned above are effective to improve the forward information propagation. However, they can never contribute on backward information propagation. To improve the both directions of information propagation, Some studies propose the joint learning of multiple sequence labelers. Sutton et al. (2007) proposes the joint learning method in case where multiple labels are assigned to each time slice of the input sequences. It enables simultaneous learning and estimation of multiple sequence labelings on the same input sequences, where time slices of the outputs of all the out sequ</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>M. Collins and N. Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 263–270. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G F Corliss</author>
<author>C Faure</author>
<author>A Griewank</author>
</authors>
<title>Automatic differentiation of algorithms: from simulation to optimization.</title>
<date>2002</date>
<publisher>Springer Verlag.</publisher>
<marker>Corliss, Faure, Griewank, 2002</marker>
<rawString>G.F. Corliss, C. Faure, and A. Griewank. 2002. Automatic differentiation of algorithms: from simulation to optimization. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>C D Manning</author>
<author>A Y Ng</author>
</authors>
<title>Solving the problem of cascading errors: Approximate bayesian inference for linguistic annotation pipelines.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>618--626</pages>
<contexts>
<context position="5097" citStr="Finkel et al. (2006)" startWordPosition="786" endWordPosition="789"> the more difficult it is to enumerate and maintain the k-best results. It is particularly prominent in sequence labeling. The essence of this orientation is that the labeler on an upper stage utilizes the information of all the possible output candidates on lower stages. However, the size of the output space can become quite large in sequence labeling. It effectively forbids explicit enumeration of all possible outputs, so it is required to represent all the labeling possibilities compactly or employ some approximation schemes. Several studies are in this direction. In the method proposed in Finkel et al. (2006), a cascades of sequence predictions is viewed as a Bayesian network, and sample sequences are drawn at each stage according to the output distribution. The samples are then used to estimate the entire distribution of the cascade. In the method proposed in Bunescu (2008), an upper labeler uses the probabilities marginalized on the parts of the output sequences on lower stages as weights for the features. The weighted features are integrated in the model of the labeler on the upper stage. A k-best approach (e.g., (Collins and Duffy, 2002)) and the methods mentioned above are effective to improv</context>
</contexts>
<marker>Finkel, Manning, Ng, 2006</marker>
<rawString>J.R. Finkel, C.D. Manning, and A.Y. Ng. 2006. Solving the problem of cascading errors: Approximate bayesian inference for linguistic annotation pipelines. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 618– 626.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="2152" citStr="Lafferty et al., 2001" startWordPosition="321" endWordPosition="324">called structured prediction. Sequence labeling is the simplest subclass of structured prediction problems. In sequence labeling, the most likely one among all the possible label sequences is predicted for a given input. Although sequence labeling is the simplest subclass, a lot of real-world tasks are modeled as problems of this simplest subclass. In addition, it might offer valuable insight and a toehold for more general and complex structured prediction problems. Many models have been proposed for sequence labeling tasks, such as Hidden Markov Models (HMM), Conditional Random Fields (CRF) (Lafferty et al., 2001), Max-Margin Markov Networks (Taskar et al., 2003) and others. These models have been applied to lots of practical tasks in natural language processing (NLP), bioinformatics, speech recognition, and so on. And they have shown great success in recent years. In real-world tasks, it is often needed to cascade multiple predictions. A cascade of predictions here means the situation in which some of predictions are made based upon the results of other predictions. Sequence labeling is not an exception. For example, in NLP, we perform named entity recognition or base-phrase chunking for given sentenc</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Li</author>
<author>J Eisner</author>
</authors>
<title>First-and second-order expectation semirings with applications to minimum-risk training on translation forests.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>1</volume>
<pages>40--51</pages>
<contexts>
<context position="23439" citStr="Li and Eisner (2009)" startWordPosition="4018" endWordPosition="4021">,x⟩] (18) defh In other where Hs1 y1⟩ ≡ Ee1∈y1 ∂¯h(1e1) ⟨1&apos;e1⟩ words ∂L2 becomes the covariance between the , ∂θ(1,k1) k1-th input feature for L1 and the hypothetical feature h′ def ≡ ∂L2 h ⟨1,e1⟩ ∂¯h(1 e1) ⟨1,e1⟩. The final problem is to derive an efficient way to compute the first term of (18). The second term of (18) can be calculated by the ordinary F-B because it consists of the marginals of arc features. There are two derivations of the algorithm for calculating the first term. We describe briefly the both derivations. One is a variant of the F-B on the expectation semi-ring proposed in Li and Eisner (2009). First, the F-B is generalized to the expectation semi-ring with respect to the hypothetical feature h′⟨1,e1⟩, and by summing up the marginals of the feature vectors f⟨1,e1,x⟩ on all the arcs under the distribution of the semi-ring, then we obtain the expectation of the feature vector f⟨1,e1,x⟩ on the semi-ring potential. This expectation is equal to the first term of (18). 1 Another derivation is to apply the automatic differentiation (AD)(Wengert, 1964; Corliss et al., ]. It 2002) on the F-B calculating EP1 [F⟨1,y1,x⟩ exploits the fact that ∂∂λ EPi [F⟨1 y1 x⟩]� is λ=0 equal to the first ter</context>
<context position="24664" citStr="Li and Eisner (2009)" startWordPosition="4242" endWordPosition="4245">(18), where A ∈ R is a dummy parameter, and P (y1 Ix� def i Z1 1exp (01 · F⟨1,y1,x⟩ + AH′ . It is easy ⟨1,y1⟩ to derive the F-B for calculating the value EP� [F⟨1,y1,x⟩] ���λ=0. AD transforms this F-B into 1 another algorithm for calculating the differentiation w.r.t. A evaluated at the point A = 0. This transformation is achieved in an automatic manner, by replacing all appearances of A in the F-B with a dual number A + ε. The dual number is a variant of the complex number, with a kind of the imaginary unit ε with the property ε2 = 0. Like the usual complex 1For the detailed description, see Li and Eisner (2009) and its references. numbers, the arithmetic operations and the exponential function are generalized to the dual numbers, and the ordinary F-B is also generalized to the dual numbers. The imaginary part of the resulting values is equal to the needed differentiation. 2 Anyway, these two derivations lead to the same algorithm, and the resulting algorithm is shown as Algorithm 2. The final line in the loop of Algorithm 1 can be implemented by various optimization routines and line search algorithms. The time and space complexity to compute the objective and gradient values for given parameter vec</context>
</contexts>
<marker>Li, Eisner, 2009</marker>
<rawString>Z. Li and J. Eisner. 2009. First-and second-order expectation semirings with applications to minimum-risk training on translation forests. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1, pages 40– 51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>M A Marcinkiewicz</author>
<author>B Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational linguistics,</title>
<date>1993</date>
<contexts>
<context position="26577" citStr="Marcus et al., 1993" startWordPosition="4561" endWordPosition="4564">f words into syntactic categories. This task is performed by annotating a chunking label on each word (Ramshaw and Marcus, 1995). The types of chunking label consist of “Begin-Category”, which represents the beginning of a chunk, “Inside-Category”, which represents the inside of a chunk, and “Other.” Usually, POS labeling runs first before base-phrase chunking is performed. Therefore, this task is a typical interesting case where a sequence labeling depends on the output from other sequence labelers. The data used for our experiment consist of English sentences from the Penn Treebank project (Marcus et al., 1993) consisting of 10948 sentences and 259104 words. We divided them into two groups, training data consisting of 8936 sentences and 211727 words and test data consisting of 2012 2For example, Berz (1992) gives a detailed description of the reason why the dual number is used for this purpose. aL2 · a h1 ah1 a01 634 Algorithm 2 Forward-backward Algorithm for Calculating Feature Covariances def def Input: f⟨1,x⟩, ϕe1 ≡ exp(�1 · f⟨1,e1,x⟩), h′ ≡ ∂L2 e1 ∂�h⟨1��1⟩ ·h⟨1,e1⟩ Output: qk1 = CovP(y1|x) I H′ F⟨1,k1,y1,x⟩1 (∀k1 ∈ K1) 1: for ∀v1 ∈ src(G1), αv1 ← 1, α′v1 ← 1 2: for all v1 ∈ V1 in a topological </context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational linguistics, 19(2):330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L A Ramshaw</author>
<author>M P Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third ACL Workshop on Very Large Corpora,</booktitle>
<pages>82--94</pages>
<location>Cambridge MA, USA.</location>
<contexts>
<context position="26085" citStr="Ramshaw and Marcus, 1995" startWordPosition="4482" endWordPosition="4486">radient computation, the time complexity of Algorithm 1 is the same as that for the ordinary F-B (up to a constant factor), and the proposed optimization procedure is only required to store additional scalar values h′⟨1,e1⟩ on each G1’s arc. 4 Experiment We examined effectiveness of the method proposed in this paper on a real task. The task is to annotate the POS tags and to perform base-phrase chunking on English sentences. Base-phrase chunking is a task to classify continuous subsequences of words into syntactic categories. This task is performed by annotating a chunking label on each word (Ramshaw and Marcus, 1995). The types of chunking label consist of “Begin-Category”, which represents the beginning of a chunk, “Inside-Category”, which represents the inside of a chunk, and “Other.” Usually, POS labeling runs first before base-phrase chunking is performed. Therefore, this task is a typical interesting case where a sequence labeling depends on the output from other sequence labelers. The data used for our experiment consist of English sentences from the Penn Treebank project (Marcus et al., 1993) consisting of 10948 sentences and 259104 words. We divided them into two groups, training data consisting o</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>L.A. Ramshaw and M.P. Marcus. 1995. Text chunking using transformation-based learning. In Proceedings of the Third ACL Workshop on Very Large Corpora, pages 82–94. Cambridge MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sarawagi</author>
<author>W W Cohen</author>
</authors>
<title>Semi-markov conditional random fields for information extraction.</title>
<date>2005</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>17--1185</pages>
<contexts>
<context position="6785" citStr="Sarawagi and Cohen, 2005" startWordPosition="1054" endWordPosition="1057">n the same input sequences, where time slices of the outputs of all the out sequences are regularly aligned. However, it puts the distribution of states into Bayesian networks with cyclic dependencies, and exact inference is not tractable in such a model in general. Therefore, it requires some approximate inference algorithms in learning or predictions. Moreover, it only considers the cases where labels of an input sequence and all output sequences are regularly aligned. It is not clear how to build a joint labeling model which handles irregular output label sequences like semi-Markov models (Sarawagi and Cohen, 2005). In this paper, we propose a middle ground for a cascade of sequence predictions. The proposed method adopts the basic idea of Bunescu (2008). We first assume that the model on all the sequence labeling stages is probabilistic one. In modeling of an upper stage, a feature is weighted by the marginal probability of the fragment of the outputs from a lower stage. However, this is not novel itself because it is just a paraphrase of Bunescu’s core idea. Our intuition behind the proposed method is as follows. Features integrated in the model on each stage are weighted by the marginal probabilities</context>
</contexts>
<marker>Sarawagi, Cohen, 2005</marker>
<rawString>S. Sarawagi and W.W. Cohen. 2005. Semi-markov conditional random fields for information extraction. Advances in Neural Information Processing Systems, 17:1185–1192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sutton</author>
<author>A McCallum</author>
<author>K Rohanimanesh</author>
</authors>
<title>Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data.</title>
<date>2007</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>8--693</pages>
<contexts>
<context position="5958" citStr="Sutton et al. (2007)" startWordPosition="923" endWordPosition="926">hod proposed in Bunescu (2008), an upper labeler uses the probabilities marginalized on the parts of the output sequences on lower stages as weights for the features. The weighted features are integrated in the model of the labeler on the upper stage. A k-best approach (e.g., (Collins and Duffy, 2002)) and the methods mentioned above are effective to improve the forward information propagation. However, they can never contribute on backward information propagation. To improve the both directions of information propagation, Some studies propose the joint learning of multiple sequence labelers. Sutton et al. (2007) proposes the joint learning method in case where multiple labels are assigned to each time slice of the input sequences. It enables simultaneous learning and estimation of multiple sequence labelings on the same input sequences, where time slices of the outputs of all the out sequences are regularly aligned. However, it puts the distribution of states into Bayesian networks with cyclic dependencies, and exact inference is not tractable in such a model in general. Therefore, it requires some approximate inference algorithms in learning or predictions. Moreover, it only considers the cases wher</context>
</contexts>
<marker>Sutton, McCallum, Rohanimanesh, 2007</marker>
<rawString>C. Sutton, A. McCallum, and K. Rohanimanesh. 2007. Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data. The Journal of Machine Learning Research, 8:693–723.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>C Guestrin</author>
<author>D Koller</author>
</authors>
<title>Max-margin Markov networks.</title>
<date>2003</date>
<booktitle>In Advances in Neural Information Processing Systems 16.</booktitle>
<contexts>
<context position="2202" citStr="Taskar et al., 2003" startWordPosition="329" endWordPosition="332">he simplest subclass of structured prediction problems. In sequence labeling, the most likely one among all the possible label sequences is predicted for a given input. Although sequence labeling is the simplest subclass, a lot of real-world tasks are modeled as problems of this simplest subclass. In addition, it might offer valuable insight and a toehold for more general and complex structured prediction problems. Many models have been proposed for sequence labeling tasks, such as Hidden Markov Models (HMM), Conditional Random Fields (CRF) (Lafferty et al., 2001), Max-Margin Markov Networks (Taskar et al., 2003) and others. These models have been applied to lots of practical tasks in natural language processing (NLP), bioinformatics, speech recognition, and so on. And they have shown great success in recent years. In real-world tasks, it is often needed to cascade multiple predictions. A cascade of predictions here means the situation in which some of predictions are made based upon the results of other predictions. Sequence labeling is not an exception. For example, in NLP, we perform named entity recognition or base-phrase chunking for given sentences based on part-of-speech (POS) labels predicted </context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2003</marker>
<rawString>B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin Markov networks. In Advances in Neural Information Processing Systems 16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>RE Wengert</author>
</authors>
<title>A simple automatic derivative evaluation program.</title>
<date>1964</date>
<journal>Communications of the ACM,</journal>
<volume>7</volume>
<issue>8</issue>
<contexts>
<context position="23898" citStr="Wengert, 1964" startWordPosition="4094" endWordPosition="4095">lating the first term. We describe briefly the both derivations. One is a variant of the F-B on the expectation semi-ring proposed in Li and Eisner (2009). First, the F-B is generalized to the expectation semi-ring with respect to the hypothetical feature h′⟨1,e1⟩, and by summing up the marginals of the feature vectors f⟨1,e1,x⟩ on all the arcs under the distribution of the semi-ring, then we obtain the expectation of the feature vector f⟨1,e1,x⟩ on the semi-ring potential. This expectation is equal to the first term of (18). 1 Another derivation is to apply the automatic differentiation (AD)(Wengert, 1964; Corliss et al., ]. It 2002) on the F-B calculating EP1 [F⟨1,y1,x⟩ exploits the fact that ∂∂λ EPi [F⟨1 y1 x⟩]� is λ=0 equal to the first term of (18), where A ∈ R is a dummy parameter, and P (y1 Ix� def i Z1 1exp (01 · F⟨1,y1,x⟩ + AH′ . It is easy ⟨1,y1⟩ to derive the F-B for calculating the value EP� [F⟨1,y1,x⟩] ���λ=0. AD transforms this F-B into 1 another algorithm for calculating the differentiation w.r.t. A evaluated at the point A = 0. This transformation is achieved in an automatic manner, by replacing all appearances of A in the F-B with a dual number A + ε. The dual number is a varia</context>
</contexts>
<marker>Wengert, 1964</marker>
<rawString>RE Wengert. 1964. A simple automatic derivative evaluation program. Communications of the ACM, 7(8):464.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>