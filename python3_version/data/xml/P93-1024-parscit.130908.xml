<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.61210225">
DISTRIBUTIONAL CLUSTERING OF ENGLISH WORDS
Fernando Pereira
AT&amp;T Bell Laboratories
600 Mountain Ave.
</note>
<address confidence="0.753292">
Murray Hill, NJ 07974, USA
</address>
<email confidence="0.773644">
pereiraOresearch.att.com
</email>
<author confidence="0.979068">
Naftali Tishby
</author>
<affiliation confidence="0.843581333333333">
Dept. of Computer Science
Hebrew University
Jerusalem 91904, Israel
</affiliation>
<email confidence="0.691961">
tishby0cs.huji.ac.ii
</email>
<author confidence="0.97467">
Lillian Lee
</author>
<affiliation confidence="0.878604666666667">
Dept. of Computer Science
Cornell University
Ithaca, NY 14850, USA
</affiliation>
<email confidence="0.94057">
lleeOcs.cornell.edu
</email>
<sectionHeader confidence="0.993293" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999804631578948">
We describe and evaluate experimentally a
method for clustering words according to their dis-
tribution in particular syntactic contexts. Words
are represented by the relative frequency distribu-
tions of contexts in which they appear, and rela-
tive entropy between those distributions is used as
the similarity measure for clustering. Clusters are
represented by average context distributions de-
rived from the given words according to their prob-
abilities of cluster membership. In many cases,
the clusters can be thought of as encoding coarse
sense distinctions. Deterministic annealing is used
to find lowest distortion sets of clusters: as the an-
nealing parameter increases, existing clusters be-
come unstable and subdivide, yielding a hierarchi-
cal &amp;quot;soft&amp;quot; clustering of the data. Clusters are used
as the basis for class models of word coocurrence,
and the models evaluated with respect to held-out
test data.
</bodyText>
<sectionHeader confidence="0.997881" genericHeader="introduction">
INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999747983333333">
Methods for automatically classifying words ac-
cording to their contexts of use have both scien-
tific and practical interest. The scientific ques-
tions arise in connection to distributional views
of linguistic (particularly lexical) structure and
also in relation to the question of lexical acqui-
sition both from psychological and computational
learning perspectives. From the practical point
of view, word classification addresses questions of
data sparseness and generalization in statistical
language models, particularly models for deciding
among alternative analyses proposed by a gram-
mar.
It is well known that a simple tabulation of fre-
quencies of certain words participating in certain
configurations, for example of frequencies of pairs
of a transitive main verb and the head noun of its
direct object, cannot be reliably used for compar-
ing the likelihoods of different alternative configu-
rations. The problem is that for large enough cor-
pora the number of possible joint events is much
larger than the number of event occurrences in
the corpus, so many events are seen rarely or
never, making their frequency counts unreliable
estimates of their probabilities.
Hindle (1990) proposed dealing with the
sparseness problem by estimating the likelihood of
unseen events from that of &amp;quot;similar&amp;quot; events that
have been seen. For instance, one may estimate
the likelihood of a particular direct object for a
verb from the likelihoods of that direct object for
similar verbs. This requires a reasonable defini-
tion of verb similarity and a similarity estimation
method. In Hindle&apos;s proposal, words are similar if
we have strong statistical evidence that they tend
to participate in the same events. His notion of
similarity seems to agree with our intuitions in
many cases, but it is not clear how it can be used
directly to construct word classes and correspond-
ing models of association.
Our research addresses some of the same ques-
tions and uses similar raw data, but we investigate
how to factor word association tendencies into as-
sociations of words to certain hidden senses classes
and associations between the classes themselves.
While it may be worth basing such a model on pre-
existing sense classes (Resnik, 1992), in the work
described here we look at how to derive the classes
directly from distributional data. More specifi-
cally, we model senses as probabilistic concepts
or clusters c with corresponding cluster member-
ship probabilities p(clw) for each word w. Most
other class-based modeling techniques for natural
language rely instead on &amp;quot;hard&amp;quot; Boolean classes
(Brown et al., 1990). Class construction is then
combinatorially very demanding and depends on
frequency counts for joint events involving partic-
ular words, a potentially unreliable source of in-
formation as noted above. Our approach avoids
both problems.
</bodyText>
<subsectionHeader confidence="0.955564">
Problem Setting
</subsectionHeader>
<bodyText confidence="0.996525416666667">
In what follows, we will consider two major word
classes, V and N., for the verbs and nouns in our
experiments, and a single relation between them,
in our experiments the relation between a tran-
sitive main verb and the head noun of its direct
object. Our raw knowledge about the relation con-
sists of the frequencies f„ of occurrence of par-
ticular pairs (v, n) in the required configuration
in a training corpus. Some form of text analy-
sis is required to collect such a collection of pairs.
The corpus used in our first experiment was de-
rived from newswire text automatically parsed by
</bodyText>
<page confidence="0.998459">
183
</page>
<bodyText confidence="0.999914366666667">
Hindle&apos;s parser Fidditch (Hindle, 1993). More re-
cently, we have constructed similar tables with the
help of a statistical part-of-speech tagger (Church,
1988) and of tools for regular expression pattern
matching on tagged corpora (Yarowsky, 1992). We
have not yet compared the accuracy and cover-
age of the two methods, or what systematic biases
they might introduce, although we took care to fil-
ter out certain systematic errors, for instance the
misparsing of the subject of a complement clause
as the direct object of a main verb for report verbs
like &amp;quot;say&amp;quot;.
We will consider here only the problem of clas-
sifying nouns according to their distribution as di-
rect objects of verbs; the converse problem is for-
mally similar. More generally, the theoretical ba-
sis for our method supports the use of clustering
to build models for any n-ary relation in terms of
associations between elements in each coordinate
and appropriate hidden units (cluster centroids)
and associations between those hidden units.
For the noun classification problem, the em-
pirical distribution of a noun n is then given by
the conditional distribution p(V) = fvn/ Ev fvn•
The problem we study is how to use the pn to clas-
sify the n E H. Our classification method will con-
struct a set C of clusters and cluster membership
probabilities p(c1n). Each cluster c is associated to
a cluster centroid pc, which is a distribution over
V obtained by averaging appropriately the pn.
</bodyText>
<subsectionHeader confidence="0.890465">
Distributional Similarity
</subsectionHeader>
<bodyText confidence="0.9992186">
To cluster nouns n according to their conditional
verb distributions prz, we need a measure of simi-
larity between distributions. We use for this pur-
pose the relative entropy or Kullback-Leibler (KL)
distance between two distributions
</bodyText>
<equation confidence="0.989604">
D(p II q) = p(x) log P(x)
q(x)
</equation>
<bodyText confidence="0.976729777777778">
This is a natural choice for a variety of reasons,
which we will just sketch here.&apos;
First of all, D(p II q) is zero just when p = q,
and it increases as the probability decreases that
p is the relative frequency distribution of a ran-
dom sample drawn according to q. More formally,
the probability mass given by q to the set of all
samples of length n with relative frequency distri-
bution p is bounded by exp —nD(p II q) (Cover
and Thomas, 1991). Therefore, if we are try-
ing to distinguish among hypotheses qi when p is
the relative frequency distribution of observations,
D(p qi) gives the relative weight of evidence in
favor of qi. Furthermore, a similar relation holds
between D(p p&apos;) for two empirical distributions p
and p&apos; and the probability that p and p&apos; are drawn
from the same distribution q. We can thus use the
relative entropy between the context distributions
for two words to measure how likely they are to
be instances of the same cluster centroid.
&apos;A more formal discussion will appear in our paper
Distributional Clustering, in preparation.
From an information theoretic perspective
D(p q) measures how inefficient on average it
would be to use a code based on q to encode a
variable distributed according to p. With respect
to our problem, D(p n II Pc)
thus gives us the infor-
mation loss in using cluster centroid pc instead of
the actual distribution pn for word n when mod-
eling the distributional properties of n.
Finally, relative entropy is a natural measure
of similarity between distributions for clustering
because its minimization leads to cluster centroids
that are a simple weighted average of member dis-
tributions.
One technical difficulty is that D(p II p&apos;) is
not defined when p&apos; (x) = 0 but p(x) &gt; 0. We
could sidestep this problem (as we did initially) by
smoothing zero frequencies appropriately (Church
and Gale, 1991). However, this is not very sat-
isfactory because one of the goals of our work is
precisely to avoid the problems of data sparseness
by grouping words into classes. It turns out that
the problem is avoided by our clustering technique,
since it does not need to compute the KL distance
between individual word distributions, but only
between a word distribution and average distri-
butions, the current cluster centroids, which are
guaranteed to be nonzero whenever the word dis-
tributions are. This is a useful advantage of our
method compared with agglomerative clustering
techniques that need to compare individual ob-
jects being considered for grouping.
</bodyText>
<sectionHeader confidence="0.933152" genericHeader="method">
THEORETICAL BASIS
</sectionHeader>
<bodyText confidence="0.999927875">
In general, we are interested in how to organize
a set of linguistic objects such as words according
to the contexts in which they occur, for instance
grammatical constructions or n-grams. We will
show elsewhere that the theoretical analysis out-
lined here applies to that more general problem,
but for now we will only address the more specific
problem in which the objects are nouns and the
contexts are verbs that take the nouns as direct
objects.
Our problem can be seen as that of learning a
joint distribution of pairs from a large sample of
pairs. The pair coordinates come from two large
sets H and V, with no preexisting internal struc-
ture, and the training data is a sequence S of N
independently drawn pairs
</bodyText>
<equation confidence="0.691591">
Si = (ni, vi) 1 &lt; i &lt; N .
</equation>
<bodyText confidence="0.99983125">
From a learning perspective, this problem falls
somewhere in between unsupervised and super-
vised learning. As in unsupervised learning, the
goal is to learn the underlying distribution of the
data. But in contrast to most unsupervised learn-
ing settings, the objects involved have no internal
structure or attributes allowing them to be com-
pared with each other. Instead, the only informa-
tion about the objects is the statistics of their joint
appearance. These statistics can thus be seen as a
weak form of object labelling analogous to super-
vision.
</bodyText>
<page confidence="0.993408">
184
</page>
<subsectionHeader confidence="0.80823">
Distributional Clustering
</subsectionHeader>
<bodyText confidence="0.999888">
While clusters based on distributional similarity
are interesting on their own, they can also be prof-
itably seen as a means of summarizing a joint dis-
tribution. In particular, we would like to find a
set of clusters C such that each conditional dis-
tribution p0(v) can be approximately decomposed
as
</bodyText>
<equation confidence="0.988517">
j3(v) = E p( c I n)pc( v ) ,
cEC
</equation>
<bodyText confidence="0.9990768">
where p(c1n) is the membership probability of n in
c and pc(v) = p(vIc) is v&apos;s conditional probability
given by the centroid distribution for cluster c.
The above decomposition can be written in a
more symmetric form as
</bodyText>
<equation confidence="0.99301925">
P(n, v) = n)P( v I c)
cEC
= E p(c)p(nic)p(vic) (1)
cEC
</equation>
<bodyText confidence="0.999929125">
assuming that p(n) and P(n) coincide. We will
take (1) as our basic clustering model.
To determine this decomposition we need to
solve the two connected problems of finding suit-
able forms for the cluster membership p(cln) and
the centroid distributions p(v lc), and of maximiz-
ing the goodness of fit between the model distri-
bution An, v) and the observed data.
Goodness of fit is determined by the model&apos;s
likelihood of the observations. The maximum like-
lihood (ML) estimation principle is thus the nat-
ural tool to determine the centroid distributions
Pc (v).
As for the membership probabilities, they
must be determined solely by the relevant mea-
sure of object-to-cluster similarity, which in the
present work is the relative entropy between ob-
ject and cluster centroid distributions. Since no
other information is available, the membership is
determined by maximizing the configuration en-
tropy for a fixed average distortion. With the max-
imum entropy (ME) membership distribution, ML
estimation is equivalent to the minimization of the
average distortion of the data. The combined en-
tropy maximization entropy and distortion min-
imization is carried out by a two-stage iterative
process similar to the EM method (Dempster et
al., 1977). The first stage of an iteration is a max-
imum likelihood, or minimum distortion, estima-
tion of the cluster centroids given fixed member-
ship probabilities. In the second stage of each iter-
ation, the entropy of the membership distribution
is maximized for a fixed average distortion. This
joint optimization searches for a saddle point in
the distortion-entropy parameters, which is equiv-
alent to minimizing a linear combination of the
two known as free energy in statistical mechanics.
This analogy with statistical mechanics is not co-
incidental, and provides a better understanding of
the clustering procedure.
</bodyText>
<subsectionHeader confidence="0.4347515">
Maximum Likelihood Cluster
Centroids
</subsectionHeader>
<bodyText confidence="0.9997315">
For the maximum likelihood argument, we start by
estimating the likelihood of the sequence S of N
independent observations of pairs (ni, vi). Using
(1), the sequence&apos;s model log likelihood is
</bodyText>
<equation confidence="0.9996755">
i(S) = E log E p(c)p(nilc)p(vilc) .
i=1 cEC
</equation>
<bodyText confidence="0.9997065">
Fixing the number of clusters (model size) ICI, we
want to maximize /(S) with respect to the distri-
butions p(nIc) and p(vIc). The variation of l(S)
with respect to these distributions is
</bodyText>
<equation confidence="0.985707">
P(VilCOP(ni1C)
bl(S).E p(c)( + ) (2)
uz) cEc p(ni Ic)(5p(vi Ic)
</equation>
<bodyText confidence="0.892788">
with p(nIc) and p(vIc) kept normalized. Using
Bayes&apos;s formula, we have
</bodyText>
<equation confidence="0.974004142857143">
1p(clni, vi)
/5(ni vi) = P(c)P(ni le)P(vi lc) (3)
for any c. 2 Substituting (3) into (2), we obtain
(45 log p(ni lc) )
61(S) = E E p( clni, vi) + (4)
cEc
b log p(vi lc)
</equation>
<bodyText confidence="0.9887755">
since b logp = 6p/p. This expression is particu-
larly useful when the cluster distributions p(nic)
and p(vIc) have an exponential form, precisely
what will be provided by the ME step described
below.
At this point we need to specify the cluster-
ing model in more detail. In the derivation so far
we have treated P(nle) and p(vIc) symmetrically,
corresponding to clusters not of verbs or nouns
but of verb-noun associations. In principle such
a symmetric model may be more accurate, but in
this paper we will concentrate on asymmetric mod-
els in which cluster memberships are associated to
just one of the components of the joint distribution
and the cluster centroids are specified only by the
other component. In particular, the model we use
in our experiments has noun clusters with cluster
memberships determined by p(n1c) and centroid
distributions determined by p(vIc).
The asymmetric model simplifies the estima-
tion significantly by dealing with a single compo-
nent, but it has the disadvantage that the joint
distribution, p(n, v) has two different and not nec-
essarily consistent expressions in terms of asym-
metric models for the two coordinates.
2As usual in clustering models (Duda and Hart,
1973), we assume that the model distribution and the
empirical distribution are interchangeable at the solu-
tion of the parameter estimation equations, since the
model is assumed to be able to represent correctly the
data at that solution point. In practice, the data may
not come exactly from the chosen model class, but the
model obtained by solving the estimation equations
may still be the closest one to the data.
</bodyText>
<page confidence="0.995911">
185
</page>
<sectionHeader confidence="0.371366" genericHeader="method">
Maximum Entropy Cluster Membership
</sectionHeader>
<bodyText confidence="0.998875125">
While variations of p(nic) and p(vic) in equation
(4) are not independent, we can treat them sep-
arately. First, for fixed average distortion be-
tween the cluster centroid distributions p(i)lc) and
the data p(vin), we find the cluster membership
probabilities, which are the Bayes inverses of the
p(n1c), that maximize the entropy of the cluster
distributions. With the membership distributions
thus obtained, we then look for the p(vic) that
maximize the log likelihood /(S). It turns out
that this will also be the values of p(v lc) that mini-
mize the average distortion between the asymmet-
ric cluster model and the data.
Given any similarity measure d(n, c) between
nouns and cluster centroids, the average cluster
distortion is
</bodyText>
<equation confidence="0.928089">
(D) = E Ep(c1n)d(n,c)
nEg cEC
If we maximize the cluster membership entropy
H = — E Ep(c1n) logp(n1c) (6)
nEg cEC
</equation>
<bodyText confidence="0.9977325">
subject to normalization of p(n lc) and fixed (5), we
obtain the following standard exponential forms
(Jaynes, 1983) for the class and membership dis-
tributions
</bodyText>
<equation confidence="0.999088333333333">
1
p(nic) = exp —13c/(n, c)
P(c1n) = exp — Pd(n, c) (8)
</equation>
<bodyText confidence="0.999977272727273">
where the normalization sums (partition func-
tions) are Zc = En exp — fld(n, c) and Zr, =
Ec exp —)3c/(n, c). Notice that d(n, c) does not
need to be symmetric for this derivation, as the
two distributions are simply related by Bayes&apos;s
rule.
Returning to the log-likelihood variation (4),
we can now use (7) for p(n1c) and the assumption
for the asymmetric model that the cluster mem-
bership stays fixed as we adjust the centroids, to
obtain
</bodyText>
<equation confidence="0.971028">
bl(S) = — EE p(clni)b Pd(ni , + 6 log Zc (9)
cEC
</equation>
<bodyText confidence="0.9992296">
where the variation of p(vic) is now included in
the variation of d(n, c).
For a large enough sample, we may replace the
sum over observations in (9) by the average over
Af.
</bodyText>
<equation confidence="0.991628857142857">
61(S) = — E p(n) Ep(cln)4513d(n, c) + 6 log Z
nEN cEC
which, applying Bayes&apos;s rule, becomes
1
61(S) = E E p(n1c)613d(n, c) + 6 log Z.
.
cEC P‘c) TIEN
</equation>
<bodyText confidence="0.997771285714286">
At the log-likelihood maximum, this variation
must vanish. We will see below that the use of rel-
ative entropy for similarity measure makes 6 log Zc
vanish at the maximum as well, so the log likeli-
hood can be maximized by minimizing the average
distortion with respect to the class centroids while
class membership is kept fixed
</bodyText>
<equation confidence="0.971829333333333">
1
p--R- 7.,Arp(n1c)6d(n, c) = 0 ,
cEC
or, sufficiently, if each of the inner sums vanish
E E p(n1c)6d(n, c) = 0 (10)
cEC nEAr
</equation>
<bodyText confidence="0.96122825">
Minimizing the Average KL Distortion We
first show that the minimization of the relative
entropy yields the natural expression for cluster
centroids
</bodyText>
<equation confidence="0.925343">
p(tdc) = E p(n1c)p(vin) (11)
nEg
</equation>
<bodyText confidence="0.98303">
To minimize the average distortion (10), we ob-
serve that the variation of the KL distance be-
tween noun and centroid distributions with re-
spect to the centroid distribution p(v lc), with each
centroid distribution normalized by the Lagrange
multiplier Ac, is given by
</bodyText>
<equation confidence="0.993636714285714">
EvEV P(vO) logp(vic)
d(n , c) =
c(Ev Ev P(v1e) — 1)
E *yin)
+ Ac)*
bp() •
Ade)
</equation>
<bodyText confidence="0.985073222222222">
Substituting this expression into (10), we obtain
c n v
Since the bp(vic) are now independent, we obtain
immediately the desired centroid expression (11),
which is the desired weighted average of noun dis-
tributions.
We can now see that the variation 6 log Z, van-
ishes for centroid distributions given by (11), since
it follows from (10) that
</bodyText>
<equation confidence="0.999359666666667">
log Ze = — Eexp—i3d(n,c)(5d(n,c)
Z n
= — fl Ep(n1c)bd(x,c) = 0 .
</equation>
<bodyText confidence="0.989795">
The Free Energy Function The combined
minimum distortion and maximum entropy opti-
mization is equivalent to the minimization of a sin-
gle function, the free energy
</bodyText>
<equation confidence="0.99772525">
1
F = — E log Zn
n
= (D) — H 1 ,
</equation>
<bodyText confidence="0.734523">
where (D) is the average distortion (5) and H is
the cluster membership entropy (6).
</bodyText>
<equation confidence="0.75270325">
(5)
(7)
EEE p(vin)p(nic) c) bp(*) = 0 .
p(vic)
</equation>
<page confidence="0.98851">
186
</page>
<bodyText confidence="0.993875">
The free energy determines both the distor-
tion and the membership entropy through root
</bodyText>
<equation confidence="0.95718225">
(D) = ooF /
H = ap
oF
OT
</equation>
<bodyText confidence="0.984691875">
where T = 0-1 is the temperature.
The most important property of the free en-
ergy is that its minimum determines the balance
between the &amp;quot;disordering&amp;quot; maximum entropy and
&amp;quot;ordering&amp;quot; distortion minimization in which the
system is most likely to be found. In fact the prob-
ability to find the system at a given configuration
is exponential in F
</bodyText>
<equation confidence="0.800407">
P cx exp —
</equation>
<bodyText confidence="0.982628">
so a system is most likely to be found in its mini-
mal free energy configuration.
</bodyText>
<subsectionHeader confidence="0.985233">
Hierarchical Clustering
</subsectionHeader>
<bodyText confidence="0.999346523809524">
The analogy with statistical mechanics suggests
a deterministic annealing procedure for clustering
(Rose et al., 1990), in which the number of clusters
is determined through a sequence of phase transi-
tions by continuously increasing the parameter 0
following an annealing schedule.
The higher is 0, the more local is the influence
of each noun on the definition of centroids. Dis-
tributional similarity plays here the role of distor-
tion. When the scale parameter 0 is close to zero,
the similarity is almost irrelevant. All words con-
tribute about equally to each centroid, and so the
lowest average distortion solution involves just one
cluster whose centroid is the average of all word
distributions. As 0 is slowly increased, a critical
point is eventually reached for which the lowest
F solution involves two distinct centroids. We say
then that the original cluster has split into the two
new clusters.
In general, if we take any cluster c and a twin
c&apos; of c such that the centroid pc, is a small ran-
dom perturbation of pc, below the critical 0 at
which c splits the membership and centroid reesti-
mation procedure given by equations (8) and (11)
will make pc and pc, converge, that is, c and c&apos;
are really the same cluster. But with p above the
critical value for c, the two centroids will diverge,
giving rise to two daughters of c.
Our clustering procedure is thus as follows.
We start with very low 0 and a single cluster
whose centroid is the average of all noun distri-
butions. For any given 0, we have a current set of
leaf clusters corresponding to the current free en-
ergy (local) minimum. To refine such a solution,
we search for the lowest which is the critical
value for some current leaf cluster splits. Ideally,
there is just one split at that critical value, but
for practical performance and numerical accuracy
reasons we may have several splits at the new crit-
ical point. The splitting procedure can then be
repeated to achieve the desired number of clusters
or model cross-entropy.
</bodyText>
<table confidence="0.982617777777778">
missile 0.835 officer 0.484
rocket 0.850 aide 0.612
bullet 0.917 chief 0.649
gun 0.940 manager 0.651
3 4
gun 0.758 shot 0.858
missile 0.786 bullet 0.925
weapon 0.862 rocket 0.930
rocket 0.875 missile 1.037
</table>
<figureCaption confidence="0.979138">
Figure 1: Direct object clusters for fire
</figureCaption>
<sectionHeader confidence="0.883027" genericHeader="method">
CLUSTERING EXAMPLES
</sectionHeader>
<bodyText confidence="0.998060571428572">
All our experiments involve the asymmetric model
described in the previous section. As explained
there, our clustering procedure yields for each
value of 0 a set Cp of clusters minimizing the free
energy F, and the asymmetric model for esti-
mates the conditional verb distribution for a noun
n by
</bodyText>
<equation confidence="0.890122">
15. = E P(c1n)p,
cEcp
</equation>
<bodyText confidence="0.999137192307692">
where p(c1n) also depends on 0.
As a first experiment, we used our method to
classify the 64 nouns appearing most frequently
as heads of direct objects of the verb &amp;quot;fire&amp;quot; in one
year (1988) of Associated Press newswire. In this
corpus, the chosen nouns appear as direct object
heads of a total of 2147 distinct verbs, so each
noun is represented by a density over the 2147
verbs.
Figure 1 shows the four words most similar to
each cluster centroid, and the corresponding word-
centroid KL distances, for the four clusters result-
ing from the first two cluster splits. It can be seen
that first split separates the objects corresponding
to the weaponry sense of &amp;quot;fire&amp;quot; (cluster 1) from the
ones corresponding to the personnel action (clus-
ter 2). The second split then further refines the
weaponry sense into a projectile sense (cluster 3)
and a gun sense (cluster 4). That split is some-
what less sharp, possibly because not enough dis-
tinguishing contexts occur in the corpus.
Figure 2 shows the four closest nouns to the
centroid of each of a set of hierarchical clus-
ters derived from verb-object pairs involving the
1000 most frequent nouns in the June 1991 elec-
tronic version of Grolier&apos;s Encyclopedia (10 mil-
</bodyText>
<page confidence="0.982183">
187
</page>
<figure confidence="0.9158464">
ecognition
cclaim
enown
omination
0.874
1.026
1.079
1.104
korm
explanation
care
control
1.201
1.317
1.363
1.366
-71.410
1.255
1.291
1.295
</figure>
<table confidence="0.952609393939394">
control
recognition
nomination
support
voyage 0.86
trip 0.972
grant 1.392 improvement 1.329 rogress 1.016
distinction 1.554 voyage 1.338 improvement 1.114
form 1.571 migration 1.428 rograrn 1.459
representation 1.577 progress 1.441 peration 1.478
tudy 1.480
nvestigation 1.481
conductor 0.457
vice-president 0.474
director 0.489
chairman 0.500
conductor 0.699
state 1.279
vice-president 0.756 people 1.417
editor 0.814 modern 1.418
director 0.825 farmer 1.425
state 1.320W residence 1.082 complex
ally 1.458 state 1.102 network
residence 1.473 conductor 1.213 community
1.534 teacher 1.233 group
material
0.999 number 1.026 salt
material 1.361 material 1.093 ring
variety 1.401 mass 1.252 number
mass 1.422 variety 1.278 number
comedy
essay
piece
</table>
<figure confidence="0.982277425">
navy
community
&amp;quot;tetwork
..ccomplex
omplex
network
lake
egion
ssay
omedy
oern
eatise
1.096
1.099
1.244
1.259
1.097
1.211
1.360
1.435
0.695
0.800
0.829
0.850
1.161
1.175
1.276
1.327
0.976
1.217
1.244
1.250
1.047
1.060
1.142
1.198
1.120
ariety 1.217
material 1.275
cluster 1.311
</figure>
<table confidence="0.936905941176471">
structure 1.371
elationship 1.460
umber 1.429 change 1.561 aspect 1.492
diversity 1.537 failure 1.562 system 1.497
structure 1.577 variation l.592_ pollution 1.187
concentration 1.582 structure 1.592 &apos;allure 1.290
increase 1.328
infection 1.432
speed 1.177
number 1.461
level 1.315 concentration 1.478
velocity 1.371 strength 1.488
size 1.440 ratio 1.488
peed 1.13(1
zenith 1.214
depth 1.244
velocity 1.253
</table>
<figureCaption confidence="0.834104">
Figure 2: Noun Clusters for Grolier&apos;s Encyclopedia
</figureCaption>
<page confidence="0.844252">
188
</page>
<figure confidence="0.998223529411765">
5
4
exceptional
o—o all
----- --------------- ------------
train
.test
________ -0- -----------------
0.8
8 0.6
.5
04
0.2
100 200 300 400
number of clusters
100 200 300 400
number of dusters
</figure>
<figureCaption confidence="0.962227">
Figure 3: Asymmetric Model Evaluation, AP88
</figureCaption>
<figure confidence="0.558846">
• Verb-Direct Object Pairs
lion words).
</figure>
<sectionHeader confidence="0.967402" genericHeader="method">
MODEL EVALUATION
</sectionHeader>
<bodyText confidence="0.999835043478261">
The preceding qualitative discussion provides
some indication of what aspects of distributional
relationships may be discovered by clustering.
However, we also need to evaluate clustering more
rigorously as a basis for models of distributional
relationships. So, far, we have looked at two kinds
of measurements of model quality: (i) relative en-
tropy between held-out data and the asymmetric
model, and (ii) performance on the task of decid-
ing which of two verbs is more likely to take a given
noun as direct object when the data relating one
of the verbs to the noun has been withheld from
the training data.
The evaluation described below was per-
formed on the largest data set we have worked
with so far, extracted from 44 million words of
1988 Associated Press newswire with the pattern
matching techniques mentioned earlier. This col-
lection process yielded 1112041 verb-object pairs.
We selected then the subset involving the 1000
most frequent nouns in the corpus for clustering,
and randomly divided it into a training set of
756721 pairs and a test set of 81240 pairs.
</bodyText>
<sectionHeader confidence="0.525893" genericHeader="method">
Relative Entropy
</sectionHeader>
<bodyText confidence="0.885688444444444">
Figure 3 plots the unweighted average relative en-
tropy, in bits, of several test sets to asymmet-
D(4,112571),
ric clustered models of different sizes, given by
W&amp;quot;,&apos;T L_anEArs where Aft is the set of di-
rect objects in the test set and tn is the relative
frequency distribution of verbs taking n as direct
object in the test set. 3 For each critical value
of we show the relative entropy with respect to
</bodyText>
<footnote confidence="0.639376">
3We use unweighted averages because we are inter-
ested her on how well the noun distributions are ap-
proximated by the cluster model. If we were interested
on the total information loss of using the asymmetric
model to encode a test corpus, we would instead use
</footnote>
<figureCaption confidence="0.9644645">
Figure 4: Pairwise Verb Comparisons, AP88 Verb-
Direct Object Pairs
</figureCaption>
<bodyText confidence="0.952210833333333">
the asymmetric model based on Cp of the train-
ing set (set train), of randomly selected held-out
test set (set iesi), and of held-out data for a fur-
ther 1000 nouns that were not clustered (set new).
Unsurprisingly, the training set relative entropy
decreases monotonically. The test set relative en-
tropy decreases to a minimum at 206 clusters, and
then starts increasing, suggesting that larger mod-
els are overtrained.
The new noun test set is intended to test
whether clusters based on the 1000 most frequent
nouns are useful classifiers for the selectional prop-
erties of nouns in general. Since the nouns in the
test set pairs do not occur in the training set, we
do not have their cluster membership probabilities
that are needed in the asymmetric model. Instead,
for each noun n in the test set, we classify it with
respect to the clusters by setting
</bodyText>
<equation confidence="0.814979">
p(eln) = exp —0D(pnlic)/Zn
</equation>
<bodyText confidence="0.999952555555556">
where p„ is the empirical conditional verb distri-
bution for n given by the test set. These cluster
membership estimates were then used in the asym-
metric model and the test set relative entropy cal-
culated as before. As the figure shows, the cluster
model provides over one bit of information about
the selectional properties of the new nouns, but
the overtraining effect is even sharper than for the
held-out data involving the 1000 clustered nouns.
</bodyText>
<sectionHeader confidence="0.743967" genericHeader="method">
Decision Task
</sectionHeader>
<bodyText confidence="0.9658414">
We also evaluated asymmetric cluster models on
a verb decision task closer to possible applications
to disambiguation in language analysis. The task
consists judging which of two verbs v and v&apos; is
more likely to take a given noun n as object, when
all occurrences of (v, n) in the training set were
deliberately deleted. Thus this test evaluates how
well the models reconstruct missing data in the
the weighted average Eneiv., fr,D(11/3) where f, is
the relative frequency of n in the test set.
</bodyText>
<page confidence="0.996979">
189
</page>
<bodyText confidence="0.9969253">
verb distribution for n from the cluster centroids
close to n.
The data for this test was built from the train-
ing data for the previous one in the following way,
based on a suggestion by Dagan et al. (1993). 104
noun-verb pairs with a fairly frequent verb (be-
tween 500 and 5000 occurrences) were randomly
picked, and all occurrences of each pair in the
training set were deleted. The resulting training
set was used to build a sequence of cluster models
as before. Each model was used to decide which of
two verbs v and v&apos; are more likely to appear with
a noun n where the (v, n) data was deleted from
the training set, and the decisions were compared
with the corresponding ones derived from the orig-
inal event frequencies in the initial data set. The
error rate for each model is simply the proportion
of disagreements for the selected (v, n, v&apos;) triples.
Figure 4 shows the error rates for each model for
all the selected (v, n, v&apos;) (all) and for just those
exceptional triples in which the conditional ratio
p(n, v) p(n, v&apos;) is on the opposite side of 1 from
the marginal ratio p(v)I p(v&apos;). In other words, the
exceptional cases are those in which predictions
based just on the marginal frequencies, which the
initial one-cluster model represents, would be con-
sistently wrong.
Here too we see some overtraining for the
largest models considered, although not for the ex-
ceptional verbs.
</bodyText>
<sectionHeader confidence="0.991753" genericHeader="conclusions">
CONCLUSIONS
</sectionHeader>
<bodyText confidence="0.999949434782609">
We have demonstrated that a general divisive clus-
tering procedure for probability distributions can
be used to group words according to their partic-
ipation in particular grammatical relations with
other words. The resulting clusters are intuitively
informative, and can be used to construct class-
based word coocurrence models with substantial
predictive power.
While the clusters derived by the proposed
method seem in many cases semantically signif-
icant, this intuition needs to be grounded in a
more rigorous assessment. In addition to predic-
tive power evaluations of the kind we have al-
ready carried out, it might be worth comparing
automatically-derived clusters with human judge-
ments in a suitable experimental setting.
Moving further in the direction of class-based
language models, we plan to consider additional
distributional relations (for instance, adjective-
noun) and apply the results of clustering to
the grouping of lexical associations in lexicalized
grammar frameworks such as stochastic lexicalized
tree-adjoining grammars (Schabes, 1992).
</bodyText>
<sectionHeader confidence="0.998752" genericHeader="acknowledgments">
ACKNOWLEDGMENTS
</sectionHeader>
<bodyText confidence="0.623188125">
We would like to thank Don Hindle for making
available the 1988 Associated Press verb-object
data set, the Fidditch parser and a verb-object
structure filter, Mats Rooth for selecting the ob-
jects of &amp;quot;fire&amp;quot; data set and many discussions,
David Yarowsky for help with his stemming and
concordancing tools, and Ido Dagan for suggesting
ways of testing cluster models.
</bodyText>
<sectionHeader confidence="0.739083" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999909766666667">
Peter F. Brown, Vincent J. Della Pietra, Peter V. deS-
ouza, Jenifer C. Lai, and Robert L. Mercer. 1990.
Class-based n-gram models of natural language.
In Proceedings of the IBM Natural Language ITL,
pages 283-298, Paris, France, March.
Kenneth W. Church and William A. Gale. 1991.
A comparison of the enhanced Good-Turing and
deleted estimation methods for estimating proba-
bilities of English bigrams. Computer Speech and
Language, 5:19-54.
Kenneth W. Church. 1988. A stochastic parts pro-
gram and noun phrase parser for unrestricted
text. In Proceedings of the Second Conference
on Applied Natural Language Processing, pages
136-143, Austin, Texas. Association for Compu-
tational Linguistics, Morristown, New Jersey.
Thomas M. Cover and Joy A. Thomas. 1991. Ele-
ments of Information Theory. Wiley-Interscience,
New York, New York.
Ido Dagan, Shaul Markus, and Shaul Markovitch.
1993. Contextual word similarity and estimation
from sparse data. In these proceedings.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical
Society, Series B, 39(1):1-38.
Richard 0. Duda and Peter E. Hart. 1973. Pat-
tern Classification and Scene Analysis. Wiley-
Interscience, New York, New York.
Donald Hindle. 1990. Noun classification from
predicate-argument structures. In 28th Annual
Meeting of the Association for Computational
Linguistics, pages 268-275, Pittsburgh, Pennsyl-
vania. Association for Computational Linguistics,
Morristown, New Jersey.
Donald Hindle. 1993. A parser for text corpora. In
B.T.S. Atkins and A. Zampoli, editors, Computa-
tional Approaches to the Lexicon. Oxford Univer-
sity Press, Oxford, England. To appear.
Edwin T. Jaynes. 1983. Brandeis lectures. In
Roger D. Rosenkrantz, editor, E. T. Jaynes:
Papers on Probability, Statistics and Statistical
Physics, number 158 in Synthese Library, chap-
ter 4, pages 40-76. D. Reidel, Dordrecht, Holland.
Philip Resnik. 1992. WordNet and distributional
analysis: A class-based approach to lexical dis-
covery. In AAAI Workshop on Statistically-
Based Natural-Language-Processing Techniques,
San Jose, California, July.
Kenneth Rose, Eitan Gurewitz, and Geoffrey C. Fox.
1990. Statistical mechanics and phase transitions
in clustering. Physical Review Letters, 65(8):945-
948.
Yves Schabes. 1992. Stochastic lexicalized tree-
adjoining grammars. In Proceeedings of the 14th
International Conference on Computational Lin-
guistics, Nantes, France.
David Yarowsky. 1992. CONC: Tools for text corpora.
Technical Memorandum 11222-921222-29, AT&amp;T
Bell Laboratories.
</reference>
<page confidence="0.997868">
190
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.911258">
<title confidence="0.998965">DISTRIBUTIONAL CLUSTERING OF ENGLISH WORDS</title>
<author confidence="0.999897">Fernando Pereira</author>
<affiliation confidence="0.999854">AT&amp;T Bell Laboratories</affiliation>
<address confidence="0.9997995">600 Mountain Ave. Murray Hill, NJ 07974, USA</address>
<email confidence="0.999555">pereiraOresearch.att.com</email>
<author confidence="0.995468">Naftali Tishby</author>
<affiliation confidence="0.9998755">Dept. of Computer Science Hebrew University</affiliation>
<address confidence="0.999669">Jerusalem 91904, Israel</address>
<email confidence="0.93749">tishby0cs.huji.ac.ii</email>
<author confidence="0.997285">Lillian Lee</author>
<affiliation confidence="0.9997465">Dept. of Computer Science Cornell University</affiliation>
<address confidence="0.999965">Ithaca, NY 14850, USA</address>
<email confidence="0.999869">lleeOcs.cornell.edu</email>
<abstract confidence="0.99910135">We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical &amp;quot;soft&amp;quot; clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V deSouza</author>
<author>Jenifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1990</date>
<booktitle>In Proceedings of the IBM Natural Language ITL,</booktitle>
<pages>283--298</pages>
<location>Paris, France,</location>
<contexts>
<context position="3877" citStr="Brown et al., 1990" startWordPosition="590" endWordPosition="593">stigate how to factor word association tendencies into associations of words to certain hidden senses classes and associations between the classes themselves. While it may be worth basing such a model on preexisting sense classes (Resnik, 1992), in the work described here we look at how to derive the classes directly from distributional data. More specifically, we model senses as probabilistic concepts or clusters c with corresponding cluster membership probabilities p(clw) for each word w. Most other class-based modeling techniques for natural language rely instead on &amp;quot;hard&amp;quot; Boolean classes (Brown et al., 1990). Class construction is then combinatorially very demanding and depends on frequency counts for joint events involving particular words, a potentially unreliable source of information as noted above. Our approach avoids both problems. Problem Setting In what follows, we will consider two major word classes, V and N., for the verbs and nouns in our experiments, and a single relation between them, in our experiments the relation between a transitive main verb and the head noun of its direct object. Our raw knowledge about the relation consists of the frequencies f„ of occurrence of particular pa</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1990</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jenifer C. Lai, and Robert L. Mercer. 1990. Class-based n-gram models of natural language. In Proceedings of the IBM Natural Language ITL, pages 283-298, Paris, France, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
<author>William A Gale</author>
</authors>
<title>A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams.</title>
<date>1991</date>
<journal>Computer Speech and Language,</journal>
<pages>5--19</pages>
<contexts>
<context position="8306" citStr="Church and Gale, 1991" startWordPosition="1344" endWordPosition="1347">th respect to our problem, D(p n II Pc) thus gives us the information loss in using cluster centroid pc instead of the actual distribution pn for word n when modeling the distributional properties of n. Finally, relative entropy is a natural measure of similarity between distributions for clustering because its minimization leads to cluster centroids that are a simple weighted average of member distributions. One technical difficulty is that D(p II p&apos;) is not defined when p&apos; (x) = 0 but p(x) &gt; 0. We could sidestep this problem (as we did initially) by smoothing zero frequencies appropriately (Church and Gale, 1991). However, this is not very satisfactory because one of the goals of our work is precisely to avoid the problems of data sparseness by grouping words into classes. It turns out that the problem is avoided by our clustering technique, since it does not need to compute the KL distance between individual word distributions, but only between a word distribution and average distributions, the current cluster centroids, which are guaranteed to be nonzero whenever the word distributions are. This is a useful advantage of our method compared with agglomerative clustering techniques that need to compar</context>
</contexts>
<marker>Church, Gale, 1991</marker>
<rawString>Kenneth W. Church and William A. Gale. 1991. A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams. Computer Speech and Language, 5:19-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.</title>
<date>1988</date>
<booktitle>In Proceedings of the Second Conference on Applied Natural Language Processing,</booktitle>
<pages>136--143</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<location>Austin, Texas.</location>
<contexts>
<context position="4875" citStr="Church, 1988" startWordPosition="757" endWordPosition="758">between them, in our experiments the relation between a transitive main verb and the head noun of its direct object. Our raw knowledge about the relation consists of the frequencies f„ of occurrence of particular pairs (v, n) in the required configuration in a training corpus. Some form of text analysis is required to collect such a collection of pairs. The corpus used in our first experiment was derived from newswire text automatically parsed by 183 Hindle&apos;s parser Fidditch (Hindle, 1993). More recently, we have constructed similar tables with the help of a statistical part-of-speech tagger (Church, 1988) and of tools for regular expression pattern matching on tagged corpora (Yarowsky, 1992). We have not yet compared the accuracy and coverage of the two methods, or what systematic biases they might introduce, although we took care to filter out certain systematic errors, for instance the misparsing of the subject of a complement clause as the direct object of a main verb for report verbs like &amp;quot;say&amp;quot;. We will consider here only the problem of classifying nouns according to their distribution as direct objects of verbs; the converse problem is formally similar. More generally, the theoretical bas</context>
</contexts>
<marker>Church, 1988</marker>
<rawString>Kenneth W. Church. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Proceedings of the Second Conference on Applied Natural Language Processing, pages 136-143, Austin, Texas. Association for Computational Linguistics, Morristown, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas M Cover</author>
<author>Joy A Thomas</author>
</authors>
<title>Elements of Information Theory.</title>
<date>1991</date>
<publisher>Wiley-Interscience,</publisher>
<location>New York, New York.</location>
<contexts>
<context position="6896" citStr="Cover and Thomas, 1991" startWordPosition="1106" endWordPosition="1109">asure of similarity between distributions. We use for this purpose the relative entropy or Kullback-Leibler (KL) distance between two distributions D(p II q) = p(x) log P(x) q(x) This is a natural choice for a variety of reasons, which we will just sketch here.&apos; First of all, D(p II q) is zero just when p = q, and it increases as the probability decreases that p is the relative frequency distribution of a random sample drawn according to q. More formally, the probability mass given by q to the set of all samples of length n with relative frequency distribution p is bounded by exp —nD(p II q) (Cover and Thomas, 1991). Therefore, if we are trying to distinguish among hypotheses qi when p is the relative frequency distribution of observations, D(p qi) gives the relative weight of evidence in favor of qi. Furthermore, a similar relation holds between D(p p&apos;) for two empirical distributions p and p&apos; and the probability that p and p&apos; are drawn from the same distribution q. We can thus use the relative entropy between the context distributions for two words to measure how likely they are to be instances of the same cluster centroid. &apos;A more formal discussion will appear in our paper Distributional Clustering, i</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>Thomas M. Cover and Joy A. Thomas. 1991. Elements of Information Theory. Wiley-Interscience, New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Shaul Markus</author>
<author>Shaul Markovitch</author>
</authors>
<title>Contextual word similarity and estimation from sparse data. In these proceedings.</title>
<date>1993</date>
<contexts>
<context position="28914" citStr="Dagan et al. (1993)" startWordPosition="4842" endWordPosition="4845"> applications to disambiguation in language analysis. The task consists judging which of two verbs v and v&apos; is more likely to take a given noun n as object, when all occurrences of (v, n) in the training set were deliberately deleted. Thus this test evaluates how well the models reconstruct missing data in the the weighted average Eneiv., fr,D(11/3) where f, is the relative frequency of n in the test set. 189 verb distribution for n from the cluster centroids close to n. The data for this test was built from the training data for the previous one in the following way, based on a suggestion by Dagan et al. (1993). 104 noun-verb pairs with a fairly frequent verb (between 500 and 5000 occurrences) were randomly picked, and all occurrences of each pair in the training set were deleted. The resulting training set was used to build a sequence of cluster models as before. Each model was used to decide which of two verbs v and v&apos; are more likely to appear with a noun n where the (v, n) data was deleted from the training set, and the decisions were compared with the corresponding ones derived from the original event frequencies in the initial data set. The error rate for each model is simply the proportion of</context>
</contexts>
<marker>Dagan, Markus, Markovitch, 1993</marker>
<rawString>Ido Dagan, Shaul Markus, and Shaul Markovitch. 1993. Contextual word similarity and estimation from sparse data. In these proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<pages>39--1</pages>
<contexts>
<context position="12143" citStr="Dempster et al., 1977" startWordPosition="1991" endWordPosition="1994">rmined solely by the relevant measure of object-to-cluster similarity, which in the present work is the relative entropy between object and cluster centroid distributions. Since no other information is available, the membership is determined by maximizing the configuration entropy for a fixed average distortion. With the maximum entropy (ME) membership distribution, ML estimation is equivalent to the minimization of the average distortion of the data. The combined entropy maximization entropy and distortion minimization is carried out by a two-stage iterative process similar to the EM method (Dempster et al., 1977). The first stage of an iteration is a maximum likelihood, or minimum distortion, estimation of the cluster centroids given fixed membership probabilities. In the second stage of each iteration, the entropy of the membership distribution is maximized for a fixed average distortion. This joint optimization searches for a saddle point in the distortion-entropy parameters, which is equivalent to minimizing a linear combination of the two known as free energy in statistical mechanics. This analogy with statistical mechanics is not coincidental, and provides a better understanding of the clustering</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39(1):1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duda</author>
<author>Peter E Hart</author>
</authors>
<title>Pattern Classification and Scene Analysis. WileyInterscience,</title>
<date>1973</date>
<location>New York, New York.</location>
<contexts>
<context position="14721" citStr="Duda and Hart, 1973" startWordPosition="2419" endWordPosition="2422"> to just one of the components of the joint distribution and the cluster centroids are specified only by the other component. In particular, the model we use in our experiments has noun clusters with cluster memberships determined by p(n1c) and centroid distributions determined by p(vIc). The asymmetric model simplifies the estimation significantly by dealing with a single component, but it has the disadvantage that the joint distribution, p(n, v) has two different and not necessarily consistent expressions in terms of asymmetric models for the two coordinates. 2As usual in clustering models (Duda and Hart, 1973), we assume that the model distribution and the empirical distribution are interchangeable at the solution of the parameter estimation equations, since the model is assumed to be able to represent correctly the data at that solution point. In practice, the data may not come exactly from the chosen model class, but the model obtained by solving the estimation equations may still be the closest one to the data. 185 Maximum Entropy Cluster Membership While variations of p(nic) and p(vic) in equation (4) are not independent, we can treat them separately. First, for fixed average distortion between</context>
</contexts>
<marker>Duda, Hart, 1973</marker>
<rawString>Richard 0. Duda and Peter E. Hart. 1973. Pattern Classification and Scene Analysis. WileyInterscience, New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
</authors>
<title>Noun classification from predicate-argument structures.</title>
<date>1990</date>
<booktitle>In 28th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>268--275</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<location>Pittsburgh, Pennsylvania.</location>
<contexts>
<context position="2466" citStr="Hindle (1990)" startWordPosition="363" endWordPosition="364"> proposed by a grammar. It is well known that a simple tabulation of frequencies of certain words participating in certain configurations, for example of frequencies of pairs of a transitive main verb and the head noun of its direct object, cannot be reliably used for comparing the likelihoods of different alternative configurations. The problem is that for large enough corpora the number of possible joint events is much larger than the number of event occurrences in the corpus, so many events are seen rarely or never, making their frequency counts unreliable estimates of their probabilities. Hindle (1990) proposed dealing with the sparseness problem by estimating the likelihood of unseen events from that of &amp;quot;similar&amp;quot; events that have been seen. For instance, one may estimate the likelihood of a particular direct object for a verb from the likelihoods of that direct object for similar verbs. This requires a reasonable definition of verb similarity and a similarity estimation method. In Hindle&apos;s proposal, words are similar if we have strong statistical evidence that they tend to participate in the same events. His notion of similarity seems to agree with our intuitions in many cases, but it is n</context>
</contexts>
<marker>Hindle, 1990</marker>
<rawString>Donald Hindle. 1990. Noun classification from predicate-argument structures. In 28th Annual Meeting of the Association for Computational Linguistics, pages 268-275, Pittsburgh, Pennsylvania. Association for Computational Linguistics, Morristown, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
</authors>
<title>A parser for text corpora.</title>
<date>1993</date>
<booktitle>Computational Approaches to the Lexicon.</booktitle>
<editor>In B.T.S. Atkins and A. Zampoli, editors,</editor>
<publisher>Oxford University Press,</publisher>
<location>Oxford, England.</location>
<note>To appear.</note>
<contexts>
<context position="4756" citStr="Hindle, 1993" startWordPosition="739" endWordPosition="740">, we will consider two major word classes, V and N., for the verbs and nouns in our experiments, and a single relation between them, in our experiments the relation between a transitive main verb and the head noun of its direct object. Our raw knowledge about the relation consists of the frequencies f„ of occurrence of particular pairs (v, n) in the required configuration in a training corpus. Some form of text analysis is required to collect such a collection of pairs. The corpus used in our first experiment was derived from newswire text automatically parsed by 183 Hindle&apos;s parser Fidditch (Hindle, 1993). More recently, we have constructed similar tables with the help of a statistical part-of-speech tagger (Church, 1988) and of tools for regular expression pattern matching on tagged corpora (Yarowsky, 1992). We have not yet compared the accuracy and coverage of the two methods, or what systematic biases they might introduce, although we took care to filter out certain systematic errors, for instance the misparsing of the subject of a complement clause as the direct object of a main verb for report verbs like &amp;quot;say&amp;quot;. We will consider here only the problem of classifying nouns according to their</context>
</contexts>
<marker>Hindle, 1993</marker>
<rawString>Donald Hindle. 1993. A parser for text corpora. In B.T.S. Atkins and A. Zampoli, editors, Computational Approaches to the Lexicon. Oxford University Press, Oxford, England. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edwin T Jaynes</author>
</authors>
<title>Brandeis lectures.</title>
<date>1983</date>
<booktitle>Papers on Probability, Statistics and Statistical Physics, number 158 in Synthese Library, chapter 4,</booktitle>
<pages>40--76</pages>
<editor>In Roger D. Rosenkrantz, editor, E. T. Jaynes:</editor>
<location>Dordrecht, Holland.</location>
<contexts>
<context position="16132" citStr="Jaynes, 1983" startWordPosition="2653" endWordPosition="2654">stributions. With the membership distributions thus obtained, we then look for the p(vic) that maximize the log likelihood /(S). It turns out that this will also be the values of p(v lc) that minimize the average distortion between the asymmetric cluster model and the data. Given any similarity measure d(n, c) between nouns and cluster centroids, the average cluster distortion is (D) = E Ep(c1n)d(n,c) nEg cEC If we maximize the cluster membership entropy H = — E Ep(c1n) logp(n1c) (6) nEg cEC subject to normalization of p(n lc) and fixed (5), we obtain the following standard exponential forms (Jaynes, 1983) for the class and membership distributions 1 p(nic) = exp —13c/(n, c) P(c1n) = exp — Pd(n, c) (8) where the normalization sums (partition functions) are Zc = En exp — fld(n, c) and Zr, = Ec exp —)3c/(n, c). Notice that d(n, c) does not need to be symmetric for this derivation, as the two distributions are simply related by Bayes&apos;s rule. Returning to the log-likelihood variation (4), we can now use (7) for p(n1c) and the assumption for the asymmetric model that the cluster membership stays fixed as we adjust the centroids, to obtain bl(S) = — EE p(clni)b Pd(ni , + 6 log Zc (9) cEC where the va</context>
</contexts>
<marker>Jaynes, 1983</marker>
<rawString>Edwin T. Jaynes. 1983. Brandeis lectures. In Roger D. Rosenkrantz, editor, E. T. Jaynes: Papers on Probability, Statistics and Statistical Physics, number 158 in Synthese Library, chapter 4, pages 40-76. D. Reidel, Dordrecht, Holland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>WordNet and distributional analysis: A class-based approach to lexical discovery.</title>
<date>1992</date>
<booktitle>In AAAI Workshop on StatisticallyBased Natural-Language-Processing Techniques,</booktitle>
<location>San Jose, California,</location>
<contexts>
<context position="3502" citStr="Resnik, 1992" startWordPosition="534" endWordPosition="535">r if we have strong statistical evidence that they tend to participate in the same events. His notion of similarity seems to agree with our intuitions in many cases, but it is not clear how it can be used directly to construct word classes and corresponding models of association. Our research addresses some of the same questions and uses similar raw data, but we investigate how to factor word association tendencies into associations of words to certain hidden senses classes and associations between the classes themselves. While it may be worth basing such a model on preexisting sense classes (Resnik, 1992), in the work described here we look at how to derive the classes directly from distributional data. More specifically, we model senses as probabilistic concepts or clusters c with corresponding cluster membership probabilities p(clw) for each word w. Most other class-based modeling techniques for natural language rely instead on &amp;quot;hard&amp;quot; Boolean classes (Brown et al., 1990). Class construction is then combinatorially very demanding and depends on frequency counts for joint events involving particular words, a potentially unreliable source of information as noted above. Our approach avoids both </context>
</contexts>
<marker>Resnik, 1992</marker>
<rawString>Philip Resnik. 1992. WordNet and distributional analysis: A class-based approach to lexical discovery. In AAAI Workshop on StatisticallyBased Natural-Language-Processing Techniques, San Jose, California, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Rose</author>
<author>Eitan Gurewitz</author>
<author>Geoffrey C Fox</author>
</authors>
<title>Statistical mechanics and phase transitions in clustering. Physical Review Letters,</title>
<date>1990</date>
<pages>65--8</pages>
<contexts>
<context position="19451" citStr="Rose et al., 1990" startWordPosition="3261" endWordPosition="3264">ropy through root (D) = ooF / H = ap oF OT where T = 0-1 is the temperature. The most important property of the free energy is that its minimum determines the balance between the &amp;quot;disordering&amp;quot; maximum entropy and &amp;quot;ordering&amp;quot; distortion minimization in which the system is most likely to be found. In fact the probability to find the system at a given configuration is exponential in F P cx exp — so a system is most likely to be found in its minimal free energy configuration. Hierarchical Clustering The analogy with statistical mechanics suggests a deterministic annealing procedure for clustering (Rose et al., 1990), in which the number of clusters is determined through a sequence of phase transitions by continuously increasing the parameter 0 following an annealing schedule. The higher is 0, the more local is the influence of each noun on the definition of centroids. Distributional similarity plays here the role of distortion. When the scale parameter 0 is close to zero, the similarity is almost irrelevant. All words contribute about equally to each centroid, and so the lowest average distortion solution involves just one cluster whose centroid is the average of all word distributions. As 0 is slowly in</context>
</contexts>
<marker>Rose, Gurewitz, Fox, 1990</marker>
<rawString>Kenneth Rose, Eitan Gurewitz, and Geoffrey C. Fox. 1990. Statistical mechanics and phase transitions in clustering. Physical Review Letters, 65(8):945-948.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
</authors>
<title>Stochastic lexicalized treeadjoining grammars.</title>
<date>1992</date>
<booktitle>In Proceeedings of the 14th International Conference on Computational Linguistics,</booktitle>
<location>Nantes, France.</location>
<marker>Schabes, 1992</marker>
<rawString>Yves Schabes. 1992. Stochastic lexicalized treeadjoining grammars. In Proceeedings of the 14th International Conference on Computational Linguistics, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>CONC: Tools for text corpora.</title>
<date>1992</date>
<tech>Technical Memorandum 11222-921222-29,</tech>
<institution>AT&amp;T Bell Laboratories.</institution>
<contexts>
<context position="4963" citStr="Yarowsky, 1992" startWordPosition="770" endWordPosition="771">ead noun of its direct object. Our raw knowledge about the relation consists of the frequencies f„ of occurrence of particular pairs (v, n) in the required configuration in a training corpus. Some form of text analysis is required to collect such a collection of pairs. The corpus used in our first experiment was derived from newswire text automatically parsed by 183 Hindle&apos;s parser Fidditch (Hindle, 1993). More recently, we have constructed similar tables with the help of a statistical part-of-speech tagger (Church, 1988) and of tools for regular expression pattern matching on tagged corpora (Yarowsky, 1992). We have not yet compared the accuracy and coverage of the two methods, or what systematic biases they might introduce, although we took care to filter out certain systematic errors, for instance the misparsing of the subject of a complement clause as the direct object of a main verb for report verbs like &amp;quot;say&amp;quot;. We will consider here only the problem of classifying nouns according to their distribution as direct objects of verbs; the converse problem is formally similar. More generally, the theoretical basis for our method supports the use of clustering to build models for any n-ary relation </context>
</contexts>
<marker>Yarowsky, 1992</marker>
<rawString>David Yarowsky. 1992. CONC: Tools for text corpora. Technical Memorandum 11222-921222-29, AT&amp;T Bell Laboratories.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>