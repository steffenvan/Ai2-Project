<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000853">
<title confidence="0.9929225">
Development and Use of a Gold-Standard Data Set for
Subjectivity Classifications
</title>
<author confidence="0.968418">
Janyce M. Wiebet and Rebecca F. Bruce t and Thomas P. O&apos;Harat
</author>
<affiliation confidence="0.998909">
t Department of Computer Science and Computing Research Laboratory
New Mexico State University, Las Cruces, NM 88003
tDepartment of Computer Science
University of North Carolina at Asheville
</affiliation>
<address confidence="0.791491">
Asheville, NC 28804-8511
</address>
<email confidence="0.998874">
wiebe,tomohara@cs.nmsu.edu, bruce@cs.unca.edu
</email>
<sectionHeader confidence="0.995647" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999973166666667">
This paper presents a case study of analyzing
and improving intercoder reliability in discourse
tagging using statistical techniques. Bias-
corrected tags are formulated and successfully
used to guide a revision of the coding manual
and develop an automatic classifier.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999930076923077">
This paper presents a case study of analyz-
ing and improving intercoder reliability in dis-
course tagging using the statistical techniques
presented in (Bruce and Wiebe, 1998; Bruce
and Wiebe, to appear). Our approach is data
driven: we refine our understanding and pre-
sentation of the classification scheme guided by
the results of the intercoder analysis. We also
present the results of a probabilistic classifier
developed on the resulting annotations.
Much research in discourse processing has
focused on task-oriented and instructional di-
alogs. The task addressed here comes to the
fore in other genres, especially news reporting.
The task is to distinguish sentences used to ob-
jectively present factual information from sen-
tences used to present opinions and evaluations.
There are many applications for which this dis-
tinction promises to be important, including
text categorization and summarization. This
research takes a large step toward developing
a reliably annotated gold standard to support
experimenting with such applications.
This research is also a case study of ana-
lyzing and improving manual tagging that is
applicable to any tagging task. We perform
a statistical analysis that provides information
that complements the information provided by
Cohen&apos;s Kappa (Cohen, 1960; Carletta, 1996).
In particular, we analyze patterns of agreement
to identify systematic disagreements that result
from relative bias among judges, because they
can potentially be corrected automatically. The
corrected tags serve two purposes in this work.
They are used to guide the revision of the cod-
ing manual, resulting in improved Kappa scores,
and they serve as a gold standard for developing
a probabilistic classifier. Using bias-corrected
tags as gold-standard tags is one way to define
a single best tag when there are multiple judges
who disagree.
The coding manual and data from our exper-
iments are available at:
hap: / /www.cs.nmsu.edur wiebe/projects.
In the remainder of this paper, we describe
the classification being performed (in section 2),
the statistical tools used to analyze the data and
produce the bias-corrected tags (in section 3),
the case study of improving intercoder agree-
ment (in section 4), and the results of the clas-
sifier for automatic subjectivity tagging (in sec-
tion 5).
</bodyText>
<sectionHeader confidence="0.9028155" genericHeader="introduction">
2 The Subjective and Objective
Categories
</sectionHeader>
<bodyText confidence="0.9996734">
We address evidentiality in text (Chafe, 1986),
which concerns issues such as what is the source
of information, and whether information is be-
ing presented as fact or opinion. These ques-
tions are particularly important in news report-
ing, in which segments presenting opinions and
verbal reactions are mixed with segments pre-
senting objective fact (van Dijk, 1988; Kan et
al., 1998).
The definitions of the categories in our cod-
</bodyText>
<page confidence="0.996714">
246
</page>
<bodyText confidence="0.986250523809524">
ing manual are intention-based: &amp;quot;If the primary
intention of a sentence is objective presentation
of material that is factual to the reporter, the
sentence is objective. Otherwise, the sentence is
subjective.&amp;quot;&apos;
We focus on sentences about private states,
such as belief, knowledge, emotions, etc. (Quirk
et al., 1985), and sentences about speech events,
such as speaking and writing. Such sentences
may be either subjective or objective. From
the coding manual: &amp;quot;Subjective speech-event
(and private-state) sentences are used to com-
municate the speaker&apos;s evaluations, opinions,
emotions, and speculations. The primary in-
tention of objective speech-event (and private-
state) sentences, on the other hand, is to ob-
jectively communicate material that is factual
to the reporter. The speaker, in these cases, is
being used as a reliable source of information.&amp;quot;
Following are examples of subjective and ob-
jective sentences:
</bodyText>
<listItem confidence="0.9407008">
1. At several different levels, it&apos;s a fascinating
tale. Subjective sentence.
2. Bell Industries Inc. increased its quarterly
to 10 cents from seven cents a share. Ob-
jective sentence.
3. Northwest Airlines settled the remaining
lawsuits filed on behalf of 156 people killed
in a 1987 crash, but claims against the
jetliner&apos;s maker are being pursued, a fed-
eral judge said. Objective speech-event sen-
tence.
4. The South African Broadcasting Corp.
said the song &amp;quot;Freedom Now&amp;quot; was &amp;quot;un-
desirable for broadcasting.&amp;quot; Subjective
speech-event sentence.
</listItem>
<bodyText confidence="0.993645777777778">
In sentence 4, there is no uncertainty or eval-
uation expressed toward the speaking event.
Thus, from one point of view, one might have
considered this sentence to be objective. How-
ever, the object of the sentence is not presented
as material that is factual to the reporter, so
the sentence is classified as subjective.
Linguistic categorizations usually do not
cover all instances perfectly. For example, sen-
</bodyText>
<footnote confidence="0.587198">
1-The category specifications in the coding manual are
based on our previous work on tracking point of view
(Wiebe, 1994), which builds on Banfield&apos;s (1982) linguis-
tic theory of subjectivity.
</footnote>
<bodyText confidence="0.999165921052632">
tences may fall on the borderline between two
categories. To allow for uncertainty in the an-
notation process, the specific tags used in this
work include certainty ratings, ranging from 0,
for least certain, to 3, for most certain. As dis-
cussed below in section 3.2, the certainty ratings
allow us to investigate whether a model positing
additional categories provides a better descrip-
tion of the judges&apos; annotations than a binary
model does.
Subjective and objective categories are poten-
tially important for many text processing ap-
plications, such as information extraction and
information retrieval, where the evidential sta-
tus of information is important. In generation
and machine translation, it is desirable to gener-
ate text that is appropriately subjective or ob-
jective (Hovy, 1987). In summarization, sub-
jectivity judgments could be included in doc-
ument profiles, to augment automatically pro-
duced document summaries, and to help the
user make relevance judgments when using a
search engine. In addition, they would be useful
in text categorization. In related work (Wiebe
et al., in preparation), we found that article
types, such as announcement and opinion piece,
are significantly correlated with the subjective
and objective classification.
Our subjective category is related to but dif-
fers from the statement-opinion category of
the Switchboard-DAMSL discourse annotation
project (Jurafsky et al., 1997), as well as the
gives opinion category of Bale&apos;s (1950) model
of small-group interaction. All involve expres-
sions of opinion, but while our category spec-
ifications focus on evidentiality in text, theirs
focus on how conversational participants inter-
act with one another in dialog.
</bodyText>
<sectionHeader confidence="0.97978" genericHeader="method">
3 Statistical Tools
</sectionHeader>
<bodyText confidence="0.995261727272727">
Table 1 presents data for two judges. The rows
correspond to the tags assigned by judge 1 and
the columns correspond to the tags assigned by
judge 2. Let nj denote the number of sentences
that judge 1 classifies as i and judge 2 classi-
fies as j, and let be the probability that a
randomly selected sentence is categorized as i
by judge 1 and j by judge 2. Then, the max-
imum likelihood estimate of fiii is 11-7-71.:+ , where
n++ = Eii nii = 504.
Table 1 shows a four-category data configu-
</bodyText>
<page confidence="0.965076">
247
</page>
<equation confidence="0.9000276875">
Judge 2 = J
Subj2 3 Subjo Objo Obj2 3
nii = 158 n12 = 43 n13 = 15 n14 = 4
n2i = 0 n22 = 0 n23 = 0 n24 = 0
n31 = 3 n32 = 2 n33 7----- 2 n34 = 0
n41 =38 n42 = 48 n43 = 49 n44 = 142
= 199 n+2 = 93 n+3 = 66 n+4 = 146
ni+ = 220
n2+ = 0
n3+ = 7
n4+ = 277
n++ = 504
Sub:12,3
Judge 1 Subjo,i
=D Objo,i
Obj2,3
</equation>
<tableCaption confidence="0.951109">
Table 1: Four-Category Contingency Table
</tableCaption>
<bodyText confidence="0.997026756097561">
ration, in which certainty ratings 0 and 1 are
combined and ratings 2 and 3 are combined.
Note that the analyses described in this section
cannot be performed on the two-category data
configuration (in which the certainty ratings are
not considered), due to insufficient degrees of
freedom (Bishop et al., 1975).
Evidence of confusion among the classifica-
tions in Table 1 can be found in the marginal
totals, ni+ and n+j. We see that judge 1 has a
relative preference, or bias, for objective, while
judge 2 has a bias for subjective. Relative bias
is one aspect of agreement among judges. A
second is whether the judges&apos; disagreements are
systematic, that is, correlated. One pattern of
systematic disagreement is symmetric disagree-
ment. When disagreement is symmetric, the
differences between the actual counts, and the
counts expected if the judges&apos; decisions were not
correlated, are symmetric; that is, Snii
for i j, where 5,i, is the difference from inde-
pendence.
Our goal is to correct correlated disagree-
ments automatically. We are particularly in-
terested in systematic disagreements resulting
from relative bias. We test for evidence of
such correlations by fitting probability models
to the data. Specifically, we study bias using
the model for marginal homogeneity, and sym-
metric disagreement using the model for quasi-
symmetry. When there is such evidence, we
propose using the latent class model to correct
the disagreements; this model posits an unob-
served (latent) variable to explain the correla-
tions among the judges&apos; observations.
The remainder of this section describes these
models in more detail. All models can be eval-
uated using the freeware package CoCo, which
was developed by Badsberg (1995) and is avail-
able at:
http: / /web.math.auc.dkr jhb/CoCo.
</bodyText>
<subsectionHeader confidence="0.999688">
3.1 Patterns of Disagreement
</subsectionHeader>
<bodyText confidence="0.999963882352941">
A probability model enforces constraints on the
counts in the data. The degree to which the
counts in the data conform to the constraints is
called the fit of the model. In this work, model
fit is reported in terms of the likelihood ra-
tio statistic, G2, and its significance (Read and
Cressie, 1988; Dunning, 1993). The higher the
G2 value, the poorer the fit. We will consider
model fit to be acceptable if its reference sig-
nificance level is greater than 0.01 (i.e., if there
is greater than a 0.01 probability that the data
sample was randomly selected from a popula-
tion described by the model).
Bias of one judge relative to another is evi-
denced as a discrepancy between the marginal
totals for the two judges (i.e., ni+ and n+j in
Table 1). Bias is measured by testing the fit of
the model for marginal homogeneity: = 25+i
for all i. The larger the G2 value, the greater
the bias. The fit of the model can be evaluated
as described on pages 293-294 of Bishop et al.
(1975).
Judges who show a relative bias do not al-
ways agree, but their judgments may still be
correlated. As an extreme example, judge 1
may assign the subjective tag whenever judge
2 assigns the objective tag. In this example,
there is a kind of symmetry in the judges&apos; re-
sponses, but their agreement would be low. Pat-
terns of symmetric disagreement can be identi-
fied using the model for quasi-symmetry. This
model constrains the off-diagonal counts, i.e.,
the counts that correspond to disagreement. It
states that these counts are the product of a
</bodyText>
<page confidence="0.988546">
248
</page>
<bodyText confidence="0.982877777777778">
table for independence and a symmetric table,
nii = Ai+ x A+i x Aii, such that Aij = ii.In
this formula, Ai+ x A+3 is the model for inde-
pendence and Ai3 is the symmetric interaction
term. Intuitively, Aii represents the difference
between the actual counts and those predicted
by independence. This model can be evaluated
using CoCo as described on pages 289-290 of
Bishop et al. (1975).
</bodyText>
<subsectionHeader confidence="0.999964">
3.2 Producing Bias-Corrected Tags
</subsectionHeader>
<bodyText confidence="0.99981516">
We use the latent class model to correct sym-
metric disagreements that appear to result from
bias. The latent class model was first intro-
duced by Lazarsfeld (1966) and was later made
computationally efficient by Goodman (1974).
Goodman&apos;s procedure is a specialization of the
EM algorithm (Dempster et al., 1977), which
is implemented in the freeware program CoCo
(Badsberg, 1995). Since its development, the
latent class model has been widely applied, and
is the underlying model in various unsupervised
machine learning algorithms, including Auto-
Class (Cheeseman and Stutz, 1996).
The form of the latent class model is that of
naive Bayes: the observed variables are all con-
ditionally independent of one another, given the
value of the latent variable. The latent variable
represents the true state of the object, and is the
source of the correlations among the observed
variables.
As applied here, the observed variables are
the classifications assigned by the judges. Let
B, D, J, and M be these variables, and let L
be the latent variable. Then, the latent class
model is:
</bodyText>
<equation confidence="0.97711">
p(b, d, j, m , 1) = P(bil)Adil)P(j11)P(mil)P(1)
(by C.I. assumptions)
p(b, Op(d,l)p(j , Op(m, 1)
p(1)3
</equation>
<bodyText confidence="0.989281617647059">
(by definition)
The parameters of the model
are {p(b, 1) , p(d, 1), p(j , 1) , p(m, 1)p(1)} . Once es-
timates of these parameters are obtained, each
clause can be assigned the most probable latent
category given the tags assigned by the judges.
The EM algorithm takes as input the number
of latent categories hypothesized, i.e., the num-
ber of values of L, and produces estimates of the
parameters. For a description of this process,
see Goodman (1974), Dawid &amp; Skene (1979), or
Pedersen &amp; Bruce (1998).
Three versions of the latent class model are
considered in this study, one with two latent
categories, one with three latent categories, and
one with four. We apply these models to three
data configurations: one with two categories
(subjective and objective with no certainty rat-
ings), one with four categories (subjective and
objective with coarse-grained certainty ratings,
as shown in Table 1), and one with eight cate-
gories (subjective and objective with fine-grained
certainty ratings). All combinations of model
and data configuration are evaluated, except the
four-category latent class model with the two-
category data configuration, due to insufficient
degrees of freedom.
In all cases, the models fit the data well, as
measured by G2. The model chosen as final
is the one for which the agreement among the
latent categories assigned to the three data con-
figurations is highest, that is, the model that is
most consistent across the three data configura-
tions.
</bodyText>
<sectionHeader confidence="0.9365875" genericHeader="method">
4 Improving Agreement in
Discourse Tagging
</sectionHeader>
<bodyText confidence="0.9967925">
Our annotation project consists of the following
steps:2
</bodyText>
<listItem confidence="0.72834425">
1. A first draft of the coding instructions is
developed.
2. Four judges annotate a corpus according
to the first coding manual, each spending
about four hours.
3. The annotated corpus is statistically ana-
lyzed using the methods presented in sec-
tion 3, and bias-corrected tags are pro-
duced.
4. The judges are given lists of sentences
for which their tags differ from the bias-
corrected tags. Judges M, D, and J par-
ticipate in interactive discussions centered
around the differences. In addition, after
reviewing his or her list of differences, each
judge provides feedback, agreeing with the
</listItem>
<footnote confidence="0.992429">
2The results of the first three steps are reported in
(Bruce and Wiebe, to appear).
</footnote>
<page confidence="0.998855">
249
</page>
<bodyText confidence="0.9481521">
bias-corrected tag in many cases, but argu-
ing for his or her own tag in some cases.
Based on the judges&apos; feedback, 22 of the
504 bias-corrected tags are changed, and a
second draft of the coding manual is writ-
ten.
5. A second corpus is annotated by the same
four judges according to the new coding
manual. Each spends about five hours.
6. The results of the second tagging experi-
ment are analyzed using the methods de-
scribed in section 3, and bias-corrected tags
are produced for the second data set.
Two disjoint corpora are used in steps 2 and
5, both consisting of complete articles taken
from the Wall Street Journal Treebank Corpus
(Marcus et al., 1993). In both corpora, judges
assign tags to each non-compound sentence and
to each conjunct of each compound sentence,
504 in the first corpus and 500 in the second.
The segmentation of compound sentences was
performed manually before the judges received
the data.
Judges J and B, the first two authors of this
paper, are NLP researchers. Judge M is an
undergraduate computer science student, and
judge D has no background in computer science
or linguistics. Judge J, with help from M, devel-
oped the original coding instructions, and Judge
J directed the process in step 4.
The analysis performed in step 3 reveals
strong evidence of relative bias among the
judges. Each pairwise comparison of judges also
shows a strong pattern of symmetric disagree-
ment. The two-category latent class model pro-
duces the most consistent clusters across the
data configurations. It, therefore, is used to de-
fine the bias-corrected tags.
In step 4, judge B was excluded from the in-
teractive discussion for logistical reasons. Dis-
cussion is apparently important, because, al-
though B&apos;s Kappa values for the first study are
on par with the others, B&apos;s Kappa values for
agreement with the other judges change very
little from the first to the second study (this
is true across the range of certainty values). In
contrast, agreement among the other judges no-
ticeably improves. Because judge B&apos;s poor per-
formance in the second tagging experiment is
linked to a difference in procedure, judge B&apos;s
</bodyText>
<table confidence="0.945051157894737">
Study 1 Study 2
is % of is % of
corpus corpus
covered covered
Certainty Values 0,1,2 or 3
M &amp; D 0.60 100 0.76 100
M &amp; J 0.63 100 0.67 100
D &amp; J 0.57 100 0.65 100
B &amp; J 0.62 100 0.64 100
B &amp; M 0.60 100 0.59 100
B &amp; D 0.58 100 0.59 100
Certainty Values 1,2 or 3
M &amp; D 0.62 96 0.84 92
M &amp; J 0.78 81 0.81 81
D &amp; J 0.67 84 0.72 82
Certainty Values 2 or 3
M &amp; D 0.67 89 0.89 81
M &amp; J 0.88 64 0.87 67
D &amp; J 0.76 68 0.88 62
</table>
<tableCaption confidence="0.999509">
Table 2: Pairwise Kappa (K) Scores
</tableCaption>
<bodyText confidence="0.999863461538462">
tags are excluded from our subsequent analysis
of the data gathered during the second tagging
experiment.
Table 2 shows the changes, from study 1 to
study 2, in the Kappa values for pairwise agree-
ment among the judges. The best results are
clearly for the two who are not authors of this
paper (D and M). The Kappa value for the
agreement between D and M considering all cer-
tainty ratings reaches .76, which allows tenta-
tive conclusions on Krippendorf&apos;s scale (1980).
If we exclude the sentences with certainty rat-
ing 0, the Kappa values for pairwise agreement
between M and D and between J and M are
both over .8, which allows definite conclusions
on Krippendorf&apos;s scale. Finally, if we only con-
sider sentences with certainty 2 or 3, the pair-
wise agreements among M, D, and J all have
high Kappa values, 0.87 and over.
We are aware of only one previous project
reporting intercoder agreement results for simi-
lar categories, the switchboard-DAMSL project
mentioned above. While their Kappa results are
very good for other tags, the opinion-statement
tagging was not very successful: &amp;quot;The distinc-
tion was very hard to make by labelers, and
</bodyText>
<page confidence="0.989632">
250
</page>
<table confidence="0.999428">
M. H.:
G2 104.912 17.343 136.660
Sig. 0.000 0.001 0.000
Q. S. :
G2 0.054 0.128 0.350
Sig. 0.997 0.998 0.95
</table>
<tableCaption confidence="0.998652">
Table 3: Tests for Patterns of Agreement
</tableCaption>
<bodyText confidence="0.998764916666667">
accounted for a large proportion of our interla-
beler error&amp;quot; (Jurafsky et al., 1997).
In step 6, as in step 3, there is strong evi-
dence of relative bias among judges D, J and M.
Each pairwise comparison of judges also shows a
strong pattern of symmetric disagreement. The
results of this analysis are presented in Table
3.3 Also as in step 3, the two-category latent
class model produces the most consistent clus-
ters across the data configurations. Thus, it is
used to define the bias-corrected tags for the
second data set as well.
</bodyText>
<sectionHeader confidence="0.99203" genericHeader="method">
5 Machine Learning Results
</sectionHeader>
<bodyText confidence="0.99992795">
Recently, there have been many successful ap-
plications of machine learning to discourse pro-
cessing, such as (Litman, 1996; Samuel et al.,
1998). In this section, we report the results
of machine learning experiments, in which we
develop probablistic classifiers to automatically
perform the subjective and objective classifica-
tion. In the method we use for developing clas-
sifiers (Bruce and Wiebe, 1999), a search is per-
formed to find a probability model that cap-
tures important interdependencies among fea-
tures. Because features can be dropped and
added during search, the method also performs
feature selection.
In these experiments, the system considers
naive Bayes, full independence, full interdepen-
dence, and models generated from those using
forward and backward search. The model se-
lected is the one with the highest accuracy on a
held-out portion of the training data.
</bodyText>
<footnote confidence="0.852486">
10-fold cross validation is performed. The
data is partitioned randomly into 10 different
3For the analysis in Table 3, certainty ratings 0 and 1,
and 2 and 3 are combined. Similar results are obtained
when all ratings are treated as distinct.
</footnote>
<bodyText confidence="0.999723916666667">
sets. On each fold, one set is used for testing,
and the other nine are used for training. Fea-
ture selection, model selection, and parameter
estimation are performed anew on each fold.
The following are the potential features con-
sidered on each fold. A binary feature is in-
cluded for each of the following: the presence
in the sentence of a pronoun, an adjective, a
cardinal number, a modal other than will, and
an adverb other than not. We also include a
binary feature representing whether or not the
sentence begins a new paragraph. Finally, a fea-
ture is included representing co-occurrence of
word tokens and punctuation marks with the
subjective and objective classification.4 There
are many other features to investigate in future
work, such as features based on tags assigned
to previous utterances (see, e.g., (Wiebe et al.,
1997; Samuel et al., 1998)), and features based
on semantic classes, such as positive and neg-
ative polarity adjectives (Hatzivassiloglou and
McKeown, 1997) and reporting verbs (Bergler,
1992).
The data consists of the concatenation of the
two corpora annotated with bias-corrected tags
as described above. The baseline accuracy, i.e.,
the frequency of the more frequent class, is only
51%.
The results of the experiments are very
promising. The average accuracy across all
folds is 72.17%, more than 20 percentage points
higher than the baseline accuracy. Interestingly,
the system performs better on the sentences for
which the judges are certain. In a post hoc anal-
ysis, we consider the sentences from the second
data set for which judges M, J, and D rate their
certainty as 2 or 3. There are 299/500 such sen-
tences. For each fold, we calculate the system&apos;s
accuracy on the subset of the test set consisting
of such sentences. The average accuracy of the
subsets across folds is 81.5%.
Taking human performance as an upper
bound, the system has room for improvement.
The average pairwise percentage agreement be-
tween D, J, and M and the bias-corrected tags in
the entire data set is 89.5%, while the system&apos;s
percentage agreement with the bias-corrected
tags (i.e., its accuracy) is 72.17%.
</bodyText>
<footnote confidence="0.992659666666667">
4The per-class enumerated feature representation
from (Wiebe et al., 1998) is used, with 60% as the con-
ditional independence cutoff threshold.
</footnote>
<note confidence="0.794088">
Test DIJ DIM JIM
</note>
<page confidence="0.99703">
251
</page>
<sectionHeader confidence="0.99878" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999966458333333">
This paper demonstrates a procedure for auto-
matically formulating a single best tag when
there are multiple judges who disagree. The
procedure is applicable to any tagging task in
which the judges exhibit symmetric disagree-
ment resulting from bias. We successfully use
bias-corrected tags for two purposes: to guide
a revision of the coding manual, and to develop
an automatic classifier. The revision of the cod-
ing manual results in as much as a 16 point im-
provement in pairwise Kappa values, and raises
the average agreement among the judges to a
Kappa value of over 0.87 for the sentences that
can be tagged with certainty.
Using only simple features, the classifier
achieves an average accuracy 21 percentage
points higher than the baseline, in 10-fold cross
validation experiments. In addition, the aver-
age accuracy of the classifier is 81.5% on the
sentences the judges tagged with certainty. The
strong performance of the classifier and its con-
sistency with the judges demonstrate the value
of this approach to developing gold-standard
tags.
</bodyText>
<sectionHeader confidence="0.998355" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.994087">
This research was supported in part by the
Office of Naval Research under grant number
N00014-95-1-0776. We are grateful to Matthew
T. Bell and Richard A. Wiebe for participating
in the annotation study, and to the anonymous
reviewers for their comments and suggestions.
</bodyText>
<sectionHeader confidence="0.99789" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998190462686567">
J. Badsberg. 1995. An Environment for Graph-
ical Models. Ph.D. thesis, Aalborg University.
R. F. Bales. 1950. Interaction Process Analysis.
University of Chicago Press, Chicago, ILL.
Ann Banfield. 1982. Unspeakable Sentences:
Narration and Representation in the Lan-
guage of Fiction. Routledge 8z Kegan Paul,
Boston.
S. Bergler. 1992. Evidential Analysis of Re-
ported Speech. Ph.D. thesis, Brandeis Univer-
sity.
Y.M. Bishop, S. Fienberg, and P. Holland.
1975. Discrete Multivariate Analysis: Theory
and Practice. The MIT Press, Cambridge.
R. Bruce and J. Wiebe. 1998. Word sense dis-
tinguishability and inter-coder agreement. In
Proc. 3rd Conference on Empirical Methods
in Natural Language Processing (EMNLP-
98), pages 53-60, Granada, Spain, June. ACL
SIGDAT.
R. Bruce and J. Wiebe. 1999. Decompos-
able modeling in natural language processing.
Computational Linguistics, 25(2).
R. Bruce and J. Wiebe. to appear. Recognizing
subjectivity: A case study of manual tagging.
Natural Language Engineering.
J. Carletta. 1996. Assessing agreement on clas-
sification tasks: The kappa statistic. Compu-
tational Linguistics, 22(2):249-254.
W. Chafe. 1986. Evidentiality in English con-
versation and academic writing. In Wallace
Chafe and Johanna Nichols, editors, Eviden-
tiality: The Linguistic Coding of Epistemol-
ogy, pages 261-272. Ablex, Norwood, NJ.
P. Cheeseman and J. Stutz. 1996. Bayesian
classification (AutoClass): Theory and re-
sults. In Fayyad, Piatetsky-Shapiro, Smyth,
and Uthurusamy, editors, Advances in
Knowledge Discovery and Data Mining.
AAAI Press/MIT Press.
J. Cohen. 1960. A coefficient of agreement for
nominal scales. Educational and Psychologi-
cal Meas., 20:37-46.
A. P. Dawid and A. M. Skene. 1979. Maximum
likelihood estimation of observer error-rates
using the EM algorithm. Applied Statistics,
28:20-28.
A. Dempster, N. Laird, and D. Rubin. 1977.
Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal
Statistical Society, 39 (Series B):1-38.
T. Dunning. 1993. Accurate methods for the
statistics of surprise and coincidence. Com-
putational Linguistics, 19(1):75-102.
L. Goodman. 1974. Exploratory latent struc-
ture analysis using both identifiable and
unidentifiable models. Biometrika, 61:2:215-
231.
V. Hatzivassiloglou and K. McKeown. 1997.
Predicting the semantic orientation of adjec-
tives. In A CL-E A CL 1997, pages 174-181,
Madrid, Spain, July.
Eduard Hovy. 1987. Generating Natural Lan-
guage under Pragmatic Constraints. Ph.D.
thesis, Yale University.
D. Jurafsky, E. Shriberg, and D. Blase&amp;
1997. Switchboard SWBD-DAMSL shallow-
</reference>
<page confidence="0.963965">
252
</page>
<reference confidence="0.9958602">
discourse-function annotation coders manual,
draft 13. Technical Report 97-01, University
of Colorado Institute of Cognitive Science.
M.-Y. Kan, J. L. Klavans, and K. R. McKe-
own. 1998. Linear segmentation and segment
significance. In Proc. 6th Workshop on Very
Large Corpora (WVLC-98), pages 197-205,
Montreal, Canada, August. ACL SIGDAT.
K. Krippendorf. 1980. Content Analysis: An
Introduction to its Methodology. Sage Publi-
cations, Beverly Hills.
P. Lazarsfeld. 1966. Latent structure analy-
sis. In S. A. Stouffer, L. Guttman, E. Such-
man, P.Lazarfeld, S. Star, and J. Claussen,
editors, Measurement and Prediction. Wiley,
New York.
D. Litman. 1996. Cue phrase classification us-
ing machine learning. Journal of Artificial
Intelligence Research, 5:53-94.
M. Marcus,
Santorini, B., and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English:
The penn treebank. Computational Linguis-
tics, 19(2):313-330.
Ted Pedersen and Rebecca Bruce. 1998.
Knowledge lean word-sense disambiguation.
In Proc. of the 15th National Conference on
Artificial Intelligence (AAAI-98), Madison,
Wisconsin, July.
R. Quirk, S. Greenbaum, G. Leech, and
J. Svartvik. 1985. A Comprehensive Gram-
mar of the English Language. Longman, New
York.
T. Read and N. Cressie. 1988. Goodness-of-
fit Statistics for Discrete Multivariate Data.
Springer-Verlag Inc., New York, NY.
K. Samuel, S. Carberry, and K. Vijay-
Shanker. 1998. Dialogue act tagging with
transformation-based learning. In Proc.
COLING-ACL 1998, pages 1150-1156, Mon-
treal, Canada, August.
T.A. van Dijk. 1988. News as Discourse.
Lawrence Erlbaum, Hillsdale, NJ.
J. Wiebe, R. Bruce, and L. Duan. 1997.
Probabilistic event categorization. In Proc.
Recent Advances in Natural Language Pro-
cessing (RANLP-97), pages 163-170, Tsigov
Chark, Bulgaria, September.
J. Wiebe, K. McKeever, and R. Bruce. 1998.
Mapping collocational properties into ma-
chine learning features. In Proc. 6th Work-
shop on Very Large Corpora (WVLC-98),
pages 225-233, Montreal, Canada, August.
ACL SIGDAT.
J. Wiebe, J. Klavans, and M.Y. Kan. in prepa-
ration. Verb profiles for subjectivity judg-
ments and text classification.
J. Wiebe. 1994. Tracking point of view
in narrative. Computational Linguistics,
20(2):233-287.
</reference>
<page confidence="0.998932">
253
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.929394">
<title confidence="0.9985055">Development and Use of a Gold-Standard Data Set for Subjectivity Classifications</title>
<author confidence="0.999885">Janyce M Wiebet</author>
<author confidence="0.999885">Rebecca F Bruce t</author>
<author confidence="0.999885">Thomas P O&apos;Harat</author>
<affiliation confidence="0.99985">t Department of Computer Science and Computing Research Laboratory</affiliation>
<address confidence="0.960109">New Mexico State University, Las Cruces, NM 88003</address>
<affiliation confidence="0.9995665">tDepartment of Computer Science University of North Carolina at Asheville</affiliation>
<address confidence="0.97913">Asheville, NC 28804-8511</address>
<email confidence="0.999962">wiebe,tomohara@cs.nmsu.edu,bruce@cs.unca.edu</email>
<abstract confidence="0.998864857142857">This paper presents a case study of analyzing and improving intercoder reliability in discourse using statistical techniques. corrected tags are formulated and successfully used to guide a revision of the coding manual and develop an automatic classifier.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Badsberg</author>
</authors>
<title>An Environment for Graphical Models.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>Aalborg University.</institution>
<contexts>
<context position="9800" citStr="Badsberg (1995)" startWordPosition="1574" endWordPosition="1575"> from relative bias. We test for evidence of such correlations by fitting probability models to the data. Specifically, we study bias using the model for marginal homogeneity, and symmetric disagreement using the model for quasisymmetry. When there is such evidence, we propose using the latent class model to correct the disagreements; this model posits an unobserved (latent) variable to explain the correlations among the judges&apos; observations. The remainder of this section describes these models in more detail. All models can be evaluated using the freeware package CoCo, which was developed by Badsberg (1995) and is available at: http: / /web.math.auc.dkr jhb/CoCo. 3.1 Patterns of Disagreement A probability model enforces constraints on the counts in the data. The degree to which the counts in the data conform to the constraints is called the fit of the model. In this work, model fit is reported in terms of the likelihood ratio statistic, G2, and its significance (Read and Cressie, 1988; Dunning, 1993). The higher the G2 value, the poorer the fit. We will consider model fit to be acceptable if its reference significance level is greater than 0.01 (i.e., if there is greater than a 0.01 probability </context>
<context position="12213" citStr="Badsberg, 1995" startWordPosition="1993" endWordPosition="1994">m. Intuitively, Aii represents the difference between the actual counts and those predicted by independence. This model can be evaluated using CoCo as described on pages 289-290 of Bishop et al. (1975). 3.2 Producing Bias-Corrected Tags We use the latent class model to correct symmetric disagreements that appear to result from bias. The latent class model was first introduced by Lazarsfeld (1966) and was later made computationally efficient by Goodman (1974). Goodman&apos;s procedure is a specialization of the EM algorithm (Dempster et al., 1977), which is implemented in the freeware program CoCo (Badsberg, 1995). Since its development, the latent class model has been widely applied, and is the underlying model in various unsupervised machine learning algorithms, including AutoClass (Cheeseman and Stutz, 1996). The form of the latent class model is that of naive Bayes: the observed variables are all conditionally independent of one another, given the value of the latent variable. The latent variable represents the true state of the object, and is the source of the correlations among the observed variables. As applied here, the observed variables are the classifications assigned by the judges. Let B, D</context>
</contexts>
<marker>Badsberg, 1995</marker>
<rawString>J. Badsberg. 1995. An Environment for Graphical Models. Ph.D. thesis, Aalborg University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R F Bales</author>
</authors>
<title>Interaction Process Analysis.</title>
<date>1950</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago, ILL.</location>
<marker>Bales, 1950</marker>
<rawString>R. F. Bales. 1950. Interaction Process Analysis. University of Chicago Press, Chicago, ILL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Banfield</author>
</authors>
<title>Unspeakable Sentences: Narration and Representation in the Language of Fiction. Routledge 8z Kegan Paul,</title>
<date>1982</date>
<location>Boston.</location>
<marker>Banfield, 1982</marker>
<rawString>Ann Banfield. 1982. Unspeakable Sentences: Narration and Representation in the Language of Fiction. Routledge 8z Kegan Paul, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bergler</author>
</authors>
<title>Evidential Analysis of Reported Speech.</title>
<date>1992</date>
<tech>Ph.D. thesis,</tech>
<institution>Brandeis University.</institution>
<contexts>
<context position="21837" citStr="Bergler, 1992" startWordPosition="3646" endWordPosition="3647">d an adverb other than not. We also include a binary feature representing whether or not the sentence begins a new paragraph. Finally, a feature is included representing co-occurrence of word tokens and punctuation marks with the subjective and objective classification.4 There are many other features to investigate in future work, such as features based on tags assigned to previous utterances (see, e.g., (Wiebe et al., 1997; Samuel et al., 1998)), and features based on semantic classes, such as positive and negative polarity adjectives (Hatzivassiloglou and McKeown, 1997) and reporting verbs (Bergler, 1992). The data consists of the concatenation of the two corpora annotated with bias-corrected tags as described above. The baseline accuracy, i.e., the frequency of the more frequent class, is only 51%. The results of the experiments are very promising. The average accuracy across all folds is 72.17%, more than 20 percentage points higher than the baseline accuracy. Interestingly, the system performs better on the sentences for which the judges are certain. In a post hoc analysis, we consider the sentences from the second data set for which judges M, J, and D rate their certainty as 2 or 3. There </context>
</contexts>
<marker>Bergler, 1992</marker>
<rawString>S. Bergler. 1992. Evidential Analysis of Reported Speech. Ph.D. thesis, Brandeis University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y M Bishop</author>
<author>S Fienberg</author>
<author>P Holland</author>
</authors>
<title>Discrete Multivariate Analysis: Theory and Practice.</title>
<date>1975</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="8396" citStr="Bishop et al., 1975" startWordPosition="1348" endWordPosition="1351">j2 3 nii = 158 n12 = 43 n13 = 15 n14 = 4 n2i = 0 n22 = 0 n23 = 0 n24 = 0 n31 = 3 n32 = 2 n33 7----- 2 n34 = 0 n41 =38 n42 = 48 n43 = 49 n44 = 142 = 199 n+2 = 93 n+3 = 66 n+4 = 146 ni+ = 220 n2+ = 0 n3+ = 7 n4+ = 277 n++ = 504 Sub:12,3 Judge 1 Subjo,i =D Objo,i Obj2,3 Table 1: Four-Category Contingency Table ration, in which certainty ratings 0 and 1 are combined and ratings 2 and 3 are combined. Note that the analyses described in this section cannot be performed on the two-category data configuration (in which the certainty ratings are not considered), due to insufficient degrees of freedom (Bishop et al., 1975). Evidence of confusion among the classifications in Table 1 can be found in the marginal totals, ni+ and n+j. We see that judge 1 has a relative preference, or bias, for objective, while judge 2 has a bias for subjective. Relative bias is one aspect of agreement among judges. A second is whether the judges&apos; disagreements are systematic, that is, correlated. One pattern of systematic disagreement is symmetric disagreement. When disagreement is symmetric, the differences between the actual counts, and the counts expected if the judges&apos; decisions were not correlated, are symmetric; that is, Snii</context>
<context position="10864" citStr="Bishop et al. (1975)" startWordPosition="1765" endWordPosition="1768">it. We will consider model fit to be acceptable if its reference significance level is greater than 0.01 (i.e., if there is greater than a 0.01 probability that the data sample was randomly selected from a population described by the model). Bias of one judge relative to another is evidenced as a discrepancy between the marginal totals for the two judges (i.e., ni+ and n+j in Table 1). Bias is measured by testing the fit of the model for marginal homogeneity: = 25+i for all i. The larger the G2 value, the greater the bias. The fit of the model can be evaluated as described on pages 293-294 of Bishop et al. (1975). Judges who show a relative bias do not always agree, but their judgments may still be correlated. As an extreme example, judge 1 may assign the subjective tag whenever judge 2 assigns the objective tag. In this example, there is a kind of symmetry in the judges&apos; responses, but their agreement would be low. Patterns of symmetric disagreement can be identified using the model for quasi-symmetry. This model constrains the off-diagonal counts, i.e., the counts that correspond to disagreement. It states that these counts are the product of a 248 table for independence and a symmetric table, nii =</context>
</contexts>
<marker>Bishop, Fienberg, Holland, 1975</marker>
<rawString>Y.M. Bishop, S. Fienberg, and P. Holland. 1975. Discrete Multivariate Analysis: Theory and Practice. The MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bruce</author>
<author>J Wiebe</author>
</authors>
<title>Word sense distinguishability and inter-coder agreement.</title>
<date>1998</date>
<booktitle>In Proc. 3rd Conference on Empirical Methods in Natural Language Processing (EMNLP98),</booktitle>
<pages>53--60</pages>
<publisher>ACL SIGDAT.</publisher>
<location>Granada, Spain,</location>
<contexts>
<context position="869" citStr="Bruce and Wiebe, 1998" startWordPosition="120" endWordPosition="123">s, NM 88003 tDepartment of Computer Science University of North Carolina at Asheville Asheville, NC 28804-8511 wiebe,tomohara@cs.nmsu.edu, bruce@cs.unca.edu Abstract This paper presents a case study of analyzing and improving intercoder reliability in discourse tagging using statistical techniques. Biascorrected tags are formulated and successfully used to guide a revision of the coding manual and develop an automatic classifier. 1 Introduction This paper presents a case study of analyzing and improving intercoder reliability in discourse tagging using the statistical techniques presented in (Bruce and Wiebe, 1998; Bruce and Wiebe, to appear). Our approach is data driven: we refine our understanding and presentation of the classification scheme guided by the results of the intercoder analysis. We also present the results of a probabilistic classifier developed on the resulting annotations. Much research in discourse processing has focused on task-oriented and instructional dialogs. The task addressed here comes to the fore in other genres, especially news reporting. The task is to distinguish sentences used to objectively present factual information from sentences used to present opinions and evaluatio</context>
</contexts>
<marker>Bruce, Wiebe, 1998</marker>
<rawString>R. Bruce and J. Wiebe. 1998. Word sense distinguishability and inter-coder agreement. In Proc. 3rd Conference on Empirical Methods in Natural Language Processing (EMNLP98), pages 53-60, Granada, Spain, June. ACL SIGDAT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bruce</author>
<author>J Wiebe</author>
</authors>
<title>Decomposable modeling in natural language processing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="20094" citStr="Bruce and Wiebe, 1999" startWordPosition="3361" endWordPosition="3364">, the two-category latent class model produces the most consistent clusters across the data configurations. Thus, it is used to define the bias-corrected tags for the second data set as well. 5 Machine Learning Results Recently, there have been many successful applications of machine learning to discourse processing, such as (Litman, 1996; Samuel et al., 1998). In this section, we report the results of machine learning experiments, in which we develop probablistic classifiers to automatically perform the subjective and objective classification. In the method we use for developing classifiers (Bruce and Wiebe, 1999), a search is performed to find a probability model that captures important interdependencies among features. Because features can be dropped and added during search, the method also performs feature selection. In these experiments, the system considers naive Bayes, full independence, full interdependence, and models generated from those using forward and backward search. The model selected is the one with the highest accuracy on a held-out portion of the training data. 10-fold cross validation is performed. The data is partitioned randomly into 10 different 3For the analysis in Table 3, certa</context>
</contexts>
<marker>Bruce, Wiebe, 1999</marker>
<rawString>R. Bruce and J. Wiebe. 1999. Decomposable modeling in natural language processing. Computational Linguistics, 25(2).</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Bruce</author>
<author>J Wiebe</author>
</authors>
<title>to appear. Recognizing subjectivity: A case study of manual tagging.</title>
<journal>Natural Language Engineering.</journal>
<marker>Bruce, Wiebe, </marker>
<rawString>R. Bruce and J. Wiebe. to appear. Recognizing subjectivity: A case study of manual tagging. Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: The kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--2</pages>
<contexts>
<context position="2003" citStr="Carletta, 1996" startWordPosition="292" endWordPosition="293">present factual information from sentences used to present opinions and evaluations. There are many applications for which this distinction promises to be important, including text categorization and summarization. This research takes a large step toward developing a reliably annotated gold standard to support experimenting with such applications. This research is also a case study of analyzing and improving manual tagging that is applicable to any tagging task. We perform a statistical analysis that provides information that complements the information provided by Cohen&apos;s Kappa (Cohen, 1960; Carletta, 1996). In particular, we analyze patterns of agreement to identify systematic disagreements that result from relative bias among judges, because they can potentially be corrected automatically. The corrected tags serve two purposes in this work. They are used to guide the revision of the coding manual, resulting in improved Kappa scores, and they serve as a gold standard for developing a probabilistic classifier. Using bias-corrected tags as gold-standard tags is one way to define a single best tag when there are multiple judges who disagree. The coding manual and data from our experiments are avai</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>J. Carletta. 1996. Assessing agreement on classification tasks: The kappa statistic. Computational Linguistics, 22(2):249-254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Chafe</author>
</authors>
<title>Evidentiality in English conversation and academic writing.</title>
<date>1986</date>
<booktitle>The Linguistic Coding of Epistemology,</booktitle>
<pages>261--272</pages>
<editor>In Wallace Chafe and Johanna Nichols, editors, Evidentiality:</editor>
<publisher>Ablex,</publisher>
<location>Norwood, NJ.</location>
<contexts>
<context position="3086" citStr="Chafe, 1986" startWordPosition="463" endWordPosition="464">to define a single best tag when there are multiple judges who disagree. The coding manual and data from our experiments are available at: hap: / /www.cs.nmsu.edur wiebe/projects. In the remainder of this paper, we describe the classification being performed (in section 2), the statistical tools used to analyze the data and produce the bias-corrected tags (in section 3), the case study of improving intercoder agreement (in section 4), and the results of the classifier for automatic subjectivity tagging (in section 5). 2 The Subjective and Objective Categories We address evidentiality in text (Chafe, 1986), which concerns issues such as what is the source of information, and whether information is being presented as fact or opinion. These questions are particularly important in news reporting, in which segments presenting opinions and verbal reactions are mixed with segments presenting objective fact (van Dijk, 1988; Kan et al., 1998). The definitions of the categories in our cod246 ing manual are intention-based: &amp;quot;If the primary intention of a sentence is objective presentation of material that is factual to the reporter, the sentence is objective. Otherwise, the sentence is subjective.&amp;quot;&apos; We f</context>
</contexts>
<marker>Chafe, 1986</marker>
<rawString>W. Chafe. 1986. Evidentiality in English conversation and academic writing. In Wallace Chafe and Johanna Nichols, editors, Evidentiality: The Linguistic Coding of Epistemology, pages 261-272. Ablex, Norwood, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cheeseman</author>
<author>J Stutz</author>
</authors>
<title>Bayesian classification (AutoClass): Theory and results.</title>
<date>1996</date>
<booktitle>Advances in Knowledge Discovery and Data Mining.</booktitle>
<editor>In Fayyad, Piatetsky-Shapiro, Smyth, and Uthurusamy, editors,</editor>
<publisher>AAAI Press/MIT Press.</publisher>
<contexts>
<context position="12414" citStr="Cheeseman and Stutz, 1996" startWordPosition="2020" endWordPosition="2023"> al. (1975). 3.2 Producing Bias-Corrected Tags We use the latent class model to correct symmetric disagreements that appear to result from bias. The latent class model was first introduced by Lazarsfeld (1966) and was later made computationally efficient by Goodman (1974). Goodman&apos;s procedure is a specialization of the EM algorithm (Dempster et al., 1977), which is implemented in the freeware program CoCo (Badsberg, 1995). Since its development, the latent class model has been widely applied, and is the underlying model in various unsupervised machine learning algorithms, including AutoClass (Cheeseman and Stutz, 1996). The form of the latent class model is that of naive Bayes: the observed variables are all conditionally independent of one another, given the value of the latent variable. The latent variable represents the true state of the object, and is the source of the correlations among the observed variables. As applied here, the observed variables are the classifications assigned by the judges. Let B, D, J, and M be these variables, and let L be the latent variable. Then, the latent class model is: p(b, d, j, m , 1) = P(bil)Adil)P(j11)P(mil)P(1) (by C.I. assumptions) p(b, Op(d,l)p(j , Op(m, 1) p(1)3 </context>
</contexts>
<marker>Cheeseman, Stutz, 1996</marker>
<rawString>P. Cheeseman and J. Stutz. 1996. Bayesian classification (AutoClass): Theory and results. In Fayyad, Piatetsky-Shapiro, Smyth, and Uthurusamy, editors, Advances in Knowledge Discovery and Data Mining. AAAI Press/MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Meas.,</booktitle>
<pages>20--37</pages>
<contexts>
<context position="1986" citStr="Cohen, 1960" startWordPosition="290" endWordPosition="291"> objectively present factual information from sentences used to present opinions and evaluations. There are many applications for which this distinction promises to be important, including text categorization and summarization. This research takes a large step toward developing a reliably annotated gold standard to support experimenting with such applications. This research is also a case study of analyzing and improving manual tagging that is applicable to any tagging task. We perform a statistical analysis that provides information that complements the information provided by Cohen&apos;s Kappa (Cohen, 1960; Carletta, 1996). In particular, we analyze patterns of agreement to identify systematic disagreements that result from relative bias among judges, because they can potentially be corrected automatically. The corrected tags serve two purposes in this work. They are used to guide the revision of the coding manual, resulting in improved Kappa scores, and they serve as a gold standard for developing a probabilistic classifier. Using bias-corrected tags as gold-standard tags is one way to define a single best tag when there are multiple judges who disagree. The coding manual and data from our exp</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>J. Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Meas., 20:37-46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dawid</author>
<author>A M Skene</author>
</authors>
<title>Maximum likelihood estimation of observer error-rates using the EM algorithm.</title>
<date>1979</date>
<journal>Applied Statistics,</journal>
<pages>28--20</pages>
<contexts>
<context position="13486" citStr="Dawid &amp; Skene (1979)" startWordPosition="2207" endWordPosition="2210">t variable. Then, the latent class model is: p(b, d, j, m , 1) = P(bil)Adil)P(j11)P(mil)P(1) (by C.I. assumptions) p(b, Op(d,l)p(j , Op(m, 1) p(1)3 (by definition) The parameters of the model are {p(b, 1) , p(d, 1), p(j , 1) , p(m, 1)p(1)} . Once estimates of these parameters are obtained, each clause can be assigned the most probable latent category given the tags assigned by the judges. The EM algorithm takes as input the number of latent categories hypothesized, i.e., the number of values of L, and produces estimates of the parameters. For a description of this process, see Goodman (1974), Dawid &amp; Skene (1979), or Pedersen &amp; Bruce (1998). Three versions of the latent class model are considered in this study, one with two latent categories, one with three latent categories, and one with four. We apply these models to three data configurations: one with two categories (subjective and objective with no certainty ratings), one with four categories (subjective and objective with coarse-grained certainty ratings, as shown in Table 1), and one with eight categories (subjective and objective with fine-grained certainty ratings). All combinations of model and data configuration are evaluated, except the fou</context>
</contexts>
<marker>Dawid, Skene, 1979</marker>
<rawString>A. P. Dawid and A. M. Skene. 1979. Maximum likelihood estimation of observer error-rates using the EM algorithm. Applied Statistics, 28:20-28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dempster</author>
<author>N Laird</author>
<author>D Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<volume>39</volume>
<pages>1--38</pages>
<contexts>
<context position="12145" citStr="Dempster et al., 1977" startWordPosition="1981" endWordPosition="1984"> A+3 is the model for independence and Ai3 is the symmetric interaction term. Intuitively, Aii represents the difference between the actual counts and those predicted by independence. This model can be evaluated using CoCo as described on pages 289-290 of Bishop et al. (1975). 3.2 Producing Bias-Corrected Tags We use the latent class model to correct symmetric disagreements that appear to result from bias. The latent class model was first introduced by Lazarsfeld (1966) and was later made computationally efficient by Goodman (1974). Goodman&apos;s procedure is a specialization of the EM algorithm (Dempster et al., 1977), which is implemented in the freeware program CoCo (Badsberg, 1995). Since its development, the latent class model has been widely applied, and is the underlying model in various unsupervised machine learning algorithms, including AutoClass (Cheeseman and Stutz, 1996). The form of the latent class model is that of naive Bayes: the observed variables are all conditionally independent of one another, given the value of the latent variable. The latent variable represents the true state of the object, and is the source of the correlations among the observed variables. As applied here, the observe</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. Dempster, N. Laird, and D. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39 (Series B):1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--1</pages>
<contexts>
<context position="10201" citStr="Dunning, 1993" startWordPosition="1643" endWordPosition="1644">correlations among the judges&apos; observations. The remainder of this section describes these models in more detail. All models can be evaluated using the freeware package CoCo, which was developed by Badsberg (1995) and is available at: http: / /web.math.auc.dkr jhb/CoCo. 3.1 Patterns of Disagreement A probability model enforces constraints on the counts in the data. The degree to which the counts in the data conform to the constraints is called the fit of the model. In this work, model fit is reported in terms of the likelihood ratio statistic, G2, and its significance (Read and Cressie, 1988; Dunning, 1993). The higher the G2 value, the poorer the fit. We will consider model fit to be acceptable if its reference significance level is greater than 0.01 (i.e., if there is greater than a 0.01 probability that the data sample was randomly selected from a population described by the model). Bias of one judge relative to another is evidenced as a discrepancy between the marginal totals for the two judges (i.e., ni+ and n+j in Table 1). Bias is measured by testing the fit of the model for marginal homogeneity: = 25+i for all i. The larger the G2 value, the greater the bias. The fit of the model can be </context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>T. Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):75-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Goodman</author>
</authors>
<title>Exploratory latent structure analysis using both identifiable and unidentifiable models.</title>
<date>1974</date>
<journal>Biometrika,</journal>
<pages>61--2</pages>
<contexts>
<context position="12060" citStr="Goodman (1974)" startWordPosition="1970" endWordPosition="1971">etric table, nii = Ai+ x A+i x Aii, such that Aij = ii.In this formula, Ai+ x A+3 is the model for independence and Ai3 is the symmetric interaction term. Intuitively, Aii represents the difference between the actual counts and those predicted by independence. This model can be evaluated using CoCo as described on pages 289-290 of Bishop et al. (1975). 3.2 Producing Bias-Corrected Tags We use the latent class model to correct symmetric disagreements that appear to result from bias. The latent class model was first introduced by Lazarsfeld (1966) and was later made computationally efficient by Goodman (1974). Goodman&apos;s procedure is a specialization of the EM algorithm (Dempster et al., 1977), which is implemented in the freeware program CoCo (Badsberg, 1995). Since its development, the latent class model has been widely applied, and is the underlying model in various unsupervised machine learning algorithms, including AutoClass (Cheeseman and Stutz, 1996). The form of the latent class model is that of naive Bayes: the observed variables are all conditionally independent of one another, given the value of the latent variable. The latent variable represents the true state of the object, and is the </context>
<context position="13464" citStr="Goodman (1974)" startWordPosition="2205" endWordPosition="2206">t L be the latent variable. Then, the latent class model is: p(b, d, j, m , 1) = P(bil)Adil)P(j11)P(mil)P(1) (by C.I. assumptions) p(b, Op(d,l)p(j , Op(m, 1) p(1)3 (by definition) The parameters of the model are {p(b, 1) , p(d, 1), p(j , 1) , p(m, 1)p(1)} . Once estimates of these parameters are obtained, each clause can be assigned the most probable latent category given the tags assigned by the judges. The EM algorithm takes as input the number of latent categories hypothesized, i.e., the number of values of L, and produces estimates of the parameters. For a description of this process, see Goodman (1974), Dawid &amp; Skene (1979), or Pedersen &amp; Bruce (1998). Three versions of the latent class model are considered in this study, one with two latent categories, one with three latent categories, and one with four. We apply these models to three data configurations: one with two categories (subjective and objective with no certainty ratings), one with four categories (subjective and objective with coarse-grained certainty ratings, as shown in Table 1), and one with eight categories (subjective and objective with fine-grained certainty ratings). All combinations of model and data configuration are eva</context>
</contexts>
<marker>Goodman, 1974</marker>
<rawString>L. Goodman. 1974. Exploratory latent structure analysis using both identifiable and unidentifiable models. Biometrika, 61:2:215-231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Hatzivassiloglou</author>
<author>K McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<journal>In A CL-E A CL</journal>
<pages>174--181</pages>
<location>Madrid, Spain,</location>
<contexts>
<context position="21801" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="3639" endWordPosition="3642">adjective, a cardinal number, a modal other than will, and an adverb other than not. We also include a binary feature representing whether or not the sentence begins a new paragraph. Finally, a feature is included representing co-occurrence of word tokens and punctuation marks with the subjective and objective classification.4 There are many other features to investigate in future work, such as features based on tags assigned to previous utterances (see, e.g., (Wiebe et al., 1997; Samuel et al., 1998)), and features based on semantic classes, such as positive and negative polarity adjectives (Hatzivassiloglou and McKeown, 1997) and reporting verbs (Bergler, 1992). The data consists of the concatenation of the two corpora annotated with bias-corrected tags as described above. The baseline accuracy, i.e., the frequency of the more frequent class, is only 51%. The results of the experiments are very promising. The average accuracy across all folds is 72.17%, more than 20 percentage points higher than the baseline accuracy. Interestingly, the system performs better on the sentences for which the judges are certain. In a post hoc analysis, we consider the sentences from the second data set for which judges M, J, and D ra</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>V. Hatzivassiloglou and K. McKeown. 1997. Predicting the semantic orientation of adjectives. In A CL-E A CL 1997, pages 174-181, Madrid, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
</authors>
<title>Generating Natural Language under Pragmatic Constraints.</title>
<date>1987</date>
<tech>Ph.D. thesis,</tech>
<institution>Yale University.</institution>
<contexts>
<context position="6322" citStr="Hovy, 1987" startWordPosition="969" endWordPosition="970">ng from 0, for least certain, to 3, for most certain. As discussed below in section 3.2, the certainty ratings allow us to investigate whether a model positing additional categories provides a better description of the judges&apos; annotations than a binary model does. Subjective and objective categories are potentially important for many text processing applications, such as information extraction and information retrieval, where the evidential status of information is important. In generation and machine translation, it is desirable to generate text that is appropriately subjective or objective (Hovy, 1987). In summarization, subjectivity judgments could be included in document profiles, to augment automatically produced document summaries, and to help the user make relevance judgments when using a search engine. In addition, they would be useful in text categorization. In related work (Wiebe et al., in preparation), we found that article types, such as announcement and opinion piece, are significantly correlated with the subjective and objective classification. Our subjective category is related to but differs from the statement-opinion category of the Switchboard-DAMSL discourse annotation pro</context>
</contexts>
<marker>Hovy, 1987</marker>
<rawString>Eduard Hovy. 1987. Generating Natural Language under Pragmatic Constraints. Ph.D. thesis, Yale University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>E Shriberg</author>
<author>D Blase&amp;</author>
</authors>
<title>Switchboard SWBD-DAMSL shallowdiscourse-function annotation coders manual, draft 13.</title>
<date>1997</date>
<tech>Technical Report 97-01,</tech>
<institution>University of Colorado Institute of Cognitive Science.</institution>
<marker>Jurafsky, Shriberg, Blase&amp;, 1997</marker>
<rawString>D. Jurafsky, E. Shriberg, and D. Blase&amp; 1997. Switchboard SWBD-DAMSL shallowdiscourse-function annotation coders manual, draft 13. Technical Report 97-01, University of Colorado Institute of Cognitive Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-Y Kan</author>
<author>J L Klavans</author>
<author>K R McKeown</author>
</authors>
<title>Linear segmentation and segment significance.</title>
<date>1998</date>
<booktitle>In Proc. 6th Workshop on Very Large Corpora (WVLC-98),</booktitle>
<pages>197--205</pages>
<publisher>ACL SIGDAT.</publisher>
<location>Montreal, Canada,</location>
<contexts>
<context position="3421" citStr="Kan et al., 1998" startWordPosition="516" endWordPosition="519">uce the bias-corrected tags (in section 3), the case study of improving intercoder agreement (in section 4), and the results of the classifier for automatic subjectivity tagging (in section 5). 2 The Subjective and Objective Categories We address evidentiality in text (Chafe, 1986), which concerns issues such as what is the source of information, and whether information is being presented as fact or opinion. These questions are particularly important in news reporting, in which segments presenting opinions and verbal reactions are mixed with segments presenting objective fact (van Dijk, 1988; Kan et al., 1998). The definitions of the categories in our cod246 ing manual are intention-based: &amp;quot;If the primary intention of a sentence is objective presentation of material that is factual to the reporter, the sentence is objective. Otherwise, the sentence is subjective.&amp;quot;&apos; We focus on sentences about private states, such as belief, knowledge, emotions, etc. (Quirk et al., 1985), and sentences about speech events, such as speaking and writing. Such sentences may be either subjective or objective. From the coding manual: &amp;quot;Subjective speech-event (and private-state) sentences are used to communicate the speak</context>
</contexts>
<marker>Kan, Klavans, McKeown, 1998</marker>
<rawString>M.-Y. Kan, J. L. Klavans, and K. R. McKeown. 1998. Linear segmentation and segment significance. In Proc. 6th Workshop on Very Large Corpora (WVLC-98), pages 197-205, Montreal, Canada, August. ACL SIGDAT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Krippendorf</author>
</authors>
<title>Content Analysis: An Introduction to its Methodology. Sage Publications,</title>
<date>1980</date>
<location>Beverly Hills.</location>
<marker>Krippendorf, 1980</marker>
<rawString>K. Krippendorf. 1980. Content Analysis: An Introduction to its Methodology. Sage Publications, Beverly Hills.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Lazarsfeld</author>
</authors>
<title>Latent structure analysis.</title>
<date>1966</date>
<booktitle>Measurement and Prediction.</booktitle>
<editor>In S. A. Stouffer, L. Guttman, E. Suchman, P.Lazarfeld, S. Star, and J. Claussen, editors,</editor>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="11997" citStr="Lazarsfeld (1966)" startWordPosition="1961" endWordPosition="1962"> counts are the product of a 248 table for independence and a symmetric table, nii = Ai+ x A+i x Aii, such that Aij = ii.In this formula, Ai+ x A+3 is the model for independence and Ai3 is the symmetric interaction term. Intuitively, Aii represents the difference between the actual counts and those predicted by independence. This model can be evaluated using CoCo as described on pages 289-290 of Bishop et al. (1975). 3.2 Producing Bias-Corrected Tags We use the latent class model to correct symmetric disagreements that appear to result from bias. The latent class model was first introduced by Lazarsfeld (1966) and was later made computationally efficient by Goodman (1974). Goodman&apos;s procedure is a specialization of the EM algorithm (Dempster et al., 1977), which is implemented in the freeware program CoCo (Badsberg, 1995). Since its development, the latent class model has been widely applied, and is the underlying model in various unsupervised machine learning algorithms, including AutoClass (Cheeseman and Stutz, 1996). The form of the latent class model is that of naive Bayes: the observed variables are all conditionally independent of one another, given the value of the latent variable. The laten</context>
</contexts>
<marker>Lazarsfeld, 1966</marker>
<rawString>P. Lazarsfeld. 1966. Latent structure analysis. In S. A. Stouffer, L. Guttman, E. Suchman, P.Lazarfeld, S. Star, and J. Claussen, editors, Measurement and Prediction. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Litman</author>
</authors>
<title>Cue phrase classification using machine learning.</title>
<date>1996</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>5--53</pages>
<contexts>
<context position="19812" citStr="Litman, 1996" startWordPosition="3320" endWordPosition="3321">y et al., 1997). In step 6, as in step 3, there is strong evidence of relative bias among judges D, J and M. Each pairwise comparison of judges also shows a strong pattern of symmetric disagreement. The results of this analysis are presented in Table 3.3 Also as in step 3, the two-category latent class model produces the most consistent clusters across the data configurations. Thus, it is used to define the bias-corrected tags for the second data set as well. 5 Machine Learning Results Recently, there have been many successful applications of machine learning to discourse processing, such as (Litman, 1996; Samuel et al., 1998). In this section, we report the results of machine learning experiments, in which we develop probablistic classifiers to automatically perform the subjective and objective classification. In the method we use for developing classifiers (Bruce and Wiebe, 1999), a search is performed to find a probability model that captures important interdependencies among features. Because features can be dropped and added during search, the method also performs feature selection. In these experiments, the system considers naive Bayes, full independence, full interdependence, and models</context>
</contexts>
<marker>Litman, 1996</marker>
<rawString>D. Litman. 1996. Cue phrase classification using machine learning. Journal of Artificial Intelligence Research, 5:53-94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="15921" citStr="Marcus et al., 1993" startWordPosition="2613" endWordPosition="2616">arguing for his or her own tag in some cases. Based on the judges&apos; feedback, 22 of the 504 bias-corrected tags are changed, and a second draft of the coding manual is written. 5. A second corpus is annotated by the same four judges according to the new coding manual. Each spends about five hours. 6. The results of the second tagging experiment are analyzed using the methods described in section 3, and bias-corrected tags are produced for the second data set. Two disjoint corpora are used in steps 2 and 5, both consisting of complete articles taken from the Wall Street Journal Treebank Corpus (Marcus et al., 1993). In both corpora, judges assign tags to each non-compound sentence and to each conjunct of each compound sentence, 504 in the first corpus and 500 in the second. The segmentation of compound sentences was performed manually before the judges received the data. Judges J and B, the first two authors of this paper, are NLP researchers. Judge M is an undergraduate computer science student, and judge D has no background in computer science or linguistics. Judge J, with help from M, developed the original coding instructions, and Judge J directed the process in step 4. The analysis performed in ste</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, Santorini, B., and M. Marcinkiewicz. 1993. Building a large annotated corpus of English: The penn treebank. Computational Linguistics, 19(2):313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Rebecca Bruce</author>
</authors>
<title>Knowledge lean word-sense disambiguation.</title>
<date>1998</date>
<booktitle>In Proc. of the 15th National Conference on Artificial Intelligence (AAAI-98),</booktitle>
<location>Madison, Wisconsin,</location>
<contexts>
<context position="13514" citStr="Pedersen &amp; Bruce (1998)" startWordPosition="2212" endWordPosition="2215">ent class model is: p(b, d, j, m , 1) = P(bil)Adil)P(j11)P(mil)P(1) (by C.I. assumptions) p(b, Op(d,l)p(j , Op(m, 1) p(1)3 (by definition) The parameters of the model are {p(b, 1) , p(d, 1), p(j , 1) , p(m, 1)p(1)} . Once estimates of these parameters are obtained, each clause can be assigned the most probable latent category given the tags assigned by the judges. The EM algorithm takes as input the number of latent categories hypothesized, i.e., the number of values of L, and produces estimates of the parameters. For a description of this process, see Goodman (1974), Dawid &amp; Skene (1979), or Pedersen &amp; Bruce (1998). Three versions of the latent class model are considered in this study, one with two latent categories, one with three latent categories, and one with four. We apply these models to three data configurations: one with two categories (subjective and objective with no certainty ratings), one with four categories (subjective and objective with coarse-grained certainty ratings, as shown in Table 1), and one with eight categories (subjective and objective with fine-grained certainty ratings). All combinations of model and data configuration are evaluated, except the four-category latent class mode</context>
</contexts>
<marker>Pedersen, Bruce, 1998</marker>
<rawString>Ted Pedersen and Rebecca Bruce. 1998. Knowledge lean word-sense disambiguation. In Proc. of the 15th National Conference on Artificial Intelligence (AAAI-98), Madison, Wisconsin, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Quirk</author>
<author>S Greenbaum</author>
<author>G Leech</author>
<author>J Svartvik</author>
</authors>
<title>A Comprehensive Grammar of the English Language.</title>
<date>1985</date>
<publisher>Longman,</publisher>
<location>New York.</location>
<contexts>
<context position="3788" citStr="Quirk et al., 1985" startWordPosition="573" endWordPosition="576">rmation is being presented as fact or opinion. These questions are particularly important in news reporting, in which segments presenting opinions and verbal reactions are mixed with segments presenting objective fact (van Dijk, 1988; Kan et al., 1998). The definitions of the categories in our cod246 ing manual are intention-based: &amp;quot;If the primary intention of a sentence is objective presentation of material that is factual to the reporter, the sentence is objective. Otherwise, the sentence is subjective.&amp;quot;&apos; We focus on sentences about private states, such as belief, knowledge, emotions, etc. (Quirk et al., 1985), and sentences about speech events, such as speaking and writing. Such sentences may be either subjective or objective. From the coding manual: &amp;quot;Subjective speech-event (and private-state) sentences are used to communicate the speaker&apos;s evaluations, opinions, emotions, and speculations. The primary intention of objective speech-event (and privatestate) sentences, on the other hand, is to objectively communicate material that is factual to the reporter. The speaker, in these cases, is being used as a reliable source of information.&amp;quot; Following are examples of subjective and objective sentences:</context>
</contexts>
<marker>Quirk, Greenbaum, Leech, Svartvik, 1985</marker>
<rawString>R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985. A Comprehensive Grammar of the English Language. Longman, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Read</author>
<author>N Cressie</author>
</authors>
<title>Goodness-offit Statistics for Discrete Multivariate Data.</title>
<date>1988</date>
<publisher>Springer-Verlag Inc.,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="10185" citStr="Read and Cressie, 1988" startWordPosition="1639" endWordPosition="1642">variable to explain the correlations among the judges&apos; observations. The remainder of this section describes these models in more detail. All models can be evaluated using the freeware package CoCo, which was developed by Badsberg (1995) and is available at: http: / /web.math.auc.dkr jhb/CoCo. 3.1 Patterns of Disagreement A probability model enforces constraints on the counts in the data. The degree to which the counts in the data conform to the constraints is called the fit of the model. In this work, model fit is reported in terms of the likelihood ratio statistic, G2, and its significance (Read and Cressie, 1988; Dunning, 1993). The higher the G2 value, the poorer the fit. We will consider model fit to be acceptable if its reference significance level is greater than 0.01 (i.e., if there is greater than a 0.01 probability that the data sample was randomly selected from a population described by the model). Bias of one judge relative to another is evidenced as a discrepancy between the marginal totals for the two judges (i.e., ni+ and n+j in Table 1). Bias is measured by testing the fit of the model for marginal homogeneity: = 25+i for all i. The larger the G2 value, the greater the bias. The fit of t</context>
</contexts>
<marker>Read, Cressie, 1988</marker>
<rawString>T. Read and N. Cressie. 1988. Goodness-offit Statistics for Discrete Multivariate Data. Springer-Verlag Inc., New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Samuel</author>
<author>S Carberry</author>
<author>K VijayShanker</author>
</authors>
<title>Dialogue act tagging with transformation-based learning.</title>
<date>1998</date>
<booktitle>In Proc. COLING-ACL</booktitle>
<pages>1150--1156</pages>
<location>Montreal, Canada,</location>
<contexts>
<context position="19834" citStr="Samuel et al., 1998" startWordPosition="3322" endWordPosition="3325">). In step 6, as in step 3, there is strong evidence of relative bias among judges D, J and M. Each pairwise comparison of judges also shows a strong pattern of symmetric disagreement. The results of this analysis are presented in Table 3.3 Also as in step 3, the two-category latent class model produces the most consistent clusters across the data configurations. Thus, it is used to define the bias-corrected tags for the second data set as well. 5 Machine Learning Results Recently, there have been many successful applications of machine learning to discourse processing, such as (Litman, 1996; Samuel et al., 1998). In this section, we report the results of machine learning experiments, in which we develop probablistic classifiers to automatically perform the subjective and objective classification. In the method we use for developing classifiers (Bruce and Wiebe, 1999), a search is performed to find a probability model that captures important interdependencies among features. Because features can be dropped and added during search, the method also performs feature selection. In these experiments, the system considers naive Bayes, full independence, full interdependence, and models generated from those </context>
<context position="21672" citStr="Samuel et al., 1998" startWordPosition="3621" endWordPosition="3624"> each fold. A binary feature is included for each of the following: the presence in the sentence of a pronoun, an adjective, a cardinal number, a modal other than will, and an adverb other than not. We also include a binary feature representing whether or not the sentence begins a new paragraph. Finally, a feature is included representing co-occurrence of word tokens and punctuation marks with the subjective and objective classification.4 There are many other features to investigate in future work, such as features based on tags assigned to previous utterances (see, e.g., (Wiebe et al., 1997; Samuel et al., 1998)), and features based on semantic classes, such as positive and negative polarity adjectives (Hatzivassiloglou and McKeown, 1997) and reporting verbs (Bergler, 1992). The data consists of the concatenation of the two corpora annotated with bias-corrected tags as described above. The baseline accuracy, i.e., the frequency of the more frequent class, is only 51%. The results of the experiments are very promising. The average accuracy across all folds is 72.17%, more than 20 percentage points higher than the baseline accuracy. Interestingly, the system performs better on the sentences for which t</context>
</contexts>
<marker>Samuel, Carberry, VijayShanker, 1998</marker>
<rawString>K. Samuel, S. Carberry, and K. VijayShanker. 1998. Dialogue act tagging with transformation-based learning. In Proc. COLING-ACL 1998, pages 1150-1156, Montreal, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T A van Dijk</author>
</authors>
<title>News as Discourse. Lawrence Erlbaum,</title>
<date>1988</date>
<location>Hillsdale, NJ.</location>
<marker>van Dijk, 1988</marker>
<rawString>T.A. van Dijk. 1988. News as Discourse. Lawrence Erlbaum, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>R Bruce</author>
<author>L Duan</author>
</authors>
<title>Probabilistic event categorization.</title>
<date>1997</date>
<booktitle>In Proc. Recent Advances in Natural Language Processing (RANLP-97),</booktitle>
<pages>163--170</pages>
<location>Tsigov Chark, Bulgaria,</location>
<contexts>
<context position="21650" citStr="Wiebe et al., 1997" startWordPosition="3617" endWordPosition="3620">atures considered on each fold. A binary feature is included for each of the following: the presence in the sentence of a pronoun, an adjective, a cardinal number, a modal other than will, and an adverb other than not. We also include a binary feature representing whether or not the sentence begins a new paragraph. Finally, a feature is included representing co-occurrence of word tokens and punctuation marks with the subjective and objective classification.4 There are many other features to investigate in future work, such as features based on tags assigned to previous utterances (see, e.g., (Wiebe et al., 1997; Samuel et al., 1998)), and features based on semantic classes, such as positive and negative polarity adjectives (Hatzivassiloglou and McKeown, 1997) and reporting verbs (Bergler, 1992). The data consists of the concatenation of the two corpora annotated with bias-corrected tags as described above. The baseline accuracy, i.e., the frequency of the more frequent class, is only 51%. The results of the experiments are very promising. The average accuracy across all folds is 72.17%, more than 20 percentage points higher than the baseline accuracy. Interestingly, the system performs better on the</context>
</contexts>
<marker>Wiebe, Bruce, Duan, 1997</marker>
<rawString>J. Wiebe, R. Bruce, and L. Duan. 1997. Probabilistic event categorization. In Proc. Recent Advances in Natural Language Processing (RANLP-97), pages 163-170, Tsigov Chark, Bulgaria, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>K McKeever</author>
<author>R Bruce</author>
</authors>
<title>Mapping collocational properties into machine learning features.</title>
<date>1998</date>
<booktitle>In Proc. 6th Workshop on Very Large Corpora (WVLC-98),</booktitle>
<pages>225--233</pages>
<publisher>ACL SIGDAT.</publisher>
<location>Montreal, Canada,</location>
<contexts>
<context position="23013" citStr="Wiebe et al., 1998" startWordPosition="3838" endWordPosition="3841">nd D rate their certainty as 2 or 3. There are 299/500 such sentences. For each fold, we calculate the system&apos;s accuracy on the subset of the test set consisting of such sentences. The average accuracy of the subsets across folds is 81.5%. Taking human performance as an upper bound, the system has room for improvement. The average pairwise percentage agreement between D, J, and M and the bias-corrected tags in the entire data set is 89.5%, while the system&apos;s percentage agreement with the bias-corrected tags (i.e., its accuracy) is 72.17%. 4The per-class enumerated feature representation from (Wiebe et al., 1998) is used, with 60% as the conditional independence cutoff threshold. Test DIJ DIM JIM 251 6 Conclusion This paper demonstrates a procedure for automatically formulating a single best tag when there are multiple judges who disagree. The procedure is applicable to any tagging task in which the judges exhibit symmetric disagreement resulting from bias. We successfully use bias-corrected tags for two purposes: to guide a revision of the coding manual, and to develop an automatic classifier. The revision of the coding manual results in as much as a 16 point improvement in pairwise Kappa values, and</context>
</contexts>
<marker>Wiebe, McKeever, Bruce, 1998</marker>
<rawString>J. Wiebe, K. McKeever, and R. Bruce. 1998. Mapping collocational properties into machine learning features. In Proc. 6th Workshop on Very Large Corpora (WVLC-98), pages 225-233, Montreal, Canada, August. ACL SIGDAT.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Wiebe</author>
<author>J Klavans</author>
<author>M Y Kan</author>
</authors>
<title>in preparation. Verb profiles for subjectivity judgments and text classification.</title>
<marker>Wiebe, Klavans, Kan, </marker>
<rawString>J. Wiebe, J. Klavans, and M.Y. Kan. in preparation. Verb profiles for subjectivity judgments and text classification.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
</authors>
<title>Tracking point of view in narrative.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--2</pages>
<contexts>
<context position="5462" citStr="Wiebe, 1994" startWordPosition="835" endWordPosition="836">reedom Now&amp;quot; was &amp;quot;undesirable for broadcasting.&amp;quot; Subjective speech-event sentence. In sentence 4, there is no uncertainty or evaluation expressed toward the speaking event. Thus, from one point of view, one might have considered this sentence to be objective. However, the object of the sentence is not presented as material that is factual to the reporter, so the sentence is classified as subjective. Linguistic categorizations usually do not cover all instances perfectly. For example, sen1-The category specifications in the coding manual are based on our previous work on tracking point of view (Wiebe, 1994), which builds on Banfield&apos;s (1982) linguistic theory of subjectivity. tences may fall on the borderline between two categories. To allow for uncertainty in the annotation process, the specific tags used in this work include certainty ratings, ranging from 0, for least certain, to 3, for most certain. As discussed below in section 3.2, the certainty ratings allow us to investigate whether a model positing additional categories provides a better description of the judges&apos; annotations than a binary model does. Subjective and objective categories are potentially important for many text processing</context>
</contexts>
<marker>Wiebe, 1994</marker>
<rawString>J. Wiebe. 1994. Tracking point of view in narrative. Computational Linguistics, 20(2):233-287.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>