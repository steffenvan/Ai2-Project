<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000702">
<title confidence="0.993983">
Discriminative Sentence Compression with Soft Syntactic Evidence
</title>
<author confidence="0.977933">
Ryan McDonald
</author>
<affiliation confidence="0.9687535">
Department of Computer and Information Science
University of Pennsylvania
</affiliation>
<address confidence="0.857642">
Philadelphia, PA 19104
</address>
<email confidence="0.999232">
ryantm@cis.upenn.edu
</email>
<sectionHeader confidence="0.998602" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999892894736842">
We present a model for sentence com-
pression that uses a discriminative large-
margin learning framework coupled with
a novel feature set defined on compressed
bigrams as well as deep syntactic repre-
sentations provided by auxiliary depen-
dency and phrase-structure parsers. The
parsers are trained out-of-domain and con-
tain a significant amount of noise. We ar-
gue that the discriminative nature of the
learning algorithm allows the model to
learn weights relative to any noise in the
feature set to optimize compression ac-
curacy directly. This differs from cur-
rent state-of-the-art models (Knight and
Marcu, 2000) that treat noisy parse trees,
for both compressed and uncompressed
sentences, as gold standard when calculat-
ing model parameters.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99898469047619">
The ability to compress sentences grammatically
with minimal information loss is an important
problem in text summarization. Most summariza-
tion systems are evaluated on the amount of rele-
vant information retained as well as their compres-
sion rate. Thus, returning highly compressed, yet
informative, sentences allows summarization sys-
tems to return larger sets of sentences and increase
the overall amount of information extracted.
We focus on the particular instantiation of sen-
tence compression when the goal is to produce the
compressed version solely by removing words or
phrases from the original, which is the most com-
mon setting in the literature (Knight and Marcu,
2000; Riezler et al., 2003; Turner and Charniak,
2005). In this framework, the goal is to find the
shortest substring of the original sentence that con-
veys the most important aspects of the meaning.
We will work in a supervised learning setting and
assume as input a training set T=(xt,yt)|? |
t�1 of
original sentences xt and their compressions yt.
We use the Ziff-Davis corpus, which is a set of
1087 pairs of sentence/compression pairs. Fur-
thermore, we use the same 32 testing examples
from Knight and Marcu (2000) and the rest for
training, except that we hold out 20 sentences for
the purpose of development. A handful of sen-
tences occur twice but with different compres-
sions. We randomly select a single compression
for each unique sentence in order to create an un-
ambiguous training set. Examples from this data
set are given in Figure 1.
Formally, sentence compression aims to shorten
a sentence x = x1 ... xn into a substring y =
y1 ... ym, where yi E {x1, ... , xn}. We define
the function I(yi) E {1, ... , n} that maps word
yi in the compression to the index of the word in
the original sentence. Finally we include the con-
straint I(yi) &lt; I(yi+1), which forces each word
in x to occur at most once in the compression y.
Compressions are evaluated on three criteria,
</bodyText>
<listItem confidence="0.987691375">
1. Grammaticality: Compressed sentences
should be grammatical.
2. Importance: How much of the important in-
formation is retained from the original.
3. Compression rate: How much compression
took place. A compression rate of 65%
means the compressed sentence is 65% the
length of the original.
</listItem>
<bodyText confidence="0.896318">
Typically grammaticality and importance are
traded off with compression rate. The longer our
</bodyText>
<page confidence="0.985018">
297
</page>
<note confidence="0.743588">
The Reverse Engineer Tool is priced from $8,000 for a single user to $90,000 for a multiuser project site.
The Reverse Engineer Tool is available now and is priced on a site-licensing basis, ranging from $8,000 for a single user to $90,000 for a multiuser project site .
Design recovery tools read existing code and translate it into deli nitions and structured diagrams .
Essentially , design recovery tools read existing code and translate it into the language in which CASE is conversant – deli nitions and structured diagrams .
</note>
<figureCaption confidence="0.966742">
Figure 1: Two examples of compressed sentences from the Ziff-Davis corpus. The compressed version
and the original sentence are given.
</figureCaption>
<bodyText confidence="0.999937416666667">
compressions, the less likely we are to remove im-
portant words or phrases crucial to maintaining
grammaticality and the intended meaning.
The paper is organized as follows: Section 2
discusses previous approaches to sentence com-
pression. In particular, we discuss the advantages
and disadvantages of the models of Knight and
Marcu (2000). In Section 3 we present our dis-
criminative large-margin model for sentence com-
pression, including the learning framework and
an efficient decoding algorithm for searching the
space of compressions. We also show how to
extract a rich feature set that includes surface-
level bigram features of the compressed sentence,
dropped words and phrases from the original sen-
tence, and features over noisy dependency and
phrase-structure trees for the original sentence.
We argue that this rich feature set allows the
model to learn which words and phrases should
be dropped and which should remain in the com-
pression. Section 4 presents an experimental eval-
uation of our model compared to the models of
Knight and Marcu (2000) and finally Section 5
discusses some areas of future work.
</bodyText>
<sectionHeader confidence="0.993929" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999269">
Knight and Marcu (2000) first tackled this prob-
lem by presenting a generative noisy-channel
model and a discriminative tree-to-tree decision
tree model. The noisy-channel model defines the
problem as finding the compressed sentence with
maximum conditional probability
</bodyText>
<equation confidence="0.9948715">
y = arg max P(ylx) = argmax P(x|y)P(y)
Y Y
</equation>
<bodyText confidence="0.999814846153846">
P(y) is the source model, which is a PCFG plus
bigram language model. P(x|y) is the channel
model, the probability that the long sentence is an
expansion of the compressed sentence. To calcu-
late the channel model, both the original and com-
pressed versions of every sentence in the training
set are assigned a phrase-structure tree. Given a
tree for a long sentence x and compressed sen-
tence y, the channel probability is the product of
the probability for each transformation required if
the tree for y is to expand to the tree for x.
The tree-to-tree decision tree model looks to
rewrite the tree for x into a tree for y. The model
uses a shift-reduce-drop parsing algorithm that
starts with the sequence of words in x and the cor-
responding tree. The algorithm then either shifts
(considers new words and subtrees for x), reduces
(combines subtrees from x into possibly new tree
constructions) or drops (drops words and subtrees
from x) on each step of the algorithm. A decision
tree model is trained on a set of indicative features
for each type of action in the parser. These mod-
els are then combined in a greedy global search
algorithm to find a single compression.
Though both models of Knight and Marcu per-
form quite well, they do have their shortcomings.
The noisy-channel model uses a source model
that is trained on uncompressed sentences, even
though the source model is meant to represent the
probability of compressed sentences. The channel
model requires aligned parse trees for both com-
pressed and uncompressed sentences in the train-
ing set in order to calculate probability estimates.
These parses are provided from a parsing model
trained on out-of-domain data (the WSJ), which
can result in parse trees with many mistakes for
both the original and compressed versions. This
makes alignment difficult and the channel proba-
bility estimates unreliable as a result. On the other
hand, the decision tree model does not rely on the
trees to align and instead simply learns a tree-to-
tree transformation model to compress sentences.
The primary problem with this model is that most
of the model features encode properties related to
including or dropping constituents from the tree
with no encoding of bigram or trigram surface fea-
tures to promote grammaticality. As a result, the
model will sometimes return very short and un-
grammatical compressions.
Both models rely heavily on the output of a
noisy parser to calculate probability estimates for
the compression. We argue in the next section that
</bodyText>
<page confidence="0.995622">
298
</page>
<bodyText confidence="0.9999133125">
ideally, parse trees should be treated solely as a
source of evidence when making compression de-
cisions to be balanced with other evidence such as
that provided by the words themselves.
Recently Turner and Charniak (2005) presented
supervised and semi-supervised versions of the
Knight and Marcu noisy-channel model. The re-
sulting systems typically return informative and
grammatical sentences, however, they do so at the
cost of compression rate. Riezler et al. (2003)
present a discriminative sentence compressor over
the output of an LFG parser that is a packed rep-
resentation of possible compressions. Though this
model is highly likely to return grammatical com-
pressions, it required the training data be human
annotated with syntactic trees.
</bodyText>
<sectionHeader confidence="0.997259" genericHeader="method">
3 Discriminative Sentence Compression
</sectionHeader>
<bodyText confidence="0.999441230769231">
For the rest of the paper we use x = x1 ... xn
to indicate an uncompressed sentence and y =
y1 ... ym a compressed version of x, i.e., each yj
indicates the position in x of the jth word in the
compression. We always pad the sentence with
dummy start and end words, x1 = -START- and
xn = -END-, which are always included in the
compressed version (i.e. y1 = x1 and ym = xn).
In this section we described a discriminative on-
line learning approach to sentence compression,
the core of which is a decoding algorithm that
searches the entire space of compressions. Let the
score of a compression y for a sentence x as
</bodyText>
<equation confidence="0.656508">
s(x, y)
</equation>
<bodyText confidence="0.999620333333333">
In particular, we are going to factor this score us-
ing a first-order Markov assumption on the words
in the compressed sentence
</bodyText>
<equation confidence="0.9939745">
s(x, y) = � |y |s(x, I(yj−1), I(yj))
j=2
</equation>
<bodyText confidence="0.999979">
Finally, we define the score function to be the dot
product between a high dimensional feature repre-
sentation and a corresponding weight vector
</bodyText>
<equation confidence="0.97488">
s(x, y) = � |y |w · f(x, I(yj−1), I(yj))
j=2
</equation>
<bodyText confidence="0.999994125">
Note that this factorization will allow us to define
features over two adjacent words in the compres-
sion as well as the words in-between that were
dropped from the original sentence to create the
compression. We will show in Section 3.2 how
this factorization also allows us to include features
on dropped phrases and subtrees from both a de-
pendency and a phrase-structure parse of the orig-
inal sentence. Note that these features are meant
to capture the same information in both the source
and channel models of Knight and Marcu (2000).
However, here they are merely treated as evidence
for the discriminative learner, which will set the
weight of each feature relative to the other (pos-
sibly overlapping) features to optimize the models
accuracy on the observed data.
</bodyText>
<subsectionHeader confidence="0.998697">
3.1 Decoding
</subsectionHeader>
<bodyText confidence="0.9999765">
We define a dynamic programming table C[i]
which represents the highest score for any com-
pression that ends at word xi for sentence x. We
define a recurrence as follows
</bodyText>
<equation confidence="0.9979295">
C[1] = 0.0
C[i] = maxj&lt;i C[j] + s(x, j, i) for i &gt; 1
</equation>
<bodyText confidence="0.999965032258064">
It is easy to show that C[n] represents the score of
the best compression for sentence x (whose length
is n) under the first-order score factorization we
made. We can show this by induction. If we as-
sume that C[j] is the highest scoring compression
that ends at word xj, for all j &lt; i, then C[i] must
also be the highest scoring compression ending at
word xi since it represents the max combination
over all high scoring shorter compressions plus
the score of extending the compression to the cur-
rent word. Thus, since xn is by definition in every
compressed version of x (see above), then it must
be the case that C[n] stores the score of the best
compression. This table can be filled in O(n2).
This algorithm is really an extension of Viterbi
to the case when scores factor over dynamic sub-
strings of the text (Sarawagi and Cohen, 2004;
McDonald et al., 2005a). As such, we can use
back-pointers to reconstruct the highest scoring
compression as well as k-best decoding algo-
rithms.
This decoding algorithm is dynamic with re-
spect to compression rate. That is, the algorithm
will return the highest scoring compression re-
gardless of length. This may seem problematic
since longer compressions might contribute more
to the score (since they contain more bigrams) and
thus be preferred. However, in Section 3.2 we de-
fine a rich feature set, including features on words
dropped from the compression that will help disfa-
vor compressions that drop very few words since
</bodyText>
<page confidence="0.989044">
299
</page>
<bodyText confidence="0.999458727272727">
this is rarely seen in the training data. In fact,
it turns out that our learned compressions have a
compression rate very similar to the gold standard.
That said, there are some instances when a static
compression rate is preferred. A user may specif-
ically want a 25% compression rate for all sen-
tences. This is not a problem for our decoding
algorithm. We simply augment the dynamic pro-
gramming table and calculate C[i][r], which is the
score of the best compression of length r that ends
at word xi. This table can be filled in as follows
</bodyText>
<equation confidence="0.998145666666667">
C[1][1] = 0.0
C[1][r] = −∞ for r &gt; 1
C[i][r] = maxj&lt;i C[j][r − 1] + s(x, j, i) for i &gt; 1
</equation>
<bodyText confidence="0.99583725">
Thus, if we require a specific compression rate, we
simple determine the number of words r that sat-
isfy this rate and calculate C[n][r]. The new com-
plexity is O(n2r).
</bodyText>
<subsectionHeader confidence="0.970077">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.999384545454545">
So far we have defined the score of a compres-
sion as well as a decoding algorithm that searches
the entire space of compressions to find the one
with highest score. This all relies on a score fac-
torization over adjacent words in the compression,
s(x, I(yj−1), I(yj)) = w · f(x, I(yj−1), I(yj)).
In Section 3.3 we describe an online large-margin
method for learning w. Here we present the fea-
ture representation f(x, I(yj−1), I(yj)) for a pair
of adjacent words in the compression. These fea-
tures were tuned on a development data set.
</bodyText>
<sectionHeader confidence="0.555678" genericHeader="method">
3.2.1 Word/POS Features
</sectionHeader>
<bodyText confidence="0.99961">
The first set of features are over adjacent words
yj−1 and yj in the compression. These include
the part-of-speech (POS) bigrams for the pair, the
POS of each word individually, and the POS con-
text (bigram and trigram) of the most recent word
being added to the compression, yj. These fea-
tures are meant to indicate likely words to in-
clude in the compression as well as some level
of grammaticality, e.g., the adjacent POS features
“JJ&amp;VB” would get a low weight since we rarely
see an adjective followed by a verb. We also add a
feature indicating if yj−1 and yj were actually ad-
jacent in the original sentence or not and we con-
join this feature with the above POS features. Note
that we have not included any lexical features. We
found during experiments on the development data
that lexical information was too sparse and led to
overfitting, so we rarely include such features. In-
stead we rely on the accuracy of POS tags to pro-
vide enough evidence.
Next we added features over every dropped
word in the original sentence between yj−1 and yj,
if there were any. These include the POS of each
dropped word, the POS of the dropped words con-
joined with the POS of yj−1 and yj. If the dropped
word is a verb, we add a feature indicating the ac-
tual verb (this is for common verbs like “is”, which
are typically in compressions). Finally we add the
POS context (bigram and trigram) of each dropped
word. These features represent common charac-
teristics of words that can or should be dropped
from the original sentence in the compressed ver-
sion (e.g. adjectives and adverbs). We also add a
feature indicating whether the dropped word is a
negation (e.g., not, never, etc.).
We also have a set of features to represent
brackets in the text, which are common in the data
set. The first measures if all the dropped words
between yj−1 and yj have a mismatched or incon-
sistent bracketing. The second measures if the left
and right-most dropped words are themselves both
brackets. These features come in handy for ex-
amples like, The Associated Press ( AP ) reported
the story, where the compressed version is The
Associated Press reported the story. Information
within brackets is often redundant.
</bodyText>
<subsectionHeader confidence="0.741914">
3.2.2 Deep Syntactic Features
</subsectionHeader>
<bodyText confidence="0.999995695652174">
The previous set of features are meant to en-
code common POS contexts that are commonly re-
tained or dropped from the original sentence dur-
ing compression. However, they do so without a
larger picture of the function of each word in the
sentence. For instance, dropping verbs is not that
uncommon - a relative clause for instance may be
dropped during compression. However, dropping
the main verb in the sentence is uncommon, since
that verb and its arguments typically encode most
of the information being conveyed.
An obvious solution to this problem is to in-
clude features over a deep syntactic analysis of
the sentence. To do this we parse every sentence
twice, once with a dependency parser (McDon-
ald et al., 2005b) and once with a phrase-structure
parser (Charniak, 2000). These parsers have been
trained out-of-domain on the Penn WSJ Treebank
and as a result contain noise. However, we are
merely going to use them as an additional source
of features. We call this soft syntactic evidence
since the deep trees are not used as a strict gold-
standard in our model but just as more evidence for
</bodyText>
<page confidence="0.949377">
300
</page>
<figure confidence="0.656319083333333">
S
VP
NP NP
PP PP
NP
NNP VBD NNP IN NNP IN NN
Mary, saw2 Ralph3 ono Tuesday, afters lunch?
NP
rooto
saw2
on� afters
Mary, Ralph3 Tuesday, lunch?
</figure>
<figureCaption confidence="0.836289333333333">
Figure 2: An example dependency tree from the McDonald et al. (2005b) parser and phrase structure
tree from the Charniak (2000) parser. In this example we want to add features from the trees for the case
when Ralph and after become adjacent in the compression, i.e., we are dropping the phrase on Tuesday.
</figureCaption>
<bodyText confidence="0.999872717391304">
or against particular compressions. The learning
algorithm will set the feature weight accordingly
depending on each features discriminative power.
It is not unique to use soft syntactic features in
this way, as it has been done for many problems
in language processing. However, we stress this
aspect of our model due to the history of compres-
sion systems using syntax to provide hard struc-
tural constraints on the output.
Lets consider the sentence x = Mary saw Ralph
on Tuesday after lunch, with corresponding parses
given in Figure 2. In particular, lets consider the
feature representation f(x,3,6). That is, the fea-
ture representation of making Ralph and after ad-
jacent in the compression and dropping the prepo-
sitional phrase on Tuesday. The first set of features
we consider are over dependency trees. For every
dropped word we add a feature indicating the POS
of the words parent in the tree. For example, if
the dropped words parent is root, then it typically
means it is the main verb of the sentence and un-
likely to be dropped. We also add a conjunction
feature of the POS tag of the word being dropped
and the POS of its parent as well as a feature in-
dicating for each word being dropped whether it
is a leaf node in the tree. We also add the same
features for the two adjacent words, but indicating
that they are part of the compression.
For the phrase-structure features we find every
node in the tree that subsumes a piece of dropped
text and is not a child of a similar node. In this case
the PP governing on Tuesday. We then add fea-
tures indicating the context from which this node
was dropped. For example we add a feature spec-
ifying that a PP was dropped which was the child
of a VP. We also add a feature indicating that a PP
was dropped which was the left sibling of another
PP, etc. Ideally, for each production in the tree we
would like to add a feature indicating every node
that was dropped, e.g. “VP→VBD NP PP PP ⇒
VP→VBD NP PP”. However, we cannot neces-
sarily calculate this feature since the extent of the
production might be well beyond the local context
of first-order feature factorization. Furthermore,
since the training set is so small, these features are
likely to be observed very few times.
</bodyText>
<subsectionHeader confidence="0.766015">
3.2.3 Feature Set Summary
</subsectionHeader>
<bodyText confidence="0.999994294117647">
In this section we have described a rich feature
set over adjacent words in the compressed sen-
tence, dropped words and phrases from the origi-
nal sentence, and properties of deep syntactic trees
of the original sentence. Note that these features in
many ways mimic the information already present
in the noisy-channel and decision-tree models of
Knight and Marcu (2000). Our bigram features
encode properties that indicate both good and bad
words to be adjacent in the compressed sentence.
This is similar in purpose to the source model from
the noisy-channel system. However, in that sys-
tem, the source model is trained on uncompressed
sentences and thus is not as representative of likely
bigram features for compressed sentences, which
is really what we desire.
Our feature set also encodes dropped words
and phrases through the properties of the words
themselves and through properties of their syntac-
tic relation to the rest of the sentence in a parse
tree. These features represent likely phrases to be
dropped in the compression and are thus similar in
nature to the channel model in the noisy-channel
system as well as the features in the tree-to-tree de-
cision tree system. However, we use these syntac-
tic constraints as soft evidence in our model. That
is, they represent just another layer of evidence to
be considered during training when setting param-
eters. Thus, if the parses have too much noise,
the learning algorithm can lower the weight of the
parse features since they are unlikely to be use-
ful discriminators on the training data. This dif-
fers from the models of Knight and Marcu (2000),
which treat the noisy parses as gold-standard when
</bodyText>
<page confidence="0.997617">
301
</page>
<bodyText confidence="0.999367333333333">
calculating probability estimates.
An important distinction we should make is the
notion of supported versus unsupported features
(Sha and Pereira, 2003). Supported features are
those that are on for the gold standard compres-
sions in the training. For instance, the bigram fea-
ture “NN&amp;VB” will be supported since there is
most likely a compression that contains a adjacent
noun and verb. However, the feature “JJ&amp;VB”
will not be supported since an adjacent adjective
and verb most likely will not be observed in any
valid compression. Our model includes all fea-
tures, including those that are unsupported. The
advantage of this is that the model can learn nega-
tive weights for features that are indicative of bad
compressions. This is not difficult to do since most
features are POS based and the feature set size
even with all these features is only 78,923.
</bodyText>
<subsectionHeader confidence="0.999018">
3.3 Learning
</subsectionHeader>
<bodyText confidence="0.99997664516129">
Having defined a feature encoding and decod-
ing algorithm, the last step is to learn the fea-
ture weights w. We do this using the Margin
Infused Relaxed Algorithm (MIRA), which is a
discriminative large-margin online learning tech-
nique shown in Figure 3 (Crammer and Singer,
2003). On each iteration, MIRA considers a single
instance from the training set (xt, yt) and updates
the weights so that the score of the correct com-
pression, yt, is greater than the score of all other
compressions by a margin proportional to their
loss. Many weight vectors will satisfy these con-
straints so we pick the one with minimum change
from the previous setting. We define the loss to be
the number of words falsely retained or dropped
in the incorrect compression relative to the correct
one. For instance, if the correct compression of the
sentence in Figure 2 is Mary saw Ralph, then the
compression Mary saw after lunch would have a
loss of 3 since it incorrectly left out one word and
included two others.
Of course, for a sentence there are exponentially
many possible compressions, which means that
this optimization will have exponentially many
constraints. We follow the method of McDon-
ald et al. (2005b) and create constraints only on
the k compressions that currently have the high-
est score, bestk(x; w). This can easily be calcu-
lated by extending the decoding algorithm with
standard Viterbi k-best techniques. On the devel-
opment data, we found that k = 10 provided the
</bodyText>
<equation confidence="0.3707">
Training data: T = {(xt, yt)}Tt=1
1. w0 = 0; v = 0; i = 0
</equation>
<listItem confidence="0.885385">
2. for n : 1..N
3. fort : 1..T
</listItem>
<figure confidence="0.763776714285714">
‚ w(i+1) − w(i)‚
4. min ‚ ‚‚‚
s.t. s(xt, yt) − s(xt, y�) &gt; L(yt, y�)
where y&apos; E bestk(x;w(i))
5. v = v + w(i+1)
6. i = i + 1
7.w=v/(N*T)
</figure>
<figureCaption confidence="0.9905655">
Figure 3: MIRA learning algorithm as presented
by McDonald et al. (2005b).
</figureCaption>
<bodyText confidence="0.9992007">
best performance, though varying k did not have a
major impact overall. Furthermore we found that
after only 3-5 training epochs performance on the
development data was maximized.
The final weight vector is the average of all
weight vectors throughout training. Averaging has
been shown to reduce overfitting (Collins, 2002)
as well as reliance on the order of the examples
during training. We found it to be particularly im-
portant for this data set.
</bodyText>
<sectionHeader confidence="0.998971" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.979916961538462">
We use the same experimental methodology as
Knight and Marcu (2000). We provide every com-
pression to four judges and ask them to evaluate
each one for grammaticality and importance on a
scale from 1 to 5. For each of the 32 sentences in
our test set we ask the judges to evaluate three sys-
tems: human annotated, the decision tree model
of Knight and Marcu (2000) and our system. The
judges were told all three compressions were au-
tomatically generated and the order in which they
were presented was randomly chosen for each sen-
tence. We compared our system to the decision
tree model of Knight and Marcu instead of the
noisy-channel model since both performed nearly
as well in their evaluation, and the compression
rate of the decision tree model is nearer to our sys-
tem (around 57-58%). The noisy-channel model
typically returned longer compressions.
Results are shown in Table 1. We present the av-
erage score over all judges as well as the standard
deviation. The evaluation for the decision tree sys-
tem of Knight and Marcu is strikingly similar to
the original evaluation in their work. This provides
strong evidence that the evaluation criteria in both
cases were very similar.
Table 1 shows that all models had similar com-
</bodyText>
<page confidence="0.988517">
302
</page>
<table confidence="0.910898571428571">
Human
Decision-Tree (K&amp;M2000)
This work
Compression Rate Grammaticality Importance
53.3% 4.96 ± 0.2 3.91 ± 1.0
57.2% 4.30 ± 1.4 3.60 ± 1.3
58.1% 4.61 ± 0.8 4.03 ± 1.0
</table>
<tableCaption confidence="0.999939">
Table 1: Compression results.
</tableCaption>
<bodyText confidence="0.996773048780488">
pressions rates, with humans preferring to com-
press a little more aggressively. Not surprisingly,
the human compressions are practically all gram-
matical. A quick scan of the evaluations shows
that the few ungrammatical human compressions
were for sentences that were not really gram-
matical in the first place. Of greater interest is
that the compressions of our system are typically
more grammatical than the decision tree model of
Knight and Marcu.
When looking at importance, we see that our
system actually does the best – even better than
humans. The most likely reason for this is that
our model returns longer sentences and is thus less
likely to prune away important information. For
example, consider the sentence
The chemical etching process used for glare protection is
effective and will help if your office has the fluorescent-light
overkill that’s typical in offices
The human compression was Glare protection is
effective, whereas our model compressed the sen-
tence to The chemical etching process used for
glare protection is effective.
A primary reason that our model does better
than the decision tree model of Knight and Marcu
is that on a handful of sentences, the decision tree
compressions were a single word or noun-phrase.
For such sentences the evaluators typically rated
the compression a 1 for both grammaticality and
importance. In contrast, our model never failed
in such drastic ways and always output something
reasonable. This is quantified in the standard de-
viation of the two systems.
Though these results are promising, more large
scale experiments are required to really ascer-
tain the significance of the performance increase.
Ideally we could sample multiple training/testing
splits and use all sentences in the data set to eval-
uate the systems. However, since these systems
require human evaluation we did not have the time
or the resources to conduct these experiments.
</bodyText>
<subsectionHeader confidence="0.987799">
4.1 Some Examples
</subsectionHeader>
<bodyText confidence="0.999994212121212">
Here we aim to give the reader a flavor of some
common outputs from the different models. Three
examples are given in Table 4.1. The first shows
two properties. First of all, the decision tree
model completely breaks and just returns a sin-
gle noun-phrase. Our system performs well, how-
ever it leaves out the complementizer of the rela-
tive clause. This actually occurred in a few exam-
ples and appears to be the most common problem
of our model. A post-processing rule should elim-
inate this.
The second example displays a case in which
our system and the human system are grammati-
cal, but the removal of a prepositional phrase hurts
the resulting meaning of the sentence. In fact,
without the knowledge that the sentence is refer-
ring to broadband, the compressions are mean-
ingless. This appears to be a harder problem –
determining which prepositional phrases can be
dropped and which cannot.
The final, and more interesting, example
presents two very different compressions by the
human and our automatic system. Here, the hu-
man kept the relative clause relating what lan-
guages the source code is available in, but dropped
the main verb phrase of the sentence. Our model
preferred to retain the main verb phrase and drop
the relative clause. This is most likely due to the
fact that dropping the main verb phrase of a sen-
tence is much less likely in the training data than
dropping a relative clause. Two out of four eval-
uators preferred the compression returned by our
system and the other two rated them equal.
</bodyText>
<sectionHeader confidence="0.999709" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.990390142857143">
In this paper we have described a new system for
sentence compression. This system uses discrim-
inative large-margin learning techniques coupled
with a decoding algorithm that searches the space
of all compressions. In addition we defined a
rich feature set of bigrams in the compression and
dropped words and phrases from the original sen-
tence. The model also incorporates soft syntactic
evidence in the form of features over dependency
and phrase-structure trees for each sentence.
This system has many advantages over previous
approaches. First of all its discriminative nature
allows us to use a rich dependent feature set and
to optimize a function directly related to compres-
</bodyText>
<page confidence="0.996653">
303
</page>
<figure confidence="0.978497083333333">
Full Sentence
Human
Decision Tree
This work
Full Sentence
Human
Decision Tree
This work
Full Sentence
Human
Decision Tree
This work
</figure>
<bodyText confidence="0.985856307692308">
The fi rst new product, ATF Protype , is a line of digital postscript typefaces that will be sold in packages of up to six fonts.
ATF Protype is a line of digital postscript typefaces that will be sold in packages of up to six fonts .
The fi rst new product .
ATF Protype is a line of digital postscript typefaces will be sold in packages of up to six fonts .
Finally, another advantage of broadband is distance.
Another advantage is distance.
Another advantage of broadband is distance.
Another advantage is distance.
The source code , which is available for C , Fortran , ADA and VHDL , can be compiled and executed on the same system or ported to other
target platforms .
The source code is available for C , Fortran , ADA and VHDL .
The source code is available for C .
The source code can be compiled and executed on the same system or ported to other target platforms .
</bodyText>
<tableCaption confidence="0.980001">
Table 2: Example compressions for the evaluation data.
</tableCaption>
<bodyText confidence="0.999466305555555">
sion accuracy during training, both of which have
been shown to be beneficial for other problems.
Furthermore, the system does not rely on the syn-
tactic parses of the sentences to calculate probabil-
ity estimates. Instead, this information is incorpo-
rated as just another form of evidence to be consid-
ered during training. This is advantageous because
these parses are trained on out-of-domain data and
often contain a significant amount of noise.
A fundamental flaw with all sentence compres-
sion systems is that model parameters are set with
the assumption that there is a single correct answer
for each sentence. Of course, like most compres-
sion and translation tasks, this is not true, consider,
TapeWare , which supports DOS and NetWare 286 , is a
value-added process that lets you directly connect the
QA150-EXAT to a file server and issue a command from any
workstation to back up the server
The human annotated compression is, TapeWare
supports DOS and NetWare 286. However, an-
other completely valid compression might be,
TapeWare lets you connect the QA150-EXAT to a
fi le server. These two compressions overlap by a
single word.
Our learning algorithm may unnecessarily
lower the score of some perfectly valid compres-
sions just because they were not the exact com-
pression chosen by the human annotator. A pos-
sible direction of research is to investigate multi-
label learning techniques for structured data (Mc-
Donald et al., 2005a) that learn a scoring function
separating a set of valid answers from all invalid
answers. Thus if a sentence has multiple valid
compressions we can learn to score each valid one
higher than all invalid compressions during train-
ing to avoid this problem.
</bodyText>
<sectionHeader confidence="0.998949" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999953875">
The author would like to thank Daniel Marcu for
providing the data as well as the output of his
and Kevin Knight’s systems. Thanks also to Hal
Daum´e and Fernando Pereira for useful discus-
sions. Finally, the author thanks the four review-
ers for evaluating the compressed sentences. This
work was supported by NSF ITR grants 0205448
and 0428193.
</bodyText>
<sectionHeader confidence="0.999441" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999792566666667">
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. NAACL.
M. Collins. 2002. Discriminative training methods
for hidden Markov models: Theory and experiments
with perceptron algorithms. In Proc. EMNLP.
K. Crammer and Y. Singer. 2003. Ultraconservative
online algorithms for multiclass problems. JMLR.
K. Knight and D. Marcu. 2000. Statistical-based sum-
marization - step one: Sentence compression. In
Proc. AAAI 2000.
R. McDonald, K. Crammer, and F. Pereira. 2005a.
Flexible text segmentation with structured multil-
abel classifi cation. In Proc. HLT-EMNLP.
R. McDonald, K. Crammer, and F. Pereira. 2005b. On-
line large-margin training of dependency parsers. In
Proc. ACL.
S. Riezler, T. H. King, R. Crouch, and A. Zaenen.
2003. Statistical sentence condensation using ambi-
guity packing and stochastic disambiguation meth-
ods for lexical-functional grammar. In Proc. HLT-
NAACL.
S. Sarawagi and W. Cohen. 2004. Semi-Markov con-
ditional random fi elds for information extraction. In
Proc. NIPS.
F. Sha and F. Pereira. 2003. Shallow parsing with con-
ditional random fi elds. In Proc. HLT-NAACL, pages
213–220.
J. Turner and E. Charniak. 2005. Supervised and un-
supervised learning for sentence compression. In
Proc. ACL.
</reference>
<page confidence="0.999327">
304
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.970177">
<title confidence="0.999164">Discriminative Sentence Compression with Soft Syntactic Evidence</title>
<author confidence="0.999997">Ryan McDonald</author>
<affiliation confidence="0.999907">Department of Computer and Information Science University of Pennsylvania</affiliation>
<address confidence="0.999405">Philadelphia, PA 19104</address>
<email confidence="0.999647">ryantm@cis.upenn.edu</email>
<abstract confidence="0.9985742">We present a model for sentence compression that uses a discriminative largemargin learning framework coupled with a novel feature set defined on compressed bigrams as well as deep syntactic representations provided by auxiliary dependency and phrase-structure parsers. The parsers are trained out-of-domain and contain a significant amount of noise. We argue that the discriminative nature of the learning algorithm allows the model to learn weights relative to any noise in the feature set to optimize compression accuracy directly. This differs from current state-of-the-art models (Knight and Marcu, 2000) that treat noisy parse trees, for both compressed and uncompressed sentences, as gold standard when calculating model parameters.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proc. NAACL.</booktitle>
<contexts>
<context position="16524" citStr="Charniak, 2000" startWordPosition="2805" endWordPosition="2806">r, they do so without a larger picture of the function of each word in the sentence. For instance, dropping verbs is not that uncommon - a relative clause for instance may be dropped during compression. However, dropping the main verb in the sentence is uncommon, since that verb and its arguments typically encode most of the information being conveyed. An obvious solution to this problem is to include features over a deep syntactic analysis of the sentence. To do this we parse every sentence twice, once with a dependency parser (McDonald et al., 2005b) and once with a phrase-structure parser (Charniak, 2000). These parsers have been trained out-of-domain on the Penn WSJ Treebank and as a result contain noise. However, we are merely going to use them as an additional source of features. We call this soft syntactic evidence since the deep trees are not used as a strict goldstandard in our model but just as more evidence for 300 S VP NP NP PP PP NP NNP VBD NNP IN NNP IN NN Mary, saw2 Ralph3 ono Tuesday, afters lunch? NP rooto saw2 on� afters Mary, Ralph3 Tuesday, lunch? Figure 2: An example dependency tree from the McDonald et al. (2005b) parser and phrase structure tree from the Charniak (2000) par</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>E. Charniak. 2000. A maximum-entropy-inspired parser. In Proc. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc. EMNLP. K. Crammer</booktitle>
<publisher>JMLR.</publisher>
<contexts>
<context position="24178" citStr="Collins, 2002" startWordPosition="4145" endWordPosition="4146">ta: T = {(xt, yt)}Tt=1 1. w0 = 0; v = 0; i = 0 2. for n : 1..N 3. fort : 1..T ‚ w(i+1) − w(i)‚ 4. min ‚ ‚‚‚ s.t. s(xt, yt) − s(xt, y�) &gt; L(yt, y�) where y&apos; E bestk(x;w(i)) 5. v = v + w(i+1) 6. i = i + 1 7.w=v/(N*T) Figure 3: MIRA learning algorithm as presented by McDonald et al. (2005b). best performance, though varying k did not have a major impact overall. Furthermore we found that after only 3-5 training epochs performance on the development data was maximized. The final weight vector is the average of all weight vectors throughout training. Averaging has been shown to reduce overfitting (Collins, 2002) as well as reliance on the order of the examples during training. We found it to be particularly important for this data set. 4 Experiments We use the same experimental methodology as Knight and Marcu (2000). We provide every compression to four judges and ask them to evaluate each one for grammaticality and importance on a scale from 1 to 5. For each of the 32 sentences in our test set we ask the judges to evaluate three systems: human annotated, the decision tree model of Knight and Marcu (2000) and our system. The judges were told all three compressions were automatically generated and the</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proc. EMNLP. K. Crammer and Y. Singer. 2003. Ultraconservative online algorithms for multiclass problems. JMLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>Statistical-based summarization - step one: Sentence compression.</title>
<date>2000</date>
<booktitle>In Proc. AAAI</booktitle>
<contexts>
<context position="815" citStr="Knight and Marcu, 2000" startWordPosition="115" endWordPosition="118">.edu Abstract We present a model for sentence compression that uses a discriminative largemargin learning framework coupled with a novel feature set defined on compressed bigrams as well as deep syntactic representations provided by auxiliary dependency and phrase-structure parsers. The parsers are trained out-of-domain and contain a significant amount of noise. We argue that the discriminative nature of the learning algorithm allows the model to learn weights relative to any noise in the feature set to optimize compression accuracy directly. This differs from current state-of-the-art models (Knight and Marcu, 2000) that treat noisy parse trees, for both compressed and uncompressed sentences, as gold standard when calculating model parameters. 1 Introduction The ability to compress sentences grammatically with minimal information loss is an important problem in text summarization. Most summarization systems are evaluated on the amount of relevant information retained as well as their compression rate. Thus, returning highly compressed, yet informative, sentences allows summarization systems to return larger sets of sentences and increase the overall amount of information extracted. We focus on the partic</context>
<context position="2151" citStr="Knight and Marcu (2000)" startWordPosition="329" endWordPosition="332">words or phrases from the original, which is the most common setting in the literature (Knight and Marcu, 2000; Riezler et al., 2003; Turner and Charniak, 2005). In this framework, the goal is to find the shortest substring of the original sentence that conveys the most important aspects of the meaning. We will work in a supervised learning setting and assume as input a training set T=(xt,yt)|? | t�1 of original sentences xt and their compressions yt. We use the Ziff-Davis corpus, which is a set of 1087 pairs of sentence/compression pairs. Furthermore, we use the same 32 testing examples from Knight and Marcu (2000) and the rest for training, except that we hold out 20 sentences for the purpose of development. A handful of sentences occur twice but with different compressions. We randomly select a single compression for each unique sentence in order to create an unambiguous training set. Examples from this data set are given in Figure 1. Formally, sentence compression aims to shorten a sentence x = x1 ... xn into a substring y = y1 ... ym, where yi E {x1, ... , xn}. We define the function I(yi) E {1, ... , n} that maps word yi in the compression to the index of the word in the original sentence. Finally </context>
<context position="4305" citStr="Knight and Marcu (2000)" startWordPosition="693" endWordPosition="696">lly , design recovery tools read existing code and translate it into the language in which CASE is conversant – deli nitions and structured diagrams . Figure 1: Two examples of compressed sentences from the Ziff-Davis corpus. The compressed version and the original sentence are given. compressions, the less likely we are to remove important words or phrases crucial to maintaining grammaticality and the intended meaning. The paper is organized as follows: Section 2 discusses previous approaches to sentence compression. In particular, we discuss the advantages and disadvantages of the models of Knight and Marcu (2000). In Section 3 we present our discriminative large-margin model for sentence compression, including the learning framework and an efficient decoding algorithm for searching the space of compressions. We also show how to extract a rich feature set that includes surfacelevel bigram features of the compressed sentence, dropped words and phrases from the original sentence, and features over noisy dependency and phrase-structure trees for the original sentence. We argue that this rich feature set allows the model to learn which words and phrases should be dropped and which should remain in the comp</context>
<context position="10223" citStr="Knight and Marcu (2000)" startWordPosition="1689" endWordPosition="1692">ntation and a corresponding weight vector s(x, y) = � |y |w · f(x, I(yj−1), I(yj)) j=2 Note that this factorization will allow us to define features over two adjacent words in the compression as well as the words in-between that were dropped from the original sentence to create the compression. We will show in Section 3.2 how this factorization also allows us to include features on dropped phrases and subtrees from both a dependency and a phrase-structure parse of the original sentence. Note that these features are meant to capture the same information in both the source and channel models of Knight and Marcu (2000). However, here they are merely treated as evidence for the discriminative learner, which will set the weight of each feature relative to the other (possibly overlapping) features to optimize the models accuracy on the observed data. 3.1 Decoding We define a dynamic programming table C[i] which represents the highest score for any compression that ends at word xi for sentence x. We define a recurrence as follows C[1] = 0.0 C[i] = maxj&lt;i C[j] + s(x, j, i) for i &gt; 1 It is easy to show that C[n] represents the score of the best compression for sentence x (whose length is n) under the first-order </context>
<context position="19922" citStr="Knight and Marcu (2000)" startWordPosition="3409" endWordPosition="3412">is feature since the extent of the production might be well beyond the local context of first-order feature factorization. Furthermore, since the training set is so small, these features are likely to be observed very few times. 3.2.3 Feature Set Summary In this section we have described a rich feature set over adjacent words in the compressed sentence, dropped words and phrases from the original sentence, and properties of deep syntactic trees of the original sentence. Note that these features in many ways mimic the information already present in the noisy-channel and decision-tree models of Knight and Marcu (2000). Our bigram features encode properties that indicate both good and bad words to be adjacent in the compressed sentence. This is similar in purpose to the source model from the noisy-channel system. However, in that system, the source model is trained on uncompressed sentences and thus is not as representative of likely bigram features for compressed sentences, which is really what we desire. Our feature set also encodes dropped words and phrases through the properties of the words themselves and through properties of their syntactic relation to the rest of the sentence in a parse tree. These </context>
<context position="21159" citStr="Knight and Marcu (2000)" startWordPosition="3618" endWordPosition="3621">sent likely phrases to be dropped in the compression and are thus similar in nature to the channel model in the noisy-channel system as well as the features in the tree-to-tree decision tree system. However, we use these syntactic constraints as soft evidence in our model. That is, they represent just another layer of evidence to be considered during training when setting parameters. Thus, if the parses have too much noise, the learning algorithm can lower the weight of the parse features since they are unlikely to be useful discriminators on the training data. This differs from the models of Knight and Marcu (2000), which treat the noisy parses as gold-standard when 301 calculating probability estimates. An important distinction we should make is the notion of supported versus unsupported features (Sha and Pereira, 2003). Supported features are those that are on for the gold standard compressions in the training. For instance, the bigram feature “NN&amp;VB” will be supported since there is most likely a compression that contains a adjacent noun and verb. However, the feature “JJ&amp;VB” will not be supported since an adjacent adjective and verb most likely will not be observed in any valid compression. Our mode</context>
<context position="24386" citStr="Knight and Marcu (2000)" startWordPosition="4180" endWordPosition="4183">+ 1 7.w=v/(N*T) Figure 3: MIRA learning algorithm as presented by McDonald et al. (2005b). best performance, though varying k did not have a major impact overall. Furthermore we found that after only 3-5 training epochs performance on the development data was maximized. The final weight vector is the average of all weight vectors throughout training. Averaging has been shown to reduce overfitting (Collins, 2002) as well as reliance on the order of the examples during training. We found it to be particularly important for this data set. 4 Experiments We use the same experimental methodology as Knight and Marcu (2000). We provide every compression to four judges and ask them to evaluate each one for grammaticality and importance on a scale from 1 to 5. For each of the 32 sentences in our test set we ask the judges to evaluate three systems: human annotated, the decision tree model of Knight and Marcu (2000) and our system. The judges were told all three compressions were automatically generated and the order in which they were presented was randomly chosen for each sentence. We compared our system to the decision tree model of Knight and Marcu instead of the noisy-channel model since both performed nearly </context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>K. Knight and D. Marcu. 2000. Statistical-based summarization - step one: Sentence compression. In Proc. AAAI 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Flexible text segmentation with structured multilabel classifi cation.</title>
<date>2005</date>
<booktitle>In Proc. HLT-EMNLP.</booktitle>
<contexts>
<context position="11553" citStr="McDonald et al., 2005" startWordPosition="1930" endWordPosition="1933">pression that ends at word xj, for all j &lt; i, then C[i] must also be the highest scoring compression ending at word xi since it represents the max combination over all high scoring shorter compressions plus the score of extending the compression to the current word. Thus, since xn is by definition in every compressed version of x (see above), then it must be the case that C[n] stores the score of the best compression. This table can be filled in O(n2). This algorithm is really an extension of Viterbi to the case when scores factor over dynamic substrings of the text (Sarawagi and Cohen, 2004; McDonald et al., 2005a). As such, we can use back-pointers to reconstruct the highest scoring compression as well as k-best decoding algorithms. This decoding algorithm is dynamic with respect to compression rate. That is, the algorithm will return the highest scoring compression regardless of length. This may seem problematic since longer compressions might contribute more to the score (since they contain more bigrams) and thus be preferred. However, in Section 3.2 we define a rich feature set, including features on words dropped from the compression that will help disfavor compressions that drop very few words s</context>
<context position="16465" citStr="McDonald et al., 2005" startWordPosition="2794" endWordPosition="2798"> or dropped from the original sentence during compression. However, they do so without a larger picture of the function of each word in the sentence. For instance, dropping verbs is not that uncommon - a relative clause for instance may be dropped during compression. However, dropping the main verb in the sentence is uncommon, since that verb and its arguments typically encode most of the information being conveyed. An obvious solution to this problem is to include features over a deep syntactic analysis of the sentence. To do this we parse every sentence twice, once with a dependency parser (McDonald et al., 2005b) and once with a phrase-structure parser (Charniak, 2000). These parsers have been trained out-of-domain on the Penn WSJ Treebank and as a result contain noise. However, we are merely going to use them as an additional source of features. We call this soft syntactic evidence since the deep trees are not used as a strict goldstandard in our model but just as more evidence for 300 S VP NP NP PP PP NP NNP VBD NNP IN NNP IN NN Mary, saw2 Ralph3 ono Tuesday, afters lunch? NP rooto saw2 on� afters Mary, Ralph3 Tuesday, lunch? Figure 2: An example dependency tree from the McDonald et al. (2005b) pa</context>
<context position="23282" citStr="McDonald et al. (2005" startWordPosition="3975" endWordPosition="3979">raints so we pick the one with minimum change from the previous setting. We define the loss to be the number of words falsely retained or dropped in the incorrect compression relative to the correct one. For instance, if the correct compression of the sentence in Figure 2 is Mary saw Ralph, then the compression Mary saw after lunch would have a loss of 3 since it incorrectly left out one word and included two others. Of course, for a sentence there are exponentially many possible compressions, which means that this optimization will have exponentially many constraints. We follow the method of McDonald et al. (2005b) and create constraints only on the k compressions that currently have the highest score, bestk(x; w). This can easily be calculated by extending the decoding algorithm with standard Viterbi k-best techniques. On the development data, we found that k = 10 provided the Training data: T = {(xt, yt)}Tt=1 1. w0 = 0; v = 0; i = 0 2. for n : 1..N 3. fort : 1..T ‚ w(i+1) − w(i)‚ 4. min ‚ ‚‚‚ s.t. s(xt, yt) − s(xt, y�) &gt; L(yt, y�) where y&apos; E bestk(x;w(i)) 5. v = v + w(i+1) 6. i = i + 1 7.w=v/(N*T) Figure 3: MIRA learning algorithm as presented by McDonald et al. (2005b). best performance, though var</context>
<context position="32380" citStr="McDonald et al., 2005" startWordPosition="5531" endWordPosition="5535">AT to a file server and issue a command from any workstation to back up the server The human annotated compression is, TapeWare supports DOS and NetWare 286. However, another completely valid compression might be, TapeWare lets you connect the QA150-EXAT to a fi le server. These two compressions overlap by a single word. Our learning algorithm may unnecessarily lower the score of some perfectly valid compressions just because they were not the exact compression chosen by the human annotator. A possible direction of research is to investigate multilabel learning techniques for structured data (McDonald et al., 2005a) that learn a scoring function separating a set of valid answers from all invalid answers. Thus if a sentence has multiple valid compressions we can learn to score each valid one higher than all invalid compressions during training to avoid this problem. Acknowledgments The author would like to thank Daniel Marcu for providing the data as well as the output of his and Kevin Knight’s systems. Thanks also to Hal Daum´e and Fernando Pereira for useful discussions. Finally, the author thanks the four reviewers for evaluating the compressed sentences. This work was supported by NSF ITR grants 020</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005a. Flexible text segmentation with structured multilabel classifi cation. In Proc. HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="11553" citStr="McDonald et al., 2005" startWordPosition="1930" endWordPosition="1933">pression that ends at word xj, for all j &lt; i, then C[i] must also be the highest scoring compression ending at word xi since it represents the max combination over all high scoring shorter compressions plus the score of extending the compression to the current word. Thus, since xn is by definition in every compressed version of x (see above), then it must be the case that C[n] stores the score of the best compression. This table can be filled in O(n2). This algorithm is really an extension of Viterbi to the case when scores factor over dynamic substrings of the text (Sarawagi and Cohen, 2004; McDonald et al., 2005a). As such, we can use back-pointers to reconstruct the highest scoring compression as well as k-best decoding algorithms. This decoding algorithm is dynamic with respect to compression rate. That is, the algorithm will return the highest scoring compression regardless of length. This may seem problematic since longer compressions might contribute more to the score (since they contain more bigrams) and thus be preferred. However, in Section 3.2 we define a rich feature set, including features on words dropped from the compression that will help disfavor compressions that drop very few words s</context>
<context position="16465" citStr="McDonald et al., 2005" startWordPosition="2794" endWordPosition="2798"> or dropped from the original sentence during compression. However, they do so without a larger picture of the function of each word in the sentence. For instance, dropping verbs is not that uncommon - a relative clause for instance may be dropped during compression. However, dropping the main verb in the sentence is uncommon, since that verb and its arguments typically encode most of the information being conveyed. An obvious solution to this problem is to include features over a deep syntactic analysis of the sentence. To do this we parse every sentence twice, once with a dependency parser (McDonald et al., 2005b) and once with a phrase-structure parser (Charniak, 2000). These parsers have been trained out-of-domain on the Penn WSJ Treebank and as a result contain noise. However, we are merely going to use them as an additional source of features. We call this soft syntactic evidence since the deep trees are not used as a strict goldstandard in our model but just as more evidence for 300 S VP NP NP PP PP NP NNP VBD NNP IN NNP IN NN Mary, saw2 Ralph3 ono Tuesday, afters lunch? NP rooto saw2 on� afters Mary, Ralph3 Tuesday, lunch? Figure 2: An example dependency tree from the McDonald et al. (2005b) pa</context>
<context position="23282" citStr="McDonald et al. (2005" startWordPosition="3975" endWordPosition="3979">raints so we pick the one with minimum change from the previous setting. We define the loss to be the number of words falsely retained or dropped in the incorrect compression relative to the correct one. For instance, if the correct compression of the sentence in Figure 2 is Mary saw Ralph, then the compression Mary saw after lunch would have a loss of 3 since it incorrectly left out one word and included two others. Of course, for a sentence there are exponentially many possible compressions, which means that this optimization will have exponentially many constraints. We follow the method of McDonald et al. (2005b) and create constraints only on the k compressions that currently have the highest score, bestk(x; w). This can easily be calculated by extending the decoding algorithm with standard Viterbi k-best techniques. On the development data, we found that k = 10 provided the Training data: T = {(xt, yt)}Tt=1 1. w0 = 0; v = 0; i = 0 2. for n : 1..N 3. fort : 1..T ‚ w(i+1) − w(i)‚ 4. min ‚ ‚‚‚ s.t. s(xt, yt) − s(xt, y�) &gt; L(yt, y�) where y&apos; E bestk(x;w(i)) 5. v = v + w(i+1) 6. i = i + 1 7.w=v/(N*T) Figure 3: MIRA learning algorithm as presented by McDonald et al. (2005b). best performance, though var</context>
<context position="32380" citStr="McDonald et al., 2005" startWordPosition="5531" endWordPosition="5535">AT to a file server and issue a command from any workstation to back up the server The human annotated compression is, TapeWare supports DOS and NetWare 286. However, another completely valid compression might be, TapeWare lets you connect the QA150-EXAT to a fi le server. These two compressions overlap by a single word. Our learning algorithm may unnecessarily lower the score of some perfectly valid compressions just because they were not the exact compression chosen by the human annotator. A possible direction of research is to investigate multilabel learning techniques for structured data (McDonald et al., 2005a) that learn a scoring function separating a set of valid answers from all invalid answers. Thus if a sentence has multiple valid compressions we can learn to score each valid one higher than all invalid compressions during training to avoid this problem. Acknowledgments The author would like to thank Daniel Marcu for providing the data as well as the output of his and Kevin Knight’s systems. Thanks also to Hal Daum´e and Fernando Pereira for useful discussions. Finally, the author thanks the four reviewers for evaluating the compressed sentences. This work was supported by NSF ITR grants 020</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005b. Online large-margin training of dependency parsers. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riezler</author>
<author>T H King</author>
<author>R Crouch</author>
<author>A Zaenen</author>
</authors>
<title>Statistical sentence condensation using ambiguity packing and stochastic disambiguation methods for lexical-functional grammar. In</title>
<date>2003</date>
<booktitle>Proc. HLTNAACL.</booktitle>
<contexts>
<context position="1660" citStr="Riezler et al., 2003" startWordPosition="245" endWordPosition="248">s an important problem in text summarization. Most summarization systems are evaluated on the amount of relevant information retained as well as their compression rate. Thus, returning highly compressed, yet informative, sentences allows summarization systems to return larger sets of sentences and increase the overall amount of information extracted. We focus on the particular instantiation of sentence compression when the goal is to produce the compressed version solely by removing words or phrases from the original, which is the most common setting in the literature (Knight and Marcu, 2000; Riezler et al., 2003; Turner and Charniak, 2005). In this framework, the goal is to find the shortest substring of the original sentence that conveys the most important aspects of the meaning. We will work in a supervised learning setting and assume as input a training set T=(xt,yt)|? | t�1 of original sentences xt and their compressions yt. We use the Ziff-Davis corpus, which is a set of 1087 pairs of sentence/compression pairs. Furthermore, we use the same 32 testing examples from Knight and Marcu (2000) and the rest for training, except that we hold out 20 sentences for the purpose of development. A handful of</context>
<context position="8391" citStr="Riezler et al. (2003)" startWordPosition="1364" endWordPosition="1367"> Both models rely heavily on the output of a noisy parser to calculate probability estimates for the compression. We argue in the next section that 298 ideally, parse trees should be treated solely as a source of evidence when making compression decisions to be balanced with other evidence such as that provided by the words themselves. Recently Turner and Charniak (2005) presented supervised and semi-supervised versions of the Knight and Marcu noisy-channel model. The resulting systems typically return informative and grammatical sentences, however, they do so at the cost of compression rate. Riezler et al. (2003) present a discriminative sentence compressor over the output of an LFG parser that is a packed representation of possible compressions. Though this model is highly likely to return grammatical compressions, it required the training data be human annotated with syntactic trees. 3 Discriminative Sentence Compression For the rest of the paper we use x = x1 ... xn to indicate an uncompressed sentence and y = y1 ... ym a compressed version of x, i.e., each yj indicates the position in x of the jth word in the compression. We always pad the sentence with dummy start and end words, x1 = -START- and </context>
</contexts>
<marker>Riezler, King, Crouch, Zaenen, 2003</marker>
<rawString>S. Riezler, T. H. King, R. Crouch, and A. Zaenen. 2003. Statistical sentence condensation using ambiguity packing and stochastic disambiguation methods for lexical-functional grammar. In Proc. HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sarawagi</author>
<author>W Cohen</author>
</authors>
<title>Semi-Markov conditional random fi elds for information extraction.</title>
<date>2004</date>
<booktitle>In Proc. NIPS.</booktitle>
<contexts>
<context position="11530" citStr="Sarawagi and Cohen, 2004" startWordPosition="1926" endWordPosition="1929">is the highest scoring compression that ends at word xj, for all j &lt; i, then C[i] must also be the highest scoring compression ending at word xi since it represents the max combination over all high scoring shorter compressions plus the score of extending the compression to the current word. Thus, since xn is by definition in every compressed version of x (see above), then it must be the case that C[n] stores the score of the best compression. This table can be filled in O(n2). This algorithm is really an extension of Viterbi to the case when scores factor over dynamic substrings of the text (Sarawagi and Cohen, 2004; McDonald et al., 2005a). As such, we can use back-pointers to reconstruct the highest scoring compression as well as k-best decoding algorithms. This decoding algorithm is dynamic with respect to compression rate. That is, the algorithm will return the highest scoring compression regardless of length. This may seem problematic since longer compressions might contribute more to the score (since they contain more bigrams) and thus be preferred. However, in Section 3.2 we define a rich feature set, including features on words dropped from the compression that will help disfavor compressions tha</context>
</contexts>
<marker>Sarawagi, Cohen, 2004</marker>
<rawString>S. Sarawagi and W. Cohen. 2004. Semi-Markov conditional random fi elds for information extraction. In Proc. NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sha</author>
<author>F Pereira</author>
</authors>
<title>Shallow parsing with conditional random fi elds.</title>
<date>2003</date>
<booktitle>In Proc. HLT-NAACL,</booktitle>
<pages>213--220</pages>
<contexts>
<context position="21369" citStr="Sha and Pereira, 2003" startWordPosition="3648" endWordPosition="3651">use these syntactic constraints as soft evidence in our model. That is, they represent just another layer of evidence to be considered during training when setting parameters. Thus, if the parses have too much noise, the learning algorithm can lower the weight of the parse features since they are unlikely to be useful discriminators on the training data. This differs from the models of Knight and Marcu (2000), which treat the noisy parses as gold-standard when 301 calculating probability estimates. An important distinction we should make is the notion of supported versus unsupported features (Sha and Pereira, 2003). Supported features are those that are on for the gold standard compressions in the training. For instance, the bigram feature “NN&amp;VB” will be supported since there is most likely a compression that contains a adjacent noun and verb. However, the feature “JJ&amp;VB” will not be supported since an adjacent adjective and verb most likely will not be observed in any valid compression. Our model includes all features, including those that are unsupported. The advantage of this is that the model can learn negative weights for features that are indicative of bad compressions. This is not difficult to d</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>F. Sha and F. Pereira. 2003. Shallow parsing with conditional random fi elds. In Proc. HLT-NAACL, pages 213–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Turner</author>
<author>E Charniak</author>
</authors>
<title>Supervised and unsupervised learning for sentence compression. In</title>
<date>2005</date>
<booktitle>Proc. ACL.</booktitle>
<contexts>
<context position="1688" citStr="Turner and Charniak, 2005" startWordPosition="249" endWordPosition="252"> in text summarization. Most summarization systems are evaluated on the amount of relevant information retained as well as their compression rate. Thus, returning highly compressed, yet informative, sentences allows summarization systems to return larger sets of sentences and increase the overall amount of information extracted. We focus on the particular instantiation of sentence compression when the goal is to produce the compressed version solely by removing words or phrases from the original, which is the most common setting in the literature (Knight and Marcu, 2000; Riezler et al., 2003; Turner and Charniak, 2005). In this framework, the goal is to find the shortest substring of the original sentence that conveys the most important aspects of the meaning. We will work in a supervised learning setting and assume as input a training set T=(xt,yt)|? | t�1 of original sentences xt and their compressions yt. We use the Ziff-Davis corpus, which is a set of 1087 pairs of sentence/compression pairs. Furthermore, we use the same 32 testing examples from Knight and Marcu (2000) and the rest for training, except that we hold out 20 sentences for the purpose of development. A handful of sentences occur twice but w</context>
<context position="8143" citStr="Turner and Charniak (2005)" startWordPosition="1328" endWordPosition="1331">tures encode properties related to including or dropping constituents from the tree with no encoding of bigram or trigram surface features to promote grammaticality. As a result, the model will sometimes return very short and ungrammatical compressions. Both models rely heavily on the output of a noisy parser to calculate probability estimates for the compression. We argue in the next section that 298 ideally, parse trees should be treated solely as a source of evidence when making compression decisions to be balanced with other evidence such as that provided by the words themselves. Recently Turner and Charniak (2005) presented supervised and semi-supervised versions of the Knight and Marcu noisy-channel model. The resulting systems typically return informative and grammatical sentences, however, they do so at the cost of compression rate. Riezler et al. (2003) present a discriminative sentence compressor over the output of an LFG parser that is a packed representation of possible compressions. Though this model is highly likely to return grammatical compressions, it required the training data be human annotated with syntactic trees. 3 Discriminative Sentence Compression For the rest of the paper we use x </context>
</contexts>
<marker>Turner, Charniak, 2005</marker>
<rawString>J. Turner and E. Charniak. 2005. Supervised and unsupervised learning for sentence compression. In Proc. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>