<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.518606">
THE RECOGNITION CAPACITY OF LOCAL SYNTACTIC CONSTRAINTS
</title>
<author confidence="0.9481035">
Mori Rimon&apos;
Jacky Herz&apos;
</author>
<affiliation confidence="0.8972525">
The Computer Science Department
The I lebrew University of Jerusalem,
</affiliation>
<address confidence="0.734232">
Giv&apos;at Ram, Jerusalem 91904, ISRAEL
</address>
<email confidence="0.998176">
E-mail: rimon@hujics.13ITNET
</email>
<sectionHeader confidence="0.995641" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999897">
Given a grammar for a language, it is possible to
create finite state mechanisms that approximate
its recognition capacity. These simple automata
consider only short context information, drawn
from local syntactic constraints which the
grammar imposes. While it is short of providing
the strong generative capacity of the grammar,
such an approximation is useful for removing
most word tagging ambiguities, identifying many
cases of ill-formed input, and assisting efficiently
in other natural language processing tasks. Our
basic approach to the acquisition and usage of
local syntactic constraints was presented else-
where; in this paper we present some formal and
empirical results pertaining to properties of the
approximating automata.
</bodyText>
<sectionHeader confidence="0.996133" genericHeader="keywords">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999912642857143">
Parsing is a process by which an input sentence
is not only recognized as belonging to the lan-
guage, but is also assigned a structure. As
[[3erwick/Weinberg 84] comment, recognition
per se (i.e. a weak generative capacity analysis) is
not of much value for a theory of language
understanding, but it can be useful &amp;quot;as a diag-
nostic&amp;quot;. We Claim that if an efficient recognition
procedure is available, it can be most valuable as
a pre-parsing reducer of lexical ambiguity (espe-
cially, as [Milne 86] points out, for deterministic
parsers), and even more useful in applications
where full parsing is not absolutely required -
e.g. identification of ill-formed inputs in a text
critique program. Still weaker than recognition
procedures are methods which approximate the
recognition capacity. This is the kind of methods
that we discuss in this paper.
More specifically, we analyze the recognition
capacity of automata based on local (short
context) considerations. In [I Ierz/Rimon 91] we
presented our approach to the acquisition and
usage of local syntactic constraints, focusing on
its use for reduction of word-level ambiguity.
After briefly reviewing this method in section 2
below, we examine in more detail various char-
acteristics of the approximating automata, and
suggest several applications.
</bodyText>
<sectionHeader confidence="0.990353" genericHeader="introduction">
2. Background: Local Syntactic
Constraints
</sectionHeader>
<bodyText confidence="0.945051923076923">
Let S = WN be a sentence of length N,
(Wi} being the words composing the sentence.
And let tm be a tag image corresponding to
the sentence S, (ti) belonging to the tag set T -
the set of word-class tags used as terminal
symbols in a given grammar G. Typically,
M=N, but in a more general environment we
allow Al &gt; N . This is useful when dealing with
languages where morphology allows cliticization,
concatenation of conjunctions, prepositions, or
determiners to a verb or a noun, etc.; in gram-
mars for I lebrew, for example, it is convenient
Rimon&apos;s main affiliation is the IBM Scientific Center, Haifa, Israel, E-mail: rimon@haifasc3.iinusLibm.c0m
</bodyText>
<footnote confidence="0.773651">
2 J. Herz was partly supported by the Leibniz Center for Research in Computer Science, the Hebrew University,
and by the Rau foundation of the Open University.
</footnote>
<equation confidence="0.302939">
- 155 -
</equation>
<bodyText confidence="0.991043340425532">
to assume that a preliminary morphological
phase separated word-forms to basic sequences
of tags, and then state syntactic rules in terms of
standard word classes.
In any case, it is reasonable to assume that the
tag image IM cannot be uniquely assigned.
Even with a coarse tag set (e.g. parts of speech
with no features) many words have more than
one interpretation, thus giving rise to exponen-
tially many tag images for a sentence.&apos;
Following [Karlsson 90], we use the term cohort
to refer to the set of lexically valid readings of a
given word. We use the term path to refer to a
sequence of NI tags (M IV) which is a tag-
image corresponding to the words W1,..., WN of
a given sentence S. This is motivated by a view
of lexical ambiguity as a graph problem: we try
to reduce the number of tentative paths in
ambiguous cases by removing arcs from the Sen-
tence Graph (SG) - a directed graph with ver-
tices for all tags in all cohorts of the words in
the given sentence, and arcs connecting each tag
to all tags in the cohort which follows it.
The removal of arcs and the testing of paths for
validity as complete sentence interpretations are
done using local constraints. A local constraint
of length k on a given tag t is a rule allowing or
disallowing a sequence of k tags from being in
its right (or left) neighborhood in any tag image
of a sentence. In our approach, the local con-
straints are extracted from the grammar (and this
is the major aspect distinguishing it from some
other short context methods such as [Beale 88],
[DeRose 88], [Karlsson 90], [Katz 85],
[Marcus 80], [Marshall 83], and [Milne 86]).
For technical convenience we add the symbol
&amp;quot;$ &lt;&amp;quot; at the beginning of tag images and &amp;quot;&gt; $&amp;quot; at
the end. Given a grammar G (which for the time
being we assume to be an unrestricted context-
free phrase structure grammar), with a:set T of
terminal symbols (tag set), a set V of *variables
(non-terminals, among which S is the root vari-
able for derivations), and a set P of production
rules of the form A ---0 a, where A is in V and a
is in (VU T), we define the Right Short
Context of length k of a terminal t (tag):
SCr (t,k) for tin T and for k = 0,1,2,3...
</bodyText>
<equation confidence="0.9156745">
{
tZ I Z E T*, Izi=k or Izi &lt;k if
</equation>
<bodyText confidence="0.931978928571429">
&apos; &gt; $&apos; is the last tag in z,
and there exists a derivation
S F. --- &gt; a t z (3 (a, 13 E Of Ii in* )
The Left Short Context of length k of a tag t rel-
ative to the grammar G is denoted by SCI (t,k)
and defined in a similar way.
It is sometimes useful to define Positional Short
Contexts. The definition is similar to the above,
with a restriction that t may start only in a given
position in a tag image of a sentence.
The basis for the automaton which checks a tag
stream (path) for validity as a tag-image relative
to the local constraints, is the function next(t),
which for any t in 1&apos; defines a set, as follows:
</bodyText>
<equation confidence="0.483038">
next (t) = ( z tz e SCr (t,l) )
</equation>
<bodyText confidence="0.99995825">
In [Ilerz/Rimon 91] we gave a procedure for
computing next(t) from a given context free
grammar, using standard practices of parsing of
formal languages (see [Aho/Ullman 72]).
</bodyText>
<sectionHeader confidence="0.924124" genericHeader="method">
3. Local Constraints Automata
</sectionHeader>
<bodyText confidence="0.983992875">
We denote by LCA(1) the simple finite state
automaton which uses the pre-processed
(next(t)) sets to check if a given tag stream
(path) satisfies the SCr(t,l) constraints.
In a similar manner it is possible to define
LCA(k), relative to the short context of length k.
We denote by L the language generated by the
3 Our studies of modern written Ilebrew suggest that about 60% of the word-forms in running texts are ambiguous
with respect to a basic tag set, and the average number of possible readings of such word-forms is 2.4. Even
when counting only &amp;quot;natural readings&amp;quot;, i.e. interpretations which are likely to occur in typical corpora, this
number is quite large, around 1.8 (it is somewhat larger for the small subset of the most common words).
- 156 -
underlying grammar, and by L(k) thc language
accepted by the automaton LCA(k). The fol-
lowing relations hold for the family of automata
{LCA(i)}:
</bodyText>
<equation confidence="0.973509">
L(1) L(2) L
</equation>
<bodyText confidence="0.995261657894737">
&apos;this guarantees a security feature: If for some i,
LCA(i) docs not recognize (accept) a string of
tags, then this string is sure to be illegal (i.e. not
in L). On the other hand, any LCA(k) may rec-
ognize sentences not in L (or, from a dual point
of view, will reject only part of the illegal tag
images). The important question is how tight arc
the inclusion relations above - i.e. how well
LCA(k) approximates the language L. In partic-
ular we are interested in LCA(I).
There is no simple analytic answer to this ques-
tion. Contradictory forces play here: the nature
of the language -- e.g a rigid word order and
constituent order yield stronger constraints; the
grain of the tag set -- better refined tags (dif-
ferent languages may require different tag sets)
help express refined syntactic claims, hence more
specific constraints, but they also create a greater
level of tagging ambiguity; the size of the
grammar -- a larger grammar offers more infor-
mation, but, covering a richer set of structures, it
allows more tag-pairs to co-occur; etc.
It is interesting to note that for I febrew, short
context methods are most needed because of the
considerable ambiguity at the lexical level, but
their effectiveness suffers from the rather free
word/constituent order.
Finally, a comment about the computational
efficiency of the LCA(k) automaton. The time
complexity of checking a tag string of length n
using 1,CA.(k) is at most 0(n x k x log ITI),
while a non-deterministic parser for a context
free grammar may require 0(n3x1G12). (Ti is
the size of the tag set, &apos;GI is the size of the
grammar). The space complexity of ICA(k) is
proportional to Mk+ ; this is why only truly
short contexts should be used.
unbounded, there is no fixed k which suffices.
</bodyText>
<sectionHeader confidence="0.889804" genericHeader="method">
4. A Sample Grammar
</sectionHeader>
<bodyText confidence="0.99869975">
To illustrate claims made in the sections below,
we will use the following toy grammar of a small
fragment of English. Statements about the cor-
rectness of sentences etc., are of course relative
to this toy grammar.
The tag set T includes: n (noun), v (verb), det
(determiner), adj ( adjective ) and prep (preposi-
tion). The context free grammar G is:
</bodyText>
<equation confidence="0.9705615">
S --&gt; $&lt; NP VP &gt;$
NP --&gt; (det) (adj) n
NP --&gt; NP PP
TT --&gt; prep NP
VP --&gt; v NP
VP --&gt; VP PP
</equation>
<bodyText confidence="0.969969">
To extract the local constraints from this
grammar, we first compute the function next(t)
for every tag t in T, and from the resulting sets
we obtain the graph below, showing valid pairs
in the short context of length 1 (again, validity is
relative to the given toy grammar):
This graph, or more conveniently the table of
&amp;quot;valid neighbors&amp;quot; below, define the LCA(1)
automaton. The table is actually the union of
the SCr(t,l) sets for all t in T, and it is derived
directly from the graph:
&gt; adj prep
&gt; &gt;$
Note that for a sentence of length k, the power $&lt; det adj prep adj
of LCA(k) is identical to the weak generative $&lt; adj V det prep n
capacity of the full underlying grammar. But $‹ adj prep
since the size of sentences (tag sequences) in L is det adj V n
det n prep det
- 157 -
</bodyText>
<sectionHeader confidence="0.90036" genericHeader="method">
5. A &amp;quot;Lucky Bag&amp;quot; Experiment
</sectionHeader>
<bodyText confidence="0.992161071428571">
Consider the following sentence, which is in the
language generated by grammar G of section 4:
(I) The charming princess kissed a frog.
The unique tag image corresponding to this sen-
tence is: [ $ &lt; , det, adj, n, v, det, n, &gt; $ 1.
Now let us look at the 720 &amp;quot;random inputs&amp;quot; gen-
erated by permutations of the six words in (1),
and the set of corresponding tag images.
Applying LCA(1), only two tag images are
recognized as valid: [ $ &lt;, det, adj, n, v, (let, n,
&gt;$ ], and [ $ &lt; , det, n, v, det, adj, n, &gt;$ ].
These are exactly the images corresponding to
the eight syntactically correct sentences (relative
to G),
</bodyText>
<listItem confidence="0.8862945">
( I a-b) The/a charming princess kissed a/the frog.
(lc-d) The/a charming frog kissed a/the princess.
(1c-f) The/a princess kissed a/the charming frog.
(1g-h) The/a frog kissed a/the charming princess.
</listItem>
<bodyText confidence="0.936265468085107">
This result is not surprising, given the simple
sentence and toy grammar. (In general, a
grammar with a small number of rules relative to
the size of the tag set cannot produce too many
valid short contexts). It is therefore interesting
to examine another example, where each word is
associated with a cohort of several interpreta-
tions. We borrow from [I lerz/Rimon 91]:
(2) All old people like books about fish.
(tagged as [$ &lt; , n, v, n, prep, n, v, n, &gt;$]).
These two examples do not suggest any kind of
proof, but they well illustrate the recognition
power of even the least powerful automaton in
the (LCA(i)) family. To get another point of
view, one may consider the simple formal lan-
guage L consisting of the strings (ambrn) for
1 m, which can be generated by a context-free
grammar G over &apos;I&apos; = (a, b). LCA(l) based on
G will recognize all strings of the form (aibk) for
1 5_ j,k, but none of the very many other strings
over T. It can be shown that, given arbitrary
strings of length n over T, the probability that
LCA(1) will not reject strings not belonging to L
is proportional to n12^, a term which tends
rapidly to 0. This is the over-recognition margin.
6. Use of LCA in Conjunction with a
Parser
The number of potentially valid tag images
(paths) for a given sentence can be exponential
in the length of the sentence if all words are
ambiguous. It is therefore desirable to filter out
invalid tag images before (or during) parsing.
To examine the power of LCAs as a pre-parsing
filter, we use example (2) again, demonstrating
lexical ambiguities as shown in the chart below.
The chart shows the Reduced Sentence Graph
(RSG) - the original SG from which invalid arcs
(relative to the SCr(t,l) table) were removed.
Assuming the word tagging shown in section 6,
there arc 256 (2 x 2 x2x 4 x 2x 2 x 2) tentative
tag images (paths) for this sentence and for each
of its 5040 permutations. This generates a very
large number of rather random tag images.
Applying LCA(I), only a small number of
images are recognized as potentially valid.
Among them are syntactically correct sentences
such as:
</bodyText>
<listItem confidence="0.535865">
(2a) Fish like old books about all people.
</listItem>
<bodyText confidence="0.8974925">
and only less than 0.1% sentences which are
locally valid but globally incorrect, such as:
</bodyText>
<equation confidence="0.453417">
(2b) + Old fish all about books like people.
ALL OLD PEOPLE LIKE BOOKS ABOUT FISH
det—÷adj--, n v n —*prep n
IN N Ii
n n v% prep --4 adj v &gt;$
adj
</equation>
<bodyText confidence="0.992832195121951">
We arc left with four valid paths through the
sentence, out of the 256 tentative paths in SG.
Two paths represent legal syntactic interpreta-
tions (of which one is the intended&amp;quot; meaning).
The other two are locally valid but globally
incorrect, having either two verbs or no verb at
- 158 -
all, in contrast to the grammar. SCr(t,2) would
have rejected one of the wrong two.
Note that in this particular example the method
was quite effective in reducing sentence-wide
interpretations (leaving an easy job even for a
deterministic parser), but it was not very good in
individual word tagging disambiguation. These
two sub-goals of tagging disambiguation -
reducing the number of paths and reducing
word-level possibilities - are not identical. It is
possible to construct sentences in which all
words arc two-way ambiguous and only two dis-
joint paths out of the 211 possible paths are legal,
thus preserving all word-level ambiguity.
We demonstrated the potential of efficient path
reduction for a pre-parsing filter. But short-con-
text techniques can also be integrated into the
parsing process itself. In this mode, when the
parser hypothesizes the existence of a. constit-
uent, it will first check if local constraints do not
rule out that hypothesis. In the example above,
a more sophisticated method could have used
the fact that our grammar does not allow verbs
in constituents other than VP, or that it requires
one and only one verb in the whole sentence.
The motivation for this method, and its princi-
ples of operation, arc similar to those behind dif-
ferent techniques combining top-down and
bottom-up considerations. The performance
gains depend on the parsing technique; in
general, allowing early decisions regarding incon-
sistent tag assignments, based on information
which may be only implicit in the grammar,
offers considerable savings.
</bodyText>
<sectionHeader confidence="0.486636" genericHeader="method">
7. Educated Guess of Unknown Words
</sectionHeader>
<bodyText confidence="0.985805692307692">
Another interesting aid Which local ,syntactic
constraints can provide for practical parsers is
an oracle&amp;quot; which makes &amp;quot;educated guesses&apos;
about unknown words. It is typical for language
analysis systems to assume a noun whenever an
unknown word is encountered. There is sense in
this strategy, but the use of LCA, even LCA(1),
can do much better.
To illustrate this feature, we go back to the prin-
cess and the frog. Suppose that an adjective
unknown to the system, say &amp;quot;Transylvanian&amp;quot; was
used rather than &amp;quot;charming&amp;quot; in example (1),
yielding the input sentence:
</bodyText>
<listItem confidence="0.603158">
(3) The Transylvanian princess kissed a frog.
</listItem>
<bodyText confidence="0.958150066666667">
Checking out all tags in 1&apos; in the second position
of the tag image of this sentence, the only tag
that satisfies the constraints of LCA(1) is adj.
8. &amp;quot;Context Sensitive&amp;quot; Spelling
Verification
A related application of local syntactic con-
straints is spelling verification beyond the basic
word level (which is, in fact, SCr(t,0) ).
Suppose that, while typing sentence (1), a user
made a typing error and instead of the adjective
&amp;quot;charming&amp;quot; wrote &amp;quot;charm&amp;quot; (or &amp;quot;arming&amp;quot;, or any
other legal word which is interpreted as a noun):
(4) The charm princess kissed a frog.
This is the kind of errors° that a full parser
would recognize but a word-based spell-checker
would not. But in many such cases there is no
need for the full power (and complexity) of a
parser; even LCA(1) can detect the error. In
general, an LCA which is based on a detailed
grammar, offers cheap and effective means for
invalidation of a large set of ill-formed inputs.
Here too, one may want to get another point of
view by considering the simple formal language
L = (ambrn). A single typo results in a string
with one &amp;quot;a&amp;quot; changed for a &amp;quot;b&amp;quot;, or vice versa.
Since LCA(1) recognizes strings of the form
{alb&apos;) for 1 given arbitrary strings of length
n over T = (a, b), LCA(1) will detect all but
two of the n single typos possible - those on the
borderline between the a&apos;s and b&apos;s.
</bodyText>
<footnote confidence="0.8825045">
4 Remember that everything is relative to the toy grammar used throughout this paper. Hence, although the
charm princess&amp;quot; may be a perfect noun phrase, it is illegal relative to our grammar.
</footnote>
<note confidence="0.284507">
- 159 -
</note>
<sectionHeader confidence="0.684261" genericHeader="method">
9. Assistance to Tagging Systems
</sectionHeader>
<bodyText confidence="0.999936277777778">
Tagged corpora are important resources for
many applications. Since manual tagging is a
slow and expensive process, it is a common
approach to try automatic heuristics and resort
to user interaction only when there is no decisive
information. A well-built tagging system can
-learn&amp;quot; and improve its performance as more
text is processed (e.g. by using the already tagged
corpus as a statistical knowledge base).
Arguments such as those given in sections 7 and
8 above suggest that the use of local constraints
can resolve many tagging ambiguities, thus
increasing the &amp;quot;speed of convergence&amp;quot; of an auto-
matic tagging system. This seems to be true even
for the rather simple and inexpensive I CA(1) for
languages with a relatively rigid word order. For
related work cf. [Greene/Rubin 71], [Church
88], [DeRose 88], and [Marshall 83].
</bodyText>
<sectionHeader confidence="0.900267" genericHeader="method">
10. Final Remarks
</sectionHeader>
<bodyText confidence="0.999994375">
To make our presentation simpler, we have
limited the discussion to straightforward context
free grammars. But the method is more general.
It can, for example, be extended to CFGs aug-
mented with conditional equations on features
(such as agreement) - either by translating such
grammars to equivalent Cl;Gs with a more
detailed tag set (assuming a finite range of
feature values), or by augmenting our automata
with conditions on arcs. It can also be extended
for a probabilistic language model, generating
probabilistic constraints on tag sequences from a
probabilistic CFG (such as of [Fujisaki et al.
89]).
Perhaps more interestingly, the method can be
used even without an underlying grammar, if a
large corpus and a lexical analyzer (which sug-
gests pre-disambiguated cohorts) are available.
This variant is based on a technique of invali-
dation of tag pairs (or longer sequences) which
satisfy certain conditions over the whole lan-
guage L, and the fact that L can be approxi-
mated by a large corpus. We cannot elaborate
on this extension here.
</bodyText>
<sectionHeader confidence="0.996482" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99319796">
[Aho/Ullman 72] Alfred V. Aho and Jeffrey D.
Ullman. The Theory of Parsing, Translation and
Compiling. Prentice-I tall, 1972-3.
[Beale 88] Andrew David Beale. Lexicon and
Grammar in Probabilistic Tagging of Written
English. Proc. of the 26th Annual Meeting of the
ACL, Buffalo NY, 1988.
[Berwick/Weinberg 84] Robert C. Berwick and
Amy S. Weinberg. The Grammatical Basis of
Linguistic Performance, The MIT Press, 1984.
[Church 88] Kenneth W. Church. A Sto-
chastic Parts Program and Noun Phrase Parser
for Running Text. Proc. of the 2nd ACL conf.
on Applied Natural Language Processing. 1988.
[DeRose 881 Steven J. DeRose. Grammatical
Category Disambiguation by Statistical Opti-
mization. Computational Linguistics, vol. 14, no.
1, 1988.
[Fujisaki et al. 89] T. Fujisaki, F. Jelinek, J.
Cocke, E. Black, 1&apos;. Nishimo. A Probabilistic
Parsing Method for Sentence Disambiguation.
Proc. of the 1st International Parsing Workshop,
Pittsburgh, June 1989.
[Grecnc/Rubin 71] Barbara Greene and Gerald
Rubin. Automated Grammatical Tagging of
l&apos;,nglish. Technical Report, Brown University,
1971.
[Iferz/Rimon 91] Jacky I lerz and Mori Rimon.
Local Syntactic Constraints. Proc. of the 2nd
International Workshop on Parsing Technologies,
Cancun, February 1991.
[Karlsson 90] Fred Karlsson. Constraint
Grammar as a Framework for Parsing Running
Text. The 13th COLING Conference, Helsinki,
1990.
[Katz 85] Slava Katz. Recursive M-gram Lan-
guage Model via Smoothing of Turing Formula.
IBM Technical Disclosure Bulletin, 1985.
[Marcus 80] Mitchell P. Marcus. A Theory of
Syntactic Recognition for Natural Language, the
MIT Press, 1980.
[Marshall 83] Ian Marshall. Choice of Gram-
matical Word-Class Without Global Syntactic
Analysis: Tagging Words in the LOB Corpus.
Computers in the humanities, vol. 17, pp.
139-150, 1983.
[Milne 86] Robert Milne. Resolving Lexical
Ambiguity in a Deterministic Parser. C&apos;omputa-
tional Linguistics, vol. 12, no. 1, pp. 1-12, 1986.
- 160 -
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.875020">
<title confidence="0.994678">THE RECOGNITION CAPACITY OF LOCAL SYNTACTIC CONSTRAINTS</title>
<author confidence="0.9511285">Mori Rimon&apos; Jacky Herz&apos;</author>
<affiliation confidence="0.9997595">The Computer Science Department The I lebrew University of Jerusalem,</affiliation>
<address confidence="0.999905">Giv&apos;at Ram, Jerusalem 91904, ISRAEL</address>
<email confidence="0.987557">E-mail:rimon@hujics.13ITNET</email>
<abstract confidence="0.999112176470588">Given a grammar for a language, it is possible to create finite state mechanisms that approximate its recognition capacity. These simple automata consider only short context information, drawn from local syntactic constraints which the grammar imposes. While it is short of providing the strong generative capacity of the grammar, such an approximation is useful for removing most word tagging ambiguities, identifying many cases of ill-formed input, and assisting efficiently in other natural language processing tasks. Our basic approach to the acquisition and usage of syntactic constraints was presented elsethis paper we present some formal and empirical results pertaining to properties of the approximating automata.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<title>The Theory of Parsing, Translation and Compiling. Prentice-I</title>
<pages>1972--3</pages>
<location>tall,</location>
<marker>[Aho/Ullman 72]</marker>
<rawString>Alfred V. Aho and Jeffrey D. Ullman. The Theory of Parsing, Translation and Compiling. Prentice-I tall, 1972-3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew David Beale</author>
</authors>
<title>Lexicon and Grammar in Probabilistic Tagging of Written English.</title>
<date>1988</date>
<booktitle>Proc. of the 26th Annual Meeting of the ACL,</booktitle>
<location>Buffalo NY,</location>
<marker>[Beale 88]</marker>
<rawString>Andrew David Beale. Lexicon and Grammar in Probabilistic Tagging of Written English. Proc. of the 26th Annual Meeting of the ACL, Buffalo NY, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Berwick</author>
<author>Amy S Weinberg</author>
</authors>
<title>The Grammatical Basis of Linguistic Performance,</title>
<date>1984</date>
<publisher>The MIT Press,</publisher>
<marker>[Berwick/Weinberg 84]</marker>
<rawString>Robert C. Berwick and Amy S. Weinberg. The Grammatical Basis of Linguistic Performance, The MIT Press, 1984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
</authors>
<title>A Stochastic Parts Program and Noun Phrase Parser for Running Text.</title>
<date>1988</date>
<journal>DeRose</journal>
<booktitle>Proc. of the 2nd ACL conf. on Applied Natural Language Processing.</booktitle>
<volume>881</volume>
<marker>[Church 88]</marker>
<rawString>Kenneth W. Church. A Stochastic Parts Program and Noun Phrase Parser for Running Text. Proc. of the 2nd ACL conf. on Applied Natural Language Processing. 1988. [DeRose 881 Steven J. DeRose. Grammatical Category Disambiguation by Statistical Optimization. Computational Linguistics, vol. 14, no. 1, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Fujisaki</author>
<author>F Jelinek</author>
<author>J Cocke</author>
<author>E Black</author>
</authors>
<title>A Probabilistic Parsing Method for Sentence Disambiguation.</title>
<date>1989</date>
<booktitle>Proc. of the 1st International Parsing Workshop,</booktitle>
<location>Pittsburgh,</location>
<marker>[Fujisaki et al. 89]</marker>
<rawString>T. Fujisaki, F. Jelinek, J. Cocke, E. Black, 1&apos;. Nishimo. A Probabilistic Parsing Method for Sentence Disambiguation. Proc. of the 1st International Parsing Workshop, Pittsburgh, June 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Greene</author>
<author>Gerald Rubin</author>
</authors>
<title>Automated Grammatical Tagging of l&apos;,nglish.</title>
<date>1971</date>
<tech>Technical Report,</tech>
<institution>Brown University,</institution>
<marker>[Grecnc/Rubin 71]</marker>
<rawString>Barbara Greene and Gerald Rubin. Automated Grammatical Tagging of l&apos;,nglish. Technical Report, Brown University, 1971.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacky I lerz</author>
<author>Mori Rimon</author>
</authors>
<title>Local Syntactic Constraints.</title>
<date>1991</date>
<booktitle>Proc. of the 2nd International Workshop on Parsing Technologies,</booktitle>
<location>Cancun,</location>
<marker>[Iferz/Rimon 91]</marker>
<rawString>Jacky I lerz and Mori Rimon. Local Syntactic Constraints. Proc. of the 2nd International Workshop on Parsing Technologies, Cancun, February 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred Karlsson</author>
</authors>
<title>Constraint Grammar as a Framework for Parsing Running Text.</title>
<date>1990</date>
<booktitle>The 13th COLING Conference,</booktitle>
<location>Helsinki,</location>
<marker>[Karlsson 90]</marker>
<rawString>Fred Karlsson. Constraint Grammar as a Framework for Parsing Running Text. The 13th COLING Conference, Helsinki, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slava Katz</author>
</authors>
<title>Recursive M-gram Language Model via Smoothing of Turing Formula.</title>
<date>1985</date>
<booktitle>IBM Technical Disclosure Bulletin,</booktitle>
<marker>[Katz 85]</marker>
<rawString>Slava Katz. Recursive M-gram Language Model via Smoothing of Turing Formula. IBM Technical Disclosure Bulletin, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
</authors>
<title>A Theory of Syntactic Recognition for Natural Language,</title>
<date>1980</date>
<publisher>the MIT Press,</publisher>
<marker>[Marcus 80]</marker>
<rawString>Mitchell P. Marcus. A Theory of Syntactic Recognition for Natural Language, the MIT Press, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Marshall</author>
</authors>
<title>Choice of Grammatical Word-Class Without Global Syntactic Analysis: Tagging Words</title>
<date>1983</date>
<booktitle>in the LOB Corpus. Computers in the humanities,</booktitle>
<volume>17</volume>
<pages>139--150</pages>
<marker>[Marshall 83]</marker>
<rawString>Ian Marshall. Choice of Grammatical Word-Class Without Global Syntactic Analysis: Tagging Words in the LOB Corpus. Computers in the humanities, vol. 17, pp. 139-150, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Milne</author>
</authors>
<title>Resolving Lexical Ambiguity in a Deterministic Parser.</title>
<date>1986</date>
<journal>C&apos;omputational Linguistics,</journal>
<volume>12</volume>
<pages>1--12</pages>
<marker>[Milne 86]</marker>
<rawString>Robert Milne. Resolving Lexical Ambiguity in a Deterministic Parser. C&apos;omputational Linguistics, vol. 12, no. 1, pp. 1-12, 1986. - 160 -</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>