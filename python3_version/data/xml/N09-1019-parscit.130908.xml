<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000694">
<title confidence="0.99585">
Structured Generative Models for Unsupervised Named-Entity Clustering
</title>
<author confidence="0.988121">
Micha Elsner, Eugene Charniak and Mark Johnson
</author>
<affiliation confidence="0.9640355">
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
</affiliation>
<address confidence="0.954311">
Providence, RI 02912
</address>
<email confidence="0.999805">
{melsner,ec,mj}@cs.brown.edu
</email>
<sectionHeader confidence="0.998606" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999862666666667">
We describe a generative model for clustering
named entities which also models named en-
tity internal structure, clustering related words
by role. The model is entirely unsupervised;
it uses features from the named entity itself
and its syntactic context, and coreference in-
formation from an unsupervised pronoun re-
solver. The model scores 86% on the MUC-7
named-entity dataset. To our knowledge, this
is the best reported score for a fully unsuper-
vised model, and the best score for a genera-
tive model.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961913043478">
Named entity clustering is a classic task in NLP, and
one for which both supervised and semi-supervised
systems have excellent performance (Mikheev et al.,
1998; Chinchor, 1998). In this paper, we describe a
fully unsupervised system (using no “seed rules” or
initial heuristics); to our knowledge this is the best
such system reported on the MUC-7 dataset. In ad-
dition, the model clusters the words which appear
in named entities, discovering groups of words with
similar roles such as first names and types of orga-
nization. Finally, the model defines a notion of con-
sistency between different references to the same en-
tity; this component of the model yields a significant
increase in performance.
The main motivation for our system is the re-
cent success of unsupervised generative models for
coreference resolution. The model of Haghighi
and Klein (2007) incorporated a latent variable for
named entity class. They report a named entity score
of 61.2 percent, well above the baseline of 46.4, but
still far behind existing named-entity systems.
We suspect that better models for named entities
could aid in the coreference task. The easiest way to
incorporate a better model is simply to run a super-
vised or semi-supervised system as a preprocess. To
perform joint inference, however, requires an unsu-
pervised generative model for named entities. As far
as we know, this work is the best such model.
Named entities also pose another problem with
the Haghighi and Klein (2007) coreference model;
since it models only the heads of NPs, it will fail to
resolve some references to named entities: (“Ford
Motor Co.”, “Ford”), while erroneously merging
others: (“Ford Motor Co.”, “Lockheed Martin Co.”).
Ng (2008) showed that better features for match-
ing named entities– exact string match and an “alias
detector” looking for acronyms, abbreviations and
name variants– improve the model’s performance
substantially. Yet building an alias detector is non-
trivial (Uryupina, 2004). English speakers know that
“President Clinton” is the same person as “Bill Clin-
ton” , not “President Bush”. But this cannot be im-
plemented by simple substring matching. It requires
some concept of the role of each word in the string.
Our model attempts to learn this role information by
clustering the words within named entities.
</bodyText>
<sectionHeader confidence="0.999953" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9997136">
Supervised named entity recognition now performs
almost as well as human annotation in English
(Chinchor, 1998) and has excellent performance on
other languages (Tjong Kim Sang and De Meul-
der, 2003). For a survey of the state of the art,
</bodyText>
<page confidence="0.978164">
164
</page>
<note confidence="0.8909755">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 164–172,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999726372881356">
see Nadeau and Sekine (2007). Of the features
we explore here, all but the pronoun information
were introduced in supervised work. Supervised ap-
proaches such as Black et al. (1998) have used clus-
tering to group together different nominals referring
to the same entity in ways similar to the “consis-
tency” approach outlined below in section 3.2.
Semi-supervised approaches have also achieved
notable success on the task. Co-training (Riloff and
Jones, 1999; Collins and Singer, 1999) begins with
a small set of labeling heuristics and gradually adds
examples to the training data. Various co-training
approaches presented in Collins and Singer (1999)
all score about 91% on a dataset of named entities;
the inital labels were assigned using 7 hand-written
seed rules. However, Collins and Singer (1999)
show that a mixture-of-naive-Bayes generative clus-
tering model (which they call an EM model), initial-
ized with the same seed rules, performs much more
poorly at 83%.
Much later work (Evans, 2003; Etzioni et al.,
2005; Cucerzan, 2007; Pasca, 2004) relies on the
use of extremely large corpora which allow very
precise, but sparse features. For instance Etzioni
et al. (2005) and Pasca (2004) use web queries to
count occurrences of “cities such as X” and simi-
lar phrases. Although our research makes use of a
fairly large amount of data, our method is designed
to make better use of relatively common contextual
features, rather than searching for high-quality se-
mantic features elsewhere.
Models of the internal structure of names have
been used for cross-document coreference (Li et al.,
2004; Bhattacharya and Getoor, 2006) and a goal in
their own right (Charniak, 2001). Li et al. (2004)
take named entity classes as a given, and develops
both generative and discriminative models to detect
coreference between members of each class. Their
generative model designates a particular mention of
a name as a “representative” and generates all other
mentions from it according to an editing process.
Bhattacharya and Getoor (2006) operates only on
authors of scientific papers. Their model accounts
for a wider variety of name variants than ours, in-
cluding misspellings and initials. In addition, they
confirm our intuition that Gibbs sampling for infer-
ence has insufficient mobility; rather than using a
heuristic algorithm as we do (see section 3.5), they
use a data-driven block sampler. Charniak (2001)
uses a Markov chain to generate 6 different com-
ponents of people’s names, again assuming that the
class of personal names can be pre-distinguished us-
ing a name list. He infers coreference relationships
between similar names appearing in the same docu-
ment, using the same notion of consistency between
names as our model. As with our model, the clusters
found are relatively good, although with some mis-
takes even on frequent items (for example, “John” is
sometimes treated as a descriptor like “Secretary”).
</bodyText>
<sectionHeader confidence="0.990447" genericHeader="method">
3 System Description
</sectionHeader>
<bodyText confidence="0.999869766666666">
Like Collins and Singer (1999), we assume that the
named entities have already been correctly extracted
from the text, and our task is merely to label them.
We assume that all entities fit into one of the three
MUC-7 categories, LOC (locations), ORG (organi-
zations), and PER (people). This is an oversimplifi-
cation; Collins and Singer (1999) show that about
12% of examples do not fit into these categories.
However, while using the MUC-7 data, we have no
way to evaluate on such examples.
As a framework for our models, we adopt adap-
tor grammars (Johnson et al., 2007), a framework
for non-parametric Bayesian inference over context-
free grammars. Although our system does not re-
quire the full expressive power of PCFGs, the adap-
tor grammar framework allows for easy develop-
ment of structured priors, and supplies a flexible
generic inference algorithm. An adaptor grammar is
a hierarchical Pitman-Yor process (Pitman and Yor,
1997). The grammar has two parts: a base PCFG
and a set of adapted nonterminals. Each adapted
nonterminal is a Pitman-Yor process which expands
either to a previously used subtree or to a sample
from the base PCFG. The end result is a posterior
distribution over PCFGs and over parse trees for
each example in our dataset.
Each of our models is an adaptor grammar based
on a particular base PCFG where the top nonter-
minal of each parse tree represents a named entity
class.
</bodyText>
<subsectionHeader confidence="0.989988">
3.1 Core NP Model
</subsectionHeader>
<bodyText confidence="0.999792">
We begin our analysis by reducing each named-
entity reference to the contiguous substring of
</bodyText>
<page confidence="0.955115">
165
</page>
<equation confidence="0.8820295">
ROOT →NE0|NE1|NE2
NE0 →(NE00)(NE10)(NE20)(NE30)(NE40)
∗NE00 →Words
∗Words →Word (Words)
Word →Bill ...
ROOT →NE0|NE1|NE2
NE0 →E00|E01 ... E0k
E00 →(E000)(E100)(E200)(E300)(E400)
∗ ∗ E000 →NE00
∗NE00 →Words . . .
</equation>
<figureCaption confidence="0.998630666666667">
Figure 1: Part of the grammar for core phrases. (Paren-
theses) mark optional nonterminals. *Starred nontermi-
nals are adapted.
</figureCaption>
<bodyText confidence="0.999961454545455">
proper nouns which surrounds its head, which we
call the core (Figure 1). To analyze the core, we use
a grammar with three main symbols (NEx), one for
each named entity class x. Each class has an asso-
ciated set of lexical symbols, which occur in a strict
order (NEix is the ith symbol for class x). We can
think of the NEi as the semantic parts of a proper
name; for people, NE0PER might generate titles and
NE1PER first names. Each NEi is adapted, and can
expand to any string of words; the ability to gen-
erate multiple words from a single symbol is use-
ful both because it can learn to group collocations
like “New York” and because it allows the system to
handle entities longer than four words. However, we
set the prior on multi-word expansions very low, to
avoid degenerate solutions where most phrases are
analyzed with a single symbol. The system learns
a separate probability for each ordered subset of the
NEi (for instance the rule NE0 → NE00 NE2 0 NE40),
so that it can represent constraints on possible refer-
ences; for instance, a last name can occur on its own,
but not a title.
</bodyText>
<subsectionHeader confidence="0.994641">
3.2 Consistency Model
</subsectionHeader>
<bodyText confidence="0.999973363636364">
This system captures some of our intuitions about
core phrases, but not all: our representation for “Bill
Clinton” does not share any information with “Presi-
dent Bill Clinton” except the named-entity class. To
remedy this, we introduce a set of “entity” nonter-
minals Ek, which enforce a weak notion of consis-
tency. We follow Charniak (2001) in assuming that
two names are consistent (can be references to the
same entity) if they do not have different expansions
for any lexical symbol. In other words, a particu-
lar entity EPER,Clinton has a title E0PER,Clinton =
</bodyText>
<figureCaption confidence="0.9894725">
Figure 2: Part of the consistency-enforcing grammar for
core phrases. There are an infinite number of entities
Exk, all with their own lexical symbols. Each lexical
symbol Eixk expands to a single NEix.
</figureCaption>
<bodyText confidence="0.999993666666667">
“President”, a first name E1PER,Clinton = “Bill” etc.
These are generated from the class-specific distribu-
tions, for instance E0PER,Clinton ∼ E0PER, which
we intend to be a distribution over titles in general.
The resulting grammar is shown in Figure 2; the
prior parameters for the entity-specific symbols Eixk
are fixed so that, with overwhelming probability,
only one expansion occurs. We can represent any
fixed number of entities Ek with a standard adap-
tor grammar, but since we do not know the correct
number, we must extend the adaptor model slightly
to allow for an unbounded number. We generate the
Ek from a Chinese Restaurant process prior. (Gen-
eral grammars with infinite numbers of nonterminals
were studied by (Liang et al., 2007b)).
</bodyText>
<subsectionHeader confidence="0.999689">
3.3 Modifiers, Prepositions and Pronouns
</subsectionHeader>
<bodyText confidence="0.999888235294118">
Next, we introduce two types of context information
derived from Collins and Singer (1999): nominal
modifiers and prepositional information. A nominal
modifier is either the head of an appositive phrase
(“Maury Cooper, a vice president”) or a non-proper
prenominal (“spokesman John Smith”)1. If the en-
tity is the complement of a preposition, we extract
the preposition and the head of the governing NP (“a
federally funded sewage plant in Georgia”). These
are added to the grammar at the named-entity class
level (separated from the core by a special punctua-
tion symbol).
Finally, we add information about pronouns and
wh-complementizers (Figure 3). Our pronoun infor-
mation is derived from an unsupervised coreference
algorithm which does not use named entity informa-
We stem modifiers with the Porter stemmer.
</bodyText>
<page confidence="0.975084">
166
</page>
<figure confidence="0.869269333333333">
ROOT →Modifiers0 # NE0 #
Prepositions0 # Pronouns0 #
. . .
Pronouns0 →Pronoun0 Pronouns0
Pronouns0 →
Pronoun0 →pers|loc|org|any
pers →i|he|she|who|me ...
loc →where|which|it|its
org →which|it|they|we ...
</figure>
<figureCaption confidence="0.994846">
Figure 3: A fragment of the full grammar. The symbol
</figureCaption>
<bodyText confidence="0.966758333333333">
# represents punctuation between different feature types.
The prior for class 0 is concentrated around personal pro-
nouns, although other types are possible.
tion (Charniak and Elsner, 2009). This algorithm
uses EM to learn a generative model with syntactic,
number and gender parameters. Like Haghighi and
Klein (2007), we give our model information about
the basic types of pronouns in English. By setting
up the base grammar so that each named-entity class
prefers to associate to a single type of pronoun, we
can also determine the correspondence between our
named-entity symbols and the actual named-entity
labels– for the models without pronoun information,
this matching is arbitrary and must be inferred dur-
ing the evaluation process.
</bodyText>
<subsectionHeader confidence="0.986204">
3.4 Data Preparation
</subsectionHeader>
<bodyText confidence="0.989440411764706">
To prepare data for clustering with our system, we
first parse it with the parser of Charniak and Johnson
(2005). We then annotate pronouns with Charniak
and Elsner (2009). For the evaluation set, we use the
named entity data from MUC-7. Here, we extract
all strings in &lt;ne&gt; tags and determine their cores,
plus any relevant modifiers, governing prepositions
and pronouns, by examining the parse trees. In addi-
tion, we supply the system with additional data from
the North American News Corpus (NANC). Here
we extract all NPs headed by proper nouns.
We then process our data by merging all exam-
ples with the same core; some merged examples
from our dataset are shown in Figure 4. When two
examples are merged, we concatenate their lists of
attack airlift airlift rescu # wing # of-commander
of-command with-run # #
</bodyText>
<equation confidence="0.5308965">
# air-india # # #
#abels##it#
# gaudreau # # they he #
# priddy # # he #
</equation>
<bodyText confidence="0.960584111111111">
spokesman bird bird bird director bird ford clin-
ton director bird # johnson # before-hearing
to-happened of-cartoon on-pressure under-medicare
to-according to-allied with-stuck of-government of-
photographs of-daughter of-photo for-embarrassing
under-instituted about-allegations for-worked
before-hearing to-secretary than-proposition of-
typical # he he his he my himself his he he he he i
he his his i i i he his #
</bodyText>
<figureCaption confidence="0.9993815">
Figure 4: Some merged examples from an input file. (#
separates different feature types.)
</figureCaption>
<bodyText confidence="0.999972583333333">
modifiers, prepositions and pronouns (capping the
length of each list at 20 to keep inference tractable).
For instance, “air-india” has no features outside the
core, while “wing” has some nominals (“attack”
&amp;c.) and some prepositions (“commander-of” &amp;c.).
This merging is useful because it allows us to do in-
ference based on types rather than tokens (Goldwa-
ter et al., 2006). It is well known that, to interpo-
late between types and tokens, Hierarchical Dirich-
let Processes (including adaptor grammars) require
a deeper hierarchy, which slows down inference and
reduces the mobility of sampling schemes. By merg-
ing examples, we avoid using this more complicated
model. Each merged example also represents many
examples from the training data, so we can summa-
rize features (such as modifiers) observed through-
out a large input corpus while keeping the size of
our input file small.
To create an input file, we first add all the MUC-
7 examples. We then draw additional examples
from NANC, ranking them by how many features
they have, until we reach a specified number (larger
datasets take longer, but without enough data, results
tend to be poor).
</bodyText>
<sectionHeader confidence="0.937087" genericHeader="method">
3.5 Inference
</sectionHeader>
<bodyText confidence="0.991464">
Our implementation of adaptor grammars is a mod-
ified version of the Pitman-Yor adaptor grammar
</bodyText>
<page confidence="0.990169">
167
</page>
<bodyText confidence="0.999989555555556">
sampler2, altered to deal with the infinite number of
entities. It carries out inference using a Metropolis-
within-Gibbs algorithm (Johnson et al., 2007), in
which it repeatedly parses each input line using the
CYK algorithm, samples a parse, and proposes this
as the new tree.
To do Gibbs sampling for our consistency-
enforcing model, we would need to sample a parse
for an example from the posterior over every pos-
sible entity. However, since there are thousands of
entities (the number grows roughly linearly with the
number of merged examples in the data file), this is
not tractable. Instead, we perform a restricted Gibbs
sampling search, where we enumerate the posterior
only for entities which share a word in their core
with the example in question. In fact, if the shared
word is very common (occuring in more than .001 of
examples), we compute the posterior for that entity
only .05 of the time3. These restrictions mean that
we do not compute the exact posterior. In particular,
the actual model allows entities to contain examples
with no words in common, but our search procedure
does not explore these solutions.
For our model, inference with the Gibbs algo-
rithm seems to lack mobility, sometimes falling into
very poor local minima from which it does not seem
to escape. This is because, if there are several ref-
erences to the same named entity with slightly dif-
ferent core phrases, once they are all assigned to
the wrong class, it requires a low-probability se-
ries of individual Gibbs moves to pull them out.
Similarly, the consistency-enforcing model gener-
ally does not fully cluster references to common en-
tities; there are usually several “Bill Clinton” clus-
ters which it would be best to combine, but the se-
quence of moves that does so is too improbable. The
data-merging process described above is one attempt
to improve mobility by reducing the number of du-
plicate examples. In addition, we found that it was a
better use of CPU time to run multiple samplers with
different initialization than to perform many itera-
tions. In the experiments below, we use 20 chains,
initializing with 50 iterations without using consis-
tency, then 50 more using the consistency model,
and evaluate the last sample from each. We discard
</bodyText>
<footnote confidence="0.998856333333333">
2Available at http://www.cog.brown.edu/ mj/Software.htm
3We ignore the corresponding Hastings correction, as in
practice it leads to too many rejections.
</footnote>
<bodyText confidence="0.9987755">
the 10 samples with worst log-likelihood and report
the average score for the other 10.
</bodyText>
<subsectionHeader confidence="0.990768">
3.6 Parameters
</subsectionHeader>
<bodyText confidence="0.999944052631579">
In addition to the base PCFG itself, the system re-
quires a few hyperparameter settings: Dirichlet pri-
ors for the rule weights of rules in the base PCFG.
Pitman-Yor parameters for the adapted nonterminals
are sampled from vague priors using a slice sam-
pler (Neal, 2003). The prior over core words was
set to the uniform distribution (Dirichlet 1.0) and the
prior for all modifiers, prepositions and pronouns to
a sparse value of .01. Beyond setting these param-
eters to a priori reasonable values, we did not opti-
mize them. To encourage the system to learn that
some lexical symbols were more common than oth-
ers, we set a sparse prior over expansions to sym-
bols4. There are two really important hyperparame-
ters: an extremely biased prior on class-to-pronoun-
type probabilities (1000 for the desired class, .0001
for everything else), and a prior of .0001 for the
Word → Word Words rule to discourage symbols
expanding to multiword strings.
</bodyText>
<sectionHeader confidence="0.999733" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9995864">
We performed experiments on the named entity
dataset from MUC-7 (Chinchor, 1998), using the
training set as development data and the formal test
set as test data. The development set has 4936
named entities, of which 1575 (31.9%) are locations,
2096 (42.5%) are organizations and 1265 (25.6%)
people. The test set has 4069 named entities, 1321
(32.5%) locations, 1862 (45.8%) organizations and
876 (21.5%) people5. We use a baseline which
gives all named entities the same label; this label is
mapped to “organization”.
In most of our experiments, we use an input file of
40000 lines. For dev experiments, the labeled data
contributes 1585 merged examples; for test experi-
ments, only 1320. The remaining lines are derived
</bodyText>
<footnote confidence="0.320033">
4Expansions that used only the middle three symbols
NE1,2,3
</footnote>
<bodyText confidence="0.99556825">
x got a prior of .005, expansions whose outermost sym-
bol was NE0,4
x got .0025, and so forth. This is not so impor-
tant for our final system, which has only 5 symbols, but was
designed during development to handle systems with up to 10
symbols.
510 entities are labeled location|organization; since this
fraction of the dataset is insignificant we score them as wrong.
</bodyText>
<page confidence="0.988097">
168
</page>
<figure confidence="0.744937833333333">
Model Accuracy
Baseline (All Org)
Core NPs (no consistency)
Core NPs (consistency)
Context Features
Pronouns
</figure>
<tableCaption confidence="0.7868755">
Table 1: Accuracy of various models on development
data.
</tableCaption>
<bodyText confidence="0.981330458333333">
using the process described in section 3.4 from 5
million words of NANC.
To evaluate our results, we map our three induced
labels to their corresponding gold label, then count
the overlap; as stated, this mapping is predictably
encoded in the prior when we use the pronoun fea-
tures. Our experimental results are shown in Table
1. All models perform above baseline, and all fea-
tures contribute significantly to the final result. Test
results for our final model are shown in Table 2.
A confusion matrix for our highest-likelihood test
solution is shown as Figure 5. The highest confusion
class is “organization”, which is confused most often
with “location” but also with “person”. “location” is
likewise confused with “organization”. “person” is
the easiest class to identify– we believe this explains
the slight decline in performance from dev to test,
since dev has proportionally more people.
Our mapping from grammar symbols to words ap-
pears in Table 3; the learned prepositional and mod-
ifier information is in Table 4. Overall the results
are good, but not perfect; for instance, the Pers
states are mostly interpretable as a sequence of ti-
tle - first name - middle name or initial - last name -
</bodyText>
<table confidence="0.95676425">
loc org per
LOC 1187 97 37
ORG 223 1517 122
PER 36 20 820
</table>
<figureCaption confidence="0.998538">
Figure 5: Confusion matrix for highest-likelihood test
run. Gold labels in CAPS, induced labels italicized. Or-
ganizations are most frequently confused.
</figureCaption>
<bodyText confidence="0.999913041666667">
last name or post-title (similar to (Charniak, 2001)).
The organization symbols tend to put nationalities
and other modifiers first, and end with institutional
types like “inc.” or “center”, although there is a sim-
ilar (but smaller) cluster of types at Org2, suggest-
ing the system has incorrectly found two analyses
for these names. Location symbols seem to put en-
tities with a single, non-analyzable name into Loc2,
and use symbols 0, 1 and 3 for compound names.
Loc4 has been recruited for time expressions, since
our NANC dataset includes many of these, but we
failed to account for them in the model. Since
they appear in a single class here, we are optimistic
that they could be clustered separately if another
class and some appropriate features were added to
the prior. Some errors do appear (“supreme court”
and “house” as locations, “minister” and “chairman”
as middle names, “newt gingrich” as a multiword
phrase). The table also reveals an unforeseen issue
with the parser: it tends to analyze the dateline be-
ginning a news story along with the following NP
(“WASHINGTON Bill Clinton said...”). Thus com-
mon datelines (“washington”, “new york” and “los
angeles”) appear in state 0 for each class.
</bodyText>
<sectionHeader confidence="0.998702" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999573142857143">
As stated above, we aim to build an unsupervised
generative model for named entity clustering, since
such a model could be integrated with unsupervised
coreference models like Haghighi and Klein (2007)
for joint inference. To our knowledge, the closest
existing system to such a model is the EM mix-
ture model used as a baseline in Collins and Singer
(1999). Our system improves on this EM system
in several ways. While they initialize with minimal
supervision in the form of 7 seed heuristics, ours is
fully unsupervised. Their results cover only exam-
ples which have a prepositional or modifier feature;
we adopt these features from their work, but label
all entities in the predefined test set, including those
that appear without these features. Finally, as dis-
cussed, we find the “person” category to be the eas-
iest to label. 33% of the test items in Collins and
Singer (1999) were people, as opposed to 21% of
ours. However, even without the pronoun features,
that is, using the same feature set, our system scores
equivalently to the EM model, at 83% (this score is
</bodyText>
<figure confidence="0.55782325">
42.5
45.5
48.5
83.3
87.1
Model Accuracy
Baseline (All Org) 45.8
Pronouns 86.0
</figure>
<tableCaption confidence="0.920772">
Table 2: Accuracy of the final model on test data.
</tableCaption>
<page confidence="0.971188">
169
</page>
<bodyText confidence="0.944911424242424">
Pers0 Pers1 Pers2 Pers3 Pers4
rep. john (767) minister brown jr.
sen. (256) robert (495) j. smith (97) a
washington david john (242) b smith (111)
dr. michael chairman johnson iii
los angeles james e. newt gingrich williams
senate president william (173) king wilson
house richard robert (155) miller brown
new york william (317) r. kennedy clinton
president sen. (236) martin simpson
republican george davis b
Org0 Org1 Org2 Org3 Org4
american (137) national university research association
washington american (182) inc. (166) medical center
washington the new york corp. (156) news inc. (257)
national international (136) college health corp. (252)
first public institute (87) services co.
los angeles united group communications committee
new house hospital development institute
royal federal museum policy council
british home press affairs fund
california world international (61) defense act
Loc0 Loc1 Loc2 Loc3 Loc4
washington (92) the texas county monday
los angeles st. new york city thursday
south new washington (22) beach river (57)
north national (69) united states valley tuesday
old east (65) baltimore island wednesday
grand mount california river (71) hotel
black fort capitol park friday
west (22) west (56) christmas bay hall
east (21) lake bosnia house center
haiti great san juan supreme court building
</bodyText>
<tableCaption confidence="0.998042">
Table 3: 10 most common words for each grammar symbol. Words which appear in multiple places have observed
counts indicated in parentheses.
</tableCaption>
<page confidence="0.849045">
170
</page>
<table confidence="0.999876818181818">
Pers-gov Pers-mod Org-gov Org-mod Loc-gov Loc-mod
according-to (1044) director president-of $ university-of calif.
played-by spokesman chairman-of giant city-of newspap[er]
directed-by leader director-of opposit[e] from-to state
led-by presid[ent] according-to (786) group town-of downtown
meeting-with attorney professor-at pp state-of n.y.
from-to candid[ate] head-of compan[y] center-in warrant
met-with lawyer department-of journal out-of va.
letter-to chairman member-of firm is-in fla.
secretary-of counsel members-of state house-of p.m.
known-as actor spokesman-for agenc[y] known-as itself
</table>
<tableCaption confidence="0.988598">
Table 4: 10 most common prepositional and modifier features for each named entity class. Modifiers were Porter-
stemmed; for clarity a reconstructed stem is shown in brackets.
</tableCaption>
<bodyText confidence="0.999966870967742">
on dev, 25% people). When the pronoun features are
added, our system’s performance increases to 86%,
significantly better than the EM system.
One motivation for our use of a structured model
which defined a notion of consistency between en-
tities was that it might allow the construction of
an unsupervised alias detector. According to the
model, two entities are consistent if they are in the
same class, and do not have conflicting assignments
of words to lexical symbols. Results here are at
best equivocal. The model is reasonable at pass-
ing basic tests– “Dr. Seuss” is not consistent with
“Dr. Strangelove”, “Dr. Quinn” etc, despite their
shared title, because the model identifies the sec-
ond element of each as a last name. Also correctly,
“Dr. William F. Gibson” is judged consistent with
“Dr. Gibson” and “Gibson” despite the missing el-
ements. But mistakes are commonplace. In the
“Gibson” case, the string “William F.” is misana-
lyzed as a multiword string, making the name in-
consistent with “William Gibson”; this is probably
the result of a search error, which, as we explained,
Gibbs sampling is unlikely to correct. In other cases,
the system clusters a family group together under
a single “entity” nonterminal by forcing their first
names into inappropriate states, for instance assign-
ing Pers1 Bruce, Pers2 Ellen, Pers3 Jarvis, where
Pers2 (usually a middle name) actually contains the
first name of a different individual. To improve this
aspect of our system, we might incorporate name-
specific features into the prior, such as abbreviations
and the concept of a family name. The most critical
improvement, however, would be integration with a
generative coreference system, since the document
context probably provides hints about which entities
are and are not coreferent.
The other key issue with our system is inference.
Currently we are extremely vulnerable to falling into
local minima, since the complex structure of the
model can easily lock a small group of examples
into a poor configuration. (The “William F. Gibson”
case above seems to be one of these.) In addition to
the block sampler used by Bhattacharya and Getoor
(2006), we are investigating general-purpose split-
merge samplers (Jain and Neal, 2000) and the per-
mutation sampler (Liang et al., 2007a). One inter-
esting question is how well these samplers perform
when faced with thousands of clusters (entities).
Despite these issues, we clearly show that it is
possible to build a good model of named entity class
while retaining compatibility with generative sys-
tems and without supervision. In addition, we do a
reasonable job learning the latent structure of names
in each named entity class. Our system improves
over the latent named-entity tagging in Haghighi
and Klein (2007), from 61% to 87%. This sug-
gests that it should indeed be possible to improve
on their coreference results without using a super-
vised named-entity model. How much improvement
is possible in practice, and whether joint inference
can also improve named-entity performance, remain
interesting questions for future work.
</bodyText>
<sectionHeader confidence="0.998558" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.995467">
We thank three reviewers for their comments, and
NSF for support via grants 0544127 and 0631667.
</bodyText>
<page confidence="0.997614">
171
</page>
<sectionHeader confidence="0.993797" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998541264150943">
Indrajit Bhattacharya and Lise Getoor. 2006. A latent
dirichlet model for unsupervised entity resolution. In
The SIAM International Conference on Data Mining
(SIAM-SDM), Bethesda, MD, USA.
William J. Black, Fabio Rinaldi, and David Mowatt.
1998. Facile: Description of the ne system used for
muc-7. In In Proceedings of the 7th Message Under-
standing Conference.
Eugene Charniak and Micha Elsner. 2009. EM works
for pronoun anaphora resolution. In Proceedings of
the Conference of the European Chapter of the As-
sociation for Computational Linguistics (EACL-09),
Athens, Greece.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proc. of the 2005 Meeting of the Assoc. for
Computational Linguistics (ACL), pages 173–180.
Eugene Charniak. 2001. Unsupervised learning of name
structure from coreference data. In NAACL-01.
Nancy A. Chinchor. 1998. Proceedings of the Sev-
enth Message Understanding Conference (MUC-7)
named entity task definition. In Proceedings of the
Seventh Message Understanding Conference (MUC-
7), page 21 pages, Fairfax, VA, April. version 3.5,
http://www.itl.nist.gov/iaui/894.02/related projects/muc/.
Michael Collins and Yorav Singer. 1999. Unsupervised
models for named entity classification. In Proceedings
of EMNLP 99.
Silviu Cucerzan. 2007. Large-scale named entity disam-
biguation based on Wikipedia data. In Proceedings of
EMNLP-CoNLL, pages 708–716, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana
maria Popescu, Tal Shaked, Stephen Soderl, Daniel S.
Weld, and Er Yates. 2005. Unsupervised named-
entity extraction from the web: An experimental study.
Artificial Intelligence, 165:91–134.
Richard Evans. 2003. A framework for named en-
tity recognition in the open domain. In Proceedings
of Recent Advances in Natural Language Processing
(RANLP-2003), pages 137 – 144, Borovetz, Bulgaria,
September.
Sharon Goldwater, Tom Griffiths, and Mark Johnson.
2006. Interpolating between types and tokens by es-
timating power-law generators. In Advances in Neural
Information Processing Systems (NIPS) 18.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric Bayesian
model. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics, pages
848–855. Association for Computational Linguistics.
Sonia Jain and Radford M. Neal. 2000. A split-merge
markov chain monte carlo procedure for the dirichlet
process mixture model. Journal of Computational and
Graphical Statistics, 13:158–182.
Mark Johnson, Tom L. Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Proceedings of NAACL 2007.
Xin Li, Paul Morie, and Dan Roth. 2004. Identification
and tracing of ambiguous names: Discriminative and
generative approaches. In AAAI, pages 419–424.
Percy Liang, Michael I. Jordan, and Ben Taskar. 2007a.
A permutation-augmented sampler for DP mixture
models. In Proceedings of ICML, pages 545–552,
New York, NY, USA. ACM.
Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein.
2007b. The infinite PCFG using hierarchical Dirichlet
processes. In Proceedings of EMNLP-CoNLL, pages
688–697, Prague, Czech Republic, June. Association
for Computational Linguistics.
A. Mikheev, C. Grover, and M. Moens. 1998. Descrip-
tion of the LTG System Used for MUC-7. In Pro-
ceedings of the 7th Message Understanding Confer-
ence (MUC-7), Fairfax, Virginia.
David Nadeau and Satoshi Sekine. 2007. A survey of
named entity recognition and classification. Journal
of Linguisticae Investigationes, 30(1).
Radford M. Neal. 2003. Slice sampling. Annals of
Statistics, 31:705–767.
Vincent Ng. 2008. Unsupervised models for coreference
resolution. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 640–649, Honolulu, Hawaii, October. Associa-
tion for Computational Linguistics.
Marius Pasca. 2004. Acquisition of categorized named
entities for web search. In CIKM ’04: Proceedings of
the thirteenth ACM international conference on Infor-
mation and knowledge management, pages 137–145,
New York, NY, USA. ACM.
Jim Pitman and Marc Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. Ann. Probab., 25:855–900.
Ellen Riloff and Rosie Jones. 1999. Learning dictio-
naries for information extraction by multi-level boot-
strapping. In Proceedings of the Sixteenth National
Conference on Artificial Intelligence, pages 472–479.
AAAI.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Walter
Daelemans and Miles Osborne, editors, Proceedings
of CoNLL-2003, pages 142–147. Edmonton, Canada.
Olga Uryupina. 2004. Evaluating name-matching for
coreference resolution. In Proceedings of LREC 04,
Lisbon.
</reference>
<page confidence="0.997877">
172
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.811125">
<title confidence="0.999435">Structured Generative Models for Unsupervised Named-Entity Clustering</title>
<author confidence="0.980449">Micha Elsner</author>
<author confidence="0.980449">Eugene Charniak</author>
<author confidence="0.980449">Mark</author>
<affiliation confidence="0.933343">Brown Laboratory for Linguistic Information Processing Brown</affiliation>
<address confidence="0.932982">Providence, RI</address>
<abstract confidence="0.997553">We describe a generative model for clustering named entities which also models named entity internal structure, clustering related words by role. The model is entirely unsupervised; it uses features from the named entity itself and its syntactic context, and coreference information from an unsupervised pronoun resolver. The model scores 86% on the MUC-7 named-entity dataset. To our knowledge, this is the best reported score for a fully unsupervised model, and the best score for a generative model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Indrajit Bhattacharya</author>
<author>Lise Getoor</author>
</authors>
<title>A latent dirichlet model for unsupervised entity resolution.</title>
<date>2006</date>
<booktitle>In The SIAM International Conference on Data Mining (SIAM-SDM),</booktitle>
<location>Bethesda, MD, USA.</location>
<contexts>
<context position="5157" citStr="Bhattacharya and Getoor, 2006" startWordPosition="810" endWordPosition="813"> Etzioni et al., 2005; Cucerzan, 2007; Pasca, 2004) relies on the use of extremely large corpora which allow very precise, but sparse features. For instance Etzioni et al. (2005) and Pasca (2004) use web queries to count occurrences of “cities such as X” and similar phrases. Although our research makes use of a fairly large amount of data, our method is designed to make better use of relatively common contextual features, rather than searching for high-quality semantic features elsewhere. Models of the internal structure of names have been used for cross-document coreference (Li et al., 2004; Bhattacharya and Getoor, 2006) and a goal in their own right (Charniak, 2001). Li et al. (2004) take named entity classes as a given, and develops both generative and discriminative models to detect coreference between members of each class. Their generative model designates a particular mention of a name as a “representative” and generates all other mentions from it according to an editing process. Bhattacharya and Getoor (2006) operates only on authors of scientific papers. Their model accounts for a wider variety of name variants than ours, including misspellings and initials. In addition, they confirm our intuition tha</context>
<context position="28469" citStr="Bhattacharya and Getoor (2006)" startWordPosition="4610" endWordPosition="4613">o the prior, such as abbreviations and the concept of a family name. The most critical improvement, however, would be integration with a generative coreference system, since the document context probably provides hints about which entities are and are not coreferent. The other key issue with our system is inference. Currently we are extremely vulnerable to falling into local minima, since the complex structure of the model can easily lock a small group of examples into a poor configuration. (The “William F. Gibson” case above seems to be one of these.) In addition to the block sampler used by Bhattacharya and Getoor (2006), we are investigating general-purpose splitmerge samplers (Jain and Neal, 2000) and the permutation sampler (Liang et al., 2007a). One interesting question is how well these samplers perform when faced with thousands of clusters (entities). Despite these issues, we clearly show that it is possible to build a good model of named entity class while retaining compatibility with generative systems and without supervision. In addition, we do a reasonable job learning the latent structure of names in each named entity class. Our system improves over the latent named-entity tagging in Haghighi and K</context>
</contexts>
<marker>Bhattacharya, Getoor, 2006</marker>
<rawString>Indrajit Bhattacharya and Lise Getoor. 2006. A latent dirichlet model for unsupervised entity resolution. In The SIAM International Conference on Data Mining (SIAM-SDM), Bethesda, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William J Black</author>
<author>Fabio Rinaldi</author>
<author>David Mowatt</author>
</authors>
<title>Facile: Description of the ne system used for muc-7. In</title>
<date>1998</date>
<booktitle>In Proceedings of the 7th Message Understanding Conference.</booktitle>
<contexts>
<context position="3710" citStr="Black et al. (1998)" startWordPosition="580" endWordPosition="583">Work Supervised named entity recognition now performs almost as well as human annotation in English (Chinchor, 1998) and has excellent performance on other languages (Tjong Kim Sang and De Meulder, 2003). For a survey of the state of the art, 164 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 164–172, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics see Nadeau and Sekine (2007). Of the features we explore here, all but the pronoun information were introduced in supervised work. Supervised approaches such as Black et al. (1998) have used clustering to group together different nominals referring to the same entity in ways similar to the “consistency” approach outlined below in section 3.2. Semi-supervised approaches have also achieved notable success on the task. Co-training (Riloff and Jones, 1999; Collins and Singer, 1999) begins with a small set of labeling heuristics and gradually adds examples to the training data. Various co-training approaches presented in Collins and Singer (1999) all score about 91% on a dataset of named entities; the inital labels were assigned using 7 hand-written seed rules. However, Coll</context>
</contexts>
<marker>Black, Rinaldi, Mowatt, 1998</marker>
<rawString>William J. Black, Fabio Rinaldi, and David Mowatt. 1998. Facile: Description of the ne system used for muc-7. In In Proceedings of the 7th Message Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Micha Elsner</author>
</authors>
<title>EM works for pronoun anaphora resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics (EACL-09),</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="12248" citStr="Charniak and Elsner, 2009" startWordPosition="1974" endWordPosition="1977">ers (Figure 3). Our pronoun information is derived from an unsupervised coreference algorithm which does not use named entity informaWe stem modifiers with the Porter stemmer. 166 ROOT →Modifiers0 # NE0 # Prepositions0 # Pronouns0 # . . . Pronouns0 →Pronoun0 Pronouns0 Pronouns0 → Pronoun0 →pers|loc|org|any pers →i|he|she|who|me ... loc →where|which|it|its org →which|it|they|we ... Figure 3: A fragment of the full grammar. The symbol # represents punctuation between different feature types. The prior for class 0 is concentrated around personal pronouns, although other types are possible. tion (Charniak and Elsner, 2009). This algorithm uses EM to learn a generative model with syntactic, number and gender parameters. Like Haghighi and Klein (2007), we give our model information about the basic types of pronouns in English. By setting up the base grammar so that each named-entity class prefers to associate to a single type of pronoun, we can also determine the correspondence between our named-entity symbols and the actual named-entity labels– for the models without pronoun information, this matching is arbitrary and must be inferred during the evaluation process. 3.4 Data Preparation To prepare data for cluste</context>
</contexts>
<marker>Charniak, Elsner, 2009</marker>
<rawString>Eugene Charniak and Micha Elsner. 2009. EM works for pronoun anaphora resolution. In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics (EACL-09), Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proc. of the 2005 Meeting of the Assoc. for Computational Linguistics (ACL),</booktitle>
<pages>173--180</pages>
<contexts>
<context position="12934" citStr="Charniak and Johnson (2005)" startWordPosition="2084" endWordPosition="2087">yntactic, number and gender parameters. Like Haghighi and Klein (2007), we give our model information about the basic types of pronouns in English. By setting up the base grammar so that each named-entity class prefers to associate to a single type of pronoun, we can also determine the correspondence between our named-entity symbols and the actual named-entity labels– for the models without pronoun information, this matching is arbitrary and must be inferred during the evaluation process. 3.4 Data Preparation To prepare data for clustering with our system, we first parse it with the parser of Charniak and Johnson (2005). We then annotate pronouns with Charniak and Elsner (2009). For the evaluation set, we use the named entity data from MUC-7. Here, we extract all strings in &lt;ne&gt; tags and determine their cores, plus any relevant modifiers, governing prepositions and pronouns, by examining the parse trees. In addition, we supply the system with additional data from the North American News Corpus (NANC). Here we extract all NPs headed by proper nouns. We then process our data by merging all examples with the same core; some merged examples from our dataset are shown in Figure 4. When two examples are merged, we</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and MaxEnt discriminative reranking. In Proc. of the 2005 Meeting of the Assoc. for Computational Linguistics (ACL), pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Unsupervised learning of name structure from coreference data.</title>
<date>2001</date>
<booktitle>In NAACL-01.</booktitle>
<contexts>
<context position="5204" citStr="Charniak, 2001" startWordPosition="821" endWordPosition="822">the use of extremely large corpora which allow very precise, but sparse features. For instance Etzioni et al. (2005) and Pasca (2004) use web queries to count occurrences of “cities such as X” and similar phrases. Although our research makes use of a fairly large amount of data, our method is designed to make better use of relatively common contextual features, rather than searching for high-quality semantic features elsewhere. Models of the internal structure of names have been used for cross-document coreference (Li et al., 2004; Bhattacharya and Getoor, 2006) and a goal in their own right (Charniak, 2001). Li et al. (2004) take named entity classes as a given, and develops both generative and discriminative models to detect coreference between members of each class. Their generative model designates a particular mention of a name as a “representative” and generates all other mentions from it according to an editing process. Bhattacharya and Getoor (2006) operates only on authors of scientific papers. Their model accounts for a wider variety of name variants than ours, including misspellings and initials. In addition, they confirm our intuition that Gibbs sampling for inference has insufficient</context>
<context position="9772" citStr="Charniak (2001)" startWordPosition="1587" endWordPosition="1588">ol. The system learns a separate probability for each ordered subset of the NEi (for instance the rule NE0 → NE00 NE2 0 NE40), so that it can represent constraints on possible references; for instance, a last name can occur on its own, but not a title. 3.2 Consistency Model This system captures some of our intuitions about core phrases, but not all: our representation for “Bill Clinton” does not share any information with “President Bill Clinton” except the named-entity class. To remedy this, we introduce a set of “entity” nonterminals Ek, which enforce a weak notion of consistency. We follow Charniak (2001) in assuming that two names are consistent (can be references to the same entity) if they do not have different expansions for any lexical symbol. In other words, a particular entity EPER,Clinton has a title E0PER,Clinton = Figure 2: Part of the consistency-enforcing grammar for core phrases. There are an infinite number of entities Exk, all with their own lexical symbols. Each lexical symbol Eixk expands to a single NEix. “President”, a first name E1PER,Clinton = “Bill” etc. These are generated from the class-specific distributions, for instance E0PER,Clinton ∼ E0PER, which we intend to be a </context>
<context position="21703" citStr="Charniak, 2001" startWordPosition="3540" endWordPosition="3541">test, since dev has proportionally more people. Our mapping from grammar symbols to words appears in Table 3; the learned prepositional and modifier information is in Table 4. Overall the results are good, but not perfect; for instance, the Pers states are mostly interpretable as a sequence of title - first name - middle name or initial - last name - loc org per LOC 1187 97 37 ORG 223 1517 122 PER 36 20 820 Figure 5: Confusion matrix for highest-likelihood test run. Gold labels in CAPS, induced labels italicized. Organizations are most frequently confused. last name or post-title (similar to (Charniak, 2001)). The organization symbols tend to put nationalities and other modifiers first, and end with institutional types like “inc.” or “center”, although there is a similar (but smaller) cluster of types at Org2, suggesting the system has incorrectly found two analyses for these names. Location symbols seem to put entities with a single, non-analyzable name into Loc2, and use symbols 0, 1 and 3 for compound names. Loc4 has been recruited for time expressions, since our NANC dataset includes many of these, but we failed to account for them in the model. Since they appear in a single class here, we ar</context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>Eugene Charniak. 2001. Unsupervised learning of name structure from coreference data. In NAACL-01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy A Chinchor</author>
</authors>
<title>named entity task definition.</title>
<date>1998</date>
<booktitle>Proceedings of the Seventh Message Understanding Conference</booktitle>
<pages>21</pages>
<location>Fairfax, VA,</location>
<note>version 3.5, http://www.itl.nist.gov/iaui/894.02/related projects/muc/.</note>
<contexts>
<context position="951" citStr="Chinchor, 1998" startWordPosition="137" endWordPosition="138">amed entity internal structure, clustering related words by role. The model is entirely unsupervised; it uses features from the named entity itself and its syntactic context, and coreference information from an unsupervised pronoun resolver. The model scores 86% on the MUC-7 named-entity dataset. To our knowledge, this is the best reported score for a fully unsupervised model, and the best score for a generative model. 1 Introduction Named entity clustering is a classic task in NLP, and one for which both supervised and semi-supervised systems have excellent performance (Mikheev et al., 1998; Chinchor, 1998). In this paper, we describe a fully unsupervised system (using no “seed rules” or initial heuristics); to our knowledge this is the best such system reported on the MUC-7 dataset. In addition, the model clusters the words which appear in named entities, discovering groups of words with similar roles such as first names and types of organization. Finally, the model defines a notion of consistency between different references to the same entity; this component of the model yields a significant increase in performance. The main motivation for our system is the recent success of unsupervised gene</context>
<context position="3207" citStr="Chinchor, 1998" startWordPosition="501" endWordPosition="502"> for acronyms, abbreviations and name variants– improve the model’s performance substantially. Yet building an alias detector is nontrivial (Uryupina, 2004). English speakers know that “President Clinton” is the same person as “Bill Clinton” , not “President Bush”. But this cannot be implemented by simple substring matching. It requires some concept of the role of each word in the string. Our model attempts to learn this role information by clustering the words within named entities. 2 Related Work Supervised named entity recognition now performs almost as well as human annotation in English (Chinchor, 1998) and has excellent performance on other languages (Tjong Kim Sang and De Meulder, 2003). For a survey of the state of the art, 164 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 164–172, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics see Nadeau and Sekine (2007). Of the features we explore here, all but the pronoun information were introduced in supervised work. Supervised approaches such as Black et al. (1998) have used clustering to group together different nominals referring to the same entity in ways s</context>
<context position="19000" citStr="Chinchor, 1998" startWordPosition="3090" endWordPosition="3091">se value of .01. Beyond setting these parameters to a priori reasonable values, we did not optimize them. To encourage the system to learn that some lexical symbols were more common than others, we set a sparse prior over expansions to symbols4. There are two really important hyperparameters: an extremely biased prior on class-to-pronountype probabilities (1000 for the desired class, .0001 for everything else), and a prior of .0001 for the Word → Word Words rule to discourage symbols expanding to multiword strings. 4 Experiments We performed experiments on the named entity dataset from MUC-7 (Chinchor, 1998), using the training set as development data and the formal test set as test data. The development set has 4936 named entities, of which 1575 (31.9%) are locations, 2096 (42.5%) are organizations and 1265 (25.6%) people. The test set has 4069 named entities, 1321 (32.5%) locations, 1862 (45.8%) organizations and 876 (21.5%) people5. We use a baseline which gives all named entities the same label; this label is mapped to “organization”. In most of our experiments, we use an input file of 40000 lines. For dev experiments, the labeled data contributes 1585 merged examples; for test experiments, o</context>
</contexts>
<marker>Chinchor, 1998</marker>
<rawString>Nancy A. Chinchor. 1998. Proceedings of the Seventh Message Understanding Conference (MUC-7) named entity task definition. In Proceedings of the Seventh Message Understanding Conference (MUC7), page 21 pages, Fairfax, VA, April. version 3.5, http://www.itl.nist.gov/iaui/894.02/related projects/muc/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yorav Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proceedings of EMNLP 99.</booktitle>
<contexts>
<context position="4012" citStr="Collins and Singer, 1999" startWordPosition="626" endWordPosition="629">erence of the North American Chapter of the ACL, pages 164–172, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics see Nadeau and Sekine (2007). Of the features we explore here, all but the pronoun information were introduced in supervised work. Supervised approaches such as Black et al. (1998) have used clustering to group together different nominals referring to the same entity in ways similar to the “consistency” approach outlined below in section 3.2. Semi-supervised approaches have also achieved notable success on the task. Co-training (Riloff and Jones, 1999; Collins and Singer, 1999) begins with a small set of labeling heuristics and gradually adds examples to the training data. Various co-training approaches presented in Collins and Singer (1999) all score about 91% on a dataset of named entities; the inital labels were assigned using 7 hand-written seed rules. However, Collins and Singer (1999) show that a mixture-of-naive-Bayes generative clustering model (which they call an EM model), initialized with the same seed rules, performs much more poorly at 83%. Much later work (Evans, 2003; Etzioni et al., 2005; Cucerzan, 2007; Pasca, 2004) relies on the use of extremely la</context>
<context position="6496" citStr="Collins and Singer (1999)" startWordPosition="1024" endWordPosition="1027">ction 3.5), they use a data-driven block sampler. Charniak (2001) uses a Markov chain to generate 6 different components of people’s names, again assuming that the class of personal names can be pre-distinguished using a name list. He infers coreference relationships between similar names appearing in the same document, using the same notion of consistency between names as our model. As with our model, the clusters found are relatively good, although with some mistakes even on frequent items (for example, “John” is sometimes treated as a descriptor like “Secretary”). 3 System Description Like Collins and Singer (1999), we assume that the named entities have already been correctly extracted from the text, and our task is merely to label them. We assume that all entities fit into one of the three MUC-7 categories, LOC (locations), ORG (organizations), and PER (people). This is an oversimplification; Collins and Singer (1999) show that about 12% of examples do not fit into these categories. However, while using the MUC-7 data, we have no way to evaluate on such examples. As a framework for our models, we adopt adaptor grammars (Johnson et al., 2007), a framework for non-parametric Bayesian inference over cont</context>
<context position="11078" citStr="Collins and Singer (1999)" startWordPosition="1796" endWordPosition="1799">the prior parameters for the entity-specific symbols Eixk are fixed so that, with overwhelming probability, only one expansion occurs. We can represent any fixed number of entities Ek with a standard adaptor grammar, but since we do not know the correct number, we must extend the adaptor model slightly to allow for an unbounded number. We generate the Ek from a Chinese Restaurant process prior. (General grammars with infinite numbers of nonterminals were studied by (Liang et al., 2007b)). 3.3 Modifiers, Prepositions and Pronouns Next, we introduce two types of context information derived from Collins and Singer (1999): nominal modifiers and prepositional information. A nominal modifier is either the head of an appositive phrase (“Maury Cooper, a vice president”) or a non-proper prenominal (“spokesman John Smith”)1. If the entity is the complement of a preposition, we extract the preposition and the head of the governing NP (“a federally funded sewage plant in Georgia”). These are added to the grammar at the named-entity class level (separated from the core by a special punctuation symbol). Finally, we add information about pronouns and wh-complementizers (Figure 3). Our pronoun information is derived from </context>
<context position="23227" citStr="Collins and Singer (1999)" startWordPosition="3792" endWordPosition="3795">unforeseen issue with the parser: it tends to analyze the dateline beginning a news story along with the following NP (“WASHINGTON Bill Clinton said...”). Thus common datelines (“washington”, “new york” and “los angeles”) appear in state 0 for each class. 5 Discussion As stated above, we aim to build an unsupervised generative model for named entity clustering, since such a model could be integrated with unsupervised coreference models like Haghighi and Klein (2007) for joint inference. To our knowledge, the closest existing system to such a model is the EM mixture model used as a baseline in Collins and Singer (1999). Our system improves on this EM system in several ways. While they initialize with minimal supervision in the form of 7 seed heuristics, ours is fully unsupervised. Their results cover only examples which have a prepositional or modifier feature; we adopt these features from their work, but label all entities in the predefined test set, including those that appear without these features. Finally, as discussed, we find the “person” category to be the easiest to label. 33% of the test items in Collins and Singer (1999) were people, as opposed to 21% of ours. However, even without the pronoun fe</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins and Yorav Singer. 1999. Unsupervised models for named entity classification. In Proceedings of EMNLP 99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu Cucerzan</author>
</authors>
<title>Large-scale named entity disambiguation based on Wikipedia data.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>708--716</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="4564" citStr="Cucerzan, 2007" startWordPosition="716" endWordPosition="717">Co-training (Riloff and Jones, 1999; Collins and Singer, 1999) begins with a small set of labeling heuristics and gradually adds examples to the training data. Various co-training approaches presented in Collins and Singer (1999) all score about 91% on a dataset of named entities; the inital labels were assigned using 7 hand-written seed rules. However, Collins and Singer (1999) show that a mixture-of-naive-Bayes generative clustering model (which they call an EM model), initialized with the same seed rules, performs much more poorly at 83%. Much later work (Evans, 2003; Etzioni et al., 2005; Cucerzan, 2007; Pasca, 2004) relies on the use of extremely large corpora which allow very precise, but sparse features. For instance Etzioni et al. (2005) and Pasca (2004) use web queries to count occurrences of “cities such as X” and similar phrases. Although our research makes use of a fairly large amount of data, our method is designed to make better use of relatively common contextual features, rather than searching for high-quality semantic features elsewhere. Models of the internal structure of names have been used for cross-document coreference (Li et al., 2004; Bhattacharya and Getoor, 2006) and a </context>
</contexts>
<marker>Cucerzan, 2007</marker>
<rawString>Silviu Cucerzan. 2007. Large-scale named entity disambiguation based on Wikipedia data. In Proceedings of EMNLP-CoNLL, pages 708–716, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michael Cafarella</author>
<author>Doug Downey</author>
<author>Ana maria Popescu</author>
<author>Tal Shaked</author>
<author>Stephen Soderl</author>
<author>Daniel S Weld</author>
<author>Er Yates</author>
</authors>
<title>Unsupervised namedentity extraction from the web: An experimental study.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<pages>165--91</pages>
<contexts>
<context position="4548" citStr="Etzioni et al., 2005" startWordPosition="712" endWordPosition="715"> success on the task. Co-training (Riloff and Jones, 1999; Collins and Singer, 1999) begins with a small set of labeling heuristics and gradually adds examples to the training data. Various co-training approaches presented in Collins and Singer (1999) all score about 91% on a dataset of named entities; the inital labels were assigned using 7 hand-written seed rules. However, Collins and Singer (1999) show that a mixture-of-naive-Bayes generative clustering model (which they call an EM model), initialized with the same seed rules, performs much more poorly at 83%. Much later work (Evans, 2003; Etzioni et al., 2005; Cucerzan, 2007; Pasca, 2004) relies on the use of extremely large corpora which allow very precise, but sparse features. For instance Etzioni et al. (2005) and Pasca (2004) use web queries to count occurrences of “cities such as X” and similar phrases. Although our research makes use of a fairly large amount of data, our method is designed to make better use of relatively common contextual features, rather than searching for high-quality semantic features elsewhere. Models of the internal structure of names have been used for cross-document coreference (Li et al., 2004; Bhattacharya and Geto</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderl, Weld, Yates, 2005</marker>
<rawString>Oren Etzioni, Michael Cafarella, Doug Downey, Ana maria Popescu, Tal Shaked, Stephen Soderl, Daniel S. Weld, and Er Yates. 2005. Unsupervised namedentity extraction from the web: An experimental study. Artificial Intelligence, 165:91–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Evans</author>
</authors>
<title>A framework for named entity recognition in the open domain.</title>
<date>2003</date>
<booktitle>In Proceedings of Recent Advances in Natural Language Processing (RANLP-2003),</booktitle>
<pages>137--144</pages>
<location>Borovetz, Bulgaria,</location>
<contexts>
<context position="4526" citStr="Evans, 2003" startWordPosition="710" endWordPosition="711">ieved notable success on the task. Co-training (Riloff and Jones, 1999; Collins and Singer, 1999) begins with a small set of labeling heuristics and gradually adds examples to the training data. Various co-training approaches presented in Collins and Singer (1999) all score about 91% on a dataset of named entities; the inital labels were assigned using 7 hand-written seed rules. However, Collins and Singer (1999) show that a mixture-of-naive-Bayes generative clustering model (which they call an EM model), initialized with the same seed rules, performs much more poorly at 83%. Much later work (Evans, 2003; Etzioni et al., 2005; Cucerzan, 2007; Pasca, 2004) relies on the use of extremely large corpora which allow very precise, but sparse features. For instance Etzioni et al. (2005) and Pasca (2004) use web queries to count occurrences of “cities such as X” and similar phrases. Although our research makes use of a fairly large amount of data, our method is designed to make better use of relatively common contextual features, rather than searching for high-quality semantic features elsewhere. Models of the internal structure of names have been used for cross-document coreference (Li et al., 2004;</context>
</contexts>
<marker>Evans, 2003</marker>
<rawString>Richard Evans. 2003. A framework for named entity recognition in the open domain. In Proceedings of Recent Advances in Natural Language Processing (RANLP-2003), pages 137 – 144, Borovetz, Bulgaria, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Tom Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Interpolating between types and tokens by estimating power-law generators.</title>
<date>2006</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS) 18.</booktitle>
<contexts>
<context position="14588" citStr="Goldwater et al., 2006" startWordPosition="2353" endWordPosition="2357">uted about-allegations for-worked before-hearing to-secretary than-proposition oftypical # he he his he my himself his he he he he i he his his i i i he his # Figure 4: Some merged examples from an input file. (# separates different feature types.) modifiers, prepositions and pronouns (capping the length of each list at 20 to keep inference tractable). For instance, “air-india” has no features outside the core, while “wing” has some nominals (“attack” &amp;c.) and some prepositions (“commander-of” &amp;c.). This merging is useful because it allows us to do inference based on types rather than tokens (Goldwater et al., 2006). It is well known that, to interpolate between types and tokens, Hierarchical Dirichlet Processes (including adaptor grammars) require a deeper hierarchy, which slows down inference and reduces the mobility of sampling schemes. By merging examples, we avoid using this more complicated model. Each merged example also represents many examples from the training data, so we can summarize features (such as modifiers) observed throughout a large input corpus while keeping the size of our input file small. To create an input file, we first add all the MUC7 examples. We then draw additional examples </context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Tom Griffiths, and Mark Johnson. 2006. Interpolating between types and tokens by estimating power-law generators. In Advances in Neural Information Processing Systems (NIPS) 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Unsupervised coreference resolution in a nonparametric Bayesian model.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>848--855</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1631" citStr="Haghighi and Klein (2007)" startWordPosition="247" endWordPosition="250">m (using no “seed rules” or initial heuristics); to our knowledge this is the best such system reported on the MUC-7 dataset. In addition, the model clusters the words which appear in named entities, discovering groups of words with similar roles such as first names and types of organization. Finally, the model defines a notion of consistency between different references to the same entity; this component of the model yields a significant increase in performance. The main motivation for our system is the recent success of unsupervised generative models for coreference resolution. The model of Haghighi and Klein (2007) incorporated a latent variable for named entity class. They report a named entity score of 61.2 percent, well above the baseline of 46.4, but still far behind existing named-entity systems. We suspect that better models for named entities could aid in the coreference task. The easiest way to incorporate a better model is simply to run a supervised or semi-supervised system as a preprocess. To perform joint inference, however, requires an unsupervised generative model for named entities. As far as we know, this work is the best such model. Named entities also pose another problem with the Hagh</context>
<context position="12377" citStr="Haghighi and Klein (2007)" startWordPosition="1994" endWordPosition="1997">rmaWe stem modifiers with the Porter stemmer. 166 ROOT →Modifiers0 # NE0 # Prepositions0 # Pronouns0 # . . . Pronouns0 →Pronoun0 Pronouns0 Pronouns0 → Pronoun0 →pers|loc|org|any pers →i|he|she|who|me ... loc →where|which|it|its org →which|it|they|we ... Figure 3: A fragment of the full grammar. The symbol # represents punctuation between different feature types. The prior for class 0 is concentrated around personal pronouns, although other types are possible. tion (Charniak and Elsner, 2009). This algorithm uses EM to learn a generative model with syntactic, number and gender parameters. Like Haghighi and Klein (2007), we give our model information about the basic types of pronouns in English. By setting up the base grammar so that each named-entity class prefers to associate to a single type of pronoun, we can also determine the correspondence between our named-entity symbols and the actual named-entity labels– for the models without pronoun information, this matching is arbitrary and must be inferred during the evaluation process. 3.4 Data Preparation To prepare data for clustering with our system, we first parse it with the parser of Charniak and Johnson (2005). We then annotate pronouns with Charniak a</context>
<context position="23072" citStr="Haghighi and Klein (2007)" startWordPosition="3763" endWordPosition="3766">ar (“supreme court” and “house” as locations, “minister” and “chairman” as middle names, “newt gingrich” as a multiword phrase). The table also reveals an unforeseen issue with the parser: it tends to analyze the dateline beginning a news story along with the following NP (“WASHINGTON Bill Clinton said...”). Thus common datelines (“washington”, “new york” and “los angeles”) appear in state 0 for each class. 5 Discussion As stated above, we aim to build an unsupervised generative model for named entity clustering, since such a model could be integrated with unsupervised coreference models like Haghighi and Klein (2007) for joint inference. To our knowledge, the closest existing system to such a model is the EM mixture model used as a baseline in Collins and Singer (1999). Our system improves on this EM system in several ways. While they initialize with minimal supervision in the form of 7 seed heuristics, ours is fully unsupervised. Their results cover only examples which have a prepositional or modifier feature; we adopt these features from their work, but label all entities in the predefined test set, including those that appear without these features. Finally, as discussed, we find the “person” category </context>
</contexts>
<marker>Haghighi, Klein, 2007</marker>
<rawString>Aria Haghighi and Dan Klein. 2007. Unsupervised coreference resolution in a nonparametric Bayesian model. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages 848–855. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonia Jain</author>
<author>Radford M Neal</author>
</authors>
<title>A split-merge markov chain monte carlo procedure for the dirichlet process mixture model.</title>
<date>2000</date>
<journal>Journal of Computational and Graphical Statistics,</journal>
<pages>13--158</pages>
<contexts>
<context position="28549" citStr="Jain and Neal, 2000" startWordPosition="4621" endWordPosition="4624">rovement, however, would be integration with a generative coreference system, since the document context probably provides hints about which entities are and are not coreferent. The other key issue with our system is inference. Currently we are extremely vulnerable to falling into local minima, since the complex structure of the model can easily lock a small group of examples into a poor configuration. (The “William F. Gibson” case above seems to be one of these.) In addition to the block sampler used by Bhattacharya and Getoor (2006), we are investigating general-purpose splitmerge samplers (Jain and Neal, 2000) and the permutation sampler (Liang et al., 2007a). One interesting question is how well these samplers perform when faced with thousands of clusters (entities). Despite these issues, we clearly show that it is possible to build a good model of named entity class while retaining compatibility with generative systems and without supervision. In addition, we do a reasonable job learning the latent structure of names in each named entity class. Our system improves over the latent named-entity tagging in Haghighi and Klein (2007), from 61% to 87%. This suggests that it should indeed be possible to</context>
</contexts>
<marker>Jain, Neal, 2000</marker>
<rawString>Sonia Jain and Radford M. Neal. 2000. A split-merge markov chain monte carlo procedure for the dirichlet process mixture model. Journal of Computational and Graphical Statistics, 13:158–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Tom L Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Bayesian inference for PCFGs via Markov chain Monte Carlo.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL</booktitle>
<contexts>
<context position="7035" citStr="Johnson et al., 2007" startWordPosition="1118" endWordPosition="1121"> descriptor like “Secretary”). 3 System Description Like Collins and Singer (1999), we assume that the named entities have already been correctly extracted from the text, and our task is merely to label them. We assume that all entities fit into one of the three MUC-7 categories, LOC (locations), ORG (organizations), and PER (people). This is an oversimplification; Collins and Singer (1999) show that about 12% of examples do not fit into these categories. However, while using the MUC-7 data, we have no way to evaluate on such examples. As a framework for our models, we adopt adaptor grammars (Johnson et al., 2007), a framework for non-parametric Bayesian inference over contextfree grammars. Although our system does not require the full expressive power of PCFGs, the adaptor grammar framework allows for easy development of structured priors, and supplies a flexible generic inference algorithm. An adaptor grammar is a hierarchical Pitman-Yor process (Pitman and Yor, 1997). The grammar has two parts: a base PCFG and a set of adapted nonterminals. Each adapted nonterminal is a Pitman-Yor process which expands either to a previously used subtree or to a sample from the base PCFG. The end result is a posteri</context>
<context position="15624" citStr="Johnson et al., 2007" startWordPosition="2523" endWordPosition="2526">) observed throughout a large input corpus while keeping the size of our input file small. To create an input file, we first add all the MUC7 examples. We then draw additional examples from NANC, ranking them by how many features they have, until we reach a specified number (larger datasets take longer, but without enough data, results tend to be poor). 3.5 Inference Our implementation of adaptor grammars is a modified version of the Pitman-Yor adaptor grammar 167 sampler2, altered to deal with the infinite number of entities. It carries out inference using a Metropoliswithin-Gibbs algorithm (Johnson et al., 2007), in which it repeatedly parses each input line using the CYK algorithm, samples a parse, and proposes this as the new tree. To do Gibbs sampling for our consistencyenforcing model, we would need to sample a parse for an example from the posterior over every possible entity. However, since there are thousands of entities (the number grows roughly linearly with the number of merged examples in the data file), this is not tractable. Instead, we perform a restricted Gibbs sampling search, where we enumerate the posterior only for entities which share a word in their core with the example in quest</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Tom L. Griffiths, and Sharon Goldwater. 2007. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Proceedings of NAACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Li</author>
<author>Paul Morie</author>
<author>Dan Roth</author>
</authors>
<title>Identification and tracing of ambiguous names: Discriminative and generative approaches.</title>
<date>2004</date>
<booktitle>In AAAI,</booktitle>
<pages>419--424</pages>
<contexts>
<context position="5125" citStr="Li et al., 2004" startWordPosition="806" endWordPosition="809">ork (Evans, 2003; Etzioni et al., 2005; Cucerzan, 2007; Pasca, 2004) relies on the use of extremely large corpora which allow very precise, but sparse features. For instance Etzioni et al. (2005) and Pasca (2004) use web queries to count occurrences of “cities such as X” and similar phrases. Although our research makes use of a fairly large amount of data, our method is designed to make better use of relatively common contextual features, rather than searching for high-quality semantic features elsewhere. Models of the internal structure of names have been used for cross-document coreference (Li et al., 2004; Bhattacharya and Getoor, 2006) and a goal in their own right (Charniak, 2001). Li et al. (2004) take named entity classes as a given, and develops both generative and discriminative models to detect coreference between members of each class. Their generative model designates a particular mention of a name as a “representative” and generates all other mentions from it according to an editing process. Bhattacharya and Getoor (2006) operates only on authors of scientific papers. Their model accounts for a wider variety of name variants than ours, including misspellings and initials. In addition</context>
</contexts>
<marker>Li, Morie, Roth, 2004</marker>
<rawString>Xin Li, Paul Morie, and Dan Roth. 2004. Identification and tracing of ambiguous names: Discriminative and generative approaches. In AAAI, pages 419–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael I Jordan</author>
<author>Ben Taskar</author>
</authors>
<title>A permutation-augmented sampler for DP mixture models.</title>
<date>2007</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>545--552</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="10942" citStr="Liang et al., 2007" startWordPosition="1777" endWordPosition="1780"> E0PER,Clinton ∼ E0PER, which we intend to be a distribution over titles in general. The resulting grammar is shown in Figure 2; the prior parameters for the entity-specific symbols Eixk are fixed so that, with overwhelming probability, only one expansion occurs. We can represent any fixed number of entities Ek with a standard adaptor grammar, but since we do not know the correct number, we must extend the adaptor model slightly to allow for an unbounded number. We generate the Ek from a Chinese Restaurant process prior. (General grammars with infinite numbers of nonterminals were studied by (Liang et al., 2007b)). 3.3 Modifiers, Prepositions and Pronouns Next, we introduce two types of context information derived from Collins and Singer (1999): nominal modifiers and prepositional information. A nominal modifier is either the head of an appositive phrase (“Maury Cooper, a vice president”) or a non-proper prenominal (“spokesman John Smith”)1. If the entity is the complement of a preposition, we extract the preposition and the head of the governing NP (“a federally funded sewage plant in Georgia”). These are added to the grammar at the named-entity class level (separated from the core by a special pun</context>
<context position="28597" citStr="Liang et al., 2007" startWordPosition="4630" endWordPosition="4633">erative coreference system, since the document context probably provides hints about which entities are and are not coreferent. The other key issue with our system is inference. Currently we are extremely vulnerable to falling into local minima, since the complex structure of the model can easily lock a small group of examples into a poor configuration. (The “William F. Gibson” case above seems to be one of these.) In addition to the block sampler used by Bhattacharya and Getoor (2006), we are investigating general-purpose splitmerge samplers (Jain and Neal, 2000) and the permutation sampler (Liang et al., 2007a). One interesting question is how well these samplers perform when faced with thousands of clusters (entities). Despite these issues, we clearly show that it is possible to build a good model of named entity class while retaining compatibility with generative systems and without supervision. In addition, we do a reasonable job learning the latent structure of names in each named entity class. Our system improves over the latent named-entity tagging in Haghighi and Klein (2007), from 61% to 87%. This suggests that it should indeed be possible to improve on their coreference results without us</context>
</contexts>
<marker>Liang, Jordan, Taskar, 2007</marker>
<rawString>Percy Liang, Michael I. Jordan, and Ben Taskar. 2007a. A permutation-augmented sampler for DP mixture models. In Proceedings of ICML, pages 545–552, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Slav Petrov</author>
<author>Michael Jordan</author>
<author>Dan Klein</author>
</authors>
<title>The infinite PCFG using hierarchical Dirichlet processes.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>688--697</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="10942" citStr="Liang et al., 2007" startWordPosition="1777" endWordPosition="1780"> E0PER,Clinton ∼ E0PER, which we intend to be a distribution over titles in general. The resulting grammar is shown in Figure 2; the prior parameters for the entity-specific symbols Eixk are fixed so that, with overwhelming probability, only one expansion occurs. We can represent any fixed number of entities Ek with a standard adaptor grammar, but since we do not know the correct number, we must extend the adaptor model slightly to allow for an unbounded number. We generate the Ek from a Chinese Restaurant process prior. (General grammars with infinite numbers of nonterminals were studied by (Liang et al., 2007b)). 3.3 Modifiers, Prepositions and Pronouns Next, we introduce two types of context information derived from Collins and Singer (1999): nominal modifiers and prepositional information. A nominal modifier is either the head of an appositive phrase (“Maury Cooper, a vice president”) or a non-proper prenominal (“spokesman John Smith”)1. If the entity is the complement of a preposition, we extract the preposition and the head of the governing NP (“a federally funded sewage plant in Georgia”). These are added to the grammar at the named-entity class level (separated from the core by a special pun</context>
<context position="28597" citStr="Liang et al., 2007" startWordPosition="4630" endWordPosition="4633">erative coreference system, since the document context probably provides hints about which entities are and are not coreferent. The other key issue with our system is inference. Currently we are extremely vulnerable to falling into local minima, since the complex structure of the model can easily lock a small group of examples into a poor configuration. (The “William F. Gibson” case above seems to be one of these.) In addition to the block sampler used by Bhattacharya and Getoor (2006), we are investigating general-purpose splitmerge samplers (Jain and Neal, 2000) and the permutation sampler (Liang et al., 2007a). One interesting question is how well these samplers perform when faced with thousands of clusters (entities). Despite these issues, we clearly show that it is possible to build a good model of named entity class while retaining compatibility with generative systems and without supervision. In addition, we do a reasonable job learning the latent structure of names in each named entity class. Our system improves over the latent named-entity tagging in Haghighi and Klein (2007), from 61% to 87%. This suggests that it should indeed be possible to improve on their coreference results without us</context>
</contexts>
<marker>Liang, Petrov, Jordan, Klein, 2007</marker>
<rawString>Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein. 2007b. The infinite PCFG using hierarchical Dirichlet processes. In Proceedings of EMNLP-CoNLL, pages 688–697, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mikheev</author>
<author>C Grover</author>
<author>M Moens</author>
</authors>
<title>Description of the LTG System Used for MUC-7.</title>
<date>1998</date>
<booktitle>In Proceedings of the 7th Message Understanding Conference (MUC-7),</booktitle>
<location>Fairfax, Virginia.</location>
<contexts>
<context position="934" citStr="Mikheev et al., 1998" startWordPosition="133" endWordPosition="136">es which also models named entity internal structure, clustering related words by role. The model is entirely unsupervised; it uses features from the named entity itself and its syntactic context, and coreference information from an unsupervised pronoun resolver. The model scores 86% on the MUC-7 named-entity dataset. To our knowledge, this is the best reported score for a fully unsupervised model, and the best score for a generative model. 1 Introduction Named entity clustering is a classic task in NLP, and one for which both supervised and semi-supervised systems have excellent performance (Mikheev et al., 1998; Chinchor, 1998). In this paper, we describe a fully unsupervised system (using no “seed rules” or initial heuristics); to our knowledge this is the best such system reported on the MUC-7 dataset. In addition, the model clusters the words which appear in named entities, discovering groups of words with similar roles such as first names and types of organization. Finally, the model defines a notion of consistency between different references to the same entity; this component of the model yields a significant increase in performance. The main motivation for our system is the recent success of </context>
</contexts>
<marker>Mikheev, Grover, Moens, 1998</marker>
<rawString>A. Mikheev, C. Grover, and M. Moens. 1998. Description of the LTG System Used for MUC-7. In Proceedings of the 7th Message Understanding Conference (MUC-7), Fairfax, Virginia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Nadeau</author>
<author>Satoshi Sekine</author>
</authors>
<title>A survey of named entity recognition and classification.</title>
<date>2007</date>
<journal>Journal of Linguisticae Investigationes,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="3558" citStr="Nadeau and Sekine (2007)" startWordPosition="555" endWordPosition="558">e concept of the role of each word in the string. Our model attempts to learn this role information by clustering the words within named entities. 2 Related Work Supervised named entity recognition now performs almost as well as human annotation in English (Chinchor, 1998) and has excellent performance on other languages (Tjong Kim Sang and De Meulder, 2003). For a survey of the state of the art, 164 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 164–172, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics see Nadeau and Sekine (2007). Of the features we explore here, all but the pronoun information were introduced in supervised work. Supervised approaches such as Black et al. (1998) have used clustering to group together different nominals referring to the same entity in ways similar to the “consistency” approach outlined below in section 3.2. Semi-supervised approaches have also achieved notable success on the task. Co-training (Riloff and Jones, 1999; Collins and Singer, 1999) begins with a small set of labeling heuristics and gradually adds examples to the training data. Various co-training approaches presented in Coll</context>
</contexts>
<marker>Nadeau, Sekine, 2007</marker>
<rawString>David Nadeau and Satoshi Sekine. 2007. A survey of named entity recognition and classification. Journal of Linguisticae Investigationes, 30(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford M Neal</author>
</authors>
<title>Slice sampling.</title>
<date>2003</date>
<journal>Annals of Statistics,</journal>
<pages>31--705</pages>
<contexts>
<context position="18237" citStr="Neal, 2003" startWordPosition="2963" endWordPosition="2964"> more using the consistency model, and evaluate the last sample from each. We discard 2Available at http://www.cog.brown.edu/ mj/Software.htm 3We ignore the corresponding Hastings correction, as in practice it leads to too many rejections. the 10 samples with worst log-likelihood and report the average score for the other 10. 3.6 Parameters In addition to the base PCFG itself, the system requires a few hyperparameter settings: Dirichlet priors for the rule weights of rules in the base PCFG. Pitman-Yor parameters for the adapted nonterminals are sampled from vague priors using a slice sampler (Neal, 2003). The prior over core words was set to the uniform distribution (Dirichlet 1.0) and the prior for all modifiers, prepositions and pronouns to a sparse value of .01. Beyond setting these parameters to a priori reasonable values, we did not optimize them. To encourage the system to learn that some lexical symbols were more common than others, we set a sparse prior over expansions to symbols4. There are two really important hyperparameters: an extremely biased prior on class-to-pronountype probabilities (1000 for the desired class, .0001 for everything else), and a prior of .0001 for the Word → W</context>
</contexts>
<marker>Neal, 2003</marker>
<rawString>Radford M. Neal. 2003. Slice sampling. Annals of Statistics, 31:705–767.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
</authors>
<title>Unsupervised models for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>640--649</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="2484" citStr="Ng (2008)" startWordPosition="388" endWordPosition="389">d aid in the coreference task. The easiest way to incorporate a better model is simply to run a supervised or semi-supervised system as a preprocess. To perform joint inference, however, requires an unsupervised generative model for named entities. As far as we know, this work is the best such model. Named entities also pose another problem with the Haghighi and Klein (2007) coreference model; since it models only the heads of NPs, it will fail to resolve some references to named entities: (“Ford Motor Co.”, “Ford”), while erroneously merging others: (“Ford Motor Co.”, “Lockheed Martin Co.”). Ng (2008) showed that better features for matching named entities– exact string match and an “alias detector” looking for acronyms, abbreviations and name variants– improve the model’s performance substantially. Yet building an alias detector is nontrivial (Uryupina, 2004). English speakers know that “President Clinton” is the same person as “Bill Clinton” , not “President Bush”. But this cannot be implemented by simple substring matching. It requires some concept of the role of each word in the string. Our model attempts to learn this role information by clustering the words within named entities. 2 R</context>
</contexts>
<marker>Ng, 2008</marker>
<rawString>Vincent Ng. 2008. Unsupervised models for coreference resolution. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 640–649, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pasca</author>
</authors>
<title>Acquisition of categorized named entities for web search.</title>
<date>2004</date>
<booktitle>In CIKM ’04: Proceedings of the thirteenth ACM international conference on Information and knowledge management,</booktitle>
<pages>137--145</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="4578" citStr="Pasca, 2004" startWordPosition="718" endWordPosition="719">off and Jones, 1999; Collins and Singer, 1999) begins with a small set of labeling heuristics and gradually adds examples to the training data. Various co-training approaches presented in Collins and Singer (1999) all score about 91% on a dataset of named entities; the inital labels were assigned using 7 hand-written seed rules. However, Collins and Singer (1999) show that a mixture-of-naive-Bayes generative clustering model (which they call an EM model), initialized with the same seed rules, performs much more poorly at 83%. Much later work (Evans, 2003; Etzioni et al., 2005; Cucerzan, 2007; Pasca, 2004) relies on the use of extremely large corpora which allow very precise, but sparse features. For instance Etzioni et al. (2005) and Pasca (2004) use web queries to count occurrences of “cities such as X” and similar phrases. Although our research makes use of a fairly large amount of data, our method is designed to make better use of relatively common contextual features, rather than searching for high-quality semantic features elsewhere. Models of the internal structure of names have been used for cross-document coreference (Li et al., 2004; Bhattacharya and Getoor, 2006) and a goal in their </context>
</contexts>
<marker>Pasca, 2004</marker>
<rawString>Marius Pasca. 2004. Acquisition of categorized named entities for web search. In CIKM ’04: Proceedings of the thirteenth ACM international conference on Information and knowledge management, pages 137–145, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jim Pitman</author>
<author>Marc Yor</author>
</authors>
<title>The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator.</title>
<date>1997</date>
<journal>Ann. Probab.,</journal>
<pages>25--855</pages>
<contexts>
<context position="7398" citStr="Pitman and Yor, 1997" startWordPosition="1173" endWordPosition="1176">ion; Collins and Singer (1999) show that about 12% of examples do not fit into these categories. However, while using the MUC-7 data, we have no way to evaluate on such examples. As a framework for our models, we adopt adaptor grammars (Johnson et al., 2007), a framework for non-parametric Bayesian inference over contextfree grammars. Although our system does not require the full expressive power of PCFGs, the adaptor grammar framework allows for easy development of structured priors, and supplies a flexible generic inference algorithm. An adaptor grammar is a hierarchical Pitman-Yor process (Pitman and Yor, 1997). The grammar has two parts: a base PCFG and a set of adapted nonterminals. Each adapted nonterminal is a Pitman-Yor process which expands either to a previously used subtree or to a sample from the base PCFG. The end result is a posterior distribution over PCFGs and over parse trees for each example in our dataset. Each of our models is an adaptor grammar based on a particular base PCFG where the top nonterminal of each parse tree represents a named entity class. 3.1 Core NP Model We begin our analysis by reducing each namedentity reference to the contiguous substring of 165 ROOT →NE0|NE1|NE2</context>
</contexts>
<marker>Pitman, Yor, 1997</marker>
<rawString>Jim Pitman and Marc Yor. 1997. The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator. Ann. Probab., 25:855–900.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Rosie Jones</author>
</authors>
<title>Learning dictionaries for information extraction by multi-level bootstrapping.</title>
<date>1999</date>
<booktitle>In Proceedings of the Sixteenth National Conference on Artificial Intelligence,</booktitle>
<pages>472--479</pages>
<publisher>AAAI.</publisher>
<contexts>
<context position="3985" citStr="Riloff and Jones, 1999" startWordPosition="622" endWordPosition="625">es: The 2009 Annual Conference of the North American Chapter of the ACL, pages 164–172, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics see Nadeau and Sekine (2007). Of the features we explore here, all but the pronoun information were introduced in supervised work. Supervised approaches such as Black et al. (1998) have used clustering to group together different nominals referring to the same entity in ways similar to the “consistency” approach outlined below in section 3.2. Semi-supervised approaches have also achieved notable success on the task. Co-training (Riloff and Jones, 1999; Collins and Singer, 1999) begins with a small set of labeling heuristics and gradually adds examples to the training data. Various co-training approaches presented in Collins and Singer (1999) all score about 91% on a dataset of named entities; the inital labels were assigned using 7 hand-written seed rules. However, Collins and Singer (1999) show that a mixture-of-naive-Bayes generative clustering model (which they call an EM model), initialized with the same seed rules, performs much more poorly at 83%. Much later work (Evans, 2003; Etzioni et al., 2005; Cucerzan, 2007; Pasca, 2004) relies</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>Ellen Riloff and Rosie Jones. 1999. Learning dictionaries for information extraction by multi-level bootstrapping. In Proceedings of the Sixteenth National Conference on Artificial Intelligence, pages 472–479. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder.</title>
<date>2003</date>
<booktitle>In Walter Daelemans</booktitle>
<pages>142--147</pages>
<editor>and Miles Osborne, editors,</editor>
<location>Edmonton, Canada.</location>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Languageindependent named entity recognition. In Walter Daelemans and Miles Osborne, editors, Proceedings of CoNLL-2003, pages 142–147. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olga Uryupina</author>
</authors>
<title>Evaluating name-matching for coreference resolution.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC 04,</booktitle>
<location>Lisbon.</location>
<contexts>
<context position="2748" citStr="Uryupina, 2004" startWordPosition="426" endWordPosition="427"> we know, this work is the best such model. Named entities also pose another problem with the Haghighi and Klein (2007) coreference model; since it models only the heads of NPs, it will fail to resolve some references to named entities: (“Ford Motor Co.”, “Ford”), while erroneously merging others: (“Ford Motor Co.”, “Lockheed Martin Co.”). Ng (2008) showed that better features for matching named entities– exact string match and an “alias detector” looking for acronyms, abbreviations and name variants– improve the model’s performance substantially. Yet building an alias detector is nontrivial (Uryupina, 2004). English speakers know that “President Clinton” is the same person as “Bill Clinton” , not “President Bush”. But this cannot be implemented by simple substring matching. It requires some concept of the role of each word in the string. Our model attempts to learn this role information by clustering the words within named entities. 2 Related Work Supervised named entity recognition now performs almost as well as human annotation in English (Chinchor, 1998) and has excellent performance on other languages (Tjong Kim Sang and De Meulder, 2003). For a survey of the state of the art, 164 Human Lang</context>
</contexts>
<marker>Uryupina, 2004</marker>
<rawString>Olga Uryupina. 2004. Evaluating name-matching for coreference resolution. In Proceedings of LREC 04, Lisbon.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>