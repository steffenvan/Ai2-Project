<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9971055">
Three Knowledge-Free Methods
for Automatic Lexical Chain Extraction
</title>
<author confidence="0.990116">
Steffen Remus and Chris Biemann
</author>
<affiliation confidence="0.934216666666667">
FG Language Technology
Department of Computer Science
Technische Universit¨at Darmstadt
</affiliation>
<email confidence="0.996766">
remus@kdsl.informatik.tu-darmstadt.de, biem@cs.tu-darmstadt.de
</email>
<sectionHeader confidence="0.995628" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99975555">
We present three approaches to lexical chain-
ing based on the LDA topic model and eval-
uate them intrinsically on a manually anno-
tated set of German documents. After motivat-
ing the choice of statistical methods for lexi-
cal chaining with their adaptability to different
languages and subject domains, we describe
our new two-level chain annotation scheme,
which rooted in the concept of cohesive har-
mony. Also, we propose a new measure
for direct evaluation of lexical chains. Our
three LDA-based approaches outperform two
knowledge-based state-of-the art methods to
lexical chaining by a large margin, which can
be attributed to lacking coverage of the knowl-
edge resource. Subsequent analysis shows that
the three methods yield a different chaining
behavior, which could be utilized in tasks that
use lexical chaining as a component within
NLP applications.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99998828">
A text that is understandable by its nature exhibits
an underlying structure which makes the text co-
herent; that is, the structure is responsible for mak-
ing the text “hang” together (Halliday and Hasan,
1976). The theoretic foundation of this structure is
defined as coherence and cohesion. While the for-
mer is concerned with the meaning of a text, the lat-
ter can be seen as a collection of devices for cre-
ating it. Cohesion and coherence build the basis
for most of the current natural language processing
problems that deal with text understanding. Lex-
ical cohesion ties together words or phrases that
are semantically related. Once all the cohesive ties
are identified the involved items can be grouped to-
gether to form so-called lexical chains, which form a
theoretically well-founded building block in various
natural language processing applications, such as
word sense disambiguation (Okumura and Honda,
1994), summarization (Barzilay and Elhadad, 1997),
malapropism detection and correction (Hirst and St-
Onge, 1998), document hyperlinking (Green, 1996),
text segmentation (Stokes et al., 2004), topic track-
ing (Carthy, 2004), and others. The performance of
the individual task heavily depends on the quality of
the identified lexical chains.
</bodyText>
<subsectionHeader confidence="0.957476">
1.1 Motivation for Corpus-driven Approach
</subsectionHeader>
<bodyText confidence="0.961509">
Previous approaches mainly focus on the use of
knowledge resources like lexical semantic databases
(Hirst and St-Onge, 1998) or thesauri (Morris and
Hirst, 1991) as background information in order to
resolve possible semantic relations. A major draw-
back of this strategy is the dependency on the cov-
erage of the resource, which has a direct impact on
the lexical chains. Their quality can be expected to
be poor for resource-scarce languages or specialized
application domains.
Statistical methods to modeling language seman-
tics have proven to deliver good results in many nat-
ural language processing applications. In particu-
lar, probabilistic topic models have been success-
fully used for tasks such as summarization (Gong
and Liu, 2001; Hennig, 2009), text segmentation
(Misra et al., 2009), lexical substitution (Dinu and
Lapata, 2010) or word sense disambiguation (Cai et
al., 2007; Boyd-Graber et al., 2007).
</bodyText>
<page confidence="0.972822">
989
</page>
<note confidence="0.4720995">
Proceedings of NAACL-HLT 2013, pages 989–999,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999944590909091">
In this work, we address the question, whether
statistical methods for the extraction of lexi-
cal chains can yield better results than existing
knowledge-based methods, especially for underre-
sourced languages or domains, following principles
of Structure Discovery (Biemann, 2012). To address
this, we have developed a methodology for evaluat-
ing the quality of lexical chains intrinsically, have
carried out an annotation study, and report results on
a corpus of manually annotated German news docu-
ments.
After defining a measure for the comparison of
(manually or automatically created) lexical chains
in Section 2, Section 3 describes our annotation
methodology and discusses issues regarding the in-
herent subjectivity of lexical chain annotation. In
Section 4, three statistical approaches for lexical
chaining are developed on the basis of the LDA topic
model. Experiments that demonstrate the advantage
of these approaches over a knowledge-baseline are
conducted and evaluated in Section 5, and Section 6
concludes and provides an outlook future directions.
</bodyText>
<subsectionHeader confidence="0.786982">
1.2 Previous Work on Lexical Chains
</subsectionHeader>
<bodyText confidence="0.999947340425532">
Morris and Hirst (1991) initially proposed an al-
gorithm for lexical chaining based on Roget’s the-
saurus (Roget, 1852), and manually assessed the
quality of their algorithm. Hirst and St-Onge (1998)
first presented a computational approach to lexical
chaining based on WordNet showing that the lexi-
cal database is a reasonable replacement to Roget’s.
The basic idea behind these algorithms is that se-
mantically close words should be connected to form
chains. Subsequent approaches mainly concentrated
on disambiguation of words to WordNet concepts
(WSD), since ambiguous words can lead to the over-
generation of connections. Barzilay and Elhadad
(1997) improved the implicit word sense disam-
biguation (WSD) by keeping a list of different inter-
pretations of the text and finally choosing the most
plausible senses for chaining. Silber and McCoy
(2002) introduced an efficient variant of the algo-
rithm with linear complexity in the number of can-
didate terms. Galley and McKeown (2003) further
improved accuracy by first performing WSD, and
then using the remaining links between the disam-
biguated concepts only. They also introduced a so-
called disambiguation graph, a representation that
has also been utilized by the method of Medelyan
(2007), where she applied a graph clustering algo-
rithm to the disambiguation graph to cut weak links,
performing implicit WSD. A combination of statis-
tical and knowledge-based methods is presented by
Marathe and Hirst (2010), who combine distribu-
tional co-occurrence information with semantic in-
formation from a lexicographic resource for extract-
ing lexical chains and evaluate them by text segmen-
tation. We are not aware of previous lexical chain-
ing algorithms that do not rely on a lexicographic
resource at all.
A major issue in developing a new lexical chain-
ing algorithm is the comparison to previous systems.
Most of previous approaches are validated by the
evaluation in a certain task like summarization, word
sense disambiguation, keyphrase extraction or infor-
mation retrieval (Stairmand, 1996). Hence, these ex-
trinsic evaluations are heavily influenced by the par-
ticular task at hand. We propose to re-consider lexi-
cal chaining as a task on its own, and propose objec-
tive criteria for directly comparing lexical chains to
this end.
</bodyText>
<sectionHeader confidence="0.943254" genericHeader="method">
2 Comparing Lexical Chains
</sectionHeader>
<bodyText confidence="0.9998681875">
The comparison of lexical chains is a non-trivial
task. We adopt the idea of interpreting lexical chains
as clusters and a particular set of lexical chains as
a clustering, and develop a suitable cluster com-
parison measure. As stated by Meil˘a (2005) and
Amig´o et al. (2009), a best clustering comparison
measure for the general case does not exist. It should
be stressed that the appropriate clustering measure
highly depends on the task at hand.
After exploring a number of measures1, we de-
cided on a combination of the adjusted Rand in-
dex (ARI, Hubert and Arabie (1985)) and the basic
merge distance (BMD, Menestrina et al. (2010))
for our new measure. Menestrina et al. (2010) in-
troduced a linear time algorithm for computing the
generalized merge distance (GMD), which counts
</bodyText>
<footnote confidence="0.994651571428571">
1Explored measures which are unsatisfactory for the given
task are: Closest Cluster F1 (Benjelloun et al., 2009), K (Ajmera
et al., 2002), Pairwise F1 (Manning et al., 2008), Variation of
Information (Meil˘a, 2005), B3(Bagga and Baldwin, 1998), V-
Measure (Rosenberg and Hirschberg, 2007), Normalized Mu-
tual Information (Strehl, 2002). The last two measures are
equal. A proof of this can be found in the appendix.
</footnote>
<page confidence="0.997004">
990
</page>
<bodyText confidence="0.999981933333333">
split and merge cluster editing operations. Using a
constant factor of 1 for both splits and merges gives
the basic merge distance (BMD): Considering T
as the most general clustering of a dataset D, where
all elements are grouped into the same cluster, and
further considering 1 as the most specific cluster-
ing of D, where each element builds its own clus-
ter, the lattice between T and 1 spans all possible
clusterings and the BMD can be interpreted as the
shortest path from a clustering C to a clustering C&apos;
in the lattice with some restrictions (see Menestrina
et al. (2010) for details). We normalize the BMD
score by the maximum BMD2 to the normalized
basic merge distance (NBMD). ARI is is based
on pair comparisons, and is computed as3:
</bodyText>
<equation confidence="0.994512">
index = TP
(TP + FP) x (TP + FN)
expected index = TP + TN + FP + FN
1
</equation>
<construct confidence="0.6947365">
max index = TP + �(FP + FN)
index − expected index
ARI(C, C&apos;) =
max index − expected index
</construct>
<bodyText confidence="0.986725866666667">
The reasons for choosing these two particular
measures are the following: ARI is a well known
measure which is adjusted (corrected) for decisions
made by chance. But since it is based on pairwise
element comparison it completely disregards single-
ton clusters (chains) and some types of errors are not
adequately penalized. The NBMD on the other hand
penalizes various errors almost equally.
We combine the two single measures into a
new lccm (lexical chain comparison measure), de-
fined as the arithmetic mean between ARI and
1 − NBMD. An lccm of 1 indicates perfect con-
gruence and an lccm = 0 indicates that not a single
pair of items in C is found in a cluster together in
C&apos;.
</bodyText>
<equation confidence="0.992276">
lccm(C, C&apos;) =
1 [1 − NBMD(C, C&apos;) + ARI(C, C&apos;)] .
2
2BMD(T,1) for |D |&lt; 2, BMD(T,1) + 1 otherwise
</equation>
<bodyText confidence="0.991367">
3TP: pairs in D and D’, FP: pairs in D’ but not in D, FN:
pairs in D but not in D’, TN: pairs not in D and not in D’, where
D is the underlying dataset of C, D’ is the underlying dataset of
C’, and pairs means all unique combinations of elements that
are in the same cluster.
</bodyText>
<sectionHeader confidence="0.863558" genericHeader="method">
3 Annotating Lexical Chains
</sectionHeader>
<bodyText confidence="0.999987967741936">
A challenge with the annotation of lexical chains is
the subjective interpretation of the text by individ-
ual annotators (Morris and Hirst, 2004), which also
substantiates the fact that currently no gold stan-
dard exist, and all previous automatic approaches
are evaluated by performing a certain NLP task.
Hollingsworth and Teufel (2005) as well as Cramer
et al. (2008) conclude from their lexical chain anno-
tation projects that high inter-annotator agreement is
very hard to achieve. We argue that directly evalu-
ating on lexical chains should enable us to optimize
towards higher-quality chain annotations, which is
a task of its own right and which has the potential
to improve all subsequent applications. For this, we
devise an annotation scheme that gets us reasonable
inter-annotator agreement, inspired by the concept
of cohesive harmony (Hasan, 1984), and report on
an annotation project for German newswire texts.
Documents from the SALSA 2.0 (Burchardt et al.,
2006) corpus were chosen to form the basis for the
annotation of lexical chain information. SALSA is
based on the semi-automatically annotated TIGER
Treebank 2.1 (Brants et al., 2002). The TIGER
treebank provides manual annotations, such as lem-
mas, part-of-speech tags, and syntactic structure, the
SALSA part of the corpus is also partially annotated
with FrameNet-style (Baker et al., 1998) frame an-
notation. The documents are general domain news
articles from a German newspaper comprising about
1,550 documents and around 50,000 sentences in to-
tal, with a median document length of 275 tokens.
</bodyText>
<subsectionHeader confidence="0.999934">
3.1 Annotation Scheme
</subsectionHeader>
<bodyText confidence="0.999845">
In order to minimize the subjectiveness of choices
by different annotators, annotation guidelines were
developed comprising a total of ten pages. We
decided to consider only nouns, noun compounds
and non-compositional adjective noun phrases like
“dirty money” as candidate terms for lexical chain-
ing, which is consistent with the procedures of
Hollingsworth and Teufel (2005) and Cramer et al.
(2008). For annotation, we used the MMAX24
(M¨uller and Strube, 2006) tool.
We introduce the term dense chain, which refers
to a type of lexical chain in which every element is
</bodyText>
<footnote confidence="0.97026">
4http://mmax2.sourceforge.net
</footnote>
<page confidence="0.98336">
991
</page>
<bodyText confidence="0.986585448979592">
lccm
related to every other element in that chain. Terms
are considered to be related if they share the same
topic, i.e. common sense and knowledge of the lan-
guage is needed to decide which terms belong to-
gether in the same topic and whether a chosen topic
is neither too broad nor too narrow. A single dense
chain can thus be assigned a definite topical descrip-
tion of its items. Whereas Hollingsworth and Teufel
(2005) dealt with the inherent fuzziness of member-
ship of terms to lexical chains by allowing terms
to occur in different lexical chains, we follow the
concept of cohesive harmony introduced by Hasan
(1984) here, where complete chains can be linked
to others. For this purpose, we introduce so-called
level two links, which are cohesive ties between lex-
ical items in distinct dense chains. Having such
a link between two chains, both chains can be as-
signed a topical description which is broader than
the description of the individual chains. This results
in a two-level representation of chains. We report
on dense lexical chains and merged lexical chains
(dense chains are merged into a common chain if a
level two link exists between them) separately.
In total, 100 documents were annotated by two
expert annotators. Documents were chosen around
the length median and consist of 248 – 304 tokens.
The two rightmost columns of Table 3 show the
characteristics of the annotated data set. It can be
concluded that there is a moderate to high agree-
ment regarding the annotator selections of candidate
terms, which is ensured by preselection of candidate
terms by part-of-speech patterns. A value of 81% in
the average agreement on lexical items (cf. Figure 1)
shows that even though the choice of lexical items
is limited to nouns and adjective noun phrases only,
the decision on candidate termhood is somewhat dif-
ferent between the annotators, but compares favor-
ably with previous findings of 63% average pairwise
agreement (Morris and Hirst, 2004).
Figure 2 shows the annotator agreement on the
individual documents using the lccm (cf. Sec. 2),
sorted in the same way as in Figure 1. In order to use
the level two link information the figure also shows
a second agreement score, which was computed on
merged chains.
The agreement scores of the assignment of lexical
items to lexical chains depend partially on the agree-
ment scores of the identified lexical items them-
</bodyText>
<figure confidence="0.988754166666666">
100
%
20
0
0600
Document ID
</figure>
<figureCaption confidence="0.99442925">
Figure 1: Agreement of lexical items annotated by anno-
tator A and annotator B as a percentage of lexical items
annotated by annotator A or annotator B. The average
agreement is 81%.
</figureCaption>
<figure confidence="0.99951825">
1.0
0.8
0.6
0.4
0.2
0.0
0600
Document ID
</figure>
<figureCaption confidence="0.997802">
Figure 2: Individual annotator agreement scores on 100
</figureCaption>
<bodyText confidence="0.985795428571429">
documents sorted by their agreement on candidate terms.
The red circles show the agreement of both annotators on
the dense lexical chains disregarding the cohesive links,
and the green dots show the agreement of both annotators
on the merged lexical chains (via the cohesive links) both
using the proposed lexical chain comparison measure.
selves, which is a desired property. Across all doc-
uments, a perfect agreement was never achieved,
which confirms the difficulty of annotating such a
subjective task: The average lccm per document on
the manual annotations is 0.56 (dense chains), re-
spectively 0.54 (merged chains). However, the con-
siderable overlap between the annotators still en-
ables us to evaluate automatic chaining methods,
and the lccm agreement score serves as an upper
bound. Note that by performing no reconciliation
of the annotations we explicitly allow the possibil-
ity of different interpretations which is in our opin-
ion appropriate here due to the subjectiveness of the
task itself. By doing so, we evaluate our algorithms
against individual annotator interpretations.
</bodyText>
<sectionHeader confidence="0.998728" genericHeader="method">
4 Statistical Methods for Lexical Chaining
</sectionHeader>
<bodyText confidence="0.982934875">
This work employs a well-studied statistical method
for creating something that Barzilay (1997) called
an automatic thesaurus which will then be adapted
for lexical chaining. For our automatic approaches,
candidate lexical items in a text are preselected by
the same heuristic that is also applied in Section 3
for the annotation process.
Topic models (TMs) are a suite of unsuper-
</bodyText>
<figure confidence="0.999134776119403">
80
60
40
2068
1069
0337
0364
0897
1323
0100
1388
0962
0545
1156
1190
0122
1194
1855
1808
0356
1779
2040
0219
1734
1216
0947
0937
2026
1548
1232
1826
1224
1251
1239
2044
1119
0174
1650
2014
0630
0199
0655
1818
0310
1150
1726
2035
0382
1639
0275
1047
1707
0228
1050
1607
0984
0755
0752
0747
1162
1018
0182
0942
0492
0675
0928
1399
1072
1310
0268
1033
1168
1523
1737
0524
0338
0976
0639
0299
1524
1387
0694
0278
1692
1534
1037
0499
0721
0283
0736
0258
1130
0988
0649
1777
0309
0110
1319
0729
0730
0390
0925
2068
1069
0337
0364
0897
1323
0100
1388
0962
0545
1156
1190
0122
1194
1855
1808
0356
1779
2040
0219
1734
1216
0947
0937
2026
1548
1232
1826
1224
1251
1239
2044
1119
0174
1650
2014
0630
0199
0655
1818
0310
1150
1726
2035
0382
1639
0275
1047
1707
0228
1050
1607
0984
0755
0752
0747
1162
1018
0182
0942
0492
0675
0928
1399
1072
1310
0268
1033
1168
1523
1737
0524
0338
0976
0639
0299
1524
1387
0694
0278
1692
1534
1037
0499
0721
0283
0736
0258
1130
0988
0649
1777
0309
0110
1319
0729
0730
0390
0925
</figure>
<page confidence="0.983174">
992
</page>
<bodyText confidence="0.999943789473684">
vised algorithms designed for unveiling some hid-
den structure in large data collections. The key idea
is that documents can be represented as compos-
ites of so-called topics where a topic itself repre-
sents as a composite of words. Hofmann (1999)
defined a topic to be a probability distribution over
words and a document to be a probability distribu-
tion over a fixed set of topics. We use the latent
Dirichlet allocation (LDA, Blei et al. (2003)) topic
model for estimating the semantic closeness of can-
didate terms, and explore different ways of utilizing
LDA’s topic information in automatic lexical chain-
ers. Specifically, we use the GibbsLDA++5 frame-
work for topic model estimation and inference, and
examine the following LDA parameters: number of
topics T, Dirichlet hyperparameters for document-
topic distribution α and topic-term distribution Q.
We now describe three LDA-based approaches to
lexical chaining.
</bodyText>
<subsectionHeader confidence="0.935709">
4.1 LDA Mode Method (LDA-MM)
</subsectionHeader>
<bodyText confidence="0.998987095238095">
The LDA-MM approach places all word tokens that
share the same topic ID into the same chain. The
point is now how to decide to which topic a word
belongs to. Since single samples of topics per word
exhibit a large variance (Riedl and Biemann, 2012),
we follow these authors by sampling several times
and using the mode (most frequently assigned) topic
ID per word as the topic assignment. This strategy
reduced the variance in the lccm to a tenth6.
More formally, let samples(d,w) be the vector
of assignments that have been collected for a cer-
tain word w in a certain document d with each
samples(d,w)
i referring to the i-th sampled topic ID
for (d, w). In other words, samples(d,w) can be seen
as the Markov chain for a particular word in a par-
ticular document. Further let z(d,w) be the topic ID
that was most assigned to the word w with respect
to the samples in samples(d,w). Precisely, z(d,w) is
defined to be the sampled mode in samples(d,w) —
in case of multiple modes a random mode is chosen,
</bodyText>
<footnote confidence="0.851225">
5http://gibbslda.sourceforge.net
6Preliminary experiments yielded a variance of 2.6 x 10−6
in lccm using the mode method and 3.07 x 10−5 using a single
sample for lexical chain assignment.
which never happened in our experiments.
</footnote>
<equation confidence="0.974926">
z(d,w) = mode (samples(d,w))
Pz arg max (P(z = j|w, d))
j
</equation>
<bodyText confidence="0.999836571428571">
The LDA-MM assigns for every word w which
is a candidate lexical item of a certain document d
which is assigned the same topic z(d,w) to the same
chain; hence implicitly disambiguating the terms.
The possibility to create level two links is given
by taking the second most occurring topic for a given
word if it exceeds a certain threshold.
</bodyText>
<subsectionHeader confidence="0.968445">
4.2 LDA Graph Method (LDA-GM)
</subsectionHeader>
<bodyText confidence="0.994476681818182">
The LDA-GM algorithm creates a similarity graph
based on the comparison of topic distributions for
given words and then applies a clustering algorithm
in order to find semantically related words.
Let 4&apos;(d,w) be the per-word topic distribution
P(z|w, d). Analogously to the LDA-MM, O(d,w)
can be obtained by counting the occurrences of
a certain topic ID z in the sample collection
samples(d,w) for a particular word w and document
d.
The semantic relatedness between any two words
wi and wj can then be measured by their similarity
score of the topic distributions O(d,wi) and O(d,wj),
which is stored in a term similarity matrix. This
matrix can also be interpreted as an adjacency ma-
trix of a graph, with candidate items being nodes
and edges being weighted with the similarity value
simij for any two nodes i, j : i =� j n i, j E
11, 2, ... , Nd}. We test two similarity measures:
Euclidian (dis-)similarity and cosine similarity.
Let G = (V, E) be the graph represen-
tation of a document with term vertices
</bodyText>
<equation confidence="0.98972">
V = 1v1, ... , vNd} and weighted edges E =
1(v1, v2, sim12), ... (vNd, vNd−1, simNdNd−1)},
</equation>
<bodyText confidence="0.9999576">
where simij is either the cosine or Euclidean
similarity of term vectors. For simplicity, we reduce
this representation to an unweighted graph by
only retaining edges (of unit weight) that have a
similarity above a parameter threshold csim. To
identify chains as clusters in this graph, we follow
Medelyan (2007) and apply the Chinese Whispers
graph clustering algorithm (CW, Biemann (2006)),
which finds the number of clusters automatically.
The CW algorithm implementation comes with
</bodyText>
<page confidence="0.997714">
993
</page>
<bodyText confidence="0.999936454545454">
three parameters to regulate the node weight based
on its degree, which influences cluster size and
granularity. We test options ”top”, ”dist log” and
”dist lin”.
The final chaining procedure is straightforward:
The LDA-GM algorithm assigns every candidate
lexical item wi of a certain document d which is
assigned the same class label ci to the same chain.
Level two links are drawn using the second domi-
nant class of a vertex’s neighborhood, which is pro-
vided by the CW implementation.
</bodyText>
<subsectionHeader confidence="0.998555">
4.3 LDA Top-N Method (LDA-TM)
</subsectionHeader>
<bodyText confidence="0.997731821428571">
The LDA-TM method is different to the others in
that it uses the information of the per-topic word dis-
tribution O(z) = P(w|z) and the per-document topic
distribution 0(d) = P(z|d). Given a parameter n re-
ferring to the top n topics to choose from 0(d) and a
parameter m referring to the top m words to choose
from O(z) the main procedure can be described as
follows: for all z E top n topics in 0(d): chain the
top m words in O(z) .
Note that although the number of chains and chain
members for each chain is bound and could lead to
the same number and sizes of chains, in practice the
number of generated chains as well as the number of
chain members still varies considerably across doc-
uments: often some of the top m words for a (glob-
ally computed) topic do not even occur in a partic-
ular document. This implies that the parameters n
and m must not be set globally but dependent on
the particular document. To overcome this to some
extent, additional thresholding parameters co and co
are used for further bounding the respective n or m
parameter. The procedure works like this: for all z
E top n topics in 0(d) n 0(d) z&lt; co: chain the top m
words w in O(z) n O(z) �&lt; Co.
Level two links are created by computing the co-
sine similarity between every pair of the top n topic
distributions, and thresholding with a link parame-
ter.
</bodyText>
<subsectionHeader confidence="0.997333">
4.4 Repetition Heuristic
</subsectionHeader>
<bodyText confidence="0.99994725">
All methods described above can be applied to new
unseen documents that are not in the training set. To
alleviate a possible vocabulary mismatch between
training set and test set, which happens when terms
in the test set have not been contained in our training
documents, we add a heuristic that chains repetitions
of (previously unknown) words as a post-processing
step to all methods.
</bodyText>
<sectionHeader confidence="0.989141" genericHeader="method">
5 Empirical Analysis
</sectionHeader>
<bodyText confidence="0.99971832">
In order to provide a realistic estimate of the qual-
ity of our methods to unseen material, we randomly
split our annotated documents in two parts of 50
documents each. One part is used as a development
set for optimizing the parameters of the methods (i.e.
model selection), the other part forms our test set for
evaluation.
The training corpus, on the other hand, consists
of all 1,211 SALSA/Tiger documents that are not
part of the development and test corpus and nei-
ther very long nor very short. These documents
are taken from the German newspaper “Frankfurter
Rundschau” around 1992. Additionally the training
corpus is enriched with 12,264 news texts from the
same newspaper around 1997 with similar charac-
teristics7, making up a total of 13,457 training doc-
uments for the estimation of topic models.
Input to the LDA model training are verbs, nouns
and adjectives, as well as candidate terms as de-
scribed in Section 3.1, all in their lemmatized form.
We further filter words that occur in more than 1/3
of the training documents, as well as known stop-
words, and words that occur in less than two doc-
uments which results in a vocabulary size of about
100K words.
</bodyText>
<subsectionHeader confidence="0.95236">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9977865">
For comparison, we implemented three baselines,
which we describe below. One baseline is trivial,
two baselines are state-of-the art knowledge-based
systems adapted to German.
Random: Candidate lexical items are randomly
tied together to form sets of lexical chains.
Level two links are created analogously. We
regulate the process to yield the same average
number of chains and links as in the develop-
ment and test data.
</bodyText>
<listItem confidence="0.677289">
S&amp;M GermaNet: Algorithm by Silber and McCoy
(2002) with GermaNet as its knowledge re-
source.
</listItem>
<footnote confidence="0.9896725">
7as provided by Projekt Deutscher Wortschatz,
http://wortschatz.uni-leipzig.de/
</footnote>
<page confidence="0.997167">
994
</page>
<bodyText confidence="0.974357625">
G&amp;M GermaNet: Algorithm by Galley and McK-
eown (2003), also using GermaNet.
GermaNet (Hamp and Feldweg, 1997) is a large
WordNet-like resource for German, containing al-
most 100,000 lexical units and over 87,000 concep-
tual relations between synsets. While its size is only
about half of WordNet, it is one of the largest non-
English lexical semantic resources.
</bodyText>
<subsectionHeader confidence="0.999432">
5.2 Model Selection
</subsectionHeader>
<bodyText confidence="0.995433571428571">
We optimize two sets of parameters: parameters for
the LDA topic model (number of topics K, Dirich-
let hyperparameters a and Q) are optimized for the
LDA-MM method only, and the same LDA model
is used in the other two LDA-based methods. Pa-
rameters particular to the respective method are op-
timized individually. For LDA, we tested sensible
combinations in the ranges K = 50..1000, a =
0.05/K..50/K and Q = 0.001..0.1. The highest
performance of the LDA-MM method was found for
K = 500, a = 50/K, Q = 0.001, and the result-
ing topic model is used across all methods. The final
parameter values for the other methods, found by ex-
haustive search, are summarized in Table 1.
</bodyText>
<figure confidence="0.4098294">
Method Parameter
LDA-GM similarityfunction = cosine similarity
labelweightscheme = dist log
Esi . = 0.95
LDA-TM n = 10, m = 20, ee = 0.2, eO = 0.2
</figure>
<tableCaption confidence="0.994516">
Table 1: Final parameter values.
</tableCaption>
<subsectionHeader confidence="0.968135">
5.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.999879307692308">
For evaluation purposes, terms that consist of multi-
ple words are mapped to its rightmost term which
is assumed to be the head, e.g. “dirty money” is
mapped to “money”. Additionally, singleton chains,
i.e. chains that contain only a single lexical item
are omitted unless the respective lexical item is not
linked by a level two link.
Dense Chains Comparative results of the ap-
proaches in terms of lccm for both annotators are
summarized in Table 2 (upper half). We observe
that all our new methods beat the random baseline
and the two knowledge-based baselines by a large
margin. The knowledge-based baselines, both using
</bodyText>
<table confidence="0.997340692307692">
Anno A Anno B Average
LDA-MM 0.320 0.306 0.313
LDA-TM 0.307 0.299 0.303
LDA-GM 0.328 0.314 0.321
G&amp;M 0.255 0.215 0.235
S&amp;M 0.248 0.209 0.229
Random 0.126 0.145 0.135
LDA-MM 0.316 0.300 0.308
LDA-TM 0.303 0.280 0.291
LDA-GM 0.279 0.267 0.273
G&amp;M 0.184 0.166 0.176
S&amp;M 0.179 0.159 0.169
Random 0.196 0.205 0.201
</table>
<tableCaption confidence="0.997522">
Table 2: Results of the evaluation based on dense chains
</tableCaption>
<bodyText confidence="0.947784806451613">
(upper half) and merged chains (lower half). The annota-
tor agreement on the test set’s chains = 0.585; on merged
chains = 0.553
GermaNet, produce very similar lccm scores, which
highlights the important role of the knowledge re-
source. Data analysis revealed that while chains pro-
duced by knowledge-based baselines are sensible,
the main problem is a lack of coverage in terms of
vocabulary and relations in GermaNet. Comparing
the statistical methods, the LDA-GM method excels
over the others.
Level Two Links Table 2 (lower half) summarizes
the evaluation results of the merged chains via level
two links. Because of merging, a text now contains
fewer chains with more lexical items each. Note that
knowledge-based baselines do not construct level
two links, which is why they are heavily penalized
in this setup.
Again, the statistical methods beat the baselines
by a substantial amount. In this evaluation, the ran-
dom baseline performs above the knowledge-based
methods, which is rooted in the fact that lccm penal-
izes small, correct chains, whereas the random base-
line with linking often produces very large chains
containing most of the terms – something that we
also observe for many manually annotated docu-
ments. The large overlap in the biggest chain then
leads to the comparatively high random baseline
score. In this evaluation, the LDA-MM is the clear
winner, with LDA-GM being clearly inferior this
time.
</bodyText>
<page confidence="0.995148">
995
</page>
<table confidence="0.9316488">
LDA-MM LDA-GM LDA-TM S&amp;M G&amp;M
38.20 29.32 30.82 14.40 15.29
13.80 9.12 7.32 5.83 5.71
8.60 2.06 1.44 – –
2.82 3.41 4.61 2.48 2.68
5.76 7.06 5.98 – –
8.29 4.45 5.57 – –
Anno A Anno B
38.66 38.96
11.25 7.38
</table>
<figure confidence="0.65965425">
5.47 2.41
3.69 5.57
6.10 4.99
7.60 8.91
</figure>
<figureCaption confidence="0.9552875">
avg. num. of lexical items per doc.
avg. num. of chains per doc.
avg. num. of links per doc.
avg. size lexical chains
avg. num. of merged lexical chains
avg. size merged lexical chains
</figureCaption>
<tableCaption confidence="0.996413">
Table 3: Quantitiative characteristics of automatic and manual lexical chains. In average, a document contains 51.58
candidate terms as extracted by our noun phrase patterns
</tableCaption>
<figure confidence="0.9528508125">
der
[the]
in
[in]
Basel
[Basle]
zur
[to the]
letzten
[last]
t¨atig,
[acting,]
wechselt
[switches]
Dramaturg
[dramaturg]
Spielzeit als
[playing period] [as]
Davud Bouchehri, seit
[Davud Bouchehri,] [since]
des
[of the]
das
[the]
an
[to]
Schauspiels
[play]
k¨unstlerischer
[art]
Gesch¨aftsf¨uhrer
[director]
Saison 1996 / 97 als
[1996/97season] [as]
Staatstheater Darmstadt. Der
[state theater] [Darmstadt.] [The]
aus dem Iran stammende 34&apos;ahrige soll daneben auch f¨ur s arten¨ubergreifende Projekte zust¨andi sein,
[from] [the] [Iran] [coming] [34-year-old] [shall] [besides] [also] [for] 1-multi discipline] [projects] [responsible] [be,]
teilte
[aquainted]
das Basler
[the] [Basle’s]
Theater
[theather]
am
[on] Donnerstag mit.
[Thursday] [with.]
LDA-GM:
c1: {Dramaturg, Theater}
c2: {Schauspiels, Staatstheater}
LDA-MM:
c1: {Spielzeit, Schauspiels, Staatstheater}
c2: {Dramaturg, Theater}
c3: {Saison}
l1: (Theater --+ Spielzeit)
l2: (Spielzeit --+ Saison) S&amp;M-GermaNet:
–
LDA-TM:
c1: {Schauspiels, Staatstheater, Theater}
c2: {Dramaturg}
c3: {Spielzeit, Saison}
l1: (Theater--+ Dramaturg)
G&amp;M-GermaNet:
c1: {Staatstheater, Theater}
</figure>
<figureCaption confidence="0.997702333333333">
Figure 3: Diverse output of the various lexical chaining systems after applying them on a short German example text
from the used TIGER/SALSA corpus. For a better understanding the text is calqued. Candidate items are highlighted
and the ci are the resulting dense lexical chains and the li are the level two links produced by the various methods.
</figureCaption>
<bodyText confidence="0.99984880952381">
Data Analysis Table 3 shows quantitative num-
bers of the extracted lexical chains in the test set.
The LDA-MM approach chains and links a lot
more items than the other statistical methods: it cre-
ates a lot more links between items that would oth-
erwise be removed because they form unlinked sin-
gleton chains. As opposed to this, the graph method
(LDA-GM), as well as the top-n method (LDA-TM)
perform an implicit filtering on the candidate lexi-
cal items by creating less level two links, yet larger
dense chains. The knowledge based algorithms by
Silber and McCoy (2002) and Galley and McKeown
(2003) extract fewer and smaller chains than the sta-
tistical approaches, which reflects GermaNet’s spar-
sity issues. While higher lexical coverage in the
underlying resource would increase the coverage of
our knowledge-based systems, this is only one part
of the story. The other part is rooted in the fact
that lexical cohesion relations, which are used in
lexical chains, encompass many more semantic re-
lations than listed in today’s lexical semantic net-
works. This especially holds for cases where sev-
eral expressions refer to the same event or theme for
which no well-defined relation exists, such as e.g.
”captain” and ”harbor”.
Comparing the three LDA-based approaches, no
overall best method could be determined. the LDA-
MM seems especially suited for a high coverage
and coarse (level two) chains, the LDA-GM appears
most suited for dense chains, and LDA-TM pro-
duces the longest chains on average.
Figure 3 shows the resulting dense lexical chains
and level two links after applying our chainers to a
short example text from our corpus. In the exam-
ple the LDA-TM produces the most adequate lexi-
cal chains, at least in our intuition. The LDA-GM
and the LDA-MM produce slightly wrong chains,
yet the LDA-MM additionally creates some mean-
ingfull level two links which the LDA-GM does not.
Both knowledge-based approaches perform poorly
compared to the knowledge-free approaches, where
the S&amp;M algorithm creates no chains at all and the
</bodyText>
<page confidence="0.996515">
996
</page>
<bodyText confidence="0.9990958">
G&amp;M algorithm produces only a single chain con-
taining only two words. This is mostly due to Ger-
maNet’s lacking lexical and relational coverage and
the scope of the algorithms for finding relations be-
tween the words.
</bodyText>
<sectionHeader confidence="0.992585" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99962806122449">
In this paper, we presented experiments for auto-
matic lexical chain annotation and evaluated them
directly on a manually annotated dataset for Ger-
man. A new two-level annotation scheme for lexi-
cal chains was proposed and motivated by the con-
cept of cohesive harmony. We further proposed a
new measure for comparing lexical chain annota-
tions that is especially suited for the characteristics
of lexical chain annotations. Three variants of sta-
tistical lexical chaining methods based on the LDA
topic model were proposed and evaluated against
two knowledge-based baseline systems. Our sta-
tistical methods exhibit a substantially higher per-
formance than the knowledge-based systems on our
dataset. This can partially be attributed to miss-
ing relations, partially to the lack of lexical cov-
erage of GermaNet, which was used in these sys-
tems. Since GermaNet is a large lexical-semantic
net, however, this strengthens our main point: Espe-
cially for under-resourced languages or subject do-
mains, statistical and data-driven methods should be
preferred over their knowledge-based counterparts,
since they do not require the development of lexical-
semantic nets and adopt easily to subject domains by
training their unsupervised models on an in-domain
collection.
In future work, we would like to explore better
ways of selecting candidate items. While our POS-
pattern-based selection mechanism works for practi-
cal purposes, it currently only extracts noun phrases
and over-generates on compositional adjective mod-
ifiers. We would like to define a better filter to re-
duce over-generation. Further, especially for com-
pounding languages such as German, we would like
to decompose one-word compounds as to be able to
link their heads in lexical chains.
While we found it important to directly evalu-
ate our lexical chaining algorithms on manually an-
notated data, a natural next step in this line of re-
search is to use our lexical chaining methods as
pre-processing steps for applications such as sum-
marization, text segmentation or word sense disam-
biguation. This would enable to find out advantages
and disadvantages of our three variants with respect
to an application.
The manually annotated data, the open source an-
notation tool, the annotation guidelines and the im-
plementations of all described methods and base-
lines are available for download8.
</bodyText>
<sectionHeader confidence="0.997395" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9845782">
This work has been supported by the Hessian re-
search excellence program Landes-Offensive zur
Entwicklung Wissenschaftlich-¨okonomischer Exzel-
lenz (LOEWE) as part of the research center Digital
Humanities.
</bodyText>
<note confidence="0.587537">
Proof: Equality of NMI and V
</note>
<table confidence="0.293641333333333">
Using the standard notation from information retrieval H(X)= Entropy,
I(X, Y )= Information, H(X|Y )= Conditional Entropy, NMI(X,Y)=
Normalized Mutual Information, V(X,Y)= V-Measure:
</table>
<equation confidence="0.960806482758621">
h × c
V (C, K) = 2 × (1)
h + c
h = 1 − H(C) ,c = 1
H(C|K) HHK) (2)
()
and
NMI(C, K) =
H(C)+H(K)
I(C, K)
2 = 2 × H(C) + H(K) (3)
I(C, K)
reformulate h and c using the fact that I(C, K) = H(C) −
H(C|K) = H(K) − H(K|C):
H(C|K)
h = 1 −
H(C)
H(C|K)
H(C)
simplifying h × c using (4) and (5):
I(C, K)
h × c = ×
H(C)
I(C, K)2
H(C)H(K)
simplifying h + c using (4) and (5):
h + c = +
I(C, K)
H(C) I(C, K)
</equation>
<figure confidence="0.910770735294118">
H(K)
I(C, K)H(K) + I(C, K)H(C)
H(C)H(K)
I(C, K)[(H(K) + H(C)]
H(C)H(K)
simplifying ���
�+� using (6) and (7):
H(C)H(K)
I(C, K)[H(K) + H(C)]
I(C, K)
H(K) + H(C)
8http://www.ukp.tu-darmstadt.de/
data/lexical-chains-for-german/
h × c I(C, K)2
=
h + c H(C)H(K) ×
H(C)
H(C)
I(C, K)
H(C)
=
=
=
(4)
c = 1 H(K|C)
H(K)
H(K) H(K|C)
H(K)
I(C, K)
H(K)
H(K)
I(C, K)
H(K)
=
</figure>
<page confidence="0.942654">
997
</page>
<equation confidence="0.869961">
substituting (8) into (1) shows that NMI and V are equal:
V (C, K) = 2x h + c = 2 x H(K) + H)(C) = NMI (C, K) (9)
</equation>
<sectionHeader confidence="0.988473" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999760196078431">
Jitendra Ajmera, Herv´e Bourlard, and I. Lapidot. 2002.
Unknown-Multiple Speaker clustering using HMM.
In Proceedings of the International Conference of Spo-
ken Language Processing, ICSLP ’02, Denver, Col-
orado, USA.
Enrique Amig´o, Julio Gonzalo, Javier Artiles, and Felisa
Verdejo. 2009. A comparison of extrinsic clustering
evaluation metrics based on formal constraints. Infor-
mation Retrieval, 12:461–486.
Amit Bagga and Breck Baldwin. 1998. Algorithms for
Scoring Coreference Chains. In Proceedings of the
Linguistic Coreference Workshop at The First Interna-
tional Conference on Language Resources and Evalu-
ation, LREC ’98, pages 563–566, Granada, Spain.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In COLING
’98: Proceedings of the 17th International Conference
on Computational Linguistics, volume 1, pages 86–90,
Montreal, Quebec, Canada.
Regina Barzilay and Michael Elhadad. 1997. Using Lex-
ical Chains for Text Summarization. In Proceedings of
the ACL Workshop on Intelligent Scalable Text Sum-
marization, pages 10–17, Madrid, Spain.
Regina Barzilay. 1997. Lexical Chains for Summariza-
tion. Master’s thesis, Ben-Gurion University of the
Negev, Beersheva, Israel.
Omar Benjelloun, Hector Garcia-Molina, David Men-
estrina, Qi Su, Steven Whang, and Jennifer Widom.
2009. Swoosh: a generic approach to entity resolu-
tion. The VLDB Journal, 18:255–276.
Chris Biemann. 2006. Chinese Whispers – an Effi-
cient Graph Clustering Algorithm and its Application
to Natural Language Processing. In Proceedings of
TextGraphs: the Second Workshop on Graph Based
Methods for Natural Language Processing, pages 73–
80, New York City, USA.
Chris Biemann. 2012. Structure Discovery in Natural
Language. Theory and Applications of Natural Lan-
guage Processing. Springer Berlin / Heidelberg.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu.
2007. A Topic Model for Word Sense Disambigua-
tion. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 1024–1033, Prague, Czech
Republic.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. TIGER Treebank.
In Proceedings of the Workshop on Treebanks and Lin-
guistic Theories (TLT02), Sozopol, Bulgaria.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pad´o, and Manfred Pinkal. 2006.
The SALSA Corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th interna-
tional conference on Language Resources and evalua-
tion (LREC-2006), Genoa, Italy.
Junfu Cai, Wee Sun Lee, and Yee Whye Teh. 2007.
Improving Word Sense Disambiguation Using Topic
Features. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 1015–1023, Prague, Czech
Republic.
Joe Carthy. 2004. Lexical Chains versus Keywords
for Topic Tracking. In A. Gelbukh, editor, Compu-
tational Linguistics and Intelligent Text Processing,
volume 2945 of Lecture Notes in Computer Science,
pages 507–510. Springer, Berlin / Heidelberg.
Irene Cramer, Marc Finthammer, Alexander Kurek,
Lukas Sowa, Melina Wachtling, and Tobias Claas.
2008. Experiments on Lexical Chaining for Ger-
man Corpora: Annotation, Extraction, and Applica-
tion. Journal for Language Technology and Computa-
tional Linguistics (JLCL), 23(2):34–48.
Georgiana Dinu and Mirella Lapata. 2010. Topic Mod-
els for Meaning Similarity in Context. In COLING
’10: Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, pages 250–
258, Beijing, China.
Michel Galley and Kathleen McKeown. 2003. Im-
proving word sense disambiguation in lexical chain-
ing. In IJCAI’03: Proceedings of the 18th interna-
tional joint conference on Artificial intelligence, pages
1486–1488, Acapulco, Mexico.
Yihong Gong and Xin Liu. 2001. Generic Text Sum-
marization Using Relevance Measure and Latent Se-
mantic Analysis. In SIGIR 2001: Proceedings of the
24th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 19–25, New Orleans, Louisiana, USA.
Stephen J. Green. 1996. Using Lexical Chains to Build
Hypertext Links in Newspaper Articles. In AAAI-
96 Workshop on Internet-based Information Systems,
pages 115–141, Portland, Oregon, USA.
Michael A. K. Halliday and Ruqaiya Hasan. 1976. Co-
hesion in English. English language series. Longman,
London.
Birgit Hamp and Helmut Feldweg. 1997. GermaNet
- a Lexical-Semantic Net for German. In Proceed-
</reference>
<page confidence="0.979437">
998
</page>
<reference confidence="0.999726113207547">
ings of the ACL/EACL-97 workshop Automatic Infor-
mation Extraction and Building of Lexical Semantic
Resources for NLP Applications, Madrid, Spain.
Ruqaiya Hasan. 1984. Coherence and Cohesive Har-
mony. In James Flood, editor, Understanding Reading
Comprehension, Cognition, Language, and the Struc-
ture of Prose, pages 181–220. International Reading
Association, Newark, Delaware, USA.
Leonhard Hennig. 2009. Topic-based multi-document
summarization with probabilistic latent semantic anal-
ysis. In Proceedings of the International Conference
RANLP-2009, pages 144–149, Borovets, Bulgaria.
Graeme Hirst and David St-Onge. 1998. Lexical Chains
as representation of context for the detection and cor-
rection malapropisms. In Christiane Fellbaum, edi-
tor, WordNet: An Electronic Lexical Database, Lan-
guage, Speech, and Communication, pages 305–332.
The MIT Press, Cambridge, Massachusetts, USA.
Thomas Hofmann. 1999. Probabilistic Latent Seman-
tic Analysis. In Proceedings of the Fifteenth Confer-
ence on Uncertainty in Artificial Intelligence, UAI ’99,
pages 289–296, Stockholm, Sweden.
William Hollingsworth and Simone Teufel. 2005. Hu-
man annotation of lexical chains: Coverage and agree-
ment measures. In Proceedings of the Workshop
ELECTRA: Methodologies and Evaluation of Lexical
Cohesion Techniques in Real-world Applications, In
Association with SIGIR ’05, Salvador, Brazil.
Lawrence Hubert and Phipps Arabie. 1985. Comparing
partitions. Journal of Classification, 2(1):193–218.
Christopher Manning, Prabhakar Raghavan, and Hinrich
Sch¨utze. 2008. An Introduction to Information Re-
trieval. Cambridge University Press, Cambridge, UK.
Meghana Marathe and Graeme Hirst. 2010. Lexical
Chains Using Distributional Measures of Concept D.
In Proceedings of the 11th International Conference
on Computational Linguistics and Intelligent Text Pro-
cessing, CICLing’10, pages 291–302, Ias¸i, Romania.
Olena Medelyan. 2007. Computing lexical chains with
graph clustering. In Proceedings of the 45th Annual
Meeting of the ACL: Student Research Workshop, ACL
’07, pages 85–90, Prague, Czech Republic.
Marina Meil˘a. 2005. Comparing clusterings: an ax-
iomatic view. In Proceedings of the 22nd Interna-
tional Conference on Machine Learning, ICML ’05,
pages 577–584, Bonn, Germany.
David Menestrina, Steven Euijong Whang, and Hec-
tor Garcia-Molina. 2010. Evaluating entity resolu-
tion results. Proceedings of the VLDB Endowment,
3(1):208–219.
Hemant Misra, Franc¸ois Yvon, Joemon Jose, and Olivier
Capp´e. 2009. Text Segmentation via Topic Mod-
eling: An Analytical Study. In Proceedings of the
18th ACM Conference on Information and Knowledge
Management, CIKM 2009, pages 1553–1556, Hong
Kong, China.
Jane Morris and Graeme Hirst. 1991. Lexical cohesion
computed by thesaural relations as an indicator of the
structure of text. Computational Linguistics, 17:21–
48.
Jane Morris and Graeme Hirst. 2004. The Subjectivity of
Lexical Cohesion in Text. In Proceedings of the AAAI
Spring Symposium on Exploring Attitude and Affect in
Text: Theories and Applications, Palo Alto, California,
USA.
Christoph M¨uller and Michael Strube. 2006. Multi-level
annotation of linguistic data with MMAX2. In Sabine
Braun, Kurt Kohn, and Joybrato Mukherjee, editors,
Corpus Technology and Language Pedagogy: New Re-
sources, New Tools, New Methods, pages 197–214. Pe-
ter Lang, Frankfurt a.M., Germany.
Manabu Okumura and Takeo Honda. 1994. Word sense
disambiguation and text segmentation based on lexical
cohesion. In COLING ’94: Proceedings of the 15th
Conference on Computational Linguistics, volume 2,
pages 755–761, Kyoto, Japan.
Martin Riedl and Chris Biemann. 2012. Sweeping
through the Topic Space: Bad luck? Roll again! In
ROBUS-UNSUP 2012: Joint Workshop on Unsuper-
vised and Semi-Supervised Learning in NLP held in
conjunction with EACL 2012, pages 19–27, Avignon,
France.
Peter Mark Roget. 1852. Roget’s Thesaurus of English
Words and Phrases. Longman Group Ltd., Harlow,
UK.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external cluster
evaluation measure. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 410–420,
Prague, Czech Republic.
H. Gregory Silber and Kathleen F. McCoy. 2002. Effi-
ciently computed lexical chains as an intermediate rep-
resentation for automatic text summarization. Compu-
tational Linguistics, 28(4):487–496.
Mark A. Stairmand. 1996. A Computational Analysis
of Lexical Cohesion with Applications in Information
Retrieval. Ph.D. thesis, Center for Computational Lin-
guistics, UMIST, Manchester.
Nicola Stokes, Joe Carthy, and Alan F. Smeaton. 2004.
SeLeCT: A Lexical Cohesion Based News Story Seg-
mentation System. AI Communications, 17(1):3–12.
Alexander Strehl. 2002. Relationship-based Cluster-
ing and Cluster Ensembles for High-dimensional Data
Mining. Ph.D. thesis, University of Texas, Austin.
</reference>
<page confidence="0.99872">
999
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.405005">
<title confidence="0.9993455">Three Knowledge-Free for Automatic Lexical Chain Extraction</title>
<author confidence="0.638919">Remus</author>
<affiliation confidence="0.825700666666667">FG Language Department of Computer Technische Universit¨at</affiliation>
<email confidence="0.878958">remus@kdsl.informatik.tu-darmstadt.de,biem@cs.tu-darmstadt.de</email>
<abstract confidence="0.995151">We present three approaches to lexical chaining based on the LDA topic model and evaluate them intrinsically on a manually annotated set of German documents. After motivating the choice of statistical methods for lexical chaining with their adaptability to different languages and subject domains, we describe our new two-level chain annotation scheme, which rooted in the concept of cohesive harmony. Also, we propose a new measure for direct evaluation of lexical chains. Our three LDA-based approaches outperform two knowledge-based state-of-the art methods to lexical chaining by a large margin, which can be attributed to lacking coverage of the knowledge resource. Subsequent analysis shows that the three methods yield a different chaining behavior, which could be utilized in tasks that use lexical chaining as a component within NLP applications.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jitendra Ajmera</author>
<author>Herv´e Bourlard</author>
<author>I Lapidot</author>
</authors>
<title>Unknown-Multiple Speaker clustering using HMM.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference of Spoken Language Processing, ICSLP ’02,</booktitle>
<location>Denver, Colorado, USA.</location>
<contexts>
<context position="7788" citStr="Ajmera et al., 2002" startWordPosition="1194" endWordPosition="1197">arison measure for the general case does not exist. It should be stressed that the appropriate clustering measure highly depends on the task at hand. After exploring a number of measures1, we decided on a combination of the adjusted Rand index (ARI, Hubert and Arabie (1985)) and the basic merge distance (BMD, Menestrina et al. (2010)) for our new measure. Menestrina et al. (2010) introduced a linear time algorithm for computing the generalized merge distance (GMD), which counts 1Explored measures which are unsatisfactory for the given task are: Closest Cluster F1 (Benjelloun et al., 2009), K (Ajmera et al., 2002), Pairwise F1 (Manning et al., 2008), Variation of Information (Meil˘a, 2005), B3(Bagga and Baldwin, 1998), VMeasure (Rosenberg and Hirschberg, 2007), Normalized Mutual Information (Strehl, 2002). The last two measures are equal. A proof of this can be found in the appendix. 990 split and merge cluster editing operations. Using a constant factor of 1 for both splits and merges gives the basic merge distance (BMD): Considering T as the most general clustering of a dataset D, where all elements are grouped into the same cluster, and further considering 1 as the most specific clustering of D, whe</context>
</contexts>
<marker>Ajmera, Bourlard, Lapidot, 2002</marker>
<rawString>Jitendra Ajmera, Herv´e Bourlard, and I. Lapidot. 2002. Unknown-Multiple Speaker clustering using HMM. In Proceedings of the International Conference of Spoken Language Processing, ICSLP ’02, Denver, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enrique Amig´o</author>
<author>Julio Gonzalo</author>
<author>Javier Artiles</author>
<author>Felisa Verdejo</author>
</authors>
<title>A comparison of extrinsic clustering evaluation metrics based on formal constraints. Information Retrieval,</title>
<date>2009</date>
<pages>12--461</pages>
<marker>Amig´o, Gonzalo, Artiles, Verdejo, 2009</marker>
<rawString>Enrique Amig´o, Julio Gonzalo, Javier Artiles, and Felisa Verdejo. 2009. A comparison of extrinsic clustering evaluation metrics based on formal constraints. Information Retrieval, 12:461–486.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Algorithms for Scoring Coreference Chains.</title>
<date>1998</date>
<booktitle>In Proceedings of the Linguistic Coreference Workshop at The First International Conference on Language Resources and Evaluation, LREC ’98,</booktitle>
<pages>563--566</pages>
<location>Granada,</location>
<contexts>
<context position="7894" citStr="Bagga and Baldwin, 1998" startWordPosition="1209" endWordPosition="1212">ing measure highly depends on the task at hand. After exploring a number of measures1, we decided on a combination of the adjusted Rand index (ARI, Hubert and Arabie (1985)) and the basic merge distance (BMD, Menestrina et al. (2010)) for our new measure. Menestrina et al. (2010) introduced a linear time algorithm for computing the generalized merge distance (GMD), which counts 1Explored measures which are unsatisfactory for the given task are: Closest Cluster F1 (Benjelloun et al., 2009), K (Ajmera et al., 2002), Pairwise F1 (Manning et al., 2008), Variation of Information (Meil˘a, 2005), B3(Bagga and Baldwin, 1998), VMeasure (Rosenberg and Hirschberg, 2007), Normalized Mutual Information (Strehl, 2002). The last two measures are equal. A proof of this can be found in the appendix. 990 split and merge cluster editing operations. Using a constant factor of 1 for both splits and merges gives the basic merge distance (BMD): Considering T as the most general clustering of a dataset D, where all elements are grouped into the same cluster, and further considering 1 as the most specific clustering of D, where each element builds its own cluster, the lattice between T and 1 spans all possible clusterings and the</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Amit Bagga and Breck Baldwin. 1998. Algorithms for Scoring Coreference Chains. In Proceedings of the Linguistic Coreference Workshop at The First International Conference on Language Resources and Evaluation, LREC ’98, pages 563–566, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet Project.</title>
<date>1998</date>
<booktitle>In COLING ’98: Proceedings of the 17th International Conference on Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>86--90</pages>
<location>Montreal, Quebec, Canada.</location>
<contexts>
<context position="11414" citStr="Baker et al., 1998" startWordPosition="1827" endWordPosition="1830">cheme that gets us reasonable inter-annotator agreement, inspired by the concept of cohesive harmony (Hasan, 1984), and report on an annotation project for German newswire texts. Documents from the SALSA 2.0 (Burchardt et al., 2006) corpus were chosen to form the basis for the annotation of lexical chain information. SALSA is based on the semi-automatically annotated TIGER Treebank 2.1 (Brants et al., 2002). The TIGER treebank provides manual annotations, such as lemmas, part-of-speech tags, and syntactic structure, the SALSA part of the corpus is also partially annotated with FrameNet-style (Baker et al., 1998) frame annotation. The documents are general domain news articles from a German newspaper comprising about 1,550 documents and around 50,000 sentences in total, with a median document length of 275 tokens. 3.1 Annotation Scheme In order to minimize the subjectiveness of choices by different annotators, annotation guidelines were developed comprising a total of ten pages. We decided to consider only nouns, noun compounds and non-compositional adjective noun phrases like “dirty money” as candidate terms for lexical chaining, which is consistent with the procedures of Hollingsworth and Teufel (20</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet Project. In COLING ’98: Proceedings of the 17th International Conference on Computational Linguistics, volume 1, pages 86–90, Montreal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Michael Elhadad</author>
</authors>
<title>Using Lexical Chains for Text Summarization.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>10--17</pages>
<location>Madrid,</location>
<contexts>
<context position="2091" citStr="Barzilay and Elhadad, 1997" startWordPosition="312" endWordPosition="315">ning of a text, the latter can be seen as a collection of devices for creating it. Cohesion and coherence build the basis for most of the current natural language processing problems that deal with text understanding. Lexical cohesion ties together words or phrases that are semantically related. Once all the cohesive ties are identified the involved items can be grouped together to form so-called lexical chains, which form a theoretically well-founded building block in various natural language processing applications, such as word sense disambiguation (Okumura and Honda, 1994), summarization (Barzilay and Elhadad, 1997), malapropism detection and correction (Hirst and StOnge, 1998), document hyperlinking (Green, 1996), text segmentation (Stokes et al., 2004), topic tracking (Carthy, 2004), and others. The performance of the individual task heavily depends on the quality of the identified lexical chains. 1.1 Motivation for Corpus-driven Approach Previous approaches mainly focus on the use of knowledge resources like lexical semantic databases (Hirst and St-Onge, 1998) or thesauri (Morris and Hirst, 1991) as background information in order to resolve possible semantic relations. A major drawback of this strate</context>
<context position="5217" citStr="Barzilay and Elhadad (1997)" startWordPosition="779" endWordPosition="782"> initially proposed an algorithm for lexical chaining based on Roget’s thesaurus (Roget, 1852), and manually assessed the quality of their algorithm. Hirst and St-Onge (1998) first presented a computational approach to lexical chaining based on WordNet showing that the lexical database is a reasonable replacement to Roget’s. The basic idea behind these algorithms is that semantically close words should be connected to form chains. Subsequent approaches mainly concentrated on disambiguation of words to WordNet concepts (WSD), since ambiguous words can lead to the overgeneration of connections. Barzilay and Elhadad (1997) improved the implicit word sense disambiguation (WSD) by keeping a list of different interpretations of the text and finally choosing the most plausible senses for chaining. Silber and McCoy (2002) introduced an efficient variant of the algorithm with linear complexity in the number of candidate terms. Galley and McKeown (2003) further improved accuracy by first performing WSD, and then using the remaining links between the disambiguated concepts only. They also introduced a socalled disambiguation graph, a representation that has also been utilized by the method of Medelyan (2007), where she</context>
</contexts>
<marker>Barzilay, Elhadad, 1997</marker>
<rawString>Regina Barzilay and Michael Elhadad. 1997. Using Lexical Chains for Text Summarization. In Proceedings of the ACL Workshop on Intelligent Scalable Text Summarization, pages 10–17, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
</authors>
<title>Lexical Chains for Summarization. Master’s thesis,</title>
<date>1997</date>
<institution>Ben-Gurion University of the Negev,</institution>
<location>Beersheva, Israel.</location>
<contexts>
<context position="16163" citStr="Barzilay (1997)" startWordPosition="2604" endWordPosition="2605">chains). However, the considerable overlap between the annotators still enables us to evaluate automatic chaining methods, and the lccm agreement score serves as an upper bound. Note that by performing no reconciliation of the annotations we explicitly allow the possibility of different interpretations which is in our opinion appropriate here due to the subjectiveness of the task itself. By doing so, we evaluate our algorithms against individual annotator interpretations. 4 Statistical Methods for Lexical Chaining This work employs a well-studied statistical method for creating something that Barzilay (1997) called an automatic thesaurus which will then be adapted for lexical chaining. For our automatic approaches, candidate lexical items in a text are preselected by the same heuristic that is also applied in Section 3 for the annotation process. Topic models (TMs) are a suite of unsuper80 60 40 2068 1069 0337 0364 0897 1323 0100 1388 0962 0545 1156 1190 0122 1194 1855 1808 0356 1779 2040 0219 1734 1216 0947 0937 2026 1548 1232 1826 1224 1251 1239 2044 1119 0174 1650 2014 0630 0199 0655 1818 0310 1150 1726 2035 0382 1639 0275 1047 1707 0228 1050 1607 0984 0755 0752 0747 1162 1018 0182 0942 0492 0</context>
</contexts>
<marker>Barzilay, 1997</marker>
<rawString>Regina Barzilay. 1997. Lexical Chains for Summarization. Master’s thesis, Ben-Gurion University of the Negev, Beersheva, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar Benjelloun</author>
<author>Hector Garcia-Molina</author>
<author>David Menestrina</author>
<author>Qi Su</author>
<author>Steven Whang</author>
<author>Jennifer Widom</author>
</authors>
<title>Swoosh: a generic approach to entity resolution.</title>
<date>2009</date>
<journal>The VLDB Journal,</journal>
<pages>18--255</pages>
<contexts>
<context position="7763" citStr="Benjelloun et al., 2009" startWordPosition="1189" endWordPosition="1192">2009), a best clustering comparison measure for the general case does not exist. It should be stressed that the appropriate clustering measure highly depends on the task at hand. After exploring a number of measures1, we decided on a combination of the adjusted Rand index (ARI, Hubert and Arabie (1985)) and the basic merge distance (BMD, Menestrina et al. (2010)) for our new measure. Menestrina et al. (2010) introduced a linear time algorithm for computing the generalized merge distance (GMD), which counts 1Explored measures which are unsatisfactory for the given task are: Closest Cluster F1 (Benjelloun et al., 2009), K (Ajmera et al., 2002), Pairwise F1 (Manning et al., 2008), Variation of Information (Meil˘a, 2005), B3(Bagga and Baldwin, 1998), VMeasure (Rosenberg and Hirschberg, 2007), Normalized Mutual Information (Strehl, 2002). The last two measures are equal. A proof of this can be found in the appendix. 990 split and merge cluster editing operations. Using a constant factor of 1 for both splits and merges gives the basic merge distance (BMD): Considering T as the most general clustering of a dataset D, where all elements are grouped into the same cluster, and further considering 1 as the most spec</context>
</contexts>
<marker>Benjelloun, Garcia-Molina, Menestrina, Su, Whang, Widom, 2009</marker>
<rawString>Omar Benjelloun, Hector Garcia-Molina, David Menestrina, Qi Su, Steven Whang, and Jennifer Widom. 2009. Swoosh: a generic approach to entity resolution. The VLDB Journal, 18:255–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
</authors>
<title>Chinese Whispers – an Efficient Graph Clustering Algorithm and its Application to Natural Language Processing.</title>
<date>2006</date>
<booktitle>In Proceedings of TextGraphs: the Second Workshop on Graph Based Methods for Natural Language Processing,</booktitle>
<pages>73--80</pages>
<location>New York City, USA.</location>
<contexts>
<context position="21549" citStr="Biemann (2006)" startWordPosition="3551" endWordPosition="3552">sures: Euclidian (dis-)similarity and cosine similarity. Let G = (V, E) be the graph representation of a document with term vertices V = 1v1, ... , vNd} and weighted edges E = 1(v1, v2, sim12), ... (vNd, vNd−1, simNdNd−1)}, where simij is either the cosine or Euclidean similarity of term vectors. For simplicity, we reduce this representation to an unweighted graph by only retaining edges (of unit weight) that have a similarity above a parameter threshold csim. To identify chains as clusters in this graph, we follow Medelyan (2007) and apply the Chinese Whispers graph clustering algorithm (CW, Biemann (2006)), which finds the number of clusters automatically. The CW algorithm implementation comes with 993 three parameters to regulate the node weight based on its degree, which influences cluster size and granularity. We test options ”top”, ”dist log” and ”dist lin”. The final chaining procedure is straightforward: The LDA-GM algorithm assigns every candidate lexical item wi of a certain document d which is assigned the same class label ci to the same chain. Level two links are drawn using the second dominant class of a vertex’s neighborhood, which is provided by the CW implementation. 4.3 LDA Top-</context>
</contexts>
<marker>Biemann, 2006</marker>
<rawString>Chris Biemann. 2006. Chinese Whispers – an Efficient Graph Clustering Algorithm and its Application to Natural Language Processing. In Proceedings of TextGraphs: the Second Workshop on Graph Based Methods for Natural Language Processing, pages 73– 80, New York City, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
</authors>
<date>2012</date>
<booktitle>Structure Discovery in Natural Language. Theory and Applications of Natural Language Processing.</booktitle>
<publisher>Springer</publisher>
<location>Berlin / Heidelberg.</location>
<contexts>
<context position="3748" citStr="Biemann, 2012" startWordPosition="558" endWordPosition="559">ation (Gong and Liu, 2001; Hennig, 2009), text segmentation (Misra et al., 2009), lexical substitution (Dinu and Lapata, 2010) or word sense disambiguation (Cai et al., 2007; Boyd-Graber et al., 2007). 989 Proceedings of NAACL-HLT 2013, pages 989–999, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics In this work, we address the question, whether statistical methods for the extraction of lexical chains can yield better results than existing knowledge-based methods, especially for underresourced languages or domains, following principles of Structure Discovery (Biemann, 2012). To address this, we have developed a methodology for evaluating the quality of lexical chains intrinsically, have carried out an annotation study, and report results on a corpus of manually annotated German news documents. After defining a measure for the comparison of (manually or automatically created) lexical chains in Section 2, Section 3 describes our annotation methodology and discusses issues regarding the inherent subjectivity of lexical chain annotation. In Section 4, three statistical approaches for lexical chaining are developed on the basis of the LDA topic model. Experiments tha</context>
<context position="18644" citStr="Biemann, 2012" startWordPosition="3049" endWordPosition="3050"> in automatic lexical chainers. Specifically, we use the GibbsLDA++5 framework for topic model estimation and inference, and examine the following LDA parameters: number of topics T, Dirichlet hyperparameters for documenttopic distribution α and topic-term distribution Q. We now describe three LDA-based approaches to lexical chaining. 4.1 LDA Mode Method (LDA-MM) The LDA-MM approach places all word tokens that share the same topic ID into the same chain. The point is now how to decide to which topic a word belongs to. Since single samples of topics per word exhibit a large variance (Riedl and Biemann, 2012), we follow these authors by sampling several times and using the mode (most frequently assigned) topic ID per word as the topic assignment. This strategy reduced the variance in the lccm to a tenth6. More formally, let samples(d,w) be the vector of assignments that have been collected for a certain word w in a certain document d with each samples(d,w) i referring to the i-th sampled topic ID for (d, w). In other words, samples(d,w) can be seen as the Markov chain for a particular word in a particular document. Further let z(d,w) be the topic ID that was most assigned to the word w with respec</context>
</contexts>
<marker>Biemann, 2012</marker>
<rawString>Chris Biemann. 2012. Structure Discovery in Natural Language. Theory and Applications of Natural Language Processing. Springer Berlin / Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="17895" citStr="Blei et al. (2003)" startWordPosition="2928" endWordPosition="2931">1310 0268 1033 1168 1523 1737 0524 0338 0976 0639 0299 1524 1387 0694 0278 1692 1534 1037 0499 0721 0283 0736 0258 1130 0988 0649 1777 0309 0110 1319 0729 0730 0390 0925 992 vised algorithms designed for unveiling some hidden structure in large data collections. The key idea is that documents can be represented as composites of so-called topics where a topic itself represents as a composite of words. Hofmann (1999) defined a topic to be a probability distribution over words and a document to be a probability distribution over a fixed set of topics. We use the latent Dirichlet allocation (LDA, Blei et al. (2003)) topic model for estimating the semantic closeness of candidate terms, and explore different ways of utilizing LDA’s topic information in automatic lexical chainers. Specifically, we use the GibbsLDA++5 framework for topic model estimation and inference, and examine the following LDA parameters: number of topics T, Dirichlet hyperparameters for documenttopic distribution α and topic-term distribution Q. We now describe three LDA-based approaches to lexical chaining. 4.1 LDA Mode Method (LDA-MM) The LDA-MM approach places all word tokens that share the same topic ID into the same chain. The po</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>David M Blei</author>
<author>Xiaojin Zhu</author>
</authors>
<title>A Topic Model for Word Sense Disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>1024--1033</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3334" citStr="Boyd-Graber et al., 2007" startWordPosition="500" endWordPosition="503">cy on the coverage of the resource, which has a direct impact on the lexical chains. Their quality can be expected to be poor for resource-scarce languages or specialized application domains. Statistical methods to modeling language semantics have proven to deliver good results in many natural language processing applications. In particular, probabilistic topic models have been successfully used for tasks such as summarization (Gong and Liu, 2001; Hennig, 2009), text segmentation (Misra et al., 2009), lexical substitution (Dinu and Lapata, 2010) or word sense disambiguation (Cai et al., 2007; Boyd-Graber et al., 2007). 989 Proceedings of NAACL-HLT 2013, pages 989–999, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics In this work, we address the question, whether statistical methods for the extraction of lexical chains can yield better results than existing knowledge-based methods, especially for underresourced languages or domains, following principles of Structure Discovery (Biemann, 2012). To address this, we have developed a methodology for evaluating the quality of lexical chains intrinsically, have carried out an annotation study, and report results on a corpus of man</context>
</contexts>
<marker>Boyd-Graber, Blei, Zhu, 2007</marker>
<rawString>Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu. 2007. A Topic Model for Word Sense Disambiguation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 1024–1033, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Brants</author>
<author>Stefanie Dipper</author>
<author>Silvia Hansen</author>
<author>Wolfgang Lezius</author>
<author>George Smith</author>
</authors>
<title>TIGER Treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of the Workshop on Treebanks and Linguistic Theories (TLT02), Sozopol,</booktitle>
<contexts>
<context position="11205" citStr="Brants et al., 2002" startWordPosition="1796" endWordPosition="1799">ns should enable us to optimize towards higher-quality chain annotations, which is a task of its own right and which has the potential to improve all subsequent applications. For this, we devise an annotation scheme that gets us reasonable inter-annotator agreement, inspired by the concept of cohesive harmony (Hasan, 1984), and report on an annotation project for German newswire texts. Documents from the SALSA 2.0 (Burchardt et al., 2006) corpus were chosen to form the basis for the annotation of lexical chain information. SALSA is based on the semi-automatically annotated TIGER Treebank 2.1 (Brants et al., 2002). The TIGER treebank provides manual annotations, such as lemmas, part-of-speech tags, and syntactic structure, the SALSA part of the corpus is also partially annotated with FrameNet-style (Baker et al., 1998) frame annotation. The documents are general domain news articles from a German newspaper comprising about 1,550 documents and around 50,000 sentences in total, with a median document length of 275 tokens. 3.1 Annotation Scheme In order to minimize the subjectiveness of choices by different annotators, annotation guidelines were developed comprising a total of ten pages. We decided to con</context>
</contexts>
<marker>Brants, Dipper, Hansen, Lezius, Smith, 2002</marker>
<rawString>Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith. 2002. TIGER Treebank. In Proceedings of the Workshop on Treebanks and Linguistic Theories (TLT02), Sozopol, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aljoscha Burchardt</author>
<author>Katrin Erk</author>
<author>Anette Frank</author>
<author>Andrea Kowalski</author>
<author>Sebastian Pad´o</author>
<author>Manfred Pinkal</author>
</authors>
<title>The SALSA Corpus: a German corpus resource for lexical semantics.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th international conference on Language Resources and evaluation (LREC-2006),</booktitle>
<location>Genoa, Italy.</location>
<marker>Burchardt, Erk, Frank, Kowalski, Pad´o, Pinkal, 2006</marker>
<rawString>Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea Kowalski, Sebastian Pad´o, and Manfred Pinkal. 2006. The SALSA Corpus: a German corpus resource for lexical semantics. In Proceedings of the 5th international conference on Language Resources and evaluation (LREC-2006), Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junfu Cai</author>
<author>Wee Sun Lee</author>
<author>Yee Whye Teh</author>
</authors>
<title>Improving Word Sense Disambiguation Using Topic Features.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>1015--1023</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3307" citStr="Cai et al., 2007" startWordPosition="496" endWordPosition="499">gy is the dependency on the coverage of the resource, which has a direct impact on the lexical chains. Their quality can be expected to be poor for resource-scarce languages or specialized application domains. Statistical methods to modeling language semantics have proven to deliver good results in many natural language processing applications. In particular, probabilistic topic models have been successfully used for tasks such as summarization (Gong and Liu, 2001; Hennig, 2009), text segmentation (Misra et al., 2009), lexical substitution (Dinu and Lapata, 2010) or word sense disambiguation (Cai et al., 2007; Boyd-Graber et al., 2007). 989 Proceedings of NAACL-HLT 2013, pages 989–999, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics In this work, we address the question, whether statistical methods for the extraction of lexical chains can yield better results than existing knowledge-based methods, especially for underresourced languages or domains, following principles of Structure Discovery (Biemann, 2012). To address this, we have developed a methodology for evaluating the quality of lexical chains intrinsically, have carried out an annotation study, and report</context>
</contexts>
<marker>Cai, Lee, Teh, 2007</marker>
<rawString>Junfu Cai, Wee Sun Lee, and Yee Whye Teh. 2007. Improving Word Sense Disambiguation Using Topic Features. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 1015–1023, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joe Carthy</author>
</authors>
<title>Lexical Chains versus Keywords for Topic Tracking.</title>
<date>2004</date>
<booktitle>Computational Linguistics and Intelligent Text Processing,</booktitle>
<volume>2945</volume>
<pages>507--510</pages>
<editor>In A. Gelbukh, editor,</editor>
<publisher>Springer,</publisher>
<location>Berlin / Heidelberg.</location>
<contexts>
<context position="2263" citStr="Carthy, 2004" startWordPosition="338" endWordPosition="339">t deal with text understanding. Lexical cohesion ties together words or phrases that are semantically related. Once all the cohesive ties are identified the involved items can be grouped together to form so-called lexical chains, which form a theoretically well-founded building block in various natural language processing applications, such as word sense disambiguation (Okumura and Honda, 1994), summarization (Barzilay and Elhadad, 1997), malapropism detection and correction (Hirst and StOnge, 1998), document hyperlinking (Green, 1996), text segmentation (Stokes et al., 2004), topic tracking (Carthy, 2004), and others. The performance of the individual task heavily depends on the quality of the identified lexical chains. 1.1 Motivation for Corpus-driven Approach Previous approaches mainly focus on the use of knowledge resources like lexical semantic databases (Hirst and St-Onge, 1998) or thesauri (Morris and Hirst, 1991) as background information in order to resolve possible semantic relations. A major drawback of this strategy is the dependency on the coverage of the resource, which has a direct impact on the lexical chains. Their quality can be expected to be poor for resource-scarce language</context>
</contexts>
<marker>Carthy, 2004</marker>
<rawString>Joe Carthy. 2004. Lexical Chains versus Keywords for Topic Tracking. In A. Gelbukh, editor, Computational Linguistics and Intelligent Text Processing, volume 2945 of Lecture Notes in Computer Science, pages 507–510. Springer, Berlin / Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Cramer</author>
<author>Marc Finthammer</author>
<author>Alexander Kurek</author>
<author>Lukas Sowa</author>
<author>Melina Wachtling</author>
<author>Tobias Claas</author>
</authors>
<title>Experiments on Lexical Chaining for German Corpora: Annotation, Extraction, and Application.</title>
<date>2008</date>
<booktitle>Journal for Language Technology and Computational Linguistics (JLCL),</booktitle>
<pages>23--2</pages>
<contexts>
<context position="10420" citStr="Cramer et al. (2008)" startWordPosition="1674" endWordPosition="1677">, FN: pairs in D but not in D’, TN: pairs not in D and not in D’, where D is the underlying dataset of C, D’ is the underlying dataset of C’, and pairs means all unique combinations of elements that are in the same cluster. 3 Annotating Lexical Chains A challenge with the annotation of lexical chains is the subjective interpretation of the text by individual annotators (Morris and Hirst, 2004), which also substantiates the fact that currently no gold standard exist, and all previous automatic approaches are evaluated by performing a certain NLP task. Hollingsworth and Teufel (2005) as well as Cramer et al. (2008) conclude from their lexical chain annotation projects that high inter-annotator agreement is very hard to achieve. We argue that directly evaluating on lexical chains should enable us to optimize towards higher-quality chain annotations, which is a task of its own right and which has the potential to improve all subsequent applications. For this, we devise an annotation scheme that gets us reasonable inter-annotator agreement, inspired by the concept of cohesive harmony (Hasan, 1984), and report on an annotation project for German newswire texts. Documents from the SALSA 2.0 (Burchardt et al.</context>
<context position="12042" citStr="Cramer et al. (2008)" startWordPosition="1923" endWordPosition="1926">nnotation. The documents are general domain news articles from a German newspaper comprising about 1,550 documents and around 50,000 sentences in total, with a median document length of 275 tokens. 3.1 Annotation Scheme In order to minimize the subjectiveness of choices by different annotators, annotation guidelines were developed comprising a total of ten pages. We decided to consider only nouns, noun compounds and non-compositional adjective noun phrases like “dirty money” as candidate terms for lexical chaining, which is consistent with the procedures of Hollingsworth and Teufel (2005) and Cramer et al. (2008). For annotation, we used the MMAX24 (M¨uller and Strube, 2006) tool. We introduce the term dense chain, which refers to a type of lexical chain in which every element is 4http://mmax2.sourceforge.net 991 lccm related to every other element in that chain. Terms are considered to be related if they share the same topic, i.e. common sense and knowledge of the language is needed to decide which terms belong together in the same topic and whether a chosen topic is neither too broad nor too narrow. A single dense chain can thus be assigned a definite topical description of its items. Whereas Hollin</context>
</contexts>
<marker>Cramer, Finthammer, Kurek, Sowa, Wachtling, Claas, 2008</marker>
<rawString>Irene Cramer, Marc Finthammer, Alexander Kurek, Lukas Sowa, Melina Wachtling, and Tobias Claas. 2008. Experiments on Lexical Chaining for German Corpora: Annotation, Extraction, and Application. Journal for Language Technology and Computational Linguistics (JLCL), 23(2):34–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Mirella Lapata</author>
</authors>
<title>Topic Models for Meaning Similarity in Context. In</title>
<date>2010</date>
<booktitle>COLING ’10: Proceedings of the 23rd International Conference on Computational Linguistics: Posters,</booktitle>
<pages>250--258</pages>
<location>Beijing, China.</location>
<contexts>
<context position="3260" citStr="Dinu and Lapata, 2010" startWordPosition="488" endWordPosition="491">e semantic relations. A major drawback of this strategy is the dependency on the coverage of the resource, which has a direct impact on the lexical chains. Their quality can be expected to be poor for resource-scarce languages or specialized application domains. Statistical methods to modeling language semantics have proven to deliver good results in many natural language processing applications. In particular, probabilistic topic models have been successfully used for tasks such as summarization (Gong and Liu, 2001; Hennig, 2009), text segmentation (Misra et al., 2009), lexical substitution (Dinu and Lapata, 2010) or word sense disambiguation (Cai et al., 2007; Boyd-Graber et al., 2007). 989 Proceedings of NAACL-HLT 2013, pages 989–999, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics In this work, we address the question, whether statistical methods for the extraction of lexical chains can yield better results than existing knowledge-based methods, especially for underresourced languages or domains, following principles of Structure Discovery (Biemann, 2012). To address this, we have developed a methodology for evaluating the quality of lexical chains intrinsically, h</context>
</contexts>
<marker>Dinu, Lapata, 2010</marker>
<rawString>Georgiana Dinu and Mirella Lapata. 2010. Topic Models for Meaning Similarity in Context. In COLING ’10: Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 250– 258, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen McKeown</author>
</authors>
<title>Improving word sense disambiguation in lexical chaining.</title>
<date>2003</date>
<booktitle>In IJCAI’03: Proceedings of the 18th international joint conference on Artificial intelligence,</booktitle>
<pages>1486--1488</pages>
<location>Acapulco, Mexico.</location>
<contexts>
<context position="5547" citStr="Galley and McKeown (2003)" startWordPosition="833" endWordPosition="836">asic idea behind these algorithms is that semantically close words should be connected to form chains. Subsequent approaches mainly concentrated on disambiguation of words to WordNet concepts (WSD), since ambiguous words can lead to the overgeneration of connections. Barzilay and Elhadad (1997) improved the implicit word sense disambiguation (WSD) by keeping a list of different interpretations of the text and finally choosing the most plausible senses for chaining. Silber and McCoy (2002) introduced an efficient variant of the algorithm with linear complexity in the number of candidate terms. Galley and McKeown (2003) further improved accuracy by first performing WSD, and then using the remaining links between the disambiguated concepts only. They also introduced a socalled disambiguation graph, a representation that has also been utilized by the method of Medelyan (2007), where she applied a graph clustering algorithm to the disambiguation graph to cut weak links, performing implicit WSD. A combination of statistical and knowledge-based methods is presented by Marathe and Hirst (2010), who combine distributional co-occurrence information with semantic information from a lexicographic resource for extracti</context>
<context position="25772" citStr="Galley and McKeown (2003)" startWordPosition="4274" endWordPosition="4278">ented three baselines, which we describe below. One baseline is trivial, two baselines are state-of-the art knowledge-based systems adapted to German. Random: Candidate lexical items are randomly tied together to form sets of lexical chains. Level two links are created analogously. We regulate the process to yield the same average number of chains and links as in the development and test data. S&amp;M GermaNet: Algorithm by Silber and McCoy (2002) with GermaNet as its knowledge resource. 7as provided by Projekt Deutscher Wortschatz, http://wortschatz.uni-leipzig.de/ 994 G&amp;M GermaNet: Algorithm by Galley and McKeown (2003), also using GermaNet. GermaNet (Hamp and Feldweg, 1997) is a large WordNet-like resource for German, containing almost 100,000 lexical units and over 87,000 conceptual relations between synsets. While its size is only about half of WordNet, it is one of the largest nonEnglish lexical semantic resources. 5.2 Model Selection We optimize two sets of parameters: parameters for the LDA topic model (number of topics K, Dirichlet hyperparameters a and Q) are optimized for the LDA-MM method only, and the same LDA model is used in the other two LDA-based methods. Parameters particular to the respectiv</context>
<context position="32057" citStr="Galley and McKeown (2003)" startWordPosition="5291" endWordPosition="5294">ed by the various methods. Data Analysis Table 3 shows quantitative numbers of the extracted lexical chains in the test set. The LDA-MM approach chains and links a lot more items than the other statistical methods: it creates a lot more links between items that would otherwise be removed because they form unlinked singleton chains. As opposed to this, the graph method (LDA-GM), as well as the top-n method (LDA-TM) perform an implicit filtering on the candidate lexical items by creating less level two links, yet larger dense chains. The knowledge based algorithms by Silber and McCoy (2002) and Galley and McKeown (2003) extract fewer and smaller chains than the statistical approaches, which reflects GermaNet’s sparsity issues. While higher lexical coverage in the underlying resource would increase the coverage of our knowledge-based systems, this is only one part of the story. The other part is rooted in the fact that lexical cohesion relations, which are used in lexical chains, encompass many more semantic relations than listed in today’s lexical semantic networks. This especially holds for cases where several expressions refer to the same event or theme for which no well-defined relation exists, such as e.</context>
</contexts>
<marker>Galley, McKeown, 2003</marker>
<rawString>Michel Galley and Kathleen McKeown. 2003. Improving word sense disambiguation in lexical chaining. In IJCAI’03: Proceedings of the 18th international joint conference on Artificial intelligence, pages 1486–1488, Acapulco, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yihong Gong</author>
<author>Xin Liu</author>
</authors>
<title>Generic Text Summarization Using Relevance Measure and Latent Semantic Analysis.</title>
<date>2001</date>
<booktitle>In SIGIR 2001: Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>pages</pages>
<location>New Orleans, Louisiana, USA.</location>
<contexts>
<context position="3159" citStr="Gong and Liu, 2001" startWordPosition="474" endWordPosition="477"> 1998) or thesauri (Morris and Hirst, 1991) as background information in order to resolve possible semantic relations. A major drawback of this strategy is the dependency on the coverage of the resource, which has a direct impact on the lexical chains. Their quality can be expected to be poor for resource-scarce languages or specialized application domains. Statistical methods to modeling language semantics have proven to deliver good results in many natural language processing applications. In particular, probabilistic topic models have been successfully used for tasks such as summarization (Gong and Liu, 2001; Hennig, 2009), text segmentation (Misra et al., 2009), lexical substitution (Dinu and Lapata, 2010) or word sense disambiguation (Cai et al., 2007; Boyd-Graber et al., 2007). 989 Proceedings of NAACL-HLT 2013, pages 989–999, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics In this work, we address the question, whether statistical methods for the extraction of lexical chains can yield better results than existing knowledge-based methods, especially for underresourced languages or domains, following principles of Structure Discovery (Biemann, 2012). To addres</context>
</contexts>
<marker>Gong, Liu, 2001</marker>
<rawString>Yihong Gong and Xin Liu. 2001. Generic Text Summarization Using Relevance Measure and Latent Semantic Analysis. In SIGIR 2001: Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 19–25, New Orleans, Louisiana, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen J Green</author>
</authors>
<title>Using Lexical Chains to Build Hypertext Links in Newspaper Articles.</title>
<date>1996</date>
<booktitle>In AAAI96 Workshop on Internet-based Information Systems,</booktitle>
<pages>115--141</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="2191" citStr="Green, 1996" startWordPosition="327" endWordPosition="328"> basis for most of the current natural language processing problems that deal with text understanding. Lexical cohesion ties together words or phrases that are semantically related. Once all the cohesive ties are identified the involved items can be grouped together to form so-called lexical chains, which form a theoretically well-founded building block in various natural language processing applications, such as word sense disambiguation (Okumura and Honda, 1994), summarization (Barzilay and Elhadad, 1997), malapropism detection and correction (Hirst and StOnge, 1998), document hyperlinking (Green, 1996), text segmentation (Stokes et al., 2004), topic tracking (Carthy, 2004), and others. The performance of the individual task heavily depends on the quality of the identified lexical chains. 1.1 Motivation for Corpus-driven Approach Previous approaches mainly focus on the use of knowledge resources like lexical semantic databases (Hirst and St-Onge, 1998) or thesauri (Morris and Hirst, 1991) as background information in order to resolve possible semantic relations. A major drawback of this strategy is the dependency on the coverage of the resource, which has a direct impact on the lexical chain</context>
</contexts>
<marker>Green, 1996</marker>
<rawString>Stephen J. Green. 1996. Using Lexical Chains to Build Hypertext Links in Newspaper Articles. In AAAI96 Workshop on Internet-based Information Systems, pages 115–141, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A K Halliday</author>
<author>Ruqaiya Hasan</author>
</authors>
<title>Cohesion in English. English language series.</title>
<date>1976</date>
<location>Longman, London.</location>
<contexts>
<context position="1339" citStr="Halliday and Hasan, 1976" startWordPosition="194" endWordPosition="197"> of lexical chains. Our three LDA-based approaches outperform two knowledge-based state-of-the art methods to lexical chaining by a large margin, which can be attributed to lacking coverage of the knowledge resource. Subsequent analysis shows that the three methods yield a different chaining behavior, which could be utilized in tasks that use lexical chaining as a component within NLP applications. 1 Introduction A text that is understandable by its nature exhibits an underlying structure which makes the text coherent; that is, the structure is responsible for making the text “hang” together (Halliday and Hasan, 1976). The theoretic foundation of this structure is defined as coherence and cohesion. While the former is concerned with the meaning of a text, the latter can be seen as a collection of devices for creating it. Cohesion and coherence build the basis for most of the current natural language processing problems that deal with text understanding. Lexical cohesion ties together words or phrases that are semantically related. Once all the cohesive ties are identified the involved items can be grouped together to form so-called lexical chains, which form a theoretically well-founded building block in v</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>Michael A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion in English. English language series. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Birgit Hamp</author>
<author>Helmut Feldweg</author>
</authors>
<title>GermaNet - a Lexical-Semantic Net for German.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL/EACL-97 workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications,</booktitle>
<location>Madrid,</location>
<contexts>
<context position="25828" citStr="Hamp and Feldweg, 1997" startWordPosition="4283" endWordPosition="4286">ne is trivial, two baselines are state-of-the art knowledge-based systems adapted to German. Random: Candidate lexical items are randomly tied together to form sets of lexical chains. Level two links are created analogously. We regulate the process to yield the same average number of chains and links as in the development and test data. S&amp;M GermaNet: Algorithm by Silber and McCoy (2002) with GermaNet as its knowledge resource. 7as provided by Projekt Deutscher Wortschatz, http://wortschatz.uni-leipzig.de/ 994 G&amp;M GermaNet: Algorithm by Galley and McKeown (2003), also using GermaNet. GermaNet (Hamp and Feldweg, 1997) is a large WordNet-like resource for German, containing almost 100,000 lexical units and over 87,000 conceptual relations between synsets. While its size is only about half of WordNet, it is one of the largest nonEnglish lexical semantic resources. 5.2 Model Selection We optimize two sets of parameters: parameters for the LDA topic model (number of topics K, Dirichlet hyperparameters a and Q) are optimized for the LDA-MM method only, and the same LDA model is used in the other two LDA-based methods. Parameters particular to the respective method are optimized individually. For LDA, we tested </context>
</contexts>
<marker>Hamp, Feldweg, 1997</marker>
<rawString>Birgit Hamp and Helmut Feldweg. 1997. GermaNet - a Lexical-Semantic Net for German. In Proceedings of the ACL/EACL-97 workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruqaiya Hasan</author>
</authors>
<title>Coherence and Cohesive Harmony.</title>
<date>1984</date>
<booktitle>Understanding Reading Comprehension, Cognition, Language, and the Structure of Prose,</booktitle>
<pages>181--220</pages>
<editor>In James Flood, editor,</editor>
<location>Newark, Delaware, USA.</location>
<contexts>
<context position="10909" citStr="Hasan, 1984" startWordPosition="1751" endWordPosition="1752"> approaches are evaluated by performing a certain NLP task. Hollingsworth and Teufel (2005) as well as Cramer et al. (2008) conclude from their lexical chain annotation projects that high inter-annotator agreement is very hard to achieve. We argue that directly evaluating on lexical chains should enable us to optimize towards higher-quality chain annotations, which is a task of its own right and which has the potential to improve all subsequent applications. For this, we devise an annotation scheme that gets us reasonable inter-annotator agreement, inspired by the concept of cohesive harmony (Hasan, 1984), and report on an annotation project for German newswire texts. Documents from the SALSA 2.0 (Burchardt et al., 2006) corpus were chosen to form the basis for the annotation of lexical chain information. SALSA is based on the semi-automatically annotated TIGER Treebank 2.1 (Brants et al., 2002). The TIGER treebank provides manual annotations, such as lemmas, part-of-speech tags, and syntactic structure, the SALSA part of the corpus is also partially annotated with FrameNet-style (Baker et al., 1998) frame annotation. The documents are general domain news articles from a German newspaper compr</context>
<context position="12867" citStr="Hasan (1984)" startWordPosition="2066" endWordPosition="2067">ated to every other element in that chain. Terms are considered to be related if they share the same topic, i.e. common sense and knowledge of the language is needed to decide which terms belong together in the same topic and whether a chosen topic is neither too broad nor too narrow. A single dense chain can thus be assigned a definite topical description of its items. Whereas Hollingsworth and Teufel (2005) dealt with the inherent fuzziness of membership of terms to lexical chains by allowing terms to occur in different lexical chains, we follow the concept of cohesive harmony introduced by Hasan (1984) here, where complete chains can be linked to others. For this purpose, we introduce so-called level two links, which are cohesive ties between lexical items in distinct dense chains. Having such a link between two chains, both chains can be assigned a topical description which is broader than the description of the individual chains. This results in a two-level representation of chains. We report on dense lexical chains and merged lexical chains (dense chains are merged into a common chain if a level two link exists between them) separately. In total, 100 documents were annotated by two exper</context>
</contexts>
<marker>Hasan, 1984</marker>
<rawString>Ruqaiya Hasan. 1984. Coherence and Cohesive Harmony. In James Flood, editor, Understanding Reading Comprehension, Cognition, Language, and the Structure of Prose, pages 181–220. International Reading Association, Newark, Delaware, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonhard Hennig</author>
</authors>
<title>Topic-based multi-document summarization with probabilistic latent semantic analysis.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference RANLP-2009,</booktitle>
<pages>144--149</pages>
<location>Borovets, Bulgaria.</location>
<contexts>
<context position="3174" citStr="Hennig, 2009" startWordPosition="478" endWordPosition="479">Morris and Hirst, 1991) as background information in order to resolve possible semantic relations. A major drawback of this strategy is the dependency on the coverage of the resource, which has a direct impact on the lexical chains. Their quality can be expected to be poor for resource-scarce languages or specialized application domains. Statistical methods to modeling language semantics have proven to deliver good results in many natural language processing applications. In particular, probabilistic topic models have been successfully used for tasks such as summarization (Gong and Liu, 2001; Hennig, 2009), text segmentation (Misra et al., 2009), lexical substitution (Dinu and Lapata, 2010) or word sense disambiguation (Cai et al., 2007; Boyd-Graber et al., 2007). 989 Proceedings of NAACL-HLT 2013, pages 989–999, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics In this work, we address the question, whether statistical methods for the extraction of lexical chains can yield better results than existing knowledge-based methods, especially for underresourced languages or domains, following principles of Structure Discovery (Biemann, 2012). To address this, we have</context>
</contexts>
<marker>Hennig, 2009</marker>
<rawString>Leonhard Hennig. 2009. Topic-based multi-document summarization with probabilistic latent semantic analysis. In Proceedings of the International Conference RANLP-2009, pages 144–149, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
<author>David St-Onge</author>
</authors>
<title>Lexical Chains as representation of context for the detection and correction malapropisms.</title>
<date>1998</date>
<booktitle>In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database, Language, Speech, and Communication,</booktitle>
<pages>305--332</pages>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts, USA.</location>
<contexts>
<context position="2547" citStr="Hirst and St-Onge, 1998" startWordPosition="377" endWordPosition="380">lding block in various natural language processing applications, such as word sense disambiguation (Okumura and Honda, 1994), summarization (Barzilay and Elhadad, 1997), malapropism detection and correction (Hirst and StOnge, 1998), document hyperlinking (Green, 1996), text segmentation (Stokes et al., 2004), topic tracking (Carthy, 2004), and others. The performance of the individual task heavily depends on the quality of the identified lexical chains. 1.1 Motivation for Corpus-driven Approach Previous approaches mainly focus on the use of knowledge resources like lexical semantic databases (Hirst and St-Onge, 1998) or thesauri (Morris and Hirst, 1991) as background information in order to resolve possible semantic relations. A major drawback of this strategy is the dependency on the coverage of the resource, which has a direct impact on the lexical chains. Their quality can be expected to be poor for resource-scarce languages or specialized application domains. Statistical methods to modeling language semantics have proven to deliver good results in many natural language processing applications. In particular, probabilistic topic models have been successfully used for tasks such as summarization (Gong a</context>
<context position="4764" citStr="Hirst and St-Onge (1998)" startWordPosition="711" endWordPosition="714">usses issues regarding the inherent subjectivity of lexical chain annotation. In Section 4, three statistical approaches for lexical chaining are developed on the basis of the LDA topic model. Experiments that demonstrate the advantage of these approaches over a knowledge-baseline are conducted and evaluated in Section 5, and Section 6 concludes and provides an outlook future directions. 1.2 Previous Work on Lexical Chains Morris and Hirst (1991) initially proposed an algorithm for lexical chaining based on Roget’s thesaurus (Roget, 1852), and manually assessed the quality of their algorithm. Hirst and St-Onge (1998) first presented a computational approach to lexical chaining based on WordNet showing that the lexical database is a reasonable replacement to Roget’s. The basic idea behind these algorithms is that semantically close words should be connected to form chains. Subsequent approaches mainly concentrated on disambiguation of words to WordNet concepts (WSD), since ambiguous words can lead to the overgeneration of connections. Barzilay and Elhadad (1997) improved the implicit word sense disambiguation (WSD) by keeping a list of different interpretations of the text and finally choosing the most pla</context>
</contexts>
<marker>Hirst, St-Onge, 1998</marker>
<rawString>Graeme Hirst and David St-Onge. 1998. Lexical Chains as representation of context for the detection and correction malapropisms. In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database, Language, Speech, and Communication, pages 305–332. The MIT Press, Cambridge, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic Latent Semantic Analysis.</title>
<date>1999</date>
<booktitle>In Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence, UAI ’99,</booktitle>
<pages>289--296</pages>
<location>Stockholm,</location>
<contexts>
<context position="17695" citStr="Hofmann (1999)" startWordPosition="2894" endWordPosition="2895"> 1232 1826 1224 1251 1239 2044 1119 0174 1650 2014 0630 0199 0655 1818 0310 1150 1726 2035 0382 1639 0275 1047 1707 0228 1050 1607 0984 0755 0752 0747 1162 1018 0182 0942 0492 0675 0928 1399 1072 1310 0268 1033 1168 1523 1737 0524 0338 0976 0639 0299 1524 1387 0694 0278 1692 1534 1037 0499 0721 0283 0736 0258 1130 0988 0649 1777 0309 0110 1319 0729 0730 0390 0925 992 vised algorithms designed for unveiling some hidden structure in large data collections. The key idea is that documents can be represented as composites of so-called topics where a topic itself represents as a composite of words. Hofmann (1999) defined a topic to be a probability distribution over words and a document to be a probability distribution over a fixed set of topics. We use the latent Dirichlet allocation (LDA, Blei et al. (2003)) topic model for estimating the semantic closeness of candidate terms, and explore different ways of utilizing LDA’s topic information in automatic lexical chainers. Specifically, we use the GibbsLDA++5 framework for topic model estimation and inference, and examine the following LDA parameters: number of topics T, Dirichlet hyperparameters for documenttopic distribution α and topic-term distribu</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probabilistic Latent Semantic Analysis. In Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence, UAI ’99, pages 289–296, Stockholm, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Hollingsworth</author>
<author>Simone Teufel</author>
</authors>
<title>Human annotation of lexical chains: Coverage and agreement measures.</title>
<date>2005</date>
<booktitle>In Proceedings of the Workshop ELECTRA: Methodologies and Evaluation of Lexical Cohesion Techniques in Real-world Applications, In Association with SIGIR ’05,</booktitle>
<location>Salvador, Brazil.</location>
<contexts>
<context position="10388" citStr="Hollingsworth and Teufel (2005)" startWordPosition="1667" endWordPosition="1670">s in D and D’, FP: pairs in D’ but not in D, FN: pairs in D but not in D’, TN: pairs not in D and not in D’, where D is the underlying dataset of C, D’ is the underlying dataset of C’, and pairs means all unique combinations of elements that are in the same cluster. 3 Annotating Lexical Chains A challenge with the annotation of lexical chains is the subjective interpretation of the text by individual annotators (Morris and Hirst, 2004), which also substantiates the fact that currently no gold standard exist, and all previous automatic approaches are evaluated by performing a certain NLP task. Hollingsworth and Teufel (2005) as well as Cramer et al. (2008) conclude from their lexical chain annotation projects that high inter-annotator agreement is very hard to achieve. We argue that directly evaluating on lexical chains should enable us to optimize towards higher-quality chain annotations, which is a task of its own right and which has the potential to improve all subsequent applications. For this, we devise an annotation scheme that gets us reasonable inter-annotator agreement, inspired by the concept of cohesive harmony (Hasan, 1984), and report on an annotation project for German newswire texts. Documents from</context>
<context position="12017" citStr="Hollingsworth and Teufel (2005)" startWordPosition="1918" endWordPosition="1921">t-style (Baker et al., 1998) frame annotation. The documents are general domain news articles from a German newspaper comprising about 1,550 documents and around 50,000 sentences in total, with a median document length of 275 tokens. 3.1 Annotation Scheme In order to minimize the subjectiveness of choices by different annotators, annotation guidelines were developed comprising a total of ten pages. We decided to consider only nouns, noun compounds and non-compositional adjective noun phrases like “dirty money” as candidate terms for lexical chaining, which is consistent with the procedures of Hollingsworth and Teufel (2005) and Cramer et al. (2008). For annotation, we used the MMAX24 (M¨uller and Strube, 2006) tool. We introduce the term dense chain, which refers to a type of lexical chain in which every element is 4http://mmax2.sourceforge.net 991 lccm related to every other element in that chain. Terms are considered to be related if they share the same topic, i.e. common sense and knowledge of the language is needed to decide which terms belong together in the same topic and whether a chosen topic is neither too broad nor too narrow. A single dense chain can thus be assigned a definite topical description of </context>
</contexts>
<marker>Hollingsworth, Teufel, 2005</marker>
<rawString>William Hollingsworth and Simone Teufel. 2005. Human annotation of lexical chains: Coverage and agreement measures. In Proceedings of the Workshop ELECTRA: Methodologies and Evaluation of Lexical Cohesion Techniques in Real-world Applications, In Association with SIGIR ’05, Salvador, Brazil.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Hubert</author>
<author>Phipps Arabie</author>
</authors>
<title>Comparing partitions.</title>
<date>1985</date>
<journal>Journal of Classification,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="7442" citStr="Hubert and Arabie (1985)" startWordPosition="1139" endWordPosition="1142">cal chains to this end. 2 Comparing Lexical Chains The comparison of lexical chains is a non-trivial task. We adopt the idea of interpreting lexical chains as clusters and a particular set of lexical chains as a clustering, and develop a suitable cluster comparison measure. As stated by Meil˘a (2005) and Amig´o et al. (2009), a best clustering comparison measure for the general case does not exist. It should be stressed that the appropriate clustering measure highly depends on the task at hand. After exploring a number of measures1, we decided on a combination of the adjusted Rand index (ARI, Hubert and Arabie (1985)) and the basic merge distance (BMD, Menestrina et al. (2010)) for our new measure. Menestrina et al. (2010) introduced a linear time algorithm for computing the generalized merge distance (GMD), which counts 1Explored measures which are unsatisfactory for the given task are: Closest Cluster F1 (Benjelloun et al., 2009), K (Ajmera et al., 2002), Pairwise F1 (Manning et al., 2008), Variation of Information (Meil˘a, 2005), B3(Bagga and Baldwin, 1998), VMeasure (Rosenberg and Hirschberg, 2007), Normalized Mutual Information (Strehl, 2002). The last two measures are equal. A proof of this can be f</context>
</contexts>
<marker>Hubert, Arabie, 1985</marker>
<rawString>Lawrence Hubert and Phipps Arabie. 1985. Comparing partitions. Journal of Classification, 2(1):193–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>An Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. An Introduction to Information Retrieval. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meghana Marathe</author>
<author>Graeme Hirst</author>
</authors>
<title>Lexical Chains Using Distributional Measures of Concept D.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th International Conference on Computational Linguistics and Intelligent Text Processing, CICLing’10,</booktitle>
<pages>291--302</pages>
<location>Ias¸i, Romania.</location>
<contexts>
<context position="6024" citStr="Marathe and Hirst (2010)" startWordPosition="907" endWordPosition="910">nd McCoy (2002) introduced an efficient variant of the algorithm with linear complexity in the number of candidate terms. Galley and McKeown (2003) further improved accuracy by first performing WSD, and then using the remaining links between the disambiguated concepts only. They also introduced a socalled disambiguation graph, a representation that has also been utilized by the method of Medelyan (2007), where she applied a graph clustering algorithm to the disambiguation graph to cut weak links, performing implicit WSD. A combination of statistical and knowledge-based methods is presented by Marathe and Hirst (2010), who combine distributional co-occurrence information with semantic information from a lexicographic resource for extracting lexical chains and evaluate them by text segmentation. We are not aware of previous lexical chaining algorithms that do not rely on a lexicographic resource at all. A major issue in developing a new lexical chaining algorithm is the comparison to previous systems. Most of previous approaches are validated by the evaluation in a certain task like summarization, word sense disambiguation, keyphrase extraction or information retrieval (Stairmand, 1996). Hence, these extrin</context>
</contexts>
<marker>Marathe, Hirst, 2010</marker>
<rawString>Meghana Marathe and Graeme Hirst. 2010. Lexical Chains Using Distributional Measures of Concept D. In Proceedings of the 11th International Conference on Computational Linguistics and Intelligent Text Processing, CICLing’10, pages 291–302, Ias¸i, Romania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olena Medelyan</author>
</authors>
<title>Computing lexical chains with graph clustering.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL: Student Research Workshop, ACL ’07,</booktitle>
<pages>85--90</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5806" citStr="Medelyan (2007)" startWordPosition="875" endWordPosition="876">arzilay and Elhadad (1997) improved the implicit word sense disambiguation (WSD) by keeping a list of different interpretations of the text and finally choosing the most plausible senses for chaining. Silber and McCoy (2002) introduced an efficient variant of the algorithm with linear complexity in the number of candidate terms. Galley and McKeown (2003) further improved accuracy by first performing WSD, and then using the remaining links between the disambiguated concepts only. They also introduced a socalled disambiguation graph, a representation that has also been utilized by the method of Medelyan (2007), where she applied a graph clustering algorithm to the disambiguation graph to cut weak links, performing implicit WSD. A combination of statistical and knowledge-based methods is presented by Marathe and Hirst (2010), who combine distributional co-occurrence information with semantic information from a lexicographic resource for extracting lexical chains and evaluate them by text segmentation. We are not aware of previous lexical chaining algorithms that do not rely on a lexicographic resource at all. A major issue in developing a new lexical chaining algorithm is the comparison to previous </context>
<context position="21471" citStr="Medelyan (2007)" startWordPosition="3540" endWordPosition="3541">y two nodes i, j : i =� j n i, j E 11, 2, ... , Nd}. We test two similarity measures: Euclidian (dis-)similarity and cosine similarity. Let G = (V, E) be the graph representation of a document with term vertices V = 1v1, ... , vNd} and weighted edges E = 1(v1, v2, sim12), ... (vNd, vNd−1, simNdNd−1)}, where simij is either the cosine or Euclidean similarity of term vectors. For simplicity, we reduce this representation to an unweighted graph by only retaining edges (of unit weight) that have a similarity above a parameter threshold csim. To identify chains as clusters in this graph, we follow Medelyan (2007) and apply the Chinese Whispers graph clustering algorithm (CW, Biemann (2006)), which finds the number of clusters automatically. The CW algorithm implementation comes with 993 three parameters to regulate the node weight based on its degree, which influences cluster size and granularity. We test options ”top”, ”dist log” and ”dist lin”. The final chaining procedure is straightforward: The LDA-GM algorithm assigns every candidate lexical item wi of a certain document d which is assigned the same class label ci to the same chain. Level two links are drawn using the second dominant class of a v</context>
</contexts>
<marker>Medelyan, 2007</marker>
<rawString>Olena Medelyan. 2007. Computing lexical chains with graph clustering. In Proceedings of the 45th Annual Meeting of the ACL: Student Research Workshop, ACL ’07, pages 85–90, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marina Meil˘a</author>
</authors>
<title>Comparing clusterings: an axiomatic view.</title>
<date>2005</date>
<booktitle>In Proceedings of the 22nd International Conference on Machine Learning, ICML ’05,</booktitle>
<pages>577--584</pages>
<location>Bonn, Germany.</location>
<marker>Meil˘a, 2005</marker>
<rawString>Marina Meil˘a. 2005. Comparing clusterings: an axiomatic view. In Proceedings of the 22nd International Conference on Machine Learning, ICML ’05, pages 577–584, Bonn, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Menestrina</author>
<author>Steven Euijong Whang</author>
<author>Hector Garcia-Molina</author>
</authors>
<title>Evaluating entity resolution results.</title>
<date>2010</date>
<booktitle>Proceedings of the VLDB Endowment,</booktitle>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="7503" citStr="Menestrina et al. (2010)" startWordPosition="1149" endWordPosition="1152">ison of lexical chains is a non-trivial task. We adopt the idea of interpreting lexical chains as clusters and a particular set of lexical chains as a clustering, and develop a suitable cluster comparison measure. As stated by Meil˘a (2005) and Amig´o et al. (2009), a best clustering comparison measure for the general case does not exist. It should be stressed that the appropriate clustering measure highly depends on the task at hand. After exploring a number of measures1, we decided on a combination of the adjusted Rand index (ARI, Hubert and Arabie (1985)) and the basic merge distance (BMD, Menestrina et al. (2010)) for our new measure. Menestrina et al. (2010) introduced a linear time algorithm for computing the generalized merge distance (GMD), which counts 1Explored measures which are unsatisfactory for the given task are: Closest Cluster F1 (Benjelloun et al., 2009), K (Ajmera et al., 2002), Pairwise F1 (Manning et al., 2008), Variation of Information (Meil˘a, 2005), B3(Bagga and Baldwin, 1998), VMeasure (Rosenberg and Hirschberg, 2007), Normalized Mutual Information (Strehl, 2002). The last two measures are equal. A proof of this can be found in the appendix. 990 split and merge cluster editing ope</context>
</contexts>
<marker>Menestrina, Whang, Garcia-Molina, 2010</marker>
<rawString>David Menestrina, Steven Euijong Whang, and Hector Garcia-Molina. 2010. Evaluating entity resolution results. Proceedings of the VLDB Endowment, 3(1):208–219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hemant Misra</author>
<author>Franc¸ois Yvon</author>
<author>Joemon Jose</author>
<author>Olivier Capp´e</author>
</authors>
<title>Text Segmentation via Topic Modeling: An Analytical Study.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM</booktitle>
<pages>1553--1556</pages>
<location>Hong Kong, China.</location>
<marker>Misra, Yvon, Jose, Capp´e, 2009</marker>
<rawString>Hemant Misra, Franc¸ois Yvon, Joemon Jose, and Olivier Capp´e. 2009. Text Segmentation via Topic Modeling: An Analytical Study. In Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM 2009, pages 1553–1556, Hong Kong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
<author>Graeme Hirst</author>
</authors>
<title>Lexical cohesion computed by thesaural relations as an indicator of the structure of text.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<pages>48</pages>
<contexts>
<context position="2584" citStr="Morris and Hirst, 1991" startWordPosition="383" endWordPosition="386">e processing applications, such as word sense disambiguation (Okumura and Honda, 1994), summarization (Barzilay and Elhadad, 1997), malapropism detection and correction (Hirst and StOnge, 1998), document hyperlinking (Green, 1996), text segmentation (Stokes et al., 2004), topic tracking (Carthy, 2004), and others. The performance of the individual task heavily depends on the quality of the identified lexical chains. 1.1 Motivation for Corpus-driven Approach Previous approaches mainly focus on the use of knowledge resources like lexical semantic databases (Hirst and St-Onge, 1998) or thesauri (Morris and Hirst, 1991) as background information in order to resolve possible semantic relations. A major drawback of this strategy is the dependency on the coverage of the resource, which has a direct impact on the lexical chains. Their quality can be expected to be poor for resource-scarce languages or specialized application domains. Statistical methods to modeling language semantics have proven to deliver good results in many natural language processing applications. In particular, probabilistic topic models have been successfully used for tasks such as summarization (Gong and Liu, 2001; Hennig, 2009), text seg</context>
<context position="4590" citStr="Morris and Hirst (1991)" startWordPosition="684" endWordPosition="687">ents. After defining a measure for the comparison of (manually or automatically created) lexical chains in Section 2, Section 3 describes our annotation methodology and discusses issues regarding the inherent subjectivity of lexical chain annotation. In Section 4, three statistical approaches for lexical chaining are developed on the basis of the LDA topic model. Experiments that demonstrate the advantage of these approaches over a knowledge-baseline are conducted and evaluated in Section 5, and Section 6 concludes and provides an outlook future directions. 1.2 Previous Work on Lexical Chains Morris and Hirst (1991) initially proposed an algorithm for lexical chaining based on Roget’s thesaurus (Roget, 1852), and manually assessed the quality of their algorithm. Hirst and St-Onge (1998) first presented a computational approach to lexical chaining based on WordNet showing that the lexical database is a reasonable replacement to Roget’s. The basic idea behind these algorithms is that semantically close words should be connected to form chains. Subsequent approaches mainly concentrated on disambiguation of words to WordNet concepts (WSD), since ambiguous words can lead to the overgeneration of connections. </context>
</contexts>
<marker>Morris, Hirst, 1991</marker>
<rawString>Jane Morris and Graeme Hirst. 1991. Lexical cohesion computed by thesaural relations as an indicator of the structure of text. Computational Linguistics, 17:21– 48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
<author>Graeme Hirst</author>
</authors>
<title>The Subjectivity of Lexical Cohesion in Text.</title>
<date>2004</date>
<booktitle>In Proceedings of the AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications,</booktitle>
<location>Palo Alto, California, USA.</location>
<contexts>
<context position="10196" citStr="Morris and Hirst, 2004" startWordPosition="1638" endWordPosition="1641">es that not a single pair of items in C is found in a cluster together in C&apos;. lccm(C, C&apos;) = 1 [1 − NBMD(C, C&apos;) + ARI(C, C&apos;)] . 2 2BMD(T,1) for |D |&lt; 2, BMD(T,1) + 1 otherwise 3TP: pairs in D and D’, FP: pairs in D’ but not in D, FN: pairs in D but not in D’, TN: pairs not in D and not in D’, where D is the underlying dataset of C, D’ is the underlying dataset of C’, and pairs means all unique combinations of elements that are in the same cluster. 3 Annotating Lexical Chains A challenge with the annotation of lexical chains is the subjective interpretation of the text by individual annotators (Morris and Hirst, 2004), which also substantiates the fact that currently no gold standard exist, and all previous automatic approaches are evaluated by performing a certain NLP task. Hollingsworth and Teufel (2005) as well as Cramer et al. (2008) conclude from their lexical chain annotation projects that high inter-annotator agreement is very hard to achieve. We argue that directly evaluating on lexical chains should enable us to optimize towards higher-quality chain annotations, which is a task of its own right and which has the potential to improve all subsequent applications. For this, we devise an annotation sc</context>
<context position="14209" citStr="Morris and Hirst, 2004" startWordPosition="2285" endWordPosition="2288"> of Table 3 show the characteristics of the annotated data set. It can be concluded that there is a moderate to high agreement regarding the annotator selections of candidate terms, which is ensured by preselection of candidate terms by part-of-speech patterns. A value of 81% in the average agreement on lexical items (cf. Figure 1) shows that even though the choice of lexical items is limited to nouns and adjective noun phrases only, the decision on candidate termhood is somewhat different between the annotators, but compares favorably with previous findings of 63% average pairwise agreement (Morris and Hirst, 2004). Figure 2 shows the annotator agreement on the individual documents using the lccm (cf. Sec. 2), sorted in the same way as in Figure 1. In order to use the level two link information the figure also shows a second agreement score, which was computed on merged chains. The agreement scores of the assignment of lexical items to lexical chains depend partially on the agreement scores of the identified lexical items them100 % 20 0 0600 Document ID Figure 1: Agreement of lexical items annotated by annotator A and annotator B as a percentage of lexical items annotated by annotator A or annotator B. </context>
</contexts>
<marker>Morris, Hirst, 2004</marker>
<rawString>Jane Morris and Graeme Hirst. 2004. The Subjectivity of Lexical Cohesion in Text. In Proceedings of the AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications, Palo Alto, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph M¨uller</author>
<author>Michael Strube</author>
</authors>
<title>Multi-level annotation of linguistic data with MMAX2.</title>
<date>2006</date>
<booktitle>Corpus Technology and Language Pedagogy: New Resources,</booktitle>
<pages>197--214</pages>
<editor>In Sabine Braun, Kurt Kohn, and Joybrato Mukherjee, editors,</editor>
<location>New Tools, New Methods,</location>
<marker>M¨uller, Strube, 2006</marker>
<rawString>Christoph M¨uller and Michael Strube. 2006. Multi-level annotation of linguistic data with MMAX2. In Sabine Braun, Kurt Kohn, and Joybrato Mukherjee, editors, Corpus Technology and Language Pedagogy: New Resources, New Tools, New Methods, pages 197–214. Peter Lang, Frankfurt a.M., Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manabu Okumura</author>
<author>Takeo Honda</author>
</authors>
<title>Word sense disambiguation and text segmentation based on lexical cohesion.</title>
<date>1994</date>
<booktitle>In COLING ’94: Proceedings of the 15th Conference on Computational Linguistics,</booktitle>
<volume>2</volume>
<pages>755--761</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="2047" citStr="Okumura and Honda, 1994" startWordPosition="307" endWordPosition="310">hile the former is concerned with the meaning of a text, the latter can be seen as a collection of devices for creating it. Cohesion and coherence build the basis for most of the current natural language processing problems that deal with text understanding. Lexical cohesion ties together words or phrases that are semantically related. Once all the cohesive ties are identified the involved items can be grouped together to form so-called lexical chains, which form a theoretically well-founded building block in various natural language processing applications, such as word sense disambiguation (Okumura and Honda, 1994), summarization (Barzilay and Elhadad, 1997), malapropism detection and correction (Hirst and StOnge, 1998), document hyperlinking (Green, 1996), text segmentation (Stokes et al., 2004), topic tracking (Carthy, 2004), and others. The performance of the individual task heavily depends on the quality of the identified lexical chains. 1.1 Motivation for Corpus-driven Approach Previous approaches mainly focus on the use of knowledge resources like lexical semantic databases (Hirst and St-Onge, 1998) or thesauri (Morris and Hirst, 1991) as background information in order to resolve possible semanti</context>
</contexts>
<marker>Okumura, Honda, 1994</marker>
<rawString>Manabu Okumura and Takeo Honda. 1994. Word sense disambiguation and text segmentation based on lexical cohesion. In COLING ’94: Proceedings of the 15th Conference on Computational Linguistics, volume 2, pages 755–761, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Riedl</author>
<author>Chris Biemann</author>
</authors>
<title>Sweeping through the Topic Space: Bad luck? Roll again!</title>
<date>2012</date>
<booktitle>In ROBUS-UNSUP 2012: Joint Workshop on Unsupervised and Semi-Supervised Learning in NLP held in conjunction with EACL 2012,</booktitle>
<pages>pages</pages>
<location>Avignon, France.</location>
<contexts>
<context position="18644" citStr="Riedl and Biemann, 2012" startWordPosition="3047" endWordPosition="3050">nformation in automatic lexical chainers. Specifically, we use the GibbsLDA++5 framework for topic model estimation and inference, and examine the following LDA parameters: number of topics T, Dirichlet hyperparameters for documenttopic distribution α and topic-term distribution Q. We now describe three LDA-based approaches to lexical chaining. 4.1 LDA Mode Method (LDA-MM) The LDA-MM approach places all word tokens that share the same topic ID into the same chain. The point is now how to decide to which topic a word belongs to. Since single samples of topics per word exhibit a large variance (Riedl and Biemann, 2012), we follow these authors by sampling several times and using the mode (most frequently assigned) topic ID per word as the topic assignment. This strategy reduced the variance in the lccm to a tenth6. More formally, let samples(d,w) be the vector of assignments that have been collected for a certain word w in a certain document d with each samples(d,w) i referring to the i-th sampled topic ID for (d, w). In other words, samples(d,w) can be seen as the Markov chain for a particular word in a particular document. Further let z(d,w) be the topic ID that was most assigned to the word w with respec</context>
</contexts>
<marker>Riedl, Biemann, 2012</marker>
<rawString>Martin Riedl and Chris Biemann. 2012. Sweeping through the Topic Space: Bad luck? Roll again! In ROBUS-UNSUP 2012: Joint Workshop on Unsupervised and Semi-Supervised Learning in NLP held in conjunction with EACL 2012, pages 19–27, Avignon, France.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Peter Mark</author>
</authors>
<title>Roget. 1852. Roget’s Thesaurus of English Words and Phrases.</title>
<publisher>Longman Group Ltd.,</publisher>
<location>Harlow, UK.</location>
<marker>Mark, </marker>
<rawString>Peter Mark Roget. 1852. Roget’s Thesaurus of English Words and Phrases. Longman Group Ltd., Harlow, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Rosenberg</author>
<author>Julia Hirschberg</author>
</authors>
<title>Vmeasure: A conditional entropy-based external cluster evaluation measure.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>410--420</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="7937" citStr="Rosenberg and Hirschberg, 2007" startWordPosition="1215" endWordPosition="1218">sk at hand. After exploring a number of measures1, we decided on a combination of the adjusted Rand index (ARI, Hubert and Arabie (1985)) and the basic merge distance (BMD, Menestrina et al. (2010)) for our new measure. Menestrina et al. (2010) introduced a linear time algorithm for computing the generalized merge distance (GMD), which counts 1Explored measures which are unsatisfactory for the given task are: Closest Cluster F1 (Benjelloun et al., 2009), K (Ajmera et al., 2002), Pairwise F1 (Manning et al., 2008), Variation of Information (Meil˘a, 2005), B3(Bagga and Baldwin, 1998), VMeasure (Rosenberg and Hirschberg, 2007), Normalized Mutual Information (Strehl, 2002). The last two measures are equal. A proof of this can be found in the appendix. 990 split and merge cluster editing operations. Using a constant factor of 1 for both splits and merges gives the basic merge distance (BMD): Considering T as the most general clustering of a dataset D, where all elements are grouped into the same cluster, and further considering 1 as the most specific clustering of D, where each element builds its own cluster, the lattice between T and 1 spans all possible clusterings and the BMD can be interpreted as the shortest pat</context>
</contexts>
<marker>Rosenberg, Hirschberg, 2007</marker>
<rawString>Andrew Rosenberg and Julia Hirschberg. 2007. Vmeasure: A conditional entropy-based external cluster evaluation measure. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 410–420, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Gregory Silber</author>
<author>Kathleen F McCoy</author>
</authors>
<title>Efficiently computed lexical chains as an intermediate representation for automatic text summarization.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="5415" citStr="Silber and McCoy (2002)" startWordPosition="811" endWordPosition="814">onal approach to lexical chaining based on WordNet showing that the lexical database is a reasonable replacement to Roget’s. The basic idea behind these algorithms is that semantically close words should be connected to form chains. Subsequent approaches mainly concentrated on disambiguation of words to WordNet concepts (WSD), since ambiguous words can lead to the overgeneration of connections. Barzilay and Elhadad (1997) improved the implicit word sense disambiguation (WSD) by keeping a list of different interpretations of the text and finally choosing the most plausible senses for chaining. Silber and McCoy (2002) introduced an efficient variant of the algorithm with linear complexity in the number of candidate terms. Galley and McKeown (2003) further improved accuracy by first performing WSD, and then using the remaining links between the disambiguated concepts only. They also introduced a socalled disambiguation graph, a representation that has also been utilized by the method of Medelyan (2007), where she applied a graph clustering algorithm to the disambiguation graph to cut weak links, performing implicit WSD. A combination of statistical and knowledge-based methods is presented by Marathe and Hir</context>
<context position="25594" citStr="Silber and McCoy (2002)" startWordPosition="4251" endWordPosition="4254">ell as known stopwords, and words that occur in less than two documents which results in a vocabulary size of about 100K words. 5.1 Experimental Setup For comparison, we implemented three baselines, which we describe below. One baseline is trivial, two baselines are state-of-the art knowledge-based systems adapted to German. Random: Candidate lexical items are randomly tied together to form sets of lexical chains. Level two links are created analogously. We regulate the process to yield the same average number of chains and links as in the development and test data. S&amp;M GermaNet: Algorithm by Silber and McCoy (2002) with GermaNet as its knowledge resource. 7as provided by Projekt Deutscher Wortschatz, http://wortschatz.uni-leipzig.de/ 994 G&amp;M GermaNet: Algorithm by Galley and McKeown (2003), also using GermaNet. GermaNet (Hamp and Feldweg, 1997) is a large WordNet-like resource for German, containing almost 100,000 lexical units and over 87,000 conceptual relations between synsets. While its size is only about half of WordNet, it is one of the largest nonEnglish lexical semantic resources. 5.2 Model Selection We optimize two sets of parameters: parameters for the LDA topic model (number of topics K, Diri</context>
<context position="32027" citStr="Silber and McCoy (2002)" startWordPosition="5286" endWordPosition="5289">e the level two links produced by the various methods. Data Analysis Table 3 shows quantitative numbers of the extracted lexical chains in the test set. The LDA-MM approach chains and links a lot more items than the other statistical methods: it creates a lot more links between items that would otherwise be removed because they form unlinked singleton chains. As opposed to this, the graph method (LDA-GM), as well as the top-n method (LDA-TM) perform an implicit filtering on the candidate lexical items by creating less level two links, yet larger dense chains. The knowledge based algorithms by Silber and McCoy (2002) and Galley and McKeown (2003) extract fewer and smaller chains than the statistical approaches, which reflects GermaNet’s sparsity issues. While higher lexical coverage in the underlying resource would increase the coverage of our knowledge-based systems, this is only one part of the story. The other part is rooted in the fact that lexical cohesion relations, which are used in lexical chains, encompass many more semantic relations than listed in today’s lexical semantic networks. This especially holds for cases where several expressions refer to the same event or theme for which no well-defin</context>
</contexts>
<marker>Silber, McCoy, 2002</marker>
<rawString>H. Gregory Silber and Kathleen F. McCoy. 2002. Efficiently computed lexical chains as an intermediate representation for automatic text summarization. Computational Linguistics, 28(4):487–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark A Stairmand</author>
</authors>
<title>A Computational Analysis of Lexical Cohesion with Applications in Information Retrieval.</title>
<date>1996</date>
<tech>Ph.D. thesis,</tech>
<institution>Center for Computational Linguistics,</institution>
<location>UMIST, Manchester.</location>
<contexts>
<context position="6603" citStr="Stairmand, 1996" startWordPosition="998" endWordPosition="999">presented by Marathe and Hirst (2010), who combine distributional co-occurrence information with semantic information from a lexicographic resource for extracting lexical chains and evaluate them by text segmentation. We are not aware of previous lexical chaining algorithms that do not rely on a lexicographic resource at all. A major issue in developing a new lexical chaining algorithm is the comparison to previous systems. Most of previous approaches are validated by the evaluation in a certain task like summarization, word sense disambiguation, keyphrase extraction or information retrieval (Stairmand, 1996). Hence, these extrinsic evaluations are heavily influenced by the particular task at hand. We propose to re-consider lexical chaining as a task on its own, and propose objective criteria for directly comparing lexical chains to this end. 2 Comparing Lexical Chains The comparison of lexical chains is a non-trivial task. We adopt the idea of interpreting lexical chains as clusters and a particular set of lexical chains as a clustering, and develop a suitable cluster comparison measure. As stated by Meil˘a (2005) and Amig´o et al. (2009), a best clustering comparison measure for the general case</context>
</contexts>
<marker>Stairmand, 1996</marker>
<rawString>Mark A. Stairmand. 1996. A Computational Analysis of Lexical Cohesion with Applications in Information Retrieval. Ph.D. thesis, Center for Computational Linguistics, UMIST, Manchester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Stokes</author>
<author>Joe Carthy</author>
<author>Alan F Smeaton</author>
</authors>
<title>SeLeCT: A Lexical Cohesion Based News Story Segmentation System.</title>
<date>2004</date>
<journal>AI Communications,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="2232" citStr="Stokes et al., 2004" startWordPosition="331" endWordPosition="334">tural language processing problems that deal with text understanding. Lexical cohesion ties together words or phrases that are semantically related. Once all the cohesive ties are identified the involved items can be grouped together to form so-called lexical chains, which form a theoretically well-founded building block in various natural language processing applications, such as word sense disambiguation (Okumura and Honda, 1994), summarization (Barzilay and Elhadad, 1997), malapropism detection and correction (Hirst and StOnge, 1998), document hyperlinking (Green, 1996), text segmentation (Stokes et al., 2004), topic tracking (Carthy, 2004), and others. The performance of the individual task heavily depends on the quality of the identified lexical chains. 1.1 Motivation for Corpus-driven Approach Previous approaches mainly focus on the use of knowledge resources like lexical semantic databases (Hirst and St-Onge, 1998) or thesauri (Morris and Hirst, 1991) as background information in order to resolve possible semantic relations. A major drawback of this strategy is the dependency on the coverage of the resource, which has a direct impact on the lexical chains. Their quality can be expected to be po</context>
</contexts>
<marker>Stokes, Carthy, Smeaton, 2004</marker>
<rawString>Nicola Stokes, Joe Carthy, and Alan F. Smeaton. 2004. SeLeCT: A Lexical Cohesion Based News Story Segmentation System. AI Communications, 17(1):3–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Strehl</author>
</authors>
<title>Relationship-based Clustering and Cluster Ensembles for High-dimensional Data Mining.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Texas,</institution>
<location>Austin.</location>
<contexts>
<context position="7983" citStr="Strehl, 2002" startWordPosition="1223" endWordPosition="1224"> a combination of the adjusted Rand index (ARI, Hubert and Arabie (1985)) and the basic merge distance (BMD, Menestrina et al. (2010)) for our new measure. Menestrina et al. (2010) introduced a linear time algorithm for computing the generalized merge distance (GMD), which counts 1Explored measures which are unsatisfactory for the given task are: Closest Cluster F1 (Benjelloun et al., 2009), K (Ajmera et al., 2002), Pairwise F1 (Manning et al., 2008), Variation of Information (Meil˘a, 2005), B3(Bagga and Baldwin, 1998), VMeasure (Rosenberg and Hirschberg, 2007), Normalized Mutual Information (Strehl, 2002). The last two measures are equal. A proof of this can be found in the appendix. 990 split and merge cluster editing operations. Using a constant factor of 1 for both splits and merges gives the basic merge distance (BMD): Considering T as the most general clustering of a dataset D, where all elements are grouped into the same cluster, and further considering 1 as the most specific clustering of D, where each element builds its own cluster, the lattice between T and 1 spans all possible clusterings and the BMD can be interpreted as the shortest path from a clustering C to a clustering C&apos; in th</context>
</contexts>
<marker>Strehl, 2002</marker>
<rawString>Alexander Strehl. 2002. Relationship-based Clustering and Cluster Ensembles for High-dimensional Data Mining. Ph.D. thesis, University of Texas, Austin.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>