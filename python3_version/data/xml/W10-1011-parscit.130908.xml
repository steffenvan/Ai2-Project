<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001426">
<title confidence="0.9996245">
A Human-Computer Collaboration Approach to Improve Accuracy of
an Automated English Scoring System
</title>
<author confidence="0.998689">
Jee Eun Kim Kong Joo Lee
</author>
<affiliation confidence="0.996308">
Hankuk University of Foreign Studies Chungnam National University
</affiliation>
<address confidence="0.666194">
Seoul, Korea Daejeon, Korea
</address>
<email confidence="0.986909">
jeeeunk@hufs.ac.kr kjoolee@cnu.ac.kr
</email>
<sectionHeader confidence="0.994765" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999898571428571">
This paper explores an issue of redundant errors
reported while automatically scoring English
learners’ sentences. We use a human-computer
collaboration approach to eliminate redundant er-
rors. The first step is to automatically select can-
didate redundant errors using PMI and RFC.
Since those errors are detected with different IDs
although they represent the same error, the can-
didacy cannot be confirmed automatically. The
errors are then handed over to human experts to
determine the candidacy. The final candidates
are provided to the system and trained with a de-
cision tree. With those redundant errors eliminat-
ed, the system accuracy has been improved.
</bodyText>
<sectionHeader confidence="0.998123" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.986017193548387">
An automated English scoring system analyzes a
student sentence and provides a score and feedback
to students. The performance of a system is eva-
luated based on the accuracy of the score and the
relevance of the feedback.
The system described in this paper scores Eng-
lish sentences composed by Korean students learn-
ing English. A detailed explanation of the system
is given in (Kim et al., 2007). The scores are calcu-
lated from three different phases including word,
syntax and mapping, each of which is designed to
assign 0~2 points. Three scores are added up to
generate the final score. A spelling error, a plural
form error, and a confusable word error are consi-
dered as typical word errors. A subject verb
agreement error, a word order error and relative
clause error are typical examples of syntactic er-
rors. Even when a student sentence is perfectly
correct in lexical and syntactic level, it may fail to
convey what is meant by the question. Such sen-
tences are evaluated as grammatical, but cannot be
a correct answer for the question. In this case, the
80
errors can only be recognized by comparing a stu-
dent sentence with its correct answers. The differ-
ences between a student answer and one of the
answers can be considered as mapping errors.
These three phases are independent from one
another since they use different processing method,
and refer different information. Independency of
three phases causes some problems.
</bodyText>
<note confidence="0.95349925">
(Ex1) Correct answer: The earth is bigger than the moon.
Student answer: The earth is small than the Moon.
Err1: MODIFIER_COMP_ERR|4-7 |syntactic
Err2: LEXICAL_ERROR|4 |mapping
</note>
<bodyText confidence="0.998457428571428">
(Ex1) is an example of error reports provided
to a student. The following two lines in (Ex1) show
the error information detected from the student
answer by the system. Err1 in (Ex1) reports a com-
parative form error of an adjective ‘small’, which
covers the 4 ~ 7th words of the student sentence.
Err2 indicates that the 4th word ‘small’ of the stu-
dent sentence is different from that of the answer
sentence. The difference was identified by compar-
ing the student sentence and the answer sentence.
Err1 was detected at the syntactic phase whereas
Err2 was at the mapping phase. These two errors
points to the same word, but have been reported as
different errors.
</bodyText>
<note confidence="0.9781">
(Ex2) Correct answer: She is too weak to carry the bag. bag.
Student answer: She is too weak to carry the her
Err1: EXTRA_DET_ERR|7-9 |syntactic
Err2: UNNECESSARY_NODE_ERR|8|(her) mapping
</note>
<bodyText confidence="0.9933371">
Similarly, Err1 in (Ex2) reports an incorrect
use of an article at the 7~9th words. The syntactic
analysis recognizes that ‘the’ and ‘her’ cannot oc-
cur consecutively, but it is not capable of deter-
mine which one to eliminate. Err2, on the other
hand, pinpoints ‘her’ as an incorrectly used word
by comparing the student sentence and the answer
sentence.
(Ex1) and (Ex2) have presented the errors
which are detected at different processing phases,
</bodyText>
<note confidence="0.878587">
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 80–83,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999968222222222">
but represent the same error. Since these redundant
errors are a hindering factor to calculate accurate
scores, one of the errors has to be removed. The
proposed system deals with 70 error types; 16 for
word, 46 for syntax, and 14 for mapping. In this
paper, we have adopted a human-computer colla-
boration approach by which linguistic experts as-
sist the system to decide which one of the
redundant errors should be removed.
</bodyText>
<sectionHeader confidence="0.968414" genericHeader="method">
2 Redundant Errors
</sectionHeader>
<bodyText confidence="0.99498">
The system-detected errors are reported in the fol-
lowing format:
</bodyText>
<subsectionHeader confidence="0.665689">
Error_ ID  |Error_ Position  |Error_Correction_Info
</subsectionHeader>
<bodyText confidence="0.9997766">
Each error report is composed of three fields which
are separated by ‘|’. The first field contains error
identification. The second includes the numbers
indicating where the error is detected in a student
input sentence. For example, if the field has num-
ber “5-7”, it can be interpreted as the input sen-
tence has an error covering from the 5th word to 7th
word. Since syntactic errors are usually detected at
a phrasal level, the position of an error covers more
than one word. The third field may or may not be
filled with a value, depending on the type of an
error. When it has a value, it is mostly a suggestion,
i.e. a corrected string which is formed by compar-
ing a student sentence with its corresponding cor-
rect answer.
</bodyText>
<subsectionHeader confidence="0.999176">
2.1 Definition of Redundant Errors
</subsectionHeader>
<bodyText confidence="0.999512384615385">
(Condition 1) The errors should share an error
position.
(Condition 2) The errors should be detected
from different error process phases.
(Condition 3) The errors should represent lin-
guistically the same phenomenon.
(Condition 1) implies that the two errors must
deal with one or more common words. The posi-
tion is indicated on the student sentence. However,
there are some exceptions in displaying the posi-
tion. An example of the exception is ‘OBLI-
GATORY_NODE_MISSING_ERR’ and ‘OP-
TIONAL NODE MISSING ERR’ which are
</bodyText>
<equation confidence="0.592293">
_ _ _
</equation>
<bodyText confidence="0.9938828">
mapping errors. Since these errors are detected
when a certain word is missing from a student in-
put but included in the answer, the position is indi-
cated on the answer sentence. Err5 and Err6 from
(Ex3) represent the case. Error position ‘(7)’ and
‘(8)’ 1 means that the 7th and 8th word of the an-
swer sentence, ‘to’ and ‘our’ are missing, respec-
tively. When an error position points to an answer
sentence not a student sentence, the error cannot be
checked with whether it includes the words shared
with the errors whose positions indicate the student
sentence. In this case, the error is assumed to have
shared words with all the other errors; Err5 and
Err6 are considered containing shared words with
Err 1~4 in (Ex3).
</bodyText>
<table confidence="0.679119777777778">
(Ex3) last week.
Correct answer: She is a teacher who came to our school week.
Student answer: She is a teacher who come school last
Err1: CONFUSABLE WORD ERR|9|week word
Err2: SUBJ_VERB_AGR_ERR|3-7 |syntactic
Err3: VERB_SUBCAT_ERR|6-7 |syntactic
Err4: TENSE_UNMATCHED_ERR|6|came[past] mapping
Err5: OPTIONAL NODE MISSING ERR|(7)|to mapping
Err6: OPTIONAL_NODE_MISSING_ERR|(8)|our mapping
</table>
<bodyText confidence="0.997466611111111">
Err1 and Err2 from (Ex3) cannot be redundant
errors since they do not share an error position and
accordingly do not satisfy Condition 1. Err2 and
Err3 share error positions 6~7, but they are not also
considered as redundant errors since both of them
were detected at the same process phase, the syn-
tactic phase. Err2 and Err4 satisfy both Condition 1
and 2, but fail to meet Condition 3. Err2 represents
the subject-predicate agreement error whereas Err4
points out a tense error. In comparison, Err3 and
Err5 are legitimate candidates of “redundant er-
rors” since they satisfy all the conditions. They
share error positions, but were detected from dif-
ferent error process phases, the syntactic phase and
the mapping phase, respectively. They also deal
with the same linguistic phenomenon that a verb
“come” does not have a transitive sense but re-
quires a prepositional phrase led by “to”.
</bodyText>
<subsectionHeader confidence="0.999641">
2.2 Detection of Redundant Errors
</subsectionHeader>
<bodyText confidence="0.969687363636364">
Two errors need to satisfy all the conditions men-
tioned in section 2.1 in order to be classified as
redundant errors. The system’s detecting process
began with scoring 14,892 student answers. From
the scoring result, the candidates which met Condi-
tion 1 and 2 were selected. In the following subsec-
tions, we have described how to determine the
final redundant errors using the system in collabo-
ration with human’s efforts.
1 Error positions in answer sentences are marked with a num-
ber surrounded by a pair of parenthesis.
</bodyText>
<page confidence="0.619289">
81
</page>
<subsectionHeader confidence="0.79475">
2.2.1 Selection of the Candidates
</subsectionHeader>
<bodyText confidence="0.96100175">
The system selected candidate errors which satis-
fied Condition 1 and 2 among the student sen-
tences. For example, Table 1 presents 8 candidates
extracted from (Ex3).
</bodyText>
<table confidence="0.9862905">
1 CONFUSABLE_WORD_ERR|9|week
OPTIONAL_NODE_MISSING_ERR|(7)|to
2 CONFUSABLE_WORD_ERR|9|week
OPTIONAL_NODE_MISSING_ERR|(8)|our
3 SUBJ_VERB_AGR_ERR|3-7|
TENSE_UNMATCHED_ERR|6|came[past]
4 SUBJ_VERB_AGR_ERR|3-7|
OPTIONAL NODE MISSING ERR|(7)|to
5 SUBJ_VERB_AGR_ERR|3-7|
OPTIONAL NODE MISSING ERR|(8)|our
6 VERB_SUBCAT_ERR|6-7|
TENSE_UNMATCHED_ERR|6|came[past]
7 VERB_SUBCAT_ERR|6-7|
OPTIONAL_NODE_MISSING_ERR|(7)|to
8 VERB_SUBCAT_ERR|6-7|
OPTIONAL_NODE_MISSING_ERR|(8)|our
</table>
<tableCaption confidence="0.998957">
Table 1 Candidate pairs of errors extracted from (Ex3).
</tableCaption>
<bodyText confidence="0.999934333333333">
As a result of the selection process, the total of
150,419 candidate pairs was selected from 14,892
scoring results of the student sentences.
</bodyText>
<subsectionHeader confidence="0.838291">
2.2.2 Filtering Candidate Errors
</subsectionHeader>
<bodyText confidence="0.9956881">
The candidates extracted through the process men-
tioned in 2.2.1 were classified based on their error
identifications only, without considering error po-
sition and error correction information. 150,419
pairs of the errors were assorted into 657 types.
The frequency of each type of the candidates was
then calculated. These candidate errors were fil-
tered by applying PMI (Pointwise Mutual Informa-
tion) and RFC (Relative Frequency Count) (Su et
al., 1994).
</bodyText>
<equation confidence="0.994168">
PMI(E1, E2) =log P(E1, E2)
P(E1)P(E2)
1 2 = (2)
freq E E
( , )
1 2
RFC E E
( , )
freq
</equation>
<bodyText confidence="0.996003">
PMI is represented by a number indicating how
frequently two errors E1 and E2 occur simulta-
neously. RFC refers to relative frequency against
average frequency of the total candidates. The fil-
tering equation is as follows:
</bodyText>
<equation confidence="0.990386">
PMI(E1, E2) x RFC(E1, E2) &gt;_ k (3)
</equation>
<bodyText confidence="0.999879">
Using this equation, the system filtered the candi-
dates whose value was above the threshold k. For
this experiment, 0.4 was assigned to k and 111 er-
ror types were selected.
</bodyText>
<subsectionHeader confidence="0.655716">
2.2.3 Human Collaborated Filtering
</subsectionHeader>
<bodyText confidence="0.999898571428571">
Filtered 111 error types include 29,588 candidate
errors; on the average 278 errors per type. These
errors were then handed over to human experts2 to
confirm their candidacy. They checked Condition
3 against each candidate. The manually filtered
result was categorized into three classes as shown
in Table 2.
</bodyText>
<table confidence="0.998824538461539">
Class A: (DET_NOUN_CV_ERR,DET_UNMATCHED_ERR)
(number: (EXTRA_DET_ERR, DET_UNMATCHED_ERR)
20) (MODIFIER_COMP_ERR, FORM_UNMATCHED_ERR)
(MISSPELLING_ERR, LEXICAL_ERR)
...
Class B: (SUBJ_VERB_AGR_ERR,TENSE_UNMATCHED_ERR)
(number: (AUX_MISSING_ERR, UNNECESSARY_NODE_ERR)
47) (CONJ_MISSING_ERR, DET_UNMATCHED_ERR)
...
Class C: (VERBFORM_ERR, ASPECT_UNMATCHED_ERR)
(number: (VERB__ING_FORM_ERR, TENSE_UNMATCHED_ERR)
44) (EXTRA_PREP_ERR, UNNECESSARY_NODE_ERR)
...
</table>
<tableCaption confidence="0.997638">
Table 2 Classes of Human Collaborated Filtering.
</tableCaption>
<bodyText confidence="0.99886675">
Class A satisfies Condition 1 and 2 and is con-
firmed as redundant errors. When a pair of errors is
a member of Class A, one of the errors can be re-
moved. Class B also meets Condition 1 and 2, but
is eliminated from the candidacy because human
experts have determined they did not deal with the
same linguistic phenomenon. Each error of Class B
has to be treated as unique. With respect to Class C,
the errors cannot be determined its candidacy with
the information available at this stage. Additional
information is required to determine the redundan-
cy.
</bodyText>
<subsectionHeader confidence="0.7019655">
2.2.4 Final Automated Filtering Using De-
cision Rules
</subsectionHeader>
<bodyText confidence="0.9955135">
In order to confirm the errors of Class C as redun-
dant, additional information is necessary.
</bodyText>
<table confidence="0.996587166666667">
(Ex4) Correct answer: I don’t know why she went there.
Student answer: I don’t know why she go to their.
Err1: CONFUSABLE WORD ERR|8|there word
Err2: SUBJ_VERB_AGR_ERR|6|went[3S] syntactic
A syntactic
Err3: EXTR_PREP_ERR|6-8|
Err4: UNNECESSARY NODE ERR|7|(to) mapping
Err5: TENSE_NMATCHED ERR|6|went[past] mapping
U
(Ex5) Correct answer: Would you like to come?
Student answer: you go to home?
Err1: FIRST WORD _CASE ERR|1 |word
R_PREP ERR|3-4 |syntactic
Err2: EXTA
Err3:OBLIGATORY_NODE_MISSING_ERR|(1,3) |mapping
Would _ like
Err4: UNNECESSARY NODE ERR|4|(home) mapping
Err5: LEXICAL ERR|2|come mapping
</table>
<bodyText confidence="0.9487685">
2 They are English teachers who have a linguistic background
and teaching experiences of 10 years or more.
</bodyText>
<equation confidence="0.7105815">
(1)
82
</equation>
<bodyText confidence="0.999253416666667">
EXTRA_PREP_ERR’ and ‘UNNECESSARY_
NODE_ERR’ were selected as a candidate from
both (Ex4) and (Ex5) through the steps mentioned
in section 2.2.1 ~ 2.2.3. The pair from (Ex4) is a
redundant error, but the one from (Ex5) is a false
alarm. (Ex4) points out a preposition ‘to’ as an un-
necessary element whereas (Ex5) indicates a noun
‘home’ as incorrect.
To determine the finalist of redundant errors,
we have adopted a decision tree. To train the deci-
sion tree, we have chosen a feature set for a pair of
errors (E1, E2) as follows.
</bodyText>
<listItem confidence="0.950572">
(1) The length of shared words in E1 and E2 divided by the
length of a shorter sentence (shared_length)
(2) The length of non-shared words in E1 and E2 divided by the
length of a shorter sentence. (non_shared_length)
(3) The Error Correction Info of E1 (E1. Correction_In
(4) The Error_ Correction__Info of E2 (E2.Correction_Info)
(5) Edit distance value between correction string of E1 and E2
(edit_distance)
(6) Error Position of E1 (E1.pos)
(7) Error Position of E2 (E2.pos)
(8) Difference of Error positions of E1 and E2 (diff error pos)
</listItem>
<bodyText confidence="0.9986726">
12,178 pairs of errors for 44 types in Class C were
used to train a decision tree. We used CART
(Breiman et al., 1984) to extract decision rules.
The followings show a part of the decision rules to
eliminate redundant errors from Class C.
</bodyText>
<figure confidence="0.908501083333333">
E1=CONJ_MISSING_ERR
E2=OPTIONAL_NODE_MISSING_ERR
If E2.Correction_Info=‘conj’ and E2.pos=1 then redundant_error
E1=EXTRA_PREP_ERR, E2=UNNECESSARY_NODE_ERR
If E2.Correction_Info=‘prep’ and E2.pos=1 then redundant_error
E1=VERB_SUBCAT_ERR,
E2=OPTIONAL_NODE_MISSING_ERR
If diff_error_pos &lt;=3 and E2.Correction_Info={‘prep’ , ‘adv’}
then redundant_error
E1=VERB_ING_FORM_ERR, E2=TENSE_UNMATCHED_ERR
If E2.Correction_Info=‘verb-ing’ then redundant_error
�
</figure>
<bodyText confidence="0.999920666666667">
The errors are removed according to a priority spe-
cified in the rules. The syntactic phase is assigned
with the highest priority since syntactic errors have
the most extensive coverage which is identified at
a phrasal level. On the other hand, the lowest prior-
ity is given to the mapping phase because mapping
errors are detected through a simple word-to-word
comparison of a student input with the correct an-
swer.
</bodyText>
<sectionHeader confidence="0.999395" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.9998038">
We evaluated the accuracy of determining redun-
dant errors. Table 3 presents the results. The evalu-
ation was performed on 200 sentences which were
not included in the training data. Even though the
redundancy of the pairs of errors in Class A and
Class B are determined by the human expert, the
accuracies of both classes did not reach 100% be-
cause the errors detected by the system were incor-
rect. The total accuracy including Class A, B, and
C was 90.2%.
</bodyText>
<tableCaption confidence="0.994522">
Table 3: The accuracy
</tableCaption>
<bodyText confidence="0.999940166666667">
The performance of our automated scoring sys-
tem was measured using exact agreement (Attali
and Burstein, 2006) of the final scores calculated
by the system and human raters. The overall per-
formance was improved by 2.6% after redundant
errors were removed.
</bodyText>
<sectionHeader confidence="0.999576" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999993875">
This paper has introduced a human collaborated
filtering method to eliminate redundant errors re-
ported during automated scoring. Since scoring
processes are performed through three separate
phases including word, syntax and mapping, some
of the errors are redundantly reported with differ-
ent IDs. In addition, it is almost impossible to pre-
dict every type of errors that could occur in student
answers. Because of these issues, it is not easy for
the system to automatically determine which errors
are reported redundantly, or to estimate all the
possible redundant errors. As a solution to these
problems, we have adopted a human assisted ap-
proach. The performance has been improved after
redundant errors were removed with the approach
implemented in the system.
</bodyText>
<sectionHeader confidence="0.998472" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997188363636364">
Jee Eun Kim, K. J. Lee and K. A. Jin. 2007. Building an Au-
tomated Scoring System for a Single Sentence. Korea In-
formation Processing Society Vol.4, No.3 (in Korean).
Keh-Yih Su, Ming-Wen We and Jing-Shin Chang. 1994. A
Corpus-based Approach to Automatic Compound Extrac-
tion, In Proceedings of the ACL 94.
Leo Breiman, Jerome Friedman, Charles J. Stone, and R.A.
Olshen. 1984. Classification and Regression Trees. Mon-
terey, Calif., U.S.A.: Wadsworth, Inc.
Yigal Attali and Jill Burstein. 2006. Automated Essay Scoring
with e-rater® V.2.
</reference>
<figure confidence="0.97858125">
Class A
Class B
Class C
Accuracy
94.1%
98.0%
82.3%
83
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.852224">
<title confidence="0.9907715">A Human-Computer Collaboration Approach to Improve Accuracy an Automated English Scoring System</title>
<author confidence="0.998775">Jee Eun Kim Kong Joo Lee</author>
<affiliation confidence="0.999936">Hankuk University of Foreign Studies Chungnam National University</affiliation>
<address confidence="0.973055">Seoul, Korea Daejeon, Korea</address>
<email confidence="0.928803">jeeeunk@hufs.ac.krkjoolee@cnu.ac.kr</email>
<abstract confidence="0.9970806">This paper explores an issue of redundant errors reported while automatically scoring English learners’ sentences. We use a human-computer collaboration approach to eliminate redundant errors. The first step is to automatically select candidate redundant errors using PMI and RFC. Since those errors are detected with different IDs although they represent the same error, the candidacy cannot be confirmed automatically. The errors are then handed over to human experts to determine the candidacy. The final candidates are provided to the system and trained with a decision tree. With those redundant errors eliminated, the system accuracy has been improved.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jee Eun Kim</author>
<author>K J Lee</author>
<author>K A Jin</author>
</authors>
<title>Building an Automated Scoring System for a Single Sentence. Korea Information Processing Society Vol.4,</title>
<date>2007</date>
<location>No.3 (in Korean).</location>
<contexts>
<context position="1333" citStr="Kim et al., 2007" startWordPosition="202" endWordPosition="205"> to human experts to determine the candidacy. The final candidates are provided to the system and trained with a decision tree. With those redundant errors eliminated, the system accuracy has been improved. 1 Introduction An automated English scoring system analyzes a student sentence and provides a score and feedback to students. The performance of a system is evaluated based on the accuracy of the score and the relevance of the feedback. The system described in this paper scores English sentences composed by Korean students learning English. A detailed explanation of the system is given in (Kim et al., 2007). The scores are calculated from three different phases including word, syntax and mapping, each of which is designed to assign 0~2 points. Three scores are added up to generate the final score. A spelling error, a plural form error, and a confusable word error are considered as typical word errors. A subject verb agreement error, a word order error and relative clause error are typical examples of syntactic errors. Even when a student sentence is perfectly correct in lexical and syntactic level, it may fail to convey what is meant by the question. Such sentences are evaluated as grammatical, </context>
</contexts>
<marker>Kim, Lee, Jin, 2007</marker>
<rawString>Jee Eun Kim, K. J. Lee and K. A. Jin. 2007. Building an Automated Scoring System for a Single Sentence. Korea Information Processing Society Vol.4, No.3 (in Korean).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keh-Yih Su</author>
<author>Ming-Wen We</author>
<author>Jing-Shin Chang</author>
</authors>
<title>A Corpus-based Approach to Automatic Compound Extraction,</title>
<date>1994</date>
<booktitle>In Proceedings of the ACL 94.</booktitle>
<contexts>
<context position="9812" citStr="Su et al., 1994" startWordPosition="1549" endWordPosition="1552"> of the selection process, the total of 150,419 candidate pairs was selected from 14,892 scoring results of the student sentences. 2.2.2 Filtering Candidate Errors The candidates extracted through the process mentioned in 2.2.1 were classified based on their error identifications only, without considering error position and error correction information. 150,419 pairs of the errors were assorted into 657 types. The frequency of each type of the candidates was then calculated. These candidate errors were filtered by applying PMI (Pointwise Mutual Information) and RFC (Relative Frequency Count) (Su et al., 1994). PMI(E1, E2) =log P(E1, E2) P(E1)P(E2) 1 2 = (2) freq E E ( , ) 1 2 RFC E E ( , ) freq PMI is represented by a number indicating how frequently two errors E1 and E2 occur simultaneously. RFC refers to relative frequency against average frequency of the total candidates. The filtering equation is as follows: PMI(E1, E2) x RFC(E1, E2) &gt;_ k (3) Using this equation, the system filtered the candidates whose value was above the threshold k. For this experiment, 0.4 was assigned to k and 111 error types were selected. 2.2.3 Human Collaborated Filtering Filtered 111 error types include 29,588 candida</context>
</contexts>
<marker>Su, We, Chang, 1994</marker>
<rawString>Keh-Yih Su, Ming-Wen We and Jing-Shin Chang. 1994. A Corpus-based Approach to Automatic Compound Extraction, In Proceedings of the ACL 94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
<author>Jerome Friedman</author>
<author>Charles J Stone</author>
<author>R A Olshen</author>
</authors>
<title>Classification and Regression Trees.</title>
<date>1984</date>
<publisher>Wadsworth, Inc.</publisher>
<location>Monterey, Calif., U.S.A.:</location>
<contexts>
<context position="13786" citStr="Breiman et al., 1984" startWordPosition="2174" endWordPosition="2177">d E2 divided by the length of a shorter sentence (shared_length) (2) The length of non-shared words in E1 and E2 divided by the length of a shorter sentence. (non_shared_length) (3) The Error Correction Info of E1 (E1. Correction_In (4) The Error_ Correction__Info of E2 (E2.Correction_Info) (5) Edit distance value between correction string of E1 and E2 (edit_distance) (6) Error Position of E1 (E1.pos) (7) Error Position of E2 (E2.pos) (8) Difference of Error positions of E1 and E2 (diff error pos) 12,178 pairs of errors for 44 types in Class C were used to train a decision tree. We used CART (Breiman et al., 1984) to extract decision rules. The followings show a part of the decision rules to eliminate redundant errors from Class C. E1=CONJ_MISSING_ERR E2=OPTIONAL_NODE_MISSING_ERR If E2.Correction_Info=‘conj’ and E2.pos=1 then redundant_error E1=EXTRA_PREP_ERR, E2=UNNECESSARY_NODE_ERR If E2.Correction_Info=‘prep’ and E2.pos=1 then redundant_error E1=VERB_SUBCAT_ERR, E2=OPTIONAL_NODE_MISSING_ERR If diff_error_pos &lt;=3 and E2.Correction_Info={‘prep’ , ‘adv’} then redundant_error E1=VERB_ING_FORM_ERR, E2=TENSE_UNMATCHED_ERR If E2.Correction_Info=‘verb-ing’ then redundant_error � The errors are removed accor</context>
</contexts>
<marker>Breiman, Friedman, Stone, Olshen, 1984</marker>
<rawString>Leo Breiman, Jerome Friedman, Charles J. Stone, and R.A. Olshen. 1984. Classification and Regression Trees. Monterey, Calif., U.S.A.: Wadsworth, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yigal Attali</author>
<author>Jill Burstein</author>
</authors>
<title>Automated Essay Scoring with e-rater® V.2.</title>
<date>2006</date>
<contexts>
<context position="15374" citStr="Attali and Burstein, 2006" startWordPosition="2403" endWordPosition="2406">with the correct answer. 3 Evaluation We evaluated the accuracy of determining redundant errors. Table 3 presents the results. The evaluation was performed on 200 sentences which were not included in the training data. Even though the redundancy of the pairs of errors in Class A and Class B are determined by the human expert, the accuracies of both classes did not reach 100% because the errors detected by the system were incorrect. The total accuracy including Class A, B, and C was 90.2%. Table 3: The accuracy The performance of our automated scoring system was measured using exact agreement (Attali and Burstein, 2006) of the final scores calculated by the system and human raters. The overall performance was improved by 2.6% after redundant errors were removed. 4 Conclusion This paper has introduced a human collaborated filtering method to eliminate redundant errors reported during automated scoring. Since scoring processes are performed through three separate phases including word, syntax and mapping, some of the errors are redundantly reported with different IDs. In addition, it is almost impossible to predict every type of errors that could occur in student answers. Because of these issues, it is not eas</context>
</contexts>
<marker>Attali, Burstein, 2006</marker>
<rawString>Yigal Attali and Jill Burstein. 2006. Automated Essay Scoring with e-rater® V.2.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>