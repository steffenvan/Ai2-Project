<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.940496">
Predictive Text Entry using Syntax and Semantics
</title>
<author confidence="0.99739">
Sebastian Ganslandt Jakob Jörwall Pierre Nugues
</author>
<affiliation confidence="0.982493">
Department of Computer Science
Lund University
</affiliation>
<address confidence="0.908144">
S-221 00 Lund, Sweden
</address>
<email confidence="0.973783">
sebastian@ganslandt.nu pierre.nugues@cs.lth.se
d02jjr@student.lth.se
</email>
<sectionHeader confidence="0.994215" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999853774193548">
Most cellular telephones use numeric key-
pads, where texting is supported by dic-
tionaries and frequency models. Given a
key sequence, the entry system recognizes
the matching words and proposes a rank-
ordered list of candidates. The ranking
quality is instrumental to an effective en-
try.
This paper describes a new method to en-
hance entry that combines syntax and lan-
guage models. We first investigate com-
ponents to improve the ranking step: lan-
guage models and semantic relatedness.
We then introduce a novel syntactic model
to capture the word context, optimize
ranking, and then reduce the number of
keystrokes per character (KSPC) needed
to write a text. We finally combine this
model with the other components and we
discuss the results.
We show that our syntax-based model
reaches an error reduction in KSPC of
12.4% on a Swedish corpus over a base-
line using word frequencies. We also show
that bigrams are superior to all the other
models. However, bigrams have a mem-
ory footprint that is unfit for most devices.
Nonetheless, bigrams can be further im-
proved by the addition of syntactic mod-
els with an error reduction that reaches
29.4%.
</bodyText>
<sectionHeader confidence="0.999165" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9994215">
The 12-key input is the most common keypad lay-
out on cellular telephones. It divides the alpha-
bet into eight lists of characters and each list is
mapped onto one key as shown in Figure 1. Since
three or four characters are assigned to a key, a
single key press is ambiguous.
</bodyText>
<figureCaption confidence="0.9326275">
Figure 1: Standard 12-button keypad layout (ISO
9995-8).
</figureCaption>
<subsectionHeader confidence="0.9974">
1.1 Multi-tap
</subsectionHeader>
<bodyText confidence="0.9999765625">
Multi-tap is an elementary method to disam-
biguate input for a 12-button keypad. Each charac-
ter on a key is assigned an index that corresponds
to its visual position, e.g. ‘A’, 1, ‘B’, 2, and ‘C’,
3 and each consecutive stroke – tap – on the same
key increments the index. When the user wants
to type a letter, s/he presses the corresponding key
until the desired index is reached. The user then
presses another key or waits a predefined time to
verify that the correct letter is selected. The key
sequence 8-4-4-3-3, for example, leads to the word
the.
Multi-tap is easy to implement and no dictio-
nary is needed. At the same time, it is slow and
tedious for the user, notably when two consecutive
characters are placed on the same key.
</bodyText>
<subsectionHeader confidence="0.987428">
1.2 Single Tap with Predictive Text
</subsectionHeader>
<bodyText confidence="0.999518428571429">
Single tap with predictive text requires only one
key press to enter a character. Given a keystroke
sequence, the system proposes words using a dic-
tionary or language modeling techniques.
Dictionary-based techniques search the words
matching the key sequence in a list that is stored
by the system (Haestrup, 2001). While some
</bodyText>
<page confidence="0.993916">
37
</page>
<note confidence="0.877442">
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 37–48,
Paris, October 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999918761904762">
keystroke sequences produce a unique word, oth-
ers are ambiguous and the system returns a list
with all the candidates. The key sequence 8-4-3,
for example, corresponds to at least three possi-
ble words: the, tie, and vie. The list of candidates
is then sorted according to certain criteria, such
as the word or character frequencies. If the word
does not exist in the dictionary, the user has to fall
back to multi-tap to enter it. The T91 commercial
product is an example of a dictionary-based sys-
tem (Grover et al., 1998).
LetterWise (MacKenzie et al., 2001) is a tech-
nique that uses letter trigrams and their frequen-
cies to predict the next character. For example,
pressing the key 3 after the letter bigram ‘th’ will
select ‘e’, because the trigram ‘the’ is far more fre-
quent than ‘thd’ or ‘thf’ in English. When the sys-
tem proposes a wrong letter, the user can access
the next most likely one by pressing a next-key.
LetterWise does not need a dictionary and has a
K5PC of 1.1500 (MacKenzie, 2002).
</bodyText>
<subsectionHeader confidence="0.994345">
1.3 Modeling the Context
</subsectionHeader>
<bodyText confidence="0.999878458333333">
Language modeling can extend the context from
letter sequences to word n-grams. In this case, the
system is not restricted to the disambiguation or
the prediction of the typed characters. It can com-
plete words and even predict phrases. HMS (Has-
selgren et al., 2003) is an example of this that uses
word bigrams in Swedish. It reports a K5PC
ranging from 0.8807 to 1.0108, depending on the
type of text. eZiText2 is a commercial example of
a word and phrase completion system. However,
having a large lexicon of bigrams still exceeds the
memory capacity of many mobile devices.
Some systems use a combination of syntac-
tic and semantic information to model the con-
text. Gong et al. (2008) is a recent example that
uses word frequencies, a part-of-speech language
model, and a semantic relatedness metric. The
part-of-speech language model acts as a lexical
n-gram language model, but occupies much less
memory since the vocabulary is restricted to the
part-of-speech tagset. The semantic relatedness,
modified from Li and Hirst (2005), is defined as
the conditional probability of two stems appearing
in the same context (the same sentence):
</bodyText>
<footnote confidence="0.9988625">
1www.t9.com
2www.zicorp.com/ezitext.htm
</footnote>
<bodyText confidence="0.999513875">
The three components are combined linearly
and their coefficients are adjusted using a devel-
opment set. Setting 1 as the limit of the K5PC
figure, Gong et al. (2008) reported an error reduc-
tion over the word frequency baseline of 4.6% for
the semantic model, 12.6% for the part-of-speech
language model, and 15.8% for the combination
of both.
</bodyText>
<subsectionHeader confidence="0.997222">
1.4 Syntax in Predictive Text
</subsectionHeader>
<bodyText confidence="0.999973272727273">
Beyond part-of-speech language modeling, there
are few examples of systems using syntax in pre-
dictive text entry. Matiasek et al. (2002) describes
a predictive text environment aimed at disabled
persons, which originally relied on language mod-
els. Gustavii and Pettersson (2003) added a syn-
tactic component to it based on grammar rules.
The rules corresponded to common grammatical
errors and were used to rerank the list of candidate
words. The evaluation results were disappointing
and the syntactic component was not added be-
cause of the large overhead it introduced (Mati-
asek, 2006).
In the same vein, Sundarkantham and Shalinie
(2007) used grammar rules to discard infeasible
grammatical constructions. The authors evaluated
their system by giving it an incomplete sentence
and seeing how often the system correctly guessed
the next word (Shannon, 1951). They achieved
better results than previously reported, although
their system has not been used in the context of
predictive text entry for mobile devices.
</bodyText>
<sectionHeader confidence="0.943454" genericHeader="method">
2 Predictive Text Entry Using Syntax
</sectionHeader>
<bodyText confidence="0.999954">
We propose a new technique that makes use of
a syntactic component to model the word context
and improve the K5PC figure. It builds on Gong
et al. (2008)’s system and combines a dependency
grammar model with word frequencies, a part-of-
speech language model, and the semantic related-
ness defined in Sect. 1.3. As far as we are aware,
no predictive text entry system has yet used a data-
driven syntactic model of the context.
We used Swedish as our target language all
over our experiments, but the results we obtained
should be replicable in any other language.
</bodyText>
<equation confidence="0.9783995">
5emR(w1Iw2) = C(stem(w1), stem(w2)) .
C(w2)
</equation>
<page confidence="0.993985">
38
</page>
<subsectionHeader confidence="0.988993">
2.1 Reranking Candidate Words
</subsectionHeader>
<bodyText confidence="0.983716368421053">
The system consists of two components. The first
one disambiguates the typed characters using a
dictionary and produces a list of candidate words.
The second component reranks the candidate list.
Although the techniques we describe could be ap-
plied to word completion, we set aside this aspect
in this paper.
More formally, we frame text input as a se-
quence of keystrokes, ksi = ksi1 ... ksin, to en-
ter a desired word, wi. The words matching
the key sequence in the system dictionary form
an ordered set of alternatives, match(ksi) =
{cw0, ... , cwm}, where it takes k extra keystrokes
to reach candidate cwk. Using our example
in Sect. 1.2, a lexical ordering would yield
match(8 − 4 − 3) = {the, tie, vie}, where two
extra keystrokes are needed to reach vie.
We assign each candidate word w member of
match(ksi) a score
</bodyText>
<equation confidence="0.988351">
�Score(w|Context) = λs · s(w|Context),
sES
</equation>
<bodyText confidence="0.999499277777778">
to rerank (sort) the prediction list, where s is a
scoring function from a set S, λs, the weight of
s, and Score(w|Context), the total score of w in
the current context.
In this framework, optimizing predictive text
entry is the task of finding the scoring functions,
s, and the weights, λs, so that they minimize k on
average.
As scoring functions, we considered lexical lan-
guage models in the form of unigrams and bi-
grams, sLM1 and sLM2, a part-of-speech model
using sequences of part-of-speech tags of a length
of up to five tags, sPOS, and a semantic affin-
ity, sSemA, derived from the semantic relatedness.
In addition, we introduce a syntactic component
in the form of a data-driven dependency syntax,
sDepSyn so that the complete scoring set consists
of
</bodyText>
<equation confidence="0.868837">
S = {sLM1, sLM2, sSemA, sPOS, sDepSyn}.
</equation>
<subsectionHeader confidence="0.997566">
2.2 Language and Part-of-Speech Models
</subsectionHeader>
<bodyText confidence="0.974288375">
The language model score is the probability of a
candidate word w, knowing the sequence entered
so far, w1,...,wi:
P(w|w1, w2, ... , wi).
We approximate it using unigrams, sLM1(w) =
P(w), or bigrams, sLM2(w) = P(w|wi) that we
derive from a corpus using the maximum like-
lihood estimate. To cope with sparse data, we
used a deleted interpolation so that sLM2(w) =
β1P(w|wi)+β2P(w), where we adjusted the val-
ues of β1 and β2 on a development corpus.
In practice, it is impossible to maintain a large
list of bigrams on cellular telephones as it would
exceed the available memory of most devices. In
our experiments, the sLM2 score serves as an indi-
cator of an upper-limit performance, while sLM1
serves as a baseline, as it is used in commercial
dictionary-based products.
Part-of-speech models offer an interesting alter-
native to lexical models as the number of parts
of speech does not exceed 100 tags in most lan-
guages. The possible number of bigrams is then at
most 10,000 and much less in practice. We defined
the part-of-speech model score, sPOS as
</bodyText>
<equation confidence="0.666443">
P(t|t1, t2, ... , ti),
</equation>
<bodyText confidence="0.9992925">
where ti is the part of speech of wi and t, the part
of speech of the candidate word w. We used a
5-gram approximation of this probability with a
simple back-off model:
</bodyText>
<equation confidence="0.999613333333333">
P(t|ti−3, ... , ti) if C(ti−3, ..., ti) =6 0
P(t|ti−2, ... , ti) if C(ti−2, ..., ti) =6 0
sPOS =
</equation>
<bodyText confidence="0.99998">
We used the Granska tagger (Carlberger and
Kann, 1999) to carry out the part-of-speech anno-
tation of the word sequence.
</bodyText>
<sectionHeader confidence="0.953465" genericHeader="method">
3 Semantic Affinity
</sectionHeader>
<bodyText confidence="0.999947125">
Because of their arbitrary length, language mod-
els miss possible relations between words that are
semantically connected in a sentence but within
a distance greater than one, two, or three words
apart, the practical length of most n-grams mod-
els. Li and Hirst (2005) introduced the semantic
relatedness between two words to measure such
relations within a sentence. They defined it as
</bodyText>
<equation confidence="0.988705">
C(wi, wj)
SemR(wi,wj) = C(wi)C(wj),
</equation>
<bodyText confidence="0.9995885">
where C(wi, wj) is the number of times the words
wi and wj co-occur in a sentence in the corpus,
</bodyText>
<figure confidence="0.671389333333333">
{
...
P(t), otherwise
</figure>
<page confidence="0.937984">
39
</page>
<bodyText confidence="0.955267714285714">
and C(wi) is the count of word wi in the corpus. • Shift pushes the next input word onto the
The relation is symmetrical, i.e. stack.
C(wi, wj) = C(wj, wi). • Reduce pops the top of the stack with the
condition that the corresponding word has a
The estimated semantic affinity of a word w is head.
defined as:
� SemR(w, wj), • LeftArc adds an arc from the next input
SemA(w|H) = word to the top of the stack and pops it.
wj∈H
where H is the context of the word w. In our case,
H consists of words to the left of the current word.
Gong et al. (2008) used a similar model in a pre-
dictive text application with a slight modification
to the SemR function:
</bodyText>
<equation confidence="0.9348295">
C(stem(wi), stem(wj))
SemR(wi, wj) = C(stem(wj))
</equation>
<bodyText confidence="0.999917">
where the stem(w) function removes suffixes
from words. We refined this model further and we
replaced the stemming function with a real lemma-
tization.
</bodyText>
<sectionHeader confidence="0.987837" genericHeader="method">
4 Dependency Parsing
</sectionHeader>
<bodyText confidence="0.999074333333333">
Dependency syntax (Tesnière, 1966) has attracted
a considerable interest in the recent years, spurred
by the availability of data-driven parsers as well
as annotated data in multiple languages includ-
ing Arabic, Chinese, Czech, English, German,
Japanese, Portuguese, or Spanish (Buchholz and
Marsi, 2006; Nivre et al., 2007). We used this
syntactic formalism because of its availability in
many languages.
</bodyText>
<subsectionHeader confidence="0.99418">
4.1 Parser Implementation
</subsectionHeader>
<bodyText confidence="0.942227954545455">
There are two main classes of data-driven de-
pendency parsers: graph-based (McDonald and
Pereira, 2006) and transition-based (Nivre, 2003).
We selected Nivre’s parser because of its imple-
mentation simplicity, small memory footprint, and
linear time complexity. Parsing is always achieved
in at most 2n − 1 actions, where n is the length of
the sentence. Both types of parser can be com-
bined, see Zhang and Clark (2008) for a discus-
sion.
Nivre’s parser is an extension to the shift–
reduce algorithm that creates a projective and
acyclic graph. It uses a stack, a list of input words,
and builds a set of arcs representing the graph of
dependencies. The parser uses two operations in
addition to shift and reduce, left-arc and right-arc:
• RightArc adds an arc from the top of the
stack to the next input word and pushes the
input word on the stack.
Table 1 shows the start and final parser states as
well as the four transitions and their conditions
and Algorithm 1 describes the parsing algorithm.
</bodyText>
<subsectionHeader confidence="0.867567">
4.2 Features
</subsectionHeader>
<bodyText confidence="0.997776571428571">
At each step of the parsing procedure, the parser
turns to a guide to decide on which transition
to apply among the set {LeftArc, RightArc,
Shift, Reduce}. We implemented this guide
as a four-class classifier that uses features it ex-
tracts from the parser state. The features consist
of words and their parts of speech in the stack, in
the queue, and in the partial graph resulting from
what has been parsed so far. The classifier is based
on a linear logistic regression function that evalu-
ates the transition probabilities from the features
and predicts the next one.
In the learning phase, we extracted a data set
of feature vectors using the gold-standard parsing
procedure (Algorithm 2) that we applied to Tal-
banken corpus of Swedish text (Einarsson, 1976;
Nilsson et al., 2005). Each vector being labeled
with one of the four possible transitions. We
trained the classifiers using the LIBLINEAR im-
plementation (Fan et al., 2008) of logistic regres-
sion.
However, classes are not always separable us-
ing linear classifiers. We combined single features
as pairs or triples. This emulates to some extent
quadratic kernels used in support vector machines,
while preserving the speed of the linear models.
Table 2 shows the complete feature set to predict
the transitions. A feature is defined by
</bodyText>
<listItem confidence="0.7547908">
• A source: S for stack and Q for the queue;
• An offset: 0 for the top of the stack and first
in the queue; 1 and 2 for levels down in the
stack or to the right in the queue;
,
</listItem>
<page confidence="0.943555">
40
</page>
<table confidence="0.993122285714286">
Name Action Condition
Initialization hnil, W, ∅i
Termination hS, nil, Ai
LeftArc hn|S,n&apos;|Q,Ai → hS,n&apos;|Q,A ∪ {hn&apos;,ni}i ¬∃n&apos;&apos;, hn,n&apos;&apos;i ∈ A
RightArc hn|S, n&apos;|Q, Ai → hn&apos;|n|S, Q, A ∪ hn, n&apos;ii ¬∃n&apos;&apos;, hn&apos;, n&apos;&apos;i ∈ A
Reduce hn|S, Q, Ai → hS, Q, Ai ∃n&apos;, hn, n&apos;i ∈ A
Shift hS,n|Q,Ai → hn|S,Q,Ai
</table>
<tableCaption confidence="0.999152">
Table 1: Parser transitions. W is the original input sentence, A is the dependency graph, S is the stack,
</tableCaption>
<bodyText confidence="0.8659915">
and Q is the queue. The triplet hS, Q, Ai represents the parser state. n, n&apos;, and n&apos;&apos; are lexical tokens. The
pair hn&apos;, ni represents an arc from the head n&apos; to the dependent n.
</bodyText>
<listItem confidence="0.997291">
• Possible applications of the function head, H,
leftmost child, LC, or righmost child, RC;
• The value: word, w, or POS tag, t, at the
specified position.
</listItem>
<equation confidence="0.536139">
Queue Q0w
Q1w
Q0t
Q1t
</equation>
<bodyText confidence="0.919232571428571">
Q0tQ0w
Q0tQ1t
Q1wQ1t
Q0tQ1tQ2t
Q0wQ1tQ2t
Stack S0t
S0w
S0tS0w
S0tS1t
Stack/Queue S0wQ0w
Q0tS0t
Q1tS0t
Q0tS1t
Q1tS1t
S0tQ0tQ1t
S0tQ0wQ0t
Partial Graph S0HtS0tQ0t
Q0LCtS0tQ0t
Q0LCtS0tQ0w
S0RCtS0tQ0t
S0RCtS0tQ0w
</bodyText>
<tableCaption confidence="0.9869815">
Table 2: Feature model for predicting parser ac-
tions with combined features.
</tableCaption>
<subsectionHeader confidence="0.997687">
4.3 Calculating Graph Probabilities
</subsectionHeader>
<bodyText confidence="0.99806225">
Nivre (2006) showed that every terminating tran-
sition sequence Am1 = (a1, ..., am) applied to
a sentence W1n = (w1, ..., wn) defines exactly
one parse tree G. We approximated the prob-
ability P(G|W1n) of a dependency graph G as
P(Am1|W1n) and we estimated the probability of
G as the product of the transition probabilities, so
that
</bodyText>
<equation confidence="0.99904825">
PParse(G|W1n) = P(Am1|W1n )
= rim k=1 P(ak|Ak−1
1 , Wφ(k−1)
1 ),
</equation>
<bodyText confidence="0.9968232">
where ak is member of the set {LeftArc,
RightArc, Shift, Reduce} and O(k) corre-
sponds to the index of the current word at tran-
sition k.
We finally approximated the term
</bodyText>
<equation confidence="0.906354">
Ak−1
1 , Wφ(k−1) to the feature set and com-
1
</equation>
<bodyText confidence="0.975609">
puted probability estimates using the logistic
regression output.
</bodyText>
<subsectionHeader confidence="0.992937">
4.4 Beam Search
</subsectionHeader>
<bodyText confidence="0.9999635">
We extended Nivre’s parser with a beam search to
mitigate error propagation that occurs with a de-
terministic parser (Johansson and Nugues, 2006).
We maintained N parser states in parallel and we
applied all the possible transitions to each state.
We scored each transition action and we ranked
the states with the product of the action’s proba-
bilities leading to this state. Algorithm 3 outlines
beam search with a diameter of N.
An alternative to training parser transitions us-
ing local features is to use an online learning al-
gorithm (Johansson and Nugues, 2007; Zhang and
Clark, 2008). The classifiers are then computed
over the graph that has already been built instead
of considering the probability of a single transi-
tion.
</bodyText>
<page confidence="0.996989">
41
</page>
<subsectionHeader confidence="0.871695">
4.5 Evaluation
</subsectionHeader>
<bodyText confidence="0.979010416666667">
We evaluated our dependency parser separately
from the rest of the application and Table 3 shows
the results. We optimized our parameter selection
for the unlabeled attachment score (UAS). This
explains the relatively high difference with the la-
beled attachment score (LAS): about –8.6.
Table 3 also shows the highest scores ob-
tained on the same Talbanken corpus of Swedish
text (Einarsson, 1976; Nilsson et al., 2005) in
the CoNLL-X evaluation (Buchholz and Marsi,
2006): 89.58 for unlabeled attachments (Corston-
Oliver and Aue, 2006) and 84.58 for labeled at-
tachments (Nivre et al., 2006). CoNLL-X systems
were optimized for the LAS category.
The figures we reached were about 1.10% be-
low those reported in CONLL-X for the UAS cat-
egory. However our results are not directly compa-
rable as the parsers or the classifiers in CONLL-X
have either a higher complexity or are more time-
consuming. We chose linear classifiers over kernel
machines as it was essential to our application to
run on mobile devices with limited resources in
both CPU power and memory size.
This paper
</bodyText>
<table confidence="0.97525625">
Beam width LAS UAS
1 79.45 88.05
2 79.76 88.41
4 79.75 88.40
8 79.77 88.41
16 79.78 88.42
32 79.77 88.41
64 79.79 88.44
</table>
<tableCaption confidence="0.941387">
Table 3: Parse results on the Swedish Talbanken
corpus obtained for this paper as well as the best
reported results in CONLL-X on the same corpus
(Buchholz and Marsi, 2006).
</tableCaption>
<sectionHeader confidence="0.922516" genericHeader="method">
5 Dependencies to Predict the Next Word
</sectionHeader>
<bodyText confidence="0.999667333333333">
We built a syntactic score to measure the grammat-
ical relevance of a candidate word w in the current
context, that is the word sequence so far w1, ..., wi.
We defined it as the weighted sum of three terms:
the score of the partial graph resulting from the
analysis of the words to the left of the candidate
word and the scores of the link from w to its head,
h(w), using their lexical forms and their parts of
speech:
</bodyText>
<equation confidence="0.988844666666667">
sDepSyn(w) = A1PParse(G(w)|w1, ..., wi, w)+
A2PLink(w, h(w))+
A3PLink(POS(w), POS(h(w))),
</equation>
<bodyText confidence="0.9995558">
where G(w) is the partial graph representing the
word sequence w1, ..., wi, w. The PLink terms are
intended to give an extra-weight to the probabil-
ity of an association between the predicted word
and a possible head to the left of it. They hint at
the strength of the ties between w and the words
before it.
We used the transition probabilities described in
Sect. 4.3 to compute the score of the partial graph,
yielding
</bodyText>
<equation confidence="0.885056">
PParse(G(w)|w1, ..., wi, w) =
</equation>
<bodyText confidence="0.999015125">
where a1,..., aj is the sequence of transition ac-
tions producing G(w) and P(ak), the probability
output of transition k given by the logistic regres-
sion engine.
The last two terms PLink(w, h(w)) and
PLink(POS(w), POS(h(w))) are computed
from counts in the training corpus using maxi-
mum likelihood estimates:
</bodyText>
<equation confidence="0.913873222222222">
PLink(w, h(w)) =
C(Link(w, h(w)) + 1 + |PW|
Ewl∈PW C(Link(wl, h(wl)))
and
PLink(POS(w), POS(h(w))) =
C(Link(POS(w), POS(h(w)))) + 1
E
wl∈PW C(Link(POS(wl), h(POS(wl))))
+|PW|,
</equation>
<bodyText confidence="0.9998768">
where PW = match(ksi), is the set of predicted
words for the current key sequence.
If the current word w has not been assigned a
head yet, we default h(w) to the root of the graph
and POS(h(w)) to the ROOT value.
</bodyText>
<sectionHeader confidence="0.997177" genericHeader="evaluation">
6 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.956995">
6.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.940869">
Figure 2 shows an overview of the three stages
to produce and evaluate our models: training,
</bodyText>
<figure confidence="0.991933">
CONLL-X
LAS UAS
84.58 89.54
� j
k=1
P(ak),
</figure>
<page confidence="0.997451">
42
</page>
<bodyText confidence="0.999971272727273">
tuning, and testing. Ideally, we would have
trained the classifiers on a corpus matching a
text entry application. However, as there is no
large available SMS corpus in Swedish, we used
the Stockholm-Umeå corpus (SUC) (Ejerhed and
Källgren, 1997). SUC is balanced and the largest
available POS-tagged corpus in Swedish with
more than 1 million words.
We parsed the corpus and we divided it ran-
domly into a training set (80%), a development set
(10%), and a test set (10%). The training set was
used to gather statistics on word n-grams, POS
n-grams, collocations, lemma frequencies, depen-
dent/head relations. We discarded hapaxes: rela-
tions and sequences occurring only once. We used
lemmas instead of stems in the semantic related-
ness score, SemR, because stemming is less ap-
propriate in Swedish than in English.
We used the development set to find optimal
weights for the scoring functions, resulting in the
lowest KSPC. We ran an exhaustive search using
all possible linear combinations with increments
of 0.1, except for two functions, where this was
too coarse. We used 0.01 then.
We applied the resulting linear combinations of
scoring functions to the test set. We first compared
the frequency-based disambiguation acting as a
baseline to linear combinations involving or not
involving syntax, but always excluding bigrams.
Table 4 shows the most significant combinations.
We then compared a set of other combinations
with the bigram model. They are shown in Ta-
ble 6.
</bodyText>
<subsectionHeader confidence="0.99789">
6.2 Metrics
</subsectionHeader>
<bodyText confidence="0.999919333333333">
We redefined the KSPC metric of MacKenzie
(2002), since the number of characters needed to
input a word is now dependent on the word’s left
context in the sentence. Let S = (wi, ... , wn) E
L be a sentence in the test corpus. The KSPC for
the test corpus then becomes
</bodyText>
<equation confidence="0.939845">
P KSPC = SEL PwES KS(w|LContext(w, S))
P PwES Chars(w)
SEL
</equation>
<bodyText confidence="0.996740875">
where KS(w|LContext) is the number of key
strokes needed to enter a word in a given context,
LContext(w, S) is the left context of w in S, and
Chars(w) is the number of characters in w.
Another performance measure is the disam-
biguation accuracy (DA), which is the percentage
of words that are correctly disambiguated after all
the keys have been pressed
</bodyText>
<equation confidence="0.993601">
PredHit(w|LContext(w, S))
DA =
</equation>
<bodyText confidence="0.9997464">
where PredHit(w|Context) = 1 if w is the
top prediction and 0 otherwise, and #w, the to-
tal number of words in L. A good DA means that
the user can more often simply accept the default
proposed word instead of navigating the prediction
list for the desired word.
As scoring tokens, we chose to keep the ones
that actually have the ability to differentiate the
models, i.e. we did not count the KSPC and DA
for words that were not in the dictionary. Neither
did we count white spaces, nor the punctuation
marks.
All our measures are without word or phrase
completion. This means that the lower-limit fig-
ure for KSPC is 1.
</bodyText>
<subsectionHeader confidence="0.948357">
6.3 Results
</subsectionHeader>
<bodyText confidence="0.99997275">
As all the KSPC figures are close to 1, we com-
puted the error reduction rate (ERR), i.e. the re-
duction in the number of extra keystrokes needed
beyond one. We carried out all the optimizations
considering KSPC, but we can observe that KSPC
ERR and DA ERR strongly correlate.
Table 5 shows the results with scoring func-
tions using the word frequencies. The columns
include KSPC and DA together with KSPC ERR
and DA ERR compared with the baseline. Table 7
shows the respective results when using a bigram-
based disambiguation instead of just frequency.
The ERR is still compared to the word frequency
baseline but attention should also be drawn on the
relative increases: how much the new models can
improve bigram-based disambiguation.
</bodyText>
<sectionHeader confidence="0.996347" genericHeader="discussions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999422909090909">
We can observe from the results that a model based
on dependency grammars improves the prediction
considerably. The DepSyn model is actually the
most effective one when applied together with the
frequency counts. Furthermore, the improvements
from the POS, SemA, and DepSyn model are
almost disjunct, as the combined model improve-
ment matches the sum of their respective individ-
ual contributions.
The 4.2% ERR observed when adding the
SemA model is consistent with the result from
</bodyText>
<equation confidence="0.6363744">
X
SEL
X
wES
#w ,
</equation>
<page confidence="0.997894">
43
</page>
<figureCaption confidence="0.998591">
Figure 2: System architecture, where the set of scoring functions is S = {sLM, sSemA, sPOS, sDepSyn}
</figureCaption>
<equation confidence="0.3204495">
�and the linear combination is = λs · s(w).
s∈S
</equation>
<bodyText confidence="0.999963470588235">
Gong et al. (2008), where a 4.6% ERR was found.
On the other hand, the POS model only con-
tributed 4.7% ERR in our case, whereas Gong et
al. (2008) observed 12.6%. One possible expla-
nation for this is that they clustered related POS
tags into 19 groups reducing the sparseness prob-
lem. By performing this grouping, we can effec-
tively ignore morphological and lexical features
that have no relevance, when deciding which word
should come next. Other possible explanations in-
clude that our backoff model is not well suited for
this problem or that the POS sequences are not an
applicable model for Swedish.
The bigram language model has the largest im-
pact on the performance. The ERR for bigrams
alone is higher than all the other models com-
bined. Still, the other models have the ability to
contribute on top of the bigram model. For exam-
ple, the POS model increases the ERR by about
5% both when using bigram- and frequency-based
disambiguation, suggesting that this information is
not captured by the bigrams. On the other hand,
DepSyn increases the ERR by a more modest 3%
when using bigrams instead of 7% with word fre-
quencies. This is likely due to the fact that about
half of the dependency links only stretch to the
next preceding or succeeding word in the corpus.
The most effective combination of models are
the bigrams together with the POS sequence and
the dependency structure, both embedding syntac-
tic information. With this combination, we were
able to reduce the number of erroneous disam-
biguations as well as extra keystrokes by almost
one third.
</bodyText>
<sectionHeader confidence="0.996751" genericHeader="acknowledgments">
8 Further Work
</sectionHeader>
<bodyText confidence="0.999984545454546">
SMS texting, which is the target of our system,
is more verbal than the genres gathered in the
Stockholm-Umeå corpus. The language models
of a final application would then change consid-
erably from the ones we extracted from the SUC.
A further work would be to collect a SMS corpus
and replicate the experiments: retrain the models
and obtain the corresponding performance figures.
Moreover, we carried out our implementation
and simulations on desktop computers. The POS
model has an estimated size of 700KB (Gong et
al., 2008). The PParse term of the DepSyn model
can be made as small as the feature model. We ex-
pect the optimized size of this model to be under
100KB in an embedded environment. The size of
the lexical variant of PLznk is comparable to the bi-
gram model. This could however be remedied by
using the probability of the action that constructed
this last link. The computational power required
by LIBLINEAR is certainly within the reach of
modern hand-held devices. However, a prototype
simulation with real hardware conditions would
</bodyText>
<page confidence="0.997114">
44
</page>
<bodyText confidence="0.999803181818182">
be needed to prove an implementability on mobile
devices.
Finally, a user might perceive subtle differences
in the presentation of the words compared with
that of popular commercial products. Gutowitz
(2003) noted the reluctance to single-tap input
methods because of their “unpredictable” behav-
ior. Introducing syntax-based disambiguation
could increase this perception. A next step would
be to carry out usability studies and assess this el-
ement.
</bodyText>
<sectionHeader confidence="0.997754" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99818732967033">
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency parsing.
In Proceedings of the Tenth Conference on Com-
putational Natural Language Learning (CoNLL-X),
pages 149–164, New York City.
Johan Carlberger and Viggo Kann. 1999. Implement-
ing an efficient part-of-speech tagger. Software –
Practice and Experience, 29(2):815–832.
Simon Corston-Oliver and Anthony Aue. 2006. De-
pendency parsing with reference to slovene, spanish
and swedish. In Proceedings of the Tenth Confer-
ence on Computational Natural Language Learning
(CoNLL-X), pages 196–200, New York City, June.
Jan Einarsson. 1976. Talbankens skriftspråkskonkor-
dans. Technical report, Lund University, Institutio-
nen för nordiska språk, Lund.
Eva Ejerhed and Gunnel Källgren. 1997. Stockholm
Umeå Corpus version 1.0, SUC 1.0.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871–1874.
Jun Gong, Peter Tarasewich, and I. Scott MacKenzie.
2008. Improved word list ordering for text entry on
ambiguous keypads. In NordiCHI ’08: Proceedings
of the 5th Nordic conference on Human-computer
interaction, pages 152–161, Lund, Sweden.
Dale L. Grover, Martin T. King, and Clifford A. Kush-
ler. 1998. Reduced keyboard disambiguating com-
puter. U.S. Patent no. 5,818,437.
Ebba Gustavii and Eva Pettersson. 2003. A Swedish
grammar for word prediction. Technical report, De-
partment of Linguistics, Uppsala University.
Howard Gutowitz. 2003. Barriers to adoption of
dictionary-based text-entry methods; a field study.
In Proceedings of the Workshop on Language Mod-
eling for Text Entry Systems (EACL 2003), pages 33–
41, Budapest.
Jan Haestrup. 2001. Communication terminal hav-
ing a predictive editor application. U.S. Patent no.
6,223,059.
Jon Hasselgren, Erik Montnemery, Pierre Nugues, and
Markus Svensson. 2003. HMS: A predictive text
entry method using bigrams. In Proceedings of
the Workshop on Language Modeling for Text Entry
Methods (EACL 2003), pages 43–49, Budapest.
Richard Johansson and Pierre Nugues. 2006. In-
vestigating multilingual dependency parsing. In
Proceedings of the Tenth Conference on Compu-
tational Natural Language Learning (CONLL-X),
pages 206–210, New York.
Richard Johansson and Pierre Nugues. 2007. Incre-
mental dependency parsing using online learning.
In Proceedings of the CoNLL Shared Task Session
of EMNLP-CoNLL, pages 1134–1138, Prague, June
28-30.
Jianhua Li and Graeme Hirst. 2005. Semantic knowl-
edge in word completion. In Assets ’05: Proceed-
ings of the 7th international ACM SIGACCESS con-
ference on Computers and accessibility, pages 121–
128, Baltimore.
I. Scott MacKenzie, Hedy Kober, Derek Smith, Terry
Jones, and Eugene Skepner. 2001. LetterWise:
Prefix-based disambiguation for mobile text input.
In 14th Annual ACM Symposium on User Interface
Software and Technology, Orlando, Florida.
I. Scott MacKenzie. 2002. KSPC (keystrokes per char-
acter) as a characteristic of text entry techniques. In
Proceedings of the Fourth International Symposium
on Human Computer Interaction with Mobile De-
vices, pages 195–210, Heidelberg, Germany.
Johannes Matiasek, Marco Baroni, and Harald Trost.
2002. FASTY – A multi-lingual approach to text
prediction. In ICCHP ’02: Proceedings of the
8th International Conference on Computers Helping
People with Special Needs, pages 243–250, London.
Johannes Matiasek. 2006. The language component
of the FASTY predictive typing system. In Karin
Harbusch, Kari-Jouko Raiha, and Kumiko Tanaka-
Ishii, editors, Efficient Text Entry, number 05382 in
Dagstuhl Seminar Proceedings, Dagstuhl, Germany.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of the 11th Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL), pages 81–88, Trento.
Jens Nilsson, Johan Hall, and Joakim Nivre. 2005.
MAMBA meets TIGER: Reconstructing a Swedish
treebank from antiquity. In Proceedings of the
NODALIDA Special Session on Treebanks, Joensuu,
Finland.
</reference>
<page confidence="0.993971">
45
</page>
<reference confidence="0.998709848484848">
Joakim Nivre, Johan Hall, Jens Nilsson, Gülsen
Eryigit, and Svetoslav Marinov. 2006. Labeled
pseudo-projective dependency parsing with support
vector machines. In Proceedings of the Tenth Con-
ference on Computational Natural Language Learn-
ing (CoNLL-X), pages 221–225, June.
Joakim Nivre, Johan Hall, Sandra Kübler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
915–932, Prague.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT), pages 149–160, Nancy.
Joakim Nivre. 2006. Inductive Dependency Parsing.
Springer, Dordrecht, The Netherlands.
Claude Elwood Shannon. 1951. Prediction and en-
tropy of printed English. The Bell System Technical
Journal, pages 50–64, January.
K. Sundarkantham and S. Mercy Shalinie. 2007. Word
predictor using natural language grammar induction
technique. Journal of Theoretical and Applied In-
formation Technology, 3:1–8.
Lucien Tesnière. 1966. Éléments de syntaxe struc-
turale. Klincksieck, Paris, 2e edition.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: Investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 562–571, Hawaii, October 25–27.
</reference>
<page confidence="0.992274">
46
</page>
<reference confidence="0.824946510638298">
Algorithm 1 Nivre’s algorithm.
1: Queue W
2: Stack nil
3: while -,Queue.isEmpty() do
4: features ExtractFeatures()
5: action guide.Predict(features)
6: if action = RightArc n canRightArc() then
7: RightArc()
8: else if action = LeftArc n canLeftArc() then
9: LeftArc
10: else if action= Reduce n canReduce() then
11: Reduce()
12: else
13: Shift()
14: end if
15: end while
16: return(A)
Algorithm 2 Reference parsing.
1: Queue W
2: Stack nil
3: while -,Queue.isEmpty() do
4: x ExtractFeatures()
5: if (Stack.peek(), Queue.get(0)) E A n canRightArc() then
6: t RightArc
7: else if (Queue.get(0), Stack.peek()) E A n canLeftArc() then
8: t LeftArc
9: else if ]w E Stack: (w, Queue.get(0)) E AV (Queue.get(0), w) E A) n canReduce() then
10: t Reduce
11: else
12: t Shift
13: end if
14: store training example (x, t)
15: end while
Algorithm 3 Beam parse.
1: Agenda.add(InititalParserState)
2: while -,done do
3: for parserState E Agenda do
4: Output.add(parserState.doLeftArc())
5: Output.add(parserState.doRightArc())
6: Output.add(parserState.doReduce())
7: Output.add(parserState.doShift())
8: end for
9: Sort(Output)
10: Clear(Agenda)
11: Take N best parse trees from Output and put in Agenda.
12: end while
13: Return best item in Agenda.
</reference>
<page confidence="0.89777">
47
</page>
<reference confidence="0.432926">
Configuration Scoring model DepSyn weights
</reference>
<equation confidence="0.958272285714286">
F1 baseline 1 x LM1 (Word frequencies) –
F2 0.9xLM1+0.1xPOS –
F3 0.7 x LM1 + 0.3 x SemA –
F4 0.6 x LM1 + 0.4 x DepSyn (0.3, 0.7, 0.0)
F5 0.6 x LM1 + 0.1 x POS + 0.3 x DepSyn (0.0 1.0 0.0)
F6 0.5 x LM1 + 0.2 x SemA + 0.3 x DepSyn (0.2 0.7 0.1)
F7 0.4 x LM1 + 0.1 x POS + 0.3 x DepSyn + 0.2 x SemA (0.2, 0.8, 0.0)
</equation>
<tableCaption confidence="0.996418">
Table 4: The different combinations of scoring models using frequency-based disambiguation as a base-
line. The DepSyn weight triples corresponds to (λ1, λ2, λ3) in Sect. 5.
</tableCaption>
<table confidence="0.99983025">
Configuration KSPC DA KSPC ERR DA ERR
F1 1.015559 94.15% 0.00% 0.00%
F2 1.014829 94.31% 4.69% 2.72%
F3 1.014902 94.36% 4.22% 3.62%
F4 1.014462 94.56% 7.05% 7.04%
F5 1.013625 94.75% 12.43% 10.28%
F6 1.014159 94.62% 9.00% 8.10%
F7 1.013438 94.86% 13.63% 12.16%
</table>
<tableCaption confidence="0.984824">
Table 5: Results for the disambiguation based on word frequencies together with the semantic and syn-
tactic models.
</tableCaption>
<table confidence="0.999092857142857">
Configuration Scoring model Bigram weights DepSyn weights
B1 1 x LM2 (Bigramfrequencies) (0.9, 0.1) –
B2 0.9 x LM2 + 0.1 x POS (0.8, 0.2) –
B3 0.95 x LM2 + 0.05 x SemA (0.8, 0.2)
B4 0.9 x LM2 + 0.1 x DepSyn (0.8, 0.2) (0.2, 0.8, 0.0)
B5 0.8 x LM2 + 0.1 x POS + 0.1 x SemA (0.8, 0.2)
B6 0.81 x LM2 + 0.08 x POS + 0.11 x DepSyn (0.8, 0.2) (0.2, 0.8, 0.0)
</table>
<tableCaption confidence="0.735626333333333">
Table 6: The different combinations of scoring models using bigram-based disambiguation as baseline.
In addition to the DepSyn weights, this table also shows the language model interpolation weights, β1
and β2 described in Sect. 2.2.
</tableCaption>
<table confidence="0.997899428571429">
Label KSPC DA KSPC ERR DA ERR
B1 1.012159254 95.48% 21.85% 22.81%
B2 1.011434213 95.75% 26.51% 27.41%
B3 1.011860573 95.50% 23.77% 23.20%
B4 1.011698693 95.62% 24.81% 25.19%
B5 1.011146932 95.80% 28.36% 28.23%
B6 1.010980592 95.91% 29.43% 30.09%
</table>
<tableCaption confidence="0.960882">
Table 7: Results for the disambiguation based on bigrams plus the semantic and syntactical models. The
error reduction rate is relative to the word frequency baseline.
</tableCaption>
<page confidence="0.998917">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.470886">
<title confidence="0.999823">Predictive Text Entry using Syntax and Semantics</title>
<author confidence="0.998521">Sebastian Ganslandt Jakob Jörwall Pierre Nugues</author>
<affiliation confidence="0.999967">Department of Computer Lund University</affiliation>
<address confidence="0.996924">S-221 00 Lund, Sweden</address>
<email confidence="0.788233">sebastian@ganslandt.nud02jjr@student.lth.se</email>
<abstract confidence="0.9915018125">Most cellular telephones use numeric keypads, where texting is supported by dictionaries and frequency models. Given a key sequence, the entry system recognizes the matching words and proposes a rankordered list of candidates. The ranking quality is instrumental to an effective entry. This paper describes a new method to enhance entry that combines syntax and language models. We first investigate components to improve the ranking step: language models and semantic relatedness. We then introduce a novel syntactic model to capture the word context, optimize ranking, and then reduce the number of keystrokes per character (KSPC) needed to write a text. We finally combine this model with the other components and we discuss the results. We show that our syntax-based model reaches an error reduction in KSPC of 12.4% on a Swedish corpus over a baseline using word frequencies. We also show that bigrams are superior to all the other models. However, bigrams have a memory footprint that is unfit for most devices. Nonetheless, bigrams can be further improved by the addition of syntactic models with an error reduction that reaches 29.4%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLLX shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X),</booktitle>
<pages>149--164</pages>
<location>New York City.</location>
<contexts>
<context position="12187" citStr="Buchholz and Marsi, 2006" startWordPosition="2055" endWordPosition="2058">ed a similar model in a predictive text application with a slight modification to the SemR function: C(stem(wi), stem(wj)) SemR(wi, wj) = C(stem(wj)) where the stem(w) function removes suffixes from words. We refined this model further and we replaced the stemming function with a real lemmatization. 4 Dependency Parsing Dependency syntax (Tesnière, 1966) has attracted a considerable interest in the recent years, spurred by the availability of data-driven parsers as well as annotated data in multiple languages including Arabic, Chinese, Czech, English, German, Japanese, Portuguese, or Spanish (Buchholz and Marsi, 2006; Nivre et al., 2007). We used this syntactic formalism because of its availability in many languages. 4.1 Parser Implementation There are two main classes of data-driven dependency parsers: graph-based (McDonald and Pereira, 2006) and transition-based (Nivre, 2003). We selected Nivre’s parser because of its implementation simplicity, small memory footprint, and linear time complexity. Parsing is always achieved in at most 2n − 1 actions, where n is the length of the sentence. Both types of parser can be combined, see Zhang and Clark (2008) for a discussion. Nivre’s parser is an extension to t</context>
<context position="17784" citStr="Buchholz and Marsi, 2006" startWordPosition="3013" endWordPosition="3016">classifiers are then computed over the graph that has already been built instead of considering the probability of a single transition. 41 4.5 Evaluation We evaluated our dependency parser separately from the rest of the application and Table 3 shows the results. We optimized our parameter selection for the unlabeled attachment score (UAS). This explains the relatively high difference with the labeled attachment score (LAS): about –8.6. Table 3 also shows the highest scores obtained on the same Talbanken corpus of Swedish text (Einarsson, 1976; Nilsson et al., 2005) in the CoNLL-X evaluation (Buchholz and Marsi, 2006): 89.58 for unlabeled attachments (CorstonOliver and Aue, 2006) and 84.58 for labeled attachments (Nivre et al., 2006). CoNLL-X systems were optimized for the LAS category. The figures we reached were about 1.10% below those reported in CONLL-X for the UAS category. However our results are not directly comparable as the parsers or the classifiers in CONLL-X have either a higher complexity or are more timeconsuming. We chose linear classifiers over kernel machines as it was essential to our application to run on mobile devices with limited resources in both CPU power and memory size. This paper</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLLX shared task on multilingual dependency parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X), pages 149–164, New York City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Carlberger</author>
<author>Viggo Kann</author>
</authors>
<title>Implementing an efficient part-of-speech tagger.</title>
<date>1999</date>
<journal>Software – Practice and Experience,</journal>
<volume>29</volume>
<issue>2</issue>
<contexts>
<context position="10383" citStr="Carlberger and Kann, 1999" startWordPosition="1743" endWordPosition="1746"> Part-of-speech models offer an interesting alternative to lexical models as the number of parts of speech does not exceed 100 tags in most languages. The possible number of bigrams is then at most 10,000 and much less in practice. We defined the part-of-speech model score, sPOS as P(t|t1, t2, ... , ti), where ti is the part of speech of wi and t, the part of speech of the candidate word w. We used a 5-gram approximation of this probability with a simple back-off model: P(t|ti−3, ... , ti) if C(ti−3, ..., ti) =6 0 P(t|ti−2, ... , ti) if C(ti−2, ..., ti) =6 0 sPOS = We used the Granska tagger (Carlberger and Kann, 1999) to carry out the part-of-speech annotation of the word sequence. 3 Semantic Affinity Because of their arbitrary length, language models miss possible relations between words that are semantically connected in a sentence but within a distance greater than one, two, or three words apart, the practical length of most n-grams models. Li and Hirst (2005) introduced the semantic relatedness between two words to measure such relations within a sentence. They defined it as C(wi, wj) SemR(wi,wj) = C(wi)C(wj), where C(wi, wj) is the number of times the words wi and wj co-occur in a sentence in the corp</context>
</contexts>
<marker>Carlberger, Kann, 1999</marker>
<rawString>Johan Carlberger and Viggo Kann. 1999. Implementing an efficient part-of-speech tagger. Software – Practice and Experience, 29(2):815–832.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Corston-Oliver</author>
<author>Anthony Aue</author>
</authors>
<title>Dependency parsing with reference to slovene, spanish and swedish.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X),</booktitle>
<pages>196--200</pages>
<location>New York City,</location>
<marker>Corston-Oliver, Aue, 2006</marker>
<rawString>Simon Corston-Oliver and Anthony Aue. 2006. Dependency parsing with reference to slovene, spanish and swedish. In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X), pages 196–200, New York City, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Einarsson</author>
</authors>
<title>Talbankens skriftspråkskonkordans.</title>
<date>1976</date>
<tech>Technical report,</tech>
<institution>Lund University, Institutionen</institution>
<location>Lund.</location>
<contexts>
<context position="14087" citStr="Einarsson, 1976" startWordPosition="2379" endWordPosition="2380">, Reduce}. We implemented this guide as a four-class classifier that uses features it extracts from the parser state. The features consist of words and their parts of speech in the stack, in the queue, and in the partial graph resulting from what has been parsed so far. The classifier is based on a linear logistic regression function that evaluates the transition probabilities from the features and predicts the next one. In the learning phase, we extracted a data set of feature vectors using the gold-standard parsing procedure (Algorithm 2) that we applied to Talbanken corpus of Swedish text (Einarsson, 1976; Nilsson et al., 2005). Each vector being labeled with one of the four possible transitions. We trained the classifiers using the LIBLINEAR implementation (Fan et al., 2008) of logistic regression. However, classes are not always separable using linear classifiers. We combined single features as pairs or triples. This emulates to some extent quadratic kernels used in support vector machines, while preserving the speed of the linear models. Table 2 shows the complete feature set to predict the transitions. A feature is defined by • A source: S for stack and Q for the queue; • An offset: 0 for </context>
<context position="17708" citStr="Einarsson, 1976" startWordPosition="3003" endWordPosition="3004">lgorithm (Johansson and Nugues, 2007; Zhang and Clark, 2008). The classifiers are then computed over the graph that has already been built instead of considering the probability of a single transition. 41 4.5 Evaluation We evaluated our dependency parser separately from the rest of the application and Table 3 shows the results. We optimized our parameter selection for the unlabeled attachment score (UAS). This explains the relatively high difference with the labeled attachment score (LAS): about –8.6. Table 3 also shows the highest scores obtained on the same Talbanken corpus of Swedish text (Einarsson, 1976; Nilsson et al., 2005) in the CoNLL-X evaluation (Buchholz and Marsi, 2006): 89.58 for unlabeled attachments (CorstonOliver and Aue, 2006) and 84.58 for labeled attachments (Nivre et al., 2006). CoNLL-X systems were optimized for the LAS category. The figures we reached were about 1.10% below those reported in CONLL-X for the UAS category. However our results are not directly comparable as the parsers or the classifiers in CONLL-X have either a higher complexity or are more timeconsuming. We chose linear classifiers over kernel machines as it was essential to our application to run on mobile </context>
</contexts>
<marker>Einarsson, 1976</marker>
<rawString>Jan Einarsson. 1976. Talbankens skriftspråkskonkordans. Technical report, Lund University, Institutionen för nordiska språk, Lund.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Ejerhed</author>
<author>Gunnel Källgren</author>
</authors>
<title>Stockholm Umeå Corpus version 1.0,</title>
<date>1997</date>
<journal>SUC</journal>
<volume>1</volume>
<contexts>
<context position="20808" citStr="Ejerhed and Källgren, 1997" startWordPosition="3537" endWordPosition="3540">), is the set of predicted words for the current key sequence. If the current word w has not been assigned a head yet, we default h(w) to the root of the graph and POS(h(w)) to the ROOT value. 6 Experiments and Results 6.1 Experimental Setup Figure 2 shows an overview of the three stages to produce and evaluate our models: training, CONLL-X LAS UAS 84.58 89.54 � j k=1 P(ak), 42 tuning, and testing. Ideally, we would have trained the classifiers on a corpus matching a text entry application. However, as there is no large available SMS corpus in Swedish, we used the Stockholm-Umeå corpus (SUC) (Ejerhed and Källgren, 1997). SUC is balanced and the largest available POS-tagged corpus in Swedish with more than 1 million words. We parsed the corpus and we divided it randomly into a training set (80%), a development set (10%), and a test set (10%). The training set was used to gather statistics on word n-grams, POS n-grams, collocations, lemma frequencies, dependent/head relations. We discarded hapaxes: relations and sequences occurring only once. We used lemmas instead of stems in the semantic relatedness score, SemR, because stemming is less appropriate in Swedish than in English. We used the development set to f</context>
</contexts>
<marker>Ejerhed, Källgren, 1997</marker>
<rawString>Eva Ejerhed and Gunnel Källgren. 1997. Stockholm Umeå Corpus version 1.0, SUC 1.0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="14261" citStr="Fan et al., 2008" startWordPosition="2405" endWordPosition="2408">h in the stack, in the queue, and in the partial graph resulting from what has been parsed so far. The classifier is based on a linear logistic regression function that evaluates the transition probabilities from the features and predicts the next one. In the learning phase, we extracted a data set of feature vectors using the gold-standard parsing procedure (Algorithm 2) that we applied to Talbanken corpus of Swedish text (Einarsson, 1976; Nilsson et al., 2005). Each vector being labeled with one of the four possible transitions. We trained the classifiers using the LIBLINEAR implementation (Fan et al., 2008) of logistic regression. However, classes are not always separable using linear classifiers. We combined single features as pairs or triples. This emulates to some extent quadratic kernels used in support vector machines, while preserving the speed of the linear models. Table 2 shows the complete feature set to predict the transitions. A feature is defined by • A source: S for stack and Q for the queue; • An offset: 0 for the top of the stack and first in the queue; 1 and 2 for levels down in the stack or to the right in the queue; , 40 Name Action Condition Initialization hnil, W, ∅i Terminat</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Gong</author>
<author>Peter Tarasewich</author>
<author>I Scott MacKenzie</author>
</authors>
<title>Improved word list ordering for text entry on ambiguous keypads.</title>
<date>2008</date>
<booktitle>In NordiCHI ’08: Proceedings of the 5th Nordic conference on Human-computer interaction,</booktitle>
<pages>152--161</pages>
<location>Lund,</location>
<contexts>
<context position="4728" citStr="Gong et al. (2008)" startWordPosition="794" endWordPosition="797">rd n-grams. In this case, the system is not restricted to the disambiguation or the prediction of the typed characters. It can complete words and even predict phrases. HMS (Hasselgren et al., 2003) is an example of this that uses word bigrams in Swedish. It reports a K5PC ranging from 0.8807 to 1.0108, depending on the type of text. eZiText2 is a commercial example of a word and phrase completion system. However, having a large lexicon of bigrams still exceeds the memory capacity of many mobile devices. Some systems use a combination of syntactic and semantic information to model the context. Gong et al. (2008) is a recent example that uses word frequencies, a part-of-speech language model, and a semantic relatedness metric. The part-of-speech language model acts as a lexical n-gram language model, but occupies much less memory since the vocabulary is restricted to the part-of-speech tagset. The semantic relatedness, modified from Li and Hirst (2005), is defined as the conditional probability of two stems appearing in the same context (the same sentence): 1www.t9.com 2www.zicorp.com/ezitext.htm The three components are combined linearly and their coefficients are adjusted using a development set. Se</context>
<context position="6800" citStr="Gong et al. (2008)" startWordPosition="1118" endWordPosition="1121"> In the same vein, Sundarkantham and Shalinie (2007) used grammar rules to discard infeasible grammatical constructions. The authors evaluated their system by giving it an incomplete sentence and seeing how often the system correctly guessed the next word (Shannon, 1951). They achieved better results than previously reported, although their system has not been used in the context of predictive text entry for mobile devices. 2 Predictive Text Entry Using Syntax We propose a new technique that makes use of a syntactic component to model the word context and improve the K5PC figure. It builds on Gong et al. (2008)’s system and combines a dependency grammar model with word frequencies, a part-ofspeech language model, and the semantic relatedness defined in Sect. 1.3. As far as we are aware, no predictive text entry system has yet used a datadriven syntactic model of the context. We used Swedish as our target language all over our experiments, but the results we obtained should be replicable in any other language. 5emR(w1Iw2) = C(stem(w1), stem(w2)) . C(w2) 38 2.1 Reranking Candidate Words The system consists of two components. The first one disambiguates the typed characters using a dictionary and produ</context>
<context position="11560" citStr="Gong et al. (2008)" startWordPosition="1961" endWordPosition="1964">and wj co-occur in a sentence in the corpus, { ... P(t), otherwise 39 and C(wi) is the count of word wi in the corpus. • Shift pushes the next input word onto the The relation is symmetrical, i.e. stack. C(wi, wj) = C(wj, wi). • Reduce pops the top of the stack with the condition that the corresponding word has a The estimated semantic affinity of a word w is head. defined as: � SemR(w, wj), • LeftArc adds an arc from the next input SemA(w|H) = word to the top of the stack and pops it. wj∈H where H is the context of the word w. In our case, H consists of words to the left of the current word. Gong et al. (2008) used a similar model in a predictive text application with a slight modification to the SemR function: C(stem(wi), stem(wj)) SemR(wi, wj) = C(stem(wj)) where the stem(w) function removes suffixes from words. We refined this model further and we replaced the stemming function with a real lemmatization. 4 Dependency Parsing Dependency syntax (Tesnière, 1966) has attracted a considerable interest in the recent years, spurred by the availability of data-driven parsers as well as annotated data in multiple languages including Arabic, Chinese, Czech, English, German, Japanese, Portuguese, or Spanis</context>
<context position="24808" citStr="Gong et al. (2008)" startWordPosition="4226" endWordPosition="4229"> on dependency grammars improves the prediction considerably. The DepSyn model is actually the most effective one when applied together with the frequency counts. Furthermore, the improvements from the POS, SemA, and DepSyn model are almost disjunct, as the combined model improvement matches the sum of their respective individual contributions. The 4.2% ERR observed when adding the SemA model is consistent with the result from X SEL X wES #w , 43 Figure 2: System architecture, where the set of scoring functions is S = {sLM, sSemA, sPOS, sDepSyn} �and the linear combination is = λs · s(w). s∈S Gong et al. (2008), where a 4.6% ERR was found. On the other hand, the POS model only contributed 4.7% ERR in our case, whereas Gong et al. (2008) observed 12.6%. One possible explanation for this is that they clustered related POS tags into 19 groups reducing the sparseness problem. By performing this grouping, we can effectively ignore morphological and lexical features that have no relevance, when deciding which word should come next. Other possible explanations include that our backoff model is not well suited for this problem or that the POS sequences are not an applicable model for Swedish. The bigram lan</context>
<context position="26893" citStr="Gong et al., 2008" startWordPosition="4582" endWordPosition="4585"> erroneous disambiguations as well as extra keystrokes by almost one third. 8 Further Work SMS texting, which is the target of our system, is more verbal than the genres gathered in the Stockholm-Umeå corpus. The language models of a final application would then change considerably from the ones we extracted from the SUC. A further work would be to collect a SMS corpus and replicate the experiments: retrain the models and obtain the corresponding performance figures. Moreover, we carried out our implementation and simulations on desktop computers. The POS model has an estimated size of 700KB (Gong et al., 2008). The PParse term of the DepSyn model can be made as small as the feature model. We expect the optimized size of this model to be under 100KB in an embedded environment. The size of the lexical variant of PLznk is comparable to the bigram model. This could however be remedied by using the probability of the action that constructed this last link. The computational power required by LIBLINEAR is certainly within the reach of modern hand-held devices. However, a prototype simulation with real hardware conditions would 44 be needed to prove an implementability on mobile devices. Finally, a user m</context>
</contexts>
<marker>Gong, Tarasewich, MacKenzie, 2008</marker>
<rawString>Jun Gong, Peter Tarasewich, and I. Scott MacKenzie. 2008. Improved word list ordering for text entry on ambiguous keypads. In NordiCHI ’08: Proceedings of the 5th Nordic conference on Human-computer interaction, pages 152–161, Lund, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dale L Grover</author>
<author>Martin T King</author>
<author>Clifford A Kushler</author>
</authors>
<title>Reduced keyboard disambiguating computer.</title>
<date>1998</date>
<journal>U.S. Patent</journal>
<volume>no.</volume>
<pages>5--818</pages>
<contexts>
<context position="3536" citStr="Grover et al., 1998" startWordPosition="585" endWordPosition="588">logies (IWPT), pages 37–48, Paris, October 2009. c�2009 Association for Computational Linguistics keystroke sequences produce a unique word, others are ambiguous and the system returns a list with all the candidates. The key sequence 8-4-3, for example, corresponds to at least three possible words: the, tie, and vie. The list of candidates is then sorted according to certain criteria, such as the word or character frequencies. If the word does not exist in the dictionary, the user has to fall back to multi-tap to enter it. The T91 commercial product is an example of a dictionary-based system (Grover et al., 1998). LetterWise (MacKenzie et al., 2001) is a technique that uses letter trigrams and their frequencies to predict the next character. For example, pressing the key 3 after the letter bigram ‘th’ will select ‘e’, because the trigram ‘the’ is far more frequent than ‘thd’ or ‘thf’ in English. When the system proposes a wrong letter, the user can access the next most likely one by pressing a next-key. LetterWise does not need a dictionary and has a K5PC of 1.1500 (MacKenzie, 2002). 1.3 Modeling the Context Language modeling can extend the context from letter sequences to word n-grams. In this case, </context>
</contexts>
<marker>Grover, King, Kushler, 1998</marker>
<rawString>Dale L. Grover, Martin T. King, and Clifford A. Kushler. 1998. Reduced keyboard disambiguating computer. U.S. Patent no. 5,818,437.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ebba Gustavii</author>
<author>Eva Pettersson</author>
</authors>
<title>A Swedish grammar for word prediction.</title>
<date>2003</date>
<tech>Technical report,</tech>
<institution>Department of Linguistics, Uppsala University.</institution>
<contexts>
<context position="5873" citStr="Gustavii and Pettersson (2003)" startWordPosition="968" endWordPosition="971">are combined linearly and their coefficients are adjusted using a development set. Setting 1 as the limit of the K5PC figure, Gong et al. (2008) reported an error reduction over the word frequency baseline of 4.6% for the semantic model, 12.6% for the part-of-speech language model, and 15.8% for the combination of both. 1.4 Syntax in Predictive Text Beyond part-of-speech language modeling, there are few examples of systems using syntax in predictive text entry. Matiasek et al. (2002) describes a predictive text environment aimed at disabled persons, which originally relied on language models. Gustavii and Pettersson (2003) added a syntactic component to it based on grammar rules. The rules corresponded to common grammatical errors and were used to rerank the list of candidate words. The evaluation results were disappointing and the syntactic component was not added because of the large overhead it introduced (Matiasek, 2006). In the same vein, Sundarkantham and Shalinie (2007) used grammar rules to discard infeasible grammatical constructions. The authors evaluated their system by giving it an incomplete sentence and seeing how often the system correctly guessed the next word (Shannon, 1951). They achieved bett</context>
</contexts>
<marker>Gustavii, Pettersson, 2003</marker>
<rawString>Ebba Gustavii and Eva Pettersson. 2003. A Swedish grammar for word prediction. Technical report, Department of Linguistics, Uppsala University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Howard Gutowitz</author>
</authors>
<title>Barriers to adoption of dictionary-based text-entry methods; a field study.</title>
<date>2003</date>
<booktitle>In Proceedings of the Workshop on Language Modeling for Text Entry Systems (EACL</booktitle>
<pages>33--41</pages>
<location>Budapest.</location>
<marker>Gutowitz, 2003</marker>
<rawString>Howard Gutowitz. 2003. Barriers to adoption of dictionary-based text-entry methods; a field study. In Proceedings of the Workshop on Language Modeling for Text Entry Systems (EACL 2003), pages 33– 41, Budapest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Haestrup</author>
</authors>
<title>Communication terminal having a predictive editor application.</title>
<date>2001</date>
<pages>6--223</pages>
<editor>U.S. Patent no.</editor>
<contexts>
<context position="2834" citStr="Haestrup, 2001" startWordPosition="472" endWordPosition="473">tter is selected. The key sequence 8-4-4-3-3, for example, leads to the word the. Multi-tap is easy to implement and no dictionary is needed. At the same time, it is slow and tedious for the user, notably when two consecutive characters are placed on the same key. 1.2 Single Tap with Predictive Text Single tap with predictive text requires only one key press to enter a character. Given a keystroke sequence, the system proposes words using a dictionary or language modeling techniques. Dictionary-based techniques search the words matching the key sequence in a list that is stored by the system (Haestrup, 2001). While some 37 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 37–48, Paris, October 2009. c�2009 Association for Computational Linguistics keystroke sequences produce a unique word, others are ambiguous and the system returns a list with all the candidates. The key sequence 8-4-3, for example, corresponds to at least three possible words: the, tie, and vie. The list of candidates is then sorted according to certain criteria, such as the word or character frequencies. If the word does not exist in the dictionary, the user has to fall back to multi-tap to</context>
</contexts>
<marker>Haestrup, 2001</marker>
<rawString>Jan Haestrup. 2001. Communication terminal having a predictive editor application. U.S. Patent no. 6,223,059.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Hasselgren</author>
<author>Erik Montnemery</author>
<author>Pierre Nugues</author>
<author>Markus Svensson</author>
</authors>
<title>HMS: A predictive text entry method using bigrams.</title>
<date>2003</date>
<booktitle>In Proceedings of the Workshop on Language Modeling for Text Entry Methods (EACL</booktitle>
<pages>43--49</pages>
<location>Budapest.</location>
<contexts>
<context position="4307" citStr="Hasselgren et al., 2003" startWordPosition="719" endWordPosition="723">essing the key 3 after the letter bigram ‘th’ will select ‘e’, because the trigram ‘the’ is far more frequent than ‘thd’ or ‘thf’ in English. When the system proposes a wrong letter, the user can access the next most likely one by pressing a next-key. LetterWise does not need a dictionary and has a K5PC of 1.1500 (MacKenzie, 2002). 1.3 Modeling the Context Language modeling can extend the context from letter sequences to word n-grams. In this case, the system is not restricted to the disambiguation or the prediction of the typed characters. It can complete words and even predict phrases. HMS (Hasselgren et al., 2003) is an example of this that uses word bigrams in Swedish. It reports a K5PC ranging from 0.8807 to 1.0108, depending on the type of text. eZiText2 is a commercial example of a word and phrase completion system. However, having a large lexicon of bigrams still exceeds the memory capacity of many mobile devices. Some systems use a combination of syntactic and semantic information to model the context. Gong et al. (2008) is a recent example that uses word frequencies, a part-of-speech language model, and a semantic relatedness metric. The part-of-speech language model acts as a lexical n-gram lan</context>
</contexts>
<marker>Hasselgren, Montnemery, Nugues, Svensson, 2003</marker>
<rawString>Jon Hasselgren, Erik Montnemery, Pierre Nugues, and Markus Svensson. 2003. HMS: A predictive text entry method using bigrams. In Proceedings of the Workshop on Language Modeling for Text Entry Methods (EACL 2003), pages 43–49, Budapest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Investigating multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning (CONLL-X),</booktitle>
<pages>206--210</pages>
<location>New York.</location>
<contexts>
<context position="16710" citStr="Johansson and Nugues, 2006" startWordPosition="2837" endWordPosition="2840">dency graph G as P(Am1|W1n) and we estimated the probability of G as the product of the transition probabilities, so that PParse(G|W1n) = P(Am1|W1n ) = rim k=1 P(ak|Ak−1 1 , Wφ(k−1) 1 ), where ak is member of the set {LeftArc, RightArc, Shift, Reduce} and O(k) corresponds to the index of the current word at transition k. We finally approximated the term Ak−1 1 , Wφ(k−1) to the feature set and com1 puted probability estimates using the logistic regression output. 4.4 Beam Search We extended Nivre’s parser with a beam search to mitigate error propagation that occurs with a deterministic parser (Johansson and Nugues, 2006). We maintained N parser states in parallel and we applied all the possible transitions to each state. We scored each transition action and we ranked the states with the product of the action’s probabilities leading to this state. Algorithm 3 outlines beam search with a diameter of N. An alternative to training parser transitions using local features is to use an online learning algorithm (Johansson and Nugues, 2007; Zhang and Clark, 2008). The classifiers are then computed over the graph that has already been built instead of considering the probability of a single transition. 41 4.5 Evaluati</context>
</contexts>
<marker>Johansson, Nugues, 2006</marker>
<rawString>Richard Johansson and Pierre Nugues. 2006. Investigating multilingual dependency parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning (CONLL-X), pages 206–210, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Incremental dependency parsing using online learning.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL,</booktitle>
<pages>1134--1138</pages>
<location>Prague,</location>
<contexts>
<context position="17129" citStr="Johansson and Nugues, 2007" startWordPosition="2908" endWordPosition="2911">stimates using the logistic regression output. 4.4 Beam Search We extended Nivre’s parser with a beam search to mitigate error propagation that occurs with a deterministic parser (Johansson and Nugues, 2006). We maintained N parser states in parallel and we applied all the possible transitions to each state. We scored each transition action and we ranked the states with the product of the action’s probabilities leading to this state. Algorithm 3 outlines beam search with a diameter of N. An alternative to training parser transitions using local features is to use an online learning algorithm (Johansson and Nugues, 2007; Zhang and Clark, 2008). The classifiers are then computed over the graph that has already been built instead of considering the probability of a single transition. 41 4.5 Evaluation We evaluated our dependency parser separately from the rest of the application and Table 3 shows the results. We optimized our parameter selection for the unlabeled attachment score (UAS). This explains the relatively high difference with the labeled attachment score (LAS): about –8.6. Table 3 also shows the highest scores obtained on the same Talbanken corpus of Swedish text (Einarsson, 1976; Nilsson et al., 200</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Incremental dependency parsing using online learning. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL, pages 1134–1138, Prague, June 28-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianhua Li</author>
<author>Graeme Hirst</author>
</authors>
<title>Semantic knowledge in word completion.</title>
<date>2005</date>
<booktitle>In Assets ’05: Proceedings of the 7th international ACM SIGACCESS conference on Computers and accessibility,</booktitle>
<pages>121--128</pages>
<location>Baltimore.</location>
<contexts>
<context position="5074" citStr="Li and Hirst (2005)" startWordPosition="845" endWordPosition="848">s a commercial example of a word and phrase completion system. However, having a large lexicon of bigrams still exceeds the memory capacity of many mobile devices. Some systems use a combination of syntactic and semantic information to model the context. Gong et al. (2008) is a recent example that uses word frequencies, a part-of-speech language model, and a semantic relatedness metric. The part-of-speech language model acts as a lexical n-gram language model, but occupies much less memory since the vocabulary is restricted to the part-of-speech tagset. The semantic relatedness, modified from Li and Hirst (2005), is defined as the conditional probability of two stems appearing in the same context (the same sentence): 1www.t9.com 2www.zicorp.com/ezitext.htm The three components are combined linearly and their coefficients are adjusted using a development set. Setting 1 as the limit of the K5PC figure, Gong et al. (2008) reported an error reduction over the word frequency baseline of 4.6% for the semantic model, 12.6% for the part-of-speech language model, and 15.8% for the combination of both. 1.4 Syntax in Predictive Text Beyond part-of-speech language modeling, there are few examples of systems usin</context>
<context position="10735" citStr="Li and Hirst (2005)" startWordPosition="1801" endWordPosition="1804">of speech of the candidate word w. We used a 5-gram approximation of this probability with a simple back-off model: P(t|ti−3, ... , ti) if C(ti−3, ..., ti) =6 0 P(t|ti−2, ... , ti) if C(ti−2, ..., ti) =6 0 sPOS = We used the Granska tagger (Carlberger and Kann, 1999) to carry out the part-of-speech annotation of the word sequence. 3 Semantic Affinity Because of their arbitrary length, language models miss possible relations between words that are semantically connected in a sentence but within a distance greater than one, two, or three words apart, the practical length of most n-grams models. Li and Hirst (2005) introduced the semantic relatedness between two words to measure such relations within a sentence. They defined it as C(wi, wj) SemR(wi,wj) = C(wi)C(wj), where C(wi, wj) is the number of times the words wi and wj co-occur in a sentence in the corpus, { ... P(t), otherwise 39 and C(wi) is the count of word wi in the corpus. • Shift pushes the next input word onto the The relation is symmetrical, i.e. stack. C(wi, wj) = C(wj, wi). • Reduce pops the top of the stack with the condition that the corresponding word has a The estimated semantic affinity of a word w is head. defined as: � SemR(w, wj)</context>
</contexts>
<marker>Li, Hirst, 2005</marker>
<rawString>Jianhua Li and Graeme Hirst. 2005. Semantic knowledge in word completion. In Assets ’05: Proceedings of the 7th international ACM SIGACCESS conference on Computers and accessibility, pages 121– 128, Baltimore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Scott MacKenzie</author>
<author>Hedy Kober</author>
<author>Derek Smith</author>
<author>Terry Jones</author>
<author>Eugene Skepner</author>
</authors>
<title>LetterWise: Prefix-based disambiguation for mobile text input.</title>
<date>2001</date>
<booktitle>In 14th Annual ACM Symposium on User Interface Software and Technology,</booktitle>
<location>Orlando, Florida.</location>
<contexts>
<context position="3573" citStr="MacKenzie et al., 2001" startWordPosition="590" endWordPosition="593"> October 2009. c�2009 Association for Computational Linguistics keystroke sequences produce a unique word, others are ambiguous and the system returns a list with all the candidates. The key sequence 8-4-3, for example, corresponds to at least three possible words: the, tie, and vie. The list of candidates is then sorted according to certain criteria, such as the word or character frequencies. If the word does not exist in the dictionary, the user has to fall back to multi-tap to enter it. The T91 commercial product is an example of a dictionary-based system (Grover et al., 1998). LetterWise (MacKenzie et al., 2001) is a technique that uses letter trigrams and their frequencies to predict the next character. For example, pressing the key 3 after the letter bigram ‘th’ will select ‘e’, because the trigram ‘the’ is far more frequent than ‘thd’ or ‘thf’ in English. When the system proposes a wrong letter, the user can access the next most likely one by pressing a next-key. LetterWise does not need a dictionary and has a K5PC of 1.1500 (MacKenzie, 2002). 1.3 Modeling the Context Language modeling can extend the context from letter sequences to word n-grams. In this case, the system is not restricted to the d</context>
</contexts>
<marker>MacKenzie, Kober, Smith, Jones, Skepner, 2001</marker>
<rawString>I. Scott MacKenzie, Hedy Kober, Derek Smith, Terry Jones, and Eugene Skepner. 2001. LetterWise: Prefix-based disambiguation for mobile text input. In 14th Annual ACM Symposium on User Interface Software and Technology, Orlando, Florida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Scott MacKenzie</author>
</authors>
<title>KSPC (keystrokes per character) as a characteristic of text entry techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the Fourth International Symposium on Human Computer Interaction with Mobile Devices,</booktitle>
<pages>195--210</pages>
<location>Heidelberg, Germany.</location>
<contexts>
<context position="4015" citStr="MacKenzie, 2002" startWordPosition="673" endWordPosition="674">er has to fall back to multi-tap to enter it. The T91 commercial product is an example of a dictionary-based system (Grover et al., 1998). LetterWise (MacKenzie et al., 2001) is a technique that uses letter trigrams and their frequencies to predict the next character. For example, pressing the key 3 after the letter bigram ‘th’ will select ‘e’, because the trigram ‘the’ is far more frequent than ‘thd’ or ‘thf’ in English. When the system proposes a wrong letter, the user can access the next most likely one by pressing a next-key. LetterWise does not need a dictionary and has a K5PC of 1.1500 (MacKenzie, 2002). 1.3 Modeling the Context Language modeling can extend the context from letter sequences to word n-grams. In this case, the system is not restricted to the disambiguation or the prediction of the typed characters. It can complete words and even predict phrases. HMS (Hasselgren et al., 2003) is an example of this that uses word bigrams in Swedish. It reports a K5PC ranging from 0.8807 to 1.0108, depending on the type of text. eZiText2 is a commercial example of a word and phrase completion system. However, having a large lexicon of bigrams still exceeds the memory capacity of many mobile devic</context>
<context position="22097" citStr="MacKenzie (2002)" startWordPosition="3749" endWordPosition="3750">PC. We ran an exhaustive search using all possible linear combinations with increments of 0.1, except for two functions, where this was too coarse. We used 0.01 then. We applied the resulting linear combinations of scoring functions to the test set. We first compared the frequency-based disambiguation acting as a baseline to linear combinations involving or not involving syntax, but always excluding bigrams. Table 4 shows the most significant combinations. We then compared a set of other combinations with the bigram model. They are shown in Table 6. 6.2 Metrics We redefined the KSPC metric of MacKenzie (2002), since the number of characters needed to input a word is now dependent on the word’s left context in the sentence. Let S = (wi, ... , wn) E L be a sentence in the test corpus. The KSPC for the test corpus then becomes P KSPC = SEL PwES KS(w|LContext(w, S)) P PwES Chars(w) SEL where KS(w|LContext) is the number of key strokes needed to enter a word in a given context, LContext(w, S) is the left context of w in S, and Chars(w) is the number of characters in w. Another performance measure is the disambiguation accuracy (DA), which is the percentage of words that are correctly disambiguated afte</context>
</contexts>
<marker>MacKenzie, 2002</marker>
<rawString>I. Scott MacKenzie. 2002. KSPC (keystrokes per character) as a characteristic of text entry techniques. In Proceedings of the Fourth International Symposium on Human Computer Interaction with Mobile Devices, pages 195–210, Heidelberg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Matiasek</author>
<author>Marco Baroni</author>
<author>Harald Trost</author>
</authors>
<title>FASTY – A multi-lingual approach to text prediction.</title>
<date>2002</date>
<booktitle>In ICCHP ’02: Proceedings of the 8th International Conference on Computers Helping People with Special Needs,</booktitle>
<pages>243--250</pages>
<location>London.</location>
<contexts>
<context position="5731" citStr="Matiasek et al. (2002)" startWordPosition="948" endWordPosition="951">bability of two stems appearing in the same context (the same sentence): 1www.t9.com 2www.zicorp.com/ezitext.htm The three components are combined linearly and their coefficients are adjusted using a development set. Setting 1 as the limit of the K5PC figure, Gong et al. (2008) reported an error reduction over the word frequency baseline of 4.6% for the semantic model, 12.6% for the part-of-speech language model, and 15.8% for the combination of both. 1.4 Syntax in Predictive Text Beyond part-of-speech language modeling, there are few examples of systems using syntax in predictive text entry. Matiasek et al. (2002) describes a predictive text environment aimed at disabled persons, which originally relied on language models. Gustavii and Pettersson (2003) added a syntactic component to it based on grammar rules. The rules corresponded to common grammatical errors and were used to rerank the list of candidate words. The evaluation results were disappointing and the syntactic component was not added because of the large overhead it introduced (Matiasek, 2006). In the same vein, Sundarkantham and Shalinie (2007) used grammar rules to discard infeasible grammatical constructions. The authors evaluated their </context>
</contexts>
<marker>Matiasek, Baroni, Trost, 2002</marker>
<rawString>Johannes Matiasek, Marco Baroni, and Harald Trost. 2002. FASTY – A multi-lingual approach to text prediction. In ICCHP ’02: Proceedings of the 8th International Conference on Computers Helping People with Special Needs, pages 243–250, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Matiasek</author>
</authors>
<title>The language component of the FASTY predictive typing system.</title>
<date>2006</date>
<booktitle>In Karin Harbusch, Kari-Jouko Raiha, and Kumiko TanakaIshii, editors, Efficient Text Entry, number 05382 in Dagstuhl Seminar Proceedings,</booktitle>
<location>Dagstuhl, Germany.</location>
<contexts>
<context position="6181" citStr="Matiasek, 2006" startWordPosition="1020" endWordPosition="1022">.4 Syntax in Predictive Text Beyond part-of-speech language modeling, there are few examples of systems using syntax in predictive text entry. Matiasek et al. (2002) describes a predictive text environment aimed at disabled persons, which originally relied on language models. Gustavii and Pettersson (2003) added a syntactic component to it based on grammar rules. The rules corresponded to common grammatical errors and were used to rerank the list of candidate words. The evaluation results were disappointing and the syntactic component was not added because of the large overhead it introduced (Matiasek, 2006). In the same vein, Sundarkantham and Shalinie (2007) used grammar rules to discard infeasible grammatical constructions. The authors evaluated their system by giving it an incomplete sentence and seeing how often the system correctly guessed the next word (Shannon, 1951). They achieved better results than previously reported, although their system has not been used in the context of predictive text entry for mobile devices. 2 Predictive Text Entry Using Syntax We propose a new technique that makes use of a syntactic component to model the word context and improve the K5PC figure. It builds on</context>
</contexts>
<marker>Matiasek, 2006</marker>
<rawString>Johannes Matiasek. 2006. The language component of the FASTY predictive typing system. In Karin Harbusch, Kari-Jouko Raiha, and Kumiko TanakaIshii, editors, Efficient Text Entry, number 05382 in Dagstuhl Seminar Proceedings, Dagstuhl, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<pages>81--88</pages>
<location>Trento.</location>
<contexts>
<context position="12418" citStr="McDonald and Pereira, 2006" startWordPosition="2089" endWordPosition="2092">further and we replaced the stemming function with a real lemmatization. 4 Dependency Parsing Dependency syntax (Tesnière, 1966) has attracted a considerable interest in the recent years, spurred by the availability of data-driven parsers as well as annotated data in multiple languages including Arabic, Chinese, Czech, English, German, Japanese, Portuguese, or Spanish (Buchholz and Marsi, 2006; Nivre et al., 2007). We used this syntactic formalism because of its availability in many languages. 4.1 Parser Implementation There are two main classes of data-driven dependency parsers: graph-based (McDonald and Pereira, 2006) and transition-based (Nivre, 2003). We selected Nivre’s parser because of its implementation simplicity, small memory footprint, and linear time complexity. Parsing is always achieved in at most 2n − 1 actions, where n is the length of the sentence. Both types of parser can be combined, see Zhang and Clark (2008) for a discussion. Nivre’s parser is an extension to the shift– reduce algorithm that creates a projective and acyclic graph. It uses a stack, a list of input words, and builds a set of arcs representing the graph of dependencies. The parser uses two operations in addition to shift an</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 81–88, Trento.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jens Nilsson</author>
<author>Johan Hall</author>
<author>Joakim Nivre</author>
</authors>
<title>MAMBA meets TIGER: Reconstructing a Swedish treebank from antiquity.</title>
<date>2005</date>
<booktitle>In Proceedings of the NODALIDA Special Session on Treebanks,</booktitle>
<location>Joensuu, Finland.</location>
<contexts>
<context position="14110" citStr="Nilsson et al., 2005" startWordPosition="2381" endWordPosition="2384">lemented this guide as a four-class classifier that uses features it extracts from the parser state. The features consist of words and their parts of speech in the stack, in the queue, and in the partial graph resulting from what has been parsed so far. The classifier is based on a linear logistic regression function that evaluates the transition probabilities from the features and predicts the next one. In the learning phase, we extracted a data set of feature vectors using the gold-standard parsing procedure (Algorithm 2) that we applied to Talbanken corpus of Swedish text (Einarsson, 1976; Nilsson et al., 2005). Each vector being labeled with one of the four possible transitions. We trained the classifiers using the LIBLINEAR implementation (Fan et al., 2008) of logistic regression. However, classes are not always separable using linear classifiers. We combined single features as pairs or triples. This emulates to some extent quadratic kernels used in support vector machines, while preserving the speed of the linear models. Table 2 shows the complete feature set to predict the transitions. A feature is defined by • A source: S for stack and Q for the queue; • An offset: 0 for the top of the stack an</context>
<context position="17731" citStr="Nilsson et al., 2005" startWordPosition="3005" endWordPosition="3008">on and Nugues, 2007; Zhang and Clark, 2008). The classifiers are then computed over the graph that has already been built instead of considering the probability of a single transition. 41 4.5 Evaluation We evaluated our dependency parser separately from the rest of the application and Table 3 shows the results. We optimized our parameter selection for the unlabeled attachment score (UAS). This explains the relatively high difference with the labeled attachment score (LAS): about –8.6. Table 3 also shows the highest scores obtained on the same Talbanken corpus of Swedish text (Einarsson, 1976; Nilsson et al., 2005) in the CoNLL-X evaluation (Buchholz and Marsi, 2006): 89.58 for unlabeled attachments (CorstonOliver and Aue, 2006) and 84.58 for labeled attachments (Nivre et al., 2006). CoNLL-X systems were optimized for the LAS category. The figures we reached were about 1.10% below those reported in CONLL-X for the UAS category. However our results are not directly comparable as the parsers or the classifiers in CONLL-X have either a higher complexity or are more timeconsuming. We chose linear classifiers over kernel machines as it was essential to our application to run on mobile devices with limited re</context>
</contexts>
<marker>Nilsson, Hall, Nivre, 2005</marker>
<rawString>Jens Nilsson, Johan Hall, and Joakim Nivre. 2005. MAMBA meets TIGER: Reconstructing a Swedish treebank from antiquity. In Proceedings of the NODALIDA Special Session on Treebanks, Joensuu, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
<author>Gülsen Eryigit</author>
<author>Svetoslav Marinov</author>
</authors>
<title>Labeled pseudo-projective dependency parsing with support vector machines.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X),</booktitle>
<pages>221--225</pages>
<contexts>
<context position="17902" citStr="Nivre et al., 2006" startWordPosition="3032" endWordPosition="3035">transition. 41 4.5 Evaluation We evaluated our dependency parser separately from the rest of the application and Table 3 shows the results. We optimized our parameter selection for the unlabeled attachment score (UAS). This explains the relatively high difference with the labeled attachment score (LAS): about –8.6. Table 3 also shows the highest scores obtained on the same Talbanken corpus of Swedish text (Einarsson, 1976; Nilsson et al., 2005) in the CoNLL-X evaluation (Buchholz and Marsi, 2006): 89.58 for unlabeled attachments (CorstonOliver and Aue, 2006) and 84.58 for labeled attachments (Nivre et al., 2006). CoNLL-X systems were optimized for the LAS category. The figures we reached were about 1.10% below those reported in CONLL-X for the UAS category. However our results are not directly comparable as the parsers or the classifiers in CONLL-X have either a higher complexity or are more timeconsuming. We chose linear classifiers over kernel machines as it was essential to our application to run on mobile devices with limited resources in both CPU power and memory size. This paper Beam width LAS UAS 1 79.45 88.05 2 79.76 88.41 4 79.75 88.40 8 79.77 88.41 16 79.78 88.42 32 79.77 88.41 64 79.79 88.</context>
</contexts>
<marker>Nivre, Hall, Nilsson, Eryigit, Marinov, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Gülsen Eryigit, and Svetoslav Marinov. 2006. Labeled pseudo-projective dependency parsing with support vector machines. In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X), pages 221–225, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra Kübler</author>
<author>Ryan McDonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>The CoNLL</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007,</booktitle>
<pages>915--932</pages>
<location>Prague.</location>
<contexts>
<context position="12208" citStr="Nivre et al., 2007" startWordPosition="2059" endWordPosition="2062">edictive text application with a slight modification to the SemR function: C(stem(wi), stem(wj)) SemR(wi, wj) = C(stem(wj)) where the stem(w) function removes suffixes from words. We refined this model further and we replaced the stemming function with a real lemmatization. 4 Dependency Parsing Dependency syntax (Tesnière, 1966) has attracted a considerable interest in the recent years, spurred by the availability of data-driven parsers as well as annotated data in multiple languages including Arabic, Chinese, Czech, English, German, Japanese, Portuguese, or Spanish (Buchholz and Marsi, 2006; Nivre et al., 2007). We used this syntactic formalism because of its availability in many languages. 4.1 Parser Implementation There are two main classes of data-driven dependency parsers: graph-based (McDonald and Pereira, 2006) and transition-based (Nivre, 2003). We selected Nivre’s parser because of its implementation simplicity, small memory footprint, and linear time complexity. Parsing is always achieved in at most 2n − 1 actions, where n is the length of the sentence. Both types of parser can be combined, see Zhang and Clark (2008) for a discussion. Nivre’s parser is an extension to the shift– reduce algo</context>
</contexts>
<marker>Nivre, Hall, Kübler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Sandra Kübler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 915–932, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>An efficient algorithm for projective dependency parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT),</booktitle>
<pages>149--160</pages>
<location>Nancy.</location>
<contexts>
<context position="12453" citStr="Nivre, 2003" startWordPosition="2095" endWordPosition="2096"> a real lemmatization. 4 Dependency Parsing Dependency syntax (Tesnière, 1966) has attracted a considerable interest in the recent years, spurred by the availability of data-driven parsers as well as annotated data in multiple languages including Arabic, Chinese, Czech, English, German, Japanese, Portuguese, or Spanish (Buchholz and Marsi, 2006; Nivre et al., 2007). We used this syntactic formalism because of its availability in many languages. 4.1 Parser Implementation There are two main classes of data-driven dependency parsers: graph-based (McDonald and Pereira, 2006) and transition-based (Nivre, 2003). We selected Nivre’s parser because of its implementation simplicity, small memory footprint, and linear time complexity. Parsing is always achieved in at most 2n − 1 actions, where n is the length of the sentence. Both types of parser can be combined, see Zhang and Clark (2008) for a discussion. Nivre’s parser is an extension to the shift– reduce algorithm that creates a projective and acyclic graph. It uses a stack, a list of input words, and builds a set of arcs representing the graph of dependencies. The parser uses two operations in addition to shift and reduce, left-arc and right-arc: •</context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT), pages 149–160, Nancy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Inductive Dependency Parsing.</title>
<date>2006</date>
<publisher>Springer,</publisher>
<location>Dordrecht, The Netherlands.</location>
<contexts>
<context position="15885" citStr="Nivre (2006)" startWordPosition="2694" endWordPosition="2695">and n&apos;&apos; are lexical tokens. The pair hn&apos;, ni represents an arc from the head n&apos; to the dependent n. • Possible applications of the function head, H, leftmost child, LC, or righmost child, RC; • The value: word, w, or POS tag, t, at the specified position. Queue Q0w Q1w Q0t Q1t Q0tQ0w Q0tQ1t Q1wQ1t Q0tQ1tQ2t Q0wQ1tQ2t Stack S0t S0w S0tS0w S0tS1t Stack/Queue S0wQ0w Q0tS0t Q1tS0t Q0tS1t Q1tS1t S0tQ0tQ1t S0tQ0wQ0t Partial Graph S0HtS0tQ0t Q0LCtS0tQ0t Q0LCtS0tQ0w S0RCtS0tQ0t S0RCtS0tQ0w Table 2: Feature model for predicting parser actions with combined features. 4.3 Calculating Graph Probabilities Nivre (2006) showed that every terminating transition sequence Am1 = (a1, ..., am) applied to a sentence W1n = (w1, ..., wn) defines exactly one parse tree G. We approximated the probability P(G|W1n) of a dependency graph G as P(Am1|W1n) and we estimated the probability of G as the product of the transition probabilities, so that PParse(G|W1n) = P(Am1|W1n ) = rim k=1 P(ak|Ak−1 1 , Wφ(k−1) 1 ), where ak is member of the set {LeftArc, RightArc, Shift, Reduce} and O(k) corresponds to the index of the current word at transition k. We finally approximated the term Ak−1 1 , Wφ(k−1) to the feature set and com1 p</context>
</contexts>
<marker>Nivre, 2006</marker>
<rawString>Joakim Nivre. 2006. Inductive Dependency Parsing. Springer, Dordrecht, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claude Elwood Shannon</author>
</authors>
<title>Prediction and entropy of printed English.</title>
<date>1951</date>
<journal>The Bell System Technical Journal,</journal>
<pages>50--64</pages>
<contexts>
<context position="6453" citStr="Shannon, 1951" startWordPosition="1061" endWordPosition="1062">ls. Gustavii and Pettersson (2003) added a syntactic component to it based on grammar rules. The rules corresponded to common grammatical errors and were used to rerank the list of candidate words. The evaluation results were disappointing and the syntactic component was not added because of the large overhead it introduced (Matiasek, 2006). In the same vein, Sundarkantham and Shalinie (2007) used grammar rules to discard infeasible grammatical constructions. The authors evaluated their system by giving it an incomplete sentence and seeing how often the system correctly guessed the next word (Shannon, 1951). They achieved better results than previously reported, although their system has not been used in the context of predictive text entry for mobile devices. 2 Predictive Text Entry Using Syntax We propose a new technique that makes use of a syntactic component to model the word context and improve the K5PC figure. It builds on Gong et al. (2008)’s system and combines a dependency grammar model with word frequencies, a part-ofspeech language model, and the semantic relatedness defined in Sect. 1.3. As far as we are aware, no predictive text entry system has yet used a datadriven syntactic model</context>
</contexts>
<marker>Shannon, 1951</marker>
<rawString>Claude Elwood Shannon. 1951. Prediction and entropy of printed English. The Bell System Technical Journal, pages 50–64, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sundarkantham</author>
<author>S Mercy Shalinie</author>
</authors>
<title>Word predictor using natural language grammar induction technique.</title>
<date>2007</date>
<journal>Journal of Theoretical and Applied Information Technology,</journal>
<volume>3</volume>
<contexts>
<context position="6234" citStr="Sundarkantham and Shalinie (2007)" startWordPosition="1027" endWordPosition="1030"> part-of-speech language modeling, there are few examples of systems using syntax in predictive text entry. Matiasek et al. (2002) describes a predictive text environment aimed at disabled persons, which originally relied on language models. Gustavii and Pettersson (2003) added a syntactic component to it based on grammar rules. The rules corresponded to common grammatical errors and were used to rerank the list of candidate words. The evaluation results were disappointing and the syntactic component was not added because of the large overhead it introduced (Matiasek, 2006). In the same vein, Sundarkantham and Shalinie (2007) used grammar rules to discard infeasible grammatical constructions. The authors evaluated their system by giving it an incomplete sentence and seeing how often the system correctly guessed the next word (Shannon, 1951). They achieved better results than previously reported, although their system has not been used in the context of predictive text entry for mobile devices. 2 Predictive Text Entry Using Syntax We propose a new technique that makes use of a syntactic component to model the word context and improve the K5PC figure. It builds on Gong et al. (2008)’s system and combines a dependenc</context>
</contexts>
<marker>Sundarkantham, Shalinie, 2007</marker>
<rawString>K. Sundarkantham and S. Mercy Shalinie. 2007. Word predictor using natural language grammar induction technique. Journal of Theoretical and Applied Information Technology, 3:1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucien Tesnière</author>
</authors>
<title>Éléments de syntaxe structurale. Klincksieck, Paris, 2e edition.</title>
<date>1966</date>
<contexts>
<context position="11919" citStr="Tesnière, 1966" startWordPosition="2018" endWordPosition="2019">head. defined as: � SemR(w, wj), • LeftArc adds an arc from the next input SemA(w|H) = word to the top of the stack and pops it. wj∈H where H is the context of the word w. In our case, H consists of words to the left of the current word. Gong et al. (2008) used a similar model in a predictive text application with a slight modification to the SemR function: C(stem(wi), stem(wj)) SemR(wi, wj) = C(stem(wj)) where the stem(w) function removes suffixes from words. We refined this model further and we replaced the stemming function with a real lemmatization. 4 Dependency Parsing Dependency syntax (Tesnière, 1966) has attracted a considerable interest in the recent years, spurred by the availability of data-driven parsers as well as annotated data in multiple languages including Arabic, Chinese, Czech, English, German, Japanese, Portuguese, or Spanish (Buchholz and Marsi, 2006; Nivre et al., 2007). We used this syntactic formalism because of its availability in many languages. 4.1 Parser Implementation There are two main classes of data-driven dependency parsers: graph-based (McDonald and Pereira, 2006) and transition-based (Nivre, 2003). We selected Nivre’s parser because of its implementation simplic</context>
</contexts>
<marker>Tesnière, 1966</marker>
<rawString>Lucien Tesnière. 1966. Éléments de syntaxe structurale. Klincksieck, Paris, 2e edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A tale of two parsers: Investigating and combining graphbased and transition-based dependency parsing using beam-search.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>562--571</pages>
<location>Hawaii,</location>
<contexts>
<context position="12733" citStr="Zhang and Clark (2008)" startWordPosition="2142" endWordPosition="2145"> English, German, Japanese, Portuguese, or Spanish (Buchholz and Marsi, 2006; Nivre et al., 2007). We used this syntactic formalism because of its availability in many languages. 4.1 Parser Implementation There are two main classes of data-driven dependency parsers: graph-based (McDonald and Pereira, 2006) and transition-based (Nivre, 2003). We selected Nivre’s parser because of its implementation simplicity, small memory footprint, and linear time complexity. Parsing is always achieved in at most 2n − 1 actions, where n is the length of the sentence. Both types of parser can be combined, see Zhang and Clark (2008) for a discussion. Nivre’s parser is an extension to the shift– reduce algorithm that creates a projective and acyclic graph. It uses a stack, a list of input words, and builds a set of arcs representing the graph of dependencies. The parser uses two operations in addition to shift and reduce, left-arc and right-arc: • RightArc adds an arc from the top of the stack to the next input word and pushes the input word on the stack. Table 1 shows the start and final parser states as well as the four transitions and their conditions and Algorithm 1 describes the parsing algorithm. 4.2 Features At eac</context>
<context position="17153" citStr="Zhang and Clark, 2008" startWordPosition="2912" endWordPosition="2915">regression output. 4.4 Beam Search We extended Nivre’s parser with a beam search to mitigate error propagation that occurs with a deterministic parser (Johansson and Nugues, 2006). We maintained N parser states in parallel and we applied all the possible transitions to each state. We scored each transition action and we ranked the states with the product of the action’s probabilities leading to this state. Algorithm 3 outlines beam search with a diameter of N. An alternative to training parser transitions using local features is to use an online learning algorithm (Johansson and Nugues, 2007; Zhang and Clark, 2008). The classifiers are then computed over the graph that has already been built instead of considering the probability of a single transition. 41 4.5 Evaluation We evaluated our dependency parser separately from the rest of the application and Table 3 shows the results. We optimized our parameter selection for the unlabeled attachment score (UAS). This explains the relatively high difference with the labeled attachment score (LAS): about –8.6. Table 3 also shows the highest scores obtained on the same Talbanken corpus of Swedish text (Einarsson, 1976; Nilsson et al., 2005) in the CoNLL-X evalua</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. A tale of two parsers: Investigating and combining graphbased and transition-based dependency parsing using beam-search. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 562–571, Hawaii, October 25–27.</rawString>
</citation>
<citation valid="false">
<title>Algorithm 1 Nivre’s algorithm. 1: Queue W 2: Stack nil 3: while -,Queue.isEmpty() do 4: features ExtractFeatures() 5: action guide.Predict(features) 6: if action = RightArc n canRightArc() then 7: RightArc() 8: else if action = LeftArc n canLeftArc() then 9: LeftArc 10: else if action= Reduce n canReduce() then 11: Reduce() 12: else 13: Shift() 14: end if 15: end while 16: return(A) Algorithm 2 Reference parsing. 1: Queue W 2: Stack nil 3: while -,Queue.isEmpty() do 4: x ExtractFeatures() 5: if (Stack.peek(), Queue.get(0)) E A n canRightArc() then 6: t RightArc 7: else if</title>
<booktitle>Queue.get(0), Stack.peek()) E A n canLeftArc() then 8: t LeftArc 9: else if ]w E Stack: (w, Queue.get(0)) E AV (Queue.get(0), w) E A) n</booktitle>
<marker></marker>
<rawString>Algorithm 1 Nivre’s algorithm. 1: Queue W 2: Stack nil 3: while -,Queue.isEmpty() do 4: features ExtractFeatures() 5: action guide.Predict(features) 6: if action = RightArc n canRightArc() then 7: RightArc() 8: else if action = LeftArc n canLeftArc() then 9: LeftArc 10: else if action= Reduce n canReduce() then 11: Reduce() 12: else 13: Shift() 14: end if 15: end while 16: return(A) Algorithm 2 Reference parsing. 1: Queue W 2: Stack nil 3: while -,Queue.isEmpty() do 4: x ExtractFeatures() 5: if (Stack.peek(), Queue.get(0)) E A n canRightArc() then 6: t RightArc 7: else if (Queue.get(0), Stack.peek()) E A n canLeftArc() then 8: t LeftArc 9: else if ]w E Stack: (w, Queue.get(0)) E AV (Queue.get(0), w) E A) n canReduce() then 10: t Reduce 11: else 12: t Shift 13: end if 14: store training example (x, t) 15: end while Algorithm 3 Beam parse. 1: Agenda.add(InititalParserState) 2: while -,done do 3: for parserState E Agenda do 4: Output.add(parserState.doLeftArc()) 5: Output.add(parserState.doRightArc()) 6: Output.add(parserState.doReduce()) 7: Output.add(parserState.doShift()) 8: end for 9: Sort(Output) 10: Clear(Agenda) 11: Take N best parse trees from Output and put in Agenda. 12: end while 13: Return best item in Agenda. Configuration Scoring model DepSyn weights</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>