<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.889926">
PRACTICAL PARSING OF GENERALIZED PHRASE STRUCTURE GRAMMARS
</title>
<author confidence="0.976723">
Anthony J. Fisher
</author>
<affiliation confidence="0.9650905">
Department of Computer Science
The University of York
</affiliation>
<bodyText confidence="0.919623714285714">
Heslington, York YO1 5DD, U.K.
An efficient algorithm is described for parsing a dialect of generalized phrase structure grammar
(GPSG). A practical parsing system, based on the algorithm, is presented. The dialect of GPSG which
the parsing system accepts is smaller, but considerably &amp;quot;purer&amp;quot; (closer to the original definition of
GPSG) and mathematically &amp;quot;cleaner&amp;quot; than that which is accepted by other practical parsing systems.
In particular, the parsing system correctly implements feature co-occurrence restrictions, subject only
to the restriction that the FCR set can be expressed in clausal form as a set of Horn clauses.
</bodyText>
<sectionHeader confidence="0.992047" genericHeader="abstract">
1 INTRODUCTION
</sectionHeader>
<bodyText confidence="0.990464683333333">
The generalized phrase structure grammar (GPSG)
(Gazdar et al. 1985) is one of the most recent, and
currently one of the most popular, formalisms used by
linguists to describe the syntax of natural (human)
languages. A GPSG is basically a context-free grammar
(CFG), whose non-terminals are complex symbols
called categories. A category is a set of features. The
CFG is augmented by:
• a set of conventions or constraints that govern the
automatic &amp;quot;propagation&amp;quot; of features between catego-
ries on different nodes of the parse tree; and
• a set of propositions (Boolean formulas whose literals
denote the presence of a feature in a category) that are
required to hold for the category on each node of the
parse tree. These propositions are known as feature
co-occurrence restrictions (FCRs).
A GPSG also contains feature specification defaults
(FSDs). An FSD behaves like an FCR in all respects
save one: if an FSD, when taken in conjunction with the
set of FCRs, cannot by the addition of features be made
to hold on a category c on a given node, the FSD is
simply ignored. Although this sounds straightforward,
the precise effect of FSDs is most unclear. The original
definition attempts to explain the effect of FSDs mainly
by giving examples, although there are a few mathemat-
ical definitions that, however, appear to confuse the
definition rather than to clarify it. A clear, formal
definition of the effect of FSDs is urgently required.
Because the present definition is so obscure, it was
reluctantly decided to exclude FSDs from consideration
in this paper.
Because a GPSG is so closely related to a CFG, it
was thought that the well-known efficient parsing tech-
niques for CFGs could be applied, with modifications,
to GPSGs, and that GPSGs would therefore be compu-
tationally tractable. Recently, however, Ristad (1985)
has shown that this is not the case, and that the
unrestricted GPSG parsing problem is NP-complete (on
the total problem size, viz, grammar plus input sen-
tence). Even before Ristad&apos;s result was known, workers
in this field had found the practical problems caused by
the interaction of FCRs, FSDs, and the propagation
conventions difficult to surmount. Briscoe&apos;s comments
(1986) are typical: &amp;quot;Finally, the concept of privileged
feature, its interaction with feature specification de-
faults and the bi-directionality of the head feature
convention are all so complex that it is debatable how
much use they would be in a practical system (even if
we did manage to implement them)&amp;quot; (p. 1); &amp;quot;The
interaction of feature co-occurrence restrictions, fea-
ture specification defaults and feature propagation
proved very hard to implement/understand&amp;quot; (p. 2).
This paper does not address the issues of ID/LP
parsing and metarules. Barton (1985) has shown that the
ID/LP parsing problem is NP-complete (on the total
problem size). He argued that a previous result of
Shieber (1983), which purported to give a G2 parsing
method for ID/LP grammars, was incorrect. Barton
claimed that Shieber&apos;s algorithm is exponential in the
worst case. Barton&apos;s result alone might be considered a
</bodyText>
<footnote confidence="0.78107275">
Copyright 1989 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided
that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page. To
copy otherwise, or to republish, requires a fee and/or specific permission.
0362-613X/ 89 /010139-148$03.00
</footnote>
<note confidence="0.8244575">
Computational Linguistics, Volume 15, Number 3, September 1989 139
Anthony J. Fisher Practical Parsing of Generalized Phrase Structure Grammars
</note>
<bodyText confidence="0.999804219047619">
strong hint that ID/LP parsing is inappropriate, both as
a practical computer-based parsing method and as a
possible model for the way in which people parse
sentences. It is of course probable that, for many
grammars, a sufficiently efficient implementation of
ID/LP could be obtained, either by means of a pre-
processor or by using Shieber&apos;s algorithm, but this has
not been attempted in the present implementation. The
omission of ID/LP does not appear seriously to limit the
usefulness of the parsing system, at least when applied
to grammars of English.
The question of metarules deserves a less cursory
discussion. It is known (Gazdar et al. 1985:65-67) that
the addition of metarules to a GPSG does not alter the
CF properties of the grammar. The finite closure rule,
which (stated informally) prohibits a metarule from
reprocessing its own output, implies that the addition of
a single metarule to a GPSG can generate only a finite
number of phrase structure (PS) rules for each PS rule in
the grammar. However, it is easy to write a metarule
that will match every PS rule in the grammar and
generate, say, two output rules. A second metarule can
then be written which does the same. It is clear,
therefore, that in the worst case the size of the induced
set of PS rules grows exponentially with the number of
metarules. For reasons put forward by Thompson
(1982), most, if not all, GPSG parsing systems handle
metarules by employing a precompilation phase to
generate the induced set of PS rules. If this method is
employed, the parsing time will, in the worst case, grow
exponentially with the number of metarules. In prac-
tice, however, exponential behaviour can safely be
presumed to be rare, since a typical metarule &amp;quot;triggers&amp;quot;
on only a small number of rules and generates only a
small number of rules. Preliminary investigations with a
&amp;quot;real&amp;quot; grammar of English suggest that the precompi-
lation phase generates an output grammar (containing
only PS rules) that is about twice as big as the input
grammar (which contains metarules and PS rules).
The metarules of GPSG are related to the hyperrules
of a van Wijngaarden grammar (VWG) (van Wijngaar-
den et al. 1976), as has often been observed (see, e.g.,
Gazdar et al. 1985:65). There exist various direct meth-
ods of parsing VWGs, which do not rely on the prior
expansion of hyperrules to generate a (usually large) set
of PS rules. Wegner&apos;s method (1980) and Fisher&apos;s
method (1985) are, unfortunately, exponential in the
worst case; however, the exponential behaviour of both
algorithms stems from the fact that the finite closure
property of GPSGs does not apply to VWGs, and the set
of induced PS rules might be infinite. It is possible that
these algorithms could be modified to handle the very
restricted metarules of GPSG in polynomial time.
Besides the omission of FSDs, ID/LP, and
metarules, there is one other respect in which the type
of grammar under consideration differs from GPSG as
originally described. For formal purposes, the original
set of feature-propagation conventions seemed rather
baroque and unduly specialised. It was felt that it would
be preferable to substitute for the &amp;quot;head feature con-
vention&amp;quot;, the &amp;quot;foot feature principle&amp;quot; and the &amp;quot;control
agreement principle&amp;quot; of the original definition a more
general mechanism. Consequently, we assume through-
out most of this paper that features may be specified
simply as &amp;quot;percolating&amp;quot; (from a node to its mother) or
&amp;quot;trickling&amp;quot; (from a node to its daughter). The propaga-
tion convention (percolating or trickling or neither or
both) can be specified individually for each feature. It is
stressed that this simplification is introduced in order to
simplify the description of the formalism and of the
algorithm. In section 4, the restrictions are relaxed, and
it is shown how conventions that are closely related to
the HFC, FFP, and CAP of &amp;quot;standard&amp;quot; GPSG (or
GPSG 85, as it is known) can be accommodated.
Having removed from consideration ID/LP parsing,
metarules, and FSDs, and having simplified (tempo-
rarily) the feature propagation conventions, we are left
with a much smaller and more manageable formalism.
The main thesis of this paper is that, given these
restrictions, the simple requirement that the FCRs be
expressible as a set of Horn clauses is sufficient to
ensure parsability in time order pK2G2n3, where p is a
measure of the degree of ambiguity of the grammar, K is
the size of the alphabet of features, G is a measure of the
size of the grammar, and n is the length of the sentence.
The exact meanings of p and G are made formal later. It
should be pointed out (lest false hopes be raised) that p
is not in general independent of n, and for some gram-
mars p is in fact a worse-than-polynomial function of n,
making the algorithm worse-than-polynomial on n in the
worst case. In practice, however, the algorithm can be
implemented quite efficiently in such a way that p n
for many linguistically plausible grammars.
Previous implementations. There have been several pre-
vious attempts to parse GPSGs. Several workers have
made wholesale changes to the definition of GPSG in
order to make the formalism easier to parse. The dialect
that the presen,t algorithm accepts is considered to be
&amp;quot;purer&amp;quot; and nearer to the original than that accepted by
most other algorithms. A brief summary of GPSG
implementations is given by Gazdar (1983). The &amp;quot;of-
ficial&amp;quot; definition of GPSG has changed since the list was
published, and some of the implementations listed are
no longer available. A more recent implementation is
that of Harrison and Maxwell (1986).
</bodyText>
<sectionHeader confidence="0.998845" genericHeader="introduction">
2 DEFINITIONS
</sectionHeader>
<bodyText confidence="0.999246285714286">
Preliminary definitions. The following definitions, of
standard terms of formal language theory, are given in,
for example, Salomaa (1973).
An alphabet is a finite non-empty set. The elements
of an alphabet are called letters. A word over an
alphabet V is a finite string consisting of zero or more
letters of V, whereby the same letter may occur
</bodyText>
<page confidence="0.972374">
140 Computational Linguistics, Volume 15, Number 3, September 1989
</page>
<note confidence="0.746478">
Anthony J. Fisher Practical Parsing of Generalized Phrase Structure Grammars
</note>
<bodyText confidence="0.814248166666667">
several times. The set of all words over an alphabet V
is denoted by W(V). For any V, W(V) is infinite.
We denote by 11(S) the powerset of S, which is the set
of all subsets of a set S.
Definitions. A generalised phrase structure grammar
(GPSG) is an ordered 7-tuple G = (VF, VT, X0, R, F, Fp,
FT), where:
VF is a finite set of features;
VT is a finite set of terminals, VF n VT = 0;
X0 is the starting category, a finite subset of VF;
R is the set of rules, a finite set of ordered pairs P -4
Q, such that P is a subset of VF (i.e. P E 11(VF)),
and Q is a word over the alphabet V = 11(VF) U
VT;
F is the FCR set, a function from 11(VF) to {true,
false};
Fp is the set of percolating features, a subset of VF;
and
</bodyText>
<equation confidence="0.48513725">
FT is the set of trickling features, a subset of VF.
For P, Q E W(V), we say that P derives Q, written
P Q, if 3 an integer n and a, 0 E W(V), P&apos;, Q&apos;
(i = 1, . . n), P&amp;quot;, Q&amp;quot;, (i = 1, . . n) E V such
</equation>
<listItem confidence="0.947736416666667">
that the following all hold:
1. a P&apos; = P and a Qi Q&apos; 2 . . • Q&apos; n13 = Q
2. P&amp;quot; --&gt; Q&amp;quot;1 Q&amp;quot;2 . . . E R
3. [Extension:]
(i) P&amp;quot; ç P&apos;
(ii) if Q&apos; E VT: = Q (i = 1, 2, . . n)
if Q&apos; E 11( VF): Q&amp;quot; c Q&apos; (i = 1, 2, . . n)
4. [FCR constraints:]
F(P&apos;)
5. [Propagation Constraints:]
(i) Q&apos; E VT v (Q&apos; n Ft,) C P&apos; (i = 1,2, . . n)
(ii) Q&apos; C VT v (P&apos; n FT) C Q&apos; (i = 1,2, . . n).
</listItem>
<bodyText confidence="0.98927">
We denote by the reflexive and transitive closure
of .
The language generated by G, written L(G), is
defined by
</bodyText>
<equation confidence="0.96958">
L(G) = {P I P C W(VT), X0
</equation>
<subsectionHeader confidence="0.957287">
End of definitions.
</subsectionHeader>
<bodyText confidence="0.996567974358974">
Informally, the definitions of GPSG, and
L(G) are the standard definitions of a context-free
grammar, modified by defining the non-terminal of the
standard CFG definition as a set of features. Further-
more, the standard definition of has been extended to
take into account feature matching by the free addition
of features to categories specified in rules (extension),
the FCR constraints, and the propagation constraints.
The original definition of GPSGs postulated a set of
FCRs whose conjunction is required to hold; this con-
junction has been collapsed into a single Boolean func-
tion F in our definition. F is required to hold on each
non-terminal node of the parse tree by virtue of condi-
tion 4 above. (It is unnecessary to specify that F(Q&apos;
must hold; this is implied by the use of the reflexive and
transitive closure of in the definition of L(G).)
Finally, condition 5(i) requires that, if P&apos; is the mother
of a non-terminal node Q&apos;, (considering 13&apos; and Q&apos;; as
nodes in a parse tree), then for each feature f which has
been defined as percolating from daughter to mother
(i.e. is a member of Fp), iff is present on the daughter
node, then it must be present also on the mother.
Condition 5(ii) is the corresponding statement for trick-
ling features.
Our definition is given in terms of features that can be
either present or absent from a category, whereas in the
original definition a feature has a value. This distinction
is merely a mathematical device to simplify the defini-
tion and the discussion of the algorithm which will
follow. Our definition can be related to the original
definition by reading &amp;quot;feature&amp;quot; as &amp;quot;feature-value pair&amp;quot;.
For example, a &amp;quot;real&amp;quot; GPSG might contain a feature
PAST which can take a value which is either + or —.
We interpret that as two separate features, say PAST+
and PAST—. The standard definition would require that
a category may not contain both PAST+ and PAST—.
This can be expressed by conjoining the FCR --(PAST+
A PAST—) to the FCR set. Such an FCR is called a
group FCR.
</bodyText>
<sectionHeader confidence="0.960106" genericHeader="method">
3 A PARSING ALGORITHM FOR GPSGs
</sectionHeader>
<bodyText confidence="0.997500285714286">
The algorithm belongs to the class of algorithms that
obtain a grammar G&apos;, variously called a skeleton gram-
mar or an underlying grammar, from a given grammar G
and then parse according to G&apos;. In these algorithms the
skeleton grammar G&apos; is chosen such that L(G) C L(G&apos;),
so, if the parse according to G&apos; fails, the sentence can be
rejected immediately. If the parse succeeds, it is neces-
sary to check some additional constraints, typically by
examining the parse tree, to ensure that the sentence is
indeed acceptable to the more restrictive, given, gram-
mar G. The extra checking process typically annotates
the parse tree with extra information but does not
change its shape. At the end of the checking process,
either the sentence is rejected as not conforming to G,
or the sentence is accepted, in which case the annotated
parse tree is the parse tree of the sentence according to
G. Wegner&apos;s (1980) algorithm for VWGs belongs to this
class.
In the present algorithm, the skeleton grammar G&apos; is
a GPSG that is obtained from a given GPSG G by
neglecting some of the FCRs and the percolating feature
propagation constraints. The skeleton grammar can be
parsed by a simple modification of Earley&apos;s (1970)
algorithm. The algorithm comprises a precompilation
phase, in which the skeleton grammar G&apos; is obtained
from the given grammar G, followed by three parse-time
phases that are executed one after the other.
Precompilation. Given a GPSG
</bodyText>
<equation confidence="0.881037">
G = (VF, VT, X0, R, F, Fp, FT),
</equation>
<bodyText confidence="0.70493075">
first define the side-effect-free FCR set F&apos; algorithmi-
cally in the following manner.
Computational Linguistics, Volume 15, Number 3, September 1989 141
Anthony J. Fisher Practical Parsing of Generalized Phrase Structure Grammars
</bodyText>
<listItem confidence="0.947414888888889">
1. Convert F to clausal form, in other words a
conjunction of disjunctions of terms, each of
which is either an unnegated literal feature or a
negated literal feature. (This can be done
uniquely, apart from questions of ordering of
terms; see, e.g., Loveland (1978:32f).)
2. Remove from the clause set all clauses (i.e.,
disjunctions) that contain one or more unnegated
literals, leaving behind only those clauses that
</listItem>
<bodyText confidence="0.991248814814815">
contain only negated literals.
The resulting set of clauses represents the side-effect-
free FCR set F&apos;. Since F&apos; was obtained from F by
removing clauses, the resulting function F&apos; cannot be
more restrictive than F; in other words, F D F&apos;.
The reason for removing clauses that contain unne-
gated literals is that the evaluation of FCRs can, in
general, cause the instantiation of new features on a
node. This can be viewed as a &amp;quot;side effect&amp;quot; of the
evaluation, whose primary function is to filter out
inadmissible parses. Side effects are difficult to handle,
because they interact with each other and with other
aspects of the grammar, in particular with the propaga-
tion constraints. For example, a new feature added as a
result of a &amp;quot;non-side-effect-free&amp;quot; FCR clause might
cause some other clause, which was satisfied before the
new feature was added, to become false. This is not
possible, however, if no clause contains an unnegated
literal: each clause can be satisfied only by the absence
of one or more stated features, and if the required
features are not absent, the clause yields false — there
is no mechanism in GPSG for removing features from a
category.
Now define the skeleton grammar G&apos; of G by
G&apos; = (VF, VT, X0, R, F&apos;, 0, FT).
Clearly L(G) C L(G&apos;), since whenever conditions 4 and
5 of section 2 hold for a derivation according to G, they
will also hold for a corresponding derivation according
to G&apos;. (Remember that F D F&apos;.) Informally, G&apos; is more
permissive than G. For the same reason, each parse
according to G has a corresponding parse according to
G&apos;, differing only in the distribution of features among
categories on nodes. In other words, each parse tree
according to G is the same &amp;quot;shape&amp;quot; as some parse tree
(of the same sentence) according to G&apos;.
Phase 1. We now parse G&apos; by applying a modified form
of Earley&apos;s algorithm (Earley 1970; see also Pulman
1985, Ritchie and Thompson 1984). (The reader is
assumed to be familiar with Earley&apos;s algorithm, in
particular with the role played by the predictor in adding
new states to a state set.) The algorithm is extended so
that it creates a parse tree as the parse progresses. A
method for doing this is described briefly by Earley
(1970) and in more detail by Earley (1968).
There is no need to handle the percolating feature
propagation constraint at this stage, because in G&apos; Fp is
empty. There is no need to consider what happens when
the evaluation of an FCR causes a new feature to be
added to a category, since the FCR set F&apos; is side-effect-
free. We can therefore treat G&apos; as a CFG whose
non-terminals are categories, provided that we allow for
extension (condition 3 of section 2) and the trickling
feature constraint (condition 5(ui)). This is done in the
following way.
A category appears on a node by virtue of the
appearance of a category c1 on the right-hand side of a
rule RI and the appearance of a &amp;quot;matching&amp;quot; category c2
on the left-hand side of a rule R2. The extension
condition permits the free addition of features to c1 and
to c2 to generate the category c which appears on the
node. By the extension condition, c1 C c and c2 C c,
which implies that c D (c1 U c2). Now let us neglect for
the moment the trickling feature constraint. Since we
are ignoring the non-side-effect-free FCRs and the prop-
agation constraints, any superset of c1 U c2 which
satisfies F&apos;(c) will suffice; consequently, we take the
smallest superset, namely c1 U c2, which is the least
upper bound of c1 and c2 under the ordering relation of
extension (see Gazdar et al. 1985:39).
Now we consider the trickling feature constraint.
The effect of this constraint is to instantiate extra
features on certain categories: those features that be-
long to a mother category and which are also members
of FT must be instantiated on each daughter category.
The category that is instantiated on the node of the
parse tree is the smallest superset of c1 U c2 which
contains all of its mother&apos;s trickling features, which is c1
U c2 U (co Cl FT), where co is the category of the mother
node.
To determine the category to place on a node of the
parse tree, therefore, the algorithm needs to know:
</bodyText>
<listItem confidence="0.969130142857143">
• the a priori category c1 on the right-hand side of a
rule;
• the a priori category c2 on the left-hand side of the
rule which &amp;quot;matches&amp;quot; ci;
• the fully-evaluated a posteriori category on the
mother of the node to which a category is currently
being assigned.
</listItem>
<bodyText confidence="0.99475325">
All of this information is available to the predictor in
Earley&apos;s algorithm. This follows from the fact that
Earley&apos;s algorithm is &amp;quot;top down&amp;quot;, which means that the
full category on a node is known before any of that
node&apos;s daughters are considered.
We now consider how Earley&apos;s algorithm can be
modified to parse the skeleton grammar in the manner
outlined above. A state in Earley&apos;s standard algorithm
can be written as X --&gt; a • p, which signifies that the
algorithm is considering the rule X —&gt; «13, and has
successfully matched the a with some portion of the
sentence being parsed. The predictor is applied to states
X —&gt; • Yp, which have a non-terminal to the right of
the dot. The predictor adds new states Y —&gt; • y for each
rule Y —&gt; y with matching non-terminal Y.
In the new algorithm, a state is written (co) c —&gt; a • 13.
</bodyText>
<page confidence="0.880077">
142 Computational Linguistics, Volume 15, Number 3, September 1989
</page>
<note confidence="0.88932">
Anthony J. Fisher Practical Parsing of Generalized Phrase Structure Grammars
</note>
<bodyText confidence="0.998509229885058">
As in the standard algorithm, this signifies that the
algorithm is considering the rule c —&gt; ap, and has
successfully matched the a. The extra category co
contains the features that are passed from mother to
daughter and which will ultimately appear on a node of
the parse tree.
The predictor in the new algorithm is applied to
states (co) c —&gt; a • clp with a non-terminal category to
the right of the dot. The predictor adds new states
(ci U c2 U (co n FT)) c2 • y
for each rule c2 ---&gt; y such that F1(c1 U c2 U (co fl FT))
holds. (At first sight it appears that this entails a search
through all of the rules of the grammar, but a means of
avoiding a full search is presented in section 4.)
The roles of the scanner and of the completer in
Earley&apos;s standard algorithm are unchanged in the new
algorithm. The initial state that is entered to start the
parse is (0) Ø-+ • Xo. The associated &amp;quot;dummy&amp;quot; rule 0
--&gt; X0 is not considered part of the grammar, and is
exempt from being matched by the predictor.
It would be quite possible to use a &amp;quot;bottom up&amp;quot;
algorithm in place of Earley&apos;s algorithm, in which the
roles of trickling and percolating features would be
reversed. It is not possible to handle both percolating
and trickling features in phase 1, since a provisional
decision at some point deep down in the parse tree to
instantiate a feature on a certain category would in
general cause changes to the membership of categories
in remote parts of the tree.
Phase 2. The &amp;quot;parse tree&amp;quot; that is generated by Earley&apos;s
algorithm is in general not a tree at all; it is a directed
graph. Besides non-terminal nodes and terminal nodes,
the graph will in general contain branching nodes that
point to alternative daughters of a non-terminal node. It
is by this means that multiple parses, arising from an
ambiguous sentence, are represented. If the degree of
ambiguity of the sentence with respect to the skeleton
grammar is infinite, the finite graph must represent
infinitely many distinct parse trees; in this case the
graph is cyclic. We assume that the degree of ambiguity
is finite, in which case the graph is a directed acyclic
graph (DAG). A DAG differs from a tree in that whereas
each node in a tree (except the root) has precisely one
parent, a node in a DAG may have more than one
parent. In other words, a DAG represents common
sub-trees only once; a single sub-tree may be descended
from several parents. DAGs are often used in the
construction of compilers for computer programming
languages.
Let p be the degree of ambiguity of the skeleton
grammar, i.e., the number of distinct parse trees repre-
sented by the DAG. We expand the DAG, generating p
distinct parse trees. This can easily be done by means of
conventional tree processing techniques, provided that
p is finite, in other words if the graph is acyclic.
Phase 3. Each distinct parse tree is examined in turn.
For each tree, sufficient features are added to the
categories on each node of the tree to cause the tree to
reflect a parse according to the original GPSG G. This
entails the evaluation of F and of the propagation
constraints on each category, and the construction of a
category on each node which satisfies all of the con-
straints. Once again, the smallest possible category is
constructed. That is, if a category c satisfies all of the
constraints, and so does a larger category c U c&apos;, we
choose c. It is debatable whether this is the correct
behaviour; some might argue that separate parse trees
ought to be constructed in which all possible legal
extensions are shown. However, the resulting set of
parse trees would then in general be very large, and it is
difficult to believe that this behaviour is desirable. Our
smallest category is similar to the most general unifier of
a set of expressions in mathematical logic; as in logic,
particular, less general instances can be derived from
the most general case, but it is the most general (least
fully specified) case that is of most interest.
We assume that the FCR set F is expressed in clausal
form and that each clause (i.e., each disjunction) is a
Horn clause (a clause with either zero or one unnegated
literal; see, e.g., Loveland (1978:99)). Number the
clauses F1, . . Fm.
We denote by M(N) the mother of the node N, if it
exists (i.e. unless N is the root of its tree). We denote by
C(N) the category on the node N.
Let the distinct parse trees produced by phase 2 be
Tp. The algorithm unify, defined below, is
applied to each Ti in turn, for i = 1, . . p.
</bodyText>
<listItem confidence="0.92347775">
unify (T): Let the non-terminal nodes in T be NI,
. . NN.
1. set again := false;
2. for j = 1, N do
</listItem>
<equation confidence="0.558862705882353">
2.1. if NJ is not the root of T and
(c(m(N.,)) n FT) C(NJ) then
2.1.1. C(111:1) := C(Ni) U (C(M(NJ)) r-1 Fr);
2.1.2. set again := true;
2.2. if 1V:i is not the root of T and
(c(N.,) n Fi,) C(M(*)) then
2.2.1. C(M(1V)) := C(M(ATJ)) U (C(Ni) fl Fp);
2.2.2. set again := true;
3. for j = 1, N do
3.1. fork= 1, ..., M do
3.1.1. let f be the set of unnegated literals in
Fk;
3.1.2. let f be the set of negated literals in Fk;
3.1.3. if f_ C(1V) then
3.1.3.1. if f± = 0 then fail;
3.1.3.2. if C(1V) then
3.1.3.2.1. C(1V) := C(1V) U f+;
</equation>
<listItem confidence="0.765191333333333">
3.1.3.2.2. set again := true;
4. if again then go to step 1;
5. output T.
</listItem>
<bodyText confidence="0.923358666666667">
Proof of the algorithm. First notice that the flag again is
set (in steps 2.1.2, 2.2.2, and 3.1.3.2.2) whenever a
feature is added to a category that is not already in that
category, and at no other time. Since there are only
finitely many features, steps 1 to 4 are repeated only
finitely many times, so the procedure terminates.
</bodyText>
<note confidence="0.5472065">
Computational Linguistics, Volume 15, Number 3, September 1989 143
Anthony J. Fisher Practical Parsing of Generalized Phrase Structure Grammars
</note>
<bodyText confidence="0.999892607142857">
Next observe that on successful termination, again is
false, so steps 2.1 and 2.2 must have been obeyed for
each node with the conditions in 2.1 and 2.2 false each
time. Consequently, on successful termination, the
propagation constraints hold for each node.
Finally, on successful termination, the FCR set F
also holds for each node, for the following reasons. Step
3.1.3 checks the negated literals in the clause Fk against
the category C(N). If the condition in 3.1.3 is false,
there is at least one negated literal in Fk which is indeed
absent from C(N), so the clause Fk is satisfied. If, on
the other hand, the condition in 3.1.3 is true, none of the
negated literals can possibly be satisfied, since features
may not be removed from a category, only added. Since
Fk is a Horn clause, there is at most one unnegated
literal in Fk, sof, is either empty or has one member. If
f, is empty, the clause can not be satisfied, so the
algorithm fails. Iff, is not empty, the feature is added to
the category if it is not already there, and the clause is
thereby satisfied. The addition of the new feature might
invalidate previously satisfied clauses or propagation
constraints, so the flag again is set which causes the
propagation constraints and FCR clauses to be checked
afresh. As noted, the process will eventually terminate
with all propagation constraints and all FCR clauses
satisfied, or else the algorithm will fail, in which case
the sentence does not belong to the language generated
by the original grammar G.
</bodyText>
<subsectionHeader confidence="0.961313">
End of proof.
</subsectionHeader>
<bodyText confidence="0.999943294117647">
Now consider what would happen if one of the
clauses were not a Horn clause. The algorithm would
not know which of the several features from!, to add in
step 3.1.3.2.1 in order to satisfy the clause. The only
solution would seem to be to generate copies of the
parse tree, and to follow through each choice of feature
from f, on a different copy of the parse tree, finally
presenting the user of the parsing system with all of the
parse trees. This would cause a combinatorial explo-
sion, since the splitting and copying would have to be
done at each level of the parse tree at which the
particular feature in question is instantiated.
The linguistic consequences of the Horn clause re-
striction are not clear, but experience with the parsing
system suggests that they are not severe. The Horn
clause restriction prohibits the grammar writer from
writing FCRs such as
</bodyText>
<subsectionHeader confidence="0.228446">
[PRD +] A [VFORM] D [VFORM PAS] v [VFORM PRP]
</subsectionHeader>
<bodyText confidence="0.951043736842105">
(Gazdar et al. 1985:111), in which a disjunction of
non-negated literals appears on the right of D . It is in
such FCRs that the Horn clause restriction appears in
its true colours, as a mechanism for curbing a combina-
torial explosion or, to put it another way, a mechanism
for prohibiting a source of non-determinism. If the
consequences of forbidding such FCRs later appear too
severe, the possibility will be investigated of moving the
non-determinism from the FCRs into the rules of the
grammar, by replacing an FCR like the one above by a
new FCR
[PRI) +1 A [VFORM] D [F]
where F is a new feature, and adding appropriate rules
to the grammar. The details of this have yet to be
worked out; it is presented as a possible solution to a
problem that has not yet arisen.
Time and space bounds. The following parameters are
relevant to a consideration of time and space bounds for
the algorithm:
</bodyText>
<listItem confidence="0.99960875">
• p, the degree of ambiguity of the skeleton grammar;
• K, the cardinality of VF;
• G, the number of rules in the grammar;
• n, the length of the sentence being parsed.
</listItem>
<bodyText confidence="0.998903170731707">
Earley&apos;s algorithm, as is well known, operates in
time order G2n3. Earley&apos;s proof of the time complexity
of his algorithm (Earley 1970) is in no way affected by
the elaboration of the predictor to handle feature match-
ing. In particular, the number of states in a state set
does not increase with K. Although K features may in
principle be combined to construct 2&amp;quot; different catego-
ries, the algorithm generates new categories by exten-
sion only when they are required. In fact, if a new
feature specification is added to a rule that is previously
unspecified for that feature, the state sets will either
remain the same size or become smaller, since adding a
feature restricts the range of rules that the rule in
question will &amp;quot;match&amp;quot;. Speaking informally, it is under-
specified rules that cause the problems; the more fully
specified are the rules, the closer is the GPSG to a CFG,
and the fewer are the states that are needed.
The factor K does, however, enter into the time
bound for phase 1 in the following manner. Although the
number of &amp;quot;primitive steps&amp;quot; (Earley&apos;s terminology)
that are executed by the modified algorithm is indepen-
dent of K, the time taken to complete certain primitive
steps, in particularly the addition of a state to a state set
and the feature matching operation in the predictor, is
proportional to K. The overall time bound is therefore
KG2 n3
The expansion of the DAG to yield p distinct parse
trees can be done by conventional tree processing
techniques in time proportional to p, the number of
nodes in each tree, and the size of a node (which affects
the time taken to copy a node). This gives a bound of
order pKGn2 for phase 2.
The algorithm unify contains three nested loops
(steps 3 and 3.1, and the again loop). An upper bound
on the number of nodes is a constant times Gn2, and an
upper bound on the number of times round the again
loop is the number of features, K. To simplify the
analysis, we take M — G. (Formally, we define G to be
the sum of the number of rules in the grammar and the
number of clauses in the FCR set.) Moreover, the set
operation C in step 3.1.3 can realistically be expected to
</bodyText>
<page confidence="0.976879">
144 Computational Linguistics, Volume 15, Number 3, September 1989
</page>
<note confidence="0.881069">
Anthony J. Fisher Practical Parsing of Generalized Phrase Structure Grammars
</note>
<bodyText confidence="0.9395643">
take time proportional to K, although the operations
involving!, can be done in constant time, since!, has
either zero or one member. A time bound for unify is
therefore K2G2n2. Finally, unify is obeyed p times,
which gives a time bound for phase 3 of order pK2G2n2.
It is unfortunate that, as noted earlier, p is not in
general independent of n. To see why this is so, con-
sider the following grammar.
Non-terminal alphabet: {S}
Terminal alphabet: fal
</bodyText>
<sectionHeader confidence="0.27489" genericHeader="method">
Rules: S --&gt; S S
</sectionHeader>
<bodyText confidence="0.98485730952381">
S--&gt; a
It has been shown (Church and Patil 1982) that the
number of distinct trees generated, for a sentence of
length n, grows factorially with n. This means that the
algorithm as a whole will take factorial time to parse a
sentence of length n according to this grammar. This is
a matter of concern, because constructions similar to
this example are commonly used to handle coordina-
tion. It is even possible in principle for p to be infinite,
in which case the algorithm will not terminate (although
the advertised time bound ofpK2G2n3 will still hold!). In
practice, however, no grammar has been encountered
which unavoidably has infinite p. (Self-referential rules
of the form X ---&gt; X have occasionally appeared, but
these were always traced to an error in the grammar.)
Considerable effort has been expended in an attempt
to improve the theoretical worst-case performance of
the algorithm when p is a finite valued but rapidly
increasing function of n. It might be possible to combine
phases 2 and 3, employing &amp;quot;lazy evaluation&amp;quot; (a tech-
nique often used in functional programming) to expand
the DAG only when necessary. If this were done, much
of the DAG might remain unexpanded, with consequent
savings in time and space. The problem with this
approach is that some features are required to percolate
right to the root of their tree, and a given branching
point might have different (and incompatible) features
percolating to it from each of its alternative descen-
dants. It turns out to be often necessary to expand the
DAG all the way back to the root, in which case little is
saved by using lazy evaluation. It is worth pointing out
that, in cases (such as the example) in which the
algorithm is least efficient, the output is often very
large, consisting of many parse trees. In many (but not
all) of these cases, the time taken is asymptotically
linear in the length of the output, i.e. the number of
nodes in the set of parse trees displayed. Surely, no
algorithm can ever behave sublinearly on the length of
its output. Furthermore, as discussed later, in cases in
which this problem does not arise, the execution time is
dominated by phase 1. We therefore have an algorithm
that:
</bodyText>
<listItem confidence="0.997946285714286">
• behaves as well as one of the best general CF parsing
algorithms, for all unambiguous grammars and for
many ambiguous grammars;
• takes time that is linear in the length of the output for
some &amp;quot;problem&amp;quot; grammars; and
• takes a very long time in a small number of really
awkward cases.
</listItem>
<bodyText confidence="0.999959583333333">
The time bounds for the three phases are KG2n3,
pKGn2, and pK2G2n2. This gives an overall worst-case
time bound of order pK2G2n3.
The space bound is of order pKGn3 in the worst case,
for the following reasons. Earley&apos;s algorithm requires
space proportional to KGn2 to hold the states, n for the
state sets (that is, the list-processing overhead), and
KGn3 for the DAG. The grammar itself requires space
proportional to KG. The p distinct parse trees require
space proportional to KGn2 each. Phase 3 does not
require any working storage. The worst-case space
bound is therefore of order pKGn3.
</bodyText>
<sectionHeader confidence="0.998776" genericHeader="method">
4 IMPLEMENTATION
</sectionHeader>
<bodyText confidence="0.999919078947369">
A practical GPSG parsing system has been constructed,
based on the algorithm just described. The system
comprises a table generator and a parser. The system
was originally written in the programming language
BCPL, and ran on a VAX 780 computer under the Unix
operating system. The system has recently been re-
implemented in C to run on a Sun 3/50 workstation. The
Sun version generally runs several times faster than the
VAX version. The parse times given below relate to the
slower VAX implementation.
The table generator performs the precompilation
phase of the algorithm. It generates a tabular represen-
tation of the skeleton grammar, which the parser can
interpret more efficiently than it could the &amp;quot;raw&amp;quot; rules,
and it converts the FCR set into clausal form. The table
generator also performs various checks to ensure, as far
as possible, that the grammar is well formed. Besides
the obvious syntactic checks (to detect such errors as a
comma in the wrong place), the table generator checks
that the FCR set is not identically false, that there are no
obvious blind alleys or non-reachable categories (this is
not checked rigorously), and that various other subtle
&amp;quot;well-formedness&amp;quot; conditions are satisfied. This error
checking has proved very useful in practice, since
GPSGs are notoriously difficult to debug.
The input grammar is written in the notation of
Gazdar et al. (1985), with a few concessions to the
limitations of the typical computer input device. In
particular, features have values, and what we have
referred to as a feature is, in the notation accepted by
the table generator, a feature-value pair, written If v].
Each distinct feature value pair is associated by the
table generator with a particular bit in a computer word.
A category is represented by a set of bits, i.e., by a word
with several bits set, one for each feature-value pair in
the set. Category valued features correspond to trees,
and a distinct bit is allocated to each terminal node of
the tree. For example, the feature PAST, with two
</bodyText>
<note confidence="0.6380565">
Computational Linguistics, Volume 15, Number 3, September 1989 145
Anthony J. Fisher Practical Parsing of Generalized Phrase Structure Grammars
</note>
<bodyText confidence="0.984286372727273">
values + and —, would have two bits allocated to it, and
for a category valued feature SLASH, the values [N +,
V —] and [N—, V +] would be allocated four bits. Note
that, in any given grammar, the depth of the tree
induced by a category valued feature is finite; further-
more, the range of possible values of a category valued
feature is known at table generation time, so it is known
at this stage how many bits to allocate to the feature.
The representation of feature-value pairs by bit posi-
tions in a computer word allows the very efficient
logical instructions of the computer ((1 , U , 7), which
operate on a whole word of bits at a time, to be used.
As explained earlier, a feature in GPSG 85 may take
at most one value at a time, since a GPSG 85 feature is
in fact a function. This restriction is expressed by
conjoining an FCR, known as a group FCR, to the FCR
set. For example, if the grammar contains the two-
valued feature PAST referred to above, the FCR
--OPAST +] A [PAST —])
would be conjoined to the FCR set. In general, the
presence of an n-valued feature f, with values v 1 , . .
v„, entails the addition of the FCRs
—1(Lf vi] A [fv]) for each i, j = 1, . . n, i &lt;j.
When converted to clausal form these FCRs become the
n(n-1)/2 clauses
i[fv1] vj] for each 1,1 = 1, . . n, i &lt; j
whose inclusion in the set of clauses presented to the
parser would make the table very large. Consequently,
these group clauses are abbreviated. For each n-valued
feature f with n 2, a group mask is included in the
parser table which has one bit set for each feature-value
pair whose conjunction is to be prohibited. The parser
checks these group masks whenever it consults the FCR
clause set. If g is a group mask and c is a mask
representing a category, the parser has only to check
that (g n c) has not more than one bit set.
It has been observed that, in practice, it is likely that
the explicit FCR set supplied by the grammar writer will
contain mostly non-side-effect-free clauses. However,
the (notional) group FCRs are, by definition, side-
effect-free. Because of this, the algorithm is modified
for implementation in the following way. The FCR set
F&apos; which is used in the definition of the skeleton
grammar is taken to be just the set of notional group
FCRs; any &amp;quot;genuine&amp;quot; FCRs, be they side-effect-free or
not, are excluded from F&apos;. Furthermore, the table
generator ensures that the full FCR set F is satisfied on
each node at table generation time. For example, if the
grammar contains the FCR
[NOM] D [NFORM NORM],
then a category [NOM +] occurring in a phrase struc-
ture rule would be rewritten by the table generator as
[NOM ±, NFORM NORM]. These modifications in-
crease the efficiency of the implementation, and enable
certain errors to be detected at table generation time.
In practice, most GPSGs closely resemble traditional
CFGs, with most categories fully specified for the
&amp;quot;major&amp;quot; features N, V. and perhaps BAR. Conse-
quently, the group FCR constraints ensure that the
skeleton grammar also resembles a traditional CFG, and
is certainly not, in practice, massively ambiguous. In-
deed, the table generator insists that categories in rules
are written as X[Y], where X is a name (a traditional
non-terminal), and Y is a category. The non-terminal X
is defined (by the grammar writer) to stand for some set
of major features. This convention is perhaps contro-
versial, but Gazdar et al. (1985) is full of such rules, and
the linguists who use the parsing system have not
grumbled yet. The convention does allow the table
generator to check the grammar more stringently than
would otherwise be the case, and it enables the parser to
be made considerably more efficient, by dividing the set
of all categories (which must be searched by the predic-
tor) into disjoint subsets. The convention has no theo-
retical significance; the program would work without it.
The head feature convention. The grammar writer is able
to denote certain non-terminals on the right-hand side of
a rule as head non-terminals, which correspond to the
head symbols of traditional X-bar syntax. This is done
by prefixing the name of the non-terminal in the rule by
a star. The percolation and trickling of features can be
restricted to occur only between a mother and a head
daughter. There are thus nine possible propagation
behaviours for any feature:
one of
not trickling
trickling, but only to head daughters
trickling to all daughters
together with one of
not percolating
percolating, but only from head daughters
percolating from any daughter
The head feature convention is simulated by defining
head features to trickle, but only to heads. This is
adequate in most situations, but it falls short of the
behaviour postulated by Gazdar et al. (1985:94f1). In
particular, the notion of free feature specification sets is
not accommodated. This causes problems in, for exam-
ple, the treatment of conjunctions, in which the con-
joined constituents are conventionally all heads. GPSG
85 allows a rule that in our notation would be written
NI&apos;: *NP [CONJ and], *NP [CONJ NIL].
In the present implementation, any PER feature (for
example) which happens to be present on the mother
would trickle to both head daughters, thereby forcing
agreement between the daughters. Our solution has
been to make the daughters non-heads, which is unat-
tractive, but which has been made to work.
The foot feature principle. The foot feature principle is
more of a problem than the head feature convention. It
</bodyText>
<page confidence="0.955079">
146 Computational Linguistics, Volume 15, Number 3, September 1989
</page>
<note confidence="0.913992">
Anthony J. Fisher Practical Parsing of Generalized Phrase Structure Grammars
</note>
<bodyText confidence="0.996906021739131">
is clear that foot features ought to percolate, but the
situation is more complicated than this. In a rule such as
S: NP, S [SLASH NP]
the SLASH feature (which is a foot feature) must be
prevented from percolating from the node that is gen-
erated by extension from the right-hand-side S. This is
achieved by forbidding any feature that has been de-
clared to be a foot feature (e.g., SLASH and WH) to
percolate from a node on which the feature appears by
virtue of its appearance on the right-hand side of a
phrase structure rule. This is easy to implement.
This is only a partial solution to the problem, how-
ever. The rule given above correctly generates
the telephone Carol tested.
It is not possible, however, by this mechanism to
prevent
* the telephone Carol tested the telephone,
in which &amp;quot;Carol tested the telephone&amp;quot; is correctly
parsed as an S, but in which a SLASH NP specification
is &amp;quot;gratuitously&amp;quot; instantiated in order to satisfy the
extension conditions imposed by the rule given above.
To solve this and other problems, a tree is now defined
to be admissible only if each non-terminal node of the
tree satisfies the foot condition, which is related to the
original FFP of GPSG 85. The foot condition is defined
as follows.
Define a lexical node of a parse tree as a node that
immediately dominates a terminal node. (A gap, which
is explicitly denoted in the grammar by the word GAP,
is a terminal node.) Define an interior node as a node
that is neither terminal nor lexical. An interior node is
said to meet the foot condition (FC) if each foot feature
that it contains appears also on at least one daughter
from which it can legally percolate. A lexical node is
said to meet the FC ill each foot feature that it contains
appears also on the left-hand side of the lexical rule that
gave rise to the lexical node.
This definition implies that the FC cannot cause the
instantiation of any features. In this respect, the FC
differs from the propagation conventions, which add the
necessary features to make the conditions hold. The FC
mechanism operates on the tree as it is after FCRs and
propagation conditions have been enforced. It does not
alter the tree; it merely checks that the foot condition is
true on each node. Note that all of this follows from the
definition. It is not necessary to put forward a proce-
dural definition of the FC, which would fit ill with the
non-procedural definition of GPSG. In contrast to the
GPSG 85 FFP, the FC readily permits a straightfor-
ward, efficient, and deterministic implementation.
The control agreement principle. A mechanism has been
provided for specifying horizontal propagation of fea-
tures in a way similar to that implied by the control
agreement principle of Gazdar et al. (1985). Sister
categories in a rule may be designated control sisters (by
prefixing the name of the non-terminal by a dollar). A
set of control features is defined by the grammar writer,
analogous to the sets of trickling and percolating fea-
tures. Each non-terminal node N has associated with it
an extra node N&apos;. If a node No has daughters NI, . .
Nn, then No&apos; is called the stepmother of each NI, . .
N. If Ni is a control sister, then any control features in
C(Ni) are required to percolate to the stepmother, and
any features on the stepmother are required to trickle to
each stepdaughter that is a control sister. The effect is
that control features present on a control sister are
forced to appear on each other control sister (which has
the same mother).
One consequence of this modified CAP is that agree-
ment is mutual, or bidirectional, whereas in the CAP of
GPSG 85 it is unidirectional. Another consequence is
that, in the present implementation, it is impossible by
these means to express agreement between (for exam-
ple) the daughter NP and the NP &amp;quot;under the slash&amp;quot; in
S: NP, S [SLASH Ni].
This has not yet proved to be a problem; such agree-
ment can easily be accommodated by defining appro-
priate propagation constraints for those features (such
as PER, PLU and NFORM) that must agree.
Metarules. Despite the misgivings expressed earlier
concerning the possible exponential growth in grammar
size, a form of metarule mechanism has been incorpo-
rated. Metarules are implemented by precompilation by
the table generator. In fact, there is a separate metarule
preprocessing program, called metagee, which runs as a
Unix filter, passing the expanded set of rules to the table
generator proper. It would be possible to process sep-
arated ID/LP rules by means of a similar preprocessor.
This has not been done.
Form of the parser table. The output from the table
generator, the table which is interpreted by the parser,
comprises:
</bodyText>
<listItem confidence="0.994968333333333">
• an encoded list of rules, with a pointer from each
occurrence of a non-terminal on the right-hand side of
a rule to a list of rules with matching left-hand side
non-terminals;
• an encoded lexicon;
• a list of non-terminal names, feature names and
feature value names;
• a set of group masks;
• a set of FCR clauses, each comprising two bit masks.
</listItem>
<bodyText confidence="0.888102888888889">
One mask (word of bits) represents the category
and the other represents the category f_.
Performance. The parsing system has been tested with a
grammar for a subset of English. The grammar contains
512 rules after metarule expansion, comprising 228
non-lexical rules and 284 lexical rules. There are 107
feature value pairs. There are 18 FCRs, which, when
converted to clausal form, yield only 39 clauses. The
size of the parser table is about 63,000 bytes, about 93%
of which is occupied by the encoded rules. The remain-
ing 7% (4,500 bytes) comprises the tables of bit masks
Computational Linguistics, Volume 15, Number 3, September 1989 147
Anthony J. Fisher Practical Parsing of Generalized Phrase Structure Grammars
that represent the FCRs and the propagation masks, a
table of non-terminal names, and the lexicon. The table
generator takes about two minutes to compile the
grammar.
Typical parse times are given in figure 1. As the table
</bodyText>
<figure confidence="0.960270888888889">
Length of
sentence Parse time (seconds)
(words) Phase 1 Phase 2+3 Total
3 0.8 0.6 1.4
6 2.3 1.1 3.4
9 3.9 2.3 6.2
12 3.6 2.9 6.5
15 4.9 7.9t 12.8
t 4.2s excluding the time taken to format and print the trees
</figure>
<figureCaption confidence="0.999659">
Figure 1.
</figureCaption>
<bodyText confidence="0.99987325">
shows, for simple short sentences (unambiguous sen-
tences of fewer than 15 words), phase 1 consistently
takes more time than phases 2 and 3 together. For
sentences of moderate ambiguity, the times for phase 1
and phase 2+3 are comparable. The 15-word sentence
for which a time is given in the table is
which number ought Carol to have dialed on the
telephone the happy engineer was testing?
which, the parser correctly reports, is ambiguous (it has
two parses). Phase 1 yields a DAG that represents four
parses. Phase 2 expands this into four distinct trees, two
of which are then ruled out by phase 3. The figures for
phase 2+3 include the time taken to format and print the
trees, which for the longer sentences is not insignificant,
amounting to almost half of the processing time for the
15—word sentence.
</bodyText>
<sectionHeader confidence="0.997604" genericHeader="acknowledgments">
ACKNOWLEDGMENTS
</sectionHeader>
<bodyText confidence="0.985122">
The parsing system described in this paper was developed for use in
a research project that is funded by British Telecom Research
Laboratories (R18).
I am grateful to C.J. Cullen and S.J. Harlow for their comments on
this paper at various stages of revision.
</bodyText>
<sectionHeader confidence="0.994298" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999929309090909">
Barton, G. Edward, Jr. 1985 On the complexity of ID/LP parsing.
Computational Linguistics 11(4): 205-218.
Briscoe, Edward J. 1986 Grammar Development Environment. Inter-
nal report, Department of Linguistics and Modern English Lan-
guage, University of Lancaster, U.K.
Church, Kenneth and Patil, Ramesh 1982 Coping with syntactic
ambiguity or how to put the block in the box on the table.
Computational Linguistics 8(3-4): 139-149.
Earley, Jay 1968 An efficient context-free parsing algorithm. Ph.D.
thesis, Department of Computer Science, Carnegie-Mellon Uni-
versity, Pittsburgh, PA.
Earley, Jay 1970 An efficient context-free parsing algorithm. Commu-
nications of the Association for Computing Machinery 13(2):
94-102.
Fisher, Anthony J. 1985 Practical LL(1)-based parsing of van Wijn-
gaarden grammars. Acta Informatica 21: 559-584.
Gazdar, Gerald 1983 Recent computer implementations of phrase
structure grammars. Internal report, Cognitive Studies Pro-
gramme, University of Sussex, U.K.
Gazdar, Gerald; Klein, Ewan; Pullum, Geoffrey K.; and Sag, Ivan
1985 Generalized Phrase Structure Grammar. Basil Blackwell,
Oxford, U.K.
Harrison and Maxwell 1986 A New Implementation for GPSG. Proc.
6th Canadian Conf. on A.I. (CSCSI-86), May 21-23, Ecole Poly-
technique de Montréal, Montreal, Québec: 78-83.
Loveland, Donald W. 1978 Automated Theorem Proving: a Logical
Basis. North-Holland, Amsterdam, the Netherlands.
Pulman, S.G. 1985 Generalised phrase structure grammar, Earley&apos;s
algorithm, and the minimisation of recursion. In: Sparck Jones,
Karen; and Wilks, Yorick (eds.) 1985 Automatic Natural Lan-
guage Parsing. Ellis Horwood, Chichester, U.K.
Ristad, Eric S. 1985 GPSG recognition is NP-hard. Artificial Intelli-
gence Memo no. 837, MIT Artificial Intelligence Laboratory,
Cambridge, MA.
Salomaa, Arto 1973 Formal Languages. Academic Press, London,
England.
Shieber, Stuart M. 1983 Direct parsing of ID/LP grammars. Technical
report 291R, SRI International, Menlo Park, CA. Also in: Linguis-
tics and Philosophy 7(2): 135-154.
Thompson, Henry S. 1982 Handling metarules in a parser for GPSG.
Research paper no. 175, Department of Artificial Intelligence,
University of Edinburgh, U.K. Also in: Barlow, M.; Flickinger,
D.; and Sag, I.A. (eds.) Developments in Generalized Phrase
Structure Grammar. Stanford Working Papers in Grammatical
Theory 2: 26-37. Indiana University Linguistics Club, U.S.A.
Ritchie, Graeme; and Thompson, Henry S. 1984 Natural language
processing. In: O&apos;Shea, Tim and Eisenstadt, Marc 1984 Artificial
Intelligence: Tools, Techniques and Applications. Harper and
Row, New York, NY.
Wegner, L.M. 1980 On parsing two-level grammars. Acta Informatica
14: 175-193.
Wijngaarden, A. van; Mailloux, B.J.; Peck, J.E.L.; Koster, C.H.A.;
Sintzoff, M.; Lindsey, C.H.; Meertens, L.G.L.T.; and Fisker,
R.G. (eds.) 1976 Revised Report on the Algorithmic Language
Algol 68. Springer-Verlag, Berlin, W. Germany.
</reference>
<page confidence="0.937201">
148 Computational Linguistics, Volume 15, Number 3, September 1989
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.770699">
<title confidence="0.999034">PRACTICAL PARSING OF GENERALIZED PHRASE STRUCTURE GRAMMARS</title>
<author confidence="0.999999">Anthony J Fisher</author>
<affiliation confidence="0.983254">Computer The University of York</affiliation>
<address confidence="0.951028">Heslington, York YO1 5DD, U.K.</address>
<abstract confidence="0.965898333333333">algorithm is described for parsing a dialect of generalized phrase structure grammar (GPSG). A practical parsing system, based on the algorithm, is presented. The dialect of GPSG which the parsing system accepts is smaller, but considerably &amp;quot;purer&amp;quot; (closer to the original definition of GPSG) and mathematically &amp;quot;cleaner&amp;quot; than that which is accepted by other practical parsing systems. In particular, the parsing system correctly implements feature co-occurrence restrictions, subject only to the restriction that the FCR set can be expressed in clausal form as a set of Horn clauses.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Edward Barton</author>
</authors>
<title>On the complexity of ID/LP parsing.</title>
<date>1985</date>
<journal>Computational Linguistics</journal>
<volume>11</volume>
<issue>4</issue>
<pages>205--218</pages>
<contexts>
<context position="3531" citStr="Barton (1985)" startWordPosition="560" endWordPosition="561">conventions difficult to surmount. Briscoe&apos;s comments (1986) are typical: &amp;quot;Finally, the concept of privileged feature, its interaction with feature specification defaults and the bi-directionality of the head feature convention are all so complex that it is debatable how much use they would be in a practical system (even if we did manage to implement them)&amp;quot; (p. 1); &amp;quot;The interaction of feature co-occurrence restrictions, feature specification defaults and feature propagation proved very hard to implement/understand&amp;quot; (p. 2). This paper does not address the issues of ID/LP parsing and metarules. Barton (1985) has shown that the ID/LP parsing problem is NP-complete (on the total problem size). He argued that a previous result of Shieber (1983), which purported to give a G2 parsing method for ID/LP grammars, was incorrect. Barton claimed that Shieber&apos;s algorithm is exponential in the worst case. Barton&apos;s result alone might be considered a Copyright 1989 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included</context>
</contexts>
<marker>Barton, 1985</marker>
<rawString>Barton, G. Edward, Jr. 1985 On the complexity of ID/LP parsing. Computational Linguistics 11(4): 205-218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward J Briscoe</author>
</authors>
<title>Grammar Development Environment.</title>
<date>1986</date>
<tech>Internal report,</tech>
<institution>Department of Linguistics and Modern English Language, University of Lancaster, U.K.</institution>
<marker>Briscoe, 1986</marker>
<rawString>Briscoe, Edward J. 1986 Grammar Development Environment. Internal report, Department of Linguistics and Modern English Language, University of Lancaster, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>Ramesh Patil</author>
</authors>
<title>Coping with syntactic ambiguity or how to put the block in the box on the table.</title>
<date>1982</date>
<journal>Computational Linguistics</journal>
<volume>8</volume>
<issue>3</issue>
<pages>139--149</pages>
<contexts>
<context position="33471" citStr="Church and Patil 1982" startWordPosition="5892" endWordPosition="5895">5, Number 3, September 1989 Anthony J. Fisher Practical Parsing of Generalized Phrase Structure Grammars take time proportional to K, although the operations involving!, can be done in constant time, since!, has either zero or one member. A time bound for unify is therefore K2G2n2. Finally, unify is obeyed p times, which gives a time bound for phase 3 of order pK2G2n2. It is unfortunate that, as noted earlier, p is not in general independent of n. To see why this is so, consider the following grammar. Non-terminal alphabet: {S} Terminal alphabet: fal Rules: S --&gt; S S S--&gt; a It has been shown (Church and Patil 1982) that the number of distinct trees generated, for a sentence of length n, grows factorially with n. This means that the algorithm as a whole will take factorial time to parse a sentence of length n according to this grammar. This is a matter of concern, because constructions similar to this example are commonly used to handle coordination. It is even possible in principle for p to be infinite, in which case the algorithm will not terminate (although the advertised time bound ofpK2G2n3 will still hold!). In practice, however, no grammar has been encountered which unavoidably has infinite p. (Se</context>
</contexts>
<marker>Church, Patil, 1982</marker>
<rawString>Church, Kenneth and Patil, Ramesh 1982 Coping with syntactic ambiguity or how to put the block in the box on the table. Computational Linguistics 8(3-4): 139-149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1968</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, Carnegie-Mellon University,</institution>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="18344" citStr="Earley (1968)" startWordPosition="3154" endWordPosition="3155">ries on nodes. In other words, each parse tree according to G is the same &amp;quot;shape&amp;quot; as some parse tree (of the same sentence) according to G&apos;. Phase 1. We now parse G&apos; by applying a modified form of Earley&apos;s algorithm (Earley 1970; see also Pulman 1985, Ritchie and Thompson 1984). (The reader is assumed to be familiar with Earley&apos;s algorithm, in particular with the role played by the predictor in adding new states to a state set.) The algorithm is extended so that it creates a parse tree as the parse progresses. A method for doing this is described briefly by Earley (1970) and in more detail by Earley (1968). There is no need to handle the percolating feature propagation constraint at this stage, because in G&apos; Fp is empty. There is no need to consider what happens when the evaluation of an FCR causes a new feature to be added to a category, since the FCR set F&apos; is side-effectfree. We can therefore treat G&apos; as a CFG whose non-terminals are categories, provided that we allow for extension (condition 3 of section 2) and the trickling feature constraint (condition 5(ui)). This is done in the following way. A category appears on a node by virtue of the appearance of a category c1 on the right-hand sid</context>
</contexts>
<marker>Earley, 1968</marker>
<rawString>Earley, Jay 1968 An efficient context-free parsing algorithm. Ph.D. thesis, Department of Computer Science, Carnegie-Mellon University, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the Association for Computing Machinery</journal>
<volume>13</volume>
<issue>2</issue>
<pages>94--102</pages>
<contexts>
<context position="17959" citStr="Earley 1970" startWordPosition="3086" endWordPosition="3087"> L(G) C L(G&apos;), since whenever conditions 4 and 5 of section 2 hold for a derivation according to G, they will also hold for a corresponding derivation according to G&apos;. (Remember that F D F&apos;.) Informally, G&apos; is more permissive than G. For the same reason, each parse according to G has a corresponding parse according to G&apos;, differing only in the distribution of features among categories on nodes. In other words, each parse tree according to G is the same &amp;quot;shape&amp;quot; as some parse tree (of the same sentence) according to G&apos;. Phase 1. We now parse G&apos; by applying a modified form of Earley&apos;s algorithm (Earley 1970; see also Pulman 1985, Ritchie and Thompson 1984). (The reader is assumed to be familiar with Earley&apos;s algorithm, in particular with the role played by the predictor in adding new states to a state set.) The algorithm is extended so that it creates a parse tree as the parse progresses. A method for doing this is described briefly by Earley (1970) and in more detail by Earley (1968). There is no need to handle the percolating feature propagation constraint at this stage, because in G&apos; Fp is empty. There is no need to consider what happens when the evaluation of an FCR causes a new feature to b</context>
<context position="30805" citStr="Earley 1970" startWordPosition="5415" endWordPosition="5416">a new feature, and adding appropriate rules to the grammar. The details of this have yet to be worked out; it is presented as a possible solution to a problem that has not yet arisen. Time and space bounds. The following parameters are relevant to a consideration of time and space bounds for the algorithm: • p, the degree of ambiguity of the skeleton grammar; • K, the cardinality of VF; • G, the number of rules in the grammar; • n, the length of the sentence being parsed. Earley&apos;s algorithm, as is well known, operates in time order G2n3. Earley&apos;s proof of the time complexity of his algorithm (Earley 1970) is in no way affected by the elaboration of the predictor to handle feature matching. In particular, the number of states in a state set does not increase with K. Although K features may in principle be combined to construct 2&amp;quot; different categories, the algorithm generates new categories by extension only when they are required. In fact, if a new feature specification is added to a rule that is previously unspecified for that feature, the state sets will either remain the same size or become smaller, since adding a feature restricts the range of rules that the rule in question will &amp;quot;match&amp;quot;. S</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Earley, Jay 1970 An efficient context-free parsing algorithm. Communications of the Association for Computing Machinery 13(2): 94-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony J Fisher</author>
</authors>
<title>Practical LL(1)-based parsing of van Wijngaarden grammars.</title>
<date>1985</date>
<journal>Acta Informatica</journal>
<volume>21</volume>
<pages>559--584</pages>
<marker>Fisher, 1985</marker>
<rawString>Fisher, Anthony J. 1985 Practical LL(1)-based parsing of van Wijngaarden grammars. Acta Informatica 21: 559-584.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
</authors>
<title>Recent computer implementations of phrase structure grammars.</title>
<date>1983</date>
<tech>Internal report,</tech>
<institution>Cognitive Studies Programme, University of Sussex, U.K.</institution>
<contexts>
<context position="9780" citStr="Gazdar (1983)" startWordPosition="1595" endWordPosition="1596">e algorithm worse-than-polynomial on n in the worst case. In practice, however, the algorithm can be implemented quite efficiently in such a way that p n for many linguistically plausible grammars. Previous implementations. There have been several previous attempts to parse GPSGs. Several workers have made wholesale changes to the definition of GPSG in order to make the formalism easier to parse. The dialect that the presen,t algorithm accepts is considered to be &amp;quot;purer&amp;quot; and nearer to the original than that accepted by most other algorithms. A brief summary of GPSG implementations is given by Gazdar (1983). The &amp;quot;official&amp;quot; definition of GPSG has changed since the list was published, and some of the implementations listed are no longer available. A more recent implementation is that of Harrison and Maxwell (1986). 2 DEFINITIONS Preliminary definitions. The following definitions, of standard terms of formal language theory, are given in, for example, Salomaa (1973). An alphabet is a finite non-empty set. The elements of an alphabet are called letters. A word over an alphabet V is a finite string consisting of zero or more letters of V, whereby the same letter may occur 140 Computational Linguistic</context>
</contexts>
<marker>Gazdar, 1983</marker>
<rawString>Gazdar, Gerald 1983 Recent computer implementations of phrase structure grammars. Internal report, Cognitive Studies Programme, University of Sussex, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
<author>Ewan Klein</author>
<author>Geoffrey K Pullum</author>
<author>Ivan Sag</author>
</authors>
<title>Generalized Phrase Structure Grammar. Basil Blackwell,</title>
<date>1985</date>
<location>Oxford, U.K.</location>
<contexts>
<context position="843" citStr="Gazdar et al. 1985" startWordPosition="121" endWordPosition="124">eneralized phrase structure grammar (GPSG). A practical parsing system, based on the algorithm, is presented. The dialect of GPSG which the parsing system accepts is smaller, but considerably &amp;quot;purer&amp;quot; (closer to the original definition of GPSG) and mathematically &amp;quot;cleaner&amp;quot; than that which is accepted by other practical parsing systems. In particular, the parsing system correctly implements feature co-occurrence restrictions, subject only to the restriction that the FCR set can be expressed in clausal form as a set of Horn clauses. 1 INTRODUCTION The generalized phrase structure grammar (GPSG) (Gazdar et al. 1985) is one of the most recent, and currently one of the most popular, formalisms used by linguists to describe the syntax of natural (human) languages. A GPSG is basically a context-free grammar (CFG), whose non-terminals are complex symbols called categories. A category is a set of features. The CFG is augmented by: • a set of conventions or constraints that govern the automatic &amp;quot;propagation&amp;quot; of features between categories on different nodes of the parse tree; and • a set of propositions (Boolean formulas whose literals denote the presence of a feature in a category) that are required to hold fo</context>
<context position="5056" citStr="Gazdar et al. 1985" startWordPosition="799" endWordPosition="802">s inappropriate, both as a practical computer-based parsing method and as a possible model for the way in which people parse sentences. It is of course probable that, for many grammars, a sufficiently efficient implementation of ID/LP could be obtained, either by means of a preprocessor or by using Shieber&apos;s algorithm, but this has not been attempted in the present implementation. The omission of ID/LP does not appear seriously to limit the usefulness of the parsing system, at least when applied to grammars of English. The question of metarules deserves a less cursory discussion. It is known (Gazdar et al. 1985:65-67) that the addition of metarules to a GPSG does not alter the CF properties of the grammar. The finite closure rule, which (stated informally) prohibits a metarule from reprocessing its own output, implies that the addition of a single metarule to a GPSG can generate only a finite number of phrase structure (PS) rules for each PS rule in the grammar. However, it is easy to write a metarule that will match every PS rule in the grammar and generate, say, two output rules. A second metarule can then be written which does the same. It is clear, therefore, that in the worst case the size of t</context>
<context position="6637" citStr="Gazdar et al. 1985" startWordPosition="1071" endWordPosition="1074">etarules. In practice, however, exponential behaviour can safely be presumed to be rare, since a typical metarule &amp;quot;triggers&amp;quot; on only a small number of rules and generates only a small number of rules. Preliminary investigations with a &amp;quot;real&amp;quot; grammar of English suggest that the precompilation phase generates an output grammar (containing only PS rules) that is about twice as big as the input grammar (which contains metarules and PS rules). The metarules of GPSG are related to the hyperrules of a van Wijngaarden grammar (VWG) (van Wijngaarden et al. 1976), as has often been observed (see, e.g., Gazdar et al. 1985:65). There exist various direct methods of parsing VWGs, which do not rely on the prior expansion of hyperrules to generate a (usually large) set of PS rules. Wegner&apos;s method (1980) and Fisher&apos;s method (1985) are, unfortunately, exponential in the worst case; however, the exponential behaviour of both algorithms stems from the fact that the finite closure property of GPSGs does not apply to VWGs, and the set of induced PS rules might be infinite. It is possible that these algorithms could be modified to handle the very restricted metarules of GPSG in polynomial time. Besides the omission of F</context>
<context position="19637" citStr="Gazdar et al. 1985" startWordPosition="3388" endWordPosition="3391">and side of a rule R2. The extension condition permits the free addition of features to c1 and to c2 to generate the category c which appears on the node. By the extension condition, c1 C c and c2 C c, which implies that c D (c1 U c2). Now let us neglect for the moment the trickling feature constraint. Since we are ignoring the non-side-effect-free FCRs and the propagation constraints, any superset of c1 U c2 which satisfies F&apos;(c) will suffice; consequently, we take the smallest superset, namely c1 U c2, which is the least upper bound of c1 and c2 under the ordering relation of extension (see Gazdar et al. 1985:39). Now we consider the trickling feature constraint. The effect of this constraint is to instantiate extra features on certain categories: those features that belong to a mother category and which are also members of FT must be instantiated on each daughter category. The category that is instantiated on the node of the parse tree is the smallest superset of c1 U c2 which contains all of its mother&apos;s trickling features, which is c1 U c2 U (co Cl FT), where co is the category of the mother node. To determine the category to place on a node of the parse tree, therefore, the algorithm needs to </context>
<context position="29624" citStr="Gazdar et al. 1985" startWordPosition="5200" endWordPosition="5203">ture from f, on a different copy of the parse tree, finally presenting the user of the parsing system with all of the parse trees. This would cause a combinatorial explosion, since the splitting and copying would have to be done at each level of the parse tree at which the particular feature in question is instantiated. The linguistic consequences of the Horn clause restriction are not clear, but experience with the parsing system suggests that they are not severe. The Horn clause restriction prohibits the grammar writer from writing FCRs such as [PRD +] A [VFORM] D [VFORM PAS] v [VFORM PRP] (Gazdar et al. 1985:111), in which a disjunction of non-negated literals appears on the right of D . It is in such FCRs that the Horn clause restriction appears in its true colours, as a mechanism for curbing a combinatorial explosion or, to put it another way, a mechanism for prohibiting a source of non-determinism. If the consequences of forbidding such FCRs later appear too severe, the possibility will be investigated of moving the non-determinism from the FCRs into the rules of the grammar, by replacing an FCR like the one above by a new FCR [PRI) +1 A [VFORM] D [F] where F is a new feature, and adding appro</context>
<context position="37898" citStr="Gazdar et al. (1985)" startWordPosition="6640" endWordPosition="6643">e table generator also performs various checks to ensure, as far as possible, that the grammar is well formed. Besides the obvious syntactic checks (to detect such errors as a comma in the wrong place), the table generator checks that the FCR set is not identically false, that there are no obvious blind alleys or non-reachable categories (this is not checked rigorously), and that various other subtle &amp;quot;well-formedness&amp;quot; conditions are satisfied. This error checking has proved very useful in practice, since GPSGs are notoriously difficult to debug. The input grammar is written in the notation of Gazdar et al. (1985), with a few concessions to the limitations of the typical computer input device. In particular, features have values, and what we have referred to as a feature is, in the notation accepted by the table generator, a feature-value pair, written If v]. Each distinct feature value pair is associated by the table generator with a particular bit in a computer word. A category is represented by a set of bits, i.e., by a word with several bits set, one for each feature-value pair in the set. Category valued features correspond to trees, and a distinct bit is allocated to each terminal node of the tre</context>
<context position="42088" citStr="Gazdar et al. (1985)" startWordPosition="7392" endWordPosition="7395"> In practice, most GPSGs closely resemble traditional CFGs, with most categories fully specified for the &amp;quot;major&amp;quot; features N, V. and perhaps BAR. Consequently, the group FCR constraints ensure that the skeleton grammar also resembles a traditional CFG, and is certainly not, in practice, massively ambiguous. Indeed, the table generator insists that categories in rules are written as X[Y], where X is a name (a traditional non-terminal), and Y is a category. The non-terminal X is defined (by the grammar writer) to stand for some set of major features. This convention is perhaps controversial, but Gazdar et al. (1985) is full of such rules, and the linguists who use the parsing system have not grumbled yet. The convention does allow the table generator to check the grammar more stringently than would otherwise be the case, and it enables the parser to be made considerably more efficient, by dividing the set of all categories (which must be searched by the predictor) into disjoint subsets. The convention has no theoretical significance; the program would work without it. The head feature convention. The grammar writer is able to denote certain non-terminals on the right-hand side of a rule as head non-termi</context>
<context position="43418" citStr="Gazdar et al. (1985" startWordPosition="7613" endWordPosition="7616">he non-terminal in the rule by a star. The percolation and trickling of features can be restricted to occur only between a mother and a head daughter. There are thus nine possible propagation behaviours for any feature: one of not trickling trickling, but only to head daughters trickling to all daughters together with one of not percolating percolating, but only from head daughters percolating from any daughter The head feature convention is simulated by defining head features to trickle, but only to heads. This is adequate in most situations, but it falls short of the behaviour postulated by Gazdar et al. (1985:94f1). In particular, the notion of free feature specification sets is not accommodated. This causes problems in, for example, the treatment of conjunctions, in which the conjoined constituents are conventionally all heads. GPSG 85 allows a rule that in our notation would be written NI&apos;: *NP [CONJ and], *NP [CONJ NIL]. In the present implementation, any PER feature (for example) which happens to be present on the mother would trickle to both head daughters, thereby forcing agreement between the daughters. Our solution has been to make the daughters non-heads, which is unattractive, but which </context>
<context position="47039" citStr="Gazdar et al. (1985)" startWordPosition="8234" endWordPosition="8237"> have been enforced. It does not alter the tree; it merely checks that the foot condition is true on each node. Note that all of this follows from the definition. It is not necessary to put forward a procedural definition of the FC, which would fit ill with the non-procedural definition of GPSG. In contrast to the GPSG 85 FFP, the FC readily permits a straightforward, efficient, and deterministic implementation. The control agreement principle. A mechanism has been provided for specifying horizontal propagation of features in a way similar to that implied by the control agreement principle of Gazdar et al. (1985). Sister categories in a rule may be designated control sisters (by prefixing the name of the non-terminal by a dollar). A set of control features is defined by the grammar writer, analogous to the sets of trickling and percolating features. Each non-terminal node N has associated with it an extra node N&apos;. If a node No has daughters NI, . . Nn, then No&apos; is called the stepmother of each NI, . . N. If Ni is a control sister, then any control features in C(Ni) are required to percolate to the stepmother, and any features on the stepmother are required to trickle to each stepdaughter that is a con</context>
</contexts>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>Gazdar, Gerald; Klein, Ewan; Pullum, Geoffrey K.; and Sag, Ivan 1985 Generalized Phrase Structure Grammar. Basil Blackwell, Oxford, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harrison</author>
<author>Maxwell</author>
</authors>
<title>A New Implementation for GPSG.</title>
<date>1986</date>
<booktitle>Proc. 6th Canadian Conf. on A.I. (CSCSI-86), May 21-23, Ecole Polytechnique de</booktitle>
<pages>78--83</pages>
<location>Montréal, Montreal, Québec:</location>
<contexts>
<context position="9989" citStr="Harrison and Maxwell (1986)" startWordPosition="1627" endWordPosition="1630">s. Previous implementations. There have been several previous attempts to parse GPSGs. Several workers have made wholesale changes to the definition of GPSG in order to make the formalism easier to parse. The dialect that the presen,t algorithm accepts is considered to be &amp;quot;purer&amp;quot; and nearer to the original than that accepted by most other algorithms. A brief summary of GPSG implementations is given by Gazdar (1983). The &amp;quot;official&amp;quot; definition of GPSG has changed since the list was published, and some of the implementations listed are no longer available. A more recent implementation is that of Harrison and Maxwell (1986). 2 DEFINITIONS Preliminary definitions. The following definitions, of standard terms of formal language theory, are given in, for example, Salomaa (1973). An alphabet is a finite non-empty set. The elements of an alphabet are called letters. A word over an alphabet V is a finite string consisting of zero or more letters of V, whereby the same letter may occur 140 Computational Linguistics, Volume 15, Number 3, September 1989 Anthony J. Fisher Practical Parsing of Generalized Phrase Structure Grammars several times. The set of all words over an alphabet V is denoted by W(V). For any V, W(V) is</context>
</contexts>
<marker>Harrison, Maxwell, 1986</marker>
<rawString>Harrison and Maxwell 1986 A New Implementation for GPSG. Proc. 6th Canadian Conf. on A.I. (CSCSI-86), May 21-23, Ecole Polytechnique de Montréal, Montreal, Québec: 78-83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald W Loveland</author>
</authors>
<title>Automated Theorem Proving: a Logical Basis. North-Holland,</title>
<date>1978</date>
<location>Amsterdam, the Netherlands.</location>
<contexts>
<context position="15946" citStr="Loveland (1978" startWordPosition="2737" endWordPosition="2738">e parse-time phases that are executed one after the other. Precompilation. Given a GPSG G = (VF, VT, X0, R, F, Fp, FT), first define the side-effect-free FCR set F&apos; algorithmically in the following manner. Computational Linguistics, Volume 15, Number 3, September 1989 141 Anthony J. Fisher Practical Parsing of Generalized Phrase Structure Grammars 1. Convert F to clausal form, in other words a conjunction of disjunctions of terms, each of which is either an unnegated literal feature or a negated literal feature. (This can be done uniquely, apart from questions of ordering of terms; see, e.g., Loveland (1978:32f).) 2. Remove from the clause set all clauses (i.e., disjunctions) that contain one or more unnegated literals, leaving behind only those clauses that contain only negated literals. The resulting set of clauses represents the side-effectfree FCR set F&apos;. Since F&apos; was obtained from F by removing clauses, the resulting function F&apos; cannot be more restrictive than F; in other words, F D F&apos;. The reason for removing clauses that contain unnegated literals is that the evaluation of FCRs can, in general, cause the instantiation of new features on a node. This can be viewed as a &amp;quot;side effect&amp;quot; of the</context>
<context position="25655" citStr="Loveland (1978" startWordPosition="4468" endWordPosition="4469">he resulting set of parse trees would then in general be very large, and it is difficult to believe that this behaviour is desirable. Our smallest category is similar to the most general unifier of a set of expressions in mathematical logic; as in logic, particular, less general instances can be derived from the most general case, but it is the most general (least fully specified) case that is of most interest. We assume that the FCR set F is expressed in clausal form and that each clause (i.e., each disjunction) is a Horn clause (a clause with either zero or one unnegated literal; see, e.g., Loveland (1978:99)). Number the clauses F1, . . Fm. We denote by M(N) the mother of the node N, if it exists (i.e. unless N is the root of its tree). We denote by C(N) the category on the node N. Let the distinct parse trees produced by phase 2 be Tp. The algorithm unify, defined below, is applied to each Ti in turn, for i = 1, . . p. unify (T): Let the non-terminal nodes in T be NI, . . NN. 1. set again := false; 2. for j = 1, N do 2.1. if NJ is not the root of T and (c(m(N.,)) n FT) C(NJ) then 2.1.1. C(111:1) := C(Ni) U (C(M(NJ)) r-1 Fr); 2.1.2. set again := true; 2.2. if 1V:i is not the root of T and (c(</context>
</contexts>
<marker>Loveland, 1978</marker>
<rawString>Loveland, Donald W. 1978 Automated Theorem Proving: a Logical Basis. North-Holland, Amsterdam, the Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S G Pulman</author>
</authors>
<title>Generalised phrase structure grammar, Earley&apos;s algorithm, and the minimisation of recursion.</title>
<date>1985</date>
<editor>In: Sparck Jones, Karen; and Wilks, Yorick (eds.)</editor>
<location>Chichester, U.K.</location>
<contexts>
<context position="17981" citStr="Pulman 1985" startWordPosition="3090" endWordPosition="3091">henever conditions 4 and 5 of section 2 hold for a derivation according to G, they will also hold for a corresponding derivation according to G&apos;. (Remember that F D F&apos;.) Informally, G&apos; is more permissive than G. For the same reason, each parse according to G has a corresponding parse according to G&apos;, differing only in the distribution of features among categories on nodes. In other words, each parse tree according to G is the same &amp;quot;shape&amp;quot; as some parse tree (of the same sentence) according to G&apos;. Phase 1. We now parse G&apos; by applying a modified form of Earley&apos;s algorithm (Earley 1970; see also Pulman 1985, Ritchie and Thompson 1984). (The reader is assumed to be familiar with Earley&apos;s algorithm, in particular with the role played by the predictor in adding new states to a state set.) The algorithm is extended so that it creates a parse tree as the parse progresses. A method for doing this is described briefly by Earley (1970) and in more detail by Earley (1968). There is no need to handle the percolating feature propagation constraint at this stage, because in G&apos; Fp is empty. There is no need to consider what happens when the evaluation of an FCR causes a new feature to be added to a category,</context>
</contexts>
<marker>Pulman, 1985</marker>
<rawString>Pulman, S.G. 1985 Generalised phrase structure grammar, Earley&apos;s algorithm, and the minimisation of recursion. In: Sparck Jones, Karen; and Wilks, Yorick (eds.) 1985 Automatic Natural Language Parsing. Ellis Horwood, Chichester, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric S Ristad</author>
</authors>
<title>GPSG recognition is NP-hard.</title>
<date>1985</date>
<journal>Artificial Intelligence Memo</journal>
<volume>837</volume>
<institution>MIT Artificial Intelligence Laboratory,</institution>
<location>Cambridge, MA.</location>
<contexts>
<context position="2600" citStr="Ristad (1985)" startWordPosition="415" endWordPosition="416"> FSDs mainly by giving examples, although there are a few mathematical definitions that, however, appear to confuse the definition rather than to clarify it. A clear, formal definition of the effect of FSDs is urgently required. Because the present definition is so obscure, it was reluctantly decided to exclude FSDs from consideration in this paper. Because a GPSG is so closely related to a CFG, it was thought that the well-known efficient parsing techniques for CFGs could be applied, with modifications, to GPSGs, and that GPSGs would therefore be computationally tractable. Recently, however, Ristad (1985) has shown that this is not the case, and that the unrestricted GPSG parsing problem is NP-complete (on the total problem size, viz, grammar plus input sentence). Even before Ristad&apos;s result was known, workers in this field had found the practical problems caused by the interaction of FCRs, FSDs, and the propagation conventions difficult to surmount. Briscoe&apos;s comments (1986) are typical: &amp;quot;Finally, the concept of privileged feature, its interaction with feature specification defaults and the bi-directionality of the head feature convention are all so complex that it is debatable how much use t</context>
</contexts>
<marker>Ristad, 1985</marker>
<rawString>Ristad, Eric S. 1985 GPSG recognition is NP-hard. Artificial Intelligence Memo no. 837, MIT Artificial Intelligence Laboratory, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Salomaa</author>
</authors>
<title>Formal Languages.</title>
<date>1973</date>
<publisher>Academic Press,</publisher>
<location>London, England.</location>
<contexts>
<context position="10143" citStr="Salomaa (1973)" startWordPosition="1650" endWordPosition="1651">o make the formalism easier to parse. The dialect that the presen,t algorithm accepts is considered to be &amp;quot;purer&amp;quot; and nearer to the original than that accepted by most other algorithms. A brief summary of GPSG implementations is given by Gazdar (1983). The &amp;quot;official&amp;quot; definition of GPSG has changed since the list was published, and some of the implementations listed are no longer available. A more recent implementation is that of Harrison and Maxwell (1986). 2 DEFINITIONS Preliminary definitions. The following definitions, of standard terms of formal language theory, are given in, for example, Salomaa (1973). An alphabet is a finite non-empty set. The elements of an alphabet are called letters. A word over an alphabet V is a finite string consisting of zero or more letters of V, whereby the same letter may occur 140 Computational Linguistics, Volume 15, Number 3, September 1989 Anthony J. Fisher Practical Parsing of Generalized Phrase Structure Grammars several times. The set of all words over an alphabet V is denoted by W(V). For any V, W(V) is infinite. We denote by 11(S) the powerset of S, which is the set of all subsets of a set S. Definitions. A generalised phrase structure grammar (GPSG) is</context>
</contexts>
<marker>Salomaa, 1973</marker>
<rawString>Salomaa, Arto 1973 Formal Languages. Academic Press, London, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>Direct parsing of ID/LP grammars.</title>
<date>1983</date>
<journal>Linguistics and Philosophy</journal>
<tech>Technical report 291R,</tech>
<volume>7</volume>
<issue>2</issue>
<pages>135--154</pages>
<institution>SRI International,</institution>
<location>Menlo Park, CA.</location>
<note>Also in:</note>
<contexts>
<context position="3667" citStr="Shieber (1983)" startWordPosition="583" endWordPosition="584">with feature specification defaults and the bi-directionality of the head feature convention are all so complex that it is debatable how much use they would be in a practical system (even if we did manage to implement them)&amp;quot; (p. 1); &amp;quot;The interaction of feature co-occurrence restrictions, feature specification defaults and feature propagation proved very hard to implement/understand&amp;quot; (p. 2). This paper does not address the issues of ID/LP parsing and metarules. Barton (1985) has shown that the ID/LP parsing problem is NP-complete (on the total problem size). He argued that a previous result of Shieber (1983), which purported to give a G2 parsing method for ID/LP grammars, was incorrect. Barton claimed that Shieber&apos;s algorithm is exponential in the worst case. Barton&apos;s result alone might be considered a Copyright 1989 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page. To copy otherwise, or to republish, requires a fee and/or specific permission. 0362-613X/ 89 /010139-148$03.00 Compu</context>
</contexts>
<marker>Shieber, 1983</marker>
<rawString>Shieber, Stuart M. 1983 Direct parsing of ID/LP grammars. Technical report 291R, SRI International, Menlo Park, CA. Also in: Linguistics and Philosophy 7(2): 135-154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry S Thompson</author>
</authors>
<title>Handling metarules in a parser for GPSG. Research paper no.</title>
<date>1982</date>
<booktitle>Developments in Generalized Phrase Structure Grammar. Stanford Working Papers in Grammatical Theory</booktitle>
<tech>175,</tech>
<volume>2</volume>
<pages>26--37</pages>
<editor>Edinburgh, U.K. Also in: Barlow, M.; Flickinger, D.; and Sag, I.A. (eds.)</editor>
<institution>Department of Artificial Intelligence, University of</institution>
<contexts>
<context position="5775" citStr="Thompson (1982)" startWordPosition="928" endWordPosition="929">nite closure rule, which (stated informally) prohibits a metarule from reprocessing its own output, implies that the addition of a single metarule to a GPSG can generate only a finite number of phrase structure (PS) rules for each PS rule in the grammar. However, it is easy to write a metarule that will match every PS rule in the grammar and generate, say, two output rules. A second metarule can then be written which does the same. It is clear, therefore, that in the worst case the size of the induced set of PS rules grows exponentially with the number of metarules. For reasons put forward by Thompson (1982), most, if not all, GPSG parsing systems handle metarules by employing a precompilation phase to generate the induced set of PS rules. If this method is employed, the parsing time will, in the worst case, grow exponentially with the number of metarules. In practice, however, exponential behaviour can safely be presumed to be rare, since a typical metarule &amp;quot;triggers&amp;quot; on only a small number of rules and generates only a small number of rules. Preliminary investigations with a &amp;quot;real&amp;quot; grammar of English suggest that the precompilation phase generates an output grammar (containing only PS rules) th</context>
</contexts>
<marker>Thompson, 1982</marker>
<rawString>Thompson, Henry S. 1982 Handling metarules in a parser for GPSG. Research paper no. 175, Department of Artificial Intelligence, University of Edinburgh, U.K. Also in: Barlow, M.; Flickinger, D.; and Sag, I.A. (eds.) Developments in Generalized Phrase Structure Grammar. Stanford Working Papers in Grammatical Theory 2: 26-37. Indiana University Linguistics Club, U.S.A.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Ritchie</author>
<author>Henry S Thompson</author>
</authors>
<title>Natural language processing. In: O&apos;Shea, Tim and Eisenstadt,</title>
<date>1984</date>
<journal>Artificial Intelligence: Tools, Techniques</journal>
<location>Marc</location>
<contexts>
<context position="18009" citStr="Ritchie and Thompson 1984" startWordPosition="3092" endWordPosition="3095">tions 4 and 5 of section 2 hold for a derivation according to G, they will also hold for a corresponding derivation according to G&apos;. (Remember that F D F&apos;.) Informally, G&apos; is more permissive than G. For the same reason, each parse according to G has a corresponding parse according to G&apos;, differing only in the distribution of features among categories on nodes. In other words, each parse tree according to G is the same &amp;quot;shape&amp;quot; as some parse tree (of the same sentence) according to G&apos;. Phase 1. We now parse G&apos; by applying a modified form of Earley&apos;s algorithm (Earley 1970; see also Pulman 1985, Ritchie and Thompson 1984). (The reader is assumed to be familiar with Earley&apos;s algorithm, in particular with the role played by the predictor in adding new states to a state set.) The algorithm is extended so that it creates a parse tree as the parse progresses. A method for doing this is described briefly by Earley (1970) and in more detail by Earley (1968). There is no need to handle the percolating feature propagation constraint at this stage, because in G&apos; Fp is empty. There is no need to consider what happens when the evaluation of an FCR causes a new feature to be added to a category, since the FCR set F&apos; is sid</context>
</contexts>
<marker>Ritchie, Thompson, 1984</marker>
<rawString>Ritchie, Graeme; and Thompson, Henry S. 1984 Natural language processing. In: O&apos;Shea, Tim and Eisenstadt, Marc 1984 Artificial Intelligence: Tools, Techniques and Applications. Harper and Row, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L M Wegner</author>
</authors>
<title>On parsing two-level grammars.</title>
<date>1980</date>
<journal>Acta Informatica</journal>
<volume>14</volume>
<pages>175--193</pages>
<marker>Wegner, 1980</marker>
<rawString>Wegner, L.M. 1980 On parsing two-level grammars. Acta Informatica 14: 175-193.</rawString>
</citation>
<citation valid="true">
<title>Revised Report on the Algorithmic Language Algol 68.</title>
<date>1976</date>
<editor>Wijngaarden, A. van; Mailloux, B.J.; Peck, J.E.L.; Koster, C.H.A.; Sintzoff, M.; Lindsey, C.H.; Meertens, L.G.L.T.; and Fisker, R.G. (eds.)</editor>
<publisher>Springer-Verlag,</publisher>
<location>Berlin, W. Germany.</location>
<marker>1976</marker>
<rawString>Wijngaarden, A. van; Mailloux, B.J.; Peck, J.E.L.; Koster, C.H.A.; Sintzoff, M.; Lindsey, C.H.; Meertens, L.G.L.T.; and Fisker, R.G. (eds.) 1976 Revised Report on the Algorithmic Language Algol 68. Springer-Verlag, Berlin, W. Germany.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>