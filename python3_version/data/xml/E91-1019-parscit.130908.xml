<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<sectionHeader confidence="0.8781665" genericHeader="abstract">
AUTOMATIC LEARNING OF WORD TRANSDUCERS
FROM EXAMPLES
</sectionHeader>
<bodyText confidence="0.463707">
Michel GiIlaux
Centre National d&apos;Etudes des Telecommunications
</bodyText>
<note confidence="0.851459666666667">
LAA/SLC/AIA
Route de Tregastel, BP 40
F-22301 Lannion Cedex, FRANCE
</note>
<email confidence="0.998617">
e-mail: gilloux@latmion.cnet.fr
</email>
<sectionHeader confidence="0.989318" genericHeader="keywords">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.99989925">
This paper describes the application of
markovian learning methods to the infer-
ence of word transducers. We show how
the proposed method dispenses from the
difficult design of hand-crafted rules, al-
lows the use of weighed non deterministic
transducers and is able to translate words
by taking into account their whole rather
than by making decisions locally. These ar-
guments are illustrated on two examples:
morphological analysis and grapheme-to-
phoneme transcription.
</bodyText>
<sectionHeader confidence="0.997495" genericHeader="introduction">
INTRODUCTION
</sectionHeader>
<bodyText confidence="0.993441411764706">
Several tasks associated with elec-
tronic lexicons may be viewed as transduc-
tions between character strings. This may
be the decomposition of words into mor-
phemes in morpholow or the grapheme-to-
phoneme transcription in phonology. In the
first case, one has for example to decom-
pose the French word &amp;quot;chronornetrage&amp;quot; into
the sequence of affixes &amp;quot;chrono+rne tre+er+-
age&amp;quot;. In the second, &amp;quot;abstertir should be
translated into &amp;quot;abstentR&amp;quot; or &amp;quot;apsteniR&amp;quot;1.
Most of the proposed methods in the
&apos;These two tasks are in fact closely related
In that (1) the correct phoneme transcription
may mirror an underlying morphological struc-
ture, like for &amp;quot;asocial&amp;quot; whose phonemic form is
&amp;quot;asosjal&amp;quot; rather than &amp;quot;azosjal&amp;quot; due to the de-
composition &amp;quot;a+sociar, , and (2) the surface form
of a derived word may depend on the pronunci-
ation of its component morphemes, like for
&amp;quot;de+harnacher which results in &amp;quot;deharnacher
and not &amp;quot;desharnacher&amp;quot;
domain (Catach 1984; Danlos et al. 1986;
Koskenniemi 1983; Laporte 1988; Ritchie
et al. 1987; Tufts 1989; Wronis 1988) are
based on the availability of local rules
whose combination, either through direct
interpretation or by being compiled, form
the target transducer.
Although these methods make it pos-
sible - at least in theory - to design suitable
transducers, provided that the rule de-
scription language has the right expressive
power, they are complex to use because of
the difficulty of writing down rules. More-
over, for a given rule language, there may
not exist an algorithm for compiling rules
into a form better suited to the translation
process. Lastly, in numerous cases, the
translation procedures are improperly de-
terministic as shown by the example of &amp;quot;ab-
stenie so that it is not possible to consider
several competing hypotheses in parallel
not to speak of ranking them according to
some certainty factor.
We have designed a program which
allows to construct transducers without re-
taining the above shortcomings. It is no
longer necessary to write down translation
rules since the transducer is obtained as
the result of an automatic learning over a
set of examples. The transducer is repre-
sented into the language of probabilistic fi-
nite state automata (Markov models) so
that its use is straightforward. Lastly, it
produces results which are assigned a
probability and makes it possible to list
them by decreasing order of likelihood.
After stating the problem of character
strings translation and defining the few
- 107 -
central notions of markovian learning, this
paper describes their adaptation to the
word translation problem in the learning
and translating phases. This adaptation is
illustrated through two applications: mor-
phological analysis and grapheme-to-pho-
neme transcription.
</bodyText>
<sectionHeader confidence="0.910602" genericHeader="method">
THE TRANSDUCTION PROBLEM
</sectionHeader>
<bodyText confidence="0.999952909090909">
In the context of character strings
transduction, we look for an application f:
Cs C&apos;* which transforms certain words
built over the alphabet C into words over
the alphabet C&apos;. For example, in the case of
grapheme-to-phoneme transcription, C is
the set of graphemes and C&apos; that of pho-
nemes.
It may be appropriate, for example in
morphology, to use an auxiliary lexicon
(Ritchie et al. 1987; Ritchie 1989) which al-
lows to discard certain translation results.
For example, the decomposition &amp;quot;sage&amp;quot; ---&gt;
&amp;quot;ser+age&amp;quot; would not be allowed because
&amp;quot;ser&amp;quot; is not a verb in the French lexicon, al-
though this is a correct result with respect
to the splitting of word forms into affixes.
The method we propose in this paper is only
concerned with describing this last type of
regularities leaving aside all non regular
phenomena better described on a case-by-
case basis such as through a lexicon.
</bodyText>
<sectionHeader confidence="0.909388" genericHeader="method">
MARKOV MODELS
</sectionHeader>
<bodyText confidence="0.988134">
A Markov model is a probabilistic fi-
nite state automaton M = (S, T, A, SD 5F, g)
where S is a finite set of states, A is a finite
alphabet, si E S and sir S are two distin-
guished states called respectively the initial
state and the final state, T is a finite set of
transitions, and g is a function g: t e T —&gt;
(0(t), D(t), S(t), p(t)) E SxSxA x10, 11 such
that
</bodyText>
<subsectionHeader confidence="0.7724615">
V(s e s), p(t) = 1
ft&apos; 0(0= sl
</subsectionHeader>
<bodyText confidence="0.961212233333333">
where p(t) is the probability of reaching
state D(t) while generating symbol S(t)
starting from state 0(t).
In general, the transition probabili-
ties p(t) are mutually independent. Yet, in
some contexts, it may be useful to have
their values depend on others transitions.
In this respect, it is possible to define a one-
to-one correspondence tsee It I 0(t) = s&apos;)
(t I 0(t) = s) such that p(t) is equal to
,(t)). States a and s&apos; are then said to be
tier
For every word w = a ... an e A*, the
set of partial paths compatible with w till 1,
Pathi(w), is the set of sequences of 1 transi-
tions t1 ti such that 0(t1) = s, D(ti) =
0(tt&amp;quot;), for j = 1, ..., 1-1 and S(ti) = a, for j
= 1, ..., 1.
The set of complete paths compatible
with w, Path(w), is in turn the set of ele-
ments in Pathiun(w), where I w I = n, the
length of word Ui I, such that D(t) = sF.
The probability for the model M of
emitting the word w is
Probm (in) = fi p(t)
path e Path(w) t e path
A Markov model for which there exist
at most one complete path for a given word
is said to be unifilctr. In this case, the above
probability is reduced to
</bodyText>
<equation confidence="0.948495666666667">
probt(w)=H p(t), if Path(w) = path
t e path
Probm(w) =0, if Path(w) =0
</equation>
<bodyText confidence="0.9999124375">
Thus the probability Pm(w) may be
generally computed by adding the probabil-
ities observed along every path compatible
with w. In practice, this renders computa-
tionally expensive the algorithm for com-
puting Pm(w) and it is tempting to assume
that the model is unifilar. Practical studies
have shown that this sub-optimal method
Is applicable without great loss (Bahl et al.
1983).
Under this hypothesis, the probabili-
ty Pm(w) may be computed through the Vit-
erbi dynamic programming algorithm. In-
deed, the probability Pm(w, 1, s), maximal
probability of reaching state s with the i
first transitions in a path compatible with w
</bodyText>
<equation confidence="0.902709">
- 108 -
is
Pm(w, 1, a) = max 11 pap, 1= 1...n
1=1
</equation>
<bodyText confidence="0.958043714285714">
where (paths ft, ...ti e Pathi(w)1 D(ti) = s})
Pm(w, 0, ai) = 1
Pm(w, 0, s) = 0, lf (a* el)
therefore
Pm(w, 1+ 1, a) = maxpath (p(ti+ 1) • Pm(w, 1, D(ti)))
where paths (t1 ...t,+ is Pathi+ i(w)l D01+ i) = s}
whereby
</bodyText>
<equation confidence="0.780848">
Pm(w, 1+ 1, s) = maxt (p(t) • Pm(w, 1, 0(t)))
</equation>
<bodyText confidence="0.8380825">
where (t e D(t) = a and S(t) = at+ )
with
</bodyText>
<equation confidence="0.8843095">
Probm(w) = Pm(w, Iwl,ar.) = n p(t)
t a MaxPath(w)
</equation>
<bodyText confidence="0.955130333333333">
It is therefore possible to compute
Pm (w, I s) recursively for i = 1, ..., n until
ProbM (w)
</bodyText>
<subsectionHeader confidence="0.7263755">
Automatic learning of Markov
models
</subsectionHeader>
<bodyText confidence="0.9999215">
Given a training set TS made of words
In A* and a number N &gt; 2 of states, that is
the set S, learning a Markov model consists
In finding a set T of transitions such that
the joint probability P of the examples in
the training set
</bodyText>
<equation confidence="0.984657">
P(TS) = fl P(w)
w e TS
</equation>
<bodyText confidence="0.988811322580645">
Is maximal.
In general, the set T is composed a
priori of all possible transitions between
states in S producing a symbol in A. The de-
termination of probabilities p associated
with these transitions Is equivalent to the
restriction of T to elements with non null
probability which induces the structure of
the associated automaton. In this case, the
model is said to be hidden because it is
hard to attach a meaning to the states in S.
On the contrary, it is possible to force those
states to have a clear-cut interpretation by
defining them, for example, as n-grams
which are sequences of n elements in A
which encode the last n symbols produced
by the model to reach the state. It is clear
that then only some transitions are mean-
ingful. In dealing with problems like those
studied in the present paper it is preferable
to use hidden models which allow states to
stand for arbitrarily complex predicates.
The learning algorithm (Bahl et al.
1983) is based upon the following remark:
given a model M whose transitions proba-
bilities are known a priori, the a posteriori
probability of a transition t may be estimat-
ed by the relative frequency with which t is
used on a training set.
The number of times a transition t is
used on TS is
</bodyText>
<equation confidence="0.97413">
freq(t) = z 6(t, t.)
w e TS t&apos; e MaxPath(w)
</equation>
<bodyText confidence="0.994529666666667">
where 8 (t, t&apos;) if to t&apos; , 0 otherwise
The relative frequency of using t on
TS is
</bodyText>
<equation confidence="0.816204666666667">
rel-freq(t) — freq(t)
freq(t&apos;))
ft&apos; (001= OM) I
</equation>
<bodyText confidence="0.936620666666667">
The learning algorithm consists then
In setting randomly the probability distri-
bution p(t) and adjusting iteratively its val-
ues through the above formula until the ad-
justment is small enough to consider the
distribution as stationary. It has been
shown (Bahl et al. 1983) that this algorithm
does converge towards a stationary value of
the p(t) which maximizes locally&apos; the prob-
ability P of the training set depending on
the initial random probability distribution.
11n order to find a global optimum. we used
a kind of simulated annealing technique (Kirk-
patrick et al. 1983) during the learning process.
- 109 -
The stationary distribution defines the
Markov model induced from the examples
in TS1.
</bodyText>
<sectionHeader confidence="0.912341" genericHeader="method">
TRANSDUCTION MODEL
</sectionHeader>
<bodyText confidence="0.998029808510638">
To be applied in both illustrative ex-
amples, the general structure of Markov
models should be related, by means of a
shift in representation, to the problem of
strings translation. The model of two-level
morphological analysis (Koskenniemi 1983)
suggests the nature of this shift. Indeed,
this method, which was successfully ap-
plied to morphologically rich natural lan-
guages (Koskenniemi 1983), is based upon
a two-level rule formalism for which there
exist a way to compile them into the lan-
guage of finite state automata (FSA) (Ritchie
1989). This result validates the idea that
FSAs are reasonable candidates for repre-
senting transduction rules, at least in the
case of morpholoe.
The shift in representation is de-
signed so as to define the alphabet A as the
set of pairs c:- or -:c&apos; where c e C and C E
C&apos;, — standing for the null character, - C,
- e C&apos;. The mapping between the transduc-
er f and the associated Markov model M is
now straightforward:
1In practice. the number N = Card(S) of
states for the model to be learned on a training
set is not known. When N is small, the model
has a tendency to generate much more charac-
ter strings that were in TS due to an overgener-
alization. At the other end of the spectrum. when
N is large. the learned model will describe the ex-
amples in TS and them only. So, it is among the
intermediate values of N that an optimum has to
be looked for,
2Ritchie (1989) has even shown that the
generative power of two-level morphological an-
alyzers is strictly bound by that of finite state
automata. He proved that all languages L gener-
ated by these analyzers are such that whenever
E2, E3 and E1E2E3E belong to L, then E2E3
also belongs to L. Although this point was not
considered in the present study, we may sup-
pose that constraining the learned automaton to
respect this last property, for example by means
of tying states, would improve the overall results
by augmenting in a sound way the generaliza-
tion from examples.
</bodyText>
<equation confidence="0.892157666666667">
w&apos; E f(w) iff
3 x = xi ... xn E(Cu(_))*,
y = y yn E (C. t.) (-1).
</equation>
<bodyText confidence="0.908614666666667">
such that
xi:yi is of the form -:c&apos; or c:-,
for 1= 1, , n,
Probm(xi:y xn:yn) 0,
w = delete(x) and w&apos; = delete(y)
where the function delete is defined as
</bodyText>
<equation confidence="0.87184125">
delete(X) = (X is the empty string),
delete(-2) = delete(Z) and
delete(zZ) = z.delete(Z) if z E C or z E
Given a training set TS = kw, ve&gt; I w
E C., w&apos; E C&apos;1, the problem is thus to find
the model M that maximizes the probability
PH max(„,y)probaxi:y1...xn:y.)
(w,v) e TS
</equation>
<bodyText confidence="0.972666958333333">
where delete(x) = w and delete(y) =w&apos;
This formula makes it clear what is
the new difficulty with this type of learning,
namely the indetermination of words x and
y, that is of the alignment induced by them
between w and its translation w&apos;. The no-
tions of partial and complete compatible
paths should thus be redefined in order to
take this into account.
The partial paths compatible with w
and w&apos; till I and j are now the set of se-
quences t1 tto , Path( w, w&apos;) such that
0(t1) = 5/, D(tk) =13(tk+1)&apos; for k= 1,..., i+j-
1, S(tk) = xic.:yk, for k = 1, ..., i+j,
= w and delete(y ...yi+j) =
w&apos;i...w&apos;i. A. partial path is also complete as
soon ag I = Iwt, J= lw&apos;l and D(t
lAel) = sF.
As before, we can define the probabil-
ity Pm(w, I, w&apos;, j, s) of reaching state s along
a partial path compatible with w and w&apos; and
generating the first I symbols in w and j first
symbols in w&apos;.
max ••• t 11 Pak)
</bodyText>
<construct confidence="0.74729225">
1 14-11t&lt;i+)
where (t1 ti + ja (Pathi,j(w, w&apos;)l D(tI + j) = a) )
Pm(w, 0, w&apos;, 0, = 1
Pm(w, 0, w&apos;, 0, s) = 0, if a *
</construct>
<bodyText confidence="0.954129230769231">
- 110 -
Here again, this probability is such
that Prob(w• w&apos;) = Pm(w, lwl, w&apos;, iw&apos;l, sF) and
may be computed through dynamic pro-
gramming according to the formula
It is now possible to compute for every
training example the optimal path corre-
sponding to a given probability distribution
p(t). This path not only defines the crossed
states but also the alignment between w
and w&apos;. The learning algorithm applicable to
general markovian models remains valid for
adjusting iteratively the probabilities p(t).
</bodyText>
<sectionHeader confidence="0.769076" genericHeader="evaluation">
EXPERIMENTS
Morphological analysis
</sectionHeader>
<bodyText confidence="0.999656583333333">
As a preliminary experiment, the
morphological analysis automaton was
learned on a set of 738 French words end-
ing with the morpheme &amp;quot;isme&amp;quot; and associ-
ated with their decomposition into two mor-
phemes, the first being a noun, or an
adjective. For example, we had the pair
&lt;&amp;quot;athletisme&amp;quot;,&amp;quot;athiete+isme&amp;quot;›. With. a 400
states only automaton, the correct decom-
position was found amongst the 10 most
probable outputs for 97.6% of the training
examples
</bodyText>
<sectionHeader confidence="0.589055" genericHeader="evaluation">
1.
</sectionHeader>
<subsectionHeader confidence="0.830719">
Grapheme-to-phoneme
transcription
</subsectionHeader>
<bodyText confidence="0.9995545">
The case of grapheme-to-phoneme
transcription is a straightforward applica-
tion of the transduction model. String w is
the graphemic form, e.g. &amp;quot;abstenie and w&apos;
</bodyText>
<footnote confidence="0.6862695">
1We are aware that a more precise assess-
ment of the method would use a test set different
from the training set. We plan to perform such a
test In the near future.
</footnote>
<bodyText confidence="0.999795136363636">
is its transcription into phonemes, e.g. &amp;quot;ap-
steniR&amp;quot; or &amp;quot;absteniR&amp;quot;. Here the training
set may feature such pairs as &lt;w, w&apos;&gt; and
&lt;w, w&amp;quot;&gt; where w&apos; w&amp;quot;.
The automaton was learned on a set
of 1170 acronyms associated to their pho-
nemic form which was described in a
coarse phonemic alphabet where, for exam-
ple, open or closed /o/ are not distin-
guished. Acronyms raise an interesting
problem in that some should be spelled let-
ter by letter (&amp;quot;ACL&amp;quot;) whereas others may be
pronounced (&amp;quot;COLING&amp;quot;). This experiment
was thus intended to show that the model
may take into account its input as a whole.
With a 400 states only automaton, more
than 50% of the training examples were
correctly transcribed when only the most
probable output was considered. This fig-
ure may be improved by augmenting the
number of states in which case the learning
phase becomes much longer.
</bodyText>
<sectionHeader confidence="0.998702" genericHeader="conclusions">
CONCLUSION
</sectionHeader>
<bodyText confidence="0.999982642857143">
We have proposed a method for learn-
ing transducers for the tasks of morpholog-
ical analysis and grapheme-to-phoneme
transcription. This method may be favor-
ably compared to others solutions based
upon writing rules in the sense that it does
not oblige to identify rules, it provides a re-
sult which is directly usable as a transduc-
er and it allows to list translations accord-
ing to a decreasing order of probability. Yet,
the learned automaton does not lend itself
to an interpretation in the form of symbolic
rules - provided that such rules exist -.
Moreover, some learning parameters are
set only as the results of empirical or ran-
dom choices: number of states, initial prob-
ability distribution, etc. Yet, other advan-
tages weigh for the proposed method. The
automaton may take into account the
whole word to be translated rather than a
limited part of it - this justifies that a set of
equivalent symbolic rules is hard to obtain
-. For example, the grapheme-to-phoneme
transcription may recognize the original
language of a word while translating it
(Oshika et al. 1988): the &amp;quot;French&amp;quot; nouns
&amp;quot;meeting&amp;quot; and &amp;quot;carpaccio&amp;quot; have kept respec-
tively their original English and Italian form
</bodyText>
<equation confidence="0.64867575">
Pm(w, 1+ 1, w&apos;,.1 + 1, a) =
[ max m
, P„„(w, 1,w&apos;, j + 1,0(t1))
`1
</equation>
<bodyText confidence="0.95343290625">
max
where (tie {te &apos;11 D(t) = et S(t) = 1:- )
and (t2 E t TI D(t) = a et S(t) = I) )
mart Pm(w, 1+ 1, w&apos;, j, 0(t2))
2
and pronunciation, etc. The learned autom-
aton is symmetrical, thus it is also revers-
ible. In other words, the morphological
analysis automaton may also be used as a
generator and the grapheme-to-phoneme
automaton may become a phoneme-to-
grapheme transducer. Another remark is in
order: since the automaton is reversible, it
may be composed with its inverse to form,
for example, a grapheme-to-grapheme
translator that keeps the phonemic form
constant without actually computing it.
Now, it has been shown elsewhere (Reape
and Thompson 1988) that the transducer
that would result is also describable in the
formalism of finite state automata and that
its number of states has a upper bound
which is the square of the number of states
In the base automaton. (Reape and Thomp-
son 1988) also describes an algorithm for
computing the resulting automaton. Lastly,
other functions than morphological analy-
sis or grapheme-to-phoneme transcription
may be envisioned like, for example, the de-
composition of words into syllables or the
computation of abbreviations by contrac-
tion.
</bodyText>
<sectionHeader confidence="0.999632" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.99186378">
Bahl, L. R.; Jelinek, F.; and Mercer, R. L.
1983. &amp;quot;A Maximum Likelihood Approach
to Continuous Speech Recognition,&amp;quot; IEEE
Trans. on Pattern Analysis and Machine
Intelligence, 5(2):179-190.
Catach, N. 1984.&amp;quot;La phonetisation automa-
t:Nue du francais,&amp;quot; CNRS.
Danlos, L.; Laporte, E.; and Emerard, F.
1986 &amp;quot;Synthesis of Spoken Messages
from Semantic Representations (Seman-
tic-Representation-to-Speech-System),&amp;quot;
Proc. of the 11th Intern. Conf. on Computa-
tional Linguistics, 599-604.
Kirkpatrick, S.; Gelatt, C. D.; and Vecchi,
M. P. 1983. &amp;quot;Optimization by Simulated
Annealing,&amp;quot; Science, 220:671-680.
Koskenniemi, K. 1983. &amp;quot;Two-Level Model
for Morphological Analysis&amp;quot;, Proc, of the
Eighth Intern. Joint Conf. on Artificial Intel-
ligence, 683-685.
Laporte, E. 1988. Methodes algorithmiques
et lexicales de phonettsation de textes,
These de doctorat, Universite Paris 7.
Oshika, B. T.; Evans, B.; Machi, F.; and
Tom, J. 1988. &amp;quot;Computational Tech-
niques for Improved Name Search,&amp;quot; Proc.
of the Second Conf on Applied Natural
Language Processing, 203-210.
Reape, W. and Thompson. H. 1988. &amp;quot;Paral-
lel Intersection and Serial Composition of
Finite State Transducers&amp;quot;, Proc. of COL-
ING&apos;88, 535-539.
Ritchie G. D.; Pulman, S. G.; Black, A. W.;
and Russell, G. J. 1987. &amp;quot;A Computation-
al Framework for Lexical Description&amp;quot;,
Computational Linguisties,13(3-4):290-
307.
Ritchie, G. 1989. &amp;quot;On the Generative Power
of Two-Level Morphological Rules,&amp;quot;
Fourth Conf of the European Chapter of
the ACL, 51-57.
Tufls D. 1989. &amp;quot;It Would Be Much Easier If
WENT Were GOED,&amp;quot; Proc. of the Fourth
Conf of the European Chapter of the ACL,
145-152.
\Won&apos;s, J. 1988. &amp;quot;Correction of Phono-
graphic Errors in Natural Language In-
terfaces&amp;quot;, 11th ACM-SIGIR Conf, 101-
115.
- 112 -
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.582616">
<title confidence="0.9854165">AUTOMATIC LEARNING OF WORD TRANSDUCERS FROM EXAMPLES</title>
<author confidence="0.999987">Michel GiIlaux</author>
<affiliation confidence="0.879381666666667">Centre National d&apos;Etudes des Telecommunications LAA/SLC/AIA Route de Tregastel, BP 40</affiliation>
<address confidence="0.999485">F-22301 Lannion Cedex, FRANCE</address>
<email confidence="0.999858">e-mail:gilloux@latmion.cnet.fr</email>
<abstract confidence="0.993581230769231">This paper describes the application of markovian learning methods to the inference of word transducers. We show how the proposed method dispenses from the difficult design of hand-crafted rules, allows the use of weighed non deterministic transducers and is able to translate words by taking into account their whole rather than by making decisions locally. These arguments are illustrated on two examples: morphological analysis and grapheme-tophoneme transcription.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L R Bahl</author>
<author>F Jelinek</author>
<author>R L Mercer</author>
</authors>
<title>A Maximum Likelihood Approach to Continuous Speech Recognition,&amp;quot;</title>
<date>1983</date>
<journal>IEEE Trans. on Pattern Analysis and Machine Intelligence,</journal>
<pages>5--2</pages>
<contexts>
<context position="6303" citStr="Bahl et al. 1983" startWordPosition="1072" endWordPosition="1075">(w) t e path A Markov model for which there exist at most one complete path for a given word is said to be unifilctr. In this case, the above probability is reduced to probt(w)=H p(t), if Path(w) = path t e path Probm(w) =0, if Path(w) =0 Thus the probability Pm(w) may be generally computed by adding the probabilities observed along every path compatible with w. In practice, this renders computationally expensive the algorithm for computing Pm(w) and it is tempting to assume that the model is unifilar. Practical studies have shown that this sub-optimal method Is applicable without great loss (Bahl et al. 1983). Under this hypothesis, the probability Pm(w) may be computed through the Viterbi dynamic programming algorithm. Indeed, the probability Pm(w, 1, s), maximal probability of reaching state s with the i first transitions in a path compatible with w - 108 - is Pm(w, 1, a) = max 11 pap, 1= 1...n 1=1 where (paths ft, ...ti e Pathi(w)1 D(ti) = s}) Pm(w, 0, ai) = 1 Pm(w, 0, s) = 0, lf (a* el) therefore Pm(w, 1+ 1, a) = maxpath (p(ti+ 1) • Pm(w, 1, D(ti))) where paths (t1 ...t,+ is Pathi+ i(w)l D01+ i) = s} whereby Pm(w, 1+ 1, s) = maxt (p(t) • Pm(w, 1, 0(t))) where (t e D(t) = a and S(t) = at+ ) wit</context>
<context position="8286" citStr="Bahl et al. 1983" startWordPosition="1454" endWordPosition="1457"> In this case, the model is said to be hidden because it is hard to attach a meaning to the states in S. On the contrary, it is possible to force those states to have a clear-cut interpretation by defining them, for example, as n-grams which are sequences of n elements in A which encode the last n symbols produced by the model to reach the state. It is clear that then only some transitions are meaningful. In dealing with problems like those studied in the present paper it is preferable to use hidden models which allow states to stand for arbitrarily complex predicates. The learning algorithm (Bahl et al. 1983) is based upon the following remark: given a model M whose transitions probabilities are known a priori, the a posteriori probability of a transition t may be estimated by the relative frequency with which t is used on a training set. The number of times a transition t is used on TS is freq(t) = z 6(t, t.) w e TS t&apos; e MaxPath(w) where 8 (t, t&apos;) if to t&apos; , 0 otherwise The relative frequency of using t on TS is rel-freq(t) — freq(t) freq(t&apos;)) ft&apos; (001= OM) I The learning algorithm consists then In setting randomly the probability distribution p(t) and adjusting iteratively its values through the</context>
</contexts>
<marker>Bahl, Jelinek, Mercer, 1983</marker>
<rawString>Bahl, L. R.; Jelinek, F.; and Mercer, R. L. 1983. &amp;quot;A Maximum Likelihood Approach to Continuous Speech Recognition,&amp;quot; IEEE Trans. on Pattern Analysis and Machine Intelligence, 5(2):179-190.</rawString>
</citation>
<citation valid="false">
<authors>
<author>N Catach</author>
</authors>
<title>1984.&amp;quot;La phonetisation automat:Nue du francais,&amp;quot;</title>
<journal>CNRS.</journal>
<marker>Catach, </marker>
<rawString>Catach, N. 1984.&amp;quot;La phonetisation automat:Nue du francais,&amp;quot; CNRS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Danlos</author>
<author>E Laporte</author>
<author>F Emerard</author>
</authors>
<title>Synthesis of Spoken Messages from Semantic Representations (Semantic-Representation-to-Speech-System),&amp;quot;</title>
<date>1986</date>
<booktitle>Proc. of the 11th Intern. Conf. on Computational Linguistics,</booktitle>
<pages>599--604</pages>
<contexts>
<context position="1663" citStr="Danlos et al. 1986" startWordPosition="247" endWordPosition="250">ixes &amp;quot;chrono+rne tre+er+- age&amp;quot;. In the second, &amp;quot;abstertir should be translated into &amp;quot;abstentR&amp;quot; or &amp;quot;apsteniR&amp;quot;1. Most of the proposed methods in the &apos;These two tasks are in fact closely related In that (1) the correct phoneme transcription may mirror an underlying morphological structure, like for &amp;quot;asocial&amp;quot; whose phonemic form is &amp;quot;asosjal&amp;quot; rather than &amp;quot;azosjal&amp;quot; due to the decomposition &amp;quot;a+sociar, , and (2) the surface form of a derived word may depend on the pronunciation of its component morphemes, like for &amp;quot;de+harnacher which results in &amp;quot;deharnacher and not &amp;quot;desharnacher&amp;quot; domain (Catach 1984; Danlos et al. 1986; Koskenniemi 1983; Laporte 1988; Ritchie et al. 1987; Tufts 1989; Wronis 1988) are based on the availability of local rules whose combination, either through direct interpretation or by being compiled, form the target transducer. Although these methods make it possible - at least in theory - to design suitable transducers, provided that the rule description language has the right expressive power, they are complex to use because of the difficulty of writing down rules. Moreover, for a given rule language, there may not exist an algorithm for compiling rules into a form better suited to the tr</context>
</contexts>
<marker>Danlos, Laporte, Emerard, 1986</marker>
<rawString>Danlos, L.; Laporte, E.; and Emerard, F. 1986 &amp;quot;Synthesis of Spoken Messages from Semantic Representations (Semantic-Representation-to-Speech-System),&amp;quot; Proc. of the 11th Intern. Conf. on Computational Linguistics, 599-604.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kirkpatrick</author>
<author>C D Gelatt</author>
<author>M P Vecchi</author>
</authors>
<title>Optimization by Simulated Annealing,&amp;quot;</title>
<date>1983</date>
<journal>Science,</journal>
<pages>220--671</pages>
<contexts>
<context position="9322" citStr="Kirkpatrick et al. 1983" startWordPosition="1640" endWordPosition="1644"> rel-freq(t) — freq(t) freq(t&apos;)) ft&apos; (001= OM) I The learning algorithm consists then In setting randomly the probability distribution p(t) and adjusting iteratively its values through the above formula until the adjustment is small enough to consider the distribution as stationary. It has been shown (Bahl et al. 1983) that this algorithm does converge towards a stationary value of the p(t) which maximizes locally&apos; the probability P of the training set depending on the initial random probability distribution. 11n order to find a global optimum. we used a kind of simulated annealing technique (Kirkpatrick et al. 1983) during the learning process. - 109 - The stationary distribution defines the Markov model induced from the examples in TS1. TRANSDUCTION MODEL To be applied in both illustrative examples, the general structure of Markov models should be related, by means of a shift in representation, to the problem of strings translation. The model of two-level morphological analysis (Koskenniemi 1983) suggests the nature of this shift. Indeed, this method, which was successfully applied to morphologically rich natural languages (Koskenniemi 1983), is based upon a two-level rule formalism for which there exis</context>
</contexts>
<marker>Kirkpatrick, Gelatt, Vecchi, 1983</marker>
<rawString>Kirkpatrick, S.; Gelatt, C. D.; and Vecchi, M. P. 1983. &amp;quot;Optimization by Simulated Annealing,&amp;quot; Science, 220:671-680.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Koskenniemi</author>
</authors>
<title>Two-Level Model for Morphological Analysis&amp;quot;,</title>
<date>1983</date>
<booktitle>Proc, of the Eighth Intern. Joint Conf. on Artificial Intelligence,</booktitle>
<pages>683--685</pages>
<contexts>
<context position="1681" citStr="Koskenniemi 1983" startWordPosition="251" endWordPosition="252">+er+- age&amp;quot;. In the second, &amp;quot;abstertir should be translated into &amp;quot;abstentR&amp;quot; or &amp;quot;apsteniR&amp;quot;1. Most of the proposed methods in the &apos;These two tasks are in fact closely related In that (1) the correct phoneme transcription may mirror an underlying morphological structure, like for &amp;quot;asocial&amp;quot; whose phonemic form is &amp;quot;asosjal&amp;quot; rather than &amp;quot;azosjal&amp;quot; due to the decomposition &amp;quot;a+sociar, , and (2) the surface form of a derived word may depend on the pronunciation of its component morphemes, like for &amp;quot;de+harnacher which results in &amp;quot;deharnacher and not &amp;quot;desharnacher&amp;quot; domain (Catach 1984; Danlos et al. 1986; Koskenniemi 1983; Laporte 1988; Ritchie et al. 1987; Tufts 1989; Wronis 1988) are based on the availability of local rules whose combination, either through direct interpretation or by being compiled, form the target transducer. Although these methods make it possible - at least in theory - to design suitable transducers, provided that the rule description language has the right expressive power, they are complex to use because of the difficulty of writing down rules. Moreover, for a given rule language, there may not exist an algorithm for compiling rules into a form better suited to the translation process.</context>
<context position="9711" citStr="Koskenniemi 1983" startWordPosition="1703" endWordPosition="1704">ich maximizes locally&apos; the probability P of the training set depending on the initial random probability distribution. 11n order to find a global optimum. we used a kind of simulated annealing technique (Kirkpatrick et al. 1983) during the learning process. - 109 - The stationary distribution defines the Markov model induced from the examples in TS1. TRANSDUCTION MODEL To be applied in both illustrative examples, the general structure of Markov models should be related, by means of a shift in representation, to the problem of strings translation. The model of two-level morphological analysis (Koskenniemi 1983) suggests the nature of this shift. Indeed, this method, which was successfully applied to morphologically rich natural languages (Koskenniemi 1983), is based upon a two-level rule formalism for which there exist a way to compile them into the language of finite state automata (FSA) (Ritchie 1989). This result validates the idea that FSAs are reasonable candidates for representing transduction rules, at least in the case of morpholoe. The shift in representation is designed so as to define the alphabet A as the set of pairs c:- or -:c&apos; where c e C and C E C&apos;, — standing for the null character,</context>
</contexts>
<marker>Koskenniemi, 1983</marker>
<rawString>Koskenniemi, K. 1983. &amp;quot;Two-Level Model for Morphological Analysis&amp;quot;, Proc, of the Eighth Intern. Joint Conf. on Artificial Intelligence, 683-685.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Laporte</author>
</authors>
<title>Methodes algorithmiques et lexicales de phonettsation de textes, These de doctorat, Universite Paris 7.</title>
<date>1988</date>
<contexts>
<context position="1695" citStr="Laporte 1988" startWordPosition="253" endWordPosition="254"> second, &amp;quot;abstertir should be translated into &amp;quot;abstentR&amp;quot; or &amp;quot;apsteniR&amp;quot;1. Most of the proposed methods in the &apos;These two tasks are in fact closely related In that (1) the correct phoneme transcription may mirror an underlying morphological structure, like for &amp;quot;asocial&amp;quot; whose phonemic form is &amp;quot;asosjal&amp;quot; rather than &amp;quot;azosjal&amp;quot; due to the decomposition &amp;quot;a+sociar, , and (2) the surface form of a derived word may depend on the pronunciation of its component morphemes, like for &amp;quot;de+harnacher which results in &amp;quot;deharnacher and not &amp;quot;desharnacher&amp;quot; domain (Catach 1984; Danlos et al. 1986; Koskenniemi 1983; Laporte 1988; Ritchie et al. 1987; Tufts 1989; Wronis 1988) are based on the availability of local rules whose combination, either through direct interpretation or by being compiled, form the target transducer. Although these methods make it possible - at least in theory - to design suitable transducers, provided that the rule description language has the right expressive power, they are complex to use because of the difficulty of writing down rules. Moreover, for a given rule language, there may not exist an algorithm for compiling rules into a form better suited to the translation process. Lastly, in nu</context>
</contexts>
<marker>Laporte, 1988</marker>
<rawString>Laporte, E. 1988. Methodes algorithmiques et lexicales de phonettsation de textes, These de doctorat, Universite Paris 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B T Oshika</author>
<author>B Evans</author>
<author>F Machi</author>
<author>J Tom</author>
</authors>
<title>Computational Techniques for Improved Name Search,&amp;quot;</title>
<date>1988</date>
<booktitle>Proc. of the Second Conf on Applied Natural Language Processing,</booktitle>
<pages>203--210</pages>
<contexts>
<context position="16204" citStr="Oshika et al. 1988" startWordPosition="2882" endWordPosition="2885">nd itself to an interpretation in the form of symbolic rules - provided that such rules exist -. Moreover, some learning parameters are set only as the results of empirical or random choices: number of states, initial probability distribution, etc. Yet, other advantages weigh for the proposed method. The automaton may take into account the whole word to be translated rather than a limited part of it - this justifies that a set of equivalent symbolic rules is hard to obtain -. For example, the grapheme-to-phoneme transcription may recognize the original language of a word while translating it (Oshika et al. 1988): the &amp;quot;French&amp;quot; nouns &amp;quot;meeting&amp;quot; and &amp;quot;carpaccio&amp;quot; have kept respectively their original English and Italian form Pm(w, 1+ 1, w&apos;,.1 + 1, a) = [ max m , P„„(w, 1,w&apos;, j + 1,0(t1)) `1 max where (tie {te &apos;11 D(t) = et S(t) = 1:- ) and (t2 E t TI D(t) = a et S(t) = I) ) mart Pm(w, 1+ 1, w&apos;, j, 0(t2)) 2 and pronunciation, etc. The learned automaton is symmetrical, thus it is also reversible. In other words, the morphological analysis automaton may also be used as a generator and the grapheme-to-phoneme automaton may become a phoneme-tographeme transducer. Another remark is in order: since the automaton </context>
</contexts>
<marker>Oshika, Evans, Machi, Tom, 1988</marker>
<rawString>Oshika, B. T.; Evans, B.; Machi, F.; and Tom, J. 1988. &amp;quot;Computational Techniques for Improved Name Search,&amp;quot; Proc. of the Second Conf on Applied Natural Language Processing, 203-210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H</author>
</authors>
<title>Parallel Intersection and Serial Composition of Finite State Transducers&amp;quot;,</title>
<date>1988</date>
<booktitle>Proc. of COLING&apos;88,</booktitle>
<pages>535--539</pages>
<marker>H, 1988</marker>
<rawString>Reape, W. and Thompson. H. 1988. &amp;quot;Parallel Intersection and Serial Composition of Finite State Transducers&amp;quot;, Proc. of COLING&apos;88, 535-539.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G D Ritchie</author>
<author>S G Pulman</author>
<author>A W Black</author>
<author>G J Russell</author>
</authors>
<title>A Computational Framework for Lexical Description&amp;quot;,</title>
<date>1987</date>
<journal>Computational</journal>
<pages>13--3</pages>
<contexts>
<context position="1716" citStr="Ritchie et al. 1987" startWordPosition="255" endWordPosition="258">ertir should be translated into &amp;quot;abstentR&amp;quot; or &amp;quot;apsteniR&amp;quot;1. Most of the proposed methods in the &apos;These two tasks are in fact closely related In that (1) the correct phoneme transcription may mirror an underlying morphological structure, like for &amp;quot;asocial&amp;quot; whose phonemic form is &amp;quot;asosjal&amp;quot; rather than &amp;quot;azosjal&amp;quot; due to the decomposition &amp;quot;a+sociar, , and (2) the surface form of a derived word may depend on the pronunciation of its component morphemes, like for &amp;quot;de+harnacher which results in &amp;quot;deharnacher and not &amp;quot;desharnacher&amp;quot; domain (Catach 1984; Danlos et al. 1986; Koskenniemi 1983; Laporte 1988; Ritchie et al. 1987; Tufts 1989; Wronis 1988) are based on the availability of local rules whose combination, either through direct interpretation or by being compiled, form the target transducer. Although these methods make it possible - at least in theory - to design suitable transducers, provided that the rule description language has the right expressive power, they are complex to use because of the difficulty of writing down rules. Moreover, for a given rule language, there may not exist an algorithm for compiling rules into a form better suited to the translation process. Lastly, in numerous cases, the tra</context>
<context position="3848" citStr="Ritchie et al. 1987" startWordPosition="597" endWordPosition="600">ir adaptation to the word translation problem in the learning and translating phases. This adaptation is illustrated through two applications: morphological analysis and grapheme-to-phoneme transcription. THE TRANSDUCTION PROBLEM In the context of character strings transduction, we look for an application f: Cs C&apos;* which transforms certain words built over the alphabet C into words over the alphabet C&apos;. For example, in the case of grapheme-to-phoneme transcription, C is the set of graphemes and C&apos; that of phonemes. It may be appropriate, for example in morphology, to use an auxiliary lexicon (Ritchie et al. 1987; Ritchie 1989) which allows to discard certain translation results. For example, the decomposition &amp;quot;sage&amp;quot; ---&gt; &amp;quot;ser+age&amp;quot; would not be allowed because &amp;quot;ser&amp;quot; is not a verb in the French lexicon, although this is a correct result with respect to the splitting of word forms into affixes. The method we propose in this paper is only concerned with describing this last type of regularities leaving aside all non regular phenomena better described on a case-bycase basis such as through a lexicon. MARKOV MODELS A Markov model is a probabilistic finite state automaton M = (S, T, A, SD 5F, g) where S is </context>
</contexts>
<marker>Ritchie, Pulman, Black, Russell, 1987</marker>
<rawString>Ritchie G. D.; Pulman, S. G.; Black, A. W.; and Russell, G. J. 1987. &amp;quot;A Computational Framework for Lexical Description&amp;quot;, Computational Linguisties,13(3-4):290-307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ritchie</author>
</authors>
<title>On the Generative Power of Two-Level Morphological Rules,&amp;quot;</title>
<date>1989</date>
<booktitle>Fourth Conf of the European Chapter of the ACL,</booktitle>
<pages>51--57</pages>
<contexts>
<context position="3863" citStr="Ritchie 1989" startWordPosition="601" endWordPosition="602">word translation problem in the learning and translating phases. This adaptation is illustrated through two applications: morphological analysis and grapheme-to-phoneme transcription. THE TRANSDUCTION PROBLEM In the context of character strings transduction, we look for an application f: Cs C&apos;* which transforms certain words built over the alphabet C into words over the alphabet C&apos;. For example, in the case of grapheme-to-phoneme transcription, C is the set of graphemes and C&apos; that of phonemes. It may be appropriate, for example in morphology, to use an auxiliary lexicon (Ritchie et al. 1987; Ritchie 1989) which allows to discard certain translation results. For example, the decomposition &amp;quot;sage&amp;quot; ---&gt; &amp;quot;ser+age&amp;quot; would not be allowed because &amp;quot;ser&amp;quot; is not a verb in the French lexicon, although this is a correct result with respect to the splitting of word forms into affixes. The method we propose in this paper is only concerned with describing this last type of regularities leaving aside all non regular phenomena better described on a case-bycase basis such as through a lexicon. MARKOV MODELS A Markov model is a probabilistic finite state automaton M = (S, T, A, SD 5F, g) where S is a finite set of</context>
<context position="10009" citStr="Ritchie 1989" startWordPosition="1752" endWordPosition="1753">the Markov model induced from the examples in TS1. TRANSDUCTION MODEL To be applied in both illustrative examples, the general structure of Markov models should be related, by means of a shift in representation, to the problem of strings translation. The model of two-level morphological analysis (Koskenniemi 1983) suggests the nature of this shift. Indeed, this method, which was successfully applied to morphologically rich natural languages (Koskenniemi 1983), is based upon a two-level rule formalism for which there exist a way to compile them into the language of finite state automata (FSA) (Ritchie 1989). This result validates the idea that FSAs are reasonable candidates for representing transduction rules, at least in the case of morpholoe. The shift in representation is designed so as to define the alphabet A as the set of pairs c:- or -:c&apos; where c e C and C E C&apos;, — standing for the null character, - C, - e C&apos;. The mapping between the transducer f and the associated Markov model M is now straightforward: 1In practice. the number N = Card(S) of states for the model to be learned on a training set is not known. When N is small, the model has a tendency to generate much more character strings </context>
</contexts>
<marker>Ritchie, 1989</marker>
<rawString>Ritchie, G. 1989. &amp;quot;On the Generative Power of Two-Level Morphological Rules,&amp;quot; Fourth Conf of the European Chapter of the ACL, 51-57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Tufls</author>
</authors>
<title>It Would Be Much Easier If WENT Were GOED,&amp;quot;</title>
<date>1989</date>
<booktitle>Proc. of the Fourth Conf of the European Chapter of the ACL,</booktitle>
<pages>145--152</pages>
<marker>Tufls, 1989</marker>
<rawString>Tufls D. 1989. &amp;quot;It Would Be Much Easier If WENT Were GOED,&amp;quot; Proc. of the Fourth Conf of the European Chapter of the ACL, 145-152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J \Won&apos;s</author>
</authors>
<title>Correction of Phonographic Errors</title>
<date>1988</date>
<booktitle>in Natural Language Interfaces&amp;quot;, 11th ACM-SIGIR Conf,</booktitle>
<pages>101--115</pages>
<marker>\Won&apos;s, 1988</marker>
<rawString>\Won&apos;s, J. 1988. &amp;quot;Correction of Phonographic Errors in Natural Language Interfaces&amp;quot;, 11th ACM-SIGIR Conf, 101-115.</rawString>
</citation>
<citation valid="false">
<pages>112</pages>
<marker></marker>
<rawString>- 112 -</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>