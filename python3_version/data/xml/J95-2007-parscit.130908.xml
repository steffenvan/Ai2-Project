<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009842">
<note confidence="0.298291">
Book Reviews
</note>
<title confidence="0.989791">
Reversible Grammar in Natural Language Processing
</title>
<author confidence="0.918473">
Tomek Strzalkowski (editor)
</author>
<affiliation confidence="0.8366756">
(New York University)
Boston: Kluwer Academic Publishers
(The Kluwer International Series in
Electrical Engineering and Computer
Science; Natural Language Processing
</affiliation>
<bodyText confidence="0.54638775">
and Machine Translation, edited by
Jaime Carbonell), 1994, xxi + 454 pp.
Hardbound, ISBN 0-7923-9416-9,
$130.00, £94.50, Dfl 265.00
</bodyText>
<note confidence="0.650372666666667">
Reviewed by
S. G. Pulman
SRI International, Cambridge, and University of Cambridge
</note>
<bodyText confidence="0.998998387096774">
This is a collection of 15 revised and expanded papers from the 1991 Berkeley ACL
workshop of the same title. Collectively, the contributions cover all the main theoretical
and practical motivations for reversibility of grammars: increased accuracy of descrip-
tion gained; economy of development and maintenance effort; consistency of language
use in translation or interface applications; and possibility of self-monitoring for am-
biguity in generation or paraphrase of parsing input. Some other connected issues are
also discussed.
&amp;quot;A reversible constraint-based logic grammar,&amp;quot; by Palmira Marrafa and Patrick
Saint-Dizier, describes a formalism derived based on Nit-Kaci&apos;s psi-terms, and shows
how the operation of type construction can be used both to generate and to parse
types representing sentences and their meanings. Some illustration of the formalism
is given by applying it to problems of Portuguese syntactic description.
Marc Dymetman, in &amp;quot;Inherently reversible grammars,&amp;quot; distinguishes different ways
in which &amp;quot;reversibility&amp;quot; can be understood. He focuses on a notion of &amp;quot;inherent finite
reversibility&amp;quot; of grammars: a property possessed by a grammar if it is possible to
derive sound, complete, and terminating parsing and generation algorithms for it.
He discusses some grammars of artificial languages that do not satisfy this condition
and tries to define a property of &amp;quot;moderation&amp;quot; that distinguishes the two classes of
grammars. The intuitive content of this is something like compositionality.
Gunter Neumann and Gertjan van Noord (&amp;quot;Reversibility and self-monitoring in
natural language generation&amp;quot;) point out that a reversible grammar is necessary for
self-monitoring for ambiguity in generation, or for paraphrasing of ambiguity when
parsing. They describe an algorithm for carrying out both of these tasks, illustrated
mostly with examples of PP-attachment ambiguity. I found this one of the most in-
teresting papers in the volume. However, there is an important practical issue not
discussed by the authors that arises when such techniques are used in practical ap-
plications, namely that in many cases multiple readings are not distinguishable by a
nonexpert. For example, each of the following has at least two parses but most non-
linguists (and some linguists) are incapable of discerning any difference in meaning:
I&apos;ll pick up the car at the airport.
Can I make a stop in Boston?
</bodyText>
<page confidence="0.977693">
269
</page>
<note confidence="0.442522">
Computational Linguistics Volume 21, Number 2
</note>
<bodyText confidence="0.998905137254902">
Under such circumstances, techniques for the avoidance of ambiguity create a problem
instead of solving one.
Remi Zajac (&amp;quot;A uniform architecture for parsing, generation, and transfer&amp;quot;) de-
scribes a typed-feature-logic implementation that allows both for analysis and gener-
ation as well as for bidirectional transfer. The system is extremely elegant and truly
nondirectional. Reading between the lines (p. 108), one observes that this lack of di-
rectionality also means that it is equally inefficient in any direction, and Zajac remarks
that the system should be seen as a grammar development tool rather than as some-
thing on which to base practical applications. In an ideal world, one would like to
see the emergence of compilation and optimizing techniques akin to those described
elsewhere in this volume for this class of formalisms.
In &amp;quot;Handling felicity conditions with a reversible architecture,&amp;quot; Masato Ishizaki
formalizes a type of planning operator to capture linguistic felicity conditions using
the notation of HPSG. It was not clear to me that any immediate advantages accrue
from this notational exercise, since—if I have understood correctly—the usual oper-
ational semantics assumed for HPSG-like formalisms is no longer assumed. Feature
equations representing preconditions that might not be able to be satisfied when they
are encountered are apparently intended to be treated like constraints, and &amp;quot;processing
continues to hold them as assumptions. They are passed up or down along deriva-
tion&amp;quot; (p. 124). I would have welcomed some more clarification of the motivation for
this way of handling felicity conditions.
Koiti Hasida (&amp;quot;Common heuristics for parsing, generation, and whatever .. . &amp;quot; [sic])
describes a constraint-solving method for first-order logic problems that is claimed to
be able to simulate some well-known parsing or generation strategies. I regret to say
that I found the details too unfamiliar and hard to follow in the time I could give it
to be able to assess this ambitious claim properly.
Hans Ulrich Block (&amp;quot;Compiling trace and unification grammar&amp;quot;) describes a unifi-
cation-based formalism that contains special notation for &amp;quot;movement rules.&amp;quot; The for-
malism is compiled in different ways for parsing and for generation. Block describes
the various stages of compilation, which lead to very efficient run-time systems while
still allowing the linguist to use a high-level notation. I found this a very clear paper
describing a good piece of computational linguistic engineering.
Tomek Strzalkowski (&amp;quot;A general computational method for grammar inversion&amp;quot;)
describes what is presumably the final and definitive version of his procedure for
inverting logic programs. The method is illustrated with admirable detail and clarity.
However, in the context of most of the discussion in this book, the starting point for
this algorithm is a peculiar one, namely a &amp;quot;parsing-oriented unification grammar,&amp;quot;
and furthermore, a grammar that is conceived of (like a DCG) as being executed as
a program. But do people write grammars like this these days? It would be nice to
see procedures similar to those described here allowing for efficient processing with
grammar formalisms such as those described by Zajac, or Marrafa and Saint-Dizier.
Grammars in such formalisms are not (intentionally) parsing- or generation-oriented.
For development purposes, the grammarian needs to see what is happening in a form
as similar as possible to the way the grammar was written. I would guess that trying
to debug either the input or output grammars of Strzalkowski-ization has all the
charm of reading core dumps: an optimal system would allow grammar development
to take place using a high-level notation, with the kind of grammars described by
Strzalkowski being the output of a compilation phase.
James Barnett, in &amp;quot;Bi-directional preferences,&amp;quot; discusses the issue of applying pref-
erences, and combinations of preferences, as a disambiguation heuristic during parsing
and generation.
</bodyText>
<page confidence="0.982292">
270
</page>
<subsectionHeader confidence="0.923932">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.990253409090909">
Lee Fedder (&amp;quot;Handling syntactic alternatives in a reversible grammar&amp;quot;) adds some
features indicating information structure properties to a unification grammar. This
simple device is quite effective in ensuring that answers to questions are generated in
a form that is appropriate to the way the question was asked.
David McDonald (&amp;quot;Reversible NLP by linking the grammar to the knowledge
base&amp;quot;) describes a system that compiles TAGs for analysis and generation, and also
links this with a semantic model. The resulting system looks sufficiently different
from TAGs that McDonald devotes some attention to the question of whether the
compilation preserves equivalence.
Dominique Estival (&amp;quot;Reversible grammars and their application in machine trans-
lation&amp;quot;) describes an implemented unification-based analysis, transfer, and generation
system. Efficiency problems caused by asymmetries of information flow in different
processing directions are alleviated by declarations that the grammar writer can make.
In &amp;quot;Reversible machine translation: What to do when the languages don&apos;t match
up,&amp;quot; James Barnett, Inderjeet Mani, and Elaine Rich discuss translation mismatches,
beginning with a good survey of well-known translation problems. They then present
an algorithm (unimplemented, at least on the evidence of this paper) for solving some
of the mismatches that arise through problems of lexical choice. This procedure pre-
supposes a reversible grammar.
The final two papers are linked. Robin Fawcett&apos;s paper (&amp;quot;A generationist approach
to grammar reversibility in natural language processing&amp;quot;) is a long discussion ampli-
fying the point that, because systemic grammar cuts the syntax—semantics—pragmatics
pie differently from other approaches, the notions of reversibility, parsing, and gen-
eration also differ. Tim O&apos;Donoghue&apos;s companion paper (&amp;quot;Semantic interpretation in
a systemic functional grammar&amp;quot;) discusses the mechanisms of semantic interpretdtion
in systemic grammar, and mentions some issues that arise when reversibility is taken
seriously. I am afraid that neither paper did anything to dispel my (no doubt unfair)
prejudice that systemic functional grammar has the strongest current claim to be the
FORTRAN of linguistic formalisms.
Should you buy this book? For me, it falls into the category of those books that
I would be happy to recommend the librarian to get, but less willing to spend my
own money on. About 30% of the papers are perhaps perfectly worthy in themselves
but not very directly related to the main theme. A more ruthless pruning might have
lost the editor some friends, but would have resulted in a more tightly focused (and
cheaper?) book.
Nevertheless, I learned something from almost every paper in this collection, and
I&apos;m sure I will go back to a couple of them more than once.
Stephen Pulman is a lecturer at the University of Cambridge Computer Laboratory and Direc-
tor of SRI International&apos;s Cambridge Computer Science Research Centre. He has previously
worked in computational morphology, syntax, and parsing. His current research interests are
in semantics and dialogue, and, in particular, in the development of descriptions of the mean-
ings of contextually dependent constructs that allow for reversible processing. Pulman&apos;s ad-
dress is: SRI International, 23 Miller&apos;s Yard, Mill Lane, Cambridge CB2 1RQ, England. E-mail:
sgp@cam.sri.com.
</bodyText>
<page confidence="0.996475">
271
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.028530">
<title confidence="0.9975895">Book Reviews Reversible Grammar in Natural Language Processing</title>
<author confidence="0.99845">Tomek Strzalkowski</author>
<affiliation confidence="0.83422825">(New York University) Boston: Kluwer Academic Publishers (The Kluwer International Series in Electrical Engineering and Computer</affiliation>
<address confidence="0.324121">Science; Natural Language Processing</address>
<note confidence="0.8051404">and Machine Translation, edited by Jaime Carbonell), 1994, xxi + 454 pp. Hardbound, ISBN 0-7923-9416-9, $130.00, £94.50, Dfl 265.00 Reviewed by</note>
<author confidence="0.976992">S G Pulman</author>
<affiliation confidence="0.904669">SRI International, Cambridge, and University of Cambridge</affiliation>
<note confidence="0.517878">This is a collection of 15 revised and expanded papers from the 1991 Berkeley ACL</note>
<abstract confidence="0.982501054263566">workshop of the same title. Collectively, the contributions cover all the main theoretical and practical motivations for reversibility of grammars: increased accuracy of description gained; economy of development and maintenance effort; consistency of language use in translation or interface applications; and possibility of self-monitoring for ambiguity in generation or paraphrase of parsing input. Some other connected issues are also discussed. &amp;quot;A reversible constraint-based logic grammar,&amp;quot; by Palmira Marrafa and Patrick Saint-Dizier, describes a formalism derived based on Nit-Kaci&apos;s psi-terms, and shows how the operation of type construction can be used both to generate and to parse types representing sentences and their meanings. Some illustration of the formalism is given by applying it to problems of Portuguese syntactic description. Marc Dymetman, in &amp;quot;Inherently reversible grammars,&amp;quot; distinguishes different ways in which &amp;quot;reversibility&amp;quot; can be understood. He focuses on a notion of &amp;quot;inherent finite reversibility&amp;quot; of grammars: a property possessed by a grammar if it is possible to derive sound, complete, and terminating parsing and generation algorithms for it. He discusses some grammars of artificial languages that do not satisfy this condition and tries to define a property of &amp;quot;moderation&amp;quot; that distinguishes the two classes of grammars. The intuitive content of this is something like compositionality. Gunter Neumann and Gertjan van Noord (&amp;quot;Reversibility and self-monitoring in natural language generation&amp;quot;) point out that a reversible grammar is necessary for self-monitoring for ambiguity in generation, or for paraphrasing of ambiguity when parsing. They describe an algorithm for carrying out both of these tasks, illustrated mostly with examples of PP-attachment ambiguity. I found this one of the most interesting papers in the volume. However, there is an important practical issue not discussed by the authors that arises when such techniques are used in practical applications, namely that in many cases multiple readings are not distinguishable by a nonexpert. For example, each of the following has at least two parses but most nonlinguists (and some linguists) are incapable of discerning any difference in meaning: I&apos;ll pick up the car at the airport. Can I make a stop in Boston? 269 Computational Linguistics Volume 21, Number 2 Under such circumstances, techniques for the avoidance of ambiguity create a problem instead of solving one. Remi Zajac (&amp;quot;A uniform architecture for parsing, generation, and transfer&amp;quot;) describes a typed-feature-logic implementation that allows both for analysis and generation as well as for bidirectional transfer. The system is extremely elegant and truly nondirectional. Reading between the lines (p. 108), one observes that this lack of directionality also means that it is equally inefficient in any direction, and Zajac remarks that the system should be seen as a grammar development tool rather than as something on which to base practical applications. In an ideal world, one would like to see the emergence of compilation and optimizing techniques akin to those described elsewhere in this volume for this class of formalisms. In &amp;quot;Handling felicity conditions with a reversible architecture,&amp;quot; Masato Ishizaki formalizes a type of planning operator to capture linguistic felicity conditions using the notation of HPSG. It was not clear to me that any immediate advantages accrue from this notational exercise, since—if I have understood correctly—the usual operational semantics assumed for HPSG-like formalisms is no longer assumed. Feature equations representing preconditions that might not be able to be satisfied when they are encountered are apparently intended to be treated like constraints, and &amp;quot;processing continues to hold them as assumptions. They are passed up or down along derivation&amp;quot; (p. 124). I would have welcomed some more clarification of the motivation for this way of handling felicity conditions. Koiti Hasida (&amp;quot;Common heuristics for parsing, generation, and whatever .. . &amp;quot; [sic]) describes a constraint-solving method for first-order logic problems that is claimed to be able to simulate some well-known parsing or generation strategies. I regret to say that I found the details too unfamiliar and hard to follow in the time I could give it to be able to assess this ambitious claim properly. Hans Ulrich Block (&amp;quot;Compiling trace and unification grammar&amp;quot;) describes a unification-based formalism that contains special notation for &amp;quot;movement rules.&amp;quot; The formalism is compiled in different ways for parsing and for generation. Block describes the various stages of compilation, which lead to very efficient run-time systems while still allowing the linguist to use a high-level notation. I found this a very clear paper describing a good piece of computational linguistic engineering. Tomek Strzalkowski (&amp;quot;A general computational method for grammar inversion&amp;quot;) describes what is presumably the final and definitive version of his procedure for inverting logic programs. The method is illustrated with admirable detail and clarity. However, in the context of most of the discussion in this book, the starting point for this algorithm is a peculiar one, namely a &amp;quot;parsing-oriented unification grammar,&amp;quot; and furthermore, a grammar that is conceived of (like a DCG) as being executed as a program. But do people write grammars like this these days? It would be nice to see procedures similar to those described here allowing for efficient processing with grammar formalisms such as those described by Zajac, or Marrafa and Saint-Dizier. Grammars in such formalisms are not (intentionally) parsingor generation-oriented. For development purposes, the grammarian needs to see what is happening in a form as similar as possible to the way the grammar was written. I would guess that trying to debug either the input or output grammars of Strzalkowski-ization has all the charm of reading core dumps: an optimal system would allow grammar development to take place using a high-level notation, with the kind of grammars described by Strzalkowski being the output of a compilation phase. James Barnett, in &amp;quot;Bi-directional preferences,&amp;quot; discusses the issue of applying preferences, and combinations of preferences, as a disambiguation heuristic during parsing and generation. 270 Book Reviews Lee Fedder (&amp;quot;Handling syntactic alternatives in a reversible grammar&amp;quot;) adds some features indicating information structure properties to a unification grammar. This simple device is quite effective in ensuring that answers to questions are generated in a form that is appropriate to the way the question was asked. David McDonald (&amp;quot;Reversible NLP by linking the grammar to the knowledge base&amp;quot;) describes a system that compiles TAGs for analysis and generation, and also links this with a semantic model. The resulting system looks sufficiently different from TAGs that McDonald devotes some attention to the question of whether the compilation preserves equivalence. Dominique Estival (&amp;quot;Reversible grammars and their application in machine translation&amp;quot;) describes an implemented unification-based analysis, transfer, and generation system. Efficiency problems caused by asymmetries of information flow in different processing directions are alleviated by declarations that the grammar writer can make. In &amp;quot;Reversible machine translation: What to do when the languages don&apos;t match up,&amp;quot; James Barnett, Inderjeet Mani, and Elaine Rich discuss translation mismatches, beginning with a good survey of well-known translation problems. They then present an algorithm (unimplemented, at least on the evidence of this paper) for solving some of the mismatches that arise through problems of lexical choice. This procedure presupposes a reversible grammar. The final two papers are linked. Robin Fawcett&apos;s paper (&amp;quot;A generationist approach to grammar reversibility in natural language processing&amp;quot;) is a long discussion amplifying the point that, because systemic grammar cuts the syntax—semantics—pragmatics pie differently from other approaches, the notions of reversibility, parsing, and generation also differ. Tim O&apos;Donoghue&apos;s companion paper (&amp;quot;Semantic interpretation in a systemic functional grammar&amp;quot;) discusses the mechanisms of semantic interpretdtion in systemic grammar, and mentions some issues that arise when reversibility is taken seriously. I am afraid that neither paper did anything to dispel my (no doubt unfair) prejudice that systemic functional grammar has the strongest current claim to be the FORTRAN of linguistic formalisms. Should you buy this book? For me, it falls into the category of those books that I would be happy to recommend the librarian to get, but less willing to spend my own money on. About 30% of the papers are perhaps perfectly worthy in themselves but not very directly related to the main theme. A more ruthless pruning might have lost the editor some friends, but would have resulted in a more tightly focused (and cheaper?) book. Nevertheless, I learned something from almost every paper in this collection, and I&apos;m sure I will go back to a couple of them more than once. Pulman a lecturer at the University of Cambridge Computer Laboratory and Director of SRI International&apos;s Cambridge Computer Science Research Centre. He has previously worked in computational morphology, syntax, and parsing. His current research interests are in semantics and dialogue, and, in particular, in the development of descriptions of the meanings of contextually dependent constructs that allow for reversible processing. Pulman&apos;s address is: SRI International, 23 Miller&apos;s Yard, Mill Lane, Cambridge CB2 1RQ, England. E-mail: sgp@cam.sri.com.</abstract>
<intro confidence="0.726778">271</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>