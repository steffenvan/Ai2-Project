<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9969235">
An Exploration of Document Impact on Graph-Based Multi-Document
Summarization
</title>
<author confidence="0.998082">
Xiaojun Wan
</author>
<affiliation confidence="0.885645">
Institute of Compute Science and Technology
Peking University
Beijing 100871, China
</affiliation>
<email confidence="0.950251">
wanxiaojun@icst.pku.edu.cn
</email>
<sectionHeader confidence="0.994136" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999807136363636">
The graph-based ranking algorithm has been
recently exploited for multi-document sum-
marization by making only use of the sen-
tence-to-sentence relationships in the
documents, under the assumption that all the
sentences are indistinguishable. However,
given a document set to be summarized, dif-
ferent documents are usually not equally im-
portant, and moreover, different sentences in a
specific document are usually differently im-
portant. This paper aims to explore document
impact on summarization performance. We
propose a document-based graph model to in-
corporate the document-level information and
the sentence-to-document relationship into the
graph-based ranking process. Various meth-
ods are employed to evaluate the two factors.
Experimental results on the DUC2001 and
DUC2002 datasets demonstrate that the good
effectiveness of the proposed model. More-
over, the results show the robustness of the
proposed model.
</bodyText>
<sectionHeader confidence="0.998127" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971666666667">
Multi-document summarization aims to produce a
summary describing the main topic in a document
set, without any prior knowledge. Multi-document
summary can be used to facilitate users to quickly
understand a document cluster. For example, a
number of news services (e.g. NewsInEssence1)
have been developed to group news articles into
news topics, and then produce a short summary for
each news topic. Users can easily understand the
topic they have interest in by taking a look at the
short summary, without looking into each individ-
ual article within the topic cluster.
</bodyText>
<footnote confidence="0.583924">
1http://lada.si.umich.edu:8080/clair/nie1/nie.cgi
</footnote>
<bodyText confidence="0.999899146341463">
Automated multi-document summarization has
drawn much attention in recent years. In the com-
munities of natural language processing and infor-
mation retrieval, a series of workshops and
conferences on automatic text summarization (e.g.
NTCIR, DUC), special topic sessions in ACL,
COLING, and SIGIR have advanced the summari-
zation techniques and produced a couple of ex-
perimental online systems.
A particular challenge for multi-document sum-
marization is that a document set might contain
diverse information, which is either related or un-
related to the main topic, and hence we need effec-
tive summarization methods to analyze the
information stored in different documents and ex-
tract the globally important information to reflect
the main topic. In recent years, both unsupervised
and supervised methods have been proposed to
analyze the information contained in a document
set and extract highly salient sentences into the
summary, based on syntactic or statistical features.
Most recently, the graph-based models have
been successfully applied for multi-document
summarization by making use of the “voting” or
“recommendations” between sentences in the
documents (Erkan and Radev, 2004; Mihalcea and
Tarau, 2005; Wan and Yang, 2006). The model
first constructs a directed or undirected graph to
reflect the relationships between the sentences and
then applies the graph-based ranking algorithm to
compute the rank scores for the sentences. The
sentences with large rank scores are chosen into
the summary. However, the model makes uniform
use of the sentences in different documents, i.e. all
the sentences are ranked without considering the
document-level information and the sentence-to-
document relationship. Actually, given a document
set, different documents are not equally important.
For example, the documents close to the main top-
ics of the document set are usually more important
than the documents far away from the main topics
</bodyText>
<note confidence="0.901425666666667">
755
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 755–762,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999994647058824">
of the document set. This document-level informa-
tion is deemed to have great impact on the sen-
tence ranking process. Moreover, the sentences in
the same document cannot be treated uniformly,
because some sentences in the document are more
important than other sentences because of their
different positions in the document or different
distances to the document’s centroid. In brief, nei-
ther the document-level information nor the sen-
tence-to-document relationship has been taken into
account in the previous graph-based model.
In order to overcome the limitations of the pre-
vious graph-based model, this study proposes the
document-based graph model to explore document
impact on the graph-based summarization, by in-
corporating both the document-level information
and the sentence-to-document relationship in the
graph-based ranking process. We develop various
methods to evaluate the document-level informa-
tion and the sentence-to-document relationship.
Experiments on the DUC2001 and DUC2002 data-
sets have been performed and the results demon-
strate the good effectiveness of the proposed model,
i.e., the incorporation of document impact can
much improve the performance of the graph-based
summarization. Moreover, the proposed model is
robust with respect to most incorporation schemes.
The rest of this paper is organized as follows.
We first introduce the related work in Section 2.
The basic graph-based summarization model and
the proposed document-based graph model are de-
scribed in detail in Sections 3 and 4, respectively.
We show the experiments and results in Section 5
and finally we conclude this paper in Section 6.
</bodyText>
<sectionHeader confidence="0.999785" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999843428571428">
Generally speaking, summarization methods can
be abstractive summarization or extractive summa-
rization. Extractive summarization is a simple but
robust method for text summarization and it in-
volves assigning saliency scores to some units (e.g.
sentences, paragraphs) of the documents and ex-
tracting those with highest scores, while abstrac-
tion summarization usually needs information
fusion (Barzilay et al., 1999), sentence compres-
sion (Knight and Marcu, 2002) and reformulation
(McKeown et al., 1999). In this study, we focus on
extractive summarization.
The centroid-based method (Radev et al., 2004)
is one of the most popular extractive summariza-
tion methods. MEAD2 is an implementation of the
centroid-based method that scores sentences based
on sentence-level and inter-sentence features, in-
cluding cluster centroids, position, TFIDF, etc.
NeATS (Lin and Hovy, 2002) is a project on multi-
document summarization at ISI based on the sin-
gle-document summarizer-SUMMARIST. Sen-
tence position, term frequency, topic signature and
term clustering are used to select important content.
MMR (Goldstein et al., 1999) is used to remove
redundancy and stigma word filters and time
stamps are used to improve cohesion and coher-
ence. To further explore user interface issues,
iNeATS (Leuski et al., 2003) is developed based
on NeATS. XDoX (Hardy et al., 1998) is a cross
document summarizer designed specifically to
summarize large document sets. It identifies the
most salient themes within the set by passage clus-
tering and then composes an extraction summary,
which reflects these main themes. Much other
work also explores to find topic themes in the
documents for summarization, e.g. Harabagiu and
Lacatusu (2005) investigate five different topic
representations and introduce a novel representa-
tion of topics based on topic themes. In addition,
Marcu (2001) selects important sentences based on
the discourse structure of the text. TNO’s system
(Kraaij et al., 2001) scores sentences by combining
a unigram language model approach with a Bayes-
ian classifier based on surface features. Nenkova
and Louis (2008) investigate how summary length
and the characteristics of the input influence the
summary quality in multi-document summarization.
Graph-based models have been proposed to rank
sentences or passages based on the PageRank algo-
rithm (Page et al., 1998) or its variants. Websumm
(Mani and Bloedorn, 2000) uses a graph-
connectivity model and operates under the assump-
tion that nodes which are connected to many other
nodes are likely to carry salient information. Lex-
PageRank (Erkan and Radev, 2004) is an approach
for computing sentence importance based on the
concept of eigenvector centrality. It constructs a
sentence connectivity matrix and compute sentence
importance based on an algorithm similar to Pag-
eRank. Mihalcea and Tarau (2005) also propose a
similar algorithm based on PageRank to compute
sentence importance for document summarization.
Wan and Yang (2006) improve the ranking algo-
</bodyText>
<footnote confidence="0.675479">
2 http://www.summarization.com/mead/
</footnote>
<page confidence="0.785134">
756
</page>
<bodyText confidence="0.9994478125">
rithm by differentiating intra-document links and
inter-document links between sentences. All these
methods make use of the relationships between
sentences and select sentences according to the
“votes” or “recommendations” from their
neighboring sentences, which is similar to PageR-
ank.
Other related work includes topic-focused multi-
document summarization (Daumé. and Marcu,
2006; Gupta et al., 2007; Wan et al., 2007), which
aims to produce summary biased to a given topic
or query. It is noteworthy that our proposed ap-
proach is inspired by (Liu and Ma, 2005), which
proposes the Conditional Markov Random Walk
Model based on two-layer web graph in the tasks
of web page retrieval.
</bodyText>
<sectionHeader confidence="0.976464" genericHeader="method">
3 The Basic Graph-Based Model (GM)
</sectionHeader>
<bodyText confidence="0.9998136">
The basic graph-based model is essentially a way
of deciding the importance of a vertex within a
graph based on global information recursively
drawn from the entire graph. The basic idea is that
of “voting” or “recommendation” between the ver-
tices. A link between two vertices is considered as
a vote cast from one vertex to the other vertex. The
score associated with a vertex is determined by the
votes that are cast for it, and the score of the verti-
ces casting these votes.
</bodyText>
<figure confidence="0.8776385">
E
Sentences
</figure>
<figureCaption confidence="0.99843">
Figure 1. One-layer link graph
</figureCaption>
<bodyText confidence="0.999889882352941">
Formally, given a document set D, let G=(V, E) be
an undirected graph to reflect the relationships be-
tween sentences in the document set, as shown in
Figure 1. V is the set of vertices and each vertex vi
in V is a sentence in the document set. E is the set
of edges. Each edge eij in E is associated with an
affinity weight f(vi, vj) between sentences vi and vj
(i≠j). The weight is computed using the standard
cosine measure between the two sentences.
where vri and vrj are the corresponding term vec-
tors of vi and vj. Here, we have f(vi, vj)=f(vj, vi).
Two vertices are connected if their affinity weight
is larger than 0 and we let f(vi, vi)=0 to avoid self
transition.
We use an affinity matrix M to describe G with
each entry corresponding to the weight of an edge
in the graph. M = (Mi,j)|V|×|V |is defined as follows:
</bodyText>
<equation confidence="0.974258722222222">
j) if and
, v v
i j
0 otherwise
,
M~ as
Then M is normalized to
follows to make
|V
i,j
∑
i,j≠0
Mi,j = i j=1 j=1
⎩
i
IM
, if
M
</equation>
<bodyText confidence="0.994629666666667">
Based on matrix M~ , the saliency score Sen-
Score(vi) for sentence vi can be deduced from those
of all other sentences linked with it and it can be
formulated in a recursive form as in the PageRank
algorithm:
And the matrix form is:
</bodyText>
<equation confidence="0.894833">
r ~ (1 µ )
r − r
λ = µ M T λ + e |V |(5) r
</equation>
<bodyText confidence="0.999985142857143">
where λ = [SenScore (vi )] i,|×1 is the vector of sen-
tence saliency scores. er is a vector with all ele-
ments equaling to 1. µ is the damping factor
usually set to 0.85, as in the PageRank algorithm.
The above process can be considered as a
Markov chain by taking the sentences as the states
and the corresponding transition matrix is given
</bodyText>
<equation confidence="0.896493">
byA= µM~ T+(1−µ)eeT
|V|
Mi,j
an
</equation>
<bodyText confidence="0.978445052631579">
the sum of each row equal to 1:
,
ability distribution of each state is obtained by the
principal eigenvector of the transition matrix.
For implementation, the initial scores of all sen-
tences are set to 1 and the iteration algorithm in
Equation (4) is adopted to compute the new scores
of the sentences. Usually the convergence of the
iteration algorithm is achieved when the difference
between the scores computed at two successive
iterations for any sentences falls below a given
threshold (0.0001 in this study).
We can see that the basic graph-based model is
built on the single-layer sentence graph and the
transition probability between two sentences in the
Markov chain depends only on the sentences them-
selves, not taking into account the document-level
information and the sentence-to-document rela-
tionship.
</bodyText>
<figure confidence="0.996775166666667">
r r
f (vi , v j) = sim cos ine (vi , v j) =
j
v v
⋅
i
r r
j
v v
×
i
j
i≠
d
~µ
)
=µ⋅∑
SenScor
e
v
j
)
⋅
j
i +
≠
(1−
)
SenScore(
v(4)i
all j
i
|V
|
(1)
= ⎧
⎪⎨⎪⎩
f
( v v
i,
is connected
; (2)
(3)
,
0
otherwise
. The stationary prob-
757
</figure>
<sectionHeader confidence="0.801548" genericHeader="method">
4 The Document-Based Graph Model
(DGM)
</sectionHeader>
<subsectionHeader confidence="0.989083">
4.1 Overview
</subsectionHeader>
<bodyText confidence="0.999343227272727">
As we mentioned in previous section, there may be
many factors that can have impact on the impor-
tance analysis of the sentences. This study aims to
examine the document impact by incorporating the
document importance and the sentence-to-
document correlation into the sentence ranking
process. Our assumption is that the sentences, whi-
ch belong to an important document and are highly
correlated with the document, will be more likely
to be chosen into the summary.
In order to incorporate the document-level in-
formation and the sentence-to-document relation-
ship, the document-based graph model is proposed
based on the two-layer link graph including both
sentences and documents. The novel representation
is shown in Figure 2. As can be seen, the lower
layer is just the traditional link graph between sen-
tences that has been well studied in previous work.
And the upper layer represents the documents. The
dashed lines between these two layers indicate the
conditional influence between the sentences and
the documents.
</bodyText>
<figure confidence="0.531217">
Documents
</figure>
<figureCaption confidence="0.999526">
Figure 2. Two-layer link graph
</figureCaption>
<bodyText confidence="0.999995230769231">
Formally, the new representation for the two-
layer graph is denoted as G*=&lt;Vs, Vd, Ess, Esd&gt;,
where Vs=V={vi} is the set of sentences and
Vd=D={dj} is the set of documents; Ess=E={eij|vi,
vjEVs} includes all possible links between sen-
tences and Esd={eij|viEVs, djEVd and dj=doc(vi)}
includes the correlation link between any sentence
and its belonging document. Here, we use doc(vi)
to denote the document containing sentence vi. For
further discussions, we let π(doc(vi)) E[0,1] de-
note the importance of document doc(vi) in the
document set, and let co(vi, doc(vi)) E[0,1] denote
the strength of the correlation between sentence vi
and its document doc(vi).
The two factors are incorporated into the affinity
weight between sentences and the new sentence-to-
sentence affinity weight is denoted as f(vi, vj|doc(vi),
doc(vj)), which is conditioned on the two docu-
ments containing the two sentences. The new con-
ditional affinity weight is computed by linearly
combining the affinity weight conditioned on the
first document (i.e. f(vi,vj|doc(vi))) and the affinity
weight conditioned on the second document (i.e.
f(vi,vj|doc(vj))).
Formally, the conditional affinity weight is
computed as follows to incorporate the two factors:
</bodyText>
<equation confidence="0.9516215">
,  |( ), ( ))
v doc v doc v
j i j
= ⋅
λ f v v doc v
( ,  |( )) (1 ) ( ,  |( ))
+ − ⋅
λ f v v doc v
i j i i j j
= ⋅ doc(vi)) ⋅ ω(vi,doc (vi ))
</equation>
<table confidence="0.991176888888889">
λ f v v
( , ) (
⋅ π
i j
+ (1 λ (doc(vj)) ⋅ ω(vj,doc(vj)))
−⋅
π
= sim cos ine (vi ,v)⋅(λ⋅π(doc(vji )) ( vi , ( ))
⋅ ω doc vi
</table>
<equation confidence="0.461799">
))
⋅ ω(vj,doc(vj)))
</equation>
<bodyText confidence="0.99989">
where AE[0,1] is the combination weight control-
ling the relative contributions from the first docu-
ment and the second document. Note that usually
f(vi, vj|doc(vi), doc(vj)) is not equal to f(vj, vi|doc(vj),
doc(vi)), but the two scores are equal when A is set
to 0.5. Various methods can be used to evaluate the
document importance and the sentence-document
correlation, which will be described in next sec-
tions.
The new affinity matrix M* is then constructed
based on the above conditional sentence-to-
sentence affinity weight.
</bodyText>
<equation confidence="0.634807833333333">
⎧f(vi,vj |doc(v),doc(vj)), ififvi and
M*i,j=⎪⎨
and i≠j
⎪l0 otherwise
,
Likewise, M* is normalized to ~ *
</equation>
<bodyText confidence="0.8320844">
M and the itera-
tive computation as in Equation (4) is then based
on~ *
M . The transition matrix in the Markov chain
is then denoted by A* = µM~ *T + (1− a) e e T and
</bodyText>
<equation confidence="0.524708">
|V|
</equation>
<bodyText confidence="0.999033">
the sentence scores is obtained by the principle
eigenvector of the new transition matrix A*.
</bodyText>
<subsectionHeader confidence="0.938218">
4.2 Evaluating Document Importance (π)
</subsectionHeader>
<bodyText confidence="0.999933">
The function π(doc(vi)) aims to evaluate the impor-
tance of document doc(vi) in the document set D.
The following three methods are developed to
evaluate the document importance.
</bodyText>
<figure confidence="0.993667909090909">
Esd
Ess
Sentences
=
+
f v v
( ,
i j
(
1
λ)
f(vi,vj)⋅π(doc(
) ⋅ (λ ⋅ π(doc(vi)) ⋅ ω(vi
⋅
j
v
))
⋅ω(vj,doc(vj))
, ( ))
doc vi
(6)
f v
( i
+
(1 λ
− ⋅
π)
v
(doc(
j
vj is connected
(7)
758
</figure>
<listItem confidence="0.785286222222222">
π1: It uses the cosine similarity value between
the document and the whole document set as the
importance score of the document3:
π doc vi = sim ine doc vi D
1 ( ( )) cos ( ( ), ) (8)
π2: It uses the average similarity value between
the document and any other document in the
document set as the importance score of the docu-
ment:
</listItem>
<equation confidence="0.984530222222222">
simcos ine (doc(vi ), d
( ( )) &apos; and &apos; ( )
d D
∈ d doc vi
≠ (9)
π doc v =
2 i
|D|
−1
</equation>
<bodyText confidence="0.978793111111111">
It constructs a weighted graph between docu-
ments and uses the PageRank algorithm to com-
pute the rank scores of the documents as the
importan
π3:
ce scores of the documents. The link
weight between two documents is computed using
the cosine measure. The equation for iterative
computation is the same with Equation (4).
</bodyText>
<subsectionHeader confidence="0.9822385">
4.3 Evaluating Sentence-Document Cor-
relation (ω)
</subsectionHeader>
<bodyText confidence="0.999852846153846">
The function ω(vi, doc(vi)) aims to evaluate the
correlation between sentence vi and its document
doc(vi). The following four methods are developed
to compute the strength of the correlation. The first
three methods are based on sentence position in the
document, under the assumption that the first sen-
tences in a document are usually more important
than other sentences. The last method is based on
the content similarity between the sentence and the
document.
ω1: The correlation strength between sentence vi
and its document doc(vi) is based on the position of
the sentence as follows:
</bodyText>
<equation confidence="0.99425">
⎧ 1 if ( ) 3
pos v ≤
i
ω v doc v
1 ( , ( )) = ⎨ (10)
i i ⎩ 0. 5 Otherwise
</equation>
<bodyText confidence="0.988941277777778">
where pos(vi) returns the position number of sen-
tence vi in its document. For example, if vi is the
first sentence in its document, pos(vi) is 1.
ω2: The correlation strength between sentence vi
and its document doc(vi) is based on the position of
the sentence as follows:
where
sen_count(doc(vi)) returns the total number
of sentences in document doc(vi).
3 A document set is treated as a single text by concatenating
all the document texts in the set.
ω3: The correlation strength between sentence vi
and its document doc(vi) is based on the position of
the sentence as follows:
ω3(vi, doc(vi))= 0. 5 + 1
pos(vi)+1 (12)ω4: The correlation strength between sentence vi
and its document doc(vi) is based on the cosine
similarity between the sentence and the document:
</bodyText>
<equation confidence="0.717331">
ω4 (vi, doc(vi )) = simcosine (vi, doc(vi )) (13)
</equation>
<sectionHeader confidence="0.996753" genericHeader="method">
5 Empirical Evaluation
</sectionHeader>
<subsectionHeader confidence="0.96608">
5.1 Dataset and Evaluation Metric
</subsectionHeader>
<bodyText confidence="0.999649">
Generic multi-document summarization has been
one of the fundamental tasks in DUC 20014 and
DUC 20025 (i.e. task 2 in DUC 2001 and task 2 in
DUC 2002), and we used the two tasks for evalua-
tion. DUC2001 provided 30 document sets and
DUC 2002 provided 59 document sets (D088 is
excluded from the original 60 document sets by
NIST) and generic abstracts of each document set
with lengths of approximately 100 words or less
were required to be created. The documents were
news articles collected from TREC-9. The sen-
tences in each article have been separated and the
sentence information has been stored into files.
The summary of the two datasets are shown in Ta-
ble 1.
</bodyText>
<note confidence="0.593285">
DUC 2001 DUC 2002
</note>
<table confidence="0.9804656">
Task Task 2 Task 2
Number of documents 309 567
Number of clusters 30 59
Data source TREC-9 TREC-9
Summary length 100 words 100 words
</table>
<tableCaption confidence="0.999671">
Table 1. Summary of datasets
</tableCaption>
<bodyText confidence="0.999944777777778">
We used the ROUGE (Lin and Hovy, 2003)
toolkit (i.e. ROUGEeval-1.4.2 in this study) for
evaluation, which has been widely adopted by
DUC for automatic summarization evaluation. It
measured summary quality by counting overlap-
ping units such as the n-gram, word sequences and
word pairs between the candidate summary and the
reference summary. ROUGE-N was an n-gram
recall measure computed as follows:
</bodyText>
<equation confidence="0.791125578947368">
∑ ∑ Countmatch (n −gram)
ROUGE N
− = S (14)
∈ {Ref Sum} n-gram S
∑ ∑
∈
Count(n gram)
−
S∈ {Ref Sum} n-gram S
∈
4 http://www-nlpir.nist.gov/projects/duc/guidelines/2001.html
5 http://www-nlpir.nist.gov/projects/duc/guidelines/2002.html
ω2 (vi, doc(vi )) =1− pos(vi) −1
t(doc(vi )) (11)
sen coun
_
∑
&apos;
)
</equation>
<page confidence="0.549328">
759
</page>
<bodyText confidence="0.992601944444444">
where n stood for the length of the n-gram, and
Countmatch(n-gram) was the maximum number of
n-grams co-occurring in a candidate summary and
a set of reference summaries. Count(n-gram) was
the number of n-grams in the reference summaries.
ROUGE toolkit reported separate scores for 1, 2,
3 and 4-gram, and also for longest common subse-
quence co-occurrences. Among these different
scores, unigram-based ROUGE score (ROUGE-1)
has been shown to agree with human judgment
most (Lin and Hovy. 2003). We showed three of
the ROUGE metrics in the experimental results:
ROUGE-1 (unigram-based), ROUGE-2 (bigram-
based), and ROUGE-W (based on weighted long-
est common subsequence, weight=1.2). In order to
truncate summaries longer than length limit, we
used the “-l” option in ROUGE toolkit. We also
used the “-m” option for word stemming.
</bodyText>
<subsectionHeader confidence="0.999726">
5.2 Evaluation Results
</subsectionHeader>
<bodyText confidence="0.999838333333333">
In the experiments, the combination weight λ for
the proposed summarization model is typically set
to 0.5 without tuning, i.e. the two documents for
two sentences have equal influence on the summa-
rization process. Note that after the saliency scores
of sentences have been obtained, a greedy algo-
rithm (Wan and Yang, 2006) is applied to remove
redundancy and finally choose both informative
and novel sentences into the summary. The algo-
rithm is actually a variant version of the MMR al-
gorithm (Goldstein et al., 1999).
The proposed document-based graph model (de-
noted as DGM) with different settings is compared
with the basic graph-based Model (denoted as GM),
the top three performing systems and two baseline
systems on DUC2001 and DUC2002, respectively.
The top three systems are the systems with highest
ROUGE scores, chosen from the performing sys-
tems on each task respectively. The lead baseline
and coverage baseline are two baselines employed
in the generic multi-document summarization tasks
of DUC2001 and DUC2002. The lead baseline
takes the first sentences one by one in the last
document in the collection, where documents are
assumed to be ordered chronologically. And the
coverage baseline takes the first sentence one by
one from the first document to the last document.
Tables 2 and 3 show the comparison results on
DUC2001 and DUC2002, respectively. In Table 1,
SystemN, SystemP and System T are the top three
performing systems for DUC2001. In Table 2, Sys-
tem19, System26, System28 are the top three per-
forming systems for DUC2002. The document-
based graph model is configured with different
settings (i.e. π1-π3, ω1-ω4). For example,
DGM(π1+ω1) refers to the DGM model with π1 to
evaluate the document importance and ω1 to evalu-
ate the correlation between a sentence and its docu-
ment.
</bodyText>
<table confidence="0.999878421052632">
System ROUGE-1 ROUGE-2 ROUGE-W
DGM(π1+ω1) 0.35658 0.05926 0.10712
DGM(π1+ω2) 0.35945 0.06304* 0.10820
DGM(π1+ω3) 0.36349* 0.06472* 0.10952
DGM(π1+ω4) 0.35421 0.05934 0.10695
DGM(π2+ω1) 0.35555 0.06554* 0.10924
DGM(π2+ω2) 0.37228* 0.06787* 0.11295*
DGM(π2+ω3) 0.37347* 0.06612* 0.11352*
DGM(π2+ω4) 0.36340 0.06397* 0.11006
DGM(π3+ω1) 0.35333 0.06353* 0.10834
DGM(π3+ω2) 0.37082* 0.06708* 0.11235
DGM(π3+ω3) 0.37056* 0.06503* 0.11227*
DGM(π3+ω4) 0.36667* 0.06585* 0.11114
GM 0.35527 0.05608 0.10641
SystemN 0.33910 0.06853 0.10240
SystemP 0.33332 0.06651 0.10068
SystemT 0.33029 0.07862 0.10215
Coverage 0.33130 0.06898 0.10182
Lead 0.29419 0.04033 0.08880
</table>
<tableCaption confidence="0.994384">
Table 2. Comparison results on DUC2001
</tableCaption>
<table confidence="0.999978789473684">
System ROUGE-1 ROUGE-2 ROUGE-W
DGM(π1+ω1) 0.37891 0.08398 0.12390
DGM(π1+ω2) 0.39013* 0.08770* 0.12726*
DGM(π1+ω3) 0.38490* 0.08355 0.12570
DGM(π1+ω4) 0.38464 0.08371 0.12443
DGM(π2+ω1) 0.38296 0.08369 0.12499
DGM(π2+ω2) 0.38143 0.08792* 0.12506
DGM(π2+ω3) 0.38177 0.08624* 0.12511
DGM(π2+ω4) 0.38576* 0.08167 0.12611
DGM(π3+ω1) 0.38079 0.08391 0.12392
DGM(π3+ω2) 0.38103 0.08608* 0.12446
DGM(π3+ω3) 0.38236 0.08675* 0.12478
DGM(π3+ω4) 0.38719* 0.08150 0.12633*
GM 0.37595 0.08304 0.12173
System26 0.35151 0.07642 0.11448
System19 0.34504 0.07936 0.11332
System28 0.34355 0.07521 0.10956
Coverage 0.32894 0.07148 0.10847
Lead 0.28684 0.05283 0.09525
</table>
<tableCaption confidence="0.999685">
Table 3. Comparison results on DUC2002
</tableCaption>
<bodyText confidence="0.96957968">
(* indicates that the improvement over the baseline GM
model is statistically significant at 95% confidence level)
Seen from the tables, the proposed document-
based graph model with different settings can out-
perform the basic graph-based model and other
baselines over almost all three metrics on both
760
DUC2001 and DUC2002 datasets. The results
demonstrate the good effectiveness of the proposed
model, i.e. the incorporation of document impact
does benefit the graph-based summarization model.
It is interesting that the three methods for comput-
ing document importance and the four methods for
computing the sentence-document correlation are
almost as effective as each other on the DUC2002
dataset. However, 7r1 does not perform as well as 7r2
and 7r3, and CO1 and CO4 does not perform as well as
CO2 and CO3 on the DUC2001 dataset.
In order to investigate the relative contributions
from the two documents for two sentences to the
summarization performance, we varies the combi-
nation weight A from 0 to 1 and Figures 3-6 show
the ROUGE-1 and ROUGE-W curves on
DUC2001 and DUC2002 respectively. The similar
ROUGE-2 curves are omitted here.
</bodyText>
<figureCaption confidence="0.99998675">
Figure 3. ROUGE-1 vs. A on DUC2001
Figure 4. ROUGE-W vs. A on DUC2001
Figure 5. ROUGE-1 vs. A on DUC2002
Figure 6. ROUGE-W vs. A on DUC2002
</figureCaption>
<bodyText confidence="0.9999765">
We can see from the figures that the proposed
document-based graph model with different set-
tings can almost always outperform the basic
graph-based model, with respect to different values
of A. The results show the robustness of the pro-
posed model. We can also see that for most set-
tings of the propose model, very large values or
very small values of A can deteriorate the summari-
zation performance, i.e. both the first document
and the second document in the computation of the
conditional affinity weight between sentences have
great impact on the summarization performance.
</bodyText>
<sectionHeader confidence="0.995638" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9994445">
This paper examines the document impact on the
graph-based model for multi-document summari-
zation. The document-level information and the
sentence-to-document relationship are incorporated
into the graph-based ranking algorithm. The ex-
perimental results on DUC2001 and DUC2002
demonstrate the good effectiveness of the proposed
model.
</bodyText>
<figure confidence="0.996819559139785">
DGM(n1+w1)
DGM(n1+w2)
DGM(n1+w3)
DGM(n1+w4)
DGM(n2+w1)
DGM(n2+w2)
DGM(n2+w3)
DGM(n2+w4)
DGM(n3+w1)
DGM(n3+w2)
DGM(n3+w3)
DGM(n3+w4)
GM
ROUGE-1
0.375
0.365
0.355
0.37
0.36
0.35
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
A
ROUGE-W
0.114
0.112
0.108
0.106
0.104
0.11
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
A
DGM(n1+w1)
DGM(n1+w2)
DGM(n1+w3)
DGM(n1+w4)
DGM(n2+w1)
DGM(n2+w2)
DGM(n2+w3)
DGM(n2+w4)
DGM(n3+w1)
DGM(n3+w2)
DGM(n3+w3)
DGM(n3+w4)
GM
ROUGE-1
0.395
0.385
0.375
0.39
0.38
0.37
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
A
DGM(n1+w1)
DGM(n1+w2)
DGM(n1+w3)
DGM(n1+w4)
DGM(n2+w1)
DGM(n2+w2)
DGM(n2+w3)
DGM(n2+w4)
DGM(n3+w1)
DGM(n3+w2)
DGM(n3+w3)
DGM(n3+w4)
GM
ROUGE-W
0.129
0.128
0.127
0.126
0.125
0.124
0.123
0.122
0.121
0.12
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
A
DGM(n1+w1)
DGM(n1+w2)
DGM(n1+w3)
DGM(n1+w4)
DGM(n2+w1)
DGM(n2+w2)
DGM(n2+w3)
DGM(n2+w4)
DGM(n3+w1)
DGM(n3+w2)
DGM(n3+w3)
DGM(n3+w4)
GM
761
</figure>
<bodyText confidence="0.999729555555555">
In this study, we directly make use of the coarse-
grained document-level information. Actually, a
document can be segmented into a few subtopic
passages by using the TextTiling algorithm (Hearst,
1997), and we believe the subtopic passage is more
fine-grained than the original document. In future
work, we will exploit this kind of subtopic-level
information to further improve the summarization
performance.
</bodyText>
<sectionHeader confidence="0.998643" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999395">
This work was supported by the National Science
Foundation of China (No.60703064), the Research
Fund for the Doctoral Program of Higher Educa-
tion of China (No.20070001059) and the National
High Technology Research and Development Pro-
gram of China (No.2008AA01Z421). We also
thank the anonymous reviewers for their useful
comments.
</bodyText>
<sectionHeader confidence="0.998475" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999966631578948">
R. Barzilay, K. R. McKeown and M. Elhadad. 1999.
Information fusion in the context of multi-document
summarization. In Proceedings of ACL1999.
H. Daumé and D. Marcu. 2006. Bayesian query-focused
summarization. 2006. In Proceedings of COLING-
ACL2006.
G. Erkan and D. Radev. 2004. LexPageRank: prestige in
multi-document text summarization. In Proceedings
of EMNLP’04.
J. Goldstein, M. Kantrowitz, V. Mittal and J. Carbonell.
1999. Summarizing text documents: sentence selec-
tion and evaluation metrics. In Proceedings of SIGIR-
99.
S. Gupta, A. Nenkova and D. Jurafsky. 2007. Measuring
importance and query relevance in topic-focused
multi-document summarization. In Proceedings of
ACL-07.
S. Harabagiu and F. Lacatusu. 2005. Topic themes for
multi-document summarization. In Proceedings of
SIGIR’05.
H. Hardy, N. Shimizu, T. Strzalkowski, L. Ting, G. B.
Wise. and X. Zhang. 2002. Cross-document summa-
rization by concept classification. In Proceedings of
SIGIR’02.
M. Hearst. 1997. TextTiling: segmenting text into multi-
paragraph subtopic passages. Computational Linguis-
tics, 23(1): 33-64.
K. Knight. and D. Marcu. 2002. Summarization beyond
sentence extraction: a probabilistic approach to sen-
tence compression, Artificial Intelligence, 139(1).
W. Kraaij, M. Spitters and M. van der Heijden. 2001.
Combining a mixture language model and Naïve
Bayes for multi-document summarization. In SIGIR
2001 Workshop on Text Summarization.
A. Leuski, C.-Y. Lin and E. Hovy. 2003. iNeATS: in-
teractive multi-document summarization. In Proceed-
ings of ACL2003.
C.-Y. Lin and E. H. Hovy. 2002. From single to multi-
document summarization: a prototype system and its
evaluation. In Proceedings of ACL-2002.
C.-Y. Lin and E. H. Hovy. 2003. Automatic evaluation
of summaries using n-gram co-occurrence statistics.
In Proceedings of HLT-NAACL2003.
T.-Y. Liu and W.-Y. Ma. 2005. Webpage importance
analysis using Conditional Markov Random Walk. In
Proceedings of WI2005.
I. Mani and E. Bloedorn. 2000. Summarizing similari-
ties and differences among related documents. Infor-
mation Retrieval, 1(1).
D. Marcu. Discourse-based summarization in DUC–
2001. 2001. In SIGIR 2001 Workshop on Text Sum-
marization.
K. McKeown, J. Klavans, V. Hatzivassiloglou, R. Barzi-
lay and E. Eskin. 1999. Towards multidocument
summarization by reformulation: progress and pros-
pects, in Proceedings of AAAI1999.
R. Mihalcea and P. Tarau. 2005. A language independ-
ent algorithm for single and multiple document sum-
marization. In Proceedings of IJCNLP’2005.
A. Nenkova and A. Louis. 2008. Can you summarize
this? Identifying correlates of input difficulty for ge-
neric multi-document summarization. In Proceedings
of ACL-08: HLT.
L. Page, S. Brin, R. Motwani and T. Winograd. 1998.
The pagerank citation ranking: Bringing order to the
web. Technical report, Stanford Digital Libraries.
D. R. Radev, H. Y. Jing, M. Stys and D. Tam. 2004.
Centroid-based summarization of multiple documents.
Information Processing and Management, 40: 919-
938.
X. Wan and J. Yang. 2006. Improved affinity graph
based multi-document summarization. In Proceedings
of HLT-NAACL2006.
X. Wan, J. Yang and J. Xiao. 2007. Manifold-ranking
based topic-focused multi-document summarization.
In Proceedings of IJCAI2007.
</reference>
<page confidence="0.823366">
762
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.500028">
<title confidence="0.9976555">An Exploration of Document Impact on Graph-Based Summarization</title>
<author confidence="0.812365">Xiaojun</author>
<affiliation confidence="0.8267625">Institute of Compute Science and Peking</affiliation>
<address confidence="0.993964">Beijing 100871,</address>
<email confidence="0.973776">wanxiaojun@icst.pku.edu.cn</email>
<abstract confidence="0.998326391304348">The graph-based ranking algorithm has been recently exploited for multi-document summarization by making only use of the sentence-to-sentence relationships in the documents, under the assumption that all the sentences are indistinguishable. However, given a document set to be summarized, different documents are usually not equally important, and moreover, different sentences in a specific document are usually differently important. This paper aims to explore document impact on summarization performance. We propose a document-based graph model to incorporate the document-level information and the sentence-to-document relationship into the graph-based ranking process. Various methods are employed to evaluate the two factors. Experimental results on the DUC2001 and DUC2002 datasets demonstrate that the good effectiveness of the proposed model. Moreover, the results show the robustness of the proposed model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>K R McKeown</author>
<author>M Elhadad</author>
</authors>
<title>Information fusion in the context of multi-document summarization.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL1999.</booktitle>
<contexts>
<context position="5943" citStr="Barzilay et al., 1999" startWordPosition="866" endWordPosition="869">proposed document-based graph model are described in detail in Sections 3 and 4, respectively. We show the experiments and results in Section 5 and finally we conclude this paper in Section 6. 2 Related Work Generally speaking, summarization methods can be abstractive summarization or extractive summarization. Extractive summarization is a simple but robust method for text summarization and it involves assigning saliency scores to some units (e.g. sentences, paragraphs) of the documents and extracting those with highest scores, while abstraction summarization usually needs information fusion (Barzilay et al., 1999), sentence compression (Knight and Marcu, 2002) and reformulation (McKeown et al., 1999). In this study, we focus on extractive summarization. The centroid-based method (Radev et al., 2004) is one of the most popular extractive summarization methods. MEAD2 is an implementation of the centroid-based method that scores sentences based on sentence-level and inter-sentence features, including cluster centroids, position, TFIDF, etc. NeATS (Lin and Hovy, 2002) is a project on multidocument summarization at ISI based on the single-document summarizer-SUMMARIST. Sentence position, term frequency, top</context>
</contexts>
<marker>Barzilay, McKeown, Elhadad, 1999</marker>
<rawString>R. Barzilay, K. R. McKeown and M. Elhadad. 1999. Information fusion in the context of multi-document summarization. In Proceedings of ACL1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daumé</author>
<author>D Marcu</author>
</authors>
<title>Bayesian query-focused summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of COLINGACL2006.</booktitle>
<marker>Daumé, Marcu, 2006</marker>
<rawString>H. Daumé and D. Marcu. 2006. Bayesian query-focused summarization. 2006. In Proceedings of COLINGACL2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Erkan</author>
<author>D Radev</author>
</authors>
<title>LexPageRank: prestige in multi-document text summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP’04.</booktitle>
<contexts>
<context position="2950" citStr="Erkan and Radev, 2004" startWordPosition="422" endWordPosition="425">nd hence we need effective summarization methods to analyze the information stored in different documents and extract the globally important information to reflect the main topic. In recent years, both unsupervised and supervised methods have been proposed to analyze the information contained in a document set and extract highly salient sentences into the summary, based on syntactic or statistical features. Most recently, the graph-based models have been successfully applied for multi-document summarization by making use of the “voting” or “recommendations” between sentences in the documents (Erkan and Radev, 2004; Mihalcea and Tarau, 2005; Wan and Yang, 2006). The model first constructs a directed or undirected graph to reflect the relationships between the sentences and then applies the graph-based ranking algorithm to compute the rank scores for the sentences. The sentences with large rank scores are chosen into the summary. However, the model makes uniform use of the sentences in different documents, i.e. all the sentences are ranked without considering the document-level information and the sentence-todocument relationship. Actually, given a document set, different documents are not equally import</context>
<context position="8132" citStr="Erkan and Radev, 2004" startWordPosition="1202" endWordPosition="1205">s by combining a unigram language model approach with a Bayesian classifier based on surface features. Nenkova and Louis (2008) investigate how summary length and the characteristics of the input influence the summary quality in multi-document summarization. Graph-based models have been proposed to rank sentences or passages based on the PageRank algorithm (Page et al., 1998) or its variants. Websumm (Mani and Bloedorn, 2000) uses a graphconnectivity model and operates under the assumption that nodes which are connected to many other nodes are likely to carry salient information. LexPageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. It constructs a sentence connectivity matrix and compute sentence importance based on an algorithm similar to PageRank. Mihalcea and Tarau (2005) also propose a similar algorithm based on PageRank to compute sentence importance for document summarization. Wan and Yang (2006) improve the ranking algo2 http://www.summarization.com/mead/ 756 rithm by differentiating intra-document links and inter-document links between sentences. All these methods make use of the relationships between sentences and s</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G. Erkan and D. Radev. 2004. LexPageRank: prestige in multi-document text summarization. In Proceedings of EMNLP’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goldstein</author>
<author>M Kantrowitz</author>
<author>V Mittal</author>
<author>J Carbonell</author>
</authors>
<title>Summarizing text documents: sentence selection and evaluation metrics.</title>
<date>1999</date>
<booktitle>In Proceedings of SIGIR99.</booktitle>
<contexts>
<context position="6642" citStr="Goldstein et al., 1999" startWordPosition="969" endWordPosition="972">t al., 1999). In this study, we focus on extractive summarization. The centroid-based method (Radev et al., 2004) is one of the most popular extractive summarization methods. MEAD2 is an implementation of the centroid-based method that scores sentences based on sentence-level and inter-sentence features, including cluster centroids, position, TFIDF, etc. NeATS (Lin and Hovy, 2002) is a project on multidocument summarization at ISI based on the single-document summarizer-SUMMARIST. Sentence position, term frequency, topic signature and term clustering are used to select important content. MMR (Goldstein et al., 1999) is used to remove redundancy and stigma word filters and time stamps are used to improve cohesion and coherence. To further explore user interface issues, iNeATS (Leuski et al., 2003) is developed based on NeATS. XDoX (Hardy et al., 1998) is a cross document summarizer designed specifically to summarize large document sets. It identifies the most salient themes within the set by passage clustering and then composes an extraction summary, which reflects these main themes. Much other work also explores to find topic themes in the documents for summarization, e.g. Harabagiu and Lacatusu (2005) i</context>
<context position="21622" citStr="Goldstein et al., 1999" startWordPosition="3600" endWordPosition="3603">the “-l” option in ROUGE toolkit. We also used the “-m” option for word stemming. 5.2 Evaluation Results In the experiments, the combination weight λ for the proposed summarization model is typically set to 0.5 without tuning, i.e. the two documents for two sentences have equal influence on the summarization process. Note that after the saliency scores of sentences have been obtained, a greedy algorithm (Wan and Yang, 2006) is applied to remove redundancy and finally choose both informative and novel sentences into the summary. The algorithm is actually a variant version of the MMR algorithm (Goldstein et al., 1999). The proposed document-based graph model (denoted as DGM) with different settings is compared with the basic graph-based Model (denoted as GM), the top three performing systems and two baseline systems on DUC2001 and DUC2002, respectively. The top three systems are the systems with highest ROUGE scores, chosen from the performing systems on each task respectively. The lead baseline and coverage baseline are two baselines employed in the generic multi-document summarization tasks of DUC2001 and DUC2002. The lead baseline takes the first sentences one by one in the last document in the collecti</context>
</contexts>
<marker>Goldstein, Kantrowitz, Mittal, Carbonell, 1999</marker>
<rawString>J. Goldstein, M. Kantrowitz, V. Mittal and J. Carbonell. 1999. Summarizing text documents: sentence selection and evaluation metrics. In Proceedings of SIGIR99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gupta</author>
<author>A Nenkova</author>
<author>D Jurafsky</author>
</authors>
<title>Measuring importance and query relevance in topic-focused multi-document summarization.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL-07.</booktitle>
<contexts>
<context position="8971" citStr="Gupta et al., 2007" startWordPosition="1319" endWordPosition="1322">alcea and Tarau (2005) also propose a similar algorithm based on PageRank to compute sentence importance for document summarization. Wan and Yang (2006) improve the ranking algo2 http://www.summarization.com/mead/ 756 rithm by differentiating intra-document links and inter-document links between sentences. All these methods make use of the relationships between sentences and select sentences according to the “votes” or “recommendations” from their neighboring sentences, which is similar to PageRank. Other related work includes topic-focused multidocument summarization (Daumé. and Marcu, 2006; Gupta et al., 2007; Wan et al., 2007), which aims to produce summary biased to a given topic or query. It is noteworthy that our proposed approach is inspired by (Liu and Ma, 2005), which proposes the Conditional Markov Random Walk Model based on two-layer web graph in the tasks of web page retrieval. 3 The Basic Graph-Based Model (GM) The basic graph-based model is essentially a way of deciding the importance of a vertex within a graph based on global information recursively drawn from the entire graph. The basic idea is that of “voting” or “recommendation” between the vertices. A link between two vertices is </context>
</contexts>
<marker>Gupta, Nenkova, Jurafsky, 2007</marker>
<rawString>S. Gupta, A. Nenkova and D. Jurafsky. 2007. Measuring importance and query relevance in topic-focused multi-document summarization. In Proceedings of ACL-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>F Lacatusu</author>
</authors>
<title>Topic themes for multi-document summarization.</title>
<date>2005</date>
<booktitle>In Proceedings of SIGIR’05.</booktitle>
<contexts>
<context position="7240" citStr="Harabagiu and Lacatusu (2005)" startWordPosition="1065" endWordPosition="1068">t. MMR (Goldstein et al., 1999) is used to remove redundancy and stigma word filters and time stamps are used to improve cohesion and coherence. To further explore user interface issues, iNeATS (Leuski et al., 2003) is developed based on NeATS. XDoX (Hardy et al., 1998) is a cross document summarizer designed specifically to summarize large document sets. It identifies the most salient themes within the set by passage clustering and then composes an extraction summary, which reflects these main themes. Much other work also explores to find topic themes in the documents for summarization, e.g. Harabagiu and Lacatusu (2005) investigate five different topic representations and introduce a novel representation of topics based on topic themes. In addition, Marcu (2001) selects important sentences based on the discourse structure of the text. TNO’s system (Kraaij et al., 2001) scores sentences by combining a unigram language model approach with a Bayesian classifier based on surface features. Nenkova and Louis (2008) investigate how summary length and the characteristics of the input influence the summary quality in multi-document summarization. Graph-based models have been proposed to rank sentences or passages bas</context>
</contexts>
<marker>Harabagiu, Lacatusu, 2005</marker>
<rawString>S. Harabagiu and F. Lacatusu. 2005. Topic themes for multi-document summarization. In Proceedings of SIGIR’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhang</author>
</authors>
<title>Cross-document summarization by concept classification.</title>
<date>2002</date>
<booktitle>In Proceedings of SIGIR’02.</booktitle>
<marker>Zhang, 2002</marker>
<rawString>H. Hardy, N. Shimizu, T. Strzalkowski, L. Ting, G. B. Wise. and X. Zhang. 2002. Cross-document summarization by concept classification. In Proceedings of SIGIR’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>TextTiling: segmenting text into multiparagraph subtopic passages.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<pages>33--64</pages>
<marker>Hearst, 1997</marker>
<rawString>M. Hearst. 1997. TextTiling: segmenting text into multiparagraph subtopic passages. Computational Linguistics, 23(1): 33-64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>Summarization beyond sentence extraction: a probabilistic approach to sentence compression,</title>
<date>2002</date>
<journal>Artificial Intelligence,</journal>
<volume>139</volume>
<issue>1</issue>
<contexts>
<context position="5990" citStr="Marcu, 2002" startWordPosition="875" endWordPosition="876">il in Sections 3 and 4, respectively. We show the experiments and results in Section 5 and finally we conclude this paper in Section 6. 2 Related Work Generally speaking, summarization methods can be abstractive summarization or extractive summarization. Extractive summarization is a simple but robust method for text summarization and it involves assigning saliency scores to some units (e.g. sentences, paragraphs) of the documents and extracting those with highest scores, while abstraction summarization usually needs information fusion (Barzilay et al., 1999), sentence compression (Knight and Marcu, 2002) and reformulation (McKeown et al., 1999). In this study, we focus on extractive summarization. The centroid-based method (Radev et al., 2004) is one of the most popular extractive summarization methods. MEAD2 is an implementation of the centroid-based method that scores sentences based on sentence-level and inter-sentence features, including cluster centroids, position, TFIDF, etc. NeATS (Lin and Hovy, 2002) is a project on multidocument summarization at ISI based on the single-document summarizer-SUMMARIST. Sentence position, term frequency, topic signature and term clustering are used to se</context>
</contexts>
<marker>Marcu, 2002</marker>
<rawString>K. Knight. and D. Marcu. 2002. Summarization beyond sentence extraction: a probabilistic approach to sentence compression, Artificial Intelligence, 139(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Kraaij</author>
<author>M Spitters</author>
<author>M van der Heijden</author>
</authors>
<title>Combining a mixture language model and Naïve Bayes for multi-document summarization.</title>
<date>2001</date>
<booktitle>In SIGIR 2001 Workshop on Text Summarization.</booktitle>
<marker>Kraaij, Spitters, van der Heijden, 2001</marker>
<rawString>W. Kraaij, M. Spitters and M. van der Heijden. 2001. Combining a mixture language model and Naïve Bayes for multi-document summarization. In SIGIR 2001 Workshop on Text Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Leuski</author>
<author>C-Y Lin</author>
<author>E Hovy</author>
</authors>
<title>iNeATS: interactive multi-document summarization.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL2003.</booktitle>
<contexts>
<context position="6826" citStr="Leuski et al., 2003" startWordPosition="1000" endWordPosition="1003">implementation of the centroid-based method that scores sentences based on sentence-level and inter-sentence features, including cluster centroids, position, TFIDF, etc. NeATS (Lin and Hovy, 2002) is a project on multidocument summarization at ISI based on the single-document summarizer-SUMMARIST. Sentence position, term frequency, topic signature and term clustering are used to select important content. MMR (Goldstein et al., 1999) is used to remove redundancy and stigma word filters and time stamps are used to improve cohesion and coherence. To further explore user interface issues, iNeATS (Leuski et al., 2003) is developed based on NeATS. XDoX (Hardy et al., 1998) is a cross document summarizer designed specifically to summarize large document sets. It identifies the most salient themes within the set by passage clustering and then composes an extraction summary, which reflects these main themes. Much other work also explores to find topic themes in the documents for summarization, e.g. Harabagiu and Lacatusu (2005) investigate five different topic representations and introduce a novel representation of topics based on topic themes. In addition, Marcu (2001) selects important sentences based on the</context>
</contexts>
<marker>Leuski, Lin, Hovy, 2003</marker>
<rawString>A. Leuski, C.-Y. Lin and E. Hovy. 2003. iNeATS: interactive multi-document summarization. In Proceedings of ACL2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
<author>E H Hovy</author>
</authors>
<title>From single to multidocument summarization: a prototype system and its evaluation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL-2002.</booktitle>
<contexts>
<context position="6402" citStr="Lin and Hovy, 2002" startWordPosition="933" endWordPosition="936"> paragraphs) of the documents and extracting those with highest scores, while abstraction summarization usually needs information fusion (Barzilay et al., 1999), sentence compression (Knight and Marcu, 2002) and reformulation (McKeown et al., 1999). In this study, we focus on extractive summarization. The centroid-based method (Radev et al., 2004) is one of the most popular extractive summarization methods. MEAD2 is an implementation of the centroid-based method that scores sentences based on sentence-level and inter-sentence features, including cluster centroids, position, TFIDF, etc. NeATS (Lin and Hovy, 2002) is a project on multidocument summarization at ISI based on the single-document summarizer-SUMMARIST. Sentence position, term frequency, topic signature and term clustering are used to select important content. MMR (Goldstein et al., 1999) is used to remove redundancy and stigma word filters and time stamps are used to improve cohesion and coherence. To further explore user interface issues, iNeATS (Leuski et al., 2003) is developed based on NeATS. XDoX (Hardy et al., 1998) is a cross document summarizer designed specifically to summarize large document sets. It identifies the most salient th</context>
</contexts>
<marker>Lin, Hovy, 2002</marker>
<rawString>C.-Y. Lin and E. H. Hovy. 2002. From single to multidocument summarization: a prototype system and its evaluation. In Proceedings of ACL-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
<author>E H Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL2003.</booktitle>
<contexts>
<context position="19585" citStr="Lin and Hovy, 2003" startWordPosition="3278" endWordPosition="3281">s (D088 is excluded from the original 60 document sets by NIST) and generic abstracts of each document set with lengths of approximately 100 words or less were required to be created. The documents were news articles collected from TREC-9. The sentences in each article have been separated and the sentence information has been stored into files. The summary of the two datasets are shown in Table 1. DUC 2001 DUC 2002 Task Task 2 Task 2 Number of documents 309 567 Number of clusters 30 59 Data source TREC-9 TREC-9 Summary length 100 words 100 words Table 1. Summary of datasets We used the ROUGE (Lin and Hovy, 2003) toolkit (i.e. ROUGEeval-1.4.2 in this study) for evaluation, which has been widely adopted by DUC for automatic summarization evaluation. It measured summary quality by counting overlapping units such as the n-gram, word sequences and word pairs between the candidate summary and the reference summary. ROUGE-N was an n-gram recall measure computed as follows: ∑ ∑ Countmatch (n −gram) ROUGE N − = S (14) ∈ {Ref Sum} n-gram S ∑ ∑ ∈ Count(n gram) − S∈ {Ref Sum} n-gram S ∈ 4 http://www-nlpir.nist.gov/projects/duc/guidelines/2001.html 5 http://www-nlpir.nist.gov/projects/duc/guidelines/2002.html ω2 </context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>C.-Y. Lin and E. H. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of HLT-NAACL2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T-Y Liu</author>
<author>W-Y Ma</author>
</authors>
<title>Webpage importance analysis using Conditional Markov Random Walk.</title>
<date>2005</date>
<booktitle>In Proceedings of WI2005.</booktitle>
<contexts>
<context position="9133" citStr="Liu and Ma, 2005" startWordPosition="1350" endWordPosition="1353">e ranking algo2 http://www.summarization.com/mead/ 756 rithm by differentiating intra-document links and inter-document links between sentences. All these methods make use of the relationships between sentences and select sentences according to the “votes” or “recommendations” from their neighboring sentences, which is similar to PageRank. Other related work includes topic-focused multidocument summarization (Daumé. and Marcu, 2006; Gupta et al., 2007; Wan et al., 2007), which aims to produce summary biased to a given topic or query. It is noteworthy that our proposed approach is inspired by (Liu and Ma, 2005), which proposes the Conditional Markov Random Walk Model based on two-layer web graph in the tasks of web page retrieval. 3 The Basic Graph-Based Model (GM) The basic graph-based model is essentially a way of deciding the importance of a vertex within a graph based on global information recursively drawn from the entire graph. The basic idea is that of “voting” or “recommendation” between the vertices. A link between two vertices is considered as a vote cast from one vertex to the other vertex. The score associated with a vertex is determined by the votes that are cast for it, and the score o</context>
</contexts>
<marker>Liu, Ma, 2005</marker>
<rawString>T.-Y. Liu and W.-Y. Ma. 2005. Webpage importance analysis using Conditional Markov Random Walk. In Proceedings of WI2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>E Bloedorn</author>
</authors>
<title>Summarizing similarities and differences among related documents.</title>
<date>2000</date>
<journal>Information Retrieval,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="7939" citStr="Mani and Bloedorn, 2000" startWordPosition="1170" endWordPosition="1173">resentation of topics based on topic themes. In addition, Marcu (2001) selects important sentences based on the discourse structure of the text. TNO’s system (Kraaij et al., 2001) scores sentences by combining a unigram language model approach with a Bayesian classifier based on surface features. Nenkova and Louis (2008) investigate how summary length and the characteristics of the input influence the summary quality in multi-document summarization. Graph-based models have been proposed to rank sentences or passages based on the PageRank algorithm (Page et al., 1998) or its variants. Websumm (Mani and Bloedorn, 2000) uses a graphconnectivity model and operates under the assumption that nodes which are connected to many other nodes are likely to carry salient information. LexPageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. It constructs a sentence connectivity matrix and compute sentence importance based on an algorithm similar to PageRank. Mihalcea and Tarau (2005) also propose a similar algorithm based on PageRank to compute sentence importance for document summarization. Wan and Yang (2006) improve the ranking algo2 http://</context>
</contexts>
<marker>Mani, Bloedorn, 2000</marker>
<rawString>I. Mani and E. Bloedorn. 2000. Summarizing similarities and differences among related documents. Information Retrieval, 1(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>Discourse-based summarization in DUC–</title>
<date>2001</date>
<booktitle>In SIGIR 2001 Workshop on Text Summarization.</booktitle>
<contexts>
<context position="7385" citStr="Marcu (2001)" startWordPosition="1088" endWordPosition="1089">e user interface issues, iNeATS (Leuski et al., 2003) is developed based on NeATS. XDoX (Hardy et al., 1998) is a cross document summarizer designed specifically to summarize large document sets. It identifies the most salient themes within the set by passage clustering and then composes an extraction summary, which reflects these main themes. Much other work also explores to find topic themes in the documents for summarization, e.g. Harabagiu and Lacatusu (2005) investigate five different topic representations and introduce a novel representation of topics based on topic themes. In addition, Marcu (2001) selects important sentences based on the discourse structure of the text. TNO’s system (Kraaij et al., 2001) scores sentences by combining a unigram language model approach with a Bayesian classifier based on surface features. Nenkova and Louis (2008) investigate how summary length and the characteristics of the input influence the summary quality in multi-document summarization. Graph-based models have been proposed to rank sentences or passages based on the PageRank algorithm (Page et al., 1998) or its variants. Websumm (Mani and Bloedorn, 2000) uses a graphconnectivity model and operates u</context>
</contexts>
<marker>Marcu, 2001</marker>
<rawString>D. Marcu. Discourse-based summarization in DUC– 2001. 2001. In SIGIR 2001 Workshop on Text Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McKeown</author>
<author>J Klavans</author>
<author>V Hatzivassiloglou</author>
<author>R Barzilay</author>
<author>E Eskin</author>
</authors>
<title>Towards multidocument summarization by reformulation: progress and prospects,</title>
<date>1999</date>
<booktitle>in Proceedings of AAAI1999.</booktitle>
<contexts>
<context position="6031" citStr="McKeown et al., 1999" startWordPosition="879" endWordPosition="882">vely. We show the experiments and results in Section 5 and finally we conclude this paper in Section 6. 2 Related Work Generally speaking, summarization methods can be abstractive summarization or extractive summarization. Extractive summarization is a simple but robust method for text summarization and it involves assigning saliency scores to some units (e.g. sentences, paragraphs) of the documents and extracting those with highest scores, while abstraction summarization usually needs information fusion (Barzilay et al., 1999), sentence compression (Knight and Marcu, 2002) and reformulation (McKeown et al., 1999). In this study, we focus on extractive summarization. The centroid-based method (Radev et al., 2004) is one of the most popular extractive summarization methods. MEAD2 is an implementation of the centroid-based method that scores sentences based on sentence-level and inter-sentence features, including cluster centroids, position, TFIDF, etc. NeATS (Lin and Hovy, 2002) is a project on multidocument summarization at ISI based on the single-document summarizer-SUMMARIST. Sentence position, term frequency, topic signature and term clustering are used to select important content. MMR (Goldstein et</context>
</contexts>
<marker>McKeown, Klavans, Hatzivassiloglou, Barzilay, Eskin, 1999</marker>
<rawString>K. McKeown, J. Klavans, V. Hatzivassiloglou, R. Barzilay and E. Eskin. 1999. Towards multidocument summarization by reformulation: progress and prospects, in Proceedings of AAAI1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>P Tarau</author>
</authors>
<title>A language independent algorithm for single and multiple document summarization.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCNLP’2005.</booktitle>
<contexts>
<context position="2976" citStr="Mihalcea and Tarau, 2005" startWordPosition="426" endWordPosition="429">ive summarization methods to analyze the information stored in different documents and extract the globally important information to reflect the main topic. In recent years, both unsupervised and supervised methods have been proposed to analyze the information contained in a document set and extract highly salient sentences into the summary, based on syntactic or statistical features. Most recently, the graph-based models have been successfully applied for multi-document summarization by making use of the “voting” or “recommendations” between sentences in the documents (Erkan and Radev, 2004; Mihalcea and Tarau, 2005; Wan and Yang, 2006). The model first constructs a directed or undirected graph to reflect the relationships between the sentences and then applies the graph-based ranking algorithm to compute the rank scores for the sentences. The sentences with large rank scores are chosen into the summary. However, the model makes uniform use of the sentences in different documents, i.e. all the sentences are ranked without considering the document-level information and the sentence-todocument relationship. Actually, given a document set, different documents are not equally important. For example, the docu</context>
<context position="8375" citStr="Mihalcea and Tarau (2005)" startWordPosition="1238" endWordPosition="1241">ent summarization. Graph-based models have been proposed to rank sentences or passages based on the PageRank algorithm (Page et al., 1998) or its variants. Websumm (Mani and Bloedorn, 2000) uses a graphconnectivity model and operates under the assumption that nodes which are connected to many other nodes are likely to carry salient information. LexPageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. It constructs a sentence connectivity matrix and compute sentence importance based on an algorithm similar to PageRank. Mihalcea and Tarau (2005) also propose a similar algorithm based on PageRank to compute sentence importance for document summarization. Wan and Yang (2006) improve the ranking algo2 http://www.summarization.com/mead/ 756 rithm by differentiating intra-document links and inter-document links between sentences. All these methods make use of the relationships between sentences and select sentences according to the “votes” or “recommendations” from their neighboring sentences, which is similar to PageRank. Other related work includes topic-focused multidocument summarization (Daumé. and Marcu, 2006; Gupta et al., 2007; Wa</context>
</contexts>
<marker>Mihalcea, Tarau, 2005</marker>
<rawString>R. Mihalcea and P. Tarau. 2005. A language independent algorithm for single and multiple document summarization. In Proceedings of IJCNLP’2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>A Louis</author>
</authors>
<title>Can you summarize this? Identifying correlates of input difficulty for generic multi-document summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT.</booktitle>
<contexts>
<context position="7637" citStr="Nenkova and Louis (2008)" startWordPosition="1125" endWordPosition="1128"> the set by passage clustering and then composes an extraction summary, which reflects these main themes. Much other work also explores to find topic themes in the documents for summarization, e.g. Harabagiu and Lacatusu (2005) investigate five different topic representations and introduce a novel representation of topics based on topic themes. In addition, Marcu (2001) selects important sentences based on the discourse structure of the text. TNO’s system (Kraaij et al., 2001) scores sentences by combining a unigram language model approach with a Bayesian classifier based on surface features. Nenkova and Louis (2008) investigate how summary length and the characteristics of the input influence the summary quality in multi-document summarization. Graph-based models have been proposed to rank sentences or passages based on the PageRank algorithm (Page et al., 1998) or its variants. Websumm (Mani and Bloedorn, 2000) uses a graphconnectivity model and operates under the assumption that nodes which are connected to many other nodes are likely to carry salient information. LexPageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. It cons</context>
</contexts>
<marker>Nenkova, Louis, 2008</marker>
<rawString>A. Nenkova and A. Louis. 2008. Can you summarize this? Identifying correlates of input difficulty for generic multi-document summarization. In Proceedings of ACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Page</author>
<author>S Brin</author>
<author>R Motwani</author>
<author>T Winograd</author>
</authors>
<title>The pagerank citation ranking: Bringing order to the web. Technical report, Stanford Digital Libraries.</title>
<date>1998</date>
<contexts>
<context position="7888" citStr="Page et al., 1998" startWordPosition="1162" endWordPosition="1165">pic representations and introduce a novel representation of topics based on topic themes. In addition, Marcu (2001) selects important sentences based on the discourse structure of the text. TNO’s system (Kraaij et al., 2001) scores sentences by combining a unigram language model approach with a Bayesian classifier based on surface features. Nenkova and Louis (2008) investigate how summary length and the characteristics of the input influence the summary quality in multi-document summarization. Graph-based models have been proposed to rank sentences or passages based on the PageRank algorithm (Page et al., 1998) or its variants. Websumm (Mani and Bloedorn, 2000) uses a graphconnectivity model and operates under the assumption that nodes which are connected to many other nodes are likely to carry salient information. LexPageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. It constructs a sentence connectivity matrix and compute sentence importance based on an algorithm similar to PageRank. Mihalcea and Tarau (2005) also propose a similar algorithm based on PageRank to compute sentence importance for document summarization. Wa</context>
</contexts>
<marker>Page, Brin, Motwani, Winograd, 1998</marker>
<rawString>L. Page, S. Brin, R. Motwani and T. Winograd. 1998. The pagerank citation ranking: Bringing order to the web. Technical report, Stanford Digital Libraries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Radev</author>
<author>H Y Jing</author>
<author>M Stys</author>
<author>D Tam</author>
</authors>
<title>Centroid-based summarization of multiple documents.</title>
<date>2004</date>
<journal>Information Processing and Management,</journal>
<volume>40</volume>
<pages>919--938</pages>
<contexts>
<context position="6132" citStr="Radev et al., 2004" startWordPosition="894" endWordPosition="897"> 2 Related Work Generally speaking, summarization methods can be abstractive summarization or extractive summarization. Extractive summarization is a simple but robust method for text summarization and it involves assigning saliency scores to some units (e.g. sentences, paragraphs) of the documents and extracting those with highest scores, while abstraction summarization usually needs information fusion (Barzilay et al., 1999), sentence compression (Knight and Marcu, 2002) and reformulation (McKeown et al., 1999). In this study, we focus on extractive summarization. The centroid-based method (Radev et al., 2004) is one of the most popular extractive summarization methods. MEAD2 is an implementation of the centroid-based method that scores sentences based on sentence-level and inter-sentence features, including cluster centroids, position, TFIDF, etc. NeATS (Lin and Hovy, 2002) is a project on multidocument summarization at ISI based on the single-document summarizer-SUMMARIST. Sentence position, term frequency, topic signature and term clustering are used to select important content. MMR (Goldstein et al., 1999) is used to remove redundancy and stigma word filters and time stamps are used to improve </context>
</contexts>
<marker>Radev, Jing, Stys, Tam, 2004</marker>
<rawString>D. R. Radev, H. Y. Jing, M. Stys and D. Tam. 2004. Centroid-based summarization of multiple documents. Information Processing and Management, 40: 919-938.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wan</author>
<author>J Yang</author>
</authors>
<title>Improved affinity graph based multi-document summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL2006.</booktitle>
<contexts>
<context position="2997" citStr="Wan and Yang, 2006" startWordPosition="430" endWordPosition="433">to analyze the information stored in different documents and extract the globally important information to reflect the main topic. In recent years, both unsupervised and supervised methods have been proposed to analyze the information contained in a document set and extract highly salient sentences into the summary, based on syntactic or statistical features. Most recently, the graph-based models have been successfully applied for multi-document summarization by making use of the “voting” or “recommendations” between sentences in the documents (Erkan and Radev, 2004; Mihalcea and Tarau, 2005; Wan and Yang, 2006). The model first constructs a directed or undirected graph to reflect the relationships between the sentences and then applies the graph-based ranking algorithm to compute the rank scores for the sentences. The sentences with large rank scores are chosen into the summary. However, the model makes uniform use of the sentences in different documents, i.e. all the sentences are ranked without considering the document-level information and the sentence-todocument relationship. Actually, given a document set, different documents are not equally important. For example, the documents close to the ma</context>
<context position="8505" citStr="Wan and Yang (2006)" startWordPosition="1257" endWordPosition="1260">8) or its variants. Websumm (Mani and Bloedorn, 2000) uses a graphconnectivity model and operates under the assumption that nodes which are connected to many other nodes are likely to carry salient information. LexPageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. It constructs a sentence connectivity matrix and compute sentence importance based on an algorithm similar to PageRank. Mihalcea and Tarau (2005) also propose a similar algorithm based on PageRank to compute sentence importance for document summarization. Wan and Yang (2006) improve the ranking algo2 http://www.summarization.com/mead/ 756 rithm by differentiating intra-document links and inter-document links between sentences. All these methods make use of the relationships between sentences and select sentences according to the “votes” or “recommendations” from their neighboring sentences, which is similar to PageRank. Other related work includes topic-focused multidocument summarization (Daumé. and Marcu, 2006; Gupta et al., 2007; Wan et al., 2007), which aims to produce summary biased to a given topic or query. It is noteworthy that our proposed approach is in</context>
<context position="21426" citStr="Wan and Yang, 2006" startWordPosition="3567" endWordPosition="3570">sults: ROUGE-1 (unigram-based), ROUGE-2 (bigrambased), and ROUGE-W (based on weighted longest common subsequence, weight=1.2). In order to truncate summaries longer than length limit, we used the “-l” option in ROUGE toolkit. We also used the “-m” option for word stemming. 5.2 Evaluation Results In the experiments, the combination weight λ for the proposed summarization model is typically set to 0.5 without tuning, i.e. the two documents for two sentences have equal influence on the summarization process. Note that after the saliency scores of sentences have been obtained, a greedy algorithm (Wan and Yang, 2006) is applied to remove redundancy and finally choose both informative and novel sentences into the summary. The algorithm is actually a variant version of the MMR algorithm (Goldstein et al., 1999). The proposed document-based graph model (denoted as DGM) with different settings is compared with the basic graph-based Model (denoted as GM), the top three performing systems and two baseline systems on DUC2001 and DUC2002, respectively. The top three systems are the systems with highest ROUGE scores, chosen from the performing systems on each task respectively. The lead baseline and coverage basel</context>
</contexts>
<marker>Wan, Yang, 2006</marker>
<rawString>X. Wan and J. Yang. 2006. Improved affinity graph based multi-document summarization. In Proceedings of HLT-NAACL2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wan</author>
<author>J Yang</author>
<author>J Xiao</author>
</authors>
<title>Manifold-ranking based topic-focused multi-document summarization.</title>
<date>2007</date>
<booktitle>In Proceedings of IJCAI2007.</booktitle>
<contexts>
<context position="8990" citStr="Wan et al., 2007" startWordPosition="1323" endWordPosition="1326">5) also propose a similar algorithm based on PageRank to compute sentence importance for document summarization. Wan and Yang (2006) improve the ranking algo2 http://www.summarization.com/mead/ 756 rithm by differentiating intra-document links and inter-document links between sentences. All these methods make use of the relationships between sentences and select sentences according to the “votes” or “recommendations” from their neighboring sentences, which is similar to PageRank. Other related work includes topic-focused multidocument summarization (Daumé. and Marcu, 2006; Gupta et al., 2007; Wan et al., 2007), which aims to produce summary biased to a given topic or query. It is noteworthy that our proposed approach is inspired by (Liu and Ma, 2005), which proposes the Conditional Markov Random Walk Model based on two-layer web graph in the tasks of web page retrieval. 3 The Basic Graph-Based Model (GM) The basic graph-based model is essentially a way of deciding the importance of a vertex within a graph based on global information recursively drawn from the entire graph. The basic idea is that of “voting” or “recommendation” between the vertices. A link between two vertices is considered as a vot</context>
</contexts>
<marker>Wan, Yang, Xiao, 2007</marker>
<rawString>X. Wan, J. Yang and J. Xiao. 2007. Manifold-ranking based topic-focused multi-document summarization. In Proceedings of IJCAI2007.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>