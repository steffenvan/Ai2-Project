<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.025823">
<title confidence="0.991465">
Automatic Parallel Fragment Extraction from Noisy Data
</title>
<author confidence="0.997294">
Jason Riesa and Daniel Marcu
</author>
<affiliation confidence="0.997349666666667">
Information Sciences Institute
Viterbi School of Engineering
University of Southern California
</affiliation>
<email confidence="0.997894">
{riesa, marcu}@isi.edu
</email>
<sectionHeader confidence="0.997359" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999437461538461">
We present a novel method to detect parallel
fragments within noisy parallel corpora. Isolat-
ing these parallel fragments from the noisy data
in which they are contained frees us from noisy
alignments and stray links that can severely
constrain translation-rule extraction. We do
this with existing machinery, making use of an
existing word alignment model for this task.
We evaluate the quality and utility of the ex-
tracted data on large-scale Chinese-English and
Arabic-English translation tasks and show sig-
nificant improvements over a state-of-the-art
baseline.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999205">
A decade ago, Banko and Brill (2001) showed that
scaling to very large corpora is game-changing for a
variety of tasks. Methods that work well in a small-
data setting often lose their luster when moving to
large data. Conversely, other methods that seem to
perform poorly in that same small-data setting, may
perform markedly differently when trained on large
data.
Perhaps most importantly, Banko and Brill
showed that there was no significant variation in per-
formance among a variety of methods trained at-
scale with large training data. The takeaway? If you
desire to scale to large datasets, use a simple solution
for your task, and throw in as much data as possible.
The community at large has taken this message to
heart, and in most cases it has been an effective way
to increase performance.
Today, for machine translation, more data than
what we already have is getting harder and harder
to come by; we require large parallel corpora to
</bodyText>
<figureCaption confidence="0.925303833333333">
Figure 1: Example of a word alignment resulting from
noisy parallel data. The structure of the resulting align-
ment makes it difficult to find and extract parallel frag-
ments via the standard heuristics or simply by inspection.
How can we discover automatically those parallel frag-
ments hidden within such data?
</figureCaption>
<bodyText confidence="0.999435777777778">
train state-of-the-art statistical, data-driven models.
Groups that depend on clearinghouses like LDC for
their data increasingly find that there is less of a man-
date to gather parallel corpora on the scale of what
was produced in the last 5-10 years. Others, who di-
rectly exploit the entire web to gather such data will
necessarily run up against a wall after all that data
has been collected.
We need to learn how to do more with the data
we already have. Previous work has focused on
detecting parallel documents and sentences on the
web, e.g. (Zhao and Vogel, 2002; Fung and Che-
ung, 2004; Wu and Fung, 2005). Munteanu and
Marcu (2006), and later Quirk et al. (2007), extend
the state-of-the-art for this task to parallel fragments.
In this paper, we present a novel method for de-
tecting parallel fragments in large, existing and po-
tentially noisy parallel corpora using existing ma-
</bodyText>
<page confidence="0.898992333333333">
538
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 538–542,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.999413333333333">
chinery and show significant improvements to two
state-of-the-art MT systems. We also depart from
previous work in that we only consider parallel cor-
pora that have previously been cleaned, sanitized,
and thought to be non-noisy, e.g. parallel corpora
available from LDC.
</bodyText>
<sectionHeader confidence="0.879718" genericHeader="method">
2 Detecting Noisy Data
</sectionHeader>
<bodyText confidence="0.9999246">
In order to extract previously unextractable good
parallel data, we must first detect the bad data. In
doing so, we will make use of existing machinery in
a novel way. We directly use the alignment model to
detect weak or undesirable data for translation.
</bodyText>
<subsectionHeader confidence="0.959747">
2.1 Alignment Model as Noisy Data Detector
</subsectionHeader>
<bodyText confidence="0.999987888888889">
The alignment model we use in our experiments is
that described in (Riesa et al., 2011), modified to
output full derivation trees and model scores along
with alignments. Our reasons for using this particu-
lar alignment method are twofold: it provides a natu-
ral way to hierarchically partition subsentential seg-
ments, and is also empirically quite accurate in mod-
eling word alignments, in general. This latter quality
is important, not solely for downstream translation
quality, but also for the basis of our claims with re-
spect to detecting noisy or unsuitable data:
The alignment model we employ is discrimina-
tively trained to know what good alignments be-
tween parallel data look like. When this model pre-
dicts an alignment with a low model score, given an
input sentence pair, we might say the model is “con-
fused.” In this case, the alignment probably doesn’t
look like the examples it has been trained on.
</bodyText>
<listItem confidence="0.99947">
1. It could be that the data is parallel, but the model
is very confused. (modeling problem)
2. It could be that the data is noisy, and the model
is very confused. (data problem)
</listItem>
<bodyText confidence="0.9996255">
The general accuracy of the alignment model we
employ makes the former case unlikely. Therefore,
a key assumption we make is to assume a low model
score accompanies noisy data, and use this data as
candidates from which to extract non-noisy parallel
segments.
</bodyText>
<subsectionHeader confidence="0.9987">
2.2 A Brief Example
</subsectionHeader>
<bodyText confidence="0.969731333333333">
As an illustrative example, consider the follow-
ing sentence pair in our training corpus taken from
LDC2005T10. This is the sentence pair shown in
</bodyText>
<figureCaption confidence="0.574118">
Figure 1:
</figureCaption>
<bodyText confidence="0.95994605">
fate brought us together on that wonderful summer day
and one year later, shou – tao and i were married not only
in the united states but also in taiwan .
t A[� � pr4 , � � � ±�±-r � 93�N 1+1 M X[A k
; ffi -?A A 4 16W M A F1 W , R fA R 40-v 9� A —k-:F -
In this sentence pair there are only two parallel
phrases, corresponding to the underlined and double-
underlined strings. There are a few scattered word
pairs which may have a natural correspondence,1 but
no other larger phrases.2
In this work we are concerned with finding large
phrases,3 since very small phrases tend to be ex-
tractible even when data is noisy. Bad alignments
tend to cause conflicts when extracting large phrases
due to unexpected, stray links in the alignment ma-
trix; smaller fragments will have less opportunity to
come into conflict with incorrect, stray links due to
noisy data or alignment model error. We consider
large enough phrases for our purposes to be phrases
of size greater than 3, and ignore smaller fragments.
</bodyText>
<subsectionHeader confidence="0.981108333333333">
2.3 Parallel Fragment Extraction
2.3.1 A Hierarchical Alignment Model and its
Derivation Trees
</subsectionHeader>
<bodyText confidence="0.9998641">
The alignment model we use, (Riesa et al.,
2011), is a discriminatively trained model which at
alignment-time walks up the English parse-tree and,
at every node in the tree, generates alignments by re-
cursively scoring and combining alignments gener-
ated at the current node’s children, building up larger
and larger alignments. This process works similarly
to a CKY parser, moving bottom-up and generating
larger and larger constituents until it has predicted
the full tree spanning the entire sentence. How-
</bodyText>
<footnote confidence="0.998173666666667">
1For example, (I, tt) and (Taiwan, pr4)
2The rest of the Chinese describes where the couple is from;
the speaker, she says, is an American raised in New Jersey.
3We count the size of the phrase according to the number of
English words it contains; one could be more conservative by
constraining both sides.
</footnote>
<page confidence="0.990653">
539
</page>
<figure confidence="0.830572333333333">
NP [-0.5130]
NP [14.2034] PP [9.5130]
IN JJ CC JJ NN
</figure>
<figureCaption confidence="0.990424571428572">
Figure 2: From LDC2004T08, when the NP fragment
shown here is combined to make a larger span with a sis-
ter PP fragment, the alignment model objects due to non-
parallel data under the PP, voicing a score of -0.5130. We
extract and append to our training corpus the NP fragment
depicted, from which we later learn 5 additional transla-
tion rules.
</figureCaption>
<bodyText confidence="0.9853567">
ever, instead of generating syntactic structures, we
are generating alignments.
In moving bottom-up along the tree, just as there
is a derivation tree for a CKY parse, we can also fol-
low backpointers to extract the derivation tree of the
1-best alignment starting from the root node. This
derivation tree gives a hierarchical partitioning of the
alignment and the associated word-spans. We can
also inspect model scores at each node in the deriva-
tion tree.
</bodyText>
<subsectionHeader confidence="0.96407">
2.3.2 Using the Alignment Model to Detect
Parallel Fragments
</subsectionHeader>
<bodyText confidence="0.99893155">
For each training example in our parallel cor-
pus, we have an alignment derivation tree. Be-
cause the derivation tree is essentially isomorphic
to the English parse tree, the derivation tree repre-
sents a hierarchical partitioning of the training ex-
ample into syntactic segments. We traverse the tree
top-down, inspecting the parallel fragments implied
by the derivation at each point, and their associated
model scores.
The idea behind this top-down traversal is that al-
though some nodes, and perhaps entire derivations,
may be low-scoring, there are often high-scoring
fragments that make up the larger derivation which
are worthy of extraction. Figure 2 shows an ex-
ample. We recursively traverse the derivation, top-
down, extracting the largest fragment possible at
any derivation node whose alignment model score is
higher than some threshold T, and whose associated
English and foreign spans meet a set of important
constraints:
</bodyText>
<listItem confidence="0.991179833333333">
1. The parent node in the derivation has a score less
than T.
2. The length of the English span is &gt; 3.
3. There are no unaligned foreign words inside the
fragment that are also aligned to English words
outside the fragment.
</listItem>
<bodyText confidence="0.983266588235294">
Once a fragment has been extracted, we do not re-
curse any further down the subtree.
Constraint 1 is a candidate constraint, and forces
us to focus on segments of parallel sentences with
low model scores; these are segments likely to con-
sist of bad alignments due to noisy data or aligner
error.
Constraint 2 is a conservativity constraint – we
are more confident in model scores over larger frag-
ments with more context than smaller ones with min-
imal context. This constraint also parameterizes the
notion that larger fragments are the type more often
precluded from extraction due to stray or incorrect
word-alignment links; additionally, we are already
likely to be able to extract smaller fragments using
standard methods, and as such, they are less useful
to us here.
Constraint 3 is a content constraint, limiting us
from extracting fragments with blocks of unaligned
foreign words that don’t belong in this particular
fragment because they are aligned elsewhere. If we
threw out this constraint, then in translating from
Chinese to English, we would erroneously learn to
delete blocks of Chinese words that otherwise should
be translated. When foreign words are unaligned ev-
erywhere within a parallel sentence, then they can
be included within the extracted fragment. Common
examples in Chinese are function words such as 的,
个, and 了. Put another way, we only allow globally
unaligned words in extracted fragments.
Computing T. In computing our extraction thresh-
old T, we must decide what proportion of fragments
we consider to be low-scoring and least likely to be
useful for translation. We make the rather strong as-
</bodyText>
<figure confidence="0.931738272727273">
with multi-sensory experiences
ADJP
奇
幻
R
X实
an
惊险
540
sumption that this is the bottom 10% of the data.4
Corpus Extracted Rules BLEU
</figure>
<sectionHeader confidence="0.914632" genericHeader="method">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999970048780488">
We evaluate our parallel fragment extraction in a
large-scale Chinese-English and Arabic-English MT
setting. In our experiments we use a tree-to-string
syntax-based MT system (Galley et al., 2004), and
evaluate on a standard test set, NIST08. We parse the
English side of our parallel corpus with the Berkeley
parser (Petrov et al., 2006), and tune parameters of
the MT system with MIRA (Chiang et al., 2008). We
decode with an integrated language model trained on
about 4 billion words of English.
Chinese-English We align a parallel corpus of
8.4M parallel segments, with 210M words of En-
glish and 193M words of Chinese. From this we
extract 868,870 parallel fragments according to the
process described in Section 2, and append these
fragments to the end of the parallel corpus. In doing
so, we have created a larger parallel corpus of 9.2M
parallel segments, consisting of 217M and 198M
words of English and Chinese, respectively.
Arabic-English We align a parallel corpus of
9.0M parallel segments, with 223M words of En-
glish and 194M words of Arabic. From this we ex-
tract 996,538 parallel fragments, and append these
fragments to the end of the parallel corpus. The re-
sulting corpus has 10M parallel segments, consisting
of 233M and 202M words of English and Arabic, re-
spectively.
Results are shown in Table 1. Using our parallel
fragment extraction, we learn 68M additional unique
Arabic-English rules that are not in the baseline sys-
tem; likewise, we learn 38M new unique Chinese-
English rules not in the baseline system for that lan-
guage pair. Note that we are not simply duplicat-
ing portions of the parallel data. While each se-
quence fragment of source and target words we ex-
tract will be found elsewhere in the larger parallel
corpus, these fragments will largely not make it into
fruitful translation rules to be used in the downstream
MT system.
We see gains in BLEU score across two differ-
ent language pairs, showing empirically that we are
</bodyText>
<footnote confidence="0.99924775">
4One may wish to experiment with different ranges here, but
each requires a separate time-consuming downstream MT ex-
periment. In this work, it turns out that scrutinizing 10% of the
data is productive and empirically reasonable.
</footnote>
<table confidence="0.99910925">
Baseline (Ara-Eng) 750M 50.0
+Extracted fragments 818M 50.4
Baseline (Chi-Eng) 270M 31.5
+Extracted fragments 308M 32.0
</table>
<tableCaption confidence="0.9894906">
Table 1: End-to-end translation experiments with and
without extracted fragments. We are learning many more
unique rules; BLEU score gains are significant with p &lt;
0.05 for Arabic-English and p &lt; 0.01 for Chinese-
English.
</tableCaption>
<bodyText confidence="0.9950275">
learning new and useful translation rules we previ-
ously were not in our grammars. These results are
significant with p &lt; 0.05 for Arabic-English and
p &lt; 0.01 for Chinese-English.
</bodyText>
<sectionHeader confidence="0.999919" genericHeader="discussions">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999947954545454">
All alignment models we have experimented with
will fall down in the presence of noisy data. Impor-
tantly, even if the alignment model were able to yield
“perfect” alignments with no alignment links among
noisy sections of the parallel data precluding us from
extracting reasonable rules or phrase pairs, we would
still have to deal with downstream rule extraction
heuristics and their tendency to blow up a translation
grammar in the presence of large swaths of unaligned
words. Absent a mechanism within the alignment
model itself to deal with this problem, we provide a
simple way to recover from noisy data without the
introduction of new tools.
Summing up, parallel data in the world is not
unlimited. We cannot always continue to double
our data for increased performance. Parallel data
creation is expensive, and automatic discovery is
resource-intensive (Uszkoreit et al., 2010). We have
presented a technique that helps to squeeze more out
of an already large, state-of-the-art MT system, us-
ing existing pieces of the pipeline to do so in a novel
way.
</bodyText>
<sectionHeader confidence="0.999018" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<reference confidence="0.6137966">
This work was supported by DARPA BOLT via BBN sub-
contract HR0011-12-C-0014. We thank our three anony-
mous reviewers for thoughtful comments. Thanks also to
Kevin Knight, David Chiang, Liang Huang, and Philipp
Koehn for helpful discussions.
</reference>
<page confidence="0.998436">
541
</page>
<sectionHeader confidence="0.998333" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999833780487805">
Michele Banko and Eric Brill. 2001. Scaling to very very
large corpora for natural language disambiguation. In
Proc. of the ACL, pages 26–33.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proc. of EMNLP, pages
224–233.
Pascale Fung and Percy Cheung. 2004. Mining very non-
parallel corpora: Parallel sentence and lexicon extrac-
tion via boostrapping and EM. In Proc. of EMNLP,
pages 57–63.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In Proc. of
HLT-NAACL, pages 273–280.
Dragos Stefan Munteanu and Daniel Marcu. 2006. Ex-
tracting parallel sub-sentential fragments from non-
parallel corpora. In Proc. of COLING/ACL, Sydney,
Australia.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. of COLING-ACL.
Chris Quirk, Raghavendra Udupa, and Arul Menezes.
2007. Generative models of noisy translations with ap-
plications to parallel fragment extraction. In Proceed-
ings ofMT Summit XI.
Jason Riesa, Ann Irvine, and Daniel Marcu. 2011.
Feature-rich language-independent syntax-based
alignment for statistical machine translation. In Proc.
of EMNLP, pages 497–507.
Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe Du-
biner. 2010. Large scale parallel document mining
for machine translation. In Proc. of COLING, pages
1101–1109.
Dekai Wu and Pascale Fung. 2005. Inversion transduc-
tion grammar constraints for mining parallel sentences
from quasi-comparable corpora. In Proc. of IJCNLP,
pages 257–268.
Bing Zhao and Stephan Vogel. 2002. Adaptive paral-
lel sentences mining from web bilingual news collec-
tion. In IEEE International Conference on Data Min-
ing, pages 745–748, Maebashi City, Japan.
</reference>
<page confidence="0.997378">
542
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.799453">
<title confidence="0.997463">Automatic Parallel Fragment Extraction from Noisy Data</title>
<author confidence="0.910668">Riesa Marcu</author>
<affiliation confidence="0.993190333333333">Information Sciences Institute Viterbi School of Engineering University of Southern California</affiliation>
<abstract confidence="0.992514714285714">We present a novel method to detect parallel fragments within noisy parallel corpora. Isolating these parallel fragments from the noisy data in which they are contained frees us from noisy alignments and stray links that can severely constrain translation-rule extraction. We do this with existing machinery, making use of an existing word alignment model for this task. We evaluate the quality and utility of the extracted data on large-scale Chinese-English and Arabic-English translation tasks and show significant improvements over a state-of-the-art baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>This work was supported by DARPA BOLT via BBN subcontract HR0011-12-C-0014. We thank our three anonymous reviewers for thoughtful comments. Thanks also to Kevin Knight, David Chiang, Liang Huang, and Philipp Koehn for helpful discussions.</title>
<marker></marker>
<rawString>This work was supported by DARPA BOLT via BBN subcontract HR0011-12-C-0014. We thank our three anonymous reviewers for thoughtful comments. Thanks also to Kevin Knight, David Chiang, Liang Huang, and Philipp Koehn for helpful discussions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Eric Brill</author>
</authors>
<title>Scaling to very very large corpora for natural language disambiguation.</title>
<date>2001</date>
<booktitle>In Proc. of the ACL,</booktitle>
<pages>26--33</pages>
<contexts>
<context position="827" citStr="Banko and Brill (2001)" startWordPosition="116" endWordPosition="119">tract We present a novel method to detect parallel fragments within noisy parallel corpora. Isolating these parallel fragments from the noisy data in which they are contained frees us from noisy alignments and stray links that can severely constrain translation-rule extraction. We do this with existing machinery, making use of an existing word alignment model for this task. We evaluate the quality and utility of the extracted data on large-scale Chinese-English and Arabic-English translation tasks and show significant improvements over a state-of-the-art baseline. 1 Introduction A decade ago, Banko and Brill (2001) showed that scaling to very large corpora is game-changing for a variety of tasks. Methods that work well in a smalldata setting often lose their luster when moving to large data. Conversely, other methods that seem to perform poorly in that same small-data setting, may perform markedly differently when trained on large data. Perhaps most importantly, Banko and Brill showed that there was no significant variation in performance among a variety of methods trained atscale with large training data. The takeaway? If you desire to scale to large datasets, use a simple solution for your task, and t</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>Michele Banko and Eric Brill. 2001. Scaling to very very large corpora for natural language disambiguation. In Proc. of the ACL, pages 26–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>224--233</pages>
<contexts>
<context position="11431" citStr="Chiang et al., 2008" startWordPosition="1897" endWordPosition="1900">likely to be useful for translation. We make the rather strong aswith multi-sensory experiences ADJP 奇 幻 R X实 an 惊险 540 sumption that this is the bottom 10% of the data.4 Corpus Extracted Rules BLEU 3 Evaluation We evaluate our parallel fragment extraction in a large-scale Chinese-English and Arabic-English MT setting. In our experiments we use a tree-to-string syntax-based MT system (Galley et al., 2004), and evaluate on a standard test set, NIST08. We parse the English side of our parallel corpus with the Berkeley parser (Petrov et al., 2006), and tune parameters of the MT system with MIRA (Chiang et al., 2008). We decode with an integrated language model trained on about 4 billion words of English. Chinese-English We align a parallel corpus of 8.4M parallel segments, with 210M words of English and 193M words of Chinese. From this we extract 868,870 parallel fragments according to the process described in Section 2, and append these fragments to the end of the parallel corpus. In doing so, we have created a larger parallel corpus of 9.2M parallel segments, consisting of 217M and 198M words of English and Chinese, respectively. Arabic-English We align a parallel corpus of 9.0M parallel segments, with</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In Proc. of EMNLP, pages 224–233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Percy Cheung</author>
</authors>
<title>Mining very nonparallel corpora: Parallel sentence and lexicon extraction via boostrapping and EM.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>57--63</pages>
<contexts>
<context position="2636" citStr="Fung and Cheung, 2004" startWordPosition="424" endWordPosition="428">ents hidden within such data? train state-of-the-art statistical, data-driven models. Groups that depend on clearinghouses like LDC for their data increasingly find that there is less of a mandate to gather parallel corpora on the scale of what was produced in the last 5-10 years. Others, who directly exploit the entire web to gather such data will necessarily run up against a wall after all that data has been collected. We need to learn how to do more with the data we already have. Previous work has focused on detecting parallel documents and sentences on the web, e.g. (Zhao and Vogel, 2002; Fung and Cheung, 2004; Wu and Fung, 2005). Munteanu and Marcu (2006), and later Quirk et al. (2007), extend the state-of-the-art for this task to parallel fragments. In this paper, we present a novel method for detecting parallel fragments in large, existing and potentially noisy parallel corpora using existing ma538 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 538–542, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics chinery and show significant improvements to two state-of-the-art MT systems. </context>
</contexts>
<marker>Fung, Cheung, 2004</marker>
<rawString>Pascale Fung and Percy Cheung. 2004. Mining very nonparallel corpora: Parallel sentence and lexicon extraction via boostrapping and EM. In Proc. of EMNLP, pages 57–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<pages>273--280</pages>
<contexts>
<context position="11219" citStr="Galley et al., 2004" startWordPosition="1859" endWordPosition="1862">other way, we only allow globally unaligned words in extracted fragments. Computing T. In computing our extraction threshold T, we must decide what proportion of fragments we consider to be low-scoring and least likely to be useful for translation. We make the rather strong aswith multi-sensory experiences ADJP 奇 幻 R X实 an 惊险 540 sumption that this is the bottom 10% of the data.4 Corpus Extracted Rules BLEU 3 Evaluation We evaluate our parallel fragment extraction in a large-scale Chinese-English and Arabic-English MT setting. In our experiments we use a tree-to-string syntax-based MT system (Galley et al., 2004), and evaluate on a standard test set, NIST08. We parse the English side of our parallel corpus with the Berkeley parser (Petrov et al., 2006), and tune parameters of the MT system with MIRA (Chiang et al., 2008). We decode with an integrated language model trained on about 4 billion words of English. Chinese-English We align a parallel corpus of 8.4M parallel segments, with 210M words of English and 193M words of Chinese. From this we extract 868,870 parallel fragments according to the process described in Section 2, and append these fragments to the end of the parallel corpus. In doing so, w</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proc. of HLT-NAACL, pages 273–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragos Stefan Munteanu</author>
<author>Daniel Marcu</author>
</authors>
<title>Extracting parallel sub-sentential fragments from nonparallel corpora.</title>
<date>2006</date>
<booktitle>In Proc. of COLING/ACL,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="2683" citStr="Munteanu and Marcu (2006)" startWordPosition="433" endWordPosition="436">f-the-art statistical, data-driven models. Groups that depend on clearinghouses like LDC for their data increasingly find that there is less of a mandate to gather parallel corpora on the scale of what was produced in the last 5-10 years. Others, who directly exploit the entire web to gather such data will necessarily run up against a wall after all that data has been collected. We need to learn how to do more with the data we already have. Previous work has focused on detecting parallel documents and sentences on the web, e.g. (Zhao and Vogel, 2002; Fung and Cheung, 2004; Wu and Fung, 2005). Munteanu and Marcu (2006), and later Quirk et al. (2007), extend the state-of-the-art for this task to parallel fragments. In this paper, we present a novel method for detecting parallel fragments in large, existing and potentially noisy parallel corpora using existing ma538 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 538–542, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics chinery and show significant improvements to two state-of-the-art MT systems. We also depart from previous work in that we on</context>
</contexts>
<marker>Munteanu, Marcu, 2006</marker>
<rawString>Dragos Stefan Munteanu and Daniel Marcu. 2006. Extracting parallel sub-sentential fragments from nonparallel corpora. In Proc. of COLING/ACL, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL.</booktitle>
<contexts>
<context position="11361" citStr="Petrov et al., 2006" startWordPosition="1884" endWordPosition="1887"> what proportion of fragments we consider to be low-scoring and least likely to be useful for translation. We make the rather strong aswith multi-sensory experiences ADJP 奇 幻 R X实 an 惊险 540 sumption that this is the bottom 10% of the data.4 Corpus Extracted Rules BLEU 3 Evaluation We evaluate our parallel fragment extraction in a large-scale Chinese-English and Arabic-English MT setting. In our experiments we use a tree-to-string syntax-based MT system (Galley et al., 2004), and evaluate on a standard test set, NIST08. We parse the English side of our parallel corpus with the Berkeley parser (Petrov et al., 2006), and tune parameters of the MT system with MIRA (Chiang et al., 2008). We decode with an integrated language model trained on about 4 billion words of English. Chinese-English We align a parallel corpus of 8.4M parallel segments, with 210M words of English and 193M words of Chinese. From this we extract 868,870 parallel fragments according to the process described in Section 2, and append these fragments to the end of the parallel corpus. In doing so, we have created a larger parallel corpus of 9.2M parallel segments, consisting of 217M and 198M words of English and Chinese, respectively. Ara</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proc. of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Raghavendra Udupa</author>
<author>Arul Menezes</author>
</authors>
<title>Generative models of noisy translations with applications to parallel fragment extraction.</title>
<date>2007</date>
<booktitle>In Proceedings ofMT Summit XI.</booktitle>
<contexts>
<context position="2714" citStr="Quirk et al. (2007)" startWordPosition="439" endWordPosition="442">dels. Groups that depend on clearinghouses like LDC for their data increasingly find that there is less of a mandate to gather parallel corpora on the scale of what was produced in the last 5-10 years. Others, who directly exploit the entire web to gather such data will necessarily run up against a wall after all that data has been collected. We need to learn how to do more with the data we already have. Previous work has focused on detecting parallel documents and sentences on the web, e.g. (Zhao and Vogel, 2002; Fung and Cheung, 2004; Wu and Fung, 2005). Munteanu and Marcu (2006), and later Quirk et al. (2007), extend the state-of-the-art for this task to parallel fragments. In this paper, we present a novel method for detecting parallel fragments in large, existing and potentially noisy parallel corpora using existing ma538 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 538–542, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics chinery and show significant improvements to two state-of-the-art MT systems. We also depart from previous work in that we only consider parallel corpora th</context>
</contexts>
<marker>Quirk, Udupa, Menezes, 2007</marker>
<rawString>Chris Quirk, Raghavendra Udupa, and Arul Menezes. 2007. Generative models of noisy translations with applications to parallel fragment extraction. In Proceedings ofMT Summit XI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Riesa</author>
<author>Ann Irvine</author>
<author>Daniel Marcu</author>
</authors>
<title>Feature-rich language-independent syntax-based alignment for statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>497--507</pages>
<contexts>
<context position="3838" citStr="Riesa et al., 2011" startWordPosition="614" endWordPosition="617">-art MT systems. We also depart from previous work in that we only consider parallel corpora that have previously been cleaned, sanitized, and thought to be non-noisy, e.g. parallel corpora available from LDC. 2 Detecting Noisy Data In order to extract previously unextractable good parallel data, we must first detect the bad data. In doing so, we will make use of existing machinery in a novel way. We directly use the alignment model to detect weak or undesirable data for translation. 2.1 Alignment Model as Noisy Data Detector The alignment model we use in our experiments is that described in (Riesa et al., 2011), modified to output full derivation trees and model scores along with alignments. Our reasons for using this particular alignment method are twofold: it provides a natural way to hierarchically partition subsentential segments, and is also empirically quite accurate in modeling word alignments, in general. This latter quality is important, not solely for downstream translation quality, but also for the basis of our claims with respect to detecting noisy or unsuitable data: The alignment model we employ is discriminatively trained to know what good alignments between parallel data look like. W</context>
<context position="6415" citStr="Riesa et al., 2011" startWordPosition="1067" endWordPosition="1070">ding large phrases,3 since very small phrases tend to be extractible even when data is noisy. Bad alignments tend to cause conflicts when extracting large phrases due to unexpected, stray links in the alignment matrix; smaller fragments will have less opportunity to come into conflict with incorrect, stray links due to noisy data or alignment model error. We consider large enough phrases for our purposes to be phrases of size greater than 3, and ignore smaller fragments. 2.3 Parallel Fragment Extraction 2.3.1 A Hierarchical Alignment Model and its Derivation Trees The alignment model we use, (Riesa et al., 2011), is a discriminatively trained model which at alignment-time walks up the English parse-tree and, at every node in the tree, generates alignments by recursively scoring and combining alignments generated at the current node’s children, building up larger and larger alignments. This process works similarly to a CKY parser, moving bottom-up and generating larger and larger constituents until it has predicted the full tree spanning the entire sentence. How1For example, (I, tt) and (Taiwan, pr4) 2The rest of the Chinese describes where the couple is from; the speaker, she says, is an American rai</context>
</contexts>
<marker>Riesa, Irvine, Marcu, 2011</marker>
<rawString>Jason Riesa, Ann Irvine, and Daniel Marcu. 2011. Feature-rich language-independent syntax-based alignment for statistical machine translation. In Proc. of EMNLP, pages 497–507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Uszkoreit</author>
<author>Jay Ponte</author>
<author>Ashok Popat</author>
<author>Moshe Dubiner</author>
</authors>
<title>Large scale parallel document mining for machine translation.</title>
<date>2010</date>
<booktitle>In Proc. of COLING,</booktitle>
<pages>1101--1109</pages>
<marker>Uszkoreit, Ponte, Popat, Dubiner, 2010</marker>
<rawString>Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe Dubiner. 2010. Large scale parallel document mining for machine translation. In Proc. of COLING, pages 1101–1109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Pascale Fung</author>
</authors>
<title>Inversion transduction grammar constraints for mining parallel sentences from quasi-comparable corpora.</title>
<date>2005</date>
<booktitle>In Proc. of IJCNLP,</booktitle>
<pages>257--268</pages>
<contexts>
<context position="2656" citStr="Wu and Fung, 2005" startWordPosition="429" endWordPosition="432"> data? train state-of-the-art statistical, data-driven models. Groups that depend on clearinghouses like LDC for their data increasingly find that there is less of a mandate to gather parallel corpora on the scale of what was produced in the last 5-10 years. Others, who directly exploit the entire web to gather such data will necessarily run up against a wall after all that data has been collected. We need to learn how to do more with the data we already have. Previous work has focused on detecting parallel documents and sentences on the web, e.g. (Zhao and Vogel, 2002; Fung and Cheung, 2004; Wu and Fung, 2005). Munteanu and Marcu (2006), and later Quirk et al. (2007), extend the state-of-the-art for this task to parallel fragments. In this paper, we present a novel method for detecting parallel fragments in large, existing and potentially noisy parallel corpora using existing ma538 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 538–542, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics chinery and show significant improvements to two state-of-the-art MT systems. We also depart from </context>
</contexts>
<marker>Wu, Fung, 2005</marker>
<rawString>Dekai Wu and Pascale Fung. 2005. Inversion transduction grammar constraints for mining parallel sentences from quasi-comparable corpora. In Proc. of IJCNLP, pages 257–268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Stephan Vogel</author>
</authors>
<title>Adaptive parallel sentences mining from web bilingual news collection.</title>
<date>2002</date>
<booktitle>In IEEE International Conference on Data Mining,</booktitle>
<pages>745--748</pages>
<location>Maebashi City, Japan.</location>
<contexts>
<context position="2613" citStr="Zhao and Vogel, 2002" startWordPosition="420" endWordPosition="423">y those parallel fragments hidden within such data? train state-of-the-art statistical, data-driven models. Groups that depend on clearinghouses like LDC for their data increasingly find that there is less of a mandate to gather parallel corpora on the scale of what was produced in the last 5-10 years. Others, who directly exploit the entire web to gather such data will necessarily run up against a wall after all that data has been collected. We need to learn how to do more with the data we already have. Previous work has focused on detecting parallel documents and sentences on the web, e.g. (Zhao and Vogel, 2002; Fung and Cheung, 2004; Wu and Fung, 2005). Munteanu and Marcu (2006), and later Quirk et al. (2007), extend the state-of-the-art for this task to parallel fragments. In this paper, we present a novel method for detecting parallel fragments in large, existing and potentially noisy parallel corpora using existing ma538 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 538–542, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics chinery and show significant improvements to two state-</context>
</contexts>
<marker>Zhao, Vogel, 2002</marker>
<rawString>Bing Zhao and Stephan Vogel. 2002. Adaptive parallel sentences mining from web bilingual news collection. In IEEE International Conference on Data Mining, pages 745–748, Maebashi City, Japan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>