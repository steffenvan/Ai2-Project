<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000015">
<title confidence="0.911937">
Making Tree Kernels practical for Natural Language Learning
</title>
<author confidence="0.971211">
Alessandro Moschitti
</author>
<affiliation confidence="0.9955375">
Department of Computer Science
University of Rome ”Tor Vergata”
</affiliation>
<address confidence="0.703739">
Rome, Italy
</address>
<email confidence="0.996777">
moschitti@info.uniroma2.it
</email>
<sectionHeader confidence="0.996623" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999940833333333">
In recent years tree kernels have been pro-
posed for the automatic learning of natural
language applications. Unfortunately, they
show (a) an inherent super linear complex-
ity and (b) a lower accuracy than tradi-
tional attribute/value methods.
In this paper, we show that tree kernels
are very helpful in the processing of nat-
ural language as (a) we provide a simple
algorithm to compute tree kernels in linear
average running time and (b) our study on
the classification properties of diverse tree
kernels show that kernel combinations al-
ways improve the traditional methods. Ex-
periments with Support Vector Machines
on the predicate argument classification
task provide empirical support to our the-
sis.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999953226415095">
In recent years tree kernels have been shown to
be interesting approaches for the modeling of syn-
tactic information in natural language tasks, e.g.
syntactic parsing (Collins and Duffy, 2002), rela-
tion extraction (Zelenko et al., 2003), Named En-
tity recognition (Cumby and Roth, 2003; Culotta
and Sorensen, 2004) and Semantic Parsing (Mos-
chitti, 2004).
The main tree kernel advantage is the possibility
to generate a high number of syntactic features and
let the learning algorithm to select those most rel-
evant for a specific application. In contrast, their
major drawback are (a) the computational time
complexity which is superlinear in the number of
tree nodes and (b) the accuracy that they produce is
often lower than the one provided by linear models
on manually designed features.
To solve problem (a), a linear complexity al-
gorithm for the subtree (ST) kernel computation,
was designed in (Vishwanathan and Smola, 2002).
Unfortunately, the ST set is rather poorer than the
one generated by the subset tree (SST) kernel de-
signed in (Collins and Duffy, 2002). Intuitively,
an ST rooted in a node n of the target tree always
contains all n’s descendants until the leaves. This
does not hold for the SSTs whose leaves can be
internal nodes.
To solve the problem (b), a study on different
tree substructure spaces should be carried out to
derive the tree kernel that provide the highest ac-
curacy. On the one hand, SSTs provide learn-
ing algorithms with richer information which may
be critical to capture syntactic properties of parse
trees as shown, for example, in (Zelenko et al.,
2003; Moschitti, 2004). On the other hand, if the
SST space contains too many irrelevant features,
overfitting may occur and decrease the classifica-
tion accuracy (Cumby and Roth, 2003). As a con-
sequence, the fewer features of the ST approach
may be more appropriate.
In this paper, we aim to solve the above prob-
lems. We present (a) an algorithm for the eval-
uation of the ST and SST kernels which runs in
linear average time and (b) a study of the impact
of diverse tree kernels on the accuracy of Support
Vector Machines (SVMs).
Our fast algorithm computes the kernels be-
tween two syntactic parse trees in O(m + n) av-
erage time, where m and n are the number of
nodes in the two trees. This low complexity al-
lows SVMs to carry out experiments on hundreds
of thousands of training instances since it is not
higher than the complexity of the polynomial ker-
</bodyText>
<page confidence="0.998481">
113
</page>
<bodyText confidence="0.99380846875">
nel, widely used on large experimentation e.g.
(Pradhan et al., 2004). To confirm such hypothe-
sis, we measured the impact of the algorithm on
the time required by SVMs for the learning of
about 122,774 predicate argument examples anno-
tated in PropBank (Kingsbury and Palmer, 2002)
and 37,948 instances annotated in FrameNet (Fill-
more, 1982).
Regarding the classification properties, we stud-
ied the argument labeling accuracy of ST and SST
kernels and their combinations with the standard
features (Gildea and Jurafsky, 2002). The re-
sults show that, on both PropBank and FrameNet
datasets, the SST-based kernel, i.e. the richest
in terms of substructures, produces the highest
SVM accuracy. When SSTs are combined with the
manual designed features, we always obtain the
best figure classifier. This suggests that the many
fragments included in the SST space are relevant
and, since their manual design may be problem-
atic (requiring a higher programming effort and
deeper knowledge of the linguistic phenomenon),
tree kernels provide a remarkable help in feature
engineering.
In the remainder of this paper, Section 2 de-
scribes the parse tree kernels and our fast algo-
rithm. Section 3 introduces the predicate argument
classification problem and its solution. Section 4
shows the comparative performance in term of the
execution time and accuracy. Finally, Section 5
discusses the related work whereas Section 6 sum-
marizes the conclusions.
</bodyText>
<sectionHeader confidence="0.985443" genericHeader="method">
2 Fast Parse Tree Kernels
</sectionHeader>
<bodyText confidence="0.999511">
The kernels that we consider represent trees in
terms of their substructures (fragments). These
latter define feature spaces which, in turn, are
mapped into vector spaces, e.g. R. The asso-
ciated kernel function measures the similarity be-
tween two trees by counting the number of their
common fragments. More precisely, a kernel func-
tion detects if a tree subpart (common to both
trees) belongs to the feature space that we intend
to generate. For such purpose, the fragment types
need to be described. We consider two important
characterizations: the subtrees (STs) and the sub-
set trees (SSTs).
</bodyText>
<subsectionHeader confidence="0.949638">
2.1 Subtrees and Subset Trees
</subsectionHeader>
<bodyText confidence="0.9930876">
In our study, we consider syntactic parse trees,
consequently, each node with its children is asso-
ciated with a grammar production rule, where the
symbol at left-hand side corresponds to the parent
node and the symbols at right-hand side are asso-
ciated with its children. The terminal symbols of
the grammar are always associated with the leaves
of the tree. For example, Figure 1 illustrates the
syntactic parse of the sentence &amp;quot;Mary brought a
cat to school&amp;quot;.
</bodyText>
<figureCaption confidence="0.997615">
Figure 1: A syntactic parse tree.
</figureCaption>
<bodyText confidence="0.999893416666667">
We define as a subtree (ST) any node of a tree
along with all its descendants. For example, the
line in Figure 1 circles the subtree rooted in the NP
node. A subset tree (SST) is a more general struc-
ture. The difference with the subtrees is that the
leaves can be associated with non-terminal sym-
bols. The SSTs satisfy the constraint that they are
generated by applying the same grammatical rule
set which generated the original tree. For exam-
ple, [S [N VP]] is a SST of the tree in Figure
1 which has two non-terminal symbols, N and VP,
as leaves.
</bodyText>
<figureCaption confidence="0.9999545">
Figure 2: A syntactic parse tree with its subtrees (STs).
Figure 3: A tree with some of its subset trees (SSTs).
</figureCaption>
<bodyText confidence="0.998550666666667">
Given a syntactic tree we can use as feature rep-
resentation the set of all its STs or SSTs. For ex-
ample, Figure 2 shows the parse tree of the sen-
tence &amp;quot;Mary brought a cat&amp;quot; together with its 6
STs, whereas Figure 3 shows 10 SSTs (out of
17) of the subtree of Figure 2 rooted in VP. The
</bodyText>
<figure confidence="0.999122283783784">
VP
N
Mary
V
PP
NP
A leaf
A subtree
N
IN
brought
N → school
school
to
D N
a cat
VP → V NP PP
PP → IN N
S
The root
S → N VP
V
NP
NP
a cat
a cat
a cat
cat
a brought
Mary
VP
VP
S
N
V
NP
Mary
brought
D N
N D V N
D N
D N
brought
VP
NP
NP
V
brought
VP
NP
D N
VP
V
NP
D N
NP
D N
cat
N D V N
V
NP
D N
NP
D N
D N
a cat
a
a cat
a cat
D N
cat
a brought
...
Mary
</figure>
<page confidence="0.992443">
114
</page>
<bodyText confidence="0.999960666666667">
high different number of substructures gives an in-
tuitive quantification of the different information
level between the two tree-based representations.
</bodyText>
<subsectionHeader confidence="0.897952">
2.2 The Tree Kernel Functions
</subsectionHeader>
<bodyText confidence="0.999261454545454">
The main idea of tree kernels is to compute the
number of the common substructures between two
trees T1 and T2 without explicitly considering
the whole fragment space. For this purpose, we
slightly modified the kernel function proposed in
(Collins and Duffy, 2002) by introducing a param-
eter σ which enables the ST or the SST evaluation.
Given the set of fragments {f1, f2, ..I = F, we
defined the indicator function Ii(n) which is equal
1 if the target fi is rooted at node n and 0 other-
wise. We define
</bodyText>
<equation confidence="0.980420333333333">
E
K(T1,T2) =
n1ENT1
</equation>
<bodyText confidence="0.999604">
where NT1 and NT2 are the sets of the T1’s
and T2’s nodes, respectively and A(n1, n2) =
</bodyText>
<equation confidence="0.539146">
�|F|
i=1 Ii(n1)Ii(n2). This latter is equal to the
</equation>
<listItem confidence="0.910926272727273">
number of common fragments rooted in the n1 and
n2 nodes. We can compute A as follows:
1. if the productions at n1 and n2 are different
then A(n1, n2) = 0;
2. if the productions at n1 and n2 are the
same, and n1 and n2 have only leaf children
(i.e. they are pre-terminals symbols) then
A(n1, n2) = 1;
3. if the productions at n1 and n2 are the same,
and n1 and n2 are not pre-terminals then
nc(n1)
</listItem>
<equation confidence="0.996978">
A(n1, n2) = H (σ + A(cjn1, cjn2)) (2)
j=1
</equation>
<bodyText confidence="0.99427684">
where σ E 10, 11, nc(n1) is the number of the
children of n1 and cjn is the j-th child of the node
n. Note that, since the productions are the same,
nc(n1) = nc(n2).
When σ = 0, A(n1, n2) is equal 1 only if
bj A(cjn1, cjn2) = 1, i.e. all the productions as-
sociated with the children are identical. By recur-
sively applying this property, it follows that the
subtrees in n1 and n2 are identical. Thus, Eq. 1
evaluates the subtree (ST) kernel. When σ = 1,
A(n1, n2) evaluates the number of SSTs common
to n1 and n2 as proved in (Collins and Duffy,
2002).
Additionally, we study some variations of the
above kernels which include the leaves in the frag-
ment space. For this purpose, it is enough to add
the condition:
0. if n1 and n2 are leaves and their associated
symbols are equal then A(n1, n2) = 1,
to the recursive rule set for the A evaluation
(Zhang and Lee, 2003). We will refer to such ex-
tended kernels as ST+bow and SST+bow (bag-of-
words).
Moreover, we add the decay factor λ by modi-
fying steps (2) and (3) as follows1:
</bodyText>
<equation confidence="0.998113">
2. A(n1, n2) = λ,
3. A(n1,n2) = λr] nc(n1)
j=1 (σ + A(cj n1, cjn2)).
</equation>
<bodyText confidence="0.972176125">
The computational complexity of Eq. 1 is
O(INT1I x INT21). We will refer to this basic im-
plementation as the Quadratic Tree Kernel (QTK).
However, as observed in (Collins and Duffy, 2002)
this worst case is quite unlikely for the syntactic
trees of natural language sentences, thus, we can
design algorithms that run in linear time on aver-
age.
</bodyText>
<tableCaption confidence="0.9670695">
Table 1: Pseudo-code for fast evaluation of the node pair
sets used in the fast Tree Kernel.
</tableCaption>
<subsectionHeader confidence="0.992258">
2.3 A Fast Tree Kernel (FTK)
</subsectionHeader>
<bodyText confidence="0.9975576">
To compute the kernels defined in the previous
section, we sum the A function for each pair
(n1,n2)E NT1 x NT2 (Eq. 1). When the pro-
ductions associated with n1 and n2 are different,
we can avoid to evaluate A(n1, n2) since it is 0.
</bodyText>
<footnote confidence="0.6948125">
1To have a similarity score between 0 and 1, we also ap-
ply the normalization in the kernel space, i.e. K&apos;(T1, T2) =
</footnote>
<equation confidence="0.940627714285714">
K(T1,T2)
√K(T1,T1)×K(T2,T2).
E A(n1, n2) (1)
n2ENT2
function Evaluate Pair Set(Tree T1, T2) returns NODE PAIR SET;
LIST L1,L2;
NODE PAIR SET Np;
begin
L1 = T1.ordered list;
L2 = T2.ordered list; /*the lists were sorted at loading time*/
n1 = extract(L1); /*get the head element and*/
n2 = extract(L2); /*remove it from the list*/
while (n1 and n2 are not NULL)
if (production of(n1) &gt; production of(n2))
then n2 = extract(L2);
else if (production of(n1) &lt; production of(n2))
then n1 = extract(L1);
else
while (production of(n1) == production of(n2))
while (production of(n1) == production of(n2))
add((n1, n2), Np);
</equation>
<bodyText confidence="0.961273">
n2=get next elem(L2); /*get the head element
and move the pointer to the next element*/
</bodyText>
<equation confidence="0.921974857142857">
end
n1 = extract(L1);
reset(L2); /*set the pointer at the first element*/
end
end
return Np ;
end
</equation>
<page confidence="0.963028">
115
</page>
<figure confidence="0.999736552631579">
VP
N
S
Mary
Arg. 0
V
NP
PP
brought
D N
IN N
Predicate
a cat
Arg. 1 Arg. M
to
school
S
SArg0
N
VP
V
Mary
brought
VP
V
NP
brought
D N
SArg1
a cat
to
school
VP
PP
IN N
SArgM
V
brought
</figure>
<figureCaption confidence="0.999945">
Figure 4: Tree substructure space for predicate argument classification.
</figureCaption>
<bodyText confidence="0.807930294117647">
Thus, we look for a node pair set Np ={(n1, n2)E
NT1 x NT2 : p(n1) = p(n2)1, where p(n) returns
the production rule associated with n.
To efficiently build Np, we (i) extract the L1 and
L2 lists of the production rules from T1 and T2,
(ii) sort them in the alphanumeric order and (iii)
scan them to find the node pairs (n1, n2) such that
(p(n1) = p(n2)) E L1nL2. Step (iii) may require
only O(|NT1 |+ |NT2|) time, but, if p(n1) appears
r1 times in T1 and p(n2) is repeated r2 times in
T2, we need to consider r1 x r2 pairs. The formal
algorithm is given in Table 1.
Note that:
(a) The list sorting can be done only once at the
data preparation time (i.e. before training) in
O(|NT1 |x log(|NT1|)).
(b) The algorithm shows that the worst case oc-
curs when the parse trees are both generated us-
ing only one production rule, i.e. the two inter-
nal while cycles carry out |NT1|x|NT2 |iterations.
In contrast, two identical parse trees may generate
a linear number of non-null pairs if there are few
groups of nodes associated with the same produc-
tion rule.
(c) Such approach is perfectly compatible with the
dynamic programming algorithm which computes
A. In fact, the only difference with the original
approach is that the matrix entries corresponding
to pairs of different production rules are not con-
sidered. Since such entries contain null values
they do not affect the application of the original
dynamic programming. Moreover, the order of
the pair evaluation can be established at run time,
starting from the root nodes towards the children.
</bodyText>
<sectionHeader confidence="0.993716" genericHeader="method">
3 A Semantic Application of Parse Tree
Kernels
</sectionHeader>
<bodyText confidence="0.971386653061225">
An interesting application of the SST kernel is
the classification of the predicate argument struc-
tures defined in PropBank (Kingsbury and Palmer,
2002) or FrameNet (Fillmore, 1982). Figure
4 shows the parse tree of the sentence: &amp;quot;Mary
brought a cat to school&amp;quot; along with the pred-
icate argument annotation proposed in the Prop-
Bank project. Only verbs are considered as pred-
icates whereas arguments are labeled sequentially
from ARG0 to ARG9.
Also in FrameNet predicate/argument informa-
tion is described but for this purpose richer seman-
tic structures called Frames are used. The Frames
are schematic representations of situations involv-
ing various participants, properties and roles in
which a word may be typically used. Frame el-
ements or semantic roles are arguments of pred-
icates called target words. For example the fol-
lowing sentence is annotated according to the AR-
REST frame:
[Time One Saturday night] [ Authorities police
in Brooklyn ] [Target apprehended ] [Suspect
sixteen teenagers].
The roles Suspect and Authorities are specific to
the frame.
The common approach to learn the classifica-
tion of predicate arguments relates to the extrac-
tion of features from the syntactic parse tree of
the target sentence. In (Gildea and Jurafsky, 2002)
seven different features2, which aim to capture the
relation between the predicate and its arguments,
were proposed. For example, the Parse Tree Path
of the pair (brought, ARG1) in the syntactic tree
of Figure 4 is V T VP 1 NP. It encodes the depen-
dency between the predicate and the argument as a
sequence of nonterminal labels linked by direction
symbols (up or down).
An alternative tree kernel representation, pro-
posed in (Moschitti, 2004), is the selection of the
minimal tree subset that includes a predicate with
only one of its arguments. For example, in Figure
4, the substructures inside the three frames are the
semantic/syntactic structures associated with the
three arguments of the verb to bring, i.e. SARG0,
SARG1 and SARGM.
Given a feature representation of predicate ar-
2Namely, they are Phrase Type, Parse Tree Path, Pred-
icate Word, Head Word, Governing Category, Position and
Voice.
</bodyText>
<page confidence="0.998157">
116
</page>
<bodyText confidence="0.99884964516129">
guments, we can build an individual ONE-vs-ALL
(OVA) classifier Ci for each argument i. As a fi-
nal decision of the multiclassifier, we select the ar-
gument type ARGt associated with the maximum
value among the scores provided by the Ci, i.e.
t = argmaxi∈S score(Ci), where S is the set
of argument types. We adopted the OVA approach
as it is simple and effective as showed in (Pradhan
et al., 2004).
Note that the representation in Figure 4 is quite
intuitive and, to conceive it, the designer requires
much less linguistic knowledge about semantic
roles than those necessary to define relevant fea-
tures manually. To understand such point, we
should make a step back before Gildea and Juraf-
sky defined the first set of features for Semantic
Role Labeling (SRL). The idea that syntax may
have been useful to derive semantic information
was already inspired by linguists, but from a ma-
chine learning point of view, to decide which tree
fragments may have been useful for semantic role
labeling was not an easy task. In principle, the de-
signer should have had to select and experiment
all possible tree subparts. This is exactly what the
tree kernels can automatically do: the designer just
need to roughly select the interesting whole sub-
tree (correlated with the linguistic phenomenon)
and the tree kernel will generate all possible syn-
tactic features from it. The task of selecting the
most relevant substructures is carried out by the
kernel machines themselves.
</bodyText>
<sectionHeader confidence="0.991139" genericHeader="method">
4 The Experiments
</sectionHeader>
<bodyText confidence="0.9995875">
The aim of the experiments is twofold. On the one
hand, we show that the FTK running time is linear
on the average case and is much faster than QTK.
This is accomplished by measuring the learning
time and the average kernel computation time. On
the other hand, we study the impact of the differ-
ent tree based kernels on the predicate argument
classification accuracy.
</bodyText>
<subsectionHeader confidence="0.903177">
4.1 Experimental Set-up
</subsectionHeader>
<bodyText confidence="0.99987758490566">
We used two different corpora: PropBank
(www.cis.upenn.edu/∼ace) along with Pen-
nTree bank 2 (Marcus et al., 1993) and FrameNet.
PropBank contains about 53,700 sentences and
a fixed split between training and testing which has
been used in other researches, e.g. (Gildea and
Palmer, 2002; Pradhan et al., 2004). In this split,
sections from 02 to 21 are used for training, sec-
tion 23 for testing and sections 1 and 22 as devel-
oping set. We considered a total of 122,774 and
7,359 arguments (from ARG0 to ARG9, ARGA
and ARGM) in training and testing, respectively.
Their tree structures were extracted from the Penn
Treebank. It should be noted that the main contri-
bution to the global accuracy is given by ARG0,
ARG1 and ARGM.
From the FrameNet corpus (http://www.icsi
.berkeley.edu/∼framenet), we extracted all
24,558 sentences of the 40 Frames selected for
the Automatic Labeling of Semantic Roles task of
Senseval 3 (www.senseval.org). We mapped to-
gether the semantic roles having the same name
and we considered only the 18 most frequent roles
associated with verbal predicates, for a total of
37,948 arguments. We randomly selected 30% of
sentences for testing and 70% for training. Addi-
tionally, 30% of training was used as a validation-
set. Note that, since the FrameNet data does not
include deep syntactic tree annotation, we pro-
cessed the FrameNet data with Collins’ parser
(Collins, 1997), consequently, the experiments on
FrameNet relate to automatic syntactic parse trees.
The classifier evaluations were carried out
with the SVM-light-TK software available at
http://ai-nlp.info.uniroma2.it/moschitti/
which encodes ST and SST kernels in the SVM-
light software (Joachims, 1999). We used the
default linear (Linear) and polynomial (Poly)
kernels for the evaluations with the standard
features defined in (Gildea and Jurafsky, 2002).
We adopted the default regularization parameter
(i.e., the average of 1/||x||) and we tried a few
cost-factor values (i.e., j E 11, 3, 7, 10, 30,100})
to adjust the rate between Precision and Recall on
the validation-set.
For the ST and SST kernels, we derived that the
best A (see Section 2.2) were 1 and 0.4, respec-
tively. The classification performance was eval-
uated using the F1 measure3 for the single argu-
ments and the accuracy for the final multiclassi-
fier. This latter choice allows us to compare our
results with previous literature work, e.g. (Gildea
and Jurafsky, 2002; Pradhan et al., 2004).
</bodyText>
<subsectionHeader confidence="0.961642">
4.2 Time Complexity Experiments
</subsectionHeader>
<bodyText confidence="0.9983596">
In this section we compare our Fast Tree Kernel
(FTK) approach with the Quadratic Tree Kernel
(QTK) algorithm. The latter refers to the naive
evaluation of Eq. 1 as presented in (Collins and
Duffy, 2002).
</bodyText>
<subsectionHeader confidence="0.770797">
3F1 assigns equal importance to Precision P and Recall
</subsectionHeader>
<bodyText confidence="0.77366">
R, i.e. f1 = 2P×
</bodyText>
<note confidence="0.381803">
P+R .
</note>
<page confidence="0.994614">
117
</page>
<bodyText confidence="0.999359">
Figure 5 shows the learning time4 of the SVMs
using QTK and FTK (over the SST structures)
for the classification of one large argument (i.e.
ARG0), according to different percentages of
training data. We note that, with 70% of the train-
ing data, FTK is about 10 times faster than QTK.
With all the training data FTK terminated in 6
hours whereas QTK required more than 1 week.
</bodyText>
<figure confidence="0.967041">
0 10 20 30 40 50 60 70 80 90 100
% Training Data
</figure>
<figureCaption confidence="0.991515">
Figure 5: ARG0 classifier learning time according to dif-
ferent training percentages.
Figure 6: Average time in seconds for the QTK and FTK
evaluations.
</figureCaption>
<figure confidence="0.990331">
0 10 20 30 40 50 60 70 80 90 100
% Training Data
</figure>
<figureCaption confidence="0.964962">
Figure 7: Multiclassifier accuracy according to different
training set percentages.
</figureCaption>
<subsectionHeader confidence="0.3444415">
4We run the experiments on a Pentium 4, 2GHz, with 1
Gb ram.
</subsectionHeader>
<bodyText confidence="0.999713190476191">
The above results are quite interesting because
they show that (1) we can use tree kernels with
SVMs on huge training sets, e.g. on 122,774 in-
stances and (2) the time needed to converge is ap-
proximately the one required by SVMs when us-
ing polynomial kernel. This latter shows the mini-
mal complexity needed to work in the dual space.
To study the FTK running time, we extracted
from PennTree bank the first 500 trees5 containing
exactly n nodes, then, we evaluated all 25,000 pos-
sible tree pairs. Each point of the Figure 6 shows
the average computation time on all the tree pairs
of a fixed size n.
In the figures, the trend lines which best inter-
polates the experimental values are also shown. It
clearly appears that the training time is quadratic
as SVMs have quadratic learning time complexity
(see Figure 5) whereas the FTK running time has
a linear behavior (Figure 6). The QTK algorithm
shows a quadratic running time complexity, as ex-
pected.
</bodyText>
<subsectionHeader confidence="0.95775">
4.3 Accuracy of the Tree Kernels
</subsectionHeader>
<bodyText confidence="0.999903909090909">
In these experiments, we investigate which ker-
nel is the most accurate for the predicate argument
classification.
First, we run ST, SST, ST+bow, SST+bow, Lin-
ear and Poly kernels over different training-set size
of PropBank. Figure 7 shows the learning curves
associated with the above kernels for the SVM-
based multiclassifier. We note that (a) SSTs have
a higher accuracy than STs, (b) bow does not im-
prove either ST or SST kernels and (c) in the fi-
nal part of the plot SST shows a higher gradient
than ST, Linear and Poly. This latter produces
the best accuracy 90.5% in line with the litera-
ture findings using standard features and polyno-
mial SVMs, e.g. 87.1%6 in (Pradhan et al., 2004).
Second, in tables 2 and 3, we report the results
using all available training data, on PropBank and
FrameNet test sets, respectively. Each row of the
two tables shows the F1 measure of the individ-
ual classifiers using different kernels whereas the
last column illustrates the global accuracy of the
multiclassifier.
</bodyText>
<footnote confidence="0.612255125">
5We measured also the computation time for the incom-
plete trees associated with the predicate argument structures
(see Section 3); we obtained the same results.
6The small difference (2.4%) is mainly due to the differ-
ent treatment of ARGMs: we built a single ARGM class for
all subclasses, e.g. ARGM-LOC and ARGM-TMP, whereas
in (Pradhan et al., 2004), the ARGMs, were evaluated sepa-
rately.
</footnote>
<figure confidence="0.998168476190476">
Hours
35
30
25
20
15
10
5
0
y = 0.0006x 2 - 0.001x
y = 0.0045x 2 + 0.1004x
FTK
QTK
120
100
80
y = 0.04x2 - 0.05x
60
FTK
QTK
20
y = 0.14x
0
10 15 20 25 30 35 40 45 50 55 60
Number of Tree Nodes
µseconds
40
Accuracy
0.90
0.88
0.86
0.84
0.82
0.80
0.78
0.76
ST
SST
ST+bow
SST+bow
Linear
Poly
</figure>
<page confidence="0.989379">
118
</page>
<bodyText confidence="0.999894285714286">
We note that, the F1 of the single arguments
across the different kernels follows the same be-
havior of the global multiclassifier accuracy. On
FrameNet, the bow impact on the ST and SST
accuracy is higher than on PropBank as it pro-
duces an improvement of about 1.5%. This sug-
gests that (1) to detect semantic roles, lexical in-
formation is very important, (2) bow give a higher
contribution as errors in POS-tagging make the
word + POS fragments less reliable and (3) as the
FrameNet trees are obtained with the Collins’ syn-
tactic parser, tree kernels seem robust to incorrect
parse trees.
Third, we point out that the polynomial ker-
nel on flat features is more accurate than tree ker-
nels but the design of such effective features re-
quired noticeable knowledge and effort (Gildea
and Jurafsky, 2002). On the contrary, the choice
of subtrees suitable to syntactically characterize a
target phenomenon seems a easier task (see Sec-
tion 3 for the predicate argument case). More-
over, by combining polynomial and SST kernels,
we can improve the classification accuracy (Mos-
chitti, 2004), i.e. tree kernels provide the learn-
ing algorithm with many relevant fragments which
hardly can be designed by hand. In fact, as many
predicate argument structures are quite large (up
to 100 nodes) they contain many fragments.
</bodyText>
<table confidence="0.999018125">
ARGs ST SST ST+bow SST+bow Linear Poly
ARG0 86.5 88.0 86.9 88.4 88.6 90.6
ARG1 83.1 87.4 82.8 86.7 85.9 90.8
ARG2 58.0 67.6 58.9 66.7 65.5 80.4
ARG3 35.7 37.5 39.3 41.2 51.9 60.4
ARG4 62.7 65.6 63.3 63.9 66.2 70.0
ARGM 92.0 94.2 92.0 93.7 94.9 95.3
Acc. 84.6 87.7 84.8 87.5 87.6 90.7
</table>
<tableCaption confidence="0.99402">
Table 2: Evaluation of Kernels on PropBank.
</tableCaption>
<table confidence="0.999623636363636">
Roles ST SST ST+bow SST+bow Linear Poly
agent 86.9 87.8 89.2 90.2 89.8 91.7
theme 76.1 79.2 78.5 80.7 82.9 90.4
goal 77.9 78.9 78.2 80.1 80.2 85.8
path 82.8 84.4 83.7 85.1 81.3 85.5
manner 79.9 82.0 81.3 82.5 70.8 80.5
source 85.6 87.7 86.9 87.8 86.5 89.8
time 76.3 78.3 77.0 79.1 61.8 68.3
reason 75.9 77.3 78.9 81.4 82.9 86.4
Acc. 80.0 81.2 81.3 82.9 82.3 85.6
18 roles
</table>
<tableCaption confidence="0.9844735">
Table 3: Evaluation of the Kernels on FrameNet semantic
roles.
</tableCaption>
<bodyText confidence="0.982086">
Finally, to study the combined kernels, we ap-
plied the K1 + γK2 formula, where K1 is either
the Linear or the Poly kernel and K2 is the ST
</bodyText>
<table confidence="0.999079333333333">
Corpus Poly ST+Linear SST+Linear ST+Poly SST+Poly
PropBank 90.7 88.6 89.4 91.1 91.3
FrameNet 85.6 85.3 85.8 87.5 87.2
</table>
<tableCaption confidence="0.9843115">
Table 4: Multiclassifier accuracy using Kernel Combina-
tions.
</tableCaption>
<bodyText confidence="0.999740642857143">
or the SST kernel. Table 4 shows the results of
four kernel combinations. We note that, (a) STs
and SSTs improve Poly (about 0.5 and 2 percent
points on PropBank and FrameNet, respectively)
and (b) the linear kernel, which uses fewer fea-
tures than Poly, is more enhanced by the SSTs than
STs (for example on PropBank we have 89.4% and
88.6% vs. 87.6%), i.e. Linear takes advantage by
the richer feature set of the SSTs. It should be
noted that our results of kernel combinations on
FrameNet are in contrast with (Moschitti, 2004),
where no improvement was obtained. Our expla-
nation is that, thanks to the fast evaluation of FTK,
we could carry out an adequate parameterization.
</bodyText>
<sectionHeader confidence="0.999954" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999947225806452">
Recently, several tree kernels have been designed.
In the following, we highlight their differences and
properties.
In (Collins and Duffy, 2002), the SST tree ker-
nel was experimented with the Voted Perceptron
for the parse-tree reranking task. The combination
with the original PCFG model improved the syn-
tactic parsing. Additionally, it was alluded that the
average execution time depends on the number of
repeated productions.
In (Vishwanathan and Smola, 2002), a linear
complexity algorithm for the computation of the
ST kernel is provided (in the worst case). The
main idea is the use of the suffix trees to store par-
tial matches for the evaluation of the string kernel
(Lodhi et al., 2000). This can be used to compute
the ST fragments once the tree is converted into a
string. To our knowledge, ours is the first applica-
tion of the ST kernel for a natural language task.
In (Kazama and Torisawa, 2005), an interesting
algorithm that speeds up the average running time
is presented. Such algorithm looks for node pairs
that have in common a large number of trees (ma-
licious nodes) and applies a transformation to the
trees rooted in such nodes to make faster the kernel
computation. The results show an increase of the
speed similar to the one produced by our method.
In (Zelenko et al., 2003), two kernels over syn-
tactic shallow parser structures were devised for
the extraction of linguistic relations, e.g. person-
affiliation. To measure the similarity between two
</bodyText>
<page confidence="0.996971">
119
</page>
<bodyText confidence="0.99996775862069">
nodes, the contiguous string kernel and the sparse
string kernel (Lodhi et al., 2000) were used. In
(Culotta and Sorensen, 2004) such kernels were
slightly generalized by providing a matching func-
tion for the node pairs. The time complexity for
their computation limited the experiments on data
set of just 200 news items. Moreover, we note that
the above tree kernels are not convolution kernels
as those proposed in this article.
In (Shen et al., 2003), a tree-kernel based on
Lexicalized Tree Adjoining Grammar (LTAG) for
the parse-reranking task was proposed. Since
QTK was used for the kernel computation, the
high learning complexity forced the authors to
train different SVMs on different slices of train-
ing data. Our FTK, adapted for the LTAG tree ker-
nel, would have allowed SVMs to be trained on
the whole data.
In (Cumby and Roth, 2003), a feature descrip-
tion language was used to extract structural fea-
tures from the syntactic shallow parse trees asso-
ciated with named entities. The experiments on
the named entity categorization showed that when
the description language selects an adequate set of
tree fragments the Voted Perceptron algorithm in-
creases its classification accuracy. The explana-
tion was that the complete tree fragment set con-
tains many irrelevant features and may cause over-
fitting.
</bodyText>
<sectionHeader confidence="0.999709" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999954263157895">
In this paper, we have shown that tree kernels
can effectively be adopted in practical natural lan-
guage applications. The main arguments against
their use are their efficiency and accuracy lower
than traditional feature based approaches. We
have shown that a fast algorithm (FTK) can evalu-
ate tree kernels in a linear average running time
and also that the overall converging time re-
quired by SVMs is compatible with very large
data sets. Regarding the accuracy, the experiments
with Support Vector Machines on the PropBank
and FrameNet predicate argument structures show
that: (a) the richer the kernel is in term of substruc-
tures (e.g. SST), the higher the accuracy is, (b)
tree kernels are effective also in case of automatic
parse trees and (c) as kernel combinations always
improve traditional feature models, the best ap-
proach is to combine scalar-based and structured
based kernels.
</bodyText>
<sectionHeader confidence="0.99725" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9513314">
I would like to thank the AI group at the University of Rome
”Tor Vergata”. Many thanks to the EACL 2006 anonymous
reviewers, Roberto Basili and Giorgio Satta who provided
me with valuable suggestions. This research is partially sup-
ported by the Presto Space EU Project#: FP6-507336.
</bodyText>
<sectionHeader confidence="0.984029" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999480543859649">
Michael Collins and Nigel Duffy. 2002. New ranking al-
gorithms for parsing and tagging: Kernels over discrete
structures, and the voted perceptron. In ACL02.
Michael Collins. 1997. Three generative, lexicalized mod-
els for statistical parsing. In proceedings of the ACL97,
Madrid, Spain.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree
kernels for relation extraction. In proceedings of ACL04,
Barcelona, Spain.
Chad Cumby and Dan Roth. 2003. Kernel methods for rela-
tional learning. In proceedings of ICML 2003. Washing-
ton, US.
Charles J. Fillmore. 1982. Frame semantics. In Linguistics
in the Morning Calm.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguistic,
28(3):496–530.
Daniel Gildea and Martha Palmer. 2002. The necessity of
parsing for predicate argument recognition. In proceed-
ings of ACL02, Philadelphia, PA.
T. Joachims. 1999. Making large-scale SVM learning prac-
tical. In B. Sch¨olkopf, C. Burges, and A. Smola, editors,
Advances in Kernel Methods - Support Vector Learning.
Junichi Kazama and Kentaro Torisawa. 2005. Speeding up
training with tree kernels for node relation labeling. In
proceedings of EMNLP 2005, Toronto, Canada.
Paul Kingsbury and Martha Palmer. 2002. From Treebank to
PropBank. In proceedings of LREC-2002, Spain.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini, and Christopher Watkins. 2000. Text clas-
sification using string kernels. In NIPS02, Vancouver,
Canada.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of english: The Penn
Treebank. Computational Linguistics, 19:313–330.
Alessandro Moschitti. 2004. A study on convolution ker-
nels for shallow semantic parsing. In proceedings ACL04,
Barcelona, Spain.
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne
Ward, James H. Martin, and Daniel Jurafsky. 2005. Sup-
port vector learning for semantic argument classification.
Machine Learning Journal.
Libin Shen, Anoop Sarkar, and Aravind Joshi. 2003. Using
LTAG based features in parse reranking. In proceedings
of EMNLP 2003, Sapporo, Japan.
Ben Taskar, Dan Klein, Mike Collins, Daphne Koller, and
Christopher Manning. 2004. Max-margin parsing. In
proceedings of EMNLP 2004 Barcelona, Spain.
S.V.N. Vishwanathan and A.J. Smola. 2002. Fast kernels on
strings and trees. In proceedings of Neural Information
Processing Systems.
D. Zelenko, C. Aone, and A. Richardella. 2003. Ker-
nel methods for relation extraction. Journal of Machine
Learning Research.
Dell Zhang and Wee Sun Lee. 2003. Question classifica-
tion using support vector machines. In proceedings of SI-
GIR’03, ACM Press.
</reference>
<page confidence="0.996283">
120
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.869556">
<title confidence="0.999988">Making Tree Kernels practical for Natural Language Learning</title>
<author confidence="0.999941">Alessandro Moschitti</author>
<affiliation confidence="0.999545">Department of Computer Science University of Rome ”Tor Vergata”</affiliation>
<address confidence="0.913168">Rome, Italy</address>
<email confidence="0.995549">moschitti@info.uniroma2.it</email>
<abstract confidence="0.997726631578948">In recent years tree kernels have been proposed for the automatic learning of natural language applications. Unfortunately, they show (a) an inherent super linear complexity and (b) a lower accuracy than traditional attribute/value methods. In this paper, we show that tree kernels are very helpful in the processing of natural language as (a) we provide a simple algorithm to compute tree kernels in linear average running time and (b) our study on the classification properties of diverse tree kernels show that kernel combinations always improve the traditional methods. Experiments with Support Vector Machines on the predicate argument classification task provide empirical support to our thesis.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In ACL02.</booktitle>
<contexts>
<context position="1101" citStr="Collins and Duffy, 2002" startWordPosition="163" endWordPosition="166">e very helpful in the processing of natural language as (a) we provide a simple algorithm to compute tree kernels in linear average running time and (b) our study on the classification properties of diverse tree kernels show that kernel combinations always improve the traditional methods. Experiments with Support Vector Machines on the predicate argument classification task provide empirical support to our thesis. 1 Introduction In recent years tree kernels have been shown to be interesting approaches for the modeling of syntactic information in natural language tasks, e.g. syntactic parsing (Collins and Duffy, 2002), relation extraction (Zelenko et al., 2003), Named Entity recognition (Cumby and Roth, 2003; Culotta and Sorensen, 2004) and Semantic Parsing (Moschitti, 2004). The main tree kernel advantage is the possibility to generate a high number of syntactic features and let the learning algorithm to select those most relevant for a specific application. In contrast, their major drawback are (a) the computational time complexity which is superlinear in the number of tree nodes and (b) the accuracy that they produce is often lower than the one provided by linear models on manually designed features. To</context>
<context position="7686" citStr="Collins and Duffy, 2002" startWordPosition="1337" endWordPosition="1340"> V NP Mary brought D N N D V N D N D N brought VP NP NP V brought VP NP D N VP V NP D N NP D N cat N D V N V NP D N NP D N D N a cat a a cat a cat D N cat a brought ... Mary 114 high different number of substructures gives an intuitive quantification of the different information level between the two tree-based representations. 2.2 The Tree Kernel Functions The main idea of tree kernels is to compute the number of the common substructures between two trees T1 and T2 without explicitly considering the whole fragment space. For this purpose, we slightly modified the kernel function proposed in (Collins and Duffy, 2002) by introducing a parameter σ which enables the ST or the SST evaluation. Given the set of fragments {f1, f2, ..I = F, we defined the indicator function Ii(n) which is equal 1 if the target fi is rooted at node n and 0 otherwise. We define E K(T1,T2) = n1ENT1 where NT1 and NT2 are the sets of the T1’s and T2’s nodes, respectively and A(n1, n2) = �|F| i=1 Ii(n1)Ii(n2). This latter is equal to the number of common fragments rooted in the n1 and n2 nodes. We can compute A as follows: 1. if the productions at n1 and n2 are different then A(n1, n2) = 0; 2. if the productions at n1 and n2 are the sa</context>
<context position="9075" citStr="Collins and Duffy, 2002" startWordPosition="1618" endWordPosition="1621">e not pre-terminals then nc(n1) A(n1, n2) = H (σ + A(cjn1, cjn2)) (2) j=1 where σ E 10, 11, nc(n1) is the number of the children of n1 and cjn is the j-th child of the node n. Note that, since the productions are the same, nc(n1) = nc(n2). When σ = 0, A(n1, n2) is equal 1 only if bj A(cjn1, cjn2) = 1, i.e. all the productions associated with the children are identical. By recursively applying this property, it follows that the subtrees in n1 and n2 are identical. Thus, Eq. 1 evaluates the subtree (ST) kernel. When σ = 1, A(n1, n2) evaluates the number of SSTs common to n1 and n2 as proved in (Collins and Duffy, 2002). Additionally, we study some variations of the above kernels which include the leaves in the fragment space. For this purpose, it is enough to add the condition: 0. if n1 and n2 are leaves and their associated symbols are equal then A(n1, n2) = 1, to the recursive rule set for the A evaluation (Zhang and Lee, 2003). We will refer to such extended kernels as ST+bow and SST+bow (bag-ofwords). Moreover, we add the decay factor λ by modifying steps (2) and (3) as follows1: 2. A(n1, n2) = λ, 3. A(n1,n2) = λr] nc(n1) j=1 (σ + A(cj n1, cjn2)). The computational complexity of Eq. 1 is O(INT1I x INT21</context>
<context position="19793" citStr="Collins and Duffy, 2002" startWordPosition="3443" endWordPosition="3446">n-set. For the ST and SST kernels, we derived that the best A (see Section 2.2) were 1 and 0.4, respectively. The classification performance was evaluated using the F1 measure3 for the single arguments and the accuracy for the final multiclassifier. This latter choice allows us to compare our results with previous literature work, e.g. (Gildea and Jurafsky, 2002; Pradhan et al., 2004). 4.2 Time Complexity Experiments In this section we compare our Fast Tree Kernel (FTK) approach with the Quadratic Tree Kernel (QTK) algorithm. The latter refers to the naive evaluation of Eq. 1 as presented in (Collins and Duffy, 2002). 3F1 assigns equal importance to Precision P and Recall R, i.e. f1 = 2P× P+R . 117 Figure 5 shows the learning time4 of the SVMs using QTK and FTK (over the SST structures) for the classification of one large argument (i.e. ARG0), according to different percentages of training data. We note that, with 70% of the training data, FTK is about 10 times faster than QTK. With all the training data FTK terminated in 6 hours whereas QTK required more than 1 week. 0 10 20 30 40 50 60 70 80 90 100 % Training Data Figure 5: ARG0 classifier learning time according to different training percentages. Figur</context>
<context position="26546" citStr="Collins and Duffy, 2002" startWordPosition="4647" endWordPosition="4650">el, which uses fewer features than Poly, is more enhanced by the SSTs than STs (for example on PropBank we have 89.4% and 88.6% vs. 87.6%), i.e. Linear takes advantage by the richer feature set of the SSTs. It should be noted that our results of kernel combinations on FrameNet are in contrast with (Moschitti, 2004), where no improvement was obtained. Our explanation is that, thanks to the fast evaluation of FTK, we could carry out an adequate parameterization. 5 Related Work Recently, several tree kernels have been designed. In the following, we highlight their differences and properties. In (Collins and Duffy, 2002), the SST tree kernel was experimented with the Voted Perceptron for the parse-tree reranking task. The combination with the original PCFG model improved the syntactic parsing. Additionally, it was alluded that the average execution time depends on the number of repeated productions. In (Vishwanathan and Smola, 2002), a linear complexity algorithm for the computation of the ST kernel is provided (in the worst case). The main idea is the use of the suffix trees to store partial matches for the evaluation of the string kernel (Lodhi et al., 2000). This can be used to compute the ST fragments onc</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In ACL02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalized models for statistical parsing.</title>
<date>1997</date>
<booktitle>In proceedings of the ACL97,</booktitle>
<location>Madrid,</location>
<contexts>
<context position="18508" citStr="Collins, 1997" startWordPosition="3243" endWordPosition="3244">framenet), we extracted all 24,558 sentences of the 40 Frames selected for the Automatic Labeling of Semantic Roles task of Senseval 3 (www.senseval.org). We mapped together the semantic roles having the same name and we considered only the 18 most frequent roles associated with verbal predicates, for a total of 37,948 arguments. We randomly selected 30% of sentences for testing and 70% for training. Additionally, 30% of training was used as a validationset. Note that, since the FrameNet data does not include deep syntactic tree annotation, we processed the FrameNet data with Collins’ parser (Collins, 1997), consequently, the experiments on FrameNet relate to automatic syntactic parse trees. The classifier evaluations were carried out with the SVM-light-TK software available at http://ai-nlp.info.uniroma2.it/moschitti/ which encodes ST and SST kernels in the SVMlight software (Joachims, 1999). We used the default linear (Linear) and polynomial (Poly) kernels for the evaluations with the standard features defined in (Gildea and Jurafsky, 2002). We adopted the default regularization parameter (i.e., the average of 1/||x||) and we tried a few cost-factor values (i.e., j E 11, 3, 7, 10, 30,100}) to </context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalized models for statistical parsing. In proceedings of the ACL97, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In proceedings of ACL04,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="1222" citStr="Culotta and Sorensen, 2004" startWordPosition="182" endWordPosition="185">inear average running time and (b) our study on the classification properties of diverse tree kernels show that kernel combinations always improve the traditional methods. Experiments with Support Vector Machines on the predicate argument classification task provide empirical support to our thesis. 1 Introduction In recent years tree kernels have been shown to be interesting approaches for the modeling of syntactic information in natural language tasks, e.g. syntactic parsing (Collins and Duffy, 2002), relation extraction (Zelenko et al., 2003), Named Entity recognition (Cumby and Roth, 2003; Culotta and Sorensen, 2004) and Semantic Parsing (Moschitti, 2004). The main tree kernel advantage is the possibility to generate a high number of syntactic features and let the learning algorithm to select those most relevant for a specific application. In contrast, their major drawback are (a) the computational time complexity which is superlinear in the number of tree nodes and (b) the accuracy that they produce is often lower than the one provided by linear models on manually designed features. To solve problem (a), a linear complexity algorithm for the subtree (ST) kernel computation, was designed in (Vishwanathan </context>
<context position="28006" citStr="Culotta and Sorensen, 2004" startWordPosition="4894" endWordPosition="4897">presented. Such algorithm looks for node pairs that have in common a large number of trees (malicious nodes) and applies a transformation to the trees rooted in such nodes to make faster the kernel computation. The results show an increase of the speed similar to the one produced by our method. In (Zelenko et al., 2003), two kernels over syntactic shallow parser structures were devised for the extraction of linguistic relations, e.g. personaffiliation. To measure the similarity between two 119 nodes, the contiguous string kernel and the sparse string kernel (Lodhi et al., 2000) were used. In (Culotta and Sorensen, 2004) such kernels were slightly generalized by providing a matching function for the node pairs. The time complexity for their computation limited the experiments on data set of just 200 news items. Moreover, we note that the above tree kernels are not convolution kernels as those proposed in this article. In (Shen et al., 2003), a tree-kernel based on Lexicalized Tree Adjoining Grammar (LTAG) for the parse-reranking task was proposed. Since QTK was used for the kernel computation, the high learning complexity forced the authors to train different SVMs on different slices of training data. Our FTK</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In proceedings of ACL04, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chad Cumby</author>
<author>Dan Roth</author>
</authors>
<title>Kernel methods for relational learning.</title>
<date>2003</date>
<booktitle>In proceedings of ICML 2003.</booktitle>
<location>Washington, US.</location>
<contexts>
<context position="1193" citStr="Cumby and Roth, 2003" startWordPosition="178" endWordPosition="181">pute tree kernels in linear average running time and (b) our study on the classification properties of diverse tree kernels show that kernel combinations always improve the traditional methods. Experiments with Support Vector Machines on the predicate argument classification task provide empirical support to our thesis. 1 Introduction In recent years tree kernels have been shown to be interesting approaches for the modeling of syntactic information in natural language tasks, e.g. syntactic parsing (Collins and Duffy, 2002), relation extraction (Zelenko et al., 2003), Named Entity recognition (Cumby and Roth, 2003; Culotta and Sorensen, 2004) and Semantic Parsing (Moschitti, 2004). The main tree kernel advantage is the possibility to generate a high number of syntactic features and let the learning algorithm to select those most relevant for a specific application. In contrast, their major drawback are (a) the computational time complexity which is superlinear in the number of tree nodes and (b) the accuracy that they produce is often lower than the one provided by linear models on manually designed features. To solve problem (a), a linear complexity algorithm for the subtree (ST) kernel computation, w</context>
<context position="2681" citStr="Cumby and Roth, 2003" startWordPosition="426" endWordPosition="429">scendants until the leaves. This does not hold for the SSTs whose leaves can be internal nodes. To solve the problem (b), a study on different tree substructure spaces should be carried out to derive the tree kernel that provide the highest accuracy. On the one hand, SSTs provide learning algorithms with richer information which may be critical to capture syntactic properties of parse trees as shown, for example, in (Zelenko et al., 2003; Moschitti, 2004). On the other hand, if the SST space contains too many irrelevant features, overfitting may occur and decrease the classification accuracy (Cumby and Roth, 2003). As a consequence, the fewer features of the ST approach may be more appropriate. In this paper, we aim to solve the above problems. We present (a) an algorithm for the evaluation of the ST and SST kernels which runs in linear average time and (b) a study of the impact of diverse tree kernels on the accuracy of Support Vector Machines (SVMs). Our fast algorithm computes the kernels between two syntactic parse trees in O(m + n) average time, where m and n are the number of nodes in the two trees. This low complexity allows SVMs to carry out experiments on hundreds of thousands of training inst</context>
<context position="28724" citStr="Cumby and Roth, 2003" startWordPosition="5015" endWordPosition="5018">time complexity for their computation limited the experiments on data set of just 200 news items. Moreover, we note that the above tree kernels are not convolution kernels as those proposed in this article. In (Shen et al., 2003), a tree-kernel based on Lexicalized Tree Adjoining Grammar (LTAG) for the parse-reranking task was proposed. Since QTK was used for the kernel computation, the high learning complexity forced the authors to train different SVMs on different slices of training data. Our FTK, adapted for the LTAG tree kernel, would have allowed SVMs to be trained on the whole data. In (Cumby and Roth, 2003), a feature description language was used to extract structural features from the syntactic shallow parse trees associated with named entities. The experiments on the named entity categorization showed that when the description language selects an adequate set of tree fragments the Voted Perceptron algorithm increases its classification accuracy. The explanation was that the complete tree fragment set contains many irrelevant features and may cause overfitting. 6 Conclusions In this paper, we have shown that tree kernels can effectively be adopted in practical natural language applications. Th</context>
</contexts>
<marker>Cumby, Roth, 2003</marker>
<rawString>Chad Cumby and Dan Roth. 2003. Kernel methods for relational learning. In proceedings of ICML 2003. Washington, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
</authors>
<title>Frame semantics.</title>
<date>1982</date>
<booktitle>In Linguistics in the Morning Calm.</booktitle>
<contexts>
<context position="3695" citStr="Fillmore, 1982" startWordPosition="610" endWordPosition="612">yntactic parse trees in O(m + n) average time, where m and n are the number of nodes in the two trees. This low complexity allows SVMs to carry out experiments on hundreds of thousands of training instances since it is not higher than the complexity of the polynomial ker113 nel, widely used on large experimentation e.g. (Pradhan et al., 2004). To confirm such hypothesis, we measured the impact of the algorithm on the time required by SVMs for the learning of about 122,774 predicate argument examples annotated in PropBank (Kingsbury and Palmer, 2002) and 37,948 instances annotated in FrameNet (Fillmore, 1982). Regarding the classification properties, we studied the argument labeling accuracy of ST and SST kernels and their combinations with the standard features (Gildea and Jurafsky, 2002). The results show that, on both PropBank and FrameNet datasets, the SST-based kernel, i.e. the richest in terms of substructures, produces the highest SVM accuracy. When SSTs are combined with the manual designed features, we always obtain the best figure classifier. This suggests that the many fragments included in the SST space are relevant and, since their manual design may be problematic (requiring a higher </context>
<context position="13264" citStr="Fillmore, 1982" startWordPosition="2375" endWordPosition="2376">t, the only difference with the original approach is that the matrix entries corresponding to pairs of different production rules are not considered. Since such entries contain null values they do not affect the application of the original dynamic programming. Moreover, the order of the pair evaluation can be established at run time, starting from the root nodes towards the children. 3 A Semantic Application of Parse Tree Kernels An interesting application of the SST kernel is the classification of the predicate argument structures defined in PropBank (Kingsbury and Palmer, 2002) or FrameNet (Fillmore, 1982). Figure 4 shows the parse tree of the sentence: &amp;quot;Mary brought a cat to school&amp;quot; along with the predicate argument annotation proposed in the PropBank project. Only verbs are considered as predicates whereas arguments are labeled sequentially from ARG0 to ARG9. Also in FrameNet predicate/argument information is described but for this purpose richer semantic structures called Frames are used. The Frames are schematic representations of situations involving various participants, properties and roles in which a word may be typically used. Frame elements or semantic roles are arguments of predicate</context>
</contexts>
<marker>Fillmore, 1982</marker>
<rawString>Charles J. Fillmore. 1982. Frame semantics. In Linguistics in the Morning Calm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistic,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="3879" citStr="Gildea and Jurafsky, 2002" startWordPosition="636" endWordPosition="639">f thousands of training instances since it is not higher than the complexity of the polynomial ker113 nel, widely used on large experimentation e.g. (Pradhan et al., 2004). To confirm such hypothesis, we measured the impact of the algorithm on the time required by SVMs for the learning of about 122,774 predicate argument examples annotated in PropBank (Kingsbury and Palmer, 2002) and 37,948 instances annotated in FrameNet (Fillmore, 1982). Regarding the classification properties, we studied the argument labeling accuracy of ST and SST kernels and their combinations with the standard features (Gildea and Jurafsky, 2002). The results show that, on both PropBank and FrameNet datasets, the SST-based kernel, i.e. the richest in terms of substructures, produces the highest SVM accuracy. When SSTs are combined with the manual designed features, we always obtain the best figure classifier. This suggests that the many fragments included in the SST space are relevant and, since their manual design may be problematic (requiring a higher programming effort and deeper knowledge of the linguistic phenomenon), tree kernels provide a remarkable help in feature engineering. In the remainder of this paper, Section 2 describe</context>
<context position="14332" citStr="Gildea and Jurafsky, 2002" startWordPosition="2545" endWordPosition="2548">ations involving various participants, properties and roles in which a word may be typically used. Frame elements or semantic roles are arguments of predicates called target words. For example the following sentence is annotated according to the ARREST frame: [Time One Saturday night] [ Authorities police in Brooklyn ] [Target apprehended ] [Suspect sixteen teenagers]. The roles Suspect and Authorities are specific to the frame. The common approach to learn the classification of predicate arguments relates to the extraction of features from the syntactic parse tree of the target sentence. In (Gildea and Jurafsky, 2002) seven different features2, which aim to capture the relation between the predicate and its arguments, were proposed. For example, the Parse Tree Path of the pair (brought, ARG1) in the syntactic tree of Figure 4 is V T VP 1 NP. It encodes the dependency between the predicate and the argument as a sequence of nonterminal labels linked by direction symbols (up or down). An alternative tree kernel representation, proposed in (Moschitti, 2004), is the selection of the minimal tree subset that includes a predicate with only one of its arguments. For example, in Figure 4, the substructures inside t</context>
<context position="18952" citStr="Gildea and Jurafsky, 2002" startWordPosition="3301" endWordPosition="3304">g was used as a validationset. Note that, since the FrameNet data does not include deep syntactic tree annotation, we processed the FrameNet data with Collins’ parser (Collins, 1997), consequently, the experiments on FrameNet relate to automatic syntactic parse trees. The classifier evaluations were carried out with the SVM-light-TK software available at http://ai-nlp.info.uniroma2.it/moschitti/ which encodes ST and SST kernels in the SVMlight software (Joachims, 1999). We used the default linear (Linear) and polynomial (Poly) kernels for the evaluations with the standard features defined in (Gildea and Jurafsky, 2002). We adopted the default regularization parameter (i.e., the average of 1/||x||) and we tried a few cost-factor values (i.e., j E 11, 3, 7, 10, 30,100}) to adjust the rate between Precision and Recall on the validation-set. For the ST and SST kernels, we derived that the best A (see Section 2.2) were 1 and 0.4, respectively. The classification performance was evaluated using the F1 measure3 for the single arguments and the accuracy for the final multiclassifier. This latter choice allows us to compare our results with previous literature work, e.g. (Gildea and Jurafsky, 2002; Pradhan et al., 2</context>
<context position="24119" citStr="Gildea and Jurafsky, 2002" startWordPosition="4224" endWordPosition="4227">he ST and SST accuracy is higher than on PropBank as it produces an improvement of about 1.5%. This suggests that (1) to detect semantic roles, lexical information is very important, (2) bow give a higher contribution as errors in POS-tagging make the word + POS fragments less reliable and (3) as the FrameNet trees are obtained with the Collins’ syntactic parser, tree kernels seem robust to incorrect parse trees. Third, we point out that the polynomial kernel on flat features is more accurate than tree kernels but the design of such effective features required noticeable knowledge and effort (Gildea and Jurafsky, 2002). On the contrary, the choice of subtrees suitable to syntactically characterize a target phenomenon seems a easier task (see Section 3 for the predicate argument case). Moreover, by combining polynomial and SST kernels, we can improve the classification accuracy (Moschitti, 2004), i.e. tree kernels provide the learning algorithm with many relevant fragments which hardly can be designed by hand. In fact, as many predicate argument structures are quite large (up to 100 nodes) they contain many fragments. ARGs ST SST ST+bow SST+bow Linear Poly ARG0 86.5 88.0 86.9 88.4 88.6 90.6 ARG1 83.1 87.4 82</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistic, 28(3):496–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Martha Palmer</author>
</authors>
<title>The necessity of parsing for predicate argument recognition.</title>
<date>2002</date>
<booktitle>In proceedings of ACL02,</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="17397" citStr="Gildea and Palmer, 2002" startWordPosition="3056" endWordPosition="3059"> we show that the FTK running time is linear on the average case and is much faster than QTK. This is accomplished by measuring the learning time and the average kernel computation time. On the other hand, we study the impact of the different tree based kernels on the predicate argument classification accuracy. 4.1 Experimental Set-up We used two different corpora: PropBank (www.cis.upenn.edu/∼ace) along with PennTree bank 2 (Marcus et al., 1993) and FrameNet. PropBank contains about 53,700 sentences and a fixed split between training and testing which has been used in other researches, e.g. (Gildea and Palmer, 2002; Pradhan et al., 2004). In this split, sections from 02 to 21 are used for training, section 23 for testing and sections 1 and 22 as developing set. We considered a total of 122,774 and 7,359 arguments (from ARG0 to ARG9, ARGA and ARGM) in training and testing, respectively. Their tree structures were extracted from the Penn Treebank. It should be noted that the main contribution to the global accuracy is given by ARG0, ARG1 and ARGM. From the FrameNet corpus (http://www.icsi .berkeley.edu/∼framenet), we extracted all 24,558 sentences of the 40 Frames selected for the Automatic Labeling of Se</context>
</contexts>
<marker>Gildea, Palmer, 2002</marker>
<rawString>Daniel Gildea and Martha Palmer. 2002. The necessity of parsing for predicate argument recognition. In proceedings of ACL02, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods - Support Vector Learning.</booktitle>
<editor>In B. Sch¨olkopf, C. Burges, and A. Smola, editors,</editor>
<contexts>
<context position="18799" citStr="Joachims, 1999" startWordPosition="3280" endWordPosition="3281">dicates, for a total of 37,948 arguments. We randomly selected 30% of sentences for testing and 70% for training. Additionally, 30% of training was used as a validationset. Note that, since the FrameNet data does not include deep syntactic tree annotation, we processed the FrameNet data with Collins’ parser (Collins, 1997), consequently, the experiments on FrameNet relate to automatic syntactic parse trees. The classifier evaluations were carried out with the SVM-light-TK software available at http://ai-nlp.info.uniroma2.it/moschitti/ which encodes ST and SST kernels in the SVMlight software (Joachims, 1999). We used the default linear (Linear) and polynomial (Poly) kernels for the evaluations with the standard features defined in (Gildea and Jurafsky, 2002). We adopted the default regularization parameter (i.e., the average of 1/||x||) and we tried a few cost-factor values (i.e., j E 11, 3, 7, 10, 30,100}) to adjust the rate between Precision and Recall on the validation-set. For the ST and SST kernels, we derived that the best A (see Section 2.2) were 1 and 0.4, respectively. The classification performance was evaluated using the F1 measure3 for the single arguments and the accuracy for the fin</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Making large-scale SVM learning practical. In B. Sch¨olkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Vector Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junichi Kazama</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Speeding up training with tree kernels for node relation labeling.</title>
<date>2005</date>
<booktitle>In proceedings of EMNLP 2005,</booktitle>
<location>Toronto, Canada.</location>
<contexts>
<context position="27309" citStr="Kazama and Torisawa, 2005" startWordPosition="4779" endWordPosition="4782">G model improved the syntactic parsing. Additionally, it was alluded that the average execution time depends on the number of repeated productions. In (Vishwanathan and Smola, 2002), a linear complexity algorithm for the computation of the ST kernel is provided (in the worst case). The main idea is the use of the suffix trees to store partial matches for the evaluation of the string kernel (Lodhi et al., 2000). This can be used to compute the ST fragments once the tree is converted into a string. To our knowledge, ours is the first application of the ST kernel for a natural language task. In (Kazama and Torisawa, 2005), an interesting algorithm that speeds up the average running time is presented. Such algorithm looks for node pairs that have in common a large number of trees (malicious nodes) and applies a transformation to the trees rooted in such nodes to make faster the kernel computation. The results show an increase of the speed similar to the one produced by our method. In (Zelenko et al., 2003), two kernels over syntactic shallow parser structures were devised for the extraction of linguistic relations, e.g. personaffiliation. To measure the similarity between two 119 nodes, the contiguous string ke</context>
</contexts>
<marker>Kazama, Torisawa, 2005</marker>
<rawString>Junichi Kazama and Kentaro Torisawa. 2005. Speeding up training with tree kernels for node relation labeling. In proceedings of EMNLP 2005, Toronto, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kingsbury</author>
<author>Martha Palmer</author>
</authors>
<title>From Treebank to PropBank.</title>
<date>2002</date>
<booktitle>In proceedings of LREC-2002,</booktitle>
<contexts>
<context position="3635" citStr="Kingsbury and Palmer, 2002" startWordPosition="600" endWordPosition="603">r Machines (SVMs). Our fast algorithm computes the kernels between two syntactic parse trees in O(m + n) average time, where m and n are the number of nodes in the two trees. This low complexity allows SVMs to carry out experiments on hundreds of thousands of training instances since it is not higher than the complexity of the polynomial ker113 nel, widely used on large experimentation e.g. (Pradhan et al., 2004). To confirm such hypothesis, we measured the impact of the algorithm on the time required by SVMs for the learning of about 122,774 predicate argument examples annotated in PropBank (Kingsbury and Palmer, 2002) and 37,948 instances annotated in FrameNet (Fillmore, 1982). Regarding the classification properties, we studied the argument labeling accuracy of ST and SST kernels and their combinations with the standard features (Gildea and Jurafsky, 2002). The results show that, on both PropBank and FrameNet datasets, the SST-based kernel, i.e. the richest in terms of substructures, produces the highest SVM accuracy. When SSTs are combined with the manual designed features, we always obtain the best figure classifier. This suggests that the many fragments included in the SST space are relevant and, since</context>
<context position="13235" citStr="Kingsbury and Palmer, 2002" startWordPosition="2369" endWordPosition="2372">amming algorithm which computes A. In fact, the only difference with the original approach is that the matrix entries corresponding to pairs of different production rules are not considered. Since such entries contain null values they do not affect the application of the original dynamic programming. Moreover, the order of the pair evaluation can be established at run time, starting from the root nodes towards the children. 3 A Semantic Application of Parse Tree Kernels An interesting application of the SST kernel is the classification of the predicate argument structures defined in PropBank (Kingsbury and Palmer, 2002) or FrameNet (Fillmore, 1982). Figure 4 shows the parse tree of the sentence: &amp;quot;Mary brought a cat to school&amp;quot; along with the predicate argument annotation proposed in the PropBank project. Only verbs are considered as predicates whereas arguments are labeled sequentially from ARG0 to ARG9. Also in FrameNet predicate/argument information is described but for this purpose richer semantic structures called Frames are used. The Frames are schematic representations of situations involving various participants, properties and roles in which a word may be typically used. Frame elements or semantic rol</context>
</contexts>
<marker>Kingsbury, Palmer, 2002</marker>
<rawString>Paul Kingsbury and Martha Palmer. 2002. From Treebank to PropBank. In proceedings of LREC-2002, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huma Lodhi</author>
<author>Craig Saunders</author>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
<author>Christopher Watkins</author>
</authors>
<title>Text classification using string kernels.</title>
<date>2000</date>
<booktitle>In NIPS02,</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="27096" citStr="Lodhi et al., 2000" startWordPosition="4739" endWordPosition="4742">hlight their differences and properties. In (Collins and Duffy, 2002), the SST tree kernel was experimented with the Voted Perceptron for the parse-tree reranking task. The combination with the original PCFG model improved the syntactic parsing. Additionally, it was alluded that the average execution time depends on the number of repeated productions. In (Vishwanathan and Smola, 2002), a linear complexity algorithm for the computation of the ST kernel is provided (in the worst case). The main idea is the use of the suffix trees to store partial matches for the evaluation of the string kernel (Lodhi et al., 2000). This can be used to compute the ST fragments once the tree is converted into a string. To our knowledge, ours is the first application of the ST kernel for a natural language task. In (Kazama and Torisawa, 2005), an interesting algorithm that speeds up the average running time is presented. Such algorithm looks for node pairs that have in common a large number of trees (malicious nodes) and applies a transformation to the trees rooted in such nodes to make faster the kernel computation. The results show an increase of the speed similar to the one produced by our method. In (Zelenko et al., 2</context>
</contexts>
<marker>Lodhi, Saunders, Shawe-Taylor, Cristianini, Watkins, 2000</marker>
<rawString>Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cristianini, and Christopher Watkins. 2000. Text classification using string kernels. In NIPS02, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="17224" citStr="Marcus et al., 1993" startWordPosition="3029" endWordPosition="3032">k of selecting the most relevant substructures is carried out by the kernel machines themselves. 4 The Experiments The aim of the experiments is twofold. On the one hand, we show that the FTK running time is linear on the average case and is much faster than QTK. This is accomplished by measuring the learning time and the average kernel computation time. On the other hand, we study the impact of the different tree based kernels on the predicate argument classification accuracy. 4.1 Experimental Set-up We used two different corpora: PropBank (www.cis.upenn.edu/∼ace) along with PennTree bank 2 (Marcus et al., 1993) and FrameNet. PropBank contains about 53,700 sentences and a fixed split between training and testing which has been used in other researches, e.g. (Gildea and Palmer, 2002; Pradhan et al., 2004). In this split, sections from 02 to 21 are used for training, section 23 for testing and sections 1 and 22 as developing set. We considered a total of 122,774 and 7,359 arguments (from ARG0 to ARG9, ARGA and ARGM) in training and testing, respectively. Their tree structures were extracted from the Penn Treebank. It should be noted that the main contribution to the global accuracy is given by ARG0, AR</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of english: The Penn Treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>A study on convolution kernels for shallow semantic parsing.</title>
<date>2004</date>
<booktitle>In proceedings ACL04,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="1261" citStr="Moschitti, 2004" startWordPosition="189" endWordPosition="191">e classification properties of diverse tree kernels show that kernel combinations always improve the traditional methods. Experiments with Support Vector Machines on the predicate argument classification task provide empirical support to our thesis. 1 Introduction In recent years tree kernels have been shown to be interesting approaches for the modeling of syntactic information in natural language tasks, e.g. syntactic parsing (Collins and Duffy, 2002), relation extraction (Zelenko et al., 2003), Named Entity recognition (Cumby and Roth, 2003; Culotta and Sorensen, 2004) and Semantic Parsing (Moschitti, 2004). The main tree kernel advantage is the possibility to generate a high number of syntactic features and let the learning algorithm to select those most relevant for a specific application. In contrast, their major drawback are (a) the computational time complexity which is superlinear in the number of tree nodes and (b) the accuracy that they produce is often lower than the one provided by linear models on manually designed features. To solve problem (a), a linear complexity algorithm for the subtree (ST) kernel computation, was designed in (Vishwanathan and Smola, 2002). Unfortunately, the ST</context>
<context position="2519" citStr="Moschitti, 2004" startWordPosition="402" endWordPosition="403">ed by the subset tree (SST) kernel designed in (Collins and Duffy, 2002). Intuitively, an ST rooted in a node n of the target tree always contains all n’s descendants until the leaves. This does not hold for the SSTs whose leaves can be internal nodes. To solve the problem (b), a study on different tree substructure spaces should be carried out to derive the tree kernel that provide the highest accuracy. On the one hand, SSTs provide learning algorithms with richer information which may be critical to capture syntactic properties of parse trees as shown, for example, in (Zelenko et al., 2003; Moschitti, 2004). On the other hand, if the SST space contains too many irrelevant features, overfitting may occur and decrease the classification accuracy (Cumby and Roth, 2003). As a consequence, the fewer features of the ST approach may be more appropriate. In this paper, we aim to solve the above problems. We present (a) an algorithm for the evaluation of the ST and SST kernels which runs in linear average time and (b) a study of the impact of diverse tree kernels on the accuracy of Support Vector Machines (SVMs). Our fast algorithm computes the kernels between two syntactic parse trees in O(m + n) averag</context>
<context position="14776" citStr="Moschitti, 2004" startWordPosition="2622" endWordPosition="2623">o learn the classification of predicate arguments relates to the extraction of features from the syntactic parse tree of the target sentence. In (Gildea and Jurafsky, 2002) seven different features2, which aim to capture the relation between the predicate and its arguments, were proposed. For example, the Parse Tree Path of the pair (brought, ARG1) in the syntactic tree of Figure 4 is V T VP 1 NP. It encodes the dependency between the predicate and the argument as a sequence of nonterminal labels linked by direction symbols (up or down). An alternative tree kernel representation, proposed in (Moschitti, 2004), is the selection of the minimal tree subset that includes a predicate with only one of its arguments. For example, in Figure 4, the substructures inside the three frames are the semantic/syntactic structures associated with the three arguments of the verb to bring, i.e. SARG0, SARG1 and SARGM. Given a feature representation of predicate ar2Namely, they are Phrase Type, Parse Tree Path, Predicate Word, Head Word, Governing Category, Position and Voice. 116 guments, we can build an individual ONE-vs-ALL (OVA) classifier Ci for each argument i. As a final decision of the multiclassifier, we sel</context>
<context position="24400" citStr="Moschitti, 2004" startWordPosition="4269" endWordPosition="4271">(3) as the FrameNet trees are obtained with the Collins’ syntactic parser, tree kernels seem robust to incorrect parse trees. Third, we point out that the polynomial kernel on flat features is more accurate than tree kernels but the design of such effective features required noticeable knowledge and effort (Gildea and Jurafsky, 2002). On the contrary, the choice of subtrees suitable to syntactically characterize a target phenomenon seems a easier task (see Section 3 for the predicate argument case). Moreover, by combining polynomial and SST kernels, we can improve the classification accuracy (Moschitti, 2004), i.e. tree kernels provide the learning algorithm with many relevant fragments which hardly can be designed by hand. In fact, as many predicate argument structures are quite large (up to 100 nodes) they contain many fragments. ARGs ST SST ST+bow SST+bow Linear Poly ARG0 86.5 88.0 86.9 88.4 88.6 90.6 ARG1 83.1 87.4 82.8 86.7 85.9 90.8 ARG2 58.0 67.6 58.9 66.7 65.5 80.4 ARG3 35.7 37.5 39.3 41.2 51.9 60.4 ARG4 62.7 65.6 63.3 63.9 66.2 70.0 ARGM 92.0 94.2 92.0 93.7 94.9 95.3 Acc. 84.6 87.7 84.8 87.5 87.6 90.7 Table 2: Evaluation of Kernels on PropBank. Roles ST SST ST+bow SST+bow Linear Poly agen</context>
<context position="26238" citStr="Moschitti, 2004" startWordPosition="4601" endWordPosition="4602"> 85.6 85.3 85.8 87.5 87.2 Table 4: Multiclassifier accuracy using Kernel Combinations. or the SST kernel. Table 4 shows the results of four kernel combinations. We note that, (a) STs and SSTs improve Poly (about 0.5 and 2 percent points on PropBank and FrameNet, respectively) and (b) the linear kernel, which uses fewer features than Poly, is more enhanced by the SSTs than STs (for example on PropBank we have 89.4% and 88.6% vs. 87.6%), i.e. Linear takes advantage by the richer feature set of the SSTs. It should be noted that our results of kernel combinations on FrameNet are in contrast with (Moschitti, 2004), where no improvement was obtained. Our explanation is that, thanks to the fast evaluation of FTK, we could carry out an adequate parameterization. 5 Related Work Recently, several tree kernels have been designed. In the following, we highlight their differences and properties. In (Collins and Duffy, 2002), the SST tree kernel was experimented with the Voted Perceptron for the parse-tree reranking task. The combination with the original PCFG model improved the syntactic parsing. Additionally, it was alluded that the average execution time depends on the number of repeated productions. In (Vis</context>
</contexts>
<marker>Moschitti, 2004</marker>
<rawString>Alessandro Moschitti. 2004. A study on convolution kernels for shallow semantic parsing. In proceedings ACL04, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Kadri Hacioglu</author>
<author>Valeri Krugler</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Support vector learning for semantic argument classification.</title>
<date>2005</date>
<journal>Machine Learning Journal.</journal>
<marker>Pradhan, Hacioglu, Krugler, Ward, Martin, Jurafsky, 2005</marker>
<rawString>Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne Ward, James H. Martin, and Daniel Jurafsky. 2005. Support vector learning for semantic argument classification. Machine Learning Journal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Anoop Sarkar</author>
<author>Aravind Joshi</author>
</authors>
<title>Using LTAG based features in parse reranking.</title>
<date>2003</date>
<booktitle>In proceedings of EMNLP 2003,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="28332" citStr="Shen et al., 2003" startWordPosition="4949" endWordPosition="4952">over syntactic shallow parser structures were devised for the extraction of linguistic relations, e.g. personaffiliation. To measure the similarity between two 119 nodes, the contiguous string kernel and the sparse string kernel (Lodhi et al., 2000) were used. In (Culotta and Sorensen, 2004) such kernels were slightly generalized by providing a matching function for the node pairs. The time complexity for their computation limited the experiments on data set of just 200 news items. Moreover, we note that the above tree kernels are not convolution kernels as those proposed in this article. In (Shen et al., 2003), a tree-kernel based on Lexicalized Tree Adjoining Grammar (LTAG) for the parse-reranking task was proposed. Since QTK was used for the kernel computation, the high learning complexity forced the authors to train different SVMs on different slices of training data. Our FTK, adapted for the LTAG tree kernel, would have allowed SVMs to be trained on the whole data. In (Cumby and Roth, 2003), a feature description language was used to extract structural features from the syntactic shallow parse trees associated with named entities. The experiments on the named entity categorization showed that w</context>
</contexts>
<marker>Shen, Sarkar, Joshi, 2003</marker>
<rawString>Libin Shen, Anoop Sarkar, and Aravind Joshi. 2003. Using LTAG based features in parse reranking. In proceedings of EMNLP 2003, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Dan Klein</author>
<author>Mike Collins</author>
<author>Daphne Koller</author>
<author>Christopher Manning</author>
</authors>
<title>Max-margin parsing.</title>
<date>2004</date>
<booktitle>In proceedings of EMNLP</booktitle>
<location>Barcelona,</location>
<marker>Taskar, Klein, Collins, Koller, Manning, 2004</marker>
<rawString>Ben Taskar, Dan Klein, Mike Collins, Daphne Koller, and Christopher Manning. 2004. Max-margin parsing. In proceedings of EMNLP 2004 Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S V N Vishwanathan</author>
<author>A J Smola</author>
</authors>
<title>Fast kernels on strings and trees.</title>
<date>2002</date>
<booktitle>In proceedings of Neural Information Processing Systems.</booktitle>
<contexts>
<context position="1838" citStr="Vishwanathan and Smola, 2002" startWordPosition="282" endWordPosition="285">rensen, 2004) and Semantic Parsing (Moschitti, 2004). The main tree kernel advantage is the possibility to generate a high number of syntactic features and let the learning algorithm to select those most relevant for a specific application. In contrast, their major drawback are (a) the computational time complexity which is superlinear in the number of tree nodes and (b) the accuracy that they produce is often lower than the one provided by linear models on manually designed features. To solve problem (a), a linear complexity algorithm for the subtree (ST) kernel computation, was designed in (Vishwanathan and Smola, 2002). Unfortunately, the ST set is rather poorer than the one generated by the subset tree (SST) kernel designed in (Collins and Duffy, 2002). Intuitively, an ST rooted in a node n of the target tree always contains all n’s descendants until the leaves. This does not hold for the SSTs whose leaves can be internal nodes. To solve the problem (b), a study on different tree substructure spaces should be carried out to derive the tree kernel that provide the highest accuracy. On the one hand, SSTs provide learning algorithms with richer information which may be critical to capture syntactic properties</context>
<context position="26864" citStr="Vishwanathan and Smola, 2002" startWordPosition="4696" endWordPosition="4699">04), where no improvement was obtained. Our explanation is that, thanks to the fast evaluation of FTK, we could carry out an adequate parameterization. 5 Related Work Recently, several tree kernels have been designed. In the following, we highlight their differences and properties. In (Collins and Duffy, 2002), the SST tree kernel was experimented with the Voted Perceptron for the parse-tree reranking task. The combination with the original PCFG model improved the syntactic parsing. Additionally, it was alluded that the average execution time depends on the number of repeated productions. In (Vishwanathan and Smola, 2002), a linear complexity algorithm for the computation of the ST kernel is provided (in the worst case). The main idea is the use of the suffix trees to store partial matches for the evaluation of the string kernel (Lodhi et al., 2000). This can be used to compute the ST fragments once the tree is converted into a string. To our knowledge, ours is the first application of the ST kernel for a natural language task. In (Kazama and Torisawa, 2005), an interesting algorithm that speeds up the average running time is presented. Such algorithm looks for node pairs that have in common a large number of </context>
</contexts>
<marker>Vishwanathan, Smola, 2002</marker>
<rawString>S.V.N. Vishwanathan and A.J. Smola. 2002. Fast kernels on strings and trees. In proceedings of Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zelenko</author>
<author>C Aone</author>
<author>A Richardella</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research.</journal>
<contexts>
<context position="1145" citStr="Zelenko et al., 2003" startWordPosition="170" endWordPosition="173">guage as (a) we provide a simple algorithm to compute tree kernels in linear average running time and (b) our study on the classification properties of diverse tree kernels show that kernel combinations always improve the traditional methods. Experiments with Support Vector Machines on the predicate argument classification task provide empirical support to our thesis. 1 Introduction In recent years tree kernels have been shown to be interesting approaches for the modeling of syntactic information in natural language tasks, e.g. syntactic parsing (Collins and Duffy, 2002), relation extraction (Zelenko et al., 2003), Named Entity recognition (Cumby and Roth, 2003; Culotta and Sorensen, 2004) and Semantic Parsing (Moschitti, 2004). The main tree kernel advantage is the possibility to generate a high number of syntactic features and let the learning algorithm to select those most relevant for a specific application. In contrast, their major drawback are (a) the computational time complexity which is superlinear in the number of tree nodes and (b) the accuracy that they produce is often lower than the one provided by linear models on manually designed features. To solve problem (a), a linear complexity algo</context>
<context position="2501" citStr="Zelenko et al., 2003" startWordPosition="398" endWordPosition="401">r than the one generated by the subset tree (SST) kernel designed in (Collins and Duffy, 2002). Intuitively, an ST rooted in a node n of the target tree always contains all n’s descendants until the leaves. This does not hold for the SSTs whose leaves can be internal nodes. To solve the problem (b), a study on different tree substructure spaces should be carried out to derive the tree kernel that provide the highest accuracy. On the one hand, SSTs provide learning algorithms with richer information which may be critical to capture syntactic properties of parse trees as shown, for example, in (Zelenko et al., 2003; Moschitti, 2004). On the other hand, if the SST space contains too many irrelevant features, overfitting may occur and decrease the classification accuracy (Cumby and Roth, 2003). As a consequence, the fewer features of the ST approach may be more appropriate. In this paper, we aim to solve the above problems. We present (a) an algorithm for the evaluation of the ST and SST kernels which runs in linear average time and (b) a study of the impact of diverse tree kernels on the accuracy of Support Vector Machines (SVMs). Our fast algorithm computes the kernels between two syntactic parse trees </context>
<context position="27700" citStr="Zelenko et al., 2003" startWordPosition="4847" endWordPosition="4850">dhi et al., 2000). This can be used to compute the ST fragments once the tree is converted into a string. To our knowledge, ours is the first application of the ST kernel for a natural language task. In (Kazama and Torisawa, 2005), an interesting algorithm that speeds up the average running time is presented. Such algorithm looks for node pairs that have in common a large number of trees (malicious nodes) and applies a transformation to the trees rooted in such nodes to make faster the kernel computation. The results show an increase of the speed similar to the one produced by our method. In (Zelenko et al., 2003), two kernels over syntactic shallow parser structures were devised for the extraction of linguistic relations, e.g. personaffiliation. To measure the similarity between two 119 nodes, the contiguous string kernel and the sparse string kernel (Lodhi et al., 2000) were used. In (Culotta and Sorensen, 2004) such kernels were slightly generalized by providing a matching function for the node pairs. The time complexity for their computation limited the experiments on data set of just 200 news items. Moreover, we note that the above tree kernels are not convolution kernels as those proposed in this</context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel methods for relation extraction. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dell Zhang</author>
<author>Wee Sun Lee</author>
</authors>
<title>Question classification using support vector machines.</title>
<date>2003</date>
<booktitle>In proceedings of SIGIR’03,</booktitle>
<publisher>ACM Press.</publisher>
<contexts>
<context position="9392" citStr="Zhang and Lee, 2003" startWordPosition="1677" endWordPosition="1680">ctions associated with the children are identical. By recursively applying this property, it follows that the subtrees in n1 and n2 are identical. Thus, Eq. 1 evaluates the subtree (ST) kernel. When σ = 1, A(n1, n2) evaluates the number of SSTs common to n1 and n2 as proved in (Collins and Duffy, 2002). Additionally, we study some variations of the above kernels which include the leaves in the fragment space. For this purpose, it is enough to add the condition: 0. if n1 and n2 are leaves and their associated symbols are equal then A(n1, n2) = 1, to the recursive rule set for the A evaluation (Zhang and Lee, 2003). We will refer to such extended kernels as ST+bow and SST+bow (bag-ofwords). Moreover, we add the decay factor λ by modifying steps (2) and (3) as follows1: 2. A(n1, n2) = λ, 3. A(n1,n2) = λr] nc(n1) j=1 (σ + A(cj n1, cjn2)). The computational complexity of Eq. 1 is O(INT1I x INT21). We will refer to this basic implementation as the Quadratic Tree Kernel (QTK). However, as observed in (Collins and Duffy, 2002) this worst case is quite unlikely for the syntactic trees of natural language sentences, thus, we can design algorithms that run in linear time on average. Table 1: Pseudo-code for fast</context>
</contexts>
<marker>Zhang, Lee, 2003</marker>
<rawString>Dell Zhang and Wee Sun Lee. 2003. Question classification using support vector machines. In proceedings of SIGIR’03, ACM Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>