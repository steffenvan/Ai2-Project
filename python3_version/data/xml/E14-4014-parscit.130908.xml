<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.149623">
<title confidence="0.999379">
Passive-Aggressive Sequence Labeling with Discriminative Post-Editing
for Recognising Person Entities in Tweets
</title>
<author confidence="0.9989">
Leon Derczynski Kalina Bontcheva
</author>
<affiliation confidence="0.999919">
University of Sheffield University of Sheffield
</affiliation>
<email confidence="0.989137">
leon@dcs.shef.ac.uk kalina@dcs.shef.ac.uk
</email>
<sectionHeader confidence="0.9936" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999571076923077">
Recognising entities in social media text is
difficult. NER on newswire text is conven-
tionally cast as a sequence labeling prob-
lem. This makes implicit assumptions re-
garding its textual structure. Social me-
dia text is rich in disfluency and often
has poor or noisy structure, and intuitively
does not always satisfy these assumptions.
We explore noise-tolerant methods for se-
quence labeling and apply discriminative
post-editing to exceed state-of-the-art per-
formance for person recognition in tweets,
reaching an F1 of 84%.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996532">
The language of social media text is unusual
and irregular (Baldwin et al., 2013), with mis-
spellings, non-standard capitalisation and jargon,
disfluency and fragmentation. Twitter is one of the
sources of social media text most challenging for
NLP (Eisenstein, 2013; Derczynski et al., 2013).
In particular, traditional approaches to Named
Entity Recognition (NER) perform poorly on
tweets, especially on person mentions – for exam-
ple, the default model of a leading system reaches
an F1 of less than 0.5 on person entities in a ma-
jor tweet corpus. This indicates a need for ap-
proaches that can cope with the linguistic phe-
nomena apparently common among social media
authors, and operate outside of newswire with its
comparatively low linguistic diversity.
So, how can we adapt? This paper contributes
two techniques. Firstly, it demonstrates that en-
tity recognition using noise-resistant sequence la-
beling outperforms state-of-the-art Twitter NER,
although we find that recall is consistently lower
than precision. Secondly, to remedy this, we intro-
duce a method for automatically post-editing the
resulting entity annotations by using a discrimina-
tive classifier. This improves recall and precision.
</bodyText>
<sectionHeader confidence="0.98516" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999911487179487">
Named entity recognition is a well-studied prob-
lem, especially on newswire and other long-
document genres (Nadeau and Sekine, 2007; Rati-
nov and Roth, 2009). However, experiments show
that state-of-the-art NER systems from these gen-
res do not transfer well to social media text.
For example, one of the best performing
general-purpose named entity recognisers (hereon
referred to as Stanford NER) is based on linear-
chain conditional random fields (CRF) (Finkel et
al., 2005). The model is trained on newswire
data and has a number of optimisations, includ-
ing distributional similarity measures and sam-
pling for remote dependencies. While excellent
on newswire (overall F1 90%), it performs poorly
on tweets (overall F1 44%) (Ritter et al., 2011).
Rule-based named entity recognition has per-
formed a little better on tweets. Another general-
purpose NER system, ANNIE (Cunningham et al.,
2002), reached F1 of 60% over the same data (Der-
czynski et al., 2013); still a large difference.
These difficulties spurred Twitter-specific NER
research, much of which has fallen into two broad
classes: semi-supervised CRF, and LDA-based.
Semi-supervised CRF: Liu et al. (2011) com-
pare the performance of a person name dictio-
nary (F1 of 33%) to a CRF-based semi-supervised
approach (F1 of 76% on person names), using a
dataset of 12 245 tweets. This, however, is based
on a proprietary corpus, and cannot be compared
to, since the system is also not available.
Another similar approach is TwiNER (Li et al.,
2012), which is focused on a single topic stream
as opposed to general-purpose NER. This leads
to high performance for a topic-sensitive classi-
fier trained to a particular stream. In contrast we
present a general-purpose approach. Further, we
extract a specific entity class, where TwiNER per-
forms entity chunking and no classification.
</bodyText>
<page confidence="0.99253">
69
</page>
<note confidence="0.6887">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 69–73,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999717857142857">
LDA and vocabularies: Ritter et al. (2011)’s
T-NER system uses 2,400 labelled tweets, unla-
belled data and Linked Data vocabularies (Free-
base), as well as co-training. These techniques
helped but did not bring person recognition accu-
racy above the supervised MaxEnt baseline in their
experiments. We use this system as our baseline.
</bodyText>
<sectionHeader confidence="0.994193" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.997438">
3.1 Corpus
</subsectionHeader>
<bodyText confidence="0.99946415">
The experiments combine person annotations
from three openly-available datasets: Ritter et
al. (2011), UMBC (Finin et al., 2010) and
MSM2013 (Basave et al., 2013). In line with pre-
vious research (Ritter et al., 2011), annotations on
@mentions are filtered out. The placeholder to-
kens in MSM data (i.e. MENTION , HASHTAG ,
URL ) are replaced with @Mention, #hashtag,
and http://url/, respectively, to give case and char-
acter n-grams more similar to the original values.
The total corpus has 4 285 tweets, around a third
the size of that in Liu et al. (2011). This dataset
contains 86 352 tokens with 1741 entity mentions.
Person entity recognition was chosen as it is a
challenging entity type. Names of persons popular
on Twitter change more frequently than e.g. loca-
tions. Person names also tend to have a long tail,
not being confined to just public figures. Lastly,
although all three corpora cover different entity
types, they all have Person annotations.
</bodyText>
<subsectionHeader confidence="0.999975">
3.2 Labeling Scheme
</subsectionHeader>
<bodyText confidence="0.999923666666667">
Following Li et al. (2009) we used two-class IO la-
beling, where each token is either in-entity or out-
of-entity. In their NER work, this performed better
than the alternative BIO format, since data sparsity
is reduced. The IO scheme has the disadvantage
of being unable to distinguish cases where multi-
ple different entities of the same type follow each
other without intervening tokens. This situation is
uncommon and does not arise in our dataset.
</bodyText>
<subsectionHeader confidence="0.975822">
3.3 Features
</subsectionHeader>
<bodyText confidence="0.999663285714286">
The Stanford NER tool was used for feature gen-
eration. When required, nominal values were con-
verted to sparse one-hot vectors. Features for
modelling context are included (e.g. ngrams, ad-
joining labels). Our feature sets were:
base: default Stanford NER features, plus the
previous and next token and its word shape.1
</bodyText>
<footnote confidence="0.958739">
1Default plus useClassFeature=true, noMidNGrams=true,
</footnote>
<figureCaption confidence="0.9939395">
Figure 1: Training curve for lem. Diagonal cross
(blue) is CRF/PA, vertical cross (red) SVM/UM.
</figureCaption>
<bodyText confidence="0.993610363636364">
lem: with added lemmas, lower-case versions
of tokens, word shape, and neighbouring lemmas
(in attempt to reduce feature sparsity &amp; cope better
with lexical and orthographic noise). Word shape
describes the capitalisation and the type of char-
acters (e.g. letters, numbers, symbols) of a word,
without specifying actual character choices. For
example, Capital may become Ww.
These representations are chosen to compare
those that work well for newswire to those with
scope for tolerance of noise, prevalent in Twitter.
</bodyText>
<subsectionHeader confidence="0.976473">
3.4 Classifiers
</subsectionHeader>
<bodyText confidence="0.999990578947368">
For structured sequence labeling, we experiment
with conditional random fields – CRF (Lafferty
et al., 2001) – using the CRFsuite implementa-
tion (Okazaki, 2007) and LBFGS. We also use
an implementation of the passive-aggressive CRF
from CRFsuite, choosing max iterations = 500.
Passive-aggressive learning (Crammer et al.,
2006) demonstrates tolerance to noise in training
data, and can be readily adapted to provide struc-
tured output, e.g. when used in combination with
CRF. Briefly, it skips updates (is passive) when
the hinge loss of a new weight vector during up-
date is zero, but when it is positive, it aggres-
sively adjusts the weight vector regardless of the
required step size. This is integrated into CRF us-
ing a damped loss function and passive-aggressive
(PA) decisions to choose when to update. We ex-
plore the PA-I variant, where the objective func-
tion scales linearly with the slack variable.
</bodyText>
<footnote confidence="0.9346508">
maxNGramLeng=6, usePrev=true, useNext=true, usePre-
vSequences=true, maxLeft=1, useTypeSeqs=true, useType-
Seqs2=true, useTypeSeqs3=true, useTypeySequences=true,
wordShape=chris2useLC, useDisjunctive=true, lowercaseN-
Grams=true, useShapeConjunctions=true
</footnote>
<page confidence="0.991415">
70
</page>
<table confidence="0.996271615384615">
Approach Precision Recall F1
Stanford 85.88 50.00 63.20
Ritter 77.23 80.18 78.68
MaxEnt 86.92 59.09 70.35
SVM 77.55 59.16 67.11
SVM/UM 73.26 69.63 71.41
CRF 82.94 62.39 71.21
CRF/PA 80.37 65.57 72.22
Entity length (tokens) Count
1 610
2 1065
3 51
4 15
</table>
<tableCaption confidence="0.9291965">
Table 3: Distribution of person entity lengths.
Table 1: With base features (base)
</tableCaption>
<table confidence="0.9999885">
Approach Precision Recall F1
Stanford 90.60 60.00 72.19
Ritter 77.23 80.18 78.68
MaxEnt 91.10 66.33 76.76
SVM 88.22 66.58 75.89
SVM/UM 81.16 74.97 77.94
CRF 89.52 70.52 78.89
CRF/PA 86.85 74.71 80.32
</table>
<tableCaption confidence="0.999137">
Table 2: With shape and lemma features (lem)
</tableCaption>
<bodyText confidence="0.999737857142857">
For independent discriminative classification,
we use SVM, SVM/UM and a maximum entropy
classifier (MegaM (Daum´e III, 2004)). SVM is
provided by the SVMlight (Joachims, 1999) im-
plementation. SVM/UM is an uneven margins
SVM model, designed to deal better with imbal-
anced training data (Li et al., 2009).
</bodyText>
<subsectionHeader confidence="0.887534">
3.5 Baselines
</subsectionHeader>
<bodyText confidence="0.9995818">
The first baseline is the Stanford NER CRF al-
gorithm, the second Ritter’s NER algorithm. We
adapted the latter to use space tokenisation, to
preserve alignment when comparing algorithms.
Baselines are trained and evaluated on our dataset.
</bodyText>
<subsectionHeader confidence="0.890215">
3.6 Evaluation
</subsectionHeader>
<bodyText confidence="0.999726">
Candidate entity labelings are compared using the
CoNLL NER evaluation tool (Sang and Meulder,
2003), using precision, recall and F1. Following
Ritter, we use 25%/75% splits made at tweet, and
not token, level.
</bodyText>
<sectionHeader confidence="0.999965" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.999923166666667">
The base feature set performs relatively poorly
on all classifiers, with only MaxEnt beating a
baseline on any score (Table 1). However, all
achieve a higher F1 score than the default Stan-
ford NER. Of these classifiers, SVM/UM achieved
the best precision and CRF/PA – the best F1. This
demonstrates that the noise-tolerance adaptations
to SVM and CRF (uneven margins and passive-
aggressive updates, respectively) did provide im-
provements over the original algorithms.
Results using the extended features (lem) are
shown in Table 2. All classifiers improved, in-
cluding the baseline Stanford NER system. The
SVM/UM and CRF/PA adaptations continued to
outperform the vanilla models. With these fea-
tures, MaxEnt achieved highest precision and CRF
variants beat both baselines, with a top F1 of
80.32%. We continue using the lem feature set.
</bodyText>
<sectionHeader confidence="0.998941" genericHeader="method">
5 Discriminative Post-Editing
</sectionHeader>
<bodyText confidence="0.999817027027027">
Precision is higher than recall for most systems,
especially the best CRF/PA (Table 2). To improve
recall, potential entities are re-examined in post-
editing (Gadde et al., 2011). Manual post-editing
improves machine translation output (Green et al.,
2013); we train an automatic editor.
We adopt a gazetteer-based approach to trig-
gering a discriminative editor, which makes deci-
sions about labels after primary classification. The
gazetteer consists of the top 200 most common
names in English speaking countries. The first
names of popular figures over the past two years
(e.g. Helle, Barack, Scarlett) are also included.
This gives 470 case-sensitive trigger terms.
Often the trigger term is just the first in a se-
quence of tokens that make up the person name.
As can be seen from the entity length statistics
shown in Table 3, examining up to two tokens cov-
ers most (96%) person names in our corpus. Based
on this observation, we look ahead just one extra
token beyond the trigger term. This gives a to-
ken sub-sequence that was marked as out-of-entity
by the original NER classifier. Its constituents be-
come candidate person name tokens.
Candidates are then labeled using a high-recall
classifier. The classifier should be instance-based,
since we are not labeling whole sequences. We
chose SVM with variable cost (Morik et al., 1999),
which can be adjusted to prefer high recall.
To train this classifier, we extract a subset of in-
stances from the current training split as follows.
Each trigger term is included. Also, if the trig-
ger term is labeled as an entity, each subsequent
in-entity token is also included. Finally, the next
out-of-entity token is also included, to give exam-
ples of when to stop. For example, these tokens
are either in or out of the training set:
</bodyText>
<page confidence="0.998296">
71
</page>
<table confidence="0.9993837">
Method Missed entity F1 P Overall F1
R
No editing - plain CRF/PA 0.00 86.85 74.71 80.32
Naive: trigger token only 5.82 86.61 78.91 82.58
Naive: trigger plus one 6.05 81.26 82.08 81.67
SVM editor, Cost = 0.1 78.26 87.38 79.16 83.07
SVM editor, Cost = 0.5 89.72 87.17 80.30 83.60
SVM editor, Cost = 1.0 90.74 87.19 80.43 83.67
SVM editor, Cost = 1.5 92.73 87.23 80.69 83.83
SVM editor, Cost = 2.0 92.73 87.23 80.69 83.83
</table>
<tableCaption confidence="0.999038">
Table 4: Post-editing performance. Higher Cost sacrifices precision for recall.
</tableCaption>
<table confidence="0.866691333333333">
Miley O in
Heights O out
Miley PERSON in
Cyrus PERSON in
is O in
famous O out
</table>
<bodyText confidence="0.99991">
When post-editing, the window is any trigger
term and the following token, regardless of initial
label. The features used were exactly the same as
with the earlier experiment, using the lem set. This
is compared with two naive baselines: always an-
notating trigger terms as Person, and always anno-
tating trigger terms and the next token as Person.
Results are shown in Table 4. Naive editing
baselines had F1 on missed entities of around 6%,
showing that post-editing needs to be intelligent.
At Cost = 1.5, recall increased to 80.69, ex-
ceeding the Ritter recall of 80.18 (raising Cost be-
yond 1.5 had no effect). This setup gave good ac-
curacy on previously-missed entities (second col-
umn) and improved overall F1 to 83.83. It also
gave better precision and recall than the best naive
baseline (trigger-only), and 6% absolute higher
precision than trigger plus one. This is a 24.2% re-
duction in error over the Ritter baseline (F1 78.68),
and a 17.84% error reduction compared to the best
non-edited system (CRF/PA+lem).
</bodyText>
<sectionHeader confidence="0.997893" genericHeader="method">
6 Error Analysis
</sectionHeader>
<bodyText confidence="0.999472314285714">
We examine two types of classification error: false
positives (spurious) and false negatives (missed).
False positives occur most often where non-
person entities are mentioned. This occurred with
mentions of organisations (Huff Post), locations
(Galveston) and products (Exodus Porter). De-
scriptive titles were also sometimes mis-included
in person names (Millionaire Rob Ford). Names of
persons used in other forms also presented as false
positives (e.g. Marie Claire – a magazine). Pol-
ysemous names (i.e. words that could have other
functions, such as a verb) were also mis-resolved
(Mark). Finally, proper nouns referring to groups
were sometimes mis-included (Haitians).
Despite these errors, precision almost always
remained higher than recall over tweets. We use
in-domain training data, and so it is unlikely that
this is due to the wrong kinds of person being cov-
ered in the training data – as can sometimes be the
case when applying tools trained on newswire.
False negatives often occurred around incorrect
capitalisation and spelling, with unusual names,
with ambiguous tokens and in low-context set-
tings. Both omitted and added capitalisation gave
false negatives (charlie gibson, or KANYE WEST).
Spelling errors also led to missed names (Rus-
sel Crowe). Ambiguous names caused false neg-
atives and false positives; our approach missed
mark used as a name, and the surname of Jack
Straw. Unusual names with words typically used
for other purposes were also not always correctly
recognised (e.g. the Duck Lady, or the last two
tokens of Spicy Pickle Jr.). Finally, names with
few or no context words were often missed (Video:
Adele 21., and 17-9-2010 Tal al-Mallohi, a 19-).
</bodyText>
<sectionHeader confidence="0.99217" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.997614722222222">
Finding named entities in social media text, par-
ticularly tweets, is harder than in newswire. This
paper demonstrated that adapted to handle noisy
input is useful in this scenario. We achieved the
good results using CRF with passive-aggressive
updates. We used representations rich in word
shape and contextual features and achieved high
precision with moderate recall (65.57–74.71).
To improve recall, we added a post-editing stage
which finds candidate person names based on trig-
ger terms and re-labels them using a cost-adjusted
SVM. This flexible and re-usable approach lead to
a final reduction in error rate of 24.2%, giving per-
formance well above that of comparable systems.
Acknowledgment This work received funding
from EU FP7 under grant agreement No. 611233,
Pheme. We thank Chris Manning and John Bauer
of Stanford University for help with the NER tool.
</bodyText>
<page confidence="0.99749">
72
</page>
<sectionHeader confidence="0.989951" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99890580952381">
T. Baldwin, P. Cook, M. Lui, A. MacKinlay, and
L. Wang. 2013. How noisy social media text,
how diffrnt social media sources. In Proceedings of
the Sixth International Joint Conference on Natural
Language Processing, pages 356–364. ACL.
A. E. C. Basave, A. Varga, M. Rowe, M. Stankovic,
and A.-S. Dadzie. 2013. Making Sense of Micro-
posts (# MSM2013) Concept Extraction Challenge.
In Proceedings of the Concept Extraction Challenge
at the Workshop on ’Making Sense of Microposts’,
volume 1019. CEUR-WS.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research,
7:551–585.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: an Architecture for Devel-
opment of Robust HLT Applications. In Proceed-
ings of the 40th Annual Meeting on Association for
Computational Linguistics, pages 168–175.
H. Daum´e III. 2004. Notes on CG and LM-
BFGS optimization of logistic regression. Pa-
per available at http://pub.hal3.name#
daume04cg-bfgs, implementation available at
http://hal3.name/megam/, August.
L. Derczynski, D. Maynard, N. Aswani, and
K. Bontcheva. 2013. Microblog-Genre Noise and
Impact on Semantic Annotation Accuracy. In Pro-
ceedings of the 24th ACM Conference on Hypertext
and Social Media. ACM.
J. Eisenstein. 2013. What to do about bad language
on the internet. In Proceedings of the 2013 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 359–369. Association for
Computational Linguistics.
T. Finin, W. Murnane, A. Karandikar, N. Keller, J. Mar-
tineau, and M. Dredze. 2010. Annotating named
entities in Twitter data with crowdsourcing. In Pro-
ceedings of the NAACL HLT 2010 Workshop on Cre-
ating Speech and Language Data with Amazon’s
Mechanical Turk, pages 80–88.
J. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information
extraction systems by Gibbs sampling. In Proceed-
ings of the 43rd Annual Meeting of the Association
for Computational Linguistics, pages 363–370. As-
sociation for Computational Linguistics.
P. Gadde, L. Subramaniam, and T. A. Faruquie. 2011.
Adapting a WSJ trained part-of-speech tagger to
noisy text: preliminary results. In Proceedings of
the 2011 Joint Workshop on Multilingual OCR and
Analytics for Noisy Unstructured Text Data. ACM.
S. Green, J. Heer, and C. D. Manning. 2013. The effi-
cacy of human post-editing for language translation.
In Proceedings of the SIGCHI Conference on Hu-
man Factors in Computing Systems, pages 439–448.
ACM.
T. Joachims. 1999. Svmlight: Support vector machine.
SVM-Light Support Vector Machine http://svmlight.
joachims. org/, University of Dortmund, 19(4).
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proceed-
ings of the Eighteenth International Conference on
Machine Learning, pages 282–289, San Francisco:
Morgan Kaufmann.
Y. Li, K. Bontcheva, and H. Cunningham. 2009.
Adapting SVM for Data Sparseness and Imbalance:
A Case Study on Information Extraction. Natural
Language Engineering, 15(2):241–271.
C. Li, J. Weng, Q. He, Y. Yao, A. Datta, A. Sun, and
B.-S. Lee. 2012. Twiner: named entity recogni-
tion in targeted twitter stream. In Proceedings of
the 35th international ACM SIGIR conference on
Research and development in information retrieval,
pages 721–730. ACM.
X. Liu, S. Zhang, F. Wei, and M. Zhou. 2011. Rec-
ognizing named entities in tweets. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 359–367.
K. Morik, P. Brockhausen, and T. Joachims. 1999.
Combining statistical learning with a knowledge-
based approach-a case study in intensive care moni-
toring. In ICML, volume 99, pages 268–277.
D. Nadeau and S. Sekine. 2007. A survey of named
entity recognition and classification. Lingvisticae
Investigationes, 30(1):3–26.
N. Okazaki. 2007. CRFsuite: a fast implementation of
Conditional Random Fields (CRFs).
L. Ratinov and D. Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning, pages 147–155.
Association for Computational Linguistics.
A. Ritter, S. Clark, Mausam, and O. Etzioni. 2011.
Named entity recognition in tweets: An experimen-
tal study. In Proc. of Empirical Methods for Natural
Language Processing (EMNLP), Edinburgh, UK.
E. F. T. K. Sang and F. D. Meulder. 2003. Introduc-
tion to the CoNLL-2003 Shared Task: Language-
Independent Named Entity Recognition. In Pro-
ceedings of CoNLL-2003, pages 142–147. Edmon-
ton, Canada.
</reference>
<page confidence="0.999299">
73
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.950933">
<title confidence="0.9998365">Passive-Aggressive Sequence Labeling with Discriminative for Recognising Person Entities in Tweets</title>
<author confidence="0.999756">Leon Derczynski Kalina Bontcheva</author>
<affiliation confidence="0.999464">University of Sheffield University of Sheffield</affiliation>
<email confidence="0.993301">leon@dcs.shef.ac.ukkalina@dcs.shef.ac.uk</email>
<abstract confidence="0.996982857142857">Recognising entities in social media text is difficult. NER on newswire text is conventionally cast as a sequence labeling problem. This makes implicit assumptions regarding its textual structure. Social media text is rich in disfluency and often has poor or noisy structure, and intuitively does not always satisfy these assumptions. We explore noise-tolerant methods for sequence labeling and apply discriminative post-editing to exceed state-of-the-art performance for person recognition in tweets, reaching an F1 of 84%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Baldwin</author>
<author>P Cook</author>
<author>M Lui</author>
<author>A MacKinlay</author>
<author>L Wang</author>
</authors>
<title>How noisy social media text, how diffrnt social media sources.</title>
<date>2013</date>
<booktitle>In Proceedings of the Sixth International Joint Conference on Natural Language Processing,</booktitle>
<pages>356--364</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="865" citStr="Baldwin et al., 2013" startWordPosition="118" endWordPosition="121">Recognising entities in social media text is difficult. NER on newswire text is conventionally cast as a sequence labeling problem. This makes implicit assumptions regarding its textual structure. Social media text is rich in disfluency and often has poor or noisy structure, and intuitively does not always satisfy these assumptions. We explore noise-tolerant methods for sequence labeling and apply discriminative post-editing to exceed state-of-the-art performance for person recognition in tweets, reaching an F1 of 84%. 1 Introduction The language of social media text is unusual and irregular (Baldwin et al., 2013), with misspellings, non-standard capitalisation and jargon, disfluency and fragmentation. Twitter is one of the sources of social media text most challenging for NLP (Eisenstein, 2013; Derczynski et al., 2013). In particular, traditional approaches to Named Entity Recognition (NER) perform poorly on tweets, especially on person mentions – for example, the default model of a leading system reaches an F1 of less than 0.5 on person entities in a major tweet corpus. This indicates a need for approaches that can cope with the linguistic phenomena apparently common among social media authors, and o</context>
</contexts>
<marker>Baldwin, Cook, Lui, MacKinlay, Wang, 2013</marker>
<rawString>T. Baldwin, P. Cook, M. Lui, A. MacKinlay, and L. Wang. 2013. How noisy social media text, how diffrnt social media sources. In Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages 356–364. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A E C Basave</author>
<author>A Varga</author>
<author>M Rowe</author>
<author>M Stankovic</author>
<author>A-S Dadzie</author>
</authors>
<title>Making Sense of Microposts (# MSM2013) Concept Extraction Challenge.</title>
<date>2013</date>
<booktitle>In Proceedings of the Concept Extraction Challenge at the Workshop on ’Making Sense of Microposts’,</booktitle>
<volume>1019</volume>
<publisher>CEUR-WS.</publisher>
<contexts>
<context position="4566" citStr="Basave et al., 2013" startWordPosition="690" endWordPosition="693">69–73, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics LDA and vocabularies: Ritter et al. (2011)’s T-NER system uses 2,400 labelled tweets, unlabelled data and Linked Data vocabularies (Freebase), as well as co-training. These techniques helped but did not bring person recognition accuracy above the supervised MaxEnt baseline in their experiments. We use this system as our baseline. 3 Experimental Setup 3.1 Corpus The experiments combine person annotations from three openly-available datasets: Ritter et al. (2011), UMBC (Finin et al., 2010) and MSM2013 (Basave et al., 2013). In line with previous research (Ritter et al., 2011), annotations on @mentions are filtered out. The placeholder tokens in MSM data (i.e. MENTION , HASHTAG , URL ) are replaced with @Mention, #hashtag, and http://url/, respectively, to give case and character n-grams more similar to the original values. The total corpus has 4 285 tweets, around a third the size of that in Liu et al. (2011). This dataset contains 86 352 tokens with 1741 entity mentions. Person entity recognition was chosen as it is a challenging entity type. Names of persons popular on Twitter change more frequently than e.g.</context>
</contexts>
<marker>Basave, Varga, Rowe, Stankovic, Dadzie, 2013</marker>
<rawString>A. E. C. Basave, A. Varga, M. Rowe, M. Stankovic, and A.-S. Dadzie. 2013. Making Sense of Microposts (# MSM2013) Concept Extraction Challenge. In Proceedings of the Concept Extraction Challenge at the Workshop on ’Making Sense of Microposts’, volume 1019. CEUR-WS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>O Dekel</author>
<author>J Keshet</author>
<author>S Shalev-Shwartz</author>
<author>Y Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="7176" citStr="Crammer et al., 2006" startWordPosition="1104" endWordPosition="1107">racters (e.g. letters, numbers, symbols) of a word, without specifying actual character choices. For example, Capital may become Ww. These representations are chosen to compare those that work well for newswire to those with scope for tolerance of noise, prevalent in Twitter. 3.4 Classifiers For structured sequence labeling, we experiment with conditional random fields – CRF (Lafferty et al., 2001) – using the CRFsuite implementation (Okazaki, 2007) and LBFGS. We also use an implementation of the passive-aggressive CRF from CRFsuite, choosing max iterations = 500. Passive-aggressive learning (Crammer et al., 2006) demonstrates tolerance to noise in training data, and can be readily adapted to provide structured output, e.g. when used in combination with CRF. Briefly, it skips updates (is passive) when the hinge loss of a new weight vector during update is zero, but when it is positive, it aggressively adjusts the weight vector regardless of the required step size. This is integrated into CRF using a damped loss function and passive-aggressive (PA) decisions to choose when to update. We explore the PA-I variant, where the objective function scales linearly with the slack variable. maxNGramLeng=6, usePre</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. 2006. Online passive-aggressive algorithms. Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cunningham</author>
<author>D Maynard</author>
<author>K Bontcheva</author>
<author>V Tablan</author>
</authors>
<title>GATE: an Architecture for Development of Robust HLT Applications.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>168--175</pages>
<contexts>
<context position="2887" citStr="Cunningham et al., 2002" startWordPosition="428" endWordPosition="431"> media text. For example, one of the best performing general-purpose named entity recognisers (hereon referred to as Stanford NER) is based on linearchain conditional random fields (CRF) (Finkel et al., 2005). The model is trained on newswire data and has a number of optimisations, including distributional similarity measures and sampling for remote dependencies. While excellent on newswire (overall F1 90%), it performs poorly on tweets (overall F1 44%) (Ritter et al., 2011). Rule-based named entity recognition has performed a little better on tweets. Another generalpurpose NER system, ANNIE (Cunningham et al., 2002), reached F1 of 60% over the same data (Derczynski et al., 2013); still a large difference. These difficulties spurred Twitter-specific NER research, much of which has fallen into two broad classes: semi-supervised CRF, and LDA-based. Semi-supervised CRF: Liu et al. (2011) compare the performance of a person name dictionary (F1 of 33%) to a CRF-based semi-supervised approach (F1 of 76% on person names), using a dataset of 12 245 tweets. This, however, is based on a proprietary corpus, and cannot be compared to, since the system is also not available. Another similar approach is TwiNER (Li et a</context>
</contexts>
<marker>Cunningham, Maynard, Bontcheva, Tablan, 2002</marker>
<rawString>H. Cunningham, D. Maynard, K. Bontcheva, and V. Tablan. 2002. GATE: an Architecture for Development of Robust HLT Applications. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 168–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daum´e</author>
</authors>
<title>Notes on CG and LMBFGS optimization of logistic regression. Paper available at http://pub.hal3.name# daume04cg-bfgs, implementation available at http://hal3.name/megam/,</title>
<date>2004</date>
<marker>Daum´e, 2004</marker>
<rawString>H. Daum´e III. 2004. Notes on CG and LMBFGS optimization of logistic regression. Paper available at http://pub.hal3.name# daume04cg-bfgs, implementation available at http://hal3.name/megam/, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Derczynski</author>
<author>D Maynard</author>
<author>N Aswani</author>
<author>K Bontcheva</author>
</authors>
<title>Microblog-Genre Noise and Impact on Semantic Annotation Accuracy.</title>
<date>2013</date>
<booktitle>In Proceedings of the 24th ACM Conference on Hypertext and Social Media.</booktitle>
<publisher>ACM.</publisher>
<contexts>
<context position="1075" citStr="Derczynski et al., 2013" startWordPosition="148" endWordPosition="151">ia text is rich in disfluency and often has poor or noisy structure, and intuitively does not always satisfy these assumptions. We explore noise-tolerant methods for sequence labeling and apply discriminative post-editing to exceed state-of-the-art performance for person recognition in tweets, reaching an F1 of 84%. 1 Introduction The language of social media text is unusual and irregular (Baldwin et al., 2013), with misspellings, non-standard capitalisation and jargon, disfluency and fragmentation. Twitter is one of the sources of social media text most challenging for NLP (Eisenstein, 2013; Derczynski et al., 2013). In particular, traditional approaches to Named Entity Recognition (NER) perform poorly on tweets, especially on person mentions – for example, the default model of a leading system reaches an F1 of less than 0.5 on person entities in a major tweet corpus. This indicates a need for approaches that can cope with the linguistic phenomena apparently common among social media authors, and operate outside of newswire with its comparatively low linguistic diversity. So, how can we adapt? This paper contributes two techniques. Firstly, it demonstrates that entity recognition using noise-resistant se</context>
<context position="2951" citStr="Derczynski et al., 2013" startWordPosition="440" endWordPosition="444">pose named entity recognisers (hereon referred to as Stanford NER) is based on linearchain conditional random fields (CRF) (Finkel et al., 2005). The model is trained on newswire data and has a number of optimisations, including distributional similarity measures and sampling for remote dependencies. While excellent on newswire (overall F1 90%), it performs poorly on tweets (overall F1 44%) (Ritter et al., 2011). Rule-based named entity recognition has performed a little better on tweets. Another generalpurpose NER system, ANNIE (Cunningham et al., 2002), reached F1 of 60% over the same data (Derczynski et al., 2013); still a large difference. These difficulties spurred Twitter-specific NER research, much of which has fallen into two broad classes: semi-supervised CRF, and LDA-based. Semi-supervised CRF: Liu et al. (2011) compare the performance of a person name dictionary (F1 of 33%) to a CRF-based semi-supervised approach (F1 of 76% on person names), using a dataset of 12 245 tweets. This, however, is based on a proprietary corpus, and cannot be compared to, since the system is also not available. Another similar approach is TwiNER (Li et al., 2012), which is focused on a single topic stream as opposed </context>
</contexts>
<marker>Derczynski, Maynard, Aswani, Bontcheva, 2013</marker>
<rawString>L. Derczynski, D. Maynard, N. Aswani, and K. Bontcheva. 2013. Microblog-Genre Noise and Impact on Semantic Annotation Accuracy. In Proceedings of the 24th ACM Conference on Hypertext and Social Media. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisenstein</author>
</authors>
<title>What to do about bad language on the internet.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>359--369</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1049" citStr="Eisenstein, 2013" startWordPosition="146" endWordPosition="147">ucture. Social media text is rich in disfluency and often has poor or noisy structure, and intuitively does not always satisfy these assumptions. We explore noise-tolerant methods for sequence labeling and apply discriminative post-editing to exceed state-of-the-art performance for person recognition in tweets, reaching an F1 of 84%. 1 Introduction The language of social media text is unusual and irregular (Baldwin et al., 2013), with misspellings, non-standard capitalisation and jargon, disfluency and fragmentation. Twitter is one of the sources of social media text most challenging for NLP (Eisenstein, 2013; Derczynski et al., 2013). In particular, traditional approaches to Named Entity Recognition (NER) perform poorly on tweets, especially on person mentions – for example, the default model of a leading system reaches an F1 of less than 0.5 on person entities in a major tweet corpus. This indicates a need for approaches that can cope with the linguistic phenomena apparently common among social media authors, and operate outside of newswire with its comparatively low linguistic diversity. So, how can we adapt? This paper contributes two techniques. Firstly, it demonstrates that entity recognitio</context>
</contexts>
<marker>Eisenstein, 2013</marker>
<rawString>J. Eisenstein. 2013. What to do about bad language on the internet. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 359–369. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Finin</author>
<author>W Murnane</author>
<author>A Karandikar</author>
<author>N Keller</author>
<author>J Martineau</author>
<author>M Dredze</author>
</authors>
<title>Annotating named entities in Twitter data with crowdsourcing.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk,</booktitle>
<pages>80--88</pages>
<contexts>
<context position="4532" citStr="Finin et al., 2010" startWordPosition="684" endWordPosition="687">Computational Linguistics, pages 69–73, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics LDA and vocabularies: Ritter et al. (2011)’s T-NER system uses 2,400 labelled tweets, unlabelled data and Linked Data vocabularies (Freebase), as well as co-training. These techniques helped but did not bring person recognition accuracy above the supervised MaxEnt baseline in their experiments. We use this system as our baseline. 3 Experimental Setup 3.1 Corpus The experiments combine person annotations from three openly-available datasets: Ritter et al. (2011), UMBC (Finin et al., 2010) and MSM2013 (Basave et al., 2013). In line with previous research (Ritter et al., 2011), annotations on @mentions are filtered out. The placeholder tokens in MSM data (i.e. MENTION , HASHTAG , URL ) are replaced with @Mention, #hashtag, and http://url/, respectively, to give case and character n-grams more similar to the original values. The total corpus has 4 285 tweets, around a third the size of that in Liu et al. (2011). This dataset contains 86 352 tokens with 1741 entity mentions. Person entity recognition was chosen as it is a challenging entity type. Names of persons popular on Twitte</context>
</contexts>
<marker>Finin, Murnane, Karandikar, Keller, Martineau, Dredze, 2010</marker>
<rawString>T. Finin, W. Murnane, A. Karandikar, N. Keller, J. Martineau, and M. Dredze. 2010. Annotating named entities in Twitter data with crowdsourcing. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 80–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Finkel</author>
<author>T Grenager</author>
<author>C Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by Gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>363--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2471" citStr="Finkel et al., 2005" startWordPosition="363" endWordPosition="366"> automatically post-editing the resulting entity annotations by using a discriminative classifier. This improves recall and precision. 2 Background Named entity recognition is a well-studied problem, especially on newswire and other longdocument genres (Nadeau and Sekine, 2007; Ratinov and Roth, 2009). However, experiments show that state-of-the-art NER systems from these genres do not transfer well to social media text. For example, one of the best performing general-purpose named entity recognisers (hereon referred to as Stanford NER) is based on linearchain conditional random fields (CRF) (Finkel et al., 2005). The model is trained on newswire data and has a number of optimisations, including distributional similarity measures and sampling for remote dependencies. While excellent on newswire (overall F1 90%), it performs poorly on tweets (overall F1 44%) (Ritter et al., 2011). Rule-based named entity recognition has performed a little better on tweets. Another generalpurpose NER system, ANNIE (Cunningham et al., 2002), reached F1 of 60% over the same data (Derczynski et al., 2013); still a large difference. These difficulties spurred Twitter-specific NER research, much of which has fallen into two </context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>J. Finkel, T. Grenager, and C. Manning. 2005. Incorporating non-local information into information extraction systems by Gibbs sampling. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 363–370. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Gadde</author>
<author>L Subramaniam</author>
<author>T A Faruquie</author>
</authors>
<title>Adapting a WSJ trained part-of-speech tagger to noisy text: preliminary results.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Joint Workshop on Multilingual OCR and Analytics for Noisy Unstructured Text Data.</booktitle>
<publisher>ACM.</publisher>
<contexts>
<context position="10423" citStr="Gadde et al., 2011" startWordPosition="1603" endWordPosition="1606">improvements over the original algorithms. Results using the extended features (lem) are shown in Table 2. All classifiers improved, including the baseline Stanford NER system. The SVM/UM and CRF/PA adaptations continued to outperform the vanilla models. With these features, MaxEnt achieved highest precision and CRF variants beat both baselines, with a top F1 of 80.32%. We continue using the lem feature set. 5 Discriminative Post-Editing Precision is higher than recall for most systems, especially the best CRF/PA (Table 2). To improve recall, potential entities are re-examined in postediting (Gadde et al., 2011). Manual post-editing improves machine translation output (Green et al., 2013); we train an automatic editor. We adopt a gazetteer-based approach to triggering a discriminative editor, which makes decisions about labels after primary classification. The gazetteer consists of the top 200 most common names in English speaking countries. The first names of popular figures over the past two years (e.g. Helle, Barack, Scarlett) are also included. This gives 470 case-sensitive trigger terms. Often the trigger term is just the first in a sequence of tokens that make up the person name. As can be seen</context>
</contexts>
<marker>Gadde, Subramaniam, Faruquie, 2011</marker>
<rawString>P. Gadde, L. Subramaniam, and T. A. Faruquie. 2011. Adapting a WSJ trained part-of-speech tagger to noisy text: preliminary results. In Proceedings of the 2011 Joint Workshop on Multilingual OCR and Analytics for Noisy Unstructured Text Data. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Green</author>
<author>J Heer</author>
<author>C D Manning</author>
</authors>
<title>The efficacy of human post-editing for language translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,</booktitle>
<pages>439--448</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="10501" citStr="Green et al., 2013" startWordPosition="1613" endWordPosition="1616"> (lem) are shown in Table 2. All classifiers improved, including the baseline Stanford NER system. The SVM/UM and CRF/PA adaptations continued to outperform the vanilla models. With these features, MaxEnt achieved highest precision and CRF variants beat both baselines, with a top F1 of 80.32%. We continue using the lem feature set. 5 Discriminative Post-Editing Precision is higher than recall for most systems, especially the best CRF/PA (Table 2). To improve recall, potential entities are re-examined in postediting (Gadde et al., 2011). Manual post-editing improves machine translation output (Green et al., 2013); we train an automatic editor. We adopt a gazetteer-based approach to triggering a discriminative editor, which makes decisions about labels after primary classification. The gazetteer consists of the top 200 most common names in English speaking countries. The first names of popular figures over the past two years (e.g. Helle, Barack, Scarlett) are also included. This gives 470 case-sensitive trigger terms. Often the trigger term is just the first in a sequence of tokens that make up the person name. As can be seen from the entity length statistics shown in Table 3, examining up to two token</context>
</contexts>
<marker>Green, Heer, Manning, 2013</marker>
<rawString>S. Green, J. Heer, and C. D. Manning. 2013. The efficacy of human post-editing for language translation. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 439–448. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Svmlight: Support vector machine. SVM-Light Support Vector Machine http://svmlight. joachims. org/,</title>
<date>1999</date>
<institution>University of Dortmund,</institution>
<contexts>
<context position="8762" citStr="Joachims, 1999" startWordPosition="1344" endWordPosition="1345"> 69.63 71.41 CRF 82.94 62.39 71.21 CRF/PA 80.37 65.57 72.22 Entity length (tokens) Count 1 610 2 1065 3 51 4 15 Table 3: Distribution of person entity lengths. Table 1: With base features (base) Approach Precision Recall F1 Stanford 90.60 60.00 72.19 Ritter 77.23 80.18 78.68 MaxEnt 91.10 66.33 76.76 SVM 88.22 66.58 75.89 SVM/UM 81.16 74.97 77.94 CRF 89.52 70.52 78.89 CRF/PA 86.85 74.71 80.32 Table 2: With shape and lemma features (lem) For independent discriminative classification, we use SVM, SVM/UM and a maximum entropy classifier (MegaM (Daum´e III, 2004)). SVM is provided by the SVMlight (Joachims, 1999) implementation. SVM/UM is an uneven margins SVM model, designed to deal better with imbalanced training data (Li et al., 2009). 3.5 Baselines The first baseline is the Stanford NER CRF algorithm, the second Ritter’s NER algorithm. We adapted the latter to use space tokenisation, to preserve alignment when comparing algorithms. Baselines are trained and evaluated on our dataset. 3.6 Evaluation Candidate entity labelings are compared using the CoNLL NER evaluation tool (Sang and Meulder, 2003), using precision, recall and F1. Following Ritter, we use 25%/75% splits made at tweet, and not token,</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Svmlight: Support vector machine. SVM-Light Support Vector Machine http://svmlight. joachims. org/, University of Dortmund, 19(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco:</location>
<contexts>
<context position="6956" citStr="Lafferty et al., 2001" startWordPosition="1072" endWordPosition="1075">as, lower-case versions of tokens, word shape, and neighbouring lemmas (in attempt to reduce feature sparsity &amp; cope better with lexical and orthographic noise). Word shape describes the capitalisation and the type of characters (e.g. letters, numbers, symbols) of a word, without specifying actual character choices. For example, Capital may become Ww. These representations are chosen to compare those that work well for newswire to those with scope for tolerance of noise, prevalent in Twitter. 3.4 Classifiers For structured sequence labeling, we experiment with conditional random fields – CRF (Lafferty et al., 2001) – using the CRFsuite implementation (Okazaki, 2007) and LBFGS. We also use an implementation of the passive-aggressive CRF from CRFsuite, choosing max iterations = 500. Passive-aggressive learning (Crammer et al., 2006) demonstrates tolerance to noise in training data, and can be readily adapted to provide structured output, e.g. when used in combination with CRF. Briefly, it skips updates (is passive) when the hinge loss of a new weight vector during update is zero, but when it is positive, it aggressively adjusts the weight vector regardless of the required step size. This is integrated int</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proceedings of the Eighteenth International Conference on Machine Learning, pages 282–289, San Francisco: Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Li</author>
<author>K Bontcheva</author>
<author>H Cunningham</author>
</authors>
<title>Adapting SVM for Data Sparseness and Imbalance: A Case Study on Information Extraction.</title>
<date>2009</date>
<journal>Natural Language Engineering,</journal>
<volume>15</volume>
<issue>2</issue>
<contexts>
<context position="5410" citStr="Li et al. (2009)" startWordPosition="833" endWordPosition="836">ively, to give case and character n-grams more similar to the original values. The total corpus has 4 285 tweets, around a third the size of that in Liu et al. (2011). This dataset contains 86 352 tokens with 1741 entity mentions. Person entity recognition was chosen as it is a challenging entity type. Names of persons popular on Twitter change more frequently than e.g. locations. Person names also tend to have a long tail, not being confined to just public figures. Lastly, although all three corpora cover different entity types, they all have Person annotations. 3.2 Labeling Scheme Following Li et al. (2009) we used two-class IO labeling, where each token is either in-entity or outof-entity. In their NER work, this performed better than the alternative BIO format, since data sparsity is reduced. The IO scheme has the disadvantage of being unable to distinguish cases where multiple different entities of the same type follow each other without intervening tokens. This situation is uncommon and does not arise in our dataset. 3.3 Features The Stanford NER tool was used for feature generation. When required, nominal values were converted to sparse one-hot vectors. Features for modelling context are in</context>
<context position="8889" citStr="Li et al., 2009" startWordPosition="1364" endWordPosition="1367">ibution of person entity lengths. Table 1: With base features (base) Approach Precision Recall F1 Stanford 90.60 60.00 72.19 Ritter 77.23 80.18 78.68 MaxEnt 91.10 66.33 76.76 SVM 88.22 66.58 75.89 SVM/UM 81.16 74.97 77.94 CRF 89.52 70.52 78.89 CRF/PA 86.85 74.71 80.32 Table 2: With shape and lemma features (lem) For independent discriminative classification, we use SVM, SVM/UM and a maximum entropy classifier (MegaM (Daum´e III, 2004)). SVM is provided by the SVMlight (Joachims, 1999) implementation. SVM/UM is an uneven margins SVM model, designed to deal better with imbalanced training data (Li et al., 2009). 3.5 Baselines The first baseline is the Stanford NER CRF algorithm, the second Ritter’s NER algorithm. We adapted the latter to use space tokenisation, to preserve alignment when comparing algorithms. Baselines are trained and evaluated on our dataset. 3.6 Evaluation Candidate entity labelings are compared using the CoNLL NER evaluation tool (Sang and Meulder, 2003), using precision, recall and F1. Following Ritter, we use 25%/75% splits made at tweet, and not token, level. 4 Results The base feature set performs relatively poorly on all classifiers, with only MaxEnt beating a baseline on an</context>
</contexts>
<marker>Li, Bontcheva, Cunningham, 2009</marker>
<rawString>Y. Li, K. Bontcheva, and H. Cunningham. 2009. Adapting SVM for Data Sparseness and Imbalance: A Case Study on Information Extraction. Natural Language Engineering, 15(2):241–271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Li</author>
<author>J Weng</author>
<author>Q He</author>
<author>Y Yao</author>
<author>A Datta</author>
<author>A Sun</author>
<author>B-S Lee</author>
</authors>
<title>Twiner: named entity recognition in targeted twitter stream.</title>
<date>2012</date>
<booktitle>In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>721--730</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3496" citStr="Li et al., 2012" startWordPosition="530" endWordPosition="533">, 2002), reached F1 of 60% over the same data (Derczynski et al., 2013); still a large difference. These difficulties spurred Twitter-specific NER research, much of which has fallen into two broad classes: semi-supervised CRF, and LDA-based. Semi-supervised CRF: Liu et al. (2011) compare the performance of a person name dictionary (F1 of 33%) to a CRF-based semi-supervised approach (F1 of 76% on person names), using a dataset of 12 245 tweets. This, however, is based on a proprietary corpus, and cannot be compared to, since the system is also not available. Another similar approach is TwiNER (Li et al., 2012), which is focused on a single topic stream as opposed to general-purpose NER. This leads to high performance for a topic-sensitive classifier trained to a particular stream. In contrast we present a general-purpose approach. Further, we extract a specific entity class, where TwiNER performs entity chunking and no classification. 69 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 69–73, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics LDA and vocabularies: Ritter et al. (2011)’s T-NER syste</context>
</contexts>
<marker>Li, Weng, He, Yao, Datta, Sun, Lee, 2012</marker>
<rawString>C. Li, J. Weng, Q. He, Y. Yao, A. Datta, A. Sun, and B.-S. Lee. 2012. Twiner: named entity recognition in targeted twitter stream. In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval, pages 721–730. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Liu</author>
<author>S Zhang</author>
<author>F Wei</author>
<author>M Zhou</author>
</authors>
<title>Recognizing named entities in tweets.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>359--367</pages>
<contexts>
<context position="3160" citStr="Liu et al. (2011)" startWordPosition="470" endWordPosition="473">s, including distributional similarity measures and sampling for remote dependencies. While excellent on newswire (overall F1 90%), it performs poorly on tweets (overall F1 44%) (Ritter et al., 2011). Rule-based named entity recognition has performed a little better on tweets. Another generalpurpose NER system, ANNIE (Cunningham et al., 2002), reached F1 of 60% over the same data (Derczynski et al., 2013); still a large difference. These difficulties spurred Twitter-specific NER research, much of which has fallen into two broad classes: semi-supervised CRF, and LDA-based. Semi-supervised CRF: Liu et al. (2011) compare the performance of a person name dictionary (F1 of 33%) to a CRF-based semi-supervised approach (F1 of 76% on person names), using a dataset of 12 245 tweets. This, however, is based on a proprietary corpus, and cannot be compared to, since the system is also not available. Another similar approach is TwiNER (Li et al., 2012), which is focused on a single topic stream as opposed to general-purpose NER. This leads to high performance for a topic-sensitive classifier trained to a particular stream. In contrast we present a general-purpose approach. Further, we extract a specific entity </context>
<context position="4960" citStr="Liu et al. (2011)" startWordPosition="760" endWordPosition="763">this system as our baseline. 3 Experimental Setup 3.1 Corpus The experiments combine person annotations from three openly-available datasets: Ritter et al. (2011), UMBC (Finin et al., 2010) and MSM2013 (Basave et al., 2013). In line with previous research (Ritter et al., 2011), annotations on @mentions are filtered out. The placeholder tokens in MSM data (i.e. MENTION , HASHTAG , URL ) are replaced with @Mention, #hashtag, and http://url/, respectively, to give case and character n-grams more similar to the original values. The total corpus has 4 285 tweets, around a third the size of that in Liu et al. (2011). This dataset contains 86 352 tokens with 1741 entity mentions. Person entity recognition was chosen as it is a challenging entity type. Names of persons popular on Twitter change more frequently than e.g. locations. Person names also tend to have a long tail, not being confined to just public figures. Lastly, although all three corpora cover different entity types, they all have Person annotations. 3.2 Labeling Scheme Following Li et al. (2009) we used two-class IO labeling, where each token is either in-entity or outof-entity. In their NER work, this performed better than the alternative BI</context>
</contexts>
<marker>Liu, Zhang, Wei, Zhou, 2011</marker>
<rawString>X. Liu, S. Zhang, F. Wei, and M. Zhou. 2011. Recognizing named entities in tweets. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 359–367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Morik</author>
<author>P Brockhausen</author>
<author>T Joachims</author>
</authors>
<title>Combining statistical learning with a knowledgebased approach-a case study in intensive care monitoring.</title>
<date>1999</date>
<booktitle>In ICML,</booktitle>
<volume>99</volume>
<pages>268--277</pages>
<contexts>
<context position="11583" citStr="Morik et al., 1999" startWordPosition="1792" endWordPosition="1795">ence of tokens that make up the person name. As can be seen from the entity length statistics shown in Table 3, examining up to two tokens covers most (96%) person names in our corpus. Based on this observation, we look ahead just one extra token beyond the trigger term. This gives a token sub-sequence that was marked as out-of-entity by the original NER classifier. Its constituents become candidate person name tokens. Candidates are then labeled using a high-recall classifier. The classifier should be instance-based, since we are not labeling whole sequences. We chose SVM with variable cost (Morik et al., 1999), which can be adjusted to prefer high recall. To train this classifier, we extract a subset of instances from the current training split as follows. Each trigger term is included. Also, if the trigger term is labeled as an entity, each subsequent in-entity token is also included. Finally, the next out-of-entity token is also included, to give examples of when to stop. For example, these tokens are either in or out of the training set: 71 Method Missed entity F1 P Overall F1 R No editing - plain CRF/PA 0.00 86.85 74.71 80.32 Naive: trigger token only 5.82 86.61 78.91 82.58 Naive: trigger plus </context>
</contexts>
<marker>Morik, Brockhausen, Joachims, 1999</marker>
<rawString>K. Morik, P. Brockhausen, and T. Joachims. 1999. Combining statistical learning with a knowledgebased approach-a case study in intensive care monitoring. In ICML, volume 99, pages 268–277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Nadeau</author>
<author>S Sekine</author>
</authors>
<title>A survey of named entity recognition and classification.</title>
<date>2007</date>
<journal>Lingvisticae Investigationes,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="2128" citStr="Nadeau and Sekine, 2007" startWordPosition="309" endWordPosition="312">aratively low linguistic diversity. So, how can we adapt? This paper contributes two techniques. Firstly, it demonstrates that entity recognition using noise-resistant sequence labeling outperforms state-of-the-art Twitter NER, although we find that recall is consistently lower than precision. Secondly, to remedy this, we introduce a method for automatically post-editing the resulting entity annotations by using a discriminative classifier. This improves recall and precision. 2 Background Named entity recognition is a well-studied problem, especially on newswire and other longdocument genres (Nadeau and Sekine, 2007; Ratinov and Roth, 2009). However, experiments show that state-of-the-art NER systems from these genres do not transfer well to social media text. For example, one of the best performing general-purpose named entity recognisers (hereon referred to as Stanford NER) is based on linearchain conditional random fields (CRF) (Finkel et al., 2005). The model is trained on newswire data and has a number of optimisations, including distributional similarity measures and sampling for remote dependencies. While excellent on newswire (overall F1 90%), it performs poorly on tweets (overall F1 44%) (Ritter</context>
</contexts>
<marker>Nadeau, Sekine, 2007</marker>
<rawString>D. Nadeau and S. Sekine. 2007. A survey of named entity recognition and classification. Lingvisticae Investigationes, 30(1):3–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Okazaki</author>
</authors>
<title>CRFsuite: a fast implementation of Conditional Random Fields (CRFs).</title>
<date>2007</date>
<contexts>
<context position="7008" citStr="Okazaki, 2007" startWordPosition="1082" endWordPosition="1083">ing lemmas (in attempt to reduce feature sparsity &amp; cope better with lexical and orthographic noise). Word shape describes the capitalisation and the type of characters (e.g. letters, numbers, symbols) of a word, without specifying actual character choices. For example, Capital may become Ww. These representations are chosen to compare those that work well for newswire to those with scope for tolerance of noise, prevalent in Twitter. 3.4 Classifiers For structured sequence labeling, we experiment with conditional random fields – CRF (Lafferty et al., 2001) – using the CRFsuite implementation (Okazaki, 2007) and LBFGS. We also use an implementation of the passive-aggressive CRF from CRFsuite, choosing max iterations = 500. Passive-aggressive learning (Crammer et al., 2006) demonstrates tolerance to noise in training data, and can be readily adapted to provide structured output, e.g. when used in combination with CRF. Briefly, it skips updates (is passive) when the hinge loss of a new weight vector during update is zero, but when it is positive, it aggressively adjusts the weight vector regardless of the required step size. This is integrated into CRF using a damped loss function and passive-aggre</context>
</contexts>
<marker>Okazaki, 2007</marker>
<rawString>N. Okazaki. 2007. CRFsuite: a fast implementation of Conditional Random Fields (CRFs).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ratinov</author>
<author>D Roth</author>
</authors>
<title>Design challenges and misconceptions in named entity recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>147--155</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2153" citStr="Ratinov and Roth, 2009" startWordPosition="313" endWordPosition="317">diversity. So, how can we adapt? This paper contributes two techniques. Firstly, it demonstrates that entity recognition using noise-resistant sequence labeling outperforms state-of-the-art Twitter NER, although we find that recall is consistently lower than precision. Secondly, to remedy this, we introduce a method for automatically post-editing the resulting entity annotations by using a discriminative classifier. This improves recall and precision. 2 Background Named entity recognition is a well-studied problem, especially on newswire and other longdocument genres (Nadeau and Sekine, 2007; Ratinov and Roth, 2009). However, experiments show that state-of-the-art NER systems from these genres do not transfer well to social media text. For example, one of the best performing general-purpose named entity recognisers (hereon referred to as Stanford NER) is based on linearchain conditional random fields (CRF) (Finkel et al., 2005). The model is trained on newswire data and has a number of optimisations, including distributional similarity measures and sampling for remote dependencies. While excellent on newswire (overall F1 90%), it performs poorly on tweets (overall F1 44%) (Ritter et al., 2011). Rule-base</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>L. Ratinov and D. Roth. 2009. Design challenges and misconceptions in named entity recognition. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning, pages 147–155. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ritter</author>
<author>S Clark</author>
<author>Mausam</author>
<author>O Etzioni</author>
</authors>
<title>Named entity recognition in tweets: An experimental study.</title>
<date>2011</date>
<booktitle>In Proc. of Empirical Methods for Natural Language Processing (EMNLP),</booktitle>
<location>Edinburgh, UK.</location>
<contexts>
<context position="2742" citStr="Ritter et al., 2011" startWordPosition="406" endWordPosition="409">, 2007; Ratinov and Roth, 2009). However, experiments show that state-of-the-art NER systems from these genres do not transfer well to social media text. For example, one of the best performing general-purpose named entity recognisers (hereon referred to as Stanford NER) is based on linearchain conditional random fields (CRF) (Finkel et al., 2005). The model is trained on newswire data and has a number of optimisations, including distributional similarity measures and sampling for remote dependencies. While excellent on newswire (overall F1 90%), it performs poorly on tweets (overall F1 44%) (Ritter et al., 2011). Rule-based named entity recognition has performed a little better on tweets. Another generalpurpose NER system, ANNIE (Cunningham et al., 2002), reached F1 of 60% over the same data (Derczynski et al., 2013); still a large difference. These difficulties spurred Twitter-specific NER research, much of which has fallen into two broad classes: semi-supervised CRF, and LDA-based. Semi-supervised CRF: Liu et al. (2011) compare the performance of a person name dictionary (F1 of 33%) to a CRF-based semi-supervised approach (F1 of 76% on person names), using a dataset of 12 245 tweets. This, however,</context>
<context position="4082" citStr="Ritter et al. (2011)" startWordPosition="616" endWordPosition="619">proach is TwiNER (Li et al., 2012), which is focused on a single topic stream as opposed to general-purpose NER. This leads to high performance for a topic-sensitive classifier trained to a particular stream. In contrast we present a general-purpose approach. Further, we extract a specific entity class, where TwiNER performs entity chunking and no classification. 69 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 69–73, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics LDA and vocabularies: Ritter et al. (2011)’s T-NER system uses 2,400 labelled tweets, unlabelled data and Linked Data vocabularies (Freebase), as well as co-training. These techniques helped but did not bring person recognition accuracy above the supervised MaxEnt baseline in their experiments. We use this system as our baseline. 3 Experimental Setup 3.1 Corpus The experiments combine person annotations from three openly-available datasets: Ritter et al. (2011), UMBC (Finin et al., 2010) and MSM2013 (Basave et al., 2013). In line with previous research (Ritter et al., 2011), annotations on @mentions are filtered out. The placeholder t</context>
</contexts>
<marker>Ritter, Clark, Mausam, Etzioni, 2011</marker>
<rawString>A. Ritter, S. Clark, Mausam, and O. Etzioni. 2011. Named entity recognition in tweets: An experimental study. In Proc. of Empirical Methods for Natural Language Processing (EMNLP), Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F T K Sang</author>
<author>F D Meulder</author>
</authors>
<title>Introduction to the CoNLL-2003 Shared Task: LanguageIndependent Named Entity Recognition.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL-2003,</booktitle>
<pages>142--147</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="9259" citStr="Sang and Meulder, 2003" startWordPosition="1420" endWordPosition="1423"> use SVM, SVM/UM and a maximum entropy classifier (MegaM (Daum´e III, 2004)). SVM is provided by the SVMlight (Joachims, 1999) implementation. SVM/UM is an uneven margins SVM model, designed to deal better with imbalanced training data (Li et al., 2009). 3.5 Baselines The first baseline is the Stanford NER CRF algorithm, the second Ritter’s NER algorithm. We adapted the latter to use space tokenisation, to preserve alignment when comparing algorithms. Baselines are trained and evaluated on our dataset. 3.6 Evaluation Candidate entity labelings are compared using the CoNLL NER evaluation tool (Sang and Meulder, 2003), using precision, recall and F1. Following Ritter, we use 25%/75% splits made at tweet, and not token, level. 4 Results The base feature set performs relatively poorly on all classifiers, with only MaxEnt beating a baseline on any score (Table 1). However, all achieve a higher F1 score than the default Stanford NER. Of these classifiers, SVM/UM achieved the best precision and CRF/PA – the best F1. This demonstrates that the noise-tolerance adaptations to SVM and CRF (uneven margins and passiveaggressive updates, respectively) did provide improvements over the original algorithms. Results usin</context>
</contexts>
<marker>Sang, Meulder, 2003</marker>
<rawString>E. F. T. K. Sang and F. D. Meulder. 2003. Introduction to the CoNLL-2003 Shared Task: LanguageIndependent Named Entity Recognition. In Proceedings of CoNLL-2003, pages 142–147. Edmonton, Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>