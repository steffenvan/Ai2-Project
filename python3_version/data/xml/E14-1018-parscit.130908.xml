<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000102">
<title confidence="0.9768885">
Regularized Structured Perceptron:
A Case Study on Chinese Word Segmentation, POS Tagging and Parsing
</title>
<author confidence="0.998233">
Kaixu Zhang Jinsong Su Changle Zhou
</author>
<affiliation confidence="0.9731335">
Xiamen University Xiamen University Xiamen University
Fujian, P.R. China Fujian, P.R. China Fujian, P.R. China
</affiliation>
<email confidence="0.99689">
kareyzhang@gmail.com jssu@xmu.edu.cn dozero@xmu.edu.cn
</email>
<sectionHeader confidence="0.997363" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99993180952381">
Structured perceptron becomes popular
for various NLP tasks such as tagging and
parsing. Practical studies on NLP did not
pay much attention to its regularization. In
this paper, we study three simple but effec-
tive task-independent regularization meth-
ods: (1) one is to average weights of dif-
ferent trained models to reduce the bias
caused by the specific order of the train-
ing examples; (2) one is to add penalty
term to the loss function; (3) and one is
to randomly corrupt the data flow during
training which is called dropout in the neu-
ral network. Experiments are conducted
on three NLP tasks, namely Chinese word
segmentation, part-of-speech tagging and
dependency parsing. Applying proper reg-
ularization methods or their combinations,
the error reductions with respect to the av-
eraged perceptron for some of these tasks
can be up to 10%.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999888156862745">
Structured perceptron is a linear classification al-
gorithm. It is used for word segmentation (Zhang
and Clark, 2011), POS (part-of-speech) tagging
(Collins, 2002), syntactical parsing (Collins and
Roark, 2004), semantical parsing (Zettlemoyer
and Collins, 2009) and other NLP tasks.
The averaged perceptron or the voted percep-
tron (Collins, 2002) is proposed for better gener-
alization. Early update (Collins and Roark, 2004;
Huang et al., 2012) is used for inexact decod-
ing algorithms such as the beam search. Dis-
tributed training (McDonald et al., 2010) and the
minibatch and parallelization method (Zhao and
Huang, 2013) are recently proposed. Some other
related work focuses on the task-specified feature
engineering.
Regularization is to improve the ability of
generalization and avoid over-fitting for machine
learning algorithms including online learning al-
gorithms (Do et al., 2009; Xiao, 2010). But prac-
tical studies on NLP did not pay much attention to
the regularization of the structured perceptron. As
a result, for some tasks the model learned using
perceptron algorithm is not as good as the model
learned using regularized condition random field.
In this paper, we treat the perceptron algorithm
as a special case of the stochastic gradient de-
scent (SGD) algorithm and study three kinds of
simple but effective task-independent regulariza-
tion methods that can be applied. The averaging
method is to average the weight vectors of differ-
ent models. We propose a “shuffle-and-average”
method to reduce the bias caused by the specific
order of the training examples. The traditional
penalty method is to add penalty term to the loss
function. The dropout method is to randomly cor-
rupt the data flow during training. We show that
this dropout method originally used in neural net-
work also helps the structured perceptron.
In Section 2, we describe the perceptron algo-
rithm as a special case of the stochastic gradient
descent algorithm. Then we discuss three kinds of
regularization methods for structured perceptron
in Section 3, 4 and 5, respectively. Experiments
conducted in Section 6 shows that these regular-
ization methods and their combinations improve
performances of NLP tasks such as Chinese word
segmentation, POS tagging and dependency pars-
ing. Applying proper regularization methods, the
error reductions of these NLP tasks can be up to
10%. We finally conclude this work in Section 7.
</bodyText>
<page confidence="0.977589">
164
</page>
<note confidence="0.9989185">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 164–173,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.9908005">
Figure 1: A structured perceptron can be seen as a
multi-layer feed-forward neural network.
</figureCaption>
<sectionHeader confidence="0.967075" genericHeader="introduction">
2 Structured Perceptron
</sectionHeader>
<bodyText confidence="0.9995351">
We treat the structured perceptron architecture as
a multi-layer feed-forward neural network as in
Figure 1 and treat the perceptron algorithm as a
special case of the stochastic gradient descent al-
gorithm in order to describe all the regularization
methods.
The network of the structured perceptron has
three layers. The input vector x and output vector
y of the structured classification task are concate-
nated as the input layer. The hidden layer is the
feature vector 4&apos;(x, y). The connections between
the input layer and the hidden layer are usually
hand-crafted and fixed during training and predict-
ing. And the output layer of the network is a scalar
w·4&apos;(x, y) which is used to evaluate the matching
of the vector x and y.
Besides the common process to calculate the
output layer given the input layer, there is a pro-
cess called decoding, which is to find a vector z to
maximum the activation of the output layer:
</bodyText>
<equation confidence="0.9552045">
zi = arg max w · 4&apos;(xi, z) (1)
Z
</equation>
<bodyText confidence="0.9987125">
By carefully designing the feature vector, the de-
coding can be efficiently performed using dynamic
programming. Beam search is also commonly
used for the decoding of syntactical parsing tasks.
In the predicting precess, the vector z is the
structured output corresponding to x. In the train-
ing precess, what we expect is that for every input
xi, the vector zi that maximums the activation of
the output layer is exactly the gold standard output
yi.
We define the loss function as the sum of the
margins of the whole training data:
</bodyText>
<equation confidence="0.9804296">
L(w) = X {w · 4&apos;(xi, zi) − w · 4&apos;(xi, yi)}
i
X= w ·A4&apos;i
i
(2)
</equation>
<bodyText confidence="0.7458534">
where
A4&apos;i = 4&apos;(xi, zi) − 4&apos;(xi, yi) (3)
The unconstrained optimization problem of the
training process is
arg min
</bodyText>
<equation confidence="0.979305">
W L(w) (4)
</equation>
<bodyText confidence="0.999962857142857">
The loss function isnot convex but calculating
the derivative is easy. One of the algorithms to
solve this optimization problem is SGD. Here we
use the minibatch with size of 1, which means in
every iteration we use only one training example
to approximate the loss function and the gradient
to update the weight vector:
</bodyText>
<equation confidence="0.9973575">
w(t+1) ← w(t) − η ∂L ≈ w(t) − ηA4&apos;(t)
∂w W(t)
</equation>
<bodyText confidence="0.968644">
(5)
where w(t) is the weight vector after t updates.
Note that in this case, the learning rate η can be set
to an arbitrary positive real number. In the percep-
tron algorithm commonly used in NLP (Collins,
2002) , η is not changed respect to t. We fix η to
be 1 in this paper without loss of generality.
</bodyText>
<sectionHeader confidence="0.998451" genericHeader="method">
3 Averaging
</sectionHeader>
<subsectionHeader confidence="0.999908">
3.1 Averaged Perceptron
</subsectionHeader>
<bodyText confidence="0.999980125">
Averaging the weight vectors in the learning pro-
cess is one of the most popular regularization
techniques of the structured perceptron (Collins,
2002). And it is also the only used regulariza-
tion technique for many practical studies on NLP
(Jiang et al., 2009; Huang and Sagae, 2010).
Suppose the learning algorithm stopped after T
updates. The final weight vector is calculated as:
</bodyText>
<equation confidence="0.997926">
w(t) (6)
</equation>
<bodyText confidence="0.999731769230769">
The intuition might be that the learned weight
vector is dependent on the order of the training ex-
amples. The final vector w(T) may be more ap-
propriate for the last few training examples than
the previous ones. The averaging method is used
to avoid such tendency. Similar treatment is used
in other sequential algorithm such as the Markov
chain Monte Carlo sampling method.
Since this regularization technique is widely
used and tested, it is used for all the models in
the experiments of this paper. Any other regular-
ization methods are applied to this basic averaged
perceptron.
</bodyText>
<figure confidence="0.6710995">
input layer
x
hidden layer
output layer
</figure>
<equation confidence="0.991741875">
4&apos;(x, y)
w · 4&apos;(x, y)
y
1
T
w =
XT
t=1
</equation>
<page confidence="0.993588">
165
</page>
<subsectionHeader confidence="0.99908">
3.2 Shuffle and Average
</subsectionHeader>
<bodyText confidence="0.999938692307692">
As we has mentioned that the learned weight vec-
tor is strongly dependent on the order of the train-
ing examples, randomly shuffling the training ex-
amples results in different weight vectors. Based
on such observation, we training different weight
vectors using the same training examples with dif-
ferent orders, and average them to get the final
weight vector. We use this method to further min-
imize the side effect caused by this online algo-
rithm.
Suppose we shuffle and train n different weight
vectors w[1], ... , w[n], the j-th component of the
final vector can be simply calculated as
</bodyText>
<equation confidence="0.997672666666667">
wj = Pn w [i] (7)
i=1 j
n
</equation>
<bodyText confidence="0.999170444444444">
Note that generally these models do not share
the same feature set. Features may be used in one
model but not in another one. When w[i]
j = 0, it
does not imply that this feature has no effect on
this problem. It only implies that this feature does
not have chances to be tested. We propose a modi-
fied equation to only average the non-zero compo-
nents:
</bodyText>
<subsectionHeader confidence="0.99869">
4.1 L2-norm penalty
</subsectionHeader>
<bodyText confidence="0.9999745">
We can add a square of the L2-norm of the weight
vector as the penalty term to the loss function as
</bodyText>
<equation confidence="0.980556">
λ2
ΔΦi + 2 kwk2 (9)
2
</equation>
<bodyText confidence="0.922263">
where λ2 is a hyper-parameter to determine the
strength of the penalty.
In the SGD algorithm, the update method of the
weight vector is thus
</bodyText>
<equation confidence="0.998653">
w(t+1) ← (1 − ηλ2)w(t) − ηΔΦ(t) (10)
</equation>
<bodyText confidence="0.979462">
The term (1 − ηλ2) is used to decay the weight in
every updates. This forces the weights to be close
to zero.
</bodyText>
<subsectionHeader confidence="0.99527">
4.2 L1-norm penalty
</subsectionHeader>
<bodyText confidence="0.999743166666666">
Another commonly used penalty term is the L1-
norm of the weight vector. This kinds of terms
usually results in sparse weight vector. Since the
averaged perceptron is used, the final averaged
weight vector will not be sparse.
The loss function using the L1-nrom penalty is
</bodyText>
<equation confidence="0.9981937">
XL = w · ΔΦi + λ1kwk1 (11)
i
XL = w ·
i
wj =
�����
�{i|w[i]
j =6 0, i = 1, · · · , n}
Pn i=1 w[i]
j (8)
</equation>
<bodyText confidence="0.999572">
This equation makes the low-frequency features
more important in the final model.
</bodyText>
<sectionHeader confidence="0.997298" genericHeader="method">
4 Penalty
</sectionHeader>
<bodyText confidence="0.99998447368421">
Adding penalty term to the loss function is a com-
mon and traditional regularization method to avoid
over-fitting. It is widely used for the optimization
problems of logistic regression, support vector
machine, conditional random field and other mod-
els. Penalty terms for probabilistic models can be
interpreted as a prior over the weights (Chen and
Rosenfeld, 1999). It is also called “weight decay”
in artificial neural network (Moody et al., 1995).
The use of the penalty term is to prevent the com-
ponents of the weight vector to become too large.
In Section 2 we have modeled the perceptron al-
gorithm as an SGD algorithm with an explicit loss
function, the additional penalty term is therefore
easy to be employed.
where λ1 is the hyper-parameter to determine the
strength of the penalty.
The derivative of the penalty term is discontin-
uous. We update the weights as
</bodyText>
<equation confidence="0.980091142857143">
max{0, |w(t)
(t+1) i  |− ηλ1}
i ← wi − ηΔφ(t)
(t)
|w(t) i
i |
(12)
</equation>
<bodyText confidence="0.999064">
This ensures that the weight decay will not change
the sign of the weight.
An modified version of the L1 penalty for the
online learning is the cumulative L1 penalty (Tsu-
ruoka et al., 2009), which is used to make the
stochastic gradient of the penalty term more close
to the true gradient. The update is divided into two
steps. In the first step, the weight vector is updated
according to the loss function without the penalty
term
</bodyText>
<equation confidence="0.994459">
(t+ 2 1 )
w← w(t)
i − ηΔφ(t) (13)
i i
</equation>
<bodyText confidence="0.980192">
And the cumulative penalty is calculated sepa-
rately
</bodyText>
<equation confidence="0.9878294">
ci ←
c
(t+2) i+ ηλ1
(t) (14)
w
</equation>
<page confidence="0.971585">
166
</page>
<bodyText confidence="0.999119333333333">
In the second step, |wi |and ci are compared and
at most one of them is non-zero before the next
update
</bodyText>
<equation confidence="0.988901166666667">
m +- mint |wi 2)|, c(t+2)} (15)
w(t+1) � max{0,|w�ti2)|−m}w(t+21) (16)
i (t+2 ) i
wi
c(t+1) c(t+2) — m (17)
i
</equation>
<sectionHeader confidence="0.994622" genericHeader="method">
5 Dropout
</sectionHeader>
<bodyText confidence="0.999892272727273">
Dropout (Hinton et al., 2012) is originally a regu-
larization method used for the artificial neural net-
work. It corrupts one or more layers of a feed-
forward network during training, by randomly
omitting some of the neurons. If the input layer
is corrupted during the training of an autoencoder,
the model is called denoising autoencoder (Vin-
cent et al., 2008).
The reason why such treatment can regularize
the parameters are explained in different ways.
Hinton et al. (2012) argued that the final model
is an average of a large number of models and the
dropout forces the model to learn good features
which are less co-adapted. Vincent et al. (2008)
argued that by using dropout of the input layer, the
model can learn how to deal with examples out-
side the low-dimensional manifold that the train-
ing data concentrate.
Models not so deep such as the structured per-
ceptron may also benefit from this idea. Follow-
ing the dropout method used in neural network, we
give the similar method for structured perceptron.
</bodyText>
<subsectionHeader confidence="0.992987">
5.1 Input Layer
</subsectionHeader>
<bodyText confidence="0.9998154">
We can perform dropout for structured perceptron
by corrupting the input layer in Figure 1. Since
we concern that what y exactly is, we only corrupt
x. The components of the corrupted vector x˜ is
calculated as
</bodyText>
<equation confidence="0.919359">
˜xi = xini (18)
</equation>
<bodyText confidence="0.99987025">
where ni — Bern(p) obey a Binomial distribution
with the hyper-parameter p.
During training, the decoding processing with
the corrupted input is
</bodyText>
<equation confidence="0.9924335">
z = arg max w · Φ(˜x, z) (19)
Z
</equation>
<bodyText confidence="0.997200277777778">
The x in the loss function is also substituted with
the corrupted version ˜x.
Note that the corruption decreases the number
of non-zero components of the feature vector Φ,
which makes the decoding algorithm harder to find
the gold standard y.
For NLP tasks, the input vector x could be a
sequence of tokens (words, POS tags, etc.). The
corruption substitutes some of the tokens with a
special token null. Any features contain such to-
ken will be omitted (This is also the case for the
out-of-vocabulary words during predicting). So
the dropout of x in NLP during training can be
explained as to randomly mask some of the input
tokens. The decoder algorithm needs to find out
the correct answer even if some parts of the input
are unseen. This harder situation could force the
learning algorithm to learn better models.
</bodyText>
<subsectionHeader confidence="0.998485">
5.2 Hidden Layer
</subsectionHeader>
<bodyText confidence="0.999896">
The dropout can also be performed at the hidden
layer. Likewise, the components of the corrupted
feature vector Φ˜ is calculated as
</bodyText>
<equation confidence="0.987587">
˜φi = φimi(20)
</equation>
<bodyText confidence="0.9999225">
where mi — Bern(q) obey a Binomial distribution
with the hyper-parameter q.
The Φ in the decoding processing during train-
ing and the loss function is substituted with ˜Φ.
</bodyText>
<sectionHeader confidence="0.999815" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999865333333333">
In this section, we first introduce three NLP tasks
using structured perceptron namely Chinese word
segmentation, POS tagging and dependency pars-
ing. Then we investigate the effects of regular-
ization methods for structured perceptron mainly
on the development set of character-based Chinese
word segmentation. Finally, we compare the final
performances on the test sets of these three tasks
using regularization methods with related work.
</bodyText>
<subsectionHeader confidence="0.959786">
6.1 Tasks
6.1.1 Chinese Word Segmentation
</subsectionHeader>
<bodyText confidence="0.9996909">
A Chinese word consists of one or more Chinese
characters. But there is no spaces in the sentences
to indicating words. Chinese word segmentation
is the task to segment words in the sentence.
We use a character-based Chinese word seg-
mentation model as the baseline. Like part-of-
speech tagging which is to assign POS tags to
words sequence, character-based Chinese word
segmentation is to assign tags to character se-
quence. The tag set of four tags is commonly used:
</bodyText>
<page confidence="0.983024">
167
</page>
<table confidence="0.697245">
Type Templates
Unigram (xi−1, yi), (xi, yi), (xi+1, yi)
Bigram (xi−2, xi−1, yi), (xi−1, xi, yi)
(xi, xi+1, yi), (xi+1, xi+2, yi)
transition (yi−1, yi)
</table>
<tableCaption confidence="0.977178">
Table 1: Feature templates for the character-
</tableCaption>
<bodyText confidence="0.899753">
based Chinese word segmentation model and the
joint Chinese word segmentation and POS tagging
model.
tag S indicates that the character forms a single-
character words; tag B / E indicates that the char-
acter is at the beginning / end of a multi-character
words; tag M indicates that the character is in the
middle of a multi-character words.
For example, if the tag sequence for the input
</bodyText>
<equation confidence="0.998538333333333">
x = MX&amp;quot;�R`IA) (21)
is
y = BMESBE, (22)
</equation>
<bodyText confidence="0.951971111111111">
the corresponding segmentation result is
&amp;I&apos;M 0`1 X). (23)
Table 1 shows the set of the feature templates
which is a subset of some related work (Ng and
Low, 2004; Jiang et al., 2009) .
Following Sun (2011), we split the Chinese
treebank 5 into training set, development set and
test set. F-measure (Emerson, 2005) is used as the
measurement of the performance.
</bodyText>
<subsubsectionHeader confidence="0.755098">
6.1.2 Part-of-Speech Tagging
</subsubsectionHeader>
<bodyText confidence="0.9998515">
The second task is joint Chinese word segmenta-
tion and POS tagging. This can also be modeled
as a character-based sequence labeling task.
The tag set is a Cartesian product of the tag set
for Chinese word segmentation and the set of POS
tags. For example, the tag B-NN indicates the
character is the first character of a multi-character
noun. The tag sequence
</bodyText>
<equation confidence="0.8523505">
y = B-NR M-NR E-NR S-DEG B-NN E-NN,
(24)
</equation>
<bodyText confidence="0.648561">
for the input sentence in Equation (21) results in
</bodyText>
<sectionHeader confidence="0.320128" genericHeader="method">
1X&amp;quot;QCs NR R`, DEG X) NN. (25)
</sectionHeader>
<bodyText confidence="0.98690025">
The same feature templates shown in Table 1
are used for joint Chinese word segmentation and
POS tagging.
Also, we use the same training set, development
set and test set based on CTB5 corpus as the Chi-
nese word segmentation task. F-measure for joint
Chinese word segmentation and POS tagging is
used as the measurement of the performance.
</bodyText>
<subsubsectionHeader confidence="0.590369">
6.1.3 Dependency Parsing
</subsubsectionHeader>
<bodyText confidence="0.999943652173913">
The syntactical parsing tasks are different with
previously introduced tagging tasks. To investi-
gate the effects of regularization methods on the
parsing tasks, we fully re-implement the linear-
time incremental shift-reduce dependency parser
by Huang and Sagae (2010). The structure per-
ceptron is used to train such model. The model
totally employs 28 feature templates proposed by
Huang and Sagae (2010).
Since the search space for parsing tasks is quite
larger than the search space for tagging tasks, Ex-
act search algorithms such as dynamic program-
ming can not be used. Besides, beam search with
state merging is used for decoding. The early up-
date strategy (Collins and Roark, 2004) is also em-
ployed.
In order to compare to the related work, un-
like the Chinese word segmentation and the POS
tagging task, we split the CTB5 corpus follow-
ing Zhang et al.(2008). Two types of accuracies
are used to measure the performances, namely
word and complete match (excluding punctua-
tions) (Huang and Sagae, 2010).
</bodyText>
<subsectionHeader confidence="0.999411">
6.2 Averaging
</subsectionHeader>
<bodyText confidence="0.9988414">
First, we investigate the effect of averaging tech-
niques for regularization. Figure 2 shows the in-
fluence of the number of the averaged models by
using the “shuffle-and-average” method described
in section 3.2. The performances of the Chinese
word segmentation, POS tagging and parsing tasks
are all increased by averaging models trained with
the same training data with different orders. The
“shuffle-and-average” method is effective to re-
duce the bias caused by the specific order of the
training examples.
For the Chinese word segmentation task which
is a relatively simple task, averaging about five dif-
ferent models can achieve the best effect; whereas
for POS tagging and parsing, averaging more
models will continually increase the performance
even when the number of models approaches 10.
The dotted lines in Figure 2 indicate the perfor-
mances by using Equation (7) for model averag-
ing. The solid lines indicate the performances by
</bodyText>
<page confidence="0.985279">
168
</page>
<figure confidence="0.996436">
(a) Chinese word segmentation (b) POS tagging (c) Dependency parsing
</figure>
<figureCaption confidence="0.994602">
Figure 2: The influence of the number of the averaged models using the “shuffle-and-average” method
for (a) Chinese word segmentation, (b) POS tagging and (c) dependency parsing. “Shuffle” means to
only average the non-zero weights (Equation (8)), while “Shuffle (average all)” means to average all
weights (Equation (7)).
</figureCaption>
<bodyText confidence="0.9773546">
using Equation (8) for model averaging. Accord-
ing to these three different tasks, Equation (8) al-
ways performs better than Equation (7). We will
use Equation (8) denoted as “Shuffle” for the rest
of the experiments.
</bodyText>
<subsectionHeader confidence="0.995346">
6.3 Penalty
</subsectionHeader>
<bodyText confidence="0.999865933333333">
Here we investigate the penalty techniques for reg-
ularization only using the character-based Chinese
word segmentation task.
Figure 3 shows the effect of adding L1-norm
and L2-norm penalty terms to the loss function.
With appropriate hyper-parameters, the perfor-
mances are increased. According to the per-
formances, adding L2 penalty is slightly better
than adding L1 penalty or adding cumulative L1
penalty.
We then combine the “shuffle-and-average”
method with the penalty methods. The perfor-
mances (solid lines in Figure 3) are further im-
proved and are better than those of models that
only use one regularization method.
</bodyText>
<subsectionHeader confidence="0.980617">
6.4 Dropout
</subsectionHeader>
<bodyText confidence="0.923659">
We also investigate the dropout method for regu-
larization using the character-based Chinese word
segmentation task.
Figure 4 shows the effect of the dropout method
(“dropout” for the input layer and “dropout (Φ)”
for the hidden layer) and the combination of the
dropout and “shuffle-and-average” method (solid
line). We observed that the dropout for the hid-
den layer is not effective for structured perceptron.
This may caused by that the connections between
the input layer and the hidden layer are fixed dur-
ing training. Neurons in the hidden layer can not
Figure 4: Influences of the hyper-parameter p (for
the input layer, denoted as “dropout”) or q (for
the hidden layer, denoted as “dropout (Φ)”) for the
dropout method.
changes the weights to learn different representa-
tions for the input layer. On the other hand, the
dropout for the input layer improves the perfor-
mance. Combining the dropout and the “shuffle-
and-average” method, the performance is further
improved.
Figure 5 shows the effect of the combination of
the three regularization methods. We see that no
matter what other regularization methods are al-
ready used, adding “shuffle-and-average” method
can always improve the performance. The effects
of the penalty method and the dropout method
have some overlap, since combining these two
method does not result in a significant improve-
ment of the performance.
</bodyText>
<subsectionHeader confidence="0.975758">
6.5 Final Results
6.5.1 Chinese Word Segmentation
</subsectionHeader>
<bodyText confidence="0.99830125">
Table 2 shows the final results of the character-
based Chinese word segmentation task on the test
set of the CTB5 corpus.
Structure perceptron with feature templates in
</bodyText>
<page confidence="0.991499">
169
</page>
<figure confidence="0.99278">
(a) L2-norm penalty (b) L1-norm penalty
</figure>
<figureCaption confidence="0.9937144">
Figure 3: influence of the hyper-parameter A2 in the L2-norm penalty term and A1 in the L1-norm penalty
term (“l1-c” indicates the cumulative L1 penalty) for the character-based Chinese word segmentation
task.
Figure 5: The combination of these three regular-
ization methods.
</figureCaption>
<bodyText confidence="0.993994772727273">
Table 1 is used. We use the “shuffle-and-average”
(5 models), the L2 penalty method (A2 = 10−4),
the dropout method (p = 3%) and their combina-
tions to regularize the structured perceptron.
To compare with the perceptron algorithm, we
use the conditional random field model (CRF)
with the same feature templates in Table 1 to train
the model parameters. The toolkit CRF++1 with
the L2-norm penalty is used to train the weights.
The hyper-parameter C = 20 is tuned using the
development set.
Jiang et al. (2009) proposed a character-based
model employing similar feature templates using
averaged perceptron. The feature templates are
following Ng and Low (2004). Zhang and Clark
(2011) proposed a word-based model employing
both character-based features and more sophis-
ticated word-based features using also averaged
perceptron. There are other related results (Jiang
et al., 2012) of open test including the final result
of Jiang et al. (2009). Since their models used
extra resources, they are not comparable with the
</bodyText>
<footnote confidence="0.976832">
1http://crfpp.googlecode.com/svn/trunk/doc/index.html
</footnote>
<table confidence="0.999513166666667">
sf
(Jiang et al., 2009) 0.9735
(Zhang and Clark, 2011) 0.9750†
CRF++ (C = 20) 0.9742
Averaged Percetron 0.9734
+ Shuffle 0.9755
+ L2 0.9736
+ L2 + Shuffle 0.9772
+ Dropout 0.9741
+ Dropout+ Shuffle 0.9765
+ L2 + Dropout 0.9749
+ L2 + Dropout+ Shuffle 0.9771
</table>
<tableCaption confidence="0.987471666666667">
Table 2: Final results of the character-based Chi-
nese word segmentation task on CTB5. † This re-
sult is read from a figure in that paper.
</tableCaption>
<table confidence="0.9996224">
sf
Word-based model 0.9758
+ Shuffle 0.9787
+ L2 + Shuffle 0.9791
+ L2 + Dropout+ Shuffle 0.9791
</table>
<tableCaption confidence="0.976296">
Table 3: Final results of the word-based Chinese
word segmentation task on CTB5.
</tableCaption>
<bodyText confidence="0.971078545454545">
results in this paper.
The results in Table 2 shows that with proper
regularization methods, the models trained using
perceptron algorithm can outperform CRF models
with the same feature templates and other models
with more sophisticated features trained using the
averaged perceptron without other regularization
methods.
We further re-implemented a word-based Chi-
nese word segmentation model with the feature
templates following Zhang et al. (2012), which
</bodyText>
<page confidence="0.988406">
170
</page>
<table confidence="0.9984907">
sf jf
(Jiang et al., 2008) 0.9785 0.9341
(Kruengkrai et al., 2009) 0.9787 0.9367
(Zhang and Clark, 2010) 0.9778 0.9367
(Sun, 2011) 0.9817 0.9402
Character-based model 0.9779 0.9336
+ Shuffle 0.9802 0.9375
+ Dropout 0.9789 0.9361
+ Dropout+ Shuffle 0.9809 0.9407
+ word-based re-ranking 0.9813 0.9438
</table>
<tableCaption confidence="0.9817105">
Table 4: Final results of the POS tagging task on
CTB5.
</tableCaption>
<table confidence="0.999017333333333">
word compl.
(Huang and Sagae, 2010) 85.20 33.72
our re-implementation 85.22 34.15
+ Shuffle 85.65 34.52
+ Dropout 85.32 34.04
+ Dropout+ Shuffle 85.71 34.57
</table>
<tableCaption confidence="0.9732515">
Table 5: Final results of the dependency parsing
task on CTB5.
</tableCaption>
<bodyText confidence="0.999771714285714">
is similar with the model proposed by Zhang and
Clark (2011). Beam search with early-update is
used for decoding instead of dynamic program-
ming. The results with different regularization
methods are shown in Figure 3. These regulariza-
tion methods show similar characteristics for the
word-based model.
</bodyText>
<subsectionHeader confidence="0.687342">
6.5.2 POS Tagging
</subsectionHeader>
<bodyText confidence="0.999992258064516">
The results of the POS tagging models on the
CTB5 corpus are shown in Table 4. Structure per-
ceptron with feature templates in Table 1 is used.
The F-measures for word segmentation (sf) and
for joint word segmentation and POS tagging (jf)
are listed.
We use the “shuffle-and-average” (10 models),
the dropout method (p = 5%) and their combina-
tion to regularize the structured perceptron.
Jiang et al. (2008) used a character-based
model using perceptron for POS tagging and a
log-linear model for re-ranking. Kruengkrai et
al. (2009) proposed a hybrid model including
character-based and word-based features. Zhang
and Clark (2010) proposed a word-based model
using perceptron. Sun (2011) proposed a frame-
work based on stacked learning consisting of four
sub-models. For the closed test, this model has
the best performance on the CTB5 corpus to our
knowledge. Other results (Wang et al., 2011; Sun
and Wan, 2012) for the open test are not listed
since they are not comparable with the results in
this paper.
If we define the error rate as 1 − jf, the error re-
duction by applying regularization methods for the
character-based model is more than 10%. Com-
paring to the related work, the character-based
model that we used is quite simple. But using
the regularization methods discussed in this paper,
it provides a comparable performance to the best
model in the literature.
</bodyText>
<subsectionHeader confidence="0.490801">
6.5.3 Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.999964666666667">
Table 5 shows the final results of the dependency
parsing task on the CTB5 corpus. We use the
“shuffle-and-average” (10 models), the dropout
method (p = 5% only for the words in the input)
and their combination to regularize the structured
perceptron based on Huang and Sagae’s (2010).
The performance of the parsing model is also
improved by using more regularization methods,
although the improvement is not as remarkable
as those for tagging tasks. For the parsing tasks,
there are many other factors that impact the per-
formance.
</bodyText>
<sectionHeader confidence="0.999173" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999229318181818">
The “shuffle-and-average” method can effectively
reduce the bias caused by the specific order of the
training examples. It can improve the performance
even if some other regularization methods are ap-
plied.
When we treat the perceptron algorithm as a
special case of the SGD algorithm, the traditional
penalty methods can be applied. And our observa-
tion is that L2 penalty is better than L1 penalty.
The dropout method is derived from the neural
network. Corrupting the input during training im-
proves the ability of generalization. The effects of
the penalty method and the dropout method have
some overlap.
Experiments showed that these regularization
methods help different NLP tasks such as Chinese
word segmentation, POS tagging and dependency
parsing. Applying proper regularization methods,
the error reductions for some of these NLP tasks
can be up to 10%. We believe that these meth-
ods can also help other models which are based on
structured perceptron.
</bodyText>
<page confidence="0.997642">
171
</page>
<sectionHeader confidence="0.999212" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997164571428572">
The authors want to thanks all the reviews for
many pertinent comments which have improved
the quality of this paper. The authors are supported
by NSFC (No. 61273338 and No. 61303082), the
Doctoral Program of Higher Education of China
(No. 20120121120046) and China Postdoctoral
Science Foundation (No. 2013M541861).
</bodyText>
<sectionHeader confidence="0.998739" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999219554455446">
Stanley F Chen and Ronald Rosenfeld. 1999. A gaus-
sian prior for smoothing maximum entropy models.
Technical report, DTIC Document.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of the 42nd Meeting of the Association for Com-
putational Linguistics (ACL’04), Main Volume, page
111118, Barcelona, Spain, July.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. pages 1–8.
Chuong B Do, Quoc V Le, and Chuan-Sheng Foo.
2009. Proximal regularization for online and batch
learning. In Proceedings of the 26th Annual Interna-
tional Conference on Machine Learning, pages 257–
264. ACM.
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proceedings of
the Fourth SIGHAN Workshop on Chinese Language
Processing, pages 123–133. Jeju Island, Korea.
Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhut-
dinov. 2012. Improving neural networks by
preventing co-adaptation of feature detectors. arXiv
preprint arXiv:1207.0580.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1077–
1086, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
142–151. Association for Computational Linguis-
tics.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan
Lü. 2008. A cascaded linear model for joint chi-
nese word segmentation and part-of-speech tagging.
In Proceedings of ACL-08: HLT, pages 897–904,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic adaptation of annotation standards: Chinese
word segmentation and POS tagging - a case study.
In Proceedings of the 47th ACL, pages 522–530,
Suntec, Singapore, August. Association for Compu-
tational Linguistics.
Wenbin Jiang, Fandong Meng, Qun Liu, and Yajuan
Lü. 2012. Iterative annotation transformation with
predict-self reestimation for chinese word segmen-
tation. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 412–420, Jeju Island, Korea, July.
Association for Computational Linguistics.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hy-
brid model for joint chinese word segmentation and
POS tagging. In Proc. of ACL-IJCNLP 2009, pages
513–521, Suntec, Singapore. Association for Com-
putational Linguistics.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 456–464. Association for Computa-
tional Linguistics.
JE Moody, SJ Hanson, Anders Krogh, and John A
Hertz. 1995. A simple weight decay can improve
generalization. Advances in neural information pro-
cessing systems, 4:950–957.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 277–284, Barcelona, Spain, July. Association
for Computational Linguistics.
Weiwei Sun and Xiaojun Wan. 2012. Reducing ap-
proximation and estimation errors for chinese lexi-
cal processing with heterogeneous annotations. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 232–241, Jeju Island, Korea,
July. Association for Computational Linguistics.
Weiwei Sun. 2011. A stacked sub-word model for
joint chinese word segmentation and part-of-speech
tagging. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1385–
1394, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Yoshimasa Tsuruoka, Jun’ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent train-
ing for l1-regularized log-linear models with cumu-
lative penalty. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
</reference>
<page confidence="0.976542">
172
</page>
<reference confidence="0.999516775862069">
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 1-Volume
1, pages 477–485. Association for Computational
Linguistics.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and
Pierre-Antoine Manzagol. 2008. Extracting and
composing robust features with denoising autoen-
coders. In Proceedings of the 25th international
conference on Machine learning, page 10961103.
Yiou Wang, Jun’ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Torisawa.
2011. Improving chinese word segmentation and
POS tagging with semi-supervised methods using
large auto-analyzed data. In Proceedings of 5th In-
ternational Joint Conference on Natural Language
Processing, pages 309–317, Chiang Mai, Thailand,
November. Asian Federation of Natural Language
Processing.
Lin Xiao. 2010. Dual averaging methods for reg-
ularized stochastic learning and online optimiza-
tion. The Journal of Machine Learning Research,
9999:2543–2596.
Luke S Zettlemoyer and Michael Collins. 2009.
Learning context-dependent mappings from sen-
tences to logical form. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP: Volume 2-
Volume 2, pages 976–984. Association for Compu-
tational Linguistics.
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, page 562571,
Honolulu, Hawaii, October. Association for Compu-
tational Linguistics.
Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and POS-Tagging using
a single discriminative model. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 843–852, Cambridge,
MA, October. Association for Computational Lin-
guistics.
Y. Zhang and S. Clark. 2011. Syntactic processing
using the generalized perceptron and beam search.
Computational Linguistics, (Early Access):1–47.
Kaixu Zhang, Maosong Sun, and Changle Zhou. 2012.
Word segmentation on chinese mirco-blog data with
a linear-time incremental model. In Proceedings of
the Second CIPS-SIGHAN Joint Conference on Chi-
nese Language Processing, pages 41–46, Tianjin,
China, December. Association for Computational
Linguistics.
Kai Zhao and Liang Huang. 2013. Minibatch and par-
allelization for online large margin structured learn-
ing. In Proceedings of NAACL-HLT, pages 370–
379.
</reference>
<page confidence="0.999102">
173
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.903578">
<title confidence="0.998738">Regularized Structured Perceptron: A Case Study on Chinese Word Segmentation, POS Tagging and Parsing</title>
<author confidence="0.994683">Kaixu Zhang Jinsong Su Changle Zhou</author>
<affiliation confidence="0.999998">Xiamen University Xiamen University Xiamen University</affiliation>
<address confidence="0.983923">Fujian, P.R. China Fujian, P.R. China Fujian, P.R. China</address>
<email confidence="0.964368">kareyzhang@gmail.comjssu@xmu.edu.cndozero@xmu.edu.cn</email>
<abstract confidence="0.998092045454545">Structured perceptron becomes popular for various NLP tasks such as tagging and parsing. Practical studies on NLP did not pay much attention to its regularization. In this paper, we study three simple but effective task-independent regularization methods: (1) one is to average weights of different trained models to reduce the bias caused by the specific order of the training examples; (2) one is to add penalty term to the loss function; (3) and one is to randomly corrupt the data flow during training which is called dropout in the neural network. Experiments are conducted on three NLP tasks, namely Chinese word segmentation, part-of-speech tagging and dependency parsing. Applying proper regularization methods or their combinations, the error reductions with respect to the averaged perceptron for some of these tasks can be up to 10%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>A gaussian prior for smoothing maximum entropy models.</title>
<date>1999</date>
<tech>Technical report, DTIC Document.</tech>
<contexts>
<context position="9622" citStr="Chen and Rosenfeld, 1999" startWordPosition="1637" endWordPosition="1640">l not be sparse. The loss function using the L1-nrom penalty is XL = w · ΔΦi + λ1kwk1 (11) i XL = w · i wj = ����� �{i|w[i] j =6 0, i = 1, · · · , n} Pn i=1 w[i] j (8) This equation makes the low-frequency features more important in the final model. 4 Penalty Adding penalty term to the loss function is a common and traditional regularization method to avoid over-fitting. It is widely used for the optimization problems of logistic regression, support vector machine, conditional random field and other models. Penalty terms for probabilistic models can be interpreted as a prior over the weights (Chen and Rosenfeld, 1999). It is also called “weight decay” in artificial neural network (Moody et al., 1995). The use of the penalty term is to prevent the components of the weight vector to become too large. In Section 2 we have modeled the perceptron algorithm as an SGD algorithm with an explicit loss function, the additional penalty term is therefore easy to be employed. where λ1 is the hyper-parameter to determine the strength of the penalty. The derivative of the penalty term is discontinuous. We update the weights as max{0, |w(t) (t+1) i |− ηλ1} i ← wi − ηΔφ(t) (t) |w(t) i i | (12) This ensures that the weight </context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>Stanley F Chen and Ronald Rosenfeld. 1999. A gaussian prior for smoothing maximum entropy models. Technical report, DTIC Document.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>111118</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="1382" citStr="Collins and Roark, 2004" startWordPosition="206" endWordPosition="209">and one is to randomly corrupt the data flow during training which is called dropout in the neural network. Experiments are conducted on three NLP tasks, namely Chinese word segmentation, part-of-speech tagging and dependency parsing. Applying proper regularization methods or their combinations, the error reductions with respect to the averaged perceptron for some of these tasks can be up to 10%. 1 Introduction Structured perceptron is a linear classification algorithm. It is used for word segmentation (Zhang and Clark, 2011), POS (part-of-speech) tagging (Collins, 2002), syntactical parsing (Collins and Roark, 2004), semantical parsing (Zettlemoyer and Collins, 2009) and other NLP tasks. The averaged perceptron or the voted perceptron (Collins, 2002) is proposed for better generalization. Early update (Collins and Roark, 2004; Huang et al., 2012) is used for inexact decoding algorithms such as the beam search. Distributed training (McDonald et al., 2010) and the minibatch and parallelization method (Zhao and Huang, 2013) are recently proposed. Some other related work focuses on the task-specified feature engineering. Regularization is to improve the ability of generalization and avoid over-fitting for ma</context>
<context position="17066" citStr="Collins and Roark, 2004" startWordPosition="2930" endWordPosition="2933">ntroduced tagging tasks. To investigate the effects of regularization methods on the parsing tasks, we fully re-implement the lineartime incremental shift-reduce dependency parser by Huang and Sagae (2010). The structure perceptron is used to train such model. The model totally employs 28 feature templates proposed by Huang and Sagae (2010). Since the search space for parsing tasks is quite larger than the search space for tagging tasks, Exact search algorithms such as dynamic programming can not be used. Besides, beam search with state merging is used for decoding. The early update strategy (Collins and Roark, 2004) is also employed. In order to compare to the related work, unlike the Chinese word segmentation and the POS tagging task, we split the CTB5 corpus following Zhang et al.(2008). Two types of accuracies are used to measure the performances, namely word and complete match (excluding punctuations) (Huang and Sagae, 2010). 6.2 Averaging First, we investigate the effect of averaging techniques for regularization. Figure 2 shows the influence of the number of the averaged models by using the “shuffle-and-average” method described in section 3.2. The performances of the Chinese word segmentation, POS</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, page 111118, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<pages>1--8</pages>
<contexts>
<context position="1335" citStr="Collins, 2002" startWordPosition="202" endWordPosition="203">nalty term to the loss function; (3) and one is to randomly corrupt the data flow during training which is called dropout in the neural network. Experiments are conducted on three NLP tasks, namely Chinese word segmentation, part-of-speech tagging and dependency parsing. Applying proper regularization methods or their combinations, the error reductions with respect to the averaged perceptron for some of these tasks can be up to 10%. 1 Introduction Structured perceptron is a linear classification algorithm. It is used for word segmentation (Zhang and Clark, 2011), POS (part-of-speech) tagging (Collins, 2002), syntactical parsing (Collins and Roark, 2004), semantical parsing (Zettlemoyer and Collins, 2009) and other NLP tasks. The averaged perceptron or the voted perceptron (Collins, 2002) is proposed for better generalization. Early update (Collins and Roark, 2004; Huang et al., 2012) is used for inexact decoding algorithms such as the beam search. Distributed training (McDonald et al., 2010) and the minibatch and parallelization method (Zhao and Huang, 2013) are recently proposed. Some other related work focuses on the task-specified feature engineering. Regularization is to improve the ability </context>
<context position="6163" citStr="Collins, 2002" startWordPosition="1007" endWordPosition="1008">aining process is arg min W L(w) (4) The loss function isnot convex but calculating the derivative is easy. One of the algorithms to solve this optimization problem is SGD. Here we use the minibatch with size of 1, which means in every iteration we use only one training example to approximate the loss function and the gradient to update the weight vector: w(t+1) ← w(t) − η ∂L ≈ w(t) − ηA4&apos;(t) ∂w W(t) (5) where w(t) is the weight vector after t updates. Note that in this case, the learning rate η can be set to an arbitrary positive real number. In the perceptron algorithm commonly used in NLP (Collins, 2002) , η is not changed respect to t. We fix η to be 1 in this paper without loss of generality. 3 Averaging 3.1 Averaged Perceptron Averaging the weight vectors in the learning process is one of the most popular regularization techniques of the structured perceptron (Collins, 2002). And it is also the only used regularization technique for many practical studies on NLP (Jiang et al., 2009; Huang and Sagae, 2010). Suppose the learning algorithm stopped after T updates. The final weight vector is calculated as: w(t) (6) The intuition might be that the learned weight vector is dependent on the order</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms. pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chuong B Do</author>
<author>Quoc V Le</author>
<author>Chuan-Sheng Foo</author>
</authors>
<title>Proximal regularization for online and batch learning.</title>
<date>2009</date>
<booktitle>In Proceedings of the 26th Annual International Conference on Machine Learning,</booktitle>
<pages>257--264</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2061" citStr="Do et al., 2009" startWordPosition="308" endWordPosition="311">LP tasks. The averaged perceptron or the voted perceptron (Collins, 2002) is proposed for better generalization. Early update (Collins and Roark, 2004; Huang et al., 2012) is used for inexact decoding algorithms such as the beam search. Distributed training (McDonald et al., 2010) and the minibatch and parallelization method (Zhao and Huang, 2013) are recently proposed. Some other related work focuses on the task-specified feature engineering. Regularization is to improve the ability of generalization and avoid over-fitting for machine learning algorithms including online learning algorithms (Do et al., 2009; Xiao, 2010). But practical studies on NLP did not pay much attention to the regularization of the structured perceptron. As a result, for some tasks the model learned using perceptron algorithm is not as good as the model learned using regularized condition random field. In this paper, we treat the perceptron algorithm as a special case of the stochastic gradient descent (SGD) algorithm and study three kinds of simple but effective task-independent regularization methods that can be applied. The averaging method is to average the weight vectors of different models. We propose a “shuffle-and-</context>
</contexts>
<marker>Do, Le, Foo, 2009</marker>
<rawString>Chuong B Do, Quoc V Le, and Chuan-Sheng Foo. 2009. Proximal regularization for online and batch learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 257– 264. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Emerson</author>
</authors>
<title>The second international chinese word segmentation bakeoff.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>123--133</pages>
<location>Jeju Island,</location>
<contexts>
<context position="15457" citStr="Emerson, 2005" startWordPosition="2662" endWordPosition="2663">character forms a singlecharacter words; tag B / E indicates that the character is at the beginning / end of a multi-character words; tag M indicates that the character is in the middle of a multi-character words. For example, if the tag sequence for the input x = MX&amp;quot;�R`IA) (21) is y = BMESBE, (22) the corresponding segmentation result is &amp;I&apos;M 0`1 X). (23) Table 1 shows the set of the feature templates which is a subset of some related work (Ng and Low, 2004; Jiang et al., 2009) . Following Sun (2011), we split the Chinese treebank 5 into training set, development set and test set. F-measure (Emerson, 2005) is used as the measurement of the performance. 6.1.2 Part-of-Speech Tagging The second task is joint Chinese word segmentation and POS tagging. This can also be modeled as a character-based sequence labeling task. The tag set is a Cartesian product of the tag set for Chinese word segmentation and the set of POS tags. For example, the tag B-NN indicates the character is the first character of a multi-character noun. The tag sequence y = B-NR M-NR E-NR S-DEG B-NN E-NN, (24) for the input sentence in Equation (21) results in 1X&amp;quot;QCs NR R`, DEG X) NN. (25) The same feature templates shown in Table</context>
</contexts>
<marker>Emerson, 2005</marker>
<rawString>Thomas Emerson. 2005. The second international chinese word segmentation bakeoff. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, pages 123–133. Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
<author>Nitish Srivastava</author>
<author>Alex Krizhevsky</author>
<author>Ilya Sutskever</author>
<author>Ruslan R Salakhutdinov</author>
</authors>
<title>Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580.</title>
<date>2012</date>
<contexts>
<context position="11003" citStr="Hinton et al., 2012" startWordPosition="1901" endWordPosition="1904"> which is used to make the stochastic gradient of the penalty term more close to the true gradient. The update is divided into two steps. In the first step, the weight vector is updated according to the loss function without the penalty term (t+ 2 1 ) w← w(t) i − ηΔφ(t) (13) i i And the cumulative penalty is calculated separately ci ← c (t+2) i+ ηλ1 (t) (14) w 166 In the second step, |wi |and ci are compared and at most one of them is non-zero before the next update m +- mint |wi 2)|, c(t+2)} (15) w(t+1) � max{0,|w�ti2)|−m}w(t+21) (16) i (t+2 ) i wi c(t+1) c(t+2) — m (17) i 5 Dropout Dropout (Hinton et al., 2012) is originally a regularization method used for the artificial neural network. It corrupts one or more layers of a feedforward network during training, by randomly omitting some of the neurons. If the input layer is corrupted during the training of an autoencoder, the model is called denoising autoencoder (Vincent et al., 2008). The reason why such treatment can regularize the parameters are explained in different ways. Hinton et al. (2012) argued that the final model is an average of a large number of models and the dropout forces the model to learn good features which are less co-adapted. Vi</context>
</contexts>
<marker>Hinton, Srivastava, Krizhevsky, Sutskever, Salakhutdinov, 2012</marker>
<rawString>Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov. 2012. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kenji Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1077--1086</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="6575" citStr="Huang and Sagae, 2010" startWordPosition="1078" endWordPosition="1081">(5) where w(t) is the weight vector after t updates. Note that in this case, the learning rate η can be set to an arbitrary positive real number. In the perceptron algorithm commonly used in NLP (Collins, 2002) , η is not changed respect to t. We fix η to be 1 in this paper without loss of generality. 3 Averaging 3.1 Averaged Perceptron Averaging the weight vectors in the learning process is one of the most popular regularization techniques of the structured perceptron (Collins, 2002). And it is also the only used regularization technique for many practical studies on NLP (Jiang et al., 2009; Huang and Sagae, 2010). Suppose the learning algorithm stopped after T updates. The final weight vector is calculated as: w(t) (6) The intuition might be that the learned weight vector is dependent on the order of the training examples. The final vector w(T) may be more appropriate for the last few training examples than the previous ones. The averaging method is used to avoid such tendency. Similar treatment is used in other sequential algorithm such as the Markov chain Monte Carlo sampling method. Since this regularization technique is widely used and tested, it is used for all the models in the experiments of th</context>
<context position="16647" citStr="Huang and Sagae (2010)" startWordPosition="2858" endWordPosition="2861">feature templates shown in Table 1 are used for joint Chinese word segmentation and POS tagging. Also, we use the same training set, development set and test set based on CTB5 corpus as the Chinese word segmentation task. F-measure for joint Chinese word segmentation and POS tagging is used as the measurement of the performance. 6.1.3 Dependency Parsing The syntactical parsing tasks are different with previously introduced tagging tasks. To investigate the effects of regularization methods on the parsing tasks, we fully re-implement the lineartime incremental shift-reduce dependency parser by Huang and Sagae (2010). The structure perceptron is used to train such model. The model totally employs 28 feature templates proposed by Huang and Sagae (2010). Since the search space for parsing tasks is quite larger than the search space for tagging tasks, Exact search algorithms such as dynamic programming can not be used. Besides, beam search with state merging is used for decoding. The early update strategy (Collins and Roark, 2004) is also employed. In order to compare to the related work, unlike the Chinese word segmentation and the POS tagging task, we split the CTB5 corpus following Zhang et al.(2008). Two</context>
<context position="24014" citStr="Huang and Sagae, 2010" startWordPosition="4034" endWordPosition="4037">icated features trained using the averaged perceptron without other regularization methods. We further re-implemented a word-based Chinese word segmentation model with the feature templates following Zhang et al. (2012), which 170 sf jf (Jiang et al., 2008) 0.9785 0.9341 (Kruengkrai et al., 2009) 0.9787 0.9367 (Zhang and Clark, 2010) 0.9778 0.9367 (Sun, 2011) 0.9817 0.9402 Character-based model 0.9779 0.9336 + Shuffle 0.9802 0.9375 + Dropout 0.9789 0.9361 + Dropout+ Shuffle 0.9809 0.9407 + word-based re-ranking 0.9813 0.9438 Table 4: Final results of the POS tagging task on CTB5. word compl. (Huang and Sagae, 2010) 85.20 33.72 our re-implementation 85.22 34.15 + Shuffle 85.65 34.52 + Dropout 85.32 34.04 + Dropout+ Shuffle 85.71 34.57 Table 5: Final results of the dependency parsing task on CTB5. is similar with the model proposed by Zhang and Clark (2011). Beam search with early-update is used for decoding instead of dynamic programming. The results with different regularization methods are shown in Figure 3. These regularization methods show similar characteristics for the word-based model. 6.5.2 POS Tagging The results of the POS tagging models on the CTB5 corpus are shown in Table 4. Structure percep</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1077– 1086, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Suphan Fayong</author>
<author>Yang Guo</author>
</authors>
<title>Structured perceptron with inexact search.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>142--151</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1617" citStr="Huang et al., 2012" startWordPosition="242" endWordPosition="245">ing proper regularization methods or their combinations, the error reductions with respect to the averaged perceptron for some of these tasks can be up to 10%. 1 Introduction Structured perceptron is a linear classification algorithm. It is used for word segmentation (Zhang and Clark, 2011), POS (part-of-speech) tagging (Collins, 2002), syntactical parsing (Collins and Roark, 2004), semantical parsing (Zettlemoyer and Collins, 2009) and other NLP tasks. The averaged perceptron or the voted perceptron (Collins, 2002) is proposed for better generalization. Early update (Collins and Roark, 2004; Huang et al., 2012) is used for inexact decoding algorithms such as the beam search. Distributed training (McDonald et al., 2010) and the minibatch and parallelization method (Zhao and Huang, 2013) are recently proposed. Some other related work focuses on the task-specified feature engineering. Regularization is to improve the ability of generalization and avoid over-fitting for machine learning algorithms including online learning algorithms (Do et al., 2009; Xiao, 2010). But practical studies on NLP did not pay much attention to the regularization of the structured perceptron. As a result, for some tasks the m</context>
</contexts>
<marker>Huang, Fayong, Guo, 2012</marker>
<rawString>Liang Huang, Suphan Fayong, and Yang Guo. 2012. Structured perceptron with inexact search. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 142–151. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
<author>Yajuan Lü</author>
</authors>
<title>A cascaded linear model for joint chinese word segmentation and part-of-speech tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>897--904</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="23649" citStr="Jiang et al., 2008" startWordPosition="3977" endWordPosition="3980">2 + Shuffle 0.9791 + L2 + Dropout+ Shuffle 0.9791 Table 3: Final results of the word-based Chinese word segmentation task on CTB5. results in this paper. The results in Table 2 shows that with proper regularization methods, the models trained using perceptron algorithm can outperform CRF models with the same feature templates and other models with more sophisticated features trained using the averaged perceptron without other regularization methods. We further re-implemented a word-based Chinese word segmentation model with the feature templates following Zhang et al. (2012), which 170 sf jf (Jiang et al., 2008) 0.9785 0.9341 (Kruengkrai et al., 2009) 0.9787 0.9367 (Zhang and Clark, 2010) 0.9778 0.9367 (Sun, 2011) 0.9817 0.9402 Character-based model 0.9779 0.9336 + Shuffle 0.9802 0.9375 + Dropout 0.9789 0.9361 + Dropout+ Shuffle 0.9809 0.9407 + word-based re-ranking 0.9813 0.9438 Table 4: Final results of the POS tagging task on CTB5. word compl. (Huang and Sagae, 2010) 85.20 33.72 our re-implementation 85.22 34.15 + Shuffle 85.65 34.52 + Dropout 85.32 34.04 + Dropout+ Shuffle 85.71 34.57 Table 5: Final results of the dependency parsing task on CTB5. is similar with the model proposed by Zhang and Cl</context>
<context position="24925" citStr="Jiang et al. (2008)" startWordPosition="4182" endWordPosition="4185">oding instead of dynamic programming. The results with different regularization methods are shown in Figure 3. These regularization methods show similar characteristics for the word-based model. 6.5.2 POS Tagging The results of the POS tagging models on the CTB5 corpus are shown in Table 4. Structure perceptron with feature templates in Table 1 is used. The F-measures for word segmentation (sf) and for joint word segmentation and POS tagging (jf) are listed. We use the “shuffle-and-average” (10 models), the dropout method (p = 5%) and their combination to regularize the structured perceptron. Jiang et al. (2008) used a character-based model using perceptron for POS tagging and a log-linear model for re-ranking. Kruengkrai et al. (2009) proposed a hybrid model including character-based and word-based features. Zhang and Clark (2010) proposed a word-based model using perceptron. Sun (2011) proposed a framework based on stacked learning consisting of four sub-models. For the closed test, this model has the best performance on the CTB5 corpus to our knowledge. Other results (Wang et al., 2011; Sun and Wan, 2012) for the open test are not listed since they are not comparable with the results in this paper</context>
</contexts>
<marker>Jiang, Huang, Liu, Lü, 2008</marker>
<rawString>Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lü. 2008. A cascaded linear model for joint chinese word segmentation and part-of-speech tagging. In Proceedings of ACL-08: HLT, pages 897–904, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Automatic adaptation of annotation standards: Chinese word segmentation and POS tagging - a case study.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th ACL,</booktitle>
<pages>522--530</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="6551" citStr="Jiang et al., 2009" startWordPosition="1074" endWordPosition="1077">) − ηA4&apos;(t) ∂w W(t) (5) where w(t) is the weight vector after t updates. Note that in this case, the learning rate η can be set to an arbitrary positive real number. In the perceptron algorithm commonly used in NLP (Collins, 2002) , η is not changed respect to t. We fix η to be 1 in this paper without loss of generality. 3 Averaging 3.1 Averaged Perceptron Averaging the weight vectors in the learning process is one of the most popular regularization techniques of the structured perceptron (Collins, 2002). And it is also the only used regularization technique for many practical studies on NLP (Jiang et al., 2009; Huang and Sagae, 2010). Suppose the learning algorithm stopped after T updates. The final weight vector is calculated as: w(t) (6) The intuition might be that the learned weight vector is dependent on the order of the training examples. The final vector w(T) may be more appropriate for the last few training examples than the previous ones. The averaging method is used to avoid such tendency. Similar treatment is used in other sequential algorithm such as the Markov chain Monte Carlo sampling method. Since this regularization technique is widely used and tested, it is used for all the models </context>
<context position="15326" citStr="Jiang et al., 2009" startWordPosition="2639" endWordPosition="2642"> characterbased Chinese word segmentation model and the joint Chinese word segmentation and POS tagging model. tag S indicates that the character forms a singlecharacter words; tag B / E indicates that the character is at the beginning / end of a multi-character words; tag M indicates that the character is in the middle of a multi-character words. For example, if the tag sequence for the input x = MX&amp;quot;�R`IA) (21) is y = BMESBE, (22) the corresponding segmentation result is &amp;I&apos;M 0`1 X). (23) Table 1 shows the set of the feature templates which is a subset of some related work (Ng and Low, 2004; Jiang et al., 2009) . Following Sun (2011), we split the Chinese treebank 5 into training set, development set and test set. F-measure (Emerson, 2005) is used as the measurement of the performance. 6.1.2 Part-of-Speech Tagging The second task is joint Chinese word segmentation and POS tagging. This can also be modeled as a character-based sequence labeling task. The tag set is a Cartesian product of the tag set for Chinese word segmentation and the set of POS tags. For example, the tag B-NN indicates the character is the first character of a multi-character noun. The tag sequence y = B-NR M-NR E-NR S-DEG B-NN E-</context>
<context position="22025" citStr="Jiang et al. (2009)" startWordPosition="3723" endWordPosition="3726">racter-based Chinese word segmentation task. Figure 5: The combination of these three regularization methods. Table 1 is used. We use the “shuffle-and-average” (5 models), the L2 penalty method (A2 = 10−4), the dropout method (p = 3%) and their combinations to regularize the structured perceptron. To compare with the perceptron algorithm, we use the conditional random field model (CRF) with the same feature templates in Table 1 to train the model parameters. The toolkit CRF++1 with the L2-norm penalty is used to train the weights. The hyper-parameter C = 20 is tuned using the development set. Jiang et al. (2009) proposed a character-based model employing similar feature templates using averaged perceptron. The feature templates are following Ng and Low (2004). Zhang and Clark (2011) proposed a word-based model employing both character-based features and more sophisticated word-based features using also averaged perceptron. There are other related results (Jiang et al., 2012) of open test including the final result of Jiang et al. (2009). Since their models used extra resources, they are not comparable with the 1http://crfpp.googlecode.com/svn/trunk/doc/index.html sf (Jiang et al., 2009) 0.9735 (Zhang</context>
</contexts>
<marker>Jiang, Huang, Liu, 2009</marker>
<rawString>Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Automatic adaptation of annotation standards: Chinese word segmentation and POS tagging - a case study. In Proceedings of the 47th ACL, pages 522–530, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Fandong Meng</author>
<author>Qun Liu</author>
<author>Yajuan Lü</author>
</authors>
<title>Iterative annotation transformation with predict-self reestimation for chinese word segmentation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>412--420</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="22395" citStr="Jiang et al., 2012" startWordPosition="3774" endWordPosition="3777"> field model (CRF) with the same feature templates in Table 1 to train the model parameters. The toolkit CRF++1 with the L2-norm penalty is used to train the weights. The hyper-parameter C = 20 is tuned using the development set. Jiang et al. (2009) proposed a character-based model employing similar feature templates using averaged perceptron. The feature templates are following Ng and Low (2004). Zhang and Clark (2011) proposed a word-based model employing both character-based features and more sophisticated word-based features using also averaged perceptron. There are other related results (Jiang et al., 2012) of open test including the final result of Jiang et al. (2009). Since their models used extra resources, they are not comparable with the 1http://crfpp.googlecode.com/svn/trunk/doc/index.html sf (Jiang et al., 2009) 0.9735 (Zhang and Clark, 2011) 0.9750† CRF++ (C = 20) 0.9742 Averaged Percetron 0.9734 + Shuffle 0.9755 + L2 0.9736 + L2 + Shuffle 0.9772 + Dropout 0.9741 + Dropout+ Shuffle 0.9765 + L2 + Dropout 0.9749 + L2 + Dropout+ Shuffle 0.9771 Table 2: Final results of the character-based Chinese word segmentation task on CTB5. † This result is read from a figure in that paper. sf Word-base</context>
</contexts>
<marker>Jiang, Meng, Liu, Lü, 2012</marker>
<rawString>Wenbin Jiang, Fandong Meng, Qun Liu, and Yajuan Lü. 2012. Iterative annotation transformation with predict-self reestimation for chinese word segmentation. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 412–420, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Canasai Kruengkrai</author>
<author>Kiyotaka Uchimoto</author>
<author>Jun’ichi Kazama</author>
<author>Yiou Wang</author>
<author>Kentaro Torisawa</author>
<author>Hitoshi Isahara</author>
</authors>
<title>An error-driven word-character hybrid model for joint chinese word segmentation and POS tagging.</title>
<date>2009</date>
<booktitle>In Proc. of ACL-IJCNLP</booktitle>
<pages>513--521</pages>
<institution>Suntec, Singapore. Association for Computational Linguistics.</institution>
<contexts>
<context position="23689" citStr="Kruengkrai et al., 2009" startWordPosition="3983" endWordPosition="3986">Shuffle 0.9791 Table 3: Final results of the word-based Chinese word segmentation task on CTB5. results in this paper. The results in Table 2 shows that with proper regularization methods, the models trained using perceptron algorithm can outperform CRF models with the same feature templates and other models with more sophisticated features trained using the averaged perceptron without other regularization methods. We further re-implemented a word-based Chinese word segmentation model with the feature templates following Zhang et al. (2012), which 170 sf jf (Jiang et al., 2008) 0.9785 0.9341 (Kruengkrai et al., 2009) 0.9787 0.9367 (Zhang and Clark, 2010) 0.9778 0.9367 (Sun, 2011) 0.9817 0.9402 Character-based model 0.9779 0.9336 + Shuffle 0.9802 0.9375 + Dropout 0.9789 0.9361 + Dropout+ Shuffle 0.9809 0.9407 + word-based re-ranking 0.9813 0.9438 Table 4: Final results of the POS tagging task on CTB5. word compl. (Huang and Sagae, 2010) 85.20 33.72 our re-implementation 85.22 34.15 + Shuffle 85.65 34.52 + Dropout 85.32 34.04 + Dropout+ Shuffle 85.71 34.57 Table 5: Final results of the dependency parsing task on CTB5. is similar with the model proposed by Zhang and Clark (2011). Beam search with early-updat</context>
<context position="25051" citStr="Kruengkrai et al. (2009)" startWordPosition="4201" endWordPosition="4204">larization methods show similar characteristics for the word-based model. 6.5.2 POS Tagging The results of the POS tagging models on the CTB5 corpus are shown in Table 4. Structure perceptron with feature templates in Table 1 is used. The F-measures for word segmentation (sf) and for joint word segmentation and POS tagging (jf) are listed. We use the “shuffle-and-average” (10 models), the dropout method (p = 5%) and their combination to regularize the structured perceptron. Jiang et al. (2008) used a character-based model using perceptron for POS tagging and a log-linear model for re-ranking. Kruengkrai et al. (2009) proposed a hybrid model including character-based and word-based features. Zhang and Clark (2010) proposed a word-based model using perceptron. Sun (2011) proposed a framework based on stacked learning consisting of four sub-models. For the closed test, this model has the best performance on the CTB5 corpus to our knowledge. Other results (Wang et al., 2011; Sun and Wan, 2012) for the open test are not listed since they are not comparable with the results in this paper. If we define the error rate as 1 − jf, the error reduction by applying regularization methods for the character-based model </context>
</contexts>
<marker>Kruengkrai, Uchimoto, Kazama, Wang, Torisawa, Isahara, 2009</marker>
<rawString>Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi Isahara. 2009. An error-driven word-character hybrid model for joint chinese word segmentation and POS tagging. In Proc. of ACL-IJCNLP 2009, pages 513–521, Suntec, Singapore. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Keith Hall</author>
<author>Gideon Mann</author>
</authors>
<title>Distributed training strategies for the structured perceptron.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>456--464</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1727" citStr="McDonald et al., 2010" startWordPosition="261" endWordPosition="264">perceptron for some of these tasks can be up to 10%. 1 Introduction Structured perceptron is a linear classification algorithm. It is used for word segmentation (Zhang and Clark, 2011), POS (part-of-speech) tagging (Collins, 2002), syntactical parsing (Collins and Roark, 2004), semantical parsing (Zettlemoyer and Collins, 2009) and other NLP tasks. The averaged perceptron or the voted perceptron (Collins, 2002) is proposed for better generalization. Early update (Collins and Roark, 2004; Huang et al., 2012) is used for inexact decoding algorithms such as the beam search. Distributed training (McDonald et al., 2010) and the minibatch and parallelization method (Zhao and Huang, 2013) are recently proposed. Some other related work focuses on the task-specified feature engineering. Regularization is to improve the ability of generalization and avoid over-fitting for machine learning algorithms including online learning algorithms (Do et al., 2009; Xiao, 2010). But practical studies on NLP did not pay much attention to the regularization of the structured perceptron. As a result, for some tasks the model learned using perceptron algorithm is not as good as the model learned using regularized condition random</context>
</contexts>
<marker>McDonald, Hall, Mann, 2010</marker>
<rawString>Ryan McDonald, Keith Hall, and Gideon Mann. 2010. Distributed training strategies for the structured perceptron. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 456–464. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>JE Moody</author>
<author>SJ Hanson</author>
<author>Anders Krogh</author>
<author>John A Hertz</author>
</authors>
<title>A simple weight decay can improve generalization. Advances in neural information processing systems,</title>
<date>1995</date>
<pages>4--950</pages>
<contexts>
<context position="9706" citStr="Moody et al., 1995" startWordPosition="1651" endWordPosition="1654"> i XL = w · i wj = ����� �{i|w[i] j =6 0, i = 1, · · · , n} Pn i=1 w[i] j (8) This equation makes the low-frequency features more important in the final model. 4 Penalty Adding penalty term to the loss function is a common and traditional regularization method to avoid over-fitting. It is widely used for the optimization problems of logistic regression, support vector machine, conditional random field and other models. Penalty terms for probabilistic models can be interpreted as a prior over the weights (Chen and Rosenfeld, 1999). It is also called “weight decay” in artificial neural network (Moody et al., 1995). The use of the penalty term is to prevent the components of the weight vector to become too large. In Section 2 we have modeled the perceptron algorithm as an SGD algorithm with an explicit loss function, the additional penalty term is therefore easy to be employed. where λ1 is the hyper-parameter to determine the strength of the penalty. The derivative of the penalty term is discontinuous. We update the weights as max{0, |w(t) (t+1) i |− ηλ1} i ← wi − ηΔφ(t) (t) |w(t) i i | (12) This ensures that the weight decay will not change the sign of the weight. An modified version of the L1 penalty </context>
</contexts>
<marker>Moody, Hanson, Krogh, Hertz, 1995</marker>
<rawString>JE Moody, SJ Hanson, Anders Krogh, and John A Hertz. 1995. A simple weight decay can improve generalization. Advances in neural information processing systems, 4:950–957.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Jin Kiat Low</author>
</authors>
<title>Chinese partof-speech tagging: One-at-a-time or all-at-once? word-based or character-based?</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>277--284</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="15305" citStr="Ng and Low, 2004" startWordPosition="2635" endWordPosition="2638"> templates for the characterbased Chinese word segmentation model and the joint Chinese word segmentation and POS tagging model. tag S indicates that the character forms a singlecharacter words; tag B / E indicates that the character is at the beginning / end of a multi-character words; tag M indicates that the character is in the middle of a multi-character words. For example, if the tag sequence for the input x = MX&amp;quot;�R`IA) (21) is y = BMESBE, (22) the corresponding segmentation result is &amp;I&apos;M 0`1 X). (23) Table 1 shows the set of the feature templates which is a subset of some related work (Ng and Low, 2004; Jiang et al., 2009) . Following Sun (2011), we split the Chinese treebank 5 into training set, development set and test set. F-measure (Emerson, 2005) is used as the measurement of the performance. 6.1.2 Part-of-Speech Tagging The second task is joint Chinese word segmentation and POS tagging. This can also be modeled as a character-based sequence labeling task. The tag set is a Cartesian product of the tag set for Chinese word segmentation and the set of POS tags. For example, the tag B-NN indicates the character is the first character of a multi-character noun. The tag sequence y = B-NR M-</context>
<context position="22175" citStr="Ng and Low (2004)" startWordPosition="3743" endWordPosition="3746">verage” (5 models), the L2 penalty method (A2 = 10−4), the dropout method (p = 3%) and their combinations to regularize the structured perceptron. To compare with the perceptron algorithm, we use the conditional random field model (CRF) with the same feature templates in Table 1 to train the model parameters. The toolkit CRF++1 with the L2-norm penalty is used to train the weights. The hyper-parameter C = 20 is tuned using the development set. Jiang et al. (2009) proposed a character-based model employing similar feature templates using averaged perceptron. The feature templates are following Ng and Low (2004). Zhang and Clark (2011) proposed a word-based model employing both character-based features and more sophisticated word-based features using also averaged perceptron. There are other related results (Jiang et al., 2012) of open test including the final result of Jiang et al. (2009). Since their models used extra resources, they are not comparable with the 1http://crfpp.googlecode.com/svn/trunk/doc/index.html sf (Jiang et al., 2009) 0.9735 (Zhang and Clark, 2011) 0.9750† CRF++ (C = 20) 0.9742 Averaged Percetron 0.9734 + Shuffle 0.9755 + L2 0.9736 + L2 + Shuffle 0.9772 + Dropout 0.9741 + Dropou</context>
</contexts>
<marker>Ng, Low, 2004</marker>
<rawString>Hwee Tou Ng and Jin Kiat Low. 2004. Chinese partof-speech tagging: One-at-a-time or all-at-once? word-based or character-based? In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 277–284, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
<author>Xiaojun Wan</author>
</authors>
<title>Reducing approximation and estimation errors for chinese lexical processing with heterogeneous annotations.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>232--241</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="25431" citStr="Sun and Wan, 2012" startWordPosition="4261" endWordPosition="4264">, the dropout method (p = 5%) and their combination to regularize the structured perceptron. Jiang et al. (2008) used a character-based model using perceptron for POS tagging and a log-linear model for re-ranking. Kruengkrai et al. (2009) proposed a hybrid model including character-based and word-based features. Zhang and Clark (2010) proposed a word-based model using perceptron. Sun (2011) proposed a framework based on stacked learning consisting of four sub-models. For the closed test, this model has the best performance on the CTB5 corpus to our knowledge. Other results (Wang et al., 2011; Sun and Wan, 2012) for the open test are not listed since they are not comparable with the results in this paper. If we define the error rate as 1 − jf, the error reduction by applying regularization methods for the character-based model is more than 10%. Comparing to the related work, the character-based model that we used is quite simple. But using the regularization methods discussed in this paper, it provides a comparable performance to the best model in the literature. 6.5.3 Dependency Parsing Table 5 shows the final results of the dependency parsing task on the CTB5 corpus. We use the “shuffle-and-average</context>
</contexts>
<marker>Sun, Wan, 2012</marker>
<rawString>Weiwei Sun and Xiaojun Wan. 2012. Reducing approximation and estimation errors for chinese lexical processing with heterogeneous annotations. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 232–241, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
</authors>
<title>A stacked sub-word model for joint chinese word segmentation and part-of-speech tagging.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1385--1394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="15349" citStr="Sun (2011)" startWordPosition="2645" endWordPosition="2646">mentation model and the joint Chinese word segmentation and POS tagging model. tag S indicates that the character forms a singlecharacter words; tag B / E indicates that the character is at the beginning / end of a multi-character words; tag M indicates that the character is in the middle of a multi-character words. For example, if the tag sequence for the input x = MX&amp;quot;�R`IA) (21) is y = BMESBE, (22) the corresponding segmentation result is &amp;I&apos;M 0`1 X). (23) Table 1 shows the set of the feature templates which is a subset of some related work (Ng and Low, 2004; Jiang et al., 2009) . Following Sun (2011), we split the Chinese treebank 5 into training set, development set and test set. F-measure (Emerson, 2005) is used as the measurement of the performance. 6.1.2 Part-of-Speech Tagging The second task is joint Chinese word segmentation and POS tagging. This can also be modeled as a character-based sequence labeling task. The tag set is a Cartesian product of the tag set for Chinese word segmentation and the set of POS tags. For example, the tag B-NN indicates the character is the first character of a multi-character noun. The tag sequence y = B-NR M-NR E-NR S-DEG B-NN E-NN, (24) for the input </context>
<context position="23753" citStr="Sun, 2011" startWordPosition="3995" endWordPosition="3996">ion task on CTB5. results in this paper. The results in Table 2 shows that with proper regularization methods, the models trained using perceptron algorithm can outperform CRF models with the same feature templates and other models with more sophisticated features trained using the averaged perceptron without other regularization methods. We further re-implemented a word-based Chinese word segmentation model with the feature templates following Zhang et al. (2012), which 170 sf jf (Jiang et al., 2008) 0.9785 0.9341 (Kruengkrai et al., 2009) 0.9787 0.9367 (Zhang and Clark, 2010) 0.9778 0.9367 (Sun, 2011) 0.9817 0.9402 Character-based model 0.9779 0.9336 + Shuffle 0.9802 0.9375 + Dropout 0.9789 0.9361 + Dropout+ Shuffle 0.9809 0.9407 + word-based re-ranking 0.9813 0.9438 Table 4: Final results of the POS tagging task on CTB5. word compl. (Huang and Sagae, 2010) 85.20 33.72 our re-implementation 85.22 34.15 + Shuffle 85.65 34.52 + Dropout 85.32 34.04 + Dropout+ Shuffle 85.71 34.57 Table 5: Final results of the dependency parsing task on CTB5. is similar with the model proposed by Zhang and Clark (2011). Beam search with early-update is used for decoding instead of dynamic programming. The resul</context>
<context position="25206" citStr="Sun (2011)" startWordPosition="4224" endWordPosition="4225">. Structure perceptron with feature templates in Table 1 is used. The F-measures for word segmentation (sf) and for joint word segmentation and POS tagging (jf) are listed. We use the “shuffle-and-average” (10 models), the dropout method (p = 5%) and their combination to regularize the structured perceptron. Jiang et al. (2008) used a character-based model using perceptron for POS tagging and a log-linear model for re-ranking. Kruengkrai et al. (2009) proposed a hybrid model including character-based and word-based features. Zhang and Clark (2010) proposed a word-based model using perceptron. Sun (2011) proposed a framework based on stacked learning consisting of four sub-models. For the closed test, this model has the best performance on the CTB5 corpus to our knowledge. Other results (Wang et al., 2011; Sun and Wan, 2012) for the open test are not listed since they are not comparable with the results in this paper. If we define the error rate as 1 − jf, the error reduction by applying regularization methods for the character-based model is more than 10%. Comparing to the related work, the character-based model that we used is quite simple. But using the regularization methods discussed in </context>
</contexts>
<marker>Sun, 2011</marker>
<rawString>Weiwei Sun. 2011. A stacked sub-word model for joint chinese word segmentation and part-of-speech tagging. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1385– 1394, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Jun’ichi Tsujii</author>
<author>Sophia Ananiadou</author>
</authors>
<title>Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>1</volume>
<pages>477--485</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10382" citStr="Tsuruoka et al., 2009" startWordPosition="1777" endWordPosition="1781">ents of the weight vector to become too large. In Section 2 we have modeled the perceptron algorithm as an SGD algorithm with an explicit loss function, the additional penalty term is therefore easy to be employed. where λ1 is the hyper-parameter to determine the strength of the penalty. The derivative of the penalty term is discontinuous. We update the weights as max{0, |w(t) (t+1) i |− ηλ1} i ← wi − ηΔφ(t) (t) |w(t) i i | (12) This ensures that the weight decay will not change the sign of the weight. An modified version of the L1 penalty for the online learning is the cumulative L1 penalty (Tsuruoka et al., 2009), which is used to make the stochastic gradient of the penalty term more close to the true gradient. The update is divided into two steps. In the first step, the weight vector is updated according to the loss function without the penalty term (t+ 2 1 ) w← w(t) i − ηΔφ(t) (13) i i And the cumulative penalty is calculated separately ci ← c (t+2) i+ ηλ1 (t) (14) w 166 In the second step, |wi |and ci are compared and at most one of them is non-zero before the next update m +- mint |wi 2)|, c(t+2)} (15) w(t+1) � max{0,|w�ti2)|−m}w(t+21) (16) i (t+2 ) i wi c(t+1) c(t+2) — m (17) i 5 Dropout Dropout </context>
</contexts>
<marker>Tsuruoka, Tsujii, Ananiadou, 2009</marker>
<rawString>Yoshimasa Tsuruoka, Jun’ichi Tsujii, and Sophia Ananiadou. 2009. Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, pages 477–485. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Vincent</author>
<author>Hugo Larochelle</author>
<author>Yoshua Bengio</author>
<author>Pierre-Antoine Manzagol</author>
</authors>
<title>Extracting and composing robust features with denoising autoencoders.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>10961103</pages>
<contexts>
<context position="11332" citStr="Vincent et al., 2008" startWordPosition="1956" endWordPosition="1960">ely ci ← c (t+2) i+ ηλ1 (t) (14) w 166 In the second step, |wi |and ci are compared and at most one of them is non-zero before the next update m +- mint |wi 2)|, c(t+2)} (15) w(t+1) � max{0,|w�ti2)|−m}w(t+21) (16) i (t+2 ) i wi c(t+1) c(t+2) — m (17) i 5 Dropout Dropout (Hinton et al., 2012) is originally a regularization method used for the artificial neural network. It corrupts one or more layers of a feedforward network during training, by randomly omitting some of the neurons. If the input layer is corrupted during the training of an autoencoder, the model is called denoising autoencoder (Vincent et al., 2008). The reason why such treatment can regularize the parameters are explained in different ways. Hinton et al. (2012) argued that the final model is an average of a large number of models and the dropout forces the model to learn good features which are less co-adapted. Vincent et al. (2008) argued that by using dropout of the input layer, the model can learn how to deal with examples outside the low-dimensional manifold that the training data concentrate. Models not so deep such as the structured perceptron may also benefit from this idea. Following the dropout method used in neural network, we</context>
</contexts>
<marker>Vincent, Larochelle, Bengio, Manzagol, 2008</marker>
<rawString>Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. 2008. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, page 10961103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiou Wang</author>
<author>Jun’ichi Kazama</author>
<author>Yoshimasa Tsuruoka</author>
<author>Wenliang Chen</author>
<author>Yujie Zhang</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Improving chinese word segmentation and POS tagging with semi-supervised methods using large auto-analyzed data.</title>
<date>2011</date>
<journal>Asian Federation of Natural Language Processing.</journal>
<booktitle>In Proceedings of 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>309--317</pages>
<location>Chiang Mai, Thailand,</location>
<contexts>
<context position="25411" citStr="Wang et al., 2011" startWordPosition="4257" endWordPosition="4260">verage” (10 models), the dropout method (p = 5%) and their combination to regularize the structured perceptron. Jiang et al. (2008) used a character-based model using perceptron for POS tagging and a log-linear model for re-ranking. Kruengkrai et al. (2009) proposed a hybrid model including character-based and word-based features. Zhang and Clark (2010) proposed a word-based model using perceptron. Sun (2011) proposed a framework based on stacked learning consisting of four sub-models. For the closed test, this model has the best performance on the CTB5 corpus to our knowledge. Other results (Wang et al., 2011; Sun and Wan, 2012) for the open test are not listed since they are not comparable with the results in this paper. If we define the error rate as 1 − jf, the error reduction by applying regularization methods for the character-based model is more than 10%. Comparing to the related work, the character-based model that we used is quite simple. But using the regularization methods discussed in this paper, it provides a comparable performance to the best model in the literature. 6.5.3 Dependency Parsing Table 5 shows the final results of the dependency parsing task on the CTB5 corpus. We use the </context>
</contexts>
<marker>Wang, Kazama, Tsuruoka, Chen, Zhang, Torisawa, 2011</marker>
<rawString>Yiou Wang, Jun’ichi Kazama, Yoshimasa Tsuruoka, Wenliang Chen, Yujie Zhang, and Kentaro Torisawa. 2011. Improving chinese word segmentation and POS tagging with semi-supervised methods using large auto-analyzed data. In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 309–317, Chiang Mai, Thailand, November. Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Xiao</author>
</authors>
<title>Dual averaging methods for regularized stochastic learning and online optimization.</title>
<date>2010</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>9999--2543</pages>
<contexts>
<context position="2074" citStr="Xiao, 2010" startWordPosition="312" endWordPosition="313">raged perceptron or the voted perceptron (Collins, 2002) is proposed for better generalization. Early update (Collins and Roark, 2004; Huang et al., 2012) is used for inexact decoding algorithms such as the beam search. Distributed training (McDonald et al., 2010) and the minibatch and parallelization method (Zhao and Huang, 2013) are recently proposed. Some other related work focuses on the task-specified feature engineering. Regularization is to improve the ability of generalization and avoid over-fitting for machine learning algorithms including online learning algorithms (Do et al., 2009; Xiao, 2010). But practical studies on NLP did not pay much attention to the regularization of the structured perceptron. As a result, for some tasks the model learned using perceptron algorithm is not as good as the model learned using regularized condition random field. In this paper, we treat the perceptron algorithm as a special case of the stochastic gradient descent (SGD) algorithm and study three kinds of simple but effective task-independent regularization methods that can be applied. The averaging method is to average the weight vectors of different models. We propose a “shuffle-and-average” meth</context>
</contexts>
<marker>Xiao, 2010</marker>
<rawString>Lin Xiao. 2010. Dual averaging methods for regularized stochastic learning and online optimization. The Journal of Machine Learning Research, 9999:2543–2596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning context-dependent mappings from sentences to logical form.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>2</volume>
<pages>976--984</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1434" citStr="Zettlemoyer and Collins, 2009" startWordPosition="212" endWordPosition="215">uring training which is called dropout in the neural network. Experiments are conducted on three NLP tasks, namely Chinese word segmentation, part-of-speech tagging and dependency parsing. Applying proper regularization methods or their combinations, the error reductions with respect to the averaged perceptron for some of these tasks can be up to 10%. 1 Introduction Structured perceptron is a linear classification algorithm. It is used for word segmentation (Zhang and Clark, 2011), POS (part-of-speech) tagging (Collins, 2002), syntactical parsing (Collins and Roark, 2004), semantical parsing (Zettlemoyer and Collins, 2009) and other NLP tasks. The averaged perceptron or the voted perceptron (Collins, 2002) is proposed for better generalization. Early update (Collins and Roark, 2004; Huang et al., 2012) is used for inexact decoding algorithms such as the beam search. Distributed training (McDonald et al., 2010) and the minibatch and parallelization method (Zhao and Huang, 2013) are recently proposed. Some other related work focuses on the task-specified feature engineering. Regularization is to improve the ability of generalization and avoid over-fitting for machine learning algorithms including online learning </context>
</contexts>
<marker>Zettlemoyer, Collins, 2009</marker>
<rawString>Luke S Zettlemoyer and Michael Collins. 2009. Learning context-dependent mappings from sentences to logical form. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 976–984. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>562571</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, page 562571, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A fast decoder for joint word segmentation and POS-Tagging using a single discriminative model.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>843--852</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="23727" citStr="Zhang and Clark, 2010" startWordPosition="3989" endWordPosition="3992"> the word-based Chinese word segmentation task on CTB5. results in this paper. The results in Table 2 shows that with proper regularization methods, the models trained using perceptron algorithm can outperform CRF models with the same feature templates and other models with more sophisticated features trained using the averaged perceptron without other regularization methods. We further re-implemented a word-based Chinese word segmentation model with the feature templates following Zhang et al. (2012), which 170 sf jf (Jiang et al., 2008) 0.9785 0.9341 (Kruengkrai et al., 2009) 0.9787 0.9367 (Zhang and Clark, 2010) 0.9778 0.9367 (Sun, 2011) 0.9817 0.9402 Character-based model 0.9779 0.9336 + Shuffle 0.9802 0.9375 + Dropout 0.9789 0.9361 + Dropout+ Shuffle 0.9809 0.9407 + word-based re-ranking 0.9813 0.9438 Table 4: Final results of the POS tagging task on CTB5. word compl. (Huang and Sagae, 2010) 85.20 33.72 our re-implementation 85.22 34.15 + Shuffle 85.65 34.52 + Dropout 85.32 34.04 + Dropout+ Shuffle 85.71 34.57 Table 5: Final results of the dependency parsing task on CTB5. is similar with the model proposed by Zhang and Clark (2011). Beam search with early-update is used for decoding instead of dyna</context>
<context position="25149" citStr="Zhang and Clark (2010)" startWordPosition="4214" endWordPosition="4217">lts of the POS tagging models on the CTB5 corpus are shown in Table 4. Structure perceptron with feature templates in Table 1 is used. The F-measures for word segmentation (sf) and for joint word segmentation and POS tagging (jf) are listed. We use the “shuffle-and-average” (10 models), the dropout method (p = 5%) and their combination to regularize the structured perceptron. Jiang et al. (2008) used a character-based model using perceptron for POS tagging and a log-linear model for re-ranking. Kruengkrai et al. (2009) proposed a hybrid model including character-based and word-based features. Zhang and Clark (2010) proposed a word-based model using perceptron. Sun (2011) proposed a framework based on stacked learning consisting of four sub-models. For the closed test, this model has the best performance on the CTB5 corpus to our knowledge. Other results (Wang et al., 2011; Sun and Wan, 2012) for the open test are not listed since they are not comparable with the results in this paper. If we define the error rate as 1 − jf, the error reduction by applying regularization methods for the character-based model is more than 10%. Comparing to the related work, the character-based model that we used is quite s</context>
</contexts>
<marker>Zhang, Clark, 2010</marker>
<rawString>Yue Zhang and Stephen Clark. 2010. A fast decoder for joint word segmentation and POS-Tagging using a single discriminative model. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 843–852, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>S Clark</author>
</authors>
<title>Syntactic processing using the generalized perceptron and beam search. Computational Linguistics,</title>
<date>2011</date>
<location>(Early Access):1–47.</location>
<contexts>
<context position="1289" citStr="Zhang and Clark, 2011" startWordPosition="195" endWordPosition="198">c order of the training examples; (2) one is to add penalty term to the loss function; (3) and one is to randomly corrupt the data flow during training which is called dropout in the neural network. Experiments are conducted on three NLP tasks, namely Chinese word segmentation, part-of-speech tagging and dependency parsing. Applying proper regularization methods or their combinations, the error reductions with respect to the averaged perceptron for some of these tasks can be up to 10%. 1 Introduction Structured perceptron is a linear classification algorithm. It is used for word segmentation (Zhang and Clark, 2011), POS (part-of-speech) tagging (Collins, 2002), syntactical parsing (Collins and Roark, 2004), semantical parsing (Zettlemoyer and Collins, 2009) and other NLP tasks. The averaged perceptron or the voted perceptron (Collins, 2002) is proposed for better generalization. Early update (Collins and Roark, 2004; Huang et al., 2012) is used for inexact decoding algorithms such as the beam search. Distributed training (McDonald et al., 2010) and the minibatch and parallelization method (Zhao and Huang, 2013) are recently proposed. Some other related work focuses on the task-specified feature engineer</context>
<context position="22199" citStr="Zhang and Clark (2011)" startWordPosition="3747" endWordPosition="3750"> the L2 penalty method (A2 = 10−4), the dropout method (p = 3%) and their combinations to regularize the structured perceptron. To compare with the perceptron algorithm, we use the conditional random field model (CRF) with the same feature templates in Table 1 to train the model parameters. The toolkit CRF++1 with the L2-norm penalty is used to train the weights. The hyper-parameter C = 20 is tuned using the development set. Jiang et al. (2009) proposed a character-based model employing similar feature templates using averaged perceptron. The feature templates are following Ng and Low (2004). Zhang and Clark (2011) proposed a word-based model employing both character-based features and more sophisticated word-based features using also averaged perceptron. There are other related results (Jiang et al., 2012) of open test including the final result of Jiang et al. (2009). Since their models used extra resources, they are not comparable with the 1http://crfpp.googlecode.com/svn/trunk/doc/index.html sf (Jiang et al., 2009) 0.9735 (Zhang and Clark, 2011) 0.9750† CRF++ (C = 20) 0.9742 Averaged Percetron 0.9734 + Shuffle 0.9755 + L2 0.9736 + L2 + Shuffle 0.9772 + Dropout 0.9741 + Dropout+ Shuffle 0.9765 + L2 +</context>
<context position="24259" citStr="Zhang and Clark (2011)" startWordPosition="4075" endWordPosition="4078">t al., 2008) 0.9785 0.9341 (Kruengkrai et al., 2009) 0.9787 0.9367 (Zhang and Clark, 2010) 0.9778 0.9367 (Sun, 2011) 0.9817 0.9402 Character-based model 0.9779 0.9336 + Shuffle 0.9802 0.9375 + Dropout 0.9789 0.9361 + Dropout+ Shuffle 0.9809 0.9407 + word-based re-ranking 0.9813 0.9438 Table 4: Final results of the POS tagging task on CTB5. word compl. (Huang and Sagae, 2010) 85.20 33.72 our re-implementation 85.22 34.15 + Shuffle 85.65 34.52 + Dropout 85.32 34.04 + Dropout+ Shuffle 85.71 34.57 Table 5: Final results of the dependency parsing task on CTB5. is similar with the model proposed by Zhang and Clark (2011). Beam search with early-update is used for decoding instead of dynamic programming. The results with different regularization methods are shown in Figure 3. These regularization methods show similar characteristics for the word-based model. 6.5.2 POS Tagging The results of the POS tagging models on the CTB5 corpus are shown in Table 4. Structure perceptron with feature templates in Table 1 is used. The F-measures for word segmentation (sf) and for joint word segmentation and POS tagging (jf) are listed. We use the “shuffle-and-average” (10 models), the dropout method (p = 5%) and their combin</context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Y. Zhang and S. Clark. 2011. Syntactic processing using the generalized perceptron and beam search. Computational Linguistics, (Early Access):1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kaixu Zhang</author>
<author>Maosong Sun</author>
<author>Changle Zhou</author>
</authors>
<title>Word segmentation on chinese mirco-blog data with a linear-time incremental model.</title>
<date>2012</date>
<booktitle>In Proceedings of the Second CIPS-SIGHAN Joint Conference on Chinese Language Processing,</booktitle>
<pages>41--46</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Tianjin, China,</location>
<contexts>
<context position="23611" citStr="Zhang et al. (2012)" startWordPosition="3969" endWordPosition="3972">ased model 0.9758 + Shuffle 0.9787 + L2 + Shuffle 0.9791 + L2 + Dropout+ Shuffle 0.9791 Table 3: Final results of the word-based Chinese word segmentation task on CTB5. results in this paper. The results in Table 2 shows that with proper regularization methods, the models trained using perceptron algorithm can outperform CRF models with the same feature templates and other models with more sophisticated features trained using the averaged perceptron without other regularization methods. We further re-implemented a word-based Chinese word segmentation model with the feature templates following Zhang et al. (2012), which 170 sf jf (Jiang et al., 2008) 0.9785 0.9341 (Kruengkrai et al., 2009) 0.9787 0.9367 (Zhang and Clark, 2010) 0.9778 0.9367 (Sun, 2011) 0.9817 0.9402 Character-based model 0.9779 0.9336 + Shuffle 0.9802 0.9375 + Dropout 0.9789 0.9361 + Dropout+ Shuffle 0.9809 0.9407 + word-based re-ranking 0.9813 0.9438 Table 4: Final results of the POS tagging task on CTB5. word compl. (Huang and Sagae, 2010) 85.20 33.72 our re-implementation 85.22 34.15 + Shuffle 85.65 34.52 + Dropout 85.32 34.04 + Dropout+ Shuffle 85.71 34.57 Table 5: Final results of the dependency parsing task on CTB5. is similar w</context>
</contexts>
<marker>Zhang, Sun, Zhou, 2012</marker>
<rawString>Kaixu Zhang, Maosong Sun, and Changle Zhou. 2012. Word segmentation on chinese mirco-blog data with a linear-time incremental model. In Proceedings of the Second CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 41–46, Tianjin, China, December. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Zhao</author>
<author>Liang Huang</author>
</authors>
<title>Minibatch and parallelization for online large margin structured learning.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>370--379</pages>
<contexts>
<context position="1795" citStr="Zhao and Huang, 2013" startWordPosition="271" endWordPosition="274">tructured perceptron is a linear classification algorithm. It is used for word segmentation (Zhang and Clark, 2011), POS (part-of-speech) tagging (Collins, 2002), syntactical parsing (Collins and Roark, 2004), semantical parsing (Zettlemoyer and Collins, 2009) and other NLP tasks. The averaged perceptron or the voted perceptron (Collins, 2002) is proposed for better generalization. Early update (Collins and Roark, 2004; Huang et al., 2012) is used for inexact decoding algorithms such as the beam search. Distributed training (McDonald et al., 2010) and the minibatch and parallelization method (Zhao and Huang, 2013) are recently proposed. Some other related work focuses on the task-specified feature engineering. Regularization is to improve the ability of generalization and avoid over-fitting for machine learning algorithms including online learning algorithms (Do et al., 2009; Xiao, 2010). But practical studies on NLP did not pay much attention to the regularization of the structured perceptron. As a result, for some tasks the model learned using perceptron algorithm is not as good as the model learned using regularized condition random field. In this paper, we treat the perceptron algorithm as a specia</context>
</contexts>
<marker>Zhao, Huang, 2013</marker>
<rawString>Kai Zhao and Liang Huang. 2013. Minibatch and parallelization for online large margin structured learning. In Proceedings of NAACL-HLT, pages 370– 379.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>