<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000269">
<title confidence="0.969368">
Rule Writing or Annotation: Cost-efficient Resource Usage
for Base Noun Phrase Chunking
</title>
<author confidence="0.980154">
Grace Ngai and David Yarowsky
</author>
<affiliation confidence="0.945946">
Department of Computer Science
Johns Hopkins University
</affiliation>
<address confidence="0.682173">
Baltimore, MD 21218, U.S.A.
</address>
<email confidence="0.989746">
Email:{gyn,yarowsky}@cs. jhu.edu
</email>
<sectionHeader confidence="0.995398" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999679625">
This paper presents a comprehensive
empirical comparison between two ap-
proaches for developing a base noun
phrase chunker: human rule writing and
active learning using interactive real-
time human annotation. Several novel
variations on active learning are inves-
tigated, and underlying cost models for
cross-modal machine learning compari-
son are presented and explored. Results
show that it is more efficient and more
successful by several measures to train a
system using active learning annotation
rather than hand-crafted rule writing at
a comparable level of human labor in-
vestment.
</bodyText>
<sectionHeader confidence="0.998523" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999983228571429">
One of the primary problems that NLP re-
searchers who work in new languages or new do-
mains encounter is a lack of available annotated
data. Collection of data is neither easy nor cheap.
The construction of the Penn Treebank signifi-
cantly improved performance for English systems
dealing in the &amp;quot;traditional&amp;quot; NLP domains (eg
parsing, part-of-speech tagging, etc). However,
for a new language, a similar investment of ef-
fort in time and money is most likely prohibitive,
if not impossible.
Faced with the costs associated with data ac-
quisition, rationalists may argue that it would be
more cost effective to construct systems of hand-
coded rule lists that capture the linguistic charac-
teristics of the task at hand, rather than spending
comparable effort annotating data and expecting
the same knowledge to be acquired indirectly by
a machine learning system. The question we are
trying to address then is: for a given cost assump-
tion, which approach would be the most effective.
Although learning curves showing performance
relative to amount of training data are common
in the machine learning literature, these are inade-
quate for comparing systems with different sources
of training data or supervision. This is especially
true when a human rule-based approach and em-
pirical learning are evaluated relative to effort in-
vested. Such a multi-factor cost analysis is long
overdue.
This paper will conclude with a comprehensive
cost model exposition and analysis, and an em-
pirical study contrasting human rule-writing ver-
sus annotation-based learning approaches that are
sensitive to these cost models.
</bodyText>
<sectionHeader confidence="0.908566" genericHeader="introduction">
2 Base Noun Phrase Chunking
</sectionHeader>
<bodyText confidence="0.990905838709678">
The domain in which our experiments are per-
formed is base noun phrase chunking. A sig-
nificant amount of work has been done in this
domain and many different methods have been
applied: Church&apos;s PARTS (1988) program used
a Markov model; Bourigault (1992) used heuris-
tics along with a grammar; Voutilainen&apos;s NPTool
(1993) used a lexicon combined with a constraint
grammar; Juteson and Katz (1995) used repeated
phrases; Veenstra (1998), Argamon, Dagan &amp;
Krymolowski (1998), Daelemans, van den Bosch
&amp; Zavrel (1999) and Tjong Kim Sang &amp; Veenstra
(1999) used memory-based systems; Ramshaw &amp;
Marcus (1999) and Cardie &amp; Pierce (1998) used
rule-based systems, Munoz et al. (1999) used a
Winnow-based system, and the XTAG Research
Group(1998) used a tree-adjoining grammar.
Of all the systems, Ramshaw &amp; Marcus&apos; trans-
formation rule-based system had the best pub-
lished performance (f-measure 92.0) for several
years, and is regarded as the de facto standard
for the domain. Although several systems have
recently achieved slightly higher published results
(Munoz et al.: 92.8, Tjong Kim Sang &amp; Veenstra:
92.37, XTAG Research Group: 92.4), their algo-
rithms are significantly more costly, or not fea-
sible, to implement in an active learning frame-
work. To facilitate contrastive studies, we have
evaluated our active learning and cost model com-
parisons using Ramshaw &amp; Marcus&apos; system as the
reference algorithm in these experiments.
</bodyText>
<sectionHeader confidence="0.9858075" genericHeader="method">
3 Active Learning from
Annotation
</sectionHeader>
<bodyText confidence="0.999912210526316">
Supervised statistical machine learning systems
have traditionally required large amounts of anno-
tated data from which to extract linguistic prop-
erties of the task at hand. However, not all data
is created equal. A random distribution of an-
notated data contains much redundant informa-
tion. By intelligently choosing the training exam-
ples which get passed to the learner, it is possible
to provide the necessary amount of information
with less data.
Active learning attempts to perform this intel-
ligent sampling of data to reduce annotation costs
without damaging performance. In general, these
methods calculate the usefulness of an example by
first having the learner classify it, and then seeing
how uncertain that classification was. The idea is
that the more uncertain the example, the less well
modeled this situation is, and therefore, the more
useful it would be to have this example annotated.
</bodyText>
<subsectionHeader confidence="0.983483">
3.1 Prior Work in Active Learning
</subsectionHeader>
<bodyText confidence="0.999740703703704">
Seung, Opper and Sompolinsky (1992) and Fre-
und et al. (1997) proposed a theoretical query-
by-committee approach. Such an approach uses
multiple models (or a committee) to evaluate the
data, and candidates for annotation (or queries)
are drawn from the pool of examples in which
the models disagree. Furthermore, Freund et al.
prove that, under some situations, the generaliza-
tion error decreases exponentially with the num-
ber of queries.
On the experimental side, active learning has
been applied to several different problems. Lewis
&amp; Gale (1994), Lewis &amp; Catlett (1994) and Liere
&amp; Tadepalli (1997) all applied it to text catego-
rization; Engelson &amp; Dagan (1996) applied it to
part-of-speech tagging.
Each approach has its own way of determin-
ing uncertainty in examples. Lewis &amp; Gale used
a probabilistic classifier and picked the examples
e whose class-conditional a posteriori probability
P(Cie) is closest to 0.5 (for a 2-class problem).
Engelson &amp; Dagan implemented a committee of
learners, and used vote entropy to pick examples
which had the highest disagreement among the
learners. In addition, Engelson &amp; Dagan also in-
vestigate several different selection techniques in
depth.
</bodyText>
<subsectionHeader confidence="0.992609">
3.2 New Applications and Algorithmic
Extensions in Active Learning
</subsectionHeader>
<bodyText confidence="0.9775082">
To our knowledge, this paper constitutes the
first work to apply active learning to base noun
phrase chunking, or to apply active learning to
a transformation-learning paradigm (Brill, 1995)
for any application. Since a transformation-based
learner does not give a probabilistic output, we are
not able to use Lewis &amp; Gale&apos;s method for deter-
mining uncertainty. Our experimental framework
thus uses the query by committee paradigm with
batch selection:
</bodyText>
<listItem confidence="0.996247529411765">
1. Given a corpus C, arbitrarily pick t sentences
for annotation.
2. Have these t sentences hand-annotated,
delete them from C and put them into a train-
ing set, T.
3. Divide T into m non-identical, but not nec-
essarily non-overlapping, subsets.
4. Use each subset as the training set for a
model.
5. Evaluate each model on the remaining sen-
tences in C
6. Using a measure of disagreement D, pick the
x sentences in C with the highest D for an-
notation.
7. Delete the x sentences from C, have them
annotated, and add them to T.
8. Repeat from 3.
</listItem>
<bodyText confidence="0.999947">
In our experiments, the initial corpus C that we
used consisted of sections 15-18 of the Wall Street
Journal Treebank (Marcus et al., 1993), which is
also the training set used by Ramshaw &amp; Mar-
cus (1999). The initial t sentences were the first
100 sentences of the training corpus, and x = 50
sentences were picked at each iteration. Sets of
50 sentences were selected because it takes ap-
proximately 15-30 minutes for humans to anno-
tate them, a reasonable amount of work and time
for the annotator to spend before taking a break
while the machine selects the next set. The pa-
rameter m, which denotes the number of models
to train, was set at 3, which could be expected
to give us reasonable labelling variation over the
samples, but also would not cause the processing
phase to take a long time.
To divide the corpus into the different subsets
in Step 3, we tried using two approaches: bagging
and n-fold partitioning. In bagging, we randomly
</bodyText>
<figure confidence="0.997507210526316">
0 5000 10000 15000 20000 25000 30000 35000 40000 45000 50000
Size of Training Set (words)
91
90
89
88
87
86
85
84
83
Active Learning
(F-complement model)
Sequential Learning
Active Learning (Vote Entropy Model)
Relative Training Size for Same Performance
Active Learning
Sequential Training
Performance (F-Measure)
</figure>
<bodyText confidence="0.9995804">
sentences selected by active learning and anno-
tated sentences selected sequentially shows that
active learning reduces the amount of data needed
to reach a given level of performance by approxi-
mately a factor of two.
</bodyText>
<subsectionHeader confidence="0.990886">
3.3 Active Learning with Real Time
Human Supervision
</subsectionHeader>
<bodyText confidence="0.999984563380282">
Most of the published work on active learning are
simulations of an idealized situation. One has a
large annotated corpus, and the new tags for the
&amp;quot;newly annotated&amp;quot; sentences are simply drawn
from what was observed in the annotated corpus,
as if the gold standard annotator was producing
this feedback in real time, while the test set it-
self is, of course, not used for this feedback. This
is an idealized situation, since it assumes that a
true active learning situation would have access
to someone who could annotate with perfect con-
sistency to the gold standard corpus annotation
conventions.
Because our goal is to investigate the relative
costs of rule writing versus annotation, it is es-
sential that we use a realistic model of annota-
tion. Therefore, we decided to do a fully-fledged
active learning annotation experiment, with real
time human supervision, rather than assume the
simulated feedback of actual Treebank annotators.
We developed an annotation tool that is mod-
eled on MITRE&apos;s Alembic Workbench software
(Day et al., 1997), but written in Java for
platform-independence. To enable data storage
and the active learning sample selection to take
place on the more powerful machines in our lab
rather than the user&apos;s home machine, the tool was
designed with network support so that it could
communicate with our servers over the internet.
Our real-time active learning experiment sub-
jects were seven graduate students in computer
science. Five of them are native English speak-
ers, but none had any formal linguistics training.
The initial training set T is the first 100 sentences
of Ramshaw &amp; Marcus&apos; training set. To acquaint
the subjects with the Treebank conventions, they
were first asked to spend some time in a feedback
phase, where they would annotate up to 50 sen-
tences (they were allowed to stop at any time)
drawn from the initial 100 sentences in T. The
sentences were annotated one at a time, and the
Treebank annotation was shown to them after ev-
ery sentence. On average, the annotators spent
around 15 minutes on this feedback phase before
deciding that they were comfortable enough with
the convention.
The active learning phase follows the feedback
phase. The f-complement disagreement measure
was used to select 50 sentences from the rest of
Ramshaw &amp; Marcus&apos; training set and the annota-
tor was instructed to annotate them. The anno-
tated sentences were then sent back to the server.
The system chose the next 50 sentences. The ex-
periment consists of 10 iterations, during which
the annotators were allowed to make use of the
original 100 sentences as a reference corpus. Af-
ter completing all 10 iterations, they were asked
to annotate a further 100 consecutive sentences
drawn randomly from the test set. The purpose
of this final annotation was to judge how well an-
notators tag sentences drawn with the true dis-
tribution from the test corpus, as we shall see in
section 5.
On average, the annotators took 17 minutes to
annotate each set of 50 sentences, ranging from 8
to 30 minutes. The average amount of time the
server took to run the active learning algorithm
and select the next batch of sentences was approx-
imately 3 minutes, a rest break for the annotators.
The analysis of the results is presented in sec-
tion 5.
</bodyText>
<sectionHeader confidence="0.846454" genericHeader="method">
4 Learning by Rules
</sectionHeader>
<bodyText confidence="0.9996666">
In previous work, Brill &amp; Ngai (1999) showed that
under certain circumstances, it is possible for hu-
mans writing rules to perform as well as a state-
of-the-art machine learning system for base noun
phrase chunking. What that study did not ad-
dress, however, was the cost of the human labor
and/or machine cycles involved to construct such
a system, nor the relative cost of obtaining the
training data for the machine learning system.
This paper will estimate and contrast these costs
relative to performance.
To investigate the costs of a human rule-writing
system, we used a similar framework to that of
Brill &amp; Ngai. The system was written as a cgi
script which could be accessed across the web from
a browser such as Netscape or Internet Explorer.
Like Brill &amp; Ngai&apos;s 1999 approach, our rules were
based on Perl regular expressions. However, in-
stead of explicitly defining rule actions and having
different kinds of rules, our rules implicitly define
their actions by using different symbols to denote
the placement of the base noun phrase-enclosing
parentheses prior to and after the application of
the rule. Table 1 presents a comparison of our
rule format against that of Brill &amp; Ngai&apos;s. The
rules presented here may be considered less cum-
bersome and more intuitive.
In a way that is similar to Brill &amp; Ngai&apos;s sys-
tem, our rules were translated into Perl regular
expressions and evaluated on the corpus. New
</bodyText>
<table confidence="0.999849642857143">
Example Task
Inserting New Brackets Splitting A Noun Phrase Moving A Bracket
Brill &amp; Ngai TY: A TY: S TY: T
1999 LC: null LC: null LC: &lt;&lt;&lt; ({1} w=abouD
Rule Format TAR: ({1} t=DT) (* t=JJ[RS]?) \ TAR1: (* t= \ w+) (± t=NNP?S?) TAR: ({1} t=$) (± t=OD)
(+ t=NNP?S?) MC: null RC: null
RC: null TAR2: (* t=JJ [RS]?) ({1} w= \w+day)
RC: null
New { _DT ADJ* NOUN+ } [ { ANYWORD* NOUN+ } t ADJ* TIMEDAY } 1 { about_ [ _$ NUM+ ] }
Rule
Format
Effect of ThepT manNN ranvpp • Rule about/N ( $$ 5cp )
Application ( ThepT man ) ranvpp • ( NewNNp YorkNNp FicidayNNp ) ( abOUt IN $$ 5CD )
( NOWNN p YOrkNNp ) ( FridaYNNP )
</table>
<tableCaption confidence="0.999909">
Table 1: Comparison of our current rule format with Brill &amp; Ngai (1999)
</tableCaption>
<bodyText confidence="0.527661">
rules are appended onto the end of the list and
each rule applied in order, in the paradigm of a
transformation-based rule list.
</bodyText>
<subsectionHeader confidence="0.971652">
4.1 Rule-Writing Experiments
</subsectionHeader>
<bodyText confidence="0.999675714285714">
The rule-writing experiments were conducted by a
group of 17 advanced computer science students,
using the identical test set as in the annotation
experiments and the same initial 100 gold stan-
dard sentences for both initial bracketing stan-
dards guidance and rule-quality feedback through-
out their work.
</bodyText>
<equation confidence="0.792408961538461">
Grab-all rule
_RB::? ADJ* ANOUN* ADJ* ANOUN+
(blah blah last Fri)-&gt;(blah blah) (last Fri)
[ ANYTHING* _JJ TIME_W ]
[ NOT_ADJ+ TIME_W ]
about $8 (an ounce) -&gt; (about $8 an ounce)
(OnlylonlylAboutlabout)_::? _(\$I#)::? \
CD: :+ [ ANYTHING+
_RBR::* _(PDTIJJ)::? _(DTIPRP\CPOS) ADJ* \
_RB::? VERB? [ ANYTHING+
( boy ) -&gt; ( that boy )
(ThatIthat)DT ANYTHING+ ]
( about 4 1/2 )
(onlylabout)_::? (\$I#)_::? _CD::+
[ ANYTHING+ [? ANYTHING* 1? 1_-LRB-
[ ANYTHING+ ] _-RRB- [ ANYTHING+ ]
Pronouns are usually baseNPs
_DT::? _PRP
&amp;quot;and&amp;quot; usually isn&apos;t in a baseNP
[ _\S+::+ ] (andl80_ [ _\S+::+ ]
more singleton baseNPs
_(DTIEXIWPIWDT) VERB
some numbers are singleton baseNPs
[ ANYTHING ] [ _CD ]
( much/most ) of
_(DTIRB)::? (muchlmost)_ I IN
</equation>
<figureCaption confidence="0.9880815">
Figure 2: An Example Rule List. Lines beginning
with hash marks (#) are comments.
</figureCaption>
<bodyText confidence="0.999984363636364">
The time that the students spent on the task
varied widely, from a minimum of 1.5 hours to a
maximum of 9 hours, with an average of 5 hours.
Because we captured and saved every change the
students made to their rule list and logged ev-
ery mouse click they made while doing the exper-
iment, it was possible for us to trace the perfor-
mance of the system as a function of time. Figure
2 shows the rule list constructed by one of the sub-
jects. The quantitative results of the rule-writing
experiments are presented in the next section.
</bodyText>
<sectionHeader confidence="0.6873365" genericHeader="method">
5 Experiment Results--- Rule
Writing vs. Annotation
</sectionHeader>
<bodyText confidence="0.9998906875">
This section will analyze and compare the per-
formance of systems constructed with hand-built
rules with systems that were trained from data
selected during real-time active learning.
The performance of Ramshaw &amp; Marcus&apos; sys-
tem trained on the annotations of each subject
in the real-time active learning experiments, and
the performance achieved by the manually con-
structed systems of the top 6 rule writers are
shown in Figures 3 and 4, depicting the perfor-
mance achieved by each individual system. The
x-axes show the time spent by each human sub-
ject (either annotating or writing rules) in min-
utes; the y-axes show the f-measure performance
achieved by the systems built using the given level
of supervision.
</bodyText>
<subsectionHeader confidence="0.959223">
5.1 Analysis of Comparative
Experimental Data
</subsectionHeader>
<bodyText confidence="0.999333">
It is important to note that when comparing the
curves in Figure 4, experimental conditions across
groups were kept as equal as possible, with any
known potential biases favoring the rules-writing
group. First, both groups began with the identical
</bodyText>
<figure confidence="0.987236276595745">
90
88
86
84
82
80
Active Learning (Expert Annotation)
Active Learning (Non-expert Annotation)
Sequential Training (Expert Annotation)
Rule Writing (Non-expert)
0 50 100 150 200 250 300 350 400
Human Labor (Minutes)
90
Trained on Annotation (expert)
85
Trained on
Annotation
(non-expert)
Hand-Coded Rules (non-expert)
70
0 50 100 150 200 250 300 350 400
Human Labor (Minutes)
Performance (F-Measure)
80
75
Performance (F-Measure)
100
Learned System Performance as percentage of
System 6
99
Human Annotator’s Performance (relative to Treebank)
98
97
System 5
96
System 4
95
94
System 3
System 2
System 1
93
92
91
90
89
0 50 100 150 200 250
</figure>
<table confidence="0.927358875">
Human Labor (Minutes)
Cost Model Parameter Annotation Rule-writing
IDGm = Infrastructure Development Cost (for tagging/RW environment) Shared Shared
So = Number of initial gold standard sentences for training 100 100
ACTB = gold standard (Treebank) Annotation Cost (per sentence) x x
LCm = Labor Cost for Annotation or RW (per hour) $12.00/hour $12.00/hour
MCA = Cost of Machine Cycles for Annotation/RW Support $0.24/hour $0.12/hour
T = Variable time investment
</table>
<tableCaption confidence="0.999934">
Table 3: Example Monetary-Based Cost Parameters for Model Comparison
</tableCaption>
<bodyText confidence="0.987883357142857">
environments, while initially variable across meth-
ods, have already been borne and to the extent
that both interface systems port to new languages
and domains with relative ease, the incremental
development costs for new trials are likely to be
relatively low and comparable. Finally, this cost
model takes into account the cost of developing or
acquiring the So gold standard tagged data (e.g.
from the Treebank) to provide initial and/or incre-
mental training feedback to the annotator or rule
writer to help force consistency with the gold stan-
dard. We have found that both learning modes
can benefit from this high quality feedback, but
the cost x of developing such a high-quality re-
source for new languages or domains is unknown,
but likely to be higher than the non-expert labor
costs employed here.
7 Rules vs. Annotation-based
Learning — Advantages and
Disadvantages
In the previous sections, we investigated the per-
formance differences and resource costs involved
for using humans to write rules vs. using them
for annotations. In this section, we will further
compare these system development paradigms.
Annotation-based human participation has a
number of significant practical advantages relative
to developing a system by manual rule-writing:
</bodyText>
<listItem confidence="0.6384744">
• Annotation-based learning can continue in-
definitely, over weeks and months, with rela-
tively self-contained annotation decisions at
each point. In contrast, rule-writers must
remain cognizant of potential previous rule
</listItem>
<bodyText confidence="0.968374814814815">
interdependencies when adding or revising
rules, ultimately bounding continued rule-
system growth by cognitive load factors.
• Annotation-based learning can more effec-
tively combine the efforts of multiple indi-
viduals. The tagged sentences from different
data sets can be simply concatenated to form
a larger data set with broader coverage. In
contrast, it is much more difficult, if not im-
possible, for a rule writer to resume where
another one left off. Furthermore, combining
rule lists is very difficult because of the tight
and complex interaction between successive
rules. Combination of rule writing systems is
therefore limited to voting or similar classifier
techniques which can be applied to annota-
tion systems as well.
• Rule-based learning requires a larger skill
set, including not only the linguistic knowl-
edge needed for annotation, but also compe-
tence in regular expressions and an ability to
grasp the complex interactions within a rule
list. These added skill requirements naturally
shrink the pool of viable participants and in-
creases their likely cost.
• Based on empirical observation, the perfor-
mance of rule writers tend to exhibit consid-
erably more variance, while systems trained
on annotation tend to yield much more con-
sistent results.
• Finally, the current performance of
annotation-based training is only a lower
bound based on the performance of current
learning algorithms. Since annotated data
can be used by other current or future
machine learning techniques, subsequent
algorithmic improvements may yield perfor-
mance improvements without any change
in the data. In contrast, the performance
achieved by a set of rules is effectively final
without additional human revision.
The potential disadvantages of annotation-
based system development for applications such
as base NP chunking are limited. Given the
cost models presented in Section 6, one poten-
tial negative scenario would be an environment
where the machine cost significantly outweighed
human labor costs, or where access to active learn-
ing and annotation infrastructure was unavailable
or costly. However, under normal circumstances
where machine analysis of text is pursued, and
public domain access to our annotation and ac-
tive learning toolkits is assumed, such a scenario
is unlikely.
</bodyText>
<sectionHeader confidence="0.979503" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999951454545455">
This paper has illustrated that there are poten-
tially compelling practical and performance ad-
vantages to pursuing active-learning based anno-
tation rather than rule-writing to develop base
noun phrase chunkers. The relative balance de-
pends ultimately on one&apos;s cost model, but given
the goal of minimizing total human labor cost, it
appears to be consistently more efficient and ef-
fective to invest these human resources in system-
development via annotation rather than rule writ-
ing.
</bodyText>
<sectionHeader confidence="0.994438" genericHeader="acknowledgments">
9 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9996644">
The authors would like to thank Jan Hajic, Eric
Brill, Radu Florian, and various members of the
Natural Language Processing Lab at Johns Hop-
kins for their valuable feedback regarding this
work.
</bodyText>
<sectionHeader confidence="0.996298" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999233838709677">
S. Argamon, I. Dagan, and Y. Krymolowski. 1998.
A memory-based approach to learning shallow lan-
guage patterns. In Proceedings of the 17th Inter-
national Conference on Computational Linguistics,
pages 67-73. COLING-ACL.
D. Bourigault. 1992. Surface grammatical analysis for
the extraction of terminological noun phrases. In
Proceedings of the 30th Annual Meeting of the As-
sociation of Computational Linguistics, pages 977-
981. Association of Computational Linguistics.
E. Brill and G. Ngai. 1999. Man vs. machine: A case
study in base noun phrase chunking. In Proceed-
ings of ACL 1999. Association for Computational
Linguistics.
E. Brill. 1995. Transformation-based error-driven
learning and natural language: A case study in
part of speech tagging. Computational Linguistics,
21 (4): 543-565.
C. Cardie and D. Pierce. 1998. Error-driven pruning
of treebank gramars for base noun phrase identifica-
tion. In Proceedings of the 36th Annual Meeting of
the Association of Computational Linguistics, pages
218-224. Association of Computational Linguistics.
K. Church. 1988. A stochastic parts program and
noun phrase parser for unrestricted text. In Pro-
ceedings of the Second Conference on Applied Nat-
ural Language Processing, pages 136-143. Associa-
tion of Computational Linguistics.
W. Daelemans, A. van den Bosch, and J. Zavrel. 1999.
Forgetting exceptions is harmful in language learn-
ing. In Machine Learning, special issue on natural
language learning, volume 11, pages 11-43. to ap-
pear.
Day, J. Aberdeen, L. Hirschman, R. Kozierok,
P. Robinson, and M. Vilain. 1997. Mixed-initiative
development of language processing systems. In
Fifth Conference on Applied Natural Language Pro-
cessing, pages 348-355. Association for Computa-
tional Linguistics, March.
Engelson and I. Dagan. 1996. Minimizing man-
ual annotation cost in supervised training from cor-
pora. In Proceedings of ACL 1996. Association for
Computational Linguistics.
Freund, H. S. Seung, E. Shamir, and N. Tishby.
1997. Selective sampling using the query by com-
mittee algorithm. Machine Learning, 28:133-168.
Juteson and S. Katz. 1995. Technical terminol-
ogy: Some linguistic properties and an algorithm
for identification in text. Natural Language Engi-
neering, 1:9-27.
Lewis and J. Catlett. 1994. Heterogeneous uncer-
tainty sampling for supervised learning. In Proceed-
ings of the 11th International Conference on Ma-
chine Learning.
Lewis and W. Gale. 1994. A sequential algorithm
for training text classifiers. In Proceedings of ACM-
SICIR 1994. AC M-SIGIR.
Liere and P. Tadepalli. 1997. Active learning with
committees for text categorization. In Proceedings
of the Fourteenth National Conference on Artificial
Intelligence, pages 591-596. AAAI.
. Marcus, M. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313-330.
. Munoz, V. Punyakanok, D. Roth, and D. Zimak.
1999. A learning approach to shallow parsing. In
Proceedings of EMNLP-WVLC)99. Association for
Computational Linguistics.
Ramshaw and M. Marcus. 1999. Text chunk-
ing using transformation-based learning. In Natu-
ral Language Processing Using Very Large Corpora.
Kluwer.
H. S. Seung, M. Upper, and H. Sompolinsky. 1992.
Query by committee. In Proceedings of the Fifth
Annual ACM Workshop on Computational Learn-
ing Theory, pages 287-294. ACM.
The XTAG Research Group. 1998. A lexicalized tree
adjoining grammar for english. Technical Report
IRCS Tech Report 98-18, University of Pennsylvan-
nia.
E. Tjong Kim Sang and J. Veenstra. 1999. Repre-
senting text chunks. In Proceedings of EACL &apos;99.
ACL.
J. Veenstra. 1998. Fast NP chunking using memory-
based learning techniques. In BENELEARN-98:
Proceedings of the Eighth Belgian-Dutch Conference
on Machine Learning, Wageningen, the Nether-
lands.
A. Voutilainen. 1993. NPTool, a detector of English
noun phrases. In Proceedings of the Workshop on
Very Large Corpora, pages 48-57. Association for
Computational Linguistics.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.700039">
<title confidence="0.995324">Rule Writing or Annotation: Cost-efficient Resource Usage for Base Noun Phrase Chunking</title>
<author confidence="0.999873">Grace Ngai</author>
<author confidence="0.999873">David Yarowsky</author>
<affiliation confidence="0.9995525">Department of Computer Science Johns Hopkins University</affiliation>
<address confidence="0.999995">Baltimore, MD 21218, U.S.A.</address>
<email confidence="0.998521">gyn,yarowsky}@cs.jhu.edu</email>
<abstract confidence="0.982615470588235">This paper presents a comprehensive empirical comparison between two approaches for developing a base noun phrase chunker: human rule writing and active learning using interactive realtime human annotation. Several novel variations on active learning are investigated, and underlying cost models for cross-modal machine learning comparison are presented and explored. Results show that it is more efficient and more successful by several measures to train a system using active learning annotation rather than hand-crafted rule writing at a comparable level of human labor investment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Argamon</author>
<author>I Dagan</author>
<author>Y Krymolowski</author>
</authors>
<title>A memory-based approach to learning shallow language patterns.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics,</booktitle>
<pages>67--73</pages>
<publisher>COLING-ACL.</publisher>
<marker>Argamon, Dagan, Krymolowski, 1998</marker>
<rawString>S. Argamon, I. Dagan, and Y. Krymolowski. 1998. A memory-based approach to learning shallow language patterns. In Proceedings of the 17th International Conference on Computational Linguistics, pages 67-73. COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bourigault</author>
</authors>
<title>Surface grammatical analysis for the extraction of terminological noun phrases.</title>
<date>1992</date>
<journal>Association of Computational Linguistics.</journal>
<booktitle>In Proceedings of the 30th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>977--981</pages>
<contexts>
<context position="2722" citStr="Bourigault (1992)" startWordPosition="421" endWordPosition="422">nd empirical learning are evaluated relative to effort invested. Such a multi-factor cost analysis is long overdue. This paper will conclude with a comprehensive cost model exposition and analysis, and an empirical study contrasting human rule-writing versus annotation-based learning approaches that are sensitive to these cost models. 2 Base Noun Phrase Chunking The domain in which our experiments are performed is base noun phrase chunking. A significant amount of work has been done in this domain and many different methods have been applied: Church&apos;s PARTS (1988) program used a Markov model; Bourigault (1992) used heuristics along with a grammar; Voutilainen&apos;s NPTool (1993) used a lexicon combined with a constraint grammar; Juteson and Katz (1995) used repeated phrases; Veenstra (1998), Argamon, Dagan &amp; Krymolowski (1998), Daelemans, van den Bosch &amp; Zavrel (1999) and Tjong Kim Sang &amp; Veenstra (1999) used memory-based systems; Ramshaw &amp; Marcus (1999) and Cardie &amp; Pierce (1998) used rule-based systems, Munoz et al. (1999) used a Winnow-based system, and the XTAG Research Group(1998) used a tree-adjoining grammar. Of all the systems, Ramshaw &amp; Marcus&apos; transformation rule-based system had the best pub</context>
</contexts>
<marker>Bourigault, 1992</marker>
<rawString>D. Bourigault. 1992. Surface grammatical analysis for the extraction of terminological noun phrases. In Proceedings of the 30th Annual Meeting of the Association of Computational Linguistics, pages 977-981. Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>G Ngai</author>
</authors>
<title>Man vs. machine: A case study in base noun phrase chunking.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL 1999. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="11946" citStr="Brill &amp; Ngai (1999)" startWordPosition="1941" endWordPosition="1944">e sentences drawn randomly from the test set. The purpose of this final annotation was to judge how well annotators tag sentences drawn with the true distribution from the test corpus, as we shall see in section 5. On average, the annotators took 17 minutes to annotate each set of 50 sentences, ranging from 8 to 30 minutes. The average amount of time the server took to run the active learning algorithm and select the next batch of sentences was approximately 3 minutes, a rest break for the annotators. The analysis of the results is presented in section 5. 4 Learning by Rules In previous work, Brill &amp; Ngai (1999) showed that under certain circumstances, it is possible for humans writing rules to perform as well as a stateof-the-art machine learning system for base noun phrase chunking. What that study did not address, however, was the cost of the human labor and/or machine cycles involved to construct such a system, nor the relative cost of obtaining the training data for the machine learning system. This paper will estimate and contrast these costs relative to performance. To investigate the costs of a human rule-writing system, we used a similar framework to that of Brill &amp; Ngai. The system was writ</context>
<context position="14001" citStr="Brill &amp; Ngai (1999)" startWordPosition="2317" endWordPosition="2320">oving A Bracket Brill &amp; Ngai TY: A TY: S TY: T 1999 LC: null LC: null LC: &lt;&lt;&lt; ({1} w=abouD Rule Format TAR: ({1} t=DT) (* t=JJ[RS]?) \ TAR1: (* t= \ w+) (± t=NNP?S?) TAR: ({1} t=$) (± t=OD) (+ t=NNP?S?) MC: null RC: null RC: null TAR2: (* t=JJ [RS]?) ({1} w= \w+day) RC: null New { _DT ADJ* NOUN+ } [ { ANYWORD* NOUN+ } t ADJ* TIMEDAY } 1 { about_ [ _$ NUM+ ] } Rule Format Effect of ThepT manNN ranvpp • Rule about/N ( $$ 5cp ) Application ( ThepT man ) ranvpp • ( NewNNp YorkNNp FicidayNNp ) ( abOUt IN $$ 5CD ) ( NOWNN p YOrkNNp ) ( FridaYNNP ) Table 1: Comparison of our current rule format with Brill &amp; Ngai (1999) rules are appended onto the end of the list and each rule applied in order, in the paradigm of a transformation-based rule list. 4.1 Rule-Writing Experiments The rule-writing experiments were conducted by a group of 17 advanced computer science students, using the identical test set as in the annotation experiments and the same initial 100 gold standard sentences for both initial bracketing standards guidance and rule-quality feedback throughout their work. Grab-all rule _RB::? ADJ* ANOUN* ADJ* ANOUN+ (blah blah last Fri)-&gt;(blah blah) (last Fri) [ ANYTHING* _JJ TIME_W ] [ NOT_ADJ+ TIME_W ] ab</context>
</contexts>
<marker>Brill, Ngai, 1999</marker>
<rawString>E. Brill and G. Ngai. 1999. Man vs. machine: A case study in base noun phrase chunking. In Proceedings of ACL 1999. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language: A case study in part of speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<pages>543--565</pages>
<contexts>
<context position="6302" citStr="Brill, 1995" startWordPosition="980" endWordPosition="981"> examples e whose class-conditional a posteriori probability P(Cie) is closest to 0.5 (for a 2-class problem). Engelson &amp; Dagan implemented a committee of learners, and used vote entropy to pick examples which had the highest disagreement among the learners. In addition, Engelson &amp; Dagan also investigate several different selection techniques in depth. 3.2 New Applications and Algorithmic Extensions in Active Learning To our knowledge, this paper constitutes the first work to apply active learning to base noun phrase chunking, or to apply active learning to a transformation-learning paradigm (Brill, 1995) for any application. Since a transformation-based learner does not give a probabilistic output, we are not able to use Lewis &amp; Gale&apos;s method for determining uncertainty. Our experimental framework thus uses the query by committee paradigm with batch selection: 1. Given a corpus C, arbitrarily pick t sentences for annotation. 2. Have these t sentences hand-annotated, delete them from C and put them into a training set, T. 3. Divide T into m non-identical, but not necessarily non-overlapping, subsets. 4. Use each subset as the training set for a model. 5. Evaluate each model on the remaining se</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>E. Brill. 1995. Transformation-based error-driven learning and natural language: A case study in part of speech tagging. Computational Linguistics, 21 (4): 543-565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cardie</author>
<author>D Pierce</author>
</authors>
<title>Error-driven pruning of treebank gramars for base noun phrase identification.</title>
<date>1998</date>
<journal>Association of Computational Linguistics.</journal>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>218--224</pages>
<contexts>
<context position="3096" citStr="Cardie &amp; Pierce (1998)" startWordPosition="477" endWordPosition="480">domain in which our experiments are performed is base noun phrase chunking. A significant amount of work has been done in this domain and many different methods have been applied: Church&apos;s PARTS (1988) program used a Markov model; Bourigault (1992) used heuristics along with a grammar; Voutilainen&apos;s NPTool (1993) used a lexicon combined with a constraint grammar; Juteson and Katz (1995) used repeated phrases; Veenstra (1998), Argamon, Dagan &amp; Krymolowski (1998), Daelemans, van den Bosch &amp; Zavrel (1999) and Tjong Kim Sang &amp; Veenstra (1999) used memory-based systems; Ramshaw &amp; Marcus (1999) and Cardie &amp; Pierce (1998) used rule-based systems, Munoz et al. (1999) used a Winnow-based system, and the XTAG Research Group(1998) used a tree-adjoining grammar. Of all the systems, Ramshaw &amp; Marcus&apos; transformation rule-based system had the best published performance (f-measure 92.0) for several years, and is regarded as the de facto standard for the domain. Although several systems have recently achieved slightly higher published results (Munoz et al.: 92.8, Tjong Kim Sang &amp; Veenstra: 92.37, XTAG Research Group: 92.4), their algorithms are significantly more costly, or not feasible, to implement in an active learni</context>
</contexts>
<marker>Cardie, Pierce, 1998</marker>
<rawString>C. Cardie and D. Pierce. 1998. Error-driven pruning of treebank gramars for base noun phrase identification. In Proceedings of the 36th Annual Meeting of the Association of Computational Linguistics, pages 218-224. Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.</title>
<date>1988</date>
<journal>Association of Computational Linguistics.</journal>
<booktitle>In Proceedings of the Second Conference on Applied Natural Language Processing,</booktitle>
<pages>136--143</pages>
<marker>Church, 1988</marker>
<rawString>K. Church. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Proceedings of the Second Conference on Applied Natural Language Processing, pages 136-143. Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>A van den Bosch</author>
<author>J Zavrel</author>
</authors>
<title>Forgetting exceptions is harmful in language learning.</title>
<date>1999</date>
<booktitle>In Machine Learning, special issue on natural language learning,</booktitle>
<volume>11</volume>
<pages>11--43</pages>
<note>to appear.</note>
<marker>Daelemans, van den Bosch, Zavrel, 1999</marker>
<rawString>W. Daelemans, A. van den Bosch, and J. Zavrel. 1999. Forgetting exceptions is harmful in language learning. In Machine Learning, special issue on natural language learning, volume 11, pages 11-43. to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Aberdeen Day</author>
<author>L Hirschman</author>
<author>R Kozierok</author>
<author>P Robinson</author>
<author>M Vilain</author>
</authors>
<title>Mixed-initiative development of language processing systems.</title>
<date>1997</date>
<booktitle>In Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>348--355</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="9676" citStr="Day et al., 1997" startWordPosition="1551" endWordPosition="1554">t assumes that a true active learning situation would have access to someone who could annotate with perfect consistency to the gold standard corpus annotation conventions. Because our goal is to investigate the relative costs of rule writing versus annotation, it is essential that we use a realistic model of annotation. Therefore, we decided to do a fully-fledged active learning annotation experiment, with real time human supervision, rather than assume the simulated feedback of actual Treebank annotators. We developed an annotation tool that is modeled on MITRE&apos;s Alembic Workbench software (Day et al., 1997), but written in Java for platform-independence. To enable data storage and the active learning sample selection to take place on the more powerful machines in our lab rather than the user&apos;s home machine, the tool was designed with network support so that it could communicate with our servers over the internet. Our real-time active learning experiment subjects were seven graduate students in computer science. Five of them are native English speakers, but none had any formal linguistics training. The initial training set T is the first 100 sentences of Ramshaw &amp; Marcus&apos; training set. To acquain</context>
</contexts>
<marker>Day, Hirschman, Kozierok, Robinson, Vilain, 1997</marker>
<rawString>Day, J. Aberdeen, L. Hirschman, R. Kozierok, P. Robinson, and M. Vilain. 1997. Mixed-initiative development of language processing systems. In Fifth Conference on Applied Natural Language Processing, pages 348-355. Association for Computational Linguistics, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Engelson</author>
<author>I Dagan</author>
</authors>
<title>Minimizing manual annotation cost in supervised training from corpora.</title>
<date>1996</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="5522" citStr="Engelson &amp; Dagan (1996)" startWordPosition="860" endWordPosition="863"> Freund et al. (1997) proposed a theoretical queryby-committee approach. Such an approach uses multiple models (or a committee) to evaluate the data, and candidates for annotation (or queries) are drawn from the pool of examples in which the models disagree. Furthermore, Freund et al. prove that, under some situations, the generalization error decreases exponentially with the number of queries. On the experimental side, active learning has been applied to several different problems. Lewis &amp; Gale (1994), Lewis &amp; Catlett (1994) and Liere &amp; Tadepalli (1997) all applied it to text categorization; Engelson &amp; Dagan (1996) applied it to part-of-speech tagging. Each approach has its own way of determining uncertainty in examples. Lewis &amp; Gale used a probabilistic classifier and picked the examples e whose class-conditional a posteriori probability P(Cie) is closest to 0.5 (for a 2-class problem). Engelson &amp; Dagan implemented a committee of learners, and used vote entropy to pick examples which had the highest disagreement among the learners. In addition, Engelson &amp; Dagan also investigate several different selection techniques in depth. 3.2 New Applications and Algorithmic Extensions in Active Learning To our kno</context>
</contexts>
<marker>Engelson, Dagan, 1996</marker>
<rawString>Engelson and I. Dagan. 1996. Minimizing manual annotation cost in supervised training from corpora. In Proceedings of ACL 1996. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H S Seung Freund</author>
<author>E Shamir</author>
<author>N Tishby</author>
</authors>
<title>Selective sampling using the query by committee algorithm.</title>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<pages>28--133</pages>
<contexts>
<context position="4920" citStr="Freund et al. (1997)" startWordPosition="764" endWordPosition="768"> possible to provide the necessary amount of information with less data. Active learning attempts to perform this intelligent sampling of data to reduce annotation costs without damaging performance. In general, these methods calculate the usefulness of an example by first having the learner classify it, and then seeing how uncertain that classification was. The idea is that the more uncertain the example, the less well modeled this situation is, and therefore, the more useful it would be to have this example annotated. 3.1 Prior Work in Active Learning Seung, Opper and Sompolinsky (1992) and Freund et al. (1997) proposed a theoretical queryby-committee approach. Such an approach uses multiple models (or a committee) to evaluate the data, and candidates for annotation (or queries) are drawn from the pool of examples in which the models disagree. Furthermore, Freund et al. prove that, under some situations, the generalization error decreases exponentially with the number of queries. On the experimental side, active learning has been applied to several different problems. Lewis &amp; Gale (1994), Lewis &amp; Catlett (1994) and Liere &amp; Tadepalli (1997) all applied it to text categorization; Engelson &amp; Dagan (199</context>
</contexts>
<marker>Freund, Shamir, Tishby, 1997</marker>
<rawString>Freund, H. S. Seung, E. Shamir, and N. Tishby. 1997. Selective sampling using the query by committee algorithm. Machine Learning, 28:133-168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juteson</author>
<author>S Katz</author>
</authors>
<title>Technical terminology: Some linguistic properties and an algorithm for identification in text. Natural Language Engineering,</title>
<date>1995</date>
<contexts>
<context position="2863" citStr="Juteson and Katz (1995)" startWordPosition="441" endWordPosition="444">clude with a comprehensive cost model exposition and analysis, and an empirical study contrasting human rule-writing versus annotation-based learning approaches that are sensitive to these cost models. 2 Base Noun Phrase Chunking The domain in which our experiments are performed is base noun phrase chunking. A significant amount of work has been done in this domain and many different methods have been applied: Church&apos;s PARTS (1988) program used a Markov model; Bourigault (1992) used heuristics along with a grammar; Voutilainen&apos;s NPTool (1993) used a lexicon combined with a constraint grammar; Juteson and Katz (1995) used repeated phrases; Veenstra (1998), Argamon, Dagan &amp; Krymolowski (1998), Daelemans, van den Bosch &amp; Zavrel (1999) and Tjong Kim Sang &amp; Veenstra (1999) used memory-based systems; Ramshaw &amp; Marcus (1999) and Cardie &amp; Pierce (1998) used rule-based systems, Munoz et al. (1999) used a Winnow-based system, and the XTAG Research Group(1998) used a tree-adjoining grammar. Of all the systems, Ramshaw &amp; Marcus&apos; transformation rule-based system had the best published performance (f-measure 92.0) for several years, and is regarded as the de facto standard for the domain. Although several systems have</context>
</contexts>
<marker>Juteson, Katz, 1995</marker>
<rawString>Juteson and S. Katz. 1995. Technical terminology: Some linguistic properties and an algorithm for identification in text. Natural Language Engineering, 1:9-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lewis</author>
<author>J Catlett</author>
</authors>
<title>Heterogeneous uncertainty sampling for supervised learning.</title>
<date>1994</date>
<booktitle>In Proceedings of the 11th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="5430" citStr="Lewis &amp; Catlett (1994)" startWordPosition="844" endWordPosition="847">xample annotated. 3.1 Prior Work in Active Learning Seung, Opper and Sompolinsky (1992) and Freund et al. (1997) proposed a theoretical queryby-committee approach. Such an approach uses multiple models (or a committee) to evaluate the data, and candidates for annotation (or queries) are drawn from the pool of examples in which the models disagree. Furthermore, Freund et al. prove that, under some situations, the generalization error decreases exponentially with the number of queries. On the experimental side, active learning has been applied to several different problems. Lewis &amp; Gale (1994), Lewis &amp; Catlett (1994) and Liere &amp; Tadepalli (1997) all applied it to text categorization; Engelson &amp; Dagan (1996) applied it to part-of-speech tagging. Each approach has its own way of determining uncertainty in examples. Lewis &amp; Gale used a probabilistic classifier and picked the examples e whose class-conditional a posteriori probability P(Cie) is closest to 0.5 (for a 2-class problem). Engelson &amp; Dagan implemented a committee of learners, and used vote entropy to pick examples which had the highest disagreement among the learners. In addition, Engelson &amp; Dagan also investigate several different selection techni</context>
</contexts>
<marker>Lewis, Catlett, 1994</marker>
<rawString>Lewis and J. Catlett. 1994. Heterogeneous uncertainty sampling for supervised learning. In Proceedings of the 11th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lewis</author>
<author>W Gale</author>
</authors>
<title>A sequential algorithm for training text classifiers.</title>
<date>1994</date>
<booktitle>In Proceedings of ACMSICIR</booktitle>
<publisher>AC M-SIGIR.</publisher>
<contexts>
<context position="5406" citStr="Lewis &amp; Gale (1994)" startWordPosition="840" endWordPosition="843">uld be to have this example annotated. 3.1 Prior Work in Active Learning Seung, Opper and Sompolinsky (1992) and Freund et al. (1997) proposed a theoretical queryby-committee approach. Such an approach uses multiple models (or a committee) to evaluate the data, and candidates for annotation (or queries) are drawn from the pool of examples in which the models disagree. Furthermore, Freund et al. prove that, under some situations, the generalization error decreases exponentially with the number of queries. On the experimental side, active learning has been applied to several different problems. Lewis &amp; Gale (1994), Lewis &amp; Catlett (1994) and Liere &amp; Tadepalli (1997) all applied it to text categorization; Engelson &amp; Dagan (1996) applied it to part-of-speech tagging. Each approach has its own way of determining uncertainty in examples. Lewis &amp; Gale used a probabilistic classifier and picked the examples e whose class-conditional a posteriori probability P(Cie) is closest to 0.5 (for a 2-class problem). Engelson &amp; Dagan implemented a committee of learners, and used vote entropy to pick examples which had the highest disagreement among the learners. In addition, Engelson &amp; Dagan also investigate several di</context>
</contexts>
<marker>Lewis, Gale, 1994</marker>
<rawString>Lewis and W. Gale. 1994. A sequential algorithm for training text classifiers. In Proceedings of ACMSICIR 1994. AC M-SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liere</author>
<author>P Tadepalli</author>
</authors>
<title>Active learning with committees for text categorization.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth National Conference on Artificial Intelligence,</booktitle>
<pages>591--596</pages>
<publisher>AAAI.</publisher>
<contexts>
<context position="5459" citStr="Liere &amp; Tadepalli (1997)" startWordPosition="849" endWordPosition="852"> Work in Active Learning Seung, Opper and Sompolinsky (1992) and Freund et al. (1997) proposed a theoretical queryby-committee approach. Such an approach uses multiple models (or a committee) to evaluate the data, and candidates for annotation (or queries) are drawn from the pool of examples in which the models disagree. Furthermore, Freund et al. prove that, under some situations, the generalization error decreases exponentially with the number of queries. On the experimental side, active learning has been applied to several different problems. Lewis &amp; Gale (1994), Lewis &amp; Catlett (1994) and Liere &amp; Tadepalli (1997) all applied it to text categorization; Engelson &amp; Dagan (1996) applied it to part-of-speech tagging. Each approach has its own way of determining uncertainty in examples. Lewis &amp; Gale used a probabilistic classifier and picked the examples e whose class-conditional a posteriori probability P(Cie) is closest to 0.5 (for a 2-class problem). Engelson &amp; Dagan implemented a committee of learners, and used vote entropy to pick examples which had the highest disagreement among the learners. In addition, Engelson &amp; Dagan also investigate several different selection techniques in depth. 3.2 New Applic</context>
</contexts>
<marker>Liere, Tadepalli, 1997</marker>
<rawString>Liere and P. Tadepalli. 1997. Active learning with committees for text categorization. In Proceedings of the Fourteenth National Conference on Artificial Intelligence, pages 591-596. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcinkiewicz Marcus</author>
<author>B Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<pages>19--2</pages>
<marker>Marcus, Santorini, 1993</marker>
<rawString>. Marcus, M. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok Munoz</author>
<author>D Roth</author>
<author>D Zimak</author>
</authors>
<title>A learning approach to shallow parsing.</title>
<date>1999</date>
<booktitle>In Proceedings of EMNLP-WVLC)99. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3141" citStr="Munoz et al. (1999)" startWordPosition="484" endWordPosition="487"> base noun phrase chunking. A significant amount of work has been done in this domain and many different methods have been applied: Church&apos;s PARTS (1988) program used a Markov model; Bourigault (1992) used heuristics along with a grammar; Voutilainen&apos;s NPTool (1993) used a lexicon combined with a constraint grammar; Juteson and Katz (1995) used repeated phrases; Veenstra (1998), Argamon, Dagan &amp; Krymolowski (1998), Daelemans, van den Bosch &amp; Zavrel (1999) and Tjong Kim Sang &amp; Veenstra (1999) used memory-based systems; Ramshaw &amp; Marcus (1999) and Cardie &amp; Pierce (1998) used rule-based systems, Munoz et al. (1999) used a Winnow-based system, and the XTAG Research Group(1998) used a tree-adjoining grammar. Of all the systems, Ramshaw &amp; Marcus&apos; transformation rule-based system had the best published performance (f-measure 92.0) for several years, and is regarded as the de facto standard for the domain. Although several systems have recently achieved slightly higher published results (Munoz et al.: 92.8, Tjong Kim Sang &amp; Veenstra: 92.37, XTAG Research Group: 92.4), their algorithms are significantly more costly, or not feasible, to implement in an active learning framework. To facilitate contrastive studi</context>
</contexts>
<marker>Munoz, Roth, Zimak, 1999</marker>
<rawString>. Munoz, V. Punyakanok, D. Roth, and D. Zimak. 1999. A learning approach to shallow parsing. In Proceedings of EMNLP-WVLC)99. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramshaw</author>
<author>M Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1999</date>
<booktitle>In Natural Language Processing Using Very Large Corpora.</booktitle>
<publisher>Kluwer.</publisher>
<contexts>
<context position="3069" citStr="Ramshaw &amp; Marcus (1999)" startWordPosition="472" endWordPosition="475">se Noun Phrase Chunking The domain in which our experiments are performed is base noun phrase chunking. A significant amount of work has been done in this domain and many different methods have been applied: Church&apos;s PARTS (1988) program used a Markov model; Bourigault (1992) used heuristics along with a grammar; Voutilainen&apos;s NPTool (1993) used a lexicon combined with a constraint grammar; Juteson and Katz (1995) used repeated phrases; Veenstra (1998), Argamon, Dagan &amp; Krymolowski (1998), Daelemans, van den Bosch &amp; Zavrel (1999) and Tjong Kim Sang &amp; Veenstra (1999) used memory-based systems; Ramshaw &amp; Marcus (1999) and Cardie &amp; Pierce (1998) used rule-based systems, Munoz et al. (1999) used a Winnow-based system, and the XTAG Research Group(1998) used a tree-adjoining grammar. Of all the systems, Ramshaw &amp; Marcus&apos; transformation rule-based system had the best published performance (f-measure 92.0) for several years, and is regarded as the de facto standard for the domain. Although several systems have recently achieved slightly higher published results (Munoz et al.: 92.8, Tjong Kim Sang &amp; Veenstra: 92.37, XTAG Research Group: 92.4), their algorithms are significantly more costly, or not feasible, to im</context>
<context position="7309" citStr="Ramshaw &amp; Marcus (1999)" startWordPosition="1157" endWordPosition="1161">om C and put them into a training set, T. 3. Divide T into m non-identical, but not necessarily non-overlapping, subsets. 4. Use each subset as the training set for a model. 5. Evaluate each model on the remaining sentences in C 6. Using a measure of disagreement D, pick the x sentences in C with the highest D for annotation. 7. Delete the x sentences from C, have them annotated, and add them to T. 8. Repeat from 3. In our experiments, the initial corpus C that we used consisted of sections 15-18 of the Wall Street Journal Treebank (Marcus et al., 1993), which is also the training set used by Ramshaw &amp; Marcus (1999). The initial t sentences were the first 100 sentences of the training corpus, and x = 50 sentences were picked at each iteration. Sets of 50 sentences were selected because it takes approximately 15-30 minutes for humans to annotate them, a reasonable amount of work and time for the annotator to spend before taking a break while the machine selects the next set. The parameter m, which denotes the number of models to train, was set at 3, which could be expected to give us reasonable labelling variation over the samples, but also would not cause the processing phase to take a long time. To divi</context>
</contexts>
<marker>Ramshaw, Marcus, 1999</marker>
<rawString>Ramshaw and M. Marcus. 1999. Text chunking using transformation-based learning. In Natural Language Processing Using Very Large Corpora. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H S Seung</author>
<author>M Upper</author>
<author>H Sompolinsky</author>
</authors>
<title>Query by committee.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory,</booktitle>
<pages>287--294</pages>
<publisher>ACM.</publisher>
<marker>Seung, Upper, Sompolinsky, 1992</marker>
<rawString>H. S. Seung, M. Upper, and H. Sompolinsky. 1992. Query by committee. In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, pages 287-294. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>The XTAG Research Group</author>
</authors>
<title>A lexicalized tree adjoining grammar for english.</title>
<date>1998</date>
<tech>Technical Report IRCS Tech Report 98-18,</tech>
<institution>University of Pennsylvannia.</institution>
<marker>Group, 1998</marker>
<rawString>The XTAG Research Group. 1998. A lexicalized tree adjoining grammar for english. Technical Report IRCS Tech Report 98-18, University of Pennsylvannia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Tjong Kim Sang</author>
<author>J Veenstra</author>
</authors>
<title>Representing text chunks.</title>
<date>1999</date>
<booktitle>In Proceedings of EACL &apos;99.</booktitle>
<publisher>ACL.</publisher>
<contexts>
<context position="3018" citStr="Sang &amp; Veenstra (1999)" startWordPosition="465" endWordPosition="468">ches that are sensitive to these cost models. 2 Base Noun Phrase Chunking The domain in which our experiments are performed is base noun phrase chunking. A significant amount of work has been done in this domain and many different methods have been applied: Church&apos;s PARTS (1988) program used a Markov model; Bourigault (1992) used heuristics along with a grammar; Voutilainen&apos;s NPTool (1993) used a lexicon combined with a constraint grammar; Juteson and Katz (1995) used repeated phrases; Veenstra (1998), Argamon, Dagan &amp; Krymolowski (1998), Daelemans, van den Bosch &amp; Zavrel (1999) and Tjong Kim Sang &amp; Veenstra (1999) used memory-based systems; Ramshaw &amp; Marcus (1999) and Cardie &amp; Pierce (1998) used rule-based systems, Munoz et al. (1999) used a Winnow-based system, and the XTAG Research Group(1998) used a tree-adjoining grammar. Of all the systems, Ramshaw &amp; Marcus&apos; transformation rule-based system had the best published performance (f-measure 92.0) for several years, and is regarded as the de facto standard for the domain. Although several systems have recently achieved slightly higher published results (Munoz et al.: 92.8, Tjong Kim Sang &amp; Veenstra: 92.37, XTAG Research Group: 92.4), their algorithms ar</context>
</contexts>
<marker>Sang, Veenstra, 1999</marker>
<rawString>E. Tjong Kim Sang and J. Veenstra. 1999. Representing text chunks. In Proceedings of EACL &apos;99. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Veenstra</author>
</authors>
<title>Fast NP chunking using memorybased learning techniques.</title>
<date>1998</date>
<booktitle>In BENELEARN-98: Proceedings of the Eighth Belgian-Dutch Conference on Machine Learning,</booktitle>
<location>Wageningen, the Netherlands.</location>
<contexts>
<context position="2902" citStr="Veenstra (1998)" startWordPosition="448" endWordPosition="449">n and analysis, and an empirical study contrasting human rule-writing versus annotation-based learning approaches that are sensitive to these cost models. 2 Base Noun Phrase Chunking The domain in which our experiments are performed is base noun phrase chunking. A significant amount of work has been done in this domain and many different methods have been applied: Church&apos;s PARTS (1988) program used a Markov model; Bourigault (1992) used heuristics along with a grammar; Voutilainen&apos;s NPTool (1993) used a lexicon combined with a constraint grammar; Juteson and Katz (1995) used repeated phrases; Veenstra (1998), Argamon, Dagan &amp; Krymolowski (1998), Daelemans, van den Bosch &amp; Zavrel (1999) and Tjong Kim Sang &amp; Veenstra (1999) used memory-based systems; Ramshaw &amp; Marcus (1999) and Cardie &amp; Pierce (1998) used rule-based systems, Munoz et al. (1999) used a Winnow-based system, and the XTAG Research Group(1998) used a tree-adjoining grammar. Of all the systems, Ramshaw &amp; Marcus&apos; transformation rule-based system had the best published performance (f-measure 92.0) for several years, and is regarded as the de facto standard for the domain. Although several systems have recently achieved slightly higher publ</context>
</contexts>
<marker>Veenstra, 1998</marker>
<rawString>J. Veenstra. 1998. Fast NP chunking using memorybased learning techniques. In BENELEARN-98: Proceedings of the Eighth Belgian-Dutch Conference on Machine Learning, Wageningen, the Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Voutilainen</author>
</authors>
<title>NPTool, a detector of English noun phrases.</title>
<date>1993</date>
<booktitle>In Proceedings of the Workshop on Very Large Corpora,</booktitle>
<pages>48--57</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Voutilainen, 1993</marker>
<rawString>A. Voutilainen. 1993. NPTool, a detector of English noun phrases. In Proceedings of the Workshop on Very Large Corpora, pages 48-57. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>