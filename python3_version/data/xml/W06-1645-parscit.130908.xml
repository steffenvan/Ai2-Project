<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.988052">
Text data acquisition for domain-specific language models
</title>
<author confidence="0.910268">
Abhinav Sethy, Panayiotis G. Georgiou, Shrikanth Narayanan
</author>
<affiliation confidence="0.9786338">
Speech Analysis and Interpretation Lab
Integrated Media Systems Center
Viterbi School of Engineering
pepartment of Electrical Engineering-Systems
University of Southern California
</affiliation>
<sectionHeader confidence="0.980152" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999414">
he language modeling community is
showing a growing interest in using large
collections of text mined from the World
Wide Web (WWW) to supplement sparse
in-domain text resources. However, in
most cases the style and content of the text
harvested from these corpora differs sig-
nificantly from the specific nature of these
domains. In this paper we present a rel-
ative entropy (r.e.) based method to se-
1ect relevant subsets of sentences whose
distribution in an n-gram sense matches
the domain of interest. Using simulations,
we provide an analysis of how the pro-
posed scheme outperforms filtering tech-
niques proposed in recent language mod-
eling literature on mining text from the
web. A comparative study is presented us-
ing a text collection of over 800M words
collected from the WWW. Experimental
results show that by using the proposed
subset selection scheme we can get per-
Formance improvement in both Word Er-
ror Rate (WER) and Perplexity (PPL) over
the models built from the entire collection
by using just 10% of the data Improve-
ments in data selection also translated to a
significant reduction in the vocabulary size
as well as the number of estimated para-
meters in the adapted language model.
</bodyText>
<sectionHeader confidence="0.999111" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.991708054054054">
domains and applications is to identify the appro-
[13.riate text resources for building language models
matched to that application or domain. In most
cases, this data is not readily available and needs
to be collected manually, which is an expensive
and time consuming process.
This has naturally led to a growing interest in
using the World Wide Web (WWW) as a corpus
for building statistical models (Lapata, 2005; La-
pata, 2002; Resnik, 2003). Text harvested from
the web combined with other large text collec-
tions such as GigaWord provides a good resource
to supplement the in-domain data for a variety of
applications. However, text gathered from such
generic sources rarely fits the demands or the na-
ture of the domain of interest completely. Even
with the best queries and web crawling schemes,
both the style and content of the data will usu-
ally differ significantly from the specific nature
of the domain of interest. For example, a speech
recognition system requires conversational style
text whereas most of the data on the web is lit-
erary.
In most cases we have available to us a set of
in-domain example sentences which we can use
in a semi-supervised (Nigam, 2000; Zhu, 2005)
fashion to refine our selection of text. Recent lit-
erature on building language models with text ac-
quired from the web addresses the issue of mis-
match, partly by using various rank-and-select
schemes for identifying sentences from the web-
data&apos; which match the in-domain data (Ostendorf,
2005; Sarikaya, 2005). The central idea behind
these schemes is to rank order sentences in terms
State of the art speech and Natural Language of their match to the seed in-domain set, and then
lprocessing (NLP) systems are data-driven, relying select the top sentences.
on learning patterns from large amounts of data. 1We will use web-data to refer to text harvested from web
</bodyText>
<note confidence="0.617675">
A key step in creating such systems for different and other generic sources.
</note>
<page confidence="0.875136">
382
</page>
<note confidence="0.808022">
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 382-389,
Sydney, July 2006. C)2006 Association for Computational Linguistics
Research in semi-supervised learning for clas- negative or noise set N. A binary classifier is then
</note>
<bodyText confidence="0.998955545454545">
sification (Zhu (2005) presents a good survey) has
Ishown the need to balance the unlabeled data.
trained using the in-domain set I and the negative
set. The classifier is then used to label sentences in
We believe that similar to the question of balance
in semi-supervised learning for classification, we
need to address the question of distributional sim-
ilarity while selecting the appropriate sentences
for building a language model from noisy data.
Rank-and-select filtering schemes select individ-
pal sentences on the merit of their match to the in-
domain model. As a result, even though individual
sentences might be good in-domain examples, the
overall distribution of the selected set will focus
solely on the high probability regions of the distri-
bution. This will be made more clear in simulation
results described in section 4.
To address the issue of distributional similar-
ity we present an incremental selection algorithm
which compares the distribution of the selected set
and the in-domain examples by using a relative en-
tropy (r.e.) criterion at each step. Some ranking
schemes, which provide baseline for performance
comparison are reviewed in section 2. The pro-
posed algorithm is described in section 3. Sec-
the web-data. The classifier can then be iteratively
refined by using a better and larger subset of the
I/N sentences selected in each iteration.
Rank ordering schemes do not address the issue
of distributional similarity and select many sen-
tences which already have a high probability in the
in-domain text. Adapting models on such data has
the tendency to skew the distribution even further
towards the center. For example, in our doctor-
patient interaction task, short sentences containing
the word &apos;okay&apos; such as cokay&apos;,`yes okay&apos;, &apos;okay
okay&apos; were very frequent in the in-domain data.
Perplexity and other similarity measures assign a
high score to all such examples in the web-data,
increasing the probability of these words even fur-
ther. In contrast other pertinent sentences seen
rarely in the in-domain data such as &apos;Can you
stand up please?&apos; receive a low rank and are more
likely to be rejected.
</bodyText>
<sectionHeader confidence="0.998264" genericHeader="method">
3 Incremental Selection
</sectionHeader>
<bodyText confidence="0.998198529411765">
ion 4 will provide an analysis of how the proposed To address the shortcomings of the rank-and-select
algorithm improves upon rank and select schemes. schemes we need to ensure that the set of sen-
Experimental results are provided in section 5. We tences that we select from web-data has a distribu--
conclude with a summary of this work and direc- tion similar to the in-domain distribution. Thus we
tions for future research.
2 Rank and select methods for text
cleaning
need to move from selecting sentences on the basis
of individual score to selecting a set of sentences
as a group. We propose an incremental greedy sen-
tence selection algorithm based on relative entropy
In recent literature, the central idea behind text which selects a sentence if adding it to the already
cleanup schemes for using web-data to build lan- selected set of sentences reduces the relative en-
guage models, has been to use a scoring function tropy with respect to the in-domain data distribu-
hat measures the similarity of each observed sen- tion. To describe our algorithm we will employ
Itence in the web-data to the in-domain set and as- unigram probabilities though the method general-
sign an appropriate score. The subsequent step izes to higher order n-grams also.
is to set a threshold in terms of either the mini-
mum score or the number of top scoring sentences. 3.1 The Basic Algorithm
The threshold can usually be fixed using a held- Let us denote the language model built from &apos;fl-
out set. Ostendorf (2005) use perplexity from an domain data by P. We also need a language model
in-domain n-gram language model as a scoring Fnit to initialize our selection algorithm. We ex-
function. More recently, a modified version of perimented with two methods for selecting the mi-
the BLEU metric which measures sentence simi- tial model. The first is to use a uniform distrib-
lanty in machine translation has been proposed by
Sarikaya (2005) as a scoring function. Instead of
explicit ranking and thresholdmg it is also possible
to design a classifier to learn from positive and un-
ution over the vocabulary. The second approach
is to sample with replacement the in-domain data
and get a bagged estimate of the in-domain model.
The results from both approaches were very close
labeled examples (LPU) (Liu, 2003). In this sys- with the bagging approach being slightly better in
tem, a subset of the unlabeled set is selected as the all cases. For reasons of implementation simplic-
</bodyText>
<page confidence="0.998704">
383
</page>
<bodyText confidence="0.999901642857143">
lity, we prefer the use of a uniform distribution. We
convert the model Ph* into a set of counts W(i)
for words i in the vocabulary V by multiplying
with the vocabulary size V. Thus the total initial
counts Ei w(i) = V.
Our selection algorithm considers every sen-
tence in the corpus sequentially. Suppose we are at
the jth sentence s. We denote the count of word
i in s3 with mi&apos;. Let ni =&gt; mi, be the number
of words in the sentence and N = Ei w(i) be the
total number of words already selected. The rela-
tive entropy of the maximum likelihood estimate
of the language model of the selected sentences to
the initial model P is given by
</bodyText>
<equation confidence="0.9983805">
P(i)
H(j) =
</equation>
<bodyText confidence="0.999967">
The model parameters and the Le. remain un-
changed if sentence s3 is not selected. If we select
,s3 , the updated te. is given by
</bodyText>
<equation confidence="0.9961635">
H-±(i) _ _12,P(z)1n P(i)
(W(i) mii)/(N nj)
</equation>
<bodyText confidence="0.987546227272727">
Direct computation of ne. using the above ex-
pressions for every sentence in the web-data will
have a very high computational cost since 0(V)
computations per sentence in the web-data are re-
quired. The number of sentences in the web-data
can be very large, easily on the order 108 to 109.
The total computation cost for even moderate vo-
cabularies (around 105) would be large.
However given the fact that &apos;mu is sparse, we
can split the summation H+ (j) into
We will select the sentence 53 if including it de-
creases the ne. with the in-domain distribution, i.e
H(j) &lt; H(j). Thus 53 is selected if Ti &gt; T2.
To make the selection more refined we can impose
a condition Ti &gt; T2 + thr (j) where thr (j) is a
function of j. A good choice for thr(j) based on
empirical studies is a function that declines at the
same rate as the ratio In (N±NR3) n3IN llkj
where k is the average number of words for every
sentence. The counts W and N are updated with
the selection of a sentence, and H(j) is set to
11+(j).
</bodyText>
<subsectionHeader confidence="0.999949">
3.2 Text permutations and resequencing
</subsectionHeader>
<bodyText confidence="0.999588857142857">
The proposed algorithm is sequential and greedy
in nature and can benefit from randomization of
the order in which it scans the corpus. We
generate permutations of the corpus by scanning
through the corpus and randomly swapping sen-
tences. Next we do sequential selection on each
permutation and merge the selected sets.
As more sentences are selected, the Le. H(j)
decreases and the distribution of the selected set
gets closer to the in-domain distribution. The sen-
tence selection becomes more refined as it be-
comes harder to improve H(j) further. This moti-
vated us to use a simple heuristic to ensure that the
sentences selected in the initial stage are useful.
At the end of a scan through the corpus we take
the list of selected sentences and reverse it. We
then scan the corpus again with the reversed list
at the top. The sentences which were selected in
the initial stages of the algorithm are retained only
if they contribute to r.e. improvement even when
they are in the latter part of the scan sequence.
</bodyText>
<equation confidence="0.957460571428571">
H(j) = P(i) ln P(i) +
3.3 Smoothing
P(i) ln W(i) +
N ni
P(i) ln (W(i) mu)
W(i)
T2
</equation>
<bodyText confidence="0.999737666666667">
Intuitively, the term Ti measures the decrease
The choice of maximum likelihood estimation for
estimating the intermediate language models for
W(j) is motivated by a simplification in the en-
tropy calculation which reduces the computation
effort significantly. However, maximum likeli-
hood estimation of language models is poor when
compared to smoothing based estimation. To bal-
ance the computation cost and estimation accu-
racy, we modify the counts W(j) using Good-
Turing smoothing periodically, after a fixed num-
ber of sentences. The choice of the number of sen-
</bodyText>
<equation confidence="0.8756475">
H(j) in
T1
</equation>
<bodyText confidence="0.99696">
lin probability mass because of the addition of n3 tences to wait before smoothing depends on com-
words to the corpus, and the term T2 measures putation time constraints.
the in-domain distribution P weighted increase in In the next section we provide an intuitive
probability for words with non-zero mu. analysis of the advantages of iterative selec-
</bodyText>
<page confidence="0.994176">
384
</page>
<bodyText confidence="0.9972545">
tion over rank-and-select schemes using simulated
data.
</bodyText>
<sectionHeader confidence="0.889726" genericHeader="method">
4 Some intuition from simulations
</sectionHeader>
<bodyText confidence="0.996197">
A measure of the relevancy of the selected adap-
tation data is the Kullback-Leibler distance be-
</bodyText>
<table confidence="0.999014">
Rand PPLSel ItSel
200K 32.2 9.1 16.1
400K 34.2 13.3 24.3
800K 31.1 22 27.3
1200K 33.7 28 29.5
2400K 32.9 31 31
</table>
<bodyText confidence="0.962043309523809">
tween the estimated n-gram model and the true Table 1: Perplexity of data selected with respect to
n-gram distribution for that domain2. By compar- Pind for varying number of selected sentences
ing the two distributions we can identify the con-
vergence properties of the data selection methods
and also explain how the selection affects the es- from webpages identified by Google using med-
timated probability mass distribution compared to ical domain queries (Section 5.1). We used a
the true distribution. Dind set (generated from Ptrue) of 200K words
To compare the n-gram language models we de- and a generic set Dgen„,, of size 20M words or
veloped a fast r.e. computation scheme for tree 3.8M sentences. The vocabulary sizes were kept
based n-gram models. The description of this small to efficiently generate text samples from
computation scheme is given in Appendix A. The the language models. The goal of the simula-
fast scheme makes it possible to compute the r.e. tions is solely to gain an insight into the differ-
etween two LMs in 0(L) computations where L ences between iterative selection and rank-and-
the number of language model terms actually select scheme. Results on real world data are pre-
resent in the two LMs, compared to Vfl compu- sented in the next section where the vocabulary
ations required in a direct implementation. This size is more realistic.
1ielps to reduce the computation effort by a factor
of 104 or more.
We cannot use KL distance to Judge relevance
with real world data since the true distribution for
real world data is unknown. However for analy-
sis purposes, we can substitute the true distribution
with a known distribution and then sample from it
to get examples of in-domain text (with reference
to the known distribution). We can then mix the
known distribution with a noise model to simulate
a generic text corpus, such as text acquired from
web.
For simulation purposes, the language model
used to generate the equivalent of in-domain text
becomes the true distribution Ptnie. To complete
the analogy with our data selection problem, text
samples Dind generated from Ptnie serve as the
equivalent of in-domain data. The equivalent of
he large generic corpus Dgen„ic can be generated
from the noisy model. The simulation equivalent
of the in-domain language model will be the lan-
guage model Pind estimated from the clean data
D Ind •
We use a Ptrue with a vocabulary of 3K words
estimated from a real world medical dialog task.
T e noise mo se is a anguage mo se o voca - lected data for all methods will be similar (iden-
</bodyText>
<subsectionHeader confidence="0.965017">
4.1 Simulation results
</subsectionHeader>
<bodyText confidence="0.987686916666667">
The first question that we address is whether it
is useful to select all data from the generic cor-
pus Dgen„„ which scores high in perplexity terms
with the in-domain model Pirid. In Table 1, we
compare the perplexity of the data selected by dif-
ferent methods. Rand selects n sentences ran-
domly, PPLSel selects the top n sentences ranked
by perplexity and ItSel is the proposed iterative
method. We build language models with the se-
lected data and merge it with Find using weights
determined from the heldout set. Table 2 shows
the relative entropy of the adapted models with the
true distribution. Our goal is to select the adapta-
tion data cleverly to reduce the r.e. between the
adapted model and the true distribution Ptnie. It
can be seen that selecting data with lowest per-
plexity does not lead to a better language model.
The perplexity of the data selected by ItSel, which
is the most beneficial in improving the language
model lies between the perplexity of random se-
lection and PPLSel. It should be noted that by
design, as the number of selected sentences ap-
roaches the size of the leneric cor us, the se-
Ulary 20K estimated from 1M words collected
</bodyText>
<footnote confidence="0.8590675">
tical if all the data is selected). Thus all methods
2We restrict ourselves to the n-gram approach for model-
essentially give same performance when selecting
ng the distribution of word sequences. high percentages of data.
</footnote>
<page confidence="0.985541">
385
</page>
<figure confidence="0.707109045454545">
500K
1M 1.5M
INumber of sentences
2M
2.5M
Rand PPLSel ItSel
200K 12.1 15.2 9.2
400K 11.3 13.2 8.3
800K 10.5 11.1 10.1
1200K 9.7 9.5 9.4
2400K 9.3 8.9 8.9
Table 2: Relative entropy of models built from the
selected data with the reference Ptrue distribution
for varying number of sentences
- ItSel
- - PplSel
3.5
3
2.5
f&apos;tt&apos;
co
0.5
</figure>
<bodyText confidence="0.999579833333333">
Next we verify our hypothesis that use of rank-
ing methods skews the distribution by focusing
solely on the high probability regions. We selected
the top 10% words which have the highest prob-
ability in Pd and 10% words with the smallest
probability. Then we computed the partial sums
</bodyText>
<equation confidence="0.9918422">
Hhigh Ptrue(W)
bi -1-
as — trueW )
In
Psei(W)
</equation>
<bodyText confidence="0.960531190476191">
for the language models Psei estimated from the
selected data. Note that the summation involves
the true density P. If the selected data is im-
palanced with respect to the true distribution Pt,
and unnecessarily biased towards the high proba-
pility regions of Pba„ the separation of the partial
sums
Himbalance &apos;&apos;bias — -high HLws
will be large. However, if the bias towards high
probability regions of P
- base is justified, Himbalance
Will be low.
In Figure 1, we plot Himbalance with increasing
itumber of selected sentences for PP1Sel and It-
Sel. High Himbalance for PPLSel especially when
iiumber of sentences selected is low confirms our
11ypothesis that selection using perplexity ranking
skews the distribution by focusing solely on the
1llgh probability regions.
The simulation results provide an easy and in-
tuitive way to understand how the proposed algo-
</bodyText>
<figureCaption confidence="0.9477265">
Figure 1: Relative entropy imbalance with number
of selected sentences
</figureCaption>
<bodyText confidence="0.885769">
world applications. We proceed to do so in the
next section.
</bodyText>
<sectionHeader confidence="0.995261" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999746666666667">
Our experiments were conducted on medical do-
main data collected as part of the English ASR
of an English-Persian speech to speech translation
project. We have 50K in-domain sentences for this
task. A generic conversational-speech language
model was built from the WSJ, Fisher and SWB
corpora interpolated with the CMU LM. All lan-
guage models built from web-data and in-domain
data were interpolated with this language model
with the interpolation weight determined on the
heldout set. The test set for perplexity evaluations
consists of 5000 sentences (35K words) and the
heldout set had 2000 sentences (12K words). The
test set for word error rate evaluation consists of
520 utterances.
</bodyText>
<subsectionHeader confidence="0.957631">
5.1 Data collection from web
</subsectionHeader>
<bodyText confidence="0.999992875">
We downloaded around 100GB data from the web
using automatically generated queries. Candi-
date query terms were generated by comparing
the probabilities of word n-grams in the in-domain
text with a background model of conversational
speech. The prominent umgram, bigram and tn-
gram word sequences were selected and combined
to form queries for Google. The top 20 URLs
</bodyText>
<equation confidence="0.9569436">
wEtop
= — 12, Ptrue (w) in
Psei(w)
-ltrue(W)
WE bottom
</equation>
<bodyText confidence="0.936824909090909">
rithm scores over rank-and-select scheme. Per-
returned by Google for each such conjunction of
i)lexity and WER results on real world tasks are queries were downloaded and converted to text.
11ard to interpret because the underlying distrib- The converted data from HTML typically does not
utions are unknown. For example, our claim of have well defined sentence boundaries. We piped
skew towards high probability regions is hard to
justify by looking at perplexity of test sets. How-
the text through a maximum entropy based sen-
tence boundary detector to insert better sentence
ever we do need to ensure that our algorithm in- boundary marks. Sentences and documents with
deed improves over the baseline methods on real high 00V rates were rejected as noise to keep the
</bodyText>
<page confidence="0.995877">
386
</page>
<table confidence="0.999658857142857">
10K 20K 40K 10K 20K 40K
No Web 60.0 49.6 39.7 No Web 19.8 18.9 17.9
AllWeb 57.1 48.1 38.2 AllWeb 19.5 19.1 17.9
PPL 56.1 48.1 38.2 PPL 19.2 18.8 17.9
BLEU 56.3 48.2 38.3 BLEU 19.3 18.8 17.9
LPU 56.3 48.2 38.3 LPU 19.2 18.8 17.8
Proposed 54.8 46.8 38.1 Proposed 18.3 18.2 17.3
</table>
<tableCaption confidence="0.9154385">
Table 3: Perplexity of testdata with the web Table 5: Word Error Rate (WER) with web
adapted model for different number of initial sen-
adapted models for different number of initial sen-
tences. Corpus size=150M tences. Corpus size=150M
</tableCaption>
<bodyText confidence="0.611998">
converted text clean. After filtering and normal-
</bodyText>
<subsectionHeader confidence="0.996192">
5.3 Final results
</subsectionHeader>
<bodyText confidence="0.998803666666667">
lization the downloaded data amounted to 320M
words. We use a 150M word subset of this down-
loaded data for our initial experiments.
To test how the performance of our algorithm
scales with increasing data, we conducted ex-
periments on a larger data set of 850M words
</bodyText>
<subsectionHeader confidence="0.997683">
5.2 Experiments on 150M web-data
</subsectionHeader>
<bodyText confidence="0.999954666666666">
We first compare our proposed algorithm against
baselines based on perplexity (PPL), BLEU and
PU classification (Section 2) in terms of test set
perplexity. As the comparison shows, the pro-
posed algorithm outperforms the rank and select
schemes with just 10% of data. Table 3 shows the
test set perplexity with different amounts of initial
lin-domain data. Table 4 shows the number of sen-
tences selected for the best perplexity on the held-
out set by the above schemes. No Web refers to
the language model built from just in-domain data
with no web-data. AllWeb refers to the case where
he entire web-data was used.
The WER results are shown in Table 5. The av-
erage reduction in WER is close to 3% (relative).
It can be seen that adding data from the web with-
out proper filtering can actually harm the perfor-
mance of the speech recognition system when the
initial in-domain data size increases. This can be
attributed to the large increase in vocabulary size
which increases the acoustic decoder perplexity.
</bodyText>
<table confidence="0.9947102">
10K 20K 40K
PPL 93 92 91
BLEU 91 90 89
LPU 90 88 87
Proposed 12 11 12
</table>
<bodyText confidence="0.99895575">
Which consisted of the medical domain collec-
tion of 320M words collected from the web and a
525M word collection published by the University
of Washington for the Fisher corpus (Cetin, 2005).
We will provide comparison with only the perplex-
ity based rank-and-select system, as LPU and the
BLEU based system are hard to scale to large text
collections. Also, our results on the 150M set sug-
gest that the performance of these systems is com-
parable to perplexity based selection.
The results on PPL and WER (Table 6, Table 7)
follow the same trend as in the 150M data set. The
importance of proper data selection is highlighted
by the fact that there was little to no improvement
in the unfiltered case (AllWeb) by adding the extra
data, whereas there were consistent improvements
when the proposed iterative selection algorithm
was used. Perplexity reduction in relative terms
was 7%,5% and 4% for the 10K,20K and 40K in-
domain set, respectively. Corresponding WER im-
provements in relative terms were 6% ,4% and 4%.
It is interesting to note that for our data selection
scheme the perplexity improvments correlate sur-
prisingly well with WER improvments. A plausi-
ble explanation is that the perplexity improvments
are accompanied by a significant reduction in the
number of language model parameters.
Table 8 shows the percentage of data selected
using the proposed scheme and PPL based rank-
and-select. We are able to achieve around a fac-
tor of 9 reduction in the selected data size. This
translates to (Table 9) a factor of 7 reduction in
</bodyText>
<tableCaption confidence="0.980763">
Table 4: Percentage of web-data selected for the number of estimated language model parame-
</tableCaption>
<footnote confidence="0.385898">
different number of initial sentences. Corpus ters (bigram+trigram) and a 30% reduction in the
size=150M
vocabulary size.
</footnote>
<page confidence="0.911656">
387
</page>
<table confidence="0.9997278">
10K 20K 40K unigram bigram tngram
No Web 60.0 49.6 39.7 NoWeb 70K 1.5M 2.7M
AllWeb 56.9 47.7 38.2 AllWeb 105K 25.3M 36.2M
PPL 55.8 47.4 38.2 PPL 99K 22.1M 32.4M
Proposed 52.6 45.3 37.1 Proposed 70K 3.2M 8.2M
</table>
<tableCaption confidence="0.981327333333333">
Table 6: Perplexity of testdata with the web Table 9: Number of estimated n-grams with web
adapted model for different number of initial sen- adapted models for different number of initial sen-
tences. Corpus size=850M tences for the case with 40K in-domain sentences.
</tableCaption>
<table confidence="0.9998262">
10K 20K 40K
No Web 19.8 18.9 17.9
AllWeb 19.3 19.1 17.9
PPL 19.1 18.7 17.9
Proposed 18.0 17.9 17.1
</table>
<tableCaption confidence="0.989365">
Table 7: Word Error Rate (WER) with web
adapted models for different number of initial sen-
tences. Corpus size=850M
</tableCaption>
<bodyText confidence="0.995334666666667">
In this paper we presented a novel and compu-
tationally efficient scheme for selecting relevant
subsets of sentences from large collections of text
acquired from the web. Our results indicate that
with this scheme, we can identify significantly
smaller sets of sentences such that the models built
from the selected data have a substantially sparser
)-epresentation and yet perform better (in terms of
.0th perplexity and WER) than models built from
he entire corpus. On our medical domain task
we were able to achieve around 3% improvment
•n WER with a factor of 7 reduction in language
model parameters while selecting a set of sen-
ences 10% the size of the original web-data. The
19 roposed method clearly outperforms text clean-
•ng methods described in recent language model-
•ng literature by moving from individual ranking
to (greedy) group selection of sentences. Although
our focus in this paper was on data acquired from
the web, we believe the proposed method can
be used for adaptation of domain specific models
</bodyText>
<table confidence="0.996588666666667">
10K 20K 40K
PPL 88.5% 87.8% 87.3%
Proposed 9.5% 10.1% 8.9%
</table>
<tableCaption confidence="0.964671333333333">
Table 8: Percentage of web-data selected for
different number of initial sentences. Corpus
size=850M
</tableCaption>
<bodyText confidence="0.848757">
Corpus size=850M
from other large generic corpora.
We also present new analysis techniques (Sec-
tion 4) based on simulated data and relative en-
tropy which help us to gain valuable insight into
the nature of different data selection algorithms.
</bodyText>
<subsectionHeader confidence="0.997745">
6.2 Scope of this work
</subsectionHeader>
<bodyText confidence="0.999937238095238">
The research effort presented in this paper is di-
rected towards selecting relevant domain specific
data from large collections of generic text. We
make no assumptions on how the data was col-
lected or the use of specific web crawling and
querying techniques. The methods we have devel-
oped can be seen to supplement the research effort
by the machine translation community on identi-
fying web resources (Resnik, 2003; Huang, 2005)
or using web counts (Lapata, 2002) for language
modeling.
The proposed method can be combined with rank-
and-select schemes described in Section 2. We are
exploring the use of ranking to reorder the data
such that the sequential selection process gives
better results. Another idea we are currently inves-
tigating is to use multiple instances of the selec-
tion algorithm with different initial language mod-
els P,,it generated by bagging. Sentence selection
can then be improved by considering the compos-
ite entropy for all the models.
</bodyText>
<reference confidence="0.954406142857143">
Bing Liu, Xiaoli Li, Yang Dai, Wee Sun Lee and Philip
Yu. Building Text Classifiers Using Positive and
Unlabeled Examples. Proceedings of ICDM. 2003.
Fei Huang, Ying Zhang and Stephan Vogel. Mining
Key Phrase Translations from Web Corpora. Pro-
ceedings of EMNLP. 2005
Frank Keller, Maria Lapata and Olga Ourioupina. Us-
</reference>
<figure confidence="0.95927175">
6 Conclusion
6.1 Contribution
6.3 Directions for future work
References
</figure>
<page confidence="0.962765">
388
</page>
<reference confidence="0.97373264516129">
ing the Web to Overcome Data Sparseness. Pro- The complement set (HO will contain histories
eedings of EMNLP. 2002.
Kamal Nigam, Andrew Kachites McCallum, Sebastian
Thrun and Tom Mitchell. Text Classification from
Labeled and Unlabeled Documents using EM. Jour-
nal of Machine Learning. 39(2:3)103-134. 2000.
Mirella Lapata and Frank Keller. Web-based models Then r.e. at level n Di, can be expressed as
for natural language processing. ACM Transactions
on Speech and Language Processing. 2(1),2005.
0.Cetin and Andreas Stolcke. Language Modeling in Dn - phDh phDh
the ICSI-SRI Spring 2005 Meeting Speech Recogni- hEHs
tion Evaluation. ICSI Technical Report TR-05-006.
2005.
PhDh&apos; PhDh PhDh&apos;
Philip Resnik and Noah A. Smith. The Web as a paral- hEH hEHs hEHs
lel corpus. Computational Linguistics. 29(3),2003.
Ruhi Sarikaya, Agustin Gravano and Yuqing Gao.
Rapid Language Model Development Using Exter-
nal Resources For New Spoken Dialog Domains.
Proceedings of ICASSP. 2005.
Tim Ng, Mari Ostendorf, Mei-Yuh Hwang, Manhung
Siu, Ivan Bulyko and Xin Lei. Web-data Augmented Dh can be split into four terms depending on
Language Model for Mandarin Speech Recognition. whether x/h is defined in the p or the q distribu-
Proceedings of ICASSP. 2005. tion
Xiaojin Zhu. Semi-Supervised Learning Litera-
ture Survey. Computer Science, University of
Wisconsin-Madison.
R. C. Carrasco. Accurate computation of the rela-
tive entropy between stochastic regular grammars.
RAIRO (Theoretical Informatics and Applications).
1997.
</reference>
<bodyText confidence="0.912368">
with a back-off 1. Hs, corresponds to histories not
seen in either language model. We define
</bodyText>
<figure confidence="0.560556928571429">
Dh = p(x1h) In P(x1h)
q(X1h)
xeW
(2)
Marginalizing iv&apos;
Dr, = Dn-i+ &gt;2, Ph(Dh - ph&apos;)
heHs
(3)
Dh=T4 ± T2 ± T3 ± T4
= &gt;p(ah)1nP(x1h)
q(x1h)
xexi
T2 = bh ln bh p(xlh&apos;)
xE,c2
</figure>
<sectionHeader confidence="0.9949765" genericHeader="method">
Appendix A: Fast Computation of Relative
Entropy
</sectionHeader>
<bodyText confidence="0.953002916666667">
We define the following symbols for the purpose
of describing the r.e. computation:
x: The current word
h: The history wj..wii_ j
h&apos;: The back off history w2..wn_i
bh: The back-off weight for p distribution for
history h
b&apos;h: The back-off weight for the q distribution
W: The vocabulary of the language model
Consider two n-gram language models p(x1h) and
q(x1h).
Le. at level n
</bodyText>
<equation confidence="0.958317090909091">
in
Dn = ph p(x1h) P(ilh)
heH xEW q(X111) &amp;quot;i
We can divide the set of histories (H) at level n
bh ?-dc,c2P(x1h1) In P(xlh&apos;)
hp hl ,n)b&apos;h
T xx 431 xcx3
1-01n q(X1h)
P(Xb1h)
3 = p(x
T4 = bhp(xlh&apos;)
qh(xl
Ilh&apos;) P(x1h)
hq(x1h1)
bh
= bh ln — (1 _
bf P(i hi)) + bhDh/
xEX
-bh p(xlh&apos;) ln
q(X1171
xEX
(4)
</equation>
<bodyText confidence="0.909348625">
Thus we are able to express Dh, in terms of
the LM terms actually seen. Using Dh computed
in this fashion in (3) we get a recursive formula-
tion for Le. at level n using LM densities actually
seen. An alternative method for r.e. computation
into Hs for all h which exist as n-1 gram and have between finite state automata can be seen in (Car-
rasco, 1997).
a back-off weight= 1 in the p or the q distribution.
</bodyText>
<page confidence="0.992223">
389
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.685065">
<title confidence="0.913606">Text data acquisition for domain-specific language models Abhinav Sethy, Panayiotis G. Georgiou, Shrikanth Speech Analysis and Interpretation Integrated Media Systems</title>
<author confidence="0.963299">Viterbi School of</author>
<affiliation confidence="0.9974195">pepartment of Electrical University of Southern California</affiliation>
<abstract confidence="0.999264612903226">language modeling community showing a growing interest in using large collections of text mined from the World Wide Web (WWW) to supplement sparse in-domain text resources. However, in most cases the style and content of the text harvested from these corpora differs significantly from the specific nature of these domains. In this paper we present a relative entropy (r.e.) based method to sesubsets sentences whose in an matches the domain of interest. Using simulations, we provide an analysis of how the proposed scheme outperforms filtering techniques proposed in recent language modeling literature on mining text from the web. A comparative study is presented using a text collection of over 800M words collected from the WWW. Experimental results show that by using the proposed subset selection scheme we can get per- Formance improvement in both Word Error Rate (WER) and Perplexity (PPL) over the models built from the entire collection by using just 10% of the data Improvements in data selection also translated to a significant reduction in the vocabulary size as well as the number of estimated parameters in the adapted language model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bing Liu</author>
<author>Xiaoli Li</author>
<author>Yang Dai</author>
<author>Wee Sun Lee</author>
<author>Philip Yu</author>
</authors>
<title>Building Text Classifiers Using Positive and Unlabeled Examples.</title>
<date>2003</date>
<booktitle>Proceedings of ICDM.</booktitle>
<marker>Liu, Li, Dai, Lee, Yu, 2003</marker>
<rawString>Bing Liu, Xiaoli Li, Yang Dai, Wee Sun Lee and Philip Yu. Building Text Classifiers Using Positive and Unlabeled Examples. Proceedings of ICDM. 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Ying Zhang</author>
<author>Stephan Vogel</author>
</authors>
<title>Mining Key Phrase Translations from Web Corpora.</title>
<date>2005</date>
<booktitle>Proceedings of EMNLP.</booktitle>
<marker>Huang, Zhang, Vogel, 2005</marker>
<rawString>Fei Huang, Ying Zhang and Stephan Vogel. Mining Key Phrase Translations from Web Corpora. Proceedings of EMNLP. 2005</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Keller</author>
<author>Maria Lapata</author>
<author>Olga Ourioupina</author>
</authors>
<title>Using the Web to Overcome Data Sparseness. Pro- The complement set (HO will contain histories eedings of EMNLP.</title>
<date>2002</date>
<marker>Keller, Lapata, Ourioupina, 2002</marker>
<rawString>Frank Keller, Maria Lapata and Olga Ourioupina. Using the Web to Overcome Data Sparseness. Pro- The complement set (HO will contain histories eedings of EMNLP. 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>Andrew Kachites McCallum</author>
<author>Sebastian Thrun</author>
<author>Tom Mitchell</author>
</authors>
<title>Text Classification from Labeled and Unlabeled Documents using EM.</title>
<date>2000</date>
<journal>Journal of Machine Learning.</journal>
<volume>2</volume>
<issue>1</issue>
<pages>39--2</pages>
<marker>Nigam, McCallum, Thrun, Mitchell, 2000</marker>
<rawString>Kamal Nigam, Andrew Kachites McCallum, Sebastian Thrun and Tom Mitchell. Text Classification from Labeled and Unlabeled Documents using EM. Journal of Machine Learning. 39(2:3)103-134. 2000. Mirella Lapata and Frank Keller. Web-based models for natural language processing. ACM Transactions on Speech and Language Processing. 2(1),2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>r e Then</author>
</authors>
<title>at level n Di, can be expressed as 0.Cetin and Andreas Stolcke. Language Modeling in the ICSI-SRI</title>
<date>2005</date>
<tech>ICSI Technical Report TR-05-006.</tech>
<publisher>Spring</publisher>
<marker>Then, 2005</marker>
<rawString>Then r.e. at level n Di, can be expressed as 0.Cetin and Andreas Stolcke. Language Modeling in the ICSI-SRI Spring 2005 Meeting Speech Recogni- tion Evaluation. ICSI Technical Report TR-05-006. 2005. Dn - phDh phDh hEHs PhDh&apos; PhDh PhDh&apos;</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>