<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000344">
<note confidence="0.9520795">
Proceedings of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), Philadelphia, July 2002, pp. 505-512.
</note>
<title confidence="0.9993205">
An Empirical Study of Active Learning with Support Vector Machines for
Japanese Word Segmentation
</title>
<author confidence="0.960572">
Manabu Sassano
</author>
<affiliation confidence="0.901188">
Fujitsu Laboratories Ltd.
</affiliation>
<address confidence="0.9347525">
4-1-1, Kamikodanaka, Nakahara-ku,
Kawasaki 211-8588, Japan
</address>
<email confidence="0.999079">
sassano@jp.fujitsu.com
</email>
<sectionHeader confidence="0.995644" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999721954545455">
We explore how active learning with Sup-
port Vector Machines works well for a
non-trivial task in natural language pro-
cessing. We use Japanese word segmenta-
tion as a test case. In particular, we discuss
how the size of a pool affects the learning
curve. It is found that in the early stage
of training with a larger pool, more la-
beled examples are required to achieve a
given level of accuracy than those with a
smaller pool. In addition, we propose a
novel technique to use a large number of
unlabeled examples effectively by adding
them gradually to a pool. The experimen-
tal results show that our technique requires
less labeled examples than those with the
technique in previous research. To achieve
97.0 % accuracy, the proposed technique
needs 59.3 % of labeled examples that
are required when using the previous tech-
nique and only 17.4 % of labeled exam-
ples with random sampling.
</bodyText>
<sectionHeader confidence="0.999335" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999970171875">
Corpus-based supervised learning is now a stan-
dard approach to achieve high-performance in nat-
ural language processing. However, the weakness
of supervised learning approach is to need an anno-
tated corpus, the size of which is reasonably large.
Even if we have a good supervised-learning method,
we cannot get high-performance without an anno-
tated corpus. The problem is that corpus annotation
is labour intensive and very expensive. In order to
overcome this, some unsupervised learning methods
and minimally-supervised methods, e.g., (Yarowsky,
1995; Yarowsky and Wicentowski, 2000), have
been proposed. However, such methods usually de-
pend on tasks or domains and their performance of-
ten does not match one with a supervised learning
method.
Another promising approach is active learning, in
which a classifier selects examples to be labeled, and
then requests a teacher to label them. It is very dif-
ferent from passive learning, in which a classifier
gets labeled examples randomly. Active learning is
a general framework and does not depend on tasks
or domains. It is expected that active learning will
reduce considerably manual annotation cost while
keeping performance. However, few papers in the
field of computational linguistics have focused on
this approach (Dagan and Engelson, 1995; Thomp-
son et al., 1999; Ngai and Yarowsky, 2000; Hwa,
2000; Banko and Brill, 2001). Although there are
many active learning methods with various classi-
fiers such as a probabilistic classifier (McCallum and
Nigam, 1998), we focus on active learning with Sup-
port Vector Machines (SVMs) because of their per-
formance.
The Support Vector Machine, which is introduced
by Vapnik (1995), is apowerful new statistical learn-
ing method. Excellent performance is reported in
hand-written character recognition, face detection,
image classification, and so forth. SVMs have
been recently applied to several natural language
tasks, including text classification (Joachims, 1998;
Dumais et al., 1998), chunking (Kudo and Mat-
sumoto, 2000b; Kudo and Matsumoto, 2001), and
dependency analysis (Kudo and Matsumoto, 2000a).
SVMs have been greatly successful in such tasks.
Additionally, SVMs as well as boosting have good
theoretical background.
The objective of our research is to develop an ef-
fective way to build a corpus and to create high-
performance NL systems with minimal cost. As
a first step, we focus on investigating how active
learning with SVMs, which have demonstrated ex-
cellent performance, works for complex tasks in nat-
ural language processing. For text classification, it
is found that this approach is effective (Tong and
Koller, 2000; Schohn and Cohn, 2000). They used
less than 10,000 binary features and less than 10,000
examples. However, it is not clear that the approach
is readily applicable to tasks which have more than
100,000 features and more than 100,000 examples.
We use Japanese word segmentation as a test case.
The task is suitable for our purpose because we have
to handle combinations of more than 1,000 charac-
ters and a very large corpus (EDR, 1995) exists.
</bodyText>
<sectionHeader confidence="0.862937" genericHeader="method">
2 Support Vector Machines
</sectionHeader>
<bodyText confidence="0.9828815">
In this section we give some theoretical definitions
of SVMs. Assume that we are given the training data
</bodyText>
<equation confidence="0.880816">
(xi; yi);::: ; (xl; yl); xi E Rn; yi E {+1; —11
</equation>
<bodyText confidence="0.9286185">
The decision function g in SVM framework is de-
fined as:
</bodyText>
<equation confidence="0.99986425">
g(x) = sgn(f(x)) (1)
l
f(x) = yiaiK(xi; x) + b (2)
i=1
</equation>
<bodyText confidence="0.990182">
where K is a kernel function, b E R is a thresh-
old, and ai are weights. Besides the oi satisfy the
following constraints:
</bodyText>
<equation confidence="0.993283">
0 &lt; ai &lt; C; di and �l Ceiyi = 0;
i=1
</equation>
<bodyText confidence="0.999635666666667">
where C is a missclassification cost. The xi with
non-zero ai are called Support Vectors. For linear
SVMs, the kernel function K is defined as:
</bodyText>
<equation confidence="0.916973">
K(xi; x) = xi • x:
In this case, Equation 2 can be written as:
f(x) = w • x + b (3)
</equation>
<listItem confidence="0.9984092">
1. Build an initial classifier
2. While a teacher can label examples
(a) Apply the current classifier to each unla-
beled example
(b) Find the m examples which are most in-
formative for the classifier
(c) Have the teacher label the subsample of m
examples
(d) Train a new classifier on all labeled exam-
ples
</listItem>
<figureCaption confidence="0.999163">
Figure 1: Algorithm of pool-based active learning
</figureCaption>
<bodyText confidence="0.998115">
where w = Eli=1 yiaixi. To train an SVM is to find
the ai and the b by solving the following optimiza-
tion problem:
</bodyText>
<equation confidence="0.990305333333333">
aiajyiyjK(xi; xj)
subject to 0 &lt; ai &lt; C; di and �l aiyi = 0:
i=1
</equation>
<sectionHeader confidence="0.993761" genericHeader="method">
3 Active Learning for Support Vector
Machines
</sectionHeader>
<subsectionHeader confidence="0.998817">
3.1 General Framework of Active Learning
</subsectionHeader>
<bodyText confidence="0.999975166666667">
We use pool-based active learning (Lewis and Gale,
1994). SVMs are used here instead of probabilistic
classifiers used by Lewis and Gale. Figure 1 shows
an algorithm of pool-based active learning1. There
can be various forms of the algorithm depending on
what kind of example is found informative.
</bodyText>
<subsectionHeader confidence="0.999387">
3.2 Previous Algorithm
</subsectionHeader>
<bodyText confidence="0.9992704">
Two groups have proposed an algorithm for SVMs
active learning (Tong and Koller, 2000; Schohn and
Cohn, 2000)2. Figure 2 shows the selection algo-
rithm proposed by them. This corresponds to (a) and
(b) in Figure 1.
</bodyText>
<footnote confidence="0.901726">
1The figure described here is based on the algorithm by
Lewis and Gale (1994) for their sequential sampling algorithm.
2Tong and Koller (2000) propose three selection algorithms.
The method described here is simplest and computationally ef-
ficient.
</footnote>
<equation confidence="0.9108625">
maximize �l ai — 1 �l
i=1 2 i&gt;j=1
</equation>
<listItem confidence="0.99952">
1. Compute f(xi) (Equation 2) over all xi in a
pool.
2. Sort xi with If(xi)I in decreasing order.
3. Select top m examples.
</listItem>
<figureCaption confidence="0.723726">
Figure 2: Selection Algorithm
</figureCaption>
<listItem confidence="0.9947203">
1. Build an initial classifier.
2. While a teacher can label examples
(a) Select m examples using the algorithm in
Figure 2.
(b) Have the teacher label the subsample of m
examples.
(c) Train a new classifier on all labeled exam-
ples.
(d) Add new unlabeled examples to the pri-
mary pool if a specified condition is true.
</listItem>
<figureCaption confidence="0.99571">
Figure 3: Outline of Tow Pool Algorithm
</figureCaption>
<subsectionHeader confidence="0.998186">
3.3 Two Pool Algorithm
</subsectionHeader>
<bodyText confidence="0.999451340000001">
We observed in our experiments that when using the
algorithm in the previous section, in the early stage
of training, a classifier with a larger pool requires
more examples than that with a smaller pool does (to
be described in Section 5). In order to overcome the
weakness, we propose two new algorithms. We call
them “Two Pool Algorithm” generically. It has two
pools, i.e., a primary pool and a secondary one, and
moves gradually unlabeled examples to the primary
pool from the secondary instead of using a large
pool from the start of training. The primary pool
is used directly for selection of examples which are
requested a teacher to label, whereas the secondary
is not. The basic idea is simple. Since we cannot
get good performance when using a large pool at the
beginning of training, we enlarge gradually a pool of
unlabeled examples.
The outline of Two Pool Algorithm is shown in
Figure 3. We describe below two variations, which
are different in the condition at (d) in Figure 3.
Our first variation, which is called Two Pool Al-
gorithm A, adds new unlabeled examples to the pri-
mary pool when the increasing ratio of support vec-
tors in the current classifier decreases, because the
gain of accuracy is very little once the ratio is down.
This phenomenon is observed in our experiments
(Section 5). This observation has also been reported
in previous studies (Schohn and Cohn, 2000).
In Two Pool Algorithm we add new unlabeled ex-
amples so that the total number of examples includ-
ing both labeled examples in the training set and un-
labeled examples in the primary pool is doubled. For
example, suppose that the size of a initial primary
pool is 1,000 examples. Before starting training,
there are no labeled examples and 1,000 unlabeled
examples. We add 1,000 new unlabeled examples to
the primary pool when the increasing ratio of sup-
port vectors is down after t examples has been la-
beled. Then, there are the t labeled examples and
the (2, 000 — t) unlabeled examples in the primary
pool. At the next time when we add new unlabeled
examples, the number of newly added examples is
2,000 and then the total number of both labeled in
the training set and unlabeled examples in the pri-
mary pool is 4,000.
Our second variation, which is called Two Pool
Algorithm B, adds new unlabeled examples to the
primary pool when the number of support vectors of
the current classifier exceeds a threshold d. The d is
defined as:
</bodyText>
<equation confidence="0.831202">
d = Nb
100;0 &lt; b &lt; 100 (4)
</equation>
<bodyText confidence="0.999952666666667">
where b is a parameter for deciding when unlabeled
examples are added to the primary pool and N is
the number of examples including both labeled ex-
amples in the training set and unlabeled ones in the
primary pool. The b must be less than the percentage
of support vectors of a training set3. When deciding
how many unlabeled examples should be added to
the primary pool, we use the strategy as described in
the paragraph above.
</bodyText>
<sectionHeader confidence="0.999089" genericHeader="method">
4 Japanese Word Segmentation
</sectionHeader>
<subsectionHeader confidence="0.995062">
4.1 Word Segmentation as a Classification Task
</subsectionHeader>
<bodyText confidence="0.998253857142857">
Many tasks in natural language processing can be
formulated as a classification task (van den Bosch
3Since typically the percentage of support vectors is small
(e.g., less than 30 %), we choose around 10 % for Æ. We need
further studies to find the best value of Æ before or during train-
ing.
et al., 1996). Japanese word segmentation can be
viewed in the same way, too (Shinnou, 2000). Let a
Japanese character sequence be s = c1c2 • • • cm and
a boundary bi exist between ci and ci+1. The bi is
either +1 (word boundary) or -1 (non-boundary).
The word segmentation task can be defined as de-
termining the class of the bi. We use an SVM to
determine it.
</bodyText>
<subsectionHeader confidence="0.703108">
4.2 Features
</subsectionHeader>
<bodyText confidence="0.999978607142857">
We assume that each character ci has two attributes.
The first attribute is a character type (ti). It can
be hiragana4, katakana, kanji (Chinese characters),
numbers, English alphabets, kanji-numbers (num-
bers written in Chinese), or symbols. A character
type gives some hints to segment a Japanese sen-
tence to words. For example, kanji is mainly used
to represent nouns or stems of verbs and adjectives.
It is never used for particles, which are always writ-
ten in hiragana. Therefore, it is more probable that a
boundary exists between a kanji character and a hi-
ragana character. Of course, there are quite a few
exceptions to this heuristics. For example, some
proper nouns are written in mixed hiragana, kanji
and katakana.
The second attribute is a character code (ki). The
range of a character code is from 1 to 6,879. JIS X
0208, which is one of Japanese character set stan-
dards, enumerates 6,879 characters.
We use here four characters to decide a word
boundary. A set of the attributes of ci-1; ci; ci+1,
and ci+2 is used to predict the label of the bi. The
set consists of twenty attributes: ten for the char-
acter type (ti-1titi+1ti+2, ti-1titi+1, ti-1ti, ti-1,
titi+1ti+2, titi+1, ti, ti+1ti+2, ti+1, ti+2), and an-
other ten for the character code (ki_1kiki+1ki+2,
ki-1kiki+1, ki-1ki, ki-1, kiki+1ki+2, kiki+1, ki,
ki+1ki+2, ki+1, and ki+2).
</bodyText>
<sectionHeader confidence="0.983225" genericHeader="evaluation">
5 Experimental Results and Discussion
</sectionHeader>
<bodyText confidence="0.999316">
We used the EDR Japanese Corpus (EDR, 1995) for
experiments. The corpus is assembled from var-
ious sources such as newspapers, magazines, and
textbooks. It contains 208,000 sentences. We se-
lected randomly 20,000 sentences for training and
</bodyText>
<footnote confidence="0.583813333333333">
4Hiragana and katakana are phonetic characters which rep-
resent Japanese syllables. Katakana is primarily used to write
foreign words.
</footnote>
<bodyText confidence="0.964426625">
10,000 sentences for testing. Then, we created ex-
amples using the feature encoding method in Sec-
tion 4. Through these experiments we used the orig-
inal SVM tools, the algorithm of which is based on
SMO (Sequential Minimal Optimization) by Platt
(1999). We used linear SVMs and set a missclas-
sification cost C to 0:2.
First, we changed the number of labeled examples
which were randomly selected. This is an experi-
ment on passive learning. Table 2 shows the accu-
racy at different sizes of labeled examples.
Second, we changed the number of examples in
a pool and ran the active learning algorithm in Sec-
tion 3.2. We use the same examples for a pool as
those used in the passive learning experiments. We
selected 1,000 examples at each iteration of the ac-
tive learning. Figure 4 shows the learning curve of
this experiment and Figure 5 is a close-up of Fig-
ure 4. We see from Figure 4 that active learning
works quite well and it significantly reduces labeled
examples to be required. Let us see how many la-
beled examples are required to achieve 96.0 % ac-
curacy. In active learning with the pool, the size of
which is 2,500 sentences (97,349 examples), only
28,813 labeled examples are needed, whereas in pas-
sive learning, about 97,000 examples are required.
That means over 70 % reduction is realized by ac-
tive learning. In the case of 97 % accuracy, approx-
imately the same percentage of reduction is realized
when using the pool, the size of which is 20,000 sen-
tences (776,586 examples).
Now let us see how the accuracy curve varies de-
pending on the size of a pool. Surprisingly, the per-
formance of a larger pool is worse than that of a
smaller pool in the early stage of trainini. One rea-
son for this could be that support vectors in selected
examples at each iteration from a larger pool make
larger clusters than those selected from a smaller
pool do. In other words, in the case of a larger pool,
more examples selected at each iteration would be
similar to each other. We computed variances6of
each 1,000 selected examples at the learning itera-
tion from 2 to 11 (Table 1). The variances of se-
5Tong and Koller (2000) have got the similar results in a
text classification task with two small pools: 500 and 1000.
However, they have concluded that a larger pool is better than
a smaller one because the final accuracy of the former is higher
than that of the latter.
</bodyText>
<footnote confidence="0.622885">
6The variance vz of a set of selected examples xi is defined
</footnote>
<tableCaption confidence="0.998837">
Table 1: Variances of Selected Examples
</tableCaption>
<table confidence="0.994199">
Iteration 2 3 4 5 6 7 8 9 10 11
1,250 Sent. Size Pool 16.87 17.25 17.85 17.63 17.24 17.37 17.34 17.73 17.94 17.57
20,000 Sent. Size Pool 16.66 17.03 16.92 16.75 16.80 16.72 16.91 16.93 16.87 16.97
</table>
<bodyText confidence="0.999635387096774">
lected examples using the 20,000 sentence size pool
is always lower than those using the 1,250 sentence
size pool. The result is not inconsistent with our hy-
pothesis.
Before we discuss the results of Two Pool Algo-
rithm, we show in Figure 6 how support vectors of
a classifier increase and the accuracy changes when
using the 2,500 sentence size pool. It is clear that
after the accuracy improvement almost stops, the in-
crement of the number of support vectors is down.
We also observed the same phenomenon with differ-
ent sizes of pools. We utilize this phenomenon in
Algorithm A.
Next, we ran Two Pool Algorithm A7. The result
is shown in Figure 7. The accuracy curve of Algo-
rithm A is better than that of the previously proposed
method at the number of labeled examples roughly
up to 20,000. After that, however, the performance
of Algorithm A does not clearly exceed that of the
previous method.
The result of Algorithm B is shown in Figure 8.
We have tried three values for Æ : 5 %, 10 %, and 20
%. The performance with Æ of 10 %, which is best,
is plotted in Figure 8. As noted above, the improve-
ment by Algorithm A is limited, whereas it is re-
markable that the accuracy curve of Algorithm B is
always the same or better than those of the previous
algorithm with different sizes of pools (the detailed
information about the performance is shown in Ta-
ble 3). To achieve 97.0 % accuracy Algorithm B re-
quires only 59,813 labeled examples, while passive
</bodyText>
<equation confidence="0.6785215">
as:
llxi - mll2
</equation>
<bodyText confidence="0.9472275">
where m = 1n �ni=1 xi and n is the number of selected exam-
ples.
</bodyText>
<footnote confidence="0.42921275">
7In order to stabilize the algorithm, we use the following
strategy at (d) in Figure 3: add new unlabeled examples to the
primary pool when the current increment of support vectors is
less than half of the average increment.
</footnote>
<tableCaption confidence="0.973609">
Table 2: Accuracy at Different Labeled Data Sizes
with Random Sampling
</tableCaption>
<table confidence="0.999675928571429">
# of # of Ex- # of Accuracy
Sen- amples Binary (%)
tences Features
21 813 5896 89.07
41 1525 10224 90.30
81 3189 18672 91.65
162 6167 32258 92.93
313 12218 56202 93.89
625 24488 98561 94.73
1250 48701 168478 95.46
2500 97349 288697 96.10
5000 194785 493942 96.66
10000 387345 827023 97.10
20000 776586 1376244 97.40
</table>
<bodyText confidence="0.999169333333333">
learning requires about 343,0008 labeled examples
and the previous method with the 200,000 sentence
size pool requires 100,813. That means 82.6 % and
40.7 % reduction compared to passive learning and
the previous method with the 200,000 sentence size
pool, respectively.
</bodyText>
<sectionHeader confidence="0.995038" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999916545454545">
To our knowledge, this is the first paper that reports
the empirical results of active learning with SVMs
for a more complex task in natural language process-
ing than a text classification task. The experimental
results show that SVM active learning works well
for Japanese word segmentation, which is one of
such complex tasks, and the naive use of a large pool
with the previous method of SVM active learning is
less effective. In addition, we have proposed a novel
technique to improve the learning curve when using
a large number of unlabeled examples and have eval-
</bodyText>
<footnote confidence="0.928395">
8We computed this by simple interpolation.
</footnote>
<equation confidence="0.3809338">
1
n
2
v
�n
</equation>
<page confidence="0.81317">
i=1
</page>
<tableCaption confidence="0.919791">
Table 3: Accuracy of Different Active Learning Al-
gorithms
</tableCaption>
<table confidence="0.998891">
# of Algo. Algo. 1250 Pool Size 20,000
Ex. A B Sent. 5,000 Sent.
Sent.
813 89.07 89.07 89.07 89.07 89.07
1813 91.70 91.70 91.48 90.89 90.61
3813 93.82 93.82 93.60 93.11 92.42
6813 94.62 94.93 94.90 94.23 93.53
12813 95.24 95.87 95.29 95.42 94.82
24813 95.98 96.43 95.46 96.20 95.80
48813 96.51 96.88 96.51 96.62
</table>
<figure confidence="0.9981255">
0 10000 20000 30000 40000 50000 60000 70000 80000 90000 100000
Number of labeled examples
0.98
0.97
0.96
0.95
0.94
0.93
0.92
0.91
0.9
0.89
0.88
0 10000 20000 30000 40000 50000 60000 70000 80000 90000 100000
Number of labeled examples
30000
25000
20000
15000
10000
5000
0
0 10000 20000 30000 40000 50000 60000 70000 80000 90000 100000
Number of labeled examples
</figure>
<figureCaption confidence="0.911133">
Figure 6: Change of Accuracy and Number of Sup-
port Vectors of Active Learning with 2500 Sentence
Size Pool
</figureCaption>
<figure confidence="0.999829">
Accuracy
0.98
0.97
0.96
0.95
0.94
0.93
0.92
0.91
0.89
0.88
0.9
Passive (Random Sampling)
Active (1250 Sent. Size Pool)
Active (2500 Sent. Size Pool)
Active (5000 Sent. Size Pool)
Active (20,000 Sent. Size Pool)
Accuracy
Number of Support Vectors
</figure>
<figureCaption confidence="0.99759">
Figure 4: Accuracy Curve with Different Pool Sizes
</figureCaption>
<figure confidence="0.996635">
0.96
0.95
0.94
Accuracy
0.93
0.92
0.91
0 5000 10000 15000 20000 25000
Number of labeled examples
0 10000 20000 30000 40000 50000 60000 70000 80000 90000 100000
Number of labeled examples
Passive (Random Sampling)
Active (1250 Sent. Size Pool)
Active (2500 Sent. Size Pool)
Active (5000 Sent. Size Pool)
Active (20,000 Sent. Size Pool)
Accuracy 0.98
0.97
0.96
0.95
0.94
0.93
0.92
0.91
0.9
0.89
0.88
Passive (Random Sampling)
Active (Algorithm A)
Active (20,000 Sent. Size Pool)
</figure>
<figureCaption confidence="0.9954">
Figure 5: Accuracy Curve with Different Pool Sizes Figure 7: Accuracy Curve of Algorithm A
</figureCaption>
<figure confidence="0.964889333333333">
(close-up)
0 10000 20000 30000 40000 50000 60000 70000 80000 90000 100000
Number of labeled examples
</figure>
<figureCaption confidence="0.999986">
Figure 8: Accuracy Curve of Algorithm B
</figureCaption>
<bodyText confidence="0.999951">
uated it by Japanese word segmentation. Our tech-
nique outperforms the method in previous research
and can significantly reduce required labeled exam-
ples to achieve a given level of accuracy.
</bodyText>
<sectionHeader confidence="0.99673" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995227115384616">
Michele Banko and Eric Brill. 2001. Scaling to very very
large corpora for natural language disambiguation. In
Proceedings ofACL-2001, pages 26–33.
Ido Dagan and Sean P. Engelson. 1995. Committee-
based sampling for training probabilistic classifiers.
In Proceedings of the Tweleveth International Confer-
ence on Machine Learning, pages 150–157.
Susan Dumais, John Platt, David Heckerman, and
Mehran Sahami. 1998. Inductive learning algorithms
and representations for text categorization. In Pro-
ceedings of the ACM CIKM International Conference
on Information and Knowledge Management, pages
148–155.
EDR (Japan Electoric Dictionary Research Institute),
1995. EDR Electoric Dictionary Technical Guide.
Rebecca Hwa. 2000. Sample selection for statitical
grammar induction. In Proceedings of EMNLP/VLC
2000, pages 45–52.
Thorsten Joachims. 1998. Text categorization with sup-
port vector machines: Learning with many relevant
features. In Proceedings of the European Conference
on Machine Learning.
Taku Kudo and Yuji Matsumoto. 2000a. Japanese depen-
dency structure analysis based on support vector ma-
chines. In Proceedings of the 2000 Joint SIGDAT Con-
ference on Empirical Methods in Natural Language
Processing and Very Large Corpora, pages 18–25.
Taku Kudo and Yuji Matsumoto. 2000b. Use of support
vector learning for chunk identification. In Proceed-
ings of the 4th Conference on CoNLL-2000 and LLL-
2000, pages 142–144.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In Proceedings of NAACL
2001, pages 192–199.
David D. Lewis and William A. Gale. 1994. A sequential
algorithm for training text classifiers. In Proceedings
of the Seventeenth Annual International ACM-SIGIR
Conference on Research and Development in Informa-
tion Rettrieval, pages 3–12.
Andrew Kachites McCallum and Kamal Nigam. 1998.
Employing EM and pool-based active learning for text
classification. In Proceedings of the Fifteenth Interna-
tional Conference on Machine Learning, pages 359–
367.
Grace Ngai and David Yarowsky. 2000. Rule writing
or annotation: Cost-efficient resource usage for base
noun phrase chunking. In Proceedings of ACL-2000,
pages 117–216.
John C. Platt. 1999. Fast training of support vec-
tor machines using sequential minimal optimization.
In Bernhard Sch¨olkopf, Christopher J.C. Burges, and
Alexander J. Smola, editors, Advances in Kernel Meth-
ods: Support Vector Learning, pages 185–208. MIT
Press.
Greg Schohn and David Cohn. 2000. Less is more: Ac-
tive learning with support vector machines. In Pro-
ceedings of the Seventeenth International Conference
on Machine Learning.
Hiroyuki Shinnou. 2000. Deterministic Japanese word
segmentation by decision list method. In Proceedings
of the Sixth Pacific Rim International Conference on
Artificial Intelligence, page 822.
Cynthia A. Thompson, Mary Leaine Califf, and Ray-
mond J. Mooney. 1999. Active learning for natural
language parsing and information extraction. In Pro-
ceedings of the Sixteenth International Conference on
Machine Learning, pages 406–414.
Simon Tong and Daphne Koller. 2000. Support vector
machine active learning with applications to text clas-
sification. In Proceedings of the Seventeenth Interna-
tional Conference on Machine Learning.
Antal van den Bosch, Walter Daelemans, and Ton Wei-
jters. 1996. Morphological analysis as classification:
an inductive-learning approach. In Proceedings of the
Second International Conference on New Methods in
Natural Language Processing, pages 79–89.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag.
</reference>
<figure confidence="0.995078866666667">
0.98
0.97
0.96
0.95
0.94
0.93
0.92
0.91
0.89
0.88
0.9
Passive (Random Sampling)
Active (Algorithm B)
Active (20,000 Sent. Size Pool)
Accuracy
</figure>
<reference confidence="0.994129285714286">
David Yarowsky and Richard Wicentowski. 2000. Min-
imally supervised morphological analysis by multi-
modal alignment. In Proceedings ofACL-2000, pages
207–216.
David Yarowsky. 1995. Unsupervised word sence dis-
ambiguation rivaling supvervised methods. In Pro-
ceedings ofACL-1995, pages 189–196.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.888889">
<note confidence="0.9964185">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 505-512.</note>
<title confidence="0.9810315">An Empirical Study of Active Learning with Support Vector Machines for Japanese Word Segmentation</title>
<author confidence="0.994922">Manabu Sassano</author>
<affiliation confidence="0.994412">Fujitsu Laboratories Ltd.</affiliation>
<address confidence="0.9926285">4-1-1, Kamikodanaka, Nakahara-ku, Kawasaki 211-8588, Japan</address>
<email confidence="0.999878">sassano@jp.fujitsu.com</email>
<abstract confidence="0.99778947826087">We explore how active learning with Support Vector Machines works well for a non-trivial task in natural language processing. We use Japanese word segmentation as a test case. In particular, we discuss how the size of a pool affects the learning curve. It is found that in the early stage of training with a larger pool, more labeled examples are required to achieve a given level of accuracy than those with a smaller pool. In addition, we propose a novel technique to use a large number of unlabeled examples effectively by adding them gradually to a pool. The experimental results show that our technique requires less labeled examples than those with the technique in previous research. To achieve 97.0 % accuracy, the proposed technique needs 59.3 % of labeled examples that are required when using the previous technique and only 17.4 % of labeled examples with random sampling.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Eric Brill</author>
</authors>
<title>Scaling to very very large corpora for natural language disambiguation.</title>
<date>2001</date>
<booktitle>In Proceedings ofACL-2001,</booktitle>
<pages>26--33</pages>
<contexts>
<context position="2641" citStr="Banko and Brill, 2001" startWordPosition="412" endWordPosition="415">mising approach is active learning, in which a classifier selects examples to be labeled, and then requests a teacher to label them. It is very different from passive learning, in which a classifier gets labeled examples randomly. Active learning is a general framework and does not depend on tasks or domains. It is expected that active learning will reduce considerably manual annotation cost while keeping performance. However, few papers in the field of computational linguistics have focused on this approach (Dagan and Engelson, 1995; Thompson et al., 1999; Ngai and Yarowsky, 2000; Hwa, 2000; Banko and Brill, 2001). Although there are many active learning methods with various classifiers such as a probabilistic classifier (McCallum and Nigam, 1998), we focus on active learning with Support Vector Machines (SVMs) because of their performance. The Support Vector Machine, which is introduced by Vapnik (1995), is apowerful new statistical learning method. Excellent performance is reported in hand-written character recognition, face detection, image classification, and so forth. SVMs have been recently applied to several natural language tasks, including text classification (Joachims, 1998; Dumais et al., 19</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>Michele Banko and Eric Brill. 2001. Scaling to very very large corpora for natural language disambiguation. In Proceedings ofACL-2001, pages 26–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Sean P Engelson</author>
</authors>
<title>Committeebased sampling for training probabilistic classifiers.</title>
<date>1995</date>
<booktitle>In Proceedings of the Tweleveth International Conference on Machine Learning,</booktitle>
<pages>150--157</pages>
<contexts>
<context position="2558" citStr="Dagan and Engelson, 1995" startWordPosition="397" endWordPosition="400">r performance often does not match one with a supervised learning method. Another promising approach is active learning, in which a classifier selects examples to be labeled, and then requests a teacher to label them. It is very different from passive learning, in which a classifier gets labeled examples randomly. Active learning is a general framework and does not depend on tasks or domains. It is expected that active learning will reduce considerably manual annotation cost while keeping performance. However, few papers in the field of computational linguistics have focused on this approach (Dagan and Engelson, 1995; Thompson et al., 1999; Ngai and Yarowsky, 2000; Hwa, 2000; Banko and Brill, 2001). Although there are many active learning methods with various classifiers such as a probabilistic classifier (McCallum and Nigam, 1998), we focus on active learning with Support Vector Machines (SVMs) because of their performance. The Support Vector Machine, which is introduced by Vapnik (1995), is apowerful new statistical learning method. Excellent performance is reported in hand-written character recognition, face detection, image classification, and so forth. SVMs have been recently applied to several natur</context>
</contexts>
<marker>Dagan, Engelson, 1995</marker>
<rawString>Ido Dagan and Sean P. Engelson. 1995. Committeebased sampling for training probabilistic classifiers. In Proceedings of the Tweleveth International Conference on Machine Learning, pages 150–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Dumais</author>
<author>John Platt</author>
<author>David Heckerman</author>
<author>Mehran Sahami</author>
</authors>
<title>Inductive learning algorithms and representations for text categorization.</title>
<date>1998</date>
<booktitle>In Proceedings of the ACM CIKM International Conference on Information and Knowledge Management,</booktitle>
<pages>148--155</pages>
<contexts>
<context position="3244" citStr="Dumais et al., 1998" startWordPosition="499" endWordPosition="502"> and Brill, 2001). Although there are many active learning methods with various classifiers such as a probabilistic classifier (McCallum and Nigam, 1998), we focus on active learning with Support Vector Machines (SVMs) because of their performance. The Support Vector Machine, which is introduced by Vapnik (1995), is apowerful new statistical learning method. Excellent performance is reported in hand-written character recognition, face detection, image classification, and so forth. SVMs have been recently applied to several natural language tasks, including text classification (Joachims, 1998; Dumais et al., 1998), chunking (Kudo and Matsumoto, 2000b; Kudo and Matsumoto, 2001), and dependency analysis (Kudo and Matsumoto, 2000a). SVMs have been greatly successful in such tasks. Additionally, SVMs as well as boosting have good theoretical background. The objective of our research is to develop an effective way to build a corpus and to create highperformance NL systems with minimal cost. As a first step, we focus on investigating how active learning with SVMs, which have demonstrated excellent performance, works for complex tasks in natural language processing. For text classification, it is found that t</context>
</contexts>
<marker>Dumais, Platt, Heckerman, Sahami, 1998</marker>
<rawString>Susan Dumais, John Platt, David Heckerman, and Mehran Sahami. 1998. Inductive learning algorithms and representations for text categorization. In Proceedings of the ACM CIKM International Conference on Information and Knowledge Management, pages 148–155.</rawString>
</citation>
<citation valid="true">
<date>1995</date>
<booktitle>EDR Electoric Dictionary Technical Guide.</booktitle>
<institution>EDR (Japan Electoric Dictionary Research Institute),</institution>
<contexts>
<context position="2937" citStr="(1995)" startWordPosition="461" endWordPosition="461">t is expected that active learning will reduce considerably manual annotation cost while keeping performance. However, few papers in the field of computational linguistics have focused on this approach (Dagan and Engelson, 1995; Thompson et al., 1999; Ngai and Yarowsky, 2000; Hwa, 2000; Banko and Brill, 2001). Although there are many active learning methods with various classifiers such as a probabilistic classifier (McCallum and Nigam, 1998), we focus on active learning with Support Vector Machines (SVMs) because of their performance. The Support Vector Machine, which is introduced by Vapnik (1995), is apowerful new statistical learning method. Excellent performance is reported in hand-written character recognition, face detection, image classification, and so forth. SVMs have been recently applied to several natural language tasks, including text classification (Joachims, 1998; Dumais et al., 1998), chunking (Kudo and Matsumoto, 2000b; Kudo and Matsumoto, 2001), and dependency analysis (Kudo and Matsumoto, 2000a). SVMs have been greatly successful in such tasks. Additionally, SVMs as well as boosting have good theoretical background. The objective of our research is to develop an effec</context>
</contexts>
<marker>1995</marker>
<rawString>EDR (Japan Electoric Dictionary Research Institute), 1995. EDR Electoric Dictionary Technical Guide.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
</authors>
<title>Sample selection for statitical grammar induction.</title>
<date>2000</date>
<booktitle>In Proceedings of EMNLP/VLC</booktitle>
<pages>45--52</pages>
<contexts>
<context position="2617" citStr="Hwa, 2000" startWordPosition="410" endWordPosition="411">Another promising approach is active learning, in which a classifier selects examples to be labeled, and then requests a teacher to label them. It is very different from passive learning, in which a classifier gets labeled examples randomly. Active learning is a general framework and does not depend on tasks or domains. It is expected that active learning will reduce considerably manual annotation cost while keeping performance. However, few papers in the field of computational linguistics have focused on this approach (Dagan and Engelson, 1995; Thompson et al., 1999; Ngai and Yarowsky, 2000; Hwa, 2000; Banko and Brill, 2001). Although there are many active learning methods with various classifiers such as a probabilistic classifier (McCallum and Nigam, 1998), we focus on active learning with Support Vector Machines (SVMs) because of their performance. The Support Vector Machine, which is introduced by Vapnik (1995), is apowerful new statistical learning method. Excellent performance is reported in hand-written character recognition, face detection, image classification, and so forth. SVMs have been recently applied to several natural language tasks, including text classification (Joachims,</context>
</contexts>
<marker>Hwa, 2000</marker>
<rawString>Rebecca Hwa. 2000. Sample selection for statitical grammar induction. In Proceedings of EMNLP/VLC 2000, pages 45–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Text categorization with support vector machines: Learning with many relevant features.</title>
<date>1998</date>
<booktitle>In Proceedings of the European Conference on Machine Learning.</booktitle>
<contexts>
<context position="3222" citStr="Joachims, 1998" startWordPosition="497" endWordPosition="498">Hwa, 2000; Banko and Brill, 2001). Although there are many active learning methods with various classifiers such as a probabilistic classifier (McCallum and Nigam, 1998), we focus on active learning with Support Vector Machines (SVMs) because of their performance. The Support Vector Machine, which is introduced by Vapnik (1995), is apowerful new statistical learning method. Excellent performance is reported in hand-written character recognition, face detection, image classification, and so forth. SVMs have been recently applied to several natural language tasks, including text classification (Joachims, 1998; Dumais et al., 1998), chunking (Kudo and Matsumoto, 2000b; Kudo and Matsumoto, 2001), and dependency analysis (Kudo and Matsumoto, 2000a). SVMs have been greatly successful in such tasks. Additionally, SVMs as well as boosting have good theoretical background. The objective of our research is to develop an effective way to build a corpus and to create highperformance NL systems with minimal cost. As a first step, we focus on investigating how active learning with SVMs, which have demonstrated excellent performance, works for complex tasks in natural language processing. For text classificati</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Thorsten Joachims. 1998. Text categorization with support vector machines: Learning with many relevant features. In Proceedings of the European Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese dependency structure analysis based on support vector machines.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>18--25</pages>
<contexts>
<context position="3280" citStr="Kudo and Matsumoto, 2000" startWordPosition="504" endWordPosition="508">e are many active learning methods with various classifiers such as a probabilistic classifier (McCallum and Nigam, 1998), we focus on active learning with Support Vector Machines (SVMs) because of their performance. The Support Vector Machine, which is introduced by Vapnik (1995), is apowerful new statistical learning method. Excellent performance is reported in hand-written character recognition, face detection, image classification, and so forth. SVMs have been recently applied to several natural language tasks, including text classification (Joachims, 1998; Dumais et al., 1998), chunking (Kudo and Matsumoto, 2000b; Kudo and Matsumoto, 2001), and dependency analysis (Kudo and Matsumoto, 2000a). SVMs have been greatly successful in such tasks. Additionally, SVMs as well as boosting have good theoretical background. The objective of our research is to develop an effective way to build a corpus and to create highperformance NL systems with minimal cost. As a first step, we focus on investigating how active learning with SVMs, which have demonstrated excellent performance, works for complex tasks in natural language processing. For text classification, it is found that this approach is effective (Tong and </context>
</contexts>
<marker>Kudo, Matsumoto, 2000</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2000a. Japanese dependency structure analysis based on support vector machines. In Proceedings of the 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 18–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Use of support vector learning for chunk identification.</title>
<date>2000</date>
<booktitle>In Proceedings of the 4th Conference on CoNLL-2000 and LLL2000,</booktitle>
<pages>142--144</pages>
<contexts>
<context position="3280" citStr="Kudo and Matsumoto, 2000" startWordPosition="504" endWordPosition="508">e are many active learning methods with various classifiers such as a probabilistic classifier (McCallum and Nigam, 1998), we focus on active learning with Support Vector Machines (SVMs) because of their performance. The Support Vector Machine, which is introduced by Vapnik (1995), is apowerful new statistical learning method. Excellent performance is reported in hand-written character recognition, face detection, image classification, and so forth. SVMs have been recently applied to several natural language tasks, including text classification (Joachims, 1998; Dumais et al., 1998), chunking (Kudo and Matsumoto, 2000b; Kudo and Matsumoto, 2001), and dependency analysis (Kudo and Matsumoto, 2000a). SVMs have been greatly successful in such tasks. Additionally, SVMs as well as boosting have good theoretical background. The objective of our research is to develop an effective way to build a corpus and to create highperformance NL systems with minimal cost. As a first step, we focus on investigating how active learning with SVMs, which have demonstrated excellent performance, works for complex tasks in natural language processing. For text classification, it is found that this approach is effective (Tong and </context>
</contexts>
<marker>Kudo, Matsumoto, 2000</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2000b. Use of support vector learning for chunk identification. In Proceedings of the 4th Conference on CoNLL-2000 and LLL2000, pages 142–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Chunking with support vector machines.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL</booktitle>
<pages>192--199</pages>
<contexts>
<context position="3308" citStr="Kudo and Matsumoto, 2001" startWordPosition="509" endWordPosition="512">methods with various classifiers such as a probabilistic classifier (McCallum and Nigam, 1998), we focus on active learning with Support Vector Machines (SVMs) because of their performance. The Support Vector Machine, which is introduced by Vapnik (1995), is apowerful new statistical learning method. Excellent performance is reported in hand-written character recognition, face detection, image classification, and so forth. SVMs have been recently applied to several natural language tasks, including text classification (Joachims, 1998; Dumais et al., 1998), chunking (Kudo and Matsumoto, 2000b; Kudo and Matsumoto, 2001), and dependency analysis (Kudo and Matsumoto, 2000a). SVMs have been greatly successful in such tasks. Additionally, SVMs as well as boosting have good theoretical background. The objective of our research is to develop an effective way to build a corpus and to create highperformance NL systems with minimal cost. As a first step, we focus on investigating how active learning with SVMs, which have demonstrated excellent performance, works for complex tasks in natural language processing. For text classification, it is found that this approach is effective (Tong and Koller, 2000; Schohn and Coh</context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines. In Proceedings of NAACL 2001, pages 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>William A Gale</author>
</authors>
<title>A sequential algorithm for training text classifiers.</title>
<date>1994</date>
<booktitle>In Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Rettrieval,</booktitle>
<pages>3--12</pages>
<contexts>
<context position="5690" citStr="Lewis and Gale, 1994" startWordPosition="945" endWordPosition="948">bel examples (a) Apply the current classifier to each unlabeled example (b) Find the m examples which are most informative for the classifier (c) Have the teacher label the subsample of m examples (d) Train a new classifier on all labeled examples Figure 1: Algorithm of pool-based active learning where w = Eli=1 yiaixi. To train an SVM is to find the ai and the b by solving the following optimization problem: aiajyiyjK(xi; xj) subject to 0 &lt; ai &lt; C; di and �l aiyi = 0: i=1 3 Active Learning for Support Vector Machines 3.1 General Framework of Active Learning We use pool-based active learning (Lewis and Gale, 1994). SVMs are used here instead of probabilistic classifiers used by Lewis and Gale. Figure 1 shows an algorithm of pool-based active learning1. There can be various forms of the algorithm depending on what kind of example is found informative. 3.2 Previous Algorithm Two groups have proposed an algorithm for SVMs active learning (Tong and Koller, 2000; Schohn and Cohn, 2000)2. Figure 2 shows the selection algorithm proposed by them. This corresponds to (a) and (b) in Figure 1. 1The figure described here is based on the algorithm by Lewis and Gale (1994) for their sequential sampling algorithm. 2T</context>
</contexts>
<marker>Lewis, Gale, 1994</marker>
<rawString>David D. Lewis and William A. Gale. 1994. A sequential algorithm for training text classifiers. In Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Rettrieval, pages 3–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
<author>Kamal Nigam</author>
</authors>
<title>Employing EM and pool-based active learning for text classification.</title>
<date>1998</date>
<booktitle>In Proceedings of the Fifteenth International Conference on Machine Learning,</booktitle>
<pages>359--367</pages>
<contexts>
<context position="2777" citStr="McCallum and Nigam, 1998" startWordPosition="432" endWordPosition="435">It is very different from passive learning, in which a classifier gets labeled examples randomly. Active learning is a general framework and does not depend on tasks or domains. It is expected that active learning will reduce considerably manual annotation cost while keeping performance. However, few papers in the field of computational linguistics have focused on this approach (Dagan and Engelson, 1995; Thompson et al., 1999; Ngai and Yarowsky, 2000; Hwa, 2000; Banko and Brill, 2001). Although there are many active learning methods with various classifiers such as a probabilistic classifier (McCallum and Nigam, 1998), we focus on active learning with Support Vector Machines (SVMs) because of their performance. The Support Vector Machine, which is introduced by Vapnik (1995), is apowerful new statistical learning method. Excellent performance is reported in hand-written character recognition, face detection, image classification, and so forth. SVMs have been recently applied to several natural language tasks, including text classification (Joachims, 1998; Dumais et al., 1998), chunking (Kudo and Matsumoto, 2000b; Kudo and Matsumoto, 2001), and dependency analysis (Kudo and Matsumoto, 2000a). SVMs have been</context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>Andrew Kachites McCallum and Kamal Nigam. 1998. Employing EM and pool-based active learning for text classification. In Proceedings of the Fifteenth International Conference on Machine Learning, pages 359– 367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grace Ngai</author>
<author>David Yarowsky</author>
</authors>
<title>Rule writing or annotation: Cost-efficient resource usage for base noun phrase chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL-2000,</booktitle>
<pages>117--216</pages>
<contexts>
<context position="2606" citStr="Ngai and Yarowsky, 2000" startWordPosition="406" endWordPosition="409">ervised learning method. Another promising approach is active learning, in which a classifier selects examples to be labeled, and then requests a teacher to label them. It is very different from passive learning, in which a classifier gets labeled examples randomly. Active learning is a general framework and does not depend on tasks or domains. It is expected that active learning will reduce considerably manual annotation cost while keeping performance. However, few papers in the field of computational linguistics have focused on this approach (Dagan and Engelson, 1995; Thompson et al., 1999; Ngai and Yarowsky, 2000; Hwa, 2000; Banko and Brill, 2001). Although there are many active learning methods with various classifiers such as a probabilistic classifier (McCallum and Nigam, 1998), we focus on active learning with Support Vector Machines (SVMs) because of their performance. The Support Vector Machine, which is introduced by Vapnik (1995), is apowerful new statistical learning method. Excellent performance is reported in hand-written character recognition, face detection, image classification, and so forth. SVMs have been recently applied to several natural language tasks, including text classification</context>
</contexts>
<marker>Ngai, Yarowsky, 2000</marker>
<rawString>Grace Ngai and David Yarowsky. 2000. Rule writing or annotation: Cost-efficient resource usage for base noun phrase chunking. In Proceedings of ACL-2000, pages 117–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Platt</author>
</authors>
<title>Fast training of support vector machines using sequential minimal optimization.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods: Support Vector Learning,</booktitle>
<pages>185--208</pages>
<editor>In Bernhard Sch¨olkopf, Christopher J.C. Burges, and Alexander J. Smola, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="12620" citStr="Platt (1999)" startWordPosition="2145" endWordPosition="2146">e EDR Japanese Corpus (EDR, 1995) for experiments. The corpus is assembled from various sources such as newspapers, magazines, and textbooks. It contains 208,000 sentences. We selected randomly 20,000 sentences for training and 4Hiragana and katakana are phonetic characters which represent Japanese syllables. Katakana is primarily used to write foreign words. 10,000 sentences for testing. Then, we created examples using the feature encoding method in Section 4. Through these experiments we used the original SVM tools, the algorithm of which is based on SMO (Sequential Minimal Optimization) by Platt (1999). We used linear SVMs and set a missclassification cost C to 0:2. First, we changed the number of labeled examples which were randomly selected. This is an experiment on passive learning. Table 2 shows the accuracy at different sizes of labeled examples. Second, we changed the number of examples in a pool and ran the active learning algorithm in Section 3.2. We use the same examples for a pool as those used in the passive learning experiments. We selected 1,000 examples at each iteration of the active learning. Figure 4 shows the learning curve of this experiment and Figure 5 is a close-up of </context>
</contexts>
<marker>Platt, 1999</marker>
<rawString>John C. Platt. 1999. Fast training of support vector machines using sequential minimal optimization. In Bernhard Sch¨olkopf, Christopher J.C. Burges, and Alexander J. Smola, editors, Advances in Kernel Methods: Support Vector Learning, pages 185–208. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Schohn</author>
<author>David Cohn</author>
</authors>
<title>Less is more: Active learning with support vector machines.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth International Conference on Machine Learning.</booktitle>
<contexts>
<context position="3916" citStr="Schohn and Cohn, 2000" startWordPosition="607" endWordPosition="610">tsumoto, 2001), and dependency analysis (Kudo and Matsumoto, 2000a). SVMs have been greatly successful in such tasks. Additionally, SVMs as well as boosting have good theoretical background. The objective of our research is to develop an effective way to build a corpus and to create highperformance NL systems with minimal cost. As a first step, we focus on investigating how active learning with SVMs, which have demonstrated excellent performance, works for complex tasks in natural language processing. For text classification, it is found that this approach is effective (Tong and Koller, 2000; Schohn and Cohn, 2000). They used less than 10,000 binary features and less than 10,000 examples. However, it is not clear that the approach is readily applicable to tasks which have more than 100,000 features and more than 100,000 examples. We use Japanese word segmentation as a test case. The task is suitable for our purpose because we have to handle combinations of more than 1,000 characters and a very large corpus (EDR, 1995) exists. 2 Support Vector Machines In this section we give some theoretical definitions of SVMs. Assume that we are given the training data (xi; yi);::: ; (xl; yl); xi E Rn; yi E {+1; —11 T</context>
<context position="6064" citStr="Schohn and Cohn, 2000" startWordPosition="1005" endWordPosition="1008">ing the following optimization problem: aiajyiyjK(xi; xj) subject to 0 &lt; ai &lt; C; di and �l aiyi = 0: i=1 3 Active Learning for Support Vector Machines 3.1 General Framework of Active Learning We use pool-based active learning (Lewis and Gale, 1994). SVMs are used here instead of probabilistic classifiers used by Lewis and Gale. Figure 1 shows an algorithm of pool-based active learning1. There can be various forms of the algorithm depending on what kind of example is found informative. 3.2 Previous Algorithm Two groups have proposed an algorithm for SVMs active learning (Tong and Koller, 2000; Schohn and Cohn, 2000)2. Figure 2 shows the selection algorithm proposed by them. This corresponds to (a) and (b) in Figure 1. 1The figure described here is based on the algorithm by Lewis and Gale (1994) for their sequential sampling algorithm. 2Tong and Koller (2000) propose three selection algorithms. The method described here is simplest and computationally efficient. maximize �l ai — 1 �l i=1 2 i&gt;j=1 1. Compute f(xi) (Equation 2) over all xi in a pool. 2. Sort xi with If(xi)I in decreasing order. 3. Select top m examples. Figure 2: Selection Algorithm 1. Build an initial classifier. 2. While a teacher can labe</context>
<context position="8380" citStr="Schohn and Cohn, 2000" startWordPosition="1408" endWordPosition="1411"> beginning of training, we enlarge gradually a pool of unlabeled examples. The outline of Two Pool Algorithm is shown in Figure 3. We describe below two variations, which are different in the condition at (d) in Figure 3. Our first variation, which is called Two Pool Algorithm A, adds new unlabeled examples to the primary pool when the increasing ratio of support vectors in the current classifier decreases, because the gain of accuracy is very little once the ratio is down. This phenomenon is observed in our experiments (Section 5). This observation has also been reported in previous studies (Schohn and Cohn, 2000). In Two Pool Algorithm we add new unlabeled examples so that the total number of examples including both labeled examples in the training set and unlabeled examples in the primary pool is doubled. For example, suppose that the size of a initial primary pool is 1,000 examples. Before starting training, there are no labeled examples and 1,000 unlabeled examples. We add 1,000 new unlabeled examples to the primary pool when the increasing ratio of support vectors is down after t examples has been labeled. Then, there are the t labeled examples and the (2, 000 — t) unlabeled examples in the primar</context>
</contexts>
<marker>Schohn, Cohn, 2000</marker>
<rawString>Greg Schohn and David Cohn. 2000. Less is more: Active learning with support vector machines. In Proceedings of the Seventeenth International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyuki Shinnou</author>
</authors>
<title>Deterministic Japanese word segmentation by decision list method.</title>
<date>2000</date>
<booktitle>In Proceedings of the Sixth Pacific Rim International Conference on Artificial Intelligence,</booktitle>
<pages>822</pages>
<contexts>
<context position="10328" citStr="Shinnou, 2000" startWordPosition="1761" endWordPosition="1762"> a training set3. When deciding how many unlabeled examples should be added to the primary pool, we use the strategy as described in the paragraph above. 4 Japanese Word Segmentation 4.1 Word Segmentation as a Classification Task Many tasks in natural language processing can be formulated as a classification task (van den Bosch 3Since typically the percentage of support vectors is small (e.g., less than 30 %), we choose around 10 % for Æ. We need further studies to find the best value of Æ before or during training. et al., 1996). Japanese word segmentation can be viewed in the same way, too (Shinnou, 2000). Let a Japanese character sequence be s = c1c2 • • • cm and a boundary bi exist between ci and ci+1. The bi is either +1 (word boundary) or -1 (non-boundary). The word segmentation task can be defined as determining the class of the bi. We use an SVM to determine it. 4.2 Features We assume that each character ci has two attributes. The first attribute is a character type (ti). It can be hiragana4, katakana, kanji (Chinese characters), numbers, English alphabets, kanji-numbers (numbers written in Chinese), or symbols. A character type gives some hints to segment a Japanese sentence to words. F</context>
</contexts>
<marker>Shinnou, 2000</marker>
<rawString>Hiroyuki Shinnou. 2000. Deterministic Japanese word segmentation by decision list method. In Proceedings of the Sixth Pacific Rim International Conference on Artificial Intelligence, page 822.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia A Thompson</author>
<author>Mary Leaine Califf</author>
<author>Raymond J Mooney</author>
</authors>
<title>Active learning for natural language parsing and information extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of the Sixteenth International Conference on Machine Learning,</booktitle>
<pages>406--414</pages>
<contexts>
<context position="2581" citStr="Thompson et al., 1999" startWordPosition="401" endWordPosition="405">ot match one with a supervised learning method. Another promising approach is active learning, in which a classifier selects examples to be labeled, and then requests a teacher to label them. It is very different from passive learning, in which a classifier gets labeled examples randomly. Active learning is a general framework and does not depend on tasks or domains. It is expected that active learning will reduce considerably manual annotation cost while keeping performance. However, few papers in the field of computational linguistics have focused on this approach (Dagan and Engelson, 1995; Thompson et al., 1999; Ngai and Yarowsky, 2000; Hwa, 2000; Banko and Brill, 2001). Although there are many active learning methods with various classifiers such as a probabilistic classifier (McCallum and Nigam, 1998), we focus on active learning with Support Vector Machines (SVMs) because of their performance. The Support Vector Machine, which is introduced by Vapnik (1995), is apowerful new statistical learning method. Excellent performance is reported in hand-written character recognition, face detection, image classification, and so forth. SVMs have been recently applied to several natural language tasks, incl</context>
</contexts>
<marker>Thompson, Califf, Mooney, 1999</marker>
<rawString>Cynthia A. Thompson, Mary Leaine Califf, and Raymond J. Mooney. 1999. Active learning for natural language parsing and information extraction. In Proceedings of the Sixteenth International Conference on Machine Learning, pages 406–414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Tong</author>
<author>Daphne Koller</author>
</authors>
<title>Support vector machine active learning with applications to text classification.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth International Conference on Machine Learning.</booktitle>
<contexts>
<context position="3892" citStr="Tong and Koller, 2000" startWordPosition="603" endWordPosition="606">oto, 2000b; Kudo and Matsumoto, 2001), and dependency analysis (Kudo and Matsumoto, 2000a). SVMs have been greatly successful in such tasks. Additionally, SVMs as well as boosting have good theoretical background. The objective of our research is to develop an effective way to build a corpus and to create highperformance NL systems with minimal cost. As a first step, we focus on investigating how active learning with SVMs, which have demonstrated excellent performance, works for complex tasks in natural language processing. For text classification, it is found that this approach is effective (Tong and Koller, 2000; Schohn and Cohn, 2000). They used less than 10,000 binary features and less than 10,000 examples. However, it is not clear that the approach is readily applicable to tasks which have more than 100,000 features and more than 100,000 examples. We use Japanese word segmentation as a test case. The task is suitable for our purpose because we have to handle combinations of more than 1,000 characters and a very large corpus (EDR, 1995) exists. 2 Support Vector Machines In this section we give some theoretical definitions of SVMs. Assume that we are given the training data (xi; yi);::: ; (xl; yl); </context>
<context position="6040" citStr="Tong and Koller, 2000" startWordPosition="1001" endWordPosition="1004">he ai and the b by solving the following optimization problem: aiajyiyjK(xi; xj) subject to 0 &lt; ai &lt; C; di and �l aiyi = 0: i=1 3 Active Learning for Support Vector Machines 3.1 General Framework of Active Learning We use pool-based active learning (Lewis and Gale, 1994). SVMs are used here instead of probabilistic classifiers used by Lewis and Gale. Figure 1 shows an algorithm of pool-based active learning1. There can be various forms of the algorithm depending on what kind of example is found informative. 3.2 Previous Algorithm Two groups have proposed an algorithm for SVMs active learning (Tong and Koller, 2000; Schohn and Cohn, 2000)2. Figure 2 shows the selection algorithm proposed by them. This corresponds to (a) and (b) in Figure 1. 1The figure described here is based on the algorithm by Lewis and Gale (1994) for their sequential sampling algorithm. 2Tong and Koller (2000) propose three selection algorithms. The method described here is simplest and computationally efficient. maximize �l ai — 1 �l i=1 2 i&gt;j=1 1. Compute f(xi) (Equation 2) over all xi in a pool. 2. Sort xi with If(xi)I in decreasing order. 3. Select top m examples. Figure 2: Selection Algorithm 1. Build an initial classifier. 2. </context>
<context position="14488" citStr="Tong and Koller (2000)" startWordPosition="2481" endWordPosition="2485"> see how the accuracy curve varies depending on the size of a pool. Surprisingly, the performance of a larger pool is worse than that of a smaller pool in the early stage of trainini. One reason for this could be that support vectors in selected examples at each iteration from a larger pool make larger clusters than those selected from a smaller pool do. In other words, in the case of a larger pool, more examples selected at each iteration would be similar to each other. We computed variances6of each 1,000 selected examples at the learning iteration from 2 to 11 (Table 1). The variances of se5Tong and Koller (2000) have got the similar results in a text classification task with two small pools: 500 and 1000. However, they have concluded that a larger pool is better than a smaller one because the final accuracy of the former is higher than that of the latter. 6The variance vz of a set of selected examples xi is defined Table 1: Variances of Selected Examples Iteration 2 3 4 5 6 7 8 9 10 11 1,250 Sent. Size Pool 16.87 17.25 17.85 17.63 17.24 17.37 17.34 17.73 17.94 17.57 20,000 Sent. Size Pool 16.66 17.03 16.92 16.75 16.80 16.72 16.91 16.93 16.87 16.97 lected examples using the 20,000 sentence size pool i</context>
</contexts>
<marker>Tong, Koller, 2000</marker>
<rawString>Simon Tong and Daphne Koller. 2000. Support vector machine active learning with applications to text classification. In Proceedings of the Seventeenth International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antal van den Bosch</author>
<author>Walter Daelemans</author>
<author>Ton Weijters</author>
</authors>
<title>Morphological analysis as classification: an inductive-learning approach.</title>
<date>1996</date>
<booktitle>In Proceedings of the Second International Conference on New Methods in Natural Language Processing,</booktitle>
<pages>79--89</pages>
<marker>van den Bosch, Daelemans, Weijters, 1996</marker>
<rawString>Antal van den Bosch, Walter Daelemans, and Ton Weijters. 1996. Morphological analysis as classification: an inductive-learning approach. In Proceedings of the Second International Conference on New Methods in Natural Language Processing, pages 79–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="2937" citStr="Vapnik (1995)" startWordPosition="460" endWordPosition="461">ains. It is expected that active learning will reduce considerably manual annotation cost while keeping performance. However, few papers in the field of computational linguistics have focused on this approach (Dagan and Engelson, 1995; Thompson et al., 1999; Ngai and Yarowsky, 2000; Hwa, 2000; Banko and Brill, 2001). Although there are many active learning methods with various classifiers such as a probabilistic classifier (McCallum and Nigam, 1998), we focus on active learning with Support Vector Machines (SVMs) because of their performance. The Support Vector Machine, which is introduced by Vapnik (1995), is apowerful new statistical learning method. Excellent performance is reported in hand-written character recognition, face detection, image classification, and so forth. SVMs have been recently applied to several natural language tasks, including text classification (Joachims, 1998; Dumais et al., 1998), chunking (Kudo and Matsumoto, 2000b; Kudo and Matsumoto, 2001), and dependency analysis (Kudo and Matsumoto, 2000a). SVMs have been greatly successful in such tasks. Additionally, SVMs as well as boosting have good theoretical background. The objective of our research is to develop an effec</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir N. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Richard Wicentowski</author>
</authors>
<title>Minimally supervised morphological analysis by multimodal alignment.</title>
<date>2000</date>
<booktitle>In Proceedings ofACL-2000,</booktitle>
<pages>207--216</pages>
<contexts>
<context position="1847" citStr="Yarowsky and Wicentowski, 2000" startWordPosition="284" endWordPosition="287"> examples with random sampling. 1 Introduction Corpus-based supervised learning is now a standard approach to achieve high-performance in natural language processing. However, the weakness of supervised learning approach is to need an annotated corpus, the size of which is reasonably large. Even if we have a good supervised-learning method, we cannot get high-performance without an annotated corpus. The problem is that corpus annotation is labour intensive and very expensive. In order to overcome this, some unsupervised learning methods and minimally-supervised methods, e.g., (Yarowsky, 1995; Yarowsky and Wicentowski, 2000), have been proposed. However, such methods usually depend on tasks or domains and their performance often does not match one with a supervised learning method. Another promising approach is active learning, in which a classifier selects examples to be labeled, and then requests a teacher to label them. It is very different from passive learning, in which a classifier gets labeled examples randomly. Active learning is a general framework and does not depend on tasks or domains. It is expected that active learning will reduce considerably manual annotation cost while keeping performance. Howeve</context>
</contexts>
<marker>Yarowsky, Wicentowski, 2000</marker>
<rawString>David Yarowsky and Richard Wicentowski. 2000. Minimally supervised morphological analysis by multimodal alignment. In Proceedings ofACL-2000, pages 207–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sence disambiguation rivaling supvervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings ofACL-1995,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="1814" citStr="Yarowsky, 1995" startWordPosition="282" endWordPosition="283">7.4 % of labeled examples with random sampling. 1 Introduction Corpus-based supervised learning is now a standard approach to achieve high-performance in natural language processing. However, the weakness of supervised learning approach is to need an annotated corpus, the size of which is reasonably large. Even if we have a good supervised-learning method, we cannot get high-performance without an annotated corpus. The problem is that corpus annotation is labour intensive and very expensive. In order to overcome this, some unsupervised learning methods and minimally-supervised methods, e.g., (Yarowsky, 1995; Yarowsky and Wicentowski, 2000), have been proposed. However, such methods usually depend on tasks or domains and their performance often does not match one with a supervised learning method. Another promising approach is active learning, in which a classifier selects examples to be labeled, and then requests a teacher to label them. It is very different from passive learning, in which a classifier gets labeled examples randomly. Active learning is a general framework and does not depend on tasks or domains. It is expected that active learning will reduce considerably manual annotation cost </context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sence disambiguation rivaling supvervised methods. In Proceedings ofACL-1995, pages 189–196.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>