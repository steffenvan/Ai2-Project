<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001983">
<title confidence="0.990139">
Analysis and Prediction of Unalignable Words in Parallel Text
</title>
<author confidence="0.999102">
Frances Yung Kevin Duh Yuji Matsumoto
</author>
<affiliation confidence="0.999719">
Nara Institute of Science and Technology
</affiliation>
<address confidence="0.718385">
8916-5 Takayama, Ikoma, Nara, 630-0192 Japan
</address>
<email confidence="0.997313">
pikyufrances-y|kevinduh|matsu@is.naist.jp
</email>
<sectionHeader confidence="0.993851" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999802857142857">
Professional human translators usually do
not employ the concept of word align-
ments, producing translations ‘sense-for-
sense’ instead of ‘word-for-word’. This
suggests that unalignable words may be
prevalent in the parallel text used for ma-
chine translation (MT). We analyze this
phenomenon in-depth for Chinese-English
translation. We further propose a sim-
ple and effective method to improve au-
tomatic word alignment by pre-removing
unalignable words, and show improve-
ments on hierarchical MT systems in both
translation directions.
</bodyText>
<sectionHeader confidence="0.987923" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.999944055555555">
It is generally acknowledged that absolute equiva-
lence between two languages is impossible, since
concept lexicalization varies across languages.
Major translation theories thus argue that texts
should be translated ‘sense-for-sense’ instead of
‘word-for-word’ (Nida, 1964). This suggests that
unalignable words may be an issue for the parallel
text used to train current statistical machine trans-
lation (SMT) systems. Although existing auto-
matic word alignment methods have some mech-
anism to handle the lack of exact word-for-word
alignment (e.g. null probabilities, fertility in the
IBM models (Brown et al., 1993)), they may be
too coarse-grained to model the ’sense-for-sense’
translations created by professional human trans-
lators.
For example, the Chinese term ‘tai-yang’ liter-
ally means ‘sun’, yet the concept it represents is
equivalent to the English term ‘the sun’. Since the
concept of a definite article is not incorporated in
the morphology of ‘tai yang’, the added ‘the’ is
not aligned to any Chinese word. Yet in another
context like ’the man’, ‘the’ can be the translation
of the Chinese demonstrative pronoun ‘na’, liter-
ally means ‘that’. A potential misunderstanding is
that unalignable words are simply function words;
but from the above example, we see that whether a
word is alignable depends very much on the con-
cept and the linguistic context.
As the quantity and quality of professionally-
created parallel text increase, we believe there is a
need to examine the question of unalignable words
in-depth. Our goal is to gain a better understand-
ing of what makes a fluent human translation and
use this insight to build better word aligners and
MT systems. Our contributions are two-fold:
</bodyText>
<listItem confidence="0.809088285714286">
1) We analyze 13000 sentences of manually word-
aligned Chinese-English parallel text, quantifying
the characteristics of unalignable words.
2) We propose a simple and effective way to im-
prove automatic word alignment, based on pre-
dicting unalignable words and temporarily remov-
ing them during the alignment training procedure.
</listItem>
<subsectionHeader confidence="0.4655">
2 Analysis of Unalignable Words
</subsectionHeader>
<bodyText confidence="0.999296176470588">
Our manually-aligned data, which we call OR-
ACLE data, is a Chinese-to-English corpus re-
leased by the LDC (Li et al., 2010)1. It con-
sists of ∼13000 Chinese sentences from news and
blog domains and their English translation . En-
glish words are manually aligned with the Chinese
characters. Characters without an exact counter-
part are annotated with categories that state the
functions of the words. These characters are ei-
ther aligned to ‘NULL’, or attached to their depen-
dency heads, if any, and aligned together to form
a multi-word alignment. For example, ‘the’ is an-
notated as [DET], for ‘determiner’, and aligned to
‘tai-yang’ together with ‘sun’.
In this work, any English word or Chinese char-
acter without an exact counterpart are called un-
alignable words, since they are not core to the
</bodyText>
<footnote confidence="0.719077">
&apos;LDC2012T16, LDC2012T20 and LDC2012T24
</footnote>
<page confidence="0.914117">
190
</page>
<note confidence="0.8150405">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 190–194,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<table confidence="0.990088333333333">
word unalignable core
types tokens tokens
core or 3581 146,693 562,801
unalignable (12%) (17%) (66%)
always 25320 / 147,373
core (88%) (17%)
</table>
<tableCaption confidence="0.91624425">
Table 1: Number of core and unalignable words in
hand aligned ORACLE corpus
multi-word alignment. All other English words or
Chinese characters are referred to as core words.
</tableCaption>
<subsectionHeader confidence="0.991856">
2.1 What kind of words are unalignable?
</subsectionHeader>
<bodyText confidence="0.9940357">
Analyzing the hand aligned corpus, we find that
words annotated as unalignable do not come from
a distinct list. Table 1 reveals that 88% of the
word types are unambiguously core words. Yet
these word types, including singletons, account
for only 17% of the word tokens. On the other
hand, another 17% of the total word tokens are
annotated as unalignable. So, most word types are
possibly unalignable but only in a small portion of
their occurrence, such as the following examples:
</bodyText>
<listItem confidence="0.986978">
(1a) Chi: yi ge di fang
one (measure word) place
Eng: one place
(1b) Chi: ge ren
personal
Eng: personal
(2a) Chi: ming tian zhong wu
(tomorrow) (midday)
Eng: tomorrow at midday
(2b) Chi: zai jia
at/in/on home
</listItem>
<bodyText confidence="0.970302625">
Eng: at home
In example (1a), ‘ge’ is a measure word that is
exclusive in Chinese, but in (1b), it is part of the
multiword unit ’ge-ren’ for ’personal’. Similarly,
prepositions, such as ‘at’, can either be omitted or
translated depending on context.
Nonetheless, unalignable words are by no
means evenly distributed among word types. Ta-
ble 2 shows that the top 100 most frequent un-
alignable word types already covers 78% and 94%
of all Chinese and English unalignable instances,
respectively. Word type is thus an important clue.
Intuitively, words with POS defined only in one
of the languages are likely to be unalignable. To
examine this, we automatically tagged the ORA-
CLE data using the Standford Tagger (Toutanova
</bodyText>
<table confidence="0.984618">
Most frequent Token count
unalignable word types
Chinese English
Top 50 34,987 83,905
(68%) (88%)
Top 100 40,121 89,609
(78%) (94%)
</table>
<tableCaption confidence="0.999071">
Table 2: Count of unalignable words by types
</tableCaption>
<bodyText confidence="0.998970111111111">
et al., 2003). We find that the unalignable words
include all POS categories of either language,
though indeed some POS are more frequent. Ta-
ble 3 lists the top 5 POS categories that most un-
alignable words belong to and the percentage they
are annotated as unalignable. Some POS cate-
gories like DEG are mostly unalignable regardless
of context, but other POS tags such as DT and IN
depend on context.
</bodyText>
<table confidence="0.917305428571429">
Chi. No. and % of Eng. No. and % of
POS unalign. POS unalign.
DEG 7411(97%) DT 27715 (75%)
NN 6138 (4%) IN 19303 (47%)
AD 6068 (17%) PRP 5780 (56%)
DEC 5572 (97%) TO 5407 (62%)
VV 4950 (6%) CC 4145 (36%)
</table>
<tableCaption confidence="0.964317">
Table 3: Top 5 POS categories of Chinese and En-
</tableCaption>
<bodyText confidence="0.820396">
glish unalignable words
Note also that many Chinese unalignable words
are nouns (NN) and verbs (VV). Clearly we cannot
indiscriminately consider all nouns as unalignable.
Some examples of unalignable content words in
Chinese are:
</bodyText>
<listItem confidence="0.99997375">
(3) Chi: can jia hui jian huo dong
participate meeting activity
Eng: participate in the meeting
(4) Chi: hui yi de yuan man ju xing
</listItem>
<bodyText confidence="0.936495846153846">
meeting ’s successful take place
Eng: success of the meeting
English verbs and adjectives are often nomi-
nalized to abstract nouns (such as ’meeting’ from
’meet’, or ’success’ from ’succeed’), but such
derivation is rare in Chinese morphology. Since
POS is not morphologically marked in Chinese,
’meeting’ and’meet’ are the same word. To reduce
the processing ambiguity and produce more nat-
ural translation, extra content words are added to
mark the nominalization of abstract concepts. For
example, ‘hui jian’ is originally ‘to meet’. Adding
‘huo dong’(activity) transforms it to a noun phrase
</bodyText>
<page confidence="0.993944">
191
</page>
<bodyText confidence="0.99975075">
(example 3), similar to the the addition of ‘ju
sing’(take place) to the adjective ‘yuan man’ (ex-
ample 4). These unalignable words are not lexi-
cally dependent but are inferred from the context,
and thus do not align to any source words.
To summarize, a small number of word types
cover 17% of word tokens that are unalignable,
but whether these words are unalignable depends
significantly on context. Although there is no list
of ‘always unalignable’ words types or POS cat-
egories, our analysis shows there are regularities
that may be exploited by an automatic classifier.
</bodyText>
<sectionHeader confidence="0.996444" genericHeader="introduction">
3 Improved Automatic Word Alignment
</sectionHeader>
<bodyText confidence="0.971349625">
We first propose a classifier for predicting whether
a word is unalignable. Let (ei , fK ) be a pair of
sentence with length J and K. For each word in
(ei, fK) that belongs to a predefined list2 of po-
tentially unalignable words, we run a binary clas-
sifier. A separate classifier is built for each word
type in the list, and an additional classifier for all
the remaining words in each language.
We train an SVM classifier based on the fol-
lowing features: Local context: Unigrams and
POS in window sizes of 1, 3, 5, 7 around the
word in question. Top token-POS pairs: This
feature is defined by whether the token in ques-
tion and its POS tag is within the top n frequent
token-POS pairs annotated as unalignable like in
Tables 2 and 3. Four features are defined with n =
10, 30, 50,100. Since the top frequent unalignable
words cover most of the counts as shown in the
previous analysis, being in the top n list is a strong
positive features. Number of likely unalignable
words per sentence: We hypothesize that the
translator will not add too many tokens to the
translation and delete too many from the source
sentence. In the ORACLE data, 68% sentences
have more than 2 unalignable words. We approx-
imate the number of likely unalignable words in
the sentence by counting the number of words
within the top 100 token-POS pairs annotated as
unalignable. Sentence length and ratio: Longer
sentences are more likely to contain unalignable
words than shorter sentences. Also sentence ra-
tios that deviate significantly from the mean are
likely to contain unalignable words. Presence of
alignment candidate: This is a negative feature
defined by whether there is an alignment candi-
2We define the list as the top 100 word types with the
highest count of unalignable words per language according
to the hand annotated data.
date in the target sentence for the source word in
question, or vice versa. The candidates are ex-
tracted from the top n frequent words aligned to
a particular word according to the manual align-
ments of the ORACLE data. Five features are de-
fined with n = 5, 10, 20, 50,100 and one ’without
limit’, such that a more possible candidate will be
detected by more features.
Next, we propose a simple yet effective mod-
ification to the word alignment training pipeline:
</bodyText>
<listItem confidence="0.999211125">
1. Predict unalignable words by the classifier
2. Remove these words from the training corpus
3. Train word alignment model (e.g. GIZA++)3
4. Combine the word alignments in both direc-
tions with heuristics (grow-diag-final-and)
5. Restore unaligned words to original position
6. Continue with rule extraction and the rest of
the MT pipeline.
</listItem>
<bodyText confidence="0.9995585">
The idea is to reduce the difficulty for the word
alignment model by removing unaligned words.
</bodyText>
<sectionHeader confidence="0.967152" genericHeader="method">
4 End-to-End Translation Experiments
</sectionHeader>
<bodyText confidence="0.999736">
In our experiments, we first show that removing
manually-annotated unaligned words in ORACLE
data leads to improvements in MT of both trans-
lation directions. Next, we show how a classifier
trained on ORACLE data can be used to improve
MT in another large-scale un-annotated dataset.4
</bodyText>
<subsectionHeader confidence="0.976878">
4.1 Experiments on ORACLE data
</subsectionHeader>
<bodyText confidence="0.999961">
We first performed an ORACLE experiment us-
ing gold standard unaligned word labels. Follow-
ing the training pipeline in Section 3, we removed
gold unalignable words before running GIZA++
and restore them afterwards. 90% of the data is
used for alignment and MT training, while 10% of
the data is reserved for testing.
The upper half of Table 4 list the alignment
precision, recall and F1 of the resulting align-
ments, and quality of the final MT outputs. Base-
line is the standard MT training pipeline with-
out removal of unaligned words. Our Proposed
approach performs better in alignment, phrase-
based (PBMT) and hierarchical (Hiero) systems.
The results, evaluated by BLEU, METEOR and
TER, support our hypothesis that removing gold
unalignable words helps improve word alignment
and the resulting SMT.
</bodyText>
<footnote confidence="0.9967506">
3We can suppress the NULL probabilities of the model.
4All experiments are done using standard settings for
Moses PBMT and Hiero with 4-gram LM and mslr-
bidirectional-fe reordering (Koehn et al., 2007). The clas-
sifier is trained using LIBSVM (Chang and Lin, 2011).
</footnote>
<page confidence="0.985008">
192
</page>
<table confidence="0.9996015">
Align PBMT Hiero
acc. C-E E-C C-E E-C
ORACLE P .711 B 11.4 17.4 10.3 15.8
Baseline R .488 T 70.9 69.0 75.9 72.3
F1.579 M 21.8 23.9 21.08 23.7
ORACLE P .802 B 11.8+ 18.3+ 11.0+17.2+
Proposed R .509 T 71.4− 65.7+ 74.7+68.7+
(gold) F1.623 M 22.1+ 24.1+ 22.0+24.0+
REAL B 18.2 18.5 17.0 17.2
Baseline T 63.4 67.2 68.0 71.4
M 22.9 24.6 22.9 24.8
REAL B 18.6 18.5 17.6+18.1+
Proposed T 63.8− 66.5+ 67.6 69.7+
(predict) M 23.2+ 24.5 23.4+24.7
</table>
<tableCaption confidence="0.9381034">
Table 4: MT results of ORACLE and REAL ex-
periments. Highest score per metric is bolded.
{+/−} indicates statistically significant improve-
ment/degradation, p &lt; 0.05. (P: precision; R: re-
call; B: BLEU; M: METEOR; T:TER)
</tableCaption>
<bodyText confidence="0.999896625">
For comparison, a naive classifier that labels
all top-30 token-POS combinations as unalignable
performs poorly as expected (PBMT BLEU: 9.87
in C-E direction). We also evaluated our proposed
classifier on this task: the accuracy is 92% and it
achieves BLEU of 11.55 for PBMT and 10.84 for
Hiero in C-E direction, which is between the re-
sults of gold-unalign and baseline.
</bodyText>
<subsectionHeader confidence="0.994972">
4.2 Experiments on large-scale REAL data
</subsectionHeader>
<bodyText confidence="0.999979428571429">
We next performed a more realistic experiment:
the classifier trained on ORACLE data is used to
automatically label a large data, which is then used
to train a MT system. This REAL data consists of
parallel text from the NIST OpenMT2008.5 MT
experiments are performed in both directions.
The lower half of Table 4 shows the perfor-
mance of the resulting MT systems. We observe
that our proposed approach is still able to improve
over the baseline. In particular, Hiero achieved
statistical significant improvements in BLEU and
METEOR. 6 Comparing to the results of PBMT,
this suggests our method may be most effective in
improving systems where rule extraction is sen-
</bodyText>
<footnote confidence="0.9973685">
5We use the standard MT08 test sets; the training
data includes LDC2004T08, 2005E47, 2005T06, 2007T23,
2008T06, 2008T08, 2008T18, 2009T02, 2009T06, 2009T15,
and 2010T03 (34M English words and 1.1M sentences).
Since we do not have access to all OpenMT data, e.g. FBIS,
our results may not be directly comparable to other systems
in the evaluation.
6Interestingly, PBMT did better than Hiero in this setup.
</footnote>
<table confidence="0.998639428571429">
Chinese English lexical translation
word
Baseline only Propose only
xie (bring) him bringing
xing (form) and model
dan (but) it, the, they yet, nevertheless
pa (scare) that, are, be fears, worried
</table>
<tableCaption confidence="0.969548">
Table 5: Examples of translations exclusively
found in the top 15 lexical translation.
</tableCaption>
<figureCaption confidence="0.981987">
Figure 1: Classifier accuracy and MT results V.S.
proportion of ORACLE data
</figureCaption>
<bodyText confidence="0.999910583333333">
sitive to the underlying alignments, such as Hi-
ero and Syntax-based MT. Table 5 shows the lex-
ical translations for some rare Chinese words: the
baseline tends to incorrectly align these to func-
tion words (garbage collection), while the pro-
posed method’s translations are more reasonable.
To evaluate how much annotation is needed for
the classifier, we repeat experiments using differ-
ent proportions of the ORACLE data. Figure 1
shows training by 20% of the data (2600 sents.)
already leads to significant improvements (p &lt;
0.05), which is a reasonable annotation effort.
</bodyText>
<sectionHeader confidence="0.998904" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999960352941177">
We analyzed in-depth the phenomenon of un-
alignable words in parallel text, and show that
what is unalignable depends on the word’s concept
and context. We argue that this is not a trivial prob-
lem, but with an unalignable word classifier and
a simple modified MT training pipeline, we can
achieve small but significant gains in end-to-end
translation. In related work, the issue of dropped
pronouns (Chung and Gildea, 2010) and function
words (Setiawan et al., 2010; Nakazawa and Kuro-
hashi, 2012) have been found important in word
alignment, and (Fossum et al., 2008) showed that
syntax features are helpful for fixing alignments.
An interesting avenue of future work is to integrate
these ideas with ours, in particular by exploiting
syntax and viewing unalignable words as aligned
at a structure above the lexical level.
</bodyText>
<page confidence="0.998907">
193
</page>
<sectionHeader confidence="0.99558" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999729734693878">
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2).
Chih-Chung Chang and Chih-Jen Lin. 2011. Lib-
svm : a library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2(27).
Tagyoung Chung and Daniel Gildea. 2010. Effects
of empty categories on machine translation. Pro-
ceedings of the Conference on Empirical Methods
on Natural Language Processing.
Victoria Fossum, Kevin Knight, and Steven Abney.
2008. Using syntax to improve word alignment pre-
cision for syntax-based machine translation. Pro-
ceedings of the Workshop on Statistical Machine
Translation.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. Proceedings of the Annual Meeting of the As-
sociation for Computational Linguistics.
Xuansong Li, Niyu Ge, Stephen Grimes, Stephanie M.
Strassel, and Kazuaki Maeda. 2010. Enriching
word alignment with linguistic tags. Proceedings
of International Conference on Language Resources
and Evaluation.
Toshiaki Nakazawa and Sado Kurohashi. 2012.
Alignment by bilingual generation and monolingual
derivation. Proceedings of the International Confer-
ence on Computational Linguistics.
Eugene A Nida. 1964. Toward a Science of Translat-
ing: with Special Reference to Principles and Pro-
cedures Involved in Bible Translating. BRILL.
Hendra Setiawan, Chris Dyer, and Philip Resnik. 2010.
Discriminative word alignment with a function word
reordering model. Proceedings of the Conference on
Empirical Methods on Natural Language Process-
ing.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
Proceedings of the Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies.
</reference>
<page confidence="0.998794">
194
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.998914">Analysis and Prediction of Unalignable Words in Parallel Text</title>
<author confidence="0.997163">Frances Yung Kevin Duh Yuji Matsumoto</author>
<affiliation confidence="0.999865">Nara Institute of Science and</affiliation>
<address confidence="0.97559">8916-5 Takayama, Ikoma, Nara, 630-0192 Japan</address>
<email confidence="0.975029">pikyufrances-y|kevinduh|matsu@is.naist.jp</email>
<abstract confidence="0.984453532608695">Professional human translators usually do not employ the concept of word alignproducing translations ‘sense-forof This suggests that unalignable words may be prevalent in the parallel text used for machine translation (MT). We analyze this phenomenon in-depth for Chinese-English translation. We further propose a simple and effective method to improve automatic word alignment by pre-removing unalignable words, and show improvements on hierarchical MT systems in both translation directions. 1 Motivation It is generally acknowledged that absolute equivalence between two languages is impossible, since concept lexicalization varies across languages. Major translation theories thus argue that texts be translated of 1964). This suggests that unalignable words may be an issue for the parallel text used to train current statistical machine translation (SMT) systems. Although existing automatic word alignment methods have some mechanism to handle the lack of exact word-for-word alignment (e.g. null probabilities, fertility in the IBM models (Brown et al., 1993)), they may be too coarse-grained to model the ’sense-for-sense’ translations created by professional human translators. example, the Chinese term litermeans yet the concept it represents is to the English term Since the concept of a definite article is not incorporated in morphology of the added not aligned to any Chinese word. Yet in another like be the translation the Chinese demonstrative pronoun litermeans A potential misunderstanding is that unalignable words are simply function words; but from the above example, we see that whether a word is alignable depends very much on the concept and the linguistic context. As the quantity and quality of professionallycreated parallel text increase, we believe there is a need to examine the question of unalignable words in-depth. Our goal is to gain a better understanding of what makes a fluent human translation and use this insight to build better word aligners and MT systems. Our contributions are two-fold: 1) We analyze 13000 sentences of manually wordaligned Chinese-English parallel text, quantifying the characteristics of unalignable words. 2) We propose a simple and effective way to improve automatic word alignment, based on predicting unalignable words and temporarily removing them during the alignment training procedure. 2 Analysis of Unalignable Words Our manually-aligned data, which we call OR- ACLE data, is a Chinese-to-English corpus reby the LDC (Li et al., It conof Chinese sentences from and their English translation . English words are manually aligned with the Chinese characters. Characters without an exact counterpart are annotated with categories that state the functions of the words. These characters are either aligned to ‘NULL’, or attached to their dependency heads, if any, and aligned together to form multi-word alignment. For example, annotated as [DET], for ‘determiner’, and aligned to with In this work, any English word or Chinese charwithout an exact counterpart are called unsince they are not core to the LDC2012T20 and LDC2012T24 190 of the 14th Conference of the European Chapter of the Association for Computational pages 190–194, Sweden, April 26-30 2014. Association for Computational Linguistics word types unalignable core tokens tokens core or 3581 146,693 562,801 unalignable (12%) (17%) (66%) always 25320 / 147,373 core (88%) (17%) Table 1: Number of core and unalignable words in hand aligned ORACLE corpus multi-word alignment. All other English words or characters are referred to as 2.1 What kind of words are unalignable? Analyzing the hand aligned corpus, we find that words annotated as unalignable do not come from a distinct list. Table 1 reveals that 88% of the word types are unambiguously core words. Yet these word types, including singletons, account for only 17% of the word tokens. On the other hand, another 17% of the total word tokens are annotated as unalignable. So, most word types are possibly unalignable but only in a small portion of their occurrence, such as the following examples: Chi: ge di fang one (measure word) place Chi: ren personal Chi: tian zhong wu (tomorrow) (midday) at midday Chi: at/in/on home home example (1a), a measure word that is exclusive in Chinese, but in (1b), it is part of the unit Similarly, such as can either be omitted or translated depending on context. Nonetheless, unalignable words are by no means evenly distributed among word types. Table 2 shows that the top 100 most frequent unalignable word types already covers 78% and 94% of all Chinese and English unalignable instances, respectively. Word type is thus an important clue. Intuitively, words with POS defined only in one of the languages are likely to be unalignable. To examine this, we automatically tagged the ORA- CLE data using the Standford Tagger (Toutanova Most frequent Token count types Chinese English Top 50 34,987 83,905 (68%) (88%) Top 100 40,121 89,609 (78%) (94%) 2: Count of by types et al., 2003). We find that the unalignable words include all POS categories of either language, though indeed some POS are more frequent. Table 3 lists the top 5 POS categories that most unalignable words belong to and the percentage they are annotated as unalignable. Some POS categories like DEG are mostly unalignable regardless of context, but other POS tags such as DT and IN depend on context. POS No. and % of unalign. Eng. POS No. and % of unalign. DEG 7411(97%) DT 27715 (75%) NN 6138 (4%) IN 19303 (47%) AD 6068 (17%) PRP 5780 (56%) DEC 5572 (97%) TO 5407 (62%) VV 4950 (6%) CC 4145 (36%) Table 3: Top 5 POS categories of Chinese and English unalignable words Note also that many Chinese unalignable words are nouns (NN) and verbs (VV). Clearly we cannot indiscriminately consider all nouns as unalignable. Some examples of unalignable content words in Chinese are: Chi: jia hui jian huo participate meeting activity in the meeting Chi: yi de yuan man ju xing meeting ’s successful take place of the meeting English verbs and adjectives are often nomito abstract nouns (such as or but such derivation is rare in Chinese morphology. Since POS is not morphologically marked in Chinese, the same word. To reduce the processing ambiguity and produce more natural translation, extra content words are added to mark the nominalization of abstract concepts. For jian’ originally Adding transforms it to a noun phrase 191 3), similar to the the addition of place) to the adjective man’ (example 4). These unalignable words are not lexically dependent but are inferred from the context, and thus do not align to any source words. To summarize, a small number of word types cover 17% of word tokens that are unalignable, but whether these words are unalignable depends significantly on context. Although there is no list unalignable’ types or POS categories, our analysis shows there are regularities that may be exploited by an automatic classifier. 3 Improved Automatic Word Alignment We first propose a classifier for predicting whether word is unalignable. Let a pair of sentence with length J and K. For each word in belongs to a predefined of potentially unalignable words, we run a binary classifier. A separate classifier is built for each word type in the list, and an additional classifier for all the remaining words in each language. We train an SVM classifier based on the folfeatures: context: and POS in window sizes of 1, 3, 5, 7 around the in question. token-POS pairs: feature is defined by whether the token in quesand its POS tag is within the top token-POS pairs annotated as unalignable like in 2 and 3. Four features are defined with = 30, Since the top frequent unalignable words cover most of the counts as shown in the analysis, being in the top is a strong features. of likely unalignable per sentence: hypothesize that the translator will not add too many tokens to the translation and delete too many from the source sentence. In the ORACLE data, 68% sentences have more than 2 unalignable words. We approximate the number of likely unalignable words in the sentence by counting the number of words within the top 100 token-POS pairs annotated as length and ratio: sentences are more likely to contain unalignable words than shorter sentences. Also sentence ratios that deviate significantly from the mean are to contain unalignable words. of candidate: is a negative feature by whether there is an alignment candidefine the list as the top 100 word types with the highest count of unalignable words per language according to the hand annotated data. date in the target sentence for the source word in or vice versa. The candidates are exfrom the top words aligned to a particular word according to the manual alignof the ORACLE data. Five features are dewith = 5, 10, 20, 50,100 one ’without limit’, such that a more possible candidate will be detected by more features. Next, we propose a simple yet effective modification to the word alignment training pipeline: 1. Predict unalignable words by the classifier 2. Remove these words from the training corpus Train word alignment model (e.g. Combine the word alignments in both tions with heuristics (grow-diag-final-and) 5. Restore unaligned words to original position 6. Continue with rule extraction and the rest of the MT pipeline. The idea is to reduce the difficulty for the word alignment model by removing unaligned words. 4 End-to-End Translation Experiments In our experiments, we first show that removing manually-annotated unaligned words in ORACLE data leads to improvements in MT of both translation directions. Next, we show how a classifier trained on ORACLE data can be used to improve in another large-scale un-annotated 4.1 Experiments on ORACLE data We first performed an ORACLE experiment using gold standard unaligned word labels. Following the training pipeline in Section 3, we removed gold unalignable words before running GIZA++ and restore them afterwards. 90% of the data is used for alignment and MT training, while 10% of the data is reserved for testing. The upper half of Table 4 list the alignment precision, recall and F1 of the resulting alignand quality of the final MT outputs. Basethe standard MT training pipeline withremoval of unaligned words. Our approach performs better in alignment, phrasebased (PBMT) and hierarchical (Hiero) systems. The results, evaluated by BLEU, METEOR and TER, support our hypothesis that removing gold unalignable words helps improve word alignment and the resulting SMT. can suppress the NULL probabilities of the model. experiments are done using standard settings for Moses PBMT and Hiero with 4-gram LM and mslrbidirectional-fe reordering (Koehn et al., 2007). The clas-</abstract>
<note confidence="0.678442882352941">sifier is trained using LIBSVM (Chang and Lin, 2011). 192 Align acc. PBMT Hiero C-E E-C C-E E-C ORACLE P .711 B 11.4 17.4 10.3 15.8 Baseline R .488 T 70.9 69.0 75.9 72.3 F1.579 M 21.8 23.9 21.08 23.7 ORACLE B Proposed T (gold) M REAL B 18.2 18.5 17.0 17.2 Baseline T 63.4 67.2 68.0 71.4 M 22.9 24.6 REAL B 18.6 18.5 Proposed T (predict) M 24.5 Table 4: MT results of ORACLE and REAL ex-</note>
<abstract confidence="0.998601918918919">periments. Highest score per metric is bolded. statistically significant improve- &lt; (P: precision; R: recall; B: BLEU; M: METEOR; T:TER) For comparison, a naive classifier that labels all top-30 token-POS combinations as unalignable performs poorly as expected (PBMT BLEU: 9.87 in C-E direction). We also evaluated our proposed classifier on this task: the accuracy is 92% and it achieves BLEU of 11.55 for PBMT and 10.84 for Hiero in C-E direction, which is between the results of gold-unalign and baseline. 4.2 Experiments on large-scale REAL data We next performed a more realistic experiment: the classifier trained on ORACLE data is used to automatically label a large data, which is then used to train a MT system. This REAL data consists of text from the NIST MT experiments are performed in both directions. The lower half of Table 4 shows the performance of the resulting MT systems. We observe that our proposed approach is still able to improve over the baseline. In particular, Hiero achieved statistical significant improvements in BLEU and 6Comparing to the results of PBMT, this suggests our method may be most effective in systems where rule extraction is senuse the standard MT08 test sets; the training data includes LDC2004T08, 2005E47, 2005T06, 2007T23, 2008T06, 2008T08, 2008T18, 2009T02, 2009T06, 2009T15, and 2010T03 (34M English words and 1.1M sentences). Since we do not have access to all OpenMT data, e.g. FBIS, our results may not be directly comparable to other systems in the evaluation. PBMT did better than Hiero in this setup. Chinese word English lexical translation Baseline only Propose only xie (bring) xing (form) dan (but) pa (scare) him bringing and model it, the, they that, are, be yet, nevertheless fears, worried Table 5: Examples of translations exclusively in the top translation. Figure 1: Classifier accuracy and MT results V.S. proportion of ORACLE data sitive to the underlying alignments, such as Hiero and Syntax-based MT. Table 5 shows the lexical translations for some rare Chinese words: the baseline tends to incorrectly align these to function words (garbage collection), while the proposed method’s translations are more reasonable. To evaluate how much annotation is needed for the classifier, we repeat experiments using different proportions of the ORACLE data. Figure 1 shows training by 20% of the data (2600 sents.) leads to significant improvements &lt; which is a reasonable annotation effort. 5 Conclusion We analyzed in-depth the phenomenon of unalignable words in parallel text, and show that what is unalignable depends on the word’s concept and context. We argue that this is not a trivial problem, but with an unalignable word classifier and a simple modified MT training pipeline, we can achieve small but significant gains in end-to-end translation. In related work, the issue of dropped pronouns (Chung and Gildea, 2010) and function words (Setiawan et al., 2010; Nakazawa and Kurohashi, 2012) have been found important in word alignment, and (Fossum et al., 2008) showed that syntax features are helpful for fixing alignments. An interesting avenue of future work is to integrate these ideas with ours, in particular by exploiting syntax and viewing unalignable words as aligned at a structure above the lexical level.</abstract>
<note confidence="0.257233">193</note>
<title confidence="0.972656">References</title>
<author confidence="0.798801">Peter F Brown</author>
<author confidence="0.798801">Stephen A Della Pietra</author>
<author confidence="0.798801">Vincent J Della Pietra</author>
<author confidence="0.798801">Robert L Mercer</author>
<abstract confidence="0.858839888888889">The mathematics of statistical machine translation: estimation. 19(2). Chih-Chung Chang and Chih-Jen Lin. 2011. Lib- : a library for support vector machines. on Intelligent Systems and 2(27). Tagyoung Chung and Daniel Gildea. 2010. Effects empty categories on machine translation. Pro-</abstract>
<title confidence="0.706534">ceedings of the Conference on Empirical Methods Natural Language</title>
<author confidence="0.760951">Victoria Fossum</author>
<author confidence="0.760951">Kevin Knight</author>
<author confidence="0.760951">Steven Abney</author>
<note confidence="0.445088">2008. Using syntax to improve word alignment prefor syntax-based machine translation. Pro-</note>
<title confidence="0.52988">ceedings of the Workshop on Statistical Machine</title>
<author confidence="0.87431175">Philipp Koehn</author>
<author confidence="0.87431175">Hieu Hoang</author>
<author confidence="0.87431175">Alexandra Birch</author>
<author confidence="0.87431175">Chris Callison-Burch</author>
<author confidence="0.87431175">Marcello Federico</author>
<author confidence="0.87431175">Nicola Bertoldi</author>
<author confidence="0.87431175">Brooke Cowan</author>
<author confidence="0.87431175">Wade Shen</author>
<author confidence="0.87431175">Christine Moran</author>
<author confidence="0.87431175">Richard Zens</author>
<author confidence="0.87431175">Chris Dyer</author>
<author confidence="0.87431175">Ondrej Bojar</author>
<author confidence="0.87431175">Alexan-</author>
<note confidence="0.831308111111111">dra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translaof the Annual Meeting of the Asfor Computational Xuansong Li, Niyu Ge, Stephen Grimes, Stephanie M. Strassel, and Kazuaki Maeda. 2010. Enriching alignment with linguistic tags. of International Conference on Language Resources Toshiaki Nakazawa and Sado Kurohashi. 2012. Alignment by bilingual generation and monolingual of the International Conferon Computational A Nida. 1964. a Science of Translating: with Special Reference to Principles and Pro- Involved in Bible BRILL. Hendra Setiawan, Chris Dyer, and Philip Resnik. 2010. Discriminative word alignment with a function word model. of the Conference on</note>
<title confidence="0.796959">Empirical Methods on Natural Language Process-</title>
<author confidence="0.89629">Kristina Toutanova</author>
<author confidence="0.89629">Dan Klein</author>
<author confidence="0.89629">Christopher Manning</author>
<abstract confidence="0.6126115">and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network.</abstract>
<note confidence="0.920517">Proceedings of the Conference of the North Amer-</note>
<title confidence="0.756896">ican Chapter of the Association for Computational Human Language</title>
<intro confidence="0.335348">194</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1399" citStr="Brown et al., 1993" startWordPosition="193" endWordPosition="196">on It is generally acknowledged that absolute equivalence between two languages is impossible, since concept lexicalization varies across languages. Major translation theories thus argue that texts should be translated ‘sense-for-sense’ instead of ‘word-for-word’ (Nida, 1964). This suggests that unalignable words may be an issue for the parallel text used to train current statistical machine translation (SMT) systems. Although existing automatic word alignment methods have some mechanism to handle the lack of exact word-for-word alignment (e.g. null probabilities, fertility in the IBM models (Brown et al., 1993)), they may be too coarse-grained to model the ’sense-for-sense’ translations created by professional human translators. For example, the Chinese term ‘tai-yang’ literally means ‘sun’, yet the concept it represents is equivalent to the English term ‘the sun’. Since the concept of a definite article is not incorporated in the morphology of ‘tai yang’, the added ‘the’ is not aligned to any Chinese word. Yet in another context like ’the man’, ‘the’ can be the translation of the Chinese demonstrative pronoun ‘na’, literally means ‘that’. A potential misunderstanding is that unalignable words are s</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Libsvm : a library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<volume>2</volume>
<issue>27</issue>
<contexts>
<context position="12191" citStr="Chang and Lin, 2011" startWordPosition="1991" endWordPosition="1994">T outputs. Baseline is the standard MT training pipeline without removal of unaligned words. Our Proposed approach performs better in alignment, phrasebased (PBMT) and hierarchical (Hiero) systems. The results, evaluated by BLEU, METEOR and TER, support our hypothesis that removing gold unalignable words helps improve word alignment and the resulting SMT. 3We can suppress the NULL probabilities of the model. 4All experiments are done using standard settings for Moses PBMT and Hiero with 4-gram LM and mslrbidirectional-fe reordering (Koehn et al., 2007). The classifier is trained using LIBSVM (Chang and Lin, 2011). 192 Align PBMT Hiero acc. C-E E-C C-E E-C ORACLE P .711 B 11.4 17.4 10.3 15.8 Baseline R .488 T 70.9 69.0 75.9 72.3 F1.579 M 21.8 23.9 21.08 23.7 ORACLE P .802 B 11.8+ 18.3+ 11.0+17.2+ Proposed R .509 T 71.4− 65.7+ 74.7+68.7+ (gold) F1.623 M 22.1+ 24.1+ 22.0+24.0+ REAL B 18.2 18.5 17.0 17.2 Baseline T 63.4 67.2 68.0 71.4 M 22.9 24.6 22.9 24.8 REAL B 18.6 18.5 17.6+18.1+ Proposed T 63.8− 66.5+ 67.6 69.7+ (predict) M 23.2+ 24.5 23.4+24.7 Table 4: MT results of ORACLE and REAL experiments. Highest score per metric is bolded. {+/−} indicates statistically significant improvement/degradation, p &lt;</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm : a library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2(27).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tagyoung Chung</author>
<author>Daniel Gildea</author>
</authors>
<title>Effects of empty categories on machine translation.</title>
<date>2010</date>
<booktitle>Proceedings of the Conference on Empirical Methods on Natural Language Processing.</booktitle>
<marker>Chung, Gildea, 2010</marker>
<rawString>Tagyoung Chung and Daniel Gildea. 2010. Effects of empty categories on machine translation. Proceedings of the Conference on Empirical Methods on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victoria Fossum</author>
<author>Kevin Knight</author>
<author>Steven Abney</author>
</authors>
<title>Using syntax to improve word alignment precision for syntax-based machine translation.</title>
<date>2008</date>
<booktitle>Proceedings of the Workshop on Statistical Machine Translation.</booktitle>
<marker>Fossum, Knight, Abney, 2008</marker>
<rawString>Victoria Fossum, Kevin Knight, and Steven Abney. 2008. Using syntax to improve word alignment precision for syntax-based machine translation. Proceedings of the Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="12129" citStr="Koehn et al., 2007" startWordPosition="1980" endWordPosition="1983">nd F1 of the resulting alignments, and quality of the final MT outputs. Baseline is the standard MT training pipeline without removal of unaligned words. Our Proposed approach performs better in alignment, phrasebased (PBMT) and hierarchical (Hiero) systems. The results, evaluated by BLEU, METEOR and TER, support our hypothesis that removing gold unalignable words helps improve word alignment and the resulting SMT. 3We can suppress the NULL probabilities of the model. 4All experiments are done using standard settings for Moses PBMT and Hiero with 4-gram LM and mslrbidirectional-fe reordering (Koehn et al., 2007). The classifier is trained using LIBSVM (Chang and Lin, 2011). 192 Align PBMT Hiero acc. C-E E-C C-E E-C ORACLE P .711 B 11.4 17.4 10.3 15.8 Baseline R .488 T 70.9 69.0 75.9 72.3 F1.579 M 21.8 23.9 21.08 23.7 ORACLE P .802 B 11.8+ 18.3+ 11.0+17.2+ Proposed R .509 T 71.4− 65.7+ 74.7+68.7+ (gold) F1.623 M 22.1+ 24.1+ 22.0+24.0+ REAL B 18.2 18.5 17.0 17.2 Baseline T 63.4 67.2 68.0 71.4 M 22.9 24.6 22.9 24.8 REAL B 18.6 18.5 17.6+18.1+ Proposed T 63.8− 66.5+ 67.6 69.7+ (predict) M 23.2+ 24.5 23.4+24.7 Table 4: MT results of ORACLE and REAL experiments. Highest score per metric is bolded. {+/−} in</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuansong Li</author>
<author>Niyu Ge</author>
<author>Stephen Grimes</author>
<author>Stephanie M Strassel</author>
<author>Kazuaki Maeda</author>
</authors>
<title>Enriching word alignment with linguistic tags.</title>
<date>2010</date>
<booktitle>Proceedings of International Conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="2972" citStr="Li et al., 2010" startWordPosition="447" endWordPosition="450">hat makes a fluent human translation and use this insight to build better word aligners and MT systems. Our contributions are two-fold: 1) We analyze 13000 sentences of manually wordaligned Chinese-English parallel text, quantifying the characteristics of unalignable words. 2) We propose a simple and effective way to improve automatic word alignment, based on predicting unalignable words and temporarily removing them during the alignment training procedure. 2 Analysis of Unalignable Words Our manually-aligned data, which we call ORACLE data, is a Chinese-to-English corpus released by the LDC (Li et al., 2010)1. It consists of ∼13000 Chinese sentences from news and blog domains and their English translation . English words are manually aligned with the Chinese characters. Characters without an exact counterpart are annotated with categories that state the functions of the words. These characters are either aligned to ‘NULL’, or attached to their dependency heads, if any, and aligned together to form a multi-word alignment. For example, ‘the’ is annotated as [DET], for ‘determiner’, and aligned to ‘tai-yang’ together with ‘sun’. In this work, any English word or Chinese character without an exact co</context>
</contexts>
<marker>Li, Ge, Grimes, Strassel, Maeda, 2010</marker>
<rawString>Xuansong Li, Niyu Ge, Stephen Grimes, Stephanie M. Strassel, and Kazuaki Maeda. 2010. Enriching word alignment with linguistic tags. Proceedings of International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toshiaki Nakazawa</author>
<author>Sado Kurohashi</author>
</authors>
<title>Alignment by bilingual generation and monolingual derivation.</title>
<date>2012</date>
<booktitle>Proceedings of the International Conference on Computational Linguistics.</booktitle>
<marker>Nakazawa, Kurohashi, 2012</marker>
<rawString>Toshiaki Nakazawa and Sado Kurohashi. 2012. Alignment by bilingual generation and monolingual derivation. Proceedings of the International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene A Nida</author>
</authors>
<title>Toward a Science of Translating: with Special Reference to Principles and Procedures Involved in Bible Translating.</title>
<date>1964</date>
<publisher>BRILL.</publisher>
<contexts>
<context position="1056" citStr="Nida, 1964" startWordPosition="141" endWordPosition="142">n the parallel text used for machine translation (MT). We analyze this phenomenon in-depth for Chinese-English translation. We further propose a simple and effective method to improve automatic word alignment by pre-removing unalignable words, and show improvements on hierarchical MT systems in both translation directions. 1 Motivation It is generally acknowledged that absolute equivalence between two languages is impossible, since concept lexicalization varies across languages. Major translation theories thus argue that texts should be translated ‘sense-for-sense’ instead of ‘word-for-word’ (Nida, 1964). This suggests that unalignable words may be an issue for the parallel text used to train current statistical machine translation (SMT) systems. Although existing automatic word alignment methods have some mechanism to handle the lack of exact word-for-word alignment (e.g. null probabilities, fertility in the IBM models (Brown et al., 1993)), they may be too coarse-grained to model the ’sense-for-sense’ translations created by professional human translators. For example, the Chinese term ‘tai-yang’ literally means ‘sun’, yet the concept it represents is equivalent to the English term ‘the sun</context>
</contexts>
<marker>Nida, 1964</marker>
<rawString>Eugene A Nida. 1964. Toward a Science of Translating: with Special Reference to Principles and Procedures Involved in Bible Translating. BRILL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hendra Setiawan</author>
<author>Chris Dyer</author>
<author>Philip Resnik</author>
</authors>
<title>Discriminative word alignment with a function word reordering model.</title>
<date>2010</date>
<booktitle>Proceedings of the Conference on Empirical Methods on Natural Language Processing.</booktitle>
<marker>Setiawan, Dyer, Resnik, 2010</marker>
<rawString>Hendra Setiawan, Chris Dyer, and Philip Resnik. 2010. Discriminative word alignment with a function word reordering model. Proceedings of the Conference on Empirical Methods on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>Proceedings of the Conference of the North American Chapter of</booktitle>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>