<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000410">
<title confidence="0.998304">
Unsupervised Segmentation of Words Using Prior Distributions of Morph
Length and Frequency
</title>
<author confidence="0.992816">
Mathias Creutz
</author>
<affiliation confidence="0.995433">
Neural Networks Research Centre, Helsinki University of Technology
</affiliation>
<address confidence="0.91856">
P.O.Box 9800, FIN-02015 HUT, Finland
</address>
<email confidence="0.991126">
Mathias.Creutz@hut.fi
</email>
<sectionHeader confidence="0.998537" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999976333333333">
We present a language-independent and
unsupervised algorithm for the segmenta-
tion of words into morphs. The algorithm
is based on a new generative probabilis-
tic model, which makes use of relevant
prior information on the length and fre-
quency distributions of morphs in a lan-
guage. Our algorithm is shown to out-
perform two competing algorithms, when
evaluated on data from a language with
agglutinative morphology (Finnish), and
to perform well also on English data.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9996382">
In order to artificially “understand” or produce nat-
ural language, a system presumably has to know the
elementary building blocks, i.e., the lexicon, of the
language. Additionally, the system needs to model
the relations between these lexical units. Many ex-
isting NLP (natural language processing) applica-
tions make use of words as such units. For in-
stance, in statistical language modelling, probabil-
ities of word sequences are typically estimated, and
bag-of-word models are common in information re-
trieval.
However, for some languages it is infeasible to
construct lexicons for NLP applications, if the lexi-
cons contain entire words. In especially agglutina-
tive languages,1 such as Finnish and Turkish, the
</bodyText>
<footnote confidence="0.8554035">
1In agglutinative languages words are formed by the con-
catenation of morphemes.
</footnote>
<bodyText confidence="0.999959806122449">
number of possible different word forms is simply
too high. For example, in Finnish, a single verb
may appear in thousands of different forms (Karls-
son, 1987).
According to linguistic theory, words are built
from smaller units, morphemes. Morphemes are the
smallest meaning-bearing elements of language and
could be used as lexical units instead of entire words.
However, the construction of a comprehensive mor-
phological lexicon or analyzer based on linguistic
theory requires a considerable amount of work by
experts. This is both time-consuming and expen-
sive and hardly applicable to all languages. Further-
more, as language evolves the lexicon must be up-
dated continuously in order to remain up-to-date.
Alternatively, an interesting field of research lies
open: Minimally supervised algorithms can be de-
signed that automatically discover morphemes or
morpheme-like units from data. There exist a num-
ber of such algorithms, some of which are entirely
unsupervised and others that use some knowledge of
the language. In the following, we discuss recent un-
supervised algorithms and refer the reader to (Gold-
smith, 2001) for a comprehensive survey of previous
research in the whole field.
Many algorithms proceed by segmenting (i.e.,
splitting) words into smaller components. Often
the limiting assumption is made that words con-
sist of only one stem followed by one (possibly
empty) suffix (D´ejean, 1998; Snover and Brent,
2001; Snover et al., 2002). This limitation is reduced
in (Goldsmith, 2001) by allowing a recursive struc-
ture, where stems can have inner structure, so that
they in turn consist of a substem and a suffix. Also
prefixes are possible. However, for languages with
agglutinative morphology this may not be enough.
In Finnish, a word can consist of lengthy sequences
of alternating stems and affixes.
Some morphology discovery algorithms learn re-
lationships between words by comparing the ortho-
graphic or semantic similarity of the words (Schone
and Jurafsky, 2000; Neuvel and Fulop, 2002; Baroni
et al., 2002). Here a small number of components
per word are assumed, which makes the approaches
difficult to apply as such to agglutinative languages.
We previously presented two segmentation algo-
rithms suitable for agglutinative languages (Creutz
and Lagus, 2002). The algorithms learn a set of
segments, which we call morphs, from a corpus.
Stems and affixes are not distinguished as sepa-
rate categories by the algorithms, and in that sense
they resemble algorithms for text segmentation and
word discovery, such as (Deligne and Bimbot, 1997;
Brent, 1999; Kit and Wilks, 1999; Yu, 2000). How-
ever, we observed that for the corpus size studied
(100 000 words), our two algorithms were somewhat
prone to excessive segmentation of words.
In this paper, we aim at overcoming the problem
of excessive segmentation, particularly when small
corpora (up to 200 000 words) are used for training.
We present a new segmentation algorithm, which is
language independent and works in an unsupervised
fashion. Since the results obtained suggest that the
algorithm performs rather well, it could possibly be
suitable for languages for which only small amounts
of written text are available.
The model is formulated in a probabilistic
Bayesian framework. It makes use of explicit prior
information in the form of probability distributions
for morph length and morph frequency. The model
is based on the same kind of reasoning as the proba-
bilistic model in (Brent, 1999). While Brent’s model
displays a prior probability that exponentially de-
creases with word length (with one character as the
most common length), our model uses a probabil-
ity distribution that more accurately models the real
length distribution. Also Brent’s frequency distribu-
tion differs from ours, which we derive from Man-
delbrot’s correction of Zipf’s law (cf. Section 2.5).
Our model requires that the values of two param-
eters be set: (i) our prior belief of the most common
morph length, and (ii) our prior belief of the pro-
portion of morph types2 that occur only once in the
corpus. These morph types are called hapax legom-
ena. While the former is a rather intuitive measure,
the latter may not appear as intuitive. However, the
proportion of hapax legomena may be interpreted as
a measure of the richness of the text. Also note that
since the most common morph length is calculated
for morph types, not tokens, it is not independent of
the corpus size. A larger corpus usually requires a
higher average morph length, a fact that is stated for
word lengths in (Baayen, 2001).
As an evaluation criterion for the performance
of our method and two reference methods we use
a measure that reflects the ability to recognize
real morphemes of the language by examining the
morphs found by the algorithm.
</bodyText>
<sectionHeader confidence="0.952048" genericHeader="method">
2 Probabilistic generative model
</sectionHeader>
<bodyText confidence="0.999944529411764">
In this section we derive the new model. We fol-
low a step-by-step process, during which a morph
lexicon and a corpus are generated. The morphs in
the lexicon are strings that emerge as a result of a
stochastic process. The corpus is formed through
another stochastic process that picks morphs from
the lexicon and places them in a sequence. At two
points of the process, prior knowledge is required
in the form of two real numbers: the most common
morph length and the proportion of hapax legomena
morphs.
The model can be used for segmentation of words
by requiring that the corpus created is exactly the
input data. By selecting the most probable morph
lexicon that can produce the input data, we obtain a
segmentation of the words in the corpus, since we
can rewrite every word as a sequence of morphs.
</bodyText>
<subsectionHeader confidence="0.998776">
2.1 Size of the morph lexicon
</subsectionHeader>
<bodyText confidence="0.994589166666667">
We start the generation process by deciding the num-
ber of morphs in the morph lexicon (type count).
This number is denoted by nµ and its probability
p(nµ) follows the uniform distribution. This means
that, a priori, no lexicon size is more probable than
another.3
</bodyText>
<footnote confidence="0.9893704">
2We use standard terminology: Morph types are the set of
different, distinct morphs. By contrast, morph tokens are the
instances (or occurrences) of morphs in the corpus.
3This is an improper prior, but it is of little practical signif-
icance for two reasons: (i) This stage of the generation process
</footnote>
<subsectionHeader confidence="0.99852">
2.2 Morph lengths
</subsectionHeader>
<bodyText confidence="0.989711">
For each morph in the lexicon, we independently
choose its length in characters according to the
gamma distribution:
</bodyText>
<equation confidence="0.9998815">
1 α−1 lµi/0
p(lµi) = F(α)βα lµi e (1)
</equation>
<bodyText confidence="0.998138333333333">
where lµi is the length in characters of the ith morph,
and α and β are constants. F(α) is the gamma func-
tion:
</bodyText>
<equation confidence="0.914033">
∞
F(α) = o zα−1e−zdz.
0 (2)
</equation>
<bodyText confidence="0.999961">
The maximum value of the density occurs at lµi =
(α − 1)β, which corresponds to the most common
morph length in the lexicon. When β is set to one,
and α to one plus our prior belief of the most com-
mon morph length, the pdf (probability density func-
tion) is completely defined.
We have chosen the gamma distribution for
morph lengths, because it corresponds rather well to
the real length distribution observed for word types
in Finnish and English corpora that we have stud-
ied. The distribution also fits the length distribution
of the morpheme labels used as a reference (cf. Sec-
tion 3). A Poisson distribution can be justified and
has been used in order to model the length distri-
bution of word and morph tokens [e.g., (Creutz and
Lagus, 2002)], but for morph types we have chosen
the gamma distribution, which has a thicker tail.
</bodyText>
<subsectionHeader confidence="0.999702">
2.3 Morph strings
</subsectionHeader>
<bodyText confidence="0.9963815">
For each morph µi, we decide the character string it
consists of: We independently choose lµi characters
at random from the alphabet in use. The probabil-
ity of each character cj is the maximum likelihood
estimate of the occurrence of this character in the
corpus:4
</bodyText>
<equation confidence="0.8276515">
p(cj) = Ek nck
ncj
</equation>
<bodyText confidence="0.998640555555556">
where ncj is the number of occurrences of the char-
acter cj in the corpus, and Ek nck is the total num-
ber of characters in the corpus.
only contributes with one probability value, which will have a
negligible effect on the model as a whole. (ii) A proper prob-
ability density function would presumably be very flat, which
would hardly help guiding the search towards an optimal model.
4Alternatively, the maximum likelihood estimate of the oc-
currence of the character in the lexicon could be used.
</bodyText>
<subsectionHeader confidence="0.998957">
2.4 Morph order in the lexicon
</subsectionHeader>
<bodyText confidence="0.999989">
The lexicon consists of a set of nµ morphs and it
makes no difference in which order these morphs
have emerged. Regardless of their initial order, the
morphs can be sorted into a uniquely defined (e.g.,
alphabetical) order. Since there are nµ! ways to or-
der nµ different elements,5 we multiply the proba-
bility accumulated so far by nµ!:
</bodyText>
<equation confidence="0.883706">
ip(cj) · nµ! (4)
</equation>
<subsectionHeader confidence="0.979217">
2.5 Morph frequencies
</subsectionHeader>
<bodyText confidence="0.994518818181818">
The next step is to generate a corpus using the morph
lexicon obtained in the previous steps. First, we in-
dependently choose the number of times each morph
occurs in the corpus. We pursue the following line
of thought:
Zipf has studied the relationship between the fre-
quency of a word, f, and its rank, z.6 He suggests
that the frequency of a word is inversely proportional
to its rank. Mandelbrot has refined Zipf’s formula,
and suggests a more general relationship [see, e.g.,
(Baayen, 2001)]:
</bodyText>
<equation confidence="0.881346">
f = C(z + b)−a, (5)
</equation>
<bodyText confidence="0.998363">
where C, a and b are parameters of a text.
Let us derive a probability distribution from Man-
delbrot’s formula. The rank of a word as a func-
tion of its frequency can be obtained by solving for
z from (5):
</bodyText>
<equation confidence="0.967647">
a− b. (6)
</equation>
<bodyText confidence="0.99999">
Suppose that one wants to know the number of
words that have a frequency close to f rather than
the rank of the word with frequency f. In order to
obtain this information, we choose an arbitrary in-
terval around f: [(1/γ)f ... γf[, where γ &gt; 1, and
compute the rank at the endpoints of the interval.
The difference is an estimate of the number of words
</bodyText>
<footnote confidence="0.624147833333333">
5Strictly speaking, our probabilistic model is not perfect,
since we do not make sure that no morph can appear more than
once in the lexicon.
6The rank of a word is the position of the word in a list,
where the words have been sorted according to falling fre-
quency.
</footnote>
<equation confidence="0.9996222">
, (3)
p(lexicon) = p(nµ) �nµ [p(lµi) �lµi
i=1 j=1
af−1
z = C
</equation>
<bodyText confidence="0.9558">
that fall within the interval, i.e., have a frequency
close to f:
</bodyText>
<equation confidence="0.997639">
1 1 1 1
nf = z1/γ − zγ = (γ a − γ−a)Caf a . (7)
</equation>
<bodyText confidence="0.990436833333333">
This can be transformed into an exponential pdf
by (i) binning the frequency axis so that there are
no overlapping intervals. (This means that the fre-
quency axis is divided into non-overlapping inter-
ˆf[, which is equivalent to having
fˆ values that are powers of γ2: ˆf0 = γ0 = 1, ˆf1 =
γ2, ˆf2 = γ4,... All frequencies f are rounded to
the closest ˆf.) Next (ii), we normalize the number
of words with a frequency close to fˆ with the to-
tal number of words P fˆ n ˆf. Furthermore (iii), fˆ
is written as elog ˆf, and (iv) C must be chosen so
that the normalization coefficient equals 1/a, which
yields a proper pdf that integrates to one. Note also
the factor log γ2. Like ˆf, log fˆ is a discrete variable.
We approximate the integral of the density function
around each value log fˆ by multiplying with the dif-
ference between two successive log fˆ values, which
equals log γ2:
</bodyText>
<equation confidence="0.9694092">
p(corpus) = �(Pnµ
i=1 fµi)! )−1 = � N!
Qnµ Qnµ
i=1 fµi! i=1 fµi!
(9)
</equation>
<bodyText confidence="0.9998626">
The numerator of the multinomial is the factorial of
the total number of morph tokens, N, which equals
the sum of frequencies of every morph type. The de-
nominator is the product of the factorial of the fre-
quency of each morph type.
</bodyText>
<subsectionHeader confidence="0.879917">
2.7 Search for the optimal model
</subsectionHeader>
<bodyText confidence="0.999990545454545">
The search for the optimal model given our input
data corresponds closely to the recursive segmen-
tation algorithm presented in (Creutz and Lagus,
2002). The search takes place in batch mode, but
could as well be done incrementally. All words in
the data are randomly shuffled, and for each word,
every split into two parts is tested. The most proba-
ble split location (or no split) is selected and in case
of a split, the two parts are recursively split in two.
All words are iteratively reprocessed until the prob-
ability of the model converges.
</bodyText>
<equation confidence="0.9966782">
vals [(1/γ)
fˆ ... γ
.
�−1
log fˆ · log γ2. (8)
</equation>
<bodyText confidence="0.999790272727273">
Now, if we assume that Zipf’s and Madelbrot’s
formulae apply to morphs as well as to words, we
can use formula (8) for every morph frequency fµi,
which is the number of occurrences (or frequency)
of the morph µi in the corpus (token count). How-
ever, values for a and γ2 must be chosen. We set
γ2 to 1.59, which is the lowest value for which no
empty frequency bins will appear.7 For fµi = 1, (8)
reduces to log γ2/a. We set this value equal to our
prior belief of the proportion of morph types that are
to occur only once in the corpus (hapax legomena).
</bodyText>
<subsectionHeader confidence="0.955791">
2.6 Corpus
</subsectionHeader>
<bodyText confidence="0.99991625">
The morphs and their frequencies have been set. The
order of the morphs in the corpus remains to be de-
cided. The probability of one particular order is the
inverse of the multinomial:
</bodyText>
<footnote confidence="0.911838">
7Empty bins can appear for small values of fµi due to fµi’s
being rounded to the closest ˆfµi, which is a power of ry2.
</footnote>
<sectionHeader confidence="0.99898" genericHeader="method">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999888090909091">
From the point of view of linguistic theory, it is pos-
sible to come up with different plausible sugges-
tions for the correct location of morpheme bound-
aries. Some of the solutions may be more elegant
than others,8 but it is difficult to say if the most el-
egant scheme will work best in practice, when real
NLP applications are concerned.
We utilize an evaluation method for segmentation
of words presented in (Creutz and Lagus, 2002). In
this method, segments are not compared to one sin-
gle “correct” segmentation. The evaluation criterion
can rather be interpreted from the point of view of
language “understanding”. A morph discovered by
the segmentation algorithm is considered to be “un-
derstood”, if there is a low-ambiguity mapping from
the morph to a corresponding morpheme. Alterna-
tively, a morph may correspond to a sequence of
morphemes, if these morphemes are very likely to
occur together. The idea is that if an entirely new
word form is encountered, the system will “under-
stand” it by decomposing it into morphs that it “un-
derstands”. A segmentation algorithm that segments
</bodyText>
<equation confidence="0.592310142857143">
8Cf. “hop +ed” vs. “hope + d” (past tense of “to hope”).
p(f E [(1/γ)
fˆ ... γ
ˆf[) = γ
P
fˆnfˆ
C1a e− a1 log fˆ
</equation>
<figure confidence="0.996072777777778">
a 1 − γ
1
a
−
1
a
e−
1
a
</figure>
<bodyText confidence="0.999326588235294">
words into too small parts will perform poorly due to
high ambiguity. At the other extreme, an algorithm
that is reluctant at splitting words will have bad gen-
eralization ability to new word forms.
Reference morpheme sequences for the words are
obtained using existing software for automatic mor-
phological analysis based on the two-level morphol-
ogy of Koskenniemi (1983). For each word form,
the analyzer outputs the base form of the word to-
gether with grammatical tags. By filtering the out-
put, we get a sequence of morpheme labels that ap-
pear in the correct order and represent correct mor-
phemes rather closely. Note, however, that the mor-
pheme labels are not necessarily orthographically
similar to the morphemes they represent.
The exact procedure for evaluating the segmenta-
tion of a set of words consists of the following steps:
</bodyText>
<listItem confidence="0.9944295">
(1) Segment the words in the corpus using the au-
tomatic segmentation algorithm.
(2) Divide the segmented data into two parts of
equal size. Collect all segmented word forms from
the first part into a training vocabulary and collect
all segmented word forms from the second part into
a test vocabulary.
(3) Align the segmentation of the words in the
</listItem>
<bodyText confidence="0.754954045454545">
training vocabulary with the corresponding refer-
ence morpheme label sequences. Each morph must
be aligned with one or more consecutive morpheme
labels and each morpheme label must be aligned
with at least one morph; e.g., for a hypothetical seg-
mentation of the English word winners’:
Morpheme labels win -ER PL GEN
Morph sequence w inn er s’
(4) Estimate conditional probabilities for the
morph/morpheme mappings computed over the
whole training vocabulary: p(morpheme  |morph).
Re-align using the Viterbi algorithm and employ the
Expectation-Maximization algorithm iteratively un-
til convergence of the probabilities.
(5) The quality of the segmentation is evaluated
on the test vocabulary. The segmented words in the
test vocabulary are aligned against their reference
morpheme label sequences according to the condi-
tional probabilities learned from the training vocab-
ulary. To measure the quality of the segmentation
we compute the expectation of the proportion of
correct mappings from morphs to morpheme labels,
</bodyText>
<equation confidence="0.991349">
E{p(morpheme  |morph)}:
pi(morpheme  |morph), (10)
</equation>
<bodyText confidence="0.998897">
where N is the number of morph/morpheme map-
pings, and pi(·) is the probability associated with
the ith mapping. Thus, we measure the proportion
of morphemes in the test vocabulary that we can ex-
pect to recognize correctly by examining the morph
segments.9
</bodyText>
<sectionHeader confidence="0.999698" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999852">
We have conducted experiments involving (i) three
different segmentation algorithms, (ii) two corpora
in different languages (Finnish and English), and
(iii) data sizes ranging from 2000 words to 200 000
words.
</bodyText>
<subsectionHeader confidence="0.998676">
4.1 Segmentation algorithms
</subsectionHeader>
<bodyText confidence="0.999880222222222">
The new probabilistic method is compared to two
existing segmentation methods: the Recursive MDL
method presented in (Creutz and Lagus, 2002)10
and John Goldsmith’s algorithm called Linguistica
(Goldsmith, 2001).11 Both methods use MDL (Min-
imum Description Length) (Rissanen, 1989) as a cri-
terion for model optimization.
The effect of using prior information on the dis-
tribution of morph length and frequency can be as-
sessed by comparing the probabilistic method to Re-
cursive MDL, since both methods utilize the same
search algorithm, but Recursive MDL does not make
use of explicit prior information.
Furthermore, the possible benefit of using the
two sources of prior information can be compared
against the possible benefit of grouping stems and
suffixes into signatures. The latter technique is em-
ployed by Linguistica.
</bodyText>
<subsectionHeader confidence="0.946771">
4.2 Data
</subsectionHeader>
<bodyText confidence="0.999635333333333">
The Finnish data consists of subsets of a news-
paper text corpus from CSC,12 from which non-
words (numbers and punctuation marks) have been
</bodyText>
<footnote confidence="0.985066142857143">
9In (Creutz and Lagus, 2002) the results are reported less
intuitively as the “alignment distance”, i.e., the negative logprob
of the entire test set: − log H pi(morpheme  |morph).
10Online demo at http://www.cis.hut.fi/projects/morpho/.
11The software can be downloaded from http://humanities.
uchicago.edu/faculty/goldsmith/Linguistica2000/.
12http://www.csc.fi/kielipankki/
</footnote>
<equation confidence="0.936685">
1 N
N i=1
</equation>
<bodyText confidence="0.999892">
removed. The reference morpheme labels have been
filtered out from a morphosyntactic analysis of the
text produced by the Connexor FDG parser.13
The English corpus consists of mainly newspaper
text (with non-words removed) from the Brown cor-
pus.14 A morphological analysis of the words has
been performed using the Lingsoft ENGTWOL an-
alyzer.15
For both languages data sizes of 2000, 5000,
10 000, 50 000, 100 000, and 200 000 have been
used. A notable difference between the morpholog-
ical structure of the languages lies in the fact that
whereas there are about 17 000 English word types
in the largest data set, the corresponding number of
Finnish word types is 58 000.
</bodyText>
<subsectionHeader confidence="0.997915">
4.3 Parameters
</subsectionHeader>
<bodyText confidence="0.999907296296296">
In order to select good prior values for the prob-
abilistic method, we have used separate develop-
ment test sets that are independent of the final data
sets. Morph length and morph frequency distribu-
tions have been computed for the reference mor-
pheme representations of the development test sets.
The prior values for most common morph length and
proportion of hapax legomena have been adjusted in
order to produce distributions that fit the reference
as well as possible.
We thus assume that we can make a good guess of
the final morph length and frequency distributions.
Note, however, that our reference is an approxima-
tion of a morpheme representation. As the segmen-
tation algorithms produce morphs, not morphemes,
we can expect to obtain a larger number of morphs
due to allomorphy. Note also that we do not op-
timize for segmentation performance on the devel-
opment test set; we only choose the best fit for the
morph length and frequency distributions.
As for the two other segmentation algorithms, Re-
cursive MDL has no parameters to adjust. In Lin-
guistica we have used Method A Suffixes + Find pre-
fixes from stems with other parameters left at their
default values. We are unaware whether another
configuration could be more advantageous for Lin-
guistica.
</bodyText>
<footnote confidence="0.962682">
13http://www.connexor.fi/
14The Brown corpus is available at the Linguistic Data Con-
sortium at http://www.ldc.upenn.edu/.
15http://www.lingsoft.fi/
</footnote>
<figure confidence="0.7377645">
Finnish
Corpus size [1000 words] (log. scaled axis)
</figure>
<figureCaption confidence="0.942356">
Figure 1: Expectation of the percentage of recog-
nized morphemes for Finnish data.
</figureCaption>
<subsectionHeader confidence="0.86774">
4.4 Results
</subsectionHeader>
<bodyText confidence="0.999814740740741">
The expected proportion of morphemes recognized
by the three segmentation methods are plotted in
Figures 1 and 2 for different sizes of the Finnish
and English corpora. The search algorithm used
in the probabilistic method and Recursive MDL in-
volve randomness and therefore every value shown
for these two methods is the average obtained over
ten runs with different random seeds. However, the
fluctuations due to random behaviour are very small
and paired t-tests show significant differences at the
significance level of 0.01 for all pair-wise compar-
isons of the methods at all corpus sizes.
For Finnish, all methods show a curve that mainly
increases as a function of the corpus size. The prob-
abilistic method is the best with morpheme recogni-
tion percentages between 23.5% and 44.2%. Lin-
guistica performs worst with percentages between
16.5% and 29.1%. None of the methods are close
to ideal performance, which, however, is lower than
100%. This is due to the fact that the test vocabu-
lary contains a number of morphemes that are not
present in the training vocabulary, and thus are im-
possible to recognize. The proportion of unrecog-
nizable morphemes is highest for the smallest corpus
size (32.5%) and decreases to 8.8% for the largest
corpus size.
The evaluation measure used unfortunately scores
</bodyText>
<figure confidence="0.986385333333333">
2 5 10 50 100 200
Probabilistic
Recursive MDL
Linguistica
No segmentation
60
50
40
30
20
10
0
Expectation(recognized morphemes) [%]
English
Corpus size [1000 words] (log. scaled axis)
</figure>
<figureCaption confidence="0.925205">
Figure 2: Expectation of the percentage of recog-
nized morphemes for English data.
</figureCaption>
<bodyText confidence="0.979397517241379">
a baseline of no segmentation fairly high. The no-
segmentation baseline corresponds to a system that
recognizes the training vocabulary fully, but has no
ability to generalize to any other word form.
The results for English are different. Linguistica
is the best method for corpus sizes below 50 000
words, but its performance degrades from the max-
imum of 39.6% at 10 000 words to 29.8% for the
largest data set. The probabilistic method is con-
stantly better than Recursive MDL and both methods
outperform Linguistica beyond 50 000 words. The
recognition percentages of the probabilistic method
vary between 28.2% and 43.6%. However, for cor-
pus sizes above 10 000 words none of the three
methods outperform the no-segmentation baseline.
Overall, the results for English are closer to ideal
performance than was the case for Finnish. This
is partly due to the fact that the proportion of un-
seen morphemes that are impossible to recognize is
higher for English (44.5% at 2000 words, 19.0% at
200 000 words).
As far as the time consumption of the algorithms
is concerned, the largest Finnish corpus took 20 min-
utes to process for the probabilistic method and Re-
cursive MDL, and 40 minutes for Linguistica. The
largest English corpus was processed in less than
three minutes by all the algorithms. The tests were
run on a 900 MHz AMD Duron processor with
256 MB RAM.
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999978413043479">
For small data sizes, Recursive MDL has a tendency
to split words into too small segments, whereas Lin-
guistica is much more reluctant at splitting words,
due to its use of signatures. The extent to which the
probabilistic method splits words lies somewhere in
between the two other methods.
Our evaluation measure favours low ambiguity as
long as the ability to generalize to new word forms
does not suffer. This works against all segmentation
methods for English at larger data sizes. The En-
glish language has rather simple morphology, which
means that the number of different possible word
forms is limited. The larger the training vocabu-
lary, the broader coverage of the test vocabulary, and
therefore the no-segmentation approach works sur-
prisingly well. Segmentation always increases am-
biguity, which especially Linguistica suffers from as
it discovers more and more signatures and short suf-
fixes as the amount of data increases. For instance,
a final ’s’ stripped off its stem can be either a noun
or a verb ending, and a final ’e’ is very ambiguous,
as it belongs to orthography rather than morphology
and does not correspond to any morpheme.
Finnish morphology is more complex and there
are endless possibilities to construct new word
forms. As can be seen from Figure 1, the proba-
bilistic method and Recursive MDL perform better
than the no-segmentation baseline for all data sizes.
The segmentations could be evaluated using other
measures, but for language modelling purposes,
we believe that the evaluation measure should not
favour shattering of very common strings, even
though they correspond to more than one morpheme.
These strings should rather work as individual vo-
cabulary items in the model. It has been shown that
increased performance of n-gram models can be ob-
tained by adding larger units consisting of common
word sequences to the vocabulary; see e.g., (Deligne
and Bimbot, 1995). Nevertheless, in the near fu-
ture we wish to explore possibilities of using com-
plementary and more standard evaluation measures,
such as precision, recall, and F-measure of the dis-
covered morph boundaries.
Concerning the length and frequency prior dis-
tributions in the probabilistic model, one notes that
they are very general and do not make far-reaching
</bodyText>
<figure confidence="0.982331307692308">
2 5 10 50 100 200
Probabilistic
Recursive MDL
Linguistica
No segmentation
60
50
40
30
20
10
0
Expectation(recognized morphemes) [%]
</figure>
<bodyText confidence="0.999748041666667">
assumptions about the behaviour of natural lan-
guage. In fact, Zipf’s law has been shown to ap-
ply to randomly generated artificial texts (Li, 1992).
In our implementation, due to the independence as-
sumptions made in the model and due to the search
algorithm used, the choice of a prior value for the
most common morph length is more important than
the hapax legomena value. If a very bad prior value
for the most common morph length is used perfor-
mance drops by twelve percentage units, whereas
extreme hapax legomena values only reduces per-
formance by two percentage units. But note that the
two values are dependent: A greater average morph
length means a greater number of hapax legomena
and vice versa.
There is always room for improvement. Our cur-
rent model does not represent contextual dependen-
cies, such as phonological rules or morphotactic lim-
itations on morph order. Nor does it identify which
morphs are allomorphs of the same morpheme, e.g.,
“city” and “citi + es”. In the future, we expect to ad-
dress these problems by using statistical language
modelling techniques. We will also study how the
algorithms scale to considerably larger corpora.
</bodyText>
<sectionHeader confidence="0.999823" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999976375">
The results we have obtained suggest that the per-
formance of a segmentation algorithm can indeed be
increased by using prior information of general na-
ture, when this information is expressed mathemati-
cally as part of a probabilistic model. Furthermore,
we have reasons to believe that the morph segments
obtained can be useful as components of a statistical
language model.
</bodyText>
<sectionHeader confidence="0.998031" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.994782666666667">
I am most grateful to Krista Lagus, Krister Lind´en,
and Anders Ahlb¨ack, as well as the anonymous re-
viewers for their valuable comments.
</bodyText>
<sectionHeader confidence="0.999536" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999952377358491">
R. H. Baayen. 2001. Word Frequency Distributions.
Kluwer Academic Publishers.
M. Baroni, J. Matiasek, and H. Trost. 2002. Unsuper-
vised learning of morphologically related words based
on orthographic and semantic similarity. In Proc. ACL
Workshop Morphol. &amp; Phonol. Learning, pp. 48–57.
M. R. Brent. 1999. An efficient, probabilistically sound
algorithm for segmentation and word discovery. Ma-
chine Learning, 34:71–105.
M. Creutz and K. Lagus. 2002. Unsupervised discovery
of morphemes. In Proc. ACL Workshop on Morphol.
and Phonological Learning, pp. 21–30, Philadelphia.
H. D´ejean. 1998. Morphemes as necessary concept
for structures discovery from untagged corpora. In
Workshop on Paradigms and Grounding in Nat. Lang.
Learning, pp. 295–299, Adelaide.
S. Deligne and F. Bimbot. 1995. Language modeling
by variable length sequences: Theoretical formulation
and evaluation of multigrams. In Proc. ICASSP.
S. Deligne and F. Bimbot. 1997. Inference of variable-
length linguistic and acoustic units by multigrams.
Speech Communication, 23:223–241.
J. Goldsmith. 2001. Unsupervised learning of the mor-
phology of a natural language. Computational Lin-
guistics, 27(2):153–198.
F. Karlsson. 1987. Finnish Grammar. WSOY, 2nd ed.
C. Kit and Y. Wilks. 1999. Unsupervised learning of
word boundary with description length gain. In Proc.
CoNLL99 ACL Workshop, Bergen.
K. Koskenniemi. 1983. Two-level morphology: A gen-
eral computational model for word-form recognition
and production. Ph.D. thesis, University of Helsinki.
W. Li. 1992. Random texts exhibit Zipf’s-Law-like word
frequency distribution. IEEE Transactions on Infor-
mation Theory, 38(6):1842–1845.
S. Neuvel and S. A. Fulop. 2002. Unsupervised learn-
ing of morphology without morphemes. In Proc. ACL
Workshop on Morphol. &amp; Phonol. Learn., pp. 31–40.
J. Rissanen. 1989. Stochastic Complexity in Statistical
Inquiry, vol. 15. World Scientific Series in Computer
Science, Singapore.
P. Schone and D. Jurafsky. 2000. Knowledge-free induc-
tion of morphology using Latent Semantic Analysis.
In Proc. CoNLL-2000 &amp; LLL-2000, pp. 67–72.
M. G. Snover and M. R. Brent. 2001. A Bayesian model
for morpheme and paradigm identification. In Proc.
39th Annual Meeting of the ACL, pp. 482–490.
M. G. Snover, G. E. Jarosz, and M. R. Brent. 2002. Un-
supervised learning of morphology using a novel di-
rected search algorithm: Taking the first step. In Proc.
ACL Worksh. Morphol. &amp; Phonol. Learn., pp. 11–20.
H. Yu. 2000. Unsupervised word induction using MDL
criterion. In Proc. ISCSL, Beijing.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.874129">
<title confidence="0.996444">Unsupervised Segmentation of Words Using Prior Distributions of Morph Length and Frequency</title>
<author confidence="0.999643">Mathias Creutz</author>
<affiliation confidence="0.998507">Neural Networks Research Centre, Helsinki University of Technology</affiliation>
<address confidence="0.999725">P.O.Box 9800, FIN-02015 HUT, Finland</address>
<email confidence="0.941119">Mathias.Creutz@hut.fi</email>
<abstract confidence="0.995090230769231">We present a language-independent and unsupervised algorithm for the segmentation of words into morphs. The algorithm is based on a new generative probabilistic model, which makes use of relevant prior information on the length and frequency distributions of morphs in a language. Our algorithm is shown to outperform two competing algorithms, when evaluated on data from a language with agglutinative morphology (Finnish), and to perform well also on English data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R H Baayen</author>
</authors>
<title>Word Frequency Distributions.</title>
<date>2001</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="6064" citStr="Baayen, 2001" startWordPosition="958" endWordPosition="959">most common morph length, and (ii) our prior belief of the proportion of morph types2 that occur only once in the corpus. These morph types are called hapax legomena. While the former is a rather intuitive measure, the latter may not appear as intuitive. However, the proportion of hapax legomena may be interpreted as a measure of the richness of the text. Also note that since the most common morph length is calculated for morph types, not tokens, it is not independent of the corpus size. A larger corpus usually requires a higher average morph length, a fact that is stated for word lengths in (Baayen, 2001). As an evaluation criterion for the performance of our method and two reference methods we use a measure that reflects the ability to recognize real morphemes of the language by examining the morphs found by the algorithm. 2 Probabilistic generative model In this section we derive the new model. We follow a step-by-step process, during which a morph lexicon and a corpus are generated. The morphs in the lexicon are strings that emerge as a result of a stochastic process. The corpus is formed through another stochastic process that picks morphs from the lexicon and places them in a sequence. At</context>
<context position="10564" citStr="Baayen, 2001" startWordPosition="1749" endWordPosition="1750">r nµ different elements,5 we multiply the probability accumulated so far by nµ!: ip(cj) · nµ! (4) 2.5 Morph frequencies The next step is to generate a corpus using the morph lexicon obtained in the previous steps. First, we independently choose the number of times each morph occurs in the corpus. We pursue the following line of thought: Zipf has studied the relationship between the frequency of a word, f, and its rank, z.6 He suggests that the frequency of a word is inversely proportional to its rank. Mandelbrot has refined Zipf’s formula, and suggests a more general relationship [see, e.g., (Baayen, 2001)]: f = C(z + b)−a, (5) where C, a and b are parameters of a text. Let us derive a probability distribution from Mandelbrot’s formula. The rank of a word as a function of its frequency can be obtained by solving for z from (5): a− b. (6) Suppose that one wants to know the number of words that have a frequency close to f rather than the rank of the word with frequency f. In order to obtain this information, we choose an arbitrary interval around f: [(1/γ)f ... γf[, where γ &gt; 1, and compute the rank at the endpoints of the interval. The difference is an estimate of the number of words 5Strictly s</context>
</contexts>
<marker>Baayen, 2001</marker>
<rawString>R. H. Baayen. 2001. Word Frequency Distributions. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>J Matiasek</author>
<author>H Trost</author>
</authors>
<title>Unsupervised learning of morphologically related words based on orthographic and semantic similarity.</title>
<date>2002</date>
<booktitle>In Proc. ACL Workshop Morphol. &amp; Phonol. Learning,</booktitle>
<pages>48--57</pages>
<contexts>
<context position="3541" citStr="Baroni et al., 2002" startWordPosition="541" endWordPosition="544">ver and Brent, 2001; Snover et al., 2002). This limitation is reduced in (Goldsmith, 2001) by allowing a recursive structure, where stems can have inner structure, so that they in turn consist of a substem and a suffix. Also prefixes are possible. However, for languages with agglutinative morphology this may not be enough. In Finnish, a word can consist of lengthy sequences of alternating stems and affixes. Some morphology discovery algorithms learn relationships between words by comparing the orthographic or semantic similarity of the words (Schone and Jurafsky, 2000; Neuvel and Fulop, 2002; Baroni et al., 2002). Here a small number of components per word are assumed, which makes the approaches difficult to apply as such to agglutinative languages. We previously presented two segmentation algorithms suitable for agglutinative languages (Creutz and Lagus, 2002). The algorithms learn a set of segments, which we call morphs, from a corpus. Stems and affixes are not distinguished as separate categories by the algorithms, and in that sense they resemble algorithms for text segmentation and word discovery, such as (Deligne and Bimbot, 1997; Brent, 1999; Kit and Wilks, 1999; Yu, 2000). However, we observed </context>
</contexts>
<marker>Baroni, Matiasek, Trost, 2002</marker>
<rawString>M. Baroni, J. Matiasek, and H. Trost. 2002. Unsupervised learning of morphologically related words based on orthographic and semantic similarity. In Proc. ACL Workshop Morphol. &amp; Phonol. Learning, pp. 48–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Brent</author>
</authors>
<title>An efficient, probabilistically sound algorithm for segmentation and word discovery.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--71</pages>
<contexts>
<context position="4086" citStr="Brent, 1999" startWordPosition="628" endWordPosition="629">e and Jurafsky, 2000; Neuvel and Fulop, 2002; Baroni et al., 2002). Here a small number of components per word are assumed, which makes the approaches difficult to apply as such to agglutinative languages. We previously presented two segmentation algorithms suitable for agglutinative languages (Creutz and Lagus, 2002). The algorithms learn a set of segments, which we call morphs, from a corpus. Stems and affixes are not distinguished as separate categories by the algorithms, and in that sense they resemble algorithms for text segmentation and word discovery, such as (Deligne and Bimbot, 1997; Brent, 1999; Kit and Wilks, 1999; Yu, 2000). However, we observed that for the corpus size studied (100 000 words), our two algorithms were somewhat prone to excessive segmentation of words. In this paper, we aim at overcoming the problem of excessive segmentation, particularly when small corpora (up to 200 000 words) are used for training. We present a new segmentation algorithm, which is language independent and works in an unsupervised fashion. Since the results obtained suggest that the algorithm performs rather well, it could possibly be suitable for languages for which only small amounts of written</context>
</contexts>
<marker>Brent, 1999</marker>
<rawString>M. R. Brent. 1999. An efficient, probabilistically sound algorithm for segmentation and word discovery. Machine Learning, 34:71–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Creutz</author>
<author>K Lagus</author>
</authors>
<title>Unsupervised discovery of morphemes.</title>
<date>2002</date>
<booktitle>In Proc. ACL Workshop on Morphol. and Phonological Learning,</booktitle>
<pages>21--30</pages>
<location>Philadelphia.</location>
<contexts>
<context position="3794" citStr="Creutz and Lagus, 2002" startWordPosition="578" endWordPosition="581">wever, for languages with agglutinative morphology this may not be enough. In Finnish, a word can consist of lengthy sequences of alternating stems and affixes. Some morphology discovery algorithms learn relationships between words by comparing the orthographic or semantic similarity of the words (Schone and Jurafsky, 2000; Neuvel and Fulop, 2002; Baroni et al., 2002). Here a small number of components per word are assumed, which makes the approaches difficult to apply as such to agglutinative languages. We previously presented two segmentation algorithms suitable for agglutinative languages (Creutz and Lagus, 2002). The algorithms learn a set of segments, which we call morphs, from a corpus. Stems and affixes are not distinguished as separate categories by the algorithms, and in that sense they resemble algorithms for text segmentation and word discovery, such as (Deligne and Bimbot, 1997; Brent, 1999; Kit and Wilks, 1999; Yu, 2000). However, we observed that for the corpus size studied (100 000 words), our two algorithms were somewhat prone to excessive segmentation of words. In this paper, we aim at overcoming the problem of excessive segmentation, particularly when small corpora (up to 200 000 words)</context>
<context position="8778" citStr="Creutz and Lagus, 2002" startWordPosition="1435" endWordPosition="1438">n. When β is set to one, and α to one plus our prior belief of the most common morph length, the pdf (probability density function) is completely defined. We have chosen the gamma distribution for morph lengths, because it corresponds rather well to the real length distribution observed for word types in Finnish and English corpora that we have studied. The distribution also fits the length distribution of the morpheme labels used as a reference (cf. Section 3). A Poisson distribution can be justified and has been used in order to model the length distribution of word and morph tokens [e.g., (Creutz and Lagus, 2002)], but for morph types we have chosen the gamma distribution, which has a thicker tail. 2.3 Morph strings For each morph µi, we decide the character string it consists of: We independently choose lµi characters at random from the alphabet in use. The probability of each character cj is the maximum likelihood estimate of the occurrence of this character in the corpus:4 p(cj) = Ek nck ncj where ncj is the number of occurrences of the character cj in the corpus, and Ek nck is the total number of characters in the corpus. only contributes with one probability value, which will have a negligible ef</context>
<context position="12959" citStr="Creutz and Lagus, 2002" startWordPosition="2211" endWordPosition="2214">e density function around each value log fˆ by multiplying with the difference between two successive log fˆ values, which equals log γ2: p(corpus) = �(Pnµ i=1 fµi)! )−1 = � N! Qnµ Qnµ i=1 fµi! i=1 fµi! (9) The numerator of the multinomial is the factorial of the total number of morph tokens, N, which equals the sum of frequencies of every morph type. The denominator is the product of the factorial of the frequency of each morph type. 2.7 Search for the optimal model The search for the optimal model given our input data corresponds closely to the recursive segmentation algorithm presented in (Creutz and Lagus, 2002). The search takes place in batch mode, but could as well be done incrementally. All words in the data are randomly shuffled, and for each word, every split into two parts is tested. The most probable split location (or no split) is selected and in case of a split, the two parts are recursively split in two. All words are iteratively reprocessed until the probability of the model converges. vals [(1/γ) fˆ ... γ . �−1 log fˆ · log γ2. (8) Now, if we assume that Zipf’s and Madelbrot’s formulae apply to morphs as well as to words, we can use formula (8) for every morph frequency fµi, which is the</context>
<context position="14715" citStr="Creutz and Lagus, 2002" startWordPosition="2539" endWordPosition="2542">f one particular order is the inverse of the multinomial: 7Empty bins can appear for small values of fµi due to fµi’s being rounded to the closest ˆfµi, which is a power of ry2. 3 Evaluation From the point of view of linguistic theory, it is possible to come up with different plausible suggestions for the correct location of morpheme boundaries. Some of the solutions may be more elegant than others,8 but it is difficult to say if the most elegant scheme will work best in practice, when real NLP applications are concerned. We utilize an evaluation method for segmentation of words presented in (Creutz and Lagus, 2002). In this method, segments are not compared to one single “correct” segmentation. The evaluation criterion can rather be interpreted from the point of view of language “understanding”. A morph discovered by the segmentation algorithm is considered to be “understood”, if there is a low-ambiguity mapping from the morph to a corresponding morpheme. Alternatively, a morph may correspond to a sequence of morphemes, if these morphemes are very likely to occur together. The idea is that if an entirely new word form is encountered, the system will “understand” it by decomposing it into morphs that it </context>
<context position="18405" citStr="Creutz and Lagus, 2002" startWordPosition="3139" endWordPosition="3142">rph/morpheme mappings, and pi(·) is the probability associated with the ith mapping. Thus, we measure the proportion of morphemes in the test vocabulary that we can expect to recognize correctly by examining the morph segments.9 4 Experiments We have conducted experiments involving (i) three different segmentation algorithms, (ii) two corpora in different languages (Finnish and English), and (iii) data sizes ranging from 2000 words to 200 000 words. 4.1 Segmentation algorithms The new probabilistic method is compared to two existing segmentation methods: the Recursive MDL method presented in (Creutz and Lagus, 2002)10 and John Goldsmith’s algorithm called Linguistica (Goldsmith, 2001).11 Both methods use MDL (Minimum Description Length) (Rissanen, 1989) as a criterion for model optimization. The effect of using prior information on the distribution of morph length and frequency can be assessed by comparing the probabilistic method to Recursive MDL, since both methods utilize the same search algorithm, but Recursive MDL does not make use of explicit prior information. Furthermore, the possible benefit of using the two sources of prior information can be compared against the possible benefit of grouping st</context>
</contexts>
<marker>Creutz, Lagus, 2002</marker>
<rawString>M. Creutz and K. Lagus. 2002. Unsupervised discovery of morphemes. In Proc. ACL Workshop on Morphol. and Phonological Learning, pp. 21–30, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H D´ejean</author>
</authors>
<title>Morphemes as necessary concept for structures discovery from untagged corpora.</title>
<date>1998</date>
<booktitle>In Workshop on Paradigms and Grounding in Nat. Lang. Learning,</booktitle>
<pages>295--299</pages>
<location>Adelaide.</location>
<marker>D´ejean, 1998</marker>
<rawString>H. D´ejean. 1998. Morphemes as necessary concept for structures discovery from untagged corpora. In Workshop on Paradigms and Grounding in Nat. Lang. Learning, pp. 295–299, Adelaide.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deligne</author>
<author>F Bimbot</author>
</authors>
<title>Language modeling by variable length sequences: Theoretical formulation and evaluation of multigrams.</title>
<date>1995</date>
<booktitle>In Proc. ICASSP.</booktitle>
<contexts>
<context position="26709" citStr="Deligne and Bimbot, 1995" startWordPosition="4482" endWordPosition="4485">the probabilistic method and Recursive MDL perform better than the no-segmentation baseline for all data sizes. The segmentations could be evaluated using other measures, but for language modelling purposes, we believe that the evaluation measure should not favour shattering of very common strings, even though they correspond to more than one morpheme. These strings should rather work as individual vocabulary items in the model. It has been shown that increased performance of n-gram models can be obtained by adding larger units consisting of common word sequences to the vocabulary; see e.g., (Deligne and Bimbot, 1995). Nevertheless, in the near future we wish to explore possibilities of using complementary and more standard evaluation measures, such as precision, recall, and F-measure of the discovered morph boundaries. Concerning the length and frequency prior distributions in the probabilistic model, one notes that they are very general and do not make far-reaching 2 5 10 50 100 200 Probabilistic Recursive MDL Linguistica No segmentation 60 50 40 30 20 10 0 Expectation(recognized morphemes) [%] assumptions about the behaviour of natural language. In fact, Zipf’s law has been shown to apply to randomly ge</context>
</contexts>
<marker>Deligne, Bimbot, 1995</marker>
<rawString>S. Deligne and F. Bimbot. 1995. Language modeling by variable length sequences: Theoretical formulation and evaluation of multigrams. In Proc. ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deligne</author>
<author>F Bimbot</author>
</authors>
<title>Inference of variablelength linguistic and acoustic units by multigrams.</title>
<date>1997</date>
<journal>Speech Communication,</journal>
<pages>23--223</pages>
<contexts>
<context position="4073" citStr="Deligne and Bimbot, 1997" startWordPosition="624" endWordPosition="627">larity of the words (Schone and Jurafsky, 2000; Neuvel and Fulop, 2002; Baroni et al., 2002). Here a small number of components per word are assumed, which makes the approaches difficult to apply as such to agglutinative languages. We previously presented two segmentation algorithms suitable for agglutinative languages (Creutz and Lagus, 2002). The algorithms learn a set of segments, which we call morphs, from a corpus. Stems and affixes are not distinguished as separate categories by the algorithms, and in that sense they resemble algorithms for text segmentation and word discovery, such as (Deligne and Bimbot, 1997; Brent, 1999; Kit and Wilks, 1999; Yu, 2000). However, we observed that for the corpus size studied (100 000 words), our two algorithms were somewhat prone to excessive segmentation of words. In this paper, we aim at overcoming the problem of excessive segmentation, particularly when small corpora (up to 200 000 words) are used for training. We present a new segmentation algorithm, which is language independent and works in an unsupervised fashion. Since the results obtained suggest that the algorithm performs rather well, it could possibly be suitable for languages for which only small amoun</context>
</contexts>
<marker>Deligne, Bimbot, 1997</marker>
<rawString>S. Deligne and F. Bimbot. 1997. Inference of variablelength linguistic and acoustic units by multigrams. Speech Communication, 23:223–241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goldsmith</author>
</authors>
<title>Unsupervised learning of the morphology of a natural language.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="2632" citStr="Goldsmith, 2001" startWordPosition="398" endWordPosition="400">ork by experts. This is both time-consuming and expensive and hardly applicable to all languages. Furthermore, as language evolves the lexicon must be updated continuously in order to remain up-to-date. Alternatively, an interesting field of research lies open: Minimally supervised algorithms can be designed that automatically discover morphemes or morpheme-like units from data. There exist a number of such algorithms, some of which are entirely unsupervised and others that use some knowledge of the language. In the following, we discuss recent unsupervised algorithms and refer the reader to (Goldsmith, 2001) for a comprehensive survey of previous research in the whole field. Many algorithms proceed by segmenting (i.e., splitting) words into smaller components. Often the limiting assumption is made that words consist of only one stem followed by one (possibly empty) suffix (D´ejean, 1998; Snover and Brent, 2001; Snover et al., 2002). This limitation is reduced in (Goldsmith, 2001) by allowing a recursive structure, where stems can have inner structure, so that they in turn consist of a substem and a suffix. Also prefixes are possible. However, for languages with agglutinative morphology this may n</context>
<context position="18475" citStr="Goldsmith, 2001" startWordPosition="3149" endWordPosition="3150">apping. Thus, we measure the proportion of morphemes in the test vocabulary that we can expect to recognize correctly by examining the morph segments.9 4 Experiments We have conducted experiments involving (i) three different segmentation algorithms, (ii) two corpora in different languages (Finnish and English), and (iii) data sizes ranging from 2000 words to 200 000 words. 4.1 Segmentation algorithms The new probabilistic method is compared to two existing segmentation methods: the Recursive MDL method presented in (Creutz and Lagus, 2002)10 and John Goldsmith’s algorithm called Linguistica (Goldsmith, 2001).11 Both methods use MDL (Minimum Description Length) (Rissanen, 1989) as a criterion for model optimization. The effect of using prior information on the distribution of morph length and frequency can be assessed by comparing the probabilistic method to Recursive MDL, since both methods utilize the same search algorithm, but Recursive MDL does not make use of explicit prior information. Furthermore, the possible benefit of using the two sources of prior information can be compared against the possible benefit of grouping stems and suffixes into signatures. The latter technique is employed by </context>
</contexts>
<marker>Goldsmith, 2001</marker>
<rawString>J. Goldsmith. 2001. Unsupervised learning of the morphology of a natural language. Computational Linguistics, 27(2):153–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Karlsson</author>
</authors>
<title>Unsupervised learning of word boundary with description length gain.</title>
<date>1987</date>
<booktitle>In Proc. CoNLL99 ACL Workshop,</booktitle>
<editor>Finnish Grammar. WSOY, 2nd ed. C. Kit and</editor>
<location>Bergen.</location>
<contexts>
<context position="1670" citStr="Karlsson, 1987" startWordPosition="251" endWordPosition="253">uch units. For instance, in statistical language modelling, probabilities of word sequences are typically estimated, and bag-of-word models are common in information retrieval. However, for some languages it is infeasible to construct lexicons for NLP applications, if the lexicons contain entire words. In especially agglutinative languages,1 such as Finnish and Turkish, the 1In agglutinative languages words are formed by the concatenation of morphemes. number of possible different word forms is simply too high. For example, in Finnish, a single verb may appear in thousands of different forms (Karlsson, 1987). According to linguistic theory, words are built from smaller units, morphemes. Morphemes are the smallest meaning-bearing elements of language and could be used as lexical units instead of entire words. However, the construction of a comprehensive morphological lexicon or analyzer based on linguistic theory requires a considerable amount of work by experts. This is both time-consuming and expensive and hardly applicable to all languages. Furthermore, as language evolves the lexicon must be updated continuously in order to remain up-to-date. Alternatively, an interesting field of research lie</context>
</contexts>
<marker>Karlsson, 1987</marker>
<rawString>F. Karlsson. 1987. Finnish Grammar. WSOY, 2nd ed. C. Kit and Y. Wilks. 1999. Unsupervised learning of word boundary with description length gain. In Proc. CoNLL99 ACL Workshop, Bergen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Koskenniemi</author>
</authors>
<title>Two-level morphology: A general computational model for word-form recognition and production.</title>
<date>1983</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Helsinki.</institution>
<contexts>
<context position="15877" citStr="Koskenniemi (1983)" startWordPosition="2746" endWordPosition="2747">l “understand” it by decomposing it into morphs that it “understands”. A segmentation algorithm that segments 8Cf. “hop +ed” vs. “hope + d” (past tense of “to hope”). p(f E [(1/γ) fˆ ... γ ˆf[) = γ P fˆnfˆ C1a e− a1 log fˆ a 1 − γ 1 a − 1 a e− 1 a words into too small parts will perform poorly due to high ambiguity. At the other extreme, an algorithm that is reluctant at splitting words will have bad generalization ability to new word forms. Reference morpheme sequences for the words are obtained using existing software for automatic morphological analysis based on the two-level morphology of Koskenniemi (1983). For each word form, the analyzer outputs the base form of the word together with grammatical tags. By filtering the output, we get a sequence of morpheme labels that appear in the correct order and represent correct morphemes rather closely. Note, however, that the morpheme labels are not necessarily orthographically similar to the morphemes they represent. The exact procedure for evaluating the segmentation of a set of words consists of the following steps: (1) Segment the words in the corpus using the automatic segmentation algorithm. (2) Divide the segmented data into two parts of equal s</context>
</contexts>
<marker>Koskenniemi, 1983</marker>
<rawString>K. Koskenniemi. 1983. Two-level morphology: A general computational model for word-form recognition and production. Ph.D. thesis, University of Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Li</author>
</authors>
<title>Random texts exhibit Zipf’s-Law-like word frequency distribution.</title>
<date>1992</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>38</volume>
<issue>6</issue>
<contexts>
<context position="27344" citStr="Li, 1992" startWordPosition="4587" endWordPosition="4588">r future we wish to explore possibilities of using complementary and more standard evaluation measures, such as precision, recall, and F-measure of the discovered morph boundaries. Concerning the length and frequency prior distributions in the probabilistic model, one notes that they are very general and do not make far-reaching 2 5 10 50 100 200 Probabilistic Recursive MDL Linguistica No segmentation 60 50 40 30 20 10 0 Expectation(recognized morphemes) [%] assumptions about the behaviour of natural language. In fact, Zipf’s law has been shown to apply to randomly generated artificial texts (Li, 1992). In our implementation, due to the independence assumptions made in the model and due to the search algorithm used, the choice of a prior value for the most common morph length is more important than the hapax legomena value. If a very bad prior value for the most common morph length is used performance drops by twelve percentage units, whereas extreme hapax legomena values only reduces performance by two percentage units. But note that the two values are dependent: A greater average morph length means a greater number of hapax legomena and vice versa. There is always room for improvement. Ou</context>
</contexts>
<marker>Li, 1992</marker>
<rawString>W. Li. 1992. Random texts exhibit Zipf’s-Law-like word frequency distribution. IEEE Transactions on Information Theory, 38(6):1842–1845.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Neuvel</author>
<author>S A Fulop</author>
</authors>
<title>Unsupervised learning of morphology without morphemes.</title>
<date>2002</date>
<booktitle>In Proc. ACL Workshop on Morphol. &amp; Phonol. Learn.,</booktitle>
<pages>31--40</pages>
<contexts>
<context position="3519" citStr="Neuvel and Fulop, 2002" startWordPosition="537" endWordPosition="540">ffix (D´ejean, 1998; Snover and Brent, 2001; Snover et al., 2002). This limitation is reduced in (Goldsmith, 2001) by allowing a recursive structure, where stems can have inner structure, so that they in turn consist of a substem and a suffix. Also prefixes are possible. However, for languages with agglutinative morphology this may not be enough. In Finnish, a word can consist of lengthy sequences of alternating stems and affixes. Some morphology discovery algorithms learn relationships between words by comparing the orthographic or semantic similarity of the words (Schone and Jurafsky, 2000; Neuvel and Fulop, 2002; Baroni et al., 2002). Here a small number of components per word are assumed, which makes the approaches difficult to apply as such to agglutinative languages. We previously presented two segmentation algorithms suitable for agglutinative languages (Creutz and Lagus, 2002). The algorithms learn a set of segments, which we call morphs, from a corpus. Stems and affixes are not distinguished as separate categories by the algorithms, and in that sense they resemble algorithms for text segmentation and word discovery, such as (Deligne and Bimbot, 1997; Brent, 1999; Kit and Wilks, 1999; Yu, 2000).</context>
</contexts>
<marker>Neuvel, Fulop, 2002</marker>
<rawString>S. Neuvel and S. A. Fulop. 2002. Unsupervised learning of morphology without morphemes. In Proc. ACL Workshop on Morphol. &amp; Phonol. Learn., pp. 31–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rissanen</author>
</authors>
<date>1989</date>
<journal>Stochastic Complexity in Statistical Inquiry,</journal>
<booktitle>World Scientific Series in Computer Science,</booktitle>
<volume>15</volume>
<contexts>
<context position="18545" citStr="Rissanen, 1989" startWordPosition="3159" endWordPosition="3160">lary that we can expect to recognize correctly by examining the morph segments.9 4 Experiments We have conducted experiments involving (i) three different segmentation algorithms, (ii) two corpora in different languages (Finnish and English), and (iii) data sizes ranging from 2000 words to 200 000 words. 4.1 Segmentation algorithms The new probabilistic method is compared to two existing segmentation methods: the Recursive MDL method presented in (Creutz and Lagus, 2002)10 and John Goldsmith’s algorithm called Linguistica (Goldsmith, 2001).11 Both methods use MDL (Minimum Description Length) (Rissanen, 1989) as a criterion for model optimization. The effect of using prior information on the distribution of morph length and frequency can be assessed by comparing the probabilistic method to Recursive MDL, since both methods utilize the same search algorithm, but Recursive MDL does not make use of explicit prior information. Furthermore, the possible benefit of using the two sources of prior information can be compared against the possible benefit of grouping stems and suffixes into signatures. The latter technique is employed by Linguistica. 4.2 Data The Finnish data consists of subsets of a newspa</context>
</contexts>
<marker>Rissanen, 1989</marker>
<rawString>J. Rissanen. 1989. Stochastic Complexity in Statistical Inquiry, vol. 15. World Scientific Series in Computer Science, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Schone</author>
<author>D Jurafsky</author>
</authors>
<title>Knowledge-free induction of morphology using Latent Semantic Analysis.</title>
<date>2000</date>
<booktitle>In Proc. CoNLL-2000 &amp; LLL-2000,</booktitle>
<pages>67--72</pages>
<contexts>
<context position="3495" citStr="Schone and Jurafsky, 2000" startWordPosition="533" endWordPosition="536"> by one (possibly empty) suffix (D´ejean, 1998; Snover and Brent, 2001; Snover et al., 2002). This limitation is reduced in (Goldsmith, 2001) by allowing a recursive structure, where stems can have inner structure, so that they in turn consist of a substem and a suffix. Also prefixes are possible. However, for languages with agglutinative morphology this may not be enough. In Finnish, a word can consist of lengthy sequences of alternating stems and affixes. Some morphology discovery algorithms learn relationships between words by comparing the orthographic or semantic similarity of the words (Schone and Jurafsky, 2000; Neuvel and Fulop, 2002; Baroni et al., 2002). Here a small number of components per word are assumed, which makes the approaches difficult to apply as such to agglutinative languages. We previously presented two segmentation algorithms suitable for agglutinative languages (Creutz and Lagus, 2002). The algorithms learn a set of segments, which we call morphs, from a corpus. Stems and affixes are not distinguished as separate categories by the algorithms, and in that sense they resemble algorithms for text segmentation and word discovery, such as (Deligne and Bimbot, 1997; Brent, 1999; Kit and</context>
</contexts>
<marker>Schone, Jurafsky, 2000</marker>
<rawString>P. Schone and D. Jurafsky. 2000. Knowledge-free induction of morphology using Latent Semantic Analysis. In Proc. CoNLL-2000 &amp; LLL-2000, pp. 67–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M G Snover</author>
<author>M R Brent</author>
</authors>
<title>A Bayesian model for morpheme and paradigm identification.</title>
<date>2001</date>
<booktitle>In Proc. 39th Annual Meeting of the ACL,</booktitle>
<pages>482--490</pages>
<contexts>
<context position="2940" citStr="Snover and Brent, 2001" startWordPosition="445" endWordPosition="448">esigned that automatically discover morphemes or morpheme-like units from data. There exist a number of such algorithms, some of which are entirely unsupervised and others that use some knowledge of the language. In the following, we discuss recent unsupervised algorithms and refer the reader to (Goldsmith, 2001) for a comprehensive survey of previous research in the whole field. Many algorithms proceed by segmenting (i.e., splitting) words into smaller components. Often the limiting assumption is made that words consist of only one stem followed by one (possibly empty) suffix (D´ejean, 1998; Snover and Brent, 2001; Snover et al., 2002). This limitation is reduced in (Goldsmith, 2001) by allowing a recursive structure, where stems can have inner structure, so that they in turn consist of a substem and a suffix. Also prefixes are possible. However, for languages with agglutinative morphology this may not be enough. In Finnish, a word can consist of lengthy sequences of alternating stems and affixes. Some morphology discovery algorithms learn relationships between words by comparing the orthographic or semantic similarity of the words (Schone and Jurafsky, 2000; Neuvel and Fulop, 2002; Baroni et al., 2002</context>
</contexts>
<marker>Snover, Brent, 2001</marker>
<rawString>M. G. Snover and M. R. Brent. 2001. A Bayesian model for morpheme and paradigm identification. In Proc. 39th Annual Meeting of the ACL, pp. 482–490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M G Snover</author>
<author>G E Jarosz</author>
<author>M R Brent</author>
</authors>
<title>Unsupervised learning of morphology using a novel directed search algorithm: Taking the first step.</title>
<date>2002</date>
<booktitle>In Proc. ACL Worksh. Morphol. &amp; Phonol. Learn.,</booktitle>
<pages>11--20</pages>
<contexts>
<context position="2962" citStr="Snover et al., 2002" startWordPosition="449" endWordPosition="452">ly discover morphemes or morpheme-like units from data. There exist a number of such algorithms, some of which are entirely unsupervised and others that use some knowledge of the language. In the following, we discuss recent unsupervised algorithms and refer the reader to (Goldsmith, 2001) for a comprehensive survey of previous research in the whole field. Many algorithms proceed by segmenting (i.e., splitting) words into smaller components. Often the limiting assumption is made that words consist of only one stem followed by one (possibly empty) suffix (D´ejean, 1998; Snover and Brent, 2001; Snover et al., 2002). This limitation is reduced in (Goldsmith, 2001) by allowing a recursive structure, where stems can have inner structure, so that they in turn consist of a substem and a suffix. Also prefixes are possible. However, for languages with agglutinative morphology this may not be enough. In Finnish, a word can consist of lengthy sequences of alternating stems and affixes. Some morphology discovery algorithms learn relationships between words by comparing the orthographic or semantic similarity of the words (Schone and Jurafsky, 2000; Neuvel and Fulop, 2002; Baroni et al., 2002). Here a small number</context>
</contexts>
<marker>Snover, Jarosz, Brent, 2002</marker>
<rawString>M. G. Snover, G. E. Jarosz, and M. R. Brent. 2002. Unsupervised learning of morphology using a novel directed search algorithm: Taking the first step. In Proc. ACL Worksh. Morphol. &amp; Phonol. Learn., pp. 11–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yu</author>
</authors>
<title>Unsupervised word induction using MDL criterion.</title>
<date>2000</date>
<booktitle>In Proc. ISCSL,</booktitle>
<location>Beijing.</location>
<contexts>
<context position="4118" citStr="Yu, 2000" startWordPosition="634" endWordPosition="635">ulop, 2002; Baroni et al., 2002). Here a small number of components per word are assumed, which makes the approaches difficult to apply as such to agglutinative languages. We previously presented two segmentation algorithms suitable for agglutinative languages (Creutz and Lagus, 2002). The algorithms learn a set of segments, which we call morphs, from a corpus. Stems and affixes are not distinguished as separate categories by the algorithms, and in that sense they resemble algorithms for text segmentation and word discovery, such as (Deligne and Bimbot, 1997; Brent, 1999; Kit and Wilks, 1999; Yu, 2000). However, we observed that for the corpus size studied (100 000 words), our two algorithms were somewhat prone to excessive segmentation of words. In this paper, we aim at overcoming the problem of excessive segmentation, particularly when small corpora (up to 200 000 words) are used for training. We present a new segmentation algorithm, which is language independent and works in an unsupervised fashion. Since the results obtained suggest that the algorithm performs rather well, it could possibly be suitable for languages for which only small amounts of written text are available. The model i</context>
</contexts>
<marker>Yu, 2000</marker>
<rawString>H. Yu. 2000. Unsupervised word induction using MDL criterion. In Proc. ISCSL, Beijing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>