<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000554">
<title confidence="0.993712">
The Lie Detector: Explorations in the Automatic Recognition
of Deceptive Language
</title>
<author confidence="0.998705">
Rada Mihalcea Carlo Strapparava
</author>
<affiliation confidence="0.999397">
University of North Texas FBK-IRST
</affiliation>
<email confidence="0.996655">
rada@cs.unt.edu strappa@fbk.eu
</email>
<sectionHeader confidence="0.993834" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999836272727273">
In this paper, we present initial experi-
ments in the recognition of deceptive lan-
guage. We introduce three data sets of true
and lying texts collected for this purpose,
and we show that automatic classification
is a viable technique to distinguish be-
tween truth and falsehood as expressed in
language. We also introduce a method for
class-based feature analysis, which sheds
some light on the features that are charac-
teristic for deceptive text.
</bodyText>
<note confidence="0.621409">
You should not trust the devil, even if he tells the truth.
– Thomas ofAquin (medieval philosopher)
</note>
<sectionHeader confidence="0.968786" genericHeader="categories and subject descriptors">
1 Introduction and Motivation
</sectionHeader>
<bodyText confidence="0.999929333333333">
The discrimination between truth and falsehood
has received significant attention from fields as
diverse as philosophy, psychology and sociology.
Recent advances in computational linguistics mo-
tivate us to approach the recognition of deceptive
language from a data-driven perspective, and at-
tempt to identify the salient features of lying texts
using natural language processing techniques.
In this paper, we explore the applicability of
computational approaches to the recognition of
deceptive language. In particular, we investigate
whether automatic classification techniques repre-
sent a viable approach to distinguish between truth
and lies as expressed in written text. Although
acoustic and other non-linguistic features were
also found to be useful for this task (Hirschberg
et al., 2005), we deliberately focus on written lan-
guage, since it represents the type of data most fre-
quently encountered on the Web (e.g., chats, fo-
rums) or in other collections of documents.
Specifically, we try to answer the following two
questions. First, are truthful and lying texts sep-
arable, and does this property hold for different
datasets? To answer this question, we use three
different data sets that we construct for this pur-
pose – consisting of true and false short statements
on three different topics – and attempt to automat-
ically separate them using standard natural lan-
guage processing techniques.
Second, if truth and lies are separable, what are
the distinctive features of deceptive texts? In an-
swer to this second question, we attempt to iden-
tify some of the most salient features of lying texts,
and analyse their occurrence in the three data sets.
The paper is organized as follows. We first
briefly review the related work, followed by a de-
scription of the three data sets that we constructed.
Next, we present our experiments and results using
automatic classification, and introduce a method
for the analysis of salient features in deceptive
texts. Lastly, we conclude with a discussion and
directions for future work.
</bodyText>
<sectionHeader confidence="0.999691" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999883076923077">
Very little work, if any, has been carried out on the
automatic detection of deceptive language in writ-
ten text. Most of the previous work has focused
on the psychological or social aspects of lying, and
there are only a few previous studies that have con-
sidered the linguistic aspects of falsehood.
In psychology, it is worthwhile mentioning the
study reported in (DePaulo et al., 2003), where
more than 100 cues to deception are mentioned.
However, only a few of them are linguistic in na-
ture, as e.g., word and phrase repetitions, while
most of the cues involve speaker’s behavior, in-
cluding facial expressions, eye shifts, etc. (New-
man et al., 2003) also report on a psycholinguistic
study, where they conduct a qualitative analysis of
true and false stories by using word counting tools.
Computational work includes the study of
(Zhou et al., 2004), which studied linguistic cues
for deception detection in the context of text-based
asynchronous computer mediated communication,
and (Hirschberg et al., 2005) who focused on de-
ception in speech using primarily acoustic and
prosodic features.
Our work is also related to the automatic clas-
sification of text genre, including work on author
profiling (Koppel et al., 2002), humor recognition
</bodyText>
<page confidence="0.99146">
309
</page>
<note confidence="0.809178">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 309–312,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
TRUTH LIE
</note>
<sectionHeader confidence="0.754233" genericHeader="method">
ABORTION
</sectionHeader>
<bodyText confidence="0.994520111111111">
I believe abortion is not an option. Once a life has been
conceived, it is precious. No one has the right to decide
to end it. Life begins at conception,because without con-
ception, there is no life.
A woman has free will and free choice over what goes
on in her body. If the child has not been born, it is under
her control. Often the circumstances an unwanted child
is born into are worse than death. The mother has the
responsibility to choose the best course for her child.
</bodyText>
<sectionHeader confidence="0.529887" genericHeader="method">
DEATH PENALTY
</sectionHeader>
<bodyText confidence="0.968428181818182">
I stand against death penalty. It is pompous of anyone
to think that they have the right to take life. No court of
law can eliminate all possibilities of doubt. Also, some
circumstances may have pushed a person to commit a
crime that would otherwise merit severe punishment.
Death penalty is very important as a deterrent against
crime. We live in a society, not as individuals. This
imposes some restrictions on our actions. If a person
doesn’t adhere to these restrictions, he or she forfeits her
life. Why should taxpayers’ money be spent on feeding
murderers?
</bodyText>
<subsectionHeader confidence="0.380585">
BEST FRIEND
</subsectionHeader>
<bodyText confidence="0.999417909090909">
I have been best friends with Jessica for about seven
years now. She has always been there to help me out.
She was even in the delivery room with me when I had
my daughter. She was also one of the Bridesmaids in
my wedding. She lives six hours away, but if we need
each other we’ll make the drive without even thinking.
I have been friends with Pam for almost four years now.
She’s the sweetest person I know. Whenever we need
help she’s always there to lend a hand. She always has
a kind word to say and has a warm heart. She is my
inspiration.
</bodyText>
<tableCaption confidence="0.992151">
Table 1: Sample true and deceptive statements
</tableCaption>
<bodyText confidence="0.684405">
(Mihalcea and Strapparava, 2006), and others.
</bodyText>
<sectionHeader confidence="0.987497" genericHeader="method">
3 Data Sets
</sectionHeader>
<bodyText confidence="0.999941265306122">
To study the distinction between true and decep-
tive statements, we required a corpus with explicit
labeling of the truth value associated with each
statement. Since we were not aware of any such
data set, we had to create one ourselves. We fo-
cused on three different topics: opinions on abor-
tion, opinions on death penalty, and feelings about
the best friend. For each of these three topics
an annotation task was defined using the Amazon
Mechanical Turk service.
For the first two topics (abortion and death
penalty), we provided instructions that asked the
contributors to imagine they were taking part in
a debate, and had 10-15 minutes available to ex-
press their opinion about the topic. First, they were
asked to prepare a brief speech expressing their
true opinion on the topic. Next, they were asked
to prepare a second brief speech expressing the op-
posite of their opinion, thus lying about their true
beliefs about the topic. In both cases, the guide-
lines asked for at least 4-5 sentences and as many
details as possible.
For the third topic (best friend), the contributors
were first asked to think about their best friend and
describe the reasons for their friendship (including
facts and anecdotes considered relevant for their
relationship). Thus, in this case, they were asked
to tell the truth about how they felt about their best
friend. Next, they were asked to think about a per-
son they could not stand, and describe it as if s/he
were their best friend. In this second case, they
had to lie about their feelings toward this person.
As before, in both cases the instructions asked for
at least 4-5 detailed sentences.
We collected 100 true and 100 false statements
for each topic, with an average of 85 words per
statement. Previous work has shown that data
collected through the Mechanical Turk service is
reliable and comparable in quality with trusted
sources (Snow et al., 2008). We also made a man-
ual verification of the quality of the contributions,
and checked by hand the quality of all the contri-
butions. With two exceptions – two entries where
the true and false statements were identical, which
were removed from the data – all the other entries
were found to be of good quality, and closely fol-
lowing our instructions.
Table 1 shows an example of true and deceptive
language for each of the three topics.
</bodyText>
<sectionHeader confidence="0.990534" genericHeader="method">
4 Experimental Setup and Results
</sectionHeader>
<bodyText confidence="0.999686357142857">
For the experiments, we used two classifiers:
Naive Bayes and SVM, selected based on their
performance and diversity of learning methodolo-
gies. Only minimal preprocessing was applied
to the three data sets, which included tokeniza-
tion and stemming. No feature selection was per-
formed, and stopwords were not removed.
Table 2 shows the ten-fold cross-validation re-
sults using the two classifiers. Since all three data
sets have an equal distribution between true and
false statements, the baseline for all the topics is
50%. The average classification performance of
70% – significantly higher than the 50% baseline
– indicates that good separation can be obtained
</bodyText>
<page confidence="0.991101">
310
</page>
<table confidence="0.998565125">
between true and deceptive language by using au- CoverageD(C) = E FrequencyD(Wi)
tomatic classifiers. Wi∈C
SizeD
Topic NB SVM
ABORTION 70.0% 67.5%
DEATH PENALTY 67.4% 65.9%
BEST FRIEND 75.0% 77.0%
AVERAGE 70.8% 70.1%
</table>
<tableCaption confidence="0.674616666666667">
Table 2: Ten-fold cross-validation classification
results, using a Naive Bayes (NB) or Support Vec-
tor Machines (SVM) classifier
</tableCaption>
<bodyText confidence="0.999340857142857">
To gain further insight into the variation of ac-
curacy with the amount of data available, we also
plotted the learning curves for each of the data
sets, as shown in Figure 1. The overall growing
trend indicates that more data is likely to improve
the accuracy, thus suggesting the collection of ad-
ditional data as a possible step for future work.
</bodyText>
<figure confidence="0.877946">
Classification learning curves
0 20 40 60 80 100
Fraction of data (%)
</figure>
<figureCaption confidence="0.999995">
Figure 1: Classification learning curves.
</figureCaption>
<bodyText confidence="0.999752625">
We also tested the portability of the classifiers
across topics, using two topics as training data and
the third topic as test. The results are shown in Ta-
ble 3. Although below the in-topic performance,
the average accuracy is still significantly higher
than the 50% baseline, indicating that the learning
process relies on clues specific to truth/deception,
and it is not bound to a particular topic.
</bodyText>
<sectionHeader confidence="0.8922325" genericHeader="method">
5 Identifying Dominant Word Classes in
Deceptive Text
</sectionHeader>
<bodyText confidence="0.9996616875">
In order to gain a better understanding of the char-
acteristics of deceptive text, we devised a method
to calculate a score associated with a given class
of words, as a measure of saliency for the given
word class inside the collection of deceptive (or
truthful) texts.
Given a class of words C = {W1, W2, ..., WN},
we define the class coverage in the deceptive cor-
pus D as the percentage of words from D belong-
ing to the class C:
where FrequencyD(Wi) represents the total
number of occurrences of word Wi inside the cor-
pus D, and SizeD represents the total size (in
words) of the corpus D.
Similarly, we define the class C coverage for the
truthful corpus T:
</bodyText>
<subsectionHeader confidence="0.42527">
E FrequencyT (Wi)
Wi∈C
SizeT
</subsectionHeader>
<bodyText confidence="0.999785">
The dominance score of the class C in the de-
ceptive corpus D is then defined as the ratio be-
tween the coverage of the class in the corpus D
with respect to the coverage of the same class in
the corpus T:
</bodyText>
<equation confidence="0.9990095">
CoverageD(C)
DominanceD(C) = CoverageT(C) (1)
</equation>
<bodyText confidence="0.999863388888889">
A dominance score close to 1 indicates a similar
distribution of the words in the class C in both the
deceptive and the truthful corpus. Instead, a score
significantly higher than 1 indicates a class that is
dominant in the deceptive corpus, and thus likely
to be a characteristic of the texts in this corpus.
Finally, a score significantly lower than 1 indicates
a class that is dominant in the truthful corpus, and
unlikely to appear in the deceptive corpus.
We use the classes of words as defined in
the Linguistic Inquiry and Word Count (LIWC),
which was developed as a resource for psycholin-
guistic analysis (Pennebaker and Francis, 1999).
The 2001 version of LIWC includes about 2,200
words and word stems grouped into about 70
broad categories relevant to psychological pro-
cesses (e.g., EMOTION, COGNITION). The LIWC
lexicon has been validated by showing significant
correlation between human ratings of a large num-
ber of written texts and the rating obtained through
LIWC-based analyses of the same texts.
All the word classes from LIWC are ranked ac-
cording to the dominance score calculated with
formula 1, using a mix of all three data sets to
create the D and T corpora. Those classes that
have a high score are the classes that are dom-
inant in deceptive text. The classes that have a
small score are the classes that are dominant in
truthful text and lack from deceptive text. Table 4
shows the top ranked classes along with their dom-
inance score and a few sample words that belong
to the given class and also appeared in the decep-
tive (truthful) texts.
Interestingly, in both truthful and deceptive lan-
guage, three of the top five dominant classes are
related to humans. In deceptive texts however, the
</bodyText>
<figure confidence="0.998602">
Classification accuracy (%)
100
40
90
80
70
60
50
Abortion
Death penalty
Best friend
CoverageT(C) =
</figure>
<page confidence="0.994837">
311
</page>
<table confidence="0.9978158">
Training Test NB SVM
DEATH PENALTY + BEST FRIEND ABORTION 62.0% 61.0%
ABORTION + BEST FRIEND DEATH PENALTY 58.7% 58.7%
ABORTION + DEATH PENALTY BEST FRIEND 58.7% 53.6%
AVERAGE 59.8% 57.8%
</table>
<tableCaption confidence="0.911714">
Table 3: Cross-topic classification results
</tableCaption>
<table confidence="0.999806818181818">
Class Score
METAPH 1.71
YOU 1.53
OTHER 1.47
HUMANS 1.31
CERTAIN 1.24
OPTIM 0.57
I 0.59
FRIENDS 0.63
SELF 0.64
INSIGHT 0.65
</table>
<tableCaption confidence="0.753066571428571">
Sample words
Deceptive Text
god, die, sacred, mercy, sin, dead, hell, soul, lord, sins
you, thou
she, her, they, his, them, him, herself, himself, themselves
person, child, human, baby, man, girl, humans, individual, male, person, adult
always, all, very, truly, completely, totally
Truthful Text
best, ready, hope, accepts, accept, determined, accepted, won, super
I, myself, mine
friend, companion, body
our, myself, mine, ours
believe, think, know, see, understand, found, thought, feels, admit
Table 4: Dominant word classes in deceptive text, along with sample words.
</tableCaption>
<bodyText confidence="0.999926052631579">
human-related word classes (YOU, OTHER, HU-
MANS) represent detachment from the self, as if
trying not to have the own self involved in the
lies. Instead, the classes of words that are closely
connected to the self (I, FRIENDS, SELF) are lack-
ing from deceptive text, being dominant instead in
truthful statements, where the speaker is comfort-
able with identifying herself with the statements
she makes.
Also interesting is the fact that words related
to certainty (CERTAIN) are more dominant in de-
ceptive texts, which is probably explained by the
need of the speaker to explicitly use truth-related
words as a means to emphasize the (fake) “truth”
and thus hide the lies. Instead, belief-oriented vo-
cabulary (INSIGHT), such as believe, feel, think,
is more frequently encountered in truthful state-
ments, where the presence of the real truth does
not require truth-related words for emphasis.
</bodyText>
<sectionHeader confidence="0.999722" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999978307692308">
In this paper, we explored automatic techniques
for the recognition of deceptive language in writ-
ten texts. Through experiments carried out on
three data sets, we showed that truthful and ly-
ing texts are separable, and this property holds
for different data sets. An analysis of classes of
salient features indicated some interesting patterns
of word usage in deceptive texts, including detach-
ment from the self and vocabulary that emphasizes
certainty. In future work, we plan to explore the
role played by affect and the possible integration
of automatic emotion analysis into the recognition
of deceptive language.
</bodyText>
<sectionHeader confidence="0.999439" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999824882352941">
B. DePaulo, J. Lindsay, B. Malone, L. Muhlenbruck,
K. Charlton, and H. Cooper. 2003. Cues to decep-
tion. Psychological Bulletin, 129(1):74—118.
J. Hirschberg, S. Benus, J. Brenier, F. Enos, S. Fried-
man, S. Gilman, C. Girand, M. Graciarena,
A. Kathol, L. Michaelis, B. Pellom, E. Shriberg,
and A. Stolcke. 2005. Distinguishing decep-
tive from non-deceptive speech. In Proceedings of
INTERSPEECH-2005, Lisbon, Portugal.
M. Koppel, S. Argamon, and A. Shimoni. 2002. Au-
tomatically categorizing written texts by author gen-
der. Literary and Linguistic Computing, 4(17):401–
412.
R. Mihalcea and C. Strapparava. 2006. Learning to
laugh (automatically): Computational models for
humor recognition. Computational Intelligence,
22(2):126–142.
M. Newman, J. Pennebaker, D. Berry, and J. Richards.
2003. Lying words: Predicting deception from lin-
guistic styles. Personality and Social Psychology
Bulletin, 29:665–675.
J. Pennebaker and M. Francis. 1999. Linguistic in-
quiry and word count: LIWC. Erlbaum Publishers.
R. Snow, B. O’Connor, D. Jurafsky, and A. Ng. 2008.
Cheap and fast – but is it good? evaluating non-
expert annotations for natural language tasks. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, Honolulu,
Hawaii.
L. Zhou, J Burgoon, J. Nunamaker, and D. Twitchell.
2004. Automating linguistics-based cues for detect-
ing deception in text-based asynchronous computer-
mediated communication. Group Decision and Ne-
gotiation, 13:81–106.
</reference>
<page confidence="0.998661">
312
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.657843">
<title confidence="0.998855">The Lie Detector: Explorations in the Automatic Recognition of Deceptive Language</title>
<author confidence="0.999734">Rada Mihalcea Carlo Strapparava</author>
<affiliation confidence="0.999199">University of North Texas FBK-IRST</affiliation>
<email confidence="0.961007">rada@cs.unt.edustrappa@fbk.eu</email>
<abstract confidence="0.993819307692308">In this paper, we present initial experiments in the recognition of deceptive language. We introduce three data sets of true and lying texts collected for this purpose, and we show that automatic classification is a viable technique to distinguish between truth and falsehood as expressed in language. We also introduce a method for class-based feature analysis, which sheds some light on the features that are characteristic for deceptive text. You should not trust the devil, even if he tells the truth.</abstract>
<intro confidence="0.694382">Thomas ofAquin (medieval philosopher)</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B DePaulo</author>
<author>J Lindsay</author>
<author>B Malone</author>
<author>L Muhlenbruck</author>
<author>K Charlton</author>
<author>H Cooper</author>
</authors>
<title>Cues to deception.</title>
<date>2003</date>
<journal>Psychological Bulletin,</journal>
<volume>129</volume>
<issue>1</issue>
<contexts>
<context position="3199" citStr="DePaulo et al., 2003" startWordPosition="500" endWordPosition="503"> present our experiments and results using automatic classification, and introduce a method for the analysis of salient features in deceptive texts. Lastly, we conclude with a discussion and directions for future work. 2 Related Work Very little work, if any, has been carried out on the automatic detection of deceptive language in written text. Most of the previous work has focused on the psychological or social aspects of lying, and there are only a few previous studies that have considered the linguistic aspects of falsehood. In psychology, it is worthwhile mentioning the study reported in (DePaulo et al., 2003), where more than 100 cues to deception are mentioned. However, only a few of them are linguistic in nature, as e.g., word and phrase repetitions, while most of the cues involve speaker’s behavior, including facial expressions, eye shifts, etc. (Newman et al., 2003) also report on a psycholinguistic study, where they conduct a qualitative analysis of true and false stories by using word counting tools. Computational work includes the study of (Zhou et al., 2004), which studied linguistic cues for deception detection in the context of text-based asynchronous computer mediated communication, and</context>
</contexts>
<marker>DePaulo, Lindsay, Malone, Muhlenbruck, Charlton, Cooper, 2003</marker>
<rawString>B. DePaulo, J. Lindsay, B. Malone, L. Muhlenbruck, K. Charlton, and H. Cooper. 2003. Cues to deception. Psychological Bulletin, 129(1):74—118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hirschberg</author>
<author>S Benus</author>
<author>J Brenier</author>
<author>F Enos</author>
<author>S Friedman</author>
<author>S Gilman</author>
<author>C Girand</author>
<author>M Graciarena</author>
<author>A Kathol</author>
<author>L Michaelis</author>
<author>B Pellom</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
</authors>
<title>Distinguishing deceptive from non-deceptive speech.</title>
<date>2005</date>
<booktitle>In Proceedings of INTERSPEECH-2005,</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="1560" citStr="Hirschberg et al., 2005" startWordPosition="226" endWordPosition="229">ational linguistics motivate us to approach the recognition of deceptive language from a data-driven perspective, and attempt to identify the salient features of lying texts using natural language processing techniques. In this paper, we explore the applicability of computational approaches to the recognition of deceptive language. In particular, we investigate whether automatic classification techniques represent a viable approach to distinguish between truth and lies as expressed in written text. Although acoustic and other non-linguistic features were also found to be useful for this task (Hirschberg et al., 2005), we deliberately focus on written language, since it represents the type of data most frequently encountered on the Web (e.g., chats, forums) or in other collections of documents. Specifically, we try to answer the following two questions. First, are truthful and lying texts separable, and does this property hold for different datasets? To answer this question, we use three different data sets that we construct for this purpose – consisting of true and false short statements on three different topics – and attempt to automatically separate them using standard natural language processing techn</context>
<context position="3825" citStr="Hirschberg et al., 2005" startWordPosition="599" endWordPosition="602">where more than 100 cues to deception are mentioned. However, only a few of them are linguistic in nature, as e.g., word and phrase repetitions, while most of the cues involve speaker’s behavior, including facial expressions, eye shifts, etc. (Newman et al., 2003) also report on a psycholinguistic study, where they conduct a qualitative analysis of true and false stories by using word counting tools. Computational work includes the study of (Zhou et al., 2004), which studied linguistic cues for deception detection in the context of text-based asynchronous computer mediated communication, and (Hirschberg et al., 2005) who focused on deception in speech using primarily acoustic and prosodic features. Our work is also related to the automatic classification of text genre, including work on author profiling (Koppel et al., 2002), humor recognition 309 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 309–312, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP TRUTH LIE ABORTION I believe abortion is not an option. Once a life has been conceived, it is precious. No one has the right to decide to end it. Life begins at conception,because without conception, there is no life. A woman has free</context>
</contexts>
<marker>Hirschberg, Benus, Brenier, Enos, Friedman, Gilman, Girand, Graciarena, Kathol, Michaelis, Pellom, Shriberg, Stolcke, 2005</marker>
<rawString>J. Hirschberg, S. Benus, J. Brenier, F. Enos, S. Friedman, S. Gilman, C. Girand, M. Graciarena, A. Kathol, L. Michaelis, B. Pellom, E. Shriberg, and A. Stolcke. 2005. Distinguishing deceptive from non-deceptive speech. In Proceedings of INTERSPEECH-2005, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Koppel</author>
<author>S Argamon</author>
<author>A Shimoni</author>
</authors>
<title>Automatically categorizing written texts by author gender.</title>
<date>2002</date>
<booktitle>Literary and Linguistic Computing,</booktitle>
<volume>4</volume>
<issue>17</issue>
<pages>412</pages>
<contexts>
<context position="4037" citStr="Koppel et al., 2002" startWordPosition="634" endWordPosition="637">ressions, eye shifts, etc. (Newman et al., 2003) also report on a psycholinguistic study, where they conduct a qualitative analysis of true and false stories by using word counting tools. Computational work includes the study of (Zhou et al., 2004), which studied linguistic cues for deception detection in the context of text-based asynchronous computer mediated communication, and (Hirschberg et al., 2005) who focused on deception in speech using primarily acoustic and prosodic features. Our work is also related to the automatic classification of text genre, including work on author profiling (Koppel et al., 2002), humor recognition 309 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 309–312, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP TRUTH LIE ABORTION I believe abortion is not an option. Once a life has been conceived, it is precious. No one has the right to decide to end it. Life begins at conception,because without conception, there is no life. A woman has free will and free choice over what goes on in her body. If the child has not been born, it is under her control. Often the circumstances an unwanted child is born into are worse than death. The mother has the respon</context>
</contexts>
<marker>Koppel, Argamon, Shimoni, 2002</marker>
<rawString>M. Koppel, S. Argamon, and A. Shimoni. 2002. Automatically categorizing written texts by author gender. Literary and Linguistic Computing, 4(17):401– 412.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>C Strapparava</author>
</authors>
<title>Learning to laugh (automatically): Computational models for humor recognition.</title>
<date>2006</date>
<journal>Computational Intelligence,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="5901" citStr="Mihalcea and Strapparava, 2006" startWordPosition="968" endWordPosition="971">FRIEND I have been best friends with Jessica for about seven years now. She has always been there to help me out. She was even in the delivery room with me when I had my daughter. She was also one of the Bridesmaids in my wedding. She lives six hours away, but if we need each other we’ll make the drive without even thinking. I have been friends with Pam for almost four years now. She’s the sweetest person I know. Whenever we need help she’s always there to lend a hand. She always has a kind word to say and has a warm heart. She is my inspiration. Table 1: Sample true and deceptive statements (Mihalcea and Strapparava, 2006), and others. 3 Data Sets To study the distinction between true and deceptive statements, we required a corpus with explicit labeling of the truth value associated with each statement. Since we were not aware of any such data set, we had to create one ourselves. We focused on three different topics: opinions on abortion, opinions on death penalty, and feelings about the best friend. For each of these three topics an annotation task was defined using the Amazon Mechanical Turk service. For the first two topics (abortion and death penalty), we provided instructions that asked the contributors to</context>
</contexts>
<marker>Mihalcea, Strapparava, 2006</marker>
<rawString>R. Mihalcea and C. Strapparava. 2006. Learning to laugh (automatically): Computational models for humor recognition. Computational Intelligence, 22(2):126–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Newman</author>
<author>J Pennebaker</author>
<author>D Berry</author>
<author>J Richards</author>
</authors>
<title>Lying words: Predicting deception from linguistic styles. Personality and Social Psychology Bulletin,</title>
<date>2003</date>
<pages>29--665</pages>
<contexts>
<context position="3465" citStr="Newman et al., 2003" startWordPosition="545" endWordPosition="549">n carried out on the automatic detection of deceptive language in written text. Most of the previous work has focused on the psychological or social aspects of lying, and there are only a few previous studies that have considered the linguistic aspects of falsehood. In psychology, it is worthwhile mentioning the study reported in (DePaulo et al., 2003), where more than 100 cues to deception are mentioned. However, only a few of them are linguistic in nature, as e.g., word and phrase repetitions, while most of the cues involve speaker’s behavior, including facial expressions, eye shifts, etc. (Newman et al., 2003) also report on a psycholinguistic study, where they conduct a qualitative analysis of true and false stories by using word counting tools. Computational work includes the study of (Zhou et al., 2004), which studied linguistic cues for deception detection in the context of text-based asynchronous computer mediated communication, and (Hirschberg et al., 2005) who focused on deception in speech using primarily acoustic and prosodic features. Our work is also related to the automatic classification of text genre, including work on author profiling (Koppel et al., 2002), humor recognition 309 Proc</context>
</contexts>
<marker>Newman, Pennebaker, Berry, Richards, 2003</marker>
<rawString>M. Newman, J. Pennebaker, D. Berry, and J. Richards. 2003. Lying words: Predicting deception from linguistic styles. Personality and Social Psychology Bulletin, 29:665–675.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pennebaker</author>
<author>M Francis</author>
</authors>
<title>Linguistic inquiry and word count: LIWC.</title>
<date>1999</date>
<publisher>Erlbaum Publishers.</publisher>
<contexts>
<context position="11804" citStr="Pennebaker and Francis, 1999" startWordPosition="1976" endWordPosition="1979">ore close to 1 indicates a similar distribution of the words in the class C in both the deceptive and the truthful corpus. Instead, a score significantly higher than 1 indicates a class that is dominant in the deceptive corpus, and thus likely to be a characteristic of the texts in this corpus. Finally, a score significantly lower than 1 indicates a class that is dominant in the truthful corpus, and unlikely to appear in the deceptive corpus. We use the classes of words as defined in the Linguistic Inquiry and Word Count (LIWC), which was developed as a resource for psycholinguistic analysis (Pennebaker and Francis, 1999). The 2001 version of LIWC includes about 2,200 words and word stems grouped into about 70 broad categories relevant to psychological processes (e.g., EMOTION, COGNITION). The LIWC lexicon has been validated by showing significant correlation between human ratings of a large number of written texts and the rating obtained through LIWC-based analyses of the same texts. All the word classes from LIWC are ranked according to the dominance score calculated with formula 1, using a mix of all three data sets to create the D and T corpora. Those classes that have a high score are the classes that are</context>
</contexts>
<marker>Pennebaker, Francis, 1999</marker>
<rawString>J. Pennebaker and M. Francis. 1999. Linguistic inquiry and word count: LIWC. Erlbaum Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>B O’Connor</author>
<author>D Jurafsky</author>
<author>A Ng</author>
</authors>
<title>Cheap and fast – but is it good? evaluating nonexpert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Honolulu, Hawaii.</location>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>R. Snow, B. O’Connor, D. Jurafsky, and A. Ng. 2008. Cheap and fast – but is it good? evaluating nonexpert annotations for natural language tasks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zhou</author>
<author>J Burgoon</author>
<author>J Nunamaker</author>
<author>D Twitchell</author>
</authors>
<title>Automating linguistics-based cues for detecting deception in text-based asynchronous computermediated communication. Group Decision and Negotiation,</title>
<date>2004</date>
<pages>13--81</pages>
<contexts>
<context position="3665" citStr="Zhou et al., 2004" startWordPosition="578" endWordPosition="581">tudies that have considered the linguistic aspects of falsehood. In psychology, it is worthwhile mentioning the study reported in (DePaulo et al., 2003), where more than 100 cues to deception are mentioned. However, only a few of them are linguistic in nature, as e.g., word and phrase repetitions, while most of the cues involve speaker’s behavior, including facial expressions, eye shifts, etc. (Newman et al., 2003) also report on a psycholinguistic study, where they conduct a qualitative analysis of true and false stories by using word counting tools. Computational work includes the study of (Zhou et al., 2004), which studied linguistic cues for deception detection in the context of text-based asynchronous computer mediated communication, and (Hirschberg et al., 2005) who focused on deception in speech using primarily acoustic and prosodic features. Our work is also related to the automatic classification of text genre, including work on author profiling (Koppel et al., 2002), humor recognition 309 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 309–312, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP TRUTH LIE ABORTION I believe abortion is not an option. Once a life has be</context>
</contexts>
<marker>Zhou, Burgoon, Nunamaker, Twitchell, 2004</marker>
<rawString>L. Zhou, J Burgoon, J. Nunamaker, and D. Twitchell. 2004. Automating linguistics-based cues for detecting deception in text-based asynchronous computermediated communication. Group Decision and Negotiation, 13:81–106.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>