<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.952219">
Dual Decomposition with Many Overlapping Components
</title>
<author confidence="0.971284">
Andr´e F. T. Martins*† Noah A. Smith* Pedro M. Q. Aguiar$ M´ario A. T. Figueiredo†
</author>
<affiliation confidence="0.923489666666667">
*School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
$Instituto de Sistemas e Rob´otica, Instituto Superior T´ecnico, Lisboa, Portugal
†Instituto de Telecomunicac¸˜oes, Instituto Superior T´ecnico, Lisboa, Portugal
</affiliation>
<email confidence="0.98123">
{afm,nasmith}@cs.cmu.edu, aguiar@isr.ist.utl.pt, mtf@lx.it.pt
</email>
<sectionHeader confidence="0.994604" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995399375">
Dual decomposition has been recently pro-
posed as a way of combining complemen-
tary models, with a boost in predictive power.
However, in cases where lightweight decom-
positions are not readily available (e.g., due to
the presence of rich features or logical con-
straints), the original subgradient algorithm
is inefficient. We sidestep that difficulty by
adopting an augmented Lagrangian method
that accelerates model consensus by regular-
izing towards the averaged votes. We show
how first-order logical constraints can be han-
dled efficiently, even though the correspond-
ing subproblems are no longer combinatorial,
and report experiments in dependency pars-
ing, with state-of-the-art results.
</bodyText>
<sectionHeader confidence="0.998431" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99998368627451">
The last years have witnessed increasingly accurate
models for syntax, semantics, and machine transla-
tion (Chiang, 2007; Finkel et al., 2008; Petrov and
Klein, 2008; Smith and Eisner, 2008; Martins et
al., 2009a; Johansson and Nugues, 2008; Koo et al.,
2010). The predictive power of such models stems
from their ability to break locality assumptions. The
resulting combinatorial explosion typically demands
some form of approximate decoding, such as sam-
pling, heuristic search, or variational inference.
In this paper, we focus on parsers built from lin-
ear programming relaxations, the so-called “turbo
parsers” (Martins et al., 2009a; Martins et al., 2010).
Rush et al. (2010) applied dual decomposition as
a way of combining models which alone permit
efficient decoding, but whose combination is in-
tractable. This results in a relaxation of the origi-
nal problem that is elegantly solved with the sub-
gradient algorithm. While this technique has proven
quite effective in parsing (Koo et al., 2010; Auli
and Lopez, 2011) as well as machine translation
(Rush and Collins, 2011), we show here that its
success is strongly tied to the ability of finding a
“good” decomposition, i.e., one involving few over-
lapping components (or slaves). With many compo-
nents, the subgradient algorithm exhibits extremely
slow convergence (cf. Fig. 2). Unfortunately, a
lightweight decomposition is not always at hand, ei-
ther because the problem does not factor in a natural
way, or because one would like to incorporate fea-
tures that cannot be easily absorbed in few tractable
components. Examples include features generated
by statements in first-order logic, features that vio-
late Markov assumptions, or history features such as
the ones employed in transition-based parsers.
To tackle the kind of problems above, we adopt
DD-ADMM (Alg. 1), a recently proposed algorithm
that accelerates dual decomposition (Martins et al.,
2011). DD-ADMM retains the modularity of the
subgradient-based method, but it speeds up consen-
sus by regularizing each slave subproblem towards
the averaged votes obtained in the previous round
(cf. Eq. 14). While this yields more involved sub-
problems (with a quadratic term), we show that ex-
act solutions can still be efficiently computed for
all cases of interest, by using sort operations. As
a result, we obtain parsers that can handle very rich
features, do not require specifying a decomposition,
and can be heavily parallelized. We demonstrate the
success of the approach by presenting experiments
in dependency parsing with state-of-the-art results.
</bodyText>
<sectionHeader confidence="0.99773" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.986137">
2.1 Structured Prediction
</subsectionHeader>
<bodyText confidence="0.999843666666667">
Let x E x be an input object (e.g., a sentence), from
which we want to predict a structured output y E
� (e.g., a parse tree). The output set � is assumed
too large for exhaustive search to be tractable. We
assume to have a model that assigns a score f(y) to
each candidate output, based on which we predict
</bodyText>
<equation confidence="0.635519">
y = arg max f(y). (1)
YO
</equation>
<page confidence="0.960176">
238
</page>
<note confidence="0.9580105">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 238–249,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999067294117647">
Designing the model must obey certain practical
considerations. If efficiency is the major concern,
a simple model is usually chosen so that Eq. 1 can
be solved efficiently, at the cost of limited expressive
power. If we care more about accuracy, a model with
richer features and more involved score functions
may be designed. Decoding, however, will be more
expensive, and approximations are often necessary.
A typical source of intractability comes from the
combinatorial explosion inherent in the composition
of two or more tractable models (Bar-Hillel et al.,
1964; Tromble and Eisner, 2006). Recently, Rush
et al. (2010) have proposed a dual decomposition
framework to address NLP problems in which the
global score decomposes as f(y) = f1(z1)+f2(z2),
where z1 and z2 are two overlapping “views” of the
output, so that Eq. 1 becomes:
</bodyText>
<equation confidence="0.99494">
maximize f1(z1) + f2 (z2L) (2)
w.r.t. z1 E Y1, z2 E 02
s.t. z1 — z2.
</equation>
<bodyText confidence="0.792771714285714">
Above, the notation z1 — z2 means that z1 and
z2 “agree on their overlaps,” and an isomorphism
Y ^- {(z1, z2) E Y1 x Y2 1 z1 — z2} is assumed. We
next formalize these notions and proceed to compo-
sitions of an arbitrary number of models. Of special
interest is the unexplored setting where this number
is very large and each component very simple.
</bodyText>
<subsectionHeader confidence="0.990751">
2.2 Decomposition into Parts
</subsectionHeader>
<bodyText confidence="0.999739529411764">
A crucial step in the design of structured predictors
is that of decomposing outputs into parts (Taskar et
al., 2003). We assume the following setup:
Basic parts. We let R be a set of basic parts, such
that each element y E Y can be identified with a
subset of R. The exact meaning of a “basic part”
is problem dependent. For example, in dependency
parsing, R can be the set of all possible dependency
arcs (see Fig. 1); in phrase-based parsing, it can be
the set of possible spans; in sequence labeling, it can
be the set of possible labels at each position. Our
only assumption is that we can “read out” y from
the basic parts it contains. For convenience, we rep-
resent y as a binary vector, y = (y(r))r∈R, where
y(r) = 1 if part r belongs to y, and 0 otherwise.
Decomposition. We generalize the decomposition
in Eq. 2 by considering sets Y1, ... , YS for S &gt; 2.
</bodyText>
<figureCaption confidence="0.875729">
Figure 1: Parts used by our parser. Arcs are the ba-
sic parts: any dependency tree can be “read out” from
the arcs it contains. Consecutive siblings and grandpar-
ent parts introduce horizontal and vertical Markovization
(McDonald et al., 2006; Carreras, 2007). We break the
horizontal Markov assumption via all siblings parts and
the vertical one through parts which indicate a directed
path between two words. Inspired by transition-based
parsers, we also adopt head bigram parts, which look at
the heads attached to consecutive words. Finally, we fol-
low Martins et al. (2009a) and have parts which indicate
if an arc is non-projective (i.e., if it spans words that do
not descend from its head).
</figureCaption>
<bodyText confidence="0.997993888888889">
Each Ys is associated with its own set of parts Rs, in
the same sense as above; we represent the elements
of Ys as binary vectors zs = (zs(r))r∈R,. Examples
are vectors indicating a tree structure, a sequence,
or an assignment of variables to a factor, in which
case it may happen that only some binary vectors
are legal. Some parts in Rs are basic, while others
are not. We denote by ¯Rs = Rs n R the subset of
the ones that are. In addition, we assume that:
</bodyText>
<listItem confidence="0.961245285714286">
• R1, ... , RS jointly cover R, i.e., R C uSs=1 Rs;
• Only basic parts may overlap, i.e., Rs n Rt C
R, Vs, t E 11,... , S};
• Each zs E Ys is completely defined by its entries
indexed by elements of ¯Rs, from which we can
guess the ones in Rs \ ¯Rs. This implies that each
y E Y has a unique decomposition (z1, ... , zS).
</listItem>
<bodyText confidence="0.6982296">
Fig. 1 shows several parts used in dependency pars-
ing models; in phrase-based parsing, these could be
spans and production rules anchored in the surface
string; in sequence labeling, they can be unigram,
bigram, and trigram labels.1
</bodyText>
<footnote confidence="0.996743">
1There is a lot of flexibility about how to decompose the
model into S components: each set R, can correspond to a sin-
</footnote>
<page confidence="0.998448">
239
</page>
<bodyText confidence="0.996323928571428">
Global consistency. We want to be able to read
out y E Y by “gluing” together the components
(z1, ... , zS). This is only meaningful if they are
“globally consistent,” a notion which we make pre-
cise. Two components zs E Ys and zt E Yt are said
to be consistent (denoted zs ti zt) if they agree on
their overlaps, i.e., if zs(r) = zt(r), dr E Rs n Rt.
A complete assignment (z1, ... , zS) is globally con-
sistent if all pairs of components are consistent. This
is equivalent to the existence of a witness vector
(u(r))r∈R such that zs(r) = u(r), ds, r E ¯Rs.
With this setup, assuming that the score function
decomposes as f(z) = PSs=1 fs(zs), the decoding
problem (which extends Eq. 2 for S &gt; 2) becomes:
</bodyText>
<equation confidence="0.9955325">
P : maximize PSs=1 fs(zs)
w.r.t. zs //E Ys, ds
(u(r))rER E R|R|,
s.t. zs(r) = u(r), ds, r E
</equation>
<bodyText confidence="0.999990857142857">
We call the equality constraints expressed in the last
line the “agreement constraints.” It is these con-
straints that complicate the problem, which would
otherwise be exactly separable into S subproblems.
The dual decomposition method (Komodakis et al.,
2007; Rush et al., 2010) builds an approximation by
dualizing out these constraints, as we describe next.
</bodyText>
<subsectionHeader confidence="0.995835">
2.3 Dual Decomposition
</subsectionHeader>
<bodyText confidence="0.999942625">
We describe dual decomposition in a slightly differ-
ent manner than Rush et al. (2010): we will first
build a relaxation of P (called P0), in which the en-
tire approximation is enclosed. Then, we dualize P0,
yielding problem D. In the second step, the duality
gap is zero, i.e., P0 and D are equivalent.2
Relaxation. For each s E {1, ... , S} we consider
the convex hull of Ys,
</bodyText>
<equation confidence="0.896003">
Zs = ( X P(zs)zs P(zs) &gt; 0, E p(Zs) = 11 .
zs EYs Z. E&apos;9. J
(4)
gle factor in a factor graph (Smith and Eisner, 2008), or to a
</equation>
<footnote confidence="0.896133714285714">
entire subgraph enclosing several factors (Koo et al., 2010), or
even to a formula in Markov logic (Richardson and Domingos,
2006). In these examples, the basic parts may correspond to
individual variable-value pairs.
2Instead of following the path P → P&apos; → D, Rush et al.
(2010) go straight from P to D via a Lagrangian relaxation.
The two formulations are equivalent for linear score functions.
</footnote>
<bodyText confidence="0.999825">
We have that Ys = Zs n Z|Rs|; hence, problem P
(Eq. 3) is equivalent to one in which each Ys is re-
placed by Zs and the z-variables are constrained to
be integer. By dropping the integer constraints, we
obtain the following relaxed problem:
</bodyText>
<equation confidence="0.99414">
P0 : maximize PSs=1 fs(zs)
w.r.t. zs E Zs, ds
(u(r))rER E R|R|,
s.t. zs(r) = u(r), ds, r E
</equation>
<bodyText confidence="0.9993045">
If the score functions fs are convex, P0 becomes a
convex program (unlike P, which is discrete); being
a relaxation, it provides an upper bound of P.
Lagrangian. Introducing a Lagrange multiplier
λs(r) for each agreement constraint in Eq. 5, one
obtains the Lagrangian function
</bodyText>
<equation confidence="0.9978735">
L(z, u, λ) = PS �fs(zs) + Pr∈¯Rs λs(r)zs(r)�
s=1
� P �Ps:r∈¯Rs λs(r)� u(r), (6)
r∈R
</equation>
<bodyText confidence="0.729528">
and the dual problem (the master)
</bodyText>
<equation confidence="0.99417075">
D : minimize PSs=1 gs(λs)
w.r.t. λ = (λ1, ... , λS)
s.t. s:rE¯Rs λs(r) = 0, dr E R,
P (7)
</equation>
<bodyText confidence="0.999703941176471">
where the gs(λs) are the solution values of the fol-
lowing subproblems (the slaves):
We assume that strong duality holds (w.r.t. Eqs. 5–
7), hence we have P G P0 = D.3
Solving the dual. Why is the dual formulation D
(Eqs. 7–8) more appealing than P0 (Eq. 5)? The an-
swer is that the components 1, ... , S are now de-
coupled, which makes things easier provided each
slave subproblem (Eq. 8) can be solved efficiently.
In fact, this is always a concern in the mind of
the model’s designer when she chooses a decom-
position (the framework that we describe in §3,
in some sense, alleviates her from this concern).
If the score functions are linear, i.e., of the form
fs(zs) = Pr∈Rs θs(r)zs(r) for some vector θs =
(θs (r))r∈Rs, then Eq. 8 becomes a linear program,
for which a solution exists at a vertex of Zs (which
</bodyText>
<footnote confidence="0.880699">
3This is guaranteed if the score functions fs are linear.
</footnote>
<figure confidence="0.800783857142857">
(3)
Rs.
(5)
Rs.
(8)
w.r.t. zs E Zs.
maximize fs(zs) + PrE¯Rs λs(r)zs(r)
</figure>
<page confidence="0.989206">
240
</page>
<bodyText confidence="0.999347846153846">
in turn is an element of Ys). Depending on the struc-
ture of the problem, Eq. 8 may be solved by brute
force, dynamic programming, or specialized combi-
natorial algorithms (Rush et al., 2010; Koo et al.,
2010; Rush and Collins, 2011).
Applying the projected subgradient method (Ko-
modakis et al., 2007; Rush et al., 2010) to the mas-
ter problem (Eq. 7) yields a remarkably simple algo-
rithm, which at each round t solves the subproblems
in Eq. 8 for s = 1, ... , S, and then gathers these
solutions (call them zt+1
s ) to compute an “averaged”
vote for each basic part,
</bodyText>
<equation confidence="0.98435">
ut+1 (r) = δir) �s:r∈¯Rs z;+1 (r), (9)
</equation>
<bodyText confidence="0.999786666666667">
where δ(r) = |{s : r E Rs is the number of com-
ponents which contain part r. An update of the La-
grange variables follows,
</bodyText>
<equation confidence="0.998543">
λt+1 s(r) = λt s(r) − ηt(zt+1
s (r) − ut+1(r)), (10)
</equation>
<bodyText confidence="0.9999761875">
where ηt is a stepsize. Intuitively, the algorithm
pushes for a consensus among the slaves (Eq. 9),
via an adjustment of the Lagrange multipliers which
takes into consideration deviations from the aver-
age (Eq. 10). The subgradient method is guaran-
teed to converge to the solution of D (Eq. 7), for
suitably chosen stepsizes (Shor, 1985; Bertsekas et
al., 1999); it also provides a certificate of optimal-
ity in case the relaxation is tight (i.e., P = D) and
the exact solution has been found. However, con-
vergence is slow when S is large (as we will show
in the experimental section), and no certificates are
available when there is a relaxation gap (P &lt; P0).
In the next section, we describe the DD-ADMM al-
gorithm (Martins et al., 2011), which does not have
these drawbacks and shares a similar simplicity.
</bodyText>
<sectionHeader confidence="0.956499" genericHeader="method">
3 Alternating Directions Method
</sectionHeader>
<bodyText confidence="0.999814">
There are two reasons why subgradient-based dual
decomposition is not completely satisfying:
</bodyText>
<listItem confidence="0.9802105">
• it may take a long time to reach a consensus;
• it puts all its resources in solving the dual problem
D, and does not attempt to make progress in the
primal P0, which is closer to our main concern.4
</listItem>
<bodyText confidence="0.96565675">
4Our main concern is P; however solving P&apos; is often a
useful step towards that goal, either because a good rounding
scheme exists, or because one may build tighter relaxations to
approach P (Sontag et al., 2008; Rush and Collins, 2011).
Taking a look back at the relaxed primal problem
P0 (Eq. 5), we see that any primal feasible solution
must satisfy the agreement constraints. This sug-
gests that penalizing violations of these constraints
could speed up consensus.
Augmented Lagrangian. By adding a penalty
term to Eq. 6, we obtain the augmented Lagrangian
function (Hestenes, 1969; Powell, 1969):
</bodyText>
<equation confidence="0.9856975">
Ap(z, u, λ) = L(z, u, λ) − ρ
2
r∈Rs
(11)
</equation>
<bodyText confidence="0.999746916666667">
where the parameter ρ &gt; 0 controls the intensity
of the penalty. Augmented Lagrangian methods
are well-known in the optimization community (see,
e.g., Bertsekas et al. (1999), §4.2). They alternate
updates to the λ-variables, while seeking to maxi-
mize Aρ with respect to z and u. In our case, how-
ever, this joint maximization poses difficulties, since
the penalty term couples the two variables. The al-
ternating directions method of multipliers (ADMM),
coined by Gabay and Mercier (1976) and Glowinski
and Marroco (1975), sidesteps this issue by perform-
ing alternate maximizations,
</bodyText>
<equation confidence="0.96634525">
zt+1 = arg max
z
ut+1 = arg max
u
</equation>
<bodyText confidence="0.999886833333333">
followed by an update of the Lagrange multipliers
as in Eq. 10. Recently, ADMM has attracted inter-
est, being applied in a variety of problems; see the
recent book by Boyd et al. (2011) for an overview.
As derived in the App. A, the u-updates in Eq. 13
have a closed form, which is precisely the averag-
ing operation performed by the subgradient method
(Eq. 9). We are left with the problem of comput-
ing the z-updates. Like in the subgradient approach,
the maximization in Eq. 12 can be separated into S
independent slave subproblems, which now take the
form:
</bodyText>
<equation confidence="0.9912415">
maximize fs(zs) + r∈¯Rs λs(r)zs(r)
−ρ �r∈¯Rs(zs(r) − ut(r))2
2
w.r.t. zs E Zs(x).
</equation>
<bodyText confidence="0.9983245">
Comparing Eq. 8 and Eq. 14, we observe that the
only difference is the presence in the latter of a
</bodyText>
<equation confidence="0.9939425">
S
s=1
(zs(r) − u(r))2,
Aρ(z, ut, λt), (12)
Aρ(zt+1, u, λt), (13)
(14)
</equation>
<page confidence="0.961828">
241
</page>
<bodyText confidence="0.99998324137931">
quadratic term which regularizes towards the pre-
vious averaged votes ut(r). Because of this term,
the solution of Eq. 14 for linear score functions may
not be at a vertex (in contrast to the subgradient
method). We devote §4 to describing exact and effi-
cient ways of solving the problem in Eq. 14 for im-
portant, widely used slaves. Before going into de-
tails, we mention another advantage of ADMM over
the subgradient algorithm: it knows when to stop.
Primal and dual residuals. Recall that the sub-
gradient method provides optimality certificates
when the relaxation is tight (P = P&apos;) and an ex-
act solution of P has been found. While this is good
enough when tight relaxations are frequent, as in the
settings explored by Rush et al. (2010), Koo et al.
(2010), and Rush and Collins (2011), it is hard to
know when to stop when a relaxation gap exists.
We would like to have similar guarantees concern-
ing the relaxed primal P&apos;.5 A general weakness of
subgradient algorithms is that they do not have this
capacity, and so are usually stopped by specifying a
maximum number of iterations. In contrast, ADMM
allows to keep track of primal and dual residuals
(Boyd et al., 2011). This allows providing certifi-
cates not only for the exact solution of P (when the
relaxation is tight), but also to terminate when a near
optimal solution of the relaxed problem P&apos; has been
found. Theprimal residual rt P measures the amount
by which the agreement constraints are violated:
</bodyText>
<equation confidence="0.993219">
P 15)
rER δ(r)
</equation>
<bodyText confidence="0.997930666666667">
the dual residual rt D is the amount by which a dual
optimality condition is violated (see Boyd et al.
(2011), p.18, for details). It is computed via:
</bodyText>
<equation confidence="0.987926">
r __ t PrER δ(r)(ut(r) − ut−1(r))2 (16)
D ErER δ(r)
</equation>
<bodyText confidence="0.8989493">
Our stopping criterion is thus that these two residu-
als are below a threshold, e.g., 1 x 10−3. The com-
plete algorithm is depicted as Alg. 1. As stated in
5This problem is more important than it may look. Problems
with many slaves tend to be less exact, hence relaxation gaps
are frequent. Also, when decoding is embedded in training, it is
useful to obtain the fractional solution of the relaxed primal P
(rather than an approximate integer solution). See Kulesza and
Pereira (2007) and Martins et al. (2009b) for details.
Algorithm 1 ADMM-based Dual Decomposition
</bodyText>
<listItem confidence="0.740991692307692">
1: input: score functions (fs(.))Ss=1, parameters ρ, η,
thresholds cP and ED.
2: initialize t +- 1
3: initialize u1(r) +- 0.5 and λ1 s(r) +- 0, ds, dr E ¯Rs
4: repeat
5: for each s = 1,...,S do
6: make a zs-update, yielding zt+1
s (Eq. 14)
7: end for
8: make a u-update, yielding ut+1 (Eq. 9)
9: make a λ-update, yielding λt+1 (Eq. 10)
10: t +- t + 1
11: until rt+1
</listItem>
<equation confidence="0.329591">
P &lt; cP and rt+1
D &lt; ED (Eqs. 15–16)
12: output: relaxed primal and dual solutions u, z, λ
</equation>
<bodyText confidence="0.9978524">
Martins et al. (2011), convergence to the solution of
P&apos; is guaranteed with a fixed stepsize ηt = τρ, with
τ E [1, 1.618] (Glowinski and Le Tallec, 1989, Thm.
4.2). In our experiments, we set τ = 1.5, and adapt
ρ as described in (Boyd et al., 2011, p.20).6
</bodyText>
<sectionHeader confidence="0.966289" genericHeader="method">
4 Solving the Subproblems
</sectionHeader>
<bodyText confidence="0.999217928571429">
In this section, we address the slave subproblems of
DD-ADMM (Eq. 14). We show how these subprob-
lems can be solved efficiently for several important
cases that arise in NLP applications. Throughout,
we assume that the score functions fs are linear, i.e.,
they can be written as fs(zs) = P rERs θs(r)zs(r).
This is the case whenever a linear model is used, in
which case θs(r) = 1
δ(r)w · O(x, r), where w is a
weight vector and O(x, r) is a feature vector. It is
also the scenario studied in previous work in dual
decomposition (Rush et al., 2010). Under this as-
sumption, and discarding constant terms, the slave
subproblem in Eq. 14 becomes:
</bodyText>
<table confidence="0.453415">
Xmax Rs θs(r)zs(r) − ρX (zs(r) − as(r))2.
zsEZs 2
rERs\ rE¯Rs
(17)
</table>
<bodyText confidence="0.928552666666667">
where as(r) = ut(r)+ρ−1(θs(r)+λts(r)). Since Zs
is a polytope, Eq. 17 is a quadratic program, which
can be solved with a general purpose solver. How-
ever, that does not exploit the structure of Zs and is
inefficient when |Rs |is large. We next show that for
many cases, a closed-form solution is available and
6Briefly, we initialize p = 0.03 and then increase/decrease
p by a factor of 2 whenever the primal residual becomes &gt; 10
times larger/smaller than the dual residual.
</bodyText>
<equation confidence="0.995706666666667">
t Ps 1 PrE¯Rs(zts(r) − ut(r))2
rP =
; (
</equation>
<page confidence="0.987112">
242
</page>
<bodyText confidence="0.852280333333333">
can be computed in O(|Rs|) time, up to log factors.7
Pairwise Factors. This is the case where RPAIR =
{r1, r2, r12}, where r1 and r2 are basic parts and
r12 is their conjunction, i.e., we have YPAIR =
{hz1, z2, z12i  |z12 = z1 ∧ z2}. This factor is use-
ful to make conjunctions of variables participate in
the score function (see e.g. the grandparent, sibling,
and head bigram parts in Fig. 1). The convex hull
of YPAIR is the polytope ZPAIR = {hz1, z2, z12i ∈
[0, 1]3  |z12 ≤ z1, z12 ≤ z2, z12 ≥ z1 + z2 − 1}, as
shown by Martins et al. (2010). In this case, problem
(17) can be written as
</bodyText>
<equation confidence="0.988308666666667">
max θ12z12 − ρ2[(z1 − a1)2 + (z2 − a2)2]
w.r.t. hz1, z2, z12i ∈ [0, 1]3 (18)
s.t. z12 ≤ z1, z12 ≤ z2, z12 ≥ z1 + z2 − 1
</equation>
<bodyText confidence="0.996874857142857">
and has a closed form solution (see App. B).
Uniqueness Quantification and XOR. Many
problems involve constraining variables to take a
single value: for example, in dependency parsing,
a modifier can only take one head. This can be
expressed as the statement ∃!y : Q(y) in first-order
logic,8 or as a one-hot XOR factor in a factor
graph (Smith and Eisner, 2008; Martins et al.,
2010). In this case, RXOR = {r1, ... , rn}, and
YXOR = {hz1, ... , zni ∈ {0, 1}n  |Pni=1 zi = 1}.
The convex hull of YXOR is ZXOR = {hz1, ... , zni ∈
[0, 1]n  |Pn i=1 zi = 1}. Assume for the sake of
simplicity that all parts in RXOR are basic.9 Up to a
constant, the slave subproblem becomes:
</bodyText>
<equation confidence="0.77535425">
minimize 1 Pn i=1(zi − ai)2
2
w.r.t. hz1, ... , zni ∈ [0,1]n (19)
s.t. Pi zi = 1.
</equation>
<bodyText confidence="0.999732333333333">
This is the problem of projecting onto the probabil-
ity simplex, which can be done in O(n log n) time
via a sort operation (see App. C).10
</bodyText>
<footnote confidence="0.9880424">
7This matches the asymptotic time that would be necessary
to solve the corresponding problems in the subgradient method,
for which algorithms are straightforward to derive. The point is
that with ADMM fewer instances of these subproblems need to
be solved, due to faster convergence of the master problem.
8The symbol ∃! means “there is one and only one.”
9A similar derivation can be made otherwise.
10Also common is the need for constraining existence of “at
most one” element. This can be reduced to uniqueness quantifi-
cation by adding a dummy NULL label.
</footnote>
<bodyText confidence="0.994660444444445">
Existential Quantification and OR. Sometimes,
only existence is required, not necessarily unique-
ness. This can be expressed with disjunctions, ex-
istential quantifiers in first-order logic (∃y : Q(y)),
or as a OR factor. In this case, ROR = {r1, ... , rn},
YOR = {hz1, ... , zni ∈ {0,1}n  |Wni=1 zi = 1},
and the convex hull is ZOR = {hz1, ... , zni ∈
[0, 1]n  |Pni=1 zi ≥ 1} (see Tab. 1 in Martins et al.
(2010)). The slave subproblem becomes:
</bodyText>
<equation confidence="0.929521">
minimize 1 Pn i=1(zi − ai)2
2
w.r.t. Pn
hz1, ... , zni ∈ [0,1]n
s.t. i zi ≥ 1.
</equation>
<bodyText confidence="0.999992571428572">
We derive a procedure in App. D to compute this
projection in O(n log n) runtime, also with a sort.
Negations. The two cases above can be extended
to allow some of their inputs to be negated. By a
change of variables in Eqs. 19–20 it is possible to
reuse the same black box that solves those problems.
The procedure is as follows:
</bodyText>
<listItem confidence="0.9929975">
1. For i = 1, ... , n, set a0i = 1−ai if the ith variable
is negated, and a0i = ai otherwise.
2. Obtain hz01, ... , z0ni as the solution of Eqs. 19 or
20 providing ha01, ... , a0ni as input.
3. For i = 1, ... , n, set zi = 1−z0 i if the ith variable
is negated, and zi = z0 i otherwise.
</listItem>
<bodyText confidence="0.981579894736842">
The ability to handle negated variables adds a
great degree of flexibility. From De Morgan’s
laws, we can now handle conjunctions and impli-
cations (since Vni=1 Qi(x) ⇒ R(x) is equivalent to
Wni=1 ¬Qi(x) ∨ R(x)).
Logical Variable Assignments. All previous ex-
amples involve taking a group of existing variables
and defining a constraint. Alternatively, we may
want to define a new variable which is the result of
an operation involving other variables. For exam-
ple, R(x) := ∃!y : Q(x, y). This corresponds to the
XOR-WITH-OUTPUT factor in Martins et al. (2010).
Interestingly, this can be expressed as a XOR where
R(x) is negated (i.e., either ¬R(x) holds or exactly
one y satisfies Q(x, y), but not both).
A more difficult problem is that of the OR-WITH-
OUTPUT factor, expressed by the formula R(x) :=
∃y : Q(x, y). We have ROR-OUT = {r0, ... , rn},{
and YOR-OUT = hz0, ... , zni ∈ {0,1}n  |z0 =
</bodyText>
<figure confidence="0.452702">
(20)
</figure>
<page confidence="0.995264">
243
</page>
<table confidence="0.999889866666667">
# Slaves Runtime Description
Tree �I!h : arc(h, m), m =�0 O(n) O(n log n) Each non-root word has a head
flow(h, m, k) ==&gt; arc(h, m) O(n3) O(1) Only active arcs may carry flow
path(m, d) := �I!h : flow(h, m, d), m =� 0 O(n2) O(nlogn) Paths and flows are consistent
path(h, d) := �I!m : flow(h, m, d) O(n log n) (see Martins et al. (2010))
path(0, m) := TRUE, flow(h, m, m) := TRUE
All siblings sibl(h, m, s) := arc(h, m) n arc(h, s) O(1) By definition
Grandp. grand(g, h, m) := arc(g, h) n arc(h, m) O(n3) O(1) By definition
Head Bigram bigram(b, h, m) := arc(b, m — 1) n arc(h, m), m =� 0 O(n3) O(1) By definition
Consec. Sibl. lastsibl(h, m, m) := arc(h, m) O(n2) O(n log n) Head automaton model
�I!m E [h, k] : lastsibl(h, m, k) O(1) (see supplementary material)
lastsibl(h, m, k) := lastsibl(h, m, k + 1) O(nlogn)
® nextsibl(h, m, k + 1)
arc(h, m) := �1!s E [h, m] : nextsibl(h, s, m)
Nonproj. Arc nonproj(h, m) := arc(h, m) n �Ik E [h, m] : —path(h, k) O(n2) O(n log n) By definition
</table>
<tableCaption confidence="0.9854695">
Table 1: First-order logic formulae underlying our dependency parser. The basic parts are the predicate variables
arc(h, m) (indicating an arc linking head h to modifier m), path(a, d) (indicating a directed path from ancestor
</tableCaption>
<bodyText confidence="0.7626094">
a to descendant d), nextsibl(h, m, s) (indicating that (h, m) and (h, s) are consecutive siblings), nonproj(h, m)
(indicating that (h, m) is a non-projective arc), as well as the auxiliary variables flow(h, m, d) (indicating that arc
(h, m) carries flow to d), and lastsibl(h, m, k) (indicating that, up to position k, the last seen modifier of h occurred
at position m). The non-basic parts are the pairwise factors sibl(h, m, s), grand(g, h, m), and bigram(b, h, m); as
well as each logical formula. Columns 3–4 indicate the number of parts of each kind, and the time complexity for
solving each subproblem. For a sentence of length n, there are O(n3) parts and the total complexity is O(n3 log n).
Vni=1 zi}. The convex hull of YOR-OUT is the follow-
ing set: ZOR-OUT = I(z0, ... , zn) E [0, 1]n I z0 &gt;
En i=1 zi, z0 � zi, bi = 1, . . . ,n} (Martins et al.,
2010, Tab.1). The slave subproblem is:
</bodyText>
<equation confidence="0.9742405">
minimize 1 En i=0(zi − ai)2
2
w.r.t. (z0, ... , zn) E [0, 1]n
s.t. z0 &gt; En=1 zi;
z0 &lt; zi, bi = 1, ... , n.
— — (21)
</equation>
<bodyText confidence="0.999141958333333">
The problem in Eq. 21 is more involved than the
ones in Eqs. 19–20. Yet, there is still an efficient
procedure with runtime O(n log n) (see App. E).
By using the result above for negated variables, we
are now endowed with a procedure for many other
cases, such that AND-WITH-OUTPUT and formu-
las with universal quantifiers (e.g., R(x) := by :
Q(x, y)). Up to a log-factor, the runtimes will be
linear in the number of predicates.
Larger Slaves. The only disadvantage of DD-
ADMM in comparison with the subgradient algo-
rithm is that there is not an obvious way of solving
the subproblem in Eq. 14 exactly for large combi-
natorial factors, such as the TREE constraint in de-
pendency parsing, or a sequence model. Hence, our
method seems to be more suitable for decomposi-
tions which involve “simple slaves,” even if their
number is large. However, this does not rule out the
possibility of using this method otherwise. Eckstein
and Bertsekas (1992) show that the ADMM algo-
rithm may still converge when the z-updates are in-
exact. Hence the method may still work if the slaves
are solved numerically up to some accuracy. We de-
fer this to future investigation.
</bodyText>
<sectionHeader confidence="0.993237" genericHeader="method">
5 Experiments: Dependency Parsing
</sectionHeader>
<bodyText confidence="0.94150425">
We used 14 datasets with non-projective depen-
dencies from the CoNLL-2006 and CoNLL-2008
shared tasks (Buchholz and Marsi, 2006; Surdeanu
et al., 2008). We also used a projective English
dataset derived from the Penn Treebank by applying
the standard head rules of Yamada and Matsumoto
(2003).11 We did not force the parser to output pro-
jective trees or unique roots for any of the datasets;
everything is learned from the data. We trained by
running 10 iterations of the cost-augmented MIRA
algorithm (Crammer et al., 2006) with LP-relaxed
decoding, as in Martins et al. (2009b). Follow-
ing common practice (Charniak and Johnson, 2005;
Carreras et al., 2008), we employed a coarse-to-fine
procedure to prune away unlikely candidate arcs, as
described by Koo and Collins (2010). To ensure
valid parse trees at test time, we rounded fractional
11As usual, we train on sections §02–21, use §22 as validation
data, and test on §23. We ran SVMTool (Gim´enez and Marquez,
2004) to obtain automatic part-of-speech tags for §22–23.
</bodyText>
<page confidence="0.996761">
244
</page>
<bodyText confidence="0.9994138">
solutions as described in Martins et al. (2009a) (yet,
solutions were integral most of the time).
The parts used in our full model are the ones
depicted in Fig. 1. Note that a subgradient-based
method could handle some of those parts efficiently
(arcs, consecutive siblings, grandparents, and head
bigrams) by composing arc-factored models, head
automata, and a sequence labeler. However, no
lightweight decomposition seems possible for incor-
porating parts for all siblings, directed paths, and
non-projective arcs. Tab. 1 shows the first-order
logical formulae that encode the constraints in our
model. Each formula gives rise to a subproblem
which is efficiently solvable (see §4). By ablating
some of rows of Tab. 1 we recover known methods:
</bodyText>
<listItem confidence="0.907947875">
• Resorting to the tree and consecutive sibling for-
mulae gives one of the models in Koo et al.
(2010), with the same linear relaxation (a proof
of this fact is included in App. F);
• Resorting to tree, all siblings, grandparent, and
non-projective arcs, recovers a multi-commodity
flow configuration proposed by Martins et al.
(2009a); the relaxation is also the same.12
</listItem>
<bodyText confidence="0.9934676">
The experimental results are shown in Tab. 2.
For comparison, we include the best published re-
sults for each dataset (at the best of our knowledge),
among transition-based parsers (Nivre et al., 2006;
Huang and Sagae, 2010), graph-based parsers (Mc-
Donald et al., 2006; Koo and Collins, 2010), hybrid
methods (Nivre and McDonald, 2008; Martins et al.,
2008), and turbo parsers (Martins et al., 2010; Koo
et al., 2010). Our full model achieved the best re-
ported scores for 7 datasets. The last two columns
show a consistent improvement (with the exceptions
of Chinese and Arabic) when using the full set of
features over a second order model with grandparent
and consecutive siblings, which is our reproduction
of the model of Koo et al. (2010).13
12Although Martins et al. (2009a) also incorporated consec-
utive siblings in one of their configurations, our constraints are
tighter than theirs. See App. F.
13Note however that the actual results of Koo et al. (2010)
are higher than our reproduction, as can be seen in the second
column. The differences are due to the features that were used
and on the way the models were trained. The cause is not search
error: exact decoding with an ILP solver (CPLEX) revealed no
significant difference with respect to our G+CS column. We
leave further analysis for future work.
</bodyText>
<table confidence="0.9998348125">
Best known UAS G+CS Full
Arabic 80.18 [Ma08] 81.12 81.10 (-0.02)
Bulgar. 92.88 [Ma10] 93.04 93.50 (+0.46)
Chinese 91.89 [Ma10] 91.05 90.62 (-0.43)
Czech 88.78 [Ma10] 88.80 89.46 (+0.66)
English 92.57 [Ko10] 92.45 92.68 (+0.23)
Danish 91.78 [Ko10] 91.70 91.86 (+0.16)
Dutch 85.81 [Ko10] 84.77 85.53 (+0.76)
German 91.49 [Ma10] 91.29 91.89 (+0.60)
Japane. 93.42 [Ma10] 93.62 93.72 (+0.10)
Portug. 93.03 [Ko10] 92.05 92.29 (+0.24)
Slovene 86.21 [Ko10] 86.09 86.95 (+0.86)
Spanish 87.04 [Ma10] 85.99 86.74 (+0.75)
Swedish 91.36 [Ko10] 89.94 90.16 (+0.22)
Turkish 77.55 [Ko10] 76.24 76.64 (+0.40)
PTB §23 93.04 [KC10] 92.19 92.53 (+0.34)
</table>
<tableCaption confidence="0.959684">
Table 2: Unlabeled attachment scores, excluding punc-
tuation. In the second column, [Ma08] denotes Martins
</tableCaption>
<bodyText confidence="0.636028166666667">
et al. (2008), [KC10] is Koo and Collins (2010), [Ma10]
is Martins et al. (2010), and [Ko10] is Koo et al. (2010).
In columns 3–4, “Full” is our full model, and “G+CS” is
our reproduction of the model of Koo et al. (2010), i.e.,
the same as “Full” but with all features ablated excepted
for grandparents and consecutive siblings.
</bodyText>
<table confidence="0.996741333333333">
AF +G+CS +AS +NP Full
PTB §22 91.02 92.13 92.32 92.36 92.41
PTB §23 91.36 92.19 92.41 92.50 92.53
</table>
<tableCaption confidence="0.6305125">
Table 3: Feature ablation experiments. AF is an arc-
factored model; +G+CS adds grandparent and consec-
utive siblings; +AS adds all-siblings; +NP adds non-
projective arcs; Full adds the bigram and directed paths.
</tableCaption>
<bodyText confidence="0.869999">
Feature ablation and error analysis. We con-
ducted a simple ablation study by training several
models on the English PTB with different sets of
features. Tab. 3 shows the results. As expected, per-
formance keeps increasing as we use models with
greater expressive power. We show some concrete
examples in App. G of sentences that the full model
parsed correctly, unlike less expressive models.
Convergence speed and optimality. Fig. 2 com-
pares the performance of DD-ADMM and the sub-
gradient algorithms in the validation section of the
PTB.14 For the second order model, the subgradient
14The learning rate in the subgradient method was set as 77t =
770/(1+Nincr(t)), as in Koo et al. (2010), where Nincr(t) is the
number of dual increases up to the tth iteration, and 770 is chosen
to maximize dual decrease after 20 iterations (in a per sentence
basis). Those preliminary iterations are not plotted in Fig. 2.
</bodyText>
<page confidence="0.996993">
245
</page>
<bodyText confidence="0.938366318181818">
method has more slaves than in Koo et al. (2010):
it has a slave imposing the TREE constraint (whose
subproblems consists on finding a minimum span-
ning tree) and several for the all-sibling parts, yield-
ing an average number of 310.5 and a maximum
of 4310 slaves. These numbers are still manage-
able, and we observe that a “good” UAS is achieved
relatively quickly. The ADMM method has many
more slaves due to the multicommodity flow con-
straints (average 1870.8, maximum 65446), yet it
attains optimality sooner, as can be observed in the
right plot. For the full model, the subgradient-based
method becomes extremely slow, and the UAS score
severely degrades (after 1000 iterations it is 2%
less than the one obtained with the ADMM-based
method, with very few instances having been solved
to optimality). The reason is the number of slaves:
in this configuration and dataset the average number
of slaves per instance is 3327.4, and the largest num-
ber is 113207. On the contrary, the ADMM method
keeps a robust performance, with a large fraction of
optimality certificates in early iterations.
Runtime and caching strategies. Despite its suit-
ability to problems with many overlapping compo-
nents, our parser is still 1.6 times slower than Koo
et al. (2010) (0.34 against 0.21 sec./sent. in PTB
§23), and is far beyond the speed of transition-based
parsers (e.g., Huang and Sagae (2010) take 0.04
sec./sent. on the same data, although accuracy is
lower, 92.1%). Our implementation, however, is not
fully optimized. We next describe how considerable
speed-ups are achieved by caching the subproblems,
following a strategy similar to Koo et al. (2010).
Fig. 3 illustrates the point. After a few iterations,
many variables u(r) see a consensus being achieved
(i.e., ut(r) = zt+1
s (r), ∀s) and enter an idle state:
they are left unchanged by the u-update in Eq. 9,
and so do the Lagrange variables At+1
s (r) (Eq. 10).
If by iteration t all variables in a subproblem s are
idle, then zt+1
s (r) = zts(r), hence the subproblem
does not need to be resolved.15 Fig. 3 shows that
</bodyText>
<footnote confidence="0.871732142857143">
15Even if not all variables are idle in s, caching may still be
useful: note that the z-updates in Eq. 14 tend to be sparse for the
subproblems described in §4 (these are Euclidean projections
onto polytopes with 0/1 vertices, which tend to hit corners). An-
other trick that may accelerate the algorithm is warm-starting:
since many subproblems involve a sort operation, storing the
sorted indexes may speedup the next round.
</footnote>
<figure confidence="0.743084125">
Full ADMM
% active msgs
% active subproblems
% active vars
40
20
200 400 600 800 1000
Iterations
</figure>
<figureCaption confidence="0.99543525">
Figure 3: Fraction of active variables, subproblems and
messages along DD-ADMM iterations (full model). The
number of active messages denotes the total number of
variables (active or not) that participate in an active factor.
</figureCaption>
<figure confidence="0.99727475">
Elapsed Times
101
100
10-1
10-2
10-3
10-3 10-2 10-1 100 101
Time ADMM (sec.)
</figure>
<figureCaption confidence="0.835701333333333">
Figure 4: Runtimes of DD-ADMM and CPLEX on PTB
§22 (each point is a sentence). Average runtimes are
0.362 (DD-ADMM) and 0.565 sec./sent. (CPLEX).
</figureCaption>
<bodyText confidence="0.999926">
many variables and subproblems are left untouched
after the first few rounds.
Finally, Fig. 4 compares the runtimes of our im-
plementation of DD-ADMM with those achieved by
a state-of-the-art LP solver, CPLEX, in its best per-
forming configuration: the simplex algorithm ap-
plied to the dual LP. We observe that DD-ADMM
is faster in some regimes but slower in others. For
short sentences (&lt; 15 words), DD-ADMM tends to
be faster. For longer sentences, CPLEX is quite ef-
fective as it uses good heuristics for the pivot steps
in the simplex algorithm; however, we observed that
it sometimes gets trapped on large problems. Note
also that DD-ADMM is not fully optimized, and that
it is much more amenable to parallelization than the
simplex algorithm, since it is composed of many in-
dependent slaves. This suggests potentially signifi-
cant speed-ups in multi-core environments.
</bodyText>
<sectionHeader confidence="0.999882" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.992452666666667">
Riedel and Clarke (2006) first formulated depen-
dency parsing as an integer program, along with
logical constraints. The multicommodity flow for-
</bodyText>
<figure confidence="0.989470702702703">
% active
100
80
60
0
Time CPLEX (sec.)
246
92
91
90
UAS (%)
89
88
87
86
ADMM Full
Subgrad Full
ADMM Sec Ord
Subgrad Sec Ord
850 200 400 600 800 1000
Iterations
Stopping Criteria
0 200 400 600 800 1000
Iterations
Certificates (%) 100
80
60
40
20
0
ADMM Full (Tol&lt;0.001)
ADMM Full (Exact)
Subgrad Full (Exact)
ADMM Sec Ord (Tol&lt;0.001)
ADMM Sec Ord (Exact)
Subgrad Sec Ord (Exact)
Accuracy
</figure>
<figureCaption confidence="0.997145857142857">
Figure 2: UAS including punctuation (left) and fraction of optimality certificates (right) accross iterations of the
subgradient and DD-ADMM algorithms, in PTB §22. “Full” is our full model; “Sec Ord” is a second-order model
with grandparents and all siblings, for which the subgradient method uses a coarser decomposition with a TREE factor.
Since subgradient and DD-ADMM are solving the same problems, the solid lines (as the dashed ones) would meet in
the limit, however subgradient converges very slowly for the full model. The right plot shows optimality certificates
for both methods, indicating that an exact solution of P has been found; for DD-ADMM we also plot the fraction of
instances that converged to an accurate solution of P0 (primal and dual residuals &lt; 10−3) and hence can be stopped.
</figureCaption>
<bodyText confidence="0.999921111111111">
mulation was introduced by Martins et al. (2009a),
along with some of the parts considered here. Koo
et al. (2010) proposed a subgradient-based dual de-
composition method that elegantly combines head
automata with maximum spanning tree algorithms;
these parsers, as well as the loopy belief propagation
method of Smith and Eisner (2008), are all instances
of turbo parsers (Martins et al., 2010).
DD-ADMM has been proposed and theoretically
analyzed by Martins et al. (2011) for problems rep-
resentable as factor graphs. The general ADMM
method has a long-standing history in optimization
(Hestenes, 1969; Powell, 1969; Glowinski and Mar-
roco, 1975; Gabay and Mercier, 1976; Boyd et al.,
2011). Other methods have been recently proposed
to accelerate dual decomposition, such as Jojic et al.
(2010) and Meshi and Globerson (2011) (the latter
applying ADMM in the dual rather than the primal).
While our paper shows limitations of the sub-
gradient method when there are many overlapping
components, this method may still be advantageous
over ADMM in problems that are nicely decom-
posable, since it often allows reusing existing com-
binatorial machinery. Yet, the scenario we con-
sider here is realistic in NLP, where we often have
to deal with not-lightly-decomposable constrained
problems (e.g., exploiting linguistic knowledge).
</bodyText>
<sectionHeader confidence="0.998773" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999953730769231">
We have introduced new feature-rich turbo parsers.
Since exact decoding is intractable, we solve an LP
relaxation through a recently proposed consensus al-
gorithm, DD-ADMM, which is suitable for prob-
lems with many overlapping components. We study
the empirical runtime and convergence properties of
DD-ADMM, complementing the theoretical treat-
ment in Martins et al. (2011). DD-ADMM com-
pares favourably against the subgradient method in
several aspects: it is faster to reach a consensus, it
has better stopping conditions, and it works better
in non-lightweight decompositions. While its slave
subproblems are more involved, we derived closed-
form solutions for many cases of interest, such as
first-order logic formulas and combinatorial factors.
DD-ADMM may be useful in other frameworks
involving logical constraints, such as the models
for compositional semantics presented by Liang
et al. (2011). Non-logical constraints may also
yield efficient subproblems, e.g., the length con-
straints in summarization and compression (Clarke
and Lapata, 2008; Martins and Smith, 2009; Berg-
Kirkpatrick et al., 2011). Finally, DD-ADMM can
be adapted to tighten its relaxations towards exact
decoding, as in Sontag et al. (2008) and Rush and
Collins (2011). We defer this for future work.
</bodyText>
<sectionHeader confidence="0.998025" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.87411075">
We thank all reviewers for their comments, Eric Xing for
helpful discussions, and Terry Koo and Sasha Rush for
answering questions about their parser and for providing
code. A. M. was supported by a FCT/ICTI grant through
the CMU-Portugal Program, and by Priberam. This
work was partially supported by the FET programme
(EU FP7), under the SIMBAD project (contract 213250).
N. S. was supported by NSF CAREER IIS-1054319.
</bodyText>
<page confidence="0.995967">
247
</page>
<sectionHeader confidence="0.982617" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999554276190476">
M. Auli and A. Lopez. 2011. A Comparison of Loopy
Belief Propagation and Dual Decomposition for Inte-
grated CCG Supertagging and Parsing. In Proc. of
ACL.
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On for-
mal properties of simple phrase structure grammars.
Language and Information: Selected Essays on their
Theory and Application, pages 116–150.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proc. of ACL.
D. Bertsekas, W. Hager, and O. Mangasarian. 1999.
Nonlinear programming. Athena Scientific.
D.P. Bertsekas, A. Nedic, and A.E. Ozdaglar. 2003. Con-
vex analysis and optimization. Athena Scientific.
S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein.
2011. Distributed Optimization and Statistical Learn-
ing via the Alternating Direction Method of Multipli-
ers. Now Publishers (to appear).
J.P. Boyle and R.L. Dykstra. 1986. A method for find-
ing projections onto the intersections of convex sets in
Hilbert spaces. In Advances in order restricted statis-
tical inference, pages 28–47. Springer Verlag.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In CoNLL.
X. Carreras, M. Collins, and T. Koo. 2008. TAG, Dy-
namic Programming, and the Perceptron for Efficient,
Feature-rich Parsing. In CONLL.
X. Carreras. 2007. Experiments with a higher-order pro-
jective dependency parser. In CoNLL.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking. In
Proc. ACL, pages 173–180. Association for Computa-
tional Linguistics Morristown, NJ, USA.
D. Chiang. 2007. Hierarchical phrase-based translation.
computational linguistics, 33(2):201–228.
J. Clarke and M. Lapata. 2008. Global Inference for Sen-
tence Compression An Integer Linear Programming
Approach. JAIR, 31:399–429.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and
Y. Singer. 2006. Online Passive-Aggressive Algo-
rithms. JMLR, 7:551–585.
J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra.
2008. Efficient projections onto the L1-ball for learn-
ing in high dimensions. In ICML.
J. Eckstein and D. Bertsekas. 1992. On the Douglas-
Rachford splitting method and the proximal point al-
gorithm for maximal monotone operators. Mathemat-
ical Programming, 55(1):293–318.
J. R. Finkel, A. Kleeman, and C. D. Manning. 2008. Effi-
cient, feature-based, conditional random field parsing.
Proceedings of ACL-08: HLT, pages 959–967.
D. Gabay and B. Mercier. 1976. A dual algorithm for
the solution of nonlinear variational problems via finite
element approximation. Computers and Mathematics
with Applications, 2(1):17–40.
J. Gim´enez and L. Marquez. 2004. Svmtool: A gen-
eral pos tagger generator based on support vector ma-
chines. In Proc. of LREC.
R. Glowinski and P. Le Tallec. 1989. Augmented La-
grangian and operator-splitting methods in nonlinear
mechanics. Society for Industrial Mathematics.
R. Glowinski and A. Marroco. 1975. Sur
l’approximation, par ´el´ements finis d’ordre un, et la
r´esolution, par penalisation-dualit´e, d’une classe de
probl`emes de Dirichlet non lin´eaires. Rev. Franc. Au-
tomat. Inform. Rech. Operat., 9:41–76.
M. Hestenes. 1969. Multiplier and gradient methods.
Jour. Optim. Theory andApplic., 4:302–320.
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proc. of ACL,
pages 1077–1086.
R. Johansson and P. Nugues. 2008. Dependency-based
Semantic Role Labeling of PropBank. In EMNLP.
V. Jojic, S. Gould, and D. Koller. 2010. Accelerated dual
decomposition for MAP inference. In ICML.
N. Komodakis, N. Paragios, and G. Tziritas. 2007.
MRF optimization via dual decomposition: Message-
passing revisited. In ICCV.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proc. of ACL, pages 1–11.
T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In EMNLP.
A. Kulesza and F. Pereira. 2007. Structured Learning
with Approximate Inference. NIPS.
P. Liang, M.I. Jordan, and D. Klein. 2011. Learning
dependency-based compositional semantics. In Proc.
Association for Computational Linguistics (ACL).
A. F. T. Martins and N. A. Smith. 2009. Summarization
with a joint model for sentence extraction and com-
pression. In NAACL-HLT Workshop on Integer Linear
Programming for NLP.
A. F. T. Martins, D. Das, N. A. Smith, and E. P. Xing.
2008. Stacking dependency parsers. In EMNLP.
A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009a.
Concise integer linear programming formulations for
dependency parsing. In ACL-IJCNLP.
A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009b.
Polyhedral outer approximations with application to
natural language parsing. In ICML.
A. F. T. Martins, N. A. Smith, E. P. Xing, M. A. T.
Figueiredo, and P. M. Q. Aguiar. 2010. Turbo parsers:
Dependency parsing by approximate variational infer-
ence. In EMNLP.
</reference>
<page confidence="0.965878">
248
</page>
<reference confidence="0.999776857142857">
A. F. T. Martins, M. A. T. Figueiredo, P. M. Q. Aguiar,
N. A. Smith, and E. P. Xing. 2011. An Augmented
Lagrangian Approach to Constrained MAP Inference.
In ICML.
R. McDonald, K. Lerman, and F. Pereira. 2006. Multi-
lingual dependency analysis with a two-stage discrim-
inative parser. In CoNLL.
O. Meshi and A. Globerson. 2011. An Alternating Direc-
tion Method for Dual MAP LP Relaxation. In ECML
PKDD.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
ACL-HLT.
J. Nivre, J. Hall, J. Nilsson, G. Eryiˇgit, and S. Marinov.
2006. Labeled pseudo-projective dependency parsing
with support vector machines. In Procs. of CoNLL.
S. Petrov and D. Klein. 2008. Sparse multi-scale gram-
mars for discriminative latent variable parsing. In
Proc. of EMNLP.
M. Powell. 1969. A method for nonlinear constraints in
minimization problems. In R. Fletcher, editor, Opti-
mization, pages 283–298. Academic Press.
M. Richardson and P. Domingos. 2006. Markov logic
networks. Machine Learning, 62(1):107–136.
S. Riedel and J. Clarke. 2006. Incremental integer linear
programming for non-projective dependency parsing.
In EMNLP.
A. M. Rush and M. Collins. 2011. Exact decoding of
syntactic translation models through lagrangian relax-
ation. In ACL.
A. Rush, D. Sontag, M. Collins, and T. Jaakkola. 2010.
On dual decomposition and linear programming relax-
ations for natural language processing. In EMNLP.
N. Shor. 1985. Minimization methods for non-
differentiable functions. Springer.
D. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In EMNLP.
D. Sontag, T. Meltzer, A. Globerson, Y. Weiss, and
T Jaakkola. 2008. Tightening LP relaxations for MAP
using message-passing. In UAI.
M. Surdeanu, R. Johansson, A. Meyers, L. M`arquez, and
J. Nivre. 2008. The CoNLL-2008 shared task on
joint parsing of syntactic and semantic dependencies.
CoNLL.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In NIPS.
R.W. Tromble and J. Eisner. 2006. A fast finite-state
relaxation method for enforcing global constraints on
sequence decoding. In Proc. of NAACL, pages 423–
430.
M. Wainwright and M. Jordan. 2008. Graphical Models,
Exponential Families, and Variational Inference. Now
Publishers.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
IWPT.
</reference>
<page confidence="0.998904">
249
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.607378">
<title confidence="0.999853">Dual Decomposition with Many Overlapping Components</title>
<author confidence="0.994382">F T A M Q A T</author>
<affiliation confidence="0.878149666666667">of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, de Sistemas e Rob´otica, Instituto Superior T´ecnico, Lisboa, de Instituto Superior T´ecnico, Lisboa,</affiliation>
<email confidence="0.977528">aguiar@isr.ist.utl.pt,mtf@lx.it.pt</email>
<abstract confidence="0.997160411764706">Dual decomposition has been recently proposed as a way of combining complementary models, with a boost in predictive power. However, in cases where lightweight decomare not readily available due to the presence of rich features or logical constraints), the original subgradient algorithm is inefficient. We sidestep that difficulty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes. We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Auli</author>
<author>A Lopez</author>
</authors>
<title>A Comparison of Loopy Belief Propagation and Dual Decomposition for Integrated CCG Supertagging and Parsing.</title>
<date>2011</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="2176" citStr="Auli and Lopez, 2011" startWordPosition="314" endWordPosition="317">demands some form of approximate decoding, such as sampling, heuristic search, or variational inference. In this paper, we focus on parsers built from linear programming relaxations, the so-called “turbo parsers” (Martins et al., 2009a; Martins et al., 2010). Rush et al. (2010) applied dual decomposition as a way of combining models which alone permit efficient decoding, but whose combination is intractable. This results in a relaxation of the original problem that is elegantly solved with the subgradient algorithm. While this technique has proven quite effective in parsing (Koo et al., 2010; Auli and Lopez, 2011) as well as machine translation (Rush and Collins, 2011), we show here that its success is strongly tied to the ability of finding a “good” decomposition, i.e., one involving few overlapping components (or slaves). With many components, the subgradient algorithm exhibits extremely slow convergence (cf. Fig. 2). Unfortunately, a lightweight decomposition is not always at hand, either because the problem does not factor in a natural way, or because one would like to incorporate features that cannot be easily absorbed in few tractable components. Examples include features generated by statements </context>
</contexts>
<marker>Auli, Lopez, 2011</marker>
<rawString>M. Auli and A. Lopez. 2011. A Comparison of Loopy Belief Propagation and Dual Decomposition for Integrated CCG Supertagging and Parsing. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bar-Hillel</author>
<author>M Perles</author>
<author>E Shamir</author>
</authors>
<title>On formal properties of simple phrase structure grammars. Language and Information: Selected Essays on their Theory and Application,</title>
<date>1964</date>
<pages>116--150</pages>
<contexts>
<context position="4860" citStr="Bar-Hillel et al., 1964" startWordPosition="741" endWordPosition="744">2011. c�2011 Association for Computational Linguistics Designing the model must obey certain practical considerations. If efficiency is the major concern, a simple model is usually chosen so that Eq. 1 can be solved efficiently, at the cost of limited expressive power. If we care more about accuracy, a model with richer features and more involved score functions may be designed. Decoding, however, will be more expensive, and approximations are often necessary. A typical source of intractability comes from the combinatorial explosion inherent in the composition of two or more tractable models (Bar-Hillel et al., 1964; Tromble and Eisner, 2006). Recently, Rush et al. (2010) have proposed a dual decomposition framework to address NLP problems in which the global score decomposes as f(y) = f1(z1)+f2(z2), where z1 and z2 are two overlapping “views” of the output, so that Eq. 1 becomes: maximize f1(z1) + f2 (z2L) (2) w.r.t. z1 E Y1, z2 E 02 s.t. z1 — z2. Above, the notation z1 — z2 means that z1 and z2 “agree on their overlaps,” and an isomorphism Y ^- {(z1, z2) E Y1 x Y2 1 z1 — z2} is assumed. We next formalize these notions and proceed to compositions of an arbitrary number of models. Of special interest is </context>
</contexts>
<marker>Bar-Hillel, Perles, Shamir, 1964</marker>
<rawString>Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On formal properties of simple phrase structure grammars. Language and Information: Selected Essays on their Theory and Application, pages 116–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Gillick</author>
<author>Dan Klein</author>
</authors>
<title>Jointly learning to extract and compress.</title>
<date>2011</date>
<booktitle>In Proc. of ACL.</booktitle>
<marker>Berg-Kirkpatrick, Gillick, Klein, 2011</marker>
<rawString>Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein. 2011. Jointly learning to extract and compress. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bertsekas</author>
<author>W Hager</author>
<author>O Mangasarian</author>
</authors>
<title>Nonlinear programming. Athena Scientific.</title>
<date>1999</date>
<contexts>
<context position="13252" citStr="Bertsekas et al., 1999" startWordPosition="2303" endWordPosition="2306">o compute an “averaged” vote for each basic part, ut+1 (r) = δir) �s:r∈¯Rs z;+1 (r), (9) where δ(r) = |{s : r E Rs is the number of components which contain part r. An update of the Lagrange variables follows, λt+1 s(r) = λt s(r) − ηt(zt+1 s (r) − ut+1(r)), (10) where ηt is a stepsize. Intuitively, the algorithm pushes for a consensus among the slaves (Eq. 9), via an adjustment of the Lagrange multipliers which takes into consideration deviations from the average (Eq. 10). The subgradient method is guaranteed to converge to the solution of D (Eq. 7), for suitably chosen stepsizes (Shor, 1985; Bertsekas et al., 1999); it also provides a certificate of optimality in case the relaxation is tight (i.e., P = D) and the exact solution has been found. However, convergence is slow when S is large (as we will show in the experimental section), and no certificates are available when there is a relaxation gap (P &lt; P0). In the next section, we describe the DD-ADMM algorithm (Martins et al., 2011), which does not have these drawbacks and shares a similar simplicity. 3 Alternating Directions Method There are two reasons why subgradient-based dual decomposition is not completely satisfying: • it may take a long time to</context>
<context position="14840" citStr="Bertsekas et al. (1999)" startWordPosition="2578" endWordPosition="2581">ontag et al., 2008; Rush and Collins, 2011). Taking a look back at the relaxed primal problem P0 (Eq. 5), we see that any primal feasible solution must satisfy the agreement constraints. This suggests that penalizing violations of these constraints could speed up consensus. Augmented Lagrangian. By adding a penalty term to Eq. 6, we obtain the augmented Lagrangian function (Hestenes, 1969; Powell, 1969): Ap(z, u, λ) = L(z, u, λ) − ρ 2 r∈Rs (11) where the parameter ρ &gt; 0 controls the intensity of the penalty. Augmented Lagrangian methods are well-known in the optimization community (see, e.g., Bertsekas et al. (1999), §4.2). They alternate updates to the λ-variables, while seeking to maximize Aρ with respect to z and u. In our case, however, this joint maximization poses difficulties, since the penalty term couples the two variables. The alternating directions method of multipliers (ADMM), coined by Gabay and Mercier (1976) and Glowinski and Marroco (1975), sidesteps this issue by performing alternate maximizations, zt+1 = arg max z ut+1 = arg max u followed by an update of the Lagrange multipliers as in Eq. 10. Recently, ADMM has attracted interest, being applied in a variety of problems; see the recent </context>
</contexts>
<marker>Bertsekas, Hager, Mangasarian, 1999</marker>
<rawString>D. Bertsekas, W. Hager, and O. Mangasarian. 1999. Nonlinear programming. Athena Scientific.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D P Bertsekas</author>
<author>A Nedic</author>
<author>A E Ozdaglar</author>
</authors>
<title>Convex analysis and optimization. Athena Scientific.</title>
<date>2003</date>
<marker>Bertsekas, Nedic, Ozdaglar, 2003</marker>
<rawString>D.P. Bertsekas, A. Nedic, and A.E. Ozdaglar. 2003. Convex analysis and optimization. Athena Scientific.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Boyd</author>
<author>N Parikh</author>
<author>E Chu</author>
<author>B Peleato</author>
<author>J Eckstein</author>
</authors>
<title>Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers.</title>
<date>2011</date>
<publisher>Now Publishers</publisher>
<note>(to appear).</note>
<contexts>
<context position="15466" citStr="Boyd et al. (2011)" startWordPosition="2686" endWordPosition="2689"> They alternate updates to the λ-variables, while seeking to maximize Aρ with respect to z and u. In our case, however, this joint maximization poses difficulties, since the penalty term couples the two variables. The alternating directions method of multipliers (ADMM), coined by Gabay and Mercier (1976) and Glowinski and Marroco (1975), sidesteps this issue by performing alternate maximizations, zt+1 = arg max z ut+1 = arg max u followed by an update of the Lagrange multipliers as in Eq. 10. Recently, ADMM has attracted interest, being applied in a variety of problems; see the recent book by Boyd et al. (2011) for an overview. As derived in the App. A, the u-updates in Eq. 13 have a closed form, which is precisely the averaging operation performed by the subgradient method (Eq. 9). We are left with the problem of computing the z-updates. Like in the subgradient approach, the maximization in Eq. 12 can be separated into S independent slave subproblems, which now take the form: maximize fs(zs) + r∈¯Rs λs(r)zs(r) −ρ �r∈¯Rs(zs(r) − ut(r))2 2 w.r.t. zs E Zs(x). Comparing Eq. 8 and Eq. 14, we observe that the only difference is the presence in the latter of a S s=1 (zs(r) − u(r))2, Aρ(z, ut, λt), (12) Aρ</context>
<context position="17267" citStr="Boyd et al., 2011" startWordPosition="3008" endWordPosition="3011"> relaxation is tight (P = P&apos;) and an exact solution of P has been found. While this is good enough when tight relaxations are frequent, as in the settings explored by Rush et al. (2010), Koo et al. (2010), and Rush and Collins (2011), it is hard to know when to stop when a relaxation gap exists. We would like to have similar guarantees concerning the relaxed primal P&apos;.5 A general weakness of subgradient algorithms is that they do not have this capacity, and so are usually stopped by specifying a maximum number of iterations. In contrast, ADMM allows to keep track of primal and dual residuals (Boyd et al., 2011). This allows providing certificates not only for the exact solution of P (when the relaxation is tight), but also to terminate when a near optimal solution of the relaxed problem P&apos; has been found. Theprimal residual rt P measures the amount by which the agreement constraints are violated: P 15) rER δ(r) the dual residual rt D is the amount by which a dual optimality condition is violated (see Boyd et al. (2011), p.18, for details). It is computed via: r __ t PrER δ(r)(ut(r) − ut−1(r))2 (16) D ErER δ(r) Our stopping criterion is thus that these two residuals are below a threshold, e.g., 1 x 1</context>
<context position="19045" citStr="Boyd et al., 2011" startWordPosition="3342" endWordPosition="3345">alize t +- 1 3: initialize u1(r) +- 0.5 and λ1 s(r) +- 0, ds, dr E ¯Rs 4: repeat 5: for each s = 1,...,S do 6: make a zs-update, yielding zt+1 s (Eq. 14) 7: end for 8: make a u-update, yielding ut+1 (Eq. 9) 9: make a λ-update, yielding λt+1 (Eq. 10) 10: t +- t + 1 11: until rt+1 P &lt; cP and rt+1 D &lt; ED (Eqs. 15–16) 12: output: relaxed primal and dual solutions u, z, λ Martins et al. (2011), convergence to the solution of P&apos; is guaranteed with a fixed stepsize ηt = τρ, with τ E [1, 1.618] (Glowinski and Le Tallec, 1989, Thm. 4.2). In our experiments, we set τ = 1.5, and adapt ρ as described in (Boyd et al., 2011, p.20).6 4 Solving the Subproblems In this section, we address the slave subproblems of DD-ADMM (Eq. 14). We show how these subproblems can be solved efficiently for several important cases that arise in NLP applications. Throughout, we assume that the score functions fs are linear, i.e., they can be written as fs(zs) = P rERs θs(r)zs(r). This is the case whenever a linear model is used, in which case θs(r) = 1 δ(r)w · O(x, r), where w is a weight vector and O(x, r) is a feature vector. It is also the scenario studied in previous work in dual decomposition (Rush et al., 2010). Under this assu</context>
<context position="39616" citStr="Boyd et al., 2011" startWordPosition="6966" endWordPosition="6969">e parts considered here. Koo et al. (2010) proposed a subgradient-based dual decomposition method that elegantly combines head automata with maximum spanning tree algorithms; these parsers, as well as the loopy belief propagation method of Smith and Eisner (2008), are all instances of turbo parsers (Martins et al., 2010). DD-ADMM has been proposed and theoretically analyzed by Martins et al. (2011) for problems representable as factor graphs. The general ADMM method has a long-standing history in optimization (Hestenes, 1969; Powell, 1969; Glowinski and Marroco, 1975; Gabay and Mercier, 1976; Boyd et al., 2011). Other methods have been recently proposed to accelerate dual decomposition, such as Jojic et al. (2010) and Meshi and Globerson (2011) (the latter applying ADMM in the dual rather than the primal). While our paper shows limitations of the subgradient method when there are many overlapping components, this method may still be advantageous over ADMM in problems that are nicely decomposable, since it often allows reusing existing combinatorial machinery. Yet, the scenario we consider here is realistic in NLP, where we often have to deal with not-lightly-decomposable constrained problems (e.g., </context>
</contexts>
<marker>Boyd, Parikh, Chu, Peleato, Eckstein, 2011</marker>
<rawString>S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. 2011. Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers. Now Publishers (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J P Boyle</author>
<author>R L Dykstra</author>
</authors>
<title>A method for finding projections onto the intersections of convex sets in Hilbert spaces.</title>
<date>1986</date>
<booktitle>In Advances in order restricted statistical inference,</booktitle>
<pages>28--47</pages>
<publisher>Springer Verlag.</publisher>
<marker>Boyle, Dykstra, 1986</marker>
<rawString>J.P. Boyle and R.L. Dykstra. 1986. A method for finding projections onto the intersections of convex sets in Hilbert spaces. In Advances in order restricted statistical inference, pages 28–47. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>E Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="28047" citStr="Buchholz and Marsi, 2006" startWordPosition="5050" endWordPosition="5053">r a sequence model. Hence, our method seems to be more suitable for decompositions which involve “simple slaves,” even if their number is large. However, this does not rule out the possibility of using this method otherwise. Eckstein and Bertsekas (1992) show that the ADMM algorithm may still converge when the z-updates are inexact. Hence the method may still work if the slaves are solved numerically up to some accuracy. We defer this to future investigation. 5 Experiments: Dependency Parsing We used 14 datasets with non-projective dependencies from the CoNLL-2006 and CoNLL-2008 shared tasks (Buchholz and Marsi, 2006; Surdeanu et al., 2008). We also used a projective English dataset derived from the Penn Treebank by applying the standard head rules of Yamada and Matsumoto (2003).11 We did not force the parser to output projective trees or unique roots for any of the datasets; everything is learned from the data. We trained by running 10 iterations of the cost-augmented MIRA algorithm (Crammer et al., 2006) with LP-relaxed decoding, as in Martins et al. (2009b). Following common practice (Charniak and Johnson, 2005; Carreras et al., 2008), we employed a coarse-to-fine procedure to prune away unlikely candi</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>M Collins</author>
<author>T Koo</author>
</authors>
<title>TAG, Dynamic Programming, and the Perceptron for Efficient, Feature-rich Parsing.</title>
<date>2008</date>
<booktitle>In CONLL.</booktitle>
<contexts>
<context position="28578" citStr="Carreras et al., 2008" startWordPosition="5138" endWordPosition="5141">tive dependencies from the CoNLL-2006 and CoNLL-2008 shared tasks (Buchholz and Marsi, 2006; Surdeanu et al., 2008). We also used a projective English dataset derived from the Penn Treebank by applying the standard head rules of Yamada and Matsumoto (2003).11 We did not force the parser to output projective trees or unique roots for any of the datasets; everything is learned from the data. We trained by running 10 iterations of the cost-augmented MIRA algorithm (Crammer et al., 2006) with LP-relaxed decoding, as in Martins et al. (2009b). Following common practice (Charniak and Johnson, 2005; Carreras et al., 2008), we employed a coarse-to-fine procedure to prune away unlikely candidate arcs, as described by Koo and Collins (2010). To ensure valid parse trees at test time, we rounded fractional 11As usual, we train on sections §02–21, use §22 as validation data, and test on §23. We ran SVMTool (Gim´enez and Marquez, 2004) to obtain automatic part-of-speech tags for §22–23. 244 solutions as described in Martins et al. (2009a) (yet, solutions were integral most of the time). The parts used in our full model are the ones depicted in Fig. 1. Note that a subgradient-based method could handle some of those pa</context>
</contexts>
<marker>Carreras, Collins, Koo, 2008</marker>
<rawString>X. Carreras, M. Collins, and T. Koo. 2008. TAG, Dynamic Programming, and the Perceptron for Efficient, Feature-rich Parsing. In CONLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
</authors>
<title>Experiments with a higher-order projective dependency parser.</title>
<date>2007</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="6698" citStr="Carreras, 2007" startWordPosition="1082" endWordPosition="1083"> can be the set of possible labels at each position. Our only assumption is that we can “read out” y from the basic parts it contains. For convenience, we represent y as a binary vector, y = (y(r))r∈R, where y(r) = 1 if part r belongs to y, and 0 otherwise. Decomposition. We generalize the decomposition in Eq. 2 by considering sets Y1, ... , YS for S &gt; 2. Figure 1: Parts used by our parser. Arcs are the basic parts: any dependency tree can be “read out” from the arcs it contains. Consecutive siblings and grandparent parts introduce horizontal and vertical Markovization (McDonald et al., 2006; Carreras, 2007). We break the horizontal Markov assumption via all siblings parts and the vertical one through parts which indicate a directed path between two words. Inspired by transition-based parsers, we also adopt head bigram parts, which look at the heads attached to consecutive words. Finally, we follow Martins et al. (2009a) and have parts which indicate if an arc is non-projective (i.e., if it spans words that do not descend from its head). Each Ys is associated with its own set of parts Rs, in the same sense as above; we represent the elements of Ys as binary vectors zs = (zs(r))r∈R,. Examples are </context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>X. Carreras. 2007. Experiments with a higher-order projective dependency parser. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-fine nbest parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>173--180</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="28554" citStr="Charniak and Johnson, 2005" startWordPosition="5134" endWordPosition="5137"> 14 datasets with non-projective dependencies from the CoNLL-2006 and CoNLL-2008 shared tasks (Buchholz and Marsi, 2006; Surdeanu et al., 2008). We also used a projective English dataset derived from the Penn Treebank by applying the standard head rules of Yamada and Matsumoto (2003).11 We did not force the parser to output projective trees or unique roots for any of the datasets; everything is learned from the data. We trained by running 10 iterations of the cost-augmented MIRA algorithm (Crammer et al., 2006) with LP-relaxed decoding, as in Martins et al. (2009b). Following common practice (Charniak and Johnson, 2005; Carreras et al., 2008), we employed a coarse-to-fine procedure to prune away unlikely candidate arcs, as described by Koo and Collins (2010). To ensure valid parse trees at test time, we rounded fractional 11As usual, we train on sections §02–21, use §22 as validation data, and test on §23. We ran SVMTool (Gim´enez and Marquez, 2004) to obtain automatic part-of-speech tags for §22–23. 244 solutions as described in Martins et al. (2009a) (yet, solutions were integral most of the time). The parts used in our full model are the ones depicted in Fig. 1. Note that a subgradient-based method could</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>E. Charniak and M. Johnson. 2005. Coarse-to-fine nbest parsing and MaxEnt discriminative reranking. In Proc. ACL, pages 173–180. Association for Computational Linguistics Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Hierarchical phrase-based translation. computational linguistics,</title>
<date>2007</date>
<pages>33--2</pages>
<contexts>
<context position="1274" citStr="Chiang, 2007" startWordPosition="173" endWordPosition="174"> to the presence of rich features or logical constraints), the original subgradient algorithm is inefficient. We sidestep that difficulty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes. We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results. 1 Introduction The last years have witnessed increasingly accurate models for syntax, semantics, and machine translation (Chiang, 2007; Finkel et al., 2008; Petrov and Klein, 2008; Smith and Eisner, 2008; Martins et al., 2009a; Johansson and Nugues, 2008; Koo et al., 2010). The predictive power of such models stems from their ability to break locality assumptions. The resulting combinatorial explosion typically demands some form of approximate decoding, such as sampling, heuristic search, or variational inference. In this paper, we focus on parsers built from linear programming relaxations, the so-called “turbo parsers” (Martins et al., 2009a; Martins et al., 2010). Rush et al. (2010) applied dual decomposition as a way of c</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>D. Chiang. 2007. Hierarchical phrase-based translation. computational linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Clarke</author>
<author>M Lapata</author>
</authors>
<title>Global Inference for Sentence Compression An Integer Linear Programming Approach.</title>
<date>2008</date>
<pages>31--399</pages>
<publisher>JAIR,</publisher>
<contexts>
<context position="41311" citStr="Clarke and Lapata, 2008" startWordPosition="7218" endWordPosition="7221">method in several aspects: it is faster to reach a consensus, it has better stopping conditions, and it works better in non-lightweight decompositions. While its slave subproblems are more involved, we derived closedform solutions for many cases of interest, such as first-order logic formulas and combinatorial factors. DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al. (2011). Non-logical constraints may also yield efficient subproblems, e.g., the length constraints in summarization and compression (Clarke and Lapata, 2008; Martins and Smith, 2009; BergKirkpatrick et al., 2011). Finally, DD-ADMM can be adapted to tighten its relaxations towards exact decoding, as in Sontag et al. (2008) and Rush and Collins (2011). We defer this for future work. Acknowledgments We thank all reviewers for their comments, Eric Xing for helpful discussions, and Terry Koo and Sasha Rush for answering questions about their parser and for providing code. A. M. was supported by a FCT/ICTI grant through the CMU-Portugal Program, and by Priberam. This work was partially supported by the FET programme (EU FP7), under the SIMBAD project (</context>
</contexts>
<marker>Clarke, Lapata, 2008</marker>
<rawString>J. Clarke and M. Lapata. 2008. Global Inference for Sentence Compression An Integer Linear Programming Approach. JAIR, 31:399–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>O Dekel</author>
<author>J Keshet</author>
<author>S Shalev-Shwartz</author>
<author>Y Singer</author>
</authors>
<title>Online Passive-Aggressive Algorithms.</title>
<date>2006</date>
<tech>JMLR,</tech>
<pages>7--551</pages>
<contexts>
<context position="28444" citStr="Crammer et al., 2006" startWordPosition="5117" endWordPosition="5120">lly up to some accuracy. We defer this to future investigation. 5 Experiments: Dependency Parsing We used 14 datasets with non-projective dependencies from the CoNLL-2006 and CoNLL-2008 shared tasks (Buchholz and Marsi, 2006; Surdeanu et al., 2008). We also used a projective English dataset derived from the Penn Treebank by applying the standard head rules of Yamada and Matsumoto (2003).11 We did not force the parser to output projective trees or unique roots for any of the datasets; everything is learned from the data. We trained by running 10 iterations of the cost-augmented MIRA algorithm (Crammer et al., 2006) with LP-relaxed decoding, as in Martins et al. (2009b). Following common practice (Charniak and Johnson, 2005; Carreras et al., 2008), we employed a coarse-to-fine procedure to prune away unlikely candidate arcs, as described by Koo and Collins (2010). To ensure valid parse trees at test time, we rounded fractional 11As usual, we train on sections §02–21, use §22 as validation data, and test on §23. We ran SVMTool (Gim´enez and Marquez, 2004) to obtain automatic part-of-speech tags for §22–23. 244 solutions as described in Martins et al. (2009a) (yet, solutions were integral most of the time)</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. 2006. Online Passive-Aggressive Algorithms. JMLR, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Duchi</author>
<author>S Shalev-Shwartz</author>
<author>Y Singer</author>
<author>T Chandra</author>
</authors>
<title>Efficient projections onto the L1-ball for learning in high dimensions. In ICML.</title>
<date>2008</date>
<marker>Duchi, Shalev-Shwartz, Singer, Chandra, 2008</marker>
<rawString>J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra. 2008. Efficient projections onto the L1-ball for learning in high dimensions. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eckstein</author>
<author>D Bertsekas</author>
</authors>
<title>On the DouglasRachford splitting method and the proximal point algorithm for maximal monotone operators.</title>
<date>1992</date>
<booktitle>Mathematical Programming,</booktitle>
<volume>55</volume>
<issue>1</issue>
<contexts>
<context position="27677" citStr="Eckstein and Bertsekas (1992)" startWordPosition="4989" endWordPosition="4992">uantifiers (e.g., R(x) := by : Q(x, y)). Up to a log-factor, the runtimes will be linear in the number of predicates. Larger Slaves. The only disadvantage of DDADMM in comparison with the subgradient algorithm is that there is not an obvious way of solving the subproblem in Eq. 14 exactly for large combinatorial factors, such as the TREE constraint in dependency parsing, or a sequence model. Hence, our method seems to be more suitable for decompositions which involve “simple slaves,” even if their number is large. However, this does not rule out the possibility of using this method otherwise. Eckstein and Bertsekas (1992) show that the ADMM algorithm may still converge when the z-updates are inexact. Hence the method may still work if the slaves are solved numerically up to some accuracy. We defer this to future investigation. 5 Experiments: Dependency Parsing We used 14 datasets with non-projective dependencies from the CoNLL-2006 and CoNLL-2008 shared tasks (Buchholz and Marsi, 2006; Surdeanu et al., 2008). We also used a projective English dataset derived from the Penn Treebank by applying the standard head rules of Yamada and Matsumoto (2003).11 We did not force the parser to output projective trees or uni</context>
</contexts>
<marker>Eckstein, Bertsekas, 1992</marker>
<rawString>J. Eckstein and D. Bertsekas. 1992. On the DouglasRachford splitting method and the proximal point algorithm for maximal monotone operators. Mathematical Programming, 55(1):293–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>A Kleeman</author>
<author>C D Manning</author>
</authors>
<title>Efficient, feature-based, conditional random field parsing.</title>
<date>2008</date>
<booktitle>Proceedings of ACL-08: HLT,</booktitle>
<pages>959--967</pages>
<contexts>
<context position="1295" citStr="Finkel et al., 2008" startWordPosition="175" endWordPosition="178">ce of rich features or logical constraints), the original subgradient algorithm is inefficient. We sidestep that difficulty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes. We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results. 1 Introduction The last years have witnessed increasingly accurate models for syntax, semantics, and machine translation (Chiang, 2007; Finkel et al., 2008; Petrov and Klein, 2008; Smith and Eisner, 2008; Martins et al., 2009a; Johansson and Nugues, 2008; Koo et al., 2010). The predictive power of such models stems from their ability to break locality assumptions. The resulting combinatorial explosion typically demands some form of approximate decoding, such as sampling, heuristic search, or variational inference. In this paper, we focus on parsers built from linear programming relaxations, the so-called “turbo parsers” (Martins et al., 2009a; Martins et al., 2010). Rush et al. (2010) applied dual decomposition as a way of combining models which</context>
</contexts>
<marker>Finkel, Kleeman, Manning, 2008</marker>
<rawString>J. R. Finkel, A. Kleeman, and C. D. Manning. 2008. Efficient, feature-based, conditional random field parsing. Proceedings of ACL-08: HLT, pages 959–967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gabay</author>
<author>B Mercier</author>
</authors>
<title>A dual algorithm for the solution of nonlinear variational problems via finite element approximation.</title>
<date>1976</date>
<journal>Computers and Mathematics with Applications,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="15153" citStr="Gabay and Mercier (1976)" startWordPosition="2629" endWordPosition="2632">alty term to Eq. 6, we obtain the augmented Lagrangian function (Hestenes, 1969; Powell, 1969): Ap(z, u, λ) = L(z, u, λ) − ρ 2 r∈Rs (11) where the parameter ρ &gt; 0 controls the intensity of the penalty. Augmented Lagrangian methods are well-known in the optimization community (see, e.g., Bertsekas et al. (1999), §4.2). They alternate updates to the λ-variables, while seeking to maximize Aρ with respect to z and u. In our case, however, this joint maximization poses difficulties, since the penalty term couples the two variables. The alternating directions method of multipliers (ADMM), coined by Gabay and Mercier (1976) and Glowinski and Marroco (1975), sidesteps this issue by performing alternate maximizations, zt+1 = arg max z ut+1 = arg max u followed by an update of the Lagrange multipliers as in Eq. 10. Recently, ADMM has attracted interest, being applied in a variety of problems; see the recent book by Boyd et al. (2011) for an overview. As derived in the App. A, the u-updates in Eq. 13 have a closed form, which is precisely the averaging operation performed by the subgradient method (Eq. 9). We are left with the problem of computing the z-updates. Like in the subgradient approach, the maximization in </context>
<context position="39596" citStr="Gabay and Mercier, 1976" startWordPosition="6962" endWordPosition="6965">a), along with some of the parts considered here. Koo et al. (2010) proposed a subgradient-based dual decomposition method that elegantly combines head automata with maximum spanning tree algorithms; these parsers, as well as the loopy belief propagation method of Smith and Eisner (2008), are all instances of turbo parsers (Martins et al., 2010). DD-ADMM has been proposed and theoretically analyzed by Martins et al. (2011) for problems representable as factor graphs. The general ADMM method has a long-standing history in optimization (Hestenes, 1969; Powell, 1969; Glowinski and Marroco, 1975; Gabay and Mercier, 1976; Boyd et al., 2011). Other methods have been recently proposed to accelerate dual decomposition, such as Jojic et al. (2010) and Meshi and Globerson (2011) (the latter applying ADMM in the dual rather than the primal). While our paper shows limitations of the subgradient method when there are many overlapping components, this method may still be advantageous over ADMM in problems that are nicely decomposable, since it often allows reusing existing combinatorial machinery. Yet, the scenario we consider here is realistic in NLP, where we often have to deal with not-lightly-decomposable constrai</context>
</contexts>
<marker>Gabay, Mercier, 1976</marker>
<rawString>D. Gabay and B. Mercier. 1976. A dual algorithm for the solution of nonlinear variational problems via finite element approximation. Computers and Mathematics with Applications, 2(1):17–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gim´enez</author>
<author>L Marquez</author>
</authors>
<title>Svmtool: A general pos tagger generator based on support vector machines.</title>
<date>2004</date>
<booktitle>In Proc. of LREC.</booktitle>
<marker>Gim´enez, Marquez, 2004</marker>
<rawString>J. Gim´enez and L. Marquez. 2004. Svmtool: A general pos tagger generator based on support vector machines. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Glowinski</author>
<author>P Le Tallec</author>
</authors>
<title>Augmented Lagrangian and operator-splitting methods in nonlinear mechanics. Society for Industrial Mathematics.</title>
<date>1989</date>
<marker>Glowinski, Le Tallec, 1989</marker>
<rawString>R. Glowinski and P. Le Tallec. 1989. Augmented Lagrangian and operator-splitting methods in nonlinear mechanics. Society for Industrial Mathematics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Glowinski</author>
<author>A Marroco</author>
</authors>
<title>Sur l’approximation, par ´el´ements finis d’ordre un, et la r´esolution, par penalisation-dualit´e, d’une classe de probl`emes de Dirichlet non lin´eaires.</title>
<date>1975</date>
<journal>Rev. Franc. Automat. Inform. Rech. Operat.,</journal>
<pages>9--41</pages>
<contexts>
<context position="15186" citStr="Glowinski and Marroco (1975)" startWordPosition="2634" endWordPosition="2637"> the augmented Lagrangian function (Hestenes, 1969; Powell, 1969): Ap(z, u, λ) = L(z, u, λ) − ρ 2 r∈Rs (11) where the parameter ρ &gt; 0 controls the intensity of the penalty. Augmented Lagrangian methods are well-known in the optimization community (see, e.g., Bertsekas et al. (1999), §4.2). They alternate updates to the λ-variables, while seeking to maximize Aρ with respect to z and u. In our case, however, this joint maximization poses difficulties, since the penalty term couples the two variables. The alternating directions method of multipliers (ADMM), coined by Gabay and Mercier (1976) and Glowinski and Marroco (1975), sidesteps this issue by performing alternate maximizations, zt+1 = arg max z ut+1 = arg max u followed by an update of the Lagrange multipliers as in Eq. 10. Recently, ADMM has attracted interest, being applied in a variety of problems; see the recent book by Boyd et al. (2011) for an overview. As derived in the App. A, the u-updates in Eq. 13 have a closed form, which is precisely the averaging operation performed by the subgradient method (Eq. 9). We are left with the problem of computing the z-updates. Like in the subgradient approach, the maximization in Eq. 12 can be separated into S in</context>
<context position="39571" citStr="Glowinski and Marroco, 1975" startWordPosition="6957" endWordPosition="6961">duced by Martins et al. (2009a), along with some of the parts considered here. Koo et al. (2010) proposed a subgradient-based dual decomposition method that elegantly combines head automata with maximum spanning tree algorithms; these parsers, as well as the loopy belief propagation method of Smith and Eisner (2008), are all instances of turbo parsers (Martins et al., 2010). DD-ADMM has been proposed and theoretically analyzed by Martins et al. (2011) for problems representable as factor graphs. The general ADMM method has a long-standing history in optimization (Hestenes, 1969; Powell, 1969; Glowinski and Marroco, 1975; Gabay and Mercier, 1976; Boyd et al., 2011). Other methods have been recently proposed to accelerate dual decomposition, such as Jojic et al. (2010) and Meshi and Globerson (2011) (the latter applying ADMM in the dual rather than the primal). While our paper shows limitations of the subgradient method when there are many overlapping components, this method may still be advantageous over ADMM in problems that are nicely decomposable, since it often allows reusing existing combinatorial machinery. Yet, the scenario we consider here is realistic in NLP, where we often have to deal with not-ligh</context>
</contexts>
<marker>Glowinski, Marroco, 1975</marker>
<rawString>R. Glowinski and A. Marroco. 1975. Sur l’approximation, par ´el´ements finis d’ordre un, et la r´esolution, par penalisation-dualit´e, d’une classe de probl`emes de Dirichlet non lin´eaires. Rev. Franc. Automat. Inform. Rech. Operat., 9:41–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hestenes</author>
</authors>
<title>Multiplier and gradient methods.</title>
<date>1969</date>
<journal>Jour. Optim. Theory andApplic.,</journal>
<pages>4--302</pages>
<contexts>
<context position="14608" citStr="Hestenes, 1969" startWordPosition="2539" endWordPosition="2540">s closer to our main concern.4 4Our main concern is P; however solving P&apos; is often a useful step towards that goal, either because a good rounding scheme exists, or because one may build tighter relaxations to approach P (Sontag et al., 2008; Rush and Collins, 2011). Taking a look back at the relaxed primal problem P0 (Eq. 5), we see that any primal feasible solution must satisfy the agreement constraints. This suggests that penalizing violations of these constraints could speed up consensus. Augmented Lagrangian. By adding a penalty term to Eq. 6, we obtain the augmented Lagrangian function (Hestenes, 1969; Powell, 1969): Ap(z, u, λ) = L(z, u, λ) − ρ 2 r∈Rs (11) where the parameter ρ &gt; 0 controls the intensity of the penalty. Augmented Lagrangian methods are well-known in the optimization community (see, e.g., Bertsekas et al. (1999), §4.2). They alternate updates to the λ-variables, while seeking to maximize Aρ with respect to z and u. In our case, however, this joint maximization poses difficulties, since the penalty term couples the two variables. The alternating directions method of multipliers (ADMM), coined by Gabay and Mercier (1976) and Glowinski and Marroco (1975), sidesteps this issue</context>
<context position="39528" citStr="Hestenes, 1969" startWordPosition="6953" endWordPosition="6954">be stopped. mulation was introduced by Martins et al. (2009a), along with some of the parts considered here. Koo et al. (2010) proposed a subgradient-based dual decomposition method that elegantly combines head automata with maximum spanning tree algorithms; these parsers, as well as the loopy belief propagation method of Smith and Eisner (2008), are all instances of turbo parsers (Martins et al., 2010). DD-ADMM has been proposed and theoretically analyzed by Martins et al. (2011) for problems representable as factor graphs. The general ADMM method has a long-standing history in optimization (Hestenes, 1969; Powell, 1969; Glowinski and Marroco, 1975; Gabay and Mercier, 1976; Boyd et al., 2011). Other methods have been recently proposed to accelerate dual decomposition, such as Jojic et al. (2010) and Meshi and Globerson (2011) (the latter applying ADMM in the dual rather than the primal). While our paper shows limitations of the subgradient method when there are many overlapping components, this method may still be advantageous over ADMM in problems that are nicely decomposable, since it often allows reusing existing combinatorial machinery. Yet, the scenario we consider here is realistic in NLP</context>
</contexts>
<marker>Hestenes, 1969</marker>
<rawString>M. Hestenes. 1969. Multiplier and gradient methods. Jour. Optim. Theory andApplic., 4:302–320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>K Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>1077--1086</pages>
<contexts>
<context position="30287" citStr="Huang and Sagae, 2010" startWordPosition="5413" endWordPosition="5416">1 we recover known methods: • Resorting to the tree and consecutive sibling formulae gives one of the models in Koo et al. (2010), with the same linear relaxation (a proof of this fact is included in App. F); • Resorting to tree, all siblings, grandparent, and non-projective arcs, recovers a multi-commodity flow configuration proposed by Martins et al. (2009a); the relaxation is also the same.12 The experimental results are shown in Tab. 2. For comparison, we include the best published results for each dataset (at the best of our knowledge), among transition-based parsers (Nivre et al., 2006; Huang and Sagae, 2010), graph-based parsers (McDonald et al., 2006; Koo and Collins, 2010), hybrid methods (Nivre and McDonald, 2008; Martins et al., 2008), and turbo parsers (Martins et al., 2010; Koo et al., 2010). Our full model achieved the best reported scores for 7 datasets. The last two columns show a consistent improvement (with the exceptions of Chinese and Arabic) when using the full set of features over a second order model with grandparent and consecutive siblings, which is our reproduction of the model of Koo et al. (2010).13 12Although Martins et al. (2009a) also incorporated consecutive siblings in o</context>
<context position="35049" citStr="Huang and Sagae (2010)" startWordPosition="6203" endWordPosition="6206">nstances having been solved to optimality). The reason is the number of slaves: in this configuration and dataset the average number of slaves per instance is 3327.4, and the largest number is 113207. On the contrary, the ADMM method keeps a robust performance, with a large fraction of optimality certificates in early iterations. Runtime and caching strategies. Despite its suitability to problems with many overlapping components, our parser is still 1.6 times slower than Koo et al. (2010) (0.34 against 0.21 sec./sent. in PTB §23), and is far beyond the speed of transition-based parsers (e.g., Huang and Sagae (2010) take 0.04 sec./sent. on the same data, although accuracy is lower, 92.1%). Our implementation, however, is not fully optimized. We next describe how considerable speed-ups are achieved by caching the subproblems, following a strategy similar to Koo et al. (2010). Fig. 3 illustrates the point. After a few iterations, many variables u(r) see a consensus being achieved (i.e., ut(r) = zt+1 s (r), ∀s) and enter an idle state: they are left unchanged by the u-update in Eq. 9, and so do the Lagrange variables At+1 s (r) (Eq. 10). If by iteration t all variables in a subproblem s are idle, then zt+1 </context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>L. Huang and K. Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proc. of ACL, pages 1077–1086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Johansson</author>
<author>P Nugues</author>
</authors>
<title>Dependency-based Semantic Role Labeling of PropBank.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1394" citStr="Johansson and Nugues, 2008" startWordPosition="191" endWordPosition="194">nt. We sidestep that difficulty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes. We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results. 1 Introduction The last years have witnessed increasingly accurate models for syntax, semantics, and machine translation (Chiang, 2007; Finkel et al., 2008; Petrov and Klein, 2008; Smith and Eisner, 2008; Martins et al., 2009a; Johansson and Nugues, 2008; Koo et al., 2010). The predictive power of such models stems from their ability to break locality assumptions. The resulting combinatorial explosion typically demands some form of approximate decoding, such as sampling, heuristic search, or variational inference. In this paper, we focus on parsers built from linear programming relaxations, the so-called “turbo parsers” (Martins et al., 2009a; Martins et al., 2010). Rush et al. (2010) applied dual decomposition as a way of combining models which alone permit efficient decoding, but whose combination is intractable. This results in a relaxatio</context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>R. Johansson and P. Nugues. 2008. Dependency-based Semantic Role Labeling of PropBank. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Jojic</author>
<author>S Gould</author>
<author>D Koller</author>
</authors>
<title>Accelerated dual decomposition for MAP inference.</title>
<date>2010</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="39721" citStr="Jojic et al. (2010)" startWordPosition="6982" endWordPosition="6985">legantly combines head automata with maximum spanning tree algorithms; these parsers, as well as the loopy belief propagation method of Smith and Eisner (2008), are all instances of turbo parsers (Martins et al., 2010). DD-ADMM has been proposed and theoretically analyzed by Martins et al. (2011) for problems representable as factor graphs. The general ADMM method has a long-standing history in optimization (Hestenes, 1969; Powell, 1969; Glowinski and Marroco, 1975; Gabay and Mercier, 1976; Boyd et al., 2011). Other methods have been recently proposed to accelerate dual decomposition, such as Jojic et al. (2010) and Meshi and Globerson (2011) (the latter applying ADMM in the dual rather than the primal). While our paper shows limitations of the subgradient method when there are many overlapping components, this method may still be advantageous over ADMM in problems that are nicely decomposable, since it often allows reusing existing combinatorial machinery. Yet, the scenario we consider here is realistic in NLP, where we often have to deal with not-lightly-decomposable constrained problems (e.g., exploiting linguistic knowledge). 7 Conclusion We have introduced new feature-rich turbo parsers. Since e</context>
</contexts>
<marker>Jojic, Gould, Koller, 2010</marker>
<rawString>V. Jojic, S. Gould, and D. Koller. 2010. Accelerated dual decomposition for MAP inference. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Komodakis</author>
<author>N Paragios</author>
<author>G Tziritas</author>
</authors>
<title>MRF optimization via dual decomposition: Messagepassing revisited.</title>
<date>2007</date>
<booktitle>In ICCV.</booktitle>
<contexts>
<context position="9328" citStr="Komodakis et al., 2007" startWordPosition="1571" endWordPosition="1574">nents are consistent. This is equivalent to the existence of a witness vector (u(r))r∈R such that zs(r) = u(r), ds, r E ¯Rs. With this setup, assuming that the score function decomposes as f(z) = PSs=1 fs(zs), the decoding problem (which extends Eq. 2 for S &gt; 2) becomes: P : maximize PSs=1 fs(zs) w.r.t. zs //E Ys, ds (u(r))rER E R|R|, s.t. zs(r) = u(r), ds, r E We call the equality constraints expressed in the last line the “agreement constraints.” It is these constraints that complicate the problem, which would otherwise be exactly separable into S subproblems. The dual decomposition method (Komodakis et al., 2007; Rush et al., 2010) builds an approximation by dualizing out these constraints, as we describe next. 2.3 Dual Decomposition We describe dual decomposition in a slightly different manner than Rush et al. (2010): we will first build a relaxation of P (called P0), in which the entire approximation is enclosed. Then, we dualize P0, yielding problem D. In the second step, the duality gap is zero, i.e., P0 and D are equivalent.2 Relaxation. For each s E {1, ... , S} we consider the convex hull of Ys, Zs = ( X P(zs)zs P(zs) &gt; 0, E p(Zs) = 11 . zs EYs Z. E&apos;9. J (4) gle factor in a factor graph (Smith</context>
<context position="12412" citStr="Komodakis et al., 2007" startWordPosition="2142" endWordPosition="2146">unctions are linear, i.e., of the form fs(zs) = Pr∈Rs θs(r)zs(r) for some vector θs = (θs (r))r∈Rs, then Eq. 8 becomes a linear program, for which a solution exists at a vertex of Zs (which 3This is guaranteed if the score functions fs are linear. (3) Rs. (5) Rs. (8) w.r.t. zs E Zs. maximize fs(zs) + PrE¯Rs λs(r)zs(r) 240 in turn is an element of Ys). Depending on the structure of the problem, Eq. 8 may be solved by brute force, dynamic programming, or specialized combinatorial algorithms (Rush et al., 2010; Koo et al., 2010; Rush and Collins, 2011). Applying the projected subgradient method (Komodakis et al., 2007; Rush et al., 2010) to the master problem (Eq. 7) yields a remarkably simple algorithm, which at each round t solves the subproblems in Eq. 8 for s = 1, ... , S, and then gathers these solutions (call them zt+1 s ) to compute an “averaged” vote for each basic part, ut+1 (r) = δir) �s:r∈¯Rs z;+1 (r), (9) where δ(r) = |{s : r E Rs is the number of components which contain part r. An update of the Lagrange variables follows, λt+1 s(r) = λt s(r) − ηt(zt+1 s (r) − ut+1(r)), (10) where ηt is a stepsize. Intuitively, the algorithm pushes for a consensus among the slaves (Eq. 9), via an adjustment of</context>
</contexts>
<marker>Komodakis, Paragios, Tziritas, 2007</marker>
<rawString>N. Komodakis, N. Paragios, and G. Tziritas. 2007. MRF optimization via dual decomposition: Messagepassing revisited. In ICCV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>M Collins</author>
</authors>
<title>Efficient third-order dependency parsers.</title>
<date>2010</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>1--11</pages>
<contexts>
<context position="28696" citStr="Koo and Collins (2010)" startWordPosition="5156" endWordPosition="5159">e also used a projective English dataset derived from the Penn Treebank by applying the standard head rules of Yamada and Matsumoto (2003).11 We did not force the parser to output projective trees or unique roots for any of the datasets; everything is learned from the data. We trained by running 10 iterations of the cost-augmented MIRA algorithm (Crammer et al., 2006) with LP-relaxed decoding, as in Martins et al. (2009b). Following common practice (Charniak and Johnson, 2005; Carreras et al., 2008), we employed a coarse-to-fine procedure to prune away unlikely candidate arcs, as described by Koo and Collins (2010). To ensure valid parse trees at test time, we rounded fractional 11As usual, we train on sections §02–21, use §22 as validation data, and test on §23. We ran SVMTool (Gim´enez and Marquez, 2004) to obtain automatic part-of-speech tags for §22–23. 244 solutions as described in Martins et al. (2009a) (yet, solutions were integral most of the time). The parts used in our full model are the ones depicted in Fig. 1. Note that a subgradient-based method could handle some of those parts efficiently (arcs, consecutive siblings, grandparents, and head bigrams) by composing arc-factored models, head au</context>
<context position="30355" citStr="Koo and Collins, 2010" startWordPosition="5424" endWordPosition="5427">sibling formulae gives one of the models in Koo et al. (2010), with the same linear relaxation (a proof of this fact is included in App. F); • Resorting to tree, all siblings, grandparent, and non-projective arcs, recovers a multi-commodity flow configuration proposed by Martins et al. (2009a); the relaxation is also the same.12 The experimental results are shown in Tab. 2. For comparison, we include the best published results for each dataset (at the best of our knowledge), among transition-based parsers (Nivre et al., 2006; Huang and Sagae, 2010), graph-based parsers (McDonald et al., 2006; Koo and Collins, 2010), hybrid methods (Nivre and McDonald, 2008; Martins et al., 2008), and turbo parsers (Martins et al., 2010; Koo et al., 2010). Our full model achieved the best reported scores for 7 datasets. The last two columns show a consistent improvement (with the exceptions of Chinese and Arabic) when using the full set of features over a second order model with grandparent and consecutive siblings, which is our reproduction of the model of Koo et al. (2010).13 12Although Martins et al. (2009a) also incorporated consecutive siblings in one of their configurations, our constraints are tighter than theirs.</context>
<context position="32164" citStr="Koo and Collins (2010)" startWordPosition="5717" endWordPosition="5720">] 88.80 89.46 (+0.66) English 92.57 [Ko10] 92.45 92.68 (+0.23) Danish 91.78 [Ko10] 91.70 91.86 (+0.16) Dutch 85.81 [Ko10] 84.77 85.53 (+0.76) German 91.49 [Ma10] 91.29 91.89 (+0.60) Japane. 93.42 [Ma10] 93.62 93.72 (+0.10) Portug. 93.03 [Ko10] 92.05 92.29 (+0.24) Slovene 86.21 [Ko10] 86.09 86.95 (+0.86) Spanish 87.04 [Ma10] 85.99 86.74 (+0.75) Swedish 91.36 [Ko10] 89.94 90.16 (+0.22) Turkish 77.55 [Ko10] 76.24 76.64 (+0.40) PTB §23 93.04 [KC10] 92.19 92.53 (+0.34) Table 2: Unlabeled attachment scores, excluding punctuation. In the second column, [Ma08] denotes Martins et al. (2008), [KC10] is Koo and Collins (2010), [Ma10] is Martins et al. (2010), and [Ko10] is Koo et al. (2010). In columns 3–4, “Full” is our full model, and “G+CS” is our reproduction of the model of Koo et al. (2010), i.e., the same as “Full” but with all features ablated excepted for grandparents and consecutive siblings. AF +G+CS +AS +NP Full PTB §22 91.02 92.13 92.32 92.36 92.41 PTB §23 91.36 92.19 92.41 92.50 92.53 Table 3: Feature ablation experiments. AF is an arcfactored model; +G+CS adds grandparent and consecutive siblings; +AS adds all-siblings; +NP adds nonprojective arcs; Full adds the bigram and directed paths. Feature ab</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>T. Koo and M. Collins. 2010. Efficient third-order dependency parsers. In Proc. of ACL, pages 1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>A M Rush</author>
<author>M Collins</author>
<author>T Jaakkola</author>
<author>D Sontag</author>
</authors>
<title>Dual decomposition for parsing with nonprojective head automata.</title>
<date>2010</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1413" citStr="Koo et al., 2010" startWordPosition="195" endWordPosition="198">lty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes. We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results. 1 Introduction The last years have witnessed increasingly accurate models for syntax, semantics, and machine translation (Chiang, 2007; Finkel et al., 2008; Petrov and Klein, 2008; Smith and Eisner, 2008; Martins et al., 2009a; Johansson and Nugues, 2008; Koo et al., 2010). The predictive power of such models stems from their ability to break locality assumptions. The resulting combinatorial explosion typically demands some form of approximate decoding, such as sampling, heuristic search, or variational inference. In this paper, we focus on parsers built from linear programming relaxations, the so-called “turbo parsers” (Martins et al., 2009a; Martins et al., 2010). Rush et al. (2010) applied dual decomposition as a way of combining models which alone permit efficient decoding, but whose combination is intractable. This results in a relaxation of the original p</context>
<context position="10016" citStr="Koo et al., 2010" startWordPosition="1702" endWordPosition="1705">straints, as we describe next. 2.3 Dual Decomposition We describe dual decomposition in a slightly different manner than Rush et al. (2010): we will first build a relaxation of P (called P0), in which the entire approximation is enclosed. Then, we dualize P0, yielding problem D. In the second step, the duality gap is zero, i.e., P0 and D are equivalent.2 Relaxation. For each s E {1, ... , S} we consider the convex hull of Ys, Zs = ( X P(zs)zs P(zs) &gt; 0, E p(Zs) = 11 . zs EYs Z. E&apos;9. J (4) gle factor in a factor graph (Smith and Eisner, 2008), or to a entire subgraph enclosing several factors (Koo et al., 2010), or even to a formula in Markov logic (Richardson and Domingos, 2006). In these examples, the basic parts may correspond to individual variable-value pairs. 2Instead of following the path P → P&apos; → D, Rush et al. (2010) go straight from P to D via a Lagrangian relaxation. The two formulations are equivalent for linear score functions. We have that Ys = Zs n Z|Rs|; hence, problem P (Eq. 3) is equivalent to one in which each Ys is replaced by Zs and the z-variables are constrained to be integer. By dropping the integer constraints, we obtain the following relaxed problem: P0 : maximize PSs=1 fs(</context>
<context position="12320" citStr="Koo et al., 2010" startWordPosition="2129" endWordPosition="2132">at we describe in §3, in some sense, alleviates her from this concern). If the score functions are linear, i.e., of the form fs(zs) = Pr∈Rs θs(r)zs(r) for some vector θs = (θs (r))r∈Rs, then Eq. 8 becomes a linear program, for which a solution exists at a vertex of Zs (which 3This is guaranteed if the score functions fs are linear. (3) Rs. (5) Rs. (8) w.r.t. zs E Zs. maximize fs(zs) + PrE¯Rs λs(r)zs(r) 240 in turn is an element of Ys). Depending on the structure of the problem, Eq. 8 may be solved by brute force, dynamic programming, or specialized combinatorial algorithms (Rush et al., 2010; Koo et al., 2010; Rush and Collins, 2011). Applying the projected subgradient method (Komodakis et al., 2007; Rush et al., 2010) to the master problem (Eq. 7) yields a remarkably simple algorithm, which at each round t solves the subproblems in Eq. 8 for s = 1, ... , S, and then gathers these solutions (call them zt+1 s ) to compute an “averaged” vote for each basic part, ut+1 (r) = δir) �s:r∈¯Rs z;+1 (r), (9) where δ(r) = |{s : r E Rs is the number of components which contain part r. An update of the Lagrange variables follows, λt+1 s(r) = λt s(r) − ηt(zt+1 s (r) − ut+1(r)), (10) where ηt is a stepsize. Intu</context>
<context position="16853" citStr="Koo et al. (2010)" startWordPosition="2935" endWordPosition="2938">ctions may not be at a vertex (in contrast to the subgradient method). We devote §4 to describing exact and efficient ways of solving the problem in Eq. 14 for important, widely used slaves. Before going into details, we mention another advantage of ADMM over the subgradient algorithm: it knows when to stop. Primal and dual residuals. Recall that the subgradient method provides optimality certificates when the relaxation is tight (P = P&apos;) and an exact solution of P has been found. While this is good enough when tight relaxations are frequent, as in the settings explored by Rush et al. (2010), Koo et al. (2010), and Rush and Collins (2011), it is hard to know when to stop when a relaxation gap exists. We would like to have similar guarantees concerning the relaxed primal P&apos;.5 A general weakness of subgradient algorithms is that they do not have this capacity, and so are usually stopped by specifying a maximum number of iterations. In contrast, ADMM allows to keep track of primal and dual residuals (Boyd et al., 2011). This allows providing certificates not only for the exact solution of P (when the relaxation is tight), but also to terminate when a near optimal solution of the relaxed problem P&apos; has</context>
<context position="29794" citStr="Koo et al. (2010)" startWordPosition="5334" endWordPosition="5337">arts efficiently (arcs, consecutive siblings, grandparents, and head bigrams) by composing arc-factored models, head automata, and a sequence labeler. However, no lightweight decomposition seems possible for incorporating parts for all siblings, directed paths, and non-projective arcs. Tab. 1 shows the first-order logical formulae that encode the constraints in our model. Each formula gives rise to a subproblem which is efficiently solvable (see §4). By ablating some of rows of Tab. 1 we recover known methods: • Resorting to the tree and consecutive sibling formulae gives one of the models in Koo et al. (2010), with the same linear relaxation (a proof of this fact is included in App. F); • Resorting to tree, all siblings, grandparent, and non-projective arcs, recovers a multi-commodity flow configuration proposed by Martins et al. (2009a); the relaxation is also the same.12 The experimental results are shown in Tab. 2. For comparison, we include the best published results for each dataset (at the best of our knowledge), among transition-based parsers (Nivre et al., 2006; Huang and Sagae, 2010), graph-based parsers (McDonald et al., 2006; Koo and Collins, 2010), hybrid methods (Nivre and McDonald, 2</context>
<context position="31027" citStr="Koo et al. (2010)" startWordPosition="5536" endWordPosition="5539">al., 2008), and turbo parsers (Martins et al., 2010; Koo et al., 2010). Our full model achieved the best reported scores for 7 datasets. The last two columns show a consistent improvement (with the exceptions of Chinese and Arabic) when using the full set of features over a second order model with grandparent and consecutive siblings, which is our reproduction of the model of Koo et al. (2010).13 12Although Martins et al. (2009a) also incorporated consecutive siblings in one of their configurations, our constraints are tighter than theirs. See App. F. 13Note however that the actual results of Koo et al. (2010) are higher than our reproduction, as can be seen in the second column. The differences are due to the features that were used and on the way the models were trained. The cause is not search error: exact decoding with an ILP solver (CPLEX) revealed no significant difference with respect to our G+CS column. We leave further analysis for future work. Best known UAS G+CS Full Arabic 80.18 [Ma08] 81.12 81.10 (-0.02) Bulgar. 92.88 [Ma10] 93.04 93.50 (+0.46) Chinese 91.89 [Ma10] 91.05 90.62 (-0.43) Czech 88.78 [Ma10] 88.80 89.46 (+0.66) English 92.57 [Ko10] 92.45 92.68 (+0.23) Danish 91.78 [Ko10] 91</context>
<context position="32338" citStr="Koo et al. (2010)" startWordPosition="5751" endWordPosition="5754">0) Japane. 93.42 [Ma10] 93.62 93.72 (+0.10) Portug. 93.03 [Ko10] 92.05 92.29 (+0.24) Slovene 86.21 [Ko10] 86.09 86.95 (+0.86) Spanish 87.04 [Ma10] 85.99 86.74 (+0.75) Swedish 91.36 [Ko10] 89.94 90.16 (+0.22) Turkish 77.55 [Ko10] 76.24 76.64 (+0.40) PTB §23 93.04 [KC10] 92.19 92.53 (+0.34) Table 2: Unlabeled attachment scores, excluding punctuation. In the second column, [Ma08] denotes Martins et al. (2008), [KC10] is Koo and Collins (2010), [Ma10] is Martins et al. (2010), and [Ko10] is Koo et al. (2010). In columns 3–4, “Full” is our full model, and “G+CS” is our reproduction of the model of Koo et al. (2010), i.e., the same as “Full” but with all features ablated excepted for grandparents and consecutive siblings. AF +G+CS +AS +NP Full PTB §22 91.02 92.13 92.32 92.36 92.41 PTB §23 91.36 92.19 92.41 92.50 92.53 Table 3: Feature ablation experiments. AF is an arcfactored model; +G+CS adds grandparent and consecutive siblings; +AS adds all-siblings; +NP adds nonprojective arcs; Full adds the bigram and directed paths. Feature ablation and error analysis. We conducted a simple ablation study by training several models on the English PTB with different sets of features. Tab. 3 shows the results. As ex</context>
<context position="33715" citStr="Koo et al. (2010)" startWordPosition="5982" endWordPosition="5985">arsed correctly, unlike less expressive models. Convergence speed and optimality. Fig. 2 compares the performance of DD-ADMM and the subgradient algorithms in the validation section of the PTB.14 For the second order model, the subgradient 14The learning rate in the subgradient method was set as 77t = 770/(1+Nincr(t)), as in Koo et al. (2010), where Nincr(t) is the number of dual increases up to the tth iteration, and 770 is chosen to maximize dual decrease after 20 iterations (in a per sentence basis). Those preliminary iterations are not plotted in Fig. 2. 245 method has more slaves than in Koo et al. (2010): it has a slave imposing the TREE constraint (whose subproblems consists on finding a minimum spanning tree) and several for the all-sibling parts, yielding an average number of 310.5 and a maximum of 4310 slaves. These numbers are still manageable, and we observe that a “good” UAS is achieved relatively quickly. The ADMM method has many more slaves due to the multicommodity flow constraints (average 1870.8, maximum 65446), yet it attains optimality sooner, as can be observed in the right plot. For the full model, the subgradient-based method becomes extremely slow, and the UAS score severely</context>
<context position="35312" citStr="Koo et al. (2010)" startWordPosition="6243" endWordPosition="6246"> large fraction of optimality certificates in early iterations. Runtime and caching strategies. Despite its suitability to problems with many overlapping components, our parser is still 1.6 times slower than Koo et al. (2010) (0.34 against 0.21 sec./sent. in PTB §23), and is far beyond the speed of transition-based parsers (e.g., Huang and Sagae (2010) take 0.04 sec./sent. on the same data, although accuracy is lower, 92.1%). Our implementation, however, is not fully optimized. We next describe how considerable speed-ups are achieved by caching the subproblems, following a strategy similar to Koo et al. (2010). Fig. 3 illustrates the point. After a few iterations, many variables u(r) see a consensus being achieved (i.e., ut(r) = zt+1 s (r), ∀s) and enter an idle state: they are left unchanged by the u-update in Eq. 9, and so do the Lagrange variables At+1 s (r) (Eq. 10). If by iteration t all variables in a subproblem s are idle, then zt+1 s (r) = zts(r), hence the subproblem does not need to be resolved.15 Fig. 3 shows that 15Even if not all variables are idle in s, caching may still be useful: note that the z-updates in Eq. 14 tend to be sparse for the subproblems described in §4 (these are Eucli</context>
<context position="39040" citStr="Koo et al. (2010)" startWordPosition="6878" endWordPosition="6881"> coarser decomposition with a TREE factor. Since subgradient and DD-ADMM are solving the same problems, the solid lines (as the dashed ones) would meet in the limit, however subgradient converges very slowly for the full model. The right plot shows optimality certificates for both methods, indicating that an exact solution of P has been found; for DD-ADMM we also plot the fraction of instances that converged to an accurate solution of P0 (primal and dual residuals &lt; 10−3) and hence can be stopped. mulation was introduced by Martins et al. (2009a), along with some of the parts considered here. Koo et al. (2010) proposed a subgradient-based dual decomposition method that elegantly combines head automata with maximum spanning tree algorithms; these parsers, as well as the loopy belief propagation method of Smith and Eisner (2008), are all instances of turbo parsers (Martins et al., 2010). DD-ADMM has been proposed and theoretically analyzed by Martins et al. (2011) for problems representable as factor graphs. The general ADMM method has a long-standing history in optimization (Hestenes, 1969; Powell, 1969; Glowinski and Marroco, 1975; Gabay and Mercier, 1976; Boyd et al., 2011). Other methods have bee</context>
</contexts>
<marker>Koo, Rush, Collins, Jaakkola, Sontag, 2010</marker>
<rawString>T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Sontag. 2010. Dual decomposition for parsing with nonprojective head automata. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kulesza</author>
<author>F Pereira</author>
</authors>
<title>Structured Learning with Approximate Inference.</title>
<date>2007</date>
<publisher>NIPS.</publisher>
<marker>Kulesza, Pereira, 2007</marker>
<rawString>A. Kulesza and F. Pereira. 2007. Structured Learning with Approximate Inference. NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>M I Jordan</author>
<author>D Klein</author>
</authors>
<title>Learning dependency-based compositional semantics.</title>
<date>2011</date>
<booktitle>In Proc. Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="41161" citStr="Liang et al. (2011)" startWordPosition="7198" endWordPosition="7201">ence properties of DD-ADMM, complementing the theoretical treatment in Martins et al. (2011). DD-ADMM compares favourably against the subgradient method in several aspects: it is faster to reach a consensus, it has better stopping conditions, and it works better in non-lightweight decompositions. While its slave subproblems are more involved, we derived closedform solutions for many cases of interest, such as first-order logic formulas and combinatorial factors. DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al. (2011). Non-logical constraints may also yield efficient subproblems, e.g., the length constraints in summarization and compression (Clarke and Lapata, 2008; Martins and Smith, 2009; BergKirkpatrick et al., 2011). Finally, DD-ADMM can be adapted to tighten its relaxations towards exact decoding, as in Sontag et al. (2008) and Rush and Collins (2011). We defer this for future work. Acknowledgments We thank all reviewers for their comments, Eric Xing for helpful discussions, and Terry Koo and Sasha Rush for answering questions about their parser and for providing code. A. M. was supported by a FCT/ICT</context>
</contexts>
<marker>Liang, Jordan, Klein, 2011</marker>
<rawString>P. Liang, M.I. Jordan, and D. Klein. 2011. Learning dependency-based compositional semantics. In Proc. Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>N A Smith</author>
</authors>
<title>Summarization with a joint model for sentence extraction and compression.</title>
<date>2009</date>
<booktitle>In NAACL-HLT Workshop on Integer Linear Programming for NLP.</booktitle>
<contexts>
<context position="41336" citStr="Martins and Smith, 2009" startWordPosition="7222" endWordPosition="7225">: it is faster to reach a consensus, it has better stopping conditions, and it works better in non-lightweight decompositions. While its slave subproblems are more involved, we derived closedform solutions for many cases of interest, such as first-order logic formulas and combinatorial factors. DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al. (2011). Non-logical constraints may also yield efficient subproblems, e.g., the length constraints in summarization and compression (Clarke and Lapata, 2008; Martins and Smith, 2009; BergKirkpatrick et al., 2011). Finally, DD-ADMM can be adapted to tighten its relaxations towards exact decoding, as in Sontag et al. (2008) and Rush and Collins (2011). We defer this for future work. Acknowledgments We thank all reviewers for their comments, Eric Xing for helpful discussions, and Terry Koo and Sasha Rush for answering questions about their parser and for providing code. A. M. was supported by a FCT/ICTI grant through the CMU-Portugal Program, and by Priberam. This work was partially supported by the FET programme (EU FP7), under the SIMBAD project (contract 213250). N. S. w</context>
</contexts>
<marker>Martins, Smith, 2009</marker>
<rawString>A. F. T. Martins and N. A. Smith. 2009. Summarization with a joint model for sentence extraction and compression. In NAACL-HLT Workshop on Integer Linear Programming for NLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>D Das</author>
<author>N A Smith</author>
<author>E P Xing</author>
</authors>
<title>Stacking dependency parsers.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="30420" citStr="Martins et al., 2008" startWordPosition="5434" endWordPosition="5437">h the same linear relaxation (a proof of this fact is included in App. F); • Resorting to tree, all siblings, grandparent, and non-projective arcs, recovers a multi-commodity flow configuration proposed by Martins et al. (2009a); the relaxation is also the same.12 The experimental results are shown in Tab. 2. For comparison, we include the best published results for each dataset (at the best of our knowledge), among transition-based parsers (Nivre et al., 2006; Huang and Sagae, 2010), graph-based parsers (McDonald et al., 2006; Koo and Collins, 2010), hybrid methods (Nivre and McDonald, 2008; Martins et al., 2008), and turbo parsers (Martins et al., 2010; Koo et al., 2010). Our full model achieved the best reported scores for 7 datasets. The last two columns show a consistent improvement (with the exceptions of Chinese and Arabic) when using the full set of features over a second order model with grandparent and consecutive siblings, which is our reproduction of the model of Koo et al. (2010).13 12Although Martins et al. (2009a) also incorporated consecutive siblings in one of their configurations, our constraints are tighter than theirs. See App. F. 13Note however that the actual results of Koo et al.</context>
<context position="32130" citStr="Martins et al. (2008)" startWordPosition="5711" endWordPosition="5714">5 90.62 (-0.43) Czech 88.78 [Ma10] 88.80 89.46 (+0.66) English 92.57 [Ko10] 92.45 92.68 (+0.23) Danish 91.78 [Ko10] 91.70 91.86 (+0.16) Dutch 85.81 [Ko10] 84.77 85.53 (+0.76) German 91.49 [Ma10] 91.29 91.89 (+0.60) Japane. 93.42 [Ma10] 93.62 93.72 (+0.10) Portug. 93.03 [Ko10] 92.05 92.29 (+0.24) Slovene 86.21 [Ko10] 86.09 86.95 (+0.86) Spanish 87.04 [Ma10] 85.99 86.74 (+0.75) Swedish 91.36 [Ko10] 89.94 90.16 (+0.22) Turkish 77.55 [Ko10] 76.24 76.64 (+0.40) PTB §23 93.04 [KC10] 92.19 92.53 (+0.34) Table 2: Unlabeled attachment scores, excluding punctuation. In the second column, [Ma08] denotes Martins et al. (2008), [KC10] is Koo and Collins (2010), [Ma10] is Martins et al. (2010), and [Ko10] is Koo et al. (2010). In columns 3–4, “Full” is our full model, and “G+CS” is our reproduction of the model of Koo et al. (2010), i.e., the same as “Full” but with all features ablated excepted for grandparents and consecutive siblings. AF +G+CS +AS +NP Full PTB §22 91.02 92.13 92.32 92.36 92.41 PTB §23 91.36 92.19 92.41 92.50 92.53 Table 3: Feature ablation experiments. AF is an arcfactored model; +G+CS adds grandparent and consecutive siblings; +AS adds all-siblings; +NP adds nonprojective arcs; Full adds the big</context>
</contexts>
<marker>Martins, Das, Smith, Xing, 2008</marker>
<rawString>A. F. T. Martins, D. Das, N. A. Smith, and E. P. Xing. 2008. Stacking dependency parsers. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>N A Smith</author>
<author>E P Xing</author>
</authors>
<title>Concise integer linear programming formulations for dependency parsing.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP.</booktitle>
<contexts>
<context position="1365" citStr="Martins et al., 2009" startWordPosition="187" endWordPosition="190"> algorithm is inefficient. We sidestep that difficulty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes. We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results. 1 Introduction The last years have witnessed increasingly accurate models for syntax, semantics, and machine translation (Chiang, 2007; Finkel et al., 2008; Petrov and Klein, 2008; Smith and Eisner, 2008; Martins et al., 2009a; Johansson and Nugues, 2008; Koo et al., 2010). The predictive power of such models stems from their ability to break locality assumptions. The resulting combinatorial explosion typically demands some form of approximate decoding, such as sampling, heuristic search, or variational inference. In this paper, we focus on parsers built from linear programming relaxations, the so-called “turbo parsers” (Martins et al., 2009a; Martins et al., 2010). Rush et al. (2010) applied dual decomposition as a way of combining models which alone permit efficient decoding, but whose combination is intractable</context>
<context position="7015" citStr="Martins et al. (2009" startWordPosition="1131" endWordPosition="1134">2 by considering sets Y1, ... , YS for S &gt; 2. Figure 1: Parts used by our parser. Arcs are the basic parts: any dependency tree can be “read out” from the arcs it contains. Consecutive siblings and grandparent parts introduce horizontal and vertical Markovization (McDonald et al., 2006; Carreras, 2007). We break the horizontal Markov assumption via all siblings parts and the vertical one through parts which indicate a directed path between two words. Inspired by transition-based parsers, we also adopt head bigram parts, which look at the heads attached to consecutive words. Finally, we follow Martins et al. (2009a) and have parts which indicate if an arc is non-projective (i.e., if it spans words that do not descend from its head). Each Ys is associated with its own set of parts Rs, in the same sense as above; we represent the elements of Ys as binary vectors zs = (zs(r))r∈R,. Examples are vectors indicating a tree structure, a sequence, or an assignment of variables to a factor, in which case it may happen that only some binary vectors are legal. Some parts in Rs are basic, while others are not. We denote by ¯Rs = Rs n R the subset of the ones that are. In addition, we assume that: • R1, ... , RS joi</context>
<context position="18284" citStr="Martins et al. (2009" startWordPosition="3189" endWordPosition="3192"> Boyd et al. (2011), p.18, for details). It is computed via: r __ t PrER δ(r)(ut(r) − ut−1(r))2 (16) D ErER δ(r) Our stopping criterion is thus that these two residuals are below a threshold, e.g., 1 x 10−3. The complete algorithm is depicted as Alg. 1. As stated in 5This problem is more important than it may look. Problems with many slaves tend to be less exact, hence relaxation gaps are frequent. Also, when decoding is embedded in training, it is useful to obtain the fractional solution of the relaxed primal P (rather than an approximate integer solution). See Kulesza and Pereira (2007) and Martins et al. (2009b) for details. Algorithm 1 ADMM-based Dual Decomposition 1: input: score functions (fs(.))Ss=1, parameters ρ, η, thresholds cP and ED. 2: initialize t +- 1 3: initialize u1(r) +- 0.5 and λ1 s(r) +- 0, ds, dr E ¯Rs 4: repeat 5: for each s = 1,...,S do 6: make a zs-update, yielding zt+1 s (Eq. 14) 7: end for 8: make a u-update, yielding ut+1 (Eq. 9) 9: make a λ-update, yielding λt+1 (Eq. 10) 10: t +- t + 1 11: until rt+1 P &lt; cP and rt+1 D &lt; ED (Eqs. 15–16) 12: output: relaxed primal and dual solutions u, z, λ Martins et al. (2011), convergence to the solution of P&apos; is guaranteed with a fixed st</context>
<context position="28497" citStr="Martins et al. (2009" startWordPosition="5126" endWordPosition="5129">tigation. 5 Experiments: Dependency Parsing We used 14 datasets with non-projective dependencies from the CoNLL-2006 and CoNLL-2008 shared tasks (Buchholz and Marsi, 2006; Surdeanu et al., 2008). We also used a projective English dataset derived from the Penn Treebank by applying the standard head rules of Yamada and Matsumoto (2003).11 We did not force the parser to output projective trees or unique roots for any of the datasets; everything is learned from the data. We trained by running 10 iterations of the cost-augmented MIRA algorithm (Crammer et al., 2006) with LP-relaxed decoding, as in Martins et al. (2009b). Following common practice (Charniak and Johnson, 2005; Carreras et al., 2008), we employed a coarse-to-fine procedure to prune away unlikely candidate arcs, as described by Koo and Collins (2010). To ensure valid parse trees at test time, we rounded fractional 11As usual, we train on sections §02–21, use §22 as validation data, and test on §23. We ran SVMTool (Gim´enez and Marquez, 2004) to obtain automatic part-of-speech tags for §22–23. 244 solutions as described in Martins et al. (2009a) (yet, solutions were integral most of the time). The parts used in our full model are the ones depic</context>
<context position="30025" citStr="Martins et al. (2009" startWordPosition="5370" endWordPosition="5373">for all siblings, directed paths, and non-projective arcs. Tab. 1 shows the first-order logical formulae that encode the constraints in our model. Each formula gives rise to a subproblem which is efficiently solvable (see §4). By ablating some of rows of Tab. 1 we recover known methods: • Resorting to the tree and consecutive sibling formulae gives one of the models in Koo et al. (2010), with the same linear relaxation (a proof of this fact is included in App. F); • Resorting to tree, all siblings, grandparent, and non-projective arcs, recovers a multi-commodity flow configuration proposed by Martins et al. (2009a); the relaxation is also the same.12 The experimental results are shown in Tab. 2. For comparison, we include the best published results for each dataset (at the best of our knowledge), among transition-based parsers (Nivre et al., 2006; Huang and Sagae, 2010), graph-based parsers (McDonald et al., 2006; Koo and Collins, 2010), hybrid methods (Nivre and McDonald, 2008; Martins et al., 2008), and turbo parsers (Martins et al., 2010; Koo et al., 2010). Our full model achieved the best reported scores for 7 datasets. The last two columns show a consistent improvement (with the exceptions of Chi</context>
<context position="38973" citStr="Martins et al. (2009" startWordPosition="6866" endWordPosition="6869">grandparents and all siblings, for which the subgradient method uses a coarser decomposition with a TREE factor. Since subgradient and DD-ADMM are solving the same problems, the solid lines (as the dashed ones) would meet in the limit, however subgradient converges very slowly for the full model. The right plot shows optimality certificates for both methods, indicating that an exact solution of P has been found; for DD-ADMM we also plot the fraction of instances that converged to an accurate solution of P0 (primal and dual residuals &lt; 10−3) and hence can be stopped. mulation was introduced by Martins et al. (2009a), along with some of the parts considered here. Koo et al. (2010) proposed a subgradient-based dual decomposition method that elegantly combines head automata with maximum spanning tree algorithms; these parsers, as well as the loopy belief propagation method of Smith and Eisner (2008), are all instances of turbo parsers (Martins et al., 2010). DD-ADMM has been proposed and theoretically analyzed by Martins et al. (2011) for problems representable as factor graphs. The general ADMM method has a long-standing history in optimization (Hestenes, 1969; Powell, 1969; Glowinski and Marroco, 1975; </context>
</contexts>
<marker>Martins, Smith, Xing, 2009</marker>
<rawString>A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009a. Concise integer linear programming formulations for dependency parsing. In ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>N A Smith</author>
<author>E P Xing</author>
</authors>
<title>Polyhedral outer approximations with application to natural language parsing.</title>
<date>2009</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="1365" citStr="Martins et al., 2009" startWordPosition="187" endWordPosition="190"> algorithm is inefficient. We sidestep that difficulty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes. We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results. 1 Introduction The last years have witnessed increasingly accurate models for syntax, semantics, and machine translation (Chiang, 2007; Finkel et al., 2008; Petrov and Klein, 2008; Smith and Eisner, 2008; Martins et al., 2009a; Johansson and Nugues, 2008; Koo et al., 2010). The predictive power of such models stems from their ability to break locality assumptions. The resulting combinatorial explosion typically demands some form of approximate decoding, such as sampling, heuristic search, or variational inference. In this paper, we focus on parsers built from linear programming relaxations, the so-called “turbo parsers” (Martins et al., 2009a; Martins et al., 2010). Rush et al. (2010) applied dual decomposition as a way of combining models which alone permit efficient decoding, but whose combination is intractable</context>
<context position="7015" citStr="Martins et al. (2009" startWordPosition="1131" endWordPosition="1134">2 by considering sets Y1, ... , YS for S &gt; 2. Figure 1: Parts used by our parser. Arcs are the basic parts: any dependency tree can be “read out” from the arcs it contains. Consecutive siblings and grandparent parts introduce horizontal and vertical Markovization (McDonald et al., 2006; Carreras, 2007). We break the horizontal Markov assumption via all siblings parts and the vertical one through parts which indicate a directed path between two words. Inspired by transition-based parsers, we also adopt head bigram parts, which look at the heads attached to consecutive words. Finally, we follow Martins et al. (2009a) and have parts which indicate if an arc is non-projective (i.e., if it spans words that do not descend from its head). Each Ys is associated with its own set of parts Rs, in the same sense as above; we represent the elements of Ys as binary vectors zs = (zs(r))r∈R,. Examples are vectors indicating a tree structure, a sequence, or an assignment of variables to a factor, in which case it may happen that only some binary vectors are legal. Some parts in Rs are basic, while others are not. We denote by ¯Rs = Rs n R the subset of the ones that are. In addition, we assume that: • R1, ... , RS joi</context>
<context position="18284" citStr="Martins et al. (2009" startWordPosition="3189" endWordPosition="3192"> Boyd et al. (2011), p.18, for details). It is computed via: r __ t PrER δ(r)(ut(r) − ut−1(r))2 (16) D ErER δ(r) Our stopping criterion is thus that these two residuals are below a threshold, e.g., 1 x 10−3. The complete algorithm is depicted as Alg. 1. As stated in 5This problem is more important than it may look. Problems with many slaves tend to be less exact, hence relaxation gaps are frequent. Also, when decoding is embedded in training, it is useful to obtain the fractional solution of the relaxed primal P (rather than an approximate integer solution). See Kulesza and Pereira (2007) and Martins et al. (2009b) for details. Algorithm 1 ADMM-based Dual Decomposition 1: input: score functions (fs(.))Ss=1, parameters ρ, η, thresholds cP and ED. 2: initialize t +- 1 3: initialize u1(r) +- 0.5 and λ1 s(r) +- 0, ds, dr E ¯Rs 4: repeat 5: for each s = 1,...,S do 6: make a zs-update, yielding zt+1 s (Eq. 14) 7: end for 8: make a u-update, yielding ut+1 (Eq. 9) 9: make a λ-update, yielding λt+1 (Eq. 10) 10: t +- t + 1 11: until rt+1 P &lt; cP and rt+1 D &lt; ED (Eqs. 15–16) 12: output: relaxed primal and dual solutions u, z, λ Martins et al. (2011), convergence to the solution of P&apos; is guaranteed with a fixed st</context>
<context position="28497" citStr="Martins et al. (2009" startWordPosition="5126" endWordPosition="5129">tigation. 5 Experiments: Dependency Parsing We used 14 datasets with non-projective dependencies from the CoNLL-2006 and CoNLL-2008 shared tasks (Buchholz and Marsi, 2006; Surdeanu et al., 2008). We also used a projective English dataset derived from the Penn Treebank by applying the standard head rules of Yamada and Matsumoto (2003).11 We did not force the parser to output projective trees or unique roots for any of the datasets; everything is learned from the data. We trained by running 10 iterations of the cost-augmented MIRA algorithm (Crammer et al., 2006) with LP-relaxed decoding, as in Martins et al. (2009b). Following common practice (Charniak and Johnson, 2005; Carreras et al., 2008), we employed a coarse-to-fine procedure to prune away unlikely candidate arcs, as described by Koo and Collins (2010). To ensure valid parse trees at test time, we rounded fractional 11As usual, we train on sections §02–21, use §22 as validation data, and test on §23. We ran SVMTool (Gim´enez and Marquez, 2004) to obtain automatic part-of-speech tags for §22–23. 244 solutions as described in Martins et al. (2009a) (yet, solutions were integral most of the time). The parts used in our full model are the ones depic</context>
<context position="30025" citStr="Martins et al. (2009" startWordPosition="5370" endWordPosition="5373">for all siblings, directed paths, and non-projective arcs. Tab. 1 shows the first-order logical formulae that encode the constraints in our model. Each formula gives rise to a subproblem which is efficiently solvable (see §4). By ablating some of rows of Tab. 1 we recover known methods: • Resorting to the tree and consecutive sibling formulae gives one of the models in Koo et al. (2010), with the same linear relaxation (a proof of this fact is included in App. F); • Resorting to tree, all siblings, grandparent, and non-projective arcs, recovers a multi-commodity flow configuration proposed by Martins et al. (2009a); the relaxation is also the same.12 The experimental results are shown in Tab. 2. For comparison, we include the best published results for each dataset (at the best of our knowledge), among transition-based parsers (Nivre et al., 2006; Huang and Sagae, 2010), graph-based parsers (McDonald et al., 2006; Koo and Collins, 2010), hybrid methods (Nivre and McDonald, 2008; Martins et al., 2008), and turbo parsers (Martins et al., 2010; Koo et al., 2010). Our full model achieved the best reported scores for 7 datasets. The last two columns show a consistent improvement (with the exceptions of Chi</context>
<context position="38973" citStr="Martins et al. (2009" startWordPosition="6866" endWordPosition="6869">grandparents and all siblings, for which the subgradient method uses a coarser decomposition with a TREE factor. Since subgradient and DD-ADMM are solving the same problems, the solid lines (as the dashed ones) would meet in the limit, however subgradient converges very slowly for the full model. The right plot shows optimality certificates for both methods, indicating that an exact solution of P has been found; for DD-ADMM we also plot the fraction of instances that converged to an accurate solution of P0 (primal and dual residuals &lt; 10−3) and hence can be stopped. mulation was introduced by Martins et al. (2009a), along with some of the parts considered here. Koo et al. (2010) proposed a subgradient-based dual decomposition method that elegantly combines head automata with maximum spanning tree algorithms; these parsers, as well as the loopy belief propagation method of Smith and Eisner (2008), are all instances of turbo parsers (Martins et al., 2010). DD-ADMM has been proposed and theoretically analyzed by Martins et al. (2011) for problems representable as factor graphs. The general ADMM method has a long-standing history in optimization (Hestenes, 1969; Powell, 1969; Glowinski and Marroco, 1975; </context>
</contexts>
<marker>Martins, Smith, Xing, 2009</marker>
<rawString>A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009b. Polyhedral outer approximations with application to natural language parsing. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>N A Smith</author>
<author>E P Xing</author>
<author>M A T Figueiredo</author>
<author>P M Q Aguiar</author>
</authors>
<title>Turbo parsers: Dependency parsing by approximate variational inference.</title>
<date>2010</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1813" citStr="Martins et al., 2010" startWordPosition="254" endWordPosition="257">ingly accurate models for syntax, semantics, and machine translation (Chiang, 2007; Finkel et al., 2008; Petrov and Klein, 2008; Smith and Eisner, 2008; Martins et al., 2009a; Johansson and Nugues, 2008; Koo et al., 2010). The predictive power of such models stems from their ability to break locality assumptions. The resulting combinatorial explosion typically demands some form of approximate decoding, such as sampling, heuristic search, or variational inference. In this paper, we focus on parsers built from linear programming relaxations, the so-called “turbo parsers” (Martins et al., 2009a; Martins et al., 2010). Rush et al. (2010) applied dual decomposition as a way of combining models which alone permit efficient decoding, but whose combination is intractable. This results in a relaxation of the original problem that is elegantly solved with the subgradient algorithm. While this technique has proven quite effective in parsing (Koo et al., 2010; Auli and Lopez, 2011) as well as machine translation (Rush and Collins, 2011), we show here that its success is strongly tied to the ability of finding a “good” decomposition, i.e., one involving few overlapping components (or slaves). With many components, </context>
<context position="20851" citStr="Martins et al. (2010)" startWordPosition="3679" endWordPosition="3682">ger/smaller than the dual residual. t Ps 1 PrE¯Rs(zts(r) − ut(r))2 rP = ; ( 242 can be computed in O(|Rs|) time, up to log factors.7 Pairwise Factors. This is the case where RPAIR = {r1, r2, r12}, where r1 and r2 are basic parts and r12 is their conjunction, i.e., we have YPAIR = {hz1, z2, z12i |z12 = z1 ∧ z2}. This factor is useful to make conjunctions of variables participate in the score function (see e.g. the grandparent, sibling, and head bigram parts in Fig. 1). The convex hull of YPAIR is the polytope ZPAIR = {hz1, z2, z12i ∈ [0, 1]3 |z12 ≤ z1, z12 ≤ z2, z12 ≥ z1 + z2 − 1}, as shown by Martins et al. (2010). In this case, problem (17) can be written as max θ12z12 − ρ2[(z1 − a1)2 + (z2 − a2)2] w.r.t. hz1, z2, z12i ∈ [0, 1]3 (18) s.t. z12 ≤ z1, z12 ≤ z2, z12 ≥ z1 + z2 − 1 and has a closed form solution (see App. B). Uniqueness Quantification and XOR. Many problems involve constraining variables to take a single value: for example, in dependency parsing, a modifier can only take one head. This can be expressed as the statement ∃!y : Q(y) in first-order logic,8 or as a one-hot XOR factor in a factor graph (Smith and Eisner, 2008; Martins et al., 2010). In this case, RXOR = {r1, ... , rn}, and YXOR =</context>
<context position="22876" citStr="Martins et al. (2010)" startWordPosition="4069" endWordPosition="4072">ly one.” 9A similar derivation can be made otherwise. 10Also common is the need for constraining existence of “at most one” element. This can be reduced to uniqueness quantification by adding a dummy NULL label. Existential Quantification and OR. Sometimes, only existence is required, not necessarily uniqueness. This can be expressed with disjunctions, existential quantifiers in first-order logic (∃y : Q(y)), or as a OR factor. In this case, ROR = {r1, ... , rn}, YOR = {hz1, ... , zni ∈ {0,1}n |Wni=1 zi = 1}, and the convex hull is ZOR = {hz1, ... , zni ∈ [0, 1]n |Pni=1 zi ≥ 1} (see Tab. 1 in Martins et al. (2010)). The slave subproblem becomes: minimize 1 Pn i=1(zi − ai)2 2 w.r.t. Pn hz1, ... , zni ∈ [0,1]n s.t. i zi ≥ 1. We derive a procedure in App. D to compute this projection in O(n log n) runtime, also with a sort. Negations. The two cases above can be extended to allow some of their inputs to be negated. By a change of variables in Eqs. 19–20 it is possible to reuse the same black box that solves those problems. The procedure is as follows: 1. For i = 1, ... , n, set a0i = 1−ai if the ith variable is negated, and a0i = ai otherwise. 2. Obtain hz01, ... , z0ni as the solution of Eqs. 19 or 20 pro</context>
<context position="24164" citStr="Martins et al. (2010)" startWordPosition="4318" endWordPosition="4321">−z0 i if the ith variable is negated, and zi = z0 i otherwise. The ability to handle negated variables adds a great degree of flexibility. From De Morgan’s laws, we can now handle conjunctions and implications (since Vni=1 Qi(x) ⇒ R(x) is equivalent to Wni=1 ¬Qi(x) ∨ R(x)). Logical Variable Assignments. All previous examples involve taking a group of existing variables and defining a constraint. Alternatively, we may want to define a new variable which is the result of an operation involving other variables. For example, R(x) := ∃!y : Q(x, y). This corresponds to the XOR-WITH-OUTPUT factor in Martins et al. (2010). Interestingly, this can be expressed as a XOR where R(x) is negated (i.e., either ¬R(x) holds or exactly one y satisfies Q(x, y), but not both). A more difficult problem is that of the OR-WITHOUTPUT factor, expressed by the formula R(x) := ∃y : Q(x, y). We have ROR-OUT = {r0, ... , rn},{ and YOR-OUT = hz0, ... , zni ∈ {0,1}n |z0 = (20) 243 # Slaves Runtime Description Tree �I!h : arc(h, m), m =�0 O(n) O(n log n) Each non-root word has a head flow(h, m, k) ==&gt; arc(h, m) O(n3) O(1) Only active arcs may carry flow path(m, d) := �I!h : flow(h, m, d), m =� 0 O(n2) O(nlogn) Paths and flows are con</context>
<context position="26586" citStr="Martins et al., 2010" startWordPosition="4787" endWordPosition="4790">rries flow to d), and lastsibl(h, m, k) (indicating that, up to position k, the last seen modifier of h occurred at position m). The non-basic parts are the pairwise factors sibl(h, m, s), grand(g, h, m), and bigram(b, h, m); as well as each logical formula. Columns 3–4 indicate the number of parts of each kind, and the time complexity for solving each subproblem. For a sentence of length n, there are O(n3) parts and the total complexity is O(n3 log n). Vni=1 zi}. The convex hull of YOR-OUT is the following set: ZOR-OUT = I(z0, ... , zn) E [0, 1]n I z0 &gt; En i=1 zi, z0 � zi, bi = 1, . . . ,n} (Martins et al., 2010, Tab.1). The slave subproblem is: minimize 1 En i=0(zi − ai)2 2 w.r.t. (z0, ... , zn) E [0, 1]n s.t. z0 &gt; En=1 zi; z0 &lt; zi, bi = 1, ... , n. — — (21) The problem in Eq. 21 is more involved than the ones in Eqs. 19–20. Yet, there is still an efficient procedure with runtime O(n log n) (see App. E). By using the result above for negated variables, we are now endowed with a procedure for many other cases, such that AND-WITH-OUTPUT and formulas with universal quantifiers (e.g., R(x) := by : Q(x, y)). Up to a log-factor, the runtimes will be linear in the number of predicates. Larger Slaves. The o</context>
<context position="30461" citStr="Martins et al., 2010" startWordPosition="5441" endWordPosition="5444">his fact is included in App. F); • Resorting to tree, all siblings, grandparent, and non-projective arcs, recovers a multi-commodity flow configuration proposed by Martins et al. (2009a); the relaxation is also the same.12 The experimental results are shown in Tab. 2. For comparison, we include the best published results for each dataset (at the best of our knowledge), among transition-based parsers (Nivre et al., 2006; Huang and Sagae, 2010), graph-based parsers (McDonald et al., 2006; Koo and Collins, 2010), hybrid methods (Nivre and McDonald, 2008; Martins et al., 2008), and turbo parsers (Martins et al., 2010; Koo et al., 2010). Our full model achieved the best reported scores for 7 datasets. The last two columns show a consistent improvement (with the exceptions of Chinese and Arabic) when using the full set of features over a second order model with grandparent and consecutive siblings, which is our reproduction of the model of Koo et al. (2010).13 12Although Martins et al. (2009a) also incorporated consecutive siblings in one of their configurations, our constraints are tighter than theirs. See App. F. 13Note however that the actual results of Koo et al. (2010) are higher than our reproduction,</context>
<context position="32197" citStr="Martins et al. (2010)" startWordPosition="5723" endWordPosition="5726">7 [Ko10] 92.45 92.68 (+0.23) Danish 91.78 [Ko10] 91.70 91.86 (+0.16) Dutch 85.81 [Ko10] 84.77 85.53 (+0.76) German 91.49 [Ma10] 91.29 91.89 (+0.60) Japane. 93.42 [Ma10] 93.62 93.72 (+0.10) Portug. 93.03 [Ko10] 92.05 92.29 (+0.24) Slovene 86.21 [Ko10] 86.09 86.95 (+0.86) Spanish 87.04 [Ma10] 85.99 86.74 (+0.75) Swedish 91.36 [Ko10] 89.94 90.16 (+0.22) Turkish 77.55 [Ko10] 76.24 76.64 (+0.40) PTB §23 93.04 [KC10] 92.19 92.53 (+0.34) Table 2: Unlabeled attachment scores, excluding punctuation. In the second column, [Ma08] denotes Martins et al. (2008), [KC10] is Koo and Collins (2010), [Ma10] is Martins et al. (2010), and [Ko10] is Koo et al. (2010). In columns 3–4, “Full” is our full model, and “G+CS” is our reproduction of the model of Koo et al. (2010), i.e., the same as “Full” but with all features ablated excepted for grandparents and consecutive siblings. AF +G+CS +AS +NP Full PTB §22 91.02 92.13 92.32 92.36 92.41 PTB §23 91.36 92.19 92.41 92.50 92.53 Table 3: Feature ablation experiments. AF is an arcfactored model; +G+CS adds grandparent and consecutive siblings; +AS adds all-siblings; +NP adds nonprojective arcs; Full adds the bigram and directed paths. Feature ablation and error analysis. We con</context>
<context position="39320" citStr="Martins et al., 2010" startWordPosition="6920" endWordPosition="6923"> both methods, indicating that an exact solution of P has been found; for DD-ADMM we also plot the fraction of instances that converged to an accurate solution of P0 (primal and dual residuals &lt; 10−3) and hence can be stopped. mulation was introduced by Martins et al. (2009a), along with some of the parts considered here. Koo et al. (2010) proposed a subgradient-based dual decomposition method that elegantly combines head automata with maximum spanning tree algorithms; these parsers, as well as the loopy belief propagation method of Smith and Eisner (2008), are all instances of turbo parsers (Martins et al., 2010). DD-ADMM has been proposed and theoretically analyzed by Martins et al. (2011) for problems representable as factor graphs. The general ADMM method has a long-standing history in optimization (Hestenes, 1969; Powell, 1969; Glowinski and Marroco, 1975; Gabay and Mercier, 1976; Boyd et al., 2011). Other methods have been recently proposed to accelerate dual decomposition, such as Jojic et al. (2010) and Meshi and Globerson (2011) (the latter applying ADMM in the dual rather than the primal). While our paper shows limitations of the subgradient method when there are many overlapping components, </context>
</contexts>
<marker>Martins, Smith, Xing, Figueiredo, Aguiar, 2010</marker>
<rawString>A. F. T. Martins, N. A. Smith, E. P. Xing, M. A. T. Figueiredo, and P. M. Q. Aguiar. 2010. Turbo parsers: Dependency parsing by approximate variational inference. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>M A T Figueiredo</author>
<author>P M Q Aguiar</author>
<author>N A Smith</author>
<author>E P Xing</author>
</authors>
<title>An Augmented Lagrangian Approach to Constrained MAP Inference.</title>
<date>2011</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="3068" citStr="Martins et al., 2011" startWordPosition="452" endWordPosition="455"> extremely slow convergence (cf. Fig. 2). Unfortunately, a lightweight decomposition is not always at hand, either because the problem does not factor in a natural way, or because one would like to incorporate features that cannot be easily absorbed in few tractable components. Examples include features generated by statements in first-order logic, features that violate Markov assumptions, or history features such as the ones employed in transition-based parsers. To tackle the kind of problems above, we adopt DD-ADMM (Alg. 1), a recently proposed algorithm that accelerates dual decomposition (Martins et al., 2011). DD-ADMM retains the modularity of the subgradient-based method, but it speeds up consensus by regularizing each slave subproblem towards the averaged votes obtained in the previous round (cf. Eq. 14). While this yields more involved subproblems (with a quadratic term), we show that exact solutions can still be efficiently computed for all cases of interest, by using sort operations. As a result, we obtain parsers that can handle very rich features, do not require specifying a decomposition, and can be heavily parallelized. We demonstrate the success of the approach by presenting experiments </context>
<context position="13628" citStr="Martins et al., 2011" startWordPosition="2373" endWordPosition="2376">nt of the Lagrange multipliers which takes into consideration deviations from the average (Eq. 10). The subgradient method is guaranteed to converge to the solution of D (Eq. 7), for suitably chosen stepsizes (Shor, 1985; Bertsekas et al., 1999); it also provides a certificate of optimality in case the relaxation is tight (i.e., P = D) and the exact solution has been found. However, convergence is slow when S is large (as we will show in the experimental section), and no certificates are available when there is a relaxation gap (P &lt; P0). In the next section, we describe the DD-ADMM algorithm (Martins et al., 2011), which does not have these drawbacks and shares a similar simplicity. 3 Alternating Directions Method There are two reasons why subgradient-based dual decomposition is not completely satisfying: • it may take a long time to reach a consensus; • it puts all its resources in solving the dual problem D, and does not attempt to make progress in the primal P0, which is closer to our main concern.4 4Our main concern is P; however solving P&apos; is often a useful step towards that goal, either because a good rounding scheme exists, or because one may build tighter relaxations to approach P (Sontag et al</context>
<context position="18819" citStr="Martins et al. (2011)" startWordPosition="3297" endWordPosition="3300">approximate integer solution). See Kulesza and Pereira (2007) and Martins et al. (2009b) for details. Algorithm 1 ADMM-based Dual Decomposition 1: input: score functions (fs(.))Ss=1, parameters ρ, η, thresholds cP and ED. 2: initialize t +- 1 3: initialize u1(r) +- 0.5 and λ1 s(r) +- 0, ds, dr E ¯Rs 4: repeat 5: for each s = 1,...,S do 6: make a zs-update, yielding zt+1 s (Eq. 14) 7: end for 8: make a u-update, yielding ut+1 (Eq. 9) 9: make a λ-update, yielding λt+1 (Eq. 10) 10: t +- t + 1 11: until rt+1 P &lt; cP and rt+1 D &lt; ED (Eqs. 15–16) 12: output: relaxed primal and dual solutions u, z, λ Martins et al. (2011), convergence to the solution of P&apos; is guaranteed with a fixed stepsize ηt = τρ, with τ E [1, 1.618] (Glowinski and Le Tallec, 1989, Thm. 4.2). In our experiments, we set τ = 1.5, and adapt ρ as described in (Boyd et al., 2011, p.20).6 4 Solving the Subproblems In this section, we address the slave subproblems of DD-ADMM (Eq. 14). We show how these subproblems can be solved efficiently for several important cases that arise in NLP applications. Throughout, we assume that the score functions fs are linear, i.e., they can be written as fs(zs) = P rERs θs(r)zs(r). This is the case whenever a line</context>
<context position="39399" citStr="Martins et al. (2011)" startWordPosition="6932" endWordPosition="6935">MM we also plot the fraction of instances that converged to an accurate solution of P0 (primal and dual residuals &lt; 10−3) and hence can be stopped. mulation was introduced by Martins et al. (2009a), along with some of the parts considered here. Koo et al. (2010) proposed a subgradient-based dual decomposition method that elegantly combines head automata with maximum spanning tree algorithms; these parsers, as well as the loopy belief propagation method of Smith and Eisner (2008), are all instances of turbo parsers (Martins et al., 2010). DD-ADMM has been proposed and theoretically analyzed by Martins et al. (2011) for problems representable as factor graphs. The general ADMM method has a long-standing history in optimization (Hestenes, 1969; Powell, 1969; Glowinski and Marroco, 1975; Gabay and Mercier, 1976; Boyd et al., 2011). Other methods have been recently proposed to accelerate dual decomposition, such as Jojic et al. (2010) and Meshi and Globerson (2011) (the latter applying ADMM in the dual rather than the primal). While our paper shows limitations of the subgradient method when there are many overlapping components, this method may still be advantageous over ADMM in problems that are nicely dec</context>
<context position="40634" citStr="Martins et al. (2011)" startWordPosition="7120" endWordPosition="7123">ce it often allows reusing existing combinatorial machinery. Yet, the scenario we consider here is realistic in NLP, where we often have to deal with not-lightly-decomposable constrained problems (e.g., exploiting linguistic knowledge). 7 Conclusion We have introduced new feature-rich turbo parsers. Since exact decoding is intractable, we solve an LP relaxation through a recently proposed consensus algorithm, DD-ADMM, which is suitable for problems with many overlapping components. We study the empirical runtime and convergence properties of DD-ADMM, complementing the theoretical treatment in Martins et al. (2011). DD-ADMM compares favourably against the subgradient method in several aspects: it is faster to reach a consensus, it has better stopping conditions, and it works better in non-lightweight decompositions. While its slave subproblems are more involved, we derived closedform solutions for many cases of interest, such as first-order logic formulas and combinatorial factors. DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al. (2011). Non-logical constraints may also yield efficient subproblems, e.g., the</context>
</contexts>
<marker>Martins, Figueiredo, Aguiar, Smith, Xing, 2011</marker>
<rawString>A. F. T. Martins, M. A. T. Figueiredo, P. M. Q. Aguiar, N. A. Smith, and E. P. Xing. 2011. An Augmented Lagrangian Approach to Constrained MAP Inference. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Lerman</author>
<author>F Pereira</author>
</authors>
<title>Multilingual dependency analysis with a two-stage discriminative parser.</title>
<date>2006</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="6681" citStr="McDonald et al., 2006" startWordPosition="1078" endWordPosition="1081">n sequence labeling, it can be the set of possible labels at each position. Our only assumption is that we can “read out” y from the basic parts it contains. For convenience, we represent y as a binary vector, y = (y(r))r∈R, where y(r) = 1 if part r belongs to y, and 0 otherwise. Decomposition. We generalize the decomposition in Eq. 2 by considering sets Y1, ... , YS for S &gt; 2. Figure 1: Parts used by our parser. Arcs are the basic parts: any dependency tree can be “read out” from the arcs it contains. Consecutive siblings and grandparent parts introduce horizontal and vertical Markovization (McDonald et al., 2006; Carreras, 2007). We break the horizontal Markov assumption via all siblings parts and the vertical one through parts which indicate a directed path between two words. Inspired by transition-based parsers, we also adopt head bigram parts, which look at the heads attached to consecutive words. Finally, we follow Martins et al. (2009a) and have parts which indicate if an arc is non-projective (i.e., if it spans words that do not descend from its head). Each Ys is associated with its own set of parts Rs, in the same sense as above; we represent the elements of Ys as binary vectors zs = (zs(r))r∈</context>
<context position="30331" citStr="McDonald et al., 2006" startWordPosition="5419" endWordPosition="5423">e tree and consecutive sibling formulae gives one of the models in Koo et al. (2010), with the same linear relaxation (a proof of this fact is included in App. F); • Resorting to tree, all siblings, grandparent, and non-projective arcs, recovers a multi-commodity flow configuration proposed by Martins et al. (2009a); the relaxation is also the same.12 The experimental results are shown in Tab. 2. For comparison, we include the best published results for each dataset (at the best of our knowledge), among transition-based parsers (Nivre et al., 2006; Huang and Sagae, 2010), graph-based parsers (McDonald et al., 2006; Koo and Collins, 2010), hybrid methods (Nivre and McDonald, 2008; Martins et al., 2008), and turbo parsers (Martins et al., 2010; Koo et al., 2010). Our full model achieved the best reported scores for 7 datasets. The last two columns show a consistent improvement (with the exceptions of Chinese and Arabic) when using the full set of features over a second order model with grandparent and consecutive siblings, which is our reproduction of the model of Koo et al. (2010).13 12Although Martins et al. (2009a) also incorporated consecutive siblings in one of their configurations, our constraints </context>
</contexts>
<marker>McDonald, Lerman, Pereira, 2006</marker>
<rawString>R. McDonald, K. Lerman, and F. Pereira. 2006. Multilingual dependency analysis with a two-stage discriminative parser. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Meshi</author>
<author>A Globerson</author>
</authors>
<title>An Alternating Direction Method for Dual MAP LP Relaxation.</title>
<date>2011</date>
<booktitle>In ECML PKDD.</booktitle>
<contexts>
<context position="39752" citStr="Meshi and Globerson (2011)" startWordPosition="6987" endWordPosition="6990">utomata with maximum spanning tree algorithms; these parsers, as well as the loopy belief propagation method of Smith and Eisner (2008), are all instances of turbo parsers (Martins et al., 2010). DD-ADMM has been proposed and theoretically analyzed by Martins et al. (2011) for problems representable as factor graphs. The general ADMM method has a long-standing history in optimization (Hestenes, 1969; Powell, 1969; Glowinski and Marroco, 1975; Gabay and Mercier, 1976; Boyd et al., 2011). Other methods have been recently proposed to accelerate dual decomposition, such as Jojic et al. (2010) and Meshi and Globerson (2011) (the latter applying ADMM in the dual rather than the primal). While our paper shows limitations of the subgradient method when there are many overlapping components, this method may still be advantageous over ADMM in problems that are nicely decomposable, since it often allows reusing existing combinatorial machinery. Yet, the scenario we consider here is realistic in NLP, where we often have to deal with not-lightly-decomposable constrained problems (e.g., exploiting linguistic knowledge). 7 Conclusion We have introduced new feature-rich turbo parsers. Since exact decoding is intractable, w</context>
</contexts>
<marker>Meshi, Globerson, 2011</marker>
<rawString>O. Meshi and A. Globerson. 2011. An Alternating Direction Method for Dual MAP LP Relaxation. In ECML PKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>R McDonald</author>
</authors>
<title>Integrating graphbased and transition-based dependency parsers.</title>
<date>2008</date>
<booktitle>In ACL-HLT.</booktitle>
<contexts>
<context position="30397" citStr="Nivre and McDonald, 2008" startWordPosition="5430" endWordPosition="5433"> in Koo et al. (2010), with the same linear relaxation (a proof of this fact is included in App. F); • Resorting to tree, all siblings, grandparent, and non-projective arcs, recovers a multi-commodity flow configuration proposed by Martins et al. (2009a); the relaxation is also the same.12 The experimental results are shown in Tab. 2. For comparison, we include the best published results for each dataset (at the best of our knowledge), among transition-based parsers (Nivre et al., 2006; Huang and Sagae, 2010), graph-based parsers (McDonald et al., 2006; Koo and Collins, 2010), hybrid methods (Nivre and McDonald, 2008; Martins et al., 2008), and turbo parsers (Martins et al., 2010; Koo et al., 2010). Our full model achieved the best reported scores for 7 datasets. The last two columns show a consistent improvement (with the exceptions of Chinese and Arabic) when using the full set of features over a second order model with grandparent and consecutive siblings, which is our reproduction of the model of Koo et al. (2010).13 12Although Martins et al. (2009a) also incorporated consecutive siblings in one of their configurations, our constraints are tighter than theirs. See App. F. 13Note however that the actua</context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>J. Nivre and R. McDonald. 2008. Integrating graphbased and transition-based dependency parsers. In ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
<author>G Eryiˇgit</author>
<author>S Marinov</author>
</authors>
<title>Labeled pseudo-projective dependency parsing with support vector machines.</title>
<date>2006</date>
<booktitle>In Procs. of CoNLL.</booktitle>
<marker>Nivre, Hall, Nilsson, Eryiˇgit, Marinov, 2006</marker>
<rawString>J. Nivre, J. Hall, J. Nilsson, G. Eryiˇgit, and S. Marinov. 2006. Labeled pseudo-projective dependency parsing with support vector machines. In Procs. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>D Klein</author>
</authors>
<title>Sparse multi-scale grammars for discriminative latent variable parsing.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1319" citStr="Petrov and Klein, 2008" startWordPosition="179" endWordPosition="182">r logical constraints), the original subgradient algorithm is inefficient. We sidestep that difficulty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes. We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results. 1 Introduction The last years have witnessed increasingly accurate models for syntax, semantics, and machine translation (Chiang, 2007; Finkel et al., 2008; Petrov and Klein, 2008; Smith and Eisner, 2008; Martins et al., 2009a; Johansson and Nugues, 2008; Koo et al., 2010). The predictive power of such models stems from their ability to break locality assumptions. The resulting combinatorial explosion typically demands some form of approximate decoding, such as sampling, heuristic search, or variational inference. In this paper, we focus on parsers built from linear programming relaxations, the so-called “turbo parsers” (Martins et al., 2009a; Martins et al., 2010). Rush et al. (2010) applied dual decomposition as a way of combining models which alone permit efficient </context>
</contexts>
<marker>Petrov, Klein, 2008</marker>
<rawString>S. Petrov and D. Klein. 2008. Sparse multi-scale grammars for discriminative latent variable parsing. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Powell</author>
</authors>
<title>A method for nonlinear constraints in minimization problems.</title>
<date>1969</date>
<booktitle>Optimization,</booktitle>
<pages>283--298</pages>
<editor>In R. Fletcher, editor,</editor>
<publisher>Academic Press.</publisher>
<contexts>
<context position="14623" citStr="Powell, 1969" startWordPosition="2541" endWordPosition="2542">main concern.4 4Our main concern is P; however solving P&apos; is often a useful step towards that goal, either because a good rounding scheme exists, or because one may build tighter relaxations to approach P (Sontag et al., 2008; Rush and Collins, 2011). Taking a look back at the relaxed primal problem P0 (Eq. 5), we see that any primal feasible solution must satisfy the agreement constraints. This suggests that penalizing violations of these constraints could speed up consensus. Augmented Lagrangian. By adding a penalty term to Eq. 6, we obtain the augmented Lagrangian function (Hestenes, 1969; Powell, 1969): Ap(z, u, λ) = L(z, u, λ) − ρ 2 r∈Rs (11) where the parameter ρ &gt; 0 controls the intensity of the penalty. Augmented Lagrangian methods are well-known in the optimization community (see, e.g., Bertsekas et al. (1999), §4.2). They alternate updates to the λ-variables, while seeking to maximize Aρ with respect to z and u. In our case, however, this joint maximization poses difficulties, since the penalty term couples the two variables. The alternating directions method of multipliers (ADMM), coined by Gabay and Mercier (1976) and Glowinski and Marroco (1975), sidesteps this issue by performing </context>
<context position="39542" citStr="Powell, 1969" startWordPosition="6955" endWordPosition="6956">tion was introduced by Martins et al. (2009a), along with some of the parts considered here. Koo et al. (2010) proposed a subgradient-based dual decomposition method that elegantly combines head automata with maximum spanning tree algorithms; these parsers, as well as the loopy belief propagation method of Smith and Eisner (2008), are all instances of turbo parsers (Martins et al., 2010). DD-ADMM has been proposed and theoretically analyzed by Martins et al. (2011) for problems representable as factor graphs. The general ADMM method has a long-standing history in optimization (Hestenes, 1969; Powell, 1969; Glowinski and Marroco, 1975; Gabay and Mercier, 1976; Boyd et al., 2011). Other methods have been recently proposed to accelerate dual decomposition, such as Jojic et al. (2010) and Meshi and Globerson (2011) (the latter applying ADMM in the dual rather than the primal). While our paper shows limitations of the subgradient method when there are many overlapping components, this method may still be advantageous over ADMM in problems that are nicely decomposable, since it often allows reusing existing combinatorial machinery. Yet, the scenario we consider here is realistic in NLP, where we oft</context>
</contexts>
<marker>Powell, 1969</marker>
<rawString>M. Powell. 1969. A method for nonlinear constraints in minimization problems. In R. Fletcher, editor, Optimization, pages 283–298. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Richardson</author>
<author>P Domingos</author>
</authors>
<title>Markov logic networks.</title>
<date>2006</date>
<booktitle>Machine Learning,</booktitle>
<volume>62</volume>
<issue>1</issue>
<contexts>
<context position="10086" citStr="Richardson and Domingos, 2006" startWordPosition="1714" endWordPosition="1717">describe dual decomposition in a slightly different manner than Rush et al. (2010): we will first build a relaxation of P (called P0), in which the entire approximation is enclosed. Then, we dualize P0, yielding problem D. In the second step, the duality gap is zero, i.e., P0 and D are equivalent.2 Relaxation. For each s E {1, ... , S} we consider the convex hull of Ys, Zs = ( X P(zs)zs P(zs) &gt; 0, E p(Zs) = 11 . zs EYs Z. E&apos;9. J (4) gle factor in a factor graph (Smith and Eisner, 2008), or to a entire subgraph enclosing several factors (Koo et al., 2010), or even to a formula in Markov logic (Richardson and Domingos, 2006). In these examples, the basic parts may correspond to individual variable-value pairs. 2Instead of following the path P → P&apos; → D, Rush et al. (2010) go straight from P to D via a Lagrangian relaxation. The two formulations are equivalent for linear score functions. We have that Ys = Zs n Z|Rs|; hence, problem P (Eq. 3) is equivalent to one in which each Ys is replaced by Zs and the z-variables are constrained to be integer. By dropping the integer constraints, we obtain the following relaxed problem: P0 : maximize PSs=1 fs(zs) w.r.t. zs E Zs, ds (u(r))rER E R|R|, s.t. zs(r) = u(r), ds, r E If</context>
</contexts>
<marker>Richardson, Domingos, 2006</marker>
<rawString>M. Richardson and P. Domingos. 2006. Markov logic networks. Machine Learning, 62(1):107–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riedel</author>
<author>J Clarke</author>
</authors>
<title>Incremental integer linear programming for non-projective dependency parsing.</title>
<date>2006</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="37617" citStr="Riedel and Clarke (2006)" startWordPosition="6637" endWordPosition="6640">al LP. We observe that DD-ADMM is faster in some regimes but slower in others. For short sentences (&lt; 15 words), DD-ADMM tends to be faster. For longer sentences, CPLEX is quite effective as it uses good heuristics for the pivot steps in the simplex algorithm; however, we observed that it sometimes gets trapped on large problems. Note also that DD-ADMM is not fully optimized, and that it is much more amenable to parallelization than the simplex algorithm, since it is composed of many independent slaves. This suggests potentially significant speed-ups in multi-core environments. 6 Related Work Riedel and Clarke (2006) first formulated dependency parsing as an integer program, along with logical constraints. The multicommodity flow for% active 100 80 60 0 Time CPLEX (sec.) 246 92 91 90 UAS (%) 89 88 87 86 ADMM Full Subgrad Full ADMM Sec Ord Subgrad Sec Ord 850 200 400 600 800 1000 Iterations Stopping Criteria 0 200 400 600 800 1000 Iterations Certificates (%) 100 80 60 40 20 0 ADMM Full (Tol&lt;0.001) ADMM Full (Exact) Subgrad Full (Exact) ADMM Sec Ord (Tol&lt;0.001) ADMM Sec Ord (Exact) Subgrad Sec Ord (Exact) Accuracy Figure 2: UAS including punctuation (left) and fraction of optimality certificates (right) acc</context>
</contexts>
<marker>Riedel, Clarke, 2006</marker>
<rawString>S. Riedel and J. Clarke. 2006. Incremental integer linear programming for non-projective dependency parsing. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Rush</author>
<author>M Collins</author>
</authors>
<title>Exact decoding of syntactic translation models through lagrangian relaxation.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2232" citStr="Rush and Collins, 2011" startWordPosition="323" endWordPosition="326">pling, heuristic search, or variational inference. In this paper, we focus on parsers built from linear programming relaxations, the so-called “turbo parsers” (Martins et al., 2009a; Martins et al., 2010). Rush et al. (2010) applied dual decomposition as a way of combining models which alone permit efficient decoding, but whose combination is intractable. This results in a relaxation of the original problem that is elegantly solved with the subgradient algorithm. While this technique has proven quite effective in parsing (Koo et al., 2010; Auli and Lopez, 2011) as well as machine translation (Rush and Collins, 2011), we show here that its success is strongly tied to the ability of finding a “good” decomposition, i.e., one involving few overlapping components (or slaves). With many components, the subgradient algorithm exhibits extremely slow convergence (cf. Fig. 2). Unfortunately, a lightweight decomposition is not always at hand, either because the problem does not factor in a natural way, or because one would like to incorporate features that cannot be easily absorbed in few tractable components. Examples include features generated by statements in first-order logic, features that violate Markov assum</context>
<context position="12345" citStr="Rush and Collins, 2011" startWordPosition="2133" endWordPosition="2136">§3, in some sense, alleviates her from this concern). If the score functions are linear, i.e., of the form fs(zs) = Pr∈Rs θs(r)zs(r) for some vector θs = (θs (r))r∈Rs, then Eq. 8 becomes a linear program, for which a solution exists at a vertex of Zs (which 3This is guaranteed if the score functions fs are linear. (3) Rs. (5) Rs. (8) w.r.t. zs E Zs. maximize fs(zs) + PrE¯Rs λs(r)zs(r) 240 in turn is an element of Ys). Depending on the structure of the problem, Eq. 8 may be solved by brute force, dynamic programming, or specialized combinatorial algorithms (Rush et al., 2010; Koo et al., 2010; Rush and Collins, 2011). Applying the projected subgradient method (Komodakis et al., 2007; Rush et al., 2010) to the master problem (Eq. 7) yields a remarkably simple algorithm, which at each round t solves the subproblems in Eq. 8 for s = 1, ... , S, and then gathers these solutions (call them zt+1 s ) to compute an “averaged” vote for each basic part, ut+1 (r) = δir) �s:r∈¯Rs z;+1 (r), (9) where δ(r) = |{s : r E Rs is the number of components which contain part r. An update of the Lagrange variables follows, λt+1 s(r) = λt s(r) − ηt(zt+1 s (r) − ut+1(r)), (10) where ηt is a stepsize. Intuitively, the algorithm pu</context>
<context position="14260" citStr="Rush and Collins, 2011" startWordPosition="2482" endWordPosition="2485">oes not have these drawbacks and shares a similar simplicity. 3 Alternating Directions Method There are two reasons why subgradient-based dual decomposition is not completely satisfying: • it may take a long time to reach a consensus; • it puts all its resources in solving the dual problem D, and does not attempt to make progress in the primal P0, which is closer to our main concern.4 4Our main concern is P; however solving P&apos; is often a useful step towards that goal, either because a good rounding scheme exists, or because one may build tighter relaxations to approach P (Sontag et al., 2008; Rush and Collins, 2011). Taking a look back at the relaxed primal problem P0 (Eq. 5), we see that any primal feasible solution must satisfy the agreement constraints. This suggests that penalizing violations of these constraints could speed up consensus. Augmented Lagrangian. By adding a penalty term to Eq. 6, we obtain the augmented Lagrangian function (Hestenes, 1969; Powell, 1969): Ap(z, u, λ) = L(z, u, λ) − ρ 2 r∈Rs (11) where the parameter ρ &gt; 0 controls the intensity of the penalty. Augmented Lagrangian methods are well-known in the optimization community (see, e.g., Bertsekas et al. (1999), §4.2). They altern</context>
<context position="16882" citStr="Rush and Collins (2011)" startWordPosition="2940" endWordPosition="2943">vertex (in contrast to the subgradient method). We devote §4 to describing exact and efficient ways of solving the problem in Eq. 14 for important, widely used slaves. Before going into details, we mention another advantage of ADMM over the subgradient algorithm: it knows when to stop. Primal and dual residuals. Recall that the subgradient method provides optimality certificates when the relaxation is tight (P = P&apos;) and an exact solution of P has been found. While this is good enough when tight relaxations are frequent, as in the settings explored by Rush et al. (2010), Koo et al. (2010), and Rush and Collins (2011), it is hard to know when to stop when a relaxation gap exists. We would like to have similar guarantees concerning the relaxed primal P&apos;.5 A general weakness of subgradient algorithms is that they do not have this capacity, and so are usually stopped by specifying a maximum number of iterations. In contrast, ADMM allows to keep track of primal and dual residuals (Boyd et al., 2011). This allows providing certificates not only for the exact solution of P (when the relaxation is tight), but also to terminate when a near optimal solution of the relaxed problem P&apos; has been found. Theprimal residu</context>
</contexts>
<marker>Rush, Collins, 2011</marker>
<rawString>A. M. Rush and M. Collins. 2011. Exact decoding of syntactic translation models through lagrangian relaxation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rush</author>
<author>D Sontag</author>
<author>M Collins</author>
<author>T Jaakkola</author>
</authors>
<title>On dual decomposition and linear programming relaxations for natural language processing.</title>
<date>2010</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1833" citStr="Rush et al. (2010)" startWordPosition="258" endWordPosition="261">or syntax, semantics, and machine translation (Chiang, 2007; Finkel et al., 2008; Petrov and Klein, 2008; Smith and Eisner, 2008; Martins et al., 2009a; Johansson and Nugues, 2008; Koo et al., 2010). The predictive power of such models stems from their ability to break locality assumptions. The resulting combinatorial explosion typically demands some form of approximate decoding, such as sampling, heuristic search, or variational inference. In this paper, we focus on parsers built from linear programming relaxations, the so-called “turbo parsers” (Martins et al., 2009a; Martins et al., 2010). Rush et al. (2010) applied dual decomposition as a way of combining models which alone permit efficient decoding, but whose combination is intractable. This results in a relaxation of the original problem that is elegantly solved with the subgradient algorithm. While this technique has proven quite effective in parsing (Koo et al., 2010; Auli and Lopez, 2011) as well as machine translation (Rush and Collins, 2011), we show here that its success is strongly tied to the ability of finding a “good” decomposition, i.e., one involving few overlapping components (or slaves). With many components, the subgradient algo</context>
<context position="4917" citStr="Rush et al. (2010)" startWordPosition="750" endWordPosition="753">ng the model must obey certain practical considerations. If efficiency is the major concern, a simple model is usually chosen so that Eq. 1 can be solved efficiently, at the cost of limited expressive power. If we care more about accuracy, a model with richer features and more involved score functions may be designed. Decoding, however, will be more expensive, and approximations are often necessary. A typical source of intractability comes from the combinatorial explosion inherent in the composition of two or more tractable models (Bar-Hillel et al., 1964; Tromble and Eisner, 2006). Recently, Rush et al. (2010) have proposed a dual decomposition framework to address NLP problems in which the global score decomposes as f(y) = f1(z1)+f2(z2), where z1 and z2 are two overlapping “views” of the output, so that Eq. 1 becomes: maximize f1(z1) + f2 (z2L) (2) w.r.t. z1 E Y1, z2 E 02 s.t. z1 — z2. Above, the notation z1 — z2 means that z1 and z2 “agree on their overlaps,” and an isomorphism Y ^- {(z1, z2) E Y1 x Y2 1 z1 — z2} is assumed. We next formalize these notions and proceed to compositions of an arbitrary number of models. Of special interest is the unexplored setting where this number is very large an</context>
<context position="9348" citStr="Rush et al., 2010" startWordPosition="1575" endWordPosition="1578">is is equivalent to the existence of a witness vector (u(r))r∈R such that zs(r) = u(r), ds, r E ¯Rs. With this setup, assuming that the score function decomposes as f(z) = PSs=1 fs(zs), the decoding problem (which extends Eq. 2 for S &gt; 2) becomes: P : maximize PSs=1 fs(zs) w.r.t. zs //E Ys, ds (u(r))rER E R|R|, s.t. zs(r) = u(r), ds, r E We call the equality constraints expressed in the last line the “agreement constraints.” It is these constraints that complicate the problem, which would otherwise be exactly separable into S subproblems. The dual decomposition method (Komodakis et al., 2007; Rush et al., 2010) builds an approximation by dualizing out these constraints, as we describe next. 2.3 Dual Decomposition We describe dual decomposition in a slightly different manner than Rush et al. (2010): we will first build a relaxation of P (called P0), in which the entire approximation is enclosed. Then, we dualize P0, yielding problem D. In the second step, the duality gap is zero, i.e., P0 and D are equivalent.2 Relaxation. For each s E {1, ... , S} we consider the convex hull of Ys, Zs = ( X P(zs)zs P(zs) &gt; 0, E p(Zs) = 11 . zs EYs Z. E&apos;9. J (4) gle factor in a factor graph (Smith and Eisner, 2008), </context>
<context position="12302" citStr="Rush et al., 2010" startWordPosition="2125" endWordPosition="2128">n (the framework that we describe in §3, in some sense, alleviates her from this concern). If the score functions are linear, i.e., of the form fs(zs) = Pr∈Rs θs(r)zs(r) for some vector θs = (θs (r))r∈Rs, then Eq. 8 becomes a linear program, for which a solution exists at a vertex of Zs (which 3This is guaranteed if the score functions fs are linear. (3) Rs. (5) Rs. (8) w.r.t. zs E Zs. maximize fs(zs) + PrE¯Rs λs(r)zs(r) 240 in turn is an element of Ys). Depending on the structure of the problem, Eq. 8 may be solved by brute force, dynamic programming, or specialized combinatorial algorithms (Rush et al., 2010; Koo et al., 2010; Rush and Collins, 2011). Applying the projected subgradient method (Komodakis et al., 2007; Rush et al., 2010) to the master problem (Eq. 7) yields a remarkably simple algorithm, which at each round t solves the subproblems in Eq. 8 for s = 1, ... , S, and then gathers these solutions (call them zt+1 s ) to compute an “averaged” vote for each basic part, ut+1 (r) = δir) �s:r∈¯Rs z;+1 (r), (9) where δ(r) = |{s : r E Rs is the number of components which contain part r. An update of the Lagrange variables follows, λt+1 s(r) = λt s(r) − ηt(zt+1 s (r) − ut+1(r)), (10) where ηt i</context>
<context position="16834" citStr="Rush et al. (2010)" startWordPosition="2931" endWordPosition="2934">for linear score functions may not be at a vertex (in contrast to the subgradient method). We devote §4 to describing exact and efficient ways of solving the problem in Eq. 14 for important, widely used slaves. Before going into details, we mention another advantage of ADMM over the subgradient algorithm: it knows when to stop. Primal and dual residuals. Recall that the subgradient method provides optimality certificates when the relaxation is tight (P = P&apos;) and an exact solution of P has been found. While this is good enough when tight relaxations are frequent, as in the settings explored by Rush et al. (2010), Koo et al. (2010), and Rush and Collins (2011), it is hard to know when to stop when a relaxation gap exists. We would like to have similar guarantees concerning the relaxed primal P&apos;.5 A general weakness of subgradient algorithms is that they do not have this capacity, and so are usually stopped by specifying a maximum number of iterations. In contrast, ADMM allows to keep track of primal and dual residuals (Boyd et al., 2011). This allows providing certificates not only for the exact solution of P (when the relaxation is tight), but also to terminate when a near optimal solution of the rel</context>
<context position="19628" citStr="Rush et al., 2010" startWordPosition="3448" endWordPosition="3451"> as described in (Boyd et al., 2011, p.20).6 4 Solving the Subproblems In this section, we address the slave subproblems of DD-ADMM (Eq. 14). We show how these subproblems can be solved efficiently for several important cases that arise in NLP applications. Throughout, we assume that the score functions fs are linear, i.e., they can be written as fs(zs) = P rERs θs(r)zs(r). This is the case whenever a linear model is used, in which case θs(r) = 1 δ(r)w · O(x, r), where w is a weight vector and O(x, r) is a feature vector. It is also the scenario studied in previous work in dual decomposition (Rush et al., 2010). Under this assumption, and discarding constant terms, the slave subproblem in Eq. 14 becomes: Xmax Rs θs(r)zs(r) − ρX (zs(r) − as(r))2. zsEZs 2 rERs\ rE¯Rs (17) where as(r) = ut(r)+ρ−1(θs(r)+λts(r)). Since Zs is a polytope, Eq. 17 is a quadratic program, which can be solved with a general purpose solver. However, that does not exploit the structure of Zs and is inefficient when |Rs |is large. We next show that for many cases, a closed-form solution is available and 6Briefly, we initialize p = 0.03 and then increase/decrease p by a factor of 2 whenever the primal residual becomes &gt; 10 times l</context>
</contexts>
<marker>Rush, Sontag, Collins, Jaakkola, 2010</marker>
<rawString>A. Rush, D. Sontag, M. Collins, and T. Jaakkola. 2010. On dual decomposition and linear programming relaxations for natural language processing. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Shor</author>
</authors>
<title>Minimization methods for nondifferentiable functions.</title>
<date>1985</date>
<publisher>Springer.</publisher>
<contexts>
<context position="13227" citStr="Shor, 1985" startWordPosition="2301" endWordPosition="2302">m zt+1 s ) to compute an “averaged” vote for each basic part, ut+1 (r) = δir) �s:r∈¯Rs z;+1 (r), (9) where δ(r) = |{s : r E Rs is the number of components which contain part r. An update of the Lagrange variables follows, λt+1 s(r) = λt s(r) − ηt(zt+1 s (r) − ut+1(r)), (10) where ηt is a stepsize. Intuitively, the algorithm pushes for a consensus among the slaves (Eq. 9), via an adjustment of the Lagrange multipliers which takes into consideration deviations from the average (Eq. 10). The subgradient method is guaranteed to converge to the solution of D (Eq. 7), for suitably chosen stepsizes (Shor, 1985; Bertsekas et al., 1999); it also provides a certificate of optimality in case the relaxation is tight (i.e., P = D) and the exact solution has been found. However, convergence is slow when S is large (as we will show in the experimental section), and no certificates are available when there is a relaxation gap (P &lt; P0). In the next section, we describe the DD-ADMM algorithm (Martins et al., 2011), which does not have these drawbacks and shares a similar simplicity. 3 Alternating Directions Method There are two reasons why subgradient-based dual decomposition is not completely satisfying: • i</context>
</contexts>
<marker>Shor, 1985</marker>
<rawString>N. Shor. 1985. Minimization methods for nondifferentiable functions. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Smith</author>
<author>J Eisner</author>
</authors>
<title>Dependency parsing by belief propagation.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1343" citStr="Smith and Eisner, 2008" startWordPosition="183" endWordPosition="186">the original subgradient algorithm is inefficient. We sidestep that difficulty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes. We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results. 1 Introduction The last years have witnessed increasingly accurate models for syntax, semantics, and machine translation (Chiang, 2007; Finkel et al., 2008; Petrov and Klein, 2008; Smith and Eisner, 2008; Martins et al., 2009a; Johansson and Nugues, 2008; Koo et al., 2010). The predictive power of such models stems from their ability to break locality assumptions. The resulting combinatorial explosion typically demands some form of approximate decoding, such as sampling, heuristic search, or variational inference. In this paper, we focus on parsers built from linear programming relaxations, the so-called “turbo parsers” (Martins et al., 2009a; Martins et al., 2010). Rush et al. (2010) applied dual decomposition as a way of combining models which alone permit efficient decoding, but whose comb</context>
<context position="9946" citStr="Smith and Eisner, 2008" startWordPosition="1690" endWordPosition="1693"> 2007; Rush et al., 2010) builds an approximation by dualizing out these constraints, as we describe next. 2.3 Dual Decomposition We describe dual decomposition in a slightly different manner than Rush et al. (2010): we will first build a relaxation of P (called P0), in which the entire approximation is enclosed. Then, we dualize P0, yielding problem D. In the second step, the duality gap is zero, i.e., P0 and D are equivalent.2 Relaxation. For each s E {1, ... , S} we consider the convex hull of Ys, Zs = ( X P(zs)zs P(zs) &gt; 0, E p(Zs) = 11 . zs EYs Z. E&apos;9. J (4) gle factor in a factor graph (Smith and Eisner, 2008), or to a entire subgraph enclosing several factors (Koo et al., 2010), or even to a formula in Markov logic (Richardson and Domingos, 2006). In these examples, the basic parts may correspond to individual variable-value pairs. 2Instead of following the path P → P&apos; → D, Rush et al. (2010) go straight from P to D via a Lagrangian relaxation. The two formulations are equivalent for linear score functions. We have that Ys = Zs n Z|Rs|; hence, problem P (Eq. 3) is equivalent to one in which each Ys is replaced by Zs and the z-variables are constrained to be integer. By dropping the integer constra</context>
<context position="21379" citStr="Smith and Eisner, 2008" startWordPosition="3782" endWordPosition="3785"> z2, z12i ∈ [0, 1]3 |z12 ≤ z1, z12 ≤ z2, z12 ≥ z1 + z2 − 1}, as shown by Martins et al. (2010). In this case, problem (17) can be written as max θ12z12 − ρ2[(z1 − a1)2 + (z2 − a2)2] w.r.t. hz1, z2, z12i ∈ [0, 1]3 (18) s.t. z12 ≤ z1, z12 ≤ z2, z12 ≥ z1 + z2 − 1 and has a closed form solution (see App. B). Uniqueness Quantification and XOR. Many problems involve constraining variables to take a single value: for example, in dependency parsing, a modifier can only take one head. This can be expressed as the statement ∃!y : Q(y) in first-order logic,8 or as a one-hot XOR factor in a factor graph (Smith and Eisner, 2008; Martins et al., 2010). In this case, RXOR = {r1, ... , rn}, and YXOR = {hz1, ... , zni ∈ {0, 1}n |Pni=1 zi = 1}. The convex hull of YXOR is ZXOR = {hz1, ... , zni ∈ [0, 1]n |Pn i=1 zi = 1}. Assume for the sake of simplicity that all parts in RXOR are basic.9 Up to a constant, the slave subproblem becomes: minimize 1 Pn i=1(zi − ai)2 2 w.r.t. hz1, ... , zni ∈ [0,1]n (19) s.t. Pi zi = 1. This is the problem of projecting onto the probability simplex, which can be done in O(n log n) time via a sort operation (see App. C).10 7This matches the asymptotic time that would be necessary to solve the </context>
<context position="39261" citStr="Smith and Eisner (2008)" startWordPosition="6910" endWordPosition="6913"> full model. The right plot shows optimality certificates for both methods, indicating that an exact solution of P has been found; for DD-ADMM we also plot the fraction of instances that converged to an accurate solution of P0 (primal and dual residuals &lt; 10−3) and hence can be stopped. mulation was introduced by Martins et al. (2009a), along with some of the parts considered here. Koo et al. (2010) proposed a subgradient-based dual decomposition method that elegantly combines head automata with maximum spanning tree algorithms; these parsers, as well as the loopy belief propagation method of Smith and Eisner (2008), are all instances of turbo parsers (Martins et al., 2010). DD-ADMM has been proposed and theoretically analyzed by Martins et al. (2011) for problems representable as factor graphs. The general ADMM method has a long-standing history in optimization (Hestenes, 1969; Powell, 1969; Glowinski and Marroco, 1975; Gabay and Mercier, 1976; Boyd et al., 2011). Other methods have been recently proposed to accelerate dual decomposition, such as Jojic et al. (2010) and Meshi and Globerson (2011) (the latter applying ADMM in the dual rather than the primal). While our paper shows limitations of the subg</context>
</contexts>
<marker>Smith, Eisner, 2008</marker>
<rawString>D. Smith and J. Eisner. 2008. Dependency parsing by belief propagation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Sontag</author>
<author>T Meltzer</author>
<author>A Globerson</author>
<author>Y Weiss</author>
<author>T Jaakkola</author>
</authors>
<title>Tightening LP relaxations for MAP using message-passing.</title>
<date>2008</date>
<booktitle>In UAI.</booktitle>
<contexts>
<context position="14235" citStr="Sontag et al., 2008" startWordPosition="2478" endWordPosition="2481">t al., 2011), which does not have these drawbacks and shares a similar simplicity. 3 Alternating Directions Method There are two reasons why subgradient-based dual decomposition is not completely satisfying: • it may take a long time to reach a consensus; • it puts all its resources in solving the dual problem D, and does not attempt to make progress in the primal P0, which is closer to our main concern.4 4Our main concern is P; however solving P&apos; is often a useful step towards that goal, either because a good rounding scheme exists, or because one may build tighter relaxations to approach P (Sontag et al., 2008; Rush and Collins, 2011). Taking a look back at the relaxed primal problem P0 (Eq. 5), we see that any primal feasible solution must satisfy the agreement constraints. This suggests that penalizing violations of these constraints could speed up consensus. Augmented Lagrangian. By adding a penalty term to Eq. 6, we obtain the augmented Lagrangian function (Hestenes, 1969; Powell, 1969): Ap(z, u, λ) = L(z, u, λ) − ρ 2 r∈Rs (11) where the parameter ρ &gt; 0 controls the intensity of the penalty. Augmented Lagrangian methods are well-known in the optimization community (see, e.g., Bertsekas et al. (</context>
</contexts>
<marker>Sontag, Meltzer, Globerson, Weiss, Jaakkola, 2008</marker>
<rawString>D. Sontag, T. Meltzer, A. Globerson, Y. Weiss, and T Jaakkola. 2008. Tightening LP relaxations for MAP using message-passing. In UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>R Johansson</author>
<author>A Meyers</author>
<author>L M`arquez</author>
<author>J Nivre</author>
</authors>
<title>The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<journal>CoNLL.</journal>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>M. Surdeanu, R. Johansson, A. Meyers, L. M`arquez, and J. Nivre. 2008. The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies. CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>C Guestrin</author>
<author>D Koller</author>
</authors>
<title>Max-margin Markov networks.</title>
<date>2003</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="5693" citStr="Taskar et al., 2003" startWordPosition="893" endWordPosition="896">overlapping “views” of the output, so that Eq. 1 becomes: maximize f1(z1) + f2 (z2L) (2) w.r.t. z1 E Y1, z2 E 02 s.t. z1 — z2. Above, the notation z1 — z2 means that z1 and z2 “agree on their overlaps,” and an isomorphism Y ^- {(z1, z2) E Y1 x Y2 1 z1 — z2} is assumed. We next formalize these notions and proceed to compositions of an arbitrary number of models. Of special interest is the unexplored setting where this number is very large and each component very simple. 2.2 Decomposition into Parts A crucial step in the design of structured predictors is that of decomposing outputs into parts (Taskar et al., 2003). We assume the following setup: Basic parts. We let R be a set of basic parts, such that each element y E Y can be identified with a subset of R. The exact meaning of a “basic part” is problem dependent. For example, in dependency parsing, R can be the set of all possible dependency arcs (see Fig. 1); in phrase-based parsing, it can be the set of possible spans; in sequence labeling, it can be the set of possible labels at each position. Our only assumption is that we can “read out” y from the basic parts it contains. For convenience, we represent y as a binary vector, y = (y(r))r∈R, where y(</context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2003</marker>
<rawString>B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin Markov networks. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R W Tromble</author>
<author>J Eisner</author>
</authors>
<title>A fast finite-state relaxation method for enforcing global constraints on sequence decoding.</title>
<date>2006</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="4887" citStr="Tromble and Eisner, 2006" startWordPosition="745" endWordPosition="748">for Computational Linguistics Designing the model must obey certain practical considerations. If efficiency is the major concern, a simple model is usually chosen so that Eq. 1 can be solved efficiently, at the cost of limited expressive power. If we care more about accuracy, a model with richer features and more involved score functions may be designed. Decoding, however, will be more expensive, and approximations are often necessary. A typical source of intractability comes from the combinatorial explosion inherent in the composition of two or more tractable models (Bar-Hillel et al., 1964; Tromble and Eisner, 2006). Recently, Rush et al. (2010) have proposed a dual decomposition framework to address NLP problems in which the global score decomposes as f(y) = f1(z1)+f2(z2), where z1 and z2 are two overlapping “views” of the output, so that Eq. 1 becomes: maximize f1(z1) + f2 (z2L) (2) w.r.t. z1 E Y1, z2 E 02 s.t. z1 — z2. Above, the notation z1 — z2 means that z1 and z2 “agree on their overlaps,” and an isomorphism Y ^- {(z1, z2) E Y1 x Y2 1 z1 — z2} is assumed. We next formalize these notions and proceed to compositions of an arbitrary number of models. Of special interest is the unexplored setting wher</context>
</contexts>
<marker>Tromble, Eisner, 2006</marker>
<rawString>R.W. Tromble and J. Eisner. 2006. A fast finite-state relaxation method for enforcing global constraints on sequence decoding. In Proc. of NAACL, pages 423– 430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wainwright</author>
<author>M Jordan</author>
</authors>
<title>Graphical Models, Exponential Families, and Variational Inference.</title>
<date>2008</date>
<publisher>Now Publishers.</publisher>
<marker>Wainwright, Jordan, 2008</marker>
<rawString>M. Wainwright and M. Jordan. 2008. Graphical Models, Exponential Families, and Variational Inference. Now Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In IWPT.</booktitle>
<contexts>
<context position="28212" citStr="Yamada and Matsumoto (2003)" startWordPosition="5077" endWordPosition="5080">s not rule out the possibility of using this method otherwise. Eckstein and Bertsekas (1992) show that the ADMM algorithm may still converge when the z-updates are inexact. Hence the method may still work if the slaves are solved numerically up to some accuracy. We defer this to future investigation. 5 Experiments: Dependency Parsing We used 14 datasets with non-projective dependencies from the CoNLL-2006 and CoNLL-2008 shared tasks (Buchholz and Marsi, 2006; Surdeanu et al., 2008). We also used a projective English dataset derived from the Penn Treebank by applying the standard head rules of Yamada and Matsumoto (2003).11 We did not force the parser to output projective trees or unique roots for any of the datasets; everything is learned from the data. We trained by running 10 iterations of the cost-augmented MIRA algorithm (Crammer et al., 2006) with LP-relaxed decoding, as in Martins et al. (2009b). Following common practice (Charniak and Johnson, 2005; Carreras et al., 2008), we employed a coarse-to-fine procedure to prune away unlikely candidate arcs, as described by Koo and Collins (2010). To ensure valid parse trees at test time, we rounded fractional 11As usual, we train on sections §02–21, use §22 a</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>H. Yamada and Y. Matsumoto. 2003. Statistical dependency analysis with support vector machines. In IWPT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>