<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000037">
<title confidence="0.999371">
A Pipeline Approach for Syntactic and Semantic Dependency Parsing
</title>
<author confidence="0.994133">
Yotaro Watanabe and Masakazu Iwatate and Masayuki Asahara and Yuji Matsumoto
</author>
<affiliation confidence="0.8745645">
Nara Institute of Science and Technology, Japan
8916-5, Takayama, Ikoma, Nara, Japan, 630-0192
</affiliation>
<email confidence="0.990556">
{yotaro-w, masakazu-i, masayu-a, matsu}@is.naist.jp
</email>
<sectionHeader confidence="0.998565" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999989916666667">
This paper describes our system for syn-
tactic and semantic dependency parsing
to participate the shared task of CoNLL-
2008. We use a pipeline approach, in
which syntactic dependency parsing, word
sense disambiguation, and semantic role
labeling are performed separately: Syn-
tactic dependency parsing is performed
by a tournament model with a support
vector machine; word sense disambigua-
tion is performed by a nearest neighbour
method in a compressed feature space by
probabilistic latent semantic indexing; and
semantic role labeling is performed by
a an online passive-aggressive algorithm.
The submitted result was 79.10 macro-
average F1 for the joint task, 87.18% syn-
tactic dependencies LAS, and 70.84 se-
mantic dependencies F1. After the dead-
line, we constructed the other configura-
tion, which achieved 80.89 F1 for the joint
task, and 74.53 semantic dependencies F1.
The result shows that the configuration of
pipeline is a crucial issue in the task.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999987571428571">
This paper presents the description of our system
in CoNLL-2008 shared task. We split the shared
task into five sub-problems – syntactic dependency
parsing, syntactic dependency label classification,
predicate identification, word sense disambigua-
tion, and semantic role labeling. The overview
of our system is illustrated in Figure 1. Our de-
</bodyText>
<note confidence="0.498358">
⃝c 2008. Licensed under the Creative Commons
</note>
<footnote confidence="0.99393">
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<figureCaption confidence="0.999835">
Figure 1: Overview of the System
</figureCaption>
<bodyText confidence="0.999956666666667">
pendency parsing module is based on a tourna-
ment model (Iida et al., 2003), in which a depen-
dency attachment is estimated in step-ladder tour-
nament matches. The relative preference of the at-
tachment is modeled by one-on-one match in the
tournament. Iwatate et al. (Iwatate et al., 2008)
initially proposed the method for Japanese depen-
dency parsing, and we applied it to other languages
by relaxing some constraints (Section 2.1). Depen-
dency label classification is performed by a linear-
chain sequential labeling on the dependency sib-
lings like McDonald’s schemata (McDonald et al.,
2006). We use an online passive-aggressive al-
gorithm (Crammer et al., 2006) for linear-chain
sequential labeling (Section 2.2). We also use
the other linear-chain sequential labeling method
to annotate whether each word is a predicate or
not (Section 2.3). If an identified predicate has
more than one sense, a nearest neighbour classifier
disambiguates the word sense candidates (Section
2.4). We use an online passive-aggressive algo-
rithm again for the semantic role labeling (Section
2.5). The machine learning algorithms used in sep-
arated modules are diverse due to role sharing.1
</bodyText>
<footnote confidence="0.970067333333333">
&apos;Unlabeled dependency parsing was done by Iwatate, de-
pendency label classification and semantic role labeling was
done by Watanabe, predicate identification and word sense
</footnote>
<page confidence="0.819666">
228
</page>
<reference confidence="0.206639">
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 228–232
Manchester, August 2008
</reference>
<bodyText confidence="0.99992325">
We attempt to construct a framework in which
each module passes k-best solutions and the last
semantic role labeling module performs rerank-
ing of the k-best solutions using the overall infor-
mation. Unfortunately, we couldn’t complete the
framework before the deadline of the test run. Our
method is not a “joint learning” approach but a
pipeline approach.
</bodyText>
<sectionHeader confidence="0.99844" genericHeader="introduction">
2 Methods
</sectionHeader>
<subsectionHeader confidence="0.99949">
2.1 Unlabeled Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.9997412">
The detailed description of the tournament model-
based Japanese dependency parsing is found in
(Iwatate et al., 2008). The original Iwatate’s pars-
ing algorithm was for Japanese, which is for a
strictly head-final language. We adapt the algo-
rithm to English in this shared task. The tour-
nament model chooses the most likely candidate
head of each of the focused words in a step-
ladder tournament. For a given word, the al-
gorithm repeats to compare two candidate heads
and finds the most plausible head in the series
of a tournament. On each comparison, the win-
ner is chosen by an SVM binary classifier with
a quadratic polynomial kernel2. The model uses
different algorithms for training example gener-
ation and parsing. Figures 2 and 3 show train-
ing example generation and parsing algorithm, re-
spectively. Time complexity of both algorithms is
O(n2) for the number of words in an input sen-
tence. Below, we present the features for SVM
</bodyText>
<listItem confidence="0.760348">
// N: # of tokens in input sentence
// true_head[j]: token j’s head at
</listItem>
<bodyText confidence="0.568726166666667">
// training data
// gen(j,i1,i2,LEFT): generate an example
// where token j is dependent of i1
// gen(j,i1,i2,RIGHT): generate an example
// where token j is dependent of i2
// Token 0 is the virtual ROOT.
</bodyText>
<equation confidence="0.981448666666667">
for j = 1 to N-1 do
h = true_head[j];
for i = 0 to h-1 do
</equation>
<construct confidence="0.9779415">
if i!=j then gen(j,i,h,RIGHT);
for i = h+1 to N do
if i!=j then gen(j,h,i,LEFT);
end-for;
</construct>
<figureCaption confidence="0.9308635">
Figure 2: Pseudo Code of Training Example Gen-
eration
</figureCaption>
<footnote confidence="0.85040575">
disambiguation was done by Asahara, and all tasks were su-
pervised by Matsumoto.
2We use TinySVM as an SVM classifier. chasen.org/
∼taku/software/TinySVM/
</footnote>
<table confidence="0.941703909090909">
// N: # of tokens in input sentence
// head[]: (analyzed-) head of tokens
// classify(j,i1,i2): ask SVM
// which candidate (i1 or i2) is
// more likely for head of j.
// return LEFT if i1 wins.
// return RIGHT if i2 wins.
// cands.push_back(k): add token index k
// to the end of cands.
// cands.erase(i): remove i-th element
// from cands.
</table>
<equation confidence="0.959910466666667">
for j = 1 to N do
cands = [];
for i = 0 to N do
if i!=j then cands.push_back(i);
end-for;
while cands.size() &gt; 1 do
if classify(j,cands[0],
cands[1]) = LEFT then
cands.erase(1);
else
cands.erase(0);
end-if;
end-while;
head[j] = cands[0];
end-for;
</equation>
<figureCaption confidence="0.99567">
Figure 3: Pseudo Code of Parsing Algorithm
</figureCaption>
<bodyText confidence="0.999942">
in our tournament model. The FORM, LEMMA,
GPOS(for training), PPOS(for testing, instead of
GPOS), SPLIT FORM, SPLIT LEMMA, PPOSS
in the following tokens were used as the features:
</bodyText>
<listItem confidence="0.9899246">
• Dependent, candidate1, candidate2
• Immediately-adjacent tokens of dependent, candidate1,
candidate2, respectively
• All tokens between dependent-candidate1, dependent-
candidate2, candidate1-candidate2, respectively
</listItem>
<bodyText confidence="0.999044384615385">
We also used the distance feature: distance (1 or
2-5 or 6+ tokens) between dependent-candidate1,
dependent-candidate2, and candidate1-candidate2.
Features corresponding to the candidates, includ-
ing the distance feature, have a prefix that indicates
its side: “L-”(the candidate appears on left-hand-
side of the dependent) or “R-”(appears on right-
hand-side of the dependent). Training an SVM
model with all examples is time-consuming, and
split the examples by the dependent GPOS for
training (PPOS for testing, instead of GPOS3) to
run SVM training in parallel. Since the number of
examples with the dependent PPOS:IN, NN, NNP
</bodyText>
<footnote confidence="0.9921415">
3We cannot use GPOS for testing due to the shared task
regulation.
</footnote>
<page confidence="0.997314">
229
</page>
<bodyText confidence="0.9994055">
is still large, we used only first 1.5 million exam-
ples for the dependent GPOS. Note that, the algo-
rithm does not check the well-formedness of de-
pendency trees 4.
</bodyText>
<subsectionHeader confidence="0.996414">
2.2 Dependency Label Classification
</subsectionHeader>
<bodyText confidence="0.9995676">
This phase labels a dependency relation label to
each word in a parse tree produced in the preced-
ing phase. (McDonald et al., 2006) suggests that
edges of head xi and its dependents xj1, ..., xjM
are highly correlated, and capturing these corre-
lation improves classification accuracy. In their
approach, edges of a head and its dependents
ei,j1, ..., ei,jM are classified sequentially, and then
Viterbi algorithm is performed to find the highest
scoring label sequence. We take a similar approach
with some simplification. In our system, each edge
is classified deterministically, and the previous de-
cision is used as a feature for the subsequent clas-
sification.
We use an online passive aggressive algorithm
(Crammer et al., 2006) 5 for dependency label clas-
sification since it converges fast, gives good per-
formance and can be implemented easily. The fea-
tures used in this phase are primarily similar to that
of (McDonald et al., 2006).
</bodyText>
<figureCaption confidence="0.868010833333333">
Word features: SPLIT LEMMA, PPOS, affix (lengths 2
and 3) of the head and the dependent.
Position: Position relation between the head and the depen-
dent (Is the head anterior to dependent?). Is the word
top of the sentence? Is the word last of the sentence?
Context features: SPLIT LEMMA, PPOS, affix (lengths 2
and 3) of the nearest left/right word. SPLIT LEMMA
and PPOS bigram (ww, wp, pw, pp) of the head and the
dependent (window size 5).
Sibling features: SPLIT LEMMA, PPOS, affix (lengths 2
and 3) of the dependent’s nearest left and right siblings
in the dependency tree.
</figureCaption>
<bodyText confidence="0.94241825">
Other features: The number of dependent’s children.
Whether the dependent and the dependent’s grand
parent SPLIT LEMMA/PPOS are the same. The
previous classification result (previous label).
</bodyText>
<subsectionHeader confidence="0.991968">
2.3 Predicate Identification
</subsectionHeader>
<bodyText confidence="0.709848">
This phase solves which word can be a predi-
cate. In the predicate spotting, the linear-chain
4We tried to make a k-best cascaded model among the
modules. The latter module can check the well-formedness
of the tree. The current implementation skips this well-
formedness checking.
</bodyText>
<footnote confidence="0.9335065">
5We use PA algorithm among PA, PA-I and PA-II in
(Crammer et al., 2006).
</footnote>
<bodyText confidence="0.987477142857143">
CRF (Lafferty et al., 2001) annotates whether the
word is a predicate or not. The FORM, LEMMA
(itself, and whether the LEMMA is registered in
the PropBank/NomBank frames), SPLIT FORM,
SPLIT LEMMA, PPOSS within 5 token window
size are used as the features. We also use bigram
features within 3 token window size and trigram
features within 5 token window size for FORM,
LEMMA, SPLIT FORM, SPLIT LEMMA, and
PPOSS. The main reason why we use a sequence
labeling method for predicate identification was
to relax the effect of the tagging error of PPOS
and PPOSS. However, we will show later that this
module aggravates the total performance.
</bodyText>
<subsectionHeader confidence="0.994189">
2.4 Word Sense Disambiguation
</subsectionHeader>
<bodyText confidence="0.999992961538462">
For the word sense disambiguation, we use 1-
nearest neighbour method in a compressed fea-
ture space by probabilistic latent semantic index-
ing (PLSI). We trained the word sense disambigua-
tion model from the example sentences in the train-
ing/development data and PropBank/NomBank
frames. The metric in the nearest neighbour
method is based on the occurrence of LEMMA
in the example sentences. However, the exam-
ples in the PropBank/NomBank do not contain the
lemma information. To lemmatize the words in
the PropBank/NomBank, we compose a lemma-
tizer from the FORM-LEMMA table in the train-
ing and development.6 Since the metric space
is very sparse, PLSI (Hofmann, 1999) is used to
reduce the metric space dimensions. We used
KL-divergence between two examples of P(di|zk)
of P(di, wj) = ∑k P(di|zk)P(wj|zk)P(zk) as
hemi-metric for the nearest neighbour method7,
in which di ∈ D is an example sentence in the
training/devel/test data and PropBank/NomBank
frames; wj ∈ W is LEMMA; and zk ∈ Z is a
latent class. We use |Z |= 100, which gave the
best performance in the development data. Note,
we transductively used the test data for the PLSI
modeling within the test run period.
</bodyText>
<subsectionHeader confidence="0.99074">
2.5 Semantic Role Labeling
</subsectionHeader>
<bodyText confidence="0.998652333333333">
While semantic role labeling task is generally per-
formed by two phases: argument identification and
argument classification, we did not divide the task
</bodyText>
<footnote confidence="0.9924565">
6We are not violating the closed track regulation to build
the lemmatizer. If a word in the PropBank/NomBank is not in
the training/development data, we give up lemmatization.
7We kP ut data zk)lo P(dinput data|zk)
</footnote>
<page confidence="0.408512">
use
</page>
<bodyText confidence="0.618073">
— (din p I 9 P(d1-nearest datalzk)
as hemi-metric. It is a non-commutative measure.
</bodyText>
<page confidence="0.987822">
230
</page>
<bodyText confidence="0.987101457627118">
into the two phases. That is, argument candidates
are directly assigned a particular semantic role la-
bel. We did not employ any candidate filtering pro-
cedure, so argument candidates consist of words in
any predicate-word pair. The argument candidates
that have no roles are assigned “NONE” label. For
the reason that described in Section 2.2 (fast con-
vergence and good performance), we use an on-
line passive aggressive algorithm for learning the
semantic role classifiers.
Useful features for argument classification of
verb and noun predicates are different. For exam-
ple, voice (active or passive) is essential for verb
predicate’s argument classification. On the other
hand, presence of a genitive word is useful for
noun predicate’s argument classification. For this
reason, we created two models: argument classifier
for verb predicates and that for noun predicates.
Semantic frames are useful information for se-
mantic role classification. Generally, obligatory
arguments not included in semantic frames do not
appear in actual texts. For this reason, we use
PropBank/NomBank semantic frames for seman-
tic role pruning. Suppose semantic roles in the se-
mantic frame are Fi = {A0, A1, A2, A3}. Since
obligatory arguments are {A0...AA}, the remain-
ing arguments {A4, A5, AA} are removed from
label candidates.
For verb predicates, the features used in our sys-
tem are based on (Hacioglu, 2004). We also em-
ployed some other features proposed in (Gildea
and Jurafsky, 2002; Pradhan et al., 2004b). For
noun predicates, the features are primarily based
on (Pradhan et al., 2004a). The features that we
defined for semantic role labeling are as follows:
Word features: SPLIT LEMMA and PPOS of the predicate,
dependent and dependent’s head, and its conjunctions.
Dependency label: The dependency label between the argu-
ment candidate and the its head.
Family: The position of the argument candidate with respect
to the predicate position over the dependency tree (e.g.,
child, sibling).
Position: The position of the head of the dependency relation
with respect to the predicate position in the sentence.
Pattern: The left-to-right chain of the PPOS/dependency la-
bels of the predicate’s children.
Context features: PPOS of the nearest left/right word.
Path features: SPLIT LEMMA, PPOS and dependency la-
bel paths between the predicate and the argument can-
didate, and its path bi-gram.
Distance: The number of paths between the predicate and
the argument candidate.
Voice: Voice of the predicate (active or passive) and voice-
position conjunction (for verb predicates).
Is predicate plural: Whether the predicate is singular or
plural (for noun predicates).
Genitives between the predicate and the argument: Is
there a genitive word between the predicate and the
argument? (for noun predicates)
</bodyText>
<sectionHeader confidence="0.999866" genericHeader="background">
3 Results
</sectionHeader>
<bodyText confidence="0.999667888888889">
Table 1 shows the result of our system. The pro-
posed method was effective in dependency pars-
ing (rank 3rd), but was not good in semantic role
labeling (rank 9th). One reason of the result of
semantic role labeling could be usages of Prop-
Bank/NomBank frames. We did not achieve the
maximum use of the resources, hence the design of
features and the choice of learning algorithm may
not be optimal.
</bodyText>
<figureCaption confidence="0.9942">
Figure 4: Overview of the Modified System
</figureCaption>
<bodyText confidence="0.999976214285714">
The other reason is the design of the pipeline.
We changed the design of the pipeline after the
test run. The overview of the modified system
is illustrated in Figure 4. After the syntactic de-
pendency parsing, we limited the predicate can-
didates as verbs and nouns by PPOSS, and fil-
tered the argument candidates by Xue’s method
(Xue and Palmer, 2004). Next, the candidate pair
of predicate-argument was classified by an online
passive-aggressive algorithm as shown in Section
2.5. Finally, the word sense of the predicate is de-
termined by the module in Section 2.4. The new
result is scores with ∗ in Table 1. The result means
that the first design was not the best for the task.
</bodyText>
<sectionHeader confidence="0.99775" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999816">
We would like to thank the CoNLL-2008 shared
task organizers and the data providers (Surdeanu
et al., 2008).
</bodyText>
<page confidence="0.991124">
231
</page>
<table confidence="0.992461111111111">
Problem All WSJ Brown Rank
Complete Problem 79.10 (80.89∗) 80.30 (82.06∗) 69.29 (71.32 ∗) 9th
Semantic Dependency 70.84 (74.53∗) 72.37 (76.01 ∗) 58.21 (62.41 ∗) 9th
Semantic Role Labeling 67.92 (72.31∗) 69.31 (73.62 ∗) 56.42 (61.64 ∗) -
Predicate Identification &amp; Word Sense Disambiguation 77.20 (79.17∗) 79.02 (80.99 ∗) 62.10 (64.03 ∗) -
Syntactic Dependency (Labeled) 87.18 88.06 80.17 3rd
Syntactic Label Accuracy 91.63 92.31 86.26 -
Unlabeled Syntactic Dependency Unlabeled 90.20 90.73 85.94 -
The scores with ∗ mark are our post-evaluation results.
</table>
<tableCaption confidence="0.999876">
Table 1: The Results – Closed Challenge
</tableCaption>
<sectionHeader confidence="0.997612" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999582094594595">
Buchholz, Sabine and Erwin Marsi. 2006. CoNLL-
X Shared Task on Multilingual Dependency Parsing.
In CoNLL-2006: Proceedings of the Tenth Confer-
ence on Computational Natural Language Learning,
pages 149–164.
Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai
Shalev-Schwarz, and Yoram Singer. 2006. Online
Passive-Agressive Algorithms. Journal of Machine
Learning Research, 7:551–585.
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic
Labeling of Semantic Roles. Computational Lin-
guistics, 28(3):245–288.
Hacioglu, Kadri. 2004. Semantic role labeling using
dependency trees. In COLING-2004: Proceedings
of the 20th International Conference on Computa-
tional Linguistics, pages 1273–1276.
Hofmann, Thomas. 1999. Probabilistic Latent Seman-
tic Indexing. In SIGIR-1999: Proceedings of the
22nd Annual International ACM SIGIR Conference
on Research and Development in Informatino Re-
trieval, pages 50–57.
Iida, Ryu, Kentaro Inui, Hiroya Takamura, and Yuji
Matsumoto. 2003. Incorporating Contextual Cues
in Trainable Models for Coreference Resolution. In
EACL Workshop ‘The Computational Treatment of
Anaphora’, pages 23–30.
Iwatate, Masakazu, Masayuki Asahara, and Yuji Mat-
sumoto. 2008. Japanese Dependency Parsing Using
a Tournament Model. In COLING-2008: Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (To Appear).
Lafferty, John D., Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional Random Fields:
Probabilistic Models for Segmenting and Labeling
Sequence Data. In ICML-1001: Proceedings of
the Eighteenth International Conference on Machine
Learning, pages 282–289.
McDonald, Ryan, Kevin Lerman, and Fernando
Pereira. 2006. Multilingual Dependency Analysis
with a Two-Stage Discriminative Parser. In CoNLL-
2006: Proceedings of the Tenth Conference on Com-
putational Natural Language Learning, pages 216–
220.
Nivre, Joakim, Johan Hall, Sandra K¨ubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 Shared Task on De-
pendency Parsing. In CoNLL-2007: Proceedings of
the CoNLL Shared Task Session of EMNLP-CoNLL-
2007, pages 915–932.
Pradhan, Sameer, Honglin Sun, Wayne Ward, James H.
Martin, and Dan Jurafsky. 2004a. Parsing Argu-
ments of Nominalizations in English and Chinese. In
HLT-NAACL-2004: Proceedings of the Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 141–144.
Pradhan, Sameer, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Dan Jurafsky. 2004b. Shal-
low Semantic Parsing Using Support Vector Ma-
chines. In HLT-NAACL-2004: Proceedings of the
Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 233–240.
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Lluis M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. In CoNLL-2008:
Proceedings of the 12th Conference on Computa-
tional Natural Language Learning.
Xue, Nianwen and Martha Palmer. 2004. Calibrating
Features for Semantic Role Labeling. In EMNLP-
2004: Proceedings of 2004 Conference on Empirical
Methods in Natural Language Processing, pages 88–
94.
</reference>
<page confidence="0.995095">
232
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.583529">
<title confidence="0.7966245">A Pipeline Approach for Syntactic and Semantic Dependency Parsing Watanabe Iwatate Asahara</title>
<affiliation confidence="0.996142">Nara Institute of Science and Technology,</affiliation>
<address confidence="0.999902">8916-5, Takayama, Ikoma, Nara, Japan,</address>
<email confidence="0.994445">masakazu-i,masayu-a,</email>
<abstract confidence="0.99968108">This paper describes our system for syntactic and semantic dependency parsing to participate the shared task of CoNLL- 2008. We use a pipeline approach, in which syntactic dependency parsing, word sense disambiguation, and semantic role labeling are performed separately: Syntactic dependency parsing is performed by a tournament model with a support vector machine; word sense disambiguation is performed by a nearest neighbour method in a compressed feature space by probabilistic latent semantic indexing; and semantic role labeling is performed by a an online passive-aggressive algorithm. The submitted result was 79.10 macroaverage F1 for the joint task, 87.18% syntactic dependencies LAS, and 70.84 semantic dependencies F1. After the deadline, we constructed the other configuration, which achieved 80.89 F1 for the joint task, and 74.53 semantic dependencies F1. The result shows that the configuration of pipeline is a crucial issue in the task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>CoNLL</author>
</authors>
<date>2008</date>
<booktitle>Proceedings of the 12th Conference on Computational Natural Language Learning,</booktitle>
<pages>228--232</pages>
<location>Manchester,</location>
<marker>CoNLL, 2008</marker>
<rawString>CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 228–232 Manchester, August 2008</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLLX Shared Task on Multilingual Dependency Parsing. In</title>
<date>2006</date>
<booktitle>CoNLL-2006: Proceedings of the Tenth Conference on Computational Natural Language Learning,</booktitle>
<pages>149--164</pages>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Buchholz, Sabine and Erwin Marsi. 2006. CoNLLX Shared Task on Multilingual Dependency Parsing. In CoNLL-2006: Proceedings of the Tenth Conference on Computational Natural Language Learning, pages 149–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai Shalev-Schwarz</author>
<author>Yoram Singer</author>
</authors>
<title>Online Passive-Agressive Algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="2480" citStr="Crammer et al., 2006" startWordPosition="364" endWordPosition="367"> tournament model (Iida et al., 2003), in which a dependency attachment is estimated in step-ladder tournament matches. The relative preference of the attachment is modeled by one-on-one match in the tournament. Iwatate et al. (Iwatate et al., 2008) initially proposed the method for Japanese dependency parsing, and we applied it to other languages by relaxing some constraints (Section 2.1). Dependency label classification is performed by a linearchain sequential labeling on the dependency siblings like McDonald’s schemata (McDonald et al., 2006). We use an online passive-aggressive algorithm (Crammer et al., 2006) for linear-chain sequential labeling (Section 2.2). We also use the other linear-chain sequential labeling method to annotate whether each word is a predicate or not (Section 2.3). If an identified predicate has more than one sense, a nearest neighbour classifier disambiguates the word sense candidates (Section 2.4). We use an online passive-aggressive algorithm again for the semantic role labeling (Section 2.5). The machine learning algorithms used in separated modules are diverse due to role sharing.1 &apos;Unlabeled dependency parsing was done by Iwatate, dependency label classification and sem</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Schwarz, Singer, 2006</marker>
<rawString>Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai Shalev-Schwarz, and Yoram Singer. 2006. Online Passive-Agressive Algorithms. Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic Labeling of Semantic Roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Gildea, Daniel and Daniel Jurafsky. 2002. Automatic Labeling of Semantic Roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kadri Hacioglu</author>
</authors>
<title>Semantic role labeling using dependency trees.</title>
<date>2004</date>
<booktitle>In COLING-2004: Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>1273--1276</pages>
<marker>Hacioglu, 2004</marker>
<rawString>Hacioglu, Kadri. 2004. Semantic role labeling using dependency trees. In COLING-2004: Proceedings of the 20th International Conference on Computational Linguistics, pages 1273–1276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic Latent Semantic Indexing. In</title>
<date>1999</date>
<booktitle>SIGIR-1999: Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Informatino Retrieval,</booktitle>
<pages>50--57</pages>
<marker>Hofmann, 1999</marker>
<rawString>Hofmann, Thomas. 1999. Probabilistic Latent Semantic Indexing. In SIGIR-1999: Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Informatino Retrieval, pages 50–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryu Iida</author>
<author>Kentaro Inui</author>
<author>Hiroya Takamura</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Incorporating Contextual Cues in Trainable Models for Coreference Resolution.</title>
<date>2003</date>
<booktitle>In EACL Workshop ‘The Computational Treatment of Anaphora’,</booktitle>
<pages>23--30</pages>
<contexts>
<context position="1896" citStr="Iida et al., 2003" startWordPosition="272" endWordPosition="275"> presents the description of our system in CoNLL-2008 shared task. We split the shared task into five sub-problems – syntactic dependency parsing, syntactic dependency label classification, predicate identification, word sense disambiguation, and semantic role labeling. The overview of our system is illustrated in Figure 1. Our de⃝c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Figure 1: Overview of the System pendency parsing module is based on a tournament model (Iida et al., 2003), in which a dependency attachment is estimated in step-ladder tournament matches. The relative preference of the attachment is modeled by one-on-one match in the tournament. Iwatate et al. (Iwatate et al., 2008) initially proposed the method for Japanese dependency parsing, and we applied it to other languages by relaxing some constraints (Section 2.1). Dependency label classification is performed by a linearchain sequential labeling on the dependency siblings like McDonald’s schemata (McDonald et al., 2006). We use an online passive-aggressive algorithm (Crammer et al., 2006) for linear-chai</context>
</contexts>
<marker>Iida, Inui, Takamura, Matsumoto, 2003</marker>
<rawString>Iida, Ryu, Kentaro Inui, Hiroya Takamura, and Yuji Matsumoto. 2003. Incorporating Contextual Cues in Trainable Models for Coreference Resolution. In EACL Workshop ‘The Computational Treatment of Anaphora’, pages 23–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masakazu Iwatate</author>
<author>Masayuki Asahara</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese Dependency Parsing Using a Tournament Model. In</title>
<date>2008</date>
<booktitle>COLING-2008: Proceedings of the 22nd International Conference on Computational Linguistics (To Appear).</booktitle>
<contexts>
<context position="2108" citStr="Iwatate et al., 2008" startWordPosition="307" endWordPosition="310">ation, word sense disambiguation, and semantic role labeling. The overview of our system is illustrated in Figure 1. Our de⃝c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Figure 1: Overview of the System pendency parsing module is based on a tournament model (Iida et al., 2003), in which a dependency attachment is estimated in step-ladder tournament matches. The relative preference of the attachment is modeled by one-on-one match in the tournament. Iwatate et al. (Iwatate et al., 2008) initially proposed the method for Japanese dependency parsing, and we applied it to other languages by relaxing some constraints (Section 2.1). Dependency label classification is performed by a linearchain sequential labeling on the dependency siblings like McDonald’s schemata (McDonald et al., 2006). We use an online passive-aggressive algorithm (Crammer et al., 2006) for linear-chain sequential labeling (Section 2.2). We also use the other linear-chain sequential labeling method to annotate whether each word is a predicate or not (Section 2.3). If an identified predicate has more than one s</context>
</contexts>
<marker>Iwatate, Asahara, Matsumoto, 2008</marker>
<rawString>Iwatate, Masakazu, Masayuki Asahara, and Yuji Matsumoto. 2008. Japanese Dependency Parsing Using a Tournament Model. In COLING-2008: Proceedings of the 22nd International Conference on Computational Linguistics (To Appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In</title>
<date>2001</date>
<booktitle>ICML-1001: Proceedings of the Eighteenth International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>Lafferty, John D., Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In ICML-1001: Proceedings of the Eighteenth International Conference on Machine Learning, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Kevin Lerman</author>
<author>Fernando Pereira</author>
</authors>
<title>Multilingual Dependency Analysis with a Two-Stage Discriminative Parser. In</title>
<date>2006</date>
<booktitle>CoNLL2006: Proceedings of the Tenth Conference on Computational Natural Language Learning,</booktitle>
<pages>216--220</pages>
<contexts>
<context position="2410" citStr="McDonald et al., 2006" startWordPosition="353" endWordPosition="356"> Figure 1: Overview of the System pendency parsing module is based on a tournament model (Iida et al., 2003), in which a dependency attachment is estimated in step-ladder tournament matches. The relative preference of the attachment is modeled by one-on-one match in the tournament. Iwatate et al. (Iwatate et al., 2008) initially proposed the method for Japanese dependency parsing, and we applied it to other languages by relaxing some constraints (Section 2.1). Dependency label classification is performed by a linearchain sequential labeling on the dependency siblings like McDonald’s schemata (McDonald et al., 2006). We use an online passive-aggressive algorithm (Crammer et al., 2006) for linear-chain sequential labeling (Section 2.2). We also use the other linear-chain sequential labeling method to annotate whether each word is a predicate or not (Section 2.3). If an identified predicate has more than one sense, a nearest neighbour classifier disambiguates the word sense candidates (Section 2.4). We use an online passive-aggressive algorithm again for the semantic role labeling (Section 2.5). The machine learning algorithms used in separated modules are diverse due to role sharing.1 &apos;Unlabeled dependenc</context>
</contexts>
<marker>McDonald, Lerman, Pereira, 2006</marker>
<rawString>McDonald, Ryan, Kevin Lerman, and Fernando Pereira. 2006. Multilingual Dependency Analysis with a Two-Stage Discriminative Parser. In CoNLL2006: Proceedings of the Tenth Conference on Computational Natural Language Learning, pages 216– 220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<date>2007</date>
<booktitle>The CoNLL 2007 Shared Task on Dependency Parsing. In CoNLL-2007: Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL2007,</booktitle>
<pages>915--932</pages>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Nivre, Joakim, Johan Hall, Sandra K¨ubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 Shared Task on Dependency Parsing. In CoNLL-2007: Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL2007, pages 915–932.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Honglin Sun</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
<author>Dan Jurafsky</author>
</authors>
<title>Parsing Arguments of Nominalizations in English and Chinese.</title>
<date>2004</date>
<booktitle>In HLT-NAACL-2004: Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>141--144</pages>
<marker>Pradhan, Sun, Ward, Martin, Jurafsky, 2004</marker>
<rawString>Pradhan, Sameer, Honglin Sun, Wayne Ward, James H. Martin, and Dan Jurafsky. 2004a. Parsing Arguments of Nominalizations in English and Chinese. In HLT-NAACL-2004: Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 141–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>Kadri Hacioglu</author>
<author>James H Martin</author>
<author>Dan Jurafsky</author>
</authors>
<title>Shallow Semantic Parsing Using Support Vector Machines. In</title>
<date>2004</date>
<booktitle>HLT-NAACL-2004: Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>233--240</pages>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2004</marker>
<rawString>Pradhan, Sameer, Wayne Ward, Kadri Hacioglu, James H. Martin, and Dan Jurafsky. 2004b. Shallow Semantic Parsing Using Support Vector Machines. In HLT-NAACL-2004: Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 233–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Lluis M`arquez</author>
<author>Joakim Nivre</author>
</authors>
<date>2008</date>
<booktitle>The CoNLL-2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies. In CoNLL-2008: Proceedings of the 12th Conference on Computational Natural Language Learning.</booktitle>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>Surdeanu, Mihai, Richard Johansson, Adam Meyers, Lluis M`arquez, and Joakim Nivre. 2008. The CoNLL-2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies. In CoNLL-2008: Proceedings of the 12th Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Calibrating Features for Semantic Role Labeling. In</title>
<date>2004</date>
<booktitle>EMNLP2004: Proceedings of 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>88--94</pages>
<marker>Xue, Palmer, 2004</marker>
<rawString>Xue, Nianwen and Martha Palmer. 2004. Calibrating Features for Semantic Role Labeling. In EMNLP2004: Proceedings of 2004 Conference on Empirical Methods in Natural Language Processing, pages 88– 94.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>