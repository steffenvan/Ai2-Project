<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000233">
<title confidence="0.974426">
Word Sense Disambiguation Improves Information Retrieval
</title>
<author confidence="0.998833">
Zhi Zhong and Hwee Tou Ng
</author>
<affiliation confidence="0.999866">
Department of Computer Science
National University of Singapore
</affiliation>
<address confidence="0.868122">
13 Computing Drive, Singapore 117417
</address>
<email confidence="0.997655">
{zhongzhi, nght}@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.995538" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9985214">
Previous research has conflicting conclu-
sions on whether word sense disambiguation
(WSD) systems can improve information re-
trieval (IR) performance. In this paper, we
propose a method to estimate sense distribu-
tions for short queries. Together with the
senses predicted for words in documents, we
propose a novel approach to incorporate word
senses into the language modeling approach
to IR and also exploit the integration of syn-
onym relations. Our experimental results on
standard TREC collections show that using the
word senses tagged by a supervised WSD sys-
tem, we obtain significant improvements over
a state-of-the-art IR system.
</bodyText>
<sectionHeader confidence="0.998986" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999925163265306">
Word sense disambiguation (WSD) is the task of
identifying the correct meaning of a word in context.
As a basic semantic understanding task at the lexi-
cal level, WSD is a fundamental problem in natural
language processing. It can be potentially used as
a component in many applications, such as machine
translation (MT) and information retrieval (IR).
In recent years, driven by Senseval/Semeval
workshops, WSD systems achieve promising perfor-
mance. In the application of WSD to MT, research
has shown that integrating WSD in appropriate ways
significantly improves the performance of MT sys-
tems (Chan et al., 2007; Carpuat and Wu, 2007).
In the application to IR, WSD can bring two kinds
of benefits. First, queries may contain ambiguous
words (terms), which have multiple meanings. The
ambiguities of these query words can hurt retrieval
precision. Identifying the correct meaning of the
ambiguous words in both queries and documents
can help improve retrieval precision. Second, query
words may have tightly related meanings with other
words not in the query. Making use of these relations
between words can improve retrieval recall.
Overall, IR systems can potentially benefit from
the correct meanings of words provided by WSD
systems. However, in previous investigations of the
usage of WSD in IR, different researchers arrived
at conflicting observations and conclusions. Some
of the early research showed a drop in retrieval per-
formance by using word senses (Krovetz and Croft,
1992; Voorhees, 1993). Some other experiments ob-
served improvements by integrating word senses in
IR systems (Sch¨utze and Pedersen, 1995; Gonzalo
et al., 1998; Stokoe et al., 2003; Kim et al., 2004).
This paper proposes the use of word senses to
improve the performance of IR. We propose an ap-
proach to annotate the senses for short queries. We
incorporate word senses into the language modeling
(LM) approach to IR (Ponte and Croft, 1998), and
utilize sense synonym relations to further improve
the performance. Our evaluation on standard TREC1
data sets shows that supervised WSD outperforms
two other WSD baselines and significantly improves
IR.
The rest of this paper is organized as follows. In
Section 2, we first review previous work using WSD
in IR. Section 3 introduces the LM approach to IR,
including the pseudo relevance feedback method.
We describe our WSD system and the method of
</bodyText>
<footnote confidence="0.984555">
1http://trec.nist.gov/
</footnote>
<page confidence="0.952664">
273
</page>
<note confidence="0.9859975">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 273–282,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999735333333333">
generating word senses for query terms in Section
4, followed by presenting our novel method of in-
corporating word senses and their synonyms into the
LM approach in Section 5. We present experiments
and analyze the results in Section 6. Finally, we con-
clude in Section 7.
</bodyText>
<sectionHeader confidence="0.999781" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999984325581395">
Many previous studies have analyzed the benefits
and the problems of applying WSD to IR. Krovetz
and Croft (1992) studied the sense matches between
terms in query and the document collection. They
concluded that the benefits of WSD in IR are not as
expected because query words have skewed sense
distribution and the collocation effect from other
query terms already performs some disambiguation.
Sanderson (1994; 2000) used pseudowords to intro-
duce artificial word ambiguity in order to study the
impact of sense ambiguity on IR. He concluded that
because the effectiveness of WSD can be negated
by inaccurate WSD performance, high accuracy of
WSD is an essential requirement to achieve im-
provement. In another work, Gonzalo et al. (1998)
used a manually sense annotated corpus, SemCor, to
study the effects of incorrect disambiguation. They
obtained significant improvements by representing
documents and queries with accurate senses as well
as synsets (synonym sets). Their experiment also
showed that with the synset representation, which
included synonym information, WSD with an error
rate of 40%–50% can still improve IR performance.
Their later work (Gonzalo et al., 1999) verified that
part of speech (POS) information is discriminatory
for IR purposes.
Several works attempted to disambiguate terms
in both queries and documents with the senses pre-
defined in hand-crafted sense inventories, and then
used the senses to perform indexing and retrieval.
Voorhees (1993) used the hyponymy (“IS-A”) rela-
tion in WordNet (Miller, 1990) to disambiguate the
polysemous nouns in a text. In her experiments, the
performance of sense-based retrieval is worse than
stem-based retrieval on all test collections. Her anal-
ysis showed that inaccurate WSD caused the poor
results.
Stokoe et al. (2003) employed a fine-grained
WSD system with an accuracy of 62.1% to dis-
ambiguate terms in both the text collections and
the queries in their experiments. Their evalua-
tion on TREC collections achieved significant im-
provements over a standard term based vector space
model. However, it is hard to judge the effect
of word senses because of the overall poor perfor-
mances of their baseline method and their system.
Instead of using fine-grained sense inventory, Kim
et al. (2004) tagged words with 25 root senses of
nouns in WordNet. Their retrieval method main-
tained the stem-based index and adjusted the term
weight in a document according to its sense match-
ing result with the query. They attributed the im-
provement achieved on TREC collections to their
coarse-grained, consistent, and flexible sense tag-
ging method. The integration of senses into the tra-
ditional stem-based index overcomes some of the
negative impact of disambiguation errors.
Different from using predefined sense inventories,
Sch¨utze and Pedersen (1995) induced the sense in-
ventory directly from the text retrieval collection.
For each word, its occurrences were clustered into
senses based on the similarities of their contexts.
Their experiments showed that using senses im-
proved retrieval performance, and the combination
of word-based ranking and sense-based ranking can
further improve performance. However, the cluster-
ing process of each word is a time consuming task.
Because the sense inventory is collection dependent,
it is also hard to expand the text collection without
re-doing preprocessing.
Many studies investigated the expansion effects
by using knowledge sources from thesauri. Some
researchers achieved improvements by expanding
the disambiguated query words with synonyms and
some other information from WordNet (Voorhees,
1994; Liu et al., 2004; Liu et al., 2005; Fang, 2008).
The usage of knowledge sources from WordNet in
document expansion also showed improvements in
IR systems (Cao et al., 2005; Agirre et al., 2010).
The previous work shows that the WSD errors can
easily neutralize its positive effect. It is important
to reduce the negative impact of erroneous disam-
biguation, and the integration of senses into tradi-
tional term index, such as stem-based index, is a pos-
sible solution. The utilization of semantic relations
has proved to be helpful for IR. It is also interest-
</bodyText>
<page confidence="0.995932">
274
</page>
<bodyText confidence="0.989168">
ing to investigate the utilization of semantic relations
among senses in IR.
</bodyText>
<sectionHeader confidence="0.822177" genericHeader="method">
3 The Language Modeling Approach to IR
</sectionHeader>
<bodyText confidence="0.9998365">
This section describes the LM approach to IR and
the pseudo relevance feedback approach.
</bodyText>
<subsectionHeader confidence="0.999037">
3.1 The language modeling approach
</subsectionHeader>
<bodyText confidence="0.999956266666667">
In the language modeling approach to IR, language
models are constructed for each query q and each
document d in a text collection C. The documents
in C are ranked by the distance to a given query q
according to the language models. The most com-
monly used language model in IR is the unigram
model, in which terms are assumed to be indepen-
dent of each other. In the rest of this paper, language
model will refer to the unigram language model.
One of the commonly used measures of the sim-
ilarity between query model and document model
is negative Kullback-Leibler (KL) divergence (Laf-
ferty and Zhai, 2001). With unigram model, the neg-
ative KL-divergence between model θq of query q
and model θd of document d is calculated as follows:
</bodyText>
<equation confidence="0.97052225">
−D(θq||θd)=−
t∈V
� p(t|θq) log p(t|θq)
p(t|θd)
p(t|θq)log p(t|θd)− � p(t|θq)log p(t|θq)
t∈V
�= p(t|θq)log p(t|θd) + E(θq), (1)
t∈V
</equation>
<bodyText confidence="0.999066">
where p(t|θq) and p(t|θd) are the generative proba-
bilities of a term t from the models θq and θd, V is
the vocabulary of C, and E(θq) is the entropy of q.
Define tf (t, d) and tf (t, q) as the frequencies of t
in d and q, respectively. Normally, p(t|θq) is calcu-
lated with maximum likelihood estimation (MLE):
</bodyText>
<equation confidence="0.999418">
p(t|θq) = tf (t,q) Et0∈q tf (t0,q). (2)
</equation>
<bodyText confidence="0.999967666666667">
In the calculation of p(t|θd), several smoothing
methods have been proposed to overcome the data
sparseness problem of a language model constructed
from one document (Zhai and Lafferty, 2001b). For
example, p(t|θd) with the Dirichlet-prior smoothing
can be calculated as follows:
</bodyText>
<equation confidence="0.999048666666667">
tf (t, d) + µ p(t|θC)
p(t|θd) = (3)
Et0 ∈V tf (t0, d) + µ ,
</equation>
<bodyText confidence="0.998091">
where µ is the prior parameter in the Dirichlet-prior
smoothing method, and p(t|θC) is the probability of
t in C, which is often calculated with MLE:
</bodyText>
<equation confidence="0.986749666666667">
( Ed0 ∈C tf (t,d0)
p(t|θ C) = E E t0∈V tf (t0,d0).
d0∈C
</equation>
<subsectionHeader confidence="0.997337">
3.2 Pseudo relevance feedback
</subsectionHeader>
<bodyText confidence="0.999951571428571">
Pseudo relevance feedback (PRF) is widely used in
IR to achieve better performance. It is constructed
with two retrieval steps. In the first step, ranked doc-
uments are retrieved from C by a normal retrieval
method with the original query q. In the second step,
a number of terms are selected from the top k ranked
documents Dq for query expansion, under the as-
sumption that these k documents are relevant to the
query. Then, the expanded query is used to retrieve
the documents from C.
There are several methods to select expansion
terms in the second step (Zhai and Lafferty, 2001a).
For example, in Indri2, the terms are first ranked by
the following score:
</bodyText>
<equation confidence="0.952852333333333">
v(t, Dq) = Ed∈Dq log(tf (t,d)
|d |X 1
p(t|θC)),
</equation>
<bodyText confidence="0.99992925">
as in Ponte (1998). Define p(q|θd) as the probability
score assigned to d. The top m terms Tq are selected
with weights calculated based on the relevance
model described in Lavrenko and Croft (2001):
</bodyText>
<equation confidence="0.961083">
w(t, Dq) = Ed∈Dq [tf (t,d)
X p(q |θd) X p(θd)] ,
|d|
</equation>
<bodyText confidence="0.998715">
which calculates the sum of weighted probabilities
of t in each document. After normalization, the
probability of t in θr q is calculated as follows:
</bodyText>
<equation confidence="0.9568805">
p(t|θrq) = w(t,Dq)
Et0∈Tq w(t0,Dq).
</equation>
<bodyText confidence="0.9920935">
Finally, the relevance model is interpolated with the
original query model:
</bodyText>
<equation confidence="0.7611415">
p(t|θ�Tf
q ) = λ p(t|θrq) + (1 − λ)p(t|θq), (4)
</equation>
<bodyText confidence="0.999604">
where parameter λ controls the amount of feedback.
The new model θ�Tf
q is used to replace the original
one θq in Equation 1.
Collection enrichment (CE) (Kwok and Chan,
1998) is a technique to improve the quality of the
feedback documents by making use of an external
target text collection X in addition to the original
target C in the first step of PRF. The usage of X is
supposed to provide more relevant feedback docu-
ments and feedback query terms.
</bodyText>
<equation confidence="0.365521">
2http://lemurproject.org/indri/
tE
</equation>
<page confidence="0.995453">
275
</page>
<sectionHeader confidence="0.972793" genericHeader="method">
4 Word Sense Disambiguation
</sectionHeader>
<bodyText confidence="0.999881">
In this section, we first describe the construction of
our WSD system. Then, we propose the method of
assigning senses to query terms.
</bodyText>
<subsectionHeader confidence="0.998258">
4.1 Word sense disambiguation system
</subsectionHeader>
<bodyText confidence="0.999650105263158">
Previous research shows that translations in another
language can be used to disambiguate the meanings
of words (Chan and Ng, 2005; Zhong and Ng, 2009).
We construct our supervised WSD system directly
from parallel corpora.
To generate the WSD training data, 7 parallel cor-
pora were used, including Chinese Treebank, FBIS
Corpus, Hong Kong Hansards, Hong Kong Laws,
Hong Kong News, Sinorama News Magazine, and
Xinhua Newswire. These corpora were already
aligned at sentence level. We tokenized English
texts with Penn Treebank Tokenizer, and performed
word segmentation on Chinese texts. Then, word
alignment was performed on the parallel corpora
with the GIZA++ software (Och and Ney, 2003).
For each English morphological root e, the En-
glish sentences containing its occurrences were ex-
tracted from the word aligned output of GIZA++,
as well as the corresponding translations of these
occurrences. To minimize noisy word alignment
result, translations with no Chinese character were
deleted, and we further removed a translation when
it only appears once, or its frequency is less than 10
and also less than 1% of the frequency of e. Finally,
only the most frequent 10 translations were kept for
efficiency consideration.
The English part of the remaining occurrences
were used as training data. Because multiple En-
glish words may have the same Chinese transla-
tion, to differentiate them, each Chinese translation
is concatenated with the English morphological root
to form a word sense. We employed a supervised
WSD system, IMS3, to train the WSD models. IMS
(Zhong and Ng, 2010) integrates multiple knowl-
edge sources as features. We used MaxEnt as the
machine learning algorithm. Finally, the system can
disambiguate the words by assigning probabilities to
different senses.
</bodyText>
<footnote confidence="0.673803">
3http://nlp.comp.nus.edu.sg/software/ims
</footnote>
<subsectionHeader confidence="0.7391565">
4.2 Estimating sense distributions for query
terms
</subsectionHeader>
<bodyText confidence="0.991145806451613">
In IR, both terms in queries and the text collection
can be ambiguous. Hence, WSD is needed to disam-
biguate these ambiguous terms. In most cases, doc-
uments in a text collection are full articles. There-
fore, a WSD system has sufficient context to dis-
ambiguate the words in the document. In contrast,
queries are usually short, often with only two or
three terms in a query. Short queries pose a chal-
lenge to WSD systems since there is insufficient
context to disambiguate a term in a short query.
One possible solution to this problem is to find
some text fragments that contain a query term. Sup-
pose we already have a basic IR method which does
not require any sense information, such as the stem-
based LM approach. Similar to the PRF method,
assuming that the top k documents retrieved by the
basic method are relevant to the query, these k docu-
ments can be used to represent query q (Broder et al.,
2007; Bendersky et al., 2010; He and Wu, 2011). We
propose a method to estimate the sense probabilities
of each query term of q from these top k retrieved
documents.
Suppose the words in all documents of the text
collection are disambiguated with a WSD system,
and each word occurrence w in document d is as-
signed a vector of senses, S(w). Define the proba-
bility of assigning sense s to w as p(w, s, d). Given
a query q, suppose Dq is the set of top k documents
retrieved by the basic method, with the probability
score p(qj0d) assigned to d E Dq.
Given a query term t E q
</bodyText>
<equation confidence="0.888609416666667">
S(t, q) = {}
sum = 0
for each document d E Dq
for each word occurrence w E d, whose stem form is
identical to the stem form of t
for each sense s E S(w)
S(t, q) = S(t, q) U {s}
p(t, s, q) = p(t, s, q) + p(qjed) p(w, s, d)
sum = sum + p(qj0d) p(w, s, d)
for each sense s E S(t, q)
p(t, s, q) = p(t, s, q)/sum
Return S(t, q), with probability p(t, s, q) for s E S(t, q)
</equation>
<figureCaption confidence="0.9998975">
Figure 1: Process of generating senses for query terms
Figure 1 shows the pseudocode of calculating the
</figureCaption>
<page confidence="0.986777">
276
</page>
<bodyText confidence="0.999982">
sense distribution for a query term t in q with Dq,
where S(t, q) is the set of senses assigned to t and
p(t, s, q) is the probability of tagging t as sense s.
Basically, we utilized the sense distribution of the
words with the same stem form in Dq as a proxy to
estimate the sense probabilities of a query term. The
retrieval scores are used to weight the information
from the corresponding retrieved documents in Dq.
</bodyText>
<sectionHeader confidence="0.7727505" genericHeader="method">
5 Incorporating Senses into Language
Modeling Approaches
</sectionHeader>
<bodyText confidence="0.999791">
In this section, we propose to incorporate senses into
the LM approach to IR. Then, we describe the inte-
gration of sense synonym relations into our model.
</bodyText>
<subsectionHeader confidence="0.974377">
5.1 Incorporating senses as smoothing
</subsectionHeader>
<bodyText confidence="0.996091">
With the method described in Section 4.2, both the
terms in queries and documents have been sense
tagged. The next problem is to incorporate the sense
information into the language modeling approach.
Suppose p(t, s, q) is the probability of tagging a
query term t ∈ q as sense s, and p(w, s, d) is the
probability of tagging a word occurrence w ∈ d as
sense s. Given a query q and a document d in text
collection C, we want to re-estimate the language
models by making use of the sense information as-
signed to them.
Define the frequency of s in d as:
stf (s, d) = PwEd p(w, s, d),
and the frequency of s in C as:
</bodyText>
<equation confidence="0.870612">
stf (s, C) = P dEC stf (s, d).
Define the frequencies of sense set S in d and C as:
stf (S, d) = PsES stf (s, d),
stf (S, C) = PsES stf (s, C).
</equation>
<bodyText confidence="0.993297736842105">
For a term t ∈ q, with senses S(t, q):{s1, ..., sn},
suppose V :{p(t, s1, q), ...,p(t, sn, q)} is the vector
of probabilities assigned to the senses of t and
W:{stf (s1, d), ..., stf (sn, d)} is the vector of fre-
quencies of S(t, q) in d. The function cos(t, q, d)
calculates the cosine similarity between vector V
and vector W. Assume D is a set of documents
in C which contain any sense in S(t, q), we define
function cos (t, q) = PdED cos(t, q, d)/|D|, which
calculates the mean of the sense cosine similarities,
and define function Acos(t, q, d) = cos(t, q, d) −
cos (t, q), which calculates the difference between
cos(t, q, d) and the corresponding mean value.
Given a query q, we re-estimate the term fre-
quency of query term t in d with sense information
integrated as smoothing:
tf sen(t, d) = tf (t, d) + sen(t, q, d), (5)
where function sen(t, q, d) is a measure of t’s sense
information in d, which is defined as follows:
sen(t, q, d) = αΔcos(t,q,d)stf (S(t, q), d). (6)
In sen(t, q, d), the last item stf (S(t, q), d) calcu-
lates the sum of the sense frequencies of t senses in
d, which represents the amount of t’s sense informa-
tion in d. The first item αΔcos(t,q,d) is a weight of the
sense information concerning the relative sense sim-
ilarity Acos(t, q, d), where α is a positive parame-
ter to control the impact of sense similarity. When
Acos(t, q, d) is larger than zero, such that the sense
similarity of d and q according to t is above the av-
erage, the weight for the sense information is larger
than 1; otherwise, it is less than 1. The more similar
they are, the larger the weight value. For t ∈/ q, be-
cause the sense set S(t, q) is empty, stf (S(t, q), d)
equals to zero and tf sen(t, d) is identical to tf (t, d).
With sense incorporated, the term frequency is in-
fluenced by the sense information. Consequently,
the estimation of probability of t in d becomes query
specific:
</bodyText>
<equation confidence="0.9788526">
tf sen(t,d) + µ p(t|θsen
C )
p(t|θsen
d ) = (7)
Pt0E V tf sen (tt, d) + µ
</equation>
<bodyText confidence="0.991501">
where the probability of t in C is re-calculated as:
</bodyText>
<equation confidence="0.99463375">
p(t|θl
sen) = Ed0∈C tf s.n(t,d0)
C f � �t0∈V tf s.n(t0,d0).
d0∈C
</equation>
<subsectionHeader confidence="0.99969">
5.2 Expanding with synonym relations
</subsectionHeader>
<bodyText confidence="0.999287272727273">
Words usually have some semantic relations with
others. Synonym relation is one of the semantic re-
lations commonly used to improve IR performance.
In this part, we further integrate the synonym rela-
tions of senses into the LM approach.
Suppose R(s) is the set of senses having syn-
onym relation with sense s. Define S(q) as the set
of senses of query q, S(q) = StEq S(t, q), and de-
fine R(s, q)=R(s)−S(q). We update the frequency
of a query term t in d by integrating the synonym
relations as follows:
</bodyText>
<equation confidence="0.736577">
tf syn(t, d) = tf sen(t, d) + sen(t, q, d), (8)
</equation>
<page confidence="0.982232">
277
</page>
<bodyText confidence="0.9893475">
where syn(t, q, d) is a function measuring the syn-
onym information in d:
</bodyText>
<equation confidence="0.988664">
syn(t, q, d) = � 0(s, q)p(t, s, q)stf (R(s, q), d).
sES(t)
</equation>
<bodyText confidence="0.995180375">
The last item stf (R(s, q), d) in syn(t, q, d) is the
sum of the sense frequencies of R(s, q) in d. Notice
that the synonym senses already appearing in S(q)
are not included in the calculation, because the infor-
mation of these senses has been used in some other
places in the retrieval function. The frequency of
synonyms, stf (R(s, q), d), is weighted by p(t, s, q)
together with a scaling function 0(s, q):
</bodyText>
<equation confidence="0.955799666666667">
,
0(s q) = min(1, stf (s C)
stf (R(s,q),C)).
</equation>
<bodyText confidence="0.998259555555556">
When stf (s, C), the frequency of sense s in C, is
less than stf (R(s, q), C), the frequency of R(s, q)
in C, the function 0(s, q) scales down the impact
of synonyms according to the ratio of these two fre-
quencies. The scaling function makes sure that the
overall impact of the synonym senses is not greater
than the original word senses.
Accordingly, we have the probability of t in d up-
dated to:
</bodyText>
<equation confidence="0.9305625">
p(t |Bsyn) = tf syn(t, d) +//µ p(t|Ogn) (9)
d Et0 EV tf syn lt&apos;, d) + µ
</equation>
<bodyText confidence="0.937277">
and the probability of t in C is calculated as:
</bodyText>
<equation confidence="0.777304">
�
p(t |BCn) =Ed0∈C Et00∈C EV tf(yn(t0,d0)
</equation>
<bodyText confidence="0.999943333333333">
With this language model, the probability of a query
term in a document is enlarged by the synonyms of
its senses; The more its synonym senses in a doc-
ument, the higher the probability. Consequently,
documents with more synonym senses of the query
terms will get higher retrieval rankings.
</bodyText>
<sectionHeader confidence="0.999586" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999670666666667">
In this section, we evaluate and analyze the mod-
els proposed in Section 5 on standard TREC collec-
tions.
</bodyText>
<subsectionHeader confidence="0.991497">
6.1 Experimental settings
</subsectionHeader>
<bodyText confidence="0.999964611111111">
We conduct experiments on the TREC collection.
The text collection C includes the documents from
TREC disk 4 and 5, minus the CR (Congressional
Record) corpus, with 528,155 documents in total. In
addition, the other documents in TREC disk 1 to 5
are used as the external text collection X.
We use 50 queries from TREC6 Ad Hoc task
as the development set, and evaluate on 50 queries
from TREC7 Ad Hoc task, 50 queries from TREC8
Ad Hoc task, 50 queries from ROBUST 2003
(RB03), and 49 queries from ROBUST 2004
(RB04). In total, our test set includes 199 queries.
We use the terms in the title field of TREC topics as
queries. Table 1 shows the statistics of the five query
sets. The first column lists the query topics, and the
column #qry is the number of queries. The column
Ave gives the average query length, and the column
Rels is the total number of relevant documents.
</bodyText>
<table confidence="0.998020833333333">
Query Set Topics #qry Ave Rels
TREC6 301–350 50 2.58 4,290
TREC7 351–400 50 2.50 4,674
TREC8 401–450 50 2.46 4,728
RB03 601–650 50 3.00 1,658
RB044 651–700 49 2.96 2,062
</table>
<tableCaption confidence="0.999846">
Table 1: Statistics of query sets
</tableCaption>
<bodyText confidence="0.999651217391304">
We use the Lemur toolkit (Ogilvie and Callan,
2001) version 4.11 as the basic retrieval tool, and se-
lect the default unigram LM approach based on KL-
divergence and Dirichlet-prior smoothing method in
Lemur as our basic retrieval approach. Stop words
are removed from queries and documents using the
standard INQUERY stop words list (Allan et al.,
2000), and then the Porter stemmer is applied to per-
form stemming. The stem forms are finally used for
indexing and retrieval.
We set the smoothing parameter µ in Equation 3
to 400 by tuning on TREC6 query set in a range of
1100, 400, 700,1000,1500, 2000, 3000, 4000, 5000}.
With this basic method, up to 10 top ranked docu-
ments Dq are retrieved for each query q from the
extended text collection C U X, for the usage of
performing PRF and generating query senses.
For PRF, we follow the implementation of Indri’s
PRF method and further apply the CE technique as
described in Section 3.2. The number of terms se-
lected from Dq for expansion is tuned from range
120, 25, 30, 35, 40} and set to 25. The interpolation
parameter A in Equation 4 is set to 0.7 from range
</bodyText>
<footnote confidence="0.980682">
4Topic 672 is eliminated, since it has no relevant document.
</footnote>
<page confidence="0.982471">
278
</page>
<table confidence="0.999933">
Method TREC7 TERC8 RB03 RB04 Comb Impr #ret-rel
Top 1 0.2530 0.3063 0.3704 0.4019 - - -
Top 2 0.2488 0.2876 0.3065 0.4008 - - -
Top 3 0.2427 0.2853 0.3037 0.3514 - - -
Stemprf (Baseline) 0.2634 0.2944 0.3586 0.3781 0.3234 - 9248
Stemprf+MFS 0.2655 0.2971 0.3626† 0.3802 0.3261† 0.84% 9281
Stemprf+Even 0.2655 0.2972 0.3623† 0.3814 0.3263‡ 0.91% 9284
Stemprf+WSD 0.2679‡ 0.2986† 0.3649‡ 0.3842 0.3286‡ 1.63% 9332
Stemprf+MFS+Syn 0.2756‡ 0.3034† 0.3649† 0.3859 0.3322‡ 2.73% 9418
Stemprf+Even+Syn 0.2713† 0.3061‡ 0.3657‡ 0.3859† 0.3320‡ 2.67% 9445
Stemprf+WSD+Syn 0.2762‡ 0.3126‡ 0.3735‡ 0.3891† 0.3376‡ 4.39% 9538
</table>
<tableCaption confidence="0.97219275">
Table 2: Results on test set in MAP score. The first three rows show the results of the top participating systems, the
next row shows the performance of the baseline method, and the rest rows are the results of our method with different
settings. Single dagger (†) and double dagger (‡) indicate statistically significant improvement over Stemprf at the
95% and 99% confidence level with a two-tailed paired t-test, respectively. The best results are highlighted in bold.
</tableCaption>
<bodyText confidence="0.99975652">
{0.1, 0.2, ..., 0.9}. The CE-PRF method with this
parameter setting is chosen as the baseline.
To estimate the sense distributions for terms in
query q, the method described in Section 4.2 is ap-
plied with Dq. To disambiguate the documents in
the text collection, besides the usage of the super-
vised WSD system described in Section 4.1, two
WSD baseline methods, Even and MFS, are applied
for comparison. The method Even assigns equal
probabilities to all senses for each word, and the
method MFS tags the words with their correspond-
ing most frequent senses. The parameter α in Equa-
tion 6 is tuned on TREC6 from 1 to 10 in increment
of 1 for each sense tagging method. It is set to 7,
6, and 9 for the supervised WSD method, the Even
method, and the MFS method, respectively.
Notice that the sense in our WSD system is con-
ducted with two parts, a morphological root and a
Chinese translation. The Chinese parts not only dis-
ambiguate senses, but also provide clues of connec-
tions among different words. Assume that the senses
with the same Chinese part are synonyms, there-
fore, we can generate a set of synonyms for each
sense, and then utilize these synonym relations in
the method proposed in Section 5.2.
</bodyText>
<subsectionHeader confidence="0.999069">
6.2 Experimental results
</subsectionHeader>
<bodyText confidence="0.9991665">
For evaluation, we use average precision (AP) as the
metric to evaluate the performance on each query q:
</bodyText>
<equation confidence="0.8998135">
�� [�(�)rel(�)]
AP(q) = relevance(q) ,
</equation>
<bodyText confidence="0.999831428571429">
where relevance(q) is the number of documents rel-
evant to q, R is the number of retrieved documents,
r is the rank, p(r) is the precision of the top r re-
trieved documents, and rel(r) equals to 1 if the rth
document is relevant, and 0 otherwise. Mean aver-
age precision (MAP) is a metric to evaluate the per-
formance on a set of queries Q:
</bodyText>
<equation confidence="0.852833">
MAP(Q) = E9EiQAP(q) ,
</equation>
<bodyText confidence="0.999285384615385">
where |Q |is the number of queries in Q.
We retrieve the top-ranked 1,000 documents for
each query, and use the MAP score as the main com-
paring metric. In Table 2, the first four columns are
the MAP scores of various methods on the TREC7,
TREC8, RB03, and RB04 query sets, respectively.
The column Comb shows the results on the union of
the four test query sets. The first three rows list the
results of the top three systems that participated in
the corresponding tasks. The row Stemprf shows the
performance of our baseline method, the stem-based
CE-PRF method. The column Impr calculates the
percentage improvement of each method over the
baseline Stemprf in column Comb. The last column
#ret-rel lists the total numbers of relevant documents
retrieved by different methods.
The rows Stemprf +{MFS, Even, WSD} are the re-
sults of Stemprf incorporating with the senses gen-
erated for the original query terms, by applying the
approach proposed in Section 5.1, with the MFS
method, the Even method, and our supervised WSD
method, respectively. Comparing to the baseline
method, all methods with sense integrated achieve
consistent improvements on all query sets. The
usage of the supervised WSD method outperforms
the other two WSD baselines, and it achieves sta-
</bodyText>
<page confidence="0.99522">
279
</page>
<bodyText confidence="0.999851796610169">
tistically significant improvements over StempTf on
TREC7, TREC8, and RB03.
The integration of senses into the baseline method
has two aspects of impact. First, the morphologi-
cal roots of senses conquer the irregular inflection
problem. Thus, the documents containing the irreg-
ular inflections are retrieved when senses are inte-
grated. For example, in topic 326 {ferry sinkings},
the stem form of sinkings is sink. As sink is an irreg-
ular verb, the usage of senses improves the retrieval
recall by retrieving the documents containing the in-
flection forms sunk, sank, and sunken.
Second, the senses output by supervised WSD
system help identify the meanings of query terms.
Take topic 357 {territorial waters dispute} for ex-
ample, the stem form of waters is water and its ap-
propriate sense in this query should be water 7104
(body of water) instead of the most frequent sense
of water 71( (H2O). In StempTf+WSD, we correctly
identify the minority sense for this query term. In
another example, topic 425 {counterfeiting money},
the stem form of counterfeiting is counterfeit. Al-
though the most frequent sense counterfeit W&apos; 9
(not genuine) is not wrong, another sense counter-
feit l/7VJ (forged money) is more accurate for this
query term. The Chinese translation in the latter
sense represents the meaning of the phrase in orig-
inal query. Thus, StempTf+WSD outperforms the
other two methods on this query by assigning the
highest probability for this sense.
Overall, the performance of StempTf+WSD is bet-
ter than StempTf+{MFS, Even} on 121 queries and
119 queries, respectively. The t-test at the confi-
dence level of 99% indicates that the improvements
are statistically significant.
The results of expanding with synonym relations
in the above three methods are shown in the last
three rows, StempTf+{MFS, Even, WSD}+Syn. The
integration of synonym relations further improves
the performance no matter what kind of sense tag-
ging method is applied. The improvement varies
with different methods on different query sets. As
shown in the last column of Table 2, the number of
relevant documents retrieved is increased for each
method. StempTf+Even+Syn retrieves more rele-
vant documents than StempTf+MFS+Syn, because
the former method expands more senses. Overall,
the improvement achieved by StempTf+WSD+Syn is
larger than the other two methods. It shows that
the WSD technique can help choose the appropriate
senses for synonym expansion.
Among the different settings, StempTf+WSD+Syn
achieves the best performance. Its improvement
over the baseline method is statistically significant
at the 95% confidence level on RB04 and at the 99%
confidence level on the other three query sets, with
an overall improvement of 4.39%. It beats the best
participated systems on three out of four query sets5,
including TREC7, TREC8, and RB03.
</bodyText>
<sectionHeader confidence="0.998372" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999980789473684">
This paper reports successful application of WSD
to IR. We proposed a method for annotating senses
to terms in short queries, and also described an ap-
proach to integrate senses into an LM approach for
IR. In the experiment on four query sets of TREC
collection, we compared the performance of a su-
pervised WSD method and two WSD baseline meth-
ods. Our experimental results showed that the incor-
poration of senses improved a state-of-the-art base-
line, a stem-based LM approach with PRF method.
The performance of applying the supervised WSD
method is better than the other two WSD base-
line methods. We also proposed a method to fur-
ther integrate the synonym relations to the LM ap-
proaches. With the integration of synonym rela-
tions, our best performance setting with the super-
vised WSD achieved an improvement of 4.39% over
the baseline method, and it outperformed the best
participating systems on three out of four query sets.
</bodyText>
<sectionHeader confidence="0.99749" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99873975">
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
</bodyText>
<sectionHeader confidence="0.997642" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9095205">
E. Agirre, X. Arregi, and A. Otegi. 2010. Document ex-
pansion based on WordNet for robust IR. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics, pages 9–17.
</reference>
<footnote confidence="0.990921666666667">
5The top two systems on RB04 are the results of the same
participant with different configurations. They used lots of web
resources, such as search engines, to improve the performance.
</footnote>
<page confidence="0.991254">
280
</page>
<reference confidence="0.999054607476636">
J. Allan, M. E. Connell, W.B. Croft, F.F. Feng, D. Fisher,
and X. Li. 2000. INQUERY and TREC-9. In Pro-
ceedings of the 9th Text REtrieval Conference, pages
551–562.
M. Bendersky, W. B. Croft, and D. A. Smith. 2010.
Structural annotation of search queries using pseudo-
relevance feedback. In Proceedings of the 19th ACM
Conference on Information and Knowledge Manage-
ment, pages 1537–1540.
A. Broder, M. Fontoura, E. Gabrilovich, A. Joshi, V. Josi-
fovski, and T. Zhang. 2007. Robust classification of
rare queries using web knowledge. In Proceedings
of the 30th International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 231–238.
G. Cao, J. Y. Nie, and J. Bai. 2005. Integrating word
relationships into language models. In Proceedings
of the 28th International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 298–305.
M. Carpuat and D. Wu. 2007. Improving statisti-
cal machine translation using word sense disambigua-
tion. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 61–72.
Y. S. Chan and H. T. Ng. 2005. Scaling up word
sense disambiguation via parallel texts. In Proceed-
ings of the 20th National Conference on Artificial In-
telligence, pages 1037–1042.
Y. S. Chan, H. T. Ng, and D. Chiang. 2007. Word sense
disambiguation improves statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics, pages
33–40.
H. Fang. 2008. A re-examination of query expansion us-
ing lexical resources. In Proceedings of the 46th An-
nual Meeting of the Association of Computational Lin-
guistics: Human Language Technologies, pages 139–
147.
J. Gonzalo, F. Verdejo, I. Chugur, and J. Cigarrin. 1998.
Indexing with WordNet synsets can improve text re-
trieval. In Proceedings of the COLING-ACL Workshop
on Usage of WordNet in Natural Language Processing
Systems, pages 38–44.
J. Gonzalo, A. Penas, and F. Verdejo. 1999. Lexical
ambiguity and information retrieval revisited. In Pro-
ceedings of the 1999 Joint SIGDAT Conference on Em-
pirical Methods in Natural Language Processing and
Very Large Corpora, pages 195–202.
D. He and D. Wu. 2011. Enhancing query transla-
tion with relevance feedback in translingual informa-
tion retrieval. Information Processing &amp; Management,
47(1):1–17.
S. B. Kim, H. C. Seo, and H. C. Rim. 2004. Informa-
tion retrieval using word senses: root sense tagging ap-
proach. In Proceedings of the 27th International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 258–265.
R. Krovetz and W. B. Croft. 1992. Lexical ambiguity
and information retrieval. ACM Transactions on In-
formation Systems, 10(2):115–141.
K. L. Kwok and M. Chan. 1998. Improving two-stage
ad-hoc retrieval for short queries. In Proceedings
of the 21st International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 250–256.
J. Lafferty and C. Zhai. 2001. Document language mod-
els, query models, and risk minimization for informa-
tion retrieval. In Proceedings of the 24th International
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval, pages 111–119.
V. Lavrenko and W. B. Croft. 2001. Relevance based
language models. In Proceedings of the 24th Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 120–127.
S. Liu, F. Liu, C. Yu, and W. Meng. 2004. An ef-
fective approach to document retrieval via utilizing
WordNet and recognizing phrases. In Proceedings
of the 27th International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 266–272.
S. Liu, C. Yu, and W. Meng. 2005. Word sense disam-
biguation in queries. In Proceedings of the 14th ACM
Conference on Information and Knowledge Manage-
ment, pages 525–532.
G. A. Miller. 1990. WordNet: An on-line lexi-
cal database. International Journal of Lexicography,
3(4):235–312.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19–51.
P. Ogilvie and J. Callan. 2001. Experiments using the
Lemur toolkit. In Proceedings of the 10th Text RE-
trieval Conference, pages 103–108.
J. M. Ponte and W. B. Croft. 1998. A language model-
ing approach to information retrieval. In Proceedings
of the 21st International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 275–281.
J. M. Ponte. 1998. A Language Modeling Approach
to Information Retreival. Ph.D. thesis, Department of
Computer Science, University of Massachusetts.
M. Sanderson. 1994. Word sense disambiguation and in-
formation retrieval. In Proceedings of the 17th Inter-
national ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 142–151.
</reference>
<page confidence="0.962615">
281
</page>
<reference confidence="0.999893333333333">
M. Sanderson. 2000. Retrieving with good sense. Infor-
mation Retrieval, 2(1):49–69.
H. Sch¨utze and J. O. Pedersen. 1995. Information re-
trieval based on word senses. In Proceedings of the
4th Annual Symposium on Document Analysis and In-
formation Retrieval, pages 161–175.
C. Stokoe, M. P. Oakes, and J. Tait. 2003. Word sense
disambiguation in information retrieval revisited. In
Proceedings of the 26th International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 159–166.
E. M. Voorhees. 1993. Using WordNet to disam-
biguate word senses for text retrieval. In Proceedings
of the 16th International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 171–180.
E. M. Voorhees. 1994. Query expansion using lexical-
semantic relations. In Proceedings of the 17th Inter-
national ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 61–69.
C. Zhai and J. Lafferty. 2001a. Model-based feedback
in the language modeling approach to information re-
trieval. In Proceedings of the 10th ACM Conference
on Information and Knowledge Management, pages
403–410.
C. Zhai and J. Lafferty. 2001b. A study of smoothing
methods for language models applied to ad hoc infor-
mation retrieval. In Proceedings of the 24th Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 334–342.
Z. Zhong and H. T. Ng. 2009. Word sense disambigua-
tion for all words without hard labor. In Proceedings
of the 21st International Joint Conference on Artificial
Intelligence, pages 1616–1621.
Z. Zhong and H. T. Ng. 2010. It Makes Sense: A wide-
coverage word sense disambiguation system for free
text. In Proceedings of the 48th Annual Meeting of
the Association of Computational Linguistics: System
Demonstrations, pages 78–83.
</reference>
<page confidence="0.997439">
282
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.938671">
<title confidence="0.996735">Word Sense Disambiguation Improves Information Retrieval</title>
<author confidence="0.999068">Zhong Tou</author>
<affiliation confidence="0.999891">Department of Computer National University of</affiliation>
<address confidence="0.975837">13 Computing Drive, Singapore</address>
<abstract confidence="0.99786075">Previous research has conflicting conclusions on whether word sense disambiguation (WSD) systems can improve information retrieval (IR) performance. In this paper, we propose a method to estimate sense distributions for short queries. Together with the senses predicted for words in documents, we propose a novel approach to incorporate word senses into the language modeling approach to IR and also exploit the integration of synonym relations. Our experimental results on show that using the word senses tagged by a supervised WSD system, we obtain significant improvements over a state-of-the-art IR system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>X Arregi</author>
<author>A Otegi</author>
</authors>
<title>Document expansion based on WordNet for robust IR.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>9--17</pages>
<contexts>
<context position="7570" citStr="Agirre et al., 2010" startWordPosition="1170" endWordPosition="1173">ering process of each word is a time consuming task. Because the sense inventory is collection dependent, it is also hard to expand the text collection without re-doing preprocessing. Many studies investigated the expansion effects by using knowledge sources from thesauri. Some researchers achieved improvements by expanding the disambiguated query words with synonyms and some other information from WordNet (Voorhees, 1994; Liu et al., 2004; Liu et al., 2005; Fang, 2008). The usage of knowledge sources from WordNet in document expansion also showed improvements in IR systems (Cao et al., 2005; Agirre et al., 2010). The previous work shows that the WSD errors can easily neutralize its positive effect. It is important to reduce the negative impact of erroneous disambiguation, and the integration of senses into traditional term index, such as stem-based index, is a possible solution. The utilization of semantic relations has proved to be helpful for IR. It is also interest274 ing to investigate the utilization of semantic relations among senses in IR. 3 The Language Modeling Approach to IR This section describes the LM approach to IR and the pseudo relevance feedback approach. 3.1 The language modeling ap</context>
</contexts>
<marker>Agirre, Arregi, Otegi, 2010</marker>
<rawString>E. Agirre, X. Arregi, and A. Otegi. 2010. Document expansion based on WordNet for robust IR. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 9–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Allan</author>
<author>M E Connell</author>
<author>W B Croft</author>
<author>F F Feng</author>
<author>D Fisher</author>
<author>X Li</author>
</authors>
<title>INQUERY and TREC-9.</title>
<date>2000</date>
<booktitle>In Proceedings of the 9th Text REtrieval Conference,</booktitle>
<pages>551--562</pages>
<contexts>
<context position="22863" citStr="Allan et al., 2000" startWordPosition="3951" endWordPosition="3954"> and the column Rels is the total number of relevant documents. Query Set Topics #qry Ave Rels TREC6 301–350 50 2.58 4,290 TREC7 351–400 50 2.50 4,674 TREC8 401–450 50 2.46 4,728 RB03 601–650 50 3.00 1,658 RB044 651–700 49 2.96 2,062 Table 1: Statistics of query sets We use the Lemur toolkit (Ogilvie and Callan, 2001) version 4.11 as the basic retrieval tool, and select the default unigram LM approach based on KLdivergence and Dirichlet-prior smoothing method in Lemur as our basic retrieval approach. Stop words are removed from queries and documents using the standard INQUERY stop words list (Allan et al., 2000), and then the Porter stemmer is applied to perform stemming. The stem forms are finally used for indexing and retrieval. We set the smoothing parameter µ in Equation 3 to 400 by tuning on TREC6 query set in a range of 1100, 400, 700,1000,1500, 2000, 3000, 4000, 5000}. With this basic method, up to 10 top ranked documents Dq are retrieved for each query q from the extended text collection C U X, for the usage of performing PRF and generating query senses. For PRF, we follow the implementation of Indri’s PRF method and further apply the CE technique as described in Section 3.2. The number of te</context>
</contexts>
<marker>Allan, Connell, Croft, Feng, Fisher, Li, 2000</marker>
<rawString>J. Allan, M. E. Connell, W.B. Croft, F.F. Feng, D. Fisher, and X. Li. 2000. INQUERY and TREC-9. In Proceedings of the 9th Text REtrieval Conference, pages 551–562.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bendersky</author>
<author>W B Croft</author>
<author>D A Smith</author>
</authors>
<title>Structural annotation of search queries using pseudorelevance feedback.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th ACM Conference on Information and Knowledge Management,</booktitle>
<pages>1537--1540</pages>
<contexts>
<context position="14730" citStr="Bendersky et al., 2010" startWordPosition="2389" endWordPosition="2392"> are usually short, often with only two or three terms in a query. Short queries pose a challenge to WSD systems since there is insufficient context to disambiguate a term in a short query. One possible solution to this problem is to find some text fragments that contain a query term. Suppose we already have a basic IR method which does not require any sense information, such as the stembased LM approach. Similar to the PRF method, assuming that the top k documents retrieved by the basic method are relevant to the query, these k documents can be used to represent query q (Broder et al., 2007; Bendersky et al., 2010; He and Wu, 2011). We propose a method to estimate the sense probabilities of each query term of q from these top k retrieved documents. Suppose the words in all documents of the text collection are disambiguated with a WSD system, and each word occurrence w in document d is assigned a vector of senses, S(w). Define the probability of assigning sense s to w as p(w, s, d). Given a query q, suppose Dq is the set of top k documents retrieved by the basic method, with the probability score p(qj0d) assigned to d E Dq. Given a query term t E q S(t, q) = {} sum = 0 for each document d E Dq for each </context>
</contexts>
<marker>Bendersky, Croft, Smith, 2010</marker>
<rawString>M. Bendersky, W. B. Croft, and D. A. Smith. 2010. Structural annotation of search queries using pseudorelevance feedback. In Proceedings of the 19th ACM Conference on Information and Knowledge Management, pages 1537–1540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Broder</author>
<author>M Fontoura</author>
<author>E Gabrilovich</author>
<author>A Joshi</author>
<author>V Josifovski</author>
<author>T Zhang</author>
</authors>
<title>Robust classification of rare queries using web knowledge.</title>
<date>2007</date>
<booktitle>In Proceedings of the 30th International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>231--238</pages>
<contexts>
<context position="14706" citStr="Broder et al., 2007" startWordPosition="2385" endWordPosition="2388"> In contrast, queries are usually short, often with only two or three terms in a query. Short queries pose a challenge to WSD systems since there is insufficient context to disambiguate a term in a short query. One possible solution to this problem is to find some text fragments that contain a query term. Suppose we already have a basic IR method which does not require any sense information, such as the stembased LM approach. Similar to the PRF method, assuming that the top k documents retrieved by the basic method are relevant to the query, these k documents can be used to represent query q (Broder et al., 2007; Bendersky et al., 2010; He and Wu, 2011). We propose a method to estimate the sense probabilities of each query term of q from these top k retrieved documents. Suppose the words in all documents of the text collection are disambiguated with a WSD system, and each word occurrence w in document d is assigned a vector of senses, S(w). Define the probability of assigning sense s to w as p(w, s, d). Given a query q, suppose Dq is the set of top k documents retrieved by the basic method, with the probability score p(qj0d) assigned to d E Dq. Given a query term t E q S(t, q) = {} sum = 0 for each d</context>
</contexts>
<marker>Broder, Fontoura, Gabrilovich, Joshi, Josifovski, Zhang, 2007</marker>
<rawString>A. Broder, M. Fontoura, E. Gabrilovich, A. Joshi, V. Josifovski, and T. Zhang. 2007. Robust classification of rare queries using web knowledge. In Proceedings of the 30th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 231–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Cao</author>
<author>J Y Nie</author>
<author>J Bai</author>
</authors>
<title>Integrating word relationships into language models.</title>
<date>2005</date>
<booktitle>In Proceedings of the 28th International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>298--305</pages>
<contexts>
<context position="7548" citStr="Cao et al., 2005" startWordPosition="1166" endWordPosition="1169">However, the clustering process of each word is a time consuming task. Because the sense inventory is collection dependent, it is also hard to expand the text collection without re-doing preprocessing. Many studies investigated the expansion effects by using knowledge sources from thesauri. Some researchers achieved improvements by expanding the disambiguated query words with synonyms and some other information from WordNet (Voorhees, 1994; Liu et al., 2004; Liu et al., 2005; Fang, 2008). The usage of knowledge sources from WordNet in document expansion also showed improvements in IR systems (Cao et al., 2005; Agirre et al., 2010). The previous work shows that the WSD errors can easily neutralize its positive effect. It is important to reduce the negative impact of erroneous disambiguation, and the integration of senses into traditional term index, such as stem-based index, is a possible solution. The utilization of semantic relations has proved to be helpful for IR. It is also interest274 ing to investigate the utilization of semantic relations among senses in IR. 3 The Language Modeling Approach to IR This section describes the LM approach to IR and the pseudo relevance feedback approach. 3.1 Th</context>
</contexts>
<marker>Cao, Nie, Bai, 2005</marker>
<rawString>G. Cao, J. Y. Nie, and J. Bai. 2005. Integrating word relationships into language models. In Proceedings of the 28th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 298–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Carpuat</author>
<author>D Wu</author>
</authors>
<title>Improving statistical machine translation using word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>61--72</pages>
<contexts>
<context position="1515" citStr="Carpuat and Wu, 2007" startWordPosition="226" endWordPosition="229">biguation (WSD) is the task of identifying the correct meaning of a word in context. As a basic semantic understanding task at the lexical level, WSD is a fundamental problem in natural language processing. It can be potentially used as a component in many applications, such as machine translation (MT) and information retrieval (IR). In recent years, driven by Senseval/Semeval workshops, WSD systems achieve promising performance. In the application of WSD to MT, research has shown that integrating WSD in appropriate ways significantly improves the performance of MT systems (Chan et al., 2007; Carpuat and Wu, 2007). In the application to IR, WSD can bring two kinds of benefits. First, queries may contain ambiguous words (terms), which have multiple meanings. The ambiguities of these query words can hurt retrieval precision. Identifying the correct meaning of the ambiguous words in both queries and documents can help improve retrieval precision. Second, query words may have tightly related meanings with other words not in the query. Making use of these relations between words can improve retrieval recall. Overall, IR systems can potentially benefit from the correct meanings of words provided by WSD syste</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>M. Carpuat and D. Wu. 2007. Improving statistical machine translation using word sense disambiguation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 61–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y S Chan</author>
<author>H T Ng</author>
</authors>
<title>Scaling up word sense disambiguation via parallel texts.</title>
<date>2005</date>
<booktitle>In Proceedings of the 20th National Conference on Artificial Intelligence,</booktitle>
<pages>1037--1042</pages>
<contexts>
<context position="12061" citStr="Chan and Ng, 2005" startWordPosition="1954" endWordPosition="1957">quality of the feedback documents by making use of an external target text collection X in addition to the original target C in the first step of PRF. The usage of X is supposed to provide more relevant feedback documents and feedback query terms. 2http://lemurproject.org/indri/ tE 275 4 Word Sense Disambiguation In this section, we first describe the construction of our WSD system. Then, we propose the method of assigning senses to query terms. 4.1 Word sense disambiguation system Previous research shows that translations in another language can be used to disambiguate the meanings of words (Chan and Ng, 2005; Zhong and Ng, 2009). We construct our supervised WSD system directly from parallel corpora. To generate the WSD training data, 7 parallel corpora were used, including Chinese Treebank, FBIS Corpus, Hong Kong Hansards, Hong Kong Laws, Hong Kong News, Sinorama News Magazine, and Xinhua Newswire. These corpora were already aligned at sentence level. We tokenized English texts with Penn Treebank Tokenizer, and performed word segmentation on Chinese texts. Then, word alignment was performed on the parallel corpora with the GIZA++ software (Och and Ney, 2003). For each English morphological root e</context>
</contexts>
<marker>Chan, Ng, 2005</marker>
<rawString>Y. S. Chan and H. T. Ng. 2005. Scaling up word sense disambiguation via parallel texts. In Proceedings of the 20th National Conference on Artificial Intelligence, pages 1037–1042.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y S Chan</author>
<author>H T Ng</author>
<author>D Chiang</author>
</authors>
<title>Word sense disambiguation improves statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="1492" citStr="Chan et al., 2007" startWordPosition="222" endWordPosition="225">on Word sense disambiguation (WSD) is the task of identifying the correct meaning of a word in context. As a basic semantic understanding task at the lexical level, WSD is a fundamental problem in natural language processing. It can be potentially used as a component in many applications, such as machine translation (MT) and information retrieval (IR). In recent years, driven by Senseval/Semeval workshops, WSD systems achieve promising performance. In the application of WSD to MT, research has shown that integrating WSD in appropriate ways significantly improves the performance of MT systems (Chan et al., 2007; Carpuat and Wu, 2007). In the application to IR, WSD can bring two kinds of benefits. First, queries may contain ambiguous words (terms), which have multiple meanings. The ambiguities of these query words can hurt retrieval precision. Identifying the correct meaning of the ambiguous words in both queries and documents can help improve retrieval precision. Second, query words may have tightly related meanings with other words not in the query. Making use of these relations between words can improve retrieval recall. Overall, IR systems can potentially benefit from the correct meanings of word</context>
</contexts>
<marker>Chan, Ng, Chiang, 2007</marker>
<rawString>Y. S. Chan, H. T. Ng, and D. Chiang. 2007. Word sense disambiguation improves statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Fang</author>
</authors>
<title>A re-examination of query expansion using lexical resources.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association of Computational Linguistics: Human Language Technologies,</booktitle>
<pages>139--147</pages>
<contexts>
<context position="7424" citStr="Fang, 2008" startWordPosition="1148" endWordPosition="1149">rieval performance, and the combination of word-based ranking and sense-based ranking can further improve performance. However, the clustering process of each word is a time consuming task. Because the sense inventory is collection dependent, it is also hard to expand the text collection without re-doing preprocessing. Many studies investigated the expansion effects by using knowledge sources from thesauri. Some researchers achieved improvements by expanding the disambiguated query words with synonyms and some other information from WordNet (Voorhees, 1994; Liu et al., 2004; Liu et al., 2005; Fang, 2008). The usage of knowledge sources from WordNet in document expansion also showed improvements in IR systems (Cao et al., 2005; Agirre et al., 2010). The previous work shows that the WSD errors can easily neutralize its positive effect. It is important to reduce the negative impact of erroneous disambiguation, and the integration of senses into traditional term index, such as stem-based index, is a possible solution. The utilization of semantic relations has proved to be helpful for IR. It is also interest274 ing to investigate the utilization of semantic relations among senses in IR. 3 The Lang</context>
</contexts>
<marker>Fang, 2008</marker>
<rawString>H. Fang. 2008. A re-examination of query expansion using lexical resources. In Proceedings of the 46th Annual Meeting of the Association of Computational Linguistics: Human Language Technologies, pages 139– 147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gonzalo</author>
<author>F Verdejo</author>
<author>I Chugur</author>
<author>J Cigarrin</author>
</authors>
<title>Indexing with WordNet synsets can improve text retrieval.</title>
<date>1998</date>
<booktitle>In Proceedings of the COLING-ACL Workshop on Usage of WordNet in Natural Language Processing Systems,</booktitle>
<pages>38--44</pages>
<contexts>
<context position="2523" citStr="Gonzalo et al., 1998" startWordPosition="381" endWordPosition="384">gs with other words not in the query. Making use of these relations between words can improve retrieval recall. Overall, IR systems can potentially benefit from the correct meanings of words provided by WSD systems. However, in previous investigations of the usage of WSD in IR, different researchers arrived at conflicting observations and conclusions. Some of the early research showed a drop in retrieval performance by using word senses (Krovetz and Croft, 1992; Voorhees, 1993). Some other experiments observed improvements by integrating word senses in IR systems (Sch¨utze and Pedersen, 1995; Gonzalo et al., 1998; Stokoe et al., 2003; Kim et al., 2004). This paper proposes the use of word senses to improve the performance of IR. We propose an approach to annotate the senses for short queries. We incorporate word senses into the language modeling (LM) approach to IR (Ponte and Croft, 1998), and utilize sense synonym relations to further improve the performance. Our evaluation on standard TREC1 data sets shows that supervised WSD outperforms two other WSD baselines and significantly improves IR. The rest of this paper is organized as follows. In Section 2, we first review previous work using WSD in IR. </context>
<context position="4507" citStr="Gonzalo et al. (1998)" startWordPosition="702" endWordPosition="705">ied the sense matches between terms in query and the document collection. They concluded that the benefits of WSD in IR are not as expected because query words have skewed sense distribution and the collocation effect from other query terms already performs some disambiguation. Sanderson (1994; 2000) used pseudowords to introduce artificial word ambiguity in order to study the impact of sense ambiguity on IR. He concluded that because the effectiveness of WSD can be negated by inaccurate WSD performance, high accuracy of WSD is an essential requirement to achieve improvement. In another work, Gonzalo et al. (1998) used a manually sense annotated corpus, SemCor, to study the effects of incorrect disambiguation. They obtained significant improvements by representing documents and queries with accurate senses as well as synsets (synonym sets). Their experiment also showed that with the synset representation, which included synonym information, WSD with an error rate of 40%–50% can still improve IR performance. Their later work (Gonzalo et al., 1999) verified that part of speech (POS) information is discriminatory for IR purposes. Several works attempted to disambiguate terms in both queries and documents </context>
</contexts>
<marker>Gonzalo, Verdejo, Chugur, Cigarrin, 1998</marker>
<rawString>J. Gonzalo, F. Verdejo, I. Chugur, and J. Cigarrin. 1998. Indexing with WordNet synsets can improve text retrieval. In Proceedings of the COLING-ACL Workshop on Usage of WordNet in Natural Language Processing Systems, pages 38–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gonzalo</author>
<author>A Penas</author>
<author>F Verdejo</author>
</authors>
<title>Lexical ambiguity and information retrieval revisited.</title>
<date>1999</date>
<booktitle>In Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>195--202</pages>
<contexts>
<context position="4948" citStr="Gonzalo et al., 1999" startWordPosition="766" endWordPosition="769">e effectiveness of WSD can be negated by inaccurate WSD performance, high accuracy of WSD is an essential requirement to achieve improvement. In another work, Gonzalo et al. (1998) used a manually sense annotated corpus, SemCor, to study the effects of incorrect disambiguation. They obtained significant improvements by representing documents and queries with accurate senses as well as synsets (synonym sets). Their experiment also showed that with the synset representation, which included synonym information, WSD with an error rate of 40%–50% can still improve IR performance. Their later work (Gonzalo et al., 1999) verified that part of speech (POS) information is discriminatory for IR purposes. Several works attempted to disambiguate terms in both queries and documents with the senses predefined in hand-crafted sense inventories, and then used the senses to perform indexing and retrieval. Voorhees (1993) used the hyponymy (“IS-A”) relation in WordNet (Miller, 1990) to disambiguate the polysemous nouns in a text. In her experiments, the performance of sense-based retrieval is worse than stem-based retrieval on all test collections. Her analysis showed that inaccurate WSD caused the poor results. Stokoe </context>
</contexts>
<marker>Gonzalo, Penas, Verdejo, 1999</marker>
<rawString>J. Gonzalo, A. Penas, and F. Verdejo. 1999. Lexical ambiguity and information retrieval revisited. In Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 195–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D He</author>
<author>D Wu</author>
</authors>
<title>Enhancing query translation with relevance feedback in translingual information retrieval.</title>
<date>2011</date>
<booktitle>Information Processing &amp; Management,</booktitle>
<volume>47</volume>
<issue>1</issue>
<contexts>
<context position="14748" citStr="He and Wu, 2011" startWordPosition="2393" endWordPosition="2396">n with only two or three terms in a query. Short queries pose a challenge to WSD systems since there is insufficient context to disambiguate a term in a short query. One possible solution to this problem is to find some text fragments that contain a query term. Suppose we already have a basic IR method which does not require any sense information, such as the stembased LM approach. Similar to the PRF method, assuming that the top k documents retrieved by the basic method are relevant to the query, these k documents can be used to represent query q (Broder et al., 2007; Bendersky et al., 2010; He and Wu, 2011). We propose a method to estimate the sense probabilities of each query term of q from these top k retrieved documents. Suppose the words in all documents of the text collection are disambiguated with a WSD system, and each word occurrence w in document d is assigned a vector of senses, S(w). Define the probability of assigning sense s to w as p(w, s, d). Given a query q, suppose Dq is the set of top k documents retrieved by the basic method, with the probability score p(qj0d) assigned to d E Dq. Given a query term t E q S(t, q) = {} sum = 0 for each document d E Dq for each word occurrence w </context>
</contexts>
<marker>He, Wu, 2011</marker>
<rawString>D. He and D. Wu. 2011. Enhancing query translation with relevance feedback in translingual information retrieval. Information Processing &amp; Management, 47(1):1–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Kim</author>
<author>H C Seo</author>
<author>H C Rim</author>
</authors>
<title>Information retrieval using word senses: root sense tagging approach.</title>
<date>2004</date>
<booktitle>In Proceedings of the 27th International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>258--265</pages>
<contexts>
<context position="2563" citStr="Kim et al., 2004" startWordPosition="389" endWordPosition="392">g use of these relations between words can improve retrieval recall. Overall, IR systems can potentially benefit from the correct meanings of words provided by WSD systems. However, in previous investigations of the usage of WSD in IR, different researchers arrived at conflicting observations and conclusions. Some of the early research showed a drop in retrieval performance by using word senses (Krovetz and Croft, 1992; Voorhees, 1993). Some other experiments observed improvements by integrating word senses in IR systems (Sch¨utze and Pedersen, 1995; Gonzalo et al., 1998; Stokoe et al., 2003; Kim et al., 2004). This paper proposes the use of word senses to improve the performance of IR. We propose an approach to annotate the senses for short queries. We incorporate word senses into the language modeling (LM) approach to IR (Ponte and Croft, 1998), and utilize sense synonym relations to further improve the performance. Our evaluation on standard TREC1 data sets shows that supervised WSD outperforms two other WSD baselines and significantly improves IR. The rest of this paper is organized as follows. In Section 2, we first review previous work using WSD in IR. Section 3 introduces the LM approach to </context>
<context position="6033" citStr="Kim et al. (2004)" startWordPosition="938" endWordPosition="941">is worse than stem-based retrieval on all test collections. Her analysis showed that inaccurate WSD caused the poor results. Stokoe et al. (2003) employed a fine-grained WSD system with an accuracy of 62.1% to disambiguate terms in both the text collections and the queries in their experiments. Their evaluation on TREC collections achieved significant improvements over a standard term based vector space model. However, it is hard to judge the effect of word senses because of the overall poor performances of their baseline method and their system. Instead of using fine-grained sense inventory, Kim et al. (2004) tagged words with 25 root senses of nouns in WordNet. Their retrieval method maintained the stem-based index and adjusted the term weight in a document according to its sense matching result with the query. They attributed the improvement achieved on TREC collections to their coarse-grained, consistent, and flexible sense tagging method. The integration of senses into the traditional stem-based index overcomes some of the negative impact of disambiguation errors. Different from using predefined sense inventories, Sch¨utze and Pedersen (1995) induced the sense inventory directly from the text </context>
</contexts>
<marker>Kim, Seo, Rim, 2004</marker>
<rawString>S. B. Kim, H. C. Seo, and H. C. Rim. 2004. Information retrieval using word senses: root sense tagging approach. In Proceedings of the 27th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 258–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Krovetz</author>
<author>W B Croft</author>
</authors>
<title>Lexical ambiguity and information retrieval.</title>
<date>1992</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context position="2368" citStr="Krovetz and Croft, 1992" startWordPosition="358" endWordPosition="361">correct meaning of the ambiguous words in both queries and documents can help improve retrieval precision. Second, query words may have tightly related meanings with other words not in the query. Making use of these relations between words can improve retrieval recall. Overall, IR systems can potentially benefit from the correct meanings of words provided by WSD systems. However, in previous investigations of the usage of WSD in IR, different researchers arrived at conflicting observations and conclusions. Some of the early research showed a drop in retrieval performance by using word senses (Krovetz and Croft, 1992; Voorhees, 1993). Some other experiments observed improvements by integrating word senses in IR systems (Sch¨utze and Pedersen, 1995; Gonzalo et al., 1998; Stokoe et al., 2003; Kim et al., 2004). This paper proposes the use of word senses to improve the performance of IR. We propose an approach to annotate the senses for short queries. We incorporate word senses into the language modeling (LM) approach to IR (Ponte and Croft, 1998), and utilize sense synonym relations to further improve the performance. Our evaluation on standard TREC1 data sets shows that supervised WSD outperforms two other</context>
<context position="3881" citStr="Krovetz and Croft (1992)" startWordPosition="602" endWordPosition="605">of 1http://trec.nist.gov/ 273 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 273–282, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics generating word senses for query terms in Section 4, followed by presenting our novel method of incorporating word senses and their synonyms into the LM approach in Section 5. We present experiments and analyze the results in Section 6. Finally, we conclude in Section 7. 2 Related Work Many previous studies have analyzed the benefits and the problems of applying WSD to IR. Krovetz and Croft (1992) studied the sense matches between terms in query and the document collection. They concluded that the benefits of WSD in IR are not as expected because query words have skewed sense distribution and the collocation effect from other query terms already performs some disambiguation. Sanderson (1994; 2000) used pseudowords to introduce artificial word ambiguity in order to study the impact of sense ambiguity on IR. He concluded that because the effectiveness of WSD can be negated by inaccurate WSD performance, high accuracy of WSD is an essential requirement to achieve improvement. In another w</context>
</contexts>
<marker>Krovetz, Croft, 1992</marker>
<rawString>R. Krovetz and W. B. Croft. 1992. Lexical ambiguity and information retrieval. ACM Transactions on Information Systems, 10(2):115–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K L Kwok</author>
<author>M Chan</author>
</authors>
<title>Improving two-stage ad-hoc retrieval for short queries.</title>
<date>1998</date>
<booktitle>In Proceedings of the 21st International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>250--256</pages>
<contexts>
<context position="11413" citStr="Kwok and Chan, 1998" startWordPosition="1847" endWordPosition="1850">ts calculated based on the relevance model described in Lavrenko and Croft (2001): w(t, Dq) = Ed∈Dq [tf (t,d) X p(q |θd) X p(θd)] , |d| which calculates the sum of weighted probabilities of t in each document. After normalization, the probability of t in θr q is calculated as follows: p(t|θrq) = w(t,Dq) Et0∈Tq w(t0,Dq). Finally, the relevance model is interpolated with the original query model: p(t|θ�Tf q ) = λ p(t|θrq) + (1 − λ)p(t|θq), (4) where parameter λ controls the amount of feedback. The new model θ�Tf q is used to replace the original one θq in Equation 1. Collection enrichment (CE) (Kwok and Chan, 1998) is a technique to improve the quality of the feedback documents by making use of an external target text collection X in addition to the original target C in the first step of PRF. The usage of X is supposed to provide more relevant feedback documents and feedback query terms. 2http://lemurproject.org/indri/ tE 275 4 Word Sense Disambiguation In this section, we first describe the construction of our WSD system. Then, we propose the method of assigning senses to query terms. 4.1 Word sense disambiguation system Previous research shows that translations in another language can be used to disam</context>
</contexts>
<marker>Kwok, Chan, 1998</marker>
<rawString>K. L. Kwok and M. Chan. 1998. Improving two-stage ad-hoc retrieval for short queries. In Proceedings of the 21st International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 250–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>C Zhai</author>
</authors>
<title>Document language models, query models, and risk minimization for information retrieval.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>111--119</pages>
<contexts>
<context position="8781" citStr="Lafferty and Zhai, 2001" startWordPosition="1378" endWordPosition="1382">e modeling approach In the language modeling approach to IR, language models are constructed for each query q and each document d in a text collection C. The documents in C are ranked by the distance to a given query q according to the language models. The most commonly used language model in IR is the unigram model, in which terms are assumed to be independent of each other. In the rest of this paper, language model will refer to the unigram language model. One of the commonly used measures of the similarity between query model and document model is negative Kullback-Leibler (KL) divergence (Lafferty and Zhai, 2001). With unigram model, the negative KL-divergence between model θq of query q and model θd of document d is calculated as follows: −D(θq||θd)=− t∈V � p(t|θq) log p(t|θq) p(t|θd) p(t|θq)log p(t|θd)− � p(t|θq)log p(t|θq) t∈V �= p(t|θq)log p(t|θd) + E(θq), (1) t∈V where p(t|θq) and p(t|θd) are the generative probabilities of a term t from the models θq and θd, V is the vocabulary of C, and E(θq) is the entropy of q. Define tf (t, d) and tf (t, q) as the frequencies of t in d and q, respectively. Normally, p(t|θq) is calculated with maximum likelihood estimation (MLE): p(t|θq) = tf (t,q) Et0∈q tf (</context>
</contexts>
<marker>Lafferty, Zhai, 2001</marker>
<rawString>J. Lafferty and C. Zhai. 2001. Document language models, query models, and risk minimization for information retrieval. In Proceedings of the 24th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 111–119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Lavrenko</author>
<author>W B Croft</author>
</authors>
<title>Relevance based language models.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>120--127</pages>
<contexts>
<context position="10874" citStr="Lavrenko and Croft (2001)" startWordPosition="1751" endWordPosition="1754">e selected from the top k ranked documents Dq for query expansion, under the assumption that these k documents are relevant to the query. Then, the expanded query is used to retrieve the documents from C. There are several methods to select expansion terms in the second step (Zhai and Lafferty, 2001a). For example, in Indri2, the terms are first ranked by the following score: v(t, Dq) = Ed∈Dq log(tf (t,d) |d |X 1 p(t|θC)), as in Ponte (1998). Define p(q|θd) as the probability score assigned to d. The top m terms Tq are selected with weights calculated based on the relevance model described in Lavrenko and Croft (2001): w(t, Dq) = Ed∈Dq [tf (t,d) X p(q |θd) X p(θd)] , |d| which calculates the sum of weighted probabilities of t in each document. After normalization, the probability of t in θr q is calculated as follows: p(t|θrq) = w(t,Dq) Et0∈Tq w(t0,Dq). Finally, the relevance model is interpolated with the original query model: p(t|θ�Tf q ) = λ p(t|θrq) + (1 − λ)p(t|θq), (4) where parameter λ controls the amount of feedback. The new model θ�Tf q is used to replace the original one θq in Equation 1. Collection enrichment (CE) (Kwok and Chan, 1998) is a technique to improve the quality of the feedback docume</context>
</contexts>
<marker>Lavrenko, Croft, 2001</marker>
<rawString>V. Lavrenko and W. B. Croft. 2001. Relevance based language models. In Proceedings of the 24th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 120–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Liu</author>
<author>F Liu</author>
<author>C Yu</author>
<author>W Meng</author>
</authors>
<title>An effective approach to document retrieval via utilizing WordNet and recognizing phrases.</title>
<date>2004</date>
<booktitle>In Proceedings of the 27th International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>266--272</pages>
<contexts>
<context position="7393" citStr="Liu et al., 2004" startWordPosition="1140" endWordPosition="1143">howed that using senses improved retrieval performance, and the combination of word-based ranking and sense-based ranking can further improve performance. However, the clustering process of each word is a time consuming task. Because the sense inventory is collection dependent, it is also hard to expand the text collection without re-doing preprocessing. Many studies investigated the expansion effects by using knowledge sources from thesauri. Some researchers achieved improvements by expanding the disambiguated query words with synonyms and some other information from WordNet (Voorhees, 1994; Liu et al., 2004; Liu et al., 2005; Fang, 2008). The usage of knowledge sources from WordNet in document expansion also showed improvements in IR systems (Cao et al., 2005; Agirre et al., 2010). The previous work shows that the WSD errors can easily neutralize its positive effect. It is important to reduce the negative impact of erroneous disambiguation, and the integration of senses into traditional term index, such as stem-based index, is a possible solution. The utilization of semantic relations has proved to be helpful for IR. It is also interest274 ing to investigate the utilization of semantic relations</context>
</contexts>
<marker>Liu, Liu, Yu, Meng, 2004</marker>
<rawString>S. Liu, F. Liu, C. Yu, and W. Meng. 2004. An effective approach to document retrieval via utilizing WordNet and recognizing phrases. In Proceedings of the 27th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 266–272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Liu</author>
<author>C Yu</author>
<author>W Meng</author>
</authors>
<title>Word sense disambiguation in queries.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th ACM Conference on Information and Knowledge Management,</booktitle>
<pages>525--532</pages>
<contexts>
<context position="7411" citStr="Liu et al., 2005" startWordPosition="1144" endWordPosition="1147">enses improved retrieval performance, and the combination of word-based ranking and sense-based ranking can further improve performance. However, the clustering process of each word is a time consuming task. Because the sense inventory is collection dependent, it is also hard to expand the text collection without re-doing preprocessing. Many studies investigated the expansion effects by using knowledge sources from thesauri. Some researchers achieved improvements by expanding the disambiguated query words with synonyms and some other information from WordNet (Voorhees, 1994; Liu et al., 2004; Liu et al., 2005; Fang, 2008). The usage of knowledge sources from WordNet in document expansion also showed improvements in IR systems (Cao et al., 2005; Agirre et al., 2010). The previous work shows that the WSD errors can easily neutralize its positive effect. It is important to reduce the negative impact of erroneous disambiguation, and the integration of senses into traditional term index, such as stem-based index, is a possible solution. The utilization of semantic relations has proved to be helpful for IR. It is also interest274 ing to investigate the utilization of semantic relations among senses in I</context>
</contexts>
<marker>Liu, Yu, Meng, 2005</marker>
<rawString>S. Liu, C. Yu, and W. Meng. 2005. Word sense disambiguation in queries. In Proceedings of the 14th ACM Conference on Information and Knowledge Management, pages 525–532.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<title>WordNet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="5306" citStr="Miller, 1990" startWordPosition="822" endWordPosition="823">ate senses as well as synsets (synonym sets). Their experiment also showed that with the synset representation, which included synonym information, WSD with an error rate of 40%–50% can still improve IR performance. Their later work (Gonzalo et al., 1999) verified that part of speech (POS) information is discriminatory for IR purposes. Several works attempted to disambiguate terms in both queries and documents with the senses predefined in hand-crafted sense inventories, and then used the senses to perform indexing and retrieval. Voorhees (1993) used the hyponymy (“IS-A”) relation in WordNet (Miller, 1990) to disambiguate the polysemous nouns in a text. In her experiments, the performance of sense-based retrieval is worse than stem-based retrieval on all test collections. Her analysis showed that inaccurate WSD caused the poor results. Stokoe et al. (2003) employed a fine-grained WSD system with an accuracy of 62.1% to disambiguate terms in both the text collections and the queries in their experiments. Their evaluation on TREC collections achieved significant improvements over a standard term based vector space model. However, it is hard to judge the effect of word senses because of the overal</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>G. A. Miller. 1990. WordNet: An on-line lexical database. International Journal of Lexicography, 3(4):235–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="12622" citStr="Och and Ney, 2003" startWordPosition="2040" endWordPosition="2043"> to disambiguate the meanings of words (Chan and Ng, 2005; Zhong and Ng, 2009). We construct our supervised WSD system directly from parallel corpora. To generate the WSD training data, 7 parallel corpora were used, including Chinese Treebank, FBIS Corpus, Hong Kong Hansards, Hong Kong Laws, Hong Kong News, Sinorama News Magazine, and Xinhua Newswire. These corpora were already aligned at sentence level. We tokenized English texts with Penn Treebank Tokenizer, and performed word segmentation on Chinese texts. Then, word alignment was performed on the parallel corpora with the GIZA++ software (Och and Ney, 2003). For each English morphological root e, the English sentences containing its occurrences were extracted from the word aligned output of GIZA++, as well as the corresponding translations of these occurrences. To minimize noisy word alignment result, translations with no Chinese character were deleted, and we further removed a translation when it only appears once, or its frequency is less than 10 and also less than 1% of the frequency of e. Finally, only the most frequent 10 translations were kept for efficiency consideration. The English part of the remaining occurrences were used as training</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ogilvie</author>
<author>J Callan</author>
</authors>
<title>Experiments using the Lemur toolkit.</title>
<date>2001</date>
<booktitle>In Proceedings of the 10th Text REtrieval Conference,</booktitle>
<pages>103--108</pages>
<contexts>
<context position="22563" citStr="Ogilvie and Callan, 2001" startWordPosition="3902" endWordPosition="3905">004 (RB04). In total, our test set includes 199 queries. We use the terms in the title field of TREC topics as queries. Table 1 shows the statistics of the five query sets. The first column lists the query topics, and the column #qry is the number of queries. The column Ave gives the average query length, and the column Rels is the total number of relevant documents. Query Set Topics #qry Ave Rels TREC6 301–350 50 2.58 4,290 TREC7 351–400 50 2.50 4,674 TREC8 401–450 50 2.46 4,728 RB03 601–650 50 3.00 1,658 RB044 651–700 49 2.96 2,062 Table 1: Statistics of query sets We use the Lemur toolkit (Ogilvie and Callan, 2001) version 4.11 as the basic retrieval tool, and select the default unigram LM approach based on KLdivergence and Dirichlet-prior smoothing method in Lemur as our basic retrieval approach. Stop words are removed from queries and documents using the standard INQUERY stop words list (Allan et al., 2000), and then the Porter stemmer is applied to perform stemming. The stem forms are finally used for indexing and retrieval. We set the smoothing parameter µ in Equation 3 to 400 by tuning on TREC6 query set in a range of 1100, 400, 700,1000,1500, 2000, 3000, 4000, 5000}. With this basic method, up to </context>
</contexts>
<marker>Ogilvie, Callan, 2001</marker>
<rawString>P. Ogilvie and J. Callan. 2001. Experiments using the Lemur toolkit. In Proceedings of the 10th Text REtrieval Conference, pages 103–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Ponte</author>
<author>W B Croft</author>
</authors>
<title>A language modeling approach to information retrieval.</title>
<date>1998</date>
<booktitle>In Proceedings of the 21st International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>275--281</pages>
<contexts>
<context position="2804" citStr="Ponte and Croft, 1998" startWordPosition="431" endWordPosition="434">different researchers arrived at conflicting observations and conclusions. Some of the early research showed a drop in retrieval performance by using word senses (Krovetz and Croft, 1992; Voorhees, 1993). Some other experiments observed improvements by integrating word senses in IR systems (Sch¨utze and Pedersen, 1995; Gonzalo et al., 1998; Stokoe et al., 2003; Kim et al., 2004). This paper proposes the use of word senses to improve the performance of IR. We propose an approach to annotate the senses for short queries. We incorporate word senses into the language modeling (LM) approach to IR (Ponte and Croft, 1998), and utilize sense synonym relations to further improve the performance. Our evaluation on standard TREC1 data sets shows that supervised WSD outperforms two other WSD baselines and significantly improves IR. The rest of this paper is organized as follows. In Section 2, we first review previous work using WSD in IR. Section 3 introduces the LM approach to IR, including the pseudo relevance feedback method. We describe our WSD system and the method of 1http://trec.nist.gov/ 273 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 273–282, Jeju, Republi</context>
</contexts>
<marker>Ponte, Croft, 1998</marker>
<rawString>J. M. Ponte and W. B. Croft. 1998. A language modeling approach to information retrieval. In Proceedings of the 21st International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 275–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Ponte</author>
</authors>
<title>A Language Modeling Approach to Information Retreival.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, University of Massachusetts.</institution>
<contexts>
<context position="10694" citStr="Ponte (1998)" startWordPosition="1723" endWordPosition="1724">rieval steps. In the first step, ranked documents are retrieved from C by a normal retrieval method with the original query q. In the second step, a number of terms are selected from the top k ranked documents Dq for query expansion, under the assumption that these k documents are relevant to the query. Then, the expanded query is used to retrieve the documents from C. There are several methods to select expansion terms in the second step (Zhai and Lafferty, 2001a). For example, in Indri2, the terms are first ranked by the following score: v(t, Dq) = Ed∈Dq log(tf (t,d) |d |X 1 p(t|θC)), as in Ponte (1998). Define p(q|θd) as the probability score assigned to d. The top m terms Tq are selected with weights calculated based on the relevance model described in Lavrenko and Croft (2001): w(t, Dq) = Ed∈Dq [tf (t,d) X p(q |θd) X p(θd)] , |d| which calculates the sum of weighted probabilities of t in each document. After normalization, the probability of t in θr q is calculated as follows: p(t|θrq) = w(t,Dq) Et0∈Tq w(t0,Dq). Finally, the relevance model is interpolated with the original query model: p(t|θ�Tf q ) = λ p(t|θrq) + (1 − λ)p(t|θq), (4) where parameter λ controls the amount of feedback. The </context>
</contexts>
<marker>Ponte, 1998</marker>
<rawString>J. M. Ponte. 1998. A Language Modeling Approach to Information Retreival. Ph.D. thesis, Department of Computer Science, University of Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sanderson</author>
</authors>
<title>Word sense disambiguation and information retrieval.</title>
<date>1994</date>
<booktitle>In Proceedings of the 17th International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>142--151</pages>
<contexts>
<context position="4180" citStr="Sanderson (1994" startWordPosition="650" endWordPosition="651">l method of incorporating word senses and their synonyms into the LM approach in Section 5. We present experiments and analyze the results in Section 6. Finally, we conclude in Section 7. 2 Related Work Many previous studies have analyzed the benefits and the problems of applying WSD to IR. Krovetz and Croft (1992) studied the sense matches between terms in query and the document collection. They concluded that the benefits of WSD in IR are not as expected because query words have skewed sense distribution and the collocation effect from other query terms already performs some disambiguation. Sanderson (1994; 2000) used pseudowords to introduce artificial word ambiguity in order to study the impact of sense ambiguity on IR. He concluded that because the effectiveness of WSD can be negated by inaccurate WSD performance, high accuracy of WSD is an essential requirement to achieve improvement. In another work, Gonzalo et al. (1998) used a manually sense annotated corpus, SemCor, to study the effects of incorrect disambiguation. They obtained significant improvements by representing documents and queries with accurate senses as well as synsets (synonym sets). Their experiment also showed that with th</context>
</contexts>
<marker>Sanderson, 1994</marker>
<rawString>M. Sanderson. 1994. Word sense disambiguation and information retrieval. In Proceedings of the 17th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 142–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sanderson</author>
</authors>
<title>Retrieving with good sense.</title>
<date>2000</date>
<journal>Information Retrieval,</journal>
<volume>2</volume>
<issue>1</issue>
<marker>Sanderson, 2000</marker>
<rawString>M. Sanderson. 2000. Retrieving with good sense. Information Retrieval, 2(1):49–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sch¨utze</author>
<author>J O Pedersen</author>
</authors>
<title>Information retrieval based on word senses.</title>
<date>1995</date>
<booktitle>In Proceedings of the 4th Annual Symposium on Document Analysis and Information Retrieval,</booktitle>
<pages>161--175</pages>
<marker>Sch¨utze, Pedersen, 1995</marker>
<rawString>H. Sch¨utze and J. O. Pedersen. 1995. Information retrieval based on word senses. In Proceedings of the 4th Annual Symposium on Document Analysis and Information Retrieval, pages 161–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Stokoe</author>
<author>M P Oakes</author>
<author>J Tait</author>
</authors>
<title>Word sense disambiguation in information retrieval revisited.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>159--166</pages>
<contexts>
<context position="2544" citStr="Stokoe et al., 2003" startWordPosition="385" endWordPosition="388">t in the query. Making use of these relations between words can improve retrieval recall. Overall, IR systems can potentially benefit from the correct meanings of words provided by WSD systems. However, in previous investigations of the usage of WSD in IR, different researchers arrived at conflicting observations and conclusions. Some of the early research showed a drop in retrieval performance by using word senses (Krovetz and Croft, 1992; Voorhees, 1993). Some other experiments observed improvements by integrating word senses in IR systems (Sch¨utze and Pedersen, 1995; Gonzalo et al., 1998; Stokoe et al., 2003; Kim et al., 2004). This paper proposes the use of word senses to improve the performance of IR. We propose an approach to annotate the senses for short queries. We incorporate word senses into the language modeling (LM) approach to IR (Ponte and Croft, 1998), and utilize sense synonym relations to further improve the performance. Our evaluation on standard TREC1 data sets shows that supervised WSD outperforms two other WSD baselines and significantly improves IR. The rest of this paper is organized as follows. In Section 2, we first review previous work using WSD in IR. Section 3 introduces </context>
<context position="5561" citStr="Stokoe et al. (2003)" startWordPosition="860" endWordPosition="863">, 1999) verified that part of speech (POS) information is discriminatory for IR purposes. Several works attempted to disambiguate terms in both queries and documents with the senses predefined in hand-crafted sense inventories, and then used the senses to perform indexing and retrieval. Voorhees (1993) used the hyponymy (“IS-A”) relation in WordNet (Miller, 1990) to disambiguate the polysemous nouns in a text. In her experiments, the performance of sense-based retrieval is worse than stem-based retrieval on all test collections. Her analysis showed that inaccurate WSD caused the poor results. Stokoe et al. (2003) employed a fine-grained WSD system with an accuracy of 62.1% to disambiguate terms in both the text collections and the queries in their experiments. Their evaluation on TREC collections achieved significant improvements over a standard term based vector space model. However, it is hard to judge the effect of word senses because of the overall poor performances of their baseline method and their system. Instead of using fine-grained sense inventory, Kim et al. (2004) tagged words with 25 root senses of nouns in WordNet. Their retrieval method maintained the stem-based index and adjusted the t</context>
</contexts>
<marker>Stokoe, Oakes, Tait, 2003</marker>
<rawString>C. Stokoe, M. P. Oakes, and J. Tait. 2003. Word sense disambiguation in information retrieval revisited. In Proceedings of the 26th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 159–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>Using WordNet to disambiguate word senses for text retrieval.</title>
<date>1993</date>
<booktitle>In Proceedings of the 16th International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>171--180</pages>
<contexts>
<context position="2385" citStr="Voorhees, 1993" startWordPosition="362" endWordPosition="363">biguous words in both queries and documents can help improve retrieval precision. Second, query words may have tightly related meanings with other words not in the query. Making use of these relations between words can improve retrieval recall. Overall, IR systems can potentially benefit from the correct meanings of words provided by WSD systems. However, in previous investigations of the usage of WSD in IR, different researchers arrived at conflicting observations and conclusions. Some of the early research showed a drop in retrieval performance by using word senses (Krovetz and Croft, 1992; Voorhees, 1993). Some other experiments observed improvements by integrating word senses in IR systems (Sch¨utze and Pedersen, 1995; Gonzalo et al., 1998; Stokoe et al., 2003; Kim et al., 2004). This paper proposes the use of word senses to improve the performance of IR. We propose an approach to annotate the senses for short queries. We incorporate word senses into the language modeling (LM) approach to IR (Ponte and Croft, 1998), and utilize sense synonym relations to further improve the performance. Our evaluation on standard TREC1 data sets shows that supervised WSD outperforms two other WSD baselines an</context>
<context position="5244" citStr="Voorhees (1993)" startWordPosition="812" endWordPosition="813">nt improvements by representing documents and queries with accurate senses as well as synsets (synonym sets). Their experiment also showed that with the synset representation, which included synonym information, WSD with an error rate of 40%–50% can still improve IR performance. Their later work (Gonzalo et al., 1999) verified that part of speech (POS) information is discriminatory for IR purposes. Several works attempted to disambiguate terms in both queries and documents with the senses predefined in hand-crafted sense inventories, and then used the senses to perform indexing and retrieval. Voorhees (1993) used the hyponymy (“IS-A”) relation in WordNet (Miller, 1990) to disambiguate the polysemous nouns in a text. In her experiments, the performance of sense-based retrieval is worse than stem-based retrieval on all test collections. Her analysis showed that inaccurate WSD caused the poor results. Stokoe et al. (2003) employed a fine-grained WSD system with an accuracy of 62.1% to disambiguate terms in both the text collections and the queries in their experiments. Their evaluation on TREC collections achieved significant improvements over a standard term based vector space model. However, it is</context>
</contexts>
<marker>Voorhees, 1993</marker>
<rawString>E. M. Voorhees. 1993. Using WordNet to disambiguate word senses for text retrieval. In Proceedings of the 16th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 171–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>Query expansion using lexicalsemantic relations.</title>
<date>1994</date>
<booktitle>In Proceedings of the 17th International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>61--69</pages>
<contexts>
<context position="7375" citStr="Voorhees, 1994" startWordPosition="1138" endWordPosition="1139">ir experiments showed that using senses improved retrieval performance, and the combination of word-based ranking and sense-based ranking can further improve performance. However, the clustering process of each word is a time consuming task. Because the sense inventory is collection dependent, it is also hard to expand the text collection without re-doing preprocessing. Many studies investigated the expansion effects by using knowledge sources from thesauri. Some researchers achieved improvements by expanding the disambiguated query words with synonyms and some other information from WordNet (Voorhees, 1994; Liu et al., 2004; Liu et al., 2005; Fang, 2008). The usage of knowledge sources from WordNet in document expansion also showed improvements in IR systems (Cao et al., 2005; Agirre et al., 2010). The previous work shows that the WSD errors can easily neutralize its positive effect. It is important to reduce the negative impact of erroneous disambiguation, and the integration of senses into traditional term index, such as stem-based index, is a possible solution. The utilization of semantic relations has proved to be helpful for IR. It is also interest274 ing to investigate the utilization of </context>
</contexts>
<marker>Voorhees, 1994</marker>
<rawString>E. M. Voorhees. 1994. Query expansion using lexicalsemantic relations. In Proceedings of the 17th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 61–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Zhai</author>
<author>J Lafferty</author>
</authors>
<title>Model-based feedback in the language modeling approach to information retrieval.</title>
<date>2001</date>
<booktitle>In Proceedings of the 10th ACM Conference on Information and Knowledge Management,</booktitle>
<pages>403--410</pages>
<contexts>
<context position="9582" citStr="Zhai and Lafferty, 2001" startWordPosition="1519" endWordPosition="1522">(t|θq)log p(t|θd)− � p(t|θq)log p(t|θq) t∈V �= p(t|θq)log p(t|θd) + E(θq), (1) t∈V where p(t|θq) and p(t|θd) are the generative probabilities of a term t from the models θq and θd, V is the vocabulary of C, and E(θq) is the entropy of q. Define tf (t, d) and tf (t, q) as the frequencies of t in d and q, respectively. Normally, p(t|θq) is calculated with maximum likelihood estimation (MLE): p(t|θq) = tf (t,q) Et0∈q tf (t0,q). (2) In the calculation of p(t|θd), several smoothing methods have been proposed to overcome the data sparseness problem of a language model constructed from one document (Zhai and Lafferty, 2001b). For example, p(t|θd) with the Dirichlet-prior smoothing can be calculated as follows: tf (t, d) + µ p(t|θC) p(t|θd) = (3) Et0 ∈V tf (t0, d) + µ , where µ is the prior parameter in the Dirichlet-prior smoothing method, and p(t|θC) is the probability of t in C, which is often calculated with MLE: ( Ed0 ∈C tf (t,d0) p(t|θ C) = E E t0∈V tf (t0,d0). d0∈C 3.2 Pseudo relevance feedback Pseudo relevance feedback (PRF) is widely used in IR to achieve better performance. It is constructed with two retrieval steps. In the first step, ranked documents are retrieved from C by a normal retrieval method </context>
</contexts>
<marker>Zhai, Lafferty, 2001</marker>
<rawString>C. Zhai and J. Lafferty. 2001a. Model-based feedback in the language modeling approach to information retrieval. In Proceedings of the 10th ACM Conference on Information and Knowledge Management, pages 403–410.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Zhai</author>
<author>J Lafferty</author>
</authors>
<title>A study of smoothing methods for language models applied to ad hoc information retrieval.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>334--342</pages>
<contexts>
<context position="9582" citStr="Zhai and Lafferty, 2001" startWordPosition="1519" endWordPosition="1522">(t|θq)log p(t|θd)− � p(t|θq)log p(t|θq) t∈V �= p(t|θq)log p(t|θd) + E(θq), (1) t∈V where p(t|θq) and p(t|θd) are the generative probabilities of a term t from the models θq and θd, V is the vocabulary of C, and E(θq) is the entropy of q. Define tf (t, d) and tf (t, q) as the frequencies of t in d and q, respectively. Normally, p(t|θq) is calculated with maximum likelihood estimation (MLE): p(t|θq) = tf (t,q) Et0∈q tf (t0,q). (2) In the calculation of p(t|θd), several smoothing methods have been proposed to overcome the data sparseness problem of a language model constructed from one document (Zhai and Lafferty, 2001b). For example, p(t|θd) with the Dirichlet-prior smoothing can be calculated as follows: tf (t, d) + µ p(t|θC) p(t|θd) = (3) Et0 ∈V tf (t0, d) + µ , where µ is the prior parameter in the Dirichlet-prior smoothing method, and p(t|θC) is the probability of t in C, which is often calculated with MLE: ( Ed0 ∈C tf (t,d0) p(t|θ C) = E E t0∈V tf (t0,d0). d0∈C 3.2 Pseudo relevance feedback Pseudo relevance feedback (PRF) is widely used in IR to achieve better performance. It is constructed with two retrieval steps. In the first step, ranked documents are retrieved from C by a normal retrieval method </context>
</contexts>
<marker>Zhai, Lafferty, 2001</marker>
<rawString>C. Zhai and J. Lafferty. 2001b. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of the 24th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 334–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Zhong</author>
<author>H T Ng</author>
</authors>
<title>Word sense disambiguation for all words without hard labor.</title>
<date>2009</date>
<booktitle>In Proceedings of the 21st International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1616--1621</pages>
<contexts>
<context position="12082" citStr="Zhong and Ng, 2009" startWordPosition="1958" endWordPosition="1961">back documents by making use of an external target text collection X in addition to the original target C in the first step of PRF. The usage of X is supposed to provide more relevant feedback documents and feedback query terms. 2http://lemurproject.org/indri/ tE 275 4 Word Sense Disambiguation In this section, we first describe the construction of our WSD system. Then, we propose the method of assigning senses to query terms. 4.1 Word sense disambiguation system Previous research shows that translations in another language can be used to disambiguate the meanings of words (Chan and Ng, 2005; Zhong and Ng, 2009). We construct our supervised WSD system directly from parallel corpora. To generate the WSD training data, 7 parallel corpora were used, including Chinese Treebank, FBIS Corpus, Hong Kong Hansards, Hong Kong Laws, Hong Kong News, Sinorama News Magazine, and Xinhua Newswire. These corpora were already aligned at sentence level. We tokenized English texts with Penn Treebank Tokenizer, and performed word segmentation on Chinese texts. Then, word alignment was performed on the parallel corpora with the GIZA++ software (Och and Ney, 2003). For each English morphological root e, the English sentenc</context>
</contexts>
<marker>Zhong, Ng, 2009</marker>
<rawString>Z. Zhong and H. T. Ng. 2009. Word sense disambiguation for all words without hard labor. In Proceedings of the 21st International Joint Conference on Artificial Intelligence, pages 1616–1621.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Zhong</author>
<author>H T Ng</author>
</authors>
<title>It Makes Sense: A widecoverage word sense disambiguation system for free text.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association of Computational Linguistics: System Demonstrations,</booktitle>
<pages>78--83</pages>
<contexts>
<context position="13513" citStr="Zhong and Ng, 2010" startWordPosition="2184" endWordPosition="2187">hinese character were deleted, and we further removed a translation when it only appears once, or its frequency is less than 10 and also less than 1% of the frequency of e. Finally, only the most frequent 10 translations were kept for efficiency consideration. The English part of the remaining occurrences were used as training data. Because multiple English words may have the same Chinese translation, to differentiate them, each Chinese translation is concatenated with the English morphological root to form a word sense. We employed a supervised WSD system, IMS3, to train the WSD models. IMS (Zhong and Ng, 2010) integrates multiple knowledge sources as features. We used MaxEnt as the machine learning algorithm. Finally, the system can disambiguate the words by assigning probabilities to different senses. 3http://nlp.comp.nus.edu.sg/software/ims 4.2 Estimating sense distributions for query terms In IR, both terms in queries and the text collection can be ambiguous. Hence, WSD is needed to disambiguate these ambiguous terms. In most cases, documents in a text collection are full articles. Therefore, a WSD system has sufficient context to disambiguate the words in the document. In contrast, queries are </context>
</contexts>
<marker>Zhong, Ng, 2010</marker>
<rawString>Z. Zhong and H. T. Ng. 2010. It Makes Sense: A widecoverage word sense disambiguation system for free text. In Proceedings of the 48th Annual Meeting of the Association of Computational Linguistics: System Demonstrations, pages 78–83.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>