<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.996475">
A Statistical Tree Annotator and Its Applications
</title>
<author confidence="0.929438">
Xiaoqiang Luo and Bing Zhao
</author>
<affiliation confidence="0.728406">
IBM T.J. Watson Research Center
</affiliation>
<address confidence="0.950168">
1101 Kitchawan Road
Yorktown Heights, NY 10598
</address>
<email confidence="0.99889">
{xiaoluo,zhaob}@us.ibm.com
</email>
<sectionHeader confidence="0.995641" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999750083333333">
In many natural language applications, there
is a need to enrich syntactical parse trees. We
present a statistical tree annotator augmenting
nodes with additional information. The anno-
tator is generic and can be applied to a va-
riety of applications. We report 3 such ap-
plications in this paper: predicting function
tags; predicting null elements; and predicting
whether a tree constituent is projectable in ma-
chine translation. Our function tag prediction
system outperforms significantly published re-
sults.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999764888888889">
Syntactic parsing has made tremendous progress in
the past 2 decades (Magerman, 1994; Ratnaparkhi,
1997; Collins, 1997; Charniak, 2000; Klein and
Manning, 2003; Carreras et al., 2008), and accu-
rate syntactic parsing is often assumed when devel-
oping other natural language applications. On the
other hand, there are plenty of language applications
where basic syntactic information is insufficient. For
instance, in question answering, it is highly desir-
able to have the semantic information of a syntactic
constituent, e.g., a noun-phrase (NP) is a person or
an organization; an adverbial phrase is locative or
temporal. As syntactic information has been widely
used in machine translation systems (Yamada and
Knight, 2001; Xiong et al., 2010; Shen et al., 2008;
Chiang, 2010; Shen et al., 2010), an interesting
question is to predict whether or not a syntactic con-
stituent is projectable1 across a language pair.
</bodyText>
<footnote confidence="0.691498">
1A constituent in the source language is projectable if it can
be aligned to a contiguous span in the target language.
</footnote>
<bodyText confidence="0.999947542857143">
Such problems can be abstracted as adding addi-
tional annotations to an existing tree structure. For
example, the English Penn treebank (Marcus et al.,
1993) contains function tags and many carry seman-
tic information. To add semantic information to the
basic syntactic trees, a logical step is to predict these
function tags after syntactic parsing. For the prob-
lem of predicting projectable syntactic constituent,
one can use a sentence alignment tool and syntac-
tic trees on source sentences to create training data
by annotating a tree node as projectable or not. A
generic tree annotator can also open the door of solv-
ing other natural language problems so long as the
problem can be cast as annotating tree nodes. As
one such example, we will present how to predict
empty elements for the Chinese language.
Some of the above-mentioned problems have
been studied before: predicting function tags were
studied in (Blaheta and Charniak, 2000; Blaheta,
2003; Lintean and Rus, 2007a), and results of pre-
dicting and recovering empty elements can be found
in (Dienes et al., 2003; Schmid, 2006; Campbell,
2004). In this work, we will show that these seem-
ingly unrelated problems can be treated uniformly
as adding annotations to an existing tree structure,
which is the first goal of this work. Second, the
proposed generic tree annotator can also be used
to solve new problems: we will show how it can
be used to predict projectable syntactic constituents.
Third, the uniform treatment not only simplifies the
model building process, but also affords us to con-
centrate on discovering most useful features for a
particular application which often leads to improved
performances, e.g, we find some features are very
effective in predicting function tags and our system
</bodyText>
<page confidence="0.901559">
1230
</page>
<note confidence="0.9800495">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1230–1238,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.986408454545454">
has significant lower error rate than (Blaheta and
Charniak, 2000; Lintean and Rus, 2007a).
The rest of the paper is organized as follows. Sec-
tion 2 describes our tree annotator, which is a con-
ditional log-linear model. Section 3 describes the
features used in our system. Next, three applications
of the proposed tree annotator are presented in Sec-
tion 4: predicting English function tags, predicting
Chinese empty elements and predicting Arabic pro-
jectable constituents. Section 5 compares our work
with some related prior arts.
</bodyText>
<sectionHeader confidence="0.977924" genericHeader="method">
2 A MaxEnt Tree Annotator Model
</sectionHeader>
<bodyText confidence="0.999622777777778">
The input to the tree annotator is a tree T. While
T can be of any type, we concentrate on the syntac-
tic parse tree in this paper. The non-terminal nodes,
N = {n : n ∈ T} of T are associated with an
order by which they are visited so that they can be
indexed as n1, n2, · · · , n|T|, where |T |is the num-
ber of non-terminal nodes in T. As an example,
Figure 1 shows a syntactic parse tree with the pre-
fix order (i.e., the number at the up-right corner of
each non-terminal node), where child nodes are vis-
ited recursively from left to right before the parent
node is visited. Thus, the NP-SBJ node is visited
first, followed by the NP spanning duo action,
followed by the PP-CLR node etc.
With a prescribed tree visit order, our tree annota-
tor model predicts a symbol li, where li takes value
from a predefined finite set L, for each non-terminal
node ni in a sequential fashion:
</bodyText>
<figure confidence="0.296222">
P(l1, ··· , l|T||T)
Newsnight returns to duo action tonight
</figure>
<figureCaption confidence="0.977814">
Figure 1: A sample tree: the number on the upright corner
of each non-terminal node is the visit order.
</figureCaption>
<equation confidence="0.7897905">
is the normalizing factor to ensure that
P(li|l1, · · · ,li−1, T) in Equation (2) is a prob-
ability and {9k(li−1
1 ,T, li)} are feature functions.
</equation>
<bodyText confidence="0.99832394117647">
There are efficient training algorithms to find op-
timal weights relative to a labeled training data set
once the feature functions {9k(li−1
1 , T, li)} are se-
lected (Berger et al., 1996; Goodman, 2002; Malouf,
2002). In our work, we use the SCGIS training al-
gorithm (Goodman, 2002), and the features used in
our systems are detailed in the next section.
Once a model is trained, at testing time it is ap-
plied to input tree nodes by the same order. Figure 1
highlights the prediction of the function tag for node
3(i.e., PP-CLR-node in the thickened box) after 2
shaded nodes (NP-SBJ node and NP node) are pre-
dicted. Note that by this time the predicted values
are available to the system, while unvisited nodes
(nodes in dashed boxes in Figure 1) can not provide
such information.
</bodyText>
<figure confidence="0.883991722222222">
NP−SBJ
NNP
1
VBZ
S
6
VP
TO
PP−CLR
5
JJ
NP
3
NN
2
NP−TMP
NN
4
</figure>
<equation confidence="0.8536375">
= �|T |P(li|l1,··· ,li−1,T) (1) 3 Features
i=1
</equation>
<bodyText confidence="0.9898475">
The visit order is important since it determines what
are in the conditioning of Eq. (1).
</bodyText>
<equation confidence="0.998827">
P(li|l1, · · · , li−1, T) in this work is a conditional
log linear (or MaxEnt) model (Berger et al., 1996):
P(li|l1,··· ,li−1,T)
exp( Ek Ak9k(li−1
1 ,T, li)) (2)
Z(li−1
1 ,T)
Ak9k(li−1
1 , T, x))
</equation>
<bodyText confidence="0.999436285714286">
The features used in our systems are tabulated in Ta-
ble 1. Numbers in the first column are the feature in-
dices. The second column contains a brief descrip-
tion of each feature, and the third column contains
the feature value when the feature at the same row
is applied to the PP-node of Figure 1 for the task of
predicting function tags.
Feature 1 through 8 are non-lexical features in that
all of them are computed based on the labels or POS
tags of neighboring nodes (e.g., Feature 4 computes
the label or POS tag of the right most child), or the
structure information (e.g., Feature 5 computes the
number of child nodes).
where
</bodyText>
<equation confidence="0.9688634">
�
Z(li−1
1 , T ) =
exp( �
x∈L k
</equation>
<page confidence="0.931625">
1231
</page>
<bodyText confidence="0.999485">
Feature 9 and 10 are computed from past pre-
dicted values. When predicting the function tag for
the PP-node in Figure 1, there is no predicted value
for its left-sibling and any of its child node. That’s
why both feature values are NONE, a special sym-
bol signifying that a node does not carry any func-
tion tag. If we were to predict the function tag for
the VP-node, the value of Feature 9 would be SBJ,
while Feature 10 will be instantiated twice with one
value being CLR, another being TMP.
</bodyText>
<figure confidence="0.675612454545454">
No. Description Value
1 current node label PP
2 parent node label VP
3 left-most child label/tag TO
4 right-most child label/tag NP
5 number of child nodes 2
6 CFG rule PP-&gt;TO NP
7 label/tag of left sibling VBZ
8 label/tag of right sibling NP
9 predicted value of left-sibling NONE
10 predicted value of child nodes NONE
</figure>
<table confidence="0.782110555555556">
11 left-most internal word to
12 right-most internal word action
13 left neighboring external word returns
14 right neighboring external word tonight
15 head word of current node to
16 head word of parent node returns
17 is current node the head child false
18 label/tag of head child TO
19 predicted value of the head child NONE
</table>
<tableCaption confidence="0.72425">
Table 1: Feature functions: the 2nd column contains the
descriptions of each feature, and the 3rd column the fea-
ture value when it is applied to the PP-node in Figure 1.
</tableCaption>
<bodyText confidence="0.999987">
Feature 11 to 19 are lexical features or computed
from head nodes. Feature 11 and 12 compute the
node-internal boundary words, while Feature 13 and
14 compute the immediate node-external boundary
words. Feature 15 to 19 rely on the head informa-
tion. For instance, Feature 15 computes the head
word of the current node, which is to for the PP-
node in Figure 1. Feature 16 computes the same for
the parent node. Feature 17 tests if the current node
is the head of its parent. Feature 18 and 19 compute
the label or POS tag and the predicted value of the
head child, respectively.
Besides the basic feature presented in Table 1, we
also use conjunction features. For instance, applying
the conjunction of Feature 1 and 18 to the PP-node
in Figure 1 would yield a feature instance that cap-
tures the fact that the current node is a PP node and
its head child’s POS tag is TO.
</bodyText>
<sectionHeader confidence="0.963331" genericHeader="method">
4 Applications and Results
</sectionHeader>
<bodyText confidence="0.9999943">
A wide variety of language problems can be treated
as or cast into a tree annotating problem. In this
section, we present three applications of the statisti-
cal tree annotator. The first application is to predict
function tags of an input syntactic parse tree; the sec-
ond one is to predict Chinese empty elements; and
the third one is to predict whether a syntactic con-
stituent of a source sentence is projectable, meaning
if the constituent will have a contiguous translation
on the target language.
</bodyText>
<subsectionHeader confidence="0.998393">
4.1 Predicting Function Tags
</subsectionHeader>
<bodyText confidence="0.999828615384616">
In the English Penn Treebank (Marcus et al., 1993)
and more recent OntoNotes data (Hovy et al.,
2006), some tree nodes are assigned a function tag,
which is of one of the four types: grammatical,
form/function, topicalization and miscellaneous. Ta-
ble 2 contains a list of function tags used in the
English Penn Treebank (Bies et al., 1995). The
“Grammatical” row contains function tags marking
the grammatical role of a constituent, e.g., DTV for
dative objects, LGS for logical subjects etc. Many
tags in the “Form/function” row carry semantic in-
formation, e.g., LOC is for locative expressions, and
TMP for temporal expressions.
</bodyText>
<table confidence="0.998506625">
Type Function Tags
Grammatical (52.2%) DTV LGS PRD
PUT SBJ VOC
Form/function (36.2%) ADV BNF DIR
EXT LOC MNR
NOM PRP TMP
Topicalization (2.2%) TPC
Miscellaneous (9.4%) CLF CLR HLN TTL
</table>
<tableCaption confidence="0.9737785">
Table 2: Four types of function tags and their relative
frequency
</tableCaption>
<subsectionHeader confidence="0.906826">
4.1.1 Comparison with Prior Arts
</subsectionHeader>
<bodyText confidence="0.99438125">
In order to have a direct comparison with (Blaheta
and Charniak, 2000; Lintean and Rus, 2007a), we
use the same English Penn Treebank (Marcus et al.,
1993) and partition the data set identically: Section
</bodyText>
<page confidence="0.966078">
1232
</page>
<bodyText confidence="0.998681857142857">
2-21 of Wall Street Journal (WSJ) data for training
and Section 23 as the test set. We use all features in
Table 1 and build four models, each of which pre-
dicting one type of function tags. The results are
tabulated in Table 3.
As can be seen, our system performs much better
than both (Blaheta and Charniak, 2000) and (Lin-
tean and Rus, 2007a). For two major categories,
namely grammatical and form/function which ac-
count for 96.84% non-null function tags in the test
set, our system achieves a relative error reduction of
77.1% (from (Blaheta and Charniak, 2000)’s 1.09%
to 0.25%) and 46.9%(from (Blaheta and Charniak,
2000)’s 2.90% to 1.54%) , respectively. The per-
formance improvements result from a clean learn-
ing framework and some new features we intro-
duced: e.g., the node-external features, i.e., Feature
13 and 14 in Table 1, can capture long-range statis-
tical dependencies in the conditional model (2) and
are proved very useful (cf. Section 4.1.2). As far as
we can tell, they are not used in previous work.
</bodyText>
<table confidence="0.9997788">
Type Blaheta00 Lintean07 Ours
Grammar 98.91% 98.45% 99.75%
Form/Func 97.10% 95.15% 98.46%
topic 99.92% 99.87% 99.98%
Misc 98.65% 98.54% 99.41%
</table>
<tableCaption confidence="0.6516438">
Table 3: Function tag prediction accuracies on gold parse
trees: breakdown by types of function tags. The 2nd col-
umn is due to (Blaheta and Charniak, 2000) and 3rd col-
umn due to (Lintean and Rus, 2007a). Our results on the
4th column compare favorably with theirs.
</tableCaption>
<sectionHeader confidence="0.506873" genericHeader="method">
4.1.2 Relative Contributions of Features
</sectionHeader>
<bodyText confidence="0.967912666666667">
Since the English WSJ data set contains newswire
text, the most recent OntoNotes (Hovy et al., 2006)
contains text from a more diversified genres such
as broadcast news and broadcast conversation, we
decide to test our system on this data set as well.
WSJ Section 24 is used for development and Sec-
tion 23 for test, and the rest is used as the training
data. Note that some WSJ files were not included in
the OntoNotes release and Section 23 in OntoNotes
contains only 1640 sentences. The OntoNotes data
statistics is tabulated in Table 4. Less than 2% of
nodes with non-empty function tags were assigned
multiple function tags. To simplify the system build-
ing, we take the first tag in training and testing and
report the aggregated accuracy only in this section.
</bodyText>
<table confidence="0.975695666666667">
#-sents #-nodes #-funcNodes
training 71,186 1,242,747 280,755
test 1,640 31,117 6,778
</table>
<tableCaption confidence="0.94891975">
Table 4: Statistics of OntoNotes: #-sents – number
of sentences; #-nodes – number of non-terminal nodes;
#-funcNodes – number of nodes containing non-empty
function tags.
</tableCaption>
<bodyText confidence="0.999959433333333">
We use this data set to test relative contributions
of different feature groups by incrementally adding
features into the system, and the results are reported
in Table 5. The dummy baseline is predicting the
most likely prior – the empty function tag, which
indicates that there are 78.21% of nodes without a
function tag. The next line reflects the performance
of a system with non-lexical features only (Feature
1 to 8 in Table 1), and the result is fairly poor with
an accuracy 91.51%. The past predictions (Feature
8 and 9) helps a bit by improving the accuracy to
92.04%. Node internal lexical features (Feature 11
and 12) are extremely useful: it added more than 3
points to the accuracy. So does the node external lex-
ical features (Feature 13 and 14) which added an ad-
ditional 1.52 points. Features computed from head
words (Feature 15 to 19) carry information comple-
mentary to the lexical features and it helps quite a
bit by improving the accuracy by 0.64%. When all
features are used, the system reached an accuracy of
97.34%.
From these results, we can conclude that, unlike
syntactic parsing (Bikel, 2004), lexical information
is extremely important for predicting and recover-
ing function tags. This is not surprising since many
function tags carry semantic information, and more
often than not, the ambiguity can only be resolved
by lexical information. E.g., whether a PP is locative
or temporal PP is heavily influenced by the lexical
choice of the NP argument.
</bodyText>
<subsectionHeader confidence="0.99151">
4.2 Predicting Chinese Empty Elements
</subsectionHeader>
<bodyText confidence="0.999819166666667">
As is well known, Chinese is a pro-drop language.
This and its lack of subordinate conjunction com-
plementizers lead to the ubiquitous use of empty el-
ements in the Chinese treebank (Xue et al., 2005).
Predicting or recovering these empty elements is
therefore important for the Chinese language pro-
</bodyText>
<page confidence="0.752013">
1233
</page>
<table confidence="0.999888">
Feature Set Accuracy
prior (guess NONE) 78.21%
Non-lexical labels only 91.52%
+past prediction 92.04%
+node-internal lexical 95.17%
+node-external lexical 96.70%
+head word 97.34%
</table>
<tableCaption confidence="0.964792">
Table 5: Effects of feature sets: the second row contains
the baseline result when always predicting NONE; Row 3
through 8 contain results by incrementally adding feature
sets.
</tableCaption>
<bodyText confidence="0.998821861111111">
cessing. Recently, Chung and Gildea (2010) has
found it useful to recover empty elements in ma-
chine translation.
Since empty elements do not have any surface
string representation, we tackle the problem by at-
taching a pseudo function tag to an empty element’s
lowest non-empty parent and then removing the sub-
tree spanning it. Figure 2 contains an example
tree before and after removing the empty element
*pro* and annotating the non-empty parent with
a pseudo function tag NoneL. The transformation
procedure is summarized in Algorithm 1.
In particular, line 2 of Algorithm 1 find the lowest
parent of an empty element that spans at least one
non-trace word. In the example in Figure 2, it would
find the top IP-node. Since *pro* is the left-most
child, line 4 of Algorithm 1 adds the pseudo function
tag NoneL to the top IP-node. Line 9 then removes
its NP child node and all lower children (i.e., shaded
subtree in Figure 2(1)), resulting in the tree in Fig-
ure 2(2).
Line 4 to 8 of Algorithm 1 indicate that there are
3 types of pseudo function tags: NoneL, NoneM,
and NoneR, encoding a trace found in the left, mid-
dle or right position of its lowest non-empty parent.
It’s trivial to recover a trace’s position in a sentence
from NoneL, and NoneR, but it may be ambiguous
for NoneM. The problem could be solved either us-
ing heuristics to determine the position of a middle
empty element, or encoding the positional informa-
tion in the pseudo function tag. Since here we just
want to show that predicting empty elements can be
cast as a tree annotation problem, we leave this op-
tion to future research.
With this transform, the problem of predicting
a trace is cast into predicting the corresponding
</bodyText>
<figure confidence="0.970754818181818">
IP
VP
IP
NP
NP
NP
NN VV NN NN
*pro* ran2hou4 you3 zhuan3men2 dui4wu3 jin4xing2 jian1du1 jian3cha2
(1) Original tree with a trace (the left−most child of the top IP−node)
ran2hou4 you3 zhuan3men2 dui4wu3 jin4xing2 jian1du1 jian3cha2
(2) After removing trace and its parent node (shaded subtree in (1))
</figure>
<figureCaption confidence="0.999643">
Figure 2: Transform of traces in a Chinese parse tree by
adding pseudo function tags.
</figureCaption>
<bodyText confidence="0.892366285714286">
Algorithm 1 Procedure to remove empty elements
and add pseudo function tags.
Input: An input tree
Output: a tree after removing traces (and their
empty parents) and adding pseudo function tags to
its lowest non-empty parent node
1:Foreach trace t
</bodyText>
<listItem confidence="0.9387308">
2: Find its lowest ancestor node p spanning at least
one non-trace word
3: if t is p’s left-most child
4: add pseudo tag NoneL to p
5: else if t is p’s right-most child
6: add pseudo tag NoneR to p
7: else
8: add pseudo tag NoneM to p
9: Remove p’s child spanning the trace t and all its
children
</listItem>
<figure confidence="0.99904475">
IP−NoneL
AD VE JJ NN VV
VP
NP
IP
VP
NP
NN NN
NONE AD
VE
JJ
VP
</figure>
<page confidence="0.992532">
1234
</page>
<bodyText confidence="0.9986045">
pseudo function tag and the statistical tree annota-
tor can thus be used to solve this problem.
</bodyText>
<sectionHeader confidence="0.827696" genericHeader="evaluation">
4.2.1 Results
</sectionHeader>
<bodyText confidence="0.973957">
We use Chinese Treebank v6.0 (Xue et al., 2005)
and the broadcast conversation data from CTB
v7.0 2. The data set is partitioned into training, de-
velopment and blind test as shown in Table 6. The
partition is created so that different genres are well
represented in different subsets. The training, de-
velopment and test set have 32925, 3297 and 3033
sentences, respectively.
</bodyText>
<table confidence="0.999476">
Subset File IDs
Training 0001-0325, 0400-0454, 0600-0840
0500-0542, 2000-3000, 0590-0596
1001-1120, cctv,cnn,msnbc, phoenix 00-06
Dev 0841-0885, 0543-0548, 3001-3075
1121-1135, phoenix 07-09
Test 0900-0931,0549-0554, 3076-3145
1136-1151, phoenix 10-11
</table>
<tableCaption confidence="0.9958035">
Table 6: Data partition for CTB6 and CTB 7’s broadcast
conversation portion
</tableCaption>
<bodyText confidence="0.999836772727273">
We then apply Algorithm 1 to transform trees and
predict pseudo function tags. Out of 1,100,506 non-
terminal nodes in the training data, 80,212 of them
contain pseudo function tags. There are 94 nodes
containing 2 pseudo function tags. The vast major-
ity of pseudo tags – more then 99.7% – are attached
to either IP, CP, or VP: 50971, 20113, 8900, respec-
tively.
We used all features in Table 1 and achieved an
accuracy of 99.70% on the development data, and
99.71% on the test data on gold trees.
To understand why the accuracies are so high, we
look into the 5 most frequent labels carrying pseudo
tags in the development set, and tabulate their per-
formance in Table 7. The 2nd column contains the
number of nodes in the reference; the 3rd column the
number of nodes of system output; the 4th column
the number of nodes with correct prediction; and the
5th column F-measure for each label.
From Table 7, it is clear that CP-NoneL and
IP-NoneL are easy to predict. This is not sur-
prising, given that the Chinese language lacks of
</bodyText>
<footnote confidence="0.892493">
2Many files are missing in LDC’s early 2010 release of CTB
7.0, but broadcast conversation portion is new and is used in our
system.
</footnote>
<table confidence="0.999901">
Label numRef numSys numCorr F1
CP-NoneL 1723 1724 1715 0.995
IP-NoneL 3874 3875 3844 0.992
VP-NoneR 660 633 597 0.923
IP-NoneM 440 432 408 0.936
VP-NoneL 135 107 105 0.868
</table>
<tableCaption confidence="0.995891">
Table 7: 5 most frequent labels carrying pseudo tags and
their performances
</tableCaption>
<bodyText confidence="0.999019842105263">
complementizers for subordinate clauses. In other
words, left-most empty elements under CP are al-
most unambiguous: if a CP node has an immediate
IP child, it almost always has a left-most empty el-
ement; similarly, if an IP node has a VP node as
the left-most child (i.e., without a subject), it almost
always should have a left empty element (e.g., mark-
ing the dropped pro). Another way to interpret these
results is as follows: when developing the Chinese
treebank, there is really no point to annotate left-
most traces for CP and IP when tree structures are
available.
On the other hand, predicting the left-most empty
elements for VP is a lot harder: the F-measure is
only 86.8% for VP-NoneL. Predicting the right-
most empty elements under VP and middle empty
elements under IP is somewhat easier: VP-NoneR
and IP-NoneM’sF-measures are 92.3% and 93.6%,
respectively.
</bodyText>
<subsectionHeader confidence="0.999702">
4.3 Predicting Projectable Constituents
</subsectionHeader>
<bodyText confidence="0.999957722222222">
The third application is predicting projectable con-
stituents for machine translation. State-of-the-art
machine translation systems (Yamada and Knight,
2001; Xiong et al., 2010; Shen et al., 2008; Chi-
ang, 2010; Shen et al., 2010) rely heavily on syn-
tactic analysis. Projectable structures are impor-
tant in that it is assumed in CFG-style translation
rules that a source span can be translated contigu-
ously. Clearly, not all source constituents can be
translated this way, but if we can predict whether
a non-terminal source node is projectable, we can
avoid translation errors by bypassing or discourag-
ing the derivation paths relying on non-projectable
constituents, or using phrase-based approaches for
non-projectable constituents.
We start from LDC’s bilingual Arabic-English
treebank with source human parse trees and align-
ments, and mark source constituents as either pro-
</bodyText>
<page confidence="0.961753">
1235
</page>
<figure confidence="0.997294285714286">
PREP
NOUN
PUNC
PP#
NOUN
NP#1
NP#2
ADJ
S
PUNC PREP
NP
PP
DET+NOUN
NP
DET+ADJ
PUNC
b# sbb IT AltzAmAt tAr}p IT l# Alms&amp;wl
AlErAqy .
Because of
&amp;quot;
the Iraqi official ’s sudden obligations . &amp;quot;
</figure>
<figureCaption confidence="0.999822">
Figure 3: An example to show how a source tree is annotated with its alignment with the target sentence.
</figureCaption>
<bodyText confidence="0.98021375862069">
jectable or non-projectable. The binary annotations
can again be treated as pseudo function tags and the
proposed tree annotator can be readily applied to this
problem.
As an example, the top half of Figure 3 con-
tains an Arabic sentence with its parse tree; the bot-
tom is its English translation with the human word-
alignment. There are three non-projectable con-
stituents marked with “#”: the top PP# spanning
the whole sentence except the final stop, and NP#1
and NP#2. The PP# node is not projectable due
to an inserted stop from outside; NP#1 is not pro-
jectable because it is involved in a 2-to-2 alignment
with the token b# outside NP#1; NP#2 is aligned
to a span the Iraqi official ’s sudden
obligations ., in which Iraqi official
breaks the contiguity of the translation. It is clear
that a CFG-like grammar will not be able to gener-
ate the translation for NP#2.
The LDC’s Arabic-English bilingual treebank
does not mark if a source node is projectable or
not, but the information can be computed from word
alignment. In our experiments, we processed 16,125
sentence pairs with human source trees for training,
and 1,151 sentence pairs for testing. The statistics
of the training and test data can be found in Table 8,
where the number of sentences, the number of non-
terminal nodes and the number of non-projectable
nodes are listed in Column 2 through 4, respectively.
</bodyText>
<table confidence="0.999258666666667">
Data Set #Sents #nodes #NonProj
Training 16,125 558,365 121,201
Test 1,151 40,674 8,671
</table>
<tableCaption confidence="0.9804255">
Table 8: Statistics of the data for predicting projectable
constituents
</tableCaption>
<bodyText confidence="0.9999636">
We get a 94.6% accuracy for predicting pro-
jectable constituents on the gold trees, and an 84.7%
F-measure on the machine-generated parse trees.
This component has been integrated into our ma-
chine translation system (Zhao et al., 2011).
</bodyText>
<sectionHeader confidence="0.999971" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.998747363636364">
Blaheta and Charniak (2000) used a feature tree
model to predict function tags. The work was
later extended to use the voted perceptron (Blaheta,
2003). There are considerable overlap in terms of
features used in (Blaheta and Charniak, 2000; Bla-
heta, 2003) and our system: for example, the label of
current node, parent node and sibling nodes. How-
ever, there are some features that are unique in our
work, e.g., lexical features at a constituent bound-
aries (node-internal and node-external words). Table
2 of (Blaheta and Charniak, 2000) contains the ac-
</bodyText>
<page confidence="0.757658">
1236
</page>
<bodyText confidence="0.997723367346939">
curacies for 4 types of function tags, and our results 6 Conclusions and Future Work
in Table 3 compare favorably with those in (Blaheta We proposed a generic statistical tree annotator in
and Charniak, 2000). Lintean and Rus (2007a; Lin- the paper. We have shown that a variety of natural
tean and Rus (2007b) also studied the function tag- language problems can be tackled with the proposed
ging problem and applied naive Bayes and decision tree annotator, from predicting function tags, pre-
tree to it. Their accuracy results are worse than dicting empty categories, to predicting projectable
(Blaheta and Charniak, 2000). Neither (Blaheta and syntactic constituents for machine translation. Our
Charniak, 2000) nor (Lintean and Rus, 2007a; Lin- results of predicting function tags compare favor-
tean and Rus, 2007b) reported the relative usefulness ably with published results on the same data set, pos-
of different features, while we found that the lexical sibly due to new features employed in the system.
features are extremely useful. We showed that empty categories can be represented
Campbell (2004) and Schmid (2006) studied the as pseudo function tags, and thus predicting empty
problem of predicting and recovering empty cate- categories can be solved with the proposed tree an-
gories, but they used very different approaches: in notator. The same technique can be used to predict
(Campbell, 2004), a rule-based approach is used projectable syntactic constituents for machine trans-
while (Schmid, 2006) used a non-lexical PCFG sim- lation.
ilar to (Klein and Manning, 2003). Chung and There are several directions to expand the work
Gildea (2010) studied the effects of empty cate- described in this paper. First, the results for predict-
gories on machine translation and they found that ing function tags and Chinese empty elements were
even with noisy machine predictions, empty cate- obtained on human-annotated trees and it would be
gories still helped machine translation. In this paper, interesting to do it on parse trees generated by sys-
we showed that empty categories can be encoded as tem. Second, predicting projectable constituents is
pseudo function tags and thus predicting and recov- for improving machine translation and we are inte-
ering empty categories can be cast as a tree anno- grating the component into a syntax-based machine
tating problem. Our results also shed light on some translation system.
empty categories can almost be determined unam- Acknowledgments
biguously, given a gold tree structure, which sug- This work was partially supported by the Defense
gests that these empty elements do not need to be Advanced Research Projects Agency under contract
annotated. No. HR0011-08-C-0110. The views and findings
Gabbard et al. (2006) modified Collins’ parser to contained in this material are those of the authors
output function tags. Since their results for predict- and do not necessarily reflect the position or policy
ing function tags are on system parses, they are not of the U.S. government and no official endorsement
comparable with ours. (Gabbard et al., 2006) also should be inferred.
contains a second stage employing multiple clas- We are also grateful to three anonymous reviewers
sifiers to recover empty categories and resolve co- for their suggestions and comments for improving
indexations between an empty element and its an- the paper.
tecedent. References
As for predicting projectable constituent, it is re- Adam L. Berger, Stephen A. Della Pietra, and Vincent
lated to the work described in (Xiong et al., 2010), J. Della Pietra. 1996. A maximum entropy approach
where they were predicting translation boundaries. to natural language processing. Computational Lin-
A major difference is that (Xiong et al., 2010) de- guistics, 22(1):39–71, March.
fines projectable spans on a left-branching deriva- Ann Bies, Mark Ferguson, and karen Katz. 1995. Brack-
tion tree solely for their phrase decoder and models, eting guidelines for treebank II-style penn treebank
while translation boundaries in our work are defined project. Technical report, Linguistic Data Consortium.
from source parse trees. Our work uses more re- Daniel M. Bikel. 2004. A distributional analysis of a
sources, but the prediction accuracy is higher (mod- lexicalized statistical parsing model. In Dekang Lin
ulated on a different test data): we get a F-measure
84.7%, in contrast with (Xiong et al., 2010)’s 71%.
1237
</bodyText>
<reference confidence="0.994896205607477">
and Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 182–189, Barcelona, Spain, July. Association
for Computational Linguistics.
Don Blaheta and Eugene Charniak. 2000. Assigning
function tags to parsed text. In Proceedings of the 1st
Meeting of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 234–240.
Don Blaheta. 2003. Function Tagging. Ph.D. thesis,
Brown University.
Richard Campbell. 2004. Using linguistic principles
to recover empty categories. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics (ACL’04), Main Volume, pages 645–652,
Barcelona, Spain, July.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for
efficient, feature-rich parsing. In Proceedings of
CoNLL.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings ofNAACL, Seattle.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proc. ACL, pages 1443–1452.
Tagyoung Chung and Daniel Gildea. 2010. Effects of
empty categories on machine translation. In Proceed-
ings of the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 636–645, Cam-
bridge, MA, October. Association for Computational
Linguistics.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proc. Annual Meet-
ing ofACL, pages 16–23.
Peter Dienes, P Eter Dienes, and Amit Dubey. 2003. An-
tecedent recovery: Experiments with a trace tagger. In
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 33–40.
Ryan Gabbard, Mitchell Marcus, and Seth Kulick. 2006.
Fully parsing the Penn Treebank. In Proceedings of
Human Language Technology Conference of the North
Amer- ican Chapter of the Association of Computa-
tional Linguistics.
Joshua Goodman. 2002. Sequential conditional general-
ized iterative scaling. In Pro. of the 40th ACL.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Com-
panion Volume: Short Papers, pages 57–60, New York
City, USA, June. Association for Computational Lin-
guistics.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Erhard Hinrichs and Dan
Roth, editors, Proceedings of the 41st Annual Meet-
ing of the Association for Computational Linguistics,
pages 423–430.
Mihai Lintean and V. Rus. 2007a. Large scale exper-
iments with function tagging. In Proceedings of the
International Conference on Knowledge Engineering,
pages 1–7.
Mihai Lintean and V. Rus. 2007b. Naive Bayes and deci-
sion trees for function tagging. In Proceedings of the
International Conference of the FLAIRS-2007.
David M. Magerman. 1994. Natural Language Parsing
As Statistical Pattern Recognition. Ph.D. thesis, Stan-
ford University.
Robert Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In the Sixth
Conference on Natural Language Learning (CoNLL-
2002), pages 49–55.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
treebank. Computational Linguistics, 19(2):313–330.
Adwait Ratnaparkhi. 1997. A Linear Observed Time
Statistical Parser Based on Maximum Entropy Mod-
els. In Second Conference on Empirical Methods in
Natural Language Processing, pages 1 – 10.
Helmut Schmid. 2006. Trace prediction and recov-
ery with unlexicalized PCFGs and slash features. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 177–184, Sydney, Australia, July. Association
for Computational Linguistics.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings ofACL.
Libin Shen, Bing Zhang, Spyros Matsoukas, Jinxi Xu,
and Ralph Weischedel. 2010. Statistical machine
translation with a factorized grammar. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 616–625, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Learn-
ing translation boundaries for phrase-based decoding.
In NAACL-HLT 2010.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207–238.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proc. Annual Meeting
of the Association for Computational Linguistics.
Bing Zhao, , Young-Suk Lee, Xiaoqiang Luo, and Liu
Li. 2011. Learning to transform and select elementary
trees for improved syntax-based machine translations.
In Proc. ofACL.
</reference>
<page confidence="0.992398">
1238
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.563166">
<title confidence="0.999555">A Statistical Tree Annotator and Its Applications</title>
<author confidence="0.985165">Xiaoqiang Luo</author>
<author confidence="0.985165">Bing</author>
<affiliation confidence="0.994093">IBM T.J. Watson Research</affiliation>
<address confidence="0.792402">1101 Kitchawan Yorktown Heights, NY</address>
<abstract confidence="0.991907384615385">In many natural language applications, there is a need to enrich syntactical parse trees. We present a statistical tree annotator augmenting nodes with additional information. The annotator is generic and can be applied to a variety of applications. We report 3 such applications in this paper: predicting function tags; predicting null elements; and predicting whether a tree constituent is projectable in machine translation. Our function tag prediction system outperforms significantly published results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<date></date>
<booktitle>Proceedings of EMNLP 2004,</booktitle>
<pages>182--189</pages>
<editor>and Dekai Wu, editors,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<marker></marker>
<rawString>and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 182–189, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Don Blaheta</author>
<author>Eugene Charniak</author>
</authors>
<title>Assigning function tags to parsed text.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>234--240</pages>
<contexts>
<context position="2690" citStr="Blaheta and Charniak, 2000" startWordPosition="418" endWordPosition="421">ion tags after syntactic parsing. For the problem of predicting projectable syntactic constituent, one can use a sentence alignment tool and syntactic trees on source sentences to create training data by annotating a tree node as projectable or not. A generic tree annotator can also open the door of solving other natural language problems so long as the problem can be cast as annotating tree nodes. As one such example, we will present how to predict empty elements for the Chinese language. Some of the above-mentioned problems have been studied before: predicting function tags were studied in (Blaheta and Charniak, 2000; Blaheta, 2003; Lintean and Rus, 2007a), and results of predicting and recovering empty elements can be found in (Dienes et al., 2003; Schmid, 2006; Campbell, 2004). In this work, we will show that these seemingly unrelated problems can be treated uniformly as adding annotations to an existing tree structure, which is the first goal of this work. Second, the proposed generic tree annotator can also be used to solve new problems: we will show how it can be used to predict projectable syntactic constituents. Third, the uniform treatment not only simplifies the model building process, but also a</context>
<context position="11026" citStr="Blaheta and Charniak, 2000" startWordPosition="1897" endWordPosition="1900">” row contains function tags marking the grammatical role of a constituent, e.g., DTV for dative objects, LGS for logical subjects etc. Many tags in the “Form/function” row carry semantic information, e.g., LOC is for locative expressions, and TMP for temporal expressions. Type Function Tags Grammatical (52.2%) DTV LGS PRD PUT SBJ VOC Form/function (36.2%) ADV BNF DIR EXT LOC MNR NOM PRP TMP Topicalization (2.2%) TPC Miscellaneous (9.4%) CLF CLR HLN TTL Table 2: Four types of function tags and their relative frequency 4.1.1 Comparison with Prior Arts In order to have a direct comparison with (Blaheta and Charniak, 2000; Lintean and Rus, 2007a), we use the same English Penn Treebank (Marcus et al., 1993) and partition the data set identically: Section 1232 2-21 of Wall Street Journal (WSJ) data for training and Section 23 as the test set. We use all features in Table 1 and build four models, each of which predicting one type of function tags. The results are tabulated in Table 3. As can be seen, our system performs much better than both (Blaheta and Charniak, 2000) and (Lintean and Rus, 2007a). For two major categories, namely grammatical and form/function which account for 96.84% non-null function tags in t</context>
<context position="12483" citStr="Blaheta and Charniak, 2000" startWordPosition="2142" endWordPosition="2145">esult from a clean learning framework and some new features we introduced: e.g., the node-external features, i.e., Feature 13 and 14 in Table 1, can capture long-range statistical dependencies in the conditional model (2) and are proved very useful (cf. Section 4.1.2). As far as we can tell, they are not used in previous work. Type Blaheta00 Lintean07 Ours Grammar 98.91% 98.45% 99.75% Form/Func 97.10% 95.15% 98.46% topic 99.92% 99.87% 99.98% Misc 98.65% 98.54% 99.41% Table 3: Function tag prediction accuracies on gold parse trees: breakdown by types of function tags. The 2nd column is due to (Blaheta and Charniak, 2000) and 3rd column due to (Lintean and Rus, 2007a). Our results on the 4th column compare favorably with theirs. 4.1.2 Relative Contributions of Features Since the English WSJ data set contains newswire text, the most recent OntoNotes (Hovy et al., 2006) contains text from a more diversified genres such as broadcast news and broadcast conversation, we decide to test our system on this data set as well. WSJ Section 24 is used for development and Section 23 for test, and the rest is used as the training data. Note that some WSJ files were not included in the OntoNotes release and Section 23 in Onto</context>
<context position="24646" citStr="Blaheta and Charniak (2000)" startWordPosition="4193" endWordPosition="4196">nd test data can be found in Table 8, where the number of sentences, the number of nonterminal nodes and the number of non-projectable nodes are listed in Column 2 through 4, respectively. Data Set #Sents #nodes #NonProj Training 16,125 558,365 121,201 Test 1,151 40,674 8,671 Table 8: Statistics of the data for predicting projectable constituents We get a 94.6% accuracy for predicting projectable constituents on the gold trees, and an 84.7% F-measure on the machine-generated parse trees. This component has been integrated into our machine translation system (Zhao et al., 2011). 5 Related Work Blaheta and Charniak (2000) used a feature tree model to predict function tags. The work was later extended to use the voted perceptron (Blaheta, 2003). There are considerable overlap in terms of features used in (Blaheta and Charniak, 2000; Blaheta, 2003) and our system: for example, the label of current node, parent node and sibling nodes. However, there are some features that are unique in our work, e.g., lexical features at a constituent boundaries (node-internal and node-external words). Table 2 of (Blaheta and Charniak, 2000) contains the ac1236 curacies for 4 types of function tags, and our results 6 Conclusions </context>
</contexts>
<marker>Blaheta, Charniak, 2000</marker>
<rawString>Don Blaheta and Eugene Charniak. 2000. Assigning function tags to parsed text. In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics, pages 234–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Don Blaheta</author>
</authors>
<title>Function Tagging.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>Brown University.</institution>
<contexts>
<context position="2705" citStr="Blaheta, 2003" startWordPosition="422" endWordPosition="423">sing. For the problem of predicting projectable syntactic constituent, one can use a sentence alignment tool and syntactic trees on source sentences to create training data by annotating a tree node as projectable or not. A generic tree annotator can also open the door of solving other natural language problems so long as the problem can be cast as annotating tree nodes. As one such example, we will present how to predict empty elements for the Chinese language. Some of the above-mentioned problems have been studied before: predicting function tags were studied in (Blaheta and Charniak, 2000; Blaheta, 2003; Lintean and Rus, 2007a), and results of predicting and recovering empty elements can be found in (Dienes et al., 2003; Schmid, 2006; Campbell, 2004). In this work, we will show that these seemingly unrelated problems can be treated uniformly as adding annotations to an existing tree structure, which is the first goal of this work. Second, the proposed generic tree annotator can also be used to solve new problems: we will show how it can be used to predict projectable syntactic constituents. Third, the uniform treatment not only simplifies the model building process, but also affords us to co</context>
<context position="24770" citStr="Blaheta, 2003" startWordPosition="4216" endWordPosition="4217">des are listed in Column 2 through 4, respectively. Data Set #Sents #nodes #NonProj Training 16,125 558,365 121,201 Test 1,151 40,674 8,671 Table 8: Statistics of the data for predicting projectable constituents We get a 94.6% accuracy for predicting projectable constituents on the gold trees, and an 84.7% F-measure on the machine-generated parse trees. This component has been integrated into our machine translation system (Zhao et al., 2011). 5 Related Work Blaheta and Charniak (2000) used a feature tree model to predict function tags. The work was later extended to use the voted perceptron (Blaheta, 2003). There are considerable overlap in terms of features used in (Blaheta and Charniak, 2000; Blaheta, 2003) and our system: for example, the label of current node, parent node and sibling nodes. However, there are some features that are unique in our work, e.g., lexical features at a constituent boundaries (node-internal and node-external words). Table 2 of (Blaheta and Charniak, 2000) contains the ac1236 curacies for 4 types of function tags, and our results 6 Conclusions and Future Work in Table 3 compare favorably with those in (Blaheta We proposed a generic statistical tree annotator in and </context>
</contexts>
<marker>Blaheta, 2003</marker>
<rawString>Don Blaheta. 2003. Function Tagging. Ph.D. thesis, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Campbell</author>
</authors>
<title>Using linguistic principles to recover empty categories.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>645--652</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="2855" citStr="Campbell, 2004" startWordPosition="447" endWordPosition="448"> to create training data by annotating a tree node as projectable or not. A generic tree annotator can also open the door of solving other natural language problems so long as the problem can be cast as annotating tree nodes. As one such example, we will present how to predict empty elements for the Chinese language. Some of the above-mentioned problems have been studied before: predicting function tags were studied in (Blaheta and Charniak, 2000; Blaheta, 2003; Lintean and Rus, 2007a), and results of predicting and recovering empty elements can be found in (Dienes et al., 2003; Schmid, 2006; Campbell, 2004). In this work, we will show that these seemingly unrelated problems can be treated uniformly as adding annotations to an existing tree structure, which is the first goal of this work. Second, the proposed generic tree annotator can also be used to solve new problems: we will show how it can be used to predict projectable syntactic constituents. Third, the uniform treatment not only simplifies the model building process, but also affords us to concentrate on discovering most useful features for a particular application which often leads to improved performances, e.g, we find some features are </context>
<context position="26283" citStr="Campbell (2004)" startWordPosition="4458" endWordPosition="4459">retree to it. Their accuracy results are worse than dicting empty categories, to predicting projectable (Blaheta and Charniak, 2000). Neither (Blaheta and syntactic constituents for machine translation. Our Charniak, 2000) nor (Lintean and Rus, 2007a; Lin- results of predicting function tags compare favortean and Rus, 2007b) reported the relative usefulness ably with published results on the same data set, posof different features, while we found that the lexical sibly due to new features employed in the system. features are extremely useful. We showed that empty categories can be represented Campbell (2004) and Schmid (2006) studied the as pseudo function tags, and thus predicting empty problem of predicting and recovering empty cate- categories can be solved with the proposed tree angories, but they used very different approaches: in notator. The same technique can be used to predict (Campbell, 2004), a rule-based approach is used projectable syntactic constituents for machine transwhile (Schmid, 2006) used a non-lexical PCFG sim- lation. ilar to (Klein and Manning, 2003). Chung and There are several directions to expand the work Gildea (2010) studied the effects of empty cate- described in thi</context>
</contexts>
<marker>Campbell, 2004</marker>
<rawString>Richard Campbell. 2004. Using linguistic principles to recover empty categories. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 645–652, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="899" citStr="Carreras et al., 2008" startWordPosition="128" endWordPosition="131">rees. We present a statistical tree annotator augmenting nodes with additional information. The annotator is generic and can be applied to a variety of applications. We report 3 such applications in this paper: predicting function tags; predicting null elements; and predicting whether a tree constituent is projectable in machine translation. Our function tag prediction system outperforms significantly published results. 1 Introduction Syntactic parsing has made tremendous progress in the past 2 decades (Magerman, 1994; Ratnaparkhi, 1997; Collins, 1997; Charniak, 2000; Klein and Manning, 2003; Carreras et al., 2008), and accurate syntactic parsing is often assumed when developing other natural language applications. On the other hand, there are plenty of language applications where basic syntactic information is insufficient. For instance, in question answering, it is highly desirable to have the semantic information of a syntactic constituent, e.g., a noun-phrase (NP) is a person or an organization; an adverbial phrase is locative or temporal. As syntactic information has been widely used in machine translation systems (Yamada and Knight, 2001; Xiong et al., 2010; Shen et al., 2008; Chiang, 2010; Shen e</context>
</contexts>
<marker>Carreras, Collins, Koo, 2008</marker>
<rawString>Xavier Carreras, Michael Collins, and Terry Koo. 2008. TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings ofNAACL,</booktitle>
<location>Seattle.</location>
<contexts>
<context position="850" citStr="Charniak, 2000" startWordPosition="122" endWordPosition="123">e is a need to enrich syntactical parse trees. We present a statistical tree annotator augmenting nodes with additional information. The annotator is generic and can be applied to a variety of applications. We report 3 such applications in this paper: predicting function tags; predicting null elements; and predicting whether a tree constituent is projectable in machine translation. Our function tag prediction system outperforms significantly published results. 1 Introduction Syntactic parsing has made tremendous progress in the past 2 decades (Magerman, 1994; Ratnaparkhi, 1997; Collins, 1997; Charniak, 2000; Klein and Manning, 2003; Carreras et al., 2008), and accurate syntactic parsing is often assumed when developing other natural language applications. On the other hand, there are plenty of language applications where basic syntactic information is insufficient. For instance, in question answering, it is highly desirable to have the semantic information of a syntactic constituent, e.g., a noun-phrase (NP) is a person or an organization; an adverbial phrase is locative or temporal. As syntactic information has been widely used in machine translation systems (Yamada and Knight, 2001; Xiong et a</context>
<context position="2690" citStr="Charniak, 2000" startWordPosition="420" endWordPosition="421">er syntactic parsing. For the problem of predicting projectable syntactic constituent, one can use a sentence alignment tool and syntactic trees on source sentences to create training data by annotating a tree node as projectable or not. A generic tree annotator can also open the door of solving other natural language problems so long as the problem can be cast as annotating tree nodes. As one such example, we will present how to predict empty elements for the Chinese language. Some of the above-mentioned problems have been studied before: predicting function tags were studied in (Blaheta and Charniak, 2000; Blaheta, 2003; Lintean and Rus, 2007a), and results of predicting and recovering empty elements can be found in (Dienes et al., 2003; Schmid, 2006; Campbell, 2004). In this work, we will show that these seemingly unrelated problems can be treated uniformly as adding annotations to an existing tree structure, which is the first goal of this work. Second, the proposed generic tree annotator can also be used to solve new problems: we will show how it can be used to predict projectable syntactic constituents. Third, the uniform treatment not only simplifies the model building process, but also a</context>
<context position="11026" citStr="Charniak, 2000" startWordPosition="1899" endWordPosition="1900">ns function tags marking the grammatical role of a constituent, e.g., DTV for dative objects, LGS for logical subjects etc. Many tags in the “Form/function” row carry semantic information, e.g., LOC is for locative expressions, and TMP for temporal expressions. Type Function Tags Grammatical (52.2%) DTV LGS PRD PUT SBJ VOC Form/function (36.2%) ADV BNF DIR EXT LOC MNR NOM PRP TMP Topicalization (2.2%) TPC Miscellaneous (9.4%) CLF CLR HLN TTL Table 2: Four types of function tags and their relative frequency 4.1.1 Comparison with Prior Arts In order to have a direct comparison with (Blaheta and Charniak, 2000; Lintean and Rus, 2007a), we use the same English Penn Treebank (Marcus et al., 1993) and partition the data set identically: Section 1232 2-21 of Wall Street Journal (WSJ) data for training and Section 23 as the test set. We use all features in Table 1 and build four models, each of which predicting one type of function tags. The results are tabulated in Table 3. As can be seen, our system performs much better than both (Blaheta and Charniak, 2000) and (Lintean and Rus, 2007a). For two major categories, namely grammatical and form/function which account for 96.84% non-null function tags in t</context>
<context position="12483" citStr="Charniak, 2000" startWordPosition="2144" endWordPosition="2145"> clean learning framework and some new features we introduced: e.g., the node-external features, i.e., Feature 13 and 14 in Table 1, can capture long-range statistical dependencies in the conditional model (2) and are proved very useful (cf. Section 4.1.2). As far as we can tell, they are not used in previous work. Type Blaheta00 Lintean07 Ours Grammar 98.91% 98.45% 99.75% Form/Func 97.10% 95.15% 98.46% topic 99.92% 99.87% 99.98% Misc 98.65% 98.54% 99.41% Table 3: Function tag prediction accuracies on gold parse trees: breakdown by types of function tags. The 2nd column is due to (Blaheta and Charniak, 2000) and 3rd column due to (Lintean and Rus, 2007a). Our results on the 4th column compare favorably with theirs. 4.1.2 Relative Contributions of Features Since the English WSJ data set contains newswire text, the most recent OntoNotes (Hovy et al., 2006) contains text from a more diversified genres such as broadcast news and broadcast conversation, we decide to test our system on this data set as well. WSJ Section 24 is used for development and Section 23 for test, and the rest is used as the training data. Note that some WSJ files were not included in the OntoNotes release and Section 23 in Onto</context>
<context position="24646" citStr="Charniak (2000)" startWordPosition="4195" endWordPosition="4196"> can be found in Table 8, where the number of sentences, the number of nonterminal nodes and the number of non-projectable nodes are listed in Column 2 through 4, respectively. Data Set #Sents #nodes #NonProj Training 16,125 558,365 121,201 Test 1,151 40,674 8,671 Table 8: Statistics of the data for predicting projectable constituents We get a 94.6% accuracy for predicting projectable constituents on the gold trees, and an 84.7% F-measure on the machine-generated parse trees. This component has been integrated into our machine translation system (Zhao et al., 2011). 5 Related Work Blaheta and Charniak (2000) used a feature tree model to predict function tags. The work was later extended to use the voted perceptron (Blaheta, 2003). There are considerable overlap in terms of features used in (Blaheta and Charniak, 2000; Blaheta, 2003) and our system: for example, the label of current node, parent node and sibling nodes. However, there are some features that are unique in our work, e.g., lexical features at a constituent boundaries (node-internal and node-external words). Table 2 of (Blaheta and Charniak, 2000) contains the ac1236 curacies for 4 types of function tags, and our results 6 Conclusions </context>
<context position="25890" citStr="Charniak, 2000" startWordPosition="4395" endWordPosition="4396">mpare favorably with those in (Blaheta We proposed a generic statistical tree annotator in and Charniak, 2000). Lintean and Rus (2007a; Lin- the paper. We have shown that a variety of natural tean and Rus (2007b) also studied the function tag- language problems can be tackled with the proposed ging problem and applied naive Bayes and decision tree annotator, from predicting function tags, pretree to it. Their accuracy results are worse than dicting empty categories, to predicting projectable (Blaheta and Charniak, 2000). Neither (Blaheta and syntactic constituents for machine translation. Our Charniak, 2000) nor (Lintean and Rus, 2007a; Lin- results of predicting function tags compare favortean and Rus, 2007b) reported the relative usefulness ably with published results on the same data set, posof different features, while we found that the lexical sibly due to new features employed in the system. features are extremely useful. We showed that empty categories can be represented Campbell (2004) and Schmid (2006) studied the as pseudo function tags, and thus predicting empty problem of predicting and recovering empty cate- categories can be solved with the proposed tree angories, but they used very</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>E. Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings ofNAACL, Seattle.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Learning to translate with source and target syntax.</title>
<date>2010</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>1443--1452</pages>
<contexts>
<context position="1491" citStr="Chiang, 2010" startWordPosition="222" endWordPosition="223">rreras et al., 2008), and accurate syntactic parsing is often assumed when developing other natural language applications. On the other hand, there are plenty of language applications where basic syntactic information is insufficient. For instance, in question answering, it is highly desirable to have the semantic information of a syntactic constituent, e.g., a noun-phrase (NP) is a person or an organization; an adverbial phrase is locative or temporal. As syntactic information has been widely used in machine translation systems (Yamada and Knight, 2001; Xiong et al., 2010; Shen et al., 2008; Chiang, 2010; Shen et al., 2010), an interesting question is to predict whether or not a syntactic constituent is projectable1 across a language pair. 1A constituent in the source language is projectable if it can be aligned to a contiguous span in the target language. Such problems can be abstracted as adding additional annotations to an existing tree structure. For example, the English Penn treebank (Marcus et al., 1993) contains function tags and many carry semantic information. To add semantic information to the basic syntactic trees, a logical step is to predict these function tags after syntactic pa</context>
<context position="21867" citStr="Chiang, 2010" startWordPosition="3730" endWordPosition="3732">leftmost traces for CP and IP when tree structures are available. On the other hand, predicting the left-most empty elements for VP is a lot harder: the F-measure is only 86.8% for VP-NoneL. Predicting the rightmost empty elements under VP and middle empty elements under IP is somewhat easier: VP-NoneR and IP-NoneM’sF-measures are 92.3% and 93.6%, respectively. 4.3 Predicting Projectable Constituents The third application is predicting projectable constituents for machine translation. State-of-the-art machine translation systems (Yamada and Knight, 2001; Xiong et al., 2010; Shen et al., 2008; Chiang, 2010; Shen et al., 2010) rely heavily on syntactic analysis. Projectable structures are important in that it is assumed in CFG-style translation rules that a source span can be translated contiguously. Clearly, not all source constituents can be translated this way, but if we can predict whether a non-terminal source node is projectable, we can avoid translation errors by bypassing or discouraging the derivation paths relying on non-projectable constituents, or using phrase-based approaches for non-projectable constituents. We start from LDC’s bilingual Arabic-English treebank with source human pa</context>
</contexts>
<marker>Chiang, 2010</marker>
<rawString>David Chiang. 2010. Learning to translate with source and target syntax. In Proc. ACL, pages 1443–1452.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tagyoung Chung</author>
<author>Daniel Gildea</author>
</authors>
<title>Effects of empty categories on machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>636--645</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="15872" citStr="Chung and Gildea (2010)" startWordPosition="2697" endWordPosition="2700">nate conjunction complementizers lead to the ubiquitous use of empty elements in the Chinese treebank (Xue et al., 2005). Predicting or recovering these empty elements is therefore important for the Chinese language pro1233 Feature Set Accuracy prior (guess NONE) 78.21% Non-lexical labels only 91.52% +past prediction 92.04% +node-internal lexical 95.17% +node-external lexical 96.70% +head word 97.34% Table 5: Effects of feature sets: the second row contains the baseline result when always predicting NONE; Row 3 through 8 contain results by incrementally adding feature sets. cessing. Recently, Chung and Gildea (2010) has found it useful to recover empty elements in machine translation. Since empty elements do not have any surface string representation, we tackle the problem by attaching a pseudo function tag to an empty element’s lowest non-empty parent and then removing the subtree spanning it. Figure 2 contains an example tree before and after removing the empty element *pro* and annotating the non-empty parent with a pseudo function tag NoneL. The transformation procedure is summarized in Algorithm 1. In particular, line 2 of Algorithm 1 find the lowest parent of an empty element that spans at least on</context>
</contexts>
<marker>Chung, Gildea, 2010</marker>
<rawString>Tagyoung Chung and Daniel Gildea. 2010. Effects of empty categories on machine translation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 636–645, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proc. Annual Meeting ofACL,</booktitle>
<pages>16--23</pages>
<contexts>
<context position="834" citStr="Collins, 1997" startWordPosition="120" endWordPosition="121">lications, there is a need to enrich syntactical parse trees. We present a statistical tree annotator augmenting nodes with additional information. The annotator is generic and can be applied to a variety of applications. We report 3 such applications in this paper: predicting function tags; predicting null elements; and predicting whether a tree constituent is projectable in machine translation. Our function tag prediction system outperforms significantly published results. 1 Introduction Syntactic parsing has made tremendous progress in the past 2 decades (Magerman, 1994; Ratnaparkhi, 1997; Collins, 1997; Charniak, 2000; Klein and Manning, 2003; Carreras et al., 2008), and accurate syntactic parsing is often assumed when developing other natural language applications. On the other hand, there are plenty of language applications where basic syntactic information is insufficient. For instance, in question answering, it is highly desirable to have the semantic information of a syntactic constituent, e.g., a noun-phrase (NP) is a person or an organization; an adverbial phrase is locative or temporal. As syntactic information has been widely used in machine translation systems (Yamada and Knight, </context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proc. Annual Meeting ofACL, pages 16–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Dienes</author>
<author>P Eter Dienes</author>
<author>Amit Dubey</author>
</authors>
<title>Antecedent recovery: Experiments with a trace tagger. In</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="2824" citStr="Dienes et al., 2003" startWordPosition="441" endWordPosition="444">syntactic trees on source sentences to create training data by annotating a tree node as projectable or not. A generic tree annotator can also open the door of solving other natural language problems so long as the problem can be cast as annotating tree nodes. As one such example, we will present how to predict empty elements for the Chinese language. Some of the above-mentioned problems have been studied before: predicting function tags were studied in (Blaheta and Charniak, 2000; Blaheta, 2003; Lintean and Rus, 2007a), and results of predicting and recovering empty elements can be found in (Dienes et al., 2003; Schmid, 2006; Campbell, 2004). In this work, we will show that these seemingly unrelated problems can be treated uniformly as adding annotations to an existing tree structure, which is the first goal of this work. Second, the proposed generic tree annotator can also be used to solve new problems: we will show how it can be used to predict projectable syntactic constituents. Third, the uniform treatment not only simplifies the model building process, but also affords us to concentrate on discovering most useful features for a particular application which often leads to improved performances, </context>
</contexts>
<marker>Dienes, Dienes, Dubey, 2003</marker>
<rawString>Peter Dienes, P Eter Dienes, and Amit Dubey. 2003. Antecedent recovery: Experiments with a trace tagger. In In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Gabbard</author>
<author>Mitchell Marcus</author>
<author>Seth Kulick</author>
</authors>
<title>Fully parsing the Penn Treebank.</title>
<date>2006</date>
<booktitle>In Proceedings of Human Language Technology Conference of the North Amer- ican Chapter of the Association of Computational Linguistics.</booktitle>
<contexts>
<context position="27945" citStr="Gabbard et al. (2006)" startWordPosition="4716" endWordPosition="4719">nts is pseudo function tags and thus predicting and recov- for improving machine translation and we are inteering empty categories can be cast as a tree anno- grating the component into a syntax-based machine tating problem. Our results also shed light on some translation system. empty categories can almost be determined unam- Acknowledgments biguously, given a gold tree structure, which sug- This work was partially supported by the Defense gests that these empty elements do not need to be Advanced Research Projects Agency under contract annotated. No. HR0011-08-C-0110. The views and findings Gabbard et al. (2006) modified Collins’ parser to contained in this material are those of the authors output function tags. Since their results for predict- and do not necessarily reflect the position or policy ing function tags are on system parses, they are not of the U.S. government and no official endorsement comparable with ours. (Gabbard et al., 2006) also should be inferred. contains a second stage employing multiple clas- We are also grateful to three anonymous reviewers sifiers to recover empty categories and resolve co- for their suggestions and comments for improving indexations between an empty element</context>
</contexts>
<marker>Gabbard, Marcus, Kulick, 2006</marker>
<rawString>Ryan Gabbard, Mitchell Marcus, and Seth Kulick. 2006. Fully parsing the Penn Treebank. In Proceedings of Human Language Technology Conference of the North Amer- ican Chapter of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Sequential conditional generalized iterative scaling.</title>
<date>2002</date>
<booktitle>In Pro. of the 40th ACL.</booktitle>
<contexts>
<context position="5662" citStr="Goodman, 2002" startWordPosition="933" endWordPosition="934"> li takes value from a predefined finite set L, for each non-terminal node ni in a sequential fashion: P(l1, ··· , l|T||T) Newsnight returns to duo action tonight Figure 1: A sample tree: the number on the upright corner of each non-terminal node is the visit order. is the normalizing factor to ensure that P(li|l1, · · · ,li−1, T) in Equation (2) is a probability and {9k(li−1 1 ,T, li)} are feature functions. There are efficient training algorithms to find optimal weights relative to a labeled training data set once the feature functions {9k(li−1 1 , T, li)} are selected (Berger et al., 1996; Goodman, 2002; Malouf, 2002). In our work, we use the SCGIS training algorithm (Goodman, 2002), and the features used in our systems are detailed in the next section. Once a model is trained, at testing time it is applied to input tree nodes by the same order. Figure 1 highlights the prediction of the function tag for node 3(i.e., PP-CLR-node in the thickened box) after 2 shaded nodes (NP-SBJ node and NP node) are predicted. Note that by this time the predicted values are available to the system, while unvisited nodes (nodes in dashed boxes in Figure 1) can not provide such information. NP−SBJ NNP 1 VBZ S </context>
</contexts>
<marker>Goodman, 2002</marker>
<rawString>Joshua Goodman. 2002. Sequential conditional generalized iterative scaling. In Pro. of the 40th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>Ontonotes: The 90% solution.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers,</booktitle>
<pages>57--60</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="10144" citStr="Hovy et al., 2006" startWordPosition="1754" endWordPosition="1757">ty of language problems can be treated as or cast into a tree annotating problem. In this section, we present three applications of the statistical tree annotator. The first application is to predict function tags of an input syntactic parse tree; the second one is to predict Chinese empty elements; and the third one is to predict whether a syntactic constituent of a source sentence is projectable, meaning if the constituent will have a contiguous translation on the target language. 4.1 Predicting Function Tags In the English Penn Treebank (Marcus et al., 1993) and more recent OntoNotes data (Hovy et al., 2006), some tree nodes are assigned a function tag, which is of one of the four types: grammatical, form/function, topicalization and miscellaneous. Table 2 contains a list of function tags used in the English Penn Treebank (Bies et al., 1995). The “Grammatical” row contains function tags marking the grammatical role of a constituent, e.g., DTV for dative objects, LGS for logical subjects etc. Many tags in the “Form/function” row carry semantic information, e.g., LOC is for locative expressions, and TMP for temporal expressions. Type Function Tags Grammatical (52.2%) DTV LGS PRD PUT SBJ VOC Form/fu</context>
<context position="12734" citStr="Hovy et al., 2006" startWordPosition="2184" endWordPosition="2187">n 4.1.2). As far as we can tell, they are not used in previous work. Type Blaheta00 Lintean07 Ours Grammar 98.91% 98.45% 99.75% Form/Func 97.10% 95.15% 98.46% topic 99.92% 99.87% 99.98% Misc 98.65% 98.54% 99.41% Table 3: Function tag prediction accuracies on gold parse trees: breakdown by types of function tags. The 2nd column is due to (Blaheta and Charniak, 2000) and 3rd column due to (Lintean and Rus, 2007a). Our results on the 4th column compare favorably with theirs. 4.1.2 Relative Contributions of Features Since the English WSJ data set contains newswire text, the most recent OntoNotes (Hovy et al., 2006) contains text from a more diversified genres such as broadcast news and broadcast conversation, we decide to test our system on this data set as well. WSJ Section 24 is used for development and Section 23 for test, and the rest is used as the training data. Note that some WSJ files were not included in the OntoNotes release and Section 23 in OntoNotes contains only 1640 sentences. The OntoNotes data statistics is tabulated in Table 4. Less than 2% of nodes with non-empty function tags were assigned multiple function tags. To simplify the system building, we take the first tag in training and </context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. Ontonotes: The 90% solution. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, pages 57–60, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<editor>In Erhard Hinrichs and Dan Roth, editors,</editor>
<contexts>
<context position="875" citStr="Klein and Manning, 2003" startWordPosition="124" endWordPosition="127">nrich syntactical parse trees. We present a statistical tree annotator augmenting nodes with additional information. The annotator is generic and can be applied to a variety of applications. We report 3 such applications in this paper: predicting function tags; predicting null elements; and predicting whether a tree constituent is projectable in machine translation. Our function tag prediction system outperforms significantly published results. 1 Introduction Syntactic parsing has made tremendous progress in the past 2 decades (Magerman, 1994; Ratnaparkhi, 1997; Collins, 1997; Charniak, 2000; Klein and Manning, 2003; Carreras et al., 2008), and accurate syntactic parsing is often assumed when developing other natural language applications. On the other hand, there are plenty of language applications where basic syntactic information is insufficient. For instance, in question answering, it is highly desirable to have the semantic information of a syntactic constituent, e.g., a noun-phrase (NP) is a person or an organization; an adverbial phrase is locative or temporal. As syntactic information has been widely used in machine translation systems (Yamada and Knight, 2001; Xiong et al., 2010; Shen et al., 20</context>
<context position="26758" citStr="Klein and Manning, 2003" startWordPosition="4530" endWordPosition="4533">l sibly due to new features employed in the system. features are extremely useful. We showed that empty categories can be represented Campbell (2004) and Schmid (2006) studied the as pseudo function tags, and thus predicting empty problem of predicting and recovering empty cate- categories can be solved with the proposed tree angories, but they used very different approaches: in notator. The same technique can be used to predict (Campbell, 2004), a rule-based approach is used projectable syntactic constituents for machine transwhile (Schmid, 2006) used a non-lexical PCFG sim- lation. ilar to (Klein and Manning, 2003). Chung and There are several directions to expand the work Gildea (2010) studied the effects of empty cate- described in this paper. First, the results for predictgories on machine translation and they found that ing function tags and Chinese empty elements were even with noisy machine predictions, empty cate- obtained on human-annotated trees and it would be gories still helped machine translation. In this paper, interesting to do it on parse trees generated by syswe showed that empty categories can be encoded as tem. Second, predicting projectable constituents is pseudo function tags and th</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Erhard Hinrichs and Dan Roth, editors, Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Lintean</author>
<author>V Rus</author>
</authors>
<title>Large scale experiments with function tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Knowledge Engineering,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="2728" citStr="Lintean and Rus, 2007" startWordPosition="424" endWordPosition="427">roblem of predicting projectable syntactic constituent, one can use a sentence alignment tool and syntactic trees on source sentences to create training data by annotating a tree node as projectable or not. A generic tree annotator can also open the door of solving other natural language problems so long as the problem can be cast as annotating tree nodes. As one such example, we will present how to predict empty elements for the Chinese language. Some of the above-mentioned problems have been studied before: predicting function tags were studied in (Blaheta and Charniak, 2000; Blaheta, 2003; Lintean and Rus, 2007a), and results of predicting and recovering empty elements can be found in (Dienes et al., 2003; Schmid, 2006; Campbell, 2004). In this work, we will show that these seemingly unrelated problems can be treated uniformly as adding annotations to an existing tree structure, which is the first goal of this work. Second, the proposed generic tree annotator can also be used to solve new problems: we will show how it can be used to predict projectable syntactic constituents. Third, the uniform treatment not only simplifies the model building process, but also affords us to concentrate on discoverin</context>
<context position="11049" citStr="Lintean and Rus, 2007" startWordPosition="1901" endWordPosition="1904"> marking the grammatical role of a constituent, e.g., DTV for dative objects, LGS for logical subjects etc. Many tags in the “Form/function” row carry semantic information, e.g., LOC is for locative expressions, and TMP for temporal expressions. Type Function Tags Grammatical (52.2%) DTV LGS PRD PUT SBJ VOC Form/function (36.2%) ADV BNF DIR EXT LOC MNR NOM PRP TMP Topicalization (2.2%) TPC Miscellaneous (9.4%) CLF CLR HLN TTL Table 2: Four types of function tags and their relative frequency 4.1.1 Comparison with Prior Arts In order to have a direct comparison with (Blaheta and Charniak, 2000; Lintean and Rus, 2007a), we use the same English Penn Treebank (Marcus et al., 1993) and partition the data set identically: Section 1232 2-21 of Wall Street Journal (WSJ) data for training and Section 23 as the test set. We use all features in Table 1 and build four models, each of which predicting one type of function tags. The results are tabulated in Table 3. As can be seen, our system performs much better than both (Blaheta and Charniak, 2000) and (Lintean and Rus, 2007a). For two major categories, namely grammatical and form/function which account for 96.84% non-null function tags in the test set, our system</context>
<context position="12528" citStr="Lintean and Rus, 2007" startWordPosition="2152" endWordPosition="2155">features we introduced: e.g., the node-external features, i.e., Feature 13 and 14 in Table 1, can capture long-range statistical dependencies in the conditional model (2) and are proved very useful (cf. Section 4.1.2). As far as we can tell, they are not used in previous work. Type Blaheta00 Lintean07 Ours Grammar 98.91% 98.45% 99.75% Form/Func 97.10% 95.15% 98.46% topic 99.92% 99.87% 99.98% Misc 98.65% 98.54% 99.41% Table 3: Function tag prediction accuracies on gold parse trees: breakdown by types of function tags. The 2nd column is due to (Blaheta and Charniak, 2000) and 3rd column due to (Lintean and Rus, 2007a). Our results on the 4th column compare favorably with theirs. 4.1.2 Relative Contributions of Features Since the English WSJ data set contains newswire text, the most recent OntoNotes (Hovy et al., 2006) contains text from a more diversified genres such as broadcast news and broadcast conversation, we decide to test our system on this data set as well. WSJ Section 24 is used for development and Section 23 for test, and the rest is used as the training data. Note that some WSJ files were not included in the OntoNotes release and Section 23 in OntoNotes contains only 1640 sentences. The OntoN</context>
<context position="25408" citStr="Lintean and Rus (2007" startWordPosition="4320" endWordPosition="4323">derable overlap in terms of features used in (Blaheta and Charniak, 2000; Blaheta, 2003) and our system: for example, the label of current node, parent node and sibling nodes. However, there are some features that are unique in our work, e.g., lexical features at a constituent boundaries (node-internal and node-external words). Table 2 of (Blaheta and Charniak, 2000) contains the ac1236 curacies for 4 types of function tags, and our results 6 Conclusions and Future Work in Table 3 compare favorably with those in (Blaheta We proposed a generic statistical tree annotator in and Charniak, 2000). Lintean and Rus (2007a; Lin- the paper. We have shown that a variety of natural tean and Rus (2007b) also studied the function tag- language problems can be tackled with the proposed ging problem and applied naive Bayes and decision tree annotator, from predicting function tags, pretree to it. Their accuracy results are worse than dicting empty categories, to predicting projectable (Blaheta and Charniak, 2000). Neither (Blaheta and syntactic constituents for machine translation. Our Charniak, 2000) nor (Lintean and Rus, 2007a; Lin- results of predicting function tags compare favortean and Rus, 2007b) reported the </context>
</contexts>
<marker>Lintean, Rus, 2007</marker>
<rawString>Mihai Lintean and V. Rus. 2007a. Large scale experiments with function tagging. In Proceedings of the International Conference on Knowledge Engineering, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Lintean</author>
<author>V Rus</author>
</authors>
<title>Naive Bayes and decision trees for function tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference of the FLAIRS-2007.</booktitle>
<contexts>
<context position="2728" citStr="Lintean and Rus, 2007" startWordPosition="424" endWordPosition="427">roblem of predicting projectable syntactic constituent, one can use a sentence alignment tool and syntactic trees on source sentences to create training data by annotating a tree node as projectable or not. A generic tree annotator can also open the door of solving other natural language problems so long as the problem can be cast as annotating tree nodes. As one such example, we will present how to predict empty elements for the Chinese language. Some of the above-mentioned problems have been studied before: predicting function tags were studied in (Blaheta and Charniak, 2000; Blaheta, 2003; Lintean and Rus, 2007a), and results of predicting and recovering empty elements can be found in (Dienes et al., 2003; Schmid, 2006; Campbell, 2004). In this work, we will show that these seemingly unrelated problems can be treated uniformly as adding annotations to an existing tree structure, which is the first goal of this work. Second, the proposed generic tree annotator can also be used to solve new problems: we will show how it can be used to predict projectable syntactic constituents. Third, the uniform treatment not only simplifies the model building process, but also affords us to concentrate on discoverin</context>
<context position="11049" citStr="Lintean and Rus, 2007" startWordPosition="1901" endWordPosition="1904"> marking the grammatical role of a constituent, e.g., DTV for dative objects, LGS for logical subjects etc. Many tags in the “Form/function” row carry semantic information, e.g., LOC is for locative expressions, and TMP for temporal expressions. Type Function Tags Grammatical (52.2%) DTV LGS PRD PUT SBJ VOC Form/function (36.2%) ADV BNF DIR EXT LOC MNR NOM PRP TMP Topicalization (2.2%) TPC Miscellaneous (9.4%) CLF CLR HLN TTL Table 2: Four types of function tags and their relative frequency 4.1.1 Comparison with Prior Arts In order to have a direct comparison with (Blaheta and Charniak, 2000; Lintean and Rus, 2007a), we use the same English Penn Treebank (Marcus et al., 1993) and partition the data set identically: Section 1232 2-21 of Wall Street Journal (WSJ) data for training and Section 23 as the test set. We use all features in Table 1 and build four models, each of which predicting one type of function tags. The results are tabulated in Table 3. As can be seen, our system performs much better than both (Blaheta and Charniak, 2000) and (Lintean and Rus, 2007a). For two major categories, namely grammatical and form/function which account for 96.84% non-null function tags in the test set, our system</context>
<context position="12528" citStr="Lintean and Rus, 2007" startWordPosition="2152" endWordPosition="2155">features we introduced: e.g., the node-external features, i.e., Feature 13 and 14 in Table 1, can capture long-range statistical dependencies in the conditional model (2) and are proved very useful (cf. Section 4.1.2). As far as we can tell, they are not used in previous work. Type Blaheta00 Lintean07 Ours Grammar 98.91% 98.45% 99.75% Form/Func 97.10% 95.15% 98.46% topic 99.92% 99.87% 99.98% Misc 98.65% 98.54% 99.41% Table 3: Function tag prediction accuracies on gold parse trees: breakdown by types of function tags. The 2nd column is due to (Blaheta and Charniak, 2000) and 3rd column due to (Lintean and Rus, 2007a). Our results on the 4th column compare favorably with theirs. 4.1.2 Relative Contributions of Features Since the English WSJ data set contains newswire text, the most recent OntoNotes (Hovy et al., 2006) contains text from a more diversified genres such as broadcast news and broadcast conversation, we decide to test our system on this data set as well. WSJ Section 24 is used for development and Section 23 for test, and the rest is used as the training data. Note that some WSJ files were not included in the OntoNotes release and Section 23 in OntoNotes contains only 1640 sentences. The OntoN</context>
<context position="25408" citStr="Lintean and Rus (2007" startWordPosition="4320" endWordPosition="4323">derable overlap in terms of features used in (Blaheta and Charniak, 2000; Blaheta, 2003) and our system: for example, the label of current node, parent node and sibling nodes. However, there are some features that are unique in our work, e.g., lexical features at a constituent boundaries (node-internal and node-external words). Table 2 of (Blaheta and Charniak, 2000) contains the ac1236 curacies for 4 types of function tags, and our results 6 Conclusions and Future Work in Table 3 compare favorably with those in (Blaheta We proposed a generic statistical tree annotator in and Charniak, 2000). Lintean and Rus (2007a; Lin- the paper. We have shown that a variety of natural tean and Rus (2007b) also studied the function tag- language problems can be tackled with the proposed ging problem and applied naive Bayes and decision tree annotator, from predicting function tags, pretree to it. Their accuracy results are worse than dicting empty categories, to predicting projectable (Blaheta and Charniak, 2000). Neither (Blaheta and syntactic constituents for machine translation. Our Charniak, 2000) nor (Lintean and Rus, 2007a; Lin- results of predicting function tags compare favortean and Rus, 2007b) reported the </context>
</contexts>
<marker>Lintean, Rus, 2007</marker>
<rawString>Mihai Lintean and V. Rus. 2007b. Naive Bayes and decision trees for function tagging. In Proceedings of the International Conference of the FLAIRS-2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Magerman</author>
</authors>
<title>Natural Language Parsing As Statistical Pattern Recognition.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="800" citStr="Magerman, 1994" startWordPosition="116" endWordPosition="117">stract In many natural language applications, there is a need to enrich syntactical parse trees. We present a statistical tree annotator augmenting nodes with additional information. The annotator is generic and can be applied to a variety of applications. We report 3 such applications in this paper: predicting function tags; predicting null elements; and predicting whether a tree constituent is projectable in machine translation. Our function tag prediction system outperforms significantly published results. 1 Introduction Syntactic parsing has made tremendous progress in the past 2 decades (Magerman, 1994; Ratnaparkhi, 1997; Collins, 1997; Charniak, 2000; Klein and Manning, 2003; Carreras et al., 2008), and accurate syntactic parsing is often assumed when developing other natural language applications. On the other hand, there are plenty of language applications where basic syntactic information is insufficient. For instance, in question answering, it is highly desirable to have the semantic information of a syntactic constituent, e.g., a noun-phrase (NP) is a person or an organization; an adverbial phrase is locative or temporal. As syntactic information has been widely used in machine transl</context>
</contexts>
<marker>Magerman, 1994</marker>
<rawString>David M. Magerman. 1994. Natural Language Parsing As Statistical Pattern Recognition. Ph.D. thesis, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
</authors>
<title>A comparison of algorithms for maximum entropy parameter estimation.</title>
<date>2002</date>
<booktitle>In the Sixth Conference on Natural Language Learning (CoNLL2002),</booktitle>
<pages>49--55</pages>
<contexts>
<context position="5677" citStr="Malouf, 2002" startWordPosition="935" endWordPosition="936"> from a predefined finite set L, for each non-terminal node ni in a sequential fashion: P(l1, ··· , l|T||T) Newsnight returns to duo action tonight Figure 1: A sample tree: the number on the upright corner of each non-terminal node is the visit order. is the normalizing factor to ensure that P(li|l1, · · · ,li−1, T) in Equation (2) is a probability and {9k(li−1 1 ,T, li)} are feature functions. There are efficient training algorithms to find optimal weights relative to a labeled training data set once the feature functions {9k(li−1 1 , T, li)} are selected (Berger et al., 1996; Goodman, 2002; Malouf, 2002). In our work, we use the SCGIS training algorithm (Goodman, 2002), and the features used in our systems are detailed in the next section. Once a model is trained, at testing time it is applied to input tree nodes by the same order. Figure 1 highlights the prediction of the function tag for node 3(i.e., PP-CLR-node in the thickened box) after 2 shaded nodes (NP-SBJ node and NP node) are predicted. Note that by this time the predicted values are available to the system, while unvisited nodes (nodes in dashed boxes in Figure 1) can not provide such information. NP−SBJ NNP 1 VBZ S 6 VP TO PP−CLR </context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>Robert Malouf. 2002. A comparison of algorithms for maximum entropy parameter estimation. In the Sixth Conference on Natural Language Learning (CoNLL2002), pages 49–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1905" citStr="Marcus et al., 1993" startWordPosition="289" endWordPosition="292">ation; an adverbial phrase is locative or temporal. As syntactic information has been widely used in machine translation systems (Yamada and Knight, 2001; Xiong et al., 2010; Shen et al., 2008; Chiang, 2010; Shen et al., 2010), an interesting question is to predict whether or not a syntactic constituent is projectable1 across a language pair. 1A constituent in the source language is projectable if it can be aligned to a contiguous span in the target language. Such problems can be abstracted as adding additional annotations to an existing tree structure. For example, the English Penn treebank (Marcus et al., 1993) contains function tags and many carry semantic information. To add semantic information to the basic syntactic trees, a logical step is to predict these function tags after syntactic parsing. For the problem of predicting projectable syntactic constituent, one can use a sentence alignment tool and syntactic trees on source sentences to create training data by annotating a tree node as projectable or not. A generic tree annotator can also open the door of solving other natural language problems so long as the problem can be cast as annotating tree nodes. As one such example, we will present ho</context>
<context position="10093" citStr="Marcus et al., 1993" startWordPosition="1745" endWordPosition="1748">OS tag is TO. 4 Applications and Results A wide variety of language problems can be treated as or cast into a tree annotating problem. In this section, we present three applications of the statistical tree annotator. The first application is to predict function tags of an input syntactic parse tree; the second one is to predict Chinese empty elements; and the third one is to predict whether a syntactic constituent of a source sentence is projectable, meaning if the constituent will have a contiguous translation on the target language. 4.1 Predicting Function Tags In the English Penn Treebank (Marcus et al., 1993) and more recent OntoNotes data (Hovy et al., 2006), some tree nodes are assigned a function tag, which is of one of the four types: grammatical, form/function, topicalization and miscellaneous. Table 2 contains a list of function tags used in the English Penn Treebank (Bies et al., 1995). The “Grammatical” row contains function tags marking the grammatical role of a constituent, e.g., DTV for dative objects, LGS for logical subjects etc. Many tags in the “Form/function” row carry semantic information, e.g., LOC is for locative expressions, and TMP for temporal expressions. Type Function Tags </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A Linear Observed Time Statistical Parser Based on Maximum Entropy Models.</title>
<date>1997</date>
<booktitle>In Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="819" citStr="Ratnaparkhi, 1997" startWordPosition="118" endWordPosition="119">atural language applications, there is a need to enrich syntactical parse trees. We present a statistical tree annotator augmenting nodes with additional information. The annotator is generic and can be applied to a variety of applications. We report 3 such applications in this paper: predicting function tags; predicting null elements; and predicting whether a tree constituent is projectable in machine translation. Our function tag prediction system outperforms significantly published results. 1 Introduction Syntactic parsing has made tremendous progress in the past 2 decades (Magerman, 1994; Ratnaparkhi, 1997; Collins, 1997; Charniak, 2000; Klein and Manning, 2003; Carreras et al., 2008), and accurate syntactic parsing is often assumed when developing other natural language applications. On the other hand, there are plenty of language applications where basic syntactic information is insufficient. For instance, in question answering, it is highly desirable to have the semantic information of a syntactic constituent, e.g., a noun-phrase (NP) is a person or an organization; an adverbial phrase is locative or temporal. As syntactic information has been widely used in machine translation systems (Yama</context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Adwait Ratnaparkhi. 1997. A Linear Observed Time Statistical Parser Based on Maximum Entropy Models. In Second Conference on Empirical Methods in Natural Language Processing, pages 1 – 10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Trace prediction and recovery with unlexicalized PCFGs and slash features.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>177--184</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="2838" citStr="Schmid, 2006" startWordPosition="445" endWordPosition="446">urce sentences to create training data by annotating a tree node as projectable or not. A generic tree annotator can also open the door of solving other natural language problems so long as the problem can be cast as annotating tree nodes. As one such example, we will present how to predict empty elements for the Chinese language. Some of the above-mentioned problems have been studied before: predicting function tags were studied in (Blaheta and Charniak, 2000; Blaheta, 2003; Lintean and Rus, 2007a), and results of predicting and recovering empty elements can be found in (Dienes et al., 2003; Schmid, 2006; Campbell, 2004). In this work, we will show that these seemingly unrelated problems can be treated uniformly as adding annotations to an existing tree structure, which is the first goal of this work. Second, the proposed generic tree annotator can also be used to solve new problems: we will show how it can be used to predict projectable syntactic constituents. Third, the uniform treatment not only simplifies the model building process, but also affords us to concentrate on discovering most useful features for a particular application which often leads to improved performances, e.g, we find s</context>
<context position="26301" citStr="Schmid (2006)" startWordPosition="4461" endWordPosition="4462">accuracy results are worse than dicting empty categories, to predicting projectable (Blaheta and Charniak, 2000). Neither (Blaheta and syntactic constituents for machine translation. Our Charniak, 2000) nor (Lintean and Rus, 2007a; Lin- results of predicting function tags compare favortean and Rus, 2007b) reported the relative usefulness ably with published results on the same data set, posof different features, while we found that the lexical sibly due to new features employed in the system. features are extremely useful. We showed that empty categories can be represented Campbell (2004) and Schmid (2006) studied the as pseudo function tags, and thus predicting empty problem of predicting and recovering empty cate- categories can be solved with the proposed tree angories, but they used very different approaches: in notator. The same technique can be used to predict (Campbell, 2004), a rule-based approach is used projectable syntactic constituents for machine transwhile (Schmid, 2006) used a non-lexical PCFG sim- lation. ilar to (Klein and Manning, 2003). Chung and There are several directions to expand the work Gildea (2010) studied the effects of empty cate- described in this paper. First, th</context>
</contexts>
<marker>Schmid, 2006</marker>
<rawString>Helmut Schmid. 2006. Trace prediction and recovery with unlexicalized PCFGs and slash features. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 177–184, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="1477" citStr="Shen et al., 2008" startWordPosition="218" endWordPosition="221">d Manning, 2003; Carreras et al., 2008), and accurate syntactic parsing is often assumed when developing other natural language applications. On the other hand, there are plenty of language applications where basic syntactic information is insufficient. For instance, in question answering, it is highly desirable to have the semantic information of a syntactic constituent, e.g., a noun-phrase (NP) is a person or an organization; an adverbial phrase is locative or temporal. As syntactic information has been widely used in machine translation systems (Yamada and Knight, 2001; Xiong et al., 2010; Shen et al., 2008; Chiang, 2010; Shen et al., 2010), an interesting question is to predict whether or not a syntactic constituent is projectable1 across a language pair. 1A constituent in the source language is projectable if it can be aligned to a contiguous span in the target language. Such problems can be abstracted as adding additional annotations to an existing tree structure. For example, the English Penn treebank (Marcus et al., 1993) contains function tags and many carry semantic information. To add semantic information to the basic syntactic trees, a logical step is to predict these function tags afte</context>
<context position="21853" citStr="Shen et al., 2008" startWordPosition="3726" endWordPosition="3729"> point to annotate leftmost traces for CP and IP when tree structures are available. On the other hand, predicting the left-most empty elements for VP is a lot harder: the F-measure is only 86.8% for VP-NoneL. Predicting the rightmost empty elements under VP and middle empty elements under IP is somewhat easier: VP-NoneR and IP-NoneM’sF-measures are 92.3% and 93.6%, respectively. 4.3 Predicting Projectable Constituents The third application is predicting projectable constituents for machine translation. State-of-the-art machine translation systems (Yamada and Knight, 2001; Xiong et al., 2010; Shen et al., 2008; Chiang, 2010; Shen et al., 2010) rely heavily on syntactic analysis. Projectable structures are important in that it is assumed in CFG-style translation rules that a source span can be translated contiguously. Clearly, not all source constituents can be translated this way, but if we can predict whether a non-terminal source node is projectable, we can avoid translation errors by bypassing or discouraging the derivation paths relying on non-projectable constituents, or using phrase-based approaches for non-projectable constituents. We start from LDC’s bilingual Arabic-English treebank with s</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Bing Zhang</author>
<author>Spyros Matsoukas</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>Statistical machine translation with a factorized grammar.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>616--625</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="1511" citStr="Shen et al., 2010" startWordPosition="224" endWordPosition="227"> 2008), and accurate syntactic parsing is often assumed when developing other natural language applications. On the other hand, there are plenty of language applications where basic syntactic information is insufficient. For instance, in question answering, it is highly desirable to have the semantic information of a syntactic constituent, e.g., a noun-phrase (NP) is a person or an organization; an adverbial phrase is locative or temporal. As syntactic information has been widely used in machine translation systems (Yamada and Knight, 2001; Xiong et al., 2010; Shen et al., 2008; Chiang, 2010; Shen et al., 2010), an interesting question is to predict whether or not a syntactic constituent is projectable1 across a language pair. 1A constituent in the source language is projectable if it can be aligned to a contiguous span in the target language. Such problems can be abstracted as adding additional annotations to an existing tree structure. For example, the English Penn treebank (Marcus et al., 1993) contains function tags and many carry semantic information. To add semantic information to the basic syntactic trees, a logical step is to predict these function tags after syntactic parsing. For the probl</context>
<context position="21887" citStr="Shen et al., 2010" startWordPosition="3733" endWordPosition="3736">s for CP and IP when tree structures are available. On the other hand, predicting the left-most empty elements for VP is a lot harder: the F-measure is only 86.8% for VP-NoneL. Predicting the rightmost empty elements under VP and middle empty elements under IP is somewhat easier: VP-NoneR and IP-NoneM’sF-measures are 92.3% and 93.6%, respectively. 4.3 Predicting Projectable Constituents The third application is predicting projectable constituents for machine translation. State-of-the-art machine translation systems (Yamada and Knight, 2001; Xiong et al., 2010; Shen et al., 2008; Chiang, 2010; Shen et al., 2010) rely heavily on syntactic analysis. Projectable structures are important in that it is assumed in CFG-style translation rules that a source span can be translated contiguously. Clearly, not all source constituents can be translated this way, but if we can predict whether a non-terminal source node is projectable, we can avoid translation errors by bypassing or discouraging the derivation paths relying on non-projectable constituents, or using phrase-based approaches for non-projectable constituents. We start from LDC’s bilingual Arabic-English treebank with source human parse trees and alignm</context>
</contexts>
<marker>Shen, Zhang, Matsoukas, Xu, Weischedel, 2010</marker>
<rawString>Libin Shen, Bing Zhang, Spyros Matsoukas, Jinxi Xu, and Ralph Weischedel. 2010. Statistical machine translation with a factorized grammar. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 616–625, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
</authors>
<title>Learning translation boundaries for phrase-based decoding.</title>
<date>2010</date>
<booktitle>In NAACL-HLT</booktitle>
<contexts>
<context position="1458" citStr="Xiong et al., 2010" startWordPosition="214" endWordPosition="217">niak, 2000; Klein and Manning, 2003; Carreras et al., 2008), and accurate syntactic parsing is often assumed when developing other natural language applications. On the other hand, there are plenty of language applications where basic syntactic information is insufficient. For instance, in question answering, it is highly desirable to have the semantic information of a syntactic constituent, e.g., a noun-phrase (NP) is a person or an organization; an adverbial phrase is locative or temporal. As syntactic information has been widely used in machine translation systems (Yamada and Knight, 2001; Xiong et al., 2010; Shen et al., 2008; Chiang, 2010; Shen et al., 2010), an interesting question is to predict whether or not a syntactic constituent is projectable1 across a language pair. 1A constituent in the source language is projectable if it can be aligned to a contiguous span in the target language. Such problems can be abstracted as adding additional annotations to an existing tree structure. For example, the English Penn treebank (Marcus et al., 1993) contains function tags and many carry semantic information. To add semantic information to the basic syntactic trees, a logical step is to predict these</context>
<context position="21834" citStr="Xiong et al., 2010" startWordPosition="3722" endWordPosition="3725">, there is really no point to annotate leftmost traces for CP and IP when tree structures are available. On the other hand, predicting the left-most empty elements for VP is a lot harder: the F-measure is only 86.8% for VP-NoneL. Predicting the rightmost empty elements under VP and middle empty elements under IP is somewhat easier: VP-NoneR and IP-NoneM’sF-measures are 92.3% and 93.6%, respectively. 4.3 Predicting Projectable Constituents The third application is predicting projectable constituents for machine translation. State-of-the-art machine translation systems (Yamada and Knight, 2001; Xiong et al., 2010; Shen et al., 2008; Chiang, 2010; Shen et al., 2010) rely heavily on syntactic analysis. Projectable structures are important in that it is assumed in CFG-style translation rules that a source span can be translated contiguously. Clearly, not all source constituents can be translated this way, but if we can predict whether a non-terminal source node is projectable, we can avoid translation errors by bypassing or discouraging the derivation paths relying on non-projectable constituents, or using phrase-based approaches for non-projectable constituents. We start from LDC’s bilingual Arabic-Engl</context>
<context position="28747" citStr="Xiong et al., 2010" startWordPosition="4844" endWordPosition="4847">policy ing function tags are on system parses, they are not of the U.S. government and no official endorsement comparable with ours. (Gabbard et al., 2006) also should be inferred. contains a second stage employing multiple clas- We are also grateful to three anonymous reviewers sifiers to recover empty categories and resolve co- for their suggestions and comments for improving indexations between an empty element and its an- the paper. tecedent. References As for predicting projectable constituent, it is re- Adam L. Berger, Stephen A. Della Pietra, and Vincent lated to the work described in (Xiong et al., 2010), J. Della Pietra. 1996. A maximum entropy approach where they were predicting translation boundaries. to natural language processing. Computational LinA major difference is that (Xiong et al., 2010) de- guistics, 22(1):39–71, March. fines projectable spans on a left-branching deriva- Ann Bies, Mark Ferguson, and karen Katz. 1995. Bracktion tree solely for their phrase decoder and models, eting guidelines for treebank II-style penn treebank while translation boundaries in our work are defined project. Technical report, Linguistic Data Consortium. from source parse trees. Our work uses more re-</context>
</contexts>
<marker>Xiong, Zhang, Li, 2010</marker>
<rawString>Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Learning translation boundaries for phrase-based decoding. In NAACL-HLT 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fei Xia</author>
<author>Fu-Dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The Penn Chinese TreeBank: Phrase structure annotation of a large corpus.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>2</issue>
<contexts>
<context position="15369" citStr="Xue et al., 2005" startWordPosition="2625" endWordPosition="2628">parsing (Bikel, 2004), lexical information is extremely important for predicting and recovering function tags. This is not surprising since many function tags carry semantic information, and more often than not, the ambiguity can only be resolved by lexical information. E.g., whether a PP is locative or temporal PP is heavily influenced by the lexical choice of the NP argument. 4.2 Predicting Chinese Empty Elements As is well known, Chinese is a pro-drop language. This and its lack of subordinate conjunction complementizers lead to the ubiquitous use of empty elements in the Chinese treebank (Xue et al., 2005). Predicting or recovering these empty elements is therefore important for the Chinese language pro1233 Feature Set Accuracy prior (guess NONE) 78.21% Non-lexical labels only 91.52% +past prediction 92.04% +node-internal lexical 95.17% +node-external lexical 96.70% +head word 97.34% Table 5: Effects of feature sets: the second row contains the baseline result when always predicting NONE; Row 3 through 8 contain results by incrementally adding feature sets. cessing. Recently, Chung and Gildea (2010) has found it useful to recover empty elements in machine translation. Since empty elements do no</context>
<context position="18686" citStr="Xue et al., 2005" startWordPosition="3204" endWordPosition="3207">y parents) and adding pseudo function tags to its lowest non-empty parent node 1:Foreach trace t 2: Find its lowest ancestor node p spanning at least one non-trace word 3: if t is p’s left-most child 4: add pseudo tag NoneL to p 5: else if t is p’s right-most child 6: add pseudo tag NoneR to p 7: else 8: add pseudo tag NoneM to p 9: Remove p’s child spanning the trace t and all its children IP−NoneL AD VE JJ NN VV VP NP IP VP NP NN NN NONE AD VE JJ VP 1234 pseudo function tag and the statistical tree annotator can thus be used to solve this problem. 4.2.1 Results We use Chinese Treebank v6.0 (Xue et al., 2005) and the broadcast conversation data from CTB v7.0 2. The data set is partitioned into training, development and blind test as shown in Table 6. The partition is created so that different genres are well represented in different subsets. The training, development and test set have 32925, 3297 and 3033 sentences, respectively. Subset File IDs Training 0001-0325, 0400-0454, 0600-0840 0500-0542, 2000-3000, 0590-0596 1001-1120, cctv,cnn,msnbc, phoenix 00-06 Dev 0841-0885, 0543-0548, 3001-3075 1121-1135, phoenix 07-09 Test 0900-0931,0549-0554, 3076-3145 1136-1151, phoenix 10-11 Table 6: Data partit</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha Palmer. 2005. The Penn Chinese TreeBank: Phrase structure annotation of a large corpus. Natural Language Engineering, 11(2):207–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proc. Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1438" citStr="Yamada and Knight, 2001" startWordPosition="210" endWordPosition="213">1997; Collins, 1997; Charniak, 2000; Klein and Manning, 2003; Carreras et al., 2008), and accurate syntactic parsing is often assumed when developing other natural language applications. On the other hand, there are plenty of language applications where basic syntactic information is insufficient. For instance, in question answering, it is highly desirable to have the semantic information of a syntactic constituent, e.g., a noun-phrase (NP) is a person or an organization; an adverbial phrase is locative or temporal. As syntactic information has been widely used in machine translation systems (Yamada and Knight, 2001; Xiong et al., 2010; Shen et al., 2008; Chiang, 2010; Shen et al., 2010), an interesting question is to predict whether or not a syntactic constituent is projectable1 across a language pair. 1A constituent in the source language is projectable if it can be aligned to a contiguous span in the target language. Such problems can be abstracted as adding additional annotations to an existing tree structure. For example, the English Penn treebank (Marcus et al., 1993) contains function tags and many carry semantic information. To add semantic information to the basic syntactic trees, a logical step</context>
<context position="21814" citStr="Yamada and Knight, 2001" startWordPosition="3718" endWordPosition="3721">ping the Chinese treebank, there is really no point to annotate leftmost traces for CP and IP when tree structures are available. On the other hand, predicting the left-most empty elements for VP is a lot harder: the F-measure is only 86.8% for VP-NoneL. Predicting the rightmost empty elements under VP and middle empty elements under IP is somewhat easier: VP-NoneR and IP-NoneM’sF-measures are 92.3% and 93.6%, respectively. 4.3 Predicting Projectable Constituents The third application is predicting projectable constituents for machine translation. State-of-the-art machine translation systems (Yamada and Knight, 2001; Xiong et al., 2010; Shen et al., 2008; Chiang, 2010; Shen et al., 2010) rely heavily on syntactic analysis. Projectable structures are important in that it is assumed in CFG-style translation rules that a source span can be translated contiguously. Clearly, not all source constituents can be translated this way, but if we can predict whether a non-terminal source node is projectable, we can avoid translation errors by bypassing or discouraging the derivation paths relying on non-projectable constituents, or using phrase-based approaches for non-projectable constituents. We start from LDC’s b</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. In Proc. Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Suk Lee</author>
<author>Xiaoqiang Luo</author>
<author>Liu Li</author>
</authors>
<title>Learning to transform and select elementary trees for improved syntax-based machine translations.</title>
<date>2011</date>
<booktitle>In Proc. ofACL.</booktitle>
<marker>Lee, Luo, Li, 2011</marker>
<rawString>Bing Zhao, , Young-Suk Lee, Xiaoqiang Luo, and Liu Li. 2011. Learning to transform and select elementary trees for improved syntax-based machine translations. In Proc. ofACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>