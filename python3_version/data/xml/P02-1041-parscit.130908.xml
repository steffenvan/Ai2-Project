<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.887386666666667">
Proceedings of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), Philadelphia, July 2002, pp. 319-326.
Coupling CCG and Hybrid Logic Dependency Semantics
</note>
<author confidence="0.963955">
Jason Baldridge
</author>
<affiliation confidence="0.9698485">
ICCS
Division of Informatics
2 Buccleuch Place
University of Edinburgh
</affiliation>
<address confidence="0.87607">
Edinburgh, UK, EH8 9LW
</address>
<email confidence="0.996325">
jmb@cogsci.ed.ac.uk
</email>
<author confidence="0.948181">
Geert-Jan M. Kruijff
</author>
<affiliation confidence="0.882563">
Universit¨at des Saarlandes
Computational Linguistics
</affiliation>
<address confidence="0.837824333333333">
Lehrstuhl Uszkoreit
Building 17, Postfach 15 11 50
66041 Saarbr¨ucken, Germany
</address>
<email confidence="0.997987">
gj@CoLi.Uni-SB.DE
</email>
<sectionHeader confidence="0.998584" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999711928571429">
Categorial grammar has traditionally used
the λ-calculus to represent meaning. We
present an alternative, dependency-based
perspective on linguistic meaning and sit-
uate it in the computational setting. This
perspective is formalized in terms of hy-
brid logic and has a rich yet perspicuous
propositional ontology that enables a wide
variety of semantic phenomena to be rep-
resented in a single meaning formalism.
Finally, we show how we can couple this
formalization to Combinatory Categorial
Grammar to produce interpretations com-
positionally.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99989416">
The λ-calculus has enjoyed many years as the stan-
dard semantic encoding for categorial grammars and
other grammatical frameworks, but recent work has
highlighted its inadequacies for both linguistic and
computational concerns of representing natural lan-
guage semantics (Copestake et al., 1999; Kruijff,
2001). The latter couples a resource-sensitive cate-
gorial proof theory (Moortgat, 1997) to hybrid logic
(Blackburn, 2000) to formalize a dependency-based
perspective on meaning, which we call here Hybrid
Logic Dependency Semantics (HLDS). In this pa-
per, we situate HLDS in the computational context
by explicating its properties as a framework for com-
putational semantics and linking it to Combinatory
Categorial Grammar (CCG).
The structure of the paper is as follows. In §2,
we briefly introduce CCG and how it links syntax
and semantics, and then discuss semantic represen-
tations that use indexes to identify subparts of logi-
cal forms. §3 introduces HLDS and evaluates it with
respect to the criteria of other computational seman-
tics frameworks. §4 shows how we can build HLDS
terms using CCG with unification and §5 shows how
intonation and information structure can be incorpo-
rated into the approach.
</bodyText>
<sectionHeader confidence="0.970807" genericHeader="method">
2 Indexed semantic representations
</sectionHeader>
<bodyText confidence="0.999955333333333">
Traditionally, categorial grammar has captured
meaning using a (simply typed) λ-calculus, build-
ing semantic structure in parallel to the categorial in-
ference (Morrill, 1994; Moortgat, 1997; Steedman,
2000b). For example, a (simplified) CCG lexical en-
try for a verb such as wrote is given in (1).
</bodyText>
<listItem confidence="0.934275666666667">
(1) wrote �- (s\n)/n : λx.λy.write(y,x)
Rules of combination are defined to operate on both
categories and λ-terms simultaneously. For exam-
ple, the rules allow the following derivation for Ed
wrote books.
(2) Ed wrote books
</listItem>
<equation confidence="0.674148">
s : write(Ed,books)
</equation>
<bodyText confidence="0.999874428571429">
Derivations like (2) give rise to the usual sort
of predicate-argument structure whereby the order
in which the arguments appear (and are bound by
the λ’s) is essentially constitutive of their meaning.
Thus, the first argument could be taken to corre-
spond to the writer, whereas the second argument
corresponds to what is being written.
</bodyText>
<equation confidence="0.60606075">
n:Ed (s\n)/n:λx.λy.write(y,x) n:books
�
s\n : λy.write(y,books)
�
</equation>
<bodyText confidence="0.939134421052632">
One deficiency of %-calculus meaning representa-
tions is that they usually have to be type-raised to
the worst case to fully model quantification, and this
can reverberate and increase the complexity of syn-
tactic categories since a verb like wrote will need to
be able to take arguments with the types of general-
ized quantifiers. The approach we advocate in this
paper does not suffer from this problem.
For CCG, the use of the %-terms is simply a con-
venient device to bind arguments when presenting
derivations (Steedman, 2000b). In implementations,
a more common strategy is to compute semantic rep-
resentations via unification, a tactic explicitly em-
ployed in Unification Categorial Grammar (UCG)
(Zeevat, 1988). Using a unification paradigm in
which atomic categories are bundles of syntactic and
semantic information, we can use an entry such as
(3) for wrote in place of (1). In the unification set-
ting, (3) permits a derivation analogous to (2).
</bodyText>
<listItem confidence="0.986469">
(3) wrote �- (s : write(y,x)\n:y)/n:x
</listItem>
<bodyText confidence="0.996169230769231">
For creating predicate-argument structures of this
kind, strategies using either %-terms or unification
to bind arguments are essentially notational vari-
ants. However, UCG goes beyond simple predicate-
argument structures to instead use a semantic repre-
sentation language called Indexed Language (InL).
The idea of using indexes stems from Davidson
(event variables), and are a commonly used mech-
anism in unification-based frameworks and theories
for discourse representation. InL attaches one to ev-
ery formula representing its discourse referent. This
results in a representation such as (4) for the sen-
tence Ed came to the party.
</bodyText>
<listItem confidence="0.858698">
(4) [e][party(x),past(e),to(e,x),come(e,Ed)]
</listItem>
<bodyText confidence="0.956373051282051">
InL thus flattens logical forms to some extent, using
the indexes to spread a given entity or event through
multiple predications. The use of indexes is crucial
for UCG’s account of modifiers, and as we will see
later, we exploit such referents to achieve similar
ends when coupling HLDS and CCG.
Minimal Recursion Semantics (MRS) (Copestake
et al., 1999; Copestake et al., 2001) is a frame-
work for computational semantics that is designed
to simplify the work of algorithms which produce
or use semantic representations. MRS provides the
means to represent interpretations with a flat, un-
derspecified semantics using terms of the predicate
calculus and generalized quantifiers. Flattening is
achieved by using an indexation scheme involving
labels that tag particular groups of elementary pred-
ications (EPs) and handles (here, h1,h2,...) that ref-
erence those EPs. Underspecification is achieved
by using unresolved handles as the arguments for
scope-bearing elements and declaring constraints
(with the =q operator) on how those handles can be
resolved. Different scopes can be reconstructed by
equating unresolved handles with the labels of the
other EPs obeying the =q constraints. For example,
(5) would be given as the representation for every
dog chases some white cat.
(5) (h0,{h1:every(x,h2,h3),h4:dog(x),
h11:cat(y),h8:some(y,h9,h10),
h11:white(y),h7:chase(x,y)1,
{h0=qh7, h2=qh4, h9=qh111)
Copestake et al. argue that these flat representa-
tions facilitate a number of computational tasks, in-
cluding machine translation and generation, without
sacrificing linguistic expressivity. Also, flatness per-
mits semantic equivalences to be checked more eas-
ily than in structures with deeper embedding, and
underspecification simplifies the work of the parser
since it does not have to compute every possible
reading for scope-bearing elements.
</bodyText>
<sectionHeader confidence="0.989919" genericHeader="method">
3 Hybrid Logic Dependency Semantics
</sectionHeader>
<bodyText confidence="0.999891333333333">
Kruijff (2001) proposes an alternative way to rep-
resenting linguistically realized meaning: namely,
as terms of hybrid modal logic (Blackburn, 2000)
explicitly encoding the dependency relations be-
tween heads and dependents, spatio-temporal struc-
ture, contextual reference, and information struc-
ture. We call this unified perspective combining
many levels of meaning Hybrid Logic Dependency
Semantics (HLDS). We begin by discussing how hy-
brid logic extends modal logic, then look at the rep-
resentation of linguistic meaning via hybrid logic
terms.
</bodyText>
<subsectionHeader confidence="0.998856">
3.1 Hybrid Logic
</subsectionHeader>
<bodyText confidence="0.999877525">
Though modal logic provides a powerful tool for
encoding relational structures and their properties,
it contains a surprising inherent asymmetry: states
(“worlds”) are at the heart of the model theory for
modal logic, but there are no means to directly
reference specific states using the object language.
This inability to state where exactly a proposition
holds makes modal logic an inadequate representa-
tion framework for practical applications like knowl-
edge representation (Areces, 2000) or temporal rea-
soning (Blackburn, 1994). Because of this, compu-
tational work in knowledge representation has usu-
ally involved re-engineering first-order logic to suit
the task, e.g., the use of metapredicates such as Hold
of Kowalski and Allen. Unfortunately, such logics
are often undecidable.
Hybrid logic extends standard modal logic while
retaining decidability and favorable complexity
(Areces, 2000) (cf. (Areces et al., 1999) for a com-
plexity roadmap). The strategy is to add nominals,
a new sort of basic formula with which we can ex-
plicitly name states in the object language. Next to
propositions, nominals are first-class citizens of the
object language: formulas can be formed using both
sorts, standard boolean operators, and the satisfac-
tion operator “@”. A formula @ip states that the
formula p holds at the state named by i.1 (There
are more powerful quantifiers ranging over nomi-
nals, such as , , but we do not consider them here.)
With nominals we obtain the possibility to explic-
itly refer to the state at which a proposition holds. As
Blackburn (1994) argues, this is essential for cap-
turing our intuitions about temporal reference. A
standard modal temporal logic with the modalities
F and P (future and past, respectively) cannot cor-
rectly represent an utterance such as Edfinished the
book because it is unable to refer to the specific time
at which the event occurred. The addition of nomi-
nals makes this possible, as shown in (6), where the
nominal i represents the Reichenbachian event time.
</bodyText>
<listItem confidence="0.987279">
(6) (P)(i A Ed-finish-book)
</listItem>
<bodyText confidence="0.810879875">
Furthermore, many temporal properties can be de-
fined in terms of pure formulas which use nominals
and contain no propositional variables. For example,
the following term defines the fact that the relations
for F and P are mutually converse:
1A few notes on our conventions: p,q,r are variables over
any hybrid logic formula; i, j,k are variables over nominals; di
and hi denote nominals (for dependent and head, respectively).
</bodyText>
<listItem confidence="0.956487">
(7) @i[F](P)i A @i[P](F)i
</listItem>
<bodyText confidence="0.9991156">
It is also possible to encode a variety of other rep-
resentations in terms of hybrid logics. For example,
nominals correspond to tags in attribute-value matri-
ces (AVMs), so the hybrid logic formula in (8) cor-
responds to the AVM in (9).
</bodyText>
<figure confidence="0.60956">
(8) (SUBJ)(i A (AGR)singular A (PRED)dog)
A (COMP)(SUBJ)i
AGR singular
SUBJ 1 PRED dog
COMP [SUBJ 1 �
</figure>
<bodyText confidence="0.999485266666667">
A crucial aspect of hybrid logic is that nominals
are at the heart of a sorting strategy. Different sorts
of nominals can be introduced to build up a rich
sortal ontology without losing the perspicuity of a
propositional setting. Additionally, we can reason
about sorts because nominals are part and parcel of
the object language. We can extend the language of
hybrid logic with {Sort:NominalI to facilitate the ex-
plicit statement of what sort a nominal is in the lan-
guage and carry this modification into one of the ex-
isting tableaux methods for hybrid logic to reason ef-
fectively with this information. This makes it possi-
ble to capture the rich ontologies of lexical databases
like WordNet in a clear and concise fashion which
would be onerous to represent in first-order logic.
</bodyText>
<subsectionHeader confidence="0.999683">
3.2 Encoding linguistic meaning
</subsectionHeader>
<bodyText confidence="0.999921285714286">
Hybrid logic enables us to logically capture two es-
sential aspects of meaning in a clean and compact
way, namely ontological richness and the possibility
to refer. Logically, we can represent an expression’s
linguistically realized meaning as a conjunction of
modalized terms, anchored by the nominal that iden-
tifies the head’s proposition:
</bodyText>
<equation confidence="0.58666">
(10) @h(proposition A(8i)(di A depi))
</equation>
<bodyText confidence="0.998771777777778">
Dependency relations are modeled as modal rela-
tions (8i), and with each dependent we associate
a nominal di, representing its discourse referent.
Technically, (10) states that each nominal di names
the state where a dependent expressed as a proposi-
tion depi should be evaluated and is a 8i successor
of h, the nominal identifying the head. As an exam-
ple, the sentence Ed wrote a long book in London
receives the represention in (11).
</bodyText>
<equation confidence="0.9128772">
(9) 26664
35777
(11) @h1(write ^ hACTi(d0^Ed)
^ hPATi(d5^book^hGRi(d7^long))
^ hLOCi(d9^London))
</equation>
<bodyText confidence="0.999295181818182">
The modal relations ACT, PAT, LOC, and GR stand
for the dependency relations Actor, Patient, Loca-
tive, and General Relationship, respectively. See
Kruijff (2001) for the model-theoretic interpretation
of expressions such as (11).
Contextual reference can be modeled as a state-
ment that from the current state (anaphor) there
should be an accessible antecedent state at which
particular conditions hold. Thus, assuming an ac-
cessibility relation XS, we can model the meaning
of the pronoun he as in (12).
</bodyText>
<listItem confidence="0.807508">
(12) @ihXSi(j ^ male)
</listItem>
<bodyText confidence="0.994511826086956">
During discourse interpretation, this statement is
evaluated against the discourse model. The pronoun
is resolvable only if a state where male holds is XS-
accessible in the discourse model. Different acces-
sibility relations can be modeled, e.g. to distinguish
a local context (for resolving reflexive anaphors like
himself) from a global context (Kruijff, 2001).
Finally, the rich temporal ontology underlying
models of tense and aspect such as Moens and
Steedman (1988) can be captured using the sorting
strategy. Earlier work like Blackburn and Lascarides
(1992) already explored such ideas. HLDS employs
hybrid logic to integrate Moens and Steedman’s no-
tion of the event nucleus directly into meaning rep-
resentations. The event nucleus is a tripartite struc-
ture reflecting the underlying semantics of a type of
event. The event is related to a preparation (an ac-
tivity bringing the event about) and a consequent (a
state ensuing to the event), which we encode as the
modal relations PREP and CONS, respectively. Dif-
ferent kinds of states and events are modeled as dif-
ferent sorts ofnominals, shown in (13) using the no-
tation introduced above.
</bodyText>
<listItem confidence="0.704882">
(13) @{Activity:e1lhPREPifAchievement:e2g
^@{Achievement:e2lhCONSifState:e3g
</listItem>
<bodyText confidence="0.998257166666667">
To tie (13) in with a representation like (11), we
equate the nominal of the head with one of the nom-
inals in the event nucleus (E)a and state its temporal
relation (e.g. hPi). Given the event nucleus in (13),
the representation in (11) becomes (14), where the
event is thus located at a specific time in the past.
</bodyText>
<listItem confidence="0.69991525">
(14) @h1(E(13) ^ write ^ hACTi(d0^Ed)
^ hPATi(d5^book^hGRi(d7^long))
^ hLOCi(d9^London))
^@h1fAchievement:e2g^hPifAchievement:e2g
</listItem>
<bodyText confidence="0.999553">
Hybrid logic’s flexibility makes it amenable to
representing a wide variety of semantic phenomena
in a propositional setting, and it can furthermore be
used to formulate a discourse theory (Kruijff and
Kruijff-Korbayov´a, 2001).
</bodyText>
<subsectionHeader confidence="0.997736">
3.3 Comparison to MRS
</subsectionHeader>
<bodyText confidence="0.999915944444445">
Here we consider the properties of HLDS with
respect to the four main criteria laid out by
Copestake et al. (1999) which a computational se-
mantics framework must meet: expressive adequacy,
grammatical compatibility, computational tractabil-
ity, and underspecifiability.
Expressive adequacy refers to a framework’s abil-
ity to correctly express linguistic meaning. HLDS
was designed not only with this in mind, but as its
central tenet. In addition to providing the means
to represent the usual predicate-valency relations,
it explicitly marks the named dependency relations
between predicates and their arguments and modi-
fiers. These different dependency relations are not
just labels: they all have unique semantic imports
which project new relations in the context of differ-
ent heads. HLDS also tackles the representation of
tense and aspect, contextual reference, and informa-
tion structure, as well as their interaction with dis-
course.
The criterion of grammatical compatibility re-
quires that a framework be linkable to other kinds of
grammatical information. Kruijff (2001) shows that
HLDS can be coupled to a rich grammatical frame-
work, and in x4 we demonstrate that it can be tied to
CCG, a much lower power formalism than that as-
sumed by Kruijff. It should furthermore be straight-
forward to use our approach to hook HLDS up to
other unification-based frameworks.
The definition of computational tractability states
that it must be possible to check semantic equiva-
lence of different formulas straightforwardly. Like
MRS, HLDS provides the means to view linguis-
tic meaning in a flattened format and thereby ease
the checking of equivalence. For example, (15) de-
scribes the same relational structure as (11).
</bodyText>
<listItem confidence="0.425112">
(15) @h1(writeA(ACT)d0 A(PAT)d5 A(LOC)d9)
A@d0Ed A @d5book A @d9London
A@d7long A @d5(GR)d7
</listItem>
<bodyText confidence="0.999965">
This example clarifies how the use of nominals is
related to the indexes of UCG’s InL and the labels
of MRS. However, there is an important difference:
nominals are full citizens of the object language with
semantic import and are not simply a device for
spreading meaning across several elementary predi-
cations. They simultaneously represent tags on sub-
parts of a logical form and discourse referents on
which relations are predicated. Because it is possi-
ble to view an HLDS term as a flat conjunction of
the heads and dependents inside it, the benefits de-
scribed by Copestake et al. with respect to MRS’s
flatness thus hold for HLDS as well.
Computational tractability also requires that it
is straightforward to express relationships between
representations. This can be done in the object lan-
guage of HLDS as hybrid logic implicational state-
ments which can be used with proof methods to dis-
cover deeper relationships. Kruijff’s model connect-
ing linguistic meaning to a discourse context is one
example of this.
Underspecifiability means that semantic represen-
tations should provide means to leave some semantic
distinctions unresolved whilst allowing partial terms
to be flexibly and monotonically resolved. (5) shows
how MRS leaves quantifier scope underspecified,
and such formulas can be transparently encoded in
HLDS. Consider (16), where the relations RESTR
and BODY represent the restriction and body argu-
ments of the generalized quantifiers, respectively.
</bodyText>
<listItem confidence="0.910392">
(16) @h7(chase A (ACT)h4 A (PAT)h11)
A@h1(every A (RESTR)i A (BODY) j)
A@h8(some A (RESTR)kA (BODY)l)
A@h4dogA@h11catA@h11(GR)(h12Awhite)
A@i(QEQ)h4 A @k(QEQ)h11
</listItem>
<bodyText confidence="0.993677222222222">
MRS-style underspecification is thus replicated by
declaring new nominals and modeling =q as a modal
relation between nominals. When constructing the
fully-scoped structures generated by an underspeci-
fied one, the =q constraints must be obeyed accord-
ing to the qeq condition of Copestake etal. Because
HLDS is couched directly in terms of hybrid logic,
we can concisely declare the qeq condition as the
following implication:
</bodyText>
<equation confidence="0.45614">
(17) @i QEQ)j --* @ijV (@i BODY)kn@k QEQ)j)
</equation>
<bodyText confidence="0.999565">
Alternatively, it would in principle be possible to
adopt a truly modal solution to the representation
of quantifiers. Following Alechina (1995), (general-
ized) quantification can be modeled as modal opera-
tors. The complexity of generalized quantification is
then pushed into the model theory instead of forcing
the representation to carry the burden.
</bodyText>
<sectionHeader confidence="0.997089" genericHeader="method">
4 CCG Coupled to HLDS
</sectionHeader>
<bodyText confidence="0.999939926829268">
In Dependency Grammar Logic (DGL),
Kruijff (2001) couples HLDS to a resource-
sensitive categorial proof theory (CTL) (Moortgat,
1997). Though DGL demonstrates a procedure for
building HLDS terms from linguistic expressions,
there are several problems we can overcome by
switching to CCG. First, parsing with CCG gram-
mars for substantial fragments is generally more
efficient than with CTL grammars with similar
coverage. Also, a wide-coverage statistical parser
which produces syntactic dependency structures
for English is available for CCG (Clark et al.,
2002). Second, syntactic features (modeled by
unary modalities) in CTL have no intuitive semantic
reflection, whereas CCG can relate syntactic and
semantic features perspicuously using unification.
Finally, CCG has a detailed syntactic account of the
realization of information structure in English.
To link syntax and semantics in derivations, ev-
ery logical form in DGL expresses a nominal iden-
tifying its head in the format @ip. This handles de-
pendents in a linguistically motivated way through
a linking theory: given the form of a dependent, its
(possible) role is established, after which its mean-
ing states that it seeks a head that can take such a
role. However, to subsequently bind that dependent
into the verb’s argument slot requires logical axioms
about the nature of various dependents. This not
only requires extra reduction steps to arrive at the
desired logical form, but could also lead to problems
depending on the underlying theory of roles.
We present an alternative approach to binding de-
pendents, which overcomes these problems without
abandoning the linguistic motivation. Because we
work in a lexicalist setting, we can compile the ef-
fects of the linguistic linking theory directly into cat-
egory assignments.
The first difference in our proposal is that argu-
ments express only their own nominal, not the nom-
inal of a head as well. For example, proper nouns
receive categories such as (18).
</bodyText>
<listItem confidence="0.624646">
(18) Ed ` n : @d1Ed
</listItem>
<bodyText confidence="0.999656416666667">
This entry highlights our relaxation of the strict con-
nection between syntactic and semantic types tradi-
tionally assumed in categorial grammars, a move in
line with the MRS approach.
In contrast with DGL, the semantic portion of a
syntactic argument in our system does not declare
the role it is to take and does not identify the head
it is to be part of. Instead it identifies only its own
referent. Without using additional inference steps,
this is transmuted via unification into a form similar
to DGL’s in the result category. (19) is an example
of the kind of head category needed.
</bodyText>
<listItem confidence="0.755269">
(19) sleeps ` s : @h2(sleep ^ hACTi(i^p))nn : @ip
To derive Ed sleeps, (18) and (19) combine via back-
ward application to produce (20), the same term as
that built in DGL using one step instead of several.
(20) @h2(sleep A (ACT)(d1AEd))
</listItem>
<bodyText confidence="0.9683753">
To produce HLDS terms that are fully compati-
ble with the way that Kruijff and Kruijff-Korbayov´a
(2001) model discourse, we need to mark the infor-
mativity of dependents as contextually bound (CB)
and contextually nonbound (NB). In DGL, these ap-
pear as modalities in logical forms that are used to
create a topic-focus articulation that is merged with
the discourse context. For example, the sentence he
wrote a book would receive the following (simpli-
fied) interpretation:
</bodyText>
<listItem confidence="0.758882">
(21) @h1([NB]write A [NB](PAT)(d5Abook)
A [CB](ACT)(d6 A (XS)(d3Amale)))
</listItem>
<bodyText confidence="0.923207">
DGL uses feature-resolving unary modalities
(Moortgat, 1997) to instantiate the values of in-
formativity. In unification-based approaches such
as CCG, the transferal of feature information into
semantic representations is standard practice. We
thus employ the feature inf and mark informativity
in logical forms with values resolved syntactically.
</bodyText>
<listItem confidence="0.9817945">
(22) Ed ` ninf=CB : @d1Ed
(23) sleeps ` s : @h2([NB]sleep ^ [q]hACTi(i^p))nninf=q:@ip
</listItem>
<bodyText confidence="0.9071195">
Combining these entries using backward application
gives the following result for Ed sleeps:
</bodyText>
<listItem confidence="0.575632">
(24) s : @h2([NB]sleep ^ [CB]hACTi(d1^Ed))
</listItem>
<bodyText confidence="0.999732625">
A major benefit of having nominals in our rep-
resentations comes with adjuncts. With HLDS, we
consider the prepositional verbal modifier in the sen-
tence Ed sleeps in the bed as an optional Locative
dependent of sleeps. To implement this, we fol-
low DGL in identifying the discourse referent of the
head with that of the adjunct. However, unlike DGL,
this is compiled into the category for the adjunct.
</bodyText>
<listItem confidence="0.685615">
(25) in ` (s : @i(p^ [r]hLOCi(j^q))ns:@ip)/ninf=r:@jq
</listItem>
<bodyText confidence="0.826187">
To derive the sentence Ed sleeps in the bed (see
Figure 1), we then need the following further entries:
</bodyText>
<listItem confidence="0.969493">
(26) the ` ninf=CB:p/ninf=NB:p
(27) bed ` ninf=NB : @d3bed
</listItem>
<bodyText confidence="0.9994635">
This approach thus allows adjuncts to insert their
semantic import into the meaning of the head, mak-
ing use of nominals in a manner similar to the use of
indexes in Unification Categorial Grammar.
</bodyText>
<sectionHeader confidence="0.982685" genericHeader="evaluation">
5 Intonation and Information Structure
</sectionHeader>
<bodyText confidence="0.997955666666667">
Information Structure (IS) in English is in part deter-
mined by intonation. For example, given the ques-
tion in (28), an appropriate response would be (29).2
</bodyText>
<listItem confidence="0.904116333333333">
(28) I know what Ed READ. But what did Ed
WRITE?
(29) (Ed WROTE) (A BOOK).
</listItem>
<subsectionHeader confidence="0.361868">
L+H* LH% H* LL%
</subsectionHeader>
<bodyText confidence="0.999708166666667">
Steedman (2000a) incorporates intonation into
CCG syntactic analyses to determine the contribu-
tion of different constituents to IS. Steedman calls
segments such as Ed wrote of (29) the theme of the
sentence, and a book the rheme. The former indi-
cates the part of the utterance that connects it with
the preceding discourse, whereas the latter provides
information that moves the discourse forward.
In the context of Discourse Representation The-
ory, Kruijff-Korbayov´a (1998) represents IS by
splitting DRT structures into a topic/focus articula-
tion of the form TOPIC N FOCUS. We represent
</bodyText>
<footnote confidence="0.988348333333333">
2Following Pierrehumbert’s notation, the intonational con-
tour L+H* indicates a low-rising pitch accent, H* a sharply-
rising pitch accent, and both LH% and LL% are boundary tones.
</footnote>
<equation confidence="0.889709">
Ed sleeps (= (24)) in the bed
s : @h2([NB]sleepn[CB](ACT)(d1nEd)) s : @i(pn[r](LOC)(jnq))\s:@ip)1ninf=r:@jq ninf=CB:s1ninf =NB:s ninf =NB:@d3bed
�
s : @i(pn [CB](LOC)(d3nbed))\s:@ip
s : @h2([NB]sleep n [CB](ACT)(d1nEd) n [CB](LOC)(d3nbed))
</equation>
<figureCaption confidence="0.993596">
Figure 1: Derivation of Ed sleeps in the bed.
</figureCaption>
<equation confidence="0.826018">
ninf=CB : @d3bed
</equation>
<bodyText confidence="0.980152666666667">
this in HLDS as a term incorporating the m opera-
tor. Equating topic and focus with Steedman’s theme
and rheme, we encode the interpretation of (29) as:
</bodyText>
<equation confidence="0.5060305">
(30) @h7([CB]write A [CB](ACT)(d1AEd)
M [NB](PAT)(d4Abook))
</equation>
<bodyText confidence="0.9994755">
DGL builds such structures by using a rewriting sys-
tem to produce terms with topic/focus articulation
from the terms produced by the syntax.
Steedman uses the pitch accents to produce lexi-
cal entries with values for the INFORMATION fea-
ture, which we call here sinf. L+H* and H* set
the value of this feature as θ (for theme) or ρ
(for rheme), respectively. He also employs cate-
gories for the boundary tones that carry blocking
values for sinf which stop incomplete intonational
phrases from combining with others, thereby avoid-
ing derivations for utterances with nonsensical into-
nation contours.
Our approach is to incorporate the syntactic as-
pects of Steedman’s analysis with DGL’s rewriting
system for using informativity to partition senten-
tial meaning. In addition to using the syntactic fea-
ture sinf, we allow intonation marking to instantiate
the values of the semantic informativity feature inf.
Thus, we have the following sort of entry:
</bodyText>
<equation confidence="0.821873333333333">
(31) WROTE (L+H*) �-
ssinf=θ:φ\ninf=w�sinf=θ:@ip/ninf=x�sinf=θ:@jq
φ=@h2([CB]writen[w](ACT)(inp)n[x](PAT)(jnq))
</equation>
<bodyText confidence="0.999950666666667">
We therefore straightforwardly reap the syntactic
benefits of Steedman’s intonation analysis, while IS
itself is determined via DGL’s logical form rewrit-
ing system operating on the modal indications of
informativity produced during the derivation. The
articulation of IS can thus be performed uniformly
across languages, which use a variety of strategies
including intonation, morphology, and word order
variation to mark the informativity of different el-
ements. The resulting logical form plugs directly
into DGL’s architecture for incorporating sentence
meaning with the discourse.
</bodyText>
<sectionHeader confidence="0.999387" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999930454545455">
Since it is couched in hybrid logic, HLDS is ide-
ally suited to be logically engineered to the task at
hand. Hybrid logic can be made to do exactly what
we want, answering to the linguistic intuitions we
want to formalize without yielding its core assets – a
rich propositional ontology, decidability, and favor-
able computational complexity.
Various aspects of meaning, like dependency re-
lations, contextual reference, tense and aspect, and
information structure can be perspicuously encoded
with HLDS, and the resulting representations can
be built compositionally using CCG. CCG has close
affinities with dependency grammar, and it provides
a competitive and explanatorily adequate basis for
a variety of phenomena ranging from coordination
and unbounded dependencies to information struc-
ture. Nonetheless, the approach we describe could
in principle be fit into other unification-based frame-
works like Head-Driven Phrase Structure Grammar.
Hybrid logic’s utility does not stop with senten-
tial meaning. It can also be used to model dis-
course interpretation and is closely related to log-
ics for knowledge representation. This way we can
cover the track from grammar to discourse with a
single meaning formalism. We do not need to trans-
late or make simplifying assumptions for different
processing modules to communicate, and we can
freely include and use information across different
levels of meaning.
We have implemented a (preliminary) Java pack-
age for creating and manipulating hybrid logic terms
and connected it to Grok, a CCG parsing system.3
The use of HLDS has made it possible to improve
</bodyText>
<footnote confidence="0.9995545">
3The software is available at http://opennlp.sf.net
and http://grok.sf.net under an open source license.
</footnote>
<bodyText confidence="0.999964782608696">
the representation of the lexicon. Hybrid logic nom-
inals provide a convenient and intuitive manner of
localizing parts of a semantic structure, which has
made it possible to greatly simplify the use of inher-
itance in the lexicon. Logical forms are created as
an accumulation of different levels in the hierarchy
including morphological information. This is partic-
ularly important since the system does not otherwise
support typed feature structures with inheritance.
Hybrid logics provide a perspicuous logical lan-
guage for representing structures in temporal logic,
description logic, AVMs, and indeed any relational
structure. Terms of HLDS can thus be marshalled
into terms of these other representations with the
potential of taking advantage of tools developed for
them or providing input to modules expecting them.
In future work, we intend to combine techniques
for building wide-coverage statistical parsers for
CCG (Hockenmaier and Steedman, 2002; Clark et
al., 2002) with corpora that have explicitly marked
semantic dependency relations (such as the Prague
Dependency Treebank and NEGRA) to produce
HLDS terms as the parse output.
</bodyText>
<sectionHeader confidence="0.994433" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.98483525">
We would like to thank Patrick Blackburn, Johan Bos, Nissim
Francez, Alex Lascarides, Mark Steedman, Bonnie Webber and
the ACL reviewers for helpful comments on earlier versions of
this paper. All errors are, of course, our own. Jason Baldridge’s
work is supported in part by Overseas Research Student Award
ORS/98014014. Geert-Jan Kruijff’s work is supported by the
DFG Sonderforschungsbereich 378 Resource-Sensitive Cogni-
tive Processes, Project NEGRA EM6.
</bodyText>
<sectionHeader confidence="0.998998" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998901875">
Natasha Alechina. 1995. Modal Quantifiers. Ph.D. thesis, Uni-
versity of Amsterdam, Amsterdam, The Netherlands.
Carlos Areces, Patrick Blackburn, and Maarten Marx. 1999. A
road-map on complexity for hybrid logics. In J. Flum and
M. Rodriguez-Artalejo, editors, Computer Science Logic,
number 1683 in Lecture Notes in Computer Science, pages
307–321. Springer-Verlag.
Carlos Areces. 2000. Logic Engineering. The Case ofDescrip-
tion and Hybrid Logics. Ph.D. thesis, University of Amster-
dam, Amsterdam, The Netherlands.
Patrick Blackburn and Alex Lascarides. 1992. Sorts and oper-
ators for temporal semantics. In Proc. of the Fourth Sympo-
sium on Logic and Language, Budapest, Hungary.
Patrick Blackburn. 1994. Tense, temporal reference and tense
logic. Journal of Semantics, 11:83–101.
Patrick Blackburn. 2000. Representation, reasoning, and rela-
tional structures: a hybrid logic manifesto. Logic Journal of
the IGPL, 8(3):339–625.
Stephen Clark, Julia Hockenmaier, and Mark Steedman. 2002.
Building deep dependency structures using a wide-coverage
CCG parser. In Proc. of the 40th Annual Meeting of the As-
sociation of Computational Linguistics, Philadelphia, PA.
Ann Copestake, Dan Flickinger, Ivan Sag, and Carl Pollard.
1999. Minimal recursion semantics: An introduction. ms,
www-csli.stanford.edu/˜aac/newmrs.ps.
Ann Copestake, Alex Lascarides, and Dan Flickinger. 2001.
An algebra for semantic construction in constraint-based
grammars. In Proc. of the 39th Annual Meeting of the
Association of Computational Linguistics, pages 132–139,
Toulouse, France.
Julia Hockenmaier and Mark Steedman. 2002. Generative
models for statistical parsing with combinatory categorial
grammar. In Proc. of the 40th Annual Meeting of the As-
sociation of Computational Linguistics, Philadelphia, PA.
Geert-Jan M. Kruijff and Ivana Kruijff-Korbayov´a. 2001. A
hybrid logic formalization of information structure sensitive
discourse interpretation. In Proc. of the Fourth Workshop
on Text, Speech and Dialogue, volume 2166 of LNCS/LNAI,
pages 31–38. Springer-Verlag.
Ivana Kruijff-Korbayov´a. 1998. The Dynamic Potential of
Topic and Focus: A Praguian Approach to Discourse Repre-
sentation Theory. Ph.D. thesis, Charles University, Prague,
Czech Republic.
Geert-Jan M. Kruijff. 2001. A Categorial Modal Architec-
ture of Informativity: Dependency Grammar Logic &amp; Infor-
mation Structure. Ph.D. thesis, Charles University, Prague,
Czech Republic.
Marc Moens and Mark Steedman. 1988. Temporal ontology
and temporal reference. Computational Linguistics, 14:15–
28.
Michael Moortgat. 1997. Categorial type logics. In Johan van
Benthem and Alice ter Meulen, editors, Handbook of Logic
and Language. Elsevier Science B.V.
Glyn V. Morrill. 1994. Type Logical Grammar: Categorial
Logic of Signs. Kluwer Academic Publishers, Dordrecht,
Boston, London.
Mark Steedman. 2000a. Information structure and the syntax-
phonology interface. Linguistic Inquiry, 34:649–689.
Mark Steedman. 2000b. The Syntactic Process. The MIT
Press, Cambridge Mass.
Henk Zeevat. 1988. Combining categorial grammar and unifi-
cation. In Uwe Reyle and Christian Rohrer, editors, Natural
Language Parsing and Linguistic Theories, pages 202–229.
Reidel, Dordrecht.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.304349">
<note confidence="0.9979505">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 319-326.</note>
<title confidence="0.976962">Coupling CCG and Hybrid Logic Dependency Semantics</title>
<author confidence="0.99996">Jason Baldridge</author>
<affiliation confidence="0.9832325">ICCS Division of Informatics 2 Buccleuch Place University of Edinburgh</affiliation>
<address confidence="0.982883">Edinburgh, UK, EH8 9LW</address>
<email confidence="0.994576">jmb@cogsci.ed.ac.uk</email>
<author confidence="0.998816">Geert-Jan M Kruijff</author>
<affiliation confidence="0.9682">Universit¨at des Saarlandes Computational Linguistics Lehrstuhl Uszkoreit</affiliation>
<address confidence="0.9764035">Building 17, Postfach 15 11 50 66041 Saarbr¨ucken, Germany</address>
<email confidence="0.99676">gj@CoLi.Uni-SB.DE</email>
<abstract confidence="0.891526066666667">Categorial grammar has traditionally used to represent meaning. We present an alternative, dependency-based perspective on linguistic meaning and situate it in the computational setting. This perspective is formalized in terms of hybrid logic and has a rich yet perspicuous propositional ontology that enables a wide variety of semantic phenomena to be represented in a single meaning formalism. Finally, we show how we can couple this formalization to Combinatory Categorial Grammar to produce interpretations compositionally.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Natasha Alechina</author>
</authors>
<title>Modal Quantifiers.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Amsterdam,</institution>
<location>Amsterdam, The Netherlands.</location>
<contexts>
<context position="18392" citStr="Alechina (1995)" startWordPosition="2832" endWordPosition="2833"> MRS-style underspecification is thus replicated by declaring new nominals and modeling =q as a modal relation between nominals. When constructing the fully-scoped structures generated by an underspecified one, the =q constraints must be obeyed according to the qeq condition of Copestake etal. Because HLDS is couched directly in terms of hybrid logic, we can concisely declare the qeq condition as the following implication: (17) @i QEQ)j --* @ijV (@i BODY)kn@k QEQ)j) Alternatively, it would in principle be possible to adopt a truly modal solution to the representation of quantifiers. Following Alechina (1995), (generalized) quantification can be modeled as modal operators. The complexity of generalized quantification is then pushed into the model theory instead of forcing the representation to carry the burden. 4 CCG Coupled to HLDS In Dependency Grammar Logic (DGL), Kruijff (2001) couples HLDS to a resourcesensitive categorial proof theory (CTL) (Moortgat, 1997). Though DGL demonstrates a procedure for building HLDS terms from linguistic expressions, there are several problems we can overcome by switching to CCG. First, parsing with CCG grammars for substantial fragments is generally more efficie</context>
</contexts>
<marker>Alechina, 1995</marker>
<rawString>Natasha Alechina. 1995. Modal Quantifiers. Ph.D. thesis, University of Amsterdam, Amsterdam, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Areces</author>
<author>Patrick Blackburn</author>
<author>Maarten Marx</author>
</authors>
<title>A road-map on complexity for hybrid logics.</title>
<date>1999</date>
<booktitle>Computer Science Logic, number 1683 in Lecture Notes in Computer Science,</booktitle>
<pages>307--321</pages>
<editor>In J. Flum and M. Rodriguez-Artalejo, editors,</editor>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="8274" citStr="Areces et al., 1999" startWordPosition="1230" endWordPosition="1233"> This inability to state where exactly a proposition holds makes modal logic an inadequate representation framework for practical applications like knowledge representation (Areces, 2000) or temporal reasoning (Blackburn, 1994). Because of this, computational work in knowledge representation has usually involved re-engineering first-order logic to suit the task, e.g., the use of metapredicates such as Hold of Kowalski and Allen. Unfortunately, such logics are often undecidable. Hybrid logic extends standard modal logic while retaining decidability and favorable complexity (Areces, 2000) (cf. (Areces et al., 1999) for a complexity roadmap). The strategy is to add nominals, a new sort of basic formula with which we can explicitly name states in the object language. Next to propositions, nominals are first-class citizens of the object language: formulas can be formed using both sorts, standard boolean operators, and the satisfaction operator “@”. A formula @ip states that the formula p holds at the state named by i.1 (There are more powerful quantifiers ranging over nominals, such as , , but we do not consider them here.) With nominals we obtain the possibility to explicitly refer to the state at which a</context>
</contexts>
<marker>Areces, Blackburn, Marx, 1999</marker>
<rawString>Carlos Areces, Patrick Blackburn, and Maarten Marx. 1999. A road-map on complexity for hybrid logics. In J. Flum and M. Rodriguez-Artalejo, editors, Computer Science Logic, number 1683 in Lecture Notes in Computer Science, pages 307–321. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Areces</author>
</authors>
<title>Logic Engineering. The Case ofDescription and Hybrid Logics.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Amsterdam,</institution>
<location>Amsterdam, The Netherlands.</location>
<contexts>
<context position="7841" citStr="Areces, 2000" startWordPosition="1169" endWordPosition="1170">ogic extends modal logic, then look at the representation of linguistic meaning via hybrid logic terms. 3.1 Hybrid Logic Though modal logic provides a powerful tool for encoding relational structures and their properties, it contains a surprising inherent asymmetry: states (“worlds”) are at the heart of the model theory for modal logic, but there are no means to directly reference specific states using the object language. This inability to state where exactly a proposition holds makes modal logic an inadequate representation framework for practical applications like knowledge representation (Areces, 2000) or temporal reasoning (Blackburn, 1994). Because of this, computational work in knowledge representation has usually involved re-engineering first-order logic to suit the task, e.g., the use of metapredicates such as Hold of Kowalski and Allen. Unfortunately, such logics are often undecidable. Hybrid logic extends standard modal logic while retaining decidability and favorable complexity (Areces, 2000) (cf. (Areces et al., 1999) for a complexity roadmap). The strategy is to add nominals, a new sort of basic formula with which we can explicitly name states in the object language. Next to propo</context>
</contexts>
<marker>Areces, 2000</marker>
<rawString>Carlos Areces. 2000. Logic Engineering. The Case ofDescription and Hybrid Logics. Ph.D. thesis, University of Amsterdam, Amsterdam, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Blackburn</author>
<author>Alex Lascarides</author>
</authors>
<title>Sorts and operators for temporal semantics.</title>
<date>1992</date>
<booktitle>In Proc. of the Fourth Symposium on Logic and Language,</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="12990" citStr="Blackburn and Lascarides (1992)" startWordPosition="2000" endWordPosition="2003">del the meaning of the pronoun he as in (12). (12) @ihXSi(j ^ male) During discourse interpretation, this statement is evaluated against the discourse model. The pronoun is resolvable only if a state where male holds is XSaccessible in the discourse model. Different accessibility relations can be modeled, e.g. to distinguish a local context (for resolving reflexive anaphors like himself) from a global context (Kruijff, 2001). Finally, the rich temporal ontology underlying models of tense and aspect such as Moens and Steedman (1988) can be captured using the sorting strategy. Earlier work like Blackburn and Lascarides (1992) already explored such ideas. HLDS employs hybrid logic to integrate Moens and Steedman’s notion of the event nucleus directly into meaning representations. The event nucleus is a tripartite structure reflecting the underlying semantics of a type of event. The event is related to a preparation (an activity bringing the event about) and a consequent (a state ensuing to the event), which we encode as the modal relations PREP and CONS, respectively. Different kinds of states and events are modeled as different sorts ofnominals, shown in (13) using the notation introduced above. (13) @{Activity:e1</context>
</contexts>
<marker>Blackburn, Lascarides, 1992</marker>
<rawString>Patrick Blackburn and Alex Lascarides. 1992. Sorts and operators for temporal semantics. In Proc. of the Fourth Symposium on Logic and Language, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Blackburn</author>
</authors>
<title>Tense, temporal reference and tense logic.</title>
<date>1994</date>
<journal>Journal of Semantics,</journal>
<pages>11--83</pages>
<contexts>
<context position="7881" citStr="Blackburn, 1994" startWordPosition="1175" endWordPosition="1176">t the representation of linguistic meaning via hybrid logic terms. 3.1 Hybrid Logic Though modal logic provides a powerful tool for encoding relational structures and their properties, it contains a surprising inherent asymmetry: states (“worlds”) are at the heart of the model theory for modal logic, but there are no means to directly reference specific states using the object language. This inability to state where exactly a proposition holds makes modal logic an inadequate representation framework for practical applications like knowledge representation (Areces, 2000) or temporal reasoning (Blackburn, 1994). Because of this, computational work in knowledge representation has usually involved re-engineering first-order logic to suit the task, e.g., the use of metapredicates such as Hold of Kowalski and Allen. Unfortunately, such logics are often undecidable. Hybrid logic extends standard modal logic while retaining decidability and favorable complexity (Areces, 2000) (cf. (Areces et al., 1999) for a complexity roadmap). The strategy is to add nominals, a new sort of basic formula with which we can explicitly name states in the object language. Next to propositions, nominals are first-class citize</context>
</contexts>
<marker>Blackburn, 1994</marker>
<rawString>Patrick Blackburn. 1994. Tense, temporal reference and tense logic. Journal of Semantics, 11:83–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Blackburn</author>
</authors>
<title>Representation, reasoning, and relational structures: a hybrid logic manifesto.</title>
<date>2000</date>
<journal>Logic Journal of the IGPL,</journal>
<volume>8</volume>
<issue>3</issue>
<contexts>
<context position="1477" citStr="Blackburn, 2000" startWordPosition="203" endWordPosition="204">e represented in a single meaning formalism. Finally, we show how we can couple this formalization to Combinatory Categorial Grammar to produce interpretations compositionally. 1 Introduction The λ-calculus has enjoyed many years as the standard semantic encoding for categorial grammars and other grammatical frameworks, but recent work has highlighted its inadequacies for both linguistic and computational concerns of representing natural language semantics (Copestake et al., 1999; Kruijff, 2001). The latter couples a resource-sensitive categorial proof theory (Moortgat, 1997) to hybrid logic (Blackburn, 2000) to formalize a dependency-based perspective on meaning, which we call here Hybrid Logic Dependency Semantics (HLDS). In this paper, we situate HLDS in the computational context by explicating its properties as a framework for computational semantics and linking it to Combinatory Categorial Grammar (CCG). The structure of the paper is as follows. In §2, we briefly introduce CCG and how it links syntax and semantics, and then discuss semantic representations that use indexes to identify subparts of logical forms. §3 introduces HLDS and evaluates it with respect to the criteria of other computat</context>
<context position="6933" citStr="Blackburn, 2000" startWordPosition="1035" endWordPosition="1036">al. argue that these flat representations facilitate a number of computational tasks, including machine translation and generation, without sacrificing linguistic expressivity. Also, flatness permits semantic equivalences to be checked more easily than in structures with deeper embedding, and underspecification simplifies the work of the parser since it does not have to compute every possible reading for scope-bearing elements. 3 Hybrid Logic Dependency Semantics Kruijff (2001) proposes an alternative way to representing linguistically realized meaning: namely, as terms of hybrid modal logic (Blackburn, 2000) explicitly encoding the dependency relations between heads and dependents, spatio-temporal structure, contextual reference, and information structure. We call this unified perspective combining many levels of meaning Hybrid Logic Dependency Semantics (HLDS). We begin by discussing how hybrid logic extends modal logic, then look at the representation of linguistic meaning via hybrid logic terms. 3.1 Hybrid Logic Though modal logic provides a powerful tool for encoding relational structures and their properties, it contains a surprising inherent asymmetry: states (“worlds”) are at the heart of </context>
</contexts>
<marker>Blackburn, 2000</marker>
<rawString>Patrick Blackburn. 2000. Representation, reasoning, and relational structures: a hybrid logic manifesto. Logic Journal of the IGPL, 8(3):339–625.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Building deep dependency structures using a wide-coverage CCG parser.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="19182" citStr="Clark et al., 2002" startWordPosition="2948" endWordPosition="2951">representation to carry the burden. 4 CCG Coupled to HLDS In Dependency Grammar Logic (DGL), Kruijff (2001) couples HLDS to a resourcesensitive categorial proof theory (CTL) (Moortgat, 1997). Though DGL demonstrates a procedure for building HLDS terms from linguistic expressions, there are several problems we can overcome by switching to CCG. First, parsing with CCG grammars for substantial fragments is generally more efficient than with CTL grammars with similar coverage. Also, a wide-coverage statistical parser which produces syntactic dependency structures for English is available for CCG (Clark et al., 2002). Second, syntactic features (modeled by unary modalities) in CTL have no intuitive semantic reflection, whereas CCG can relate syntactic and semantic features perspicuously using unification. Finally, CCG has a detailed syntactic account of the realization of information structure in English. To link syntax and semantics in derivations, every logical form in DGL expresses a nominal identifying its head in the format @ip. This handles dependents in a linguistically motivated way through a linking theory: given the form of a dependent, its (possible) role is established, after which its meaning</context>
<context position="29282" citStr="Clark et al., 2002" startWordPosition="4528" endWordPosition="4531">particularly important since the system does not otherwise support typed feature structures with inheritance. Hybrid logics provide a perspicuous logical language for representing structures in temporal logic, description logic, AVMs, and indeed any relational structure. Terms of HLDS can thus be marshalled into terms of these other representations with the potential of taking advantage of tools developed for them or providing input to modules expecting them. In future work, we intend to combine techniques for building wide-coverage statistical parsers for CCG (Hockenmaier and Steedman, 2002; Clark et al., 2002) with corpora that have explicitly marked semantic dependency relations (such as the Prague Dependency Treebank and NEGRA) to produce HLDS terms as the parse output. Acknowledgements We would like to thank Patrick Blackburn, Johan Bos, Nissim Francez, Alex Lascarides, Mark Steedman, Bonnie Webber and the ACL reviewers for helpful comments on earlier versions of this paper. All errors are, of course, our own. Jason Baldridge’s work is supported in part by Overseas Research Student Award ORS/98014014. Geert-Jan Kruijff’s work is supported by the DFG Sonderforschungsbereich 378 Resource-Sensitive</context>
</contexts>
<marker>Clark, Hockenmaier, Steedman, 2002</marker>
<rawString>Stephen Clark, Julia Hockenmaier, and Mark Steedman. 2002. Building deep dependency structures using a wide-coverage CCG parser. In Proc. of the 40th Annual Meeting of the Association of Computational Linguistics, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
<author>Ivan Sag</author>
<author>Carl Pollard</author>
</authors>
<title>Minimal recursion semantics: An introduction. ms, www-csli.stanford.edu/˜aac/newmrs.ps.</title>
<date>1999</date>
<contexts>
<context position="1345" citStr="Copestake et al., 1999" startWordPosition="183" endWordPosition="186">ized in terms of hybrid logic and has a rich yet perspicuous propositional ontology that enables a wide variety of semantic phenomena to be represented in a single meaning formalism. Finally, we show how we can couple this formalization to Combinatory Categorial Grammar to produce interpretations compositionally. 1 Introduction The λ-calculus has enjoyed many years as the standard semantic encoding for categorial grammars and other grammatical frameworks, but recent work has highlighted its inadequacies for both linguistic and computational concerns of representing natural language semantics (Copestake et al., 1999; Kruijff, 2001). The latter couples a resource-sensitive categorial proof theory (Moortgat, 1997) to hybrid logic (Blackburn, 2000) to formalize a dependency-based perspective on meaning, which we call here Hybrid Logic Dependency Semantics (HLDS). In this paper, we situate HLDS in the computational context by explicating its properties as a framework for computational semantics and linking it to Combinatory Categorial Grammar (CCG). The structure of the paper is as follows. In §2, we briefly introduce CCG and how it links syntax and semantics, and then discuss semantic representations that u</context>
<context position="5258" citStr="Copestake et al., 1999" startWordPosition="797" endWordPosition="800">ication-based frameworks and theories for discourse representation. InL attaches one to every formula representing its discourse referent. This results in a representation such as (4) for the sentence Ed came to the party. (4) [e][party(x),past(e),to(e,x),come(e,Ed)] InL thus flattens logical forms to some extent, using the indexes to spread a given entity or event through multiple predications. The use of indexes is crucial for UCG’s account of modifiers, and as we will see later, we exploit such referents to achieve similar ends when coupling HLDS and CCG. Minimal Recursion Semantics (MRS) (Copestake et al., 1999; Copestake et al., 2001) is a framework for computational semantics that is designed to simplify the work of algorithms which produce or use semantic representations. MRS provides the means to represent interpretations with a flat, underspecified semantics using terms of the predicate calculus and generalized quantifiers. Flattening is achieved by using an indexation scheme involving labels that tag particular groups of elementary predications (EPs) and handles (here, h1,h2,...) that reference those EPs. Underspecification is achieved by using unresolved handles as the arguments for scope-bea</context>
<context position="14459" citStr="Copestake et al. (1999)" startWordPosition="2229" endWordPosition="2232">event nucleus in (13), the representation in (11) becomes (14), where the event is thus located at a specific time in the past. (14) @h1(E(13) ^ write ^ hACTi(d0^Ed) ^ hPATi(d5^book^hGRi(d7^long)) ^ hLOCi(d9^London)) ^@h1fAchievement:e2g^hPifAchievement:e2g Hybrid logic’s flexibility makes it amenable to representing a wide variety of semantic phenomena in a propositional setting, and it can furthermore be used to formulate a discourse theory (Kruijff and Kruijff-Korbayov´a, 2001). 3.3 Comparison to MRS Here we consider the properties of HLDS with respect to the four main criteria laid out by Copestake et al. (1999) which a computational semantics framework must meet: expressive adequacy, grammatical compatibility, computational tractability, and underspecifiability. Expressive adequacy refers to a framework’s ability to correctly express linguistic meaning. HLDS was designed not only with this in mind, but as its central tenet. In addition to providing the means to represent the usual predicate-valency relations, it explicitly marks the named dependency relations between predicates and their arguments and modifiers. These different dependency relations are not just labels: they all have unique semantic </context>
</contexts>
<marker>Copestake, Flickinger, Sag, Pollard, 1999</marker>
<rawString>Ann Copestake, Dan Flickinger, Ivan Sag, and Carl Pollard. 1999. Minimal recursion semantics: An introduction. ms, www-csli.stanford.edu/˜aac/newmrs.ps.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Alex Lascarides</author>
<author>Dan Flickinger</author>
</authors>
<title>An algebra for semantic construction in constraint-based grammars.</title>
<date>2001</date>
<booktitle>In Proc. of the 39th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>132--139</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="5283" citStr="Copestake et al., 2001" startWordPosition="801" endWordPosition="804"> and theories for discourse representation. InL attaches one to every formula representing its discourse referent. This results in a representation such as (4) for the sentence Ed came to the party. (4) [e][party(x),past(e),to(e,x),come(e,Ed)] InL thus flattens logical forms to some extent, using the indexes to spread a given entity or event through multiple predications. The use of indexes is crucial for UCG’s account of modifiers, and as we will see later, we exploit such referents to achieve similar ends when coupling HLDS and CCG. Minimal Recursion Semantics (MRS) (Copestake et al., 1999; Copestake et al., 2001) is a framework for computational semantics that is designed to simplify the work of algorithms which produce or use semantic representations. MRS provides the means to represent interpretations with a flat, underspecified semantics using terms of the predicate calculus and generalized quantifiers. Flattening is achieved by using an indexation scheme involving labels that tag particular groups of elementary predications (EPs) and handles (here, h1,h2,...) that reference those EPs. Underspecification is achieved by using unresolved handles as the arguments for scope-bearing elements and declari</context>
</contexts>
<marker>Copestake, Lascarides, Flickinger, 2001</marker>
<rawString>Ann Copestake, Alex Lascarides, and Dan Flickinger. 2001. An algebra for semantic construction in constraint-based grammars. In Proc. of the 39th Annual Meeting of the Association of Computational Linguistics, pages 132–139, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Generative models for statistical parsing with combinatory categorial grammar.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="29261" citStr="Hockenmaier and Steedman, 2002" startWordPosition="4524" endWordPosition="4527">phological information. This is particularly important since the system does not otherwise support typed feature structures with inheritance. Hybrid logics provide a perspicuous logical language for representing structures in temporal logic, description logic, AVMs, and indeed any relational structure. Terms of HLDS can thus be marshalled into terms of these other representations with the potential of taking advantage of tools developed for them or providing input to modules expecting them. In future work, we intend to combine techniques for building wide-coverage statistical parsers for CCG (Hockenmaier and Steedman, 2002; Clark et al., 2002) with corpora that have explicitly marked semantic dependency relations (such as the Prague Dependency Treebank and NEGRA) to produce HLDS terms as the parse output. Acknowledgements We would like to thank Patrick Blackburn, Johan Bos, Nissim Francez, Alex Lascarides, Mark Steedman, Bonnie Webber and the ACL reviewers for helpful comments on earlier versions of this paper. All errors are, of course, our own. Jason Baldridge’s work is supported in part by Overseas Research Student Award ORS/98014014. Geert-Jan Kruijff’s work is supported by the DFG Sonderforschungsbereich 3</context>
</contexts>
<marker>Hockenmaier, Steedman, 2002</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2002. Generative models for statistical parsing with combinatory categorial grammar. In Proc. of the 40th Annual Meeting of the Association of Computational Linguistics, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geert-Jan M Kruijff</author>
<author>Ivana Kruijff-Korbayov´a</author>
</authors>
<title>A hybrid logic formalization of information structure sensitive discourse interpretation.</title>
<date>2001</date>
<booktitle>In Proc. of the Fourth Workshop on Text, Speech and Dialogue,</booktitle>
<volume>2166</volume>
<pages>31--38</pages>
<publisher>Springer-Verlag.</publisher>
<marker>Kruijff, Kruijff-Korbayov´a, 2001</marker>
<rawString>Geert-Jan M. Kruijff and Ivana Kruijff-Korbayov´a. 2001. A hybrid logic formalization of information structure sensitive discourse interpretation. In Proc. of the Fourth Workshop on Text, Speech and Dialogue, volume 2166 of LNCS/LNAI, pages 31–38. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivana Kruijff-Korbayov´a</author>
</authors>
<title>The Dynamic Potential of Topic and Focus: A Praguian Approach to Discourse Representation Theory.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>Charles University,</institution>
<location>Prague, Czech Republic.</location>
<marker>Kruijff-Korbayov´a, 1998</marker>
<rawString>Ivana Kruijff-Korbayov´a. 1998. The Dynamic Potential of Topic and Focus: A Praguian Approach to Discourse Representation Theory. Ph.D. thesis, Charles University, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geert-Jan M Kruijff</author>
</authors>
<title>A Categorial Modal Architecture of Informativity:</title>
<date>2001</date>
<booktitle>Dependency Grammar Logic &amp; Information Structure. Ph.D. thesis,</booktitle>
<institution>Charles University,</institution>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1361" citStr="Kruijff, 2001" startWordPosition="187" endWordPosition="188">logic and has a rich yet perspicuous propositional ontology that enables a wide variety of semantic phenomena to be represented in a single meaning formalism. Finally, we show how we can couple this formalization to Combinatory Categorial Grammar to produce interpretations compositionally. 1 Introduction The λ-calculus has enjoyed many years as the standard semantic encoding for categorial grammars and other grammatical frameworks, but recent work has highlighted its inadequacies for both linguistic and computational concerns of representing natural language semantics (Copestake et al., 1999; Kruijff, 2001). The latter couples a resource-sensitive categorial proof theory (Moortgat, 1997) to hybrid logic (Blackburn, 2000) to formalize a dependency-based perspective on meaning, which we call here Hybrid Logic Dependency Semantics (HLDS). In this paper, we situate HLDS in the computational context by explicating its properties as a framework for computational semantics and linking it to Combinatory Categorial Grammar (CCG). The structure of the paper is as follows. In §2, we briefly introduce CCG and how it links syntax and semantics, and then discuss semantic representations that use indexes to id</context>
<context position="6799" citStr="Kruijff (2001)" startWordPosition="1016" endWordPosition="1017">h0,{h1:every(x,h2,h3),h4:dog(x), h11:cat(y),h8:some(y,h9,h10), h11:white(y),h7:chase(x,y)1, {h0=qh7, h2=qh4, h9=qh111) Copestake et al. argue that these flat representations facilitate a number of computational tasks, including machine translation and generation, without sacrificing linguistic expressivity. Also, flatness permits semantic equivalences to be checked more easily than in structures with deeper embedding, and underspecification simplifies the work of the parser since it does not have to compute every possible reading for scope-bearing elements. 3 Hybrid Logic Dependency Semantics Kruijff (2001) proposes an alternative way to representing linguistically realized meaning: namely, as terms of hybrid modal logic (Blackburn, 2000) explicitly encoding the dependency relations between heads and dependents, spatio-temporal structure, contextual reference, and information structure. We call this unified perspective combining many levels of meaning Hybrid Logic Dependency Semantics (HLDS). We begin by discussing how hybrid logic extends modal logic, then look at the representation of linguistic meaning via hybrid logic terms. 3.1 Hybrid Logic Though modal logic provides a powerful tool for en</context>
<context position="12063" citStr="Kruijff (2001)" startWordPosition="1858" endWordPosition="1859">ent we associate a nominal di, representing its discourse referent. Technically, (10) states that each nominal di names the state where a dependent expressed as a proposition depi should be evaluated and is a 8i successor of h, the nominal identifying the head. As an example, the sentence Ed wrote a long book in London receives the represention in (11). (9) 26664 35777 (11) @h1(write ^ hACTi(d0^Ed) ^ hPATi(d5^book^hGRi(d7^long)) ^ hLOCi(d9^London)) The modal relations ACT, PAT, LOC, and GR stand for the dependency relations Actor, Patient, Locative, and General Relationship, respectively. See Kruijff (2001) for the model-theoretic interpretation of expressions such as (11). Contextual reference can be modeled as a statement that from the current state (anaphor) there should be an accessible antecedent state at which particular conditions hold. Thus, assuming an accessibility relation XS, we can model the meaning of the pronoun he as in (12). (12) @ihXSi(j ^ male) During discourse interpretation, this statement is evaluated against the discourse model. The pronoun is resolvable only if a state where male holds is XSaccessible in the discourse model. Different accessibility relations can be modele</context>
<context position="15420" citStr="Kruijff (2001)" startWordPosition="2370" endWordPosition="2371">e means to represent the usual predicate-valency relations, it explicitly marks the named dependency relations between predicates and their arguments and modifiers. These different dependency relations are not just labels: they all have unique semantic imports which project new relations in the context of different heads. HLDS also tackles the representation of tense and aspect, contextual reference, and information structure, as well as their interaction with discourse. The criterion of grammatical compatibility requires that a framework be linkable to other kinds of grammatical information. Kruijff (2001) shows that HLDS can be coupled to a rich grammatical framework, and in x4 we demonstrate that it can be tied to CCG, a much lower power formalism than that assumed by Kruijff. It should furthermore be straightforward to use our approach to hook HLDS up to other unification-based frameworks. The definition of computational tractability states that it must be possible to check semantic equivalence of different formulas straightforwardly. Like MRS, HLDS provides the means to view linguistic meaning in a flattened format and thereby ease the checking of equivalence. For example, (15) describes th</context>
<context position="18670" citStr="Kruijff (2001)" startWordPosition="2875" endWordPosition="2876">Copestake etal. Because HLDS is couched directly in terms of hybrid logic, we can concisely declare the qeq condition as the following implication: (17) @i QEQ)j --* @ijV (@i BODY)kn@k QEQ)j) Alternatively, it would in principle be possible to adopt a truly modal solution to the representation of quantifiers. Following Alechina (1995), (generalized) quantification can be modeled as modal operators. The complexity of generalized quantification is then pushed into the model theory instead of forcing the representation to carry the burden. 4 CCG Coupled to HLDS In Dependency Grammar Logic (DGL), Kruijff (2001) couples HLDS to a resourcesensitive categorial proof theory (CTL) (Moortgat, 1997). Though DGL demonstrates a procedure for building HLDS terms from linguistic expressions, there are several problems we can overcome by switching to CCG. First, parsing with CCG grammars for substantial fragments is generally more efficient than with CTL grammars with similar coverage. Also, a wide-coverage statistical parser which produces syntactic dependency structures for English is available for CCG (Clark et al., 2002). Second, syntactic features (modeled by unary modalities) in CTL have no intuitive sema</context>
</contexts>
<marker>Kruijff, 2001</marker>
<rawString>Geert-Jan M. Kruijff. 2001. A Categorial Modal Architecture of Informativity: Dependency Grammar Logic &amp; Information Structure. Ph.D. thesis, Charles University, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Moens</author>
<author>Mark Steedman</author>
</authors>
<title>Temporal ontology and temporal reference.</title>
<date>1988</date>
<journal>Computational Linguistics,</journal>
<volume>14</volume>
<pages>28</pages>
<contexts>
<context position="12896" citStr="Moens and Steedman (1988)" startWordPosition="1986" endWordPosition="1989">which particular conditions hold. Thus, assuming an accessibility relation XS, we can model the meaning of the pronoun he as in (12). (12) @ihXSi(j ^ male) During discourse interpretation, this statement is evaluated against the discourse model. The pronoun is resolvable only if a state where male holds is XSaccessible in the discourse model. Different accessibility relations can be modeled, e.g. to distinguish a local context (for resolving reflexive anaphors like himself) from a global context (Kruijff, 2001). Finally, the rich temporal ontology underlying models of tense and aspect such as Moens and Steedman (1988) can be captured using the sorting strategy. Earlier work like Blackburn and Lascarides (1992) already explored such ideas. HLDS employs hybrid logic to integrate Moens and Steedman’s notion of the event nucleus directly into meaning representations. The event nucleus is a tripartite structure reflecting the underlying semantics of a type of event. The event is related to a preparation (an activity bringing the event about) and a consequent (a state ensuing to the event), which we encode as the modal relations PREP and CONS, respectively. Different kinds of states and events are modeled as dif</context>
</contexts>
<marker>Moens, Steedman, 1988</marker>
<rawString>Marc Moens and Mark Steedman. 1988. Temporal ontology and temporal reference. Computational Linguistics, 14:15– 28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Moortgat</author>
</authors>
<title>Categorial type logics.</title>
<date>1997</date>
<booktitle>In Johan van Benthem and Alice ter Meulen, editors, Handbook of Logic and Language. Elsevier Science B.V.</booktitle>
<contexts>
<context position="1443" citStr="Moortgat, 1997" startWordPosition="198" endWordPosition="199">ariety of semantic phenomena to be represented in a single meaning formalism. Finally, we show how we can couple this formalization to Combinatory Categorial Grammar to produce interpretations compositionally. 1 Introduction The λ-calculus has enjoyed many years as the standard semantic encoding for categorial grammars and other grammatical frameworks, but recent work has highlighted its inadequacies for both linguistic and computational concerns of representing natural language semantics (Copestake et al., 1999; Kruijff, 2001). The latter couples a resource-sensitive categorial proof theory (Moortgat, 1997) to hybrid logic (Blackburn, 2000) to formalize a dependency-based perspective on meaning, which we call here Hybrid Logic Dependency Semantics (HLDS). In this paper, we situate HLDS in the computational context by explicating its properties as a framework for computational semantics and linking it to Combinatory Categorial Grammar (CCG). The structure of the paper is as follows. In §2, we briefly introduce CCG and how it links syntax and semantics, and then discuss semantic representations that use indexes to identify subparts of logical forms. §3 introduces HLDS and evaluates it with respect</context>
<context position="18753" citStr="Moortgat, 1997" startWordPosition="2887" endWordPosition="2888">concisely declare the qeq condition as the following implication: (17) @i QEQ)j --* @ijV (@i BODY)kn@k QEQ)j) Alternatively, it would in principle be possible to adopt a truly modal solution to the representation of quantifiers. Following Alechina (1995), (generalized) quantification can be modeled as modal operators. The complexity of generalized quantification is then pushed into the model theory instead of forcing the representation to carry the burden. 4 CCG Coupled to HLDS In Dependency Grammar Logic (DGL), Kruijff (2001) couples HLDS to a resourcesensitive categorial proof theory (CTL) (Moortgat, 1997). Though DGL demonstrates a procedure for building HLDS terms from linguistic expressions, there are several problems we can overcome by switching to CCG. First, parsing with CCG grammars for substantial fragments is generally more efficient than with CTL grammars with similar coverage. Also, a wide-coverage statistical parser which produces syntactic dependency structures for English is available for CCG (Clark et al., 2002). Second, syntactic features (modeled by unary modalities) in CTL have no intuitive semantic reflection, whereas CCG can relate syntactic and semantic features perspicuous</context>
<context position="22040" citStr="Moortgat, 1997" startWordPosition="3418" endWordPosition="3419">(d1AEd)) To produce HLDS terms that are fully compatible with the way that Kruijff and Kruijff-Korbayov´a (2001) model discourse, we need to mark the informativity of dependents as contextually bound (CB) and contextually nonbound (NB). In DGL, these appear as modalities in logical forms that are used to create a topic-focus articulation that is merged with the discourse context. For example, the sentence he wrote a book would receive the following (simplified) interpretation: (21) @h1([NB]write A [NB](PAT)(d5Abook) A [CB](ACT)(d6 A (XS)(d3Amale))) DGL uses feature-resolving unary modalities (Moortgat, 1997) to instantiate the values of informativity. In unification-based approaches such as CCG, the transferal of feature information into semantic representations is standard practice. We thus employ the feature inf and mark informativity in logical forms with values resolved syntactically. (22) Ed ` ninf=CB : @d1Ed (23) sleeps ` s : @h2([NB]sleep ^ [q]hACTi(i^p))nninf=q:@ip Combining these entries using backward application gives the following result for Ed sleeps: (24) s : @h2([NB]sleep ^ [CB]hACTi(d1^Ed)) A major benefit of having nominals in our representations comes with adjuncts. With HLDS, w</context>
</contexts>
<marker>Moortgat, 1997</marker>
<rawString>Michael Moortgat. 1997. Categorial type logics. In Johan van Benthem and Alice ter Meulen, editors, Handbook of Logic and Language. Elsevier Science B.V.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glyn V Morrill</author>
</authors>
<title>Type Logical Grammar: Categorial Logic of Signs.</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht, Boston, London.</location>
<contexts>
<context position="2469" citStr="Morrill, 1994" startWordPosition="358" endWordPosition="359">G and how it links syntax and semantics, and then discuss semantic representations that use indexes to identify subparts of logical forms. §3 introduces HLDS and evaluates it with respect to the criteria of other computational semantics frameworks. §4 shows how we can build HLDS terms using CCG with unification and §5 shows how intonation and information structure can be incorporated into the approach. 2 Indexed semantic representations Traditionally, categorial grammar has captured meaning using a (simply typed) λ-calculus, building semantic structure in parallel to the categorial inference (Morrill, 1994; Moortgat, 1997; Steedman, 2000b). For example, a (simplified) CCG lexical entry for a verb such as wrote is given in (1). (1) wrote �- (s\n)/n : λx.λy.write(y,x) Rules of combination are defined to operate on both categories and λ-terms simultaneously. For example, the rules allow the following derivation for Ed wrote books. (2) Ed wrote books s : write(Ed,books) Derivations like (2) give rise to the usual sort of predicate-argument structure whereby the order in which the arguments appear (and are bound by the λ’s) is essentially constitutive of their meaning. Thus, the first argument could</context>
</contexts>
<marker>Morrill, 1994</marker>
<rawString>Glyn V. Morrill. 1994. Type Logical Grammar: Categorial Logic of Signs. Kluwer Academic Publishers, Dordrecht, Boston, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>Information structure and the syntaxphonology interface. Linguistic Inquiry,</title>
<date>2000</date>
<pages>34--649</pages>
<contexts>
<context position="2501" citStr="Steedman, 2000" startWordPosition="362" endWordPosition="363">emantics, and then discuss semantic representations that use indexes to identify subparts of logical forms. §3 introduces HLDS and evaluates it with respect to the criteria of other computational semantics frameworks. §4 shows how we can build HLDS terms using CCG with unification and §5 shows how intonation and information structure can be incorporated into the approach. 2 Indexed semantic representations Traditionally, categorial grammar has captured meaning using a (simply typed) λ-calculus, building semantic structure in parallel to the categorial inference (Morrill, 1994; Moortgat, 1997; Steedman, 2000b). For example, a (simplified) CCG lexical entry for a verb such as wrote is given in (1). (1) wrote �- (s\n)/n : λx.λy.write(y,x) Rules of combination are defined to operate on both categories and λ-terms simultaneously. For example, the rules allow the following derivation for Ed wrote books. (2) Ed wrote books s : write(Ed,books) Derivations like (2) give rise to the usual sort of predicate-argument structure whereby the order in which the arguments appear (and are bound by the λ’s) is essentially constitutive of their meaning. Thus, the first argument could be taken to correspond to the w</context>
<context position="3766" citStr="Steedman, 2000" startWordPosition="571" endWordPosition="572">hat is being written. n:Ed (s\n)/n:λx.λy.write(y,x) n:books � s\n : λy.write(y,books) � One deficiency of %-calculus meaning representations is that they usually have to be type-raised to the worst case to fully model quantification, and this can reverberate and increase the complexity of syntactic categories since a verb like wrote will need to be able to take arguments with the types of generalized quantifiers. The approach we advocate in this paper does not suffer from this problem. For CCG, the use of the %-terms is simply a convenient device to bind arguments when presenting derivations (Steedman, 2000b). In implementations, a more common strategy is to compute semantic representations via unification, a tactic explicitly employed in Unification Categorial Grammar (UCG) (Zeevat, 1988). Using a unification paradigm in which atomic categories are bundles of syntactic and semantic information, we can use an entry such as (3) for wrote in place of (1). In the unification setting, (3) permits a derivation analogous to (2). (3) wrote �- (s : write(y,x)\n:y)/n:x For creating predicate-argument structures of this kind, strategies using either %-terms or unification to bind arguments are essentially</context>
<context position="23663" citStr="Steedman (2000" startWordPosition="3683" endWordPosition="3684">ee Figure 1), we then need the following further entries: (26) the ` ninf=CB:p/ninf=NB:p (27) bed ` ninf=NB : @d3bed This approach thus allows adjuncts to insert their semantic import into the meaning of the head, making use of nominals in a manner similar to the use of indexes in Unification Categorial Grammar. 5 Intonation and Information Structure Information Structure (IS) in English is in part determined by intonation. For example, given the question in (28), an appropriate response would be (29).2 (28) I know what Ed READ. But what did Ed WRITE? (29) (Ed WROTE) (A BOOK). L+H* LH% H* LL% Steedman (2000a) incorporates intonation into CCG syntactic analyses to determine the contribution of different constituents to IS. Steedman calls segments such as Ed wrote of (29) the theme of the sentence, and a book the rheme. The former indicates the part of the utterance that connects it with the preceding discourse, whereas the latter provides information that moves the discourse forward. In the context of Discourse Representation Theory, Kruijff-Korbayov´a (1998) represents IS by splitting DRT structures into a topic/focus articulation of the form TOPIC N FOCUS. We represent 2Following Pierrehumbert’</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000a. Information structure and the syntaxphonology interface. Linguistic Inquiry, 34:649–689.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge Mass.</location>
<contexts>
<context position="2501" citStr="Steedman, 2000" startWordPosition="362" endWordPosition="363">emantics, and then discuss semantic representations that use indexes to identify subparts of logical forms. §3 introduces HLDS and evaluates it with respect to the criteria of other computational semantics frameworks. §4 shows how we can build HLDS terms using CCG with unification and §5 shows how intonation and information structure can be incorporated into the approach. 2 Indexed semantic representations Traditionally, categorial grammar has captured meaning using a (simply typed) λ-calculus, building semantic structure in parallel to the categorial inference (Morrill, 1994; Moortgat, 1997; Steedman, 2000b). For example, a (simplified) CCG lexical entry for a verb such as wrote is given in (1). (1) wrote �- (s\n)/n : λx.λy.write(y,x) Rules of combination are defined to operate on both categories and λ-terms simultaneously. For example, the rules allow the following derivation for Ed wrote books. (2) Ed wrote books s : write(Ed,books) Derivations like (2) give rise to the usual sort of predicate-argument structure whereby the order in which the arguments appear (and are bound by the λ’s) is essentially constitutive of their meaning. Thus, the first argument could be taken to correspond to the w</context>
<context position="3766" citStr="Steedman, 2000" startWordPosition="571" endWordPosition="572">hat is being written. n:Ed (s\n)/n:λx.λy.write(y,x) n:books � s\n : λy.write(y,books) � One deficiency of %-calculus meaning representations is that they usually have to be type-raised to the worst case to fully model quantification, and this can reverberate and increase the complexity of syntactic categories since a verb like wrote will need to be able to take arguments with the types of generalized quantifiers. The approach we advocate in this paper does not suffer from this problem. For CCG, the use of the %-terms is simply a convenient device to bind arguments when presenting derivations (Steedman, 2000b). In implementations, a more common strategy is to compute semantic representations via unification, a tactic explicitly employed in Unification Categorial Grammar (UCG) (Zeevat, 1988). Using a unification paradigm in which atomic categories are bundles of syntactic and semantic information, we can use an entry such as (3) for wrote in place of (1). In the unification setting, (3) permits a derivation analogous to (2). (3) wrote �- (s : write(y,x)\n:y)/n:x For creating predicate-argument structures of this kind, strategies using either %-terms or unification to bind arguments are essentially</context>
<context position="23663" citStr="Steedman (2000" startWordPosition="3683" endWordPosition="3684">ee Figure 1), we then need the following further entries: (26) the ` ninf=CB:p/ninf=NB:p (27) bed ` ninf=NB : @d3bed This approach thus allows adjuncts to insert their semantic import into the meaning of the head, making use of nominals in a manner similar to the use of indexes in Unification Categorial Grammar. 5 Intonation and Information Structure Information Structure (IS) in English is in part determined by intonation. For example, given the question in (28), an appropriate response would be (29).2 (28) I know what Ed READ. But what did Ed WRITE? (29) (Ed WROTE) (A BOOK). L+H* LH% H* LL% Steedman (2000a) incorporates intonation into CCG syntactic analyses to determine the contribution of different constituents to IS. Steedman calls segments such as Ed wrote of (29) the theme of the sentence, and a book the rheme. The former indicates the part of the utterance that connects it with the preceding discourse, whereas the latter provides information that moves the discourse forward. In the context of Discourse Representation Theory, Kruijff-Korbayov´a (1998) represents IS by splitting DRT structures into a topic/focus articulation of the form TOPIC N FOCUS. We represent 2Following Pierrehumbert’</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000b. The Syntactic Process. The MIT Press, Cambridge Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henk Zeevat</author>
</authors>
<title>Combining categorial grammar and unification.</title>
<date>1988</date>
<booktitle>In Uwe Reyle</booktitle>
<pages>202--229</pages>
<editor>and Christian Rohrer, editors,</editor>
<location>Reidel, Dordrecht.</location>
<contexts>
<context position="3952" citStr="Zeevat, 1988" startWordPosition="597" endWordPosition="598"> the worst case to fully model quantification, and this can reverberate and increase the complexity of syntactic categories since a verb like wrote will need to be able to take arguments with the types of generalized quantifiers. The approach we advocate in this paper does not suffer from this problem. For CCG, the use of the %-terms is simply a convenient device to bind arguments when presenting derivations (Steedman, 2000b). In implementations, a more common strategy is to compute semantic representations via unification, a tactic explicitly employed in Unification Categorial Grammar (UCG) (Zeevat, 1988). Using a unification paradigm in which atomic categories are bundles of syntactic and semantic information, we can use an entry such as (3) for wrote in place of (1). In the unification setting, (3) permits a derivation analogous to (2). (3) wrote �- (s : write(y,x)\n:y)/n:x For creating predicate-argument structures of this kind, strategies using either %-terms or unification to bind arguments are essentially notational variants. However, UCG goes beyond simple predicateargument structures to instead use a semantic representation language called Indexed Language (InL). The idea of using inde</context>
</contexts>
<marker>Zeevat, 1988</marker>
<rawString>Henk Zeevat. 1988. Combining categorial grammar and unification. In Uwe Reyle and Christian Rohrer, editors, Natural Language Parsing and Linguistic Theories, pages 202–229. Reidel, Dordrecht.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>