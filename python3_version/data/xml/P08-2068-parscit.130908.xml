<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002675">
<title confidence="0.990869">
Text Segmentation with LDA-Based Fisher Kernel
</title>
<author confidence="0.953715">
Qi Sun, Runxin Li, Dingsheng Luo and Xihong Wu
</author>
<affiliation confidence="0.87698875">
Speech and Hearing Research Center, and
Key Laboratory of Machine Perception (Ministry of Education)
Peking University
100871, Beijing, China
</affiliation>
<email confidence="0.991985">
isunq,lirx,dsluo,wxhl@cis.pku.edu.cn
</email>
<sectionHeader confidence="0.995754" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.993820166666667">
In this paper we propose a domain-
independent text segmentation method,
which consists of three components. Latent
Dirichlet allocation (LDA) is employed to
compute words semantic distribution, and we
measure semantic similarity by the Fisher
kernel. Finally global best segmentation is
achieved by dynamic programming. Experi-
ments on Chinese data sets with the technique
show it can be effective. Introducing latent
semantic information, our algorithm is robust
on irregular-sized segments.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999977833333333">
The aim of text segmentation is to partition a doc-
ument into a set of segments, each of which is co-
herent about a specific topic. This task is inspired
by problems in information retrieval, summariza-
tion, and language modeling, in which the ability
to provide access to smaller, coherent segments in
a document is desired.
A lot of research has been done on text seg-
mentation. Some of them utilize linguistic criteria
(Beeferman et al., 1999; Mochizuki et al., 1998),
while others use statistical similarity measures to
uncover lexical cohesion. Lexical cohesion meth-
ods believe a coherent topic segment contains parts
with similar vocabularies. For example, the Text-
Tiling algorithm, introduced by (Hearst, 1994), as-
sumes that the local minima of the word similarity
curve are the points of low lexical cohesion and thus
the natural boundary candidates. (Reynar, 1998)
has proposed a method called dotplotting depending
on the distribution of word repetitions to find tight
regions of topic similarity graphically. One of the
problems with those works is that they treat terms
uncorrelated, assigning them orthogonal directions
in the feature space. But in reality words are corre-
lated, and sometimes even synonymous, so that texts
with very few common terms can potentially be on
closely related topics. So (Choi et al., 2001; Brants
et al., 2002) utilize semantic similarity to identify
cohesion. Unsupervised models of texts that capture
semantic information would be useful, particularly
if they could be achieved with a ”semantic kernel”
(Cristianini et al., 2001) , which computes the simi-
larity between texts by also considering relations be-
tween different terms. A Fisher kernel is a function
that measures the similarity between two data items
not in isolation, but rather in the context provided
by a probability distribution. In this paper, we use
the Fisher kernel to describe semantic information
similarity. In addition, (Fragkou et al., 2004; Ji and
Zha, 2004) has treated this task as an optimization
problem with global cost function and used dynamic
programming for segments selection.
The remainder of the paper is organized as fol-
lows. In section 2, after a brief overview of our
method, some key aspects of the algorithm are de-
scribed. In section 3, some experiments are pre-
sented. Finally conclusion and future research di-
rections are drawn in section 4.
</bodyText>
<sectionHeader confidence="0.995558" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.999638333333333">
This paper considers the sentence to be the smallest
unit, and a block b is the segment candidate which
consists of one or more sentences. We employ LDA
</bodyText>
<page confidence="0.984049">
269
</page>
<reference confidence="0.221711">
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 269–272,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</reference>
<bodyText confidence="0.999515857142857">
model (Blei et al., 2003) in order to find out latent
semantic topics in blocks, and LDA-based Fisher
kernel is used to measure the similarity of adjacent
blocks. Each block is then given a final score based
on its length and semantic similarity with its previ-
ous block. Finally the segmentation points are de-
cided by dynamic programming.
</bodyText>
<subsectionHeader confidence="0.873785">
2.1 LDA Model
</subsectionHeader>
<bodyText confidence="0.999936444444445">
We adopt LDA framework, which regards the cor-
pus as mixture of latent topics and uses document as
the unit of topic mixtures. In our method, the blocks
defined in previous paragraph are regarded as ”doc-
uments” in LDA model.
The LDA model defines two corpus-level parame-
ters α and Q. In its generative process, the marginal
distribution of a document p(d|α, Q) is given by the
following formula:
</bodyText>
<equation confidence="0.779294666666667">
f N
J p(B|α)(Y
n=1
</equation>
<bodyText confidence="0.999854444444444">
where d is a word sequence (w1, w2, ...wN) of
length N. α parameterizes a Dirichlet distribution
and derives the document-related random variable
Bd, then we choose a topic zk, k E 11...KI from the
multinomial distribution of Bd. Word probabilities
are parameterized by a k x V matrix Q with V being
the size of vocabulary and Qvk = P(w = v|zk). We
use variational EM (Blei et al., 2003) to estimate the
parameters.
</bodyText>
<subsectionHeader confidence="0.999835">
2.2 LDA-Based Fisher Kernel
</subsectionHeader>
<bodyText confidence="0.999949714285714">
In general, a kernel function k(x, y) is a way of mea-
suring the resemblance between two data items x
and y. The Fisher kernel’s key idea is to derive a ker-
nel function from a generative probability model. In
this paper we follow (Hofmann, 2000) to consider
the average log-probability of a block, utilizing the
LDA model. The likelihood of b is given by:
</bodyText>
<equation confidence="0.998058666666667">
l(b) = XN Pb(wi|b) log K Qwik�(k)
i=1 X b
k=1
</equation>
<bodyText confidence="0.9935948">
where the empirical distribution of words in the
block Pb (wi|b) can be obtained from the number of
word-block co-occurrence n(b, wi), normalized by
the length of the block.
The Fisher kernel is defined as
</bodyText>
<equation confidence="0.936327">
K(b1, b2) = 7Tθ l(b1)I−1 7θ l(b2)
</equation>
<bodyText confidence="0.99988875">
which engenders a measure of similarity between
any two blocks b1 and b2. The derivation of the
kernel is quite straightforward and following (Hof-
mann, 2000) we finally have the result:
</bodyText>
<equation confidence="0.9986314">
K(b1, b2) = K1(b1, b2) + K2(b1, b2), with
K1(b1, b2) = X 0(k)0(2)/0(k�b1 pus
k
P(zk|b1,wi)P (zk|b2,wi)
Pb (wi|b2) Pk P (wi|zk)
</equation>
<bodyText confidence="0.999982666666667">
where K1(b1, b2) is a measure of how much b1 and
b2 share the same latent topic, taking synonymy
into account. And K2(b1, b2) is the traditional inner
product of common term frequencies, but weighted
by the degree to which these terms belong to the
same latent topic, taking polysemy into account.
</bodyText>
<subsectionHeader confidence="0.999875">
2.3 Cost Function and Dynamic Programming
</subsectionHeader>
<bodyText confidence="0.9999313">
The local minima of LDA-based Fisher kernel sim-
ilarities indicate low semantic cohesion and seg-
mentation candidates, which is not enough to get
reasonably-sized segments. The lengths of segmen-
tation candidates have to be considered, thus we
build a cost function including two parts of infor-
mation. Segmentation points can be given in terms
of a vector t� = (t0, ..., tm, ..., tM), where tm is the
sentence label with m indicating the mth block. We
define a cost function as follows:
</bodyText>
<equation confidence="0.854037666666667">
M
J(t; A) = X AF(ltm+1,tm+1)
m=1
+ K(btm−1+1,tm, btm+1,tm+1)
where F(ltm+1,tm+1) is equal to (ltm+1,tm+1−µ)2 and
2σ2
</equation>
<bodyText confidence="0.998133909090909">
ltm+1,tm+1 is equal to tm+1−tm indicating the num-
ber of sentences in block m. The LDA-based ker-
nel function measures similarity of block m − 1 and
block m, where block m−1 spans sentence tm−1+1
to tm and block m spans sentence tm + 1 to tm+1
The cost function is the sum of the costs of as-
sumed unknown M segments, each of which is
made up of the length probability of block m and the
similarity score of block m with its previous block
m − 1. The optimal segmentation t� gives a global
minimum of J(t; A).
</bodyText>
<equation confidence="0.9850338">
X p(zk|ed)p(wn|zk, Q))d9
k
K2(b1, b2) =
P Pb(wi|b1)
i
</equation>
<page confidence="0.991389">
270
</page>
<sectionHeader confidence="0.99894" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.992092">
3.1 Preparation
</subsectionHeader>
<bodyText confidence="0.999994192307693">
In our experiments, we evaluate the performance of
our algorithms on Chinese corpus. With news docu-
ments from Chinese websites, collected from 10 dif-
ferent categories, we design an artificial test corpus
in the similar way of (Choi, 2000), in which we
take each n-sentence document as a coherent topic
segment, randomly choose ten such segments and
concatenate them as a sample. Three data sets, Set
3-5, Set 13-15 and Set 5-20, are prepared in our ex-
periments, each of which contains 100 samples. The
data sets’ names are represented by a range number
n of sentences in a segment.
Due to generality, we take three indices to eval-
uate our algorithm: precision, recall and error rate
metric (Beeferman et al., 1999) . And all exper-
imental results are averaged scores generated from
the individual results of different samples. In order
to determine appropriate parameters, some hold-out
data are used.
We compare the performance of our methods with
the algorithm in (Fragkou et al., 2004) on our test
set. In particular, the similarity representation is a
main difference between those two methods. While
we pay attention to latent topic information behind
words of adjacent blocks, (Fragkou et al., 2004) cal-
culates word density as the similarity score function.
</bodyText>
<subsectionHeader confidence="0.864323">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.986092529411765">
In order to demonstrate the improvement of LDA-
based Fisher kernel technique in text similarity eval-
uation, we omit the length probability part in the cost
function and compare the LDA-based Fisher kernel
and the word-frequency cosine similarity by the er-
ror rate Pk of segmenting texts. Figure 1 shows
the error rates for different sets of data. On av-
erage, the error rates are reduced by as much as
about 30% over word-frequency cosine similarity
with our methods, which shows Fisher kernel sim-
ilarity measure,with latent topic information added
by LDA, outperforms traditional word similarity
measure. The performance comparisons drawn from
Set 3-5 and Set 13-15 indicates that our similarity al-
gorithm can uncover more descriptive statistics than
traditional one especially for segments with less sen-
tences due to its prediction on latent topics.
</bodyText>
<figureCaption confidence="0.9972905">
Figure 1: Error Rate Pk on different data sets with differ-
ent similarity metrics.
</figureCaption>
<bodyText confidence="0.999723833333333">
In the cost function, there are three parameters p
, Q and A. We determine appropriate p and Q with
hold-out data. For the value of A, we take it between
0 and 1 because the length part is less important than
the similarity part according to our preliminary ex-
periments. We design the experiment to study A’s
impact on segmentation by varying it over a certain
range. Experimental results in Figure 2 show that
the reduce of error rate achieved by our algorithm
is in a range from 14.71% to 53.93%. Set 13-15
achieves best segmentation performance, which in-
dicates the importance of text structure: it is easier
to segment the topic with regular length and more
sentences. The performance on Set 5-20 obtains the
best improvement with our methods, which illus-
trates that LDA-based Fisher kernel can express text
similarity more exactly than word density similarity
on irregular-sized segments.
</bodyText>
<tableCaption confidence="0.982702">
Table 1: Evaluation against different algorithms on Set
5-20.
</tableCaption>
<table confidence="0.9968225">
Algo. Pk Recall Precision
TextTiling 0.226 66.00% 60.72 %
P. Fragkou Algo. 0.344 69.00% 37.92 %
Our Algo. 0.205 59.00% 62.27 %
</table>
<bodyText confidence="0.999494">
While most experiments of other authors were
taken on short regular-sized segments which was
firstly presented by (Choi, 2000), we use compar-
atively long range of segments, Set 5-20, to evaluate
different algorithms. Table 1 shows that, in terms of
</bodyText>
<note confidence="0.348437">
Pk
</note>
<page confidence="0.925794">
271
</page>
<figure confidence="0.733535">
lambda
</figure>
<figureCaption confidence="0.962433">
Figure 2: Error Rate Pk when the A changes. There are
two groups of lines, the solid lines representing algorithm
of (Fragkou et al., 2004) while the dash ones indicate
performance of our algorithm, and each line in a group
shows error rates in different data sets.
</figureCaption>
<bodyText confidence="0.999929636363636">
Pk, our algorithm employing dynamic programming
as P. Fragkou Algo. achieves the best performance
among those three. As for long irregular-sized text
segmentation, although local even-sized blocks sim-
ilarity provides more exact information than the sim-
ilarity between global irregular-sized texts, with the
consideration of latent topic information, the latter
will perform better in the task of text segmentation.
Though the performance of the proposed method is
not superior to TextTiling method, it avoids thresh-
olds selection, which makes it robust in applications.
</bodyText>
<sectionHeader confidence="0.997628" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999982666666667">
We present a new method for topic-based text seg-
mentation that yields better results than previously
methods. The method introduces a LDA-based
Fisher kernel to exploit text semantic similarities and
employs dynamic programming to obtain global op-
timization. Our algorithm is robust and insensitive
to the variation of segment length. In the future,
we plan to investigate more other similarity mea-
sures based on semantic information and to deal
with more complicated segmentation tasks. Also,
we want to exam the factor importance of similar-
ity and length in this text segmentation task.
</bodyText>
<sectionHeader confidence="0.998622" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999815111111111">
The authors would like to thank Jiazhong Nie for his help
and constructive suggestions. The work was supported
in part by the National Natural Science Foundation of
China (60435010; 60535030; 60605016), the National
High Technology Research and Development Program of
China (2006AA01Z196; 2006AA010103), the National
Key Basic Research Program of China (2004CB318005),
and the New-Century Training Program Foundation for
the Talents by the Ministry of Education of China.
</bodyText>
<sectionHeader confidence="0.998481" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999466454545454">
Doug Beeferman, Adam Berger and John D. Lafferty.
1999. Statistical Models for Text Segmentation. Ma-
chine Learning, 34(1-3):177–210.
David M. Blei and Andrew Y. Ng and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of machine
Learning Research 3: 993–1022.
Thorsten Brants, Francine Chen and Ioannis Tsochan-
taridis. 2002. Topic-Based Document Segmentation
with Probabilistic Latent Semantic Analysis. CIKM
’02211–218
Freddy Choi, Peter Wiemer-Hastings and Johanna
Moore. 2001. Latent Semantic Analysis for Text Seg-
mentation. Proceedings of 6th EMNLP, 109–117.
Freddy Y. Y. Choi. 2000. Advances in Domain Inde-
pendent Linear Text Segmentation. Proceedings of
NAACL-00.
Nello Cristianini, John Shawe-Taylor and Huma Lodhi.
2001. Latent Semantic Kernels. Proceedings of
ICML-01, 18th International Conference on Machine
Learning 66–73.
Pavlina Fragkou, Petridis Vassilios and Kehagias Athana-
sios. 2004. A Dynamic Programming Algorithm for
Linear Text Segmentation. J. Intell. Inf. Syst., 23(2):
179–197.
Marti Hearst. 1994. Multi-Paragraph Segmentation of
Expository Text. Proceedings of the 32nd. Annual
Meeting of the ACL, 9–16.
Thomas Hofmann. 2000. Learning the Similarity of
Documents: An Information-Geometric Approach to
Document Retrieval and Categorization. Advances in
Neural Information Processing Systems 12: 914–920.
Xiang Ji and Hongyuan Zha. 2003. Domain-
Independent Text Segmentation Using Anisotropic
Diffusion and Dynamic Programming. Proceedings
of the 26th annual international ACM SIGIR Confer-
ence on Research and Development in Informaion Re-
trieval, 322–329.
Hajime Mochizuki, Takeo Honda and Manabu Okumura.
1998. Text Segmentation with Multiple Surface Lin-
guistic Cues. Proceedings of the COLING-ACL’98,
881-885.
Jeffrey C. Reynar. 1998. Topic Segmentation: Algo-
rithms and Applications. PhD thesis. University of
Pennsylvania.
</reference>
<figure confidence="0.988448176470588">
0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
00 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Set 3−5
Set 13−15
Set 5−20
Set 3−5
Set 13−15
Set 5−20
</figure>
<page confidence="0.948477">
272
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.928280">
<title confidence="0.995371">Text Segmentation with LDA-Based Fisher Kernel</title>
<author confidence="0.995128">Qi Sun</author>
<author confidence="0.995128">Runxin Li</author>
<author confidence="0.995128">Dingsheng Luo</author>
<author confidence="0.995128">Xihong Wu</author>
<affiliation confidence="0.977969333333333">Speech and Hearing Research Center, and Key Laboratory of Machine Perception (Ministry of Education) Peking University</affiliation>
<address confidence="0.999978">100871, Beijing, China</address>
<abstract confidence="0.999684692307692">In this paper we propose a domainindependent text segmentation method, which consists of three components. Latent Dirichlet allocation (LDA) is employed to compute words semantic distribution, and we measure semantic similarity by the Fisher kernel. Finally global best segmentation is achieved by dynamic programming. Experiments on Chinese data sets with the technique show it can be effective. Introducing latent semantic information, our algorithm is robust on irregular-sized segments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Proceedings of ACL-08: HLT, Short Papers (Companion Volume),</booktitle>
<pages>269--272</pages>
<marker></marker>
<rawString>Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 269–272,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Columbus</author>
</authors>
<date>2008</date>
<booktitle>c�2008 Association for Computational Linguistics</booktitle>
<location>Ohio, USA,</location>
<marker>Columbus, 2008</marker>
<rawString>Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Beeferman</author>
<author>Adam Berger</author>
<author>John D Lafferty</author>
</authors>
<title>Statistical Models for Text Segmentation.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="1229" citStr="Beeferman et al., 1999" startWordPosition="181" endWordPosition="184">ments on Chinese data sets with the technique show it can be effective. Introducing latent semantic information, our algorithm is robust on irregular-sized segments. 1 Introduction The aim of text segmentation is to partition a document into a set of segments, each of which is coherent about a specific topic. This task is inspired by problems in information retrieval, summarization, and language modeling, in which the ability to provide access to smaller, coherent segments in a document is desired. A lot of research has been done on text segmentation. Some of them utilize linguistic criteria (Beeferman et al., 1999; Mochizuki et al., 1998), while others use statistical similarity measures to uncover lexical cohesion. Lexical cohesion methods believe a coherent topic segment contains parts with similar vocabularies. For example, the TextTiling algorithm, introduced by (Hearst, 1994), assumes that the local minima of the word similarity curve are the points of low lexical cohesion and thus the natural boundary candidates. (Reynar, 1998) has proposed a method called dotplotting depending on the distribution of word repetitions to find tight regions of topic similarity graphically. One of the problems with </context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1999</marker>
<rawString>Doug Beeferman, Adam Berger and John D. Lafferty. 1999. Statistical Models for Text Segmentation. Machine Learning, 34(1-3):177–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of machine Learning Research</journal>
<volume>3</volume>
<pages>993--1022</pages>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei and Andrew Y. Ng and Michael I. Jordan. 2003. Latent Dirichlet Allocation. Journal of machine Learning Research 3: 993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>Francine Chen and Ioannis Tsochantaridis.</title>
<date>2002</date>
<journal>CIKM</journal>
<pages>02211--218</pages>
<marker>Brants, 2002</marker>
<rawString>Thorsten Brants, Francine Chen and Ioannis Tsochantaridis. 2002. Topic-Based Document Segmentation with Probabilistic Latent Semantic Analysis. CIKM ’02211–218</rawString>
</citation>
<citation valid="true">
<authors>
<author>Freddy Choi</author>
<author>Peter Wiemer-Hastings</author>
<author>Johanna Moore</author>
</authors>
<title>Latent Semantic Analysis for Text Segmentation.</title>
<date>2001</date>
<booktitle>Proceedings of 6th EMNLP,</booktitle>
<pages>109--117</pages>
<contexts>
<context position="2115" citStr="Choi et al., 2001" startWordPosition="318" endWordPosition="321">94), assumes that the local minima of the word similarity curve are the points of low lexical cohesion and thus the natural boundary candidates. (Reynar, 1998) has proposed a method called dotplotting depending on the distribution of word repetitions to find tight regions of topic similarity graphically. One of the problems with those works is that they treat terms uncorrelated, assigning them orthogonal directions in the feature space. But in reality words are correlated, and sometimes even synonymous, so that texts with very few common terms can potentially be on closely related topics. So (Choi et al., 2001; Brants et al., 2002) utilize semantic similarity to identify cohesion. Unsupervised models of texts that capture semantic information would be useful, particularly if they could be achieved with a ”semantic kernel” (Cristianini et al., 2001) , which computes the similarity between texts by also considering relations between different terms. A Fisher kernel is a function that measures the similarity between two data items not in isolation, but rather in the context provided by a probability distribution. In this paper, we use the Fisher kernel to describe semantic information similarity. In a</context>
</contexts>
<marker>Choi, Wiemer-Hastings, Moore, 2001</marker>
<rawString>Freddy Choi, Peter Wiemer-Hastings and Johanna Moore. 2001. Latent Semantic Analysis for Text Segmentation. Proceedings of 6th EMNLP, 109–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Freddy Y Y Choi</author>
</authors>
<date>2000</date>
<booktitle>Advances in Domain Independent Linear Text Segmentation. Proceedings of NAACL-00.</booktitle>
<marker>Choi, 2000</marker>
<rawString>Freddy Y. Y. Choi. 2000. Advances in Domain Independent Linear Text Segmentation. Proceedings of NAACL-00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nello Cristianini</author>
<author>John Shawe-Taylor</author>
<author>Huma Lodhi</author>
</authors>
<title>Latent Semantic Kernels.</title>
<date>2001</date>
<booktitle>Proceedings of ICML-01, 18th International Conference on Machine Learning</booktitle>
<pages>66--73</pages>
<contexts>
<context position="2358" citStr="Cristianini et al., 2001" startWordPosition="353" endWordPosition="356">ord repetitions to find tight regions of topic similarity graphically. One of the problems with those works is that they treat terms uncorrelated, assigning them orthogonal directions in the feature space. But in reality words are correlated, and sometimes even synonymous, so that texts with very few common terms can potentially be on closely related topics. So (Choi et al., 2001; Brants et al., 2002) utilize semantic similarity to identify cohesion. Unsupervised models of texts that capture semantic information would be useful, particularly if they could be achieved with a ”semantic kernel” (Cristianini et al., 2001) , which computes the similarity between texts by also considering relations between different terms. A Fisher kernel is a function that measures the similarity between two data items not in isolation, but rather in the context provided by a probability distribution. In this paper, we use the Fisher kernel to describe semantic information similarity. In addition, (Fragkou et al., 2004; Ji and Zha, 2004) has treated this task as an optimization problem with global cost function and used dynamic programming for segments selection. The remainder of the paper is organized as follows. In section 2,</context>
</contexts>
<marker>Cristianini, Shawe-Taylor, Lodhi, 2001</marker>
<rawString>Nello Cristianini, John Shawe-Taylor and Huma Lodhi. 2001. Latent Semantic Kernels. Proceedings of ICML-01, 18th International Conference on Machine Learning 66–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pavlina Fragkou</author>
</authors>
<title>Petridis Vassilios and Kehagias Athanasios.</title>
<date>2004</date>
<journal>J. Intell. Inf. Syst.,</journal>
<volume>23</volume>
<issue>2</issue>
<pages>179--197</pages>
<marker>Fragkou, 2004</marker>
<rawString>Pavlina Fragkou, Petridis Vassilios and Kehagias Athanasios. 2004. A Dynamic Programming Algorithm for Linear Text Segmentation. J. Intell. Inf. Syst., 23(2): 179–197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>Multi-Paragraph Segmentation of Expository Text.</title>
<date>1994</date>
<booktitle>Proceedings of the 32nd. Annual Meeting of the ACL,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="1501" citStr="Hearst, 1994" startWordPosition="221" endWordPosition="222">erent about a specific topic. This task is inspired by problems in information retrieval, summarization, and language modeling, in which the ability to provide access to smaller, coherent segments in a document is desired. A lot of research has been done on text segmentation. Some of them utilize linguistic criteria (Beeferman et al., 1999; Mochizuki et al., 1998), while others use statistical similarity measures to uncover lexical cohesion. Lexical cohesion methods believe a coherent topic segment contains parts with similar vocabularies. For example, the TextTiling algorithm, introduced by (Hearst, 1994), assumes that the local minima of the word similarity curve are the points of low lexical cohesion and thus the natural boundary candidates. (Reynar, 1998) has proposed a method called dotplotting depending on the distribution of word repetitions to find tight regions of topic similarity graphically. One of the problems with those works is that they treat terms uncorrelated, assigning them orthogonal directions in the feature space. But in reality words are correlated, and sometimes even synonymous, so that texts with very few common terms can potentially be on closely related topics. So (Cho</context>
</contexts>
<marker>Hearst, 1994</marker>
<rawString>Marti Hearst. 1994. Multi-Paragraph Segmentation of Expository Text. Proceedings of the 32nd. Annual Meeting of the ACL, 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Learning the Similarity of Documents: An Information-Geometric Approach to Document Retrieval and Categorization.</title>
<date>2000</date>
<booktitle>Advances in Neural Information Processing Systems 12:</booktitle>
<pages>914--920</pages>
<marker>Hofmann, 2000</marker>
<rawString>Thomas Hofmann. 2000. Learning the Similarity of Documents: An Information-Geometric Approach to Document Retrieval and Categorization. Advances in Neural Information Processing Systems 12: 914–920.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiang Ji</author>
<author>Hongyuan Zha</author>
</authors>
<title>DomainIndependent Text Segmentation Using Anisotropic Diffusion and Dynamic Programming.</title>
<date>2003</date>
<booktitle>Proceedings of the 26th annual international ACM SIGIR Conference on Research and Development in Informaion Retrieval,</booktitle>
<pages>322--329</pages>
<marker>Ji, Zha, 2003</marker>
<rawString>Xiang Ji and Hongyuan Zha. 2003. DomainIndependent Text Segmentation Using Anisotropic Diffusion and Dynamic Programming. Proceedings of the 26th annual international ACM SIGIR Conference on Research and Development in Informaion Retrieval, 322–329.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hajime Mochizuki</author>
</authors>
<title>Takeo Honda and Manabu Okumura.</title>
<date>1998</date>
<booktitle>Proceedings of the COLING-ACL’98,</booktitle>
<pages>881--885</pages>
<marker>Mochizuki, 1998</marker>
<rawString>Hajime Mochizuki, Takeo Honda and Manabu Okumura. 1998. Text Segmentation with Multiple Surface Linguistic Cues. Proceedings of the COLING-ACL’98, 881-885.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey C Reynar</author>
</authors>
<title>Topic Segmentation: Algorithms and Applications. PhD thesis.</title>
<date>1998</date>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1657" citStr="Reynar, 1998" startWordPosition="247" endWordPosition="248">ide access to smaller, coherent segments in a document is desired. A lot of research has been done on text segmentation. Some of them utilize linguistic criteria (Beeferman et al., 1999; Mochizuki et al., 1998), while others use statistical similarity measures to uncover lexical cohesion. Lexical cohesion methods believe a coherent topic segment contains parts with similar vocabularies. For example, the TextTiling algorithm, introduced by (Hearst, 1994), assumes that the local minima of the word similarity curve are the points of low lexical cohesion and thus the natural boundary candidates. (Reynar, 1998) has proposed a method called dotplotting depending on the distribution of word repetitions to find tight regions of topic similarity graphically. One of the problems with those works is that they treat terms uncorrelated, assigning them orthogonal directions in the feature space. But in reality words are correlated, and sometimes even synonymous, so that texts with very few common terms can potentially be on closely related topics. So (Choi et al., 2001; Brants et al., 2002) utilize semantic similarity to identify cohesion. Unsupervised models of texts that capture semantic information would </context>
</contexts>
<marker>Reynar, 1998</marker>
<rawString>Jeffrey C. Reynar. 1998. Topic Segmentation: Algorithms and Applications. PhD thesis. University of Pennsylvania.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>