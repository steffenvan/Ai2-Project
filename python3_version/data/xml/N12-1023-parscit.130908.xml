<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.967037">
Structured Ramp Loss Minimization for Machine Translation
</title>
<author confidence="0.997895">
Kevin Gimpel and Noah A. Smith
</author>
<affiliation confidence="0.890324">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.998866">
{kgimpel,nasmith}@cs.cmu.edu
</email>
<sectionHeader confidence="0.998599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999769375">
This paper seeks to close the gap between
training algorithms used in statistical machine
translation and machine learning, specifically
the framework of empirical risk minimization.
We review well-known algorithms, arguing
that they do not optimize the loss functions
they are assumed to optimize when applied to
machine translation. Instead, most have im-
plicit connections to particular forms of ramp
loss. We propose to minimize ramp loss di-
rectly and present a training algorithm that is
easy to implement and that performs compa-
rably to others. Most notably, our structured
ramp loss minimization algorithm, RAMPION,
is less sensitive to initialization and random
seeds than standard approaches.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999906877551021">
Every statistical MT system relies on a training al-
gorithm to fit the parameters of a scoring function to
examples from parallel text. Well-known examples
include MERT (Och, 2003), MIRA (Chiang et al.,
2008), and PRO (Hopkins and May, 2011). While
such procedures can be analyzed as machine learn-
ing algorithms—e.g., in the general framework of
empirical risk minimization (Vapnik, 1998)—their
procedural specifications have made this difficult.
From a practical perspective, such algorithms are of-
ten complex, difficult to replicate, and sensitive to
initialization, random seeds, and other hyperparam-
eters.
In this paper, we consider training algorithms that
are first specified declaratively, as loss functions to
be minimized. We relate well-known training algo-
rithms for MT to particular loss functions. We show
that a family of structured ramp loss functions (Do
et al., 2008) is useful for this analysis. For example,
McAllester and Keshet (2011) recently suggested
that, while Chiang et al. (2008, 2009) described their
algorithm as “MIRA” (Crammer et al., 2006), in fact
it targets a kind of ramp loss. We note here other ex-
amples: Liang et al. (2006) described their algorithm
as a variant of the perceptron (Collins, 2002), which
has a unique loss function, but the loss actually opti-
mized is closer to a particular ramp loss (that differs
from the one targeted by Chiang et al.). Och and
Ney (2002) sought to optimize log loss (likelihood
in a probabilistic model; Lafferty et al., 2001) but
actually optimized a version of the soft ramp loss.
Why isn’t the application of ML to MT more
straightforward? We note two key reasons: (i) ML
generally assumes that the correct output can always
be scored by a model, but in MT the reference trans-
lation is often unreachable, due to a model’s limited
expressive power or search error, requiring the use
of “surrogate” references; (ii) MT models nearly al-
ways include latent derivation variables, leading to
non-convex losses that have generally received little
attention in ML. In this paper, we discuss how these
two have caused a disconnect between the loss func-
tion minimized by an algorithm in ML and the loss
minimized when it is adapted for MT.
From a practical perspective, our framework leads
to a simple training algorithm for structured ramp
loss based on general optimization techniques. Our
algorithm is simple to implement and, being a batch
algorithm like MERT and PRO, can easily be inte-
</bodyText>
<page confidence="0.653775">
221
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 221–231,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.995888">
grated with any decoder. Our experiments show that
our algorithm, which we call RAMPION, performs
comparably to MERT and PRO, is less sensitive to
randomization and initialization conditions, and is
robust in large-feature scenarios.
</bodyText>
<sectionHeader confidence="0.803798" genericHeader="introduction">
2 Notation and Background
</sectionHeader>
<bodyText confidence="0.998388263157895">
Let X denote the set of all strings in a source lan-
guage and, for a particular x ∈ X, let Y(x) denote
the set of its possible translations (correct and incor-
rect) in the target language. In typical models for
machine translation, a hidden variable is assumed
to be constructed during the translation process.1
Regardless of its specific form, we will refer to it as
a derivation and denote it h ∈ H(x), where H(x)
is the set of possible values of h for the input x.
Derivations will always be coupled with translations
and therefore we define the set T(x) ⊆ Y(x)×H(x)
of valid output pairs hy, hi for x.
To model translation, we use a linear model pa-
rameterized by a parameter vector θ ∈ O. Given a
vector f(x, y, h) of feature functions on x, y, and
h, and assuming θ contains a component for each
feature function, output pairs hy, hi for a given in-
put x are selected using a simple argmax decision
rule: hy*, h*i = argmax
</bodyText>
<equation confidence="0.991180666666667">
(y,h)ET(x) • v •
θTf(x, y, h)
score(x,y,h;o)
</equation>
<bodyText confidence="0.99982925">
The training problem for machine translation cor-
responds to choosing θ. There are many ways to do
this, and we will describe each in terms of a partic-
ular loss function loss : XN × YN × O → R that
maps an input corpus, its reference translations, and
the model parameters to a real value indicating the
quality of the parameters. Risk minimization cor-
responds to choosing
</bodyText>
<equation confidence="0.453489">
argminoE© Ep(X,Y) [loss (X, Y , θ)] (1)
</equation>
<bodyText confidence="0.999078666666667">
where p(X, Y ) is the (unknown) true joint distri-
bution over corpora. We note that the loss function
depends on the entire corpus, while the decoder op-
erates independently on one sentence at a time. This
is done to fit the standard assumptions in MT sys-
tems: the evaluation metric (e.g., BLEU) depends on
</bodyText>
<footnote confidence="0.9931485">
1For phrase-based MT, a segmentation of the source
and target sentences into phrases and an alignment between
them (Koehn et al., 2003). For hierarchical phrase-based MT, a
derivation under a synchronous CFG (Chiang, 2005).
</footnote>
<bodyText confidence="0.992492">
the entire corpus and does not decompose linearly,
while the model score does. Since in practice we do
not know p(X, Y ), but we do have access to an ac-
tual corpus pair h�X, Y�i, where X = {x(i)}Ni=1 and
Y� = {y(i)}N i=1,we instead consider regularized
empirical risk minimization:
</bodyText>
<equation confidence="0.509781">
argminoEe loss(X, Y, θ) + R(θ) (2)
</equation>
<bodyText confidence="0.999439">
where R(θ) is the regularization function used to
mitigate overfitting. The regularization function is
frequently a squared norm of the parameter vector,
such as the E1 or E2 norm, but many other choices
are possible. In this paper, we use E2.
Models are evaluated using a task-specific notion
of error, here encoded as a cost function, cost :
YN × YN → R&gt;0, such that the worse a translation
is, the higher its cost. The cost function will typi-
cally make use of an automatic evaluation metric for
machine translation; e.g., cost might be 1 minus the
BLEU score (Papineni et al., 2001).2
We note that our analysis in this paper is appli-
cable for understanding the loss function being op-
timized given a fixed set of k-best lists.3 However,
most training procedures periodically invoke the de-
coder to generate new k-best lists, which are then
typically merged with those from previous training
iterations. It is an open question how this practice
affects the loss function being optimized by the pro-
cedure as a whole.
Example 1: MERT. The most commonly-used
training algorithm for machine translation is mini-
mum error rate training, which seeks to directly
minimize the cost of the predictions on the training
data. This idea has been used in the pattern recogni-
tion and speech recognition communities (Duda and
Hart, 1973; Juang et al., 1997); its first application
to MT was by Och (2003). The loss function takes
</bodyText>
<equation confidence="0.918893666666667">
� �
the following form: losscost �X, Y� , θ =
(3)
</equation>
<bodyText confidence="0.8269468">
2We will abuse notation and allow cost to operate on both
sets of sentences as well as individual sentences. For nota-
tional convenience we also let cost accept hidden variables but
assume that the hidden variables do not affect the value; i.e.,
cost((y, h), (y&apos;, h&apos;)) = cost(y, (y&apos;, h&apos;)) = cost(y, y&apos;).
</bodyText>
<footnote confidence="0.9432715">
3Cherry and Foster (2012) have concurrently performed a
similar analysis.
</footnote>
<equation confidence="0.997620142857143">
.
�
cost �
� �
N
Y, argmax score(x(i), y, h; θ) �
(y,h)ET(x(z)) i=1
</equation>
<page confidence="0.993842">
222
</page>
<bodyText confidence="0.999756307692308">
MERT directly minimizes the corpus-level cost
function of the best outputs from the decoder with-
out any regularization (i.e., R(θ) = 0).4 The loss is
non-convex and not differentiable for cost functions
like BLEU, so Och (2003) developed a coordinate
ascent procedure with a specialized line search.
MERT avoids the need to compute feature vec-
tors for the references (§1(i)) and allows corpus-
level metrics like BLEU to be easily incorporated.
However, the complexity of the loss and the diffi-
culty of the search lead to instabilities during learn-
ing. Remedies have been suggested, typically in-
volving additional search directions and experiment
replicates (Cer et al., 2008; Moore and Quirk, 2008;
Foster and Kuhn, 2009; Clark et al., 2011). But de-
spite these improvements, MERT is ineffectual for
training weights for large numbers of features; in
addition to anecdotal evidence from the MT com-
munity, Hopkins and May (2011) illustrated with
synthetic data experiments that MERT struggles in-
creasingly to find the optimal solution as the number
of parameters grows.
Example 2: Probabilistic Models. By exponenti-
ating and normalizing score(x, y, h; θ), we obtain
a conditional log-linear model, which is useful for
training criteria with probabilistic interpretations:
</bodyText>
<equation confidence="0.981603">
pe(y, h x) = 1
z(„,e) exp{score(x, y, h; θ)} (4)
The log loss then defines losslog( X, Y�, θ) =
− EN1 log pe(y(i) x(i)).
</equation>
<bodyText confidence="0.9811645">
Example 3: Bayes Risk. The term “risk” as used
above should not be confused with the Bayes risk
framework, which uses a probability distribution
(Eq. 4) and a cost function to define a loss:
</bodyText>
<equation confidence="0.954516">
lossB risk = E�i=1 L&apos;pO(,/,h|„(z))[cost(y(i),y)I (5)
</equation>
<bodyText confidence="0.902639846153846">
The use of this loss is often simply called “risk
minimization” in the speech and MT communities.
Bayes risk is non-convex, whether or not latent vari-
ables are present. Like MERT, it naturally avoids
the need to compute features for y(i) and uses a
cost function, making it appealing for MT. Bayes
risk minimization first appeared in the speech recog-
nition community (Kaiser et al., 2000; Povey and
4However, Cer et al. (2008) and Macherey et al. (2008)
achieved a sort of regularization by altering MERT’s line search.
Woodland, 2002) and more recently has been ap-
plied to MT (Smith and Eisner, 2006; Zens et al.,
2007; Li and Eisner, 2009).
</bodyText>
<sectionHeader confidence="0.998933" genericHeader="method">
3 Training Methods for MT
</sectionHeader>
<bodyText confidence="0.999981928571429">
In this section we consider other ML-inspired ap-
proaches to MT training, situating each in the frame-
work from §2: ramp, perceptron, hinge, and “soft”
losses. Each of the first three kinds of losses can be
understood as a way of selecting, for each x(i), two
candidate translation/derivation pairs: (yT, hT) and
(y�, h�). During training, the loss function can be
improved by increasing the score of the former and
decreasing the score of the latter, through manipu-
lation of the parameters θ. Figure 1 gives a general
visualization of some of the key output pairs that are
considered for these roles. Learning alters the score
function, or, in the figure, moves points horizontally
so that scores approximate negated costs.
</bodyText>
<subsectionHeader confidence="0.999261">
3.1 Structured Ramp Loss Minimization
</subsectionHeader>
<bodyText confidence="0.999974730769231">
The structured ramp loss (Do et al., 2008) is a
non-convex loss function with certain attractive the-
oretical properties. It is an upper bound on losscost
(Eq. 3) and is a tighter bound than other loss func-
tions (Collobert et al., 2006). Ramp loss has been
shown to be statistically consistent in the sense
that, in the limit of infinite training data, mini-
mizing structured ramp loss reaches the minimum
value of losscost that is achievable with a linear
model (McAllester and Keshet, 2011). This is true
whether or not latent variables are present.
Consistency in this sense is not a common prop-
erty of loss functions; commonly-used convex loss
functions such as the perceptron, hinge, and log
losses (discussed below) are not consistent, because
they are all sensitive to outliers or otherwise noisy
training examples. Ramp loss is better at dealing
with outliers in the training data (Collobert et al.,
2006).
There are three forms of latent structured ramp
loss: Eq. 6–8 (Fig. 2). Ramp losses are appealing for
MT because they do not require computing the fea-
ture vector of y(i) (§1(i)). The first form, Eq. 6, sets
(yT, hT) to be the current model prediction ((y, h)
in Fig. 1) and (y�, h�) to be an output that is both
favored by the model and has high cost. Such an
</bodyText>
<page confidence="0.937232">
223
</page>
<bodyText confidence="0.7115305">
- cost
- cost
</bodyText>
<equation confidence="0.9522776">
score(x(i), y, h; θ) − cost(y(i), y)
argmin cost(y(i), y)
y,h∈K(x(i))
y−,h− = argmax
(y,h)ET(x(i))
y+, h+ = argmax
y,h∈T(x(i))
score(x(i), y, h; θ)
score
score(x(i), y, h; B) + cost(y(i), y)
y*, h* = argmin cost(y(i), y)
(y,h)ET(x(i))
ˆy, ˆh =argmax
y,h∈T(x(i))
score
</equation>
<figureCaption confidence="0.99725225">
Figure 1: Hypothetical output space of a translation model for an input sentence x(0. Each point corresponds to a
single translation/derivation output pair. Horizontal “bands” are caused by output pairs with the same translation (and
hence the same cost) but different derivations. The left plot shows the entire output space and the right plot highlights
outputs in the k-best list. Choosing the output with the lowest cost in the k-best list is similar to finding (y+, h+).
</figureCaption>
<bodyText confidence="0.996226278688525">
output is shown as (y−, h−) in Fig. 1; finding y↓
is often called cost-augmented decoding, which is
also used to define hinge loss (§3.3).
The second form, Eq. 7, penalizes the model
prediction ((y↓, h↓) = (y, h)) and favors an out-
put pair that has both high model score and low
cost; this is the converse of cost-augmented decod-
ing and therefore we call it cost-diminished decod-
ing; (y↑, h↑) = (y+, h+) in Fig. 1. The third form,
Eq. 8, sets (y↑, h↑) = (y+, h+) and (y↓, h↓) =
(y−, h−). This loss underlies RAMPION. It is sim-
ilar to the loss optimized by the MIRA-inspired al-
gorithm used by Chiang et al. (2008, 2009).
Optimization The ramp losses are continuous but
non-convex and non-differentiable, so gradient-
based optimization methods are not available.5 For-
tunately, Eq. 8 can be optimized by using a concave-
convex procedure (CCCP; Yuille and Rangarajan,
2002). CCCP is a batch optimization algorithm for
any function that is the the sum of a concave and a
convex function. The idea is to approximate the sum
as the convex term plus a tangent line to the con-
cavecfunction at the current parameter values; the
resulting sum is convex and can be optimized with
(sub)gradient methods.
5For non-differentiable, continuous, convex functions, sub-
gradient-based methods are available, such as stochastic sub-
gradient descent (SSD), and it is tempting to apply them here.
However, non-convex functions are not everywhere subdiffer-
entiable and so a straightforward application of SSD may en-
counter problems in practice.
With our loss functions, CCCP first imputes the
outputs in the concave terms in each loss (i.e., solves
the negated max expressions) for the entire training
set and then uses an optimization procedure to op-
timize the loss with the imputed values fixed. Any
convex optimization procedure can be used once the
negated max terms are solved; we use stochastic
subgradient descent (SSD) but MIRA could be eas-
ily used instead.
The CCCP algorithm we use for optimizing
lossramp 3, which we call RAMPION, is shown as
Alg. 1. Similar algorithms can easily be derived for
the other ramp losses. The first step done on each
iteration is to generate k-best lists for the full tun-
ing set (line 3). We then run CCCP on the k-best
lists for V iterations (lines 4–15). This involves first
finding the translation to update towards for all sen-
tences in the tuning set (lines 5–7), then making pa-
rameter updates in an online fashion with V epochs
of stochastic subgradient descent (lines 8–14). The
subgradient update for the E2 regularization term is
done in line 11 and then for the loss in line 12.6
Unlike prior work that targeted similar loss func-
- cost
tions (Watanabe et al., 2007; Chiang et al., 2008;
Chiang et al., 2009), we do not use a fully online al-
gorithm such as MIRA in an outer loop because we
are not aware of an online learning algorithm with
theoretical guarantees for non-differentiable, non-
convex loss functions like the ramp losses. CCCP
</bodyText>
<footnote confidence="0.631422">
6f2 regularization done here regularizes toward 00, not 0.
</footnote>
<page confidence="0.990132">
224
</page>
<table confidence="0.953265454545454">
lossramp 1 = N
lossramp 2 = − max (scorei(y,h;B)) + max (scorei(y,h;B) + costi(y))
lossramp 3 = (y,h)ETi (y,h)ETi
lossperc = i=1
lossperc kbest = N
� − max (scorei(y,h;B) − costi(y)) + max (scorei(y,h;B))
(y,h)ETi (y,h)ETi
i=1
N
− max (scorei(y,h;B) − costi(y)) + max (scorei(y,h;B) + costi(y))
(y,h)ETi (y,h)ETi
i=1
N
− max scorei(y(i), h; B) + max scorei(y,h;B)
h:(y(i),h)ETi (y,h)ETi
i=1
�n −score x(i), argmin (costi(y)) ; B I + max scorei(y, h; B)
i=1 (y,h)EKi / (y,h) ETi
N
− max (scorei(y,h;B) − yicosti(y)) + max scorei(y,h;B)
(y,h)ETi (y,h)ETi
i=1
</table>
<figureCaption confidence="0.9723215">
Figure 2: Formulae mentioned in text for latent-variable loss functions. Each loss is actually a function loss(X, Y, B);
we suppress the arguments for clarity. “Ti” is shorthand for “T(x(i)).” “Xi” is shorthand for the k-best list for x(i).
“costi(·)” is shorthand for “cost(y(i), ·).” “scorei(·)” is shorthand for “score(x(i), ·).” As noted in §3.4, any operator
of the form maxsES can be replaced by log EsES exp, known as softmax, giving many additional loss functions.
</figureCaption>
<bodyText confidence="0.999787">
is fundamentally a batch optimization algorithm and
has been used for solving many non-convex learn-
ing problems, such as latent structured SVMs (Yu
and Joachims, 2009).
</bodyText>
<subsectionHeader confidence="0.998872">
3.2 Structured Perceptron
</subsectionHeader>
<bodyText confidence="0.98992452631579">
The stuctured perceptron algorithm (Collins, 2002)
was considered by Liang et al. (2006) as an alterna-
tive to MERT. It requires only a decoder and comes
with some attractive guarantees, at least for mod-
els without latent variables. Liang et al. modified
the perceptron in several ways for use in MT. The
first was to generalize it to handle latent variables.
The second change relates to the need to compute
the feature vector for the reference translation y(i),
which may be unreachable (§1(i)). To address this,
researchers have proposed the use of surrogates that
are both favored by the current model parameters
and similar to the reference. Och and Ney (2002)
were the first to do so, using the translation on a
k-best list with the highest evaluation metric score
as yT. This practice was followed by Liang et al.
(2006) and others with success (Arun and Koehn,
2007; Watanabe et al., 2007).7
Perceptron Loss Though typically described and
</bodyText>
<footnote confidence="0.759994">
7Liang et al. (2006) also tried a variant that updated directly
to the reference when it is reachable (“bold updating”), but they
and others found that Och and Ney’s strategy worked better.
</footnote>
<listItem confidence="0.811001666666667">
analyzed procedurally, it is straightforward to show
that Collins’ perceptron (without latent variables)
equates to SSD with fixed step size 1 on loss:
</listItem>
<equation confidence="0.717048833333333">
−score(x(i), y(i); B)+ max
yE%X(&apos;))
(12)
This loss is convex but ignores cost functions.
In our notation, yT = y(i) and yi =
argmaxyE%X(&apos;)) score(x(i), y; B).
</equation>
<bodyText confidence="0.999644388888889">
Adaptation for MT We chart the transformations
from Eq. 12 toward the loss Liang et al.’s algorithm
actually optimized. First, generalize to latent vari-
ables; see Eq. 9 (Fig. 2), sacrificing convexity. Sec-
ond, to cope with unreachable references, use a k-
best surrogate as shown in Eq. 10 (Fig. 2), where
Xi E T(x(i))k is a set containing the k best out-
put pairs for x(i). Now the loss only depends on
y(i) through the cost function. (Even without hid-
den variables, this loss can only be convex when the
k-best list is fixed, keeping yT unchanged across it-
erations. Updating the k-best lists makes yT depend
on B, resulting in a non-convex loss.)
It appears that Eq. 10 (Fig. 2) is the loss that
Liang et al. (2006) sought to optimize, using SSD. In
light of footnote 5 and the non-convexity of Eq. 10
(Fig. 2), we have no theoretical guarantee that such
an algorithm will find a (local) optimum.
</bodyText>
<equation confidence="0.995422666666667">
N
i=1
score(x(i), y; B)
</equation>
<page confidence="0.838328">
225
11
</page>
<table confidence="0.852386166666667">
Input: inputs {x(i)}Ni=1, references {y(i)}N i=1, init.
weights θ0, k-best list size k, step size 77, E2
reg. coeff. C, # iters T, # CCCP iters T&apos;, #
SSD iters T&apos;&apos;
Output: learned weights: θ
1 θ ← θ0;
</table>
<tableCaption confidence="0.226236">
2 for iter ← 1 to T do
</tableCaption>
<figure confidence="0.773292368421053">
3 {`JCi}Ni=1 ← Decode({x(i)}Ni=1, θ, k);
4 for iter&apos; ← 1 to T&apos; do
5 for i ← 1 to N do
6 hy+i ,h+i i ←
argmax(y,h)EX: scorei(y, h; θ) − costi(y);
7 end
8 for iter&apos;&apos; ← 1 to T&apos;&apos; do
9 for i ← 1 to N do
10 hy , h i ←
argmax(y,h)EJC: scorei(y, h; θ) + costi(y);
θ −= 77C (θ θ0 �;
N
i,hi )- f(�(Z),y ,h ));
12 0+=77(f(x(i),y
13 end
14 end
15 end
16 end
17 return θ;
</figure>
<figureCaption confidence="0.584939">
Algorithm 1: RAMPION.
</figureCaption>
<bodyText confidence="0.999924411764706">
We note that Eq. 10 is similar to Eq. 11 (Fig. 2),
where each -y is used to trade off between model and
cost. Fig. 1 illustrates the similarity by showing that
the min-cost output on a k-best list resides in a simi-
lar region of the output space as hy+, h+i computed
from the full output space. While it is not the case
that we can always choose &apos;Yi so as to make the two
losses equivalent, they are similar in that they up-
date towards some yT with high model score and
low cost. Eq. 11 corresponds to Eq. 7 (Fig. 2), the
second form of the latent structured ramp loss.
Thus, one way to understand Liang et al.’s algo-
rithm is as a form of structured ramp loss. However,
another interpretation is given by McAllester et al.
(2010), who showed that procedures like that used
by Liang et al. approach direct cost minimization in
the limiting case.
</bodyText>
<subsectionHeader confidence="0.687112">
3.3 Large-Margin Methods
</subsectionHeader>
<bodyText confidence="0.9029375">
A related family of approaches for training MT mod-
els involves the margin-infused relaxed algorithm
(MIRA; Crammer et al., 2006), an online large-
margin training algorithm. It has recently shown
success for MT, particularly when training models
with large feature sets (Watanabe et al., 2007; Chi-
ang et al., 2008; Chiang et al., 2009). In order to
apply it to MT, Watanabe et al. and Chiang et al.
made modifications similar to those made by Liang
et al. for perceptron training, namely the extension
to latent variables and the use of a surrogate refer-
ence with high model score and low cost.
Hinge Loss It can be shown that 1-best MIRA corre-
sponds to dual coordinate ascent for the structured
hinge loss when using E2 regularization (Martins et
al., 2010). The structured hinge is the loss underly-
ing maximum-margin Markov networks (Taskar et
al., 2003): setting yT = y(i) and:
</bodyText>
<equation confidence="0.9419935">
� )
yi = argmax score(x(i), y; θ) + cost(y(i), y)
���(����)
(13)
</equation>
<bodyText confidence="0.999567103448276">
Unlike the perceptron losses, which penalize the
highest-scoring outputs, hinge loss penalizes an out-
put that is both favored by the model and has high
cost. Such an output is shown as hy−, h−i in Fig. 1;
the structured hinge loss focuses on pushing such
outputs to the left. As mentioned in §3.1, finding yi
is often called cost-augmented decoding.
Structured hinge loss is convex, can incorporate
a cost function, and can be optimized with several
algorithms, including SSD (Ratliff et al., 2006).
Adaptation for MT While prior work has used
MIRA-like algorithms for training machine transla-
tion systems, the proposed algorithms did not actu-
ally optimize the structured hinge loss, for similar
reasons to those mentioned above for the perceptron:
latent variables and surrogate references. Incorpo-
rating latent variables in the hinge loss results in
the latent structured hinge loss (Yu and Joachims,
2009). Like the latent perceptron, this loss is non-
convex and inappropriate for MT because it requires
computing the feature vector for y(i). By using a
surrogate instead of y(i), the actual loss optimized
becomes closer to Eq. 8 (Fig. 2), the third form of
the latent structured ramp loss.
Watanabe et al. (2007) and Arun and Koehn
(2007) used k-best oracles like Liang et al., but Chi-
ang et al. (2008, 2009) used a different approach, ex-
plicitly defining the surrogate as hy+, h+i in Fig. 1.
While the method of Chiang et al. showed impres-
</bodyText>
<page confidence="0.996401">
226
</page>
<bodyText confidence="0.999983076923077">
sive performance improvements, its implementation
is non-trivial, involving a complex cost function and
a parallel architecture, and it has not yet been em-
braced by the MT community. Indeed, the com-
plexity of Chiang et al’s algorithm was one of the
reasons cited for the development of PRO (Hopkins
and May, 2011). In this paper, we have sought to
isolate the loss functions used in prior work like that
by Chiang et al. and identify simple, generic opti-
mization procedures for optimizing them. We offer
RAMPION as an alternative to Chiang et al’s MIRA
that is simpler to implement and achieves empirical
success in experiments (§4).
</bodyText>
<subsectionHeader confidence="0.972623">
3.4 Likelihood and Softened Losses
</subsectionHeader>
<bodyText confidence="0.995927064516129">
We can derive new loss functions from the above
by converting any “max” operator to a “softmax”
(log E exp, where the set of elements under the
summation is the same as under the max). For exam-
ple, the softmax version of the perceptron loss is the
well-known log loss (§2, Ex. 2), the loss underlying
the conditional likelihood training criterion which
is frequently used when a probabilistic interpreta-
tion of the learned model is desired, as in conditional
random fields (Lafferty et al., 2001).
Och and Ney (2002) popularized the use of log-
linear models for MT and initially sought to opti-
mize log loss, but by using the min-cost transla-
tion on a k-best list as their surrogate, we argue that
their loss was closer to the soft ramp loss obtained
by softening the second max in lossramp 2 in Eq. 7
(Fig. 2). The same is true for others who aimed to
optimize log loss for MT (Smith and Eisner, 2006;
Zens et al., 2007; Cer, 2011).
The softmax version of the latent variable percep-
tron loss, Eq. 9 (Fig. 2), is the latent log loss inher-
ent in latent-variable CRFs (Quattoni et al., 2004).
Blunsom et al. (2008) and Blunsom and Osborne
(2008) actually did optimize latent log loss for MT,
discarding training examples for which y(O was un-
reachable by the model.
Finally, we note that “softening” the ramp loss
in Eq. 6 (Fig. 2) results in the Jensen risk
bound from Gimpel and Smith (2010), which is
a computationally-attractive upper bound on the
Bayes risk.
</bodyText>
<sectionHeader confidence="0.999566" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.95760670212766">
The goal of our experiments is to compare RAM-
PION (Alg. 1) to state-of-the-art methods for train-
ing MT systems. RAMPION minimizes lossramp 3,
which we found in preliminary experiments to work
better than other loss functions tested.8
System and Datasets We use the Moses phrase-
based MT system (Koehn et al., 2007) and consider
Urdu—*English (UR—*EN), Chinese—*English
(ZH—*EN) translation, and Arabic—*English
(AR—*EN) translation.9 We trained a Moses system
using default settings and features, except for
setting the distortion limit to 10. Word alignment
was performed using GIZA++ (Och and Ney, 2003)
in both directions, the grow-diag-final-and
heuristic was used to symmetrize the alignments,
and a max phrase length of 7 was used for phrase
extraction. We estimated 5-gram language models
using the SRI toolkit (Stolcke, 2002) with modified
Kneser-Ney smoothing (Chen and Goodman, 1998).
For each language pair, we used the English side
of the parallel text and 600M words of randomly-
selected sentences from the Gigaword v4 corpus
(excluding NYT and LAT).
For UR—*EN, we used parallel data from the
NIST MT08 evaluation consisting of 1.2M Urdu
words and 1.1M English words. We used half of
the documents (882 sentences) from the MT08 test
set for tuning. We used the remaining half for
one test set (“MT08*”) and MT09 as our other test
set. For ZH—*EN, we used 303k sentence pairs
from the FBIS corpus (LDC2003E14). We seg-
mented the Chinese data using the Stanford Chi-
nese segmenter (Chang et al., 2008) in “CTB” mode,
giving us 7.9M Chinese words and 9.4M English
words. We used MT03 for tuning and used MT02
and MT05 for testing.
For AR—*EN, we used data provided by the LDC
8We only present full results using lossramp 3. We found
that minimizing lossramp 1 did poorly, resulting in single-digit
BLEU scores, and that lossramp 2 reached high BLEU scores on
the tuning data but failed to generalize well. Softened versions
of the ramp losses performed comparably to lossramp 3 but were
slightly worse on both tuning and held-out data.
9We found similar trends for other language pairs and sys-
tems, including Hiero (Chiang, 2005). A forthcoming report
will present these results, as well as experiments with additional
loss functions, in detail.
</bodyText>
<page confidence="0.992804">
227
</page>
<note confidence="0.808292666666667">
MERT
PRO
Rampion
</note>
<bodyText confidence="0.99996121875">
for the NIST evaluations, including 3.29M sentence
pairs of UN data and 982k sentence pairs of non-
UN data. The Arabic data was preprocessed using
an HMM segmenter that splits off attached prepo-
sitional phrases, personal pronouns, and the future
marker (Lee et al., 2003). The common stylistic
sentence-initial wa# (and ...) was removed from the
training and test data. The resulting corpus con-
tained 130M Arabic tokens and 130M English to-
kens. We used MT06 for tuning and three test sets:
MT05, the MT08 newswire test set (“MT08 NW”),
and the MT08 weblog test set (“MT08 WB”).
For all languages we evaluated translation output
using case-insensitive IBM BLEU (Papineni et al.,
2001).
Training Algorithms Our baselines are MERT and
PRO as implemented in the Moses toolkit.10 PRO
uses the hyperparameter settings from Hopkins and
May (2011), including k-best lists of size 1500 and
25 training iterations.11 MERT uses k-best lists of
size 100 and was run to convergence. For both
MERT and PRO, previous iterations’ k-best lists
were merged in.
For RAMPION, we used T = 20, T&apos; = 10,
T&apos;&apos; = 5, k = 500, q = 0.0001, and C = 1.
Our cost function is α(1 − BLEU+1(y, y&apos;)) where
BLEU+1(y, y&apos;) returns the BLEU+1 score (Lin and
Och, 2004) for reference y and hypothesis y&apos;. We
used α = 10. We used these same hyperparameter
values for all experiments reported here and found
them to perform well across other language pairs
and systems.12
</bodyText>
<subsectionHeader confidence="0.772538">
4.1 Results
</subsectionHeader>
<bodyText confidence="0.9889860625">
Table 1 shows our results. MERT and PRO were run
3 times with differing random seeds and averages
10The PRO algorithm samples pairs of translations from k-
best lists on each iteration and trains a binary classifier to rank
pairs according to the cost function. The loss function under-
lying PRO depends on the choice of binary classifier and also
on the sampling strategy. We leave an analysis of PRO’s loss
function to future work.
11Hopkins and May used 30 iterations, but showed that train-
ing had converged by 25.
12We found performance to be better when using a smaller
value of T&apos;; we suspect that using small T&apos; guards against over-
fitting to any particular set of k-best lists. We also found the
value of α to affect performance, although α E {1, 5, 10} all
worked well. Performance was generally insensitive to C. We
fixed rl = 0.0001 early on and did little tuning to it.
</bodyText>
<figure confidence="0.9010684">
36
34
35
35 36 35 36
Tune BLEU Tune BLEU
</figure>
<figureCaption confidence="0.99376875">
Figure 3: ZH→EN training runs. The cluster of PRO
points to the left corresponds to one of the random initial
models; MERT and RAMPION were able to recover while
PRO was not.
</figureCaption>
<bodyText confidence="0.999837625">
and standard deviations are shown. The three al-
gorithms perform very similarly on the whole, with
certain algorithms performing better on certain lan-
guages. MERT shows larger variation across ran-
dom seeds, as reported by many others in the com-
munity. On average across all language pairs and
test sets, RAMPION leads to slightly higher BLEU
scores.
</bodyText>
<subsectionHeader confidence="0.999227">
4.2 Sensitivity Analysis
</subsectionHeader>
<bodyText confidence="0.95832415">
We now measure the sensitivity of these training
methods to different initializers and to randomness
in the algorithms. RAMPION is deterministic, but
MERT uses random starting points and search di-
rections and PRO uses random sampling to choose
pairs for training its binary classifier.
For initial models, we used the default parame-
ters in Moses as well as two randomly-generated
models.13 We ran RAMPION once with each of the
three initial models, and MERT and PRO three times
with each. This allows us to compare variance due
to initializers as well as due to the nondeterminism
in each algorithm. Fig. 3 plots the results. While
PRO exhibits a small variance for a given initializer,
as also reported by Hopkins and May (2011), it had
13The default weights are 0.3 for reordering features, 0.2 for
phrase table features, 0.5 for the language model, and -1 for the
word penalty. We generated each random model by sampling
each feature weight from a N(µ, v2) with µ equal to the default
weight for that feature and v = lµ/21.
</bodyText>
<figure confidence="0.668305333333333">
MT02 BLEU
35
MT05 BLEU
</figure>
<page confidence="0.936462">
228
</page>
<table confidence="0.9990194">
Method UR→EN ZH→EN MT05 AR→EN MT08 WB avg
MT08* MT09 MT02 MT05 MT08 NW
MERT 24.5 (0.1) 24.6 (0.0) 35.7 (0.3) 34.2 (0.2) 55.0 (0.7) 49.8 (0.3) 32.6 (0.2) 36.6
PRO 24.2 (0.1) 24.2 (0.1) 36.3 (0.1) 34.5 (0.0) 55.6 (0.1) 49.6 (0.0) 31.7 (0.0) 36.6
RAMPION 24.5 24.6 36.4 34.7 55.5 49.8 32.1 36.8
</table>
<tableCaption confidence="0.9856956">
Table 1: %BLEU on several test sets for UR→EN, ZH→EN, and AR→EN translation. Algorithms with randomization
(MERT and PRO) were run three times with different random seeds and averages are shown in each cell followed by
standard deviations in parentheses. All results in this table used a single initial model (the default Moses weights).
The final column shows the average %BLEU across all individual test set scores, so 21 scores were used for MERT
and PRO and 7 for RAMPION.
</tableCaption>
<table confidence="0.99969">
Method UR→EN ZH→EN
Tune MT08* MT09 Tune MT02 MT05
PRO 29.4 22.3 23.0 40.9 35.7 33.6
RAMPION 27.8 24.2 24.6 38.8 36.2 34.3
</table>
<tableCaption confidence="0.99986">
Table 2: %BLEU with large feature sets.
</tableCaption>
<bodyText confidence="0.9995628">
trouble recovering from one of the random initializ-
ers. Therefore, while the within-initializer variance
for PRO tended to be smaller than that of MERT,
PRO’s overall range was larger. RAMPION found
very similar weights regardless of 00.
</bodyText>
<subsectionHeader confidence="0.999627">
4.3 Adding Features
</subsectionHeader>
<bodyText confidence="0.99699159375">
Finally, we compare RAMPION and PRO with an ex-
tended feature set; MERT is excluded as it fails in
such settings (Hopkins and May, 2011).
We added count features for common monolin-
gual and bilingual lexical patterns from the parallel
corpus: the 1k most common bilingual word pairs
from phrase extraction, 200 top unigrams, 1k top bi-
grams, 1k top trigrams, and 4k top trigger pairs ex-
tracted with the method of Rosenfeld (1996), ranked
by mutual information. We integrated the features
with our training procedure by using Moses to gen-
erate lattices instead of k-best lists. We used cube
pruning (Chiang, 2007) to incorporate the additional
(potentially non-local) features while extracting k-
best lists from the lattices to pass to the training al-
gorithms.14
Results are shown in Table 2. We find that PRO
finds much higher BLEU scores on the tuning data
but fails to generalize, leading to poor performance
on the held-out test sets. We suspect that incorporat-
ing regularization into training the binary classifier
within PRO may mitigate this overfitting. RAMPION
is more stable by contrast. This is a challenging
learning task, as lexical features are prone to over-
14In cube pruning, each node’s local n-best list had n = 100.
fitting with a small tuning set. Hopkins and May
(2011) similarly found little gain on test data when
using extended feature sets in phrase-based transla-
tion for these two language pairs.
Results for AR→EN translation were similar and
are omitted for space; these and additional experi-
ments will be included in a forthcoming report.
</bodyText>
<sectionHeader confidence="0.999643" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999751">
We have framed MT training as empirical risk min-
imization and clarified loss functions that were op-
timized by well-known procedures. We have pro-
posed directly optimizing the structured ramp loss
implicit in prior work with a novel algorithm—
RAMPION—which performs comparably to state-
of-the-art training algorithms and is empirically
more stable. Our source code, which integrates
easily with Moses, is available at www.ark.cs.
cmu.edu/MT.
</bodyText>
<sectionHeader confidence="0.998632" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998727375">
We thank Colin Cherry, Chris Dyer, Joseph Keshet,
David McAllester, and members of the ARK research
group for helpful comments that improved this paper.
This research was supported in part by the NSF through
CAREER grant IIS-1054319, the U. S. Army Research
Laboratory and the U. S. Army Research Office under
contract/grant number W911NF-10-1-0533, and Sandia
National Laboratories (fellowship to K. Gimpel).
</bodyText>
<sectionHeader confidence="0.990792" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.520749125">
A. Arun and P. Koehn. 2007. Online learning methods
for discriminative training of phrase based statistical
machine translation. In Proc. of MT Summit XI.
P. Blunsom and M. Osborne. 2008. Probabilistic infer-
ence for machine translation. In Proc. of EMNLP.
P. Blunsom, T. Cohn, and M. Osborne. 2008. A discrim-
inative latent variable model for statistical machine
translation. In Proc. of ACL.
</reference>
<page confidence="0.995274">
229
</page>
<reference confidence="0.999491235849057">
D. Cer, D. Jurafsky, and C. Manning. 2008. Regular-
ization and search for minimum error rate training. In
Proc. of ACL-2008 Workshop on Statistical Machine
Translation.
D. Cer. 2011. Parameterizing Phrase Based Statisti-
cal Machine Translation Models: An Analytic Study.
Ph.D. thesis, Stanford University.
P. Chang, M. Galley, and C. Manning. 2008. Optimiz-
ing Chinese word segmentation for machine transla-
tion performance. In Proc. of ACL-2008 Workshop on
Statistical Machine Translation.
S. Chen and J. Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Techni-
cal report 10-98, Harvard University.
C. Cherry and G. Foster. 2012. Batch tuning strategies
for statistical machine translation. In Proc. of NAACL.
D. Chiang, Y. Marton, and P. Resnik. 2008. Online large-
margin training of syntactic and structural translation
features. In Proc. of EMNLP.
D. Chiang, W. Wang, and K. Knight. 2009. 11,001 new
features for statistical machine translation. In Proc. of
NAACL-HLT.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proc. ofACL.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201–228.
J. H. Clark, C. Dyer, A. Lavie, and N. A. Smith. 2011.
Better hypothesis testing for statistical machine trans-
lation: Controlling for optimizer instability. In Proc.
of ACL.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. In Proc. of EMNLP.
R. Collobert, F. Sinz, J. Weston, and L. Bottou. 2006.
Trading convexity for scalability. In ICML.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research,
7:551–585.
C. B. Do, Q. Le, C. H. Teo, O. Chapelle, and A. Smola.
2008. Tighter bounds for structured estimation. In
Proc. of NIPS.
R. O. Duda and P. E. Hart. 1973. Pattern classification
and scene analysis. John Wiley, New York.
G. Foster and R. Kuhn. 2009. Stabilizing minimum error
rate training. In Proc. of Fourth Workshop on Statisti-
cal Machine Translation.
K. Gimpel and N. A. Smith. 2010. Softmax-margin
CRFs: Training log-linear models with cost functions.
In Proc. of NAACL.
M. Hopkins and J. May. 2011. Tuning as ranking. In
Proc. of EMNLP.
B. H. Juang, W. Chou, and C. H. Lee. 1997. Minimum
classification error rate methods for speech recogni-
tion. Speech and Audio Processing, IEEE Transac-
tions on, 5(3):257–265, may.
J. Kaiser, B. Horvat, and Z. Kacic. 2000. A novel loss
function for the overall risk criterion based discrimina-
tive training of hmm models. In Proc. of ICSLP.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of HLT-NAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proc. of ACL (demo
session).
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. ofICML.
Y. Lee, K. Papineni, S. Roukos, O. Emam, and H. Hassan.
2003. Language model based Arabic word segmenta-
tion. In Proc. of ACL.
Z. Li and J. Eisner. 2009. First- and second-order ex-
pectation semirings with applications to minimum-risk
training on translation forests. In Proc. of EMNLP.
P. Liang, A. Bouchard-Cˆot´e, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In Proc. of COLING-ACL.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluating automatic evaluation metrics for
machine translation. In Proc. of Coling.
W. Macherey, F. Och, I. Thayer, and J. Uszkoreit. 2008.
Lattice-based minimum error rate training for statisti-
cal machine translation. In EMNLP.
A. F. T. Martins, K. Gimpel, N. A. Smith, E. P. Xing,
P. M. Q. Aguiar, and M A. T. Figueiredo. 2010. Learn-
ing structured classifiers with dual coordinate descent.
Technical report, Carnegie Mellon University.
D. McAllester and J. Keshet. 2011. Generalization
bounds and consistency for latent structural probit and
ramp loss. In Proc. of NIPS.
D. McAllester, T. Hazan, and J. Keshet. 2010. Direct
loss minimization for structured prediction. In Proc.
of NIPS.
R. C. Moore and C. Quirk. 2008. Random restarts
in minimum error rate training for statistical machine
translation. In Proc. of Coling.
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proc. of ACL.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1).
F. J. Och. 2003. Minimum error rate training for statisti-
cal machine translation. In Proc. of ACL.
</reference>
<page confidence="0.951548">
230
</page>
<reference confidence="0.999901657142857">
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2001.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
D. Povey and P. C. Woodland. 2002. Minimum phone
error and I-smoothing for improved discrimative train-
ing. In Proc. of ICASSP.
A. Quattoni, M. Collins, and T. Darrell. 2004. Condi-
tional random fields for object recognition. In NIPS
17.
N. Ratliff, J. A. Bagnell, and M. Zinkevich. 2006.
Subgradient methods for maximum margin structured
learning. In ICML Workshop on Learning in Struc-
tured Output Spaces.
R. Rosenfeld. 1996. A maximum entropy approach
to adaptive statistical language modeling. Computer,
Speech and Language, 10(3).
D. A. Smith and J. Eisner. 2006. Minimum risk an-
nealing for training log-linear models. In Proc. of
COLING-ACL.
A. Stolcke. 2002. SRILM—an extensible language mod-
eling toolkit. In Proc. of ICSLP.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In Advances in NIPS 16.
V. Vapnik. 1998. Statistical learning theory. Wiley.
T. Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki.
2007. Online large-margin training for statistical ma-
chine translation. In Proc. of EMNLP-CoNLL.
C. J. Yu and T. Joachims. 2009. Learning structural
SVMs with latent variables. In Proc. of ICML.
A. L. Yuille and Anand Rangarajan. 2002. The concave-
convex procedure (CCCP). In Proc. of NIPS. MIT
Press.
R. Zens, S. Hasan, and H. Ney. 2007. A systematic com-
parison of training criteria for statistical machine trans-
lation. In Proc. of EMNLP.
</reference>
<page confidence="0.997937">
231
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.514795">
<title confidence="0.997311">Structured Ramp Loss Minimization for Machine Translation</title>
<author confidence="0.653081">A Gimpel</author>
<affiliation confidence="0.768343">Language Technologies Carnegie Mellon</affiliation>
<address confidence="0.992162">Pittsburgh, PA 15213,</address>
<abstract confidence="0.999436764705882">This paper seeks to close the gap between training algorithms used in statistical machine translation and machine learning, specifically the framework of empirical risk minimization. We review well-known algorithms, arguing that they do not optimize the loss functions they are assumed to optimize when applied to machine translation. Instead, most have imconnections to particular forms of We propose to minimize ramp loss directly and present a training algorithm that is easy to implement and that performs comparably to others. Most notably, our structured loss minimization algorithm, is less sensitive to initialization and random seeds than standard approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Arun</author>
<author>P Koehn</author>
</authors>
<title>Online learning methods for discriminative training of phrase based statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of MT</booktitle>
<location>Summit XI.</location>
<contexts>
<context position="18248" citStr="Arun and Koehn, 2007" startWordPosition="3060" endWordPosition="3063">d the perceptron in several ways for use in MT. The first was to generalize it to handle latent variables. The second change relates to the need to compute the feature vector for the reference translation y(i), which may be unreachable (§1(i)). To address this, researchers have proposed the use of surrogates that are both favored by the current model parameters and similar to the reference. Och and Ney (2002) were the first to do so, using the translation on a k-best list with the highest evaluation metric score as yT. This practice was followed by Liang et al. (2006) and others with success (Arun and Koehn, 2007; Watanabe et al., 2007).7 Perceptron Loss Though typically described and 7Liang et al. (2006) also tried a variant that updated directly to the reference when it is reachable (“bold updating”), but they and others found that Och and Ney’s strategy worked better. analyzed procedurally, it is straightforward to show that Collins’ perceptron (without latent variables) equates to SSD with fixed step size 1 on loss: −score(x(i), y(i); B)+ max yE%X(&apos;)) (12) This loss is convex but ignores cost functions. In our notation, yT = y(i) and yi = argmaxyE%X(&apos;)) score(x(i), y; B). Adaptation for MT We char</context>
<context position="23410" citStr="Arun and Koehn (2007)" startWordPosition="3988" endWordPosition="3991">lgorithms did not actually optimize the structured hinge loss, for similar reasons to those mentioned above for the perceptron: latent variables and surrogate references. Incorporating latent variables in the hinge loss results in the latent structured hinge loss (Yu and Joachims, 2009). Like the latent perceptron, this loss is nonconvex and inappropriate for MT because it requires computing the feature vector for y(i). By using a surrogate instead of y(i), the actual loss optimized becomes closer to Eq. 8 (Fig. 2), the third form of the latent structured ramp loss. Watanabe et al. (2007) and Arun and Koehn (2007) used k-best oracles like Liang et al., but Chiang et al. (2008, 2009) used a different approach, explicitly defining the surrogate as hy+, h+i in Fig. 1. While the method of Chiang et al. showed impres226 sive performance improvements, its implementation is non-trivial, involving a complex cost function and a parallel architecture, and it has not yet been embraced by the MT community. Indeed, the complexity of Chiang et al’s algorithm was one of the reasons cited for the development of PRO (Hopkins and May, 2011). In this paper, we have sought to isolate the loss functions used in prior work </context>
</contexts>
<marker>Arun, Koehn, 2007</marker>
<rawString>A. Arun and P. Koehn. 2007. Online learning methods for discriminative training of phrase based statistical machine translation. In Proc. of MT Summit XI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blunsom</author>
<author>M Osborne</author>
</authors>
<title>Probabilistic inference for machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="25426" citStr="Blunsom and Osborne (2008)" startWordPosition="4345" endWordPosition="4348">) popularized the use of loglinear models for MT and initially sought to optimize log loss, but by using the min-cost translation on a k-best list as their surrogate, we argue that their loss was closer to the soft ramp loss obtained by softening the second max in lossramp 2 in Eq. 7 (Fig. 2). The same is true for others who aimed to optimize log loss for MT (Smith and Eisner, 2006; Zens et al., 2007; Cer, 2011). The softmax version of the latent variable perceptron loss, Eq. 9 (Fig. 2), is the latent log loss inherent in latent-variable CRFs (Quattoni et al., 2004). Blunsom et al. (2008) and Blunsom and Osborne (2008) actually did optimize latent log loss for MT, discarding training examples for which y(O was unreachable by the model. Finally, we note that “softening” the ramp loss in Eq. 6 (Fig. 2) results in the Jensen risk bound from Gimpel and Smith (2010), which is a computationally-attractive upper bound on the Bayes risk. 4 Experiments The goal of our experiments is to compare RAMPION (Alg. 1) to state-of-the-art methods for training MT systems. RAMPION minimizes lossramp 3, which we found in preliminary experiments to work better than other loss functions tested.8 System and Datasets We use the Mos</context>
</contexts>
<marker>Blunsom, Osborne, 2008</marker>
<rawString>P. Blunsom and M. Osborne. 2008. Probabilistic inference for machine translation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blunsom</author>
<author>T Cohn</author>
<author>M Osborne</author>
</authors>
<title>A discriminative latent variable model for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="25395" citStr="Blunsom et al. (2008)" startWordPosition="4340" endWordPosition="4343">, 2001). Och and Ney (2002) popularized the use of loglinear models for MT and initially sought to optimize log loss, but by using the min-cost translation on a k-best list as their surrogate, we argue that their loss was closer to the soft ramp loss obtained by softening the second max in lossramp 2 in Eq. 7 (Fig. 2). The same is true for others who aimed to optimize log loss for MT (Smith and Eisner, 2006; Zens et al., 2007; Cer, 2011). The softmax version of the latent variable perceptron loss, Eq. 9 (Fig. 2), is the latent log loss inherent in latent-variable CRFs (Quattoni et al., 2004). Blunsom et al. (2008) and Blunsom and Osborne (2008) actually did optimize latent log loss for MT, discarding training examples for which y(O was unreachable by the model. Finally, we note that “softening” the ramp loss in Eq. 6 (Fig. 2) results in the Jensen risk bound from Gimpel and Smith (2010), which is a computationally-attractive upper bound on the Bayes risk. 4 Experiments The goal of our experiments is to compare RAMPION (Alg. 1) to state-of-the-art methods for training MT systems. RAMPION minimizes lossramp 3, which we found in preliminary experiments to work better than other loss functions tested.8 Sys</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>P. Blunsom, T. Cohn, and M. Osborne. 2008. A discriminative latent variable model for statistical machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cer</author>
<author>D Jurafsky</author>
<author>C Manning</author>
</authors>
<title>Regularization and search for minimum error rate training.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-2008 Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="8687" citStr="Cer et al., 2008" startWordPosition="1442" endWordPosition="1445"> the best outputs from the decoder without any regularization (i.e., R(θ) = 0).4 The loss is non-convex and not differentiable for cost functions like BLEU, so Och (2003) developed a coordinate ascent procedure with a specialized line search. MERT avoids the need to compute feature vectors for the references (§1(i)) and allows corpuslevel metrics like BLEU to be easily incorporated. However, the complexity of the loss and the difficulty of the search lead to instabilities during learning. Remedies have been suggested, typically involving additional search directions and experiment replicates (Cer et al., 2008; Moore and Quirk, 2008; Foster and Kuhn, 2009; Clark et al., 2011). But despite these improvements, MERT is ineffectual for training weights for large numbers of features; in addition to anecdotal evidence from the MT community, Hopkins and May (2011) illustrated with synthetic data experiments that MERT struggles increasingly to find the optimal solution as the number of parameters grows. Example 2: Probabilistic Models. By exponentiating and normalizing score(x, y, h; θ), we obtain a conditional log-linear model, which is useful for training criteria with probabilistic interpretations: pe(y</context>
<context position="10074" citStr="Cer et al. (2008)" startWordPosition="1672" endWordPosition="1675">should not be confused with the Bayes risk framework, which uses a probability distribution (Eq. 4) and a cost function to define a loss: lossB risk = E�i=1 L&apos;pO(,/,h|„(z))[cost(y(i),y)I (5) The use of this loss is often simply called “risk minimization” in the speech and MT communities. Bayes risk is non-convex, whether or not latent variables are present. Like MERT, it naturally avoids the need to compute features for y(i) and uses a cost function, making it appealing for MT. Bayes risk minimization first appeared in the speech recognition community (Kaiser et al., 2000; Povey and 4However, Cer et al. (2008) and Macherey et al. (2008) achieved a sort of regularization by altering MERT’s line search. Woodland, 2002) and more recently has been applied to MT (Smith and Eisner, 2006; Zens et al., 2007; Li and Eisner, 2009). 3 Training Methods for MT In this section we consider other ML-inspired approaches to MT training, situating each in the framework from §2: ramp, perceptron, hinge, and “soft” losses. Each of the first three kinds of losses can be understood as a way of selecting, for each x(i), two candidate translation/derivation pairs: (yT, hT) and (y�, h�). During training, the loss function c</context>
</contexts>
<marker>Cer, Jurafsky, Manning, 2008</marker>
<rawString>D. Cer, D. Jurafsky, and C. Manning. 2008. Regularization and search for minimum error rate training. In Proc. of ACL-2008 Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cer</author>
</authors>
<title>Parameterizing Phrase Based Statistical Machine Translation Models: An Analytic Study.</title>
<date>2011</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="25215" citStr="Cer, 2011" startWordPosition="4310" endWordPosition="4311">elihood training criterion which is frequently used when a probabilistic interpretation of the learned model is desired, as in conditional random fields (Lafferty et al., 2001). Och and Ney (2002) popularized the use of loglinear models for MT and initially sought to optimize log loss, but by using the min-cost translation on a k-best list as their surrogate, we argue that their loss was closer to the soft ramp loss obtained by softening the second max in lossramp 2 in Eq. 7 (Fig. 2). The same is true for others who aimed to optimize log loss for MT (Smith and Eisner, 2006; Zens et al., 2007; Cer, 2011). The softmax version of the latent variable perceptron loss, Eq. 9 (Fig. 2), is the latent log loss inherent in latent-variable CRFs (Quattoni et al., 2004). Blunsom et al. (2008) and Blunsom and Osborne (2008) actually did optimize latent log loss for MT, discarding training examples for which y(O was unreachable by the model. Finally, we note that “softening” the ramp loss in Eq. 6 (Fig. 2) results in the Jensen risk bound from Gimpel and Smith (2010), which is a computationally-attractive upper bound on the Bayes risk. 4 Experiments The goal of our experiments is to compare RAMPION (Alg. 1</context>
</contexts>
<marker>Cer, 2011</marker>
<rawString>D. Cer. 2011. Parameterizing Phrase Based Statistical Machine Translation Models: An Analytic Study. Ph.D. thesis, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Chang</author>
<author>M Galley</author>
<author>C Manning</author>
</authors>
<title>Optimizing Chinese word segmentation for machine translation performance.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-2008 Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="27267" citStr="Chang et al., 2008" startWordPosition="4645" endWordPosition="4648">8). For each language pair, we used the English side of the parallel text and 600M words of randomlyselected sentences from the Gigaword v4 corpus (excluding NYT and LAT). For UR—*EN, we used parallel data from the NIST MT08 evaluation consisting of 1.2M Urdu words and 1.1M English words. We used half of the documents (882 sentences) from the MT08 test set for tuning. We used the remaining half for one test set (“MT08*”) and MT09 as our other test set. For ZH—*EN, we used 303k sentence pairs from the FBIS corpus (LDC2003E14). We segmented the Chinese data using the Stanford Chinese segmenter (Chang et al., 2008) in “CTB” mode, giving us 7.9M Chinese words and 9.4M English words. We used MT03 for tuning and used MT02 and MT05 for testing. For AR—*EN, we used data provided by the LDC 8We only present full results using lossramp 3. We found that minimizing lossramp 1 did poorly, resulting in single-digit BLEU scores, and that lossramp 2 reached high BLEU scores on the tuning data but failed to generalize well. Softened versions of the ramp losses performed comparably to lossramp 3 but were slightly worse on both tuning and held-out data. 9We found similar trends for other language pairs and systems, inc</context>
</contexts>
<marker>Chang, Galley, Manning, 2008</marker>
<rawString>P. Chang, M. Galley, and C. Manning. 2008. Optimizing Chinese word segmentation for machine translation performance. In Proc. of ACL-2008 Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical report 10-98,</tech>
<institution>Harvard University.</institution>
<contexts>
<context position="26650" citStr="Chen and Goodman, 1998" startWordPosition="4536" endWordPosition="4539">s phrasebased MT system (Koehn et al., 2007) and consider Urdu—*English (UR—*EN), Chinese—*English (ZH—*EN) translation, and Arabic—*English (AR—*EN) translation.9 We trained a Moses system using default settings and features, except for setting the distortion limit to 10. Word alignment was performed using GIZA++ (Och and Ney, 2003) in both directions, the grow-diag-final-and heuristic was used to symmetrize the alignments, and a max phrase length of 7 was used for phrase extraction. We estimated 5-gram language models using the SRI toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). For each language pair, we used the English side of the parallel text and 600M words of randomlyselected sentences from the Gigaword v4 corpus (excluding NYT and LAT). For UR—*EN, we used parallel data from the NIST MT08 evaluation consisting of 1.2M Urdu words and 1.1M English words. We used half of the documents (882 sentences) from the MT08 test set for tuning. We used the remaining half for one test set (“MT08*”) and MT09 as our other test set. For ZH—*EN, we used 303k sentence pairs from the FBIS corpus (LDC2003E14). We segmented the Chinese data using the Stanford Chinese segmenter (Ch</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>S. Chen and J. Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical report 10-98, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cherry</author>
<author>G Foster</author>
</authors>
<title>Batch tuning strategies for statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="7893" citStr="Cherry and Foster (2012)" startWordPosition="1312" endWordPosition="1315">cost of the predictions on the training data. This idea has been used in the pattern recognition and speech recognition communities (Duda and Hart, 1973; Juang et al., 1997); its first application to MT was by Och (2003). The loss function takes � � the following form: losscost �X, Y� , θ = (3) 2We will abuse notation and allow cost to operate on both sets of sentences as well as individual sentences. For notational convenience we also let cost accept hidden variables but assume that the hidden variables do not affect the value; i.e., cost((y, h), (y&apos;, h&apos;)) = cost(y, (y&apos;, h&apos;)) = cost(y, y&apos;). 3Cherry and Foster (2012) have concurrently performed a similar analysis. . � cost � � � N Y, argmax score(x(i), y, h; θ) � (y,h)ET(x(z)) i=1 222 MERT directly minimizes the corpus-level cost function of the best outputs from the decoder without any regularization (i.e., R(θ) = 0).4 The loss is non-convex and not differentiable for cost functions like BLEU, so Och (2003) developed a coordinate ascent procedure with a specialized line search. MERT avoids the need to compute feature vectors for the references (§1(i)) and allows corpuslevel metrics like BLEU to be easily incorporated. However, the complexity of the loss </context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>C. Cherry and G. Foster. 2012. Batch tuning strategies for statistical machine translation. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
<author>Y Marton</author>
<author>P Resnik</author>
</authors>
<title>Online largemargin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1135" citStr="Chiang et al., 2008" startWordPosition="162" endWordPosition="165">applied to machine translation. Instead, most have implicit connections to particular forms of ramp loss. We propose to minimize ramp loss directly and present a training algorithm that is easy to implement and that performs comparably to others. Most notably, our structured ramp loss minimization algorithm, RAMPION, is less sensitive to initialization and random seeds than standard approaches. 1 Introduction Every statistical MT system relies on a training algorithm to fit the parameters of a scoring function to examples from parallel text. Well-known examples include MERT (Och, 2003), MIRA (Chiang et al., 2008), and PRO (Hopkins and May, 2011). While such procedures can be analyzed as machine learning algorithms—e.g., in the general framework of empirical risk minimization (Vapnik, 1998)—their procedural specifications have made this difficult. From a practical perspective, such algorithms are often complex, difficult to replicate, and sensitive to initialization, random seeds, and other hyperparameters. In this paper, we consider training algorithms that are first specified declaratively, as loss functions to be minimized. We relate well-known training algorithms for MT to particular loss functions</context>
<context position="13738" citStr="Chiang et al. (2008" startWordPosition="2307" endWordPosition="2310">+, h+). output is shown as (y−, h−) in Fig. 1; finding y↓ is often called cost-augmented decoding, which is also used to define hinge loss (§3.3). The second form, Eq. 7, penalizes the model prediction ((y↓, h↓) = (y, h)) and favors an output pair that has both high model score and low cost; this is the converse of cost-augmented decoding and therefore we call it cost-diminished decoding; (y↑, h↑) = (y+, h+) in Fig. 1. The third form, Eq. 8, sets (y↑, h↑) = (y+, h+) and (y↓, h↓) = (y−, h−). This loss underlies RAMPION. It is similar to the loss optimized by the MIRA-inspired algorithm used by Chiang et al. (2008, 2009). Optimization The ramp losses are continuous but non-convex and non-differentiable, so gradientbased optimization methods are not available.5 Fortunately, Eq. 8 can be optimized by using a concaveconvex procedure (CCCP; Yuille and Rangarajan, 2002). CCCP is a batch optimization algorithm for any function that is the the sum of a concave and a convex function. The idea is to approximate the sum as the convex term plus a tangent line to the concavecfunction at the current parameter values; the resulting sum is convex and can be optimized with (sub)gradient methods. 5For non-differentiabl</context>
<context position="15834" citStr="Chiang et al., 2008" startWordPosition="2657" endWordPosition="2660"> ramp losses. The first step done on each iteration is to generate k-best lists for the full tuning set (line 3). We then run CCCP on the k-best lists for V iterations (lines 4–15). This involves first finding the translation to update towards for all sentences in the tuning set (lines 5–7), then making parameter updates in an online fashion with V epochs of stochastic subgradient descent (lines 8–14). The subgradient update for the E2 regularization term is done in line 11 and then for the loss in line 12.6 Unlike prior work that targeted similar loss func- cost tions (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), we do not use a fully online algorithm such as MIRA in an outer loop because we are not aware of an online learning algorithm with theoretical guarantees for non-differentiable, nonconvex loss functions like the ramp losses. CCCP 6f2 regularization done here regularizes toward 00, not 0. 224 lossramp 1 = N lossramp 2 = − max (scorei(y,h;B)) + max (scorei(y,h;B) + costi(y)) lossramp 3 = (y,h)ETi (y,h)ETi lossperc = i=1 lossperc kbest = N � − max (scorei(y,h;B) − costi(y)) + max (scorei(y,h;B)) (y,h)ETi (y,h)ETi i=1 N − max (scorei(y,h;B) − costi(y)) + max (scorei(y,h;B) </context>
<context position="21535" citStr="Chiang et al., 2008" startWordPosition="3671" endWordPosition="3675">p loss. Thus, one way to understand Liang et al.’s algorithm is as a form of structured ramp loss. However, another interpretation is given by McAllester et al. (2010), who showed that procedures like that used by Liang et al. approach direct cost minimization in the limiting case. 3.3 Large-Margin Methods A related family of approaches for training MT models involves the margin-infused relaxed algorithm (MIRA; Crammer et al., 2006), an online largemargin training algorithm. It has recently shown success for MT, particularly when training models with large feature sets (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009). In order to apply it to MT, Watanabe et al. and Chiang et al. made modifications similar to those made by Liang et al. for perceptron training, namely the extension to latent variables and the use of a surrogate reference with high model score and low cost. Hinge Loss It can be shown that 1-best MIRA corresponds to dual coordinate ascent for the structured hinge loss when using E2 regularization (Martins et al., 2010). The structured hinge is the loss underlying maximum-margin Markov networks (Taskar et al., 2003): setting yT = y(i) and: � ) yi = argmax score(x(i), y; θ</context>
<context position="23473" citStr="Chiang et al. (2008" startWordPosition="4000" endWordPosition="4004">r similar reasons to those mentioned above for the perceptron: latent variables and surrogate references. Incorporating latent variables in the hinge loss results in the latent structured hinge loss (Yu and Joachims, 2009). Like the latent perceptron, this loss is nonconvex and inappropriate for MT because it requires computing the feature vector for y(i). By using a surrogate instead of y(i), the actual loss optimized becomes closer to Eq. 8 (Fig. 2), the third form of the latent structured ramp loss. Watanabe et al. (2007) and Arun and Koehn (2007) used k-best oracles like Liang et al., but Chiang et al. (2008, 2009) used a different approach, explicitly defining the surrogate as hy+, h+i in Fig. 1. While the method of Chiang et al. showed impres226 sive performance improvements, its implementation is non-trivial, involving a complex cost function and a parallel architecture, and it has not yet been embraced by the MT community. Indeed, the complexity of Chiang et al’s algorithm was one of the reasons cited for the development of PRO (Hopkins and May, 2011). In this paper, we have sought to isolate the loss functions used in prior work like that by Chiang et al. and identify simple, generic optimiz</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>D. Chiang, Y. Marton, and P. Resnik. 2008. Online largemargin training of syntactic and structural translation features. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
<author>W Wang</author>
<author>K Knight</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL-HLT.</booktitle>
<contexts>
<context position="15856" citStr="Chiang et al., 2009" startWordPosition="2661" endWordPosition="2664">st step done on each iteration is to generate k-best lists for the full tuning set (line 3). We then run CCCP on the k-best lists for V iterations (lines 4–15). This involves first finding the translation to update towards for all sentences in the tuning set (lines 5–7), then making parameter updates in an online fashion with V epochs of stochastic subgradient descent (lines 8–14). The subgradient update for the E2 regularization term is done in line 11 and then for the loss in line 12.6 Unlike prior work that targeted similar loss func- cost tions (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), we do not use a fully online algorithm such as MIRA in an outer loop because we are not aware of an online learning algorithm with theoretical guarantees for non-differentiable, nonconvex loss functions like the ramp losses. CCCP 6f2 regularization done here regularizes toward 00, not 0. 224 lossramp 1 = N lossramp 2 = − max (scorei(y,h;B)) + max (scorei(y,h;B) + costi(y)) lossramp 3 = (y,h)ETi (y,h)ETi lossperc = i=1 lossperc kbest = N � − max (scorei(y,h;B) − costi(y)) + max (scorei(y,h;B)) (y,h)ETi (y,h)ETi i=1 N − max (scorei(y,h;B) − costi(y)) + max (scorei(y,h;B) + costi(y)) (y,h)ETi (</context>
<context position="21557" citStr="Chiang et al., 2009" startWordPosition="3676" endWordPosition="3679"> to understand Liang et al.’s algorithm is as a form of structured ramp loss. However, another interpretation is given by McAllester et al. (2010), who showed that procedures like that used by Liang et al. approach direct cost minimization in the limiting case. 3.3 Large-Margin Methods A related family of approaches for training MT models involves the margin-infused relaxed algorithm (MIRA; Crammer et al., 2006), an online largemargin training algorithm. It has recently shown success for MT, particularly when training models with large feature sets (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009). In order to apply it to MT, Watanabe et al. and Chiang et al. made modifications similar to those made by Liang et al. for perceptron training, namely the extension to latent variables and the use of a surrogate reference with high model score and low cost. Hinge Loss It can be shown that 1-best MIRA corresponds to dual coordinate ascent for the structured hinge loss when using E2 regularization (Martins et al., 2010). The structured hinge is the loss underlying maximum-margin Markov networks (Taskar et al., 2003): setting yT = y(i) and: � ) yi = argmax score(x(i), y; θ) + cost(y(i), y) ���(</context>
</contexts>
<marker>Chiang, Wang, Knight, 2009</marker>
<rawString>D. Chiang, W. Wang, and K. Knight. 2009. 11,001 new features for statistical machine translation. In Proc. of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="5782" citStr="Chiang, 2005" startWordPosition="946" endWordPosition="947">rs. Risk minimization corresponds to choosing argminoE© Ep(X,Y) [loss (X, Y , θ)] (1) where p(X, Y ) is the (unknown) true joint distribution over corpora. We note that the loss function depends on the entire corpus, while the decoder operates independently on one sentence at a time. This is done to fit the standard assumptions in MT systems: the evaluation metric (e.g., BLEU) depends on 1For phrase-based MT, a segmentation of the source and target sentences into phrases and an alignment between them (Koehn et al., 2003). For hierarchical phrase-based MT, a derivation under a synchronous CFG (Chiang, 2005). the entire corpus and does not decompose linearly, while the model score does. Since in practice we do not know p(X, Y ), but we do have access to an actual corpus pair h�X, Y�i, where X = {x(i)}Ni=1 and Y� = {y(i)}N i=1,we instead consider regularized empirical risk minimization: argminoEe loss(X, Y, θ) + R(θ) (2) where R(θ) is the regularization function used to mitigate overfitting. The regularization function is frequently a squared norm of the parameter vector, such as the E1 or E2 norm, but many other choices are possible. In this paper, we use E2. Models are evaluated using a task-spe</context>
<context position="27894" citStr="Chiang, 2005" startWordPosition="4754" endWordPosition="4755">, giving us 7.9M Chinese words and 9.4M English words. We used MT03 for tuning and used MT02 and MT05 for testing. For AR—*EN, we used data provided by the LDC 8We only present full results using lossramp 3. We found that minimizing lossramp 1 did poorly, resulting in single-digit BLEU scores, and that lossramp 2 reached high BLEU scores on the tuning data but failed to generalize well. Softened versions of the ramp losses performed comparably to lossramp 3 but were slightly worse on both tuning and held-out data. 9We found similar trends for other language pairs and systems, including Hiero (Chiang, 2005). A forthcoming report will present these results, as well as experiments with additional loss functions, in detail. 227 MERT PRO Rampion for the NIST evaluations, including 3.29M sentence pairs of UN data and 982k sentence pairs of nonUN data. The Arabic data was preprocessed using an HMM segmenter that splits off attached prepositional phrases, personal pronouns, and the future marker (Lee et al., 2003). The common stylistic sentence-initial wa# (and ...) was removed from the training and test data. The resulting corpus contained 130M Arabic tokens and 130M English tokens. We used MT06 for t</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>D. Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="33792" citStr="Chiang, 2007" startWordPosition="5775" endWordPosition="5776">res Finally, we compare RAMPION and PRO with an extended feature set; MERT is excluded as it fails in such settings (Hopkins and May, 2011). We added count features for common monolingual and bilingual lexical patterns from the parallel corpus: the 1k most common bilingual word pairs from phrase extraction, 200 top unigrams, 1k top bigrams, 1k top trigrams, and 4k top trigger pairs extracted with the method of Rosenfeld (1996), ranked by mutual information. We integrated the features with our training procedure by using Moses to generate lattices instead of k-best lists. We used cube pruning (Chiang, 2007) to incorporate the additional (potentially non-local) features while extracting kbest lists from the lattices to pass to the training algorithms.14 Results are shown in Table 2. We find that PRO finds much higher BLEU scores on the tuning data but fails to generalize, leading to poor performance on the held-out test sets. We suspect that incorporating regularization into training the binary classifier within PRO may mitigate this overfitting. RAMPION is more stable by contrast. This is a challenging learning task, as lexical features are prone to over14In cube pruning, each node’s local n-bes</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>D. Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Clark</author>
<author>C Dyer</author>
<author>A Lavie</author>
<author>N A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="8754" citStr="Clark et al., 2011" startWordPosition="1454" endWordPosition="1457">.e., R(θ) = 0).4 The loss is non-convex and not differentiable for cost functions like BLEU, so Och (2003) developed a coordinate ascent procedure with a specialized line search. MERT avoids the need to compute feature vectors for the references (§1(i)) and allows corpuslevel metrics like BLEU to be easily incorporated. However, the complexity of the loss and the difficulty of the search lead to instabilities during learning. Remedies have been suggested, typically involving additional search directions and experiment replicates (Cer et al., 2008; Moore and Quirk, 2008; Foster and Kuhn, 2009; Clark et al., 2011). But despite these improvements, MERT is ineffectual for training weights for large numbers of features; in addition to anecdotal evidence from the MT community, Hopkins and May (2011) illustrated with synthetic data experiments that MERT struggles increasingly to find the optimal solution as the number of parameters grows. Example 2: Probabilistic Models. By exponentiating and normalizing score(x, y, h; θ), we obtain a conditional log-linear model, which is useful for training criteria with probabilistic interpretations: pe(y, h x) = 1 z(„,e) exp{score(x, y, h; θ)} (4) The log loss then defi</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>J. H. Clark, C. Dyer, A. Lavie, and N. A. Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="2161" citStr="Collins, 2002" startWordPosition="323" endWordPosition="324">per, we consider training algorithms that are first specified declaratively, as loss functions to be minimized. We relate well-known training algorithms for MT to particular loss functions. We show that a family of structured ramp loss functions (Do et al., 2008) is useful for this analysis. For example, McAllester and Keshet (2011) recently suggested that, while Chiang et al. (2008, 2009) described their algorithm as “MIRA” (Crammer et al., 2006), in fact it targets a kind of ramp loss. We note here other examples: Liang et al. (2006) described their algorithm as a variant of the perceptron (Collins, 2002), which has a unique loss function, but the loss actually optimized is closer to a particular ramp loss (that differs from the one targeted by Chiang et al.). Och and Ney (2002) sought to optimize log loss (likelihood in a probabilistic model; Lafferty et al., 2001) but actually optimized a version of the soft ramp loss. Why isn’t the application of ML to MT more straightforward? We note two key reasons: (i) ML generally assumes that the correct output can always be scored by a model, but in MT the reference translation is often unreachable, due to a model’s limited expressive power or search </context>
<context position="17426" citStr="Collins, 2002" startWordPosition="2919" endWordPosition="2920">n loss(X, Y, B); we suppress the arguments for clarity. “Ti” is shorthand for “T(x(i)).” “Xi” is shorthand for the k-best list for x(i). “costi(·)” is shorthand for “cost(y(i), ·).” “scorei(·)” is shorthand for “score(x(i), ·).” As noted in §3.4, any operator of the form maxsES can be replaced by log EsES exp, known as softmax, giving many additional loss functions. is fundamentally a batch optimization algorithm and has been used for solving many non-convex learning problems, such as latent structured SVMs (Yu and Joachims, 2009). 3.2 Structured Perceptron The stuctured perceptron algorithm (Collins, 2002) was considered by Liang et al. (2006) as an alternative to MERT. It requires only a decoder and comes with some attractive guarantees, at least for models without latent variables. Liang et al. modified the perceptron in several ways for use in MT. The first was to generalize it to handle latent variables. The second change relates to the need to compute the feature vector for the reference translation y(i), which may be unreachable (§1(i)). To address this, researchers have proposed the use of surrogates that are both favored by the current model parameters and similar to the reference. Och </context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>F Sinz</author>
<author>J Weston</author>
<author>L Bottou</author>
</authors>
<title>Trading convexity for scalability.</title>
<date>2006</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="11313" citStr="Collobert et al., 2006" startWordPosition="1882" endWordPosition="1885">y increasing the score of the former and decreasing the score of the latter, through manipulation of the parameters θ. Figure 1 gives a general visualization of some of the key output pairs that are considered for these roles. Learning alters the score function, or, in the figure, moves points horizontally so that scores approximate negated costs. 3.1 Structured Ramp Loss Minimization The structured ramp loss (Do et al., 2008) is a non-convex loss function with certain attractive theoretical properties. It is an upper bound on losscost (Eq. 3) and is a tighter bound than other loss functions (Collobert et al., 2006). Ramp loss has been shown to be statistically consistent in the sense that, in the limit of infinite training data, minimizing structured ramp loss reaches the minimum value of losscost that is achievable with a linear model (McAllester and Keshet, 2011). This is true whether or not latent variables are present. Consistency in this sense is not a common property of loss functions; commonly-used convex loss functions such as the perceptron, hinge, and log losses (discussed below) are not consistent, because they are all sensitive to outliers or otherwise noisy training examples. Ramp loss is b</context>
</contexts>
<marker>Collobert, Sinz, Weston, Bottou, 2006</marker>
<rawString>R. Collobert, F. Sinz, J. Weston, and L. Bottou. 2006. Trading convexity for scalability. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>O Dekel</author>
<author>J Keshet</author>
<author>S Shalev-Shwartz</author>
<author>Y Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="1998" citStr="Crammer et al., 2006" startWordPosition="291" endWordPosition="294">m a practical perspective, such algorithms are often complex, difficult to replicate, and sensitive to initialization, random seeds, and other hyperparameters. In this paper, we consider training algorithms that are first specified declaratively, as loss functions to be minimized. We relate well-known training algorithms for MT to particular loss functions. We show that a family of structured ramp loss functions (Do et al., 2008) is useful for this analysis. For example, McAllester and Keshet (2011) recently suggested that, while Chiang et al. (2008, 2009) described their algorithm as “MIRA” (Crammer et al., 2006), in fact it targets a kind of ramp loss. We note here other examples: Liang et al. (2006) described their algorithm as a variant of the perceptron (Collins, 2002), which has a unique loss function, but the loss actually optimized is closer to a particular ramp loss (that differs from the one targeted by Chiang et al.). Och and Ney (2002) sought to optimize log loss (likelihood in a probabilistic model; Lafferty et al., 2001) but actually optimized a version of the soft ramp loss. Why isn’t the application of ML to MT more straightforward? We note two key reasons: (i) ML generally assumes that</context>
<context position="21352" citStr="Crammer et al., 2006" startWordPosition="3642" endWordPosition="3645">s equivalent, they are similar in that they update towards some yT with high model score and low cost. Eq. 11 corresponds to Eq. 7 (Fig. 2), the second form of the latent structured ramp loss. Thus, one way to understand Liang et al.’s algorithm is as a form of structured ramp loss. However, another interpretation is given by McAllester et al. (2010), who showed that procedures like that used by Liang et al. approach direct cost minimization in the limiting case. 3.3 Large-Margin Methods A related family of approaches for training MT models involves the margin-infused relaxed algorithm (MIRA; Crammer et al., 2006), an online largemargin training algorithm. It has recently shown success for MT, particularly when training models with large feature sets (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009). In order to apply it to MT, Watanabe et al. and Chiang et al. made modifications similar to those made by Liang et al. for perceptron training, namely the extension to latent variables and the use of a surrogate reference with high model score and low cost. Hinge Loss It can be shown that 1-best MIRA corresponds to dual coordinate ascent for the structured hinge loss when using E2 regulariz</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. 2006. Online passive-aggressive algorithms. Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C B Do</author>
<author>Q Le</author>
<author>C H Teo</author>
<author>O Chapelle</author>
<author>A Smola</author>
</authors>
<title>Tighter bounds for structured estimation.</title>
<date>2008</date>
<booktitle>In Proc. of NIPS.</booktitle>
<contexts>
<context position="1810" citStr="Do et al., 2008" startWordPosition="262" endWordPosition="265">n be analyzed as machine learning algorithms—e.g., in the general framework of empirical risk minimization (Vapnik, 1998)—their procedural specifications have made this difficult. From a practical perspective, such algorithms are often complex, difficult to replicate, and sensitive to initialization, random seeds, and other hyperparameters. In this paper, we consider training algorithms that are first specified declaratively, as loss functions to be minimized. We relate well-known training algorithms for MT to particular loss functions. We show that a family of structured ramp loss functions (Do et al., 2008) is useful for this analysis. For example, McAllester and Keshet (2011) recently suggested that, while Chiang et al. (2008, 2009) described their algorithm as “MIRA” (Crammer et al., 2006), in fact it targets a kind of ramp loss. We note here other examples: Liang et al. (2006) described their algorithm as a variant of the perceptron (Collins, 2002), which has a unique loss function, but the loss actually optimized is closer to a particular ramp loss (that differs from the one targeted by Chiang et al.). Och and Ney (2002) sought to optimize log loss (likelihood in a probabilistic model; Laffe</context>
<context position="11120" citStr="Do et al., 2008" startWordPosition="1848" endWordPosition="1851"> of losses can be understood as a way of selecting, for each x(i), two candidate translation/derivation pairs: (yT, hT) and (y�, h�). During training, the loss function can be improved by increasing the score of the former and decreasing the score of the latter, through manipulation of the parameters θ. Figure 1 gives a general visualization of some of the key output pairs that are considered for these roles. Learning alters the score function, or, in the figure, moves points horizontally so that scores approximate negated costs. 3.1 Structured Ramp Loss Minimization The structured ramp loss (Do et al., 2008) is a non-convex loss function with certain attractive theoretical properties. It is an upper bound on losscost (Eq. 3) and is a tighter bound than other loss functions (Collobert et al., 2006). Ramp loss has been shown to be statistically consistent in the sense that, in the limit of infinite training data, minimizing structured ramp loss reaches the minimum value of losscost that is achievable with a linear model (McAllester and Keshet, 2011). This is true whether or not latent variables are present. Consistency in this sense is not a common property of loss functions; commonly-used convex l</context>
</contexts>
<marker>Do, Le, Teo, Chapelle, Smola, 2008</marker>
<rawString>C. B. Do, Q. Le, C. H. Teo, O. Chapelle, and A. Smola. 2008. Tighter bounds for structured estimation. In Proc. of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R O Duda</author>
<author>P E Hart</author>
</authors>
<title>Pattern classification and scene analysis.</title>
<date>1973</date>
<publisher>John Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="7421" citStr="Duda and Hart, 1973" startWordPosition="1226" endWordPosition="1229"> fixed set of k-best lists.3 However, most training procedures periodically invoke the decoder to generate new k-best lists, which are then typically merged with those from previous training iterations. It is an open question how this practice affects the loss function being optimized by the procedure as a whole. Example 1: MERT. The most commonly-used training algorithm for machine translation is minimum error rate training, which seeks to directly minimize the cost of the predictions on the training data. This idea has been used in the pattern recognition and speech recognition communities (Duda and Hart, 1973; Juang et al., 1997); its first application to MT was by Och (2003). The loss function takes � � the following form: losscost �X, Y� , θ = (3) 2We will abuse notation and allow cost to operate on both sets of sentences as well as individual sentences. For notational convenience we also let cost accept hidden variables but assume that the hidden variables do not affect the value; i.e., cost((y, h), (y&apos;, h&apos;)) = cost(y, (y&apos;, h&apos;)) = cost(y, y&apos;). 3Cherry and Foster (2012) have concurrently performed a similar analysis. . � cost � � � N Y, argmax score(x(i), y, h; θ) � (y,h)ET(x(z)) i=1 222 MERT di</context>
</contexts>
<marker>Duda, Hart, 1973</marker>
<rawString>R. O. Duda and P. E. Hart. 1973. Pattern classification and scene analysis. John Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Foster</author>
<author>R Kuhn</author>
</authors>
<title>Stabilizing minimum error rate training.</title>
<date>2009</date>
<booktitle>In Proc. of Fourth Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="8733" citStr="Foster and Kuhn, 2009" startWordPosition="1450" endWordPosition="1453">t any regularization (i.e., R(θ) = 0).4 The loss is non-convex and not differentiable for cost functions like BLEU, so Och (2003) developed a coordinate ascent procedure with a specialized line search. MERT avoids the need to compute feature vectors for the references (§1(i)) and allows corpuslevel metrics like BLEU to be easily incorporated. However, the complexity of the loss and the difficulty of the search lead to instabilities during learning. Remedies have been suggested, typically involving additional search directions and experiment replicates (Cer et al., 2008; Moore and Quirk, 2008; Foster and Kuhn, 2009; Clark et al., 2011). But despite these improvements, MERT is ineffectual for training weights for large numbers of features; in addition to anecdotal evidence from the MT community, Hopkins and May (2011) illustrated with synthetic data experiments that MERT struggles increasingly to find the optimal solution as the number of parameters grows. Example 2: Probabilistic Models. By exponentiating and normalizing score(x, y, h; θ), we obtain a conditional log-linear model, which is useful for training criteria with probabilistic interpretations: pe(y, h x) = 1 z(„,e) exp{score(x, y, h; θ)} (4) T</context>
</contexts>
<marker>Foster, Kuhn, 2009</marker>
<rawString>G. Foster and R. Kuhn. 2009. Stabilizing minimum error rate training. In Proc. of Fourth Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Gimpel</author>
<author>N A Smith</author>
</authors>
<title>Softmax-margin CRFs: Training log-linear models with cost functions.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="25673" citStr="Gimpel and Smith (2010)" startWordPosition="4389" endWordPosition="4392"> second max in lossramp 2 in Eq. 7 (Fig. 2). The same is true for others who aimed to optimize log loss for MT (Smith and Eisner, 2006; Zens et al., 2007; Cer, 2011). The softmax version of the latent variable perceptron loss, Eq. 9 (Fig. 2), is the latent log loss inherent in latent-variable CRFs (Quattoni et al., 2004). Blunsom et al. (2008) and Blunsom and Osborne (2008) actually did optimize latent log loss for MT, discarding training examples for which y(O was unreachable by the model. Finally, we note that “softening” the ramp loss in Eq. 6 (Fig. 2) results in the Jensen risk bound from Gimpel and Smith (2010), which is a computationally-attractive upper bound on the Bayes risk. 4 Experiments The goal of our experiments is to compare RAMPION (Alg. 1) to state-of-the-art methods for training MT systems. RAMPION minimizes lossramp 3, which we found in preliminary experiments to work better than other loss functions tested.8 System and Datasets We use the Moses phrasebased MT system (Koehn et al., 2007) and consider Urdu—*English (UR—*EN), Chinese—*English (ZH—*EN) translation, and Arabic—*English (AR—*EN) translation.9 We trained a Moses system using default settings and features, except for setting </context>
</contexts>
<marker>Gimpel, Smith, 2010</marker>
<rawString>K. Gimpel and N. A. Smith. 2010. Softmax-margin CRFs: Training log-linear models with cost functions. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hopkins</author>
<author>J May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1168" citStr="Hopkins and May, 2011" startWordPosition="168" endWordPosition="171"> Instead, most have implicit connections to particular forms of ramp loss. We propose to minimize ramp loss directly and present a training algorithm that is easy to implement and that performs comparably to others. Most notably, our structured ramp loss minimization algorithm, RAMPION, is less sensitive to initialization and random seeds than standard approaches. 1 Introduction Every statistical MT system relies on a training algorithm to fit the parameters of a scoring function to examples from parallel text. Well-known examples include MERT (Och, 2003), MIRA (Chiang et al., 2008), and PRO (Hopkins and May, 2011). While such procedures can be analyzed as machine learning algorithms—e.g., in the general framework of empirical risk minimization (Vapnik, 1998)—their procedural specifications have made this difficult. From a practical perspective, such algorithms are often complex, difficult to replicate, and sensitive to initialization, random seeds, and other hyperparameters. In this paper, we consider training algorithms that are first specified declaratively, as loss functions to be minimized. We relate well-known training algorithms for MT to particular loss functions. We show that a family of struct</context>
<context position="8939" citStr="Hopkins and May (2011)" startWordPosition="1484" endWordPosition="1487">T avoids the need to compute feature vectors for the references (§1(i)) and allows corpuslevel metrics like BLEU to be easily incorporated. However, the complexity of the loss and the difficulty of the search lead to instabilities during learning. Remedies have been suggested, typically involving additional search directions and experiment replicates (Cer et al., 2008; Moore and Quirk, 2008; Foster and Kuhn, 2009; Clark et al., 2011). But despite these improvements, MERT is ineffectual for training weights for large numbers of features; in addition to anecdotal evidence from the MT community, Hopkins and May (2011) illustrated with synthetic data experiments that MERT struggles increasingly to find the optimal solution as the number of parameters grows. Example 2: Probabilistic Models. By exponentiating and normalizing score(x, y, h; θ), we obtain a conditional log-linear model, which is useful for training criteria with probabilistic interpretations: pe(y, h x) = 1 z(„,e) exp{score(x, y, h; θ)} (4) The log loss then defines losslog( X, Y�, θ) = − EN1 log pe(y(i) x(i)). Example 3: Bayes Risk. The term “risk” as used above should not be confused with the Bayes risk framework, which uses a probability dis</context>
<context position="23929" citStr="Hopkins and May, 2011" startWordPosition="4079" endWordPosition="4082">2), the third form of the latent structured ramp loss. Watanabe et al. (2007) and Arun and Koehn (2007) used k-best oracles like Liang et al., but Chiang et al. (2008, 2009) used a different approach, explicitly defining the surrogate as hy+, h+i in Fig. 1. While the method of Chiang et al. showed impres226 sive performance improvements, its implementation is non-trivial, involving a complex cost function and a parallel architecture, and it has not yet been embraced by the MT community. Indeed, the complexity of Chiang et al’s algorithm was one of the reasons cited for the development of PRO (Hopkins and May, 2011). In this paper, we have sought to isolate the loss functions used in prior work like that by Chiang et al. and identify simple, generic optimization procedures for optimizing them. We offer RAMPION as an alternative to Chiang et al’s MIRA that is simpler to implement and achieves empirical success in experiments (§4). 3.4 Likelihood and Softened Losses We can derive new loss functions from the above by converting any “max” operator to a “softmax” (log E exp, where the set of elements under the summation is the same as under the max). For example, the softmax version of the perceptron loss is </context>
<context position="28870" citStr="Hopkins and May (2011)" startWordPosition="4910" endWordPosition="4913">nal pronouns, and the future marker (Lee et al., 2003). The common stylistic sentence-initial wa# (and ...) was removed from the training and test data. The resulting corpus contained 130M Arabic tokens and 130M English tokens. We used MT06 for tuning and three test sets: MT05, the MT08 newswire test set (“MT08 NW”), and the MT08 weblog test set (“MT08 WB”). For all languages we evaluated translation output using case-insensitive IBM BLEU (Papineni et al., 2001). Training Algorithms Our baselines are MERT and PRO as implemented in the Moses toolkit.10 PRO uses the hyperparameter settings from Hopkins and May (2011), including k-best lists of size 1500 and 25 training iterations.11 MERT uses k-best lists of size 100 and was run to convergence. For both MERT and PRO, previous iterations’ k-best lists were merged in. For RAMPION, we used T = 20, T&apos; = 10, T&apos;&apos; = 5, k = 500, q = 0.0001, and C = 1. Our cost function is α(1 − BLEU+1(y, y&apos;)) where BLEU+1(y, y&apos;) returns the BLEU+1 score (Lin and Och, 2004) for reference y and hypothesis y&apos;. We used α = 10. We used these same hyperparameter values for all experiments reported here and found them to perform well across other language pairs and systems.12 4.1 Result</context>
<context position="31669" citStr="Hopkins and May (2011)" startWordPosition="5405" endWordPosition="5408"> algorithms. RAMPION is deterministic, but MERT uses random starting points and search directions and PRO uses random sampling to choose pairs for training its binary classifier. For initial models, we used the default parameters in Moses as well as two randomly-generated models.13 We ran RAMPION once with each of the three initial models, and MERT and PRO three times with each. This allows us to compare variance due to initializers as well as due to the nondeterminism in each algorithm. Fig. 3 plots the results. While PRO exhibits a small variance for a given initializer, as also reported by Hopkins and May (2011), it had 13The default weights are 0.3 for reordering features, 0.2 for phrase table features, 0.5 for the language model, and -1 for the word penalty. We generated each random model by sampling each feature weight from a N(µ, v2) with µ equal to the default weight for that feature and v = lµ/21. MT02 BLEU 35 MT05 BLEU 228 Method UR→EN ZH→EN MT05 AR→EN MT08 WB avg MT08* MT09 MT02 MT05 MT08 NW MERT 24.5 (0.1) 24.6 (0.0) 35.7 (0.3) 34.2 (0.2) 55.0 (0.7) 49.8 (0.3) 32.6 (0.2) 36.6 PRO 24.2 (0.1) 24.2 (0.1) 36.3 (0.1) 34.5 (0.0) 55.6 (0.1) 49.6 (0.0) 31.7 (0.0) 36.6 RAMPION 24.5 24.6 36.4 34.7 55.</context>
<context position="33318" citStr="Hopkins and May, 2011" startWordPosition="5695" endWordPosition="5698">scores, so 21 scores were used for MERT and PRO and 7 for RAMPION. Method UR→EN ZH→EN Tune MT08* MT09 Tune MT02 MT05 PRO 29.4 22.3 23.0 40.9 35.7 33.6 RAMPION 27.8 24.2 24.6 38.8 36.2 34.3 Table 2: %BLEU with large feature sets. trouble recovering from one of the random initializers. Therefore, while the within-initializer variance for PRO tended to be smaller than that of MERT, PRO’s overall range was larger. RAMPION found very similar weights regardless of 00. 4.3 Adding Features Finally, we compare RAMPION and PRO with an extended feature set; MERT is excluded as it fails in such settings (Hopkins and May, 2011). We added count features for common monolingual and bilingual lexical patterns from the parallel corpus: the 1k most common bilingual word pairs from phrase extraction, 200 top unigrams, 1k top bigrams, 1k top trigrams, and 4k top trigger pairs extracted with the method of Rosenfeld (1996), ranked by mutual information. We integrated the features with our training procedure by using Moses to generate lattices instead of k-best lists. We used cube pruning (Chiang, 2007) to incorporate the additional (potentially non-local) features while extracting kbest lists from the lattices to pass to the </context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>M. Hopkins and J. May. 2011. Tuning as ranking. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B H Juang</author>
<author>W Chou</author>
<author>C H Lee</author>
</authors>
<title>Minimum classification error rate methods for speech recognition. Speech and Audio Processing,</title>
<date>1997</date>
<journal>IEEE Transactions on,</journal>
<volume>5</volume>
<issue>3</issue>
<contexts>
<context position="7442" citStr="Juang et al., 1997" startWordPosition="1230" endWordPosition="1233">lists.3 However, most training procedures periodically invoke the decoder to generate new k-best lists, which are then typically merged with those from previous training iterations. It is an open question how this practice affects the loss function being optimized by the procedure as a whole. Example 1: MERT. The most commonly-used training algorithm for machine translation is minimum error rate training, which seeks to directly minimize the cost of the predictions on the training data. This idea has been used in the pattern recognition and speech recognition communities (Duda and Hart, 1973; Juang et al., 1997); its first application to MT was by Och (2003). The loss function takes � � the following form: losscost �X, Y� , θ = (3) 2We will abuse notation and allow cost to operate on both sets of sentences as well as individual sentences. For notational convenience we also let cost accept hidden variables but assume that the hidden variables do not affect the value; i.e., cost((y, h), (y&apos;, h&apos;)) = cost(y, (y&apos;, h&apos;)) = cost(y, y&apos;). 3Cherry and Foster (2012) have concurrently performed a similar analysis. . � cost � � � N Y, argmax score(x(i), y, h; θ) � (y,h)ET(x(z)) i=1 222 MERT directly minimizes the </context>
</contexts>
<marker>Juang, Chou, Lee, 1997</marker>
<rawString>B. H. Juang, W. Chou, and C. H. Lee. 1997. Minimum classification error rate methods for speech recognition. Speech and Audio Processing, IEEE Transactions on, 5(3):257–265, may.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kaiser</author>
<author>B Horvat</author>
<author>Z Kacic</author>
</authors>
<title>A novel loss function for the overall risk criterion based discriminative training of hmm models.</title>
<date>2000</date>
<booktitle>In Proc. of ICSLP.</booktitle>
<contexts>
<context position="10035" citStr="Kaiser et al., 2000" startWordPosition="1665" endWordPosition="1668">ayes Risk. The term “risk” as used above should not be confused with the Bayes risk framework, which uses a probability distribution (Eq. 4) and a cost function to define a loss: lossB risk = E�i=1 L&apos;pO(,/,h|„(z))[cost(y(i),y)I (5) The use of this loss is often simply called “risk minimization” in the speech and MT communities. Bayes risk is non-convex, whether or not latent variables are present. Like MERT, it naturally avoids the need to compute features for y(i) and uses a cost function, making it appealing for MT. Bayes risk minimization first appeared in the speech recognition community (Kaiser et al., 2000; Povey and 4However, Cer et al. (2008) and Macherey et al. (2008) achieved a sort of regularization by altering MERT’s line search. Woodland, 2002) and more recently has been applied to MT (Smith and Eisner, 2006; Zens et al., 2007; Li and Eisner, 2009). 3 Training Methods for MT In this section we consider other ML-inspired approaches to MT training, situating each in the framework from §2: ramp, perceptron, hinge, and “soft” losses. Each of the first three kinds of losses can be understood as a way of selecting, for each x(i), two candidate translation/derivation pairs: (yT, hT) and (y�, h�</context>
</contexts>
<marker>Kaiser, Horvat, Kacic, 2000</marker>
<rawString>J. Kaiser, B. Horvat, and Z. Kacic. 2000. A novel loss function for the overall risk criterion based discriminative training of hmm models. In Proc. of ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<contexts>
<context position="5695" citStr="Koehn et al., 2003" startWordPosition="932" endWordPosition="935">translations, and the model parameters to a real value indicating the quality of the parameters. Risk minimization corresponds to choosing argminoE© Ep(X,Y) [loss (X, Y , θ)] (1) where p(X, Y ) is the (unknown) true joint distribution over corpora. We note that the loss function depends on the entire corpus, while the decoder operates independently on one sentence at a time. This is done to fit the standard assumptions in MT systems: the evaluation metric (e.g., BLEU) depends on 1For phrase-based MT, a segmentation of the source and target sentences into phrases and an alignment between them (Koehn et al., 2003). For hierarchical phrase-based MT, a derivation under a synchronous CFG (Chiang, 2005). the entire corpus and does not decompose linearly, while the model score does. Since in practice we do not know p(X, Y ), but we do have access to an actual corpus pair h�X, Y�i, where X = {x(i)}Ni=1 and Y� = {y(i)}N i=1,we instead consider regularized empirical risk minimization: argminoEe loss(X, Y, θ) + R(θ) (2) where R(θ) is the regularization function used to mitigate overfitting. The regularization function is frequently a squared norm of the parameter vector, such as the E1 or E2 norm, but many othe</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-based translation. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL</booktitle>
<note>(demo session).</note>
<contexts>
<context position="26071" citStr="Koehn et al., 2007" startWordPosition="4454" endWordPosition="4457">ent log loss for MT, discarding training examples for which y(O was unreachable by the model. Finally, we note that “softening” the ramp loss in Eq. 6 (Fig. 2) results in the Jensen risk bound from Gimpel and Smith (2010), which is a computationally-attractive upper bound on the Bayes risk. 4 Experiments The goal of our experiments is to compare RAMPION (Alg. 1) to state-of-the-art methods for training MT systems. RAMPION minimizes lossramp 3, which we found in preliminary experiments to work better than other loss functions tested.8 System and Datasets We use the Moses phrasebased MT system (Koehn et al., 2007) and consider Urdu—*English (UR—*EN), Chinese—*English (ZH—*EN) translation, and Arabic—*English (AR—*EN) translation.9 We trained a Moses system using default settings and features, except for setting the distortion limit to 10. Word alignment was performed using GIZA++ (Och and Ney, 2003) in both directions, the grow-diag-final-and heuristic was used to symmetrize the alignments, and a max phrase length of 7 was used for phrase extraction. We estimated 5-gram language models using the SRI toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). For each language p</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of ACL (demo session).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. ofICML.</booktitle>
<contexts>
<context position="2427" citStr="Lafferty et al., 2001" startWordPosition="368" endWordPosition="371">2008) is useful for this analysis. For example, McAllester and Keshet (2011) recently suggested that, while Chiang et al. (2008, 2009) described their algorithm as “MIRA” (Crammer et al., 2006), in fact it targets a kind of ramp loss. We note here other examples: Liang et al. (2006) described their algorithm as a variant of the perceptron (Collins, 2002), which has a unique loss function, but the loss actually optimized is closer to a particular ramp loss (that differs from the one targeted by Chiang et al.). Och and Ney (2002) sought to optimize log loss (likelihood in a probabilistic model; Lafferty et al., 2001) but actually optimized a version of the soft ramp loss. Why isn’t the application of ML to MT more straightforward? We note two key reasons: (i) ML generally assumes that the correct output can always be scored by a model, but in MT the reference translation is often unreachable, due to a model’s limited expressive power or search error, requiring the use of “surrogate” references; (ii) MT models nearly always include latent derivation variables, leading to non-convex losses that have generally received little attention in ML. In this paper, we discuss how these two have caused a disconnect b</context>
<context position="24781" citStr="Lafferty et al., 2001" startWordPosition="4223" endWordPosition="4226">al’s MIRA that is simpler to implement and achieves empirical success in experiments (§4). 3.4 Likelihood and Softened Losses We can derive new loss functions from the above by converting any “max” operator to a “softmax” (log E exp, where the set of elements under the summation is the same as under the max). For example, the softmax version of the perceptron loss is the well-known log loss (§2, Ex. 2), the loss underlying the conditional likelihood training criterion which is frequently used when a probabilistic interpretation of the learned model is desired, as in conditional random fields (Lafferty et al., 2001). Och and Ney (2002) popularized the use of loglinear models for MT and initially sought to optimize log loss, but by using the min-cost translation on a k-best list as their surrogate, we argue that their loss was closer to the soft ramp loss obtained by softening the second max in lossramp 2 in Eq. 7 (Fig. 2). The same is true for others who aimed to optimize log loss for MT (Smith and Eisner, 2006; Zens et al., 2007; Cer, 2011). The softmax version of the latent variable perceptron loss, Eq. 9 (Fig. 2), is the latent log loss inherent in latent-variable CRFs (Quattoni et al., 2004). Blunsom</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. ofICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Lee</author>
<author>K Papineni</author>
<author>S Roukos</author>
<author>O Emam</author>
<author>H Hassan</author>
</authors>
<title>Language model based Arabic word segmentation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="28302" citStr="Lee et al., 2003" startWordPosition="4818" endWordPosition="4821">ons of the ramp losses performed comparably to lossramp 3 but were slightly worse on both tuning and held-out data. 9We found similar trends for other language pairs and systems, including Hiero (Chiang, 2005). A forthcoming report will present these results, as well as experiments with additional loss functions, in detail. 227 MERT PRO Rampion for the NIST evaluations, including 3.29M sentence pairs of UN data and 982k sentence pairs of nonUN data. The Arabic data was preprocessed using an HMM segmenter that splits off attached prepositional phrases, personal pronouns, and the future marker (Lee et al., 2003). The common stylistic sentence-initial wa# (and ...) was removed from the training and test data. The resulting corpus contained 130M Arabic tokens and 130M English tokens. We used MT06 for tuning and three test sets: MT05, the MT08 newswire test set (“MT08 NW”), and the MT08 weblog test set (“MT08 WB”). For all languages we evaluated translation output using case-insensitive IBM BLEU (Papineni et al., 2001). Training Algorithms Our baselines are MERT and PRO as implemented in the Moses toolkit.10 PRO uses the hyperparameter settings from Hopkins and May (2011), including k-best lists of size</context>
</contexts>
<marker>Lee, Papineni, Roukos, Emam, Hassan, 2003</marker>
<rawString>Y. Lee, K. Papineni, S. Roukos, O. Emam, and H. Hassan. 2003. Language model based Arabic word segmentation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Li</author>
<author>J Eisner</author>
</authors>
<title>First- and second-order expectation semirings with applications to minimum-risk training on translation forests.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="10289" citStr="Li and Eisner, 2009" startWordPosition="1710" endWordPosition="1713">is often simply called “risk minimization” in the speech and MT communities. Bayes risk is non-convex, whether or not latent variables are present. Like MERT, it naturally avoids the need to compute features for y(i) and uses a cost function, making it appealing for MT. Bayes risk minimization first appeared in the speech recognition community (Kaiser et al., 2000; Povey and 4However, Cer et al. (2008) and Macherey et al. (2008) achieved a sort of regularization by altering MERT’s line search. Woodland, 2002) and more recently has been applied to MT (Smith and Eisner, 2006; Zens et al., 2007; Li and Eisner, 2009). 3 Training Methods for MT In this section we consider other ML-inspired approaches to MT training, situating each in the framework from §2: ramp, perceptron, hinge, and “soft” losses. Each of the first three kinds of losses can be understood as a way of selecting, for each x(i), two candidate translation/derivation pairs: (yT, hT) and (y�, h�). During training, the loss function can be improved by increasing the score of the former and decreasing the score of the latter, through manipulation of the parameters θ. Figure 1 gives a general visualization of some of the key output pairs that are </context>
</contexts>
<marker>Li, Eisner, 2009</marker>
<rawString>Z. Li and J. Eisner. 2009. First- and second-order expectation semirings with applications to minimum-risk training on translation forests. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>A Bouchard-Cˆot´e</author>
<author>D Klein</author>
<author>B Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL.</booktitle>
<marker>Liang, Bouchard-Cˆot´e, Klein, Taskar, 2006</marker>
<rawString>P. Liang, A. Bouchard-Cˆot´e, D. Klein, and B. Taskar. 2006. An end-to-end discriminative approach to machine translation. In Proc. of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>Orange: a method for evaluating automatic evaluation metrics for machine translation.</title>
<date>2004</date>
<booktitle>In Proc. of Coling.</booktitle>
<contexts>
<context position="29259" citStr="Lin and Och, 2004" startWordPosition="4986" endWordPosition="4989">ranslation output using case-insensitive IBM BLEU (Papineni et al., 2001). Training Algorithms Our baselines are MERT and PRO as implemented in the Moses toolkit.10 PRO uses the hyperparameter settings from Hopkins and May (2011), including k-best lists of size 1500 and 25 training iterations.11 MERT uses k-best lists of size 100 and was run to convergence. For both MERT and PRO, previous iterations’ k-best lists were merged in. For RAMPION, we used T = 20, T&apos; = 10, T&apos;&apos; = 5, k = 500, q = 0.0001, and C = 1. Our cost function is α(1 − BLEU+1(y, y&apos;)) where BLEU+1(y, y&apos;) returns the BLEU+1 score (Lin and Och, 2004) for reference y and hypothesis y&apos;. We used α = 10. We used these same hyperparameter values for all experiments reported here and found them to perform well across other language pairs and systems.12 4.1 Results Table 1 shows our results. MERT and PRO were run 3 times with differing random seeds and averages 10The PRO algorithm samples pairs of translations from kbest lists on each iteration and trains a binary classifier to rank pairs according to the cost function. The loss function underlying PRO depends on the choice of binary classifier and also on the sampling strategy. We leave an anal</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och. 2004. Orange: a method for evaluating automatic evaluation metrics for machine translation. In Proc. of Coling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Macherey</author>
<author>F Och</author>
<author>I Thayer</author>
<author>J Uszkoreit</author>
</authors>
<title>Lattice-based minimum error rate training for statistical machine translation.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="10101" citStr="Macherey et al. (2008)" startWordPosition="1677" endWordPosition="1680"> with the Bayes risk framework, which uses a probability distribution (Eq. 4) and a cost function to define a loss: lossB risk = E�i=1 L&apos;pO(,/,h|„(z))[cost(y(i),y)I (5) The use of this loss is often simply called “risk minimization” in the speech and MT communities. Bayes risk is non-convex, whether or not latent variables are present. Like MERT, it naturally avoids the need to compute features for y(i) and uses a cost function, making it appealing for MT. Bayes risk minimization first appeared in the speech recognition community (Kaiser et al., 2000; Povey and 4However, Cer et al. (2008) and Macherey et al. (2008) achieved a sort of regularization by altering MERT’s line search. Woodland, 2002) and more recently has been applied to MT (Smith and Eisner, 2006; Zens et al., 2007; Li and Eisner, 2009). 3 Training Methods for MT In this section we consider other ML-inspired approaches to MT training, situating each in the framework from §2: ramp, perceptron, hinge, and “soft” losses. Each of the first three kinds of losses can be understood as a way of selecting, for each x(i), two candidate translation/derivation pairs: (yT, hT) and (y�, h�). During training, the loss function can be improved by increasin</context>
</contexts>
<marker>Macherey, Och, Thayer, Uszkoreit, 2008</marker>
<rawString>W. Macherey, F. Och, I. Thayer, and J. Uszkoreit. 2008. Lattice-based minimum error rate training for statistical machine translation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>K Gimpel</author>
<author>N A Smith</author>
<author>E P Xing</author>
<author>P M Q Aguiar</author>
<author>M A T Figueiredo</author>
</authors>
<title>Learning structured classifiers with dual coordinate descent.</title>
<date>2010</date>
<tech>Technical report,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="21980" citStr="Martins et al., 2010" startWordPosition="3752" endWordPosition="3755">line largemargin training algorithm. It has recently shown success for MT, particularly when training models with large feature sets (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009). In order to apply it to MT, Watanabe et al. and Chiang et al. made modifications similar to those made by Liang et al. for perceptron training, namely the extension to latent variables and the use of a surrogate reference with high model score and low cost. Hinge Loss It can be shown that 1-best MIRA corresponds to dual coordinate ascent for the structured hinge loss when using E2 regularization (Martins et al., 2010). The structured hinge is the loss underlying maximum-margin Markov networks (Taskar et al., 2003): setting yT = y(i) and: � ) yi = argmax score(x(i), y; θ) + cost(y(i), y) ���(����) (13) Unlike the perceptron losses, which penalize the highest-scoring outputs, hinge loss penalizes an output that is both favored by the model and has high cost. Such an output is shown as hy−, h−i in Fig. 1; the structured hinge loss focuses on pushing such outputs to the left. As mentioned in §3.1, finding yi is often called cost-augmented decoding. Structured hinge loss is convex, can incorporate a cost functi</context>
</contexts>
<marker>Martins, Gimpel, Smith, Xing, Aguiar, Figueiredo, 2010</marker>
<rawString>A. F. T. Martins, K. Gimpel, N. A. Smith, E. P. Xing, P. M. Q. Aguiar, and M A. T. Figueiredo. 2010. Learning structured classifiers with dual coordinate descent. Technical report, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McAllester</author>
<author>J Keshet</author>
</authors>
<title>Generalization bounds and consistency for latent structural probit and ramp loss.</title>
<date>2011</date>
<booktitle>In Proc. of NIPS.</booktitle>
<contexts>
<context position="1881" citStr="McAllester and Keshet (2011)" startWordPosition="273" endWordPosition="276">eneral framework of empirical risk minimization (Vapnik, 1998)—their procedural specifications have made this difficult. From a practical perspective, such algorithms are often complex, difficult to replicate, and sensitive to initialization, random seeds, and other hyperparameters. In this paper, we consider training algorithms that are first specified declaratively, as loss functions to be minimized. We relate well-known training algorithms for MT to particular loss functions. We show that a family of structured ramp loss functions (Do et al., 2008) is useful for this analysis. For example, McAllester and Keshet (2011) recently suggested that, while Chiang et al. (2008, 2009) described their algorithm as “MIRA” (Crammer et al., 2006), in fact it targets a kind of ramp loss. We note here other examples: Liang et al. (2006) described their algorithm as a variant of the perceptron (Collins, 2002), which has a unique loss function, but the loss actually optimized is closer to a particular ramp loss (that differs from the one targeted by Chiang et al.). Och and Ney (2002) sought to optimize log loss (likelihood in a probabilistic model; Lafferty et al., 2001) but actually optimized a version of the soft ramp los</context>
<context position="11568" citStr="McAllester and Keshet, 2011" startWordPosition="1924" endWordPosition="1927">re function, or, in the figure, moves points horizontally so that scores approximate negated costs. 3.1 Structured Ramp Loss Minimization The structured ramp loss (Do et al., 2008) is a non-convex loss function with certain attractive theoretical properties. It is an upper bound on losscost (Eq. 3) and is a tighter bound than other loss functions (Collobert et al., 2006). Ramp loss has been shown to be statistically consistent in the sense that, in the limit of infinite training data, minimizing structured ramp loss reaches the minimum value of losscost that is achievable with a linear model (McAllester and Keshet, 2011). This is true whether or not latent variables are present. Consistency in this sense is not a common property of loss functions; commonly-used convex loss functions such as the perceptron, hinge, and log losses (discussed below) are not consistent, because they are all sensitive to outliers or otherwise noisy training examples. Ramp loss is better at dealing with outliers in the training data (Collobert et al., 2006). There are three forms of latent structured ramp loss: Eq. 6–8 (Fig. 2). Ramp losses are appealing for MT because they do not require computing the feature vector of y(i) (§1(i))</context>
</contexts>
<marker>McAllester, Keshet, 2011</marker>
<rawString>D. McAllester and J. Keshet. 2011. Generalization bounds and consistency for latent structural probit and ramp loss. In Proc. of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McAllester</author>
<author>T Hazan</author>
<author>J Keshet</author>
</authors>
<title>Direct loss minimization for structured prediction.</title>
<date>2010</date>
<booktitle>In Proc. of NIPS.</booktitle>
<contexts>
<context position="21083" citStr="McAllester et al. (2010)" startWordPosition="3600" endWordPosition="3603">cost. Fig. 1 illustrates the similarity by showing that the min-cost output on a k-best list resides in a similar region of the output space as hy+, h+i computed from the full output space. While it is not the case that we can always choose &apos;Yi so as to make the two losses equivalent, they are similar in that they update towards some yT with high model score and low cost. Eq. 11 corresponds to Eq. 7 (Fig. 2), the second form of the latent structured ramp loss. Thus, one way to understand Liang et al.’s algorithm is as a form of structured ramp loss. However, another interpretation is given by McAllester et al. (2010), who showed that procedures like that used by Liang et al. approach direct cost minimization in the limiting case. 3.3 Large-Margin Methods A related family of approaches for training MT models involves the margin-infused relaxed algorithm (MIRA; Crammer et al., 2006), an online largemargin training algorithm. It has recently shown success for MT, particularly when training models with large feature sets (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009). In order to apply it to MT, Watanabe et al. and Chiang et al. made modifications similar to those made by Liang et al. for p</context>
</contexts>
<marker>McAllester, Hazan, Keshet, 2010</marker>
<rawString>D. McAllester, T. Hazan, and J. Keshet. 2010. Direct loss minimization for structured prediction. In Proc. of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Moore</author>
<author>C Quirk</author>
</authors>
<title>Random restarts in minimum error rate training for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of Coling.</booktitle>
<contexts>
<context position="8710" citStr="Moore and Quirk, 2008" startWordPosition="1446" endWordPosition="1449">from the decoder without any regularization (i.e., R(θ) = 0).4 The loss is non-convex and not differentiable for cost functions like BLEU, so Och (2003) developed a coordinate ascent procedure with a specialized line search. MERT avoids the need to compute feature vectors for the references (§1(i)) and allows corpuslevel metrics like BLEU to be easily incorporated. However, the complexity of the loss and the difficulty of the search lead to instabilities during learning. Remedies have been suggested, typically involving additional search directions and experiment replicates (Cer et al., 2008; Moore and Quirk, 2008; Foster and Kuhn, 2009; Clark et al., 2011). But despite these improvements, MERT is ineffectual for training weights for large numbers of features; in addition to anecdotal evidence from the MT community, Hopkins and May (2011) illustrated with synthetic data experiments that MERT struggles increasingly to find the optimal solution as the number of parameters grows. Example 2: Probabilistic Models. By exponentiating and normalizing score(x, y, h; θ), we obtain a conditional log-linear model, which is useful for training criteria with probabilistic interpretations: pe(y, h x) = 1 z(„,e) exp{s</context>
</contexts>
<marker>Moore, Quirk, 2008</marker>
<rawString>R. C. Moore and C. Quirk. 2008. Random restarts in minimum error rate training for statistical machine translation. In Proc. of Coling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="2338" citStr="Och and Ney (2002)" startWordPosition="354" endWordPosition="357"> loss functions. We show that a family of structured ramp loss functions (Do et al., 2008) is useful for this analysis. For example, McAllester and Keshet (2011) recently suggested that, while Chiang et al. (2008, 2009) described their algorithm as “MIRA” (Crammer et al., 2006), in fact it targets a kind of ramp loss. We note here other examples: Liang et al. (2006) described their algorithm as a variant of the perceptron (Collins, 2002), which has a unique loss function, but the loss actually optimized is closer to a particular ramp loss (that differs from the one targeted by Chiang et al.). Och and Ney (2002) sought to optimize log loss (likelihood in a probabilistic model; Lafferty et al., 2001) but actually optimized a version of the soft ramp loss. Why isn’t the application of ML to MT more straightforward? We note two key reasons: (i) ML generally assumes that the correct output can always be scored by a model, but in MT the reference translation is often unreachable, due to a model’s limited expressive power or search error, requiring the use of “surrogate” references; (ii) MT models nearly always include latent derivation variables, leading to non-convex losses that have generally received l</context>
<context position="18040" citStr="Och and Ney (2002)" startWordPosition="3022" endWordPosition="3025">002) was considered by Liang et al. (2006) as an alternative to MERT. It requires only a decoder and comes with some attractive guarantees, at least for models without latent variables. Liang et al. modified the perceptron in several ways for use in MT. The first was to generalize it to handle latent variables. The second change relates to the need to compute the feature vector for the reference translation y(i), which may be unreachable (§1(i)). To address this, researchers have proposed the use of surrogates that are both favored by the current model parameters and similar to the reference. Och and Ney (2002) were the first to do so, using the translation on a k-best list with the highest evaluation metric score as yT. This practice was followed by Liang et al. (2006) and others with success (Arun and Koehn, 2007; Watanabe et al., 2007).7 Perceptron Loss Though typically described and 7Liang et al. (2006) also tried a variant that updated directly to the reference when it is reachable (“bold updating”), but they and others found that Och and Ney’s strategy worked better. analyzed procedurally, it is straightforward to show that Collins’ perceptron (without latent variables) equates to SSD with fix</context>
<context position="24801" citStr="Och and Ney (2002)" startWordPosition="4227" endWordPosition="4230">r to implement and achieves empirical success in experiments (§4). 3.4 Likelihood and Softened Losses We can derive new loss functions from the above by converting any “max” operator to a “softmax” (log E exp, where the set of elements under the summation is the same as under the max). For example, the softmax version of the perceptron loss is the well-known log loss (§2, Ex. 2), the loss underlying the conditional likelihood training criterion which is frequently used when a probabilistic interpretation of the learned model is desired, as in conditional random fields (Lafferty et al., 2001). Och and Ney (2002) popularized the use of loglinear models for MT and initially sought to optimize log loss, but by using the min-cost translation on a k-best list as their surrogate, we argue that their loss was closer to the soft ramp loss obtained by softening the second max in lossramp 2 in Eq. 7 (Fig. 2). The same is true for others who aimed to optimize log loss for MT (Smith and Eisner, 2006; Zens et al., 2007; Cer, 2011). The softmax version of the latent variable perceptron loss, Eq. 9 (Fig. 2), is the latent log loss inherent in latent-variable CRFs (Quattoni et al., 2004). Blunsom et al. (2008) and B</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>F. J. Och and H. Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="26362" citStr="Och and Ney, 2003" startWordPosition="4493" endWordPosition="4496">4 Experiments The goal of our experiments is to compare RAMPION (Alg. 1) to state-of-the-art methods for training MT systems. RAMPION minimizes lossramp 3, which we found in preliminary experiments to work better than other loss functions tested.8 System and Datasets We use the Moses phrasebased MT system (Koehn et al., 2007) and consider Urdu—*English (UR—*EN), Chinese—*English (ZH—*EN) translation, and Arabic—*English (AR—*EN) translation.9 We trained a Moses system using default settings and features, except for setting the distortion limit to 10. Word alignment was performed using GIZA++ (Och and Ney, 2003) in both directions, the grow-diag-final-and heuristic was used to symmetrize the alignments, and a max phrase length of 7 was used for phrase extraction. We estimated 5-gram language models using the SRI toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). For each language pair, we used the English side of the parallel text and 600M words of randomlyselected sentences from the Gigaword v4 corpus (excluding NYT and LAT). For UR—*EN, we used parallel data from the NIST MT08 evaluation consisting of 1.2M Urdu words and 1.1M English words. We used half of the docu</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1107" citStr="Och, 2003" startWordPosition="159" endWordPosition="160"> to optimize when applied to machine translation. Instead, most have implicit connections to particular forms of ramp loss. We propose to minimize ramp loss directly and present a training algorithm that is easy to implement and that performs comparably to others. Most notably, our structured ramp loss minimization algorithm, RAMPION, is less sensitive to initialization and random seeds than standard approaches. 1 Introduction Every statistical MT system relies on a training algorithm to fit the parameters of a scoring function to examples from parallel text. Well-known examples include MERT (Och, 2003), MIRA (Chiang et al., 2008), and PRO (Hopkins and May, 2011). While such procedures can be analyzed as machine learning algorithms—e.g., in the general framework of empirical risk minimization (Vapnik, 1998)—their procedural specifications have made this difficult. From a practical perspective, such algorithms are often complex, difficult to replicate, and sensitive to initialization, random seeds, and other hyperparameters. In this paper, we consider training algorithms that are first specified declaratively, as loss functions to be minimized. We relate well-known training algorithms for MT </context>
<context position="7489" citStr="Och (2003)" startWordPosition="1241" endWordPosition="1242">nvoke the decoder to generate new k-best lists, which are then typically merged with those from previous training iterations. It is an open question how this practice affects the loss function being optimized by the procedure as a whole. Example 1: MERT. The most commonly-used training algorithm for machine translation is minimum error rate training, which seeks to directly minimize the cost of the predictions on the training data. This idea has been used in the pattern recognition and speech recognition communities (Duda and Hart, 1973; Juang et al., 1997); its first application to MT was by Och (2003). The loss function takes � � the following form: losscost �X, Y� , θ = (3) 2We will abuse notation and allow cost to operate on both sets of sentences as well as individual sentences. For notational convenience we also let cost accept hidden variables but assume that the hidden variables do not affect the value; i.e., cost((y, h), (y&apos;, h&apos;)) = cost(y, (y&apos;, h&apos;)) = cost(y, y&apos;). 3Cherry and Foster (2012) have concurrently performed a similar analysis. . � cost � � � N Y, argmax score(x(i), y, h; θ) � (y,h)ET(x(z)) i=1 222 MERT directly minimizes the corpus-level cost function of the best outputs </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training for statistical machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="6686" citStr="Papineni et al., 2001" startWordPosition="1105" endWordPosition="1108">: argminoEe loss(X, Y, θ) + R(θ) (2) where R(θ) is the regularization function used to mitigate overfitting. The regularization function is frequently a squared norm of the parameter vector, such as the E1 or E2 norm, but many other choices are possible. In this paper, we use E2. Models are evaluated using a task-specific notion of error, here encoded as a cost function, cost : YN × YN → R&gt;0, such that the worse a translation is, the higher its cost. The cost function will typically make use of an automatic evaluation metric for machine translation; e.g., cost might be 1 minus the BLEU score (Papineni et al., 2001).2 We note that our analysis in this paper is applicable for understanding the loss function being optimized given a fixed set of k-best lists.3 However, most training procedures periodically invoke the decoder to generate new k-best lists, which are then typically merged with those from previous training iterations. It is an open question how this practice affects the loss function being optimized by the procedure as a whole. Example 1: MERT. The most commonly-used training algorithm for machine translation is minimum error rate training, which seeks to directly minimize the cost of the predi</context>
<context position="28714" citStr="Papineni et al., 2001" startWordPosition="4886" endWordPosition="4889">UN data and 982k sentence pairs of nonUN data. The Arabic data was preprocessed using an HMM segmenter that splits off attached prepositional phrases, personal pronouns, and the future marker (Lee et al., 2003). The common stylistic sentence-initial wa# (and ...) was removed from the training and test data. The resulting corpus contained 130M Arabic tokens and 130M English tokens. We used MT06 for tuning and three test sets: MT05, the MT08 newswire test set (“MT08 NW”), and the MT08 weblog test set (“MT08 WB”). For all languages we evaluated translation output using case-insensitive IBM BLEU (Papineni et al., 2001). Training Algorithms Our baselines are MERT and PRO as implemented in the Moses toolkit.10 PRO uses the hyperparameter settings from Hopkins and May (2011), including k-best lists of size 1500 and 25 training iterations.11 MERT uses k-best lists of size 100 and was run to convergence. For both MERT and PRO, previous iterations’ k-best lists were merged in. For RAMPION, we used T = 20, T&apos; = 10, T&apos;&apos; = 5, k = 500, q = 0.0001, and C = 1. Our cost function is α(1 − BLEU+1(y, y&apos;)) where BLEU+1(y, y&apos;) returns the BLEU+1 score (Lin and Och, 2004) for reference y and hypothesis y&apos;. We used α = 10. We </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2001. BLEU: a method for automatic evaluation of machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Povey</author>
<author>P C Woodland</author>
</authors>
<title>Minimum phone error and I-smoothing for improved discrimative training.</title>
<date>2002</date>
<booktitle>In Proc. of ICASSP.</booktitle>
<marker>Povey, Woodland, 2002</marker>
<rawString>D. Povey and P. C. Woodland. 2002. Minimum phone error and I-smoothing for improved discrimative training. In Proc. of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Quattoni</author>
<author>M Collins</author>
<author>T Darrell</author>
</authors>
<title>Conditional random fields for object recognition.</title>
<date>2004</date>
<booktitle>In NIPS 17.</booktitle>
<contexts>
<context position="25372" citStr="Quattoni et al., 2004" startWordPosition="4336" endWordPosition="4339"> fields (Lafferty et al., 2001). Och and Ney (2002) popularized the use of loglinear models for MT and initially sought to optimize log loss, but by using the min-cost translation on a k-best list as their surrogate, we argue that their loss was closer to the soft ramp loss obtained by softening the second max in lossramp 2 in Eq. 7 (Fig. 2). The same is true for others who aimed to optimize log loss for MT (Smith and Eisner, 2006; Zens et al., 2007; Cer, 2011). The softmax version of the latent variable perceptron loss, Eq. 9 (Fig. 2), is the latent log loss inherent in latent-variable CRFs (Quattoni et al., 2004). Blunsom et al. (2008) and Blunsom and Osborne (2008) actually did optimize latent log loss for MT, discarding training examples for which y(O was unreachable by the model. Finally, we note that “softening” the ramp loss in Eq. 6 (Fig. 2) results in the Jensen risk bound from Gimpel and Smith (2010), which is a computationally-attractive upper bound on the Bayes risk. 4 Experiments The goal of our experiments is to compare RAMPION (Alg. 1) to state-of-the-art methods for training MT systems. RAMPION minimizes lossramp 3, which we found in preliminary experiments to work better than other loss</context>
</contexts>
<marker>Quattoni, Collins, Darrell, 2004</marker>
<rawString>A. Quattoni, M. Collins, and T. Darrell. 2004. Conditional random fields for object recognition. In NIPS 17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ratliff</author>
<author>J A Bagnell</author>
<author>M Zinkevich</author>
</authors>
<title>Subgradient methods for maximum margin structured learning.</title>
<date>2006</date>
<booktitle>In ICML Workshop on Learning in Structured Output Spaces.</booktitle>
<contexts>
<context position="22666" citStr="Ratliff et al., 2006" startWordPosition="3868" endWordPosition="3871">ov networks (Taskar et al., 2003): setting yT = y(i) and: � ) yi = argmax score(x(i), y; θ) + cost(y(i), y) ���(����) (13) Unlike the perceptron losses, which penalize the highest-scoring outputs, hinge loss penalizes an output that is both favored by the model and has high cost. Such an output is shown as hy−, h−i in Fig. 1; the structured hinge loss focuses on pushing such outputs to the left. As mentioned in §3.1, finding yi is often called cost-augmented decoding. Structured hinge loss is convex, can incorporate a cost function, and can be optimized with several algorithms, including SSD (Ratliff et al., 2006). Adaptation for MT While prior work has used MIRA-like algorithms for training machine translation systems, the proposed algorithms did not actually optimize the structured hinge loss, for similar reasons to those mentioned above for the perceptron: latent variables and surrogate references. Incorporating latent variables in the hinge loss results in the latent structured hinge loss (Yu and Joachims, 2009). Like the latent perceptron, this loss is nonconvex and inappropriate for MT because it requires computing the feature vector for y(i). By using a surrogate instead of y(i), the actual loss</context>
</contexts>
<marker>Ratliff, Bagnell, Zinkevich, 2006</marker>
<rawString>N. Ratliff, J. A. Bagnell, and M. Zinkevich. 2006. Subgradient methods for maximum margin structured learning. In ICML Workshop on Learning in Structured Output Spaces.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenfeld</author>
</authors>
<title>A maximum entropy approach to adaptive statistical language modeling.</title>
<date>1996</date>
<journal>Computer, Speech and Language,</journal>
<volume>10</volume>
<issue>3</issue>
<contexts>
<context position="33609" citStr="Rosenfeld (1996)" startWordPosition="5746" endWordPosition="5747">hile the within-initializer variance for PRO tended to be smaller than that of MERT, PRO’s overall range was larger. RAMPION found very similar weights regardless of 00. 4.3 Adding Features Finally, we compare RAMPION and PRO with an extended feature set; MERT is excluded as it fails in such settings (Hopkins and May, 2011). We added count features for common monolingual and bilingual lexical patterns from the parallel corpus: the 1k most common bilingual word pairs from phrase extraction, 200 top unigrams, 1k top bigrams, 1k top trigrams, and 4k top trigger pairs extracted with the method of Rosenfeld (1996), ranked by mutual information. We integrated the features with our training procedure by using Moses to generate lattices instead of k-best lists. We used cube pruning (Chiang, 2007) to incorporate the additional (potentially non-local) features while extracting kbest lists from the lattices to pass to the training algorithms.14 Results are shown in Table 2. We find that PRO finds much higher BLEU scores on the tuning data but fails to generalize, leading to poor performance on the held-out test sets. We suspect that incorporating regularization into training the binary classifier within PRO </context>
</contexts>
<marker>Rosenfeld, 1996</marker>
<rawString>R. Rosenfeld. 1996. A maximum entropy approach to adaptive statistical language modeling. Computer, Speech and Language, 10(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Smith</author>
<author>J Eisner</author>
</authors>
<title>Minimum risk annealing for training log-linear models.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL.</booktitle>
<contexts>
<context position="10248" citStr="Smith and Eisner, 2006" startWordPosition="1702" endWordPosition="1705">z))[cost(y(i),y)I (5) The use of this loss is often simply called “risk minimization” in the speech and MT communities. Bayes risk is non-convex, whether or not latent variables are present. Like MERT, it naturally avoids the need to compute features for y(i) and uses a cost function, making it appealing for MT. Bayes risk minimization first appeared in the speech recognition community (Kaiser et al., 2000; Povey and 4However, Cer et al. (2008) and Macherey et al. (2008) achieved a sort of regularization by altering MERT’s line search. Woodland, 2002) and more recently has been applied to MT (Smith and Eisner, 2006; Zens et al., 2007; Li and Eisner, 2009). 3 Training Methods for MT In this section we consider other ML-inspired approaches to MT training, situating each in the framework from §2: ramp, perceptron, hinge, and “soft” losses. Each of the first three kinds of losses can be understood as a way of selecting, for each x(i), two candidate translation/derivation pairs: (yT, hT) and (y�, h�). During training, the loss function can be improved by increasing the score of the former and decreasing the score of the latter, through manipulation of the parameters θ. Figure 1 gives a general visualization </context>
<context position="25184" citStr="Smith and Eisner, 2006" startWordPosition="4302" endWordPosition="4305">2), the loss underlying the conditional likelihood training criterion which is frequently used when a probabilistic interpretation of the learned model is desired, as in conditional random fields (Lafferty et al., 2001). Och and Ney (2002) popularized the use of loglinear models for MT and initially sought to optimize log loss, but by using the min-cost translation on a k-best list as their surrogate, we argue that their loss was closer to the soft ramp loss obtained by softening the second max in lossramp 2 in Eq. 7 (Fig. 2). The same is true for others who aimed to optimize log loss for MT (Smith and Eisner, 2006; Zens et al., 2007; Cer, 2011). The softmax version of the latent variable perceptron loss, Eq. 9 (Fig. 2), is the latent log loss inherent in latent-variable CRFs (Quattoni et al., 2004). Blunsom et al. (2008) and Blunsom and Osborne (2008) actually did optimize latent log loss for MT, discarding training examples for which y(O was unreachable by the model. Finally, we note that “softening” the ramp loss in Eq. 6 (Fig. 2) results in the Jensen risk bound from Gimpel and Smith (2010), which is a computationally-attractive upper bound on the Bayes risk. 4 Experiments The goal of our experiment</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>D. A. Smith and J. Eisner. 2006. Minimum risk annealing for training log-linear models. In Proc. of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM—an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of ICSLP.</booktitle>
<contexts>
<context position="26590" citStr="Stolcke, 2002" startWordPosition="4530" endWordPosition="4531">ctions tested.8 System and Datasets We use the Moses phrasebased MT system (Koehn et al., 2007) and consider Urdu—*English (UR—*EN), Chinese—*English (ZH—*EN) translation, and Arabic—*English (AR—*EN) translation.9 We trained a Moses system using default settings and features, except for setting the distortion limit to 10. Word alignment was performed using GIZA++ (Och and Ney, 2003) in both directions, the grow-diag-final-and heuristic was used to symmetrize the alignments, and a max phrase length of 7 was used for phrase extraction. We estimated 5-gram language models using the SRI toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). For each language pair, we used the English side of the parallel text and 600M words of randomlyselected sentences from the Gigaword v4 corpus (excluding NYT and LAT). For UR—*EN, we used parallel data from the NIST MT08 evaluation consisting of 1.2M Urdu words and 1.1M English words. We used half of the documents (882 sentences) from the MT08 test set for tuning. We used the remaining half for one test set (“MT08*”) and MT09 as our other test set. For ZH—*EN, we used 303k sentence pairs from the FBIS corpus (LDC2003E14). We segment</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM—an extensible language modeling toolkit. In Proc. of ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>C Guestrin</author>
<author>D Koller</author>
</authors>
<title>Max-margin Markov networks.</title>
<date>2003</date>
<booktitle>In Advances in NIPS 16.</booktitle>
<contexts>
<context position="22078" citStr="Taskar et al., 2003" startWordPosition="3767" endWordPosition="3770">ng models with large feature sets (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009). In order to apply it to MT, Watanabe et al. and Chiang et al. made modifications similar to those made by Liang et al. for perceptron training, namely the extension to latent variables and the use of a surrogate reference with high model score and low cost. Hinge Loss It can be shown that 1-best MIRA corresponds to dual coordinate ascent for the structured hinge loss when using E2 regularization (Martins et al., 2010). The structured hinge is the loss underlying maximum-margin Markov networks (Taskar et al., 2003): setting yT = y(i) and: � ) yi = argmax score(x(i), y; θ) + cost(y(i), y) ���(����) (13) Unlike the perceptron losses, which penalize the highest-scoring outputs, hinge loss penalizes an output that is both favored by the model and has high cost. Such an output is shown as hy−, h−i in Fig. 1; the structured hinge loss focuses on pushing such outputs to the left. As mentioned in §3.1, finding yi is often called cost-augmented decoding. Structured hinge loss is convex, can incorporate a cost function, and can be optimized with several algorithms, including SSD (Ratliff et al., 2006). Adaptation</context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2003</marker>
<rawString>B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin Markov networks. In Advances in NIPS 16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vapnik</author>
</authors>
<title>Statistical learning</title>
<date>1998</date>
<booktitle>In Proc. of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="1315" citStr="Vapnik, 1998" startWordPosition="191" endWordPosition="192">s easy to implement and that performs comparably to others. Most notably, our structured ramp loss minimization algorithm, RAMPION, is less sensitive to initialization and random seeds than standard approaches. 1 Introduction Every statistical MT system relies on a training algorithm to fit the parameters of a scoring function to examples from parallel text. Well-known examples include MERT (Och, 2003), MIRA (Chiang et al., 2008), and PRO (Hopkins and May, 2011). While such procedures can be analyzed as machine learning algorithms—e.g., in the general framework of empirical risk minimization (Vapnik, 1998)—their procedural specifications have made this difficult. From a practical perspective, such algorithms are often complex, difficult to replicate, and sensitive to initialization, random seeds, and other hyperparameters. In this paper, we consider training algorithms that are first specified declaratively, as loss functions to be minimized. We relate well-known training algorithms for MT to particular loss functions. We show that a family of structured ramp loss functions (Do et al., 2008) is useful for this analysis. For example, McAllester and Keshet (2011) recently suggested that, while Ch</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>V. Vapnik. 1998. Statistical learning theory. Wiley. T. Watanabe, J. Suzuki, H. Tsukada, and H. Isozaki. 2007. Online large-margin training for statistical machine translation. In Proc. of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Yu</author>
<author>T Joachims</author>
</authors>
<title>Learning structural SVMs with latent variables.</title>
<date>2009</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="17348" citStr="Yu and Joachims, 2009" startWordPosition="2908" endWordPosition="2911"> mentioned in text for latent-variable loss functions. Each loss is actually a function loss(X, Y, B); we suppress the arguments for clarity. “Ti” is shorthand for “T(x(i)).” “Xi” is shorthand for the k-best list for x(i). “costi(·)” is shorthand for “cost(y(i), ·).” “scorei(·)” is shorthand for “score(x(i), ·).” As noted in §3.4, any operator of the form maxsES can be replaced by log EsES exp, known as softmax, giving many additional loss functions. is fundamentally a batch optimization algorithm and has been used for solving many non-convex learning problems, such as latent structured SVMs (Yu and Joachims, 2009). 3.2 Structured Perceptron The stuctured perceptron algorithm (Collins, 2002) was considered by Liang et al. (2006) as an alternative to MERT. It requires only a decoder and comes with some attractive guarantees, at least for models without latent variables. Liang et al. modified the perceptron in several ways for use in MT. The first was to generalize it to handle latent variables. The second change relates to the need to compute the feature vector for the reference translation y(i), which may be unreachable (§1(i)). To address this, researchers have proposed the use of surrogates that are b</context>
<context position="23076" citStr="Yu and Joachims, 2009" startWordPosition="3930" endWordPosition="3933">ed in §3.1, finding yi is often called cost-augmented decoding. Structured hinge loss is convex, can incorporate a cost function, and can be optimized with several algorithms, including SSD (Ratliff et al., 2006). Adaptation for MT While prior work has used MIRA-like algorithms for training machine translation systems, the proposed algorithms did not actually optimize the structured hinge loss, for similar reasons to those mentioned above for the perceptron: latent variables and surrogate references. Incorporating latent variables in the hinge loss results in the latent structured hinge loss (Yu and Joachims, 2009). Like the latent perceptron, this loss is nonconvex and inappropriate for MT because it requires computing the feature vector for y(i). By using a surrogate instead of y(i), the actual loss optimized becomes closer to Eq. 8 (Fig. 2), the third form of the latent structured ramp loss. Watanabe et al. (2007) and Arun and Koehn (2007) used k-best oracles like Liang et al., but Chiang et al. (2008, 2009) used a different approach, explicitly defining the surrogate as hy+, h+i in Fig. 1. While the method of Chiang et al. showed impres226 sive performance improvements, its implementation is non-tri</context>
</contexts>
<marker>Yu, Joachims, 2009</marker>
<rawString>C. J. Yu and T. Joachims. 2009. Learning structural SVMs with latent variables. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Yuille</author>
<author>Anand Rangarajan</author>
</authors>
<title>The concaveconvex procedure (CCCP).</title>
<date>2002</date>
<booktitle>In Proc. of NIPS.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="13994" citStr="Yuille and Rangarajan, 2002" startWordPosition="2344" endWordPosition="2347"> that has both high model score and low cost; this is the converse of cost-augmented decoding and therefore we call it cost-diminished decoding; (y↑, h↑) = (y+, h+) in Fig. 1. The third form, Eq. 8, sets (y↑, h↑) = (y+, h+) and (y↓, h↓) = (y−, h−). This loss underlies RAMPION. It is similar to the loss optimized by the MIRA-inspired algorithm used by Chiang et al. (2008, 2009). Optimization The ramp losses are continuous but non-convex and non-differentiable, so gradientbased optimization methods are not available.5 Fortunately, Eq. 8 can be optimized by using a concaveconvex procedure (CCCP; Yuille and Rangarajan, 2002). CCCP is a batch optimization algorithm for any function that is the the sum of a concave and a convex function. The idea is to approximate the sum as the convex term plus a tangent line to the concavecfunction at the current parameter values; the resulting sum is convex and can be optimized with (sub)gradient methods. 5For non-differentiable, continuous, convex functions, subgradient-based methods are available, such as stochastic subgradient descent (SSD), and it is tempting to apply them here. However, non-convex functions are not everywhere subdifferentiable and so a straightforward appli</context>
</contexts>
<marker>Yuille, Rangarajan, 2002</marker>
<rawString>A. L. Yuille and Anand Rangarajan. 2002. The concaveconvex procedure (CCCP). In Proc. of NIPS. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>S Hasan</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of training criteria for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="10267" citStr="Zens et al., 2007" startWordPosition="1706" endWordPosition="1709">e use of this loss is often simply called “risk minimization” in the speech and MT communities. Bayes risk is non-convex, whether or not latent variables are present. Like MERT, it naturally avoids the need to compute features for y(i) and uses a cost function, making it appealing for MT. Bayes risk minimization first appeared in the speech recognition community (Kaiser et al., 2000; Povey and 4However, Cer et al. (2008) and Macherey et al. (2008) achieved a sort of regularization by altering MERT’s line search. Woodland, 2002) and more recently has been applied to MT (Smith and Eisner, 2006; Zens et al., 2007; Li and Eisner, 2009). 3 Training Methods for MT In this section we consider other ML-inspired approaches to MT training, situating each in the framework from §2: ramp, perceptron, hinge, and “soft” losses. Each of the first three kinds of losses can be understood as a way of selecting, for each x(i), two candidate translation/derivation pairs: (yT, hT) and (y�, h�). During training, the loss function can be improved by increasing the score of the former and decreasing the score of the latter, through manipulation of the parameters θ. Figure 1 gives a general visualization of some of the key </context>
<context position="25203" citStr="Zens et al., 2007" startWordPosition="4306" endWordPosition="4309">the conditional likelihood training criterion which is frequently used when a probabilistic interpretation of the learned model is desired, as in conditional random fields (Lafferty et al., 2001). Och and Ney (2002) popularized the use of loglinear models for MT and initially sought to optimize log loss, but by using the min-cost translation on a k-best list as their surrogate, we argue that their loss was closer to the soft ramp loss obtained by softening the second max in lossramp 2 in Eq. 7 (Fig. 2). The same is true for others who aimed to optimize log loss for MT (Smith and Eisner, 2006; Zens et al., 2007; Cer, 2011). The softmax version of the latent variable perceptron loss, Eq. 9 (Fig. 2), is the latent log loss inherent in latent-variable CRFs (Quattoni et al., 2004). Blunsom et al. (2008) and Blunsom and Osborne (2008) actually did optimize latent log loss for MT, discarding training examples for which y(O was unreachable by the model. Finally, we note that “softening” the ramp loss in Eq. 6 (Fig. 2) results in the Jensen risk bound from Gimpel and Smith (2010), which is a computationally-attractive upper bound on the Bayes risk. 4 Experiments The goal of our experiments is to compare RAM</context>
</contexts>
<marker>Zens, Hasan, Ney, 2007</marker>
<rawString>R. Zens, S. Hasan, and H. Ney. 2007. A systematic comparison of training criteria for statistical machine translation. In Proc. of EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>