<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.9972125">
Statistical Sense Disambiguation with Relatively Small Corpora
Using Dictionary Definitions
</title>
<author confidence="0.652184">
Alpha K. Luk
</author>
<affiliation confidence="0.95482">
Microsoft Institute Department of Computing
</affiliation>
<address confidence="0.653133">
North Ryde, NSW 2113, Australia Macquarie University
</address>
<email confidence="0.902067">
t-alphal@microsoft.com NSW 2109, Australia
</email>
<sectionHeader confidence="0.990716" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996587">
Corpus-based sense disambiguation methods, like
most other statistical NLP approaches, suffer from
the problem of data sparseness. In this paper, we
describe an approach which overcomes this problem
using dictionary definitions. Using the definition-
based conceptual co-occurrence data collected from
the relatively small Brown corpus, our sense
disambiguation system achieves an average accuracy
comparable to human performance given the same
contextual information.
</bodyText>
<sectionHeader confidence="0.998787" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999795266666667">
Previous corpus-based sense disambiguation methods
require substantial amounts of sense-tagged training
data (Kelly and Stone, 1975; Black, 1988 and
Hearst, 1991) or aligned bilingual corpora (Brown et
al., 1991; Dagari, 1991 and Gale et al. 1992).
Yarowsky (1992) introduces a thesaurus-based
approach to statistical sense disambiguation which
works on monolingual corpora without the need for
sense-tagged training data. By collecting statistical
data of word occurrences in the context of different
thesaurus categories from a relatively large corpus
(10 million words), the system can identify salient
words for each category. Using these salient words,
the system is able to disambiguate polysemous words
with respect to thesaurus categories.
Statistical approaches like these generally suffer
from the problem of data sparseness. To estimate the
salience of a word with reasonable accuracy, the
system needs the word to have a significant number
of occurrences in the corpus. Having large corpora
will help but some words are simply too infrequent
to make a significant statistical contribution even in
a rather large corpus. Moreover, huge corpora are
not generally available in all domains and storage
and processing of very huge corpora can be
problematic in some cases.1
In this paper, we describe an approach which
attacks the problem of. data sparseness in automatic
statistical sense disambiguation. Using definitions
from LDOCE (Longman Dictionary of
Contemporary English; Procter, 1978), co-
occurrence data of concepts, rather than words, is
collected from a relatively small corpus, the one
million word Brown corpus. Since all the definitions
in LDOCE are written using words from the 2000
word controlled vocabulary (or in our terminology,
defining concepts), even our small corpus is found to
be capable of providing statistically significant co-
occurrence data at the level of the defining concepts.
This data is then used in a sense disambiguation
system. The system is tested on twelve words
previously discussed in the sense disambiguation
literature. The results are found to be comparable to
human performance given the same contextual
information.
</bodyText>
<sectionHeader confidence="0.9953855" genericHeader="introduction">
2 Statistical Sense Disambiguation Using
Dictionary Definitions
</sectionHeader>
<bodyText confidence="0.875239769230769">
It is well known that some words tend to co-occur
with some words more often than with others.
Similarly, looking at the meaning of the words, one
should find that some concepts co-occur more often
with some concepts than with others. For example,
the concept crime is found to co-occur frequently
with the concept punishment. This kind of
conceptual relationship is not always reflected at the
lexical level. For instance, in legal reports, the
1 Statistical data is domain dependent. Data
extracted from a corpus of one particular domain is
usually not very useful for processing text of another
domain.
</bodyText>
<page confidence="0.997425">
181
</page>
<bodyText confidence="0.99994075">
concept crime will usually be expressed by words
like offence or felony, etc., and punishment will be
expressed by words such as sentence, fine or penalty,
etc. The large number of different words of similar
meaning is the major cause of the data sparseness
problem.
The meaning or underlying concepts of a word
are very difficult to capture accurately but dictionary
definitions provide a reasonable representation and
are readily available.2 For instance, the LDOCE
definitions of both offence and felony contain the
word crime, and all of the definitions of sentence,
fine and penalty contain the word punishment. To
disambiguate a polysemous word, a system can select
the sense with a dictionary definition containing
defining concepts that co-occur most frequently with
the defining concepts in the definitions of the other
words in the context. In the current experiment, this
conceptual co-occurrence data is collected from the
Brown corpus.
</bodyText>
<subsectionHeader confidence="0.998502">
2.1 Collecting Conceptual Co-occurrence Data
</subsectionHeader>
<bodyText confidence="0.99962105">
Our system constructs a two-dimensional table
which records the frequency of co-occurrence of each
pair of defining concepts. The controlled vocabulary
provided by Longman is a list of all the words used
in the definitions but, in its crude form, it does not
suit our purpose. From the controlled vocabulary, we
manually constructed a list of 1792 defining
concepts. To minimise the size of the table and the
processing time, all the closed class words and words
which are rarely used in definitions (e.g., the days of
the week, the months) are excluded from the list. To
strengthen the signals, words which have the same
semantic root are combined as one element in the list
(e.g., habit and habitual are combined as (habit,
habitual)).
The whole LDOCE is pre-processed first. For
each entry in LDOCE, we construct its
corresponding conceptual expansion. The conceptual
expansion of an entry whose headword is not a
defining concept is a set of conceptual sets. Each
conceptual set corresponds to a sense in the entry
and contains all the defining concepts which occur
in the definition of the sense. The entry of the noun
sentence and its corresponding conceptual expansion
2 Manually constructed semantic frames could be
more useful computationally but building semantic
frames for a huge lexicon is an extremely expensive
exercise.
are shown in Figure 1. If the headword of an entry is
a defining concept DC, the conceptual expansion is
given as { {DC} }.
The corpus is pre-segmented into sentences but
not pre-processed in any other way (sense-tagged or
part-of-speech-tagged). The context of a word is
defined to be the current sentence.3 The system
processes the corpus sentence by sentence and
collects conceptual co-occurrence data for each
defining concept which occurs in the sentence. This
allows the whole table to be constructed in a single
run through the corpus.
Since the training data is not sense tagged, the
data collected will contain noise due to spurious
senses of polysemous words. Like the thesaurus-
based approach of Yarowsky (1992), our approach
relies on the dilution of this noise by their
distribution through all the 1792 defining concepts.
Different words in the corpus have different
numbers of senses and different senses have
definitions of varying lengths. The principle adopted
in collecting co-occurrence data is that every pair of
content words which co-occur in a sentence should
have equal contribution to the conceptual co-
occurrence data regardless of the number of
definitions (senses) of the words and the lengths of
the definitions. In addition, the contribution of a
word should be evenly distributed between all the
senses of a word and the contribution of a sense
should be evenly distributed between all the concepts
in a sense. The algorithm for conceptual co-
occurrence data collection is shown in Figure 2.
</bodyText>
<subsectionHeader confidence="0.999551">
2.2 Using the Conceptual Co-occurrence Data
for Sense Disambiguation
</subsectionHeader>
<bodyText confidence="0.9990604">
To disambiguate a polysemous word W in a context
C, which is taken to be the sentence containing W,
the system scores each sense S of W, as defined in
LDOCE, with respect to C using the following
equations.
</bodyText>
<equation confidence="0.994191">
score(S,C) = score(CS,C&apos;)— score(CS,GlobalCS) [1]
</equation>
<bodyText confidence="0.6582844">
where CS is the corresponding conceptual set of S,
C&apos; is the set of conceptual expansions of all content
words (which are defined in LDOCE) in C and
GlobalCS is the conceptual set containing all the
1792 defining concepts.
</bodyText>
<footnote confidence="0.9657945">
3 The average sentence length of the Brown corpus is
19.4 words.
</footnote>
<page confidence="0.993277">
182
</page>
<bodyText confidence="0.345933">
Entry in LDOCE conceptual expansion
</bodyText>
<listItem confidence="0.9054754">
1. (an order given by a judge which fixes) a punishment for a criminal
found guilty in court
2. a group of words that forms a statement, command, exclamation, or
question, usu. contains a subject and a verb, and (in writing) begins
with a capital letter and ends with one of the marks . ! ?
</listItem>
<construct confidence="0.909025833333333">
{ {order, judge, punish, crime, criminal,
find, guilt, court},
{group, word, form, statement,
command, question, contain, subject,
verb, write, begin, capital, letter, end,
mark} }
</construct>
<figureCaption confidence="0.99408">
Figure 1. The entry of sentence (n.) in LDOCE and its corresponding conceptual expansion
</figureCaption>
<listItem confidence="0.95648575">
1. Initialise the Conceptual Co-occurrence Data Table (CCDT) with initial value of 0 for each cell.
2. For each sentence S in the corpus, do
a. Construct S&apos;, the set of conceptual expansions of all content words (which are
defined in LDOCE) in S.
</listItem>
<bodyText confidence="0.972158666666667">
b. For each unique pair of conceptual expansions (CE„ CEO in S&apos;, do
For each defining concept DC,,,r, in each conceptual set CS,m in CE,, do
For each defining concept DCinc, in each conceptual set CSp, in CEJ, do
increase the values of the cells CCDT(DCimp, Deing)
and CCDT(DCing, Dcimp) by the product of w(DC,mp) and w(DCinci)
where w(DC) is the weight of DC,„„,, given by
</bodyText>
<figure confidence="0.697855333333333">
1
w(DC„,„ ) —
CE.HCS, I
</figure>
<figureCaption confidence="0.999009">
Figure 2. The algorithm for collecting conceptual co-occurrence data
</figureCaption>
<equation confidence="0.899763615384615">
score(CS,C) = E score(CS,CEI)
VCE&apos;eC&apos;
for any concp. set CS and concp. exp. set C&apos; [2]
score(CS,CE&apos;)= max score(CS,CS&apos;)
cs-GcE.
for any concp. set CS and concp. exp. CE&apos;
score(CS,CS&apos;) = I score(CS ,DC&apos;)
VDC&apos;eCS&apos;
for any concp. sets CS and CS&apos; [4]
score(CS, DC) = E score(DC, DC)IICSI
VDCECS
for any concp. set CS and def. concept DC&apos;
score(DC,DC&apos;)= max(0, I(DC,DC&apos;))
</equation>
<bodyText confidence="0.992085666666667">
for any def. concepts DC and DC&apos; [6]
I(DC,DC) is the mutual information4 (Fano, 1961)
between the 2 defining concepts DC and DC&apos; given
by:
(using the Maximum Likelihood Estimator).
f(x,y) is looked up directly from the conceptual co-
occurrence data table, f(x) and f(y) are looked up
from a pre-constructed list of f(DC) values, for each
defining concept DC:
</bodyText>
<equation confidence="0.631988">
f (DC) = Ef(DC,DC&apos;)
VDC&apos;
</equation>
<footnote confidence="0.452316">
4 Church and Hanks (1989) use Mutual Information
to measure word association norms.
</footnote>
<equation confidence="0.810045222222222">
[5]
[3]
13(x,Y)
1(x,y) ---E log2
P(x) • P(y)
r.l
f(x,y)• N
og ,
I (x). (y)
</equation>
<page confidence="0.980032">
183
</page>
<bodyText confidence="0.9985575">
N is taken to be the total number of pairs of words
processed, given by
</bodyText>
<equation confidence="0.827457">
f(DC)/2
VDC
since for each pair of surface words processed,
Ef(DC)
VLC
</equation>
<bodyText confidence="0.959847333333333">
is increased by 2.
Our scoring method is based on a probabilistic
model at the conceptual level. In a standard model,
the logarithm of the probability of occurrence of a
conceptual set {x1, xz x,n} in the context of the
conceptual set {y,, yz ..y„} is given by
</bodyText>
<equation confidence="0.99781125">
log, P(x,,x2,...,x„,ly1,y2,...,y0
m (n
iog2 P(x,))
1.1 J.1
</equation>
<bodyText confidence="0.9982642">
assuming that each P(x) is independent of each
other given y,yz y. and each P(y) is independent
of each other given x„ for all xi.5
Our scoring method deviates from the standard
model in a number of aspects:
</bodyText>
<listItem confidence="0.783048">
1. log2 P(x,), the term of the occurrence probability
</listItem>
<bodyText confidence="0.991064903225806">
of each of the defining concepts in the sense, is
excluded in our scoring method. Since the training
data is not sense-tagged, the occurrence probability
is highly unreliable. Moreover, the magnitude of
mutual information is decreased due to the noise of
the spurious senses while the average magnitude of
the occurrence probability is unaffected.6 Inclusion
of the occurrence probability term will lead to the
dominance of this term over the mutual information
term, resulting in the system flavouring the sense
with the more frequently occurring defining concepts
most of the time.
2. The score of a sense with respect to the current
context is normalised by subtracting the score of the
sense calculated with respect to the GlobalCS (which
contains all defining concepts) from it (see formula
5 The occurrence probabilities of some defining
concepts will not be independent in some contexts.
However, modelling the dependency between
different concepts in different contexts will lead to
an explosion of the complexity of the model.
6 The noise only leads to incorrect distribution of the
occurrence probability.
[1]). In effect, we are comparing the score between
the sense with the current context and the score
between the sense and an artificially constructed
&amp;quot;average&amp;quot; context. This is needed to rectify the bias
towards the sense(s) with defining concepts of higher
average mutual information (over the set of all
defining concepts), which is intensified by the
ambiguity of the context words.
</bodyText>
<listItem confidence="0.9691815">
3. Negative mutual information score is taken to be 0
([6]). Negative mutual information is unreliable due
to the smaller number of data points.
4. The evidence (mutual information score) from
</listItem>
<bodyText confidence="0.640252222222222">
multiple defining concepts/words is averaged rather
than summed ([2], [4] &amp; [5]). This is to compensate
for the different lengths of definitions of different
senses and different lengths of the context. The
evidence from a polysemous context word is taken to
be the evidence from its sense with the highest
mutual information score ([3]). This is due to the
fact that only one of the senses is used in the given
sentence.
</bodyText>
<sectionHeader confidence="0.999027" genericHeader="method">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.99815025">
Our system is tested on the twelve words discussed
in Yarowsky (1992) and previous publications on
sense disambiguation. Results are shown in Table 1.
Our system achieves an average accuracy of 77% on
a mean 3-way sense distinction over the twelve
words. Numerically, the result is not as good as the
92% as reported in Yarowslcy (1992). However,
direct comparison between the numerical results can
be misleading since the experiments are carried out
on two very different corpora both in size and genre.
Firstly, Yarowsky&apos; s system is trained with the 10
million word Grolier&apos;s Encyclopedia, which is a
magnitude larger than the Brown corpus used by our
system. Secondly, and more importantly, the two
corpora, which are also the test corpora, are very
different in genre. Semantic coherence of text, on
which both systems rely, is generally stronger in
technical writing than in most other kinds of text.
Statistical disambiguation systems which rely on
semantic coherence will generally perform better on
technical writing, which encyclopedia entry can be
regarded as one kind of, than on most other kinds of
text. On the other hand, the Brown corpus is a
collection of text with all kinds of genre.
People make use of syntactic, semantic and
pragmatic knowledge in sense disambiguation. It is
not very realistic to expect any system which only
possesses semantic coherence knowledge (including
</bodyText>
<page confidence="0.994836">
184
</page>
<bodyText confidence="0.999970871794872">
ours as well as Yarowsky&apos;s) to achieve a very high
level of accuracy for all words in general text. To
provide a better evaluation of our approach, we have
conducted an informal experiment aiming at
establishing a more reasonable upper bound of the
performance of such systems. In the experiment, a
human subject is asked to perform the same
disambiguation task as our system, given the same
contextual information.&apos; Since our system only uses
semantic coherence information and has no deeper
understanding of the meaning of the text, the human
subject is asked to disambiguate the target word,
given a list of all the content words in the context
(sentence) of the target word in random order. The
words are put in random order because the system
does not make use of syntactic information of the
sentence either. The human subject is also allowed
access to a copy of LDOCE which the system also
uses. The results are listed in Table 1. The actual
upper bound of the performance of statistical
methods using semantic coherence information only
should be slightly better than the performance of
human since the human is disadvantaged by a
number of factors, including but not limited to: 1. it
is unnatural for human to disambiguate in the
described manner; 2. the semantic coherence
knowledge used by the human is not complete or
specific to the current corpus8; 3. human error.
However, the results provide a rough approximation
of the upper bound of performance of such systems.
The human subject achieves an average accuracy
of 71% over the twelve words, which is 6% lower
than our system. More interestingly, the results of
the human subject are found to exhibit a similar
pattern to the results of our system - the human
subject performs better on words and senses for
which our system achieve higher accuracy and less
well on words and senses for which our system has a
lower accuracy.
</bodyText>
<sectionHeader confidence="0.953194" genericHeader="method">
4 The Use of Sentence as Local Context
</sectionHeader>
<bodyText confidence="0.987418918367347">
Another significant point our experiments have
shown is that the sentence can also provide enough
contextual information for semantic coherence based
7 The result is less than conclusive since only one
human subject is tested. In order to acquire more
reliable results, we are currently seeking a few more
subjects to repeat the experiment.
8 The subject has not read through the whole corpus.
approaches in a large proportion of cases.° The
average sentence length in the Brown corpus is
19.410 words which is 5 times smaller than the 100
word window used in Gale et al. (1992) and
Yarowslcy (1992). Our approach works well even
with a small &amp;quot;window&amp;quot; because it is based on the
identification of salient concepts rather than salient
words. In salient word based approaches, due to the
problem of data sparseness, many less frequently
occurring words which are intuitively salient to a
particular word sense will not be identified in
practice unless an extremely large corpus is used.
Therefore the sentence usually does not contain
enough identified salient words to provide enough
contextual information. Using conceptual co-
occurrence data, contextual information from the
salient but less frequently used words in the sentence
will also be utilised through the salient concepts in
the conceptual expansions of these words. Obviously,
there are still cases where the sentence does not
provide enough contextual information even using
conceptual co-occurrence data, such as when the
sentence is too short, and contextual information
from a larger context has to be used. However, the
ability to make use of information in a smaller
context is very important because the smaller context
always overrules the larger context if their sense
preferences are different. For example, in a legal
trial context, the correct sense of sentence in the
clause she was asked to repeat the last word of her
previous sentence will be its word sense rather than
its legal sense which would have been selected if a
larger context is used instead.
9 Analysis of the test samples which our system fails
to correctly disambiguate also shows that increasing
the window size will benefit the disambiguation
process only in a very small proportion of these
samples. The main cause of errors is the polysemous
words in dictionary definitions which we will discuss
in Section 6.
1° Based on 1004998 words and 51763 sentences.
</bodyText>
<page confidence="0.999328">
185
</page>
<tableCaption confidence="0.999873">
Table 1. Results of Experiments
</tableCaption>
<table confidence="0.997582833333334">
Sense N i DBCC Human Thes.
BASS
Fish 1 100% 100% 100%
Musical senses 15 93% 100% 99%
----16 94% 100% —9-3/(T
BOW 1 0% 100% –
bending forward
weapon o — — 92%
violin part 2 100% 100% 100%
knot 4 100% 100% 25%
front of ship 2 50% 100% 94%
bend in object* _– – – 50%
–9 78% 100% 91%
CONE 5 100% 100% 61%
shaped object
fruit of a plant 0 – – 99%
part of eye* – – – 69%
5 100% 100% 77%
DUTY 54 57% 72% 96%
obligation
tax 2 100% 100% 96%
56 :-r 59% 73% 96%
GALLEY 0 – – 97%
ancient ship
ship&apos;s kitchen 4 100% 50% 50%
printer&apos;s tray 0_ – – 100%_
4 100% 50% 95%
INTEREST 187 43% 41% 88%
curiosity
advantage 59 42% 47% 34%
share 8 25% 38% 38%
money paid 48 88% 75% 90%
302 49% 47% 72%
ISSUE 36 64% 75% 89%
bringing out
important point 87 56% 40% 94%
stock* – – ----r 100%
–
123 : -59% 50% 94%
i
MOLE 2 50% 50% 100%
skin blemish
animal 0 – – 100%
stone wall ** 1 100% 100% –
quantity * – – – 98%
machine* – – –
3 67% 67% 99%
SENTENCE 11 91% 100% 99%
punishment
group of words 20_ 80% 45% 98%
_ - -31 84% 65% 9113/c7
Sense N i DBCC Human Thes.
SLUG
animal 1 0% 0% 100%
fake coin 0 – – 50%
type strip 0 – – 100%
bullet 4 100% 50% 100%
mass unit* – – – 100%
metallurgy* – – – __100%
__ _
–87)320 40A7– &amp;quot;TY
.; -0
STAR 4 75% 75% 96%
space object
shaped object 0 – – 95%
celebrity 11 45% 64% 82%
15 53% 67% 96%
TASTE 21 100% 95% 93%
flavour
preference 26 96% 85% 93%
– –47— –973c70-- – 19/0 7– –93%
Notes:
</table>
<listItem confidence="0.857571692307692">
1. N marks the column with the number of test samples for
each sense. DBCC (Definition-Based Conceptual Co-
occurrence) and Human mark the columns with the results
of our system and the human subject in disambiguating the
occurrences of the 12 words in the Brown corpus,
respectively. Thes. (thesaurus) marks the column with the
results of Yarowslcy (1992) tested on the Grolier&apos;s
Encyclopedia.
2. The &amp;quot;correct&amp;quot; sense of each test sample is chosen by
hand disambiguation carried out by the author using the
sentence as the context. A small proportion of test samples
cannot be disambiguated within the given context and are
excluded from the experiment.
3. The senses marked with * are used in Yarowsky (1992)
but no corresponding sense is found in LDOCE.
4. The sense marked with ** is defmed in LDOCE but not
used in Yarowslcy (1992).
6. In our experiment, the words are disambiguated
between all the senses listed except the ones marked with
*.
7. The rare senses listed in LDOCE are not listed here.
For some of the words, more than one sense listed in
LDOCE corresponds to a sense as used in Yarowslcy
(1992). In these cases, the senses used by Yarowslcy are
adopted for easier comparison.
8. All results are based on 100% recall.
</listItem>
<page confidence="0.995866">
186
</page>
<sectionHeader confidence="0.405353" genericHeader="method">
5 Related Work it is based on salient words) and may not perform as
</sectionHeader>
<bodyText confidence="0.979612541666667">
well on general text as our approach.
Previous attempts to tackle the data sparseness
problem in general corpus-based work include the
class-based approaches and similarity-based
approaches. In these approaches, relationships
between a given pair of words are modelled by
analogy with other words that resemble the given
pair in some way. The class-based approaches
(Brown et al., 1992; Resnik, 1992; Pereira et al.,
1993) calculate co-occurrence data of words
belonging to different classes,&amp;quot; rather than
individual words, to enhance the co-occurrence data
collected and to cover words which have low
occurrence frequencies. Dagan et al. (1993) argue
that using a relatively small number of classes to
model the similarity between words may lead to
substantial loss of information. In the similarity-
based approaches (Dagan et al., 1993 &amp; 1994;
Grishman et al., 1993), rather than a class, each
word is modelled by its own set of similar words
derived from statistical data collected from corpora.
However, deriving these sets of similar words
requires a substantial amount of statistical data and
thus these approaches require relatively large
corpora to start with.12
Our definition-based approach to statistical sense
disambiguation is similar in spirit to the similarity-
based approaches, with respect to the &amp;quot;specificity&amp;quot; of
modelling individual words. However, using
definitions from existing dictionaries rather than
derived sets of similar words allows our method to
work on corpora of much smaller sizes. In our
approach, each word is modelled by its own set of
defining concepts. Although only 1792 defining
concepts are used, the set of all possible
combinations (a power set of the defining concepts)
is so huge that it is very unlikely two word senses
will have the same combination of defining concepts
unless they are almost identical in meaning. On the
other hand, the thesaurus-based method of Yarowsky
(1992) may suffer from loss of information (since it
is semi-class-based) as well as data sparseness (since
11 Classes used in Resnik (1992) are based on the
WordNet taxonomy while classes of Brown et al.
(1992) and Pereira et al. (1993) are derived from
statistical data collected from corpora.
12 The corpus used in Dagan et al. (1994) contains
40.5 million words.
</bodyText>
<sectionHeader confidence="0.937693" genericHeader="evaluation">
6 Limitation and Further work
</sectionHeader>
<bodyText confidence="0.999902466666667">
Being a dictionary-based method, the natural
limitation of our approach is the dictionary. The
most serious problem is that many of the words in
the controlled vocabulary of LDOCE are polysemous
themselves. The result is that many of our list of
1792 defining concepts actually stand for a number
of distinct concepts. For example, the defining
concept point is used in its place sense, idea sense
and sharp end sense in different definitions. This
affects the accuracy of disambiguating senses which
have definitions containing these polysemous words
and is found to be the main cause of errors for most
of the senses with below-average results.
We are currently working on ways to
disambiguate the words in the dictionary definitions.
One possible way is to apply the current method of
disambiguation on the defining text of dictionary
itself. The LDOCE defining text has roughly half a
million words in its 41000 entries, which is half the
size of the Brown corpus used in the current
experiment. Although the result on the dictionary
cannot be expected to be as good as the result on the
Brown corpus due to the smaller size of the
dictionary, the reliability of further co-occurrence
data collected and, thus, the performance of the
disambiguation system can be improved significantly
as long as the disambiguation of the dictionary is
considerably more accurate than by chance.
Our success in using definitions of word senses to
overcome the data sparseness problem may also lead
to further improvement of sense disambiguation
technologies. In many cases, semantic coherence
information is not adequate to select the correct
sense, and knowledge about local constraints is
needed.13 For disambiguation of polysemous nouns,
these constraints include the modifiers of these
nouns and the verbs which take these nouns as
objects, etc. This knowledge has been successfully
acquired from corpora in manual or semi-automatic
approaches such as that described in Hearst (1991).
However, fully automatic lexically based approaches
13 Hatzivassiloglou (1994) shows that the
introduction of linguistic cues improves the
performance of a statistical semantic knowledge
acquisition system in the context of word grouping.
</bodyText>
<page confidence="0.99359">
187
</page>
<bodyText confidence="0.999975333333333">
such as that described in Yarowslcy (1992) are very
unlikely to be capable of acquiring this finer
knowledge because the problem of data sparseness
becomes even more serious with the introduction of
syntactic constraints. Our approach has overcome
the data sparseness problem by using the defining
concepts of words. It is found to be effective in
acquiring semantic coherence knowledge from a
relatively small corpus. It is possible that a similar
approach based on dictionary definitions will be
successful in acquiring knowledge of local
constraints from a reasonably sized corpus.
</bodyText>
<sectionHeader confidence="0.999348" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999988444444444">
We have shown that using definition-based
conceptual co-occurrence data collected from a
relatively small corpus, our sense disambiguation
system has achieved accuracy comparable to human
performance given the same amount of contextual
information. By overcoming the data sparseness
problem, contextual information from a smaller local
context becomes sufficient for disambiguation in a
large proportion of cases.
</bodyText>
<sectionHeader confidence="0.999075" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99998475">
I would like to thank Robert Dale and Vance
Gledhill for their helpful comments on earlier drafts
of this paper, and Richard Buckland and Mark Dras
for their help with the statistics.
</bodyText>
<sectionHeader confidence="0.999536" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999837047619047">
Black, E., 1988. An Experiment In Computational
Discrimination of English Word Senses. IBM
Journal of research and development, vol. 32,
pp.185-194.
Brown, P., et al., 1991. Word-sense Disambiguation
using Statistical Methods. In Proceedings of 29th
annual meeting of ACL, pp.264-270.
Brown, P. et at., 1992. Class-based n-gram Models
of Natural Language. Computational Linguistics,
18(4):467-479.
Church, K. and P. Hanks, 1989. Word Association
Norms, Mutual Information, and Lexicography. In
Proceedings of the 27th Annual Meeting of the
Association for Computational Linguistics, pp.76-
83.
Dagan, I. et al., 1991. Two Languages Are More
Informative Than One. In Proceedings of the 29th
Annual Meeting of the ACL, pp130-137.
Dagan, I. et al., 1993. Contextual Word Similarity
and Estimation From Sparse Data. In Proceedings of
the 31st Annual Meeting of the ACL.
Dagan, I. et al., 1994. Similarity-Based Estimation
of Word Cooccurrence Probabilities. In Proceedings
of the 32nd Annual Meeting of the ACL, Las Cruces,
pp272-278.
Fano, R., 1961. Transmission of Information. MIT
Press, Cambridge, Mass.
Gale, W., et al., 1992. A Method for Disambiguating
Word Senses in a Large Corpus. Computer and
Humanities, vol. 26 pp.415-439.
Grislunan, R. and J. Sterling, 1993. Smoothing of
automatically generated selectional constraints. In
Human Language Technology, pp.254-259, San
Francisco, California. Advanced Research Projects
Agency, Software and Intelligent Systems
Technology Office, Morgan Kaufmann.
Hatzivassiloglou, V., 1994. Do We Need Linguistics
When We Have Statistics? A Comparative Analysis
of the Contributions of Linguistic Cues to a
Statistical Word Grouping System. In Proceedings
of Workshop The Balancing Act: Combining
Symbolic and Statistical Approaches to Language,
Las Cruces, New Mexico. Association of
Computational Linguistics.
Hearst, M., .1991. Noun Homograph Disambiguation
Using Local Context in Large Text Corpora, Using
Corpora, University of Waterloo, Waterloo, Ontario.
Kelly, E. and P. Stone, 1975. Computer Recognition
of English Word Senses, North-Holland, Amsterdam.
Pereira F., et al., 1993. Distributional Clustering of
English words. In Proceedings of the 31st Annual
Meeting of the ACL. pp183-190.
Procter, P., et al. (eds.), 1978. Longman Dictionary
of Contemporary English, Longman Group.
Resnik, P., 1992. WordNet and distributional
analysis: A class-based approach to lexical
discovery. In Proceedings of AAAI Workshop on
Statistically-based NLP Techniques, San Jose,
California.
Yarowslcy, D., 1992. Word-sense Disambiguation
using Statistical Models of Roget&apos;s Categories
Trained on Large Corpora. In Proceedings of
COLING92, pp.454-460.
</reference>
<page confidence="0.997523">
188
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.839820">
<title confidence="0.997359">Statistical Sense Disambiguation with Relatively Small Corpora Using Dictionary Definitions</title>
<author confidence="0.999979">Alpha K Luk</author>
<affiliation confidence="0.9446345">Microsoft Institute Department of Computing North Ryde, NSW 2113, Australia Macquarie University</affiliation>
<address confidence="0.977162">t-alphal@microsoft.com NSW 2109, Australia</address>
<abstract confidence="0.996792181818182">Corpus-based sense disambiguation methods, like most other statistical NLP approaches, suffer from the problem of data sparseness. In this paper, we describe an approach which overcomes this problem using dictionary definitions. Using the definitionbased conceptual co-occurrence data collected from the relatively small Brown corpus, our sense disambiguation system achieves an average accuracy comparable to human performance given the same contextual information.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Black</author>
</authors>
<title>An Experiment In Computational Discrimination of English Word Senses.</title>
<date>1988</date>
<journal>IBM Journal of research and development,</journal>
<volume>32</volume>
<pages>185--194</pages>
<contexts>
<context position="880" citStr="Black, 1988" startWordPosition="110" endWordPosition="111">nse disambiguation methods, like most other statistical NLP approaches, suffer from the problem of data sparseness. In this paper, we describe an approach which overcomes this problem using dictionary definitions. Using the definitionbased conceptual co-occurrence data collected from the relatively small Brown corpus, our sense disambiguation system achieves an average accuracy comparable to human performance given the same contextual information. 1 Introduction Previous corpus-based sense disambiguation methods require substantial amounts of sense-tagged training data (Kelly and Stone, 1975; Black, 1988 and Hearst, 1991) or aligned bilingual corpora (Brown et al., 1991; Dagari, 1991 and Gale et al. 1992). Yarowsky (1992) introduces a thesaurus-based approach to statistical sense disambiguation which works on monolingual corpora without the need for sense-tagged training data. By collecting statistical data of word occurrences in the context of different thesaurus categories from a relatively large corpus (10 million words), the system can identify salient words for each category. Using these salient words, the system is able to disambiguate polysemous words with respect to thesaurus categori</context>
</contexts>
<marker>Black, 1988</marker>
<rawString>Black, E., 1988. An Experiment In Computational Discrimination of English Word Senses. IBM Journal of research and development, vol. 32, pp.185-194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
</authors>
<title>Word-sense Disambiguation using Statistical Methods.</title>
<date>1991</date>
<booktitle>In Proceedings of 29th annual meeting of ACL,</booktitle>
<pages>264--270</pages>
<marker>Brown, 1991</marker>
<rawString>Brown, P., et al., 1991. Word-sense Disambiguation using Statistical Methods. In Proceedings of 29th annual meeting of ACL, pp.264-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P et at Brown</author>
</authors>
<title>Class-based n-gram Models of Natural Language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--4</pages>
<marker>Brown, 1992</marker>
<rawString>Brown, P. et at., 1992. Class-based n-gram Models of Natural Language. Computational Linguistics, 18(4):467-479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>P Hanks</author>
</authors>
<title>Word Association Norms, Mutual Information, and Lexicography.</title>
<date>1989</date>
<booktitle>In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>76--83</pages>
<contexts>
<context position="10138" citStr="Church and Hanks (1989)" startWordPosition="1605" endWordPosition="1608">d concp. exp. CE&apos; score(CS,CS&apos;) = I score(CS ,DC&apos;) VDC&apos;eCS&apos; for any concp. sets CS and CS&apos; [4] score(CS, DC) = E score(DC, DC)IICSI VDCECS for any concp. set CS and def. concept DC&apos; score(DC,DC&apos;)= max(0, I(DC,DC&apos;)) for any def. concepts DC and DC&apos; [6] I(DC,DC) is the mutual information4 (Fano, 1961) between the 2 defining concepts DC and DC&apos; given by: (using the Maximum Likelihood Estimator). f(x,y) is looked up directly from the conceptual cooccurrence data table, f(x) and f(y) are looked up from a pre-constructed list of f(DC) values, for each defining concept DC: f (DC) = Ef(DC,DC&apos;) VDC&apos; 4 Church and Hanks (1989) use Mutual Information to measure word association norms. [5] [3] 13(x,Y) 1(x,y) ---E log2 P(x) • P(y) r.l f(x,y)• N og , I (x). (y) 183 N is taken to be the total number of pairs of words processed, given by f(DC)/2 VDC since for each pair of surface words processed, Ef(DC) VLC is increased by 2. Our scoring method is based on a probabilistic model at the conceptual level. In a standard model, the logarithm of the probability of occurrence of a conceptual set {x1, xz x,n} in the context of the conceptual set {y,, yz ..y„} is given by log, P(x,,x2,...,x„,ly1,y2,...,y0 m (n iog2 P(x,)) 1.1 J.1</context>
</contexts>
<marker>Church, Hanks, 1989</marker>
<rawString>Church, K. and P. Hanks, 1989. Word Association Norms, Mutual Information, and Lexicography. In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics, pp.76-83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
</authors>
<title>Two Languages Are More Informative Than One.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting of the ACL,</booktitle>
<pages>130--137</pages>
<marker>Dagan, 1991</marker>
<rawString>Dagan, I. et al., 1991. Two Languages Are More Informative Than One. In Proceedings of the 29th Annual Meeting of the ACL, pp130-137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
</authors>
<title>Contextual Word Similarity and Estimation From Sparse Data.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the ACL.</booktitle>
<marker>Dagan, 1993</marker>
<rawString>Dagan, I. et al., 1993. Contextual Word Similarity and Estimation From Sparse Data. In Proceedings of the 31st Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
</authors>
<title>Similarity-Based Estimation of Word Cooccurrence Probabilities.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the ACL, Las Cruces,</booktitle>
<pages>272--278</pages>
<marker>Dagan, 1994</marker>
<rawString>Dagan, I. et al., 1994. Similarity-Based Estimation of Word Cooccurrence Probabilities. In Proceedings of the 32nd Annual Meeting of the ACL, Las Cruces, pp272-278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Fano</author>
</authors>
<title>Transmission of Information.</title>
<date>1961</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="9815" citStr="Fano, 1961" startWordPosition="1552" endWordPosition="1553">C,mp) and w(DCinci) where w(DC) is the weight of DC,„„,, given by 1 w(DC„,„ ) — CE.HCS, I Figure 2. The algorithm for collecting conceptual co-occurrence data score(CS,C) = E score(CS,CEI) VCE&apos;eC&apos; for any concp. set CS and concp. exp. set C&apos; [2] score(CS,CE&apos;)= max score(CS,CS&apos;) cs-GcE. for any concp. set CS and concp. exp. CE&apos; score(CS,CS&apos;) = I score(CS ,DC&apos;) VDC&apos;eCS&apos; for any concp. sets CS and CS&apos; [4] score(CS, DC) = E score(DC, DC)IICSI VDCECS for any concp. set CS and def. concept DC&apos; score(DC,DC&apos;)= max(0, I(DC,DC&apos;)) for any def. concepts DC and DC&apos; [6] I(DC,DC) is the mutual information4 (Fano, 1961) between the 2 defining concepts DC and DC&apos; given by: (using the Maximum Likelihood Estimator). f(x,y) is looked up directly from the conceptual cooccurrence data table, f(x) and f(y) are looked up from a pre-constructed list of f(DC) values, for each defining concept DC: f (DC) = Ef(DC,DC&apos;) VDC&apos; 4 Church and Hanks (1989) use Mutual Information to measure word association norms. [5] [3] 13(x,Y) 1(x,y) ---E log2 P(x) • P(y) r.l f(x,y)• N og , I (x). (y) 183 N is taken to be the total number of pairs of words processed, given by f(DC)/2 VDC since for each pair of surface words processed, Ef(DC) </context>
</contexts>
<marker>Fano, 1961</marker>
<rawString>Fano, R., 1961. Transmission of Information. MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gale</author>
</authors>
<title>A Method for Disambiguating Word Senses in a Large Corpus.</title>
<date>1992</date>
<journal>Computer and Humanities,</journal>
<volume>26</volume>
<pages>415--439</pages>
<marker>Gale, 1992</marker>
<rawString>Gale, W., et al., 1992. A Method for Disambiguating Word Senses in a Large Corpus. Computer and Humanities, vol. 26 pp.415-439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grislunan</author>
<author>J Sterling</author>
</authors>
<title>Smoothing of automatically generated selectional constraints.</title>
<date>1993</date>
<booktitle>In Human Language Technology,</booktitle>
<pages>254--259</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco, California.</location>
<marker>Grislunan, Sterling, 1993</marker>
<rawString>Grislunan, R. and J. Sterling, 1993. Smoothing of automatically generated selectional constraints. In Human Language Technology, pp.254-259, San Francisco, California. Advanced Research Projects Agency, Software and Intelligent Systems Technology Office, Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Hatzivassiloglou</author>
</authors>
<title>Do We Need Linguistics When We Have Statistics? A Comparative Analysis of the Contributions of Linguistic Cues to a Statistical Word Grouping System.</title>
<date>1994</date>
<journal>Association of Computational Linguistics.</journal>
<booktitle>In Proceedings of Workshop The Balancing Act: Combining Symbolic and Statistical Approaches to Language,</booktitle>
<location>Las Cruces, New</location>
<contexts>
<context position="25970" citStr="Hatzivassiloglou (1994)" startWordPosition="4295" endWordPosition="4296">to overcome the data sparseness problem may also lead to further improvement of sense disambiguation technologies. In many cases, semantic coherence information is not adequate to select the correct sense, and knowledge about local constraints is needed.13 For disambiguation of polysemous nouns, these constraints include the modifiers of these nouns and the verbs which take these nouns as objects, etc. This knowledge has been successfully acquired from corpora in manual or semi-automatic approaches such as that described in Hearst (1991). However, fully automatic lexically based approaches 13 Hatzivassiloglou (1994) shows that the introduction of linguistic cues improves the performance of a statistical semantic knowledge acquisition system in the context of word grouping. 187 such as that described in Yarowslcy (1992) are very unlikely to be capable of acquiring this finer knowledge because the problem of data sparseness becomes even more serious with the introduction of syntactic constraints. Our approach has overcome the data sparseness problem by using the defining concepts of words. It is found to be effective in acquiring semantic coherence knowledge from a relatively small corpus. It is possible t</context>
</contexts>
<marker>Hatzivassiloglou, 1994</marker>
<rawString>Hatzivassiloglou, V., 1994. Do We Need Linguistics When We Have Statistics? A Comparative Analysis of the Contributions of Linguistic Cues to a Statistical Word Grouping System. In Proceedings of Workshop The Balancing Act: Combining Symbolic and Statistical Approaches to Language, Las Cruces, New Mexico. Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Noun Homograph Disambiguation Using Local Context in</title>
<date>1991</date>
<institution>Large Text Corpora, Using Corpora, University of Waterloo,</institution>
<location>Waterloo, Ontario.</location>
<contexts>
<context position="898" citStr="Hearst, 1991" startWordPosition="113" endWordPosition="114">on methods, like most other statistical NLP approaches, suffer from the problem of data sparseness. In this paper, we describe an approach which overcomes this problem using dictionary definitions. Using the definitionbased conceptual co-occurrence data collected from the relatively small Brown corpus, our sense disambiguation system achieves an average accuracy comparable to human performance given the same contextual information. 1 Introduction Previous corpus-based sense disambiguation methods require substantial amounts of sense-tagged training data (Kelly and Stone, 1975; Black, 1988 and Hearst, 1991) or aligned bilingual corpora (Brown et al., 1991; Dagari, 1991 and Gale et al. 1992). Yarowsky (1992) introduces a thesaurus-based approach to statistical sense disambiguation which works on monolingual corpora without the need for sense-tagged training data. By collecting statistical data of word occurrences in the context of different thesaurus categories from a relatively large corpus (10 million words), the system can identify salient words for each category. Using these salient words, the system is able to disambiguate polysemous words with respect to thesaurus categories. Statistical ap</context>
<context position="25890" citStr="Hearst (1991)" startWordPosition="4286" endWordPosition="4287">urate than by chance. Our success in using definitions of word senses to overcome the data sparseness problem may also lead to further improvement of sense disambiguation technologies. In many cases, semantic coherence information is not adequate to select the correct sense, and knowledge about local constraints is needed.13 For disambiguation of polysemous nouns, these constraints include the modifiers of these nouns and the verbs which take these nouns as objects, etc. This knowledge has been successfully acquired from corpora in manual or semi-automatic approaches such as that described in Hearst (1991). However, fully automatic lexically based approaches 13 Hatzivassiloglou (1994) shows that the introduction of linguistic cues improves the performance of a statistical semantic knowledge acquisition system in the context of word grouping. 187 such as that described in Yarowslcy (1992) are very unlikely to be capable of acquiring this finer knowledge because the problem of data sparseness becomes even more serious with the introduction of syntactic constraints. Our approach has overcome the data sparseness problem by using the defining concepts of words. It is found to be effective in acquiri</context>
</contexts>
<marker>Hearst, 1991</marker>
<rawString>Hearst, M., .1991. Noun Homograph Disambiguation Using Local Context in Large Text Corpora, Using Corpora, University of Waterloo, Waterloo, Ontario.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Kelly</author>
<author>P Stone</author>
</authors>
<title>Computer Recognition of English Word Senses,</title>
<date>1975</date>
<location>North-Holland, Amsterdam.</location>
<contexts>
<context position="867" citStr="Kelly and Stone, 1975" startWordPosition="106" endWordPosition="109">bstract Corpus-based sense disambiguation methods, like most other statistical NLP approaches, suffer from the problem of data sparseness. In this paper, we describe an approach which overcomes this problem using dictionary definitions. Using the definitionbased conceptual co-occurrence data collected from the relatively small Brown corpus, our sense disambiguation system achieves an average accuracy comparable to human performance given the same contextual information. 1 Introduction Previous corpus-based sense disambiguation methods require substantial amounts of sense-tagged training data (Kelly and Stone, 1975; Black, 1988 and Hearst, 1991) or aligned bilingual corpora (Brown et al., 1991; Dagari, 1991 and Gale et al. 1992). Yarowsky (1992) introduces a thesaurus-based approach to statistical sense disambiguation which works on monolingual corpora without the need for sense-tagged training data. By collecting statistical data of word occurrences in the context of different thesaurus categories from a relatively large corpus (10 million words), the system can identify salient words for each category. Using these salient words, the system is able to disambiguate polysemous words with respect to thesa</context>
</contexts>
<marker>Kelly, Stone, 1975</marker>
<rawString>Kelly, E. and P. Stone, 1975. Computer Recognition of English Word Senses, North-Holland, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
</authors>
<title>Distributional Clustering of English words.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the ACL.</booktitle>
<pages>183--190</pages>
<marker>Pereira, 1993</marker>
<rawString>Pereira F., et al., 1993. Distributional Clustering of English words. In Proceedings of the 31st Annual Meeting of the ACL. pp183-190.</rawString>
</citation>
<citation valid="true">
<title>Longman Dictionary of Contemporary English,</title>
<date>1978</date>
<editor>Procter, P., et al. (eds.),</editor>
<publisher>Longman Group.</publisher>
<marker>1978</marker>
<rawString>Procter, P., et al. (eds.), 1978. Longman Dictionary of Contemporary English, Longman Group.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>WordNet and distributional analysis: A class-based approach to lexical discovery.</title>
<date>1992</date>
<booktitle>In Proceedings of AAAI Workshop on Statistically-based NLP Techniques,</booktitle>
<location>San Jose, California.</location>
<contexts>
<context position="21998" citStr="Resnik, 1992" startWordPosition="3666" endWordPosition="3667">n Yarowslcy (1992). In these cases, the senses used by Yarowslcy are adopted for easier comparison. 8. All results are based on 100% recall. 186 5 Related Work it is based on salient words) and may not perform as well on general text as our approach. Previous attempts to tackle the data sparseness problem in general corpus-based work include the class-based approaches and similarity-based approaches. In these approaches, relationships between a given pair of words are modelled by analogy with other words that resemble the given pair in some way. The class-based approaches (Brown et al., 1992; Resnik, 1992; Pereira et al., 1993) calculate co-occurrence data of words belonging to different classes,&amp;quot; rather than individual words, to enhance the co-occurrence data collected and to cover words which have low occurrence frequencies. Dagan et al. (1993) argue that using a relatively small number of classes to model the similarity between words may lead to substantial loss of information. In the similaritybased approaches (Dagan et al., 1993 &amp; 1994; Grishman et al., 1993), rather than a class, each word is modelled by its own set of similar words derived from statistical data collected from corpora. H</context>
<context position="23658" citStr="Resnik (1992)" startWordPosition="3927" endWordPosition="3928"> words allows our method to work on corpora of much smaller sizes. In our approach, each word is modelled by its own set of defining concepts. Although only 1792 defining concepts are used, the set of all possible combinations (a power set of the defining concepts) is so huge that it is very unlikely two word senses will have the same combination of defining concepts unless they are almost identical in meaning. On the other hand, the thesaurus-based method of Yarowsky (1992) may suffer from loss of information (since it is semi-class-based) as well as data sparseness (since 11 Classes used in Resnik (1992) are based on the WordNet taxonomy while classes of Brown et al. (1992) and Pereira et al. (1993) are derived from statistical data collected from corpora. 12 The corpus used in Dagan et al. (1994) contains 40.5 million words. 6 Limitation and Further work Being a dictionary-based method, the natural limitation of our approach is the dictionary. The most serious problem is that many of the words in the controlled vocabulary of LDOCE are polysemous themselves. The result is that many of our list of 1792 defining concepts actually stand for a number of distinct concepts. For example, the definin</context>
</contexts>
<marker>Resnik, 1992</marker>
<rawString>Resnik, P., 1992. WordNet and distributional analysis: A class-based approach to lexical discovery. In Proceedings of AAAI Workshop on Statistically-based NLP Techniques, San Jose, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowslcy</author>
</authors>
<title>Word-sense Disambiguation using Statistical Models of Roget&apos;s Categories Trained on Large Corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING92,</booktitle>
<pages>454--460</pages>
<contexts>
<context position="13468" citStr="Yarowslcy (1992)" startWordPosition="2162" endWordPosition="2163">nt senses and different lengths of the context. The evidence from a polysemous context word is taken to be the evidence from its sense with the highest mutual information score ([3]). This is due to the fact that only one of the senses is used in the given sentence. 3 Evaluation Our system is tested on the twelve words discussed in Yarowsky (1992) and previous publications on sense disambiguation. Results are shown in Table 1. Our system achieves an average accuracy of 77% on a mean 3-way sense distinction over the twelve words. Numerically, the result is not as good as the 92% as reported in Yarowslcy (1992). However, direct comparison between the numerical results can be misleading since the experiments are carried out on two very different corpora both in size and genre. Firstly, Yarowsky&apos; s system is trained with the 10 million word Grolier&apos;s Encyclopedia, which is a magnitude larger than the Brown corpus used by our system. Secondly, and more importantly, the two corpora, which are also the test corpora, are very different in genre. Semantic coherence of text, on which both systems rely, is generally stronger in technical writing than in most other kinds of text. Statistical disambiguation sy</context>
<context position="17065" citStr="Yarowslcy (1992)" startWordPosition="2764" endWordPosition="2765">as Local Context Another significant point our experiments have shown is that the sentence can also provide enough contextual information for semantic coherence based 7 The result is less than conclusive since only one human subject is tested. In order to acquire more reliable results, we are currently seeking a few more subjects to repeat the experiment. 8 The subject has not read through the whole corpus. approaches in a large proportion of cases.° The average sentence length in the Brown corpus is 19.410 words which is 5 times smaller than the 100 word window used in Gale et al. (1992) and Yarowslcy (1992). Our approach works well even with a small &amp;quot;window&amp;quot; because it is based on the identification of salient concepts rather than salient words. In salient word based approaches, due to the problem of data sparseness, many less frequently occurring words which are intuitively salient to a particular word sense will not be identified in practice unless an extremely large corpus is used. Therefore the sentence usually does not contain enough identified salient words to provide enough contextual information. Using conceptual cooccurrence data, contextual information from the salient but less frequen</context>
<context position="20647" citStr="Yarowslcy (1992)" startWordPosition="3436" endWordPosition="3437">unit* – – – 100% metallurgy* – – – __100% __ _ –87)320 40A7– &amp;quot;TY .; -0 STAR 4 75% 75% 96% space object shaped object 0 – – 95% celebrity 11 45% 64% 82% 15 53% 67% 96% TASTE 21 100% 95% 93% flavour preference 26 96% 85% 93% – –47— –973c70-- – 19/0 7– –93% Notes: 1. N marks the column with the number of test samples for each sense. DBCC (Definition-Based Conceptual Cooccurrence) and Human mark the columns with the results of our system and the human subject in disambiguating the occurrences of the 12 words in the Brown corpus, respectively. Thes. (thesaurus) marks the column with the results of Yarowslcy (1992) tested on the Grolier&apos;s Encyclopedia. 2. The &amp;quot;correct&amp;quot; sense of each test sample is chosen by hand disambiguation carried out by the author using the sentence as the context. A small proportion of test samples cannot be disambiguated within the given context and are excluded from the experiment. 3. The senses marked with * are used in Yarowsky (1992) but no corresponding sense is found in LDOCE. 4. The sense marked with ** is defmed in LDOCE but not used in Yarowslcy (1992). 6. In our experiment, the words are disambiguated between all the senses listed except the ones marked with *. 7. The r</context>
<context position="26177" citStr="Yarowslcy (1992)" startWordPosition="4326" endWordPosition="4327">ledge about local constraints is needed.13 For disambiguation of polysemous nouns, these constraints include the modifiers of these nouns and the verbs which take these nouns as objects, etc. This knowledge has been successfully acquired from corpora in manual or semi-automatic approaches such as that described in Hearst (1991). However, fully automatic lexically based approaches 13 Hatzivassiloglou (1994) shows that the introduction of linguistic cues improves the performance of a statistical semantic knowledge acquisition system in the context of word grouping. 187 such as that described in Yarowslcy (1992) are very unlikely to be capable of acquiring this finer knowledge because the problem of data sparseness becomes even more serious with the introduction of syntactic constraints. Our approach has overcome the data sparseness problem by using the defining concepts of words. It is found to be effective in acquiring semantic coherence knowledge from a relatively small corpus. It is possible that a similar approach based on dictionary definitions will be successful in acquiring knowledge of local constraints from a reasonably sized corpus. 7 Conclusion We have shown that using definition-based co</context>
</contexts>
<marker>Yarowslcy, 1992</marker>
<rawString>Yarowslcy, D., 1992. Word-sense Disambiguation using Statistical Models of Roget&apos;s Categories Trained on Large Corpora. In Proceedings of COLING92, pp.454-460.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>