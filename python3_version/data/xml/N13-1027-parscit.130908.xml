<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000147">
<title confidence="0.99019">
Unsupervised Learning Summarization Templates from Concise Summaries
</title>
<author confidence="0.977255">
Horacio Saggion*
</author>
<affiliation confidence="0.762139666666667">
Universitat Pompeu Fabra
Department of Information and Communication Technologies
TALN Group
</affiliation>
<address confidence="0.918346333333333">
C/Tanger 122 - Campus de la Comunicaci´on
Barcelona - 08018
Spain
</address>
<email confidence="0.94242">
http://www.dtic.upf.edu/˜hsaggion/
</email>
<sectionHeader confidence="0.994181" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9989955">
We here present and compare two unsuper-
vised approaches for inducing the main con-
ceptual information in rather stereotypical
summaries in two different languages. We
evaluate the two approaches in two differ-
ent information extraction settings: mono-
lingual and cross-lingual information extrac-
tion. The extraction systems are trained on
auto-annotated summaries (containing the in-
duced concepts) and evaluated on human-
annotated documents. Extraction results are
promising, being close in performance to
those achieved when the system is trained on
human-annotated summaries.
</bodyText>
<sectionHeader confidence="0.998973" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995330916666667">
Information Extraction (Piskorski and Yangarber,
2013) and Automatic Text Summarization (Saggion
and Poibeau, 2013) are two Natural Language Pro-
cessing tasks which require domain and language
adaptation. For over two decades (Riloff, 1993;
Riloff, 1996) the natural language processing com-
munity has been interested in automatic or semi-
automatic methods which could be used to port sys-
tems from one domain or task to another, aiming at
reducing at least in part the cost associated with the
creation of human annotated datasets. Automatic
system adaptation can take different forms: if high
</bodyText>
<footnote confidence="0.9122968">
&apos;Phis work is partially supported by Ministerio de Economfa
y Competitividad, Secretarfa de Estado de Investigaci´on, De-
sarrollo e Innovaci´on, Spain under project number TIN2012-
38584-C06-03 and Advanced Research Fellowship RYC-2009-
04291. We thank Biljana Drndarevi´c for proofreading the paper.
</footnote>
<bodyText confidence="0.956488444444444">
quality human annotated data is available, then rule-
based or statistical systems can be trained on this
data (Brill, 1994), reducing the efforts of writing
rules and handcrafting dictionaries. If high quality
human annotated data is unavailable, a large non-
annotated corpus and a bootstrapping procedure can
be used to produce annotated data (Ciravegna and
Wilks, 2003; Yangarber, 2003). Here, we concen-
trate on developing and evaluating automatic proce-
dures to learn the main concepts of a domain and
at the same time auto-annotate texts so that they be-
come available for training information extraction or
text summarization applications. However, it would
be naive to think that in the current state of the art we
would be able to learn all knowledge from text au-
tomatically (Poon and Domingos, 2010; Biemann,
2005; Buitelaar and Magnini, 2005). We therefore
here concentrate on learning template-like represen-
tations from concise event summaries which should
contain the key information of an event.
18 de julio de 1994DateOfAttack . Un atentado
contra la sede de la Asociaci´on Mutual Israelita
ArgentinaTarget de Buenos AiresPlaceOfAttack causa la
muerte de 86NumberOfVictims personas.
(18th July 1994. An attack against the headquarters
of the Jewish Mutual Association in Buenos Aires, Ar-
gentina, kills 86people.)
</bodyText>
<figureCaption confidence="0.9568395">
Figure 1: Sample of Human Annotated Summary in
Spanish
</figureCaption>
<bodyText confidence="0.996431666666667">
An example of the summaries we want to learn
from is presented in Figure 1. It is a summary in
the terrorist attack domain in Spanish. It has been
</bodyText>
<page confidence="0.946026">
270
</page>
<note confidence="0.4710115">
Proceedings of NAACL-HLT 2013, pages 270–279,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.99994275862069">
manually annotated with concepts such as DateO-
fAttack, Target, PlaceOfAttack, and NumberOfVic-
tims, which are key in the domain. Our task is to
discover from this kind of summary what the con-
cepts are and how to recognise them automatically.
As will be shown in this paper and unlike current
approaches (Chambers and Jurafsky, 2011; Leung et
al., 2011), the methods to be presented here do not
require parsing or semantic dictionaries to work or
specification of the underlying number of concepts
in the domain to be learn. The approach we take
learns concepts in the set of domain summaries, re-
lying on noun phrase contextual information. They
are able to generate reasonable domain conceptual-
izations from relatively small datasets and in differ-
ent languages.
The rest of the paper is structured as follows: In
Section 2 we overview related work in the area of
concept induction from text. Next, in Section 3 we
describe the dataset used and how we have processed
it while in Section 4 we outline the two unsuper-
vised learning algorithms we compare in this paper
for template induction from text. Then, in Section 5,
we describe the experiments on template induction
indicating how we have instantiated the algorithms
and in Section 6 we explain how we have extrinsi-
cally evaluated the induction process. In Section 7
we discuss the obtained results and in Section 8 we
summarize our findings and close the paper.
</bodyText>
<sectionHeader confidence="0.999745" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999850117647059">
A long standing issue in natural language process-
ing is how to learn conceptualizations from text in
automatic or semi-automatic ways. The availabil-
ity of redundant data has been used, for example,
to discover template-like representations (Barzilay
and Lee, 2003) or sentence-level paraphrases which
could be used for extraction or generation. Vari-
ous approaches to concept learning use clustering
techniques. (Leung et al., 2011) apply various clus-
tering procedures to learn a small number of slots
in three typical information extraction domains, us-
ing manually annotated data and fixing the num-
ber of concepts to be learnt. (Li et al., 2010) gen-
erate templates and extraction patterns for specific
entity types (actors, companies, etc.). (Chambers
and Jurafsky, 2011) learn the structure of MUC tem-
plates from raw data in English, an approach that
needs both full parsing and semantic interpretation
using WordNet (Fellbaum, 1998) in order to extract
verb arguments and measure the similarity betweern
verbs. In (Saggion, 2012) an iterative learning pro-
cedure is used to discover core domain conceptual
information from short summaries in two languages.
However, the obtained results were not assessed in a
real information extraction scenario. There are ap-
proaches which do not need any human interven-
tion or sophisticated text processing, but learn based
on redundancy of the input dataset and some well
grounded linguistic intuitions (Banko and Etzioni,
2008; Etzioni et al., 2004). Related to the work pre-
sented here are approaches that aim at generating
short stereotypical summaries (DeJong, 1982; Paice
and Jones, 1993; Ratnaparkhi, 2000; Saggion and
Lapalme, 2002; Konstas and Lapata, 2012).
</bodyText>
<sectionHeader confidence="0.97859" genericHeader="method">
3 Dataset and Text Processing Steps
</sectionHeader>
<bodyText confidence="0.999569608695652">
For the experiments reported here we rely on the
CONCISUS corpus1 (Saggion and Szasz, 2012)
which is distributed free of charge. It is a corpus
of Web summaries in Spanish and English in four
different application domains: Aviation Accidents
(32 English, 32 Spanish), Earthquakes (44 English,
56 Spanish), Train Accidents (36 English, 43 Span-
ish), and Terrorist Attacks (42 English, 53 Spanish).
The dataset contains original and comparable sum-
mary pairs, automatic translations of Spanish sum-
maries into English, automatic translation of English
summaries into Spanish, and associated original full
documents in Spanish and English for two of the do-
mains (Aviation Accidents and Earthquakes). The
dataset comes with human annotations representing
the key information in each domain. In Table 1
we detail the concepts used in each of the domains.
Note that not all concepts are represented in each
of the summaries. Creation of such a dataset can
take up to 500 hours for a human annotator, con-
sidering data collection, cleansing, and annotation
proper. Only one human annotator and one curator
were responsible for the annotation process.
</bodyText>
<footnote confidence="0.990823">
1http://www.taln.upf.edu/pages/concisus/.
</footnote>
<page confidence="0.992282">
271
</page>
<table confidence="0.969528428571429">
Aviation Accident Airline, Cause, DateOfAccident, Destination, FlightNumber, NumberOfVictims, Origin, Passengers, Place, Sur-
vivors, Crew, TypeOfAccident, TypeOfAircraft, Year
Earthquake City, Country, DateOfEarthquake, Depth, Duration, Epicentre, Fatalities, Homeless, Injured, Magnitude, Other-
PlacesAffected, Province, Region, Survivors, TimeOfEarthquake
Terrorist Attack City, Country, DateOfAccident, Fatalities, Injured, Target, Perpetrator, Place, NumberOfVictims, TypeOfAttack
Train Accident Cause, DateOfAccident, Destination, NumberOfVictims, Origin, Passenger, Place, Survivors, TypeOfAccident,
TypeOfTrain
</table>
<tableCaption confidence="0.999745">
Table 1: Conceptual Information in Summaries
</tableCaption>
<subsectionHeader confidence="0.999068">
3.1 Text Processing
</subsectionHeader>
<bodyText confidence="0.999954666666667">
In order to carry out experimentation we adopt the
GATE infrastructure for document representation
and annotation (Maynard et al., 2002). All doc-
uments in the dataset are processed with available
natural language processors to compute shallow lin-
gustic information. Documents in English are pro-
cessed with the ANNIE system, a morphological an-
alyzer, and a noun chunker, all three from GATE.
The documents in Spanish are analyzed with Tree-
Tagger (Schmid, 1995), a rule-base noun chunker,
and an SVM-based named entity recognition and
classification system.
</bodyText>
<sectionHeader confidence="0.995376" genericHeader="method">
4 Concept Induction Algorithms
</sectionHeader>
<bodyText confidence="0.999880888888889">
Two algorithms are used to induce conceptual in-
formation in a domain from a set of textual sum-
maries. The algorithms form concepts based on tar-
get strings (or chunks) in the set of summaries us-
ing token-level linguistic information. The chunks
are represented with different features which are ex-
plained later in Section 5.1. One algorithm we use
is based on clustering, while the other is based on
iterative learning.
</bodyText>
<subsectionHeader confidence="0.978112">
4.1 Clustering-based Induction
</subsectionHeader>
<bodyText confidence="0.999972888888889">
The procedure for learning conceptual information
by clustering is straithforward: the chunks in the set
of summaries are represented as instances consider-
ing both internal and sourrounding linguistic infor-
mation. These instances are the input to a clustering
procedure which returns a list of clusters each con-
taining a set of chunks. We consider each cluster as
a key concept in the set of domain summaries and
the chunks in each cluster as the concept extension.
</bodyText>
<subsectionHeader confidence="0.953988">
4.2 Iterative Induction
</subsectionHeader>
<bodyText confidence="0.9007488">
We use the iterative learning algorithm described in
(Saggion, 2012) which learns from a set of sum-
maries S, also annotated with target strings (e.g.
chunks) and shallow linguistic information. In a nut-
shell the algorithm is as follows:
</bodyText>
<listItem confidence="0.996234833333333">
(1) Choose a document D from the set of summaries S
and add it to a training set TRAIN. Set REST to
S − TRAIN.
(2) Choose an available target concept T from D, i.e. a
target concept not tried before by the algorithm.
(3) Train a classifier on TRAIN to learn instances of
the target concept using the available linguistic fea-
tures; the classifier uses the linguistic information
provided.
(4) Apply the classifier to REST (all summaries minus
those in TRAIN) to annotate all instances of the tar-
get concept T.
(5) Select a document BEST in REST, where there is an
instance of the concept recognised with the highest
probability in the REST set.
(6) Remove BEST from REST and add BEST to the
training set, remove all identified instances of T
from REST, and go to step 3.
</listItem>
<bodyText confidence="0.999915214285715">
The algorithm is executed a number of times (see
Section 5.1 for parametrization of the algorithms)
to learn all concepts in the set of summaries, and
at each iteration a single concept is formed. There
are two circumstances when a concept being formed
is discarded and their associated initial target con-
cept removed from the learning process: one case is
when there are not enough occurrences of the con-
cept across a set of summaries; another case is when
too many identical strings are proposed as instances
for the concept in the set of summaries. This latter
restriction is only valid if we consider sets of non-
redundant documents, which is the case to which we
restrict our experiments.
</bodyText>
<subsectionHeader confidence="0.999176">
4.3 Text Chunks
</subsectionHeader>
<bodyText confidence="0.99975">
Given that the algorithms presented above try to in-
duce a concept from the chunks in the summaries,
</bodyText>
<page confidence="0.980152">
272
</page>
<bodyText confidence="0.99975053125">
we are interested in assessing how the type of chunk
influences the learning process. Also, given that our
objective is to test methods which learn with mini-
mal human intervention, we are interested in inves-
tigating differences between the use of manual and
automatic chunks. We therefore use the following
chunk types in this work: gold chunks (gold) are the
human produced annotations (as in Figure 1); named
entity chunks (ne) are named entities computed by an
off-the-shelf named entity recognizer; noun chunks
(nc) are text chunks identified by rule-based off-the-
shelf NP chunkers and finally, wiki chunks (wiki) are
strings of text in the summaries which happen to be
Wikipedia titles.
In order to automatically compute these chunk
types, different levels of knowledge are needed.
For example, NP chunks require syntactic infor-
mation, while named entities and wiki chunks re-
quire some external form of knowledge, such as
precompiled gazetteer lists or access to an ency-
clopædia or a semantic dictionary. Named enti-
ties and noun chunks are computed as described in
Section 3, while wiki chunks are computed as fol-
lows: string n-grams w1w2...w,,, are computed in
each summary and strings w1 w2 ... w,,, are checked
against the Wikipedia on-line encyclopædia, if a
hit occurs (i.e. if for an English n-gram the page
en.wikipedia.org/wiki/w1... w,,, exists or for a Span-
ish n-gram the page es.wikipedia.org/wiki/w1... w,,,
exists), the n-gram is annotated in the summary as a
wiki chunk. Wiki chunks are cached to speed up the
automatic annotation process.
</bodyText>
<table confidence="0.999637583333334">
Spanish R F
P
Terrorist Attack 0.47 0.10 0.17
Aviation Accident 0.52 0.08 0.14
Earthquake 0.24 0.06 0.10
Train Accident 0.59 0.15 0.24
English R F
P
Terrorist Attack 0.46 0.39 0.42
Aviation Accident 0.40 0.27 0.32
Earthquake 0.27 0.22 0.24
Train Accident 0.57 0.27 0.36
</table>
<tableCaption confidence="0.993566">
Table 2: Baseline Induction Performance
</tableCaption>
<subsectionHeader confidence="0.9952095">
4.4 Mapping the Induced Concepts onto
Human Concepts
</subsectionHeader>
<bodyText confidence="0.999942058823529">
For evaluation purposes, each induced concept is
mapped onto one human concept applying the fol-
lowing procedure: let HCi be the set of summary
offsets where human concept i occurs, and let ICi be
the set of summary offsets where automatic concept
i occurs, then the induced concept j is mapped onto
concept k such that: k = arg maxi(|HCi n ICS|),
where |X |is the size of set X. That is, the in-
duced concept is mapped onto the label it gives it
a best match. As an example, one induced concept
in the terrorist attack domain containing the follow-
ing string instances: two bombs, car bomb, pair of
bombs, 10 coordinated shooting and bombing, two
car bombs, suicide bomb, the attack, guerrilla war-
fare, the coca-growing regions, etc. This induced
concept is mapped onto the TypeOfAttack human
concept in that domain.
</bodyText>
<subsectionHeader confidence="0.984419">
4.5 Baseline Concept Induction
</subsectionHeader>
<bodyText confidence="0.999826285714286">
A baseline induction mechanism is designed for
comparison with the two learning procedures pro-
posed here. It is based on the mapping of named en-
tity chunks onto concepts in a straightforward way:
each named entity type is considered a different con-
cept and therefore mapped onto human concepts as
in Section 4.4. For example, in the terrorist attack
domain, Organization named entity type is mapped
by this procedure onto the human concept Target
(i.e. churches, government buildings, etc., are com-
mon targets in terrorist attacks) while in the Avia-
tion Accident domain the Organization named en-
tity type is mapped onto TypeOfAircraft (i.e. Boe-
ing, Airbus, etc. are names of organizations).
</bodyText>
<sectionHeader confidence="0.8219235" genericHeader="method">
5 Experimental Setting and Results of the
Induction Process
</sectionHeader>
<bodyText confidence="0.999807666666667">
In this section we detail the different parameters
used by the algorithms and report the performance
of the induction process with different inputs.
</bodyText>
<subsectionHeader confidence="0.952975">
5.1 Settings
</subsectionHeader>
<bodyText confidence="0.99927175">
The features used by the induction procedure are ex-
tracted from the text tokens. We extract the POS tag,
root, and string of each token. The clustering-based
algorithm uses a standard Expectation Maximization
</bodyText>
<page confidence="0.994102">
273
</page>
<table confidence="0.991390785714286">
Spanish
Iterative Clustering
P R F P R F
Terrorist Attack 0.25 0.59 0.35 0.59 0.59 0.591
Aviation Accident 0.50 0.62 0.55 0.66 0.66 0.661
Earthquake 0.34 0.51 0.41 0.56 0.53 0.551
Train Accident 0.41 0.69 0.52 0.58 0.58 0.58
English
Iterative Clustering
P R F P R F
Terrorist Attack 0.23 0.39 0.29 0.50 0.50 0.501
Aviation Accident 0.57 0.68 0.62 0.79 0.79 0.791
Earthquake 0.26 0.53 0.34 0.39 0.39 0.39
Train Accident 0.50 0.59 0.54 0.61 0.61 0.611
</table>
<tableCaption confidence="0.971243">
Table 3: Conceptual induction (Spanish and English) Using Gold
Chunks for Learning
</tableCaption>
<bodyText confidence="0.99990575">
implementation from the Weka machine learning li-
brary (Witten and Frank, 1999). We instruct the al-
gorithm to decide on the number of clusters based on
the data, instead of setting the number of clusters by
hand. The instances to cluster are representations of
the input chunks; these representations contain the
internal features of the chunks, as well as the infor-
mation of 5 tokens to the left of the beginning of
the chunk and 5 tokens to the right of the end of the
chunk. The transformation from GATE documents
into arff Weka files and the mapping from Weka onto
the GATE documents, is carried out using specific
programs. The classification algorithm used for the
iterative learning process is an SVM classifier dis-
tributed with the GATE system and tuned to per-
form chunk learning using the same features as the
clustering procedure (Li et al., 2004). This classifier
outputs a probability which we use for selecting the
best document at step (5) of the iterative procedure.
The document selected to start the process is the one
with more target strings, and the target string chosen
is the next available in textual order. The iterative
learning procedure is set to stop when the number
of concepts induced reaches the average number of
chunks in the corpus. Induced concepts not covering
at least 10% of the number of documents are dis-
carded, as are concepts with strings repeated at least
10% of the concept extension.
</bodyText>
<subsectionHeader confidence="0.981753">
5.2 Experiments and Results
</subsectionHeader>
<bodyText confidence="0.999377">
We carry out a number of experiments per domain
where we run the algorithms using as input the sum-
maries annotated with a different chunk type each
time. After each experiment all concepts induced are
</bodyText>
<table confidence="0.983321892857143">
Terrorist Attacks
Iterative F Clustering F
P R P R
nc 0.22 0.53 0.311 0.15 0.51 0.23
ne 0.27 0.14 0.18 0.12 0.42 0.18
wiki 0.15 0.26 0.19 0.22 0.18 0.20
all 0.25 0.53 0.341 0.12 0.51 0.20
Aviation Accidents
Iterative Clustering
P R F P R F
nc 0.30 0.50 0.381 0.21 0.51 0.30
ne 0.84 0.07 0.14 0.57 0.07 0.13
wiki 0.29 0.28 0.281 0.27 0.17 0.21
all 0.39 0.62 0.481 0.16 0.31 0.21
Earthquakes
Iterative Clustering
P R F P R F
nc 0.29 0.42 0.341 0.14 0.42 0.21
ne 0.20 0.19 0.201 0.38 0.02 0.05
wiki 0.16 0.16 0.16 0.24 0.11 0.15
all 0.28 0.50 0.361 0.12 0.46 0.19
Train Accidents
Iterative Clustering
P R F P R F
nc 0.36 0.66 0.471 0.23 0.51 0.32
ne 0.33 0.66 0.441 0.65 0.12 0.20
wiki 0.25 0.25 0.25 0.51 0.13 0.21
all 0.33 0.62 0.441 0.16 0.50 0.24
</table>
<tableCaption confidence="0.999938">
Table 4: Comparison of conceptual induction in Spanish
</tableCaption>
<bodyText confidence="0.999962083333333">
mapped onto the human concepts (see Section 4.4)
producing auto-annotated summaries. The auto-
matic annotations are then compared with the gold
annotations, and precision, recall, and f-score fig-
ures are computed to observe the performance of the
two algorithms, the baseline, and the effect of type
of chunk on the learning process.
In Table 2 we report baseline performance on the
entire dataset. As can be appreciated by the obtained
numbers, directly mapping named entity types onto
concepts does not provide a very good performance,
especially for Spanish; we expected the learning
procedures to produce better results. In Table 3 we
present the results of inducing concepts from the
gold chunks by the two algorithms. In almost all
cases, using gold chunks improves over the baseline
procedure, except for the Terrorist Attack domain
in English, where the iterative learning procedure
underperforms the baseline. In all tested domains,
the clustering-based induction procedure has a very
competitive performance. A t-test is run to verify
differences in performance between the two systems
in terms of f-score. In all tested domains in Span-
ish, except the Train Accident domain, there are sta-
</bodyText>
<page confidence="0.996779">
274
</page>
<table confidence="0.990414392857143">
Terrorist Attacks
Iterative Clustering F
P R F P R
nc 0.43 0.50 0.461 0.23 0.42 0.30
ne 0.28 0.44 0.34 0.42 0.29 0.34
wiki 0.24 0.33 0.281 0.15 0.25 0.19
all 0.31 0.49 0.381 0.09 0.39 0.15
Aviation Accidents
Iterative Clustering
P R F P R F
nc 0.48 0.31 0.38 0.33 0.34 0.34
ne 0.53 0.38 0.441 0.63 0.27 0.38
wiki 0.31 0.44 0.361 0.28 0.37 0.32
all 0.50 0.67 0.581 0.15 0.47 0.23
Earthquakes
Iterative Clustering
P R F P R F
nc 0.29 0.48 0.361 0.06 0.40 0.10
ne 0.28 0.34 0.30 0.30 0.25 0.28
wiki 0.21 0.30 0.251 0.16 0.23 0.19
all 0.31 0.44 0.371 0.08 0.40 0.13
Train Accidents
Iterative Clustering
P R F P R F
nc 0.45 0.54 0.491 0.32 0.50 0.39
ne 0.47 0.29 0.36 0.58 0.27 0.36
wiki 0.51 0.32 0.391 0.30 0.29 0.29
all 0.50 0.58 0.531 0.16 0.49 0.24
</table>
<tableCaption confidence="0.998553">
Table 5: Comparison of conceptual induction in English
</tableCaption>
<table confidence="0.9998188">
Spanish F
P R
Aviation Accident 0.83 0.60 0.70
Earthquake 0.61 0.48 0.53
Train Accident 0.77 0.54 0.64
English
P R F
Aviation Accident 0.88 0.38 0.53
Earthquake 0.86 0.56 0.68
Train Accident 0.84 0.43 0.57
</table>
<tableCaption confidence="0.938258">
Table 6: Cross-lingual Information Extraction. System Trained with
Gold Summaries.
</tableCaption>
<table confidence="0.997530722222222">
Aviation Accidents
Iterative F Clustering F
P R P R
gold 0.85 0.52 0.651 0.84 0.41 0.55
all 0.88 0.49 0.631 0.87 0.19 0.32
nc 0.87 0.46 0.60 0.88 0.46 0.60
Earthquakes
Iterative Clustering
P R F P R F
gold 0.65 0.41 0.501 0.66 0.31 0.43
all 0.64 0.36 0.46 0.62 0.40 0.49
nc 0.63 0.33 0.43 0.67 0.38 0.49
Train Accidents
Iterative Clustering
P R F P R F
gold 0.81 0.54 0.65 0.82 0.52 0.64
all 0.81 0.52 0.641 0.72 0.31 0.43
nc 0.79 0.54 0.641 0.79 0.42 0.55
</table>
<tableCaption confidence="0.991673333333333">
Table 7: Cross-lingual Information Extraction Results in Spanish
Translations. System trained with auto-annotated summaries in Span-
ish.
</tableCaption>
<table confidence="0.995259833333334">
Aviation Accidents
Iterative F Clustering F
P R P R
gold 0.87 0.35 0.50 0.87 0.37 0.52
all 0.87 0.37 0.521 0.82 0.18 0.29
nc 0.90 0.21 0.341 0.90 0.17 0.29
Earthquakes
Iterative Clustering
P R F P R F
gold 0.87 0.53 0.661 0.87 0.36 0.51
all 0.88 0.51 0.641 0.87 0.30 0.45
nc 0.88 0.51 0.651 0.93 0.43 0.59
Train Accidents
Iterative Clustering
P R F P R F
gold 0.82 0.30 0.44 0.87 0.32 0.47
all 0.84 0.39 0.531 0.91 0.24 0.38
nc 0.89 0.36 0.511 0.46 0.25 0.32
</table>
<tableCaption confidence="0.9994065">
Table 8: Cross-lingual Information Extraction Results in English
Translations. System trained with auto-annotated summaries in English.
</tableCaption>
<bodyText confidence="0.999866357142857">
tistically significant differences between the cluster-
ing procedure and the iterative learning procedure
(p = 0.01). In all tested domains in English, except
for the Earthquake domain, there are statistically
significant differences between the performance of
clustering and iterative learning (p = 0.01).
Now we turn to the results of both algorithms
when automatic chunks are used, that is, when no
human annotation is provided to the learners. Re-
sults are reported in Tables 4 (Spanish) and 5 (En-
glish). The results are presented by the chunk type
used during the learning procedure. In addition
to the chunk types specified above, we include a
type all, which represents the use of all automat-
ically computed chunks (i.e. nc, ne, wiki). We
observe that, in general, when presented with au-
tomatic chuks, the iterative learning procedure is
able to induce concepts with a better f-score than
the clustering-based algorithm. A t-test is run to
verify differences between the two induction pro-
cedures within each chunk condition (differences
shown with a † in the tables). In 11 out of 16 cases
in Spanish and in 12 out of 16 cases in English,
statistically significant differences are observed. In
three out of four domains the combination of au-
tomatic chunks outperforms the use of individual
chunk types. Generally, named entity chunks and
wiki chunks have the lowest performance. This is
</bodyText>
<page confidence="0.995929">
275
</page>
<table confidence="0.962586">
Spanish
P R F
Aviation Accident 0.56 0.47 0.51
Earthquake 0.64 0.41 0.50
English
P R F
Aviation Accident 0.61 0.35 0.44
Earthquake 0.78 0.41 0.54
</table>
<tableCaption confidence="0.981252">
Table 9: Extraction from Full Documents. System Trained on Gold
Summaries.
</tableCaption>
<table confidence="0.884585333333333">
Aviation Accidents
Iterative Clustering
P R F P R F
gold 0.55 0.37 0.44 0.54 0.31 0.39
all 0.55 0.36 0.43† 0.69 0.17 0.27
nc 0.45 0.22 0.30† 0.52 0.26 0.35
Earthquake
Iterative Clustering
P R F P R F
gold 0.62 0.31 0.41† 0.63 0.22 0.33
all 0.61 0.26 0.37 0.63 0.31 0.41†
nc 0.60 0.24 0.35 0.70 0.28 0.40†
</table>
<tableCaption confidence="0.972924">
Table 10: Full-text Information Extraction Results in Spanish. Sys-
tem trained with auto-annotated summaries in Spanish.
</tableCaption>
<bodyText confidence="0.9998092">
not an unexpected result since named entities, for
example, cover much fewer strings which may form
part of a concept extension. Additionally, off-the-
shelf entity recogizers only identify a limited num-
ber of entity types.
</bodyText>
<sectionHeader confidence="0.9266275" genericHeader="method">
6 Information Extraction Evaluation
Framework
</sectionHeader>
<bodyText confidence="0.999944466666667">
The numbers above are interesting because they pro-
vide intrinsic evaluation of the concept induction
procedure, but they do not tell us much about their
usability. Therefore, and in order to better assess
the value of the discovered concepts, we decided to
carry out two extrinsic evaluations using an informa-
tion extraction task. Once the conceps are induced
and, as a result, the summaries are auto-annotated
with domain specific concepts, we decide to train
an off-the-shelf SVM token classification procedure
and apply it to unseen human annotated documents.
The SVM classifier uses the same linguistic infor-
mation as the induction procedures: token level in-
formation and a window size of 5 around each token
to be classified.
</bodyText>
<subsectionHeader confidence="0.6874865">
Aviation Accidents
Iterative Clustering
</subsectionHeader>
<table confidence="0.968455">
P R F P R F
gold 0.60 0.28 0.39 0.62 0.31 0.41†
all 0.62 0.30 0.41† 0.54 0.14 0.23
nc 0.53 0.15 0.23† 0.46 0.10 0.16
Earthquake
Iterative Clustering
P R F P R F
gold 0.70 0.35 0.47† 0.72 0.32 0.44
all 0.74 0.37 0.49 0.70 0.22 0.34
nc 0.73 0.36 0.48† 0.73 0.30 0.42
</table>
<tableCaption confidence="0.9831895">
Table 11: Full-text Information Extraction Results in English. Sys-
tem trained with auto-annotated summaries in English.
</tableCaption>
<subsectionHeader confidence="0.99921">
6.1 Extraction from Automatic Translations
</subsectionHeader>
<bodyText confidence="0.999970454545455">
The first task we carry out is cross-lingual informa-
tion extraction where the input documents are auto-
matic translations of summaries in Spanish and En-
glish2. Note that the expriment is performed in three
domains for which such translations are manually
annotated. We first run an experiment to assess the
extraction performance of the SVM when trained on
human annotated data. Results of the experiment
are reported in Table 6 and they should be taken
as an upperbound of the performance of a system
trained on auto-annotated summaries. We then train
the SVM on the different auto-annotated datasets,
but note that due to space restrictions, we here only
report the three most revealing experiments per lan-
guage: concepts induced with gold chunks, noun
chunks, and all automatic chunks. Results are re-
ported in Table 7 (Spanish) and in Table 8 (English).
In most cases the SVM trained with auto-annotated
summaries produced by the iterative learning proce-
dure outperforms the clustering-based method with
statistically significant differences († shown in the
tables) (p = 0.01).
</bodyText>
<subsectionHeader confidence="0.999664">
6.2 Extraction from Full Documents
</subsectionHeader>
<bodyText confidence="0.999312777777778">
The second and the last evaluation consists in the ap-
plication of the SVM extraction system to full doc-
uments. In this case, the experiment can be run only
in two domains for which full documents have been
provided and manually annotated. We first test the
performance of the system when trained on human
annotated summaries and present the results in Ta-
ble 9. Results of the experiments when the system
is trained on auto-annotated datasets are shown in
</bodyText>
<footnote confidence="0.941262">
2The translations were produced by Google translator.
</footnote>
<page confidence="0.997043">
276
</page>
<bodyText confidence="0.999340666666667">
Tables 10 (Spanish) and 11 (English). Results are
lower than when training on clean human annotated
summaries. It is unclear which approach is more
competitive when training with auto-annotated sum-
maries. What is clear is that the performance of
the iterative learning algorithm when training with
concepts induced from gold chunks is not statisti-
cally different (according to a t-test and p = 0.01)
from the performance of the algorithm when training
with concepts induced from automatically computed
chunks. We consider this to be a positive outcome of
the experiments.
</bodyText>
<sectionHeader confidence="0.999687" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999984129032258">
The two methods presented here are able to produce
partial domain conceptualizations from a relatively
small set of domain summaries3. We have found
that the clustering-based procedure is very competi-
tive when presented with gold chunks. On the other
hand, the iterative learning procedure performs very
well when presented with automatic chunks in all
tested domains and the two languages. We have also
found that the performance of the iterative induction
system is not much affected by the use of automati-
cally computed chunks. We have run a t-test to ver-
ify the differences in induction performance when
learning with gold and automatic chunks (all con-
dition) and have found statistically significant dif-
ferences in only one domain out of four in Spanish
(Terrorist Attack) and in two domains out of four
in English (Aviation Accident and Train Accident)
(p = 0.01). The applicability of the induction pro-
cess, that is, if the auto-annotated data could be used
for specific tasks, has been tested in two information
extraction experiments. In a cross-lingual informa-
tion extraction setting (Riloff et al., 2002; Saggion
and Szasz, 2011) we have observed that a system
trained on automatically computed chunks has a per-
formance close to one trained on concepts induced
from gold chunks. No statistically significant differ-
ences exist (p = 0.01) between the use of automatic
chunks and gold chunks, except for the Train Acci-
dent domain in English, where the system trained
on fully automatically annotated summaries has a
better performance. In a full document information
</bodyText>
<footnote confidence="0.4831995">
3Depending on the language and domain, between 50% and
77% of all concepts are generated.
</footnote>
<bodyText confidence="0.999921736842105">
extraction task, although the best system trained on
auto-annotated summaries in Spanish has a big dif-
ference with respect to a system trained on human-
annotated summaries, in English the differences are
slight. We belive that this is due to the differences
in performance between the underlying text process-
ing components. Our methods work by grouping to-
gether sets of chunks, unlike (Chambers and Juraf-
sky, 2011), whose approach is centered around verb
arguments and clustering, and relies on the avail-
ability of considerable amounts of data. Ontology
learning approaches such as OntoUSP (Poon and
Domingos, 2010) are also clustering-based but fo-
cus on learning is-a relations only. Unlike (Leung et
al., 2011) whose approach is based on gold-standard
humman annotations, we here test the performance
of the induction process using automatically com-
puted candidate strings, and we additionally learn
the number of concepts automatically.
</bodyText>
<sectionHeader confidence="0.995536" genericHeader="conclusions">
8 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999643136363636">
In this paper we have concentrated on the prob-
lem of knowledge induction from text summaries.
The approaches we have presented are fully unsu-
pervised and are able to produce reasonable con-
ceptualizations (close to human concepts) without
relying on annotated data. Unlike previous work,
our approach does not require full syntactic parsing
or a semantic dictionary. In fact, it only requires
a process of text chunking and named entity recog-
nition, which we have carefully assessed here. We
believe our work contributes with a viable method-
ology to induce conceptual information from texts,
and at the same time with an auto-annotation mech-
anism which could be used to train information ex-
traction systems. Since our procedure requires very
little linguistic information, we believe it can be suc-
cessfully applied to a number of languages. We also
believe that there is much work to be carried out and
that induction from summaries should be comple-
mented with a process that explores full event re-
ports, in order to reinforce some induced concepts,
discard others, and discover additional ones.
</bodyText>
<sectionHeader confidence="0.994674" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.711698">
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
</reference>
<page confidence="0.984923">
277
</page>
<reference confidence="0.999668858490566">
Proceedings of ACL-08, pages 28–36. Association for
Computational Linguistics, June.
R. Barzilay and L. Lee. 2003. Learning to paraphrase: an
unsupervised approach using multiple-sequence align-
ment. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy - Volume 1, NAACL ’03, pages 16–23, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Chris Biemann. 2005. Ontology Learning from Text: A
Survey of Methods. LDV Forum, 20(2):75–93.
E. Brill. 1994. Some Advances in Transformation-Based
Part of Speech Tagging. In Proceedings of the Twelfth
National Conference on AI (AAAI-94), Seattle, Wash-
ington.
P. Buitelaar and B. Magnini. 2005. Ontology learning
from text: An overview. In In Paul Buitelaar, P., Cimi-
ano, P., Magnini B. (Eds.), Ontology Learning from
Text: Methods, Applications and Evaluation, pages 3–
12. IOS Press.
N. Chambers and D. Jurafsky. 2011. Template-Based In-
formation Extraction without the Templates. In ACL,
pages 976–986.
Fabio Ciravegna and Yorick Wilks. 2003. Designing
adaptive information extraction for the semantic web
in amilcare. In Annotation for the Semantic Web,
Frontiers in Artificial Intelligence and Applications.
IOS. Press.
Gerald DeJong. 1982. An Overview of the FRUMP Sys-
tem. In W.G. Lehnert and M.H. Ringle, editors, Strate-
gies for Natural Language Processing, pages 149–
176. Lawrence Erlbaum Associates, Publishers.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. S. Weld, and A. Yates.
2004. Methods for Domain-Indepedent Information
Extraction from the Web: An Experimental Compari-
son. In Proceedings ofAAAI-2004.
Christiane Fellbaum, editor. 1998. WordNet - An Elec-
tronic Lexical Database. MIT Press.
I. Konstas and M. Lapata. 2012. Concept-to-text gener-
ation via discriminative reranking. In Proceedings of
the 50th Annual Meeting of the Association for Com-
putational Linguistics, pages 369–378, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
Cane Wing-ki Leung, Jing Jiang, Kian Ming A. Chai,
Hai Leong Chieu, and Loo-Nin Teow. 2011. Unsuper-
vised information extraction with distributional prior
knowledge. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Process-
ing, pages 814–824, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.
Y. Li, K. Bontcheva, and H. Cunningham. 2004. An
SVM Based Learning Algorithm for Information Ex-
traction. Machine Learning Workshop, Sheffield.
P. Li, J. Jiang, and Y. Wang. 2010. Generating Templates
of Entity Summaries with an Entity-Aspect Model and
Pattern Mining. In Proceedings of ACL, Uppsala.
ACL.
D. Maynard, V. Tablan, H. Cunningham, C. Ursu, H. Sag-
gion, K. Bontcheva, and Y. Wilks. 2002. Architectural
Elements of Language Engineering Robustness. Jour-
nal of Natural Language Engineering – Special Issue
on Robust Methods in Analysis of Natural Language
Data, 8(2/3):257–274.
Chris D. Paice and Paul A. Jones. 1993. The Identi-
fication of Important Concepts in Highly Structured
Technical Papers. In R. Korfhage, E. Rasmussen, and
P. Willett, editors, Proc. of the 16th ACM-SIGIR Con-
ference, pages 69–78.
J. Piskorski and R. Yangarber. 2013. Information ex-
traction: Past, present and future. In Thierry Poibeau,
Horacio Saggion, Jakub Piskorski, and Roman Yan-
garber, editors, Multi-source, Multilingual Informa-
tion Extraction and Summarization, Theory and Ap-
plications of Natural Language Processing, pages 23–
49. Springer Berlin Heidelberg.
H. Poon and P. Domingos. 2010. Unsupervised ontology
induction from text. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, ACL ’10, pages 296–305, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Adwait Ratnaparkhi. 2000. Trainable methods for sur-
face natural language generation. In Proceedings of
the 1st North American chapter of the Association for
Computational Linguistics conference, NAACL 2000,
pages 194–201, Stroudsburg, PA, USA. Association
for Computational Linguistics.
E. Riloff, C. Schafer, and D. Yarowsky. 2002. Induc-
ing information extraction systems for new languages
via cross-language projection. In Proceedings of the
19th international conference on Computational lin-
guistics, pages 1–7, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
E. Riloff. 1993. Automatically constructing a dictionary
for information extraction tasks. Proceedings of the
Eleventh Annual Conference on Artificial Intelligence,
pages 811–816.
E. Riloff. 1996. Automatically generating extraction pat-
terns from untagged text. Proceedings of the Thir-
teenth Annual Conference on Artificial Intelligence,
pages 1044–1049.
H. Saggion and G. Lapalme. 2002. Generat-
ing Indicative-Informative Summaries with SumUM.
Computational Linguistics.
</reference>
<page confidence="0.966398">
278
</page>
<reference confidence="0.99938396875">
H. Saggion and T. Poibeau. 2013. Automatic text
summarization: Past, present and future. In Thierry
Poibeau, Horacio Saggion, Jakub Piskorski, and Ro-
man Yangarber, editors, Multi-source, Multilingual In-
formation Extraction and Summarization, Theory and
Applications of Natural Language Processing, pages
3–21. Springer Berlin Heidelberg.
H. Saggion and S. Szasz. 2011. Multi-domain Cross-
lingual Information Extraction from Clean and Noisy
Texts. In Proceedings of the 8th Brazilian Sympo-
sium in Information and Human Language Technol-
ogy, Cuiab´a, Brazil. BCS.
H. Saggion and S. Szasz. 2012. The CONCISUS Corpus
of Event Summaries. In Proceedings of the 8th Lan-
guage Resources and Evaluation Conference (LREC),
Istanbul, Turkey. ELDA.
H. Saggion. 2012. Unsupervised content discovery
from concise summaries. In Proceedings of the Joint
Workshop on Automatic Knowledge Base Construc-
tion and Web-scale Knowledge Extraction, AKBC-
WEKEX ’12, pages 13–18, Stroudsburg, PA, USA.
Association for Computational Linguistics.
H. Schmid. 1995. Improvements In Part-of-Speech Tag-
ging With an Application To German. In In Proceed-
ings of the ACL SIGDAT-Workshop, pages 47–50.
I. H. Witten and E. Frank. 1999. Data Mining: Practi-
cal Machine Learning Tools and Techniques with Java
Implementations. Morgan Kaufmann.
R. Yangarber. 2003. Counter-Training in Discovery of
Semantic Patterns. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics (ACL’03).
</reference>
<page confidence="0.998543">
279
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.104812">
<title confidence="0.4167935">Unsupervised Learning Summarization Templates from Concise Summaries Universitat Pompeu Department of Information and Communication TALN</title>
<author confidence="0.341713">Campus de_la Barcelona</author>
<web confidence="0.997857">http://www.dtic.upf.edu/˜hsaggion/</web>
<abstract confidence="0.998807666666667">We here present and compare two unsupervised approaches for inducing the main conceptual information in rather stereotypical summaries in two different languages. We evaluate the two approaches in two different information extraction settings: monolingual and cross-lingual information extraction. The extraction systems are trained on auto-annotated summaries (containing the induced concepts) and evaluated on humanannotated documents. Extraction results are promising, being close in performance to those achieved when the system is trained on human-annotated summaries.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Oren Etzioni</author>
</authors>
<title>The tradeoffs between open and traditional relation extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08,</booktitle>
<pages>28--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="6321" citStr="Banko and Etzioni, 2008" startWordPosition="970" endWordPosition="973">oach that needs both full parsing and semantic interpretation using WordNet (Fellbaum, 1998) in order to extract verb arguments and measure the similarity betweern verbs. In (Saggion, 2012) an iterative learning procedure is used to discover core domain conceptual information from short summaries in two languages. However, the obtained results were not assessed in a real information extraction scenario. There are approaches which do not need any human intervention or sophisticated text processing, but learn based on redundancy of the input dataset and some well grounded linguistic intuitions (Banko and Etzioni, 2008; Etzioni et al., 2004). Related to the work presented here are approaches that aim at generating short stereotypical summaries (DeJong, 1982; Paice and Jones, 1993; Ratnaparkhi, 2000; Saggion and Lapalme, 2002; Konstas and Lapata, 2012). 3 Dataset and Text Processing Steps For the experiments reported here we rely on the CONCISUS corpus1 (Saggion and Szasz, 2012) which is distributed free of charge. It is a corpus of Web summaries in Spanish and English in four different application domains: Aviation Accidents (32 English, 32 Spanish), Earthquakes (44 English, 56 Spanish), Train Accidents (36</context>
</contexts>
<marker>Banko, Etzioni, 2008</marker>
<rawString>Michele Banko and Oren Etzioni. 2008. The tradeoffs between open and traditional relation extraction. In Proceedings of ACL-08, pages 28–36. Association for Computational Linguistics, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>L Lee</author>
</authors>
<title>Learning to paraphrase: an unsupervised approach using multiple-sequence alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03,</booktitle>
<pages>16--23</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5120" citStr="Barzilay and Lee, 2003" startWordPosition="784" endWordPosition="787">plate induction from text. Then, in Section 5, we describe the experiments on template induction indicating how we have instantiated the algorithms and in Section 6 we explain how we have extrinsically evaluated the induction process. In Section 7 we discuss the obtained results and in Section 8 we summarize our findings and close the paper. 2 Related Work A long standing issue in natural language processing is how to learn conceptualizations from text in automatic or semi-automatic ways. The availability of redundant data has been used, for example, to discover template-like representations (Barzilay and Lee, 2003) or sentence-level paraphrases which could be used for extraction or generation. Various approaches to concept learning use clustering techniques. (Leung et al., 2011) apply various clustering procedures to learn a small number of slots in three typical information extraction domains, using manually annotated data and fixing the number of concepts to be learnt. (Li et al., 2010) generate templates and extraction patterns for specific entity types (actors, companies, etc.). (Chambers and Jurafsky, 2011) learn the structure of MUC templates from raw data in English, an approach that needs both f</context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>R. Barzilay and L. Lee. 2003. Learning to paraphrase: an unsupervised approach using multiple-sequence alignment. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03, pages 16–23, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
</authors>
<title>Ontology Learning from Text: A Survey of Methods.</title>
<date>2005</date>
<journal>LDV Forum,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="2586" citStr="Biemann, 2005" startWordPosition="377" endWordPosition="378">lity human annotated data is unavailable, a large nonannotated corpus and a bootstrapping procedure can be used to produce annotated data (Ciravegna and Wilks, 2003; Yangarber, 2003). Here, we concentrate on developing and evaluating automatic procedures to learn the main concepts of a domain and at the same time auto-annotate texts so that they become available for training information extraction or text summarization applications. However, it would be naive to think that in the current state of the art we would be able to learn all knowledge from text automatically (Poon and Domingos, 2010; Biemann, 2005; Buitelaar and Magnini, 2005). We therefore here concentrate on learning template-like representations from concise event summaries which should contain the key information of an event. 18 de julio de 1994DateOfAttack . Un atentado contra la sede de la Asociaci´on Mutual Israelita ArgentinaTarget de Buenos AiresPlaceOfAttack causa la muerte de 86NumberOfVictims personas. (18th July 1994. An attack against the headquarters of the Jewish Mutual Association in Buenos Aires, Argentina, kills 86people.) Figure 1: Sample of Human Annotated Summary in Spanish An example of the summaries we want to l</context>
</contexts>
<marker>Biemann, 2005</marker>
<rawString>Chris Biemann. 2005. Ontology Learning from Text: A Survey of Methods. LDV Forum, 20(2):75–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Some Advances in Transformation-Based Part of Speech Tagging.</title>
<date>1994</date>
<booktitle>In Proceedings of the Twelfth National Conference on AI (AAAI-94),</booktitle>
<location>Seattle, Washington.</location>
<contexts>
<context position="1891" citStr="Brill, 1994" startWordPosition="265" endWordPosition="266">main or task to another, aiming at reducing at least in part the cost associated with the creation of human annotated datasets. Automatic system adaptation can take different forms: if high &apos;Phis work is partially supported by Ministerio de Economfa y Competitividad, Secretarfa de Estado de Investigaci´on, Desarrollo e Innovaci´on, Spain under project number TIN2012- 38584-C06-03 and Advanced Research Fellowship RYC-2009- 04291. We thank Biljana Drndarevi´c for proofreading the paper. quality human annotated data is available, then rulebased or statistical systems can be trained on this data (Brill, 1994), reducing the efforts of writing rules and handcrafting dictionaries. If high quality human annotated data is unavailable, a large nonannotated corpus and a bootstrapping procedure can be used to produce annotated data (Ciravegna and Wilks, 2003; Yangarber, 2003). Here, we concentrate on developing and evaluating automatic procedures to learn the main concepts of a domain and at the same time auto-annotate texts so that they become available for training information extraction or text summarization applications. However, it would be naive to think that in the current state of the art we would</context>
</contexts>
<marker>Brill, 1994</marker>
<rawString>E. Brill. 1994. Some Advances in Transformation-Based Part of Speech Tagging. In Proceedings of the Twelfth National Conference on AI (AAAI-94), Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Buitelaar</author>
<author>B Magnini</author>
</authors>
<title>Ontology learning from text: An overview. In In</title>
<date>2005</date>
<booktitle>Ontology Learning from Text: Methods, Applications and Evaluation,</booktitle>
<pages>3--12</pages>
<publisher>IOS Press.</publisher>
<contexts>
<context position="2616" citStr="Buitelaar and Magnini, 2005" startWordPosition="379" endWordPosition="382">tated data is unavailable, a large nonannotated corpus and a bootstrapping procedure can be used to produce annotated data (Ciravegna and Wilks, 2003; Yangarber, 2003). Here, we concentrate on developing and evaluating automatic procedures to learn the main concepts of a domain and at the same time auto-annotate texts so that they become available for training information extraction or text summarization applications. However, it would be naive to think that in the current state of the art we would be able to learn all knowledge from text automatically (Poon and Domingos, 2010; Biemann, 2005; Buitelaar and Magnini, 2005). We therefore here concentrate on learning template-like representations from concise event summaries which should contain the key information of an event. 18 de julio de 1994DateOfAttack . Un atentado contra la sede de la Asociaci´on Mutual Israelita ArgentinaTarget de Buenos AiresPlaceOfAttack causa la muerte de 86NumberOfVictims personas. (18th July 1994. An attack against the headquarters of the Jewish Mutual Association in Buenos Aires, Argentina, kills 86people.) Figure 1: Sample of Human Annotated Summary in Spanish An example of the summaries we want to learn from is presented in Figu</context>
</contexts>
<marker>Buitelaar, Magnini, 2005</marker>
<rawString>P. Buitelaar and B. Magnini. 2005. Ontology learning from text: An overview. In In Paul Buitelaar, P., Cimiano, P., Magnini B. (Eds.), Ontology Learning from Text: Methods, Applications and Evaluation, pages 3– 12. IOS Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chambers</author>
<author>D Jurafsky</author>
</authors>
<title>Template-Based Information Extraction without the Templates. In</title>
<date>2011</date>
<booktitle>ACL,</booktitle>
<pages>976--986</pages>
<contexts>
<context position="3756" citStr="Chambers and Jurafsky, 2011" startWordPosition="557" endWordPosition="560">mmary in Spanish An example of the summaries we want to learn from is presented in Figure 1. It is a summary in the terrorist attack domain in Spanish. It has been 270 Proceedings of NAACL-HLT 2013, pages 270–279, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics manually annotated with concepts such as DateOfAttack, Target, PlaceOfAttack, and NumberOfVictims, which are key in the domain. Our task is to discover from this kind of summary what the concepts are and how to recognise them automatically. As will be shown in this paper and unlike current approaches (Chambers and Jurafsky, 2011; Leung et al., 2011), the methods to be presented here do not require parsing or semantic dictionaries to work or specification of the underlying number of concepts in the domain to be learn. The approach we take learns concepts in the set of domain summaries, relying on noun phrase contextual information. They are able to generate reasonable domain conceptualizations from relatively small datasets and in different languages. The rest of the paper is structured as follows: In Section 2 we overview related work in the area of concept induction from text. Next, in Section 3 we describe the data</context>
<context position="5627" citStr="Chambers and Jurafsky, 2011" startWordPosition="862" endWordPosition="865">lability of redundant data has been used, for example, to discover template-like representations (Barzilay and Lee, 2003) or sentence-level paraphrases which could be used for extraction or generation. Various approaches to concept learning use clustering techniques. (Leung et al., 2011) apply various clustering procedures to learn a small number of slots in three typical information extraction domains, using manually annotated data and fixing the number of concepts to be learnt. (Li et al., 2010) generate templates and extraction patterns for specific entity types (actors, companies, etc.). (Chambers and Jurafsky, 2011) learn the structure of MUC templates from raw data in English, an approach that needs both full parsing and semantic interpretation using WordNet (Fellbaum, 1998) in order to extract verb arguments and measure the similarity betweern verbs. In (Saggion, 2012) an iterative learning procedure is used to discover core domain conceptual information from short summaries in two languages. However, the obtained results were not assessed in a real information extraction scenario. There are approaches which do not need any human intervention or sophisticated text processing, but learn based on redunda</context>
<context position="29979" citStr="Chambers and Jurafsky, 2011" startWordPosition="4880" endWordPosition="4884">where the system trained on fully automatically annotated summaries has a better performance. In a full document information 3Depending on the language and domain, between 50% and 77% of all concepts are generated. extraction task, although the best system trained on auto-annotated summaries in Spanish has a big difference with respect to a system trained on humanannotated summaries, in English the differences are slight. We belive that this is due to the differences in performance between the underlying text processing components. Our methods work by grouping together sets of chunks, unlike (Chambers and Jurafsky, 2011), whose approach is centered around verb arguments and clustering, and relies on the availability of considerable amounts of data. Ontology learning approaches such as OntoUSP (Poon and Domingos, 2010) are also clustering-based but focus on learning is-a relations only. Unlike (Leung et al., 2011) whose approach is based on gold-standard humman annotations, we here test the performance of the induction process using automatically computed candidate strings, and we additionally learn the number of concepts automatically. 8 Conclusions and Future Work In this paper we have concentrated on the pr</context>
</contexts>
<marker>Chambers, Jurafsky, 2011</marker>
<rawString>N. Chambers and D. Jurafsky. 2011. Template-Based Information Extraction without the Templates. In ACL, pages 976–986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Ciravegna</author>
<author>Yorick Wilks</author>
</authors>
<title>Designing adaptive information extraction for the semantic web in amilcare.</title>
<date>2003</date>
<booktitle>In Annotation for the Semantic Web, Frontiers in Artificial Intelligence and Applications.</booktitle>
<publisher>IOS. Press.</publisher>
<contexts>
<context position="2137" citStr="Ciravegna and Wilks, 2003" startWordPosition="300" endWordPosition="303">erio de Economfa y Competitividad, Secretarfa de Estado de Investigaci´on, Desarrollo e Innovaci´on, Spain under project number TIN2012- 38584-C06-03 and Advanced Research Fellowship RYC-2009- 04291. We thank Biljana Drndarevi´c for proofreading the paper. quality human annotated data is available, then rulebased or statistical systems can be trained on this data (Brill, 1994), reducing the efforts of writing rules and handcrafting dictionaries. If high quality human annotated data is unavailable, a large nonannotated corpus and a bootstrapping procedure can be used to produce annotated data (Ciravegna and Wilks, 2003; Yangarber, 2003). Here, we concentrate on developing and evaluating automatic procedures to learn the main concepts of a domain and at the same time auto-annotate texts so that they become available for training information extraction or text summarization applications. However, it would be naive to think that in the current state of the art we would be able to learn all knowledge from text automatically (Poon and Domingos, 2010; Biemann, 2005; Buitelaar and Magnini, 2005). We therefore here concentrate on learning template-like representations from concise event summaries which should conta</context>
</contexts>
<marker>Ciravegna, Wilks, 2003</marker>
<rawString>Fabio Ciravegna and Yorick Wilks. 2003. Designing adaptive information extraction for the semantic web in amilcare. In Annotation for the Semantic Web, Frontiers in Artificial Intelligence and Applications. IOS. Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald DeJong</author>
</authors>
<title>An Overview of the FRUMP System.</title>
<date>1982</date>
<booktitle>Strategies for Natural Language Processing,</booktitle>
<pages>149--176</pages>
<editor>In W.G. Lehnert and M.H. Ringle, editors,</editor>
<publisher>Lawrence Erlbaum Associates, Publishers.</publisher>
<contexts>
<context position="6462" citStr="DeJong, 1982" startWordPosition="994" endWordPosition="995">ty betweern verbs. In (Saggion, 2012) an iterative learning procedure is used to discover core domain conceptual information from short summaries in two languages. However, the obtained results were not assessed in a real information extraction scenario. There are approaches which do not need any human intervention or sophisticated text processing, but learn based on redundancy of the input dataset and some well grounded linguistic intuitions (Banko and Etzioni, 2008; Etzioni et al., 2004). Related to the work presented here are approaches that aim at generating short stereotypical summaries (DeJong, 1982; Paice and Jones, 1993; Ratnaparkhi, 2000; Saggion and Lapalme, 2002; Konstas and Lapata, 2012). 3 Dataset and Text Processing Steps For the experiments reported here we rely on the CONCISUS corpus1 (Saggion and Szasz, 2012) which is distributed free of charge. It is a corpus of Web summaries in Spanish and English in four different application domains: Aviation Accidents (32 English, 32 Spanish), Earthquakes (44 English, 56 Spanish), Train Accidents (36 English, 43 Spanish), and Terrorist Attacks (42 English, 53 Spanish). The dataset contains original and comparable summary pairs, automatic </context>
</contexts>
<marker>DeJong, 1982</marker>
<rawString>Gerald DeJong. 1982. An Overview of the FRUMP System. In W.G. Lehnert and M.H. Ringle, editors, Strategies for Natural Language Processing, pages 149– 176. Lawrence Erlbaum Associates, Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Etzioni</author>
<author>M Cafarella</author>
<author>D Downey</author>
<author>A Popescu</author>
<author>T Shaked</author>
<author>S Soderland</author>
<author>D S Weld</author>
<author>A Yates</author>
</authors>
<title>Methods for Domain-Indepedent Information Extraction from the Web: An Experimental Comparison.</title>
<date>2004</date>
<booktitle>In Proceedings ofAAAI-2004.</booktitle>
<contexts>
<context position="6344" citStr="Etzioni et al., 2004" startWordPosition="974" endWordPosition="977"> parsing and semantic interpretation using WordNet (Fellbaum, 1998) in order to extract verb arguments and measure the similarity betweern verbs. In (Saggion, 2012) an iterative learning procedure is used to discover core domain conceptual information from short summaries in two languages. However, the obtained results were not assessed in a real information extraction scenario. There are approaches which do not need any human intervention or sophisticated text processing, but learn based on redundancy of the input dataset and some well grounded linguistic intuitions (Banko and Etzioni, 2008; Etzioni et al., 2004). Related to the work presented here are approaches that aim at generating short stereotypical summaries (DeJong, 1982; Paice and Jones, 1993; Ratnaparkhi, 2000; Saggion and Lapalme, 2002; Konstas and Lapata, 2012). 3 Dataset and Text Processing Steps For the experiments reported here we rely on the CONCISUS corpus1 (Saggion and Szasz, 2012) which is distributed free of charge. It is a corpus of Web summaries in Spanish and English in four different application domains: Aviation Accidents (32 English, 32 Spanish), Earthquakes (44 English, 56 Spanish), Train Accidents (36 English, 43 Spanish), </context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2004</marker>
<rawString>O. Etzioni, M. Cafarella, D. Downey, A. Popescu, T. Shaked, S. Soderland, D. S. Weld, and A. Yates. 2004. Methods for Domain-Indepedent Information Extraction from the Web: An Experimental Comparison. In Proceedings ofAAAI-2004.</rawString>
</citation>
<citation valid="true">
<title>WordNet - An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet - An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Konstas</author>
<author>M Lapata</author>
</authors>
<title>Concept-to-text generation via discriminative reranking.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>369--378</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="6558" citStr="Konstas and Lapata, 2012" startWordPosition="1006" endWordPosition="1009">scover core domain conceptual information from short summaries in two languages. However, the obtained results were not assessed in a real information extraction scenario. There are approaches which do not need any human intervention or sophisticated text processing, but learn based on redundancy of the input dataset and some well grounded linguistic intuitions (Banko and Etzioni, 2008; Etzioni et al., 2004). Related to the work presented here are approaches that aim at generating short stereotypical summaries (DeJong, 1982; Paice and Jones, 1993; Ratnaparkhi, 2000; Saggion and Lapalme, 2002; Konstas and Lapata, 2012). 3 Dataset and Text Processing Steps For the experiments reported here we rely on the CONCISUS corpus1 (Saggion and Szasz, 2012) which is distributed free of charge. It is a corpus of Web summaries in Spanish and English in four different application domains: Aviation Accidents (32 English, 32 Spanish), Earthquakes (44 English, 56 Spanish), Train Accidents (36 English, 43 Spanish), and Terrorist Attacks (42 English, 53 Spanish). The dataset contains original and comparable summary pairs, automatic translations of Spanish summaries into English, automatic translation of English summaries into </context>
</contexts>
<marker>Konstas, Lapata, 2012</marker>
<rawString>I. Konstas and M. Lapata. 2012. Concept-to-text generation via discriminative reranking. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 369–378, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cane Wing-ki Leung</author>
<author>Jing Jiang</author>
<author>Kian Ming A Chai</author>
<author>Hai Leong Chieu</author>
<author>Loo-Nin Teow</author>
</authors>
<title>Unsupervised information extraction with distributional prior knowledge.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>814--824</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="3777" citStr="Leung et al., 2011" startWordPosition="561" endWordPosition="564">f the summaries we want to learn from is presented in Figure 1. It is a summary in the terrorist attack domain in Spanish. It has been 270 Proceedings of NAACL-HLT 2013, pages 270–279, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics manually annotated with concepts such as DateOfAttack, Target, PlaceOfAttack, and NumberOfVictims, which are key in the domain. Our task is to discover from this kind of summary what the concepts are and how to recognise them automatically. As will be shown in this paper and unlike current approaches (Chambers and Jurafsky, 2011; Leung et al., 2011), the methods to be presented here do not require parsing or semantic dictionaries to work or specification of the underlying number of concepts in the domain to be learn. The approach we take learns concepts in the set of domain summaries, relying on noun phrase contextual information. They are able to generate reasonable domain conceptualizations from relatively small datasets and in different languages. The rest of the paper is structured as follows: In Section 2 we overview related work in the area of concept induction from text. Next, in Section 3 we describe the dataset used and how we h</context>
<context position="5287" citStr="Leung et al., 2011" startWordPosition="808" endWordPosition="811">lain how we have extrinsically evaluated the induction process. In Section 7 we discuss the obtained results and in Section 8 we summarize our findings and close the paper. 2 Related Work A long standing issue in natural language processing is how to learn conceptualizations from text in automatic or semi-automatic ways. The availability of redundant data has been used, for example, to discover template-like representations (Barzilay and Lee, 2003) or sentence-level paraphrases which could be used for extraction or generation. Various approaches to concept learning use clustering techniques. (Leung et al., 2011) apply various clustering procedures to learn a small number of slots in three typical information extraction domains, using manually annotated data and fixing the number of concepts to be learnt. (Li et al., 2010) generate templates and extraction patterns for specific entity types (actors, companies, etc.). (Chambers and Jurafsky, 2011) learn the structure of MUC templates from raw data in English, an approach that needs both full parsing and semantic interpretation using WordNet (Fellbaum, 1998) in order to extract verb arguments and measure the similarity betweern verbs. In (Saggion, 2012)</context>
<context position="30277" citStr="Leung et al., 2011" startWordPosition="4927" endWordPosition="4930"> a big difference with respect to a system trained on humanannotated summaries, in English the differences are slight. We belive that this is due to the differences in performance between the underlying text processing components. Our methods work by grouping together sets of chunks, unlike (Chambers and Jurafsky, 2011), whose approach is centered around verb arguments and clustering, and relies on the availability of considerable amounts of data. Ontology learning approaches such as OntoUSP (Poon and Domingos, 2010) are also clustering-based but focus on learning is-a relations only. Unlike (Leung et al., 2011) whose approach is based on gold-standard humman annotations, we here test the performance of the induction process using automatically computed candidate strings, and we additionally learn the number of concepts automatically. 8 Conclusions and Future Work In this paper we have concentrated on the problem of knowledge induction from text summaries. The approaches we have presented are fully unsupervised and are able to produce reasonable conceptualizations (close to human concepts) without relying on annotated data. Unlike previous work, our approach does not require full syntactic parsing or</context>
</contexts>
<marker>Leung, Jiang, Chai, Chieu, Teow, 2011</marker>
<rawString>Cane Wing-ki Leung, Jing Jiang, Kian Ming A. Chai, Hai Leong Chieu, and Loo-Nin Teow. 2011. Unsupervised information extraction with distributional prior knowledge. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 814–824, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Li</author>
<author>K Bontcheva</author>
<author>H Cunningham</author>
</authors>
<title>An SVM Based Learning Algorithm for Information Extraction. Machine Learning Workshop,</title>
<date>2004</date>
<location>Sheffield.</location>
<contexts>
<context position="17093" citStr="Li et al., 2004" startWordPosition="2701" endWordPosition="2704"> representations of the input chunks; these representations contain the internal features of the chunks, as well as the information of 5 tokens to the left of the beginning of the chunk and 5 tokens to the right of the end of the chunk. The transformation from GATE documents into arff Weka files and the mapping from Weka onto the GATE documents, is carried out using specific programs. The classification algorithm used for the iterative learning process is an SVM classifier distributed with the GATE system and tuned to perform chunk learning using the same features as the clustering procedure (Li et al., 2004). This classifier outputs a probability which we use for selecting the best document at step (5) of the iterative procedure. The document selected to start the process is the one with more target strings, and the target string chosen is the next available in textual order. The iterative learning procedure is set to stop when the number of concepts induced reaches the average number of chunks in the corpus. Induced concepts not covering at least 10% of the number of documents are discarded, as are concepts with strings repeated at least 10% of the concept extension. 5.2 Experiments and Results </context>
</contexts>
<marker>Li, Bontcheva, Cunningham, 2004</marker>
<rawString>Y. Li, K. Bontcheva, and H. Cunningham. 2004. An SVM Based Learning Algorithm for Information Extraction. Machine Learning Workshop, Sheffield.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Li</author>
<author>J Jiang</author>
<author>Y Wang</author>
</authors>
<title>Generating Templates of Entity Summaries with an Entity-Aspect Model and Pattern Mining.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<publisher>ACL.</publisher>
<location>Uppsala.</location>
<contexts>
<context position="5501" citStr="Li et al., 2010" startWordPosition="845" endWordPosition="848">ral language processing is how to learn conceptualizations from text in automatic or semi-automatic ways. The availability of redundant data has been used, for example, to discover template-like representations (Barzilay and Lee, 2003) or sentence-level paraphrases which could be used for extraction or generation. Various approaches to concept learning use clustering techniques. (Leung et al., 2011) apply various clustering procedures to learn a small number of slots in three typical information extraction domains, using manually annotated data and fixing the number of concepts to be learnt. (Li et al., 2010) generate templates and extraction patterns for specific entity types (actors, companies, etc.). (Chambers and Jurafsky, 2011) learn the structure of MUC templates from raw data in English, an approach that needs both full parsing and semantic interpretation using WordNet (Fellbaum, 1998) in order to extract verb arguments and measure the similarity betweern verbs. In (Saggion, 2012) an iterative learning procedure is used to discover core domain conceptual information from short summaries in two languages. However, the obtained results were not assessed in a real information extraction scenar</context>
</contexts>
<marker>Li, Jiang, Wang, 2010</marker>
<rawString>P. Li, J. Jiang, and Y. Wang. 2010. Generating Templates of Entity Summaries with an Entity-Aspect Model and Pattern Mining. In Proceedings of ACL, Uppsala. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Maynard</author>
<author>V Tablan</author>
<author>H Cunningham</author>
<author>C Ursu</author>
<author>H Saggion</author>
<author>K Bontcheva</author>
<author>Y Wilks</author>
</authors>
<title>Architectural Elements of Language Engineering Robustness.</title>
<date>2002</date>
<journal>Journal of Natural Language Engineering – Special Issue on Robust Methods in Analysis of Natural Language Data,</journal>
<pages>8--2</pages>
<contexts>
<context position="8599" citStr="Maynard et al., 2002" startWordPosition="1280" endWordPosition="1283">try, DateOfEarthquake, Depth, Duration, Epicentre, Fatalities, Homeless, Injured, Magnitude, OtherPlacesAffected, Province, Region, Survivors, TimeOfEarthquake Terrorist Attack City, Country, DateOfAccident, Fatalities, Injured, Target, Perpetrator, Place, NumberOfVictims, TypeOfAttack Train Accident Cause, DateOfAccident, Destination, NumberOfVictims, Origin, Passenger, Place, Survivors, TypeOfAccident, TypeOfTrain Table 1: Conceptual Information in Summaries 3.1 Text Processing In order to carry out experimentation we adopt the GATE infrastructure for document representation and annotation (Maynard et al., 2002). All documents in the dataset are processed with available natural language processors to compute shallow lingustic information. Documents in English are processed with the ANNIE system, a morphological analyzer, and a noun chunker, all three from GATE. The documents in Spanish are analyzed with TreeTagger (Schmid, 1995), a rule-base noun chunker, and an SVM-based named entity recognition and classification system. 4 Concept Induction Algorithms Two algorithms are used to induce conceptual information in a domain from a set of textual summaries. The algorithms form concepts based on target st</context>
</contexts>
<marker>Maynard, Tablan, Cunningham, Ursu, Saggion, Bontcheva, Wilks, 2002</marker>
<rawString>D. Maynard, V. Tablan, H. Cunningham, C. Ursu, H. Saggion, K. Bontcheva, and Y. Wilks. 2002. Architectural Elements of Language Engineering Robustness. Journal of Natural Language Engineering – Special Issue on Robust Methods in Analysis of Natural Language Data, 8(2/3):257–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris D Paice</author>
<author>Paul A Jones</author>
</authors>
<title>The Identification of Important Concepts in Highly Structured Technical Papers. In</title>
<date>1993</date>
<booktitle>Proc. of the 16th ACM-SIGIR Conference,</booktitle>
<pages>69--78</pages>
<editor>R. Korfhage, E. Rasmussen, and P. Willett, editors,</editor>
<contexts>
<context position="6485" citStr="Paice and Jones, 1993" startWordPosition="996" endWordPosition="999">rbs. In (Saggion, 2012) an iterative learning procedure is used to discover core domain conceptual information from short summaries in two languages. However, the obtained results were not assessed in a real information extraction scenario. There are approaches which do not need any human intervention or sophisticated text processing, but learn based on redundancy of the input dataset and some well grounded linguistic intuitions (Banko and Etzioni, 2008; Etzioni et al., 2004). Related to the work presented here are approaches that aim at generating short stereotypical summaries (DeJong, 1982; Paice and Jones, 1993; Ratnaparkhi, 2000; Saggion and Lapalme, 2002; Konstas and Lapata, 2012). 3 Dataset and Text Processing Steps For the experiments reported here we rely on the CONCISUS corpus1 (Saggion and Szasz, 2012) which is distributed free of charge. It is a corpus of Web summaries in Spanish and English in four different application domains: Aviation Accidents (32 English, 32 Spanish), Earthquakes (44 English, 56 Spanish), Train Accidents (36 English, 43 Spanish), and Terrorist Attacks (42 English, 53 Spanish). The dataset contains original and comparable summary pairs, automatic translations of Spanish</context>
</contexts>
<marker>Paice, Jones, 1993</marker>
<rawString>Chris D. Paice and Paul A. Jones. 1993. The Identification of Important Concepts in Highly Structured Technical Papers. In R. Korfhage, E. Rasmussen, and P. Willett, editors, Proc. of the 16th ACM-SIGIR Conference, pages 69–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Piskorski</author>
<author>R Yangarber</author>
</authors>
<title>Information extraction: Past, present and future.</title>
<date>2013</date>
<booktitle>Multilingual Information Extraction and Summarization, Theory and Applications of Natural Language Processing,</booktitle>
<pages>23--49</pages>
<editor>In Thierry Poibeau, Horacio Saggion, Jakub Piskorski, and Roman Yangarber, editors, Multi-source,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="932" citStr="Piskorski and Yangarber, 2013" startWordPosition="118" endWordPosition="121">nt and compare two unsupervised approaches for inducing the main conceptual information in rather stereotypical summaries in two different languages. We evaluate the two approaches in two different information extraction settings: monolingual and cross-lingual information extraction. The extraction systems are trained on auto-annotated summaries (containing the induced concepts) and evaluated on humanannotated documents. Extraction results are promising, being close in performance to those achieved when the system is trained on human-annotated summaries. 1 Introduction Information Extraction (Piskorski and Yangarber, 2013) and Automatic Text Summarization (Saggion and Poibeau, 2013) are two Natural Language Processing tasks which require domain and language adaptation. For over two decades (Riloff, 1993; Riloff, 1996) the natural language processing community has been interested in automatic or semiautomatic methods which could be used to port systems from one domain or task to another, aiming at reducing at least in part the cost associated with the creation of human annotated datasets. Automatic system adaptation can take different forms: if high &apos;Phis work is partially supported by Ministerio de Economfa y C</context>
</contexts>
<marker>Piskorski, Yangarber, 2013</marker>
<rawString>J. Piskorski and R. Yangarber. 2013. Information extraction: Past, present and future. In Thierry Poibeau, Horacio Saggion, Jakub Piskorski, and Roman Yangarber, editors, Multi-source, Multilingual Information Extraction and Summarization, Theory and Applications of Natural Language Processing, pages 23– 49. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Poon</author>
<author>P Domingos</author>
</authors>
<title>Unsupervised ontology induction from text.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>296--305</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2571" citStr="Poon and Domingos, 2010" startWordPosition="373" endWordPosition="376">dictionaries. If high quality human annotated data is unavailable, a large nonannotated corpus and a bootstrapping procedure can be used to produce annotated data (Ciravegna and Wilks, 2003; Yangarber, 2003). Here, we concentrate on developing and evaluating automatic procedures to learn the main concepts of a domain and at the same time auto-annotate texts so that they become available for training information extraction or text summarization applications. However, it would be naive to think that in the current state of the art we would be able to learn all knowledge from text automatically (Poon and Domingos, 2010; Biemann, 2005; Buitelaar and Magnini, 2005). We therefore here concentrate on learning template-like representations from concise event summaries which should contain the key information of an event. 18 de julio de 1994DateOfAttack . Un atentado contra la sede de la Asociaci´on Mutual Israelita ArgentinaTarget de Buenos AiresPlaceOfAttack causa la muerte de 86NumberOfVictims personas. (18th July 1994. An attack against the headquarters of the Jewish Mutual Association in Buenos Aires, Argentina, kills 86people.) Figure 1: Sample of Human Annotated Summary in Spanish An example of the summari</context>
<context position="30180" citStr="Poon and Domingos, 2010" startWordPosition="4911" endWordPosition="4914">enerated. extraction task, although the best system trained on auto-annotated summaries in Spanish has a big difference with respect to a system trained on humanannotated summaries, in English the differences are slight. We belive that this is due to the differences in performance between the underlying text processing components. Our methods work by grouping together sets of chunks, unlike (Chambers and Jurafsky, 2011), whose approach is centered around verb arguments and clustering, and relies on the availability of considerable amounts of data. Ontology learning approaches such as OntoUSP (Poon and Domingos, 2010) are also clustering-based but focus on learning is-a relations only. Unlike (Leung et al., 2011) whose approach is based on gold-standard humman annotations, we here test the performance of the induction process using automatically computed candidate strings, and we additionally learn the number of concepts automatically. 8 Conclusions and Future Work In this paper we have concentrated on the problem of knowledge induction from text summaries. The approaches we have presented are fully unsupervised and are able to produce reasonable conceptualizations (close to human concepts) without relying</context>
</contexts>
<marker>Poon, Domingos, 2010</marker>
<rawString>H. Poon and P. Domingos. 2010. Unsupervised ontology induction from text. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 296–305, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Trainable methods for surface natural language generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference, NAACL</booktitle>
<pages>194--201</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6504" citStr="Ratnaparkhi, 2000" startWordPosition="1000" endWordPosition="1001"> an iterative learning procedure is used to discover core domain conceptual information from short summaries in two languages. However, the obtained results were not assessed in a real information extraction scenario. There are approaches which do not need any human intervention or sophisticated text processing, but learn based on redundancy of the input dataset and some well grounded linguistic intuitions (Banko and Etzioni, 2008; Etzioni et al., 2004). Related to the work presented here are approaches that aim at generating short stereotypical summaries (DeJong, 1982; Paice and Jones, 1993; Ratnaparkhi, 2000; Saggion and Lapalme, 2002; Konstas and Lapata, 2012). 3 Dataset and Text Processing Steps For the experiments reported here we rely on the CONCISUS corpus1 (Saggion and Szasz, 2012) which is distributed free of charge. It is a corpus of Web summaries in Spanish and English in four different application domains: Aviation Accidents (32 English, 32 Spanish), Earthquakes (44 English, 56 Spanish), Train Accidents (36 English, 43 Spanish), and Terrorist Attacks (42 English, 53 Spanish). The dataset contains original and comparable summary pairs, automatic translations of Spanish summaries into Eng</context>
</contexts>
<marker>Ratnaparkhi, 2000</marker>
<rawString>Adwait Ratnaparkhi. 2000. Trainable methods for surface natural language generation. In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference, NAACL 2000, pages 194–201, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>C Schafer</author>
<author>D Yarowsky</author>
</authors>
<title>Inducing information extraction systems for new languages via cross-language projection.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th international conference on Computational linguistics,</booktitle>
<pages>1--7</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="29015" citStr="Riloff et al., 2002" startWordPosition="4725" endWordPosition="4728">e of automatically computed chunks. We have run a t-test to verify the differences in induction performance when learning with gold and automatic chunks (all condition) and have found statistically significant differences in only one domain out of four in Spanish (Terrorist Attack) and in two domains out of four in English (Aviation Accident and Train Accident) (p = 0.01). The applicability of the induction process, that is, if the auto-annotated data could be used for specific tasks, has been tested in two information extraction experiments. In a cross-lingual information extraction setting (Riloff et al., 2002; Saggion and Szasz, 2011) we have observed that a system trained on automatically computed chunks has a performance close to one trained on concepts induced from gold chunks. No statistically significant differences exist (p = 0.01) between the use of automatic chunks and gold chunks, except for the Train Accident domain in English, where the system trained on fully automatically annotated summaries has a better performance. In a full document information 3Depending on the language and domain, between 50% and 77% of all concepts are generated. extraction task, although the best system trained</context>
</contexts>
<marker>Riloff, Schafer, Yarowsky, 2002</marker>
<rawString>E. Riloff, C. Schafer, and D. Yarowsky. 2002. Inducing information extraction systems for new languages via cross-language projection. In Proceedings of the 19th international conference on Computational linguistics, pages 1–7, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
</authors>
<title>Automatically constructing a dictionary for information extraction tasks.</title>
<date>1993</date>
<booktitle>Proceedings of the Eleventh Annual Conference on Artificial Intelligence,</booktitle>
<pages>811--816</pages>
<contexts>
<context position="1116" citStr="Riloff, 1993" startWordPosition="147" endWordPosition="148">rmation extraction settings: monolingual and cross-lingual information extraction. The extraction systems are trained on auto-annotated summaries (containing the induced concepts) and evaluated on humanannotated documents. Extraction results are promising, being close in performance to those achieved when the system is trained on human-annotated summaries. 1 Introduction Information Extraction (Piskorski and Yangarber, 2013) and Automatic Text Summarization (Saggion and Poibeau, 2013) are two Natural Language Processing tasks which require domain and language adaptation. For over two decades (Riloff, 1993; Riloff, 1996) the natural language processing community has been interested in automatic or semiautomatic methods which could be used to port systems from one domain or task to another, aiming at reducing at least in part the cost associated with the creation of human annotated datasets. Automatic system adaptation can take different forms: if high &apos;Phis work is partially supported by Ministerio de Economfa y Competitividad, Secretarfa de Estado de Investigaci´on, Desarrollo e Innovaci´on, Spain under project number TIN2012- 38584-C06-03 and Advanced Research Fellowship RYC-2009- 04291. We t</context>
</contexts>
<marker>Riloff, 1993</marker>
<rawString>E. Riloff. 1993. Automatically constructing a dictionary for information extraction tasks. Proceedings of the Eleventh Annual Conference on Artificial Intelligence, pages 811–816.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
</authors>
<title>Automatically generating extraction patterns from untagged text.</title>
<date>1996</date>
<booktitle>Proceedings of the Thirteenth Annual Conference on Artificial Intelligence,</booktitle>
<pages>1044--1049</pages>
<contexts>
<context position="1131" citStr="Riloff, 1996" startWordPosition="149" endWordPosition="150">tion settings: monolingual and cross-lingual information extraction. The extraction systems are trained on auto-annotated summaries (containing the induced concepts) and evaluated on humanannotated documents. Extraction results are promising, being close in performance to those achieved when the system is trained on human-annotated summaries. 1 Introduction Information Extraction (Piskorski and Yangarber, 2013) and Automatic Text Summarization (Saggion and Poibeau, 2013) are two Natural Language Processing tasks which require domain and language adaptation. For over two decades (Riloff, 1993; Riloff, 1996) the natural language processing community has been interested in automatic or semiautomatic methods which could be used to port systems from one domain or task to another, aiming at reducing at least in part the cost associated with the creation of human annotated datasets. Automatic system adaptation can take different forms: if high &apos;Phis work is partially supported by Ministerio de Economfa y Competitividad, Secretarfa de Estado de Investigaci´on, Desarrollo e Innovaci´on, Spain under project number TIN2012- 38584-C06-03 and Advanced Research Fellowship RYC-2009- 04291. We thank Biljana Dr</context>
</contexts>
<marker>Riloff, 1996</marker>
<rawString>E. Riloff. 1996. Automatically generating extraction patterns from untagged text. Proceedings of the Thirteenth Annual Conference on Artificial Intelligence, pages 1044–1049.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Saggion</author>
<author>G Lapalme</author>
</authors>
<title>Generating Indicative-Informative Summaries with SumUM. Computational Linguistics.</title>
<date>2002</date>
<contexts>
<context position="6531" citStr="Saggion and Lapalme, 2002" startWordPosition="1002" endWordPosition="1005">ing procedure is used to discover core domain conceptual information from short summaries in two languages. However, the obtained results were not assessed in a real information extraction scenario. There are approaches which do not need any human intervention or sophisticated text processing, but learn based on redundancy of the input dataset and some well grounded linguistic intuitions (Banko and Etzioni, 2008; Etzioni et al., 2004). Related to the work presented here are approaches that aim at generating short stereotypical summaries (DeJong, 1982; Paice and Jones, 1993; Ratnaparkhi, 2000; Saggion and Lapalme, 2002; Konstas and Lapata, 2012). 3 Dataset and Text Processing Steps For the experiments reported here we rely on the CONCISUS corpus1 (Saggion and Szasz, 2012) which is distributed free of charge. It is a corpus of Web summaries in Spanish and English in four different application domains: Aviation Accidents (32 English, 32 Spanish), Earthquakes (44 English, 56 Spanish), Train Accidents (36 English, 43 Spanish), and Terrorist Attacks (42 English, 53 Spanish). The dataset contains original and comparable summary pairs, automatic translations of Spanish summaries into English, automatic translation</context>
</contexts>
<marker>Saggion, Lapalme, 2002</marker>
<rawString>H. Saggion and G. Lapalme. 2002. Generating Indicative-Informative Summaries with SumUM. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Saggion</author>
<author>T Poibeau</author>
</authors>
<title>Automatic text summarization: Past, present and future.</title>
<date>2013</date>
<booktitle>Multilingual Information Extraction and Summarization, Theory and Applications of Natural Language Processing,</booktitle>
<pages>3--21</pages>
<editor>In Thierry Poibeau, Horacio Saggion, Jakub Piskorski, and Roman Yangarber, editors, Multi-source,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="993" citStr="Saggion and Poibeau, 2013" startWordPosition="126" endWordPosition="129">conceptual information in rather stereotypical summaries in two different languages. We evaluate the two approaches in two different information extraction settings: monolingual and cross-lingual information extraction. The extraction systems are trained on auto-annotated summaries (containing the induced concepts) and evaluated on humanannotated documents. Extraction results are promising, being close in performance to those achieved when the system is trained on human-annotated summaries. 1 Introduction Information Extraction (Piskorski and Yangarber, 2013) and Automatic Text Summarization (Saggion and Poibeau, 2013) are two Natural Language Processing tasks which require domain and language adaptation. For over two decades (Riloff, 1993; Riloff, 1996) the natural language processing community has been interested in automatic or semiautomatic methods which could be used to port systems from one domain or task to another, aiming at reducing at least in part the cost associated with the creation of human annotated datasets. Automatic system adaptation can take different forms: if high &apos;Phis work is partially supported by Ministerio de Economfa y Competitividad, Secretarfa de Estado de Investigaci´on, Desarr</context>
</contexts>
<marker>Saggion, Poibeau, 2013</marker>
<rawString>H. Saggion and T. Poibeau. 2013. Automatic text summarization: Past, present and future. In Thierry Poibeau, Horacio Saggion, Jakub Piskorski, and Roman Yangarber, editors, Multi-source, Multilingual Information Extraction and Summarization, Theory and Applications of Natural Language Processing, pages 3–21. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Saggion</author>
<author>S Szasz</author>
</authors>
<title>Multi-domain Crosslingual Information Extraction from Clean and Noisy Texts.</title>
<date>2011</date>
<booktitle>In Proceedings of the 8th Brazilian Symposium in Information and Human Language Technology,</booktitle>
<location>Cuiab´a, Brazil. BCS.</location>
<contexts>
<context position="29041" citStr="Saggion and Szasz, 2011" startWordPosition="4729" endWordPosition="4732">mputed chunks. We have run a t-test to verify the differences in induction performance when learning with gold and automatic chunks (all condition) and have found statistically significant differences in only one domain out of four in Spanish (Terrorist Attack) and in two domains out of four in English (Aviation Accident and Train Accident) (p = 0.01). The applicability of the induction process, that is, if the auto-annotated data could be used for specific tasks, has been tested in two information extraction experiments. In a cross-lingual information extraction setting (Riloff et al., 2002; Saggion and Szasz, 2011) we have observed that a system trained on automatically computed chunks has a performance close to one trained on concepts induced from gold chunks. No statistically significant differences exist (p = 0.01) between the use of automatic chunks and gold chunks, except for the Train Accident domain in English, where the system trained on fully automatically annotated summaries has a better performance. In a full document information 3Depending on the language and domain, between 50% and 77% of all concepts are generated. extraction task, although the best system trained on auto-annotated summari</context>
</contexts>
<marker>Saggion, Szasz, 2011</marker>
<rawString>H. Saggion and S. Szasz. 2011. Multi-domain Crosslingual Information Extraction from Clean and Noisy Texts. In Proceedings of the 8th Brazilian Symposium in Information and Human Language Technology, Cuiab´a, Brazil. BCS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Saggion</author>
<author>S Szasz</author>
</authors>
<title>The CONCISUS Corpus of Event Summaries.</title>
<date>2012</date>
<booktitle>In Proceedings of the 8th Language Resources and Evaluation Conference (LREC),</booktitle>
<location>Istanbul, Turkey. ELDA.</location>
<contexts>
<context position="6687" citStr="Saggion and Szasz, 2012" startWordPosition="1027" endWordPosition="1030">n a real information extraction scenario. There are approaches which do not need any human intervention or sophisticated text processing, but learn based on redundancy of the input dataset and some well grounded linguistic intuitions (Banko and Etzioni, 2008; Etzioni et al., 2004). Related to the work presented here are approaches that aim at generating short stereotypical summaries (DeJong, 1982; Paice and Jones, 1993; Ratnaparkhi, 2000; Saggion and Lapalme, 2002; Konstas and Lapata, 2012). 3 Dataset and Text Processing Steps For the experiments reported here we rely on the CONCISUS corpus1 (Saggion and Szasz, 2012) which is distributed free of charge. It is a corpus of Web summaries in Spanish and English in four different application domains: Aviation Accidents (32 English, 32 Spanish), Earthquakes (44 English, 56 Spanish), Train Accidents (36 English, 43 Spanish), and Terrorist Attacks (42 English, 53 Spanish). The dataset contains original and comparable summary pairs, automatic translations of Spanish summaries into English, automatic translation of English summaries into Spanish, and associated original full documents in Spanish and English for two of the domains (Aviation Accidents and Earthquakes</context>
</contexts>
<marker>Saggion, Szasz, 2012</marker>
<rawString>H. Saggion and S. Szasz. 2012. The CONCISUS Corpus of Event Summaries. In Proceedings of the 8th Language Resources and Evaluation Conference (LREC), Istanbul, Turkey. ELDA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Saggion</author>
</authors>
<title>Unsupervised content discovery from concise summaries.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, AKBCWEKEX ’12,</booktitle>
<pages>13--18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5887" citStr="Saggion, 2012" startWordPosition="905" endWordPosition="906"> et al., 2011) apply various clustering procedures to learn a small number of slots in three typical information extraction domains, using manually annotated data and fixing the number of concepts to be learnt. (Li et al., 2010) generate templates and extraction patterns for specific entity types (actors, companies, etc.). (Chambers and Jurafsky, 2011) learn the structure of MUC templates from raw data in English, an approach that needs both full parsing and semantic interpretation using WordNet (Fellbaum, 1998) in order to extract verb arguments and measure the similarity betweern verbs. In (Saggion, 2012) an iterative learning procedure is used to discover core domain conceptual information from short summaries in two languages. However, the obtained results were not assessed in a real information extraction scenario. There are approaches which do not need any human intervention or sophisticated text processing, but learn based on redundancy of the input dataset and some well grounded linguistic intuitions (Banko and Etzioni, 2008; Etzioni et al., 2004). Related to the work presented here are approaches that aim at generating short stereotypical summaries (DeJong, 1982; Paice and Jones, 1993; </context>
<context position="10058" citStr="Saggion, 2012" startWordPosition="1514" endWordPosition="1515">d on iterative learning. 4.1 Clustering-based Induction The procedure for learning conceptual information by clustering is straithforward: the chunks in the set of summaries are represented as instances considering both internal and sourrounding linguistic information. These instances are the input to a clustering procedure which returns a list of clusters each containing a set of chunks. We consider each cluster as a key concept in the set of domain summaries and the chunks in each cluster as the concept extension. 4.2 Iterative Induction We use the iterative learning algorithm described in (Saggion, 2012) which learns from a set of summaries S, also annotated with target strings (e.g. chunks) and shallow linguistic information. In a nutshell the algorithm is as follows: (1) Choose a document D from the set of summaries S and add it to a training set TRAIN. Set REST to S − TRAIN. (2) Choose an available target concept T from D, i.e. a target concept not tried before by the algorithm. (3) Train a classifier on TRAIN to learn instances of the target concept using the available linguistic features; the classifier uses the linguistic information provided. (4) Apply the classifier to REST (all summa</context>
</contexts>
<marker>Saggion, 2012</marker>
<rawString>H. Saggion. 2012. Unsupervised content discovery from concise summaries. In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, AKBCWEKEX ’12, pages 13–18, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Improvements In Part-of-Speech Tagging With an Application To German. In</title>
<date>1995</date>
<booktitle>In Proceedings of the ACL SIGDAT-Workshop,</booktitle>
<pages>47--50</pages>
<contexts>
<context position="8922" citStr="Schmid, 1995" startWordPosition="1334" endWordPosition="1335">ation, NumberOfVictims, Origin, Passenger, Place, Survivors, TypeOfAccident, TypeOfTrain Table 1: Conceptual Information in Summaries 3.1 Text Processing In order to carry out experimentation we adopt the GATE infrastructure for document representation and annotation (Maynard et al., 2002). All documents in the dataset are processed with available natural language processors to compute shallow lingustic information. Documents in English are processed with the ANNIE system, a morphological analyzer, and a noun chunker, all three from GATE. The documents in Spanish are analyzed with TreeTagger (Schmid, 1995), a rule-base noun chunker, and an SVM-based named entity recognition and classification system. 4 Concept Induction Algorithms Two algorithms are used to induce conceptual information in a domain from a set of textual summaries. The algorithms form concepts based on target strings (or chunks) in the set of summaries using token-level linguistic information. The chunks are represented with different features which are explained later in Section 5.1. One algorithm we use is based on clustering, while the other is based on iterative learning. 4.1 Clustering-based Induction The procedure for lear</context>
</contexts>
<marker>Schmid, 1995</marker>
<rawString>H. Schmid. 1995. Improvements In Part-of-Speech Tagging With an Application To German. In In Proceedings of the ACL SIGDAT-Workshop, pages 47–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>E Frank</author>
</authors>
<date>1999</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations.</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="16315" citStr="Witten and Frank, 1999" startWordPosition="2565" endWordPosition="2568">ization 273 Spanish Iterative Clustering P R F P R F Terrorist Attack 0.25 0.59 0.35 0.59 0.59 0.591 Aviation Accident 0.50 0.62 0.55 0.66 0.66 0.661 Earthquake 0.34 0.51 0.41 0.56 0.53 0.551 Train Accident 0.41 0.69 0.52 0.58 0.58 0.58 English Iterative Clustering P R F P R F Terrorist Attack 0.23 0.39 0.29 0.50 0.50 0.501 Aviation Accident 0.57 0.68 0.62 0.79 0.79 0.791 Earthquake 0.26 0.53 0.34 0.39 0.39 0.39 Train Accident 0.50 0.59 0.54 0.61 0.61 0.611 Table 3: Conceptual induction (Spanish and English) Using Gold Chunks for Learning implementation from the Weka machine learning library (Witten and Frank, 1999). We instruct the algorithm to decide on the number of clusters based on the data, instead of setting the number of clusters by hand. The instances to cluster are representations of the input chunks; these representations contain the internal features of the chunks, as well as the information of 5 tokens to the left of the beginning of the chunk and 5 tokens to the right of the end of the chunk. The transformation from GATE documents into arff Weka files and the mapping from Weka onto the GATE documents, is carried out using specific programs. The classification algorithm used for the iterativ</context>
</contexts>
<marker>Witten, Frank, 1999</marker>
<rawString>I. H. Witten and E. Frank. 1999. Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Yangarber</author>
</authors>
<title>Counter-Training in Discovery of Semantic Patterns.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL’03).</booktitle>
<contexts>
<context position="2155" citStr="Yangarber, 2003" startWordPosition="304" endWordPosition="305">ividad, Secretarfa de Estado de Investigaci´on, Desarrollo e Innovaci´on, Spain under project number TIN2012- 38584-C06-03 and Advanced Research Fellowship RYC-2009- 04291. We thank Biljana Drndarevi´c for proofreading the paper. quality human annotated data is available, then rulebased or statistical systems can be trained on this data (Brill, 1994), reducing the efforts of writing rules and handcrafting dictionaries. If high quality human annotated data is unavailable, a large nonannotated corpus and a bootstrapping procedure can be used to produce annotated data (Ciravegna and Wilks, 2003; Yangarber, 2003). Here, we concentrate on developing and evaluating automatic procedures to learn the main concepts of a domain and at the same time auto-annotate texts so that they become available for training information extraction or text summarization applications. However, it would be naive to think that in the current state of the art we would be able to learn all knowledge from text automatically (Poon and Domingos, 2010; Biemann, 2005; Buitelaar and Magnini, 2005). We therefore here concentrate on learning template-like representations from concise event summaries which should contain the key informa</context>
</contexts>
<marker>Yangarber, 2003</marker>
<rawString>R. Yangarber. 2003. Counter-Training in Discovery of Semantic Patterns. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL’03).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>