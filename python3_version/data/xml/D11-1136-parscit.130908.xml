<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003131">
<title confidence="0.9860805">
Closing the Loop: Fast, Interactive Semi-Supervised Annotation
With Queries on Features and Instances
</title>
<author confidence="0.994235">
Burr Settles
</author>
<affiliation confidence="0.822512">
Machine Learning Department
Carnegie Mellon University
Pittsburgh, PA 15213 USA
</affiliation>
<email confidence="0.998986">
bsettles@cs.cmu.edu
</email>
<sectionHeader confidence="0.995642" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999816571428571">
This paper describes DUALIST, an active
learning annotation paradigm which solicits
and learns from labels on both features (e.g.,
words) and instances (e.g., documents). We
present a novel semi-supervised training al-
gorithm developed for this setting, which is
(1) fast enough to support real-time interac-
tive speeds, and (2) at least as accurate as pre-
existing methods for learning with mixed fea-
ture and instance labels. Human annotators in
user studies were able to produce near-state-
of-the-art classifiers—on several corpora in a
variety of application domains—with only a
few minutes of effort.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9860439375">
In active learning, a classifier participates in its own
training process by posing queries, such as request-
ing labels for documents in a text classification task.
The goal is to maximize the accuracy of the trained
system in the most economically efficient way. This
paradigm is well-motivated for natural language ap-
plications, where unlabeled data may be readily
available (e.g., text on the Internet), but the anno-
tation process can be slow and expensive.
Nearly all previous work in active learning, how-
ever, has focused on selecting queries from the
learner’s perspective. For example, experiments are
often run in simulation rather than with user stud-
ies, and results are routinely evaluated in terms of
training set size rather than human annotation time
or labor costs (which are more reasonable measures
of labeling effort). Many state-of-the-art algorithms
are also too slow to run or too tedious to implement
to be useful for real-time interaction with human an-
notators, and few analyses have taken these factors
into account. Furthermore, there is very little work
on actively soliciting domain knowledge from hu-
mans (e.g., information about features) and incorpo-
rating this into the learning process.
While selecting good queries is clearly important,
if our goal is to reduce actual annotation effort these
human factors must be taken into account. In this
work, we propose a new interactive annotation inter-
face which addresses some of these issues; in partic-
ular it has the ability to pose queries on both features
(e.g., words) and instances (e.g., documents). We
present a novel semi-supervised learning algorithm
that is fast, flexible, and accurate enough to support
these interface design constraints interactively.
2 DUALIST: Utility for Active Learning
with Instances and Semantic Terms
Figure 1 shows a screenshot of the DUALIST an-
notation tool, which is freely available as an open-
source software project1. On the left panel, users
are presented with unlabeled documents: in this case
Usenet messages that belong to one of two sports-
related topics: baseball and hockey. Users may label
documents by clicking on the class buttons listed be-
low each text. In cases of extreme ambiguity, users
may ignore a document by clicking the “X” to re-
move it from the pool of possible queries.
On the right panel, users are given a list of fea-
ture queries organized into columns by class label.
</bodyText>
<footnote confidence="0.987766">
1http://code.google.com/p/dualist/
</footnote>
<page confidence="0.859314">
1467
</page>
<note confidence="0.981639">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1467–1478,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999887">
Figure 1: A screenshot of DUALIST.
</figureCaption>
<bodyText confidence="0.99999219047619">
The rationale for these columns is that they should
reduce cognitive load (i.e., once a user is in the base-
ball mindset, s/he can simply go down the list, label-
ing features in context: “plate,” “pitcher,” “bases,”
etc.). Within each column, words are sorted by how
informative they are the to classifier, and users may
click on words to label them. Each column also con-
tains a text box, where users may “inject” domain
knowledge by typing in arbitrary words (whether
they appear in any of the columns or not). The list
of previously labeled words appears at the bottom of
each list (highlighted), and can be unlabeled at any
time, if users later feel they made any errors.
Finally, a large submit button is located at the top
of the screen, which users must click to re-train the
classifier and receive a new set of queries. The learn-
ing algorithm is actually fast enough to do this au-
tomatically after each labeling action. However, we
found such a dynamically changing interface to be
frustrating for users (e.g., words they wanted to la-
bel would move or disappear).
</bodyText>
<subsectionHeader confidence="0.7865095">
2.1 A Generative Model for Learning from
Feature and Instance Labels
</subsectionHeader>
<bodyText confidence="0.99925445">
For the underlying model in this system, we use
multinomial naive Bayes (MNB) since it is sim-
ple, fast, and known to work well for several nat-
ural language applications—text classification in
particular—despite its simplistic and often violated
independence assumptions (McCallum and Nigam,
1998; Rennie et al., 2003).
MNB models the distribution of features as a
multinomial: documents are sequences of words,
with the “naive” assumption that words in each
position are generated independently. Each docu-
ment is treated as a mixture of classes, which have
their own multinomial distributions over words. Let
the model be parameterized by the vector θ, with
θj = P(yj) denoting the probability of class yj, and
θjk = P(fk|yj) denoting the probability of generat-
ing word fk given class yj. Note that for class pri-
ors Ej θj = 1, and for per-class word multinomials
Ek θjk = 1. The likelihood of document x being
generated by class yj is given by:
</bodyText>
<equation confidence="0.985836">
Pθ(x|yj) = P (|x|) rl (θjk)fk(x),
k
</equation>
<bodyText confidence="0.997866952380952">
where fk(x) is the frequency count of word fk in
document x. If we assume P(|x|) is distributed in-
dependently of class, and since document length |x|
is fixed, we can drop the first term for classification
purposes. Then, we can use Bayes’ rule to calculate
the posterior probability under the model of a label,
given the input document for classification:
(1)
where Z(x) is shorthand for a normalization con-
stant, summing over all possible class labels.
The task of training such a classifier involves es-
timating the parameters in θ, given a set of labeled
instances L = {hx(l), y(l)i}Ll=1. To do this, we use
a Dirichlet prior and take the expectation of each
parameter with respect to the posterior, which is a
simple way to estimate a multinomial (Heckerman,
1995). In other words, we count the fraction of times
the word fk occurs in the labeled set among doc-
uments of class yj, and the prior adds mjk “hallu-
cinated” occurrences for a smoothed version of the
maximum likelihood estimate:
</bodyText>
<equation confidence="0.988833">
θjk = mjk + Ei P(yj|x(i))fk(x(i))
Z(fk)
Here, mjk is the prior for word fk under class yj,
P(yj|x(i)) ∈ {0, 1} indicates the true labeling of the
</equation>
<bodyText confidence="0.973082">
ith document in the training set, and Z(fk) is a nor-
malization constant summing over all words in the
vocabulary. Typically, a uniform prior is used, such
as the Laplacian (a value of 1 for all mjk). Class pa-
rameters θj are estimated a similar way, by counting
</bodyText>
<equation confidence="0.9998335">
Pθ(yj)Pθ(x |yj) _ θj Hk(θjk)fk(x)
Pθ(yj|x) = —
,
P Z(
θ(x) x)
. (2)
</equation>
<page confidence="0.924627">
1468
</page>
<bodyText confidence="0.999965666666667">
the fraction of documents that are labeled with that
class, subject to a prior mj. This prior is important
in the event that no documents are yet labeled with
yj, which can be quite common early on in the active
learning process.
Recall that our scenario lets human annotators
provide not only document labels, but feature labels
as well. To make use of this additional information,
we assume that labeling the word fk with a class yj
increases the probability P (fk|yj) of the word ap-
pearing in documents of that class. The natural in-
terpretation of this under our model is to increase the
prior mjk for the corresponding multinomial. To do
this we introduce a new parameter α, and define the
elements of the Dirichlet prior as follows:
</bodyText>
<equation confidence="0.8424065">
_ ( 1 + α if fk is labeled with yj,
mjk Sl 1 otherwise.
</equation>
<bodyText confidence="0.997945782608696">
This approach is extremely flexible, and offers three
particular advantages over the previous “pooling
multinomials” approach for incorporating feature la-
bels into MNB (Melville et al., 2009). The pooling
multinomials algorithm averages together two sets
of 0jk parameters: one that is estimated from labeled
data, and another derived from feature labels under
the assumption of a boolean output variable (treating
labeled features are “polarizing” factors). Therefore,
pooling multinomials can only be applied to binary
classification tasks, while our method works equally
well for problems with multiple classes. The second
advantage is that feature labels need not be mutu-
ally exclusive, so the word “score” could be labeled
with both baseball and hockey, if necessary (e.g.,
if the task also includes several non-sports labels).
Finally, our framework allows users to conceivably
provide feature-specific priors αjk to, for example,
imply that the word “inning” is a stronger indicator
for baseball than the word “score” (which is a more
general sports term). However, we leave this aspect
for future work and employ the fixed-α approach as
described above in this study.
</bodyText>
<subsectionHeader confidence="0.999551">
2.2 Exploiting Unlabeled Data
</subsectionHeader>
<bodyText confidence="0.999722513513513">
In addition to document and feature labels, we usu-
ally have access to a large unlabeled corpus. In fact,
these texts form the pool of possible instance queries
in active learning. We can take advantage of this ad-
ditional data in generative models like MNB by em-
ploying the Expectation-Maximization (EM) algo-
rithm. Combining EM with pool-based active learn-
ing was previously studied in the context of instance
labeling (McCallum and Nigam, 1998), and we ex-
tend the method to our interactive scenario, which
supports feature labeling as well.
First, we estimate initial parameters 0&apos; as in Sec-
tion 2.1, but using only the priors (and no instances).
Then, we apply the induced classifier on the unla-
beled pool U = {x(u)}U u=1 (Eq. 1). This is the “E”
step of EM. Next we re-estimate feature multino-
mials 0jk, using both labeled instances from L and
probabilistically-labeled instances from U (Eq. 2).
In other words, P(yj|x) ∈ {0, 1} for x ∈ L, and
P(yj|x) = Pθi(yj|x) for x ∈ U. We also weight
the data in U by a factor of 0.1, so as not to over-
whelm the training signal coming from true instance
labels in L. Class parameters 0j are re-estimated in
the analogous fashion. This is the “M” step.
For speed and interactivity, we actually stop train-
ing after this first iteration. When feature labels are
available, we found that EM generally converges in
four to 10 iterations, requiring more training time
but rarely improving accuracy (the largest gains con-
sistently come in the first iteration). Also, we ignore
labeled data in the initial estimation of 0&apos; because L
is too small early in active learning to yield good re-
sults with EM. Perhaps this can be improved by us-
ing an ensemble (McCallum and Nigam, 1998), but
that comes at further computational expense. Fea-
ture labels, on the other hand, seem generally more
reliable for probabilistically labeling U.
</bodyText>
<subsectionHeader confidence="0.999974">
2.3 Selecting Instance and Feature Queries
</subsectionHeader>
<bodyText confidence="0.999143357142857">
The final algorithmic component to our system is
the selection of informative queries (i.e., unlabeled
words and documents) to present to the annotator.
Querying instances is the traditional mode of ac-
tive learning, and is well-studied in the literature;
see Settles (2009) for a review. In this work we use
entropy-based uncertainty sampling, which ranks all
instances in U by the posterior class entropy under
the model Hθ(Y |x) = −Ej Pθ(yj|x)log Pθ(yj|x),
and asks the user to label the top D unlabeled doc-
uments. This simple heuristic is an approximation
to querying the instance with the maximum infor-
mation gain (since the class entropy, once labeled,
is zero), under the assumption that each x is repre-
</bodyText>
<page confidence="0.992916">
1469
</page>
<bodyText confidence="0.999977705882353">
sentative of the underlying natural data distribution.
Moreover, it is extremely fast to compute, which is
important for our interactive environment.
Querying features, though, is a newer idea with
significantly less research behind it. Previous work
has either assumed that (1) features are not assigned
to classes, but instead flagged for “relevance” to the
task (Godbole et al., 2004; Raghavan et al., 2006),
or (2) feature queries are posed just like instance
queries: a word is presented to the annotator, who
must choose among the labels (Druck et al., 2009;
Attenberg et al., 2010). Recall from Figure 1 that
we want to organize feature queries into columns by
class label. This means our active learner must pro-
duce queries that are class-specific.
To select these feature queries, we first rank ele-
ments in the vocabulary by information gain (IG):
</bodyText>
<equation confidence="0.9916695">
P (Ik, yj)
P(Ik, yj)log P(Ik)P(yj),
</equation>
<bodyText confidence="0.999991352941176">
where Ik E {0, 1} is a variable indicating the pres-
ence or absence of a feature. This is essentially
the common feature-selection method for identify-
ing the most salient features in text classification
(Sebastiani, 2002). However, we use both L and
probabilistically-labeled instances from U to com-
pute IG(fk), to better reflect what the model be-
lieves it has learned. To organize queries into
classes, we take the top V unlabeled features and
pose fk for the class yj with which it occurs most
frequently, as well as any other class with which it
occurs at least 75% as often. Intuitively, this ap-
proach (1) queries features that the model believes
are most informative, and (2) automatically identi-
fies classes that seem most correlated. To our knowl-
edge, DUALIST is the first active learning environ-
ment with both of these properties.
</bodyText>
<sectionHeader confidence="0.999501" genericHeader="introduction">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999930892857143">
We conduct four sets of experiments to evaluate our
approach. The first two are “offline” experiments,
designed to better understand (1) how our training
algorithm compares to existing methods for feature-
label learning, and (2) the effects of tuning the α
parameter. The other experiments are user studies
designed to empirically gauge how well human an-
notators make use of DUALIST in practice.
We use a variety of benchmark corpora in the fol-
lowing evaluations. Reuters (Rose et al., 2002) is
a collection of news articles organized into topics,
such as acquisitions, corn, earnings, etc. As in pre-
vious work (Raghavan et al., 2006) we use the 10
most frequent topics, but further process the cor-
pus by removing ambiguous documents (i.e., that
belong to multiple topics) so that all articles have
a unique label, resulting in a corpus of 9,002 arti-
cles. WebKB (Craven et al., 1998) consists of 4,199
university web pages of four types: course, faculty,
project, and student. 20 Newsgroups (Lang, 1995)
is a set of 18,828 Usenet messages from 20 different
online discussion groups. For certain experiments
(such as the one shown in Figure 1), we also use
topical subsets. Movie Reviews (Pang et al., 2002)
is a set of 2,000 online movie reviews categorized as
positive or negative in sentiment. All data sets were
processed using lowercased unigram features, with
punctuation and common stop-words removed.
</bodyText>
<subsectionHeader confidence="0.999953">
3.1 Comparison of Learning Algorithms
</subsectionHeader>
<bodyText confidence="0.999977916666667">
An important question is how well our learning al-
gorithm, “MNB/Priors,” performs relative to exist-
ing baseline methods for learning with labeled fea-
tures. We compare against two such approaches
from the literature. “MaxEnt/GE” is a maximum en-
tropy classifier trained using generalized expectation
(GE) criteria (Druck et al., 2008), which are con-
straints used in training discriminative linear mod-
els. For labeled features, these take the form of ex-
pected “reference distributions” conditioned on the
presence of the feature (e.g., 95% of documents con-
taining the word “inning” should be labeled base-
ball). For each constraint, a term is added to the
objective function to encourage parameter settings
that yield predictions conforming to the reference
distribution on unlabeled instances. “MNB/Pool” is
naive Bayes trained using the pooling multinomials
approach (Melville et al., 2009) mentioned in Sec-
tion 2.1. We also expand upon MNB/Pool using an
EM variant to make it semi-supervised.
We use the implementation of GE training from
the open-source MALLET toolkit2, and implement
both MNB variants in the same data-processing
pipeline. Because the GE implementation available
</bodyText>
<equation confidence="0.807861666666667">
2http://mallet.cs.umass.edu
IG(fk) = � �
Ik j
</equation>
<page confidence="0.950067">
1470
</page>
<table confidence="0.999437888888889">
Corpus MaxEnt/GE MNB/Pool Pool+EM1 MNB/Priors Priors+EM1
Reuters 82.8 (22.9) – – – – 83.7 (&lt;0.1) 86.6 (0.3)
WebKB 22.2 (4.9) – – – – 67.5 (&lt;0.1) 67.8 (0.1)
20 Newsgroups 49.7 (326.6) – – – – 50.1 (0.2) 70.7 (6.9)
Science 86.9 (5.7) – – – – 71.4 (&lt;0.1) 92.8 (0.1)
Autos/Motorcycles 90.8 (0.8) 90.1 (&lt;0.1) 97.5 (&lt;0.1) 89.9 (&lt;0.1) 97.6 (&lt;0.1)
Baseball/Hockey 49.9 (0.8) 90.7 (&lt;0.1) 96.7 (&lt;0.1) 90.5 (&lt;0.1) 96.9 (&lt;0.1)
Mac/PC 50.5 (0.6) 86.7 (&lt;0.1) 91.2 (&lt;0.1) 86.6 (&lt;0.1) 90.2 (&lt;0.1)
Movie Reviews 68.8 (1.8) 68.0 (&lt;0.1) 73.4 (0.1) 67.7 (&lt;0.1) 72.0 (0.1)
</table>
<tableCaption confidence="0.9634065">
Table 1: Accuracies and training times for different feature-label learning algorithms on benchmark corpora. Classi-
fication accuracy is reported for each model, using only the top 10 oracle-ranked features per label (and no labeled
instances) for training. The best model for each corpus is highlighted in bold. Training time (in seconds) is shown in
parentheses on the right side of each column. All results are averaged across 10 folds using cross-validation.
</tableCaption>
<bodyText confidence="0.999838592592593">
to us only supports labeled features (and not labeled
instances as well), we limit the MNB methods to
features for a fair comparison. To obtain feature la-
bels in this experiment, we simulate a “feature or-
acle” as in previous work (Druck et al., 2008; At-
tenberg et al., 2010), which is essentially the query
selection algorithm from Section 2.3, but using com-
plete labeled data to compute IG(fk). We con-
servatively use only the top 10 features per class,
which is meant to resemble a handful of very salient
features that a human might brainstorm to jump-
start the learning process. We experiment with
EM1 (one-step EM) variants of both MNB/Pool
and MNB/Priors, and set α = 50 for the latter
(see Section 3.2 for details on tuning this parame-
ter). Results are averaged over 10 folds using cross-
validation, and all experiments are conducted on a
single 2.53GHz processor machine.
Results are shown in Table 1. As expected, adding
one iteration of EM for semi-supervised training im-
proves the accuracy of both MNB methods across all
data sets. These improvements come without signif-
icant overhead in terms of time: training still rou-
tinely finishes in a fraction of a second per fold.
MNB/Pool and MNB/Priors, where they can be
compared, perform virtually the same as each other
with or without EM, in terms of accuracy and speed
alike. However, MNB/Pool is only applicable to bi-
nary classification problems. As explained in Sec-
tion 2.1, MNB/Priors is more flexible, and prefer-
able for a more general-use interactive annotation
tool like DUALIST.
The semi-supervised MNB methods are also con-
sistently more accurate than GE training—and are
about 40 times faster as well. The gains of
Priors+EM1 over MaxEnt/GE are statistically sig-
nificant in all cases but two: Autos/Motorcycles and
Movie Reviews3. MNB is superior when using any-
where from five to 20 oracle-ranked features per
class, but as the number of feature labels increases
beyond 30, GE is often more accurate (results not
shown). If we think of MaxEnt/GE as a discrim-
inative analog of MNB/Priors+EM, this is consis-
tent with what is known about labeled set size in su-
pervised learning for generative/discriminative pairs
(Ng and Jordan, 2002). However, the time complex-
ity of GE training increases sharply with each new
labeled feature, since it adds a new constraint to the
objective function whose gradient must be computed
using all the unlabeled data. In short, GE train-
ing is too slow and too inaccurate early in the ac-
tive learning process (where labels are more scarce)
to be appropriate for our scenario. Thus, we select
MNB/Priors to power the DUALIST interface.
</bodyText>
<subsectionHeader confidence="0.999842">
3.2 Tuning the Parameter α
</subsectionHeader>
<bodyText confidence="0.980754666666667">
A second question is how sensitive the accuracy of
MNB/Priors is to the parameter α. To study this,
we ran experiments varying α from from one to 212,
using different combinations of labeled instances
and/or features (again using the simulated oracle and
10-fold cross-validation).
</bodyText>
<footnote confidence="0.983058">
3Paired 2-tailed t-test, p &lt; 0.05, correcting for multiple tests
using the Bonferroni method.
</footnote>
<page confidence="0.983404">
1471
</page>
<figureCaption confidence="0.992206">
Figure 2: The effects of varying α on accuracy for four
</figureCaption>
<bodyText confidence="0.965978157894737">
corpora, using differing amounts of training data (labeled
features and/or instances). For clarity, vertical axes are
scaled differently for each data set, and horizontal axes
are plotted on a logarithmic scale. Classifier performance
remains generally stable across data sets for α &lt; 100.
Figure 2 plots these results for four of the corpora.
The first thing to note is that in all cases, accuracy is
relatively stable for α &lt; 100, so tuning this value
seems not to be a significant concern; we chose 50
for all other experiments in this paper. A second ob-
servation is that, for all but the Reuters corpus, label-
ing 90 additional features improves accuracy much
more than labeling 100 documents. This is encour-
aging, since labeling features (e.g., words) is known
to be generally faster and easier for humans than la-
beling entire instances (e.g., documents).
For Reuters, however, the additional feature la-
bels appear harmful. The anomaly can be explained
in part by previous work with this corpus, which
found that a few expertly-chosen keywords can
outperform machine learning methods (Cohen and
Singer, 1996), or that aggressive feature selection—
i.e., using only three or four features per class—
helps tremendously (Moulinier, 1996). Corpora like
Reuters may naturally lend themselves to feature se-
lection, which is (in some sense) what happens when
labeling features. The simulated oracle here was
forced to label 100 features, some with very low
information gain (e.g., “south” for acquisitions, or
“proven” for gold); we would not expect humans an-
notators to provide such misleading information. In-
stead, we hypothesize that in practice there may be a
limited set of features with high enough information
content for humans to feel confident labeling, after
which they switch their attention to labeling instance
queries instead. This further indicates that the user-
guided flexibility of annotation in DUALIST is an
appropriate design choice.
</bodyText>
<subsectionHeader confidence="0.998175">
3.3 User Experiments
</subsectionHeader>
<bodyText confidence="0.997198189189189">
To evaluate our system in practice, we conducted
a series of user experiments. This is in contrast to
most previous work, which simulates active learning
by using known document labels and feature labels
from a simulated oracle (which can be flawed, as we
saw in the previous section). We argue that this is an
important contribution, as it gives us a better sense
of how well the approach actually works in practice.
It also allows us to analyze behavioral results, which
in turn may help inform future protocols for human
interaction in active learning.
DUALIST is implemented as a web-based appli-
cation in Java and was deployed online. We used
three different configurations: active dual (as in Fig-
ure 1, implementing everything from Section 2), ac-
tive instance (instance queries only, no features), and
a passive instance baseline (instances only, but se-
lected at random). We also began by randomly se-
lecting instances in the active configurations, until
every class has at least one labeled instance or one
labeled feature. D = 2 documents and V = 100 fea-
tures were selected for each round of active learning.
We recruited five members of our research group
to label three data sets using each configuration, in
an order of their choosing. Users were first allowed
to spend a minute or two familiarizing themselves
with DUALIST, but received no training regarding
the interface or data sets. All experiments used a
fixed 90% train, 10% test split which was consistent
across all users, and annotators were not allowed to
see the accuracy of the classifier they were train-
ing at any time. Each annotation action was times-
tamped and logged for analysis, and each experi-
ment automatically terminated after six minutes.
Figure 3 shows learning curves, in terms of accu-
racy vs. annotation time, for each trial in the user
study. The first thing to note is that the active
</bodyText>
<figure confidence="0.976596903225806">
Reuters
10 feat
10 feat, 100 inst
100 feat
100 feat, 100 inst
1 10 100 1000
10 feat
10 feat, 100 inst
100 feat
100 feat, 100 inst
Science
1 10 100 1000
0.74
WebKB
0.72
0.7
0.68
0.66
0.64
0.62
10 feat
10 feat, 100 inst
100 feat
100 feat, 100 inst
1 10 100 1000
0.78
0.76
0.74
0.72
0.68
0.66
0.64
0.62
0.8
0.7
10 feat
10 feat, 100 inst
100 feat
100 feat, 100 inst
Movie Reviews
1 10 100 1000
0.9
0.88
0.86
0.84
0.82
0.8
0.78
0.76
0.74
0.72
0.96
0.95
0.94
0.93
0.92
0.91
0.9
alpha alpha
1472
WebKB Science Movie Reviews
annotation time (sec) annotation time (sec) annotation time (sec)
</figure>
<figureCaption confidence="0.780177">
Figure 3: User experiments involving human annotators for text classification. Each row plots accuracy vs. time
</figureCaption>
<bodyText confidence="0.8658495">
learning curves for a particular user (under all three experimental conditions) for each of the three corpora (one
column per data set). For clarity, vertical axes are scaled differently for each corpus, but held constant across all users.
The thin dashed lines at the top of each plot represents the idealized fully-supervised accuracy. Horizontal axes show
labeling cost in terms of actual elapsed annotation time (in seconds).
</bodyText>
<page confidence="0.955863">
1473
</page>
<figure confidence="0.999831666666667">
user 1 0.9
user 2 0.8
user 3 0.7
user 4 0.6
user 5 0.5
0.4
0.3
0.2
0.1
0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
active dual
active inst
passive inst
0 60 120 180 240 300 360
active dual
active inst
passive inst
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0 60 120 180 240 300 360
0 60 120 180 240 300 360
1
0.8
0.6
0.4
0.2
0
active dual
active inst
passive inst
active dual
active inst
passive inst
1
0.8
0.6
0.4
0.2
0
active dual
active inst
passive inst
active dual
active inst
passive inst
0 60 120 180 240 300 360
0 60 120 180 240 300 360
1
0.8
0.6
0.4
0.2
0
active dual
active inst
passive inst
active dual
active inst
passive inst
active dual
active inst
passive inst
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0 60 120 180 240 300 360 0 60 120 180 240 300 360 0 60 120 180 240 300 360
active dual
active inst
passive inst
active dual
active inst
passive inst
0.8
0.6
0.4
0.2
0
1
active dual
active inst
passive inst
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0 60 120 180 240 300 360 0 60 120 180 240 300 360 0 60 120 180 240 300 360
active dual
active inst
passive inst
0 60 120 180 240 300 360
active dual
active inst
passive inst
0 60 120 180 240 300 360
active dual
active inst
passive inst
0 60 120 180 240 300 360
1
0.8
0.6
0.4
0.2
0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0 60 120 180 240 300 360
</figure>
<bodyText confidence="0.99036894">
dual configuration yields consistently better learn- instance-labeling error per trial (relative to the gold-
ing curves than either active or passive learning with standard labels), but in the passive case this rose to
instances alone, often getting within 90% of fully- 1.6, suggesting they are more accurate on the active
supervised accuracy (in under six minutes). The queries. However, they also explicitly ignored more
only two exceptions make interesting (and differ- instances in the active dual condition (7.7) than ei-
ent) case studies. User 4 only provided four la- ther active instance (5.9) or passive (2.5), indicating
beled features in the Movie Review corpus, which that they find these queries more ambiguous. This
partially explains the similarity in performance to seems reasonable, since these are the instances the
the instance-only cases. Moreover, these were classifier is least certain about. But if we look at
manually-added features, i.e., he never answered the time users spent on these actions, they are much
any of the classifier’s feature queries, thus depriving faster to label/ignore (9.7s/7.5s) in the active dual
the learner of the information it requested. User 5, scenario than in the active instance (10.0s/10.7s) or
on the other hand, never manually added features passive (12.3s/15.4s) cases, which means they are
and only answered queries. With the WebKB cor- being more efficient. The differences in time be-
pus, however, he apparently found feature queries tween dual and passive are statistically significant4.
for the course label to be easier than the other 3.4 Additional Use Cases
classes, and 71% of all his feature labels came Here we discuss the application of DUALIST to a
from that class (sometimes noisily, e.g., “instructor” few other natural language processing tasks. This
might also indicate faculty pages). This imbalance section is not meant to show its superiority relative
ultimately biased the learner toward the course la- to other methods, but rather to demonstrate the flex-
bel, which led to classification errors. These patho- ibility and potential of our approach in a variety of
logical cases represent potential pitfalls that could problems in human language technology.
be alleviated with additional user studies and train- 3.4.1 Word Sense Disambiguation
ing. However, we note that the active dual interface Word Sense Disambiguation (WSD) is the prob-
is not particularly worse in these cases, it is simply lem of determining which meaning of a word is be-
not significantly better, as in the other 13 trials. ing used in a particular context (e.g., “hard” in the
Feature queries were less costly than instances, sense of a challenging task vs. a marble floor). We
which is consistent with findings in previous work asked a user to employ DUALIST for 10 minutes
(Raghavan et al., 2006; Druck et al., 2009). The for each of three benchmark WSD corpora (Moham-
least expensive actions in these experiments were mad and Pedersen, 2004): Hard (3 senses), Line
labeling (mean 3.2 seconds) and unlabeling (1.8s) (6 senses), and Serve (4 senses). Each instance rep-
features, while manually adding new features took resents a sentence using the ambiguous word, and
only slightly longer (5.9s). The most expensive ac- features are lowercased unigram and bigram terms
tions were labeling (10.8s) and ignoring (9.9s) in- from the surrounding context in the sentence. The
stance queries. Interestingly, we observed that the learned models’ prediction accuracies (on the sen-
human annotators spent most of the first three min- tences not labeled by the user) were: 83.0%, 78.4%,
utes performing feature-labeling actions ( ), and and 78.7% for Hard, Line, and Serve (respectively),
switched to more instance-labeling activity for the which appears to be comparable to recent supervised
final three minutes ( ). As hypothesized in Sec- learning results in the WSD literature on these data
tion 3.2, it seems that the active learner is exhausting sets. However, our results were achieved in less than
the most salient feature queries early on, and users 10 minutes of effort each, by labeling an average of
begin to focus on more interpretable instance queries 76 sentences and 32 words or phrases per task (com-
over time. However, more study (and longer annota- pared to the thousands of labeled training sentences
tion periods) are warranted to better understand this used in previous work).
phenomenon, which may suggest additional user in-
terface design improvements.
We also saw surprising trends in annotation qual-
ity. In active settings, users made an average of one
1474
4Kolmogorov-Smirnov test, P &lt; 0.01.
</bodyText>
<subsectionHeader confidence="0.845947">
3.4.2 Information Extraction
</subsectionHeader>
<bodyText confidence="0.999967866666667">
DUALIST is also well-suited to a kind of large-
scale information extraction known as semantic
class learning: given a set of semantic categories
and a very large unlabeled text corpus, learn to pop-
ulate a knowledge base with words or phrases that
belong to each class (Riloff and Jones, 1999; Carl-
son et al., 2010). For this task, we first processed
500 million English Web pages from the ClueWeb09
corpus (Callan and Hoy, 2009) by using a shallow
parser. Then we represented noun phrases (e.g., “Al
Gore,” “World Trade Organization,” “upholstery”)
as instances, using a vector of their co-occurrences
with heuristic contextual patterns (e.g., “visit to X”
or “X’s mission”) as well as a few orthographic pat-
terns (e.g., capitalization, head nouns, affixes) as
features. We filtered out instances or contexts that
occurred fewer than 200 times in the corpus, result-
ing in 49,923 noun phrases and 87,760 features.
We then had a user annotate phrases and patterns
into five semantic classes using DUALIST: person,
location, organization, date/time, and other (the
background or null class). The user began by insert-
ing simple hyponym patterns (Hearst, 1992) for their
corresponding classes (e.g., “people such as X” for
person, or “organizations like X” for organization)
and proceeded from there for 20 minutes. Since
there was no gold-standard for evaluation, we ran-
domly sampled 300 predicted extractions for each
of the four non-null classes, and hired human eval-
uators using the Amazon Mechanical Turk service5
to estimate precision. Each instance was assigned
to three evaluators, using majority vote to score for
correctness.
Table 2 shows the estimated precision, total ex-
tracted instances, and the number of user-labeled
features and instances for each class. While there
is room for improvement (published results for this
kind of task are often above 80% precision), it is
worth noting that in this experiment the user did not
provide any initial “seed examples” for each class,
which is fairly common in semantic class learning.
In practice, such additional seeding should help, as
the active learner acquired 115 labeled instances for
the null class, but fewer than a dozen for each non-
null class (in the first 20 minutes).
</bodyText>
<footnote confidence="0.989836">
5http://www.mturk.com
</footnote>
<table confidence="0.999314">
Class Prec. # Ext. # Feat. # Inst.
person 74.7 6,478 37 6
location 76.3 5,307 47 5
organization 59.7 4,613 51 7
date/time 85.7 494 51 12
other – 32,882 13 115
</table>
<tableCaption confidence="0.996017">
Table 2: Summary of results using DUALIST for web-
scale information extraction.
</tableCaption>
<subsectionHeader confidence="0.907705">
3.4.3 Twitter Filtering and Sentiment Analysis
</subsectionHeader>
<bodyText confidence="0.999983542857143">
There is growing interest in language analysis
for online social media services such as Twitter6
(Petrovi´c et al., 2010; Ritter et al., 2010), which al-
lows users to broadcast short messages limited to
140 characters. Two basic but interesting tasks in
this domain are (1) language filtering and (2) sen-
timent classification, both of which are difficult be-
cause of the extreme brevity and informal use of lan-
guage in the messages.
Even though Twitter attempts to provide language
metadata for its “tweets,” English is the default set-
ting for most users, so about 35% of English-tagged
tweets are actually in a different language. Further-
more, the length constraints encourage acronyms,
emphatic misspellings, and orthographic shortcuts
even among English-speaking users, so many tweets
in English actually contain no proper English words
(e.g., “OMG ur sooo gr8!! #luvya”). This may
render existing lexicon-based language filters—and
possibly character n-gram filters—ineffective.
To quickly build an English-language filter for
Twitter, we sampled 150,000 tweets from the Twit-
ter Streaming API and asked an annotator spend 10
minutes with DUALIST labeling English and non-
English messages and features. Features were rep-
resented as unigrams and bigrams without any stop-
word filtering, plus a few Twitter-specific features
such as emoticons (text-based representations of fa-
cial expressions such as :) or :( used to convey feel-
ing or tone), the presence of anonymized usernames
(preceded by ‘@’) or URL links, and hashtags (com-
pound words preceded by ‘#’ and used to label mes-
sages, e.g., “#loveit”). Following the same method-
ology as Section 3.4.2, we evaluated 300 random
predictions using the Mechanical Turk service. The
</bodyText>
<footnote confidence="0.994074">
6http://twitter.com
</footnote>
<page confidence="0.991656">
1475
</page>
<bodyText confidence="0.996646595744681">
estimated accuracy of the trained language filter was From a machine learning perspective, there is an
85.2% (inter-annotator agreement among the evalu- open empirical question of how useful the labels
ators was 94.3%). gathered by DUALIST’s internal naive Bayes model
We then took the 97,813 tweets predicted to be in might be in later training machine learning systems
English and used them as the corpus for a sentiment with different inductive biases (e.g., MaxEnt models
classifier, which attempts to predict the mood con- or decision trees), since the data are not IID. So far,
veyed by the author of a piece of text (Liu, 2010). attempts to “reuse” active learning data have yielded
Using the same feature representation as the lan- mixed results (Lewis and Catlett, 1994; Baldridge
guage filter, the annotator spent 20 minutes with and Osborne, 2004). Practically speaking, DUAL-
DUALIST, labeling tweets and features into three IST is designed to run on a single machine, and
mood classes: positive, negative, and neutral. The supports a few hundred thousand instances and fea-
annotator began by labeling emoticons, by which tures at interactive speeds on modern hardware. Dis-
the active learner was able to uncover some interest- tributed data storage (Chang et al., 2008) and paral-
ing domain-specific salient terms, e.g., “cant wait” lelized learning algorithms (Chu et al., 2007) may
and “#win” for positive tweets or “#tiredofthat” for help scale this approach into the millions.
negative tweets. Using a 300-instance Mechanical Finally, modifying the learning algorithm to better
Turk evaluation, the estimated accuracy of the sen- cope with violated independence assumptions may
timent classifier was 65.9% (inter-annotator agree- be necessary for interesting language applications
ment among the evaluators was 77.4%). beyond those presented here. TAN-Trees (Fried-
4 Discussion and Future Work man et al., 1997), for example, might be able to ac-
We have presented DUALIST, a new type of dual- complish this while retaining speed and interactiv-
strategy annotation interface for semi-supervised ac- ity. Alternatively, one could imagine online stochas-
tive learning. To support this dual-query interface, tic learning algorithms for discriminatively-trained
we developed a novel, fast, and practical semi- classifiers, which are semi-supervised and can ex-
supervised learning algorithm, and demonstrated ploit feature labels. To our knowledge, such flexi-
how users can employ it to rapidly develop use- ble and efficient learning algorithms do not currently
ful natural language systems for a variety of tasks. exist, but they could be easily incorporated into the
For several of these applications, the interactively- DUALIST framework in the future.
trained systems are able to achieve 90% of state- Acknowledgments
of-the-art performance after only a few minutes of Thanks to members of Carnegie Mellon’s “Read the
labeling effort on the part of a human annotator. Web” research project for helpful discussions and
By releasing DUALIST as an open-source tool, we participation in the user studies. This work is sup-
hope to facilitate language annotation projects and ported in part by DARPA (under contracts FA8750-
encourage more user experiments in active learning. 08-1-0009 and AF8750-09-C-0179), the National
This represents one of the first studies of an ac- Science Foundation (IIS-0968487), and Google.
tive learning system designed to compliment the References
strengths of both learner and annotator. Future di- J. Attenberg, P. Melville, and F. Provost. 2010. A uni-
rections along these lines include user studies of effi- fied approach to active dual supervision for labeling
cient annotation behaviors, which in turn might lead features and examples. In Proceedings of the Euro-
to new types of queries or improvements to the user pean Conference on Machine Learning and Principles
interface design. An obvious extension in the natural and Practice of Knowledge Discovery in Databases
language domain is to go beyond classification tasks (ECML PKDD). Springer.
and query domain knowledge for structured predic- J. Baldridge and M. Osborne. 2004. Active learning and
tion in this way. Another interesting potential appli- the total cost of annotation. In Proceedings of the Con-
cation is human-driven active feature induction and ference on Empirical Methods in Natural Language
engineering, after Della Pietra et al. (1997). Processing (EMNLP), pages 9–16. ACL Press.
1476
</bodyText>
<reference confidence="0.999467588785046">
J. Callan and M. Hoy. 2009. The clueweb09 dataset.
http://lemurproject.org/clueweb09/.
A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E.R. Hr-
uschka Jr., and T.M. Mitchell. 2010. Toward an ar-
chitecture for never-ending language learning. In Pro-
ceedings of the Conference on Artificial Intelligence
(AAAI), pages 1306–1313. AAAI Press.
F. Chang, J. Dean, S. Ghemawat, W.C. Hsieh, D.A. Wal-
lach, M. Burrows, T. Chandra, A. Fikes, and R.E. Gru-
ber. 2008. Bigtable: A distributed storage system for
structured data. ACM Transactions on Computer Sys-
tems, 26(2):1–26.
C.T. Chu, S.K. Kim, Y.A. Lin, Y. Yu, G. Bradski, A.Y.
Ng, and K. Olukotun. 2007. Map-reduce for machine
learning on multicore. In B. Sch¨olkopf, J. Platt, and
T. Hoffman, editors, Advances in Neural Information
Processing Systems, volume 19, pages 281–288. MIT
Press.
W. Cohen and Y. Singer. 1996. Context-sensitive learn-
ing methods for text categorization. In Proceedings of
the ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval, pages 307–315. ACM
Press.
M. Craven, D. DiPasquo, D. Freitag, A. McCallum,
T. Mitchell, K. Nigam, and S. Slattery. 1998. Learn-
ing to extract symbolic knowledge from the world
wide web. In Proceedings of the National Confer-
ence on Artificial Intelligence (AAAI), pages 509–516.
AAAI Press.
S. Della Pietra, V. Della Pietra, and J. Lafferty. 1997.
Inducing features of random fields. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
19(4):380–393.
G. Druck, G. Mann, and A. McCallum. 2008. Learn-
ing from labeled features using generalized expecta-
tion criteria. In Proceedings of the ACM SIGIR Con-
ference on Research and Development in Information
Retrieval, pages 595–602. ACM Press.
G. Druck, B. Settles, and A. McCallum. 2009. Ac-
tive learning by labeling features. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 81–90. ACL Press.
N. Friedman, D. Geiger, and M. Goldszmidt. 1997.
Bayesian network classifiers. Machine learning,
29(2):131–163.
S. Godbole, A. Harpale, S. Sarawagi, and S. Chakrabarti.
2004. Document classification through interactive su-
pervision of document and term labels. In Proceed-
ings of the Conference on Principles and Practice of
Knowledge Discovery in Databases (PKDD), pages
185–196. Springer.
M.A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the Confer-
ence on Computational Linguistics (COLING), pages
539–545. ACL.
D. Heckerman. 1995. A tutorial on learning with
bayesian networks. Technical Report MSR-TR-95-06,
Microsoft Research.
K. Lang. 1995. Newsweeder: Learning to filter net-
news. In Proceedings of the International Conference
on Machine Learning (ICML), pages 331–339. Mor-
gan Kaufmann.
D. Lewis and J. Catlett. 1994. Heterogeneous un-
certainty sampling for supervised learning. In Pro-
ceedings of the International Conference on Machine
Learning (ICML), pages 148–156. Morgan Kaufmann.
B. Liu. 2010. Sentiment analysis and subjectivity. In
N. Indurkhya and F.J. Damerau, editors, Handbook of
Natural Language Processing,. CRC Press.
A. McCallum and K. Nigam. 1998. Employing EM
in pool-based active learning for text classification.
In Proceedings of the International Conference on
Machine Learning (ICML), pages 359–367. Morgan
Kaufmann.
P. Melville, W. Gryc, and R.D. Lawrence. 2009. Sen-
timent analysis of blogs by combining lexical knowl-
edge with text classification. In Proceedings of the In-
ternational Conference on Knowledge Discovery and
Data Mining (KDD), pages 1275–1284. ACM Press.
S. Mohammad and T. Pedersen. 2004. Combining lex-
ical and syntactic features for supervised word sense
disambiguation. In Hwee Tou Ng and Ellen Riloff,
editors, Proceedings of the Conference on Compu-
tational Natural Language Learning (CoNLL), pages
25–32. ACL Press.
I. Moulinier. 1996. A framework for comparing text cat-
egorization approaches. In Proceedings of the AAAI
Symposium on Machine Learning in Information Ac-
cess. AAAI Press.
A.Y. Ng and M. Jordan. 2002. On discriminative vs.
generative classifiers: A comparison of logistic regres-
sion and naive bayes. In Advances in Neural Infor-
mation Processing Systems (NIPS), volume 14, pages
841–848. MIT Press.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up: Sentiment classification using machine learning
techniques. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 79–86. ACL Press.
S. Petrovi´c, M. Osborne, and V. Lavrenko. 2010.
Streaming first story detection with application to
Twitter. In Proceedings of the North American Asso-
ciation for Computational Linguistics (NAACL), pages
181–189. ACL Press.
H. Raghavan, O. Madani, and R. Jones. 2006. Active
learning with feedback on both features and instances.
Journal of Machine Learning Research, 7:1655–1686.
</reference>
<page confidence="0.850872">
1477
</page>
<reference confidence="0.999407956521739">
J.D. Rennie, L. Shih, J. Teevan, and D. Karger. 2003.
Tackling the poor assumptions of naive bayes text clas-
sifiers. In Proceedings of the International Conference
on Machine Learning (ICML), pages 285–295. Mor-
gan Kaufmann.
E. Riloff and R. Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping.
In Proceedings of the Conference on Artificial Intel-
ligence (AAAI), pages 474–479. AAAI Press.
A. Ritter, C. Cherry, and B. Dolan. 2010. Unsupervised
modeling of Twitter conversations. In Proceedings
of the North American Association for Computational
Linguistics (NAACL), pages 172–180. ACL Press.
T. Rose, M. Stevenson, and M. Whitehead. 2002. The
Reuters corpus vol. 1 - from yesterday’s news to to-
morrow’s language resources. In Proceedings of the
Conference on Language Resources and Evaluation
(LREC), pages 29–31.
F. Sebastiani. 2002. Machine learning in automated text
categorization. ACM Computing Surveys, 34(1):1–47.
B. Settles. 2009. Active learning literature survey. Com-
puter Sciences Technical Report 1648, University of
Wisconsin–Madison.
</reference>
<page confidence="0.994078">
1478
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.454932">
<title confidence="0.9997515">Closing the Loop: Fast, Interactive Semi-Supervised With Queries on Features and Instances</title>
<author confidence="0.994345">Burr Settles</author>
<affiliation confidence="0.888919">Machine Learning Department Carnegie Mellon University</affiliation>
<address confidence="0.590156">Pittsburgh, PA 15213 USA</address>
<email confidence="0.999769">bsettles@cs.cmu.edu</email>
<abstract confidence="0.999482866666667">This paper describes DUALIST, an active learning annotation paradigm which solicits and learns from labels on both features (e.g., words) and instances (e.g., documents). We present a novel semi-supervised training algorithm developed for this setting, which is (1) fast enough to support real-time interactive speeds, and (2) at least as accurate as preexisting methods for learning with mixed feature and instance labels. Human annotators in user studies were able to produce near-stateof-the-art classifiers—on several corpora in a variety of application domains—with only a few minutes of effort.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Callan</author>
<author>M Hoy</author>
</authors>
<date>2009</date>
<booktitle>The clueweb09 dataset. http://lemurproject.org/clueweb09/.</booktitle>
<contexts>
<context position="31737" citStr="Callan and Hoy, 2009" startWordPosition="5309" endWordPosition="5312"> design improvements. We also saw surprising trends in annotation quality. In active settings, users made an average of one 1474 4Kolmogorov-Smirnov test, P &lt; 0.01. 3.4.2 Information Extraction DUALIST is also well-suited to a kind of largescale information extraction known as semantic class learning: given a set of semantic categories and a very large unlabeled text corpus, learn to populate a knowledge base with words or phrases that belong to each class (Riloff and Jones, 1999; Carlson et al., 2010). For this task, we first processed 500 million English Web pages from the ClueWeb09 corpus (Callan and Hoy, 2009) by using a shallow parser. Then we represented noun phrases (e.g., “Al Gore,” “World Trade Organization,” “upholstery”) as instances, using a vector of their co-occurrences with heuristic contextual patterns (e.g., “visit to X” or “X’s mission”) as well as a few orthographic patterns (e.g., capitalization, head nouns, affixes) as features. We filtered out instances or contexts that occurred fewer than 200 times in the corpus, resulting in 49,923 noun phrases and 87,760 features. We then had a user annotate phrases and patterns into five semantic classes using DUALIST: person, location, organi</context>
</contexts>
<marker>Callan, Hoy, 2009</marker>
<rawString>J. Callan and M. Hoy. 2009. The clueweb09 dataset. http://lemurproject.org/clueweb09/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Carlson</author>
<author>J Betteridge</author>
<author>B Kisiel</author>
<author>B Settles</author>
<author>E R Hruschka Jr</author>
<author>T M Mitchell</author>
</authors>
<title>Toward an architecture for never-ending language learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Artificial Intelligence (AAAI),</booktitle>
<pages>1306--1313</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="31623" citStr="Carlson et al., 2010" startWordPosition="5289" endWordPosition="5293">arranted to better understand this used in previous work). phenomenon, which may suggest additional user interface design improvements. We also saw surprising trends in annotation quality. In active settings, users made an average of one 1474 4Kolmogorov-Smirnov test, P &lt; 0.01. 3.4.2 Information Extraction DUALIST is also well-suited to a kind of largescale information extraction known as semantic class learning: given a set of semantic categories and a very large unlabeled text corpus, learn to populate a knowledge base with words or phrases that belong to each class (Riloff and Jones, 1999; Carlson et al., 2010). For this task, we first processed 500 million English Web pages from the ClueWeb09 corpus (Callan and Hoy, 2009) by using a shallow parser. Then we represented noun phrases (e.g., “Al Gore,” “World Trade Organization,” “upholstery”) as instances, using a vector of their co-occurrences with heuristic contextual patterns (e.g., “visit to X” or “X’s mission”) as well as a few orthographic patterns (e.g., capitalization, head nouns, affixes) as features. We filtered out instances or contexts that occurred fewer than 200 times in the corpus, resulting in 49,923 noun phrases and 87,760 features. W</context>
</contexts>
<marker>Carlson, Betteridge, Kisiel, Settles, Jr, Mitchell, 2010</marker>
<rawString>A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E.R. Hruschka Jr., and T.M. Mitchell. 2010. Toward an architecture for never-ending language learning. In Proceedings of the Conference on Artificial Intelligence (AAAI), pages 1306–1313. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Chang</author>
<author>J Dean</author>
<author>S Ghemawat</author>
<author>W C Hsieh</author>
<author>D A Wallach</author>
<author>M Burrows</author>
<author>T Chandra</author>
<author>A Fikes</author>
<author>R E Gruber</author>
</authors>
<title>Bigtable: A distributed storage system for structured data.</title>
<date>2008</date>
<journal>ACM Transactions on Computer Systems,</journal>
<volume>26</volume>
<issue>2</issue>
<contexts>
<context position="36878" citStr="Chang et al., 2008" startWordPosition="6122" endWordPosition="6125">ive learning data have yielded Using the same feature representation as the lan- mixed results (Lewis and Catlett, 1994; Baldridge guage filter, the annotator spent 20 minutes with and Osborne, 2004). Practically speaking, DUALDUALIST, labeling tweets and features into three IST is designed to run on a single machine, and mood classes: positive, negative, and neutral. The supports a few hundred thousand instances and feaannotator began by labeling emoticons, by which tures at interactive speeds on modern hardware. Disthe active learner was able to uncover some interest- tributed data storage (Chang et al., 2008) and paraling domain-specific salient terms, e.g., “cant wait” lelized learning algorithms (Chu et al., 2007) may and “#win” for positive tweets or “#tiredofthat” for help scale this approach into the millions. negative tweets. Using a 300-instance Mechanical Finally, modifying the learning algorithm to better Turk evaluation, the estimated accuracy of the sen- cope with violated independence assumptions may timent classifier was 65.9% (inter-annotator agree- be necessary for interesting language applications ment among the evaluators was 77.4%). beyond those presented here. TAN-Trees (Fried4 </context>
</contexts>
<marker>Chang, Dean, Ghemawat, Hsieh, Wallach, Burrows, Chandra, Fikes, Gruber, 2008</marker>
<rawString>F. Chang, J. Dean, S. Ghemawat, W.C. Hsieh, D.A. Wallach, M. Burrows, T. Chandra, A. Fikes, and R.E. Gruber. 2008. Bigtable: A distributed storage system for structured data. ACM Transactions on Computer Systems, 26(2):1–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C T Chu</author>
<author>S K Kim</author>
<author>Y A Lin</author>
<author>Y Yu</author>
<author>G Bradski</author>
<author>A Y Ng</author>
<author>K Olukotun</author>
</authors>
<title>Map-reduce for machine learning on multicore.</title>
<date>2007</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<volume>19</volume>
<pages>281--288</pages>
<editor>In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="36987" citStr="Chu et al., 2007" startWordPosition="6138" endWordPosition="6141">tt, 1994; Baldridge guage filter, the annotator spent 20 minutes with and Osborne, 2004). Practically speaking, DUALDUALIST, labeling tweets and features into three IST is designed to run on a single machine, and mood classes: positive, negative, and neutral. The supports a few hundred thousand instances and feaannotator began by labeling emoticons, by which tures at interactive speeds on modern hardware. Disthe active learner was able to uncover some interest- tributed data storage (Chang et al., 2008) and paraling domain-specific salient terms, e.g., “cant wait” lelized learning algorithms (Chu et al., 2007) may and “#win” for positive tweets or “#tiredofthat” for help scale this approach into the millions. negative tweets. Using a 300-instance Mechanical Finally, modifying the learning algorithm to better Turk evaluation, the estimated accuracy of the sen- cope with violated independence assumptions may timent classifier was 65.9% (inter-annotator agree- be necessary for interesting language applications ment among the evaluators was 77.4%). beyond those presented here. TAN-Trees (Fried4 Discussion and Future Work man et al., 1997), for example, might be able to acWe have presented DUALIST, a ne</context>
</contexts>
<marker>Chu, Kim, Lin, Yu, Bradski, Ng, Olukotun, 2007</marker>
<rawString>C.T. Chu, S.K. Kim, Y.A. Lin, Y. Yu, G. Bradski, A.Y. Ng, and K. Olukotun. 2007. Map-reduce for machine learning on multicore. In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems, volume 19, pages 281–288. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Cohen</author>
<author>Y Singer</author>
</authors>
<title>Context-sensitive learning methods for text categorization.</title>
<date>1996</date>
<booktitle>In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>307--315</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="21366" citStr="Cohen and Singer, 1996" startWordPosition="3520" endWordPosition="3523">rn; we chose 50 for all other experiments in this paper. A second observation is that, for all but the Reuters corpus, labeling 90 additional features improves accuracy much more than labeling 100 documents. This is encouraging, since labeling features (e.g., words) is known to be generally faster and easier for humans than labeling entire instances (e.g., documents). For Reuters, however, the additional feature labels appear harmful. The anomaly can be explained in part by previous work with this corpus, which found that a few expertly-chosen keywords can outperform machine learning methods (Cohen and Singer, 1996), or that aggressive feature selection— i.e., using only three or four features per class— helps tremendously (Moulinier, 1996). Corpora like Reuters may naturally lend themselves to feature selection, which is (in some sense) what happens when labeling features. The simulated oracle here was forced to label 100 features, some with very low information gain (e.g., “south” for acquisitions, or “proven” for gold); we would not expect humans annotators to provide such misleading information. Instead, we hypothesize that in practice there may be a limited set of features with high enough informati</context>
</contexts>
<marker>Cohen, Singer, 1996</marker>
<rawString>W. Cohen and Y. Singer. 1996. Context-sensitive learning methods for text categorization. In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval, pages 307–315. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Craven</author>
<author>D DiPasquo</author>
<author>D Freitag</author>
<author>A McCallum</author>
<author>T Mitchell</author>
<author>K Nigam</author>
<author>S Slattery</author>
</authors>
<title>Learning to extract symbolic knowledge from the world wide web.</title>
<date>1998</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence (AAAI),</booktitle>
<pages>509--516</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="14355" citStr="Craven et al., 1998" startWordPosition="2379" endWordPosition="2382">ter. The other experiments are user studies designed to empirically gauge how well human annotators make use of DUALIST in practice. We use a variety of benchmark corpora in the following evaluations. Reuters (Rose et al., 2002) is a collection of news articles organized into topics, such as acquisitions, corn, earnings, etc. As in previous work (Raghavan et al., 2006) we use the 10 most frequent topics, but further process the corpus by removing ambiguous documents (i.e., that belong to multiple topics) so that all articles have a unique label, resulting in a corpus of 9,002 articles. WebKB (Craven et al., 1998) consists of 4,199 university web pages of four types: course, faculty, project, and student. 20 Newsgroups (Lang, 1995) is a set of 18,828 Usenet messages from 20 different online discussion groups. For certain experiments (such as the one shown in Figure 1), we also use topical subsets. Movie Reviews (Pang et al., 2002) is a set of 2,000 online movie reviews categorized as positive or negative in sentiment. All data sets were processed using lowercased unigram features, with punctuation and common stop-words removed. 3.1 Comparison of Learning Algorithms An important question is how well our</context>
</contexts>
<marker>Craven, DiPasquo, Freitag, McCallum, Mitchell, Nigam, Slattery, 1998</marker>
<rawString>M. Craven, D. DiPasquo, D. Freitag, A. McCallum, T. Mitchell, K. Nigam, and S. Slattery. 1998. Learning to extract symbolic knowledge from the world wide web. In Proceedings of the National Conference on Artificial Intelligence (AAAI), pages 509–516. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>J Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>19</volume>
<issue>4</issue>
<marker>Pietra, Pietra, Lafferty, 1997</marker>
<rawString>S. Della Pietra, V. Della Pietra, and J. Lafferty. 1997. Inducing features of random fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4):380–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Druck</author>
<author>G Mann</author>
<author>A McCallum</author>
</authors>
<title>Learning from labeled features using generalized expectation criteria.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>595--602</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="15249" citStr="Druck et al., 2008" startWordPosition="2517" endWordPosition="2520">pical subsets. Movie Reviews (Pang et al., 2002) is a set of 2,000 online movie reviews categorized as positive or negative in sentiment. All data sets were processed using lowercased unigram features, with punctuation and common stop-words removed. 3.1 Comparison of Learning Algorithms An important question is how well our learning algorithm, “MNB/Priors,” performs relative to existing baseline methods for learning with labeled features. We compare against two such approaches from the literature. “MaxEnt/GE” is a maximum entropy classifier trained using generalized expectation (GE) criteria (Druck et al., 2008), which are constraints used in training discriminative linear models. For labeled features, these take the form of expected “reference distributions” conditioned on the presence of the feature (e.g., 95% of documents containing the word “inning” should be labeled baseball). For each constraint, a term is added to the objective function to encourage parameter settings that yield predictions conforming to the reference distribution on unlabeled instances. “MNB/Pool” is naive Bayes trained using the pooling multinomials approach (Melville et al., 2009) mentioned in Section 2.1. We also expand up</context>
<context position="17412" citStr="Druck et al., 2008" startWordPosition="2866" endWordPosition="2869">chmark corpora. Classification accuracy is reported for each model, using only the top 10 oracle-ranked features per label (and no labeled instances) for training. The best model for each corpus is highlighted in bold. Training time (in seconds) is shown in parentheses on the right side of each column. All results are averaged across 10 folds using cross-validation. to us only supports labeled features (and not labeled instances as well), we limit the MNB methods to features for a fair comparison. To obtain feature labels in this experiment, we simulate a “feature oracle” as in previous work (Druck et al., 2008; Attenberg et al., 2010), which is essentially the query selection algorithm from Section 2.3, but using complete labeled data to compute IG(fk). We conservatively use only the top 10 features per class, which is meant to resemble a handful of very salient features that a human might brainstorm to jumpstart the learning process. We experiment with EM1 (one-step EM) variants of both MNB/Pool and MNB/Priors, and set α = 50 for the latter (see Section 3.2 for details on tuning this parameter). Results are averaged over 10 folds using crossvalidation, and all experiments are conducted on a single</context>
</contexts>
<marker>Druck, Mann, McCallum, 2008</marker>
<rawString>G. Druck, G. Mann, and A. McCallum. 2008. Learning from labeled features using generalized expectation criteria. In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval, pages 595–602. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Druck</author>
<author>B Settles</author>
<author>A McCallum</author>
</authors>
<title>Active learning by labeling features.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>81--90</pages>
<publisher>ACL Press.</publisher>
<contexts>
<context position="12291" citStr="Druck et al., 2009" startWordPosition="2032" endWordPosition="2035">s zero), under the assumption that each x is repre1469 sentative of the underlying natural data distribution. Moreover, it is extremely fast to compute, which is important for our interactive environment. Querying features, though, is a newer idea with significantly less research behind it. Previous work has either assumed that (1) features are not assigned to classes, but instead flagged for “relevance” to the task (Godbole et al., 2004; Raghavan et al., 2006), or (2) feature queries are posed just like instance queries: a word is presented to the annotator, who must choose among the labels (Druck et al., 2009; Attenberg et al., 2010). Recall from Figure 1 that we want to organize feature queries into columns by class label. This means our active learner must produce queries that are class-specific. To select these feature queries, we first rank elements in the vocabulary by information gain (IG): P (Ik, yj) P(Ik, yj)log P(Ik)P(yj), where Ik E {0, 1} is a variable indicating the presence or absence of a feature. This is essentially the common feature-selection method for identifying the most salient features in text classification (Sebastiani, 2002). However, we use both L and probabilistically-lab</context>
<context position="29493" citStr="Druck et al., 2009" startWordPosition="4948" endWordPosition="4951">with additional user studies and train- 3.4.1 Word Sense Disambiguation ing. However, we note that the active dual interface Word Sense Disambiguation (WSD) is the probis not particularly worse in these cases, it is simply lem of determining which meaning of a word is benot significantly better, as in the other 13 trials. ing used in a particular context (e.g., “hard” in the Feature queries were less costly than instances, sense of a challenging task vs. a marble floor). We which is consistent with findings in previous work asked a user to employ DUALIST for 10 minutes (Raghavan et al., 2006; Druck et al., 2009). The for each of three benchmark WSD corpora (Mohamleast expensive actions in these experiments were mad and Pedersen, 2004): Hard (3 senses), Line labeling (mean 3.2 seconds) and unlabeling (1.8s) (6 senses), and Serve (4 senses). Each instance repfeatures, while manually adding new features took resents a sentence using the ambiguous word, and only slightly longer (5.9s). The most expensive ac- features are lowercased unigram and bigram terms tions were labeling (10.8s) and ignoring (9.9s) in- from the surrounding context in the sentence. The stance queries. Interestingly, we observed that </context>
</contexts>
<marker>Druck, Settles, McCallum, 2009</marker>
<rawString>G. Druck, B. Settles, and A. McCallum. 2009. Active learning by labeling features. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 81–90. ACL Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Friedman</author>
<author>D Geiger</author>
<author>M Goldszmidt</author>
</authors>
<title>Bayesian network classifiers.</title>
<date>1997</date>
<booktitle>Machine learning,</booktitle>
<pages>29--2</pages>
<marker>Friedman, Geiger, Goldszmidt, 1997</marker>
<rawString>N. Friedman, D. Geiger, and M. Goldszmidt. 1997. Bayesian network classifiers. Machine learning, 29(2):131–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Godbole</author>
<author>A Harpale</author>
<author>S Sarawagi</author>
<author>S Chakrabarti</author>
</authors>
<title>Document classification through interactive supervision of document and term labels.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD),</booktitle>
<pages>185--196</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="12114" citStr="Godbole et al., 2004" startWordPosition="2001" endWordPosition="2004">label the top D unlabeled documents. This simple heuristic is an approximation to querying the instance with the maximum information gain (since the class entropy, once labeled, is zero), under the assumption that each x is repre1469 sentative of the underlying natural data distribution. Moreover, it is extremely fast to compute, which is important for our interactive environment. Querying features, though, is a newer idea with significantly less research behind it. Previous work has either assumed that (1) features are not assigned to classes, but instead flagged for “relevance” to the task (Godbole et al., 2004; Raghavan et al., 2006), or (2) feature queries are posed just like instance queries: a word is presented to the annotator, who must choose among the labels (Druck et al., 2009; Attenberg et al., 2010). Recall from Figure 1 that we want to organize feature queries into columns by class label. This means our active learner must produce queries that are class-specific. To select these feature queries, we first rank elements in the vocabulary by information gain (IG): P (Ik, yj) P(Ik, yj)log P(Ik)P(yj), where Ik E {0, 1} is a variable indicating the presence or absence of a feature. This is esse</context>
</contexts>
<marker>Godbole, Harpale, Sarawagi, Chakrabarti, 2004</marker>
<rawString>S. Godbole, A. Harpale, S. Sarawagi, and S. Chakrabarti. 2004. Document classification through interactive supervision of document and term labels. In Proceedings of the Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD), pages 185–196. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the Conference on Computational Linguistics (COLING),</booktitle>
<pages>539--545</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="32464" citStr="Hearst, 1992" startWordPosition="5423" endWordPosition="5424">tery”) as instances, using a vector of their co-occurrences with heuristic contextual patterns (e.g., “visit to X” or “X’s mission”) as well as a few orthographic patterns (e.g., capitalization, head nouns, affixes) as features. We filtered out instances or contexts that occurred fewer than 200 times in the corpus, resulting in 49,923 noun phrases and 87,760 features. We then had a user annotate phrases and patterns into five semantic classes using DUALIST: person, location, organization, date/time, and other (the background or null class). The user began by inserting simple hyponym patterns (Hearst, 1992) for their corresponding classes (e.g., “people such as X” for person, or “organizations like X” for organization) and proceeded from there for 20 minutes. Since there was no gold-standard for evaluation, we randomly sampled 300 predicted extractions for each of the four non-null classes, and hired human evaluators using the Amazon Mechanical Turk service5 to estimate precision. Each instance was assigned to three evaluators, using majority vote to score for correctness. Table 2 shows the estimated precision, total extracted instances, and the number of user-labeled features and instances for </context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M.A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the Conference on Computational Linguistics (COLING), pages 539–545. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Heckerman</author>
</authors>
<title>A tutorial on learning with bayesian networks.</title>
<date>1995</date>
<tech>Technical Report MSR-TR-95-06, Microsoft Research.</tech>
<contexts>
<context position="6406" citStr="Heckerman, 1995" startWordPosition="1038" endWordPosition="1039"> fixed, we can drop the first term for classification purposes. Then, we can use Bayes’ rule to calculate the posterior probability under the model of a label, given the input document for classification: (1) where Z(x) is shorthand for a normalization constant, summing over all possible class labels. The task of training such a classifier involves estimating the parameters in θ, given a set of labeled instances L = {hx(l), y(l)i}Ll=1. To do this, we use a Dirichlet prior and take the expectation of each parameter with respect to the posterior, which is a simple way to estimate a multinomial (Heckerman, 1995). In other words, we count the fraction of times the word fk occurs in the labeled set among documents of class yj, and the prior adds mjk “hallucinated” occurrences for a smoothed version of the maximum likelihood estimate: θjk = mjk + Ei P(yj|x(i))fk(x(i)) Z(fk) Here, mjk is the prior for word fk under class yj, P(yj|x(i)) ∈ {0, 1} indicates the true labeling of the ith document in the training set, and Z(fk) is a normalization constant summing over all words in the vocabulary. Typically, a uniform prior is used, such as the Laplacian (a value of 1 for all mjk). Class parameters θj are estim</context>
</contexts>
<marker>Heckerman, 1995</marker>
<rawString>D. Heckerman. 1995. A tutorial on learning with bayesian networks. Technical Report MSR-TR-95-06, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lang</author>
</authors>
<title>Newsweeder: Learning to filter netnews.</title>
<date>1995</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML),</booktitle>
<pages>331--339</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="14475" citStr="Lang, 1995" startWordPosition="2399" endWordPosition="2400">ce. We use a variety of benchmark corpora in the following evaluations. Reuters (Rose et al., 2002) is a collection of news articles organized into topics, such as acquisitions, corn, earnings, etc. As in previous work (Raghavan et al., 2006) we use the 10 most frequent topics, but further process the corpus by removing ambiguous documents (i.e., that belong to multiple topics) so that all articles have a unique label, resulting in a corpus of 9,002 articles. WebKB (Craven et al., 1998) consists of 4,199 university web pages of four types: course, faculty, project, and student. 20 Newsgroups (Lang, 1995) is a set of 18,828 Usenet messages from 20 different online discussion groups. For certain experiments (such as the one shown in Figure 1), we also use topical subsets. Movie Reviews (Pang et al., 2002) is a set of 2,000 online movie reviews categorized as positive or negative in sentiment. All data sets were processed using lowercased unigram features, with punctuation and common stop-words removed. 3.1 Comparison of Learning Algorithms An important question is how well our learning algorithm, “MNB/Priors,” performs relative to existing baseline methods for learning with labeled features. We</context>
</contexts>
<marker>Lang, 1995</marker>
<rawString>K. Lang. 1995. Newsweeder: Learning to filter netnews. In Proceedings of the International Conference on Machine Learning (ICML), pages 331–339. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lewis</author>
<author>J Catlett</author>
</authors>
<title>Heterogeneous uncertainty sampling for supervised learning.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML),</booktitle>
<pages>148--156</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="36378" citStr="Lewis and Catlett, 1994" startWordPosition="6044" endWordPosition="6047">en empirical question of how useful the labels ators was 94.3%). gathered by DUALIST’s internal naive Bayes model We then took the 97,813 tweets predicted to be in might be in later training machine learning systems English and used them as the corpus for a sentiment with different inductive biases (e.g., MaxEnt models classifier, which attempts to predict the mood con- or decision trees), since the data are not IID. So far, veyed by the author of a piece of text (Liu, 2010). attempts to “reuse” active learning data have yielded Using the same feature representation as the lan- mixed results (Lewis and Catlett, 1994; Baldridge guage filter, the annotator spent 20 minutes with and Osborne, 2004). Practically speaking, DUALDUALIST, labeling tweets and features into three IST is designed to run on a single machine, and mood classes: positive, negative, and neutral. The supports a few hundred thousand instances and feaannotator began by labeling emoticons, by which tures at interactive speeds on modern hardware. Disthe active learner was able to uncover some interest- tributed data storage (Chang et al., 2008) and paraling domain-specific salient terms, e.g., “cant wait” lelized learning algorithms (Chu et a</context>
</contexts>
<marker>Lewis, Catlett, 1994</marker>
<rawString>D. Lewis and J. Catlett. 1994. Heterogeneous uncertainty sampling for supervised learning. In Proceedings of the International Conference on Machine Learning (ICML), pages 148–156. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Liu</author>
</authors>
<title>Sentiment analysis and subjectivity.</title>
<date>2010</date>
<booktitle>Handbook of Natural Language Processing,.</booktitle>
<editor>In N. Indurkhya and F.J. Damerau, editors,</editor>
<publisher>CRC Press.</publisher>
<contexts>
<context position="36234" citStr="Liu, 2010" startWordPosition="6024" endWordPosition="6025">e trained language filter was From a machine learning perspective, there is an 85.2% (inter-annotator agreement among the evalu- open empirical question of how useful the labels ators was 94.3%). gathered by DUALIST’s internal naive Bayes model We then took the 97,813 tweets predicted to be in might be in later training machine learning systems English and used them as the corpus for a sentiment with different inductive biases (e.g., MaxEnt models classifier, which attempts to predict the mood con- or decision trees), since the data are not IID. So far, veyed by the author of a piece of text (Liu, 2010). attempts to “reuse” active learning data have yielded Using the same feature representation as the lan- mixed results (Lewis and Catlett, 1994; Baldridge guage filter, the annotator spent 20 minutes with and Osborne, 2004). Practically speaking, DUALDUALIST, labeling tweets and features into three IST is designed to run on a single machine, and mood classes: positive, negative, and neutral. The supports a few hundred thousand instances and feaannotator began by labeling emoticons, by which tures at interactive speeds on modern hardware. Disthe active learner was able to uncover some interest</context>
</contexts>
<marker>Liu, 2010</marker>
<rawString>B. Liu. 2010. Sentiment analysis and subjectivity. In N. Indurkhya and F.J. Damerau, editors, Handbook of Natural Language Processing,. CRC Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>K Nigam</author>
</authors>
<title>Employing EM in pool-based active learning for text classification.</title>
<date>1998</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML),</booktitle>
<pages>359--367</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="4952" citStr="McCallum and Nigam, 1998" startWordPosition="783" endWordPosition="786">set of queries. The learning algorithm is actually fast enough to do this automatically after each labeling action. However, we found such a dynamically changing interface to be frustrating for users (e.g., words they wanted to label would move or disappear). 2.1 A Generative Model for Learning from Feature and Instance Labels For the underlying model in this system, we use multinomial naive Bayes (MNB) since it is simple, fast, and known to work well for several natural language applications—text classification in particular—despite its simplistic and often violated independence assumptions (McCallum and Nigam, 1998; Rennie et al., 2003). MNB models the distribution of features as a multinomial: documents are sequences of words, with the “naive” assumption that words in each position are generated independently. Each document is treated as a mixture of classes, which have their own multinomial distributions over words. Let the model be parameterized by the vector θ, with θj = P(yj) denoting the probability of class yj, and θjk = P(fk|yj) denoting the probability of generating word fk given class yj. Note that for class priors Ej θj = 1, and for per-class word multinomials Ek θjk = 1. The likelihood of do</context>
<context position="9555" citStr="McCallum and Nigam, 1998" startWordPosition="1568" endWordPosition="1571">(which is a more general sports term). However, we leave this aspect for future work and employ the fixed-α approach as described above in this study. 2.2 Exploiting Unlabeled Data In addition to document and feature labels, we usually have access to a large unlabeled corpus. In fact, these texts form the pool of possible instance queries in active learning. We can take advantage of this additional data in generative models like MNB by employing the Expectation-Maximization (EM) algorithm. Combining EM with pool-based active learning was previously studied in the context of instance labeling (McCallum and Nigam, 1998), and we extend the method to our interactive scenario, which supports feature labeling as well. First, we estimate initial parameters 0&apos; as in Section 2.1, but using only the priors (and no instances). Then, we apply the induced classifier on the unlabeled pool U = {x(u)}U u=1 (Eq. 1). This is the “E” step of EM. Next we re-estimate feature multinomials 0jk, using both labeled instances from L and probabilistically-labeled instances from U (Eq. 2). In other words, P(yj|x) ∈ {0, 1} for x ∈ L, and P(yj|x) = Pθi(yj|x) for x ∈ U. We also weight the data in U by a factor of 0.1, so as not to overw</context>
<context position="10822" citStr="McCallum and Nigam, 1998" startWordPosition="1796" endWordPosition="1799">instance labels in L. Class parameters 0j are re-estimated in the analogous fashion. This is the “M” step. For speed and interactivity, we actually stop training after this first iteration. When feature labels are available, we found that EM generally converges in four to 10 iterations, requiring more training time but rarely improving accuracy (the largest gains consistently come in the first iteration). Also, we ignore labeled data in the initial estimation of 0&apos; because L is too small early in active learning to yield good results with EM. Perhaps this can be improved by using an ensemble (McCallum and Nigam, 1998), but that comes at further computational expense. Feature labels, on the other hand, seem generally more reliable for probabilistically labeling U. 2.3 Selecting Instance and Feature Queries The final algorithmic component to our system is the selection of informative queries (i.e., unlabeled words and documents) to present to the annotator. Querying instances is the traditional mode of active learning, and is well-studied in the literature; see Settles (2009) for a review. In this work we use entropy-based uncertainty sampling, which ranks all instances in U by the posterior class entropy un</context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>A. McCallum and K. Nigam. 1998. Employing EM in pool-based active learning for text classification. In Proceedings of the International Conference on Machine Learning (ICML), pages 359–367. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Melville</author>
<author>W Gryc</author>
<author>R D Lawrence</author>
</authors>
<title>Sentiment analysis of blogs by combining lexical knowledge with text classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference on Knowledge Discovery and Data Mining (KDD),</booktitle>
<pages>1275--1284</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="8096" citStr="Melville et al., 2009" startWordPosition="1340" endWordPosition="1343">tional information, we assume that labeling the word fk with a class yj increases the probability P (fk|yj) of the word appearing in documents of that class. The natural interpretation of this under our model is to increase the prior mjk for the corresponding multinomial. To do this we introduce a new parameter α, and define the elements of the Dirichlet prior as follows: _ ( 1 + α if fk is labeled with yj, mjk Sl 1 otherwise. This approach is extremely flexible, and offers three particular advantages over the previous “pooling multinomials” approach for incorporating feature labels into MNB (Melville et al., 2009). The pooling multinomials algorithm averages together two sets of 0jk parameters: one that is estimated from labeled data, and another derived from feature labels under the assumption of a boolean output variable (treating labeled features are “polarizing” factors). Therefore, pooling multinomials can only be applied to binary classification tasks, while our method works equally well for problems with multiple classes. The second advantage is that feature labels need not be mutually exclusive, so the word “score” could be labeled with both baseball and hockey, if necessary (e.g., if the task </context>
<context position="15805" citStr="Melville et al., 2009" startWordPosition="2601" endWordPosition="2604">d using generalized expectation (GE) criteria (Druck et al., 2008), which are constraints used in training discriminative linear models. For labeled features, these take the form of expected “reference distributions” conditioned on the presence of the feature (e.g., 95% of documents containing the word “inning” should be labeled baseball). For each constraint, a term is added to the objective function to encourage parameter settings that yield predictions conforming to the reference distribution on unlabeled instances. “MNB/Pool” is naive Bayes trained using the pooling multinomials approach (Melville et al., 2009) mentioned in Section 2.1. We also expand upon MNB/Pool using an EM variant to make it semi-supervised. We use the implementation of GE training from the open-source MALLET toolkit2, and implement both MNB variants in the same data-processing pipeline. Because the GE implementation available 2http://mallet.cs.umass.edu IG(fk) = � � Ik j 1470 Corpus MaxEnt/GE MNB/Pool Pool+EM1 MNB/Priors Priors+EM1 Reuters 82.8 (22.9) – – – – 83.7 (&lt;0.1) 86.6 (0.3) WebKB 22.2 (4.9) – – – – 67.5 (&lt;0.1) 67.8 (0.1) 20 Newsgroups 49.7 (326.6) – – – – 50.1 (0.2) 70.7 (6.9) Science 86.9 (5.7) – – – – 71.4 (&lt;0.1) 92.8</context>
</contexts>
<marker>Melville, Gryc, Lawrence, 2009</marker>
<rawString>P. Melville, W. Gryc, and R.D. Lawrence. 2009. Sentiment analysis of blogs by combining lexical knowledge with text classification. In Proceedings of the International Conference on Knowledge Discovery and Data Mining (KDD), pages 1275–1284. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Mohammad</author>
<author>T Pedersen</author>
</authors>
<title>Combining lexical and syntactic features for supervised word sense disambiguation.</title>
<date>2004</date>
<booktitle>In Hwee Tou Ng and Ellen Riloff, editors, Proceedings of the Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>25--32</pages>
<publisher>ACL Press.</publisher>
<marker>Mohammad, Pedersen, 2004</marker>
<rawString>S. Mohammad and T. Pedersen. 2004. Combining lexical and syntactic features for supervised word sense disambiguation. In Hwee Tou Ng and Ellen Riloff, editors, Proceedings of the Conference on Computational Natural Language Learning (CoNLL), pages 25–32. ACL Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Moulinier</author>
</authors>
<title>A framework for comparing text categorization approaches.</title>
<date>1996</date>
<booktitle>In Proceedings of the AAAI Symposium on Machine Learning in Information Access.</booktitle>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="21493" citStr="Moulinier, 1996" startWordPosition="3540" endWordPosition="3541">itional features improves accuracy much more than labeling 100 documents. This is encouraging, since labeling features (e.g., words) is known to be generally faster and easier for humans than labeling entire instances (e.g., documents). For Reuters, however, the additional feature labels appear harmful. The anomaly can be explained in part by previous work with this corpus, which found that a few expertly-chosen keywords can outperform machine learning methods (Cohen and Singer, 1996), or that aggressive feature selection— i.e., using only three or four features per class— helps tremendously (Moulinier, 1996). Corpora like Reuters may naturally lend themselves to feature selection, which is (in some sense) what happens when labeling features. The simulated oracle here was forced to label 100 features, some with very low information gain (e.g., “south” for acquisitions, or “proven” for gold); we would not expect humans annotators to provide such misleading information. Instead, we hypothesize that in practice there may be a limited set of features with high enough information content for humans to feel confident labeling, after which they switch their attention to labeling instance queries instead.</context>
</contexts>
<marker>Moulinier, 1996</marker>
<rawString>I. Moulinier. 1996. A framework for comparing text categorization approaches. In Proceedings of the AAAI Symposium on Machine Learning in Information Access. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Y Ng</author>
<author>M Jordan</author>
</authors>
<title>On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes.</title>
<date>2002</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS),</booktitle>
<volume>14</volume>
<pages>841--848</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="19360" citStr="Ng and Jordan, 2002" startWordPosition="3190" endWordPosition="3193">e also consistently more accurate than GE training—and are about 40 times faster as well. The gains of Priors+EM1 over MaxEnt/GE are statistically significant in all cases but two: Autos/Motorcycles and Movie Reviews3. MNB is superior when using anywhere from five to 20 oracle-ranked features per class, but as the number of feature labels increases beyond 30, GE is often more accurate (results not shown). If we think of MaxEnt/GE as a discriminative analog of MNB/Priors+EM, this is consistent with what is known about labeled set size in supervised learning for generative/discriminative pairs (Ng and Jordan, 2002). However, the time complexity of GE training increases sharply with each new labeled feature, since it adds a new constraint to the objective function whose gradient must be computed using all the unlabeled data. In short, GE training is too slow and too inaccurate early in the active learning process (where labels are more scarce) to be appropriate for our scenario. Thus, we select MNB/Priors to power the DUALIST interface. 3.2 Tuning the Parameter α A second question is how sensitive the accuracy of MNB/Priors is to the parameter α. To study this, we ran experiments varying α from from one </context>
</contexts>
<marker>Ng, Jordan, 2002</marker>
<rawString>A.Y. Ng and M. Jordan. 2002. On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. In Advances in Neural Information Processing Systems (NIPS), volume 14, pages 841–848. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up: Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>79--86</pages>
<publisher>ACL Press.</publisher>
<contexts>
<context position="14678" citStr="Pang et al., 2002" startWordPosition="2432" endWordPosition="2435">c. As in previous work (Raghavan et al., 2006) we use the 10 most frequent topics, but further process the corpus by removing ambiguous documents (i.e., that belong to multiple topics) so that all articles have a unique label, resulting in a corpus of 9,002 articles. WebKB (Craven et al., 1998) consists of 4,199 university web pages of four types: course, faculty, project, and student. 20 Newsgroups (Lang, 1995) is a set of 18,828 Usenet messages from 20 different online discussion groups. For certain experiments (such as the one shown in Figure 1), we also use topical subsets. Movie Reviews (Pang et al., 2002) is a set of 2,000 online movie reviews categorized as positive or negative in sentiment. All data sets were processed using lowercased unigram features, with punctuation and common stop-words removed. 3.1 Comparison of Learning Algorithms An important question is how well our learning algorithm, “MNB/Priors,” performs relative to existing baseline methods for learning with labeled features. We compare against two such approaches from the literature. “MaxEnt/GE” is a maximum entropy classifier trained using generalized expectation (GE) criteria (Druck et al., 2008), which are constraints used </context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up: Sentiment classification using machine learning techniques. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 79–86. ACL Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrovi´c</author>
<author>M Osborne</author>
<author>V Lavrenko</author>
</authors>
<title>Streaming first story detection with application to Twitter.</title>
<date>2010</date>
<booktitle>In Proceedings of the North American Association for Computational Linguistics (NAACL),</booktitle>
<pages>181--189</pages>
<publisher>ACL Press.</publisher>
<marker>Petrovi´c, Osborne, Lavrenko, 2010</marker>
<rawString>S. Petrovi´c, M. Osborne, and V. Lavrenko. 2010. Streaming first story detection with application to Twitter. In Proceedings of the North American Association for Computational Linguistics (NAACL), pages 181–189. ACL Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Raghavan</author>
<author>O Madani</author>
<author>R Jones</author>
</authors>
<title>Active learning with feedback on both features and instances.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--1655</pages>
<contexts>
<context position="12138" citStr="Raghavan et al., 2006" startWordPosition="2005" endWordPosition="2008">led documents. This simple heuristic is an approximation to querying the instance with the maximum information gain (since the class entropy, once labeled, is zero), under the assumption that each x is repre1469 sentative of the underlying natural data distribution. Moreover, it is extremely fast to compute, which is important for our interactive environment. Querying features, though, is a newer idea with significantly less research behind it. Previous work has either assumed that (1) features are not assigned to classes, but instead flagged for “relevance” to the task (Godbole et al., 2004; Raghavan et al., 2006), or (2) feature queries are posed just like instance queries: a word is presented to the annotator, who must choose among the labels (Druck et al., 2009; Attenberg et al., 2010). Recall from Figure 1 that we want to organize feature queries into columns by class label. This means our active learner must produce queries that are class-specific. To select these feature queries, we first rank elements in the vocabulary by information gain (IG): P (Ik, yj) P(Ik, yj)log P(Ik)P(yj), where Ik E {0, 1} is a variable indicating the presence or absence of a feature. This is essentially the common featu</context>
<context position="14106" citStr="Raghavan et al., 2006" startWordPosition="2335" endWordPosition="2338">four sets of experiments to evaluate our approach. The first two are “offline” experiments, designed to better understand (1) how our training algorithm compares to existing methods for featurelabel learning, and (2) the effects of tuning the α parameter. The other experiments are user studies designed to empirically gauge how well human annotators make use of DUALIST in practice. We use a variety of benchmark corpora in the following evaluations. Reuters (Rose et al., 2002) is a collection of news articles organized into topics, such as acquisitions, corn, earnings, etc. As in previous work (Raghavan et al., 2006) we use the 10 most frequent topics, but further process the corpus by removing ambiguous documents (i.e., that belong to multiple topics) so that all articles have a unique label, resulting in a corpus of 9,002 articles. WebKB (Craven et al., 1998) consists of 4,199 university web pages of four types: course, faculty, project, and student. 20 Newsgroups (Lang, 1995) is a set of 18,828 Usenet messages from 20 different online discussion groups. For certain experiments (such as the one shown in Figure 1), we also use topical subsets. Movie Reviews (Pang et al., 2002) is a set of 2,000 online mo</context>
<context position="29472" citStr="Raghavan et al., 2006" startWordPosition="4944" endWordPosition="4947">hnology. be alleviated with additional user studies and train- 3.4.1 Word Sense Disambiguation ing. However, we note that the active dual interface Word Sense Disambiguation (WSD) is the probis not particularly worse in these cases, it is simply lem of determining which meaning of a word is benot significantly better, as in the other 13 trials. ing used in a particular context (e.g., “hard” in the Feature queries were less costly than instances, sense of a challenging task vs. a marble floor). We which is consistent with findings in previous work asked a user to employ DUALIST for 10 minutes (Raghavan et al., 2006; Druck et al., 2009). The for each of three benchmark WSD corpora (Mohamleast expensive actions in these experiments were mad and Pedersen, 2004): Hard (3 senses), Line labeling (mean 3.2 seconds) and unlabeling (1.8s) (6 senses), and Serve (4 senses). Each instance repfeatures, while manually adding new features took resents a sentence using the ambiguous word, and only slightly longer (5.9s). The most expensive ac- features are lowercased unigram and bigram terms tions were labeling (10.8s) and ignoring (9.9s) in- from the surrounding context in the sentence. The stance queries. Interesting</context>
</contexts>
<marker>Raghavan, Madani, Jones, 2006</marker>
<rawString>H. Raghavan, O. Madani, and R. Jones. 2006. Active learning with feedback on both features and instances. Journal of Machine Learning Research, 7:1655–1686.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Rennie</author>
<author>L Shih</author>
<author>J Teevan</author>
<author>D Karger</author>
</authors>
<title>Tackling the poor assumptions of naive bayes text classifiers.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML),</booktitle>
<pages>285--295</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="4974" citStr="Rennie et al., 2003" startWordPosition="787" endWordPosition="790">ng algorithm is actually fast enough to do this automatically after each labeling action. However, we found such a dynamically changing interface to be frustrating for users (e.g., words they wanted to label would move or disappear). 2.1 A Generative Model for Learning from Feature and Instance Labels For the underlying model in this system, we use multinomial naive Bayes (MNB) since it is simple, fast, and known to work well for several natural language applications—text classification in particular—despite its simplistic and often violated independence assumptions (McCallum and Nigam, 1998; Rennie et al., 2003). MNB models the distribution of features as a multinomial: documents are sequences of words, with the “naive” assumption that words in each position are generated independently. Each document is treated as a mixture of classes, which have their own multinomial distributions over words. Let the model be parameterized by the vector θ, with θj = P(yj) denoting the probability of class yj, and θjk = P(fk|yj) denoting the probability of generating word fk given class yj. Note that for class priors Ej θj = 1, and for per-class word multinomials Ek θjk = 1. The likelihood of document x being generat</context>
</contexts>
<marker>Rennie, Shih, Teevan, Karger, 2003</marker>
<rawString>J.D. Rennie, L. Shih, J. Teevan, and D. Karger. 2003. Tackling the poor assumptions of naive bayes text classifiers. In Proceedings of the International Conference on Machine Learning (ICML), pages 285–295. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>R Jones</author>
</authors>
<title>Learning dictionaries for information extraction by multi-level bootstrapping.</title>
<date>1999</date>
<booktitle>In Proceedings of the Conference on Artificial Intelligence (AAAI),</booktitle>
<pages>474--479</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="31600" citStr="Riloff and Jones, 1999" startWordPosition="5285" endWordPosition="5288">nces tion periods) are warranted to better understand this used in previous work). phenomenon, which may suggest additional user interface design improvements. We also saw surprising trends in annotation quality. In active settings, users made an average of one 1474 4Kolmogorov-Smirnov test, P &lt; 0.01. 3.4.2 Information Extraction DUALIST is also well-suited to a kind of largescale information extraction known as semantic class learning: given a set of semantic categories and a very large unlabeled text corpus, learn to populate a knowledge base with words or phrases that belong to each class (Riloff and Jones, 1999; Carlson et al., 2010). For this task, we first processed 500 million English Web pages from the ClueWeb09 corpus (Callan and Hoy, 2009) by using a shallow parser. Then we represented noun phrases (e.g., “Al Gore,” “World Trade Organization,” “upholstery”) as instances, using a vector of their co-occurrences with heuristic contextual patterns (e.g., “visit to X” or “X’s mission”) as well as a few orthographic patterns (e.g., capitalization, head nouns, affixes) as features. We filtered out instances or contexts that occurred fewer than 200 times in the corpus, resulting in 49,923 noun phrases</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>E. Riloff and R. Jones. 1999. Learning dictionaries for information extraction by multi-level bootstrapping. In Proceedings of the Conference on Artificial Intelligence (AAAI), pages 474–479. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ritter</author>
<author>C Cherry</author>
<author>B Dolan</author>
</authors>
<title>Unsupervised modeling of Twitter conversations.</title>
<date>2010</date>
<booktitle>In Proceedings of the North American Association for Computational Linguistics (NAACL),</booktitle>
<pages>172--180</pages>
<publisher>ACL Press.</publisher>
<contexts>
<context position="33993" citStr="Ritter et al., 2010" startWordPosition="5670" endWordPosition="5673">nal seeding should help, as the active learner acquired 115 labeled instances for the null class, but fewer than a dozen for each nonnull class (in the first 20 minutes). 5http://www.mturk.com Class Prec. # Ext. # Feat. # Inst. person 74.7 6,478 37 6 location 76.3 5,307 47 5 organization 59.7 4,613 51 7 date/time 85.7 494 51 12 other – 32,882 13 115 Table 2: Summary of results using DUALIST for webscale information extraction. 3.4.3 Twitter Filtering and Sentiment Analysis There is growing interest in language analysis for online social media services such as Twitter6 (Petrovi´c et al., 2010; Ritter et al., 2010), which allows users to broadcast short messages limited to 140 characters. Two basic but interesting tasks in this domain are (1) language filtering and (2) sentiment classification, both of which are difficult because of the extreme brevity and informal use of language in the messages. Even though Twitter attempts to provide language metadata for its “tweets,” English is the default setting for most users, so about 35% of English-tagged tweets are actually in a different language. Furthermore, the length constraints encourage acronyms, emphatic misspellings, and orthographic shortcuts even a</context>
</contexts>
<marker>Ritter, Cherry, Dolan, 2010</marker>
<rawString>A. Ritter, C. Cherry, and B. Dolan. 2010. Unsupervised modeling of Twitter conversations. In Proceedings of the North American Association for Computational Linguistics (NAACL), pages 172–180. ACL Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Rose</author>
<author>M Stevenson</author>
<author>M Whitehead</author>
</authors>
<title>The Reuters corpus vol. 1 - from yesterday’s news to tomorrow’s language resources.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>29--31</pages>
<contexts>
<context position="13963" citStr="Rose et al., 2002" startWordPosition="2311" endWordPosition="2314">ost correlated. To our knowledge, DUALIST is the first active learning environment with both of these properties. 3 Experiments We conduct four sets of experiments to evaluate our approach. The first two are “offline” experiments, designed to better understand (1) how our training algorithm compares to existing methods for featurelabel learning, and (2) the effects of tuning the α parameter. The other experiments are user studies designed to empirically gauge how well human annotators make use of DUALIST in practice. We use a variety of benchmark corpora in the following evaluations. Reuters (Rose et al., 2002) is a collection of news articles organized into topics, such as acquisitions, corn, earnings, etc. As in previous work (Raghavan et al., 2006) we use the 10 most frequent topics, but further process the corpus by removing ambiguous documents (i.e., that belong to multiple topics) so that all articles have a unique label, resulting in a corpus of 9,002 articles. WebKB (Craven et al., 1998) consists of 4,199 university web pages of four types: course, faculty, project, and student. 20 Newsgroups (Lang, 1995) is a set of 18,828 Usenet messages from 20 different online discussion groups. For cert</context>
</contexts>
<marker>Rose, Stevenson, Whitehead, 2002</marker>
<rawString>T. Rose, M. Stevenson, and M. Whitehead. 2002. The Reuters corpus vol. 1 - from yesterday’s news to tomorrow’s language resources. In Proceedings of the Conference on Language Resources and Evaluation (LREC), pages 29–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sebastiani</author>
</authors>
<title>Machine learning in automated text categorization.</title>
<date>2002</date>
<journal>ACM Computing Surveys,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="12841" citStr="Sebastiani, 2002" startWordPosition="2125" endWordPosition="2126">the annotator, who must choose among the labels (Druck et al., 2009; Attenberg et al., 2010). Recall from Figure 1 that we want to organize feature queries into columns by class label. This means our active learner must produce queries that are class-specific. To select these feature queries, we first rank elements in the vocabulary by information gain (IG): P (Ik, yj) P(Ik, yj)log P(Ik)P(yj), where Ik E {0, 1} is a variable indicating the presence or absence of a feature. This is essentially the common feature-selection method for identifying the most salient features in text classification (Sebastiani, 2002). However, we use both L and probabilistically-labeled instances from U to compute IG(fk), to better reflect what the model believes it has learned. To organize queries into classes, we take the top V unlabeled features and pose fk for the class yj with which it occurs most frequently, as well as any other class with which it occurs at least 75% as often. Intuitively, this approach (1) queries features that the model believes are most informative, and (2) automatically identifies classes that seem most correlated. To our knowledge, DUALIST is the first active learning environment with both of </context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>F. Sebastiani. 2002. Machine learning in automated text categorization. ACM Computing Surveys, 34(1):1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Settles</author>
</authors>
<title>Active learning literature survey. Computer Sciences</title>
<date>2009</date>
<tech>Technical Report 1648,</tech>
<institution>University of Wisconsin–Madison.</institution>
<contexts>
<context position="11287" citStr="Settles (2009)" startWordPosition="1868" endWordPosition="1869">e L is too small early in active learning to yield good results with EM. Perhaps this can be improved by using an ensemble (McCallum and Nigam, 1998), but that comes at further computational expense. Feature labels, on the other hand, seem generally more reliable for probabilistically labeling U. 2.3 Selecting Instance and Feature Queries The final algorithmic component to our system is the selection of informative queries (i.e., unlabeled words and documents) to present to the annotator. Querying instances is the traditional mode of active learning, and is well-studied in the literature; see Settles (2009) for a review. In this work we use entropy-based uncertainty sampling, which ranks all instances in U by the posterior class entropy under the model Hθ(Y |x) = −Ej Pθ(yj|x)log Pθ(yj|x), and asks the user to label the top D unlabeled documents. This simple heuristic is an approximation to querying the instance with the maximum information gain (since the class entropy, once labeled, is zero), under the assumption that each x is repre1469 sentative of the underlying natural data distribution. Moreover, it is extremely fast to compute, which is important for our interactive environment. Querying </context>
</contexts>
<marker>Settles, 2009</marker>
<rawString>B. Settles. 2009. Active learning literature survey. Computer Sciences Technical Report 1648, University of Wisconsin–Madison.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>