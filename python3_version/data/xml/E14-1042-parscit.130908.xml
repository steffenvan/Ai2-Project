<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000630">
<title confidence="0.9982">
Learning from Post-Editing:
Online Model Adaptation for Statistical Machine Translation
</title>
<author confidence="0.993702">
Michael Denkowski Chris Dyer Alon Lavie
</author>
<affiliation confidence="0.824447666666667">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
</affiliation>
<email confidence="0.998966">
{mdenkows,cdyer,alavie}@cs.cmu.edu
</email>
<sectionHeader confidence="0.993903" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999575684210526">
Using machine translation output as a
starting point for human translation has
become an increasingly common applica-
tion of MT. We propose and evaluate three
computationally efficient online methods
for updating statistical MT systems in a
scenario where post-edited MT output is
constantly being returned to the system:
(1) adding new rules to the translation
model from the post-edited content, (2)
updating a Bayesian language model of
the target language that is used by the
MT system, and (3) updating the MT
system’s discriminative parameters with
a MIRA step. Individually, these tech-
niques can substantially improve MT qual-
ity, even over strong baselines. Moreover,
we see super-additive improvements when
all three techniques are used in tandem.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999901709090909">
Using machine translation outputs as a starting
point for human translators is becoming increas-
ingly common and is now arguably one of the most
commercially important applications of MT. Con-
siderable evidence has accumulated showing that
human translators are more productive and accu-
rate when post-editing MT output than when trans-
lating from scratch (Guerberof, 2009; Carl et al.,
2011; Koehn, 2012; Zhechev, 2012, inter alia).
An important (if unsurprising) insight from prior
research in this area is that translators become
more productive as MT quality improves (Tat-
sumi, 2009). While general improvements to MT
continue to lead to further productivity gains, we
explore how MT quality can be improved specifi-
cally in an online post-editing scenario in which
sentence-level MT outputs are constantly being
presented to human experts, edited, and then re-
turned to the system for immediate learning. This
task is challenging in two regards. First, from a
technical perspective, post-edited outputs must be
processed rapidly: a productive post-editor cannot
wait for a standard batch MT training pipeline to
be rerun after each sentence is corrected! Sec-
ond, from a methodological perspective, it is ex-
pensive to run many human subject experiments,
in particular when the human subjects must have
translation expertise. We therefore use a sim-
ulated post-editing paradigm in which either
non-post-edited reference translations or manually
post-edited translations from a similar MT system
are used in lieu of human post-editors (§2). This
paradigm allows us to efficiently develop and eval-
uate systems that can go on to function in real-time
post-editing scenarios without modification.
We present and evaluate three online methods
for improving translation models using feedback
from editors: adding new translations rules to
the translation grammar (§3), updating a Bayesian
language model with observations of the post-
edited output (§4), and using an online discrimi-
native parameter update to minimize model error
(§5). These techniques are computationally effi-
cient and make minimal use of approximation or
heuristics, handling initial and incremental data in
a uniform way. We evaluate these techniques in a
variety of language and data scenarios that mimic
the demands of real-world translation tasks. Com-
pared to a competitive baseline, we show substan-
tial improvement from updating the translation
grammar or language model independently and
super-additive gains from combining these tech-
niques with a MIRA update (§6). We then discuss
how our techniques relate to prior work (§7) and
conclude (§8).
</bodyText>
<sectionHeader confidence="0.955927" genericHeader="introduction">
2 Simulated Post-Editing Paradigm
</sectionHeader>
<bodyText confidence="0.986810666666667">
In post-editing scenarios, humans continuously
edit machine translation outputs into production-
quality translations, providing an additional, con-
</bodyText>
<page confidence="0.985267">
395
</page>
<note confidence="0.992995">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 395–404,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999886261904762">
stant stream of data absent in batch translation.
This data consists of highly domain-relevant ref-
erence translations that are minimally different
from MT outputs, making them ideal for learn-
ing. However, true post-editing data is infeasi-
ble to collect during system development and in-
ternal testing as standard MT pipelines require
tens of thousands of sentences to be translated
with low latency. To address this problem, Hardt
and Elming (2010) formulate the task of sim-
ulated post-editing, wherein pre-generated refer-
ence translations are used as a stand-in for actual
post-editing. This approximation is equivalent to
the case where humans edit each translation hy-
pothesis to be identical to the reference rather than
simply correcting the MT output to be grammat-
ical and meaning-equivalent to the source. Our
work uses this approximation for tuning and eval-
uation. We also introduce a more accurate approx-
imation wherein MT output from the target sys-
tem (or a similar system) is post-edited in advance,
creating “offline” post-edited data that is similar
to expected system outputs and should thus min-
imize unnecessary edits. An experiment in §6.4
compares the two approximations.
In our simulated post-editing tasks, decoding
(for both the test corpus and each pass over the
development corpus during optimization) begins
with baseline models trained on standard bilin-
gual and monolingual data. After each sentence
is translated, the following take place in order:
First, MIRA uses the new source–reference pair
to update weights for the current models. Second,
the source is aligned to the reference and used to
update the translation grammar. Third, the refer-
ence is added to the Bayesian language model. As
sentences are translated, the models gain valuable
context information, allowing them to zero in on
the target document and translator. Context is re-
set at the start of each development or test corpus.1
This setup, which allows a uniform approach to
tuning and decoding, is visualized in Figure 1.
</bodyText>
<sectionHeader confidence="0.972262" genericHeader="method">
3 Translation Grammar Adaptation
</sectionHeader>
<bodyText confidence="0.9996934">
Translation models (either phrase tables or syn-
chronous grammars) are typically generated of-
fline from large bilingual text. This is reasonable
in scenarios where available training data is fixed
over long periods of time. However, this approach
</bodyText>
<footnote confidence="0.956498666666667">
1Initial experiments show this to outperform resetting
models on more fine-grained document boundaries, although
further investigation is warranted.
</footnote>
<subsectionHeader confidence="0.413896">
Source Target (Reference)
</subsectionHeader>
<figureCaption confidence="0.645218833333333">
Figure 1: Context when translating an input sen-
tence (bold) with simulated post-editing. Previ-
ous sentences and references (shaded) are added
to the training data. After the current sentence is
translated, it is aligned to the reference (italic) and
added to the context for the next sentence.
</figureCaption>
<bodyText confidence="0.999293090909091">
does not allow adding new data without repeating
model estimation in its entirety, which may take
hours or days. In this section, we describe a simple
technique for incorporating new bilingual training
data as soon as it is available. Our approach is
an extension of the on-demand grammar extractor
described by Lopez (2008a). We extend the work
initially designed for on-the-fly grammar extrac-
tion from static data (to mitigate the expense of
storing large translation grammars), to specifically
handle incremental data from post-editing.
</bodyText>
<subsectionHeader confidence="0.999229">
3.1 Suffix Array Grammar Extraction
</subsectionHeader>
<bodyText confidence="0.999908368421053">
Lopez (2008a) introduces an alternative to tradi-
tional model estimation for hierarchical phrase-
based statistical machine translation (Chiang,
2007). Rather than estimating a single grammar
from all training data, the aligned bitext is indexed
using a source-side suffix array (Manber and My-
ers, 1993). When an input sentence is to be trans-
lated, a grammar extraction program samples in-
stances of aligned phrase pairs from the suffix ar-
ray that match the source side of the sentence.
Using statistics from these samples rather than
the entire bitext, a sentence-specific grammar is
rapidly generated. In addition to speed gains from
sampling, indexing the source side of the bitext fa-
cilitates a more powerful feature set. Rules in on-
demand grammars are generated using a sample S
for each source phrase f in the input sentence. The
sample, containing pairs (f, e), is used to calculate
the following statistics:
</bodyText>
<figure confidence="0.865668733333333">
Hola contestadora ...
He llamado a servicio ...
Ignor´e la advertencia ...
Ahora anochece, ...
Todavia sigo en espera ...
No creo que me hayas ...
Ya he presionado cada ...
Incremental training data
Hello voicemail, ...
I’ve called for tech ...
I ignored my boss’ ...
Now it’s evening, and ...
I’m still on hold ...
I don’t think you ...
I punched every touch ...
</figure>
<page confidence="0.977811">
396
</page>
<table confidence="0.999686">
Feature Baseline Adaptive
coherent CS(f,e) CS(f,e)+CL(f,e)
p(e|f)
|S ||S |+ |L|
sample size |S ||S |+ |L|
co-occur- CS(f, e) CS(f, e)+CL(f, e)
rence (f, e)
singleton f CS(f) CS(f) + CL(f) =
= 1 1
singleton CS(f, e) CS(f, e) + CL(f, e)
(f, e) = 1 = 1
post-edit sup- 0 C�(f� e) &gt; 0
port (f, e)
</table>
<tableCaption confidence="0.856758">
Table 1: Phrase feature definitions for baseline and
adaptive translation models.
</tableCaption>
<listItem confidence="0.990494857142857">
• CS(f,e): count of instances in S where f
aligns to e (phrase co-occurrence count).
• CS(f): count of instances in S where f aligns
to any target phrase.
• |S|: total number of instances in S, equal to
number of occurrences of f in training data,
capped by the sample size limit.
</listItem>
<bodyText confidence="0.999660333333333">
These statistics are used to instantiate translation
rules X→(f, e) and calculate scores for the phrase
feature set shown in the “Baseline” column of Ta-
ble 1. Notably, the coherent phrase translation
probability that conditions on f occurring in the
data (|S|) rather than f being extracted as part of a
phrase pair (CS(f)) is shown by Lopez (2008b) to
yield significant improvement over the traditional
translation probability.
</bodyText>
<subsectionHeader confidence="0.998585">
3.2 Online Grammar Extraction
</subsectionHeader>
<bodyText confidence="0.992462142857143">
When a human translator post-edits MT output, a
new bilingual sentence pair is created. However,
in typical settings, it can be weeks or months be-
fore these training instances are incorporated into
bilingual data and models retrained. Our exten-
sion to on-demand grammar extraction incorpo-
rates these new training instances into the model
immediately. In addition to a static suffix array
that indexes initial data, our system maintains a
dynamic lookup table. Each new sentence pair is
word-aligned with the model estimated from the
initial data (a process often called forced align-
ment). This makes a generally insignificant ap-
proximation with respect to the original alignment
model. Extractable phrase pairs are stored in the
lookup table and phrase occurrences are counted
on the source side. When subsequent grammars
are extracted, the suffix array sample S for each
f is accompanied by an exhaustive lookup L from
the lookup table. Matching statistics are calculated
from L:
</bodyText>
<listItem confidence="0.997658666666667">
• CL(f,e): count of instances in L where f
aligns to e.
• CL(f): count of instances in L where f aligns
to any target phrase.
• |L|: total number of instances of f in post-
editing data (no size limit).
</listItem>
<bodyText confidence="0.998993782608696">
We use combined statistics from S and L to calcu-
late scores for the “Adaptive” feature set defined in
Table 1. In addition to updating existing features,
we introduce a new indicator feature that identi-
fies rules supported by post-editor feedback. Fur-
ther, our approach allows us to extract rules that
encode translations (phrase mappings and reorder-
ings) only observed in the incremental post-editing
data. This process, which can be seen as influ-
encing the distribution from which grammars are
sampled over time, produces comparable results
to the infeasible process of rebuilding the transla-
tion model after every sentence is translated with
the added benefit of allowing an optimizer to learn
a weight for the post-edited data via the post-edit
support feature. The simple aggregation of statis-
tics allows our model to handle initial and incre-
mental data in a formally consistent way. Further,
any additional features that can be calculated on a
suffix array sample can be matched by an incre-
mental data lookup, making our translation model
a viable platform for further exploration in online
learning for MT.
</bodyText>
<sectionHeader confidence="0.997151" genericHeader="method">
4 Language Model Adaptation
</sectionHeader>
<bodyText confidence="0.999989933333334">
Adapting language models in an online manner
based on the content they are generating has long
been seen as a promising technique for improving
automatic speech recognition and machine transla-
tion (Kuhn and de Mori, 1990; Zhao et al., 2004;
Sanchis-Trilles, 2012, inter alia). The post-editing
scenario we are considering simplifies this process
somewhat since rather than only having a poste-
rior distribution over machine-generated outputs
(any of which may be ungrammatical), the out-
puts, once edited by human translators, may be
presumed to be grammatical.
We thus take a novel approach to language
model adaptation, building on recent work show-
ing that state-of-the-art language models can be
</bodyText>
<page confidence="0.990637">
397
</page>
<bodyText confidence="0.999835342857142">
inferred as the posterior predictive distribution
of a Bayesian language model with hierarchi-
cal Pitman-Yor process priors, conditioned on the
training corpus (Teh, 2006). The Bayesian formu-
lation provides a natural way to incorporate pro-
gressively more data: by updating the posterior
distribution given subsequent observations. Fur-
thermore, the nonparametric nature of the model
means that the model is well suited to poten-
tially unbounded growth of vocabulary. Unfortu-
nately, in general, Bayesian techniques are com-
putationally difficult to work with. However, hi-
erarchical Pitman-Yor process language models
(HPYPLMs) are convenient in this regard since
(1) inference can be carried out efficiently in a
convenient collapsed representation (the “Chinese
restaurant franchise”) and (2) the posterior predic-
tive distribution from a single sample provides a
high quality language model.
We thus use the following procedure. Using
the target side of the bitext as observations, we
run the Gibbs sampling procedure described by
Teh (2006) for 100 iterations in a 3-gram HPY-
PLM. The inferred “seating configuration” defines
a posterior predictive distribution over words in 2-
gram contexts (as with any 3-gram LM) as well
as a posterior distribution over how the model will
generate subsequent observations. We use the for-
mer as a language model component of a transla-
tion model. And, as post-edited sentences become
available, we add their n-grams to the model us-
ing the later. We do not run any Gibbs sampling.
Just updating the language model in this way, we
obtain the results shown in Table 2 for the experi-
mental conditions described in §6.
</bodyText>
<sectionHeader confidence="0.859879" genericHeader="method">
5 Learning Feature Weights
</sectionHeader>
<bodyText confidence="0.999960733333333">
MT system parameter optimization (learning fea-
ture weights for the decoder) is also typically con-
ducted as a batch process. Discriminative learn-
ing techniques such as minimum error rate train-
ing (Och, 2003) are used to find feature weights
that maximize automatic metric score on a small
development corpus. The resulting weight vector
is then used to decode given input sentences. Us-
ing this approach with post-editing tasks presents
two major issues. First, reference translation are
only considered after all sentences are translated,
a mismatch with post-editing where references are
available incrementally. Second, despite the fact
that adaptive feature sets become more powerful
as post-editing data increases, an optimizer must
</bodyText>
<table confidence="0.999614888888889">
Spanish–English WMT10 WMT11 TED1 TED2
HPYPLM 25.5 24.8 29.4 26.6
+data 25.8 25.2 29.5 27.0
English–Spanish WMT10 WMT11 TED1 TED2
HPYPLM 25.1 26.8 26.0 24.3
+data 25.4 27.2 26.2 25.0
Arabic–English MT08 MT09 TED1 TED2
HPYPLM 19.3 24.7 9.5 10.0
+data 19.6 24.9 9.8 10.5
</table>
<tableCaption confidence="0.988403">
Table 2: BLEU scores for systems with trigram
</tableCaption>
<bodyText confidence="0.979014705882353">
HPYPLM (no large language model), with and
without incremental updates from simulated post-
editing data. Scores are averages over 3 optimizer
runs. Bold scores indicate statistically significant
improvement. Tuning set scores are italicized.
learn a single corpus-level weight for each fea-
ture. This forces an averaging effect that can lead
to decoding individual sentences with suboptimal
weights. We address the first issue by using ref-
erence translations to simulate post-editing (Hardt
and Elming, 2010) at tuning time and the second
by using a version of the margin-infused relaxed
algorithm (Crammer et al., 2006; Eidelman, 2012)
to make online parameter updates during decod-
ing. The result is a consistent approach to tuning
and decoding that brings out the potential of adap-
tive models.
</bodyText>
<subsectionHeader confidence="0.968803">
5.1 Parameter Optimization
</subsectionHeader>
<bodyText confidence="0.99998165">
In order to make our decoding process fully con-
sistent with tuning, we introduce an online dis-
criminative parameter update that allows our adap-
tive translation and language models be weighted
appropriately as more data is available. This re-
quires an optimization algorithm that can func-
tion as an online learner during decoding as well
as a batch optimizer during tuning. Popular opti-
mizers such as MERT (Och, 2003) and pairwise
rank optimization (Hopkins and May, 2011) can-
not be used due to their reliance on corpus-level
optimization. We select the cutting-plane variant
of the margin-infused relaxed algorithm (Chiang,
2012; Crammer et al., 2006) with additional exten-
sions described by Eidelman (2012). MIRA is an
online large-margin learner that makes a param-
eter update after each model prediction with the
objective of choosing the correct output over the
incorrect output by a margin at least as large as the
cost of predicting the incorrect output. Applied
</bodyText>
<page confidence="0.99458">
398
</page>
<bodyText confidence="0.999646095238095">
to MT system optimization on a development cor-
pus, MIRA proceeds as follows. The MT system
generates a list of the k best translations for a sin-
gle input sentence. From the list, a “hope” hy-
pothesis is selected as a translation with both high
model score and high automatic metric score. A
“fear” hypothesis is selected as a translation with
high model score but low metric score. Parameters
are updated away from the fear hypothesis, toward
the hope hypothesis, and the system processes the
next input sentence. This process continues for a
set number of passes over the development corpus.
All adaptive systems used in our work are opti-
mized with this variant of MIRA using the param-
eter settings described by Eidelman (2012). For
each pass over the data, translation and language
models have incremental access to reference trans-
lations (simulated post-editing data) as input sen-
tences are translated. Translation and language
models reset to using background data only at the
beginning of each MIRA iteration.2
</bodyText>
<subsectionHeader confidence="0.996452">
5.2 Online Parameter Updates
</subsectionHeader>
<bodyText confidence="0.999990380952381">
Our optimization strategy allows us to treat de-
coding as if it were simply the next iteration of
MIRA (or alternatively that MIRA makes a single
pass over an input corpus that consists of the de-
velopment data concatenated n times followed by
unseen input data). After each sentence is trans-
lated, a reference translation (resulting from ac-
tual human post-editing in production or simulated
post-editing for our experiments) is provided to
the models and MIRA makes a parameter update.
In the only departure from our optimization setup,
we decrease the maximum step size for MIRA (de-
scribed in §6.2), effectively increasing regulariza-
tion strength. This allows us to prefer small ad-
justments to already optimized decoding parame-
ters over the large changes needed during tuning.
It is also important to note that by using MIRA
for updating weights during both tuning and de-
coding, we avoid scaling issues between multiple
optimizers (such as when tuning with MERT and
updating with a passive-aggressive algorithm).
</bodyText>
<sectionHeader confidence="0.999781" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.685725857142857">
We evaluate our online extensions to standard
machine translation systems in a series of sim-
2Resetting translation and language models prevents con-
tamination. If models retained state from previous passes
over the development set, they would include data for input
sentences before they were translated, rather than after as in
post-editing.
</bodyText>
<table confidence="0.999975714285714">
Spanish–English WMT10 WMT11 TED1 TED2
Base MERT 29.1 27.9 32.8 29.6
Base MIRA 29.2 28.0 32.7 29.7
G 29.8 28.3 34.2 30.7
L 29.2 28.1 33.0 29.8
M 29.2 28.1 33.1 29.8
G+L+M 30.0 28.8 35.2 31.3
English–Spanish WMT10 WMT11 TED1 TED2
Base MERT 27.8 29.4 26.5 25.7
Base MIRA 27.7 29.6 26.8 26.7
G 28.1 29.8 27.9 27.5
L 27.9 29.7 26.8 26.5
M 27.9 29.7 27.2 26.6
G+L+M 28.4 30.4 28.6 27.9
Arabic–English MT08 MT09 TED1 TED2
Base MERT 21.5 25.0 10.4 10.5
Base MIRA 21.2 25.9 10.6 10.9
G 21.8 26.2 11.0 11.7
L 20.6 25.7 10.6 10.9
M 21.3 25.7 10.8 11.0
G+L+M 21.8 26.5 11.4 11.8
</table>
<tableCaption confidence="0.995347">
Table 3: BLEU scores for baseline and adap-
</tableCaption>
<bodyText confidence="0.964649166666667">
tive systems. Scores are averages over three opti-
mizer runs. Highest scores are bold and tuning set
scores are italicized. All fully adaptive systems
(G+L+M) show statistically significant improve-
ment over both MERT and MIRA baselines.
ulated post-editing experiments that cover high-
traffic languages and challenging domains. We
show incremental improvement from our adaptive
models and significantly larger gains when pair-
ing our models with an online parameter update.
We finally validate our adaptive system on actual
post-edited data.
</bodyText>
<subsectionHeader confidence="0.988415">
6.1 Data
</subsectionHeader>
<bodyText confidence="0.999900214285714">
We conduct a series of simulated post-editing
experiments in three full scale language sce-
narios: Spanish–English, English–Spanish, and
Arabic–English. Spanish–English and English–
Spanish systems are trained on the 2012 NAACL
WMT (Callison-Burch et al., 2012) constrained
resources (2 million bilingual sentences, 300 mil-
lion words of monolingual Spanish, and 1.1 billion
words of monolingual English). Arabic–English
systems are trained on the 2012 NIST OpenMT
(Przybocki, 2012) constrained bilingual resources
plus a selection from the English Gigaword cor-
pus (Parker et al., 2011) (5 million bilingual sen-
tences and 650 million words of monolingual En-
</bodyText>
<page confidence="0.996009">
399
</page>
<bodyText confidence="0.99995825">
glish). We tune and evaluate on standard news
sets: WMT10 and WMT11 for Spanish–English
and English–Spanish, and MT08 and MT09 for
Arabic–English. To simulate real-world post edit-
ing where one translator works on a document at a
time, we use only one of the four available refer-
ence translation sets for MT08 and MT09.
We also evaluate on a blind domain adapta-
tion scenario that mimics the demands placed
on MT systems in real-world translation tasks.
The Web Inventory of Transcribed and Translated
Talks (WITS) corpus (Cettolo et al., 2012) makes
transcriptions of TED talks3 available in several
languages, including English, Spanish, and Ara-
bic. For each language pair, we select two sets of
10 talk transcripts each (2000-3000 sentences) as
blind evaluation sets. These sets consist of spoken
language covering a broad range of topics. Sys-
tems have no access to any training or develop-
ment data in this domain prior to translation.
</bodyText>
<subsectionHeader confidence="0.999192">
6.2 Translation Systems
</subsectionHeader>
<bodyText confidence="0.99995837037037">
For each language scenario, we first construct a
competitive baseline system. Bilingual data is
word aligned using the model described by Dyer
et al. (2013) and suffix array-backed transla-
tion grammars are extracted using the method
described by Lopez (2008a). We add the stan-
dard lexical and derivation features4 from Lopez
(2008b) and Dyer et al. (2010). An unpruned,
modified Kneser-Ney-smoothed 4-gram language
model is estimated using the KenLM toolkit
(Heafield et al., 2013). Feature weights are op-
timized using the lattice-based variant of MERT
(Macherey et al., 2008; Och, 2003) on either
WMT10 or MT08. Evaluation sets are translated
using the cdec decoder (Dyer et al., 2010) and
evaluated with the BLEU metric (Papineni et al.,
2002). These results are listed as “Base MERT”
in Table 3. To establish a baseline for our adap-
tive systems, we tune the same baseline system
using cutting-plane MIRA with 500-best lists, the
pseudo-document approximation described by Ei-
delman (2012), and a maximum update size of
0.01. We begin with uniform weights and make
20 passes over the development corpus. Results
for this system are listed as “Base MIRA”.
To evaluate the impact of each online model
adaptation technique, we report the results for the
</bodyText>
<footnote confidence="0.994731">
3http://www.ted.com/talks
4Derivation features consist of word count, discretized
rule-level non-terminal count (0, 1, or 2), glue rule count,
and out-of-vocabulary pass-through count.
</footnote>
<table confidence="0.9974938">
News TED Talks
New Supp New Supp
Spanish–English 15% 19% 14% 18%
English–Spanish 12% 16% 9% 13%
Arabic–English 9% 12% 23% 28%
</table>
<tableCaption confidence="0.75961">
Table 5: Percentages of new rules (only seen
in incremental data) and post-edit supported rules
(Rules from all data for which the “post-edit sup-
port (f, e)” feature fires) in grammars by domain.
</tableCaption>
<bodyText confidence="0.851173">
following systems in Table 3:
</bodyText>
<listItem confidence="0.904232">
• G: Baseline MIRA system with online gram-
mar extraction, including incrementally up-
dating existing phrase features plus an addi-
tional indicator feature for post-edit support.
• L: Baseline MIRA with a trigram hierarchi-
cal Pitman-Yor process language model that
is incrementally updated, including a sepa-
rate out-of-vocabulary feature.
• M: Baseline MIRA with online feature
weight updates from cutting-plane MIRA.
</listItem>
<bodyText confidence="0.839352">
Finally, we report results for a fully adaptive
system that includes online grammar, language
model, and feature weight updates. This system
is reported as “G+L+M”. To account for optimizer
instability, all systems are tuned (consisting of
running either MERT or MIRA) and evaluated 3
times. We report average scores over optimizer
runs and conduct statistical significance tests us-
ing the methods described by Clark et al. (2011).
</bodyText>
<subsectionHeader confidence="0.748893">
6.3 Results
</subsectionHeader>
<bodyText confidence="0.999963157894737">
Our simulated translation post-editing experi-
ments are summarized in Table 3. Simply mov-
ing from MERT to cutting-plane MIRA for pa-
rameter optimization yields improvement in most
cases, corroborating existing work (Eidelman,
2012). Using incremental post-editing data to up-
date translation grammars (G) yields further im-
provement in all cases evaluated. Gains are signif-
icantly larger for TED talks where translator feed-
back can bridge the gap between domains. Table 5
shows the aggregate percentages of rules in online
grammars that are entirely new (extracted from
post-editing instances only) or post-edit supported
(superset of new rules). While percentages vary
by data set, the overall trend is a combination of
learning new vocabulary and reordering and dis-
ambiguating existing translation choices.
The introduction of a trigram Bayesian lan-
guage model (L) yields mixed results: in some
</bodyText>
<page confidence="0.992083">
400
</page>
<note confidence="0.796288666666667">
Base MERT and changing the definition of what the Zona Cero is .
G+L+M and the changing definition of what the Ground Zero is .
Reference and the changing definition of what Ground Zero is .
</note>
<table confidence="0.963531333333333">
Base MERT was that when we side by side comparisons with coal, timber
G+L+M was that when we did side-by-side comparisons with wood charcoal,
Reference was when we did side-by-side comparisons with wood charcoal,
Base MERT There was a way – there was one –
G+L+M There was a way – there had to be a way –
Reference There was a way – there had to be a way –
</table>
<tableCaption confidence="0.984341">
Table 4: Translation examples from baseline and fully adaptive systems of Spanish TED talks into En-
</tableCaption>
<bodyText confidence="0.989580616666667">
glish. Examples illustrate (from top to bottom) learning translations for new vocabulary items, selecting
correct translation candidates for the domain, and learning domain-appropriate phrasing.
cases it leads to slight improvement and in oth-
ers, degradation. It appears that a static but large
4-gram language model often outperforms an in-
crementally updated but smaller trigram model.
Further, learning a single weight for the Bayesian
model can lead to a harmful mismatch. As a tun-
ing pass over the development corpus proceeds,
the model incorporates additional data and MIRA
learns a weight corresponding to its predictive
ability at the end of the corpus. During decod-
ing, all sentences are translated with this language
model weight, even before the model can ade-
quately adapt itself to the target domain. This
problem is alleviated in our fully adaptive system.
Using cutting-plane MIRA to incrementally up-
date weights during decoding (M) also leads to
mixed results, frequently resulting in both small
increases and decreases in score. This could be
due to the noise incurred when making small ad-
justments to static features after each sentence:
depending on the similarity between the previous
and current sentence and the limit of the step size
(regularization strength), a parameter update may
slightly improve or degrade translation.
Finally, we see significantly larger gains for
our fully adaptive system (G+L+M) that com-
bines adaptive translation grammars and language
models with online parameter updates. In many
cases, the difference between the baseline sys-
tems and our adaptive system is greater than the
sum of the differences from our individual tech-
niques, demonstrating the effectiveness of com-
bining online learning methods. Our final sys-
tem has two key advantages over any individual
extension. First, incremental updates from MIRA
can rescale weights for features that change over
time, keeping the model consistent. Second, the
Bayesian language model’s out-of-vocabulary fea-
ture can discriminate between true OOV items
and vocabulary items in the post-editing data not
present in the monolingual data. By contrast, the
only OOVs in the baseline system are untranslated
items, as the target side of the bitext is included in
the language model training data. This interplay
between the adaptive components in our transla-
tion system leads to significant gains over MERT
and MIRA baselines. Table 4 contains examples
from our system’s output that exemplify key im-
provements in translation quality. With respect to
performance, our fully adaptive system translates
an average of 1.5 sentences per second per CPU
core. The additional cost incurred updating trans-
lation grammars and language models is less than
one second per sentence (though the baseline cost
of on-demand grammar extraction can be up to a
few seconds). In total, the system is well within
the acceptable speed range needed to function in
real-time human translation scenarios.
</bodyText>
<subsectionHeader confidence="0.998329">
6.4 Evaluation Using Post-Edited References
</subsectionHeader>
<bodyText confidence="0.99982175">
The 2012 ACL Workshop on Machine Translation
(Callison-Burch et al., 2012) makes available a set
of 1832 English–Spanish parallel news source sen-
tences, independent references, initial MT outputs,
and post-edited MT outputs. The employed MT
system is trained on largely the same resources as
our own English–Spanish system, granting the op-
portunity for a much closer approximation to an
actual post-editing task; our system configurations
score between 54 and 56 BLEU against the sam-
ple MT, indicating that humans post-edited trans-
lations similar but not identical to our own. We
split the data into development and test sets, each
916 sentences, and run 3 iterations of optimizing
on the development set and evaluating on the test
set with both the MERT baseline and our G+L+M
</bodyText>
<page confidence="0.997284">
401
</page>
<bodyText confidence="0.9998858">
system on both types of references. Using inde-
pendent references for tuning and evaluation (as
before), our system yields an improvement of 0.6
BLEU (23.3 to 23.9). With post-edited references,
our system yields an improvement of 1.3 BLEU
(43.0 to 44.3). This provides strong evidence that
our adaptive systems would provide better trans-
lations (both in terms of absolute quality and im-
provement over a standard baseline) for real-world
post-editing scenarios.
</bodyText>
<sectionHeader confidence="0.999775" genericHeader="method">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999930078947369">
Prior work has led to the extension of standard
phrase-based translation systems to make use of
incrementally available data.5 Approaches gen-
erally fall into categories of adding new data to
translation models and of using incremental data
to adjust model parameters (feature weights). In
the first case, Nepveu et al. (2004) use cache-based
translation and language models to incorporate
data from the current document into a computer-
aided translation scenario. Ortiz-Martinez et al.
(2010) augment a standard translation model by
storing sufficient statistics in addition to feature
scores for phrase pairs, allowing feature values to
be incrementally updated as new sentence pairs
are available for phrase extraction. Hardt and Elm-
ing (2010) demonstrate the benefit of maintain-
ing a distinction between background and post-
editing data in an adaptive model with simulated
post-editing. Though not targeted at post-editing
applications, the most similar work to our online
grammar adaptation is the stream-based transla-
tion model described by Levenberg et al. (2010).
The authors introduce a dynamic suffix array that
can incorporate new training text as it becomes
available. Sanchis-Trilles (2012) proposes a strat-
egy for online language model adaptation wherein
several smaller domain-specific models are built
and their scores interpolated for each sentence
translated based on the target domain.
Focusing on incrementally updating model pa-
rameters with post-editing data, Mart´ınez-G´omez
et al. (2012) and L´opez-Salcedo et al. (2012)
show improvement under some conditions when
using techniques including passive-aggressive al-
gorithms, perceptron, and discriminative ridge re-
gression to adapt feature weights for systems ini-
tially tuned using MERT. This work also uses ref-
erence translations to simulate post-editing. Saluja
</bodyText>
<footnote confidence="0.9108355">
5Prior to phrase-based systems, NISHIDA et al. (1988)
use post-editing data to correct errors in transfer-based MT.
</footnote>
<bodyText confidence="0.999949466666667">
et al. (2012) introduce a support vector machine-
based algorithm capable of learning from binary-
labeled examples. This learning algorithm is used
to incrementally adjust feature weights given user
feedback on whether a translation is “good” or
“bad”. As with our work, this strategy can be used
during both optimization and decoding.
Finally, Simard and Foster (2013) apply a
pipeline solution to the post-editing task wherein
a second stage automatic post-editor (APE) sys-
tem learns to replicate the corrections made to ini-
tial MT output by human translators. As incre-
mental data accumulates, the APE (itself a statisti-
cal phrase-based system) attempts to “correct” the
MT output before it is shown to humans.
</bodyText>
<sectionHeader confidence="0.998353" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.99989716">
Casting machine translation for post-editing as
an online learning task, we have presented three
methods for incremental model adaptation: adding
data to the indexed bitext from which gram-
mars are extracted, updating a Bayesian language
model with incremental data, and using an on-
line discriminative parameter update during de-
coding. These methods, which allow the sys-
tem to handle all data in a uniform way, are ap-
plied to a strong baseline system optimized using
MIRA in conjunction with simulated post-editing.
In addition to showing gains for individual meth-
ods under various circumstances, we report super-
additive improvement from combining our tech-
niques to produce a fully adaptive system. Im-
provements generalize over language and data sce-
narios, with the greatest gains realized in blind
out-of-domain tasks where the system must rely
heavily on post-editor feedback to improve qual-
ity. Gains are also more significant when using of-
fline post-edited references, showing promise for
applying our techniques to real-world post-editing
tasks. All software used for our online model
adaptation experiments is freely available under an
open source license as part of the cdec toolkit.6
</bodyText>
<sectionHeader confidence="0.996521" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.995185">
This work is supported in part by the National Sci-
ence Foundation under grant IIS-0915327, by the
Qatar National Research Fund (a member of the
Qatar Foundation) under grant NPRP 09-1140-1-
177, and by the NSF-sponsored XSEDE program
under grant TG-CCR110017.
</bodyText>
<footnote confidence="0.9826715">
6http://www.cs.cmu.edu/˜mdenkows/
cdec-realtime.html
</footnote>
<page confidence="0.995972">
402
</page>
<sectionHeader confidence="0.97291" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997600342342342">
[Callison-Burch et al.2012] Chris Callison-Burch,
Philipp Koehn, Christof Monz, Matt Post, Radu
Soricut, and Lucia Specia. 2012. Findings of the
2012 workshop on statistical machine translation.
In Proceedings of the Seventh Workshop on Statis-
tical Machine Translation, pages 10–51, Montr´eal,
Canada, June. Association for Computational
Linguistics.
[Carl et al.2011] Michael Carl, Barbara Dragsted,
Jakob Elming, Daniel Hardt, and Arnt Lykke
Jakobsen. 2011. The process of post-editing: A
pilot study. Copenhagen Studies in Language,
41:131–142.
[Cettolo et al.2012] Mauro Cettolo, Christian Girardi,
and Marcello Federico. 2012. Wit3: Web inventory
of transcribed and translated talks. In Proceedings
of the Sixteenth Annual Conference of the European
Association for Machine Translation.
[Chiang2007] David Chiang. 2007. Hierarchical
phrase-based translation. Computational Linguis-
tics, 33.
[Chiang2012] David Chiang. 2012. Hope and fear for
discriminative training of statistical translation mod-
els. Journal of Machine Learning Research, pages
1159–1187, April.
[Clark et al.2011] Jonathan H. Clark, Chris Dyer, Alon
Lavie, and Noah A. Smith. 2011. Better hypothe-
sis testing for statistical machine translation: Con-
trolling for optimizer instability. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 176–181, Portland, Oregon, USA, June.
Association for Computational Linguistics.
[Crammer et al.2006] Koby Crammer, Ofer Dekel,
Joseph Keshet, Shai Shalev-Shwartz, and Yoram
Singer. 2006. Online passive-aggressive algo-
rithms. Journal of Machine Learning Research,
pages 551–558, March.
[Dyer et al.2010] Chris Dyer, Adam Lopez, Juri Gan-
itkevitch, Jonathan Weese, Ferhan Ture, Phil Blun-
som, Hendra Setiawan, Vladimir Eidelman, and
Philip Resnik. 2010. cdec: A decoder, alignment,
and learning framework for finite-state and context-
free translation models. In Proceedings of the ACL
2010 System Demonstrations, pages 7–12, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
[Dyer et al.2013] Chris Dyer, Victor Chahuneau, and
Noah A. Smith. 2013. A simple, fast, and effective
reparameterization of IBM model 2. In The 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies.
[Eidelman2012] Vladimir Eidelman. 2012. Optimiza-
tion strategies for online large-margin learning in
machine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
480–489, Montr´eal, Canada, June. Association for
Computational Linguistics.
[Guerberof2009] Ana Guerberof. 2009. Productivity
and quality in mt post-editing. In Proceedings of MT
Summit XII - Workshop: Beyond Translation Memo-
ries: New Tools for Translators MT.
[Hardt and Elming2010] Daniel Hardt and Jakob Elm-
ing. 2010. Incremental re-training for post-editing
smt. In Proceedings of the Ninth Conference of the
Association for Machine Translation in the Ameri-
cas.
[Heafield et al.2013] Kenneth Heafield, Ivan
Pouzyrevsky, Jonathan H. Clark, and Philipp
Koehn. 2013. Scalable modified Kneser-Ney
language model estimation. In Proceedings of
the 51st Annual Meeting of the Association for
Computational Linguistics, Sofia, Bulgaria, August.
[Hopkins and May2011] Mark Hopkins and Jonathan
May. 2011. Tuning as ranking. In Proceedings of
the 2011 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1352–1362, Edin-
burgh, Scotland, UK., July. Association for Compu-
tational Linguistics.
[Koehn2012] Philipp Koehn. 2012. Computer-aided
translation. Machine Translation Marathon.
[Kuhn and de Mori1990] Roland Kuhn and Renato
de Mori. 1990. A cache-based natural language
model for speech recognition. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
12(6).
[Levenberg et al.2010] Abby Levenberg, Chris
Callison-Burch, and Miles Osborne. 2010.
Stream-based translation models for statistical
machine translation. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 394–402, Los Angeles,
California, June. Association for Computational
Linguistics.
[Lopez2008a] Adam Lopez. 2008a. Machine transla-
tion by pattern matching. In Dissertation, Univer-
sity of Maryland, March.
[Lopez2008b] Adam Lopez. 2008b. Tera-scale transla-
tion models via pattern matching. In Proceedings
of the 22nd International Conference on Compu-
tational Linguistics (Coling 2008), pages 505–512,
Manchester, UK, August. Coling 2008 Organizing
Committee.
[L´opez-Salcedo et al.2012] Francisco-Javier L´opez-
Salcedo, Germ´an Sanchis-Trilles, and Francisco
Casacuberta. 2012. Online learning of log-linear
weights in interactive machine translation. Ad-
vances in Speech and Language Technologies for
Iberian Languages, pages 277–286.
</reference>
<page confidence="0.990548">
403
</page>
<reference confidence="0.999309068181818">
[Macherey et al.2008] Wolfgang Macherey, Franz Och,
Ignacio Thayer, and Jakob Uszkoreit. 2008. Lattice-
based minimum error rate training for statistical ma-
chine translation. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 725–734, Honolulu, Hawaii, Oc-
tober. Association for Computational Linguistics.
[Manber and Myers1993] Udi Manber and Gene My-
ers. 1993. Suffix arrays: A new method for on-
line string searches. SIAM Journal of Computing,
22:935–948.
[Martinez-G´omez et al.2012] Pascual Martinez-
G´omez, Germ´an Sanchis-Trilles, and Francisco
Casacuberta. 2012. Online adaptation strategies
for statistical machine translation in post-editing
scenarios. Pattern Recognition, 45:3193–3203.
[Nepveu et al.2004] Laurent Nepveu, Guy Lapalme,
Philippe Langlais, and George Foster. 2004. Adap-
tive language and translation models for interactive
machine translation. In Dekang Lin and Dekai Wu,
editors, Proceedings of EMNLP 2004, pages 190–
197, Barcelona, Spain, July. Association for Com-
putational Linguistics.
[NISHIDA et al.1988] Fujio NISHIDA, Shinobu
TAKAMATSU, Tadaaki TANI, and Tsunehisa
DOI. 1988. Feedback of correcting information
in postediting to a machine translation system. In
Proc. of COLING.
[Och2003] Franz Josef Och. 2003. Minimum error rate
training in statistical machine translation. In Pro-
ceedings of the 41st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 160–167,
Sapporo, Japan, July. Association for Computational
Linguistics.
[Ortiz-Martinez et al.2010] Daniel Ortiz-Martinez, Is-
mael Garcia-Varea, and Francisco Casacuberta.
2010. Online learning for interactive statistical ma-
chine translation. In Human Language Technolo-
gies: The 2010 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 546–554, Los Ange-
les, California, June. Association for Computational
Linguistics.
[Papineni et al.2002] Kishore Papineni, Salim Roukos,
Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a
method for automatic evaluation of machine trans-
lation. In Proceedings of 40th Annual Meeting
of the Association for Computational Linguistics,
pages 311–318, Philadelphia, Pennsylvania, USA,
July. Association for Computational Linguistics.
[Parker et al.2011] Robert Parker, David Graff, Junbo
Kong, Ke Chen, and Kazuaki Maeda. 2011. En-
glish Gigaword Fifth Edition, June. Linguistic Data
Consortium, LDC2011T07.
[Przybocki2012] Mark Przybocki. 2012. Nist open
machine translation 2012 evaluation (openmt12).
http://www.nist.gov/itl/iad/mig/openmt12.cfm.
[Saluja et al.2012] Avneesh Saluja, Ian Lane, and Ying
Zhang. 2012. Machine translation with binary feed-
back: a large-margin approach. In Proceedings of
the Tenth Biennial Conference of the Association for
Machine Translation in the Americas.
[Sanchis-Trilles2012] Germ´an Sanchis-Trilles. 2012.
Building task-oriented machine translation systems.
In Ph.D. Thesis, Universitat Politcnica de Valncia.
[Simard and Foster2013] Michel Simard and George
Foster. 2013. PEPr: Post-edit propagation using
phrase-based statistical machine translation. In Pro-
ceedings of the XIV Machine Translation Summit,
pages 191–198,, September.
[Tatsumi2009] Midori Tatsumi. 2009. Correlation
between automatic evaluation metric scores, post-
editing speed, and some other factors. In Proceed-
ings of the Twelfth Machine Translation Summit.
[Teh2006] Yee Whye Teh. 2006. A hierarchical
Bayesian language model based on Pitman-Yor pro-
cesses. In Proc. of ACL.
[Zhao et al.2004] Bing Zhao, Matthias Eck, and
Stephan Vogel. 2004. Language model adaptation
for statistical machine translation with structured
query models. In Proc. of COLING.
[Zhechev2012] Ventsislav Zhechev. 2012. Machine
Translation Infrastructure and Post-editing Perfor-
mance at Autodesk. In AMTA 2012 Workshop
on Post-Editing Technology and Practice (WPTP
2012), pages 87–96, San Diego, USA, October. As-
sociation for Machine Translation in the Americas
(AMTA).
</reference>
<page confidence="0.999013">
404
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.687915">
<title confidence="0.9995605">Learning from Post-Editing: Online Model Adaptation for Statistical Machine Translation</title>
<author confidence="0.984929">Michael Denkowski Chris Dyer Alon</author>
<affiliation confidence="0.8455385">Language Technologies Carnegie Mellon</affiliation>
<address confidence="0.999436">Pittsburgh, PA 15213</address>
<abstract confidence="0.9997472">Using machine translation output as a starting point for human translation has become an increasingly common application of MT. We propose and evaluate three computationally efficient online methods for updating statistical MT systems in a scenario where post-edited MT output is constantly being returned to the system: (1) adding new rules to the translation model from the post-edited content, (2) updating a Bayesian language model of the target language that is used by the MT system, and (3) updating the MT system’s discriminative parameters with a MIRA step. Individually, these techniques can substantially improve MT quality, even over strong baselines. Moreover, we see super-additive improvements when all three techniques are used in tandem.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<title>Findings of the 2012 workshop on statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>10--51</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<marker>[Callison-Burch et al.2012]</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 workshop on statistical machine translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 10–51, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Carl</author>
<author>Barbara Dragsted</author>
<author>Jakob Elming</author>
<author>Daniel Hardt</author>
<author>Arnt Lykke Jakobsen</author>
</authors>
<title>The process of post-editing: A pilot study.</title>
<date>2011</date>
<booktitle>Copenhagen Studies in Language,</booktitle>
<pages>41--131</pages>
<marker>[Carl et al.2011]</marker>
<rawString>Michael Carl, Barbara Dragsted, Jakob Elming, Daniel Hardt, and Arnt Lykke Jakobsen. 2011. The process of post-editing: A pilot study. Copenhagen Studies in Language, 41:131–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mauro Cettolo</author>
<author>Christian Girardi</author>
<author>Marcello Federico</author>
</authors>
<title>Wit3: Web inventory of transcribed and translated talks.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixteenth Annual Conference of the European Association for Machine Translation.</booktitle>
<marker>[Cettolo et al.2012]</marker>
<rawString>Mauro Cettolo, Christian Girardi, and Marcello Federico. 2012. Wit3: Web inventory of transcribed and translated talks. In Proceedings of the Sixteenth Annual Conference of the European Association for Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<marker>[Chiang2007]</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hope and fear for discriminative training of statistical translation models.</title>
<date>2012</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>1159--1187</pages>
<marker>[Chiang2012]</marker>
<rawString>David Chiang. 2012. Hope and fear for discriminative training of statistical translation models. Journal of Machine Learning Research, pages 1159–1187, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>176--181</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<marker>[Clark et al.2011]</marker>
<rawString>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 176–181, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>551--558</pages>
<marker>[Crammer et al.2006]</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online passive-aggressive algorithms. Journal of Machine Learning Research, pages 551–558, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Jonathan Weese</author>
<author>Ferhan Ture</author>
<author>Phil Blunsom</author>
<author>Hendra Setiawan</author>
<author>Vladimir Eidelman</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finite-state and contextfree translation models.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 System Demonstrations,</booktitle>
<pages>7--12</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<marker>[Dyer et al.2010]</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finite-state and contextfree translation models. In Proceedings of the ACL 2010 System Demonstrations, pages 7–12, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Victor Chahuneau</author>
<author>Noah A Smith</author>
</authors>
<title>A simple, fast, and effective reparameterization of IBM model 2.</title>
<date>2013</date>
<booktitle>In The 2013 Conference of the North American Chapter of</booktitle>
<marker>[Dyer et al.2013]</marker>
<rawString>Chris Dyer, Victor Chahuneau, and Noah A. Smith. 2013. A simple, fast, and effective reparameterization of IBM model 2. In The 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Eidelman</author>
</authors>
<title>Optimization strategies for online large-margin learning in machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>480--489</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<marker>[Eidelman2012]</marker>
<rawString>Vladimir Eidelman. 2012. Optimization strategies for online large-margin learning in machine translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 480–489, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana Guerberof</author>
</authors>
<title>Productivity and quality in mt post-editing.</title>
<date>2009</date>
<booktitle>In Proceedings of MT Summit XII - Workshop: Beyond Translation Memories: New Tools for Translators MT.</booktitle>
<marker>[Guerberof2009]</marker>
<rawString>Ana Guerberof. 2009. Productivity and quality in mt post-editing. In Proceedings of MT Summit XII - Workshop: Beyond Translation Memories: New Tools for Translators MT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Hardt</author>
<author>Jakob Elming</author>
</authors>
<title>Incremental re-training for post-editing smt.</title>
<date>2010</date>
<booktitle>In Proceedings of the Ninth Conference of the Association for Machine Translation in the Americas.</booktitle>
<marker>[Hardt and Elming2010]</marker>
<rawString>Daniel Hardt and Jakob Elming. 2010. Incremental re-training for post-editing smt. In Proceedings of the Ninth Conference of the Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
<author>Ivan Pouzyrevsky</author>
<author>Jonathan H Clark</author>
<author>Philipp Koehn</author>
</authors>
<title>Scalable modified Kneser-Ney language model estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Sofia, Bulgaria,</location>
<marker>[Heafield et al.2013]</marker>
<rawString>Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H. Clark, and Philipp Koehn. 2013. Scalable modified Kneser-Ney language model estimation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1352--1362</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<marker>[Hopkins and May2011]</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1352–1362, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Computer-aided translation. Machine Translation Marathon.</title>
<date>2012</date>
<marker>[Koehn2012]</marker>
<rawString>Philipp Koehn. 2012. Computer-aided translation. Machine Translation Marathon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roland Kuhn</author>
<author>Renato de Mori</author>
</authors>
<title>A cache-based natural language model for speech recognition.</title>
<date>1990</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>12</volume>
<issue>6</issue>
<marker>[Kuhn and de Mori1990]</marker>
<rawString>Roland Kuhn and Renato de Mori. 1990. A cache-based natural language model for speech recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 12(6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abby Levenberg</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
</authors>
<title>Stream-based translation models for statistical machine translation.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>394--402</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<marker>[Levenberg et al.2010]</marker>
<rawString>Abby Levenberg, Chris Callison-Burch, and Miles Osborne. 2010. Stream-based translation models for statistical machine translation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 394–402, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Machine translation by pattern matching.</title>
<date>2008</date>
<institution>In Dissertation, University of Maryland,</institution>
<marker>[Lopez2008a]</marker>
<rawString>Adam Lopez. 2008a. Machine translation by pattern matching. In Dissertation, University of Maryland, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Tera-scale translation models via pattern matching.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>505--512</pages>
<location>Manchester, UK,</location>
<marker>[Lopez2008b]</marker>
<rawString>Adam Lopez. 2008b. Tera-scale translation models via pattern matching. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 505–512, Manchester, UK, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francisco-Javier L´opezSalcedo</author>
<author>Germ´an Sanchis-Trilles</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Online learning of log-linear weights in interactive machine translation.</title>
<date>2012</date>
<booktitle>Advances in Speech and Language Technologies for Iberian Languages,</booktitle>
<pages>277--286</pages>
<marker>[L´opez-Salcedo et al.2012]</marker>
<rawString>Francisco-Javier L´opezSalcedo, Germ´an Sanchis-Trilles, and Francisco Casacuberta. 2012. Online learning of log-linear weights in interactive machine translation. Advances in Speech and Language Technologies for Iberian Languages, pages 277–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Macherey</author>
<author>Franz Och</author>
<author>Ignacio Thayer</author>
<author>Jakob Uszkoreit</author>
</authors>
<title>Latticebased minimum error rate training for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>725--734</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<marker>[Macherey et al.2008]</marker>
<rawString>Wolfgang Macherey, Franz Och, Ignacio Thayer, and Jakob Uszkoreit. 2008. Latticebased minimum error rate training for statistical machine translation. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 725–734, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Udi Manber</author>
<author>Gene Myers</author>
</authors>
<title>Suffix arrays: A new method for online string searches.</title>
<date>1993</date>
<journal>SIAM Journal of Computing,</journal>
<pages>22--935</pages>
<marker>[Manber and Myers1993]</marker>
<rawString>Udi Manber and Gene Myers. 1993. Suffix arrays: A new method for online string searches. SIAM Journal of Computing, 22:935–948.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascual MartinezG´omez</author>
<author>Germ´an Sanchis-Trilles</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Online adaptation strategies for statistical machine translation in post-editing scenarios. Pattern Recognition,</title>
<date>2012</date>
<pages>45--3193</pages>
<marker>[Martinez-G´omez et al.2012]</marker>
<rawString>Pascual MartinezG´omez, Germ´an Sanchis-Trilles, and Francisco Casacuberta. 2012. Online adaptation strategies for statistical machine translation in post-editing scenarios. Pattern Recognition, 45:3193–3203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurent Nepveu</author>
<author>Guy Lapalme</author>
<author>Philippe Langlais</author>
<author>George Foster</author>
</authors>
<title>Adaptive language and translation models for interactive machine translation.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>190--197</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<marker>[Nepveu et al.2004]</marker>
<rawString>Laurent Nepveu, Guy Lapalme, Philippe Langlais, and George Foster. 2004. Adaptive language and translation models for interactive machine translation. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 190– 197, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fujio NISHIDA</author>
<author>Shinobu TAKAMATSU</author>
<author>Tadaaki TANI</author>
<author>Tsunehisa DOI</author>
</authors>
<title>Feedback of correcting information in postediting to a machine translation system.</title>
<date>1988</date>
<booktitle>In Proc. of COLING.</booktitle>
<marker>[NISHIDA et al.1988]</marker>
<rawString>Fujio NISHIDA, Shinobu TAKAMATSU, Tadaaki TANI, and Tsunehisa DOI. 1988. Feedback of correcting information in postediting to a machine translation system. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan,</location>
<marker>[Och2003]</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ortiz-Martinez</author>
<author>Ismael Garcia-Varea</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Online learning for interactive statistical machine translation.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>546--554</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<marker>[Ortiz-Martinez et al.2010]</marker>
<rawString>Daniel Ortiz-Martinez, Ismael Garcia-Varea, and Francisco Casacuberta. 2010. Online learning for interactive statistical machine translation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 546–554, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, Pennsylvania, USA,</location>
<marker>[Papineni et al.2002]</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Parker</author>
<author>David Graff</author>
<author>Junbo Kong</author>
<author>Ke Chen</author>
<author>Kazuaki Maeda</author>
</authors>
<date>2011</date>
<booktitle>English Gigaword Fifth Edition, June. Linguistic Data Consortium, LDC2011T07.</booktitle>
<marker>[Parker et al.2011]</marker>
<rawString>Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2011. English Gigaword Fifth Edition, June. Linguistic Data Consortium, LDC2011T07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Przybocki</author>
</authors>
<title>Nist open machine translation</title>
<date>2012</date>
<marker>[Przybocki2012]</marker>
<rawString>Mark Przybocki. 2012. Nist open machine translation 2012 evaluation (openmt12). http://www.nist.gov/itl/iad/mig/openmt12.cfm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avneesh Saluja</author>
<author>Ian Lane</author>
<author>Ying Zhang</author>
</authors>
<title>Machine translation with binary feedback: a large-margin approach.</title>
<date>2012</date>
<booktitle>In Proceedings of the Tenth Biennial Conference of the Association for Machine Translation in the Americas.</booktitle>
<marker>[Saluja et al.2012]</marker>
<rawString>Avneesh Saluja, Ian Lane, and Ying Zhang. 2012. Machine translation with binary feedback: a large-margin approach. In Proceedings of the Tenth Biennial Conference of the Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Germ´an Sanchis-Trilles</author>
</authors>
<title>Building task-oriented machine translation systems.</title>
<date>2012</date>
<booktitle>In Ph.D. Thesis, Universitat Politcnica de Valncia.</booktitle>
<marker>[Sanchis-Trilles2012]</marker>
<rawString>Germ´an Sanchis-Trilles. 2012. Building task-oriented machine translation systems. In Ph.D. Thesis, Universitat Politcnica de Valncia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Simard</author>
<author>George Foster</author>
</authors>
<title>PEPr: Post-edit propagation using phrase-based statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the XIV Machine Translation Summit,</booktitle>
<pages>191--198</pages>
<marker>[Simard and Foster2013]</marker>
<rawString>Michel Simard and George Foster. 2013. PEPr: Post-edit propagation using phrase-based statistical machine translation. In Proceedings of the XIV Machine Translation Summit, pages 191–198,, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Midori Tatsumi</author>
</authors>
<title>Correlation between automatic evaluation metric scores, postediting speed, and some other factors.</title>
<date>2009</date>
<booktitle>In Proceedings of the Twelfth Machine Translation Summit.</booktitle>
<marker>[Tatsumi2009]</marker>
<rawString>Midori Tatsumi. 2009. Correlation between automatic evaluation metric scores, postediting speed, and some other factors. In Proceedings of the Twelfth Machine Translation Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
</authors>
<title>A hierarchical Bayesian language model based on Pitman-Yor processes.</title>
<date>2006</date>
<booktitle>In Proc. of ACL.</booktitle>
<marker>[Teh2006]</marker>
<rawString>Yee Whye Teh. 2006. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Matthias Eck</author>
<author>Stephan Vogel</author>
</authors>
<title>Language model adaptation for statistical machine translation with structured query models.</title>
<date>2004</date>
<booktitle>In Proc. of COLING.</booktitle>
<marker>[Zhao et al.2004]</marker>
<rawString>Bing Zhao, Matthias Eck, and Stephan Vogel. 2004. Language model adaptation for statistical machine translation with structured query models. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ventsislav Zhechev</author>
</authors>
<title>Machine Translation Infrastructure and Post-editing Performance at Autodesk.</title>
<date>2012</date>
<booktitle>In AMTA 2012 Workshop on Post-Editing Technology and Practice (WPTP 2012),</booktitle>
<pages>87--96</pages>
<location>San Diego, USA,</location>
<marker>[Zhechev2012]</marker>
<rawString>Ventsislav Zhechev. 2012. Machine Translation Infrastructure and Post-editing Performance at Autodesk. In AMTA 2012 Workshop on Post-Editing Technology and Practice (WPTP 2012), pages 87–96, San Diego, USA, October. Association for Machine Translation in the Americas (AMTA).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>