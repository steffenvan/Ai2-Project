<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002097">
<title confidence="0.998194">
Online Acquisition of Japanese Unknown Morphemes
using Morphological Constraints
</title>
<author confidence="0.985568">
Yugo Murawaki Sadao Kurohashi
</author>
<affiliation confidence="0.987208">
Graduate School of Informatics, Kyoto University
</affiliation>
<address confidence="0.696094">
Yoshida-honmachi, Sakyo-ku, Kyoto, 606-8501, Japan
</address>
<email confidence="0.998734">
murawaki@nlp.kuee.kyoto-u.ac.jp kuro@i.kyoto-u.ac.jp
</email>
<sectionHeader confidence="0.99564" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999884352941177">
We propose a novel lexicon acquirer that
works in concert with the morphological ana-
lyzer and has the ability to run in online mode.
Every time a sentence is analyzed, it detects
unknown morphemes, enumerates candidates
and selects the best candidates by comparing
multiple examples kept in the storage. When
a morpheme is unambiguously selected, the
lexicon acquirer updates the dictionary of the
analyzer, and it will be used in subsequent
analysis. We use the constraints of Japanese
morphology and effectively reduce the num-
ber of examples required to acquire a mor-
pheme. Experiments show that unknown mor-
phemes were acquired with high accuracy and
improved the quality of morphological analy-
sis.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999979916666667">
Morphological analysis is the first step for most nat-
ural language processing applications. In Japanese
morphological analysis, segmentation is processed
simultaneously with the assignment of a part of
speech (POS) tag to each morpheme. Segmentation
is a nontrivial task in Japanese because it does not
delimit words by white-space.
Japanese morphological analysis has successfully
adopted dictionary-based approaches (Kurohashi et
al., 1994; Asahara and Matsumoto, 2000; Kudo et
al., 2004). In these approaches, a sentence is trans-
formed into a lattice of morphemes by searching a
pre-defined dictionary, and an optimal path in the
lattice is selected.
This area of research may be considered almost
completed, as previous studies reported the F-score
of nearly 99% (Kudo et al., 2004). When applied
to web texts, however, more errors are made due to
unknown morphemes. In previous studies, exper-
iments were performed on newspaper articles, but
web texts include slang words, informal spelling al-
ternates (Nishimura, 2003) and technical terms. For
example, the verb “ググる” (gugu-ru, to google) is
erroneously segmented into “ググ” (gugu) and “る”
(ru).
One solution to this problem is to augment the
lexicon of the morphological analyzer by extracting
unknown morphemes from texts (Mori and Nagao,
1996). In the previous method, a morpheme extrac-
tion module worked independently of the morpho-
logical analyzer and ran in off-line (batch) mode.
It is inefficient because almost all high-frequency
morphemes have already been registered to the pre-
defined dictionary. Moreover, it is inconvenient
when applied to web texts because the web corpus
is huge and diverse compared to newspaper corpora.
It is not necessarily easy to build subcorpora before
lexicon acquisition. Suppose that we want to ana-
lyze whaling-related documents. It is unnecessary
and probably harmful to acquire morphemes that are
irrelevant to the topic. A whaling-related subcorpus
should be extracted from the whole corpus but it is
not clear how large it must be.
We propose a novel lexicon acquirer that works
in concert with the morphological analyzer and has
the ability to run in online mode. As shown in Fig-
ure 1, every time a sentence is analyzed, the lexicon
acquirer detects unknown morphemes, enumerates
</bodyText>
<page confidence="0.987918">
429
</page>
<note confidence="0.9771285">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 429–437,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<figure confidence="0.5582555">
Analyzer
Lexicon Acquirer
</figure>
<figureCaption confidence="0.999771">
Figure 1: System architecture
</figureCaption>
<bodyText confidence="0.9999150625">
candidates and selects the best candidates by com-
paring multiple examples kept in the storage. When
a morpheme is unambiguously selected, the lexicon
acquirer updates the automatically constructed dic-
tionary, and it will be used in subsequent analysis.
The proposed method is flexible and gives the sys-
tem more control over the process. We do not have
to limit the target corpus beforehand and the system
can stop whenever appropriate.
We use the constraints of Japanese morphology
that have already been coded in the morphological
analyzer. These constraints effectively reduce the
number of examples required to acquire an unknown
morpheme. Experiments show that unknown mor-
phemes were acquired with high accuracy and im-
proved the quality of morphological analysis.
</bodyText>
<sectionHeader confidence="0.985821" genericHeader="method">
2 Japanese Morphology
</sectionHeader>
<bodyText confidence="0.9993675">
In order to understand the task of lexicon acquisi-
tion, we briefly describe the Japanese morpholog-
ical analyzer JUMAN.1 We explain Japanese mor-
phemes in Section 2.1, morphological constraints in
Section 2.2, and unknown morpheme processing in
Section 2.3.
</bodyText>
<subsectionHeader confidence="0.971839">
2.1 Morpheme
</subsectionHeader>
<bodyText confidence="0.9972824">
In JUMAN, the POS tagset consists of four ele-
ments: class, subclass, conjugation type and con-
jugation form. The classes are noun, verb, adjec-
tive and others. Noun has subclasses such as com-
mon noun, sa-group noun, proper noun, organiza-
</bodyText>
<footnote confidence="0.9298715">
1http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/juman.html
</footnote>
<bodyText confidence="0.994356472222222">
tion, place, personal name. Verb and adjective have
no subclasses.
Verbs and adjectives among others change their
form according to the morphemes that occur after
them, which is called conjugation. Conjugable mor-
phemes are grouped by conjugation types such as
vowel verb, ra-row verb, i-type adjective and na-
type adjective. Each conjugable morpheme takes
one of conjugation forms in texts. It has an invari-
ant stem and an ending which changes according to
conjugation type and conjugation form.
In this paper, the tuple of class, subclass and con-
jugation type is referred to as a POS tag. For sim-
plicity, POS tags for nouns are called by their sub-
classes and those for verbs and adjectives by their
conjugation types.
There are two types of morphemes: abstract dic-
tionary entries, and examples or actual occurrences
in texts. An entry consists of a stem and a POS tag
while an example consists of a stem, a POS tag and
a conjugation form. For example, the entry of the
ra-row verb “it;5” (hashi-ru, to run) can be repre-
sented as
(“it” (hashi), ra-row verb),
and their examples “it;” (hashi-ra) and “itD”
(hashi-ri) as
(“it” (hashi), ra-row verb, imperfective),
and
(“it” (hashi), ra-row verb, plain continu-
ative)
respectively. As nouns do not conjugate, the entry
of the sa-group noun “t�” (kibou, hope) can be
represented as
(“t�” (kibou), sa-group noun)
and its sole example form is
(“t�” (kibou), sa-group noun, NIL).
</bodyText>
<subsectionHeader confidence="0.997012">
2.2 Morphological Constraints
</subsectionHeader>
<bodyText confidence="0.999979125">
Japanese is an agglutinative language. Depending
on its grammatical roles, a morpheme is followed by
a sequence of grammatical suffixes, auxiliary verbs
and particles, and the connectivity of these elements
is bound by morphological constraints. For exam-
ple, the particle “,�_7” (wo, accusative case) can fol-
low a verb with the conjugation form of plain contin-
uative, as in “itD�_7” (hashi-ri-wo, running-ACC),
</bodyText>
<figure confidence="0.997969222222222">
update
automatically constructed
dictionary
text JUMAN
Selector
accumulated
examples
(morph.
analyzer)
lookup
Enumerator
hand-crafted
dictionary
KNP
(parser)
Detector
analysis
analysis
</figure>
<page confidence="0.997295">
430
</page>
<bodyText confidence="0.9996474">
but it cannot follow an imperfective verb (“*走らを”
(*hashi-ra-wo)).
These constraints are used by JUMAN to reduce
the ambiguity. They can be also used in lexicon ac-
quisition.
</bodyText>
<subsectionHeader confidence="0.999081">
2.3 Unknown Morpheme Processing
</subsectionHeader>
<bodyText confidence="0.999985962962963">
Given a sentence, JUMAN builds a lattice of mor-
phemes by searching a pre-defined dictionary, and
then selects an optimal path in the lattice. To han-
dle morphemes that cannot be found in the dictio-
nary, JUMAN enumerates unknown morpheme can-
didates using character type-based heuristics, and
adds them to the morpheme lattice. Unknown mor-
phemes are given the special POS tag “undefined,”
which is treated as noun.
Character type-based heuristics are based on the
fact that Japanese is written with several different
character types such as kanji, hiragana and katakana,
and that the choice of character types gives some
clues on morpheme boundaries. For example, a se-
quence of katakana characters are considered as an
unknown morpheme candidate, as in “グーグル”
(gˆuguru, Google) out of “グーグルが” (gˆuguru-ga,
Google-NOM). Kanji characters are segmented per
character, which is sometimes wrong but prevents
error propagation.
These heuristics are simple and effective, but far
from perfect. They cannot identify mixed-character
morphemes, verbs and adjectives correctly. For ex-
ample, the verb “ググる” (gugu-ru, to google) is
wrongly divided into the katakana unknown mor-
pheme ” ググ” (gugu) and the hiragana suffix “る”
(ru).
</bodyText>
<sectionHeader confidence="0.968424" genericHeader="method">
3 Lexicon Acquisition
</sectionHeader>
<subsectionHeader confidence="0.99298">
3.1 Task
</subsectionHeader>
<bodyText confidence="0.998318818181818">
The task of lexicon acquisition is to generate dictio-
nary entries inductively from their examples in texts.
Since the morphological analyzer provides a basic
lexicon, the morphemes to be acquired are limited
to those unknown to the analyzer.
In order to generate an entry, its stem and POS
tag need to be identified. Determining the stem of
an example is to draw the front and rear boundaries
in a character sequence in texts which corresponds
to the stem. The POS tag is selected from the tagset
given by the morphological analyzer.
</bodyText>
<subsectionHeader confidence="0.999347">
3.2 System Architecture
</subsectionHeader>
<bodyText confidence="0.999974785714286">
Figure 1 shows the system architecture. Each sen-
tence in texts is processed by the morphological an-
alyzer JUMAN and the dependency parser KNP.2
JUMAN consults a hand-crafted dictionary and an
automatically constructed dictionary. KNP is used
to form a phrasal unit called bunsetsu by chunking
morphemes.
Every time a sentence is analyzed, the lexicon
acquirer receives the analysis. It detects examples
of unknown morphemes and keeps them in storage.
When an entry is unambiguously selected, the lex-
icon acquirer updates the automatically constructed
dictionary, and it will be used in subsequent analy-
sis.
</bodyText>
<subsectionHeader confidence="0.999115">
3.3 Algorithm Overview
</subsectionHeader>
<bodyText confidence="0.999982529411765">
The process of lexicon acquisition has four phases:
detection, candidate enumeration, aggregation and
selection. First the analysis is scanned to detect ex-
amples of unknown morphemes. For each exam-
ple, one or more candidates for dictionary entries are
enumerated. It is added to the storage, and multiple
examples in the storage that share the candidates are
aggregated. They are compared and the best candi-
date is selected from it.
Take the ra-row verb “ググる” (gugu-ru) for ex-
ample. Its example “ググってみた。” (gugu-tte-
mi-ta, to have tried to google) can be interpreted in
many ways as shown in Figure 2. Similarly, multi-
ple candidates are enumerated for another example
“ググるのは” (gugu-ru-no-ha, to google-TOPIC). If
these examples are compared, we can see that the
ra-row verb “ググる” (gugu-ru) can explain them.
</bodyText>
<subsectionHeader confidence="0.970174">
3.4 Suffixes
</subsectionHeader>
<bodyText confidence="0.99991675">
Morphological constraints are used for candidate
enumeration. Since they are coded in JUMAN, we
first transform them into a set of strings called suf-
fixes. A suffix is created by concatenating the end-
ing of a morpheme (if any) and subsequent ancillary
morphemes. Each POS tag is associated with a set
of suffixes, as shown in Table 1. This means that a
stem can be followed by one of the suffixes specified
</bodyText>
<footnote confidence="0.944439">
2http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp.html
</footnote>
<page confidence="0.998912">
431
</page>
<tableCaption confidence="0.999564">
Table 1: Examples of suffixes
</tableCaption>
<table confidence="0.931722111111111">
POS tag base form stem ending conjugation form1 suffixes
ra-row verb hashi-ru hashi ra imperfective razu, ranaide
ri plain continuative riwo, riwomo
ru plain ru, rukawo
vowel verb akogare-ru akogare 0 imperfective zu, naide
0 plain continuative wo, womo
ru plain ru, rukawo
sa-group noun kibou kibou NIL wo wo, womo
NIL suru suru, shitara
</table>
<figure confidence="0.8864706">
1 The conjugation form of a noun is substituted with the base form of its immediate
ancillary morpheme because nouns do not conjugate.
stem
• (EOB)
ࠣࠣߞߡߺߚޕ
google -CONT try-PAST
stem
stem
stem
suffix
[POS tags]
• ra-row verb
• wa-row verb
• ta-row verb
• ma-row verb
• vowel verb
• ta-row verb
suffix
suffix
sent unknown morphemes are detected.
</figure>
<bodyText confidence="0.997936625">
Currently, we use the POS tag “undefined” to de-
tect unknown morphemes. For example, the exam-
ple “YY-�)ZZ;ko” is detected because “YY”
is given “undefined.” This simple method cannot
detect unknown morphemes if they are falsely seg-
mented into combinations of registered morphemes.
We leave the comprehensive detection of unknown
morphemes to future work.
</bodyText>
<figureCaption confidence="0.997774">
Figure 2: Candidate enumeration
</figureCaption>
<bodyText confidence="0.99945575">
by its POS tag and cannot be followed by any other
suffix.
In preparation for lexicon acquisition, suffixes are
acquired from a corpus. We used a web corpus that
was compiled through the procedures proposed by
Kawahara and Kurohashi (2006). Suffixes were ex-
tracted from examples of registered morphemes and
were aggregated per POS tag.
We found that the number of suffixes did not con-
verge even in this large-scale corpus. It was because
ancillary morphemes included the wide variety of
auxiliary verbs and formal nouns. Alternatively, we
used the first five characters as a suffix. In the exper-
iments, we obtained 500 thousand unique suffixes
from 100 million pages. The number of POS tags
that corresponded to a suffix was 1.33 on average.
</bodyText>
<subsectionHeader confidence="0.877737">
3.5 Unknown Morpheme Detection
</subsectionHeader>
<bodyText confidence="0.999963">
The first step of lexicon acquisition is unknown mor-
pheme detection. Every time the analysis of a sen-
tence was given, the sequence of morphemes are
scanned, and suspicious points that probably repre-
</bodyText>
<subsectionHeader confidence="0.983992">
3.6 Candidate Enumeration
</subsectionHeader>
<bodyText confidence="0.9969674">
For each example, one or more candidates for the
dictionary entry are enumerated. Each candidate is
represented by a combination of a front boundary
and the pair of a rear boundary and a POS tag.
The search range for enumeration is based on bun-
setsu phrases, which is created by chunking mor-
phemes. The range is at most the corresponding
bunsetsu and the two immediately preceding and
succeeding bunsetsu, which we found wide enough
to contain correct candidates.
The candidates for the rear boundary and the POS
tag are enumerated by string matching of suffixes as
shown in Figure 2. If a suffix matches, the start-
ing position of the suffix becomes a candidate for
the rear boundary and the suffix is mapped to one or
more corresponding POS tags.
In addition, the candidates for the front and
rear boundaries are enumerated by scanning the se-
quence of morphemes. The boundary markers we
use are
</bodyText>
<listItem confidence="0.998079666666667">
• punctuations,
• grammatical prefixes such as “W” (go-, hon-
orific prefix), for front boundaries,
</listItem>
<page confidence="0.875393">
432
</page>
<listItem confidence="0.999477666666667">
• grammatical suffixes such as “様” (-sama, hon-
orific title), for rear boundaries, and
• bunsetsu boundaries given by KNP.
</listItem>
<bodyText confidence="0.999218714285714">
Each rear boundary candidate whose correspond-
ing POS tag is not decided is given the special tag
“EOB” (end-of-bunsetsu). This means that no suf-
fix is attached to the candidate. Since nouns, vowel
verbs and na-type adjectives can appear in isolation,
it will be expanded to these POS tags when selecting
the best POS tag.
</bodyText>
<subsectionHeader confidence="0.99977">
3.7 Aggregation of Examples
</subsectionHeader>
<bodyText confidence="0.999993866666667">
Selection of the best candidate is done by compar-
ing multiple examples. Each example is added to
the storage, and then examples that possibly repre-
sent the same entry with it are extracted from the
storage. Examples aggregated at this phase share the
front boundary but may be unrelated to the example
in question. They are pruned in the next phase.
In order to manage examples efficiently, we im-
plement a trie. The example is added to the trie for
each front boundary candidate. The key is the char-
acter sequence determined by the front boundary
and the leftmost rear boundary. To retrieve examples
that share the front boundary with it, we check every
node in the path from the root to the node where it is
stored, and collect examples stored in each node.
</bodyText>
<subsectionHeader confidence="0.988307">
3.8 Selection
</subsectionHeader>
<bodyText confidence="0.999966333333334">
The best candidate is selected by identifying the
front boundary, the rear boundary and the POS tag
in this order. Starting from the rightmost front
boundary candidate, multiple rear boundary candi-
dates that share the front boundary are compared and
some are dropped. Then starting from the leftmost
surviving rear boundary candidate, the best POS tag
is selected from the examples that share the stem.
If the selected candidate satisfies simple termination
conditions, it is added to the dictionary and the ex-
amples are removed from the storage.
For each front boundary candidate, some inappro-
priate rear boundary candidates are dropped by ex-
amining the inclusion relation between the examples
of a pair of candidates. The assumption behind this
is that an appropriate candidate can interpret more
examples than incorrect ones. Let p and q be a pair
of the candidates for the rear boundary, and Rp and
RQ be the sets of examples for which p and q are
enumerated. If p is a prefix of q and p is the correct
stem, then RQ must be contained in Rp. In practice
we loosen this condition, considering possible errors
in candidate enumeration
For each stem candidate, the appropriate POS tag
is identified. Similarly to rear boundary identifica-
tion, POS identification is done by checking inclu-
sion relation.
If the POS tag is successfully disambiguated, sim-
ple termination conditions is checked to prevent the
accidental acquisition of erroneous candidates. The
first condition is that the number of unique conjuga-
tion forms that appear in the examples should be 3 or
more. If the candidate is a noun, it is substituted with
the number of the unique base forms of their imme-
diate ancillary morphemes. The second condition is
that the front boundaries of some examples are de-
cided by clear boundary markers such as punctua-
tions and the beginning of sentence. This prevents
oversegmentation. For example, the stem candidate
“*撰組” (*sengumi) is always enumerated for exam-
ples of “新撰組” (Shingengumi, a historical organi-
zation) since “新” (shin-, new) is a prefix. This can-
didate is not acquired because “*撰組” (*sengumi)
does not occur alone and is always accompanied by
“新” (shin-). Thresholds are chosen empirically.
</bodyText>
<subsectionHeader confidence="0.936323">
3.9 Decompositionality
</subsectionHeader>
<bodyText confidence="0.999987052631579">
Since a morpheme is extracted from a small num-
ber of examples, it is inherently possible that the ac-
quired morpheme actually consists of two or more
morphemes. For example, the noun phrase “顆粒
タイプ” (karyuu-taipu, granular type) may be ac-
quired as a morpheme before “顆粒” (karyuu, gran-
ule) is extracted. To handle this phenomenon, it
is checked at the time of acquisition whether the
new morpheme (kairyuu) can decompose registered
morphemes (kairyuu-taipu). If found, a composite
“morpheme” is removed from the dictionary.
Currently we leave the decompositionality check
to the morphological analyzer. Possible compounds
are enumerated by string matching and temporar-
ily removed from the dictionary. Each candidate
is analyzed by the morphological analyzer and it is
checked whether the candidate is divided into a com-
bination of registered morphemes. If not, the candi-
date is restored to the dictionary.
</bodyText>
<page confidence="0.999811">
433
</page>
<tableCaption confidence="0.998661">
Table 2: Statistical information per query
</tableCaption>
<table confidence="0.992969866666667">
query number of number of number of number of number of
sentences affected acquired correct examples1
sentences morphs morphs
(ratio) (precision)
捕鯨問題 135,379 2,444 293 290 4
(whaling issue) (1.81%) (99.0%)
赤ちゃんポスト 74,572 775 107 105 4
(baby hatch) (1.04%) (98.1%)
ジャスラック 195,928 6,259 913 907 4
(JASRAC) (3.19%) (99.3%)
ツンデレ 77,962 12,012 243 238 5
(tsundere) (15.4%) (97.4%)
アガリクス 78,922 3,037 114 107 9
(agaricus) (3.85%) (93.9%)
1 The median number of examples used for acquisition.
</table>
<sectionHeader confidence="0.99901" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.991451">
4.1 Experimental Design
</subsectionHeader>
<bodyText confidence="0.9998783">
We used the default dictionary of the morphological
analyzer JUMAN as the initial lexicon. It contained
30 thousand basic morphemes. If spelling variants
were expanded and proper nouns were counted, the
total number of morphemes was 120 thousands.
We used domain-specific corpora as target texts
because efficient acquisition was expected. If target
texts shared a topic, relevant unknown morphemes
were used frequently. In the experiments, we used
search engine TSUBAKI (Shinzato et al., 2008) and
casted the search results as domain-specific corpora.
For each query, our system sequentially read pages
from the top of the result and acquired morphemes.
We terminated the acquisition at the 1000th page
and analyzed the same 1000 pages with the aug-
mented lexicon. The queries used were “捕鯨問
題” (whaling issue), “赤ちゃんポスト” (baby hatch),
“ジャスラック” (JASRAC, a copyright collective),
“ツンデレ” (tsundere, a slang word) and “アガリク
ス” (agaricus).
</bodyText>
<subsectionHeader confidence="0.972791">
4.2 Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.998714675675676">
The proposed method is evaluated by measuring the
accuracy of acquired morphemes and their contri-
bution to the improvement of morphological analy-
sis. A morpheme is considered accurate if both seg-
mentation and the POS tag are correct. Note that
segmentation is a nontrivial problem for evaluation.
In fact, the disagreement over segmentation criteria
was considered one of the main reasons for reported
errors by Nagata (1999) and Uchimoto et al. (2001).
It is difficult to judge whether a compound term
should be divided because there is no definite stan-
dard for morpheme boundaries in Japanese. For ex-
ample, “ミンク鯨” (minku-kujira, minke whale) can
be extracted as a single morpheme or decomposed
into “ミンク” and “鯨.” While segmentation is an
open question in Japanese morphological analysis,
“correct” segmentation is not necessarily important
for applications using morphological analysis. Even
if a noun is split into two or more morphemes in
morphological analysis, they are chunked to form
a phrasal unit called bunsetsu in dependency pars-
ing, and to extract a keyword (Nakagawa and Mori,
2002).
To avoid the decompositionality problem, we
adopted manual evaluation. We analyzed the tar-
get texts with both the initial lexicon and the aug-
mented lexicon. Then we checked differences be-
tween the two analyses and extracted sentences that
were affected by the augmentation. Among these
sentences, we evaluated randomly selected 50 sen-
tences per query. We checked the accuracy of seg-
mentation and POS tagging of each “diff” block,
which is illustrated in Figure 3. The segmentation of
a block was judged correct unless morpheme bound-
aries were clearly wrong.
In the evaluation of POS tagging, we did not dis-
tinguish subclasses of noun3 such as common noun
</bodyText>
<footnote confidence="0.992872">
3In the experiments, we regarded demonstrative pronouns as
</footnote>
<page confidence="0.998706">
434
</page>
<tableCaption confidence="0.6456455">
Table 3: Examples of acquired morphemes
query examples
whaling issue モラトリアム (moratorium), ツチクジラ (giant beaked whale), 混獲 (bycatch)
baby hatch ダンナ (husband), 助産師 (midwife), 棄てる (to abandon), 訊く (to inquire)
JASRAC ソフ倫 (an organization), シャ乱 Q (a pop-rock band), ヲタ (geek)
tsundere アキバ (abbr. of Akihabara), 腐女子 (fujoshi, a slang word), モテる (to be popular)
agaricus サプリ (abbr. of suppliment), アロマ (aroma), 食効 (enhanced nutritional function)
Table 4: Evaluation of “diff” blocks
</tableCaption>
<table confidence="0.937460333333333">
query segmentation POS tagging total
E → C C → C E → E C → E E → C C → C E → E C → E
whaling issue 11 45 0 2 11 45 0 2 58
baby hatch 37 12 0 3 37 12 0 3 52
JASRAC 16 23 1 12 16 23 1 12 52
tsundere 17 39 0 1 17 39 0 1 57
agaricus 22 31 0 0 22 31 0 0 53
(Legend – C: correct; E: erroneous)
Google it and we will find a lot.
&lt; YY undefined - katakana
&lt; -6 suffix - verbal suffix
&gt; YY-6 verb - ra�row verb
</table>
<figureCaption confidence="0.997002">
Figure 3: A “diff” block in a sentence
</figureCaption>
<bodyText confidence="0.992878">
and proper noun. The special POS tag “undefined”
given by JUMAN was treated as noun.
</bodyText>
<subsectionHeader confidence="0.926297">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999739051282051">
Table 2 summarizes statistical information per
query. The number of sentences affected by the
augmentation varied considerably (1.04%–15.4%).
The initial lexicon of the morphological analyzer
lacked morphemes that appeared frequently in some
corpora because morphological analysis had been
tested mainly with newspaper articles.
The precision of acquired morphemes was high
(97.4%–99.3%), and the number of examples used
for acquisition was as little as 4–9. These results are
astonishing considering that Mori and Nagao (1996)
ignored candidates that appeared less than 10 times
(because they were unreliable).
nouns because their morphological behaviors were the same as
those of nouns. Although demonstrative nouns are closed class
morphemes, their katakana forms such as “コレ” (this) were
acquired as nouns. The morphological analyzer assumed that
demonstrative pronouns were written in hiragana, e.g., “これ,”
as they always are in a newspaper.
Table 3 shows some acquired morphemes. As
expected, the overwhelming majority were nouns
(93.0%–100%) and katakana morphemes (80.7%–
91.6%). Some were mixed-character morphemes
(“ソフ倫” and “シャ乱 Q”), which cannot be recog-
nized by character-type based heuristics, and slang
words (“腐女子,” “ヲタ,” etc.) which did not ap-
pear in newspaper articles. Some morphemes were
spelling variants of those in the pre-defined dictio-
nary. Uncommon kanji characters were used in ba-
sic words (“棄てる” for “捨てる” and “訊く” for
“聞く”) and katakana was used to change nuances
(“モテる” for “もてる” and “ダンナ” for “旦那”).
Table 4 shows the results of manual evaluation of
“diff” blocks. The overwhelming majority of blocks
were correctly analyzed with the augmented lexicon
(E → C and C → C). On the other hand, adverse
effects were observed only in a few blocks (C →
E). In conclusion, acquired morphemes improve the
quality of morphological analysis.
</bodyText>
<subsectionHeader confidence="0.984596">
4.4 Error Analysis
</subsectionHeader>
<bodyText confidence="0.967495555555556">
Some short katakana morphemes oversegmented
other katakana nouns. For example, “サーバー”
(sˆabˆa, server) was wrongly segmented by newly-
acquired “サー” (sˆa, sir) and preregistered “バー”
(bˆa, bar). Neither the morphological analyzer and
the lexicon acquirer could detect this semantic mis-
match. Curiously, one example of “サー” (sˆa) was
actuallly part of “サーバー” (sˆabˆa), which was erro-
YY-6LUSWZ&lt;-6.
</bodyText>
<page confidence="0.968804">
435
</page>
<figure confidence="0.994866888888889">
1000
800
600
400
200
0
num. of acquired morphemes
40000
32000
24000
16000
8000
0
0 100000 200000
num. of sentences
acquired morphemes
stored examples
acquired morphemes in re-analysis
</figure>
<figureCaption confidence="0.999973">
Figure 4: Process of online acquisition
</figureCaption>
<bodyText confidence="0.9998285">
neously segmented when extracting sentences from
HTML.
The katakana adjective “イイ” (i-i, good), a
spelling variant of the basic morpheme “いい,” was
falsely identified as a noun because its ending “イ”
was written in katakana. The morphological ana-
lyzer, and hence the lexicon acquirer, assume that
the ending of a verb or adjective is written in hi-
ragana. This assumption is reasonable for stan-
dard Japanese, but does not always hold when we
analyze web texts. In order to recognize uncon-
ventional spellings that are widely used in web
texts (Nishimura, 2003), more flexible analysis is
needed.
</bodyText>
<subsectionHeader confidence="0.750345">
4.5 Discussion
</subsectionHeader>
<bodyText confidence="0.999971875">
It is too costly or impractical to calculate the re-
call of acquisition, or the ratio of the number of ac-
quired morphemes against the total number of un-
known morphemes because it requires human judges
to find undetected unknown morphemes from a large
amount of raw texts.
Alternatively, we examined the ratio against the
number of detected unknown morphemes. Figure 4
shows the process of online acquisition for the query
“JASRAC.” The monotonic increase of the num-
bers of acquired morphemes and stored examples
suggests that the vocabulary size did not converge.
The number of occurrences of acquired morphemes
in re-analysis was approximately the same with the
number of examples kept in the storage during ac-
quisition. This means that, in terms of frequency of
occurrence, about half of unknown morphemes were
acquired. Most unknown morphemes belong to the
“long tail” and the proposed method seems to have
seized a “head” of the long tail.
Although some previous studies emphasized cor-
rect identification of low frequency terms (Nagata,
1999; Asahara and Matsumoto, 2004), it is no longer
necessary because very large scale web texts are
available today. If a small set of texts needs to
be analyzed with high accuracy, we can incorporate
similar texts retrieved from the web, to increase the
number of examples of unknown morphemes. The
proposed method can be modified to check if un-
known morphemes detected in the initial set are ac-
quired and to terminate whenever sufficient acquisi-
tion coverage is achieved.
</bodyText>
<sectionHeader confidence="0.999968" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.997126833333333">
Since most languages delimit words by white-space,
morphological analysis in these languages is to seg-
ment words into morphemes. For example, Mor-
pho Challenge 2007 (Kurimo et al., 2007) was eval-
uations of unsupervised segmentation for English,
Finnish, German and Turkish.
While Japanese is an agglutinative language,
other non-segmented languages such as Chinese and
Thai are analytic languages. Among them, Chinese
has been a subject of intensive research. Peng et
al. (2004) integrated new word detection into word
segmentation. They detected new words by comput-
ing segment confidence and re-analyzed the inputs
with detected words as features.
The Japanese language is unique in that it is writ-
ten with several different character types. Heuris-
tics widely used in unknown morpheme process-
ing are based on character types. They were also
used as important clues in statistical methods. Na-
gata (1999) integrated a probabilistic unknown word
models into the word segmentation model. Uchi-
moto et al. (2001) incorporated them as feature func-
tions of a Maximum Entropy-based morphological
analyzer. Asahara and Matsumoto (2004) used them
as a feature of character-based chunking of unknown
words using Support Vector Machines.
Mori (1996) extracted words from texts and esti-
mated their POSs using distributional analysis. The
appropriateness of a word candidate was measured
num. of examples
</bodyText>
<page confidence="0.977917">
436
</page>
<bodyText confidence="0.9999594">
by the distance between probability distributions of
the candidate and a model. In this method, mor-
phological constraints were indirectly represented
by distributions.
Nakagawa and Matsumoto (2006) presented a
method for guessing POS tags of pre-segmented un-
known words that took into consideration all the oc-
currences of each unknown word in a document.
This setting is impractical in Japanese because POS
tagging is inseparable from segmentation.
</bodyText>
<sectionHeader confidence="0.999491" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999993571428571">
We propose a novel method that augments the lexi-
con of a Japanese morphological analyzer by acquir-
ing unknown morphemes from texts in online mode.
Unknown morphemes are acquired with high accu-
racy and improve the quality of morphological anal-
ysis.
Unknown morphemes are one of the main sources
of error in morphological analysis when we analyze
web texts. The proposed method has the potential
to overcome the unknown morpheme problem, but
it cannot be achieved without recognizing or being
robust over various phenomena such as unconven-
tional spellings and typos. These phenomena are not
observed in newspaper articles but cannot be ignored
in web texts. In the future, we will work on these
phenomena.
Morphological analysis is now very mature. It
is widely applied as preprocessing for NLP appli-
cations such as parsing and information retrieval.
Hence in the future, we aim to use the proposed
method to improve the quality of these applications.
</bodyText>
<sectionHeader confidence="0.999269" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999851696428572">
Masayuki Asahara and Yuji Matsumoto. 2000. Extended
models and tools for high-performance part-of-speech
tagger. In Procs. of COLING 2000, pages 21–27.
Masayuki Asahara and Yuji Matsumoto. 2004.
Japanese unknown word identification by character-
based chunking. In Procs. of COLING 2004, pages
459–465.
Daisuke Kawahara and Sadao Kurohashi. 2006.
Case frame compilation from the web using high-
performance computing. In Procs. of LREC-06, pages
1344–1347.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to Japanese
morphological analysis. In Procs. of EMNLP 2004,
pages 230–237.
Mikko Kurimo, Mathias Creutz, and Ville Turunen.
2007. Overview of Morpho Challenge in CLEF 2007.
In Working Notes of the CLEF 2007 Workshop, pages
19–21.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto,
and Makoto Nagao. 1994. Improvements of Japanese
morphological analyzer JUMAN. In Procs. of The In-
ternational Workshop on Sharable Natural Language
Resources, pages 22–38.
Shinsuke Mori and Makoto Nagao. 1996. Word extrac-
tion from corpora and its part-of-speech estimation us-
ing distributional analysis. In Procs. of COLING 1996,
pages 1119–1122.
Masaaki Nagata. 1999. A part of speech estimation
method for Japanese unknown words using a statistical
model of morphology and context. In Procs. of ACL
1999, pages 277–284.
Tetsuji Nakagawa and Yuji Matsumoto. 2006. Guessing
parts-of-speech of unknown words using global infor-
mation. In Procs. of COLING-ACL 2006, pages 705–
712.
Hiroshi Nakagawa and Tatsunori Mori. 2002. A sim-
ple but powerful automatic term extraction method. In
COLING-02 on COMPUTERM 2002, pages 29–35.
Yukiko Nishimura. 2003. Linguistic innovations and in-
teractional features of casual online communication in
Japanese. Journal of Computer-Mediated Communi-
cation, 9(1).
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detection
using conditional random fields. In Procs. of COLING
’04, pages 562–568.
Keiji Shinzato, Tomohide Shibata, Daisuke Kawahara,
Chikara Hashimoto, and Sadao Kurohashi. 2008.
TSUBAKI: An open search engine infrastructure for
developing new information access methodology. In
Procs. of IJCNLP-08, pages 189–196.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara.
2001. The unknown word problem: a morphological
analysis of Japanese using maximum entropy aided by
a dictionary. In Procs. of EMNLP 2001, pages 91–99.
</reference>
<page confidence="0.998634">
437
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.755147">
<title confidence="0.997292">Online Acquisition of Japanese Unknown using Morphological Constraints</title>
<author confidence="0.993979">Yugo Murawaki Sadao Kurohashi</author>
<affiliation confidence="0.989791">Graduate School of Informatics, Kyoto</affiliation>
<address confidence="0.942803">Yoshida-honmachi, Sakyo-ku, Kyoto, 606-8501,</address>
<email confidence="0.844096">murawaki@nlp.kuee.kyoto-u.ac.jpkuro@i.kyoto-u.ac.jp</email>
<abstract confidence="0.996528166666667">We propose a novel lexicon acquirer that works in concert with the morphological analyzer and has the ability to run in online mode. Every time a sentence is analyzed, it detects unknown morphemes, enumerates candidates and selects the best candidates by comparing multiple examples kept in the storage. When a morpheme is unambiguously selected, the lexicon acquirer updates the dictionary of the analyzer, and it will be used in subsequent analysis. We use the constraints of Japanese morphology and effectively reduce the number of examples required to acquire a morpheme. Experiments show that unknown morphemes were acquired with high accuracy and improved the quality of morphological analysis.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Masayuki Asahara</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Extended models and tools for high-performance part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Procs. of COLING</booktitle>
<pages>21--27</pages>
<contexts>
<context position="1459" citStr="Asahara and Matsumoto, 2000" startWordPosition="202" endWordPosition="205">uire a morpheme. Experiments show that unknown morphemes were acquired with high accuracy and improved the quality of morphological analysis. 1 Introduction Morphological analysis is the first step for most natural language processing applications. In Japanese morphological analysis, segmentation is processed simultaneously with the assignment of a part of speech (POS) tag to each morpheme. Segmentation is a nontrivial task in Japanese because it does not delimit words by white-space. Japanese morphological analysis has successfully adopted dictionary-based approaches (Kurohashi et al., 1994; Asahara and Matsumoto, 2000; Kudo et al., 2004). In these approaches, a sentence is transformed into a lattice of morphemes by searching a pre-defined dictionary, and an optimal path in the lattice is selected. This area of research may be considered almost completed, as previous studies reported the F-score of nearly 99% (Kudo et al., 2004). When applied to web texts, however, more errors are made due to unknown morphemes. In previous studies, experiments were performed on newspaper articles, but web texts include slang words, informal spelling alternates (Nishimura, 2003) and technical terms. For example, the verb “ググ</context>
</contexts>
<marker>Asahara, Matsumoto, 2000</marker>
<rawString>Masayuki Asahara and Yuji Matsumoto. 2000. Extended models and tools for high-performance part-of-speech tagger. In Procs. of COLING 2000, pages 21–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masayuki Asahara</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese unknown word identification by characterbased chunking.</title>
<date>2004</date>
<booktitle>In Procs. of COLING</booktitle>
<pages>459--465</pages>
<contexts>
<context position="26848" citStr="Asahara and Matsumoto, 2004" startWordPosition="4301" endWordPosition="4304">ncrease of the numbers of acquired morphemes and stored examples suggests that the vocabulary size did not converge. The number of occurrences of acquired morphemes in re-analysis was approximately the same with the number of examples kept in the storage during acquisition. This means that, in terms of frequency of occurrence, about half of unknown morphemes were acquired. Most unknown morphemes belong to the “long tail” and the proposed method seems to have seized a “head” of the long tail. Although some previous studies emphasized correct identification of low frequency terms (Nagata, 1999; Asahara and Matsumoto, 2004), it is no longer necessary because very large scale web texts are available today. If a small set of texts needs to be analyzed with high accuracy, we can incorporate similar texts retrieved from the web, to increase the number of examples of unknown morphemes. The proposed method can be modified to check if unknown morphemes detected in the initial set are acquired and to terminate whenever sufficient acquisition coverage is achieved. 5 Related Work Since most languages delimit words by white-space, morphological analysis in these languages is to segment words into morphemes. For example, Mo</context>
<context position="28429" citStr="Asahara and Matsumoto (2004)" startWordPosition="4551" endWordPosition="4554"> detection into word segmentation. They detected new words by computing segment confidence and re-analyzed the inputs with detected words as features. The Japanese language is unique in that it is written with several different character types. Heuristics widely used in unknown morpheme processing are based on character types. They were also used as important clues in statistical methods. Nagata (1999) integrated a probabilistic unknown word models into the word segmentation model. Uchimoto et al. (2001) incorporated them as feature functions of a Maximum Entropy-based morphological analyzer. Asahara and Matsumoto (2004) used them as a feature of character-based chunking of unknown words using Support Vector Machines. Mori (1996) extracted words from texts and estimated their POSs using distributional analysis. The appropriateness of a word candidate was measured num. of examples 436 by the distance between probability distributions of the candidate and a model. In this method, morphological constraints were indirectly represented by distributions. Nakagawa and Matsumoto (2006) presented a method for guessing POS tags of pre-segmented unknown words that took into consideration all the occurrences of each unkn</context>
</contexts>
<marker>Asahara, Matsumoto, 2004</marker>
<rawString>Masayuki Asahara and Yuji Matsumoto. 2004. Japanese unknown word identification by characterbased chunking. In Procs. of COLING 2004, pages 459–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Case frame compilation from the web using highperformance computing.</title>
<date>2006</date>
<booktitle>In Procs. of LREC-06,</booktitle>
<pages>1344--1347</pages>
<contexts>
<context position="12164" citStr="Kawahara and Kurohashi (2006)" startWordPosition="1892" endWordPosition="1895">ed. Currently, we use the POS tag “undefined” to detect unknown morphemes. For example, the example “YY-�)ZZ;ko” is detected because “YY” is given “undefined.” This simple method cannot detect unknown morphemes if they are falsely segmented into combinations of registered morphemes. We leave the comprehensive detection of unknown morphemes to future work. Figure 2: Candidate enumeration by its POS tag and cannot be followed by any other suffix. In preparation for lexicon acquisition, suffixes are acquired from a corpus. We used a web corpus that was compiled through the procedures proposed by Kawahara and Kurohashi (2006). Suffixes were extracted from examples of registered morphemes and were aggregated per POS tag. We found that the number of suffixes did not converge even in this large-scale corpus. It was because ancillary morphemes included the wide variety of auxiliary verbs and formal nouns. Alternatively, we used the first five characters as a suffix. In the experiments, we obtained 500 thousand unique suffixes from 100 million pages. The number of POS tags that corresponded to a suffix was 1.33 on average. 3.5 Unknown Morpheme Detection The first step of lexicon acquisition is unknown morpheme detectio</context>
</contexts>
<marker>Kawahara, Kurohashi, 2006</marker>
<rawString>Daisuke Kawahara and Sadao Kurohashi. 2006. Case frame compilation from the web using highperformance computing. In Procs. of LREC-06, pages 1344–1347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Kaoru Yamamoto</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Applying conditional random fields to Japanese morphological analysis.</title>
<date>2004</date>
<booktitle>In Procs. of EMNLP</booktitle>
<pages>230--237</pages>
<contexts>
<context position="1479" citStr="Kudo et al., 2004" startWordPosition="206" endWordPosition="209">show that unknown morphemes were acquired with high accuracy and improved the quality of morphological analysis. 1 Introduction Morphological analysis is the first step for most natural language processing applications. In Japanese morphological analysis, segmentation is processed simultaneously with the assignment of a part of speech (POS) tag to each morpheme. Segmentation is a nontrivial task in Japanese because it does not delimit words by white-space. Japanese morphological analysis has successfully adopted dictionary-based approaches (Kurohashi et al., 1994; Asahara and Matsumoto, 2000; Kudo et al., 2004). In these approaches, a sentence is transformed into a lattice of morphemes by searching a pre-defined dictionary, and an optimal path in the lattice is selected. This area of research may be considered almost completed, as previous studies reported the F-score of nearly 99% (Kudo et al., 2004). When applied to web texts, however, more errors are made due to unknown morphemes. In previous studies, experiments were performed on newspaper articles, but web texts include slang words, informal spelling alternates (Nishimura, 2003) and technical terms. For example, the verb “ググる” (gugu-ru, to goog</context>
</contexts>
<marker>Kudo, Yamamoto, Matsumoto, 2004</marker>
<rawString>Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. 2004. Applying conditional random fields to Japanese morphological analysis. In Procs. of EMNLP 2004, pages 230–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikko Kurimo</author>
<author>Mathias Creutz</author>
<author>Ville Turunen</author>
</authors>
<title>Overview of Morpho Challenge in CLEF</title>
<date>2007</date>
<booktitle>In Working Notes of the CLEF 2007 Workshop,</booktitle>
<pages>pages</pages>
<contexts>
<context position="27489" citStr="Kurimo et al., 2007" startWordPosition="4408" endWordPosition="4411">essary because very large scale web texts are available today. If a small set of texts needs to be analyzed with high accuracy, we can incorporate similar texts retrieved from the web, to increase the number of examples of unknown morphemes. The proposed method can be modified to check if unknown morphemes detected in the initial set are acquired and to terminate whenever sufficient acquisition coverage is achieved. 5 Related Work Since most languages delimit words by white-space, morphological analysis in these languages is to segment words into morphemes. For example, Morpho Challenge 2007 (Kurimo et al., 2007) was evaluations of unsupervised segmentation for English, Finnish, German and Turkish. While Japanese is an agglutinative language, other non-segmented languages such as Chinese and Thai are analytic languages. Among them, Chinese has been a subject of intensive research. Peng et al. (2004) integrated new word detection into word segmentation. They detected new words by computing segment confidence and re-analyzed the inputs with detected words as features. The Japanese language is unique in that it is written with several different character types. Heuristics widely used in unknown morpheme </context>
</contexts>
<marker>Kurimo, Creutz, Turunen, 2007</marker>
<rawString>Mikko Kurimo, Mathias Creutz, and Ville Turunen. 2007. Overview of Morpho Challenge in CLEF 2007. In Working Notes of the CLEF 2007 Workshop, pages 19–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Toshihisa Nakamura</author>
<author>Yuji Matsumoto</author>
<author>Makoto Nagao</author>
</authors>
<title>Improvements of Japanese morphological analyzer JUMAN.</title>
<date>1994</date>
<booktitle>In Procs. of The International Workshop on Sharable Natural Language Resources,</booktitle>
<pages>22--38</pages>
<contexts>
<context position="1430" citStr="Kurohashi et al., 1994" startWordPosition="198" endWordPosition="201">examples required to acquire a morpheme. Experiments show that unknown morphemes were acquired with high accuracy and improved the quality of morphological analysis. 1 Introduction Morphological analysis is the first step for most natural language processing applications. In Japanese morphological analysis, segmentation is processed simultaneously with the assignment of a part of speech (POS) tag to each morpheme. Segmentation is a nontrivial task in Japanese because it does not delimit words by white-space. Japanese morphological analysis has successfully adopted dictionary-based approaches (Kurohashi et al., 1994; Asahara and Matsumoto, 2000; Kudo et al., 2004). In these approaches, a sentence is transformed into a lattice of morphemes by searching a pre-defined dictionary, and an optimal path in the lattice is selected. This area of research may be considered almost completed, as previous studies reported the F-score of nearly 99% (Kudo et al., 2004). When applied to web texts, however, more errors are made due to unknown morphemes. In previous studies, experiments were performed on newspaper articles, but web texts include slang words, informal spelling alternates (Nishimura, 2003) and technical ter</context>
</contexts>
<marker>Kurohashi, Nakamura, Matsumoto, Nagao, 1994</marker>
<rawString>Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto, and Makoto Nagao. 1994. Improvements of Japanese morphological analyzer JUMAN. In Procs. of The International Workshop on Sharable Natural Language Resources, pages 22–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shinsuke Mori</author>
<author>Makoto Nagao</author>
</authors>
<title>Word extraction from corpora and its part-of-speech estimation using distributional analysis.</title>
<date>1996</date>
<booktitle>In Procs. of COLING</booktitle>
<pages>1119--1122</pages>
<contexts>
<context position="2289" citStr="Mori and Nagao, 1996" startWordPosition="335" endWordPosition="338"> be considered almost completed, as previous studies reported the F-score of nearly 99% (Kudo et al., 2004). When applied to web texts, however, more errors are made due to unknown morphemes. In previous studies, experiments were performed on newspaper articles, but web texts include slang words, informal spelling alternates (Nishimura, 2003) and technical terms. For example, the verb “ググる” (gugu-ru, to google) is erroneously segmented into “ググ” (gugu) and “る” (ru). One solution to this problem is to augment the lexicon of the morphological analyzer by extracting unknown morphemes from texts (Mori and Nagao, 1996). In the previous method, a morpheme extraction module worked independently of the morphological analyzer and ran in off-line (batch) mode. It is inefficient because almost all high-frequency morphemes have already been registered to the predefined dictionary. Moreover, it is inconvenient when applied to web texts because the web corpus is huge and diverse compared to newspaper corpora. It is not necessarily easy to build subcorpora before lexicon acquisition. Suppose that we want to analyze whaling-related documents. It is unnecessary and probably harmful to acquire morphemes that are irrelev</context>
<context position="23190" citStr="Mori and Nagao (1996)" startWordPosition="3721" endWordPosition="3724"> proper noun. The special POS tag “undefined” given by JUMAN was treated as noun. 4.3 Results Table 2 summarizes statistical information per query. The number of sentences affected by the augmentation varied considerably (1.04%–15.4%). The initial lexicon of the morphological analyzer lacked morphemes that appeared frequently in some corpora because morphological analysis had been tested mainly with newspaper articles. The precision of acquired morphemes was high (97.4%–99.3%), and the number of examples used for acquisition was as little as 4–9. These results are astonishing considering that Mori and Nagao (1996) ignored candidates that appeared less than 10 times (because they were unreliable). nouns because their morphological behaviors were the same as those of nouns. Although demonstrative nouns are closed class morphemes, their katakana forms such as “コレ” (this) were acquired as nouns. The morphological analyzer assumed that demonstrative pronouns were written in hiragana, e.g., “これ,” as they always are in a newspaper. Table 3 shows some acquired morphemes. As expected, the overwhelming majority were nouns (93.0%–100%) and katakana morphemes (80.7%– 91.6%). Some were mixed-character morphemes (“ソ</context>
</contexts>
<marker>Mori, Nagao, 1996</marker>
<rawString>Shinsuke Mori and Makoto Nagao. 1996. Word extraction from corpora and its part-of-speech estimation using distributional analysis. In Procs. of COLING 1996, pages 1119–1122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Nagata</author>
</authors>
<title>A part of speech estimation method for Japanese unknown words using a statistical model of morphology and context.</title>
<date>1999</date>
<booktitle>In Procs. of ACL</booktitle>
<pages>277--284</pages>
<contexts>
<context position="20257" citStr="Nagata (1999)" startWordPosition="3216" endWordPosition="3217">ries used were “捕鯨問 題” (whaling issue), “赤ちゃんポスト” (baby hatch), “ジャスラック” (JASRAC, a copyright collective), “ツンデレ” (tsundere, a slang word) and “アガリク ス” (agaricus). 4.2 Evaluation Measures The proposed method is evaluated by measuring the accuracy of acquired morphemes and their contribution to the improvement of morphological analysis. A morpheme is considered accurate if both segmentation and the POS tag are correct. Note that segmentation is a nontrivial problem for evaluation. In fact, the disagreement over segmentation criteria was considered one of the main reasons for reported errors by Nagata (1999) and Uchimoto et al. (2001). It is difficult to judge whether a compound term should be divided because there is no definite standard for morpheme boundaries in Japanese. For example, “ミンク鯨” (minku-kujira, minke whale) can be extracted as a single morpheme or decomposed into “ミンク” and “鯨.” While segmentation is an open question in Japanese morphological analysis, “correct” segmentation is not necessarily important for applications using morphological analysis. Even if a noun is split into two or more morphemes in morphological analysis, they are chunked to form a phrasal unit called bunsetsu i</context>
<context position="26818" citStr="Nagata, 1999" startWordPosition="4299" endWordPosition="4300">he monotonic increase of the numbers of acquired morphemes and stored examples suggests that the vocabulary size did not converge. The number of occurrences of acquired morphemes in re-analysis was approximately the same with the number of examples kept in the storage during acquisition. This means that, in terms of frequency of occurrence, about half of unknown morphemes were acquired. Most unknown morphemes belong to the “long tail” and the proposed method seems to have seized a “head” of the long tail. Although some previous studies emphasized correct identification of low frequency terms (Nagata, 1999; Asahara and Matsumoto, 2004), it is no longer necessary because very large scale web texts are available today. If a small set of texts needs to be analyzed with high accuracy, we can incorporate similar texts retrieved from the web, to increase the number of examples of unknown morphemes. The proposed method can be modified to check if unknown morphemes detected in the initial set are acquired and to terminate whenever sufficient acquisition coverage is achieved. 5 Related Work Since most languages delimit words by white-space, morphological analysis in these languages is to segment words i</context>
<context position="28206" citStr="Nagata (1999)" startWordPosition="4520" endWordPosition="4522">s an agglutinative language, other non-segmented languages such as Chinese and Thai are analytic languages. Among them, Chinese has been a subject of intensive research. Peng et al. (2004) integrated new word detection into word segmentation. They detected new words by computing segment confidence and re-analyzed the inputs with detected words as features. The Japanese language is unique in that it is written with several different character types. Heuristics widely used in unknown morpheme processing are based on character types. They were also used as important clues in statistical methods. Nagata (1999) integrated a probabilistic unknown word models into the word segmentation model. Uchimoto et al. (2001) incorporated them as feature functions of a Maximum Entropy-based morphological analyzer. Asahara and Matsumoto (2004) used them as a feature of character-based chunking of unknown words using Support Vector Machines. Mori (1996) extracted words from texts and estimated their POSs using distributional analysis. The appropriateness of a word candidate was measured num. of examples 436 by the distance between probability distributions of the candidate and a model. In this method, morphologica</context>
</contexts>
<marker>Nagata, 1999</marker>
<rawString>Masaaki Nagata. 1999. A part of speech estimation method for Japanese unknown words using a statistical model of morphology and context. In Procs. of ACL 1999, pages 277–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Guessing parts-of-speech of unknown words using global information.</title>
<date>2006</date>
<booktitle>In Procs. of COLING-ACL</booktitle>
<pages>705--712</pages>
<contexts>
<context position="28895" citStr="Nakagawa and Matsumoto (2006)" startWordPosition="4619" endWordPosition="4622">d segmentation model. Uchimoto et al. (2001) incorporated them as feature functions of a Maximum Entropy-based morphological analyzer. Asahara and Matsumoto (2004) used them as a feature of character-based chunking of unknown words using Support Vector Machines. Mori (1996) extracted words from texts and estimated their POSs using distributional analysis. The appropriateness of a word candidate was measured num. of examples 436 by the distance between probability distributions of the candidate and a model. In this method, morphological constraints were indirectly represented by distributions. Nakagawa and Matsumoto (2006) presented a method for guessing POS tags of pre-segmented unknown words that took into consideration all the occurrences of each unknown word in a document. This setting is impractical in Japanese because POS tagging is inseparable from segmentation. 6 Conclusion We propose a novel method that augments the lexicon of a Japanese morphological analyzer by acquiring unknown morphemes from texts in online mode. Unknown morphemes are acquired with high accuracy and improve the quality of morphological analysis. Unknown morphemes are one of the main sources of error in morphological analysis when w</context>
</contexts>
<marker>Nakagawa, Matsumoto, 2006</marker>
<rawString>Tetsuji Nakagawa and Yuji Matsumoto. 2006. Guessing parts-of-speech of unknown words using global information. In Procs. of COLING-ACL 2006, pages 705– 712.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Nakagawa</author>
<author>Tatsunori Mori</author>
</authors>
<title>A simple but powerful automatic term extraction method.</title>
<date>2002</date>
<booktitle>In COLING-02 on COMPUTERM</booktitle>
<pages>29--35</pages>
<contexts>
<context position="20929" citStr="Nakagawa and Mori, 2002" startWordPosition="3321" endWordPosition="3324">judge whether a compound term should be divided because there is no definite standard for morpheme boundaries in Japanese. For example, “ミンク鯨” (minku-kujira, minke whale) can be extracted as a single morpheme or decomposed into “ミンク” and “鯨.” While segmentation is an open question in Japanese morphological analysis, “correct” segmentation is not necessarily important for applications using morphological analysis. Even if a noun is split into two or more morphemes in morphological analysis, they are chunked to form a phrasal unit called bunsetsu in dependency parsing, and to extract a keyword (Nakagawa and Mori, 2002). To avoid the decompositionality problem, we adopted manual evaluation. We analyzed the target texts with both the initial lexicon and the augmented lexicon. Then we checked differences between the two analyses and extracted sentences that were affected by the augmentation. Among these sentences, we evaluated randomly selected 50 sentences per query. We checked the accuracy of segmentation and POS tagging of each “diff” block, which is illustrated in Figure 3. The segmentation of a block was judged correct unless morpheme boundaries were clearly wrong. In the evaluation of POS tagging, we did</context>
</contexts>
<marker>Nakagawa, Mori, 2002</marker>
<rawString>Hiroshi Nakagawa and Tatsunori Mori. 2002. A simple but powerful automatic term extraction method. In COLING-02 on COMPUTERM 2002, pages 29–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yukiko Nishimura</author>
</authors>
<title>Linguistic innovations and interactional features of casual online communication in Japanese.</title>
<date>2003</date>
<journal>Journal of Computer-Mediated Communication,</journal>
<volume>9</volume>
<issue>1</issue>
<contexts>
<context position="2012" citStr="Nishimura, 2003" startWordPosition="293" endWordPosition="294">approaches (Kurohashi et al., 1994; Asahara and Matsumoto, 2000; Kudo et al., 2004). In these approaches, a sentence is transformed into a lattice of morphemes by searching a pre-defined dictionary, and an optimal path in the lattice is selected. This area of research may be considered almost completed, as previous studies reported the F-score of nearly 99% (Kudo et al., 2004). When applied to web texts, however, more errors are made due to unknown morphemes. In previous studies, experiments were performed on newspaper articles, but web texts include slang words, informal spelling alternates (Nishimura, 2003) and technical terms. For example, the verb “ググる” (gugu-ru, to google) is erroneously segmented into “ググ” (gugu) and “る” (ru). One solution to this problem is to augment the lexicon of the morphological analyzer by extracting unknown morphemes from texts (Mori and Nagao, 1996). In the previous method, a morpheme extraction module worked independently of the morphological analyzer and ran in off-line (batch) mode. It is inefficient because almost all high-frequency morphemes have already been registered to the predefined dictionary. Moreover, it is inconvenient when applied to web texts because</context>
<context position="25723" citStr="Nishimura, 2003" startWordPosition="4122" endWordPosition="4123">mes in re-analysis Figure 4: Process of online acquisition neously segmented when extracting sentences from HTML. The katakana adjective “イイ” (i-i, good), a spelling variant of the basic morpheme “いい,” was falsely identified as a noun because its ending “イ” was written in katakana. The morphological analyzer, and hence the lexicon acquirer, assume that the ending of a verb or adjective is written in hiragana. This assumption is reasonable for standard Japanese, but does not always hold when we analyze web texts. In order to recognize unconventional spellings that are widely used in web texts (Nishimura, 2003), more flexible analysis is needed. 4.5 Discussion It is too costly or impractical to calculate the recall of acquisition, or the ratio of the number of acquired morphemes against the total number of unknown morphemes because it requires human judges to find undetected unknown morphemes from a large amount of raw texts. Alternatively, we examined the ratio against the number of detected unknown morphemes. Figure 4 shows the process of online acquisition for the query “JASRAC.” The monotonic increase of the numbers of acquired morphemes and stored examples suggests that the vocabulary size did </context>
</contexts>
<marker>Nishimura, 2003</marker>
<rawString>Yukiko Nishimura. 2003. Linguistic innovations and interactional features of casual online communication in Japanese. Journal of Computer-Mediated Communication, 9(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Fangfang Feng</author>
<author>Andrew McCallum</author>
</authors>
<title>Chinese segmentation and new word detection using conditional random fields.</title>
<date>2004</date>
<booktitle>In Procs. of COLING ’04,</booktitle>
<pages>562--568</pages>
<contexts>
<context position="27781" citStr="Peng et al. (2004)" startWordPosition="4451" endWordPosition="4454">own morphemes detected in the initial set are acquired and to terminate whenever sufficient acquisition coverage is achieved. 5 Related Work Since most languages delimit words by white-space, morphological analysis in these languages is to segment words into morphemes. For example, Morpho Challenge 2007 (Kurimo et al., 2007) was evaluations of unsupervised segmentation for English, Finnish, German and Turkish. While Japanese is an agglutinative language, other non-segmented languages such as Chinese and Thai are analytic languages. Among them, Chinese has been a subject of intensive research. Peng et al. (2004) integrated new word detection into word segmentation. They detected new words by computing segment confidence and re-analyzed the inputs with detected words as features. The Japanese language is unique in that it is written with several different character types. Heuristics widely used in unknown morpheme processing are based on character types. They were also used as important clues in statistical methods. Nagata (1999) integrated a probabilistic unknown word models into the word segmentation model. Uchimoto et al. (2001) incorporated them as feature functions of a Maximum Entropy-based morp</context>
</contexts>
<marker>Peng, Feng, McCallum, 2004</marker>
<rawString>Fuchun Peng, Fangfang Feng, and Andrew McCallum. 2004. Chinese segmentation and new word detection using conditional random fields. In Procs. of COLING ’04, pages 562–568.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keiji Shinzato</author>
<author>Tomohide Shibata</author>
<author>Daisuke Kawahara</author>
<author>Chikara Hashimoto</author>
<author>Sadao Kurohashi</author>
</authors>
<title>TSUBAKI: An open search engine infrastructure for developing new information access methodology.</title>
<date>2008</date>
<booktitle>In Procs. of IJCNLP-08,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="19366" citStr="Shinzato et al., 2008" startWordPosition="3075" endWordPosition="3078">14 107 9 (agaricus) (3.85%) (93.9%) 1 The median number of examples used for acquisition. 4 Experiments 4.1 Experimental Design We used the default dictionary of the morphological analyzer JUMAN as the initial lexicon. It contained 30 thousand basic morphemes. If spelling variants were expanded and proper nouns were counted, the total number of morphemes was 120 thousands. We used domain-specific corpora as target texts because efficient acquisition was expected. If target texts shared a topic, relevant unknown morphemes were used frequently. In the experiments, we used search engine TSUBAKI (Shinzato et al., 2008) and casted the search results as domain-specific corpora. For each query, our system sequentially read pages from the top of the result and acquired morphemes. We terminated the acquisition at the 1000th page and analyzed the same 1000 pages with the augmented lexicon. The queries used were “捕鯨問 題” (whaling issue), “赤ちゃんポスト” (baby hatch), “ジャスラック” (JASRAC, a copyright collective), “ツンデレ” (tsundere, a slang word) and “アガリク ス” (agaricus). 4.2 Evaluation Measures The proposed method is evaluated by measuring the accuracy of acquired morphemes and their contribution to the improvement of morpholo</context>
</contexts>
<marker>Shinzato, Shibata, Kawahara, Hashimoto, Kurohashi, 2008</marker>
<rawString>Keiji Shinzato, Tomohide Shibata, Daisuke Kawahara, Chikara Hashimoto, and Sadao Kurohashi. 2008. TSUBAKI: An open search engine infrastructure for developing new information access methodology. In Procs. of IJCNLP-08, pages 189–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyotaka Uchimoto</author>
<author>Satoshi Sekine</author>
<author>Hitoshi Isahara</author>
</authors>
<title>The unknown word problem: a morphological analysis of Japanese using maximum entropy aided by a dictionary.</title>
<date>2001</date>
<booktitle>In Procs. of EMNLP</booktitle>
<pages>91--99</pages>
<contexts>
<context position="20284" citStr="Uchimoto et al. (2001)" startWordPosition="3219" endWordPosition="3222">問 題” (whaling issue), “赤ちゃんポスト” (baby hatch), “ジャスラック” (JASRAC, a copyright collective), “ツンデレ” (tsundere, a slang word) and “アガリク ス” (agaricus). 4.2 Evaluation Measures The proposed method is evaluated by measuring the accuracy of acquired morphemes and their contribution to the improvement of morphological analysis. A morpheme is considered accurate if both segmentation and the POS tag are correct. Note that segmentation is a nontrivial problem for evaluation. In fact, the disagreement over segmentation criteria was considered one of the main reasons for reported errors by Nagata (1999) and Uchimoto et al. (2001). It is difficult to judge whether a compound term should be divided because there is no definite standard for morpheme boundaries in Japanese. For example, “ミンク鯨” (minku-kujira, minke whale) can be extracted as a single morpheme or decomposed into “ミンク” and “鯨.” While segmentation is an open question in Japanese morphological analysis, “correct” segmentation is not necessarily important for applications using morphological analysis. Even if a noun is split into two or more morphemes in morphological analysis, they are chunked to form a phrasal unit called bunsetsu in dependency parsing, and t</context>
<context position="28310" citStr="Uchimoto et al. (2001)" startWordPosition="4534" endWordPosition="4538">c languages. Among them, Chinese has been a subject of intensive research. Peng et al. (2004) integrated new word detection into word segmentation. They detected new words by computing segment confidence and re-analyzed the inputs with detected words as features. The Japanese language is unique in that it is written with several different character types. Heuristics widely used in unknown morpheme processing are based on character types. They were also used as important clues in statistical methods. Nagata (1999) integrated a probabilistic unknown word models into the word segmentation model. Uchimoto et al. (2001) incorporated them as feature functions of a Maximum Entropy-based morphological analyzer. Asahara and Matsumoto (2004) used them as a feature of character-based chunking of unknown words using Support Vector Machines. Mori (1996) extracted words from texts and estimated their POSs using distributional analysis. The appropriateness of a word candidate was measured num. of examples 436 by the distance between probability distributions of the candidate and a model. In this method, morphological constraints were indirectly represented by distributions. Nakagawa and Matsumoto (2006) presented a me</context>
</contexts>
<marker>Uchimoto, Sekine, Isahara, 2001</marker>
<rawString>Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara. 2001. The unknown word problem: a morphological analysis of Japanese using maximum entropy aided by a dictionary. In Procs. of EMNLP 2001, pages 91–99.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>