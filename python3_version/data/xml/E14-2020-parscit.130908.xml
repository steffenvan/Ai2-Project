<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.036789">
<title confidence="0.995574">
Annotating by Proving using SemAnTE
</title>
<author confidence="0.985751333333333">
Assaf Toledo1 Stavroula Alexandropoulou1 Sophie Chesney2
Robert Grimm1 Pepijn Kokke1 Benno Kruit3
Kyriaki Neophytou1 Antony Nguyen1 Yoad Winter1
</author>
<affiliation confidence="0.960016">
1 - Utrecht University 2 - University College London 3 - University of Amsterdam
</affiliation>
<email confidence="0.973869666666667">
{a.toledo,s.alexandropoulou,y.winter}@uu.nl
sophie.chesney.10@ucl.ac.uk, {pepijn.kokke,bennokr}@gmail.com
{r.m.grimm,k.neophytou,a.h.nguyen}@students.uu.nl
</email>
<sectionHeader confidence="0.998503" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999991047619048">
We present SemAnTE, a platform for
marking and substantiating a semantic an-
notation scheme of textual entailment ac-
cording to a formal model. The plat-
form introduces a novel approach to an-
notation by providing annotators immedi-
ate feedback whether the data they mark
are substantiated: for positive entailment
pairs, the system uses the annotations to
search for a formal logical proof that val-
idates the entailment relation; for negative
pairs, the system verifies that a counter-
model can be constructed. By integrating
a web-based user-interface, a formal lexi-
con, a lambda-calculus engine and an off-
the-shelf theorem prover, this platform fa-
cilitates the creation of annotated corpora
of textual entailment. A corpus of several
hundred annotated entailments is currently
in preparation using the platform and will
be available for the research community.
</bodyText>
<sectionHeader confidence="0.999632" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.973915727272727">
The Recognizing Textual Entailment (RTE) chal-
lenges (Dagan et al., 2006) advance the devel-
opment of systems that automatically determine
whether an entailment relation obtains between a
naturally occurring text T and a manually com-
posed hypothesis H. The RTE corpus (Bar Haim
et al., 2006; Giampiccolo et al., 2008), which is
currently the only available resource of textual en-
tailments, marks entailment candidates as posi-
tive/negative.1 For example:
Example 1
</bodyText>
<listItem confidence="0.99983425">
• T: The book contains short stories by the fa-
mous Bulgarian writer, Nikolai Haitov.
• H: Nikolai Haitov is a writer.2
• Entailment: Positive
</listItem>
<bodyText confidence="0.999907277777778">
This categorization does not indicate the linguistic
phenomena that underlie entailment or their con-
tribution to inferential processes. In default of
a gold standard identifying linguistic phenomena
triggering inferences, entailment systems can be
compared based on their performance, but the in-
ferential processes they employ to recognize en-
tailment are not directly accessible and conse-
quently cannot be either evaluated or improved
straightforwardly.
We address this problem by elucidating some
of the central inferential processes underlying en-
tailments in the RTE corpus, which we model for-
mally within a standard semantic theory. This al-
lows us not only to indicate linguistic phenomena
that are involved in the recognition of entailment
by speakers, but also to provide formal proofs that
substantiate the annotations and explain how the
</bodyText>
<footnote confidence="0.999495857142857">
1Pairs of sentences in RTE 1-3 are categorized in two
classes: yes- or no-entailment; pairs in RTE 4-5 are cate-
gorized in three classes: entailment, contradiction and un-
known. We label the judgments yes-entailment from RTE 1-3
and entailment from RTE 4-5 as positive, and the other judg-
ments as negative.
2Pair 622 from the development set of RTE 2.
</footnote>
<page confidence="0.983268">
77
</page>
<note confidence="0.7774465">
Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 77–80,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.99984455">
modeled phenomena interact and contribute to the
recognition process. In this sense the we adopt an
Annotating by Proving approach to textual entail-
ment annotation.
The annotation work is done using the Se-
mAnTE (Semantic Annotation of Textual Entail-
ment) platform, which incorporates a web-based
user-interface, a formal lexicon, a lambda-calculus
engine and an off-the-shelf theorem prover. We
are currently using this platform to build a new
corpus of several hundred annotated entailments
comprising both positive and negative pairs. We
decided to focus on the semantic phenomena of
appositive, restrictive and intersective modifica-
tion as these semantic phenomena are prevalent in
the RTE datasets and can be annotated with high
consistency, and as their various syntactic expres-
sions can be captured by a limited set of concepts.3
In the future, we plan to extend this sematic model
to cover other, more complex phenomena.
</bodyText>
<sectionHeader confidence="0.996739" genericHeader="method">
2 Semantic Model
</sectionHeader>
<bodyText confidence="0.9999894">
To model entailment in natural language, we as-
sume that entailment describes a preorder on sen-
tences. Thus, any sentence trivially entails itself
(reflexivity); and given two entailments T1 ==&gt;. H1
and T2 ==&gt;. H2 where H1 and T2 are identical sen-
tences, we assume T1 ==&gt;. H2 (transitivity). We
use a standard model-theoretical extensional se-
mantics, based on the simple partial order on the
domain of truth-values. Each model M assigns
sentences a truth-value in the set 10, 11. Such a
Tarskian theory of entailment is considered ade-
quate if the intuitive entailment preorder on sen-
tences can be described as the pairs of sentences
T and H whose truth-values QT]m and QH]m sat-
isfy QT]m G QH]m for all models M.
We use annotations to link between textual
representations in natural language and model-
theoretic representations. This link is established
by marking the words and structural configura-
tions in T and H with lexical items that encode
semantic meanings for the linguistic phenomena
that we model. The lexical items are defined for-
mally in a lexicon, as illustrated in Table 1 for ma-
jor lexical categories over type:s e for entities, t
for truth-values, and their functional compounds.
</bodyText>
<footnote confidence="0.76132075">
3This conclusion is based on an analysis of RTE 1-4, in
which these modification phenomena were found to occur in
80.65% of the entailments and were annotated with cross-
annotator agreement of 68% on average.
</footnote>
<table confidence="0.999807363636364">
Category Type Example Denotation
Proper Name e Dan dan
Indef. Article (et)(et) a A
Def. Article (et)e the ι
Copula (et)(et) is IS
Noun et book book
Intrans. verb et sit sit
Trans. verb eet contain contain
Pred. Conj. (et)((et)(et)) and AND
Res. Adj. (et)(et) short R.(short)
Exist. Quant. (et)(et)t some SOME
</table>
<tableCaption confidence="0.999598">
Table 1: Lexicon Illustration
</tableCaption>
<bodyText confidence="0.999724384615385">
Denotations that are assumed to be arbitrary are
given in boldface. For example, the intransitive
verb sit is assigned the type et, which describes
functions from entities to a truth-values, and its
denotation sit is an arbitrary function of this type.
By contrast, other lexical items have their denota-
tions restricted by the given model M. As illus-
trated in Figure 1, the coordinator and is assigned
the type (et)((et)(et)) and its denotation is a func-
tion that takes a function A of type et and returns
a function that takes a function B, also of type et,
and returns a function that takes an entity x and
returns 1 if and only if x satisfies both A and B.
</bodyText>
<equation confidence="0.998933375">
A = IS = λAet.A
� a A = (λxe.x = a)
ι = λAet.
undefined otherwise
WHOA = λAet.λxe.ι(λy.y = x ∧ A(x))
Rm = λM(et)(et).λAet.λxe.M(A)(x) ∧ A(x)
SOME = λAet.λBet.]x.A(x) ∧ B(x)
AND = λAet.λBet.λxe.A(x) ∧ B(x)
</equation>
<figureCaption confidence="0.99955">
Figure 1: Functions in the Lexicon
</figureCaption>
<bodyText confidence="0.9999405">
By marking words and syntactic constructions
with lexical items, annotators indicate the under-
lying linguistic phenomena in the data. Further-
more, the formal foundation of this approach al-
lows annotators to verify that the entailment re-
lation (or lack thereof) that obtains between the
textual forms of T and H also obtains between
their respective semantic forms. This verification
guarantees that the annotations are sufficient in the
sense of providing enough information for recog-
nizing the entailment relation based on the seman-
tic abstraction. For example, consider the simple
entailment Dan sat and sang ==&gt;. Dan sang and as-
sume annotations of Dan as a proper name, sat
and sang as intransitive verbs and and as predi-
cate conjunction. The formal model can be used
to verify these annotations by constructing a proof
as follows: for each model M:
</bodyText>
<page confidence="0.922787">
78
</page>
<equation confidence="0.9739758">
Q Dan [sat [and sang]] ]M
= ((AND(sing))(sit))(dan) analysis
= (((λAet.λBet.λxe.A(x) ∧ def. of AND
B(x))(sing))(sit))(dan)
= sit(dan) ∧ sing(dan) func. app. to sing,
</equation>
<bodyText confidence="0.585492333333333">
sit and dan
≤ sing(dan) def. of ∧
= Q Dan sang ]M analysis
</bodyText>
<sectionHeader confidence="0.985993" genericHeader="method">
3 Platform Architecture
</sectionHeader>
<bodyText confidence="0.9958465">
The platform’s architecture is based on a client-
server model, as illustrated in Figure 2.
</bodyText>
<figureCaption confidence="0.998352">
Figure 2: Platform Architecture
</figureCaption>
<bodyText confidence="0.9975903125">
The user interface (UI) is implemented as a
web-based client using Google Web Toolkit (Ol-
son, 2007) and allows multiple annotators to ac-
cess the RTE data, to annotate, and to substanti-
ate their annotations. These operations are done
by invoking corresponding remote procedure calls
at the server side. Below we describe the system
components as we go over the work-flow of anno-
tating Example 1.
Data Preparation: We extract T-H pairs from
the RTE datasets XML files and use the Stanford
CoreNLP (Klein and Manning, 2003; Toutanova
et al., 2003; de Marneffe et al., 2006) to parse each
pair and to annotate it with part-of-speech tags.4
Consequently, we apply a naive heuristic to map
the PoS tags to the lexicon.5 This process is called
</bodyText>
<footnote confidence="0.7138754">
4Version 1.3.4
5This heuristic is naive in the sense of not disambiguating
verbs, adjectives and other types of terms according to their
semantic features. It is meant to provide a starting point for
the annotators to correct and fine-tune.
</footnote>
<bodyText confidence="0.999969473684211">
as part of the platform’s installation and when an-
notators need to simplify the original RTE data in
order to avoid syntactic/semantic phenomena that
the semantic engine does not support. For exam-
ple, the bare plural short stories is simplified to
some short stories as otherwise the engine is un-
able to determine the quantification of this noun.
Annotation: The annotation is done by mark-
ing the tree-leaves with entries from the lexicon.
For example, short is annotated as a restrictive
modifier (MR) of the noun stories, and contains
is annotated as a transitive verb (V 2). In addition,
annotators manipulate the tree structure to fix pars-
ing mistakes and to add leaves that mark semantic
relations. For instance, a leaf that indicates the ap-
position between the famous Bulgarian writer and
Nikolai Haitov is added and annotated as WHOA.
The server stores a list of all annotation actions.
Figure 3 shows the tree-view, lexicon, prover and
annotation history panels in the UI.
Proving: When annotating all leaves and ma-
nipulating the tree structures of T and H are done,
the annotators use the prover interface to request
a search for a proof that indicates that their anno-
tations are substantiated. Firstly, the system uses
lambda calculus reductions to create logical forms
that represent the meanings of T and H in higher-
order logic. At this stage, type errors may be re-
ported due to erroneous parse-trees or annotations.
In this case an annotator will fix the errors and re-
run the proving step. Secondly, once all type er-
rors are resolved, the higher-order representations
are lowered to first order and Prover9 (McCune,
2010) is executed to search for a proof between
the logical expressions of T and H.6 The proofs
are recorded in order to be included in the corpus
release. Figure 4 shows the result of translating T
and H to an input to Prover9.
</bodyText>
<sectionHeader confidence="0.97858" genericHeader="method">
4 Corpus Preparation
</sectionHeader>
<bodyText confidence="0.9994519">
We have so far completed annotating 40 positive
entailments based on data from RTE 1-4. The an-
notation is a work in progress, done by four Master
students of Linguistics who are experts in the data
and focus on entailments whose recognition re-
lies on a mixture of appositive, restrictive or inter-
sective modification. As we progress towards the
compilation of a corpus of several hundred pairs,
we extend the semantic model to support more in-
ferences with less phenomena simplification.
</bodyText>
<footnote confidence="0.985027">
6Version 2009-11A
</footnote>
<page confidence="0.996832">
79
</page>
<figureCaption confidence="0.993993">
Figure 3: User Interface Panels: Annotation History, Tree-View, Prover Interface and Lexicon Toolbox
</figureCaption>
<figure confidence="0.816952272727273">
formulas(assumptions).
all x0 (((writer(x0) &amp; bulgarian(x0)) &amp;
famous writer bulgarian(x0)) ↔ x0=c1).
all x0 (((stories(x0) &amp; short stories(x0)) &amp; exists x1 (by
stories short stories(x1, x0) &amp; (x1=c1 &amp; x1=Nikolai
Haitov))) ↔ x0=c2). all x0 (book(x0) ↔ x0=c3).
contains(c2, c3).
end of list.
formulas(goals).
exists x0 (writer(x0) &amp; x0=Nikolai Haitov).
end of list.
</figure>
<figureCaption confidence="0.99242">
Figure 4: Input for Theorem Prover
</figureCaption>
<sectionHeader confidence="0.999203" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999961">
We introduced a new concept of an annotation
platform which implements an Annotating by
Proving approach. The platform is currently in
use by annotators to indicate linguistic phenomena
in entailment data and to provide logical proofs
that substantiate their annotations. This method
guarantees that the annotations constitute a com-
plete description of the entailment relation and can
serve as a gold-standard for entailment recogniz-
ers. The new corpus will be publicly available.
</bodyText>
<sectionHeader confidence="0.998137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999658166666667">
The work of Stavroula Alexandropoulou, Robert
Grimm, Sophie Chesney, Pepijn Kokke, Benno
Kruit, Kyriaki Neophytou, Antony Nguyen, Assaf
Toledo and Yoad Winter was supported by a VICI
grant number 277-80-002 by the Netherlands Or-
ganisation for Scientific Research (NWO).
</bodyText>
<sectionHeader confidence="0.99962" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999953307692308">
Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro,
Danilo Giampiccolo, Bernardo Magnini, and Idan
Szpektor. 2006. The second pascal recognising
textual entailment challenge. In Proceedings of the
Second PASCAL Challenges Workshop on Recognis-
ing Textual Entailment.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. Machine Learning Challenges. Evaluat-
ing Predictive Uncertainty, Visual Object Classifi-
cation, and Recognising Tectual Entailment, pages
177–190.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the IEEE/ ACL 2006 Workshop on
Spoken Language Technology. The Stanford Natural
Language Processing Group.
Danilo Giampiccolo, Hoa Trang Dang, Bernardo
Magnini, Ido Dagan, and Elena Cabrio. 2008. The
fourth pascal recognising textual entailment chal-
lenge. In TAC 2008 Proceedings.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ’03, pages 423–
430, Stroudsburg, PA, USA. ACL.
William McCune. 2010. Prover9 and Mace4. http:
//www.cs.unm.edu/˜mccune/prover9/.
Steven Douglas Olson. 2007. Ajax on Java. O’Reilly
Media.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology
- Volume 1, NAACL ’03, pages 173–180, Strouds-
burg, PA, USA. ACL.
</reference>
<page confidence="0.998249">
80
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.188024">
<title confidence="0.9072865">Annotating by Proving using SemAnTE Stavroula Sophie</title>
<author confidence="0.953643">Pepijn Benno</author>
<note confidence="0.4602755">Antony 1- Utrecht University 2- University College London 3- University of</note>
<abstract confidence="0.998998863636364">We present SemAnTE, a platform for marking and substantiating a semantic annotation scheme of textual entailment according to a formal model. The platform introduces a novel approach to annotation by providing annotators immediate feedback whether the data they mark are substantiated: for positive entailment pairs, the system uses the annotations to search for a formal logical proof that validates the entailment relation; for negative pairs, the system verifies that a countermodel can be constructed. By integrating a web-based user-interface, a formal lexicon, a lambda-calculus engine and an offthe-shelf theorem prover, this platform facilitates the creation of annotated corpora of textual entailment. A corpus of several hundred annotated entailments is currently in preparation using the platform and will be available for the research community.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Roy Bar Haim</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
<author>Lisa Ferro</author>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
<author>Idan Szpektor</author>
</authors>
<title>The second pascal recognising textual entailment challenge.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment.</booktitle>
<contexts>
<context position="1588" citStr="Haim et al., 2006" startWordPosition="219" endWordPosition="222">ace, a formal lexicon, a lambda-calculus engine and an offthe-shelf theorem prover, this platform facilitates the creation of annotated corpora of textual entailment. A corpus of several hundred annotated entailments is currently in preparation using the platform and will be available for the research community. 1 Introduction The Recognizing Textual Entailment (RTE) challenges (Dagan et al., 2006) advance the development of systems that automatically determine whether an entailment relation obtains between a naturally occurring text T and a manually composed hypothesis H. The RTE corpus (Bar Haim et al., 2006; Giampiccolo et al., 2008), which is currently the only available resource of textual entailments, marks entailment candidates as positive/negative.1 For example: Example 1 • T: The book contains short stories by the famous Bulgarian writer, Nikolai Haitov. • H: Nikolai Haitov is a writer.2 • Entailment: Positive This categorization does not indicate the linguistic phenomena that underlie entailment or their contribution to inferential processes. In default of a gold standard identifying linguistic phenomena triggering inferences, entailment systems can be compared based on their performance,</context>
</contexts>
<marker>Haim, Dagan, Dolan, Ferro, Giampiccolo, Magnini, Szpektor, 2006</marker>
<rawString>Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. 2006. The second pascal recognising textual entailment challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The pascal recognising textual entailment challenge. Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment,</title>
<date>2006</date>
<pages>177--190</pages>
<contexts>
<context position="1372" citStr="Dagan et al., 2006" startWordPosition="184" endWordPosition="187">m uses the annotations to search for a formal logical proof that validates the entailment relation; for negative pairs, the system verifies that a countermodel can be constructed. By integrating a web-based user-interface, a formal lexicon, a lambda-calculus engine and an offthe-shelf theorem prover, this platform facilitates the creation of annotated corpora of textual entailment. A corpus of several hundred annotated entailments is currently in preparation using the platform and will be available for the research community. 1 Introduction The Recognizing Textual Entailment (RTE) challenges (Dagan et al., 2006) advance the development of systems that automatically determine whether an entailment relation obtains between a naturally occurring text T and a manually composed hypothesis H. The RTE corpus (Bar Haim et al., 2006; Giampiccolo et al., 2008), which is currently the only available resource of textual entailments, marks entailment candidates as positive/negative.1 For example: Example 1 • T: The book contains short stories by the famous Bulgarian writer, Nikolai Haitov. • H: Nikolai Haitov is a writer.2 • Entailment: Positive This categorization does not indicate the linguistic phenomena that </context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The pascal recognising textual entailment challenge. Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment, pages 177–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating Typed Dependency Parses from Phrase Structure Parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the IEEE/ ACL 2006 Workshop on Spoken Language Technology. The Stanford Natural Language Processing Group.</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating Typed Dependency Parses from Phrase Structure Parses. In Proceedings of the IEEE/ ACL 2006 Workshop on Spoken Language Technology. The Stanford Natural Language Processing Group.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Giampiccolo</author>
<author>Hoa Trang Dang</author>
<author>Bernardo Magnini</author>
<author>Ido Dagan</author>
<author>Elena Cabrio</author>
</authors>
<title>The fourth pascal recognising textual entailment challenge.</title>
<date>2008</date>
<booktitle>In TAC</booktitle>
<note>Proceedings.</note>
<contexts>
<context position="1615" citStr="Giampiccolo et al., 2008" startWordPosition="223" endWordPosition="226">on, a lambda-calculus engine and an offthe-shelf theorem prover, this platform facilitates the creation of annotated corpora of textual entailment. A corpus of several hundred annotated entailments is currently in preparation using the platform and will be available for the research community. 1 Introduction The Recognizing Textual Entailment (RTE) challenges (Dagan et al., 2006) advance the development of systems that automatically determine whether an entailment relation obtains between a naturally occurring text T and a manually composed hypothesis H. The RTE corpus (Bar Haim et al., 2006; Giampiccolo et al., 2008), which is currently the only available resource of textual entailments, marks entailment candidates as positive/negative.1 For example: Example 1 • T: The book contains short stories by the famous Bulgarian writer, Nikolai Haitov. • H: Nikolai Haitov is a writer.2 • Entailment: Positive This categorization does not indicate the linguistic phenomena that underlie entailment or their contribution to inferential processes. In default of a gold standard identifying linguistic phenomena triggering inferences, entailment systems can be compared based on their performance, but the inferential proces</context>
</contexts>
<marker>Giampiccolo, Dang, Magnini, Dagan, Cabrio, 2008</marker>
<rawString>Danilo Giampiccolo, Hoa Trang Dang, Bernardo Magnini, Ido Dagan, and Elena Cabrio. 2008. The fourth pascal recognising textual entailment challenge. In TAC 2008 Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03,</booktitle>
<pages>423--430</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8665" citStr="Klein and Manning, 2003" startWordPosition="1375" endWordPosition="1378">hitecture is based on a clientserver model, as illustrated in Figure 2. Figure 2: Platform Architecture The user interface (UI) is implemented as a web-based client using Google Web Toolkit (Olson, 2007) and allows multiple annotators to access the RTE data, to annotate, and to substantiate their annotations. These operations are done by invoking corresponding remote procedure calls at the server side. Below we describe the system components as we go over the work-flow of annotating Example 1. Data Preparation: We extract T-H pairs from the RTE datasets XML files and use the Stanford CoreNLP (Klein and Manning, 2003; Toutanova et al., 2003; de Marneffe et al., 2006) to parse each pair and to annotate it with part-of-speech tags.4 Consequently, we apply a naive heuristic to map the PoS tags to the lexicon.5 This process is called 4Version 1.3.4 5This heuristic is naive in the sense of not disambiguating verbs, adjectives and other types of terms according to their semantic features. It is meant to provide a starting point for the annotators to correct and fine-tune. as part of the platform’s installation and when annotators need to simplify the original RTE data in order to avoid syntactic/semantic phenom</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03, pages 423– 430, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William McCune</author>
</authors>
<date>2010</date>
<booktitle>Prover9 and Mace4. http: //www.cs.unm.edu/˜mccune/prover9/.</booktitle>
<contexts>
<context position="10758" citStr="McCune, 2010" startWordPosition="1728" endWordPosition="1729">manipulating the tree structures of T and H are done, the annotators use the prover interface to request a search for a proof that indicates that their annotations are substantiated. Firstly, the system uses lambda calculus reductions to create logical forms that represent the meanings of T and H in higherorder logic. At this stage, type errors may be reported due to erroneous parse-trees or annotations. In this case an annotator will fix the errors and rerun the proving step. Secondly, once all type errors are resolved, the higher-order representations are lowered to first order and Prover9 (McCune, 2010) is executed to search for a proof between the logical expressions of T and H.6 The proofs are recorded in order to be included in the corpus release. Figure 4 shows the result of translating T and H to an input to Prover9. 4 Corpus Preparation We have so far completed annotating 40 positive entailments based on data from RTE 1-4. The annotation is a work in progress, done by four Master students of Linguistics who are experts in the data and focus on entailments whose recognition relies on a mixture of appositive, restrictive or intersective modification. As we progress towards the compilatio</context>
</contexts>
<marker>McCune, 2010</marker>
<rawString>William McCune. 2010. Prover9 and Mace4. http: //www.cs.unm.edu/˜mccune/prover9/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Douglas Olson</author>
</authors>
<date>2007</date>
<booktitle>Ajax on Java. O’Reilly Media.</booktitle>
<contexts>
<context position="8245" citStr="Olson, 2007" startWordPosition="1306" endWordPosition="1308">d as predicate conjunction. The formal model can be used to verify these annotations by constructing a proof as follows: for each model M: 78 Q Dan [sat [and sang]] ]M = ((AND(sing))(sit))(dan) analysis = (((λAet.λBet.λxe.A(x) ∧ def. of AND B(x))(sing))(sit))(dan) = sit(dan) ∧ sing(dan) func. app. to sing, sit and dan ≤ sing(dan) def. of ∧ = Q Dan sang ]M analysis 3 Platform Architecture The platform’s architecture is based on a clientserver model, as illustrated in Figure 2. Figure 2: Platform Architecture The user interface (UI) is implemented as a web-based client using Google Web Toolkit (Olson, 2007) and allows multiple annotators to access the RTE data, to annotate, and to substantiate their annotations. These operations are done by invoking corresponding remote procedure calls at the server side. Below we describe the system components as we go over the work-flow of annotating Example 1. Data Preparation: We extract T-H pairs from the RTE datasets XML files and use the Stanford CoreNLP (Klein and Manning, 2003; Toutanova et al., 2003; de Marneffe et al., 2006) to parse each pair and to annotate it with part-of-speech tags.4 Consequently, we apply a naive heuristic to map the PoS tags to</context>
</contexts>
<marker>Olson, 2007</marker>
<rawString>Steven Douglas Olson. 2007. Ajax on Java. O’Reilly Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03,</booktitle>
<pages>173--180</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8689" citStr="Toutanova et al., 2003" startWordPosition="1379" endWordPosition="1382">lientserver model, as illustrated in Figure 2. Figure 2: Platform Architecture The user interface (UI) is implemented as a web-based client using Google Web Toolkit (Olson, 2007) and allows multiple annotators to access the RTE data, to annotate, and to substantiate their annotations. These operations are done by invoking corresponding remote procedure calls at the server side. Below we describe the system components as we go over the work-flow of annotating Example 1. Data Preparation: We extract T-H pairs from the RTE datasets XML files and use the Stanford CoreNLP (Klein and Manning, 2003; Toutanova et al., 2003; de Marneffe et al., 2006) to parse each pair and to annotate it with part-of-speech tags.4 Consequently, we apply a naive heuristic to map the PoS tags to the lexicon.5 This process is called 4Version 1.3.4 5This heuristic is naive in the sense of not disambiguating verbs, adjectives and other types of terms according to their semantic features. It is meant to provide a starting point for the annotators to correct and fine-tune. as part of the platform’s installation and when annotators need to simplify the original RTE data in order to avoid syntactic/semantic phenomena that the semantic en</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03, pages 173–180, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>