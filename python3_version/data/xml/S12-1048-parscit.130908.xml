<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003461">
<title confidence="0.931845">
SemEval-2012 Task 3: Spatial Role Labeling
</title>
<author confidence="0.95717">
Parisa Kordjamshidi Steven Bethard Marie-Francine Moens
</author>
<affiliation confidence="0.931506">
Katholieke Universiteit Leuven University of Colorado Katholieke Universiteit Leuven
</affiliation>
<bodyText confidence="0.641885">
parisa.kordjamshidi@ steven.bethard@ sien.moens@
cs.kuleuven.be colorado.edu cs.kuleuven.be
</bodyText>
<sectionHeader confidence="0.974408" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.993059352941176">
This SemEval2012 shared task is based on a
recently introduced spatial annotation scheme
called Spatial Role Labeling. The Spatial Role
Labeling task concerns the extraction of main
components of the spatial semantics from nat-
ural language: trajectors, landmarks and spa-
tial indicators. In addition to these major
components, the links between them and the
general-type of spatial relationships includ-
ing region, direction and distance are targeted.
The annotated dataset contains about 1213
sentences which describe 612 images of the
CLEF IAPR TC-12 Image Benchmark. We
have one participant system with two runs.
The participant’s runs are compared to the sys-
tem in (Kordjamshidi et al., 2011c) which is
provided by task organizers.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999972653061225">
One of the essential functions of natural language is
to talk about spatial relationships between objects.
The sentence “Give me the book on AI on the big
table behind the wall.” expresses information about
the spatial configuration of the objects (book, table,
wall) in some space. Particularly, it explains the re-
gion occupied by the book with respect to the table
and the direction (orientation) of the table with re-
spect to the wall. Understanding such spatial utter-
ances is a problem in many areas, including robotics,
navigation, traffic management, and query answer-
ing systems (Tappan, 2004).
Linguistic constructs can express highly complex,
relational structures of objects, spatial relations be-
tween them, and patterns of motion through space
relative to some reference point. Compared to nat-
ural language, formal spatial models focus on one
particular spatial aspect such as orientation, topol-
ogy or distance and specify its underlying spatial
logic in detail (Hois and Kutz, 2008). These for-
mal models enable spatial reasoning that is difficult
to perform on natural language expressions.
Learning how to map natural language spatial in-
formation onto a formal representation is a challeng-
ing problem. The complexity of spatial semantics
from the cognitive-linguistic point of view on the
one hand, the diversity of formal spatial represen-
tation models in different applications on the other
hand and the gap between the specification level of
the two sides has led to the present situation that no
well-defined framework for automatic spatial infor-
mation extraction exists that can handle all of these
aspects.
In a previous paper (Kordjamshidi et al., 2010b),
we introduced the task of spatial role labeling
(SpRL) and proposed an annotation scheme that is
language-independent and practically facilitates the
application of machine learning techniques. Our
framework consists of a set of spatial roles based
on the theory of holistic spatial semantics (Zlat-
evl, 2007) with the intent of covering the main as-
pects of spatial concepts at a course level, includ-
ing both static and dynamic spatial semantics. This
shared task is defined on the basis of that annota-
tion scheme. Since this is the first shared task on the
spatial information and this particular data, we pro-
posed a simplified version of the original scheme.
The intention of this simplification was to make this
practice feasible in the given timeframe. However,
</bodyText>
<page confidence="0.985154">
365
</page>
<note confidence="0.972706">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 365–373,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999857265625">
the current task is very challenging particularly for
learning the spatial links and relations.
The core problem of SpRL is: i) the identification
of the words that play a role in describing spatial
concepts, and ii) the classification of the relational
role that these words play in the spatial configura-
tion.
For example, consider again the sentence “Give
me the book on AI on the big table behind the wall.”.
The phrase headed by the token book is referring
to a trajector object. The trajector (TR) is an en-
tity whose location is described in the sentence. The
phrase headed by the token table is referring to the
role of a landmark (LM). The landmark is a refer-
ence object for describing the location of a trajector.
These two spatial entities are related by the spatial
expression on denoted as spatial indicator (SP). The
spatial indicator (often a preposition in English, but
sometimes a verb, noun, adjective, or adverb) indi-
cates the existence of spatial information in the sen-
tence and establishes the type of a spatial relation.
The spatial relations that can be extracted from the
whole sentence are &lt;onSP bookTR tableLM&gt; and
&lt;behindSP tableTR wallLM&gt;. One could also use
spatial reasoning to infer that the statement &lt;behind
book wall&gt; holds, however, such inferred relations
are not considered in this task. Although the spa-
tial indicators are mostly prepositions, the reverse
may not hold- for example, the first preposition
on only states the topic of the book, so &lt;on book
AI&gt; is not a spatial relation. For each of the true
spatial relations, a general type is assigned. The
&lt;onSP bookTR tableLM&gt; relation expresses a kind
of topological relationship between the two objects
and we assign it a general type named region. The
&lt;behindSP tableTR wallLM&gt; relation expresses di-
rectional information and we assign it a general type
named direction.
In general we assume two main abstraction layers
for the extraction of spatial information (Bateman,
2010; Kordjamshidi et al., 2010a; Kordjamshidi et
al., 2011a): (a) a linguistic layer, corresponding to
the annotation scheme described above, which starts
with unrestricted natural language and predicts the
existence of spatial information at the sentence level
by identifying the words that play a particular spa-
tial role as well as their spatial relationship; (b) a
formal layer, in which the spatial roles are mapped
onto a spatial calculus model (Galton, 2009). For
example, the linguistic layer recognizes that the spa-
tial relation (on) holds between book and table, and
the formal layer maps this to a specific, formal spa-
tial representation, e.g., a logical representation like
AboveExternallyConnected(book,table) or a
formal qualitative spatial representation like EC (ex-
ternally connected) in the RCC model (Regional
Connection Calculus) (Cohn and Renz, 2008).
In this shared task we focus on the first (linguistic)
level which is a necessary step for mapping natural
language to any formal spatial calculus. The main
roles that are considered here are trajector, land-
mark, spatial indicator, their links and the general
type of their spatial relation. The general type of a
relation can be direction, region or distance.
</bodyText>
<sectionHeader confidence="0.755226" genericHeader="introduction">
2 Motivation and related work
</sectionHeader>
<bodyText confidence="0.999869448275862">
Spatial role labeling is a key task for applications
that are required to answer questions or reason about
spatial relationships between entities. Examples in-
clude systems that perform text-to-scene conversion,
generation of textual descriptions from visual data,
robot navigation tasks, giving directional instruc-
tions, and geographical information systems (GIS).
Recent research trends (Ross et al., 2010; Hois et
al., 2011; Tellex et al., 2011) indicate an increasing
interest in the area of extracting spatial information
from language and mapping it to a formal spatial
representation. Although cognitive-linguistic stud-
ies have investigated this problem extensively, the
computational aspect of making this bridge between
language and formal spatial representation (Hois
and Kutz, 2008) is still in its elementary stages. The
possession of a practical and appropriate annotation
scheme along with data is the first requirement. To
obtain this one has to investigate and schematize
both linguistic and spatial ontologies. This process
needs to cover the necessary information and seman-
tics on the one hand, and to maintain the practical
feasibility of the automatic annotation of unobserved
data on the other hand.
In recent research on spatial information and nat-
ural language, several annotation schemes have been
proposed such as ACE, GUM, GML, KML, TRML
which are briefly described and compared to Spa-
tialML scheme in (MITRE Corporation, 2010). But
</bodyText>
<page confidence="0.998374">
366
</page>
<bodyText confidence="0.999984212765957">
to our knowledge, the main obstacles for employing
machine learning in this context and the very limited
usage of this effective approach have been (a) the
lack of an agreement on a unique semantic model
for spatial information; (b) the diversity of formal
spatial relations; and consequently (c) the lack of
annotated data on which machine learning can be
employed to learn and extract the spatial relations.
The most systematic work in this area includes the
SpatialML (Mani et al., 2008) scheme which fo-
cuses on geographical information, and the work of
(Pustejovsky and Moszkowicz, 2009) in which the
pivot of the spatial information is the spatial verb.
The most recent and active work is the ISO-Space
scheme (Pustejovsky et al., 2011) which is based
on the above two schemes. The ideas behind ISO-
Space are closely related to our annotation scheme
in (Kordjamshidi et al., 2010b), however it consid-
ers more detailed and fine-grained spatial and lin-
guistic elements which makes the preparation of the
data for machine learning more difficult.
Spatial information is directly related to the part
of the language that can be visualized. Thus, the
extraction of spatial information is useful for mul-
timodal environments. One advantage of our pro-
posed scheme is that it considers this dimension. Be-
cause it abstracts the spatial elements that could be
aligned with the objects in images/videos and used
for annotation of audio-visual descriptions (Butko et
al., 2011). This is useful in the multimodal environ-
ments where, for example, natural language instruc-
tions are given to a robot for finding the way or ob-
jects.
Not much work exists on using annotations for
learning models to extract spatial information. Our
previous work (Kordjamshidi et al., 2011c) is a first
step in this direction and provides a domain indepen-
dent linguistic and spatial analysis to this problem.
This shared task invites interested research groups
for a similar effort. The idea behind this task is
firstly to motivate the application of different ma-
chine learning approaches, secondly to investigate
effective features for this task, and thirdly to reveal
the practical problems in the annotation schemes and
the annotated concepts. This will help to enrich the
data and the annotation in parallel with the machine
learning practice.
</bodyText>
<sectionHeader confidence="0.884244" genericHeader="method">
3 Annotation scheme
</sectionHeader>
<bodyText confidence="0.982217571428571">
As mentioned in the introduction, the annotation of
the data set is according to the general spatial role
labeling scheme (Kordjamshidi et al., 2010b). The
below example presents the annotated elements in
this scheme.
A womanTR and a childTR are
walkingMOTION overSP the squareLM.
</bodyText>
<figure confidence="0.675277142857143">
General-type: region
Specific type: RCC
Spatial value: PP (proper part)
Dynamic
Path: middle
Frame of reference: –
According to this scheme the main spatial roles are,
</figure>
<bodyText confidence="0.998861482758621">
Trajector (TR). The entity, i.e., person, object or
event whose location is described, which can
be static or dynamic; (also called: local/figure
object, locatum). In the above example woman
and child are two trajectors.
Landmark (LM). The reference entity in relation
to which the location or the motion of the tra-
jector is specified. (also called: reference ob-
ject or relatum). square is the landmark in the
above example.
Spatial indicator (SP). The element that defines
constraints on spatial properties such as the lo-
cation of the trajector with respect to the land-
mark. The spatial indicator determines the type
of spatial relation. The preposition over is an-
notated as the spatial indicator in the current
example.
Moreover, the links between the three roles are an-
notated as a spatial Relation. Since each spatial
relation is defined with three arguments we call
it a spatial triplet. Each triplet indicates a re-
lation between the three above mentioned spatial
roles. The sentence contains two spatial relations
of &lt;overSP womanTR squareLM&gt; and &lt;overSP
childTR squareLM&gt;, with the same spatial at-
tributes listed below the example. In spatial infor-
mation theory the relations and properties are usu-
ally grouped into the domains of topological, direc-
tional, and distance relations and also shape (Stock,
</bodyText>
<page confidence="0.990508">
367
</page>
<bodyText confidence="0.99864405">
1997). Accordingly, we propose a mapping between
the extracted spatial triplets to the coarse-grained
type of spatial relationships including region, direc-
tion or distance. We call these types as general-
type of the spatial relations and briefly describe
these below:
Region. refers to a region of space which is always
defined in relation to a landmark, e.g. the inte-
rior or exterior, e.g. “the flower is in the vase”.
Direction. denotes a direction along the axes pro-
vided by the different frames of reference, in
case the trajector of motion is not characterized
in terms of its relation to the region of a land-
mark, e.g. “the vase is on the left”.
Distance. states information about the spatial dis-
tance of the objects and could be a qualitative
expression such as close, far or quantitative
such as 12 km, e.g. “the kids are close to the
blackboard”.
The general-type of the relation in the example is
annotated as region.
After extraction of these relations a next fine-
grained step will be to map each general spatial re-
lationship to an appropriate spatial calculi represen-
tation. This step is not intended for this task and
the additional tags in the scheme will be consid-
ered in the future shared tasks. For example Re-
gion Connection Calculus RCC-8 (Cohn and Renz,
2008) representation reflects region-based topolog-
ical relations. Topological or region-based spatial
information has been researched in depth in the area
of qualitative spatial representation and reasoning.
We assume that the trajectors and landmarks can of-
ten be interpreted as spatial regions and, as a conse-
quence, their relation can be annotated with a spe-
cific RCC-8 relation. The RCC type in the above
example is specifically annotated as the PP (proper
part). Similarly, the direction and distance relations
are mapped to more specific formal representations.
Two additional annotations are about motion
verbs and dynamism. Dynamic spatial information
are associated with spatial movements and spatial
changes. In dynamic spatial relations mostly mo-
tion verbs are involved. Motion verbs carry spatial
information and influence the spatial semantics. In
the above example the spatial indicator over is re-
lated to a motion verb walking. Hence the spatial
relation is dynamic and walking is annotated as the
motion. In contrast to the dynamic spatial relations,
the static ones explain a static spatial configuration
such as the example of the previous section &lt;onSP
bookTR tableLM&gt; .
In the case of dynamic spatial information a path
is associated with the location of the trajector. In our
scheme the path is characterized by the three values
of beginning, middle, end and zero. The frame of
reference can be intrinsic, relative or absolute and is
typically relevant for directional relations. For more
details about the scheme, see (Kordjamshidi et al.,
2010b).
</bodyText>
<sectionHeader confidence="0.998694" genericHeader="method">
4 Tasks
</sectionHeader>
<bodyText confidence="0.990673">
The SemEval-2012 shared task is defined in three
parts.
</bodyText>
<listItem confidence="0.966711733333333">
• The first part considers labeling the spatial
indicators and trajector(s) / landmark(s). In
other words at this step we consider the
extraction of the individual roles that are
tagged with TRAJECTOR, LANDMARK and
SPATIAL INDICATOR.
• The second part is a kind of relation prediction
task and the goal is to extract triples contain-
ing (spatial-indicator, trajector, landmark). The
prediction of the tag of RELATION with its three
arguments of SP, TR, LM at the same time is
considered.
• The third part concerns the classification of the
type of the spatial relation. At the most coarse-
grained level this includes labeling the spatial
</listItem>
<bodyText confidence="0.825817571428571">
relations i.e. the triplets of (spatial indicator,
trajector, landmark) with region, direction, and
distance labels. This means the general-type
of the RELATION should be predicted. The
general-type is an attribute of the RELATION
tag, see the example represented in XML for-
mat in section 5.1.
</bodyText>
<sectionHeader confidence="0.806908" genericHeader="method">
5 Preparation of the dataset
</sectionHeader>
<bodyText confidence="0.998025666666667">
The annotated corpus that we used for this shared
task is a subset of IAPR TC-12 image Bench-
mark (Grubinger et al., 2006). It contains 613 text
</bodyText>
<page confidence="0.994525">
368
</page>
<bodyText confidence="0.997115066666667">
files that include 1213 sentences in total. This is an
extension of the dataset used in (Kordjamshidi et
al., 2011c). The original corpus was available free
of charge and without copyright restrictions. The
corpus contains images taken by tourists with de-
scriptions in different languages. The texts describe
objects, and their absolute and relative positions in
the image. This makes the corpus a rich resource for
spatial information. However the descriptions are
not always limited to spatial information. Therefore
they are less domain-specific and contain free expla-
nations about the images. Table 1 shows the detailed
statistics of this data. The average length of the sen-
tences in this data is about 15 words including punc-
tuation marks with a standard deviation of 8.
The spatial roles are assigned both to phrases and
their headwords, but only the headwords are eval-
uated for this task. The spatial relations indicate a
triplet of these roles. The general-type is assigned to
each triplet of spatial indicator, trajector and land-
mark.
At the starting point two annotators including one
task-organizer and another non-expert annotator, an-
notated 325 sentences for the spatial roles and rela-
tions. The purpose was to realize the disagreement
points and prepare a set of instructions in a way to
achieve highest-possible agreement. From the first
effort an inter-annotator agreement (Carletta, 1996)
of 0.89 for Cohen’s kappa was obtained. We contin-
ued with the a third annotator for the remaining 888
sentences. The annotator had an explanatory session
and received a set of instructions and annotated ex-
amples to decrease the ambiguity in the annotations.
To avoid complexity only the relations that are di-
rectly expressed in the sentence are annotated and
spatial reasoning was avoided during the annota-
tions. Sometimes the trajectors and landmarks or
both are implicit, meaning that there is no word in
the sentence to represent them. For example in the
sentence Come over here, the trajector you is only
implicitly present. To be consistent with the number
of arguments in spatial relations, in these cases we
use the term undefined for the implicit roles. There-
fore, the spatial relation in the above example is
&lt;overSP undefinedTR hereLM&gt;.
</bodyText>
<subsectionHeader confidence="0.983737">
5.1 Data format
</subsectionHeader>
<bodyText confidence="0.9999945">
The data is released in XML format. The original
textual files are split into sentences. Each sentence
is placed in a &lt;SENTENCE/&gt; tag and assigned an
identifier. This tag contains all the other tags which
describe the content and spatial relations of one sen-
tence.
The content of the sentence is placed in the
&lt;CONTENT/&gt; tag. The words in each sentence
are assigned identifiers depending on their specific
roles. Trajectors, landmarks and spatial indicators
are identified by &lt;TRAJECTOR/&gt;, &lt;LANDMARK/&gt;
and &lt;SPATIAL INDICATOR/&gt; tags, respectively.
Each of these XML elements has an “ID” attribute
that identifies a related word by its index. The “ID”
prefixed by either “TW”, “LW” or “SW”, respec-
tively for the mentioned roles. For example, a tra-
jector with ID=“TW2” corresponds to the word at
index 2 in the sentence. Indexes start at 0. Com-
mas, parentheses and apostrophes are also counted
as tokens.
Spatial relations are assigned identifiers too, and
relate the role-playing words to each other. Spa-
tial relations are identified by the &lt;RELATION/&gt;
tag. The spatial indicator, trajector and land-
mark for the relation are identified by the “SP”,
“TR” and “LM” attributes, respectively. The val-
ues of these attributes correspond to the “ID” at-
tributes in the &lt;TRAJECTOR/&gt;, &lt;LANDMARK/&gt;
and &lt;SPATIAL INDICATOR/&gt; elements. If a tra-
jector or landmark is implicit, then the index of
“TR” or “LM” attribute will be set to a dummy
index. This dummy index is equal to the in-
dex of the last word in the sentence plus one.
In this case, the value of TRAJECTOR or LAND-
MARK is set to “undefined”. The coarse-grained
spatial type of the relation is indicated by the
“GENERAL TYPE” attribute and gets one value
in {REGION, DIRECTION, DISTANCE}. In the
original data set there are cases annotated with
multiple spatial types. This is due to the ambi-
guity and/or under-specificity of natural language
compared to formal spatial representations (Kord-
jamshidi et al., 2010a). In this task the general-
type with a higher priority by the annotator is pro-
vided. Here, by the high priority type, we mean the
general type which has been the most informative
</bodyText>
<page confidence="0.99764">
369
</page>
<table confidence="0.973148">
Spatial Roles Relations General Types
Sentences TR LM SP Spatial triplets Region Direction Distance
1213 1593 1408 1464 1715 1036 644 35
</table>
<tableCaption confidence="0.999859">
Table 1: Number of annotated components in the data set.
</tableCaption>
<bodyText confidence="0.99990775">
and relevant type for a relation, from the annotator’s
point of view. This task considers labeling words
rather than phrases for all spatial roles. However, in
the XML file for spatial indicators often the whole
phrase is tagged. In these cases, the index of the
indicator refers to one word which is typically the
spatial preposition of the phrase. For evaluation only
the indexed words are compared and should be pre-
dicted correctly.
Below is one example copied from the data. For
more examples and details about the general anno-
tation scheme see (Kordjamshidi et al., 2010b).
</bodyText>
<figure confidence="0.86078219047619">
&lt;SENTENCE ID=‘S11’&gt;
&lt;CONTENT &gt;
there are red umbrellas in a park on the right.
&lt;/CONTENT&gt;
&lt;TRAJECTOR ID=‘TW3’&gt;
umbrellas
&lt;/TRAJECTOR&gt;
&lt;LANDMARK ID=‘LW6’&gt;
park
&lt;/LANDMARK&gt;
&lt;SPATIAL INDICATOR ID=‘SW4’&gt;
in
&lt;/SPATIAL INDICATOR&gt;
&lt;RELATION ID=‘R0’ SP=‘SW4’ TR=‘TW3’
LM=‘LW6’ GENERAL TYPE=‘REGION’/&gt;
&lt;SPATIAL INDICATOR ID=‘SW7’&gt;
on the right
&lt;/SPATIAL INDICATOR&gt;
&lt;RELATION ID=‘R1’ SP=‘SW7’ TR=‘TW3’
LM=‘LW6’ GENERAL TYPE=‘DIRECTION’/&gt;
&lt;/SENTENCE&gt;
</figure>
<bodyText confidence="0.998706666666667">
The dataset, both train and test, also the 10-fold
splits are made available in the LIIR research group
webpage of KU Leuven.1
</bodyText>
<sectionHeader confidence="0.997923" genericHeader="method">
6 Evaluation methodology
</sectionHeader>
<bodyText confidence="0.99984125">
According to the usual setting of the shared tasks
our evaluation setting was based on splitting the data
set into a training and a testing set. Each set con-
tained about 50% of the whole data. The test set re-
</bodyText>
<footnote confidence="0.99152">
1http://www.cs.kuleuven.be/groups/liir/software/
SpRL Data/
</footnote>
<bodyText confidence="0.99947125">
leased without the ground-truth labels. However, af-
ter the systems submission deadline the ground-truth
test was released. Hence the participant group per-
formed an additional 10-fold cross validation eval-
uation too. We report the results of both evaluation
settings.
Prediction of each component including TRAJEC-
TORs, LANDMARKs and SPATIAL-INDICATORs is
evaluated on the test set using their individual spatial
element XML tags. The evaluation metrics of pre-
cision, recall and F1-measure are used, which are
defined as:
</bodyText>
<equation confidence="0.99995">
recall = Tp+FA, (1)
precision = Tp+FP (2)
</equation>
<bodyText confidence="0.991028608695652">
where:
TP = the number of system-produced
XML tags that match an annotated XML
tag,
FP = the number of system-produced
XML tags that do not match an annotated
tag,
FN = the number of annotated XML tags
that do not match a system-produced tag.
For the roles evaluation two XML tags match
when they have exactly same identifier. In fact,
when the identifiers are the same then the role and
the word index are the same. In addition, systems
are evaluated on how well they are able to retrieve
triplets of (trajector, spatial-indicator, landmark), in
terms of precision, recall and F1-measure. The TP,
FP, FN are counted in a similar way but two RELA-
TION tags match if the combination of their TR, LM
and SP is exactly the same. In other words a true pre-
diction requires all the three elements are correctly
predicted at the same time.
The last evaluation is on how well the systems are
able to retrieve the relations and their general type
</bodyText>
<equation confidence="0.998587">
F1 = 2*recall*precision
(
recall+precision)1 (3)
</equation>
<page confidence="0.984708">
370
</page>
<bodyText confidence="0.999893333333334">
i.e {region, direction, distance} at the same time.
To evaluate the GENERAL-TYPE similarly the RELA-
TION tag is checked. For a true prediction, an exact
match between the ground-truth and all the elements
of the predicted RELATION tag including TR, LM,SP
and GENERAL-TYPE is required.
</bodyText>
<sectionHeader confidence="0.852592" genericHeader="evaluation">
7 Systems and results
</sectionHeader>
<bodyText confidence="0.999764461538462">
One system with two runs was submitted from the
University of Texas Dallas. The two runs (Roberts
and Harabagiu, 2012), UTDSPRL-SUPERVISED1
and UTDSPRL-SUPERVISED2 are based on the
joint classification of the spatial triplets in a bi-
nary classification setting. To produce the candi-
date (indicator, trajector, landmark) triples, in the
first stage heuristic rules targeting a high recall are
used. Then a binary support vector machine clas-
sifier is employed to predict whether a triple is a
spatial relation or not. Both runs start with a large
number of manually engineered features, and use
floating forward feature selection to select the most
important ones. The difference between the two
runs of UTDSPRL-SUPERVISED1 and UTDSPRL-
SUPERVISED2 is their feature set. Particularly, in
UTDSPRL-SUPERVISED1 a joint feature based on
the conjunctions (e.g. and, but) is considered before
running feature selection but this feature is removed
in UTDSPRL-SUPERVISED2.
The submitted runs are compared to a previous
system from the task organizers (Kordjamshidi et
al., 2011c) which is evaluated on the current data
with the same settings. This system, KUL-SKIP-
CHAIN-CRF, uses a skip chain conditional random
field (CRF) model (Sutton and MacCallum, 2006)
to annotate the sentence as a sequence. It considers
the long distance dependencies between the prepo-
sitions and nouns in the sentence.
The type and structure of the features used in the
UTD and KUL systems are different. In the UTD
system, the classifier works on triples and the fea-
tures are of two main types: (a) argument-specific
features about the trajector, landmark, or indicator
e.g., the landmark’s hypernyms, or the indicator’s
first token; and (b) joint features that consider two
or more of the arguments, e.g. the dependency path
between indicator and landmark. For more detail,
see (Roberts and Harabagiu, 2012). In the KUL sys-
</bodyText>
<table confidence="0.999664166666667">
Label Precsion Recall F1
TRAJECTOR 0.731 0.621 0.672
LANDMARK 0.871 0.645 0.741
SPATIAL-INDICATOR 0.928 0.712 0.806
RELATION 0.567 0.500 0.531
GENERAL-TYPE 0.561 0.494 0.526
</table>
<tableCaption confidence="0.975526333333333">
Table 2: UTDSPRL-SUPERVISED1: The University
of Texas-Dallas system with a larger number of fea-
tures,test/train one split.
</tableCaption>
<table confidence="0.9999455">
Label Precsion Recall F1
TRAJECTOR 0.782 0.646 0.707
LANDMARK 0.894 0.680 0.772
SPATIAL-INDICATOR 0.940 0.732 0.823
RELATION 0.610 0.540 0.573
GENERAL-TYPE 0.603 0.534 0.566
</table>
<tableCaption confidence="0.947407666666667">
Table 3: UTDSPRL-SUPERVISED2: The University of
Texas-Dallas system with a smaller number of features,
test/train one split.
</tableCaption>
<table confidence="0.9999308">
Label Precsion Recall F1
TRAJECTOR 0.697 0.603 0.646
LANDMARK 0.773 0.740 0.756
SPATIAL-INDICATOR 0.913 0.887 0.900
RELATION 0.487 0.512 0.500
</table>
<tableCaption confidence="0.9940355">
Table 4: KUL-SKIP-CHAIN-CRF: The organizers’ sys-
tem (Kordjamshidi et al., 2011c)- test/train one split.
</tableCaption>
<bodyText confidence="0.9999454">
tem, the classifier works on all tokens in a sentence,
and a number of linguistically motivated local and
pairwise features over candidate words and preposi-
tions are used. To consider long distance dependen-
cies a template, called a preposition template, is used
in the general CRF framework. Loopy belief prop-
agation is used for inference. Mallet2 and GRMM:3
implementations are employed there.
Tables 2, 3 and 4 show the results of the three
runs in the standard setting of the shared task us-
ing the train/test split. In this evaluation setting the
UTDSPRL-SUPERVISED2 run achieves the highest
performance on the test set, with F1 of 0.573 for
the full triplet identification task, and an F1 of 0.566
for additionally classifying the triplet’s general-type
</bodyText>
<footnote confidence="0.9998895">
2http://mallet.cs.umass.edu/download.php
3http://mallet.cs.umass.edu/grmm/index.php
</footnote>
<page confidence="0.987795">
371
</page>
<table confidence="0.999054">
System Precsion Recall F1
KUL-SKIP-CHAIN-CRF 0.745 0.773 0.758
UTDSPRL-SUPERVISED2 0.773 0.679 0.723
</table>
<tableCaption confidence="0.9861855">
Table 5: The RELATION extraction of KUL-SKIP-CHAIN-CRF (Kordjamshidi et al., 2011c) vs. UTDSPRL-
SUPERVISED2 evaluated with 10-fold cross validation
</tableCaption>
<bodyText confidence="0.99849388372093">
correctly. It also consistently outperforms both the
UTDSPRL-SUPERVISED1 run and the KUL-SKIP-
CHAIN-CRF system on each of the individual trajec-
tor, landmark and spatial-indicator extraction.
The dataset was relatively small, so we released
the test data and the two systems were addition-
ally evaluated using 10-fold cross validation. The
results of this cross-validation are shown in Ta-
ble 5. The UTDSPRL-SUPERVISED2 run achieves
a higher precision, while the KUL-SKIP-CHAIN-
CRF system achieves a higher recall. It should be
mentioned the 10-fold splits used by KUL and UTD
are not the same. This implies that the results with
exactly the same cross-folds may vary slightly from
these reported in Table 5.
Using 10-fold cross validation, we also evaluated
the classification of the general-type of a relation
given the manually annotated positive triplets. The
UTDSPRL-SUPERVISED2 system achieved F1=
0.974, and similar experiments using SMO-SVM in
(Kordjamshidi et al., 2011b; Kordjamshidi et al.,
2011a) achieved F1= 0.973. Thus it appears that
identifying the general-type of a relation is a rela-
tively easy task on this data.
Discussion. Since the feature sets of the two sys-
tems are different and given the evaluation results
in the two evaluation settings, it is difficult to assert
which model is better in general. Obviously using
joint features potentially inputs richer information to
the model. However, it can increase the sparsity in
one hand and overfitting on the training data on the
other hand. Another problem is that finding heuris-
tics for high recall that are sufficiently general to be
used in every domain is not an easy task. By increas-
ing the number of candidates the dataset imbalance
will increase dramatically. This can cause a lower
performance of a joint model based on a binary clas-
sification setting when applied on different data sets.
It seems that this task might require a more elabo-
rated structured output prediction model which can
consider the joint features and alleviate the problem
of huge negatives in that framework while consider-
ing the correlations between the output components.
</bodyText>
<sectionHeader confidence="0.99747" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999938647058824">
The SemEval-2012 spatial role labeling task is a
starting point to formally consider the extraction of
spatial semantics from the language. The aim is
to consider this task as a standalone linguistic task
which is important for many applications. Our first
practice on this task and the current submitted sys-
tem to SemEval 2012 clarify the type of the features
and the machine learning approaches appropriate for
it. The proposed features and models help to per-
form this task automatically in a reasonable accu-
racy. Although the spatial scheme is domain inde-
pendent, the achieved accuracy is dependent on the
domain of the used data for training a model. Our
future plan is to extend the data for the next work-
shops and to cover more semantic aspects of spatial
information particularly for mapping to formal spa-
tial representation models and spatial calculus.
</bodyText>
<sectionHeader confidence="0.997642" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999811111111111">
Special thanks to Martijn Van Otterlo for his great
cooperation from the initiation phase and in the
growth of this task. Many thanks to Sabine
Drebusch for her kind and open cooperation in an-
notating the very first dataset. Thanks to Tigist Kas-
sahun for her help in annotating the current dataset.
Thanks the participant team of the University of
Texas Dallas and their useful feedback on the an-
notated data.
</bodyText>
<sectionHeader confidence="0.998449" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9647455">
J. A. Bateman. 2010. Language and space: a two-level
semantic approach based on principles of ontological
engineering. International Journal of Speech Technol-
ogy, 13(1):29–48.
</reference>
<page confidence="0.981214">
372
</page>
<reference confidence="0.998756510204081">
T. Butko, C. Nadeu, and A. Moreno. 2011. A multi-
lingual corpus for rich audio-visual scenedescription
in a meeting-room environment. In ICMI workshop
on multimodal corpora for machine learning: Taking
Stock and Roadmapping the Future.
J. Carletta. 1996. Assessing agreement on classification
tasks: the kappa statistic. Computational Linguistics,
22(2):249–254.
A. G. Cohn and J. Renz. 2008. Qualitative spatial repre-
sentation and reasoning. In Handbook of Knowledge
Representation, volume 3 of Foundations of Artificial
Intelligence, pages 551 – 596. Elsevier.
A. Galton. 2009. Spatial and temporal knowledge rep-
resentation. Journal of Earth Science Informatics,
2(3):169–187.
M. Grubinger, P. Clough, Henning M¨uller, and Thomas
Deselaers. 2006. The IAPR benchmark: A new evalu-
ation resource for visual information systems. In In-
ternational Conference on Language Resources and
Evaluation (LREC).
J. Hois and O. Kutz. 2008. Natural language meets spa-
tial calculi. In Christian Freksa, Nora S. Newcombe,
Peter Girdenfors, and Stefan W¨olfl, editors, Spatial
Cognition, volume 5248 of Lecture Notes in Computer
Science, pages 266–282. Springer.
J. Hois, R. J. Ross, J. D. Kelleher, and J. A. Bateman.
2011. Computational models of spatial language in-
terpretation and generation. In COSLI-2011.
P. Kordjamshidi, M. van Otterlo, and M. F. Moens.
2010a. From language towards formal spatial calculi.
In Workshop on Computational Models of Spatial Lan-
guage Interpretation (CoSLI 2010, at Spatial Cogni-
tion 2010).
P. Kordjamshidi, M. van Otterlo, and M. F. Moens.
2010b. Spatial role labeling: Task definition and anno-
tation scheme. In Proceedings of the Seventh confer-
ence on International Language Resources and Eval-
uation (LREC’10).
P. Kordjamshidi, J. Hois, M. van Otterlo, and M.-F.
Moens. 2011a. Machine learning for interpretation of
spatial natural language in terms of qsr. Poster Presen-
tation at the 10th International Conference on Spatial
Information Theory (COSIT’11).
P. Kordjamshidi, J. Hois, M. van Otterlo, and M.F.
Moens. 2011b. Learning to interpret spatial natural
language in terms of qualitative spatial relations. Rep-
resenting space in cognition: Interrelations of behav-
ior, language, and formal models. Series Explorations
in Language and Space, Oxford University Press, sub-
mitted.
P. Kordjamshidi, M. Van Otterlo, and M.F. Moens.
2011c. Spatial role labeling: Towards extraction of
spatial relations from natural language. ACM Trans.
Speech Lang. Process., 8:1–36, December.
I. Mani, J. Hitzeman, J. Richer, D. Harris, R. Quimby, and
B. Wellner. 2008. SpatialML: Annotation scheme,
corpora, and tools. In Nicoletta Calzolari, Khalid
Choukri, Bente Maegaard, Joseph Mariani, Jan Odjik,
Stelios Piperidis, and Daniel Tapias, editors, Proceed-
ings of the Sixth International Language Resources
and Evaluation (LREC’08). European Language Re-
sources Association (ELRA).
MITRE Corporation. 2010. SpatialML: Annotation
scheme for marking spatial expression in natural lan-
guage. Technical Report Version 3.0.1, The MITRE
Corporation.
J. Pustejovsky and J.L. Moszkowicz. 2009. Integrat-
ing motion predicate classes with spatial and tempo-
ral annotations. In CoLing 2008: Companion volume
Posters and Demonstrations, pages 95–98.
J. Pustejovsky, J. Moszkowicz, and M. Verhagen. 2011.
Iso-space: The annotation of spatial information in
language. In Proceedings of ISA-6: ACL-ISO Inter-
national Workshop on Semantic Annotation.
K. Roberts and S.M. Harabagiu. 2012. Utd-sprl: A joint
approach to spatial role labeling. In Submitted to this
workshop of SemEval-2012.
R. Ross, J. Hois, and J. Kelleher. 2010. Computational
models of spatial language interpretation. In COSLI-
2010.
O. Stock, editor. 1997. Spatial and Temporal Reasoning.
Kluwer.
C. Sutton and A. MacCallum. 2006. Introduction to con-
ditional random fields for relational learning. In Lise
Getoor and Ben Taskar, editors, Introduction to Statis-
tical Relational Learning. MIT Press.
D. A. Tappan. 2004. Knowledge-Based Spatial Rea-
soning for Automated Scene Generation from Text De-
scriptions. Ph.D. thesis.
S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, and
N. Roy A. G. Banerjee, S. Teller. 2011. Understand-
ing natural language commands for robotic naviga-
tion and mobile manipulation. In Proceedings of the
National Conference on Artificial Intelligence (AAAI),
San Francisco, CA.
J. Zlatevl. 2007. Spatial semantics. In Hubert Cuyck-
ens and Dirk Geeraerts (eds.) The Oxford Handbook
of Cognitive Linguistics, Chapter 13, pages 318–350.
</reference>
<page confidence="0.999385">
373
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.252291">
<title confidence="0.997803">SemEval-2012 Task 3: Spatial Role Labeling</title>
<author confidence="0.576098">Parisa Kordjamshidi Steven Bethard Marie-Francine Moens</author>
<affiliation confidence="0.587297">Katholieke Universiteit Leuven University of Colorado Katholieke Universiteit Leuven</affiliation>
<abstract confidence="0.94156315">parisa.kordjamshidi@ steven.bethard@ sien.moens@ cs.kuleuven.be colorado.edu cs.kuleuven.be Abstract This SemEval2012 shared task is based on a recently introduced spatial annotation scheme called Spatial Role Labeling. The Spatial Role Labeling task concerns the extraction of main components of the spatial semantics from natural language: trajectors, landmarks and spatial indicators. In addition to these major components, the links between them and the general-type of spatial relationships including region, direction and distance are targeted. The annotated dataset contains about 1213 sentences which describe 612 images of the CLEF IAPR TC-12 Image Benchmark. We have one participant system with two runs. The participant’s runs are compared to the system in (Kordjamshidi et al., 2011c) which is provided by task organizers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J A Bateman</author>
</authors>
<title>Language and space: a two-level semantic approach based on principles of ontological engineering.</title>
<date>2010</date>
<journal>International Journal of Speech Technology,</journal>
<volume>13</volume>
<issue>1</issue>
<contexts>
<context position="5612" citStr="Bateman, 2010" startWordPosition="876" endWordPosition="877"> are mostly prepositions, the reverse may not hold- for example, the first preposition on only states the topic of the book, so &lt;on book AI&gt; is not a spatial relation. For each of the true spatial relations, a general type is assigned. The &lt;onSP bookTR tableLM&gt; relation expresses a kind of topological relationship between the two objects and we assign it a general type named region. The &lt;behindSP tableTR wallLM&gt; relation expresses directional information and we assign it a general type named direction. In general we assume two main abstraction layers for the extraction of spatial information (Bateman, 2010; Kordjamshidi et al., 2010a; Kordjamshidi et al., 2011a): (a) a linguistic layer, corresponding to the annotation scheme described above, which starts with unrestricted natural language and predicts the existence of spatial information at the sentence level by identifying the words that play a particular spatial role as well as their spatial relationship; (b) a formal layer, in which the spatial roles are mapped onto a spatial calculus model (Galton, 2009). For example, the linguistic layer recognizes that the spatial relation (on) holds between book and table, and the formal layer maps this </context>
</contexts>
<marker>Bateman, 2010</marker>
<rawString>J. A. Bateman. 2010. Language and space: a two-level semantic approach based on principles of ontological engineering. International Journal of Speech Technology, 13(1):29–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Butko</author>
<author>C Nadeu</author>
<author>A Moreno</author>
</authors>
<title>A multilingual corpus for rich audio-visual scenedescription in a meeting-room environment.</title>
<date>2011</date>
<booktitle>In ICMI workshop on multimodal corpora for machine learning: Taking Stock and Roadmapping the Future.</booktitle>
<contexts>
<context position="9801" citStr="Butko et al., 2011" startWordPosition="1525" endWordPosition="1528">heme in (Kordjamshidi et al., 2010b), however it considers more detailed and fine-grained spatial and linguistic elements which makes the preparation of the data for machine learning more difficult. Spatial information is directly related to the part of the language that can be visualized. Thus, the extraction of spatial information is useful for multimodal environments. One advantage of our proposed scheme is that it considers this dimension. Because it abstracts the spatial elements that could be aligned with the objects in images/videos and used for annotation of audio-visual descriptions (Butko et al., 2011). This is useful in the multimodal environments where, for example, natural language instructions are given to a robot for finding the way or objects. Not much work exists on using annotations for learning models to extract spatial information. Our previous work (Kordjamshidi et al., 2011c) is a first step in this direction and provides a domain independent linguistic and spatial analysis to this problem. This shared task invites interested research groups for a similar effort. The idea behind this task is firstly to motivate the application of different machine learning approaches, secondly t</context>
</contexts>
<marker>Butko, Nadeu, Moreno, 2011</marker>
<rawString>T. Butko, C. Nadeu, and A. Moreno. 2011. A multilingual corpus for rich audio-visual scenedescription in a meeting-room environment. In ICMI workshop on multimodal corpora for machine learning: Taking Stock and Roadmapping the Future.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: the kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="17850" citStr="Carletta, 1996" startWordPosition="2825" endWordPosition="2826">les are assigned both to phrases and their headwords, but only the headwords are evaluated for this task. The spatial relations indicate a triplet of these roles. The general-type is assigned to each triplet of spatial indicator, trajector and landmark. At the starting point two annotators including one task-organizer and another non-expert annotator, annotated 325 sentences for the spatial roles and relations. The purpose was to realize the disagreement points and prepare a set of instructions in a way to achieve highest-possible agreement. From the first effort an inter-annotator agreement (Carletta, 1996) of 0.89 for Cohen’s kappa was obtained. We continued with the a third annotator for the remaining 888 sentences. The annotator had an explanatory session and received a set of instructions and annotated examples to decrease the ambiguity in the annotations. To avoid complexity only the relations that are directly expressed in the sentence are annotated and spatial reasoning was avoided during the annotations. Sometimes the trajectors and landmarks or both are implicit, meaning that there is no word in the sentence to represent them. For example in the sentence Come over here, the trajector yo</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>J. Carletta. 1996. Assessing agreement on classification tasks: the kappa statistic. Computational Linguistics, 22(2):249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A G Cohn</author>
<author>J Renz</author>
</authors>
<title>Qualitative spatial representation and reasoning.</title>
<date>2008</date>
<booktitle>In Handbook of Knowledge Representation,</booktitle>
<volume>3</volume>
<pages>551--596</pages>
<publisher>Elsevier.</publisher>
<contexts>
<context position="6478" citStr="Cohn and Renz, 2008" startWordPosition="1003" endWordPosition="1006">ntence level by identifying the words that play a particular spatial role as well as their spatial relationship; (b) a formal layer, in which the spatial roles are mapped onto a spatial calculus model (Galton, 2009). For example, the linguistic layer recognizes that the spatial relation (on) holds between book and table, and the formal layer maps this to a specific, formal spatial representation, e.g., a logical representation like AboveExternallyConnected(book,table) or a formal qualitative spatial representation like EC (externally connected) in the RCC model (Regional Connection Calculus) (Cohn and Renz, 2008). In this shared task we focus on the first (linguistic) level which is a necessary step for mapping natural language to any formal spatial calculus. The main roles that are considered here are trajector, landmark, spatial indicator, their links and the general type of their spatial relation. The general type of a relation can be direction, region or distance. 2 Motivation and related work Spatial role labeling is a key task for applications that are required to answer questions or reason about spatial relationships between entities. Examples include systems that perform text-to-scene conversi</context>
<context position="13709" citStr="Cohn and Renz, 2008" startWordPosition="2165" endWordPosition="2168">ce. states information about the spatial distance of the objects and could be a qualitative expression such as close, far or quantitative such as 12 km, e.g. “the kids are close to the blackboard”. The general-type of the relation in the example is annotated as region. After extraction of these relations a next finegrained step will be to map each general spatial relationship to an appropriate spatial calculi representation. This step is not intended for this task and the additional tags in the scheme will be considered in the future shared tasks. For example Region Connection Calculus RCC-8 (Cohn and Renz, 2008) representation reflects region-based topological relations. Topological or region-based spatial information has been researched in depth in the area of qualitative spatial representation and reasoning. We assume that the trajectors and landmarks can often be interpreted as spatial regions and, as a consequence, their relation can be annotated with a specific RCC-8 relation. The RCC type in the above example is specifically annotated as the PP (proper part). Similarly, the direction and distance relations are mapped to more specific formal representations. Two additional annotations are about </context>
</contexts>
<marker>Cohn, Renz, 2008</marker>
<rawString>A. G. Cohn and J. Renz. 2008. Qualitative spatial representation and reasoning. In Handbook of Knowledge Representation, volume 3 of Foundations of Artificial Intelligence, pages 551 – 596. Elsevier.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Galton</author>
</authors>
<title>Spatial and temporal knowledge representation.</title>
<date>2009</date>
<journal>Journal of Earth Science Informatics,</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="6073" citStr="Galton, 2009" startWordPosition="947" endWordPosition="948">d we assign it a general type named direction. In general we assume two main abstraction layers for the extraction of spatial information (Bateman, 2010; Kordjamshidi et al., 2010a; Kordjamshidi et al., 2011a): (a) a linguistic layer, corresponding to the annotation scheme described above, which starts with unrestricted natural language and predicts the existence of spatial information at the sentence level by identifying the words that play a particular spatial role as well as their spatial relationship; (b) a formal layer, in which the spatial roles are mapped onto a spatial calculus model (Galton, 2009). For example, the linguistic layer recognizes that the spatial relation (on) holds between book and table, and the formal layer maps this to a specific, formal spatial representation, e.g., a logical representation like AboveExternallyConnected(book,table) or a formal qualitative spatial representation like EC (externally connected) in the RCC model (Regional Connection Calculus) (Cohn and Renz, 2008). In this shared task we focus on the first (linguistic) level which is a necessary step for mapping natural language to any formal spatial calculus. The main roles that are considered here are t</context>
</contexts>
<marker>Galton, 2009</marker>
<rawString>A. Galton. 2009. Spatial and temporal knowledge representation. Journal of Earth Science Informatics, 2(3):169–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Grubinger</author>
<author>P Clough</author>
<author>Henning M¨uller</author>
<author>Thomas Deselaers</author>
</authors>
<title>The IAPR benchmark: A new evaluation resource for visual information systems.</title>
<date>2006</date>
<booktitle>In International Conference on Language Resources and Evaluation (LREC).</booktitle>
<marker>Grubinger, Clough, M¨uller, Deselaers, 2006</marker>
<rawString>M. Grubinger, P. Clough, Henning M¨uller, and Thomas Deselaers. 2006. The IAPR benchmark: A new evaluation resource for visual information systems. In International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hois</author>
<author>O Kutz</author>
</authors>
<title>Natural language meets spatial calculi.</title>
<date>2008</date>
<journal>Spatial Cognition,</journal>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>5248</volume>
<pages>266--282</pages>
<editor>In Christian Freksa, Nora S. Newcombe, Peter Girdenfors, and Stefan W¨olfl, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="2026" citStr="Hois and Kutz, 2008" startWordPosition="294" endWordPosition="297">he direction (orientation) of the table with respect to the wall. Understanding such spatial utterances is a problem in many areas, including robotics, navigation, traffic management, and query answering systems (Tappan, 2004). Linguistic constructs can express highly complex, relational structures of objects, spatial relations between them, and patterns of motion through space relative to some reference point. Compared to natural language, formal spatial models focus on one particular spatial aspect such as orientation, topology or distance and specify its underlying spatial logic in detail (Hois and Kutz, 2008). These formal models enable spatial reasoning that is difficult to perform on natural language expressions. Learning how to map natural language spatial information onto a formal representation is a challenging problem. The complexity of spatial semantics from the cognitive-linguistic point of view on the one hand, the diversity of formal spatial representation models in different applications on the other hand and the gap between the specification level of the two sides has led to the present situation that no well-defined framework for automatic spatial information extraction exists that ca</context>
<context position="7663" citStr="Hois and Kutz, 2008" startWordPosition="1179" endWordPosition="1182">that perform text-to-scene conversion, generation of textual descriptions from visual data, robot navigation tasks, giving directional instructions, and geographical information systems (GIS). Recent research trends (Ross et al., 2010; Hois et al., 2011; Tellex et al., 2011) indicate an increasing interest in the area of extracting spatial information from language and mapping it to a formal spatial representation. Although cognitive-linguistic studies have investigated this problem extensively, the computational aspect of making this bridge between language and formal spatial representation (Hois and Kutz, 2008) is still in its elementary stages. The possession of a practical and appropriate annotation scheme along with data is the first requirement. To obtain this one has to investigate and schematize both linguistic and spatial ontologies. This process needs to cover the necessary information and semantics on the one hand, and to maintain the practical feasibility of the automatic annotation of unobserved data on the other hand. In recent research on spatial information and natural language, several annotation schemes have been proposed such as ACE, GUM, GML, KML, TRML which are briefly described a</context>
</contexts>
<marker>Hois, Kutz, 2008</marker>
<rawString>J. Hois and O. Kutz. 2008. Natural language meets spatial calculi. In Christian Freksa, Nora S. Newcombe, Peter Girdenfors, and Stefan W¨olfl, editors, Spatial Cognition, volume 5248 of Lecture Notes in Computer Science, pages 266–282. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hois</author>
<author>R J Ross</author>
<author>J D Kelleher</author>
<author>J A Bateman</author>
</authors>
<title>Computational models of spatial language interpretation and generation.</title>
<date>2011</date>
<booktitle>In COSLI-2011.</booktitle>
<contexts>
<context position="7296" citStr="Hois et al., 2011" startWordPosition="1128" endWordPosition="1131">or, landmark, spatial indicator, their links and the general type of their spatial relation. The general type of a relation can be direction, region or distance. 2 Motivation and related work Spatial role labeling is a key task for applications that are required to answer questions or reason about spatial relationships between entities. Examples include systems that perform text-to-scene conversion, generation of textual descriptions from visual data, robot navigation tasks, giving directional instructions, and geographical information systems (GIS). Recent research trends (Ross et al., 2010; Hois et al., 2011; Tellex et al., 2011) indicate an increasing interest in the area of extracting spatial information from language and mapping it to a formal spatial representation. Although cognitive-linguistic studies have investigated this problem extensively, the computational aspect of making this bridge between language and formal spatial representation (Hois and Kutz, 2008) is still in its elementary stages. The possession of a practical and appropriate annotation scheme along with data is the first requirement. To obtain this one has to investigate and schematize both linguistic and spatial ontologies</context>
</contexts>
<marker>Hois, Ross, Kelleher, Bateman, 2011</marker>
<rawString>J. Hois, R. J. Ross, J. D. Kelleher, and J. A. Bateman. 2011. Computational models of spatial language interpretation and generation. In COSLI-2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kordjamshidi</author>
<author>M van Otterlo</author>
<author>M F Moens</author>
</authors>
<title>From language towards formal spatial calculi.</title>
<date>2010</date>
<booktitle>In Workshop on Computational Models of Spatial Language Interpretation (CoSLI</booktitle>
<marker>Kordjamshidi, van Otterlo, Moens, 2010</marker>
<rawString>P. Kordjamshidi, M. van Otterlo, and M. F. Moens. 2010a. From language towards formal spatial calculi. In Workshop on Computational Models of Spatial Language Interpretation (CoSLI 2010, at Spatial Cognition 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kordjamshidi</author>
<author>M van Otterlo</author>
<author>M F Moens</author>
</authors>
<title>Spatial role labeling: Task definition and annotation scheme.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10).</booktitle>
<marker>Kordjamshidi, van Otterlo, Moens, 2010</marker>
<rawString>P. Kordjamshidi, M. van Otterlo, and M. F. Moens. 2010b. Spatial role labeling: Task definition and annotation scheme. In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kordjamshidi</author>
<author>J Hois</author>
<author>M van Otterlo</author>
<author>M-F Moens</author>
</authors>
<title>Machine learning for interpretation of spatial natural language in terms of qsr.</title>
<date>2011</date>
<booktitle>Poster Presentation at the 10th International Conference on Spatial Information Theory (COSIT’11).</booktitle>
<marker>Kordjamshidi, Hois, van Otterlo, Moens, 2011</marker>
<rawString>P. Kordjamshidi, J. Hois, M. van Otterlo, and M.-F. Moens. 2011a. Machine learning for interpretation of spatial natural language in terms of qsr. Poster Presentation at the 10th International Conference on Spatial Information Theory (COSIT’11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kordjamshidi</author>
<author>J Hois</author>
<author>M van Otterlo</author>
<author>M F Moens</author>
</authors>
<title>Learning to interpret spatial natural language in terms of qualitative spatial relations. Representing space in cognition: Interrelations of behavior, language, and formal models. Series Explorations in Language and Space,</title>
<date>2011</date>
<pages>submitted.</pages>
<publisher>University Press,</publisher>
<location>Oxford</location>
<marker>Kordjamshidi, Hois, van Otterlo, Moens, 2011</marker>
<rawString>P. Kordjamshidi, J. Hois, M. van Otterlo, and M.F. Moens. 2011b. Learning to interpret spatial natural language in terms of qualitative spatial relations. Representing space in cognition: Interrelations of behavior, language, and formal models. Series Explorations in Language and Space, Oxford University Press, submitted.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kordjamshidi</author>
<author>M Van Otterlo</author>
<author>M F Moens</author>
</authors>
<title>Spatial role labeling: Towards extraction of spatial relations from natural language.</title>
<date>2011</date>
<journal>ACM Trans. Speech Lang. Process.,</journal>
<pages>8--1</pages>
<marker>Kordjamshidi, Van Otterlo, Moens, 2011</marker>
<rawString>P. Kordjamshidi, M. Van Otterlo, and M.F. Moens. 2011c. Spatial role labeling: Towards extraction of spatial relations from natural language. ACM Trans. Speech Lang. Process., 8:1–36, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>J Hitzeman</author>
<author>J Richer</author>
<author>D Harris</author>
<author>R Quimby</author>
<author>B Wellner</author>
</authors>
<title>SpatialML: Annotation scheme, corpora, and tools.</title>
<date>2008</date>
<booktitle>Proceedings of the Sixth International Language Resources and Evaluation (LREC’08). European Language Resources Association (ELRA).</booktitle>
<editor>In Nicoletta Calzolari, Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odjik, Stelios Piperidis, and Daniel Tapias, editors,</editor>
<contexts>
<context position="8823" citStr="Mani et al., 2008" startWordPosition="1367" endWordPosition="1370"> as ACE, GUM, GML, KML, TRML which are briefly described and compared to SpatialML scheme in (MITRE Corporation, 2010). But 366 to our knowledge, the main obstacles for employing machine learning in this context and the very limited usage of this effective approach have been (a) the lack of an agreement on a unique semantic model for spatial information; (b) the diversity of formal spatial relations; and consequently (c) the lack of annotated data on which machine learning can be employed to learn and extract the spatial relations. The most systematic work in this area includes the SpatialML (Mani et al., 2008) scheme which focuses on geographical information, and the work of (Pustejovsky and Moszkowicz, 2009) in which the pivot of the spatial information is the spatial verb. The most recent and active work is the ISO-Space scheme (Pustejovsky et al., 2011) which is based on the above two schemes. The ideas behind ISOSpace are closely related to our annotation scheme in (Kordjamshidi et al., 2010b), however it considers more detailed and fine-grained spatial and linguistic elements which makes the preparation of the data for machine learning more difficult. Spatial information is directly related to</context>
</contexts>
<marker>Mani, Hitzeman, Richer, Harris, Quimby, Wellner, 2008</marker>
<rawString>I. Mani, J. Hitzeman, J. Richer, D. Harris, R. Quimby, and B. Wellner. 2008. SpatialML: Annotation scheme, corpora, and tools. In Nicoletta Calzolari, Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odjik, Stelios Piperidis, and Daniel Tapias, editors, Proceedings of the Sixth International Language Resources and Evaluation (LREC’08). European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>MITRE Corporation</author>
</authors>
<title>SpatialML: Annotation scheme for marking spatial expression in natural language.</title>
<date>2010</date>
<tech>Technical Report Version 3.0.1,</tech>
<institution>The MITRE Corporation.</institution>
<contexts>
<context position="8323" citStr="Corporation, 2010" startWordPosition="1286" endWordPosition="1287">ssession of a practical and appropriate annotation scheme along with data is the first requirement. To obtain this one has to investigate and schematize both linguistic and spatial ontologies. This process needs to cover the necessary information and semantics on the one hand, and to maintain the practical feasibility of the automatic annotation of unobserved data on the other hand. In recent research on spatial information and natural language, several annotation schemes have been proposed such as ACE, GUM, GML, KML, TRML which are briefly described and compared to SpatialML scheme in (MITRE Corporation, 2010). But 366 to our knowledge, the main obstacles for employing machine learning in this context and the very limited usage of this effective approach have been (a) the lack of an agreement on a unique semantic model for spatial information; (b) the diversity of formal spatial relations; and consequently (c) the lack of annotated data on which machine learning can be employed to learn and extract the spatial relations. The most systematic work in this area includes the SpatialML (Mani et al., 2008) scheme which focuses on geographical information, and the work of (Pustejovsky and Moszkowicz, 2009</context>
</contexts>
<marker>Corporation, 2010</marker>
<rawString>MITRE Corporation. 2010. SpatialML: Annotation scheme for marking spatial expression in natural language. Technical Report Version 3.0.1, The MITRE Corporation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>J L Moszkowicz</author>
</authors>
<title>Integrating motion predicate classes with spatial and temporal annotations.</title>
<date>2009</date>
<booktitle>In CoLing 2008: Companion volume Posters and Demonstrations,</booktitle>
<pages>95--98</pages>
<contexts>
<context position="8924" citStr="Pustejovsky and Moszkowicz, 2009" startWordPosition="1382" endWordPosition="1385">eme in (MITRE Corporation, 2010). But 366 to our knowledge, the main obstacles for employing machine learning in this context and the very limited usage of this effective approach have been (a) the lack of an agreement on a unique semantic model for spatial information; (b) the diversity of formal spatial relations; and consequently (c) the lack of annotated data on which machine learning can be employed to learn and extract the spatial relations. The most systematic work in this area includes the SpatialML (Mani et al., 2008) scheme which focuses on geographical information, and the work of (Pustejovsky and Moszkowicz, 2009) in which the pivot of the spatial information is the spatial verb. The most recent and active work is the ISO-Space scheme (Pustejovsky et al., 2011) which is based on the above two schemes. The ideas behind ISOSpace are closely related to our annotation scheme in (Kordjamshidi et al., 2010b), however it considers more detailed and fine-grained spatial and linguistic elements which makes the preparation of the data for machine learning more difficult. Spatial information is directly related to the part of the language that can be visualized. Thus, the extraction of spatial information is usef</context>
</contexts>
<marker>Pustejovsky, Moszkowicz, 2009</marker>
<rawString>J. Pustejovsky and J.L. Moszkowicz. 2009. Integrating motion predicate classes with spatial and temporal annotations. In CoLing 2008: Companion volume Posters and Demonstrations, pages 95–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>J Moszkowicz</author>
<author>M Verhagen</author>
</authors>
<title>Iso-space: The annotation of spatial information in language.</title>
<date>2011</date>
<booktitle>In Proceedings of ISA-6: ACL-ISO International Workshop on Semantic Annotation.</booktitle>
<contexts>
<context position="9074" citStr="Pustejovsky et al., 2011" startWordPosition="1408" endWordPosition="1411">is effective approach have been (a) the lack of an agreement on a unique semantic model for spatial information; (b) the diversity of formal spatial relations; and consequently (c) the lack of annotated data on which machine learning can be employed to learn and extract the spatial relations. The most systematic work in this area includes the SpatialML (Mani et al., 2008) scheme which focuses on geographical information, and the work of (Pustejovsky and Moszkowicz, 2009) in which the pivot of the spatial information is the spatial verb. The most recent and active work is the ISO-Space scheme (Pustejovsky et al., 2011) which is based on the above two schemes. The ideas behind ISOSpace are closely related to our annotation scheme in (Kordjamshidi et al., 2010b), however it considers more detailed and fine-grained spatial and linguistic elements which makes the preparation of the data for machine learning more difficult. Spatial information is directly related to the part of the language that can be visualized. Thus, the extraction of spatial information is useful for multimodal environments. One advantage of our proposed scheme is that it considers this dimension. Because it abstracts the spatial elements th</context>
</contexts>
<marker>Pustejovsky, Moszkowicz, Verhagen, 2011</marker>
<rawString>J. Pustejovsky, J. Moszkowicz, and M. Verhagen. 2011. Iso-space: The annotation of spatial information in language. In Proceedings of ISA-6: ACL-ISO International Workshop on Semantic Annotation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Roberts</author>
<author>S M Harabagiu</author>
</authors>
<title>Utd-sprl: A joint approach to spatial role labeling.</title>
<date>2012</date>
<note>In Submitted to this workshop of SemEval-2012.</note>
<contexts>
<context position="24461" citStr="Roberts and Harabagiu, 2012" startWordPosition="3901" endWordPosition="3904">nts are correctly predicted at the same time. The last evaluation is on how well the systems are able to retrieve the relations and their general type F1 = 2*recall*precision ( recall+precision)1 (3) 370 i.e {region, direction, distance} at the same time. To evaluate the GENERAL-TYPE similarly the RELATION tag is checked. For a true prediction, an exact match between the ground-truth and all the elements of the predicted RELATION tag including TR, LM,SP and GENERAL-TYPE is required. 7 Systems and results One system with two runs was submitted from the University of Texas Dallas. The two runs (Roberts and Harabagiu, 2012), UTDSPRL-SUPERVISED1 and UTDSPRL-SUPERVISED2 are based on the joint classification of the spatial triplets in a binary classification setting. To produce the candidate (indicator, trajector, landmark) triples, in the first stage heuristic rules targeting a high recall are used. Then a binary support vector machine classifier is employed to predict whether a triple is a spatial relation or not. Both runs start with a large number of manually engineered features, and use floating forward feature selection to select the most important ones. The difference between the two runs of UTDSPRL-SUPERVIS</context>
<context position="26216" citStr="Roberts and Harabagiu, 2012" startWordPosition="4173" endWordPosition="4176">) to annotate the sentence as a sequence. It considers the long distance dependencies between the prepositions and nouns in the sentence. The type and structure of the features used in the UTD and KUL systems are different. In the UTD system, the classifier works on triples and the features are of two main types: (a) argument-specific features about the trajector, landmark, or indicator e.g., the landmark’s hypernyms, or the indicator’s first token; and (b) joint features that consider two or more of the arguments, e.g. the dependency path between indicator and landmark. For more detail, see (Roberts and Harabagiu, 2012). In the KUL sysLabel Precsion Recall F1 TRAJECTOR 0.731 0.621 0.672 LANDMARK 0.871 0.645 0.741 SPATIAL-INDICATOR 0.928 0.712 0.806 RELATION 0.567 0.500 0.531 GENERAL-TYPE 0.561 0.494 0.526 Table 2: UTDSPRL-SUPERVISED1: The University of Texas-Dallas system with a larger number of features,test/train one split. Label Precsion Recall F1 TRAJECTOR 0.782 0.646 0.707 LANDMARK 0.894 0.680 0.772 SPATIAL-INDICATOR 0.940 0.732 0.823 RELATION 0.610 0.540 0.573 GENERAL-TYPE 0.603 0.534 0.566 Table 3: UTDSPRL-SUPERVISED2: The University of Texas-Dallas system with a smaller number of features, test/train</context>
</contexts>
<marker>Roberts, Harabagiu, 2012</marker>
<rawString>K. Roberts and S.M. Harabagiu. 2012. Utd-sprl: A joint approach to spatial role labeling. In Submitted to this workshop of SemEval-2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ross</author>
<author>J Hois</author>
<author>J Kelleher</author>
</authors>
<title>Computational models of spatial language interpretation.</title>
<date>2010</date>
<booktitle>In COSLI2010.</booktitle>
<contexts>
<context position="7277" citStr="Ross et al., 2010" startWordPosition="1124" endWordPosition="1127">ed here are trajector, landmark, spatial indicator, their links and the general type of their spatial relation. The general type of a relation can be direction, region or distance. 2 Motivation and related work Spatial role labeling is a key task for applications that are required to answer questions or reason about spatial relationships between entities. Examples include systems that perform text-to-scene conversion, generation of textual descriptions from visual data, robot navigation tasks, giving directional instructions, and geographical information systems (GIS). Recent research trends (Ross et al., 2010; Hois et al., 2011; Tellex et al., 2011) indicate an increasing interest in the area of extracting spatial information from language and mapping it to a formal spatial representation. Although cognitive-linguistic studies have investigated this problem extensively, the computational aspect of making this bridge between language and formal spatial representation (Hois and Kutz, 2008) is still in its elementary stages. The possession of a practical and appropriate annotation scheme along with data is the first requirement. To obtain this one has to investigate and schematize both linguistic and</context>
</contexts>
<marker>Ross, Hois, Kelleher, 2010</marker>
<rawString>R. Ross, J. Hois, and J. Kelleher. 2010. Computational models of spatial language interpretation. In COSLI2010.</rawString>
</citation>
<citation valid="true">
<title>Spatial and Temporal Reasoning.</title>
<date>1997</date>
<editor>O. Stock, editor.</editor>
<publisher>Kluwer.</publisher>
<marker>1997</marker>
<rawString>O. Stock, editor. 1997. Spatial and Temporal Reasoning. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sutton</author>
<author>A MacCallum</author>
</authors>
<title>Introduction to conditional random fields for relational learning.</title>
<date>2006</date>
<booktitle>In Lise Getoor and Ben Taskar, editors, Introduction to Statistical Relational Learning.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="25589" citStr="Sutton and MacCallum, 2006" startWordPosition="4071" endWordPosition="4074">election to select the most important ones. The difference between the two runs of UTDSPRL-SUPERVISED1 and UTDSPRLSUPERVISED2 is their feature set. Particularly, in UTDSPRL-SUPERVISED1 a joint feature based on the conjunctions (e.g. and, but) is considered before running feature selection but this feature is removed in UTDSPRL-SUPERVISED2. The submitted runs are compared to a previous system from the task organizers (Kordjamshidi et al., 2011c) which is evaluated on the current data with the same settings. This system, KUL-SKIPCHAIN-CRF, uses a skip chain conditional random field (CRF) model (Sutton and MacCallum, 2006) to annotate the sentence as a sequence. It considers the long distance dependencies between the prepositions and nouns in the sentence. The type and structure of the features used in the UTD and KUL systems are different. In the UTD system, the classifier works on triples and the features are of two main types: (a) argument-specific features about the trajector, landmark, or indicator e.g., the landmark’s hypernyms, or the indicator’s first token; and (b) joint features that consider two or more of the arguments, e.g. the dependency path between indicator and landmark. For more detail, see (R</context>
</contexts>
<marker>Sutton, MacCallum, 2006</marker>
<rawString>C. Sutton and A. MacCallum. 2006. Introduction to conditional random fields for relational learning. In Lise Getoor and Ben Taskar, editors, Introduction to Statistical Relational Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Tappan</author>
</authors>
<title>Knowledge-Based Spatial Reasoning for Automated Scene Generation from Text Descriptions.</title>
<date>2004</date>
<tech>Ph.D. thesis.</tech>
<contexts>
<context position="1632" citStr="Tappan, 2004" startWordPosition="237" endWordPosition="238">1 Introduction One of the essential functions of natural language is to talk about spatial relationships between objects. The sentence “Give me the book on AI on the big table behind the wall.” expresses information about the spatial configuration of the objects (book, table, wall) in some space. Particularly, it explains the region occupied by the book with respect to the table and the direction (orientation) of the table with respect to the wall. Understanding such spatial utterances is a problem in many areas, including robotics, navigation, traffic management, and query answering systems (Tappan, 2004). Linguistic constructs can express highly complex, relational structures of objects, spatial relations between them, and patterns of motion through space relative to some reference point. Compared to natural language, formal spatial models focus on one particular spatial aspect such as orientation, topology or distance and specify its underlying spatial logic in detail (Hois and Kutz, 2008). These formal models enable spatial reasoning that is difficult to perform on natural language expressions. Learning how to map natural language spatial information onto a formal representation is a challe</context>
</contexts>
<marker>Tappan, 2004</marker>
<rawString>D. A. Tappan. 2004. Knowledge-Based Spatial Reasoning for Automated Scene Generation from Text Descriptions. Ph.D. thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tellex</author>
<author>T Kollar</author>
<author>S Dickerson</author>
<author>M R Walter</author>
<author>N Roy A G Banerjee</author>
<author>S Teller</author>
</authors>
<title>Understanding natural language commands for robotic navigation and mobile manipulation.</title>
<date>2011</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence (AAAI),</booktitle>
<location>San Francisco, CA.</location>
<contexts>
<context position="7318" citStr="Tellex et al., 2011" startWordPosition="1132" endWordPosition="1135">al indicator, their links and the general type of their spatial relation. The general type of a relation can be direction, region or distance. 2 Motivation and related work Spatial role labeling is a key task for applications that are required to answer questions or reason about spatial relationships between entities. Examples include systems that perform text-to-scene conversion, generation of textual descriptions from visual data, robot navigation tasks, giving directional instructions, and geographical information systems (GIS). Recent research trends (Ross et al., 2010; Hois et al., 2011; Tellex et al., 2011) indicate an increasing interest in the area of extracting spatial information from language and mapping it to a formal spatial representation. Although cognitive-linguistic studies have investigated this problem extensively, the computational aspect of making this bridge between language and formal spatial representation (Hois and Kutz, 2008) is still in its elementary stages. The possession of a practical and appropriate annotation scheme along with data is the first requirement. To obtain this one has to investigate and schematize both linguistic and spatial ontologies. This process needs t</context>
</contexts>
<marker>Tellex, Kollar, Dickerson, Walter, Banerjee, Teller, 2011</marker>
<rawString>S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, and N. Roy A. G. Banerjee, S. Teller. 2011. Understanding natural language commands for robotic navigation and mobile manipulation. In Proceedings of the National Conference on Artificial Intelligence (AAAI), San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zlatevl</author>
</authors>
<title>Spatial semantics.</title>
<date>2007</date>
<booktitle>In Hubert Cuyckens and Dirk Geeraerts (eds.) The Oxford Handbook of Cognitive Linguistics, Chapter 13,</booktitle>
<pages>318--350</pages>
<contexts>
<context position="3015" citStr="Zlatevl, 2007" startWordPosition="448" endWordPosition="450">different applications on the other hand and the gap between the specification level of the two sides has led to the present situation that no well-defined framework for automatic spatial information extraction exists that can handle all of these aspects. In a previous paper (Kordjamshidi et al., 2010b), we introduced the task of spatial role labeling (SpRL) and proposed an annotation scheme that is language-independent and practically facilitates the application of machine learning techniques. Our framework consists of a set of spatial roles based on the theory of holistic spatial semantics (Zlatevl, 2007) with the intent of covering the main aspects of spatial concepts at a course level, including both static and dynamic spatial semantics. This shared task is defined on the basis of that annotation scheme. Since this is the first shared task on the spatial information and this particular data, we proposed a simplified version of the original scheme. The intention of this simplification was to make this practice feasible in the given timeframe. However, 365 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 365–373, Montr´eal, Canada, June 7-8, 2012. c�2012 Association </context>
</contexts>
<marker>Zlatevl, 2007</marker>
<rawString>J. Zlatevl. 2007. Spatial semantics. In Hubert Cuyckens and Dirk Geeraerts (eds.) The Oxford Handbook of Cognitive Linguistics, Chapter 13, pages 318–350.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>