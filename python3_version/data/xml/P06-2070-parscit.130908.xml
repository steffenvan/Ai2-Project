<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.994507">
Stochastic Iterative Alignment for Machine Translation Evaluation
</title>
<author confidence="0.998821">
Ding Liu and Daniel Gildea
</author>
<affiliation confidence="0.970721">
Department of Computer Science
University of Rochester
</affiliation>
<note confidence="0.546941">
Rochester, NY 14627
</note>
<sectionHeader confidence="0.980335" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997439">
A number of metrics for automatic eval-
uation of machine translation have been
proposed in recent years, with some met-
rics focusing on measuring the adequacy
of MT output, and other metrics focus-
ing on fluency. Adequacy-oriented met-
rics such as BLEU measure n-gram over-
lap of MT outputs and their references, but
do not represent sentence-level informa-
tion. In contrast, fluency-oriented metrics
such as ROUGE-W compute longest com-
mon subsequences, but ignore words not
aligned by the LCS. We propose a metric
based on stochastic iterative string align-
ment (SIA), which aims to combine the
strengths of both approaches. We com-
pare SIA with existing metrics, and find
that it outperforms them in overall evalu-
ation, and works specially well in fluency
evaluation.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965872727273">
Evaluation has long been a stumbling block in
the development of machine translation systems,
due to the simple fact that there are many correct
translations for a given sentence. Human evalu-
ation of system output is costly in both time and
money, leading to the rise of automatic evalua-
tion metrics in recent years. In the 2003 Johns
Hopkins Workshop on Speech and Language En-
gineering, experiments on MT evaluation showed
that BLEU and NIST do not correlate well with
human judgments at the sentence level, even when
they correlate well over large test sets (Blatz et
al., 2003). Liu and Gildea (2005) also pointed
out that due to the limited references for every
MT output, using the overlapping ratio of n-grams
longer than 2 did not improve sentence level eval-
uation performance of BLEU. The problem leads
to an even worse result in BLEU’S fluency eval-
uation, which is supposed to rely on the long n-
grams. In order to improve sentence-level evalu-
ation performance, several metrics have been pro-
posed, including ROUGE-W, ROUGE-S (Lin and
Och, 2004) and METEOR (Banerjee and Lavie,
2005). ROUGE-W differs from BLEU and NIST
in that it doesn’t require the common sequence be-
tween MT output and the references to be consec-
utive, and thus longer common sequences can be
found. There is a problem with loose-sequence-
based metrics: the words outside the longest com-
mon sequence are not considered in the metric,
even if they appear both in MT output and the
reference. ROUGE-S is meant to alleviate this
problem by computing the common skipped bi-
grams instead of the LCS. But the price ROUGE-
S pays is falling back to the shorter sequences and
losing the advantage of long common sequences.
METEOR is essentially a unigram based metric,
which prefers the monotonic word alignment be-
tween MT output and the references by penalizing
crossing word alignments. There are two prob-
lems with METEOR. First, it doesn’t consider
gaps in the aligned words, which is an important
feature for evaluating the sentence fluency; sec-
ond, it cannot use multiple references simultane-
ously.) ROUGE and METEOR both use WordNet
and Porter Stemmer to increase the chance of the
MT output words matching the reference words.
Such morphological processing and synonym ex-
traction tools are available for English, but are not
always available for other languages. In order to
take advantage of loose-sequence-based metrics
and avoid the problems in ROUGE and METEOR,
we propose a new metric SIA, which is based on
loose sequence alignment but enhanced with the
following features:
</bodyText>
<note confidence="0.785232">
&apos;METEOR and ROUGE both compute the score based on
the best reference
</note>
<page confidence="0.954126">
539
</page>
<note confidence="0.733869">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 539–546,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.878930166666666">
• Computing the string alignment score based
on the gaps in the common sequence. Though
ROUGE-W also takes into consider the gaps
in the common sequence between the MT
output and the reference by giving more cred-
its to the n-grams in the common sequence,
our method is more flexible in that not only
do the strict n-grams get more credits, but
also the tighter sequences.
• Stochastic word matching. For the purpose
of increasing hitting chance of MT outputs in
references, we use a stochastic word match-
ing in the string alignment instead of WORD-
STEM and WORD-NET used in METEOR
and ROUGE. Instead of using exact match-
ing, we use a soft matching based on the sim-
ilarity between two words, which is trained
in a bilingual corpus. The corpus is aligned
in the word level using IBM Model4 (Brown
et al., 1993). Stochastic word matching is a
uniform replacement for both morphological
processing and synonym matching. More im-
portantly, it can be easily adapted for differ-
ent kinds of languages, as long as there are
bilingual parallel corpora available (which is
always true for statistical machine transla-
tion).
• Iterative alignment scheme. In this scheme,
the string alignment will be continued until
there are no more co-occuring words to be
found between the MT output and any one of
the references. In this way, every co-occuring
word between the MT output and the refer-
ences can be considered and contribute to the
final score, and multiple references can be
used simultaneously.
The remainder of the paper is organized as fol-
lows: section 2 gives a recap of BLEU, ROUGE-
W and METEOR; section 3 describes the three
components of SIA; section 4 compares the per-
formance of different metrics based on experimen-
tal results; section 5 presents our conclusion.
</bodyText>
<sectionHeader confidence="0.9989865" genericHeader="method">
2 Recap of BLEU, ROUGE-W and
METEOR
</sectionHeader>
<bodyText confidence="0.9980585">
The most commonly used automatic evaluation
metrics, BLEU (Papineni et al., 2002) and NIST
(Doddington, 2002), are based on the assumption
that “The closer a machine translation is to a pro-
</bodyText>
<note confidence="0.324570333333333">
ref: Life is just like a box of tasty chocolate
mt1: Life is like one nice chocolate in box
ref: Life is just like a box of tasty chocolate
</note>
<figureCaption confidence="0.8582615">
mt2: Life is of one nice chocolate in box
Figure 1: Alignment Example for ROUGE-W
</figureCaption>
<bodyText confidence="0.997394707317073">
fessional human translation, the better it is” (Pa-
pineni et al., 2002). For every hypothesis, BLEU
computes the fraction of n-grams which also ap-
pear in the reference sentences, as well as a brevity
penalty. NIST uses a similar strategy to BLEU but
further considers that n-grams with different fre-
quency should be treated differently in the evalu-
ation (Doddington, 2002). BLEU and NIST have
been shown to correlate closely with human judg-
ments in ranking MT systems with different qual-
ities (Papineni et al., 2002; Doddington, 2002).
ROUGE-W is based on the weighted longest
common subsequence (LCS) between the MT out-
put and the reference. The common subsequences
in ROUGE-W are not necessarily strict n-grams,
and gaps are allowed in both the MT output and
the reference. Because of the flexibility, long
common subsequences are feasible in ROUGE-
W and can help to reflect the sentence-wide sim-
ilarity of MT output and references. ROUGE-W
uses a weighting strategy where the LCS contain-
ing strict n-grams is favored. Figure 1 gives two
examples that show how ROUGE-W searches for
the LCS. For mt1, ROUGE-W will choose either
life is like chocolate or life is like box as the LCS,
since neither of the sequences ’like box’ and ’like
chocolate’ are strict n-grams and thus make no dif-
ference in ROUGE-W (the only strict n-grams in
the two candidate LCS is life is). For mt2, there
is only one choice of the LCS: life is of chocolate.
The LCS of mt1 and mt2 have the same length and
the same number of strict n-grams, thus they get
the same score in ROUGE-W. But it is clear to us
that mt1 is better than mt2. It is easy to verify that
mt1 and mt2 have the same number of common 1-
grams, 2-grams, and skipped 2-grams with the ref-
erence (they don’t have common n-grams longer
than 2 words), thus BLEU and ROUGE-S are also
not able to differentiate them.
METEOR is a metric sitting in the middle
of the n-gram based metrics and the loose se-
</bodyText>
<page confidence="0.993424">
540
</page>
<figureCaption confidence="0.788219333333333">
ref: Life is just like a box of tasty chocolate
mt2: Life is of one nice chocolate in box
Figure 2: Alignment Example for METEOR
</figureCaption>
<bodyText confidence="0.999941647058824">
quence based metrics. It has several phases and
in each phase different matching techniques (EX-
ACT, PORTER-STEM, WORD-NET) are used to
make an alignment for the MT output and the ref-
erence. METEOR doesn’t require the alignment to
be monotonic, which means crossing word map-
pings (e.g. a b is mapped to b a) are allowed,
though doing so will get a penalty. Figure 2 shows
the alignments of METEOR based on the same
example as ROUGE. Though the two alignments
have the same number of word mappings, mt2 gets
more crossed word mappings than mt1, thus it will
get less credits in METEOR. Both ROUGE and
METEOR normalize their evaluation result based
on the MT output length (precision) and the ref-
erence length (recall), and the final score is com-
puted as the F-mean of them.
</bodyText>
<sectionHeader confidence="0.828265" genericHeader="method">
3 Stochastic Iterative Alignment (SIA)
for Machine Translation Evaluation
</sectionHeader>
<bodyText confidence="0.999983">
We introduce three techniques to allow more sen-
sitive scores to be computed.
</bodyText>
<subsectionHeader confidence="0.999202">
3.1 Modified String Alignment
</subsectionHeader>
<bodyText confidence="0.9735875">
This section introduces how to compute the string
alignment based on the word gaps. Given a pair
of strings, the task of string alignment is to obtain
the longest monotonic common sequence (where
gaps are allowed). SIA uses a different weighting
strategy from ROUGE-W, which is more flexible.
In SIA, the alignments are evaluated based on the
geometric mean of the gaps in the reference side
and the MT output side. Thus in the dynamic pro-
gramming, the state not only includes the current
covering length of the MT output and the refer-
ence, but also includes the last aligned positions in
them. The algorithm for computing the alignment
score in SIA is described in Figure 3. The sub-
routine COMPUTE SCORE, which computes the
score gained from the current aligned positions, is
shown in Figure 4. From the algorithm, we can
function GET ALIGN SCORE(mt, M, ref, N)
� Compute the alignment score of the MT output mt
with length M and the reference ref with length N
</bodyText>
<equation confidence="0.913572166666667">
for i = 1; i &lt;_ M; i = i +1 do
forj= 1; j &lt; N; j = j +1 do
fork= 1; k &lt;_ i; k = k +1 do
form= 1; m &lt;_ j; m = m +1 do
scorei,j,k,n,,
= max{scorei−1,j,k,n,,,scorei,j−1,k,n,, } ;
end for
end for
scorei,j,i,j =
max {scorei,j,i,j, scorei−1,j−1,n,p
n=1,M;p=1,N
+ COMPUTE SCORE(mt,ref, i, j, n, p)};
</equation>
<tableCaption confidence="0.594587">
end for
end for
</tableCaption>
<figure confidence="0.87443925">
scoreM,N,M,N
return ;
M
end function
</figure>
<figureCaption confidence="0.993817">
Figure 3: Alignment Algorithm Based on Gaps
</figureCaption>
<figure confidence="0.440749125">
function COMPUTE SCORE(mt, ref, i, j, n, p)
if mt[i] == ref [j] then
-,/
return 1/ (i − n) x (j − p);
else
return 0;
end if
end function
</figure>
<figureCaption confidence="0.7119285">
Figure 4: Compute Word Matching Score Based
on Gaps
</figureCaption>
<bodyText confidence="0.999793666666667">
see that not only will strict n-grams get higher
scores than non-consecutive sequences, but also
the non-consecutive sequences with smaller gaps
will get higher scores than those with larger gaps.
This weighting method can help SIA capture more
subtle difference of MT outputs than ROUGE-W
does. For example, if SIA is used to align mt1
and ref in Figure 1, it will choose life is like box
instead of life is like chocolate, because the aver-
age distance of ’box-box’ to its previous mapping
’like-like’ is less than ’chocolate-chocolate’. Then
the score SIA assigns to mt1 is:
</bodyText>
<equation confidence="0.967211">
(1 × 1 +1
1 × 1 +√11× 2 +5)×18 = 0.399
</equation>
<bodyText confidence="0.9920415">
For mt2, there is only one possible alignment,
its score in SIA is computed as:
</bodyText>
<equation confidence="0.997043">
(1 × 1 +1
1 × 1 +√11× 5 +3)×18 = 0.357
</equation>
<bodyText confidence="0.999623">
Thus, mt1 will be considered better than mt2 in
SIA, which is reasonable. As mentioned in sec-
tion 1, though loose-sequence-based metrics give
a better reflection of the sentence-wide similarity
of the MT output and the reference, they cannot
</bodyText>
<note confidence="0.8290435">
ref: Life is just like a box of tasty chocolate
mt1: Life is like one nice chocolate in box
</note>
<page confidence="0.99551">
541
</page>
<bodyText confidence="0.999857">
make full use of word-level information. This de-
fect could potentially lead to a poor performance
in adequacy evaluation, considering the case that
the ignored words are crucial to the evaluation. In
the later part of this section, we will describe an it-
erative alignment scheme which is meant to com-
pensate for this defect.
</bodyText>
<subsectionHeader confidence="0.999579">
3.2 Stochastic Word Mapping
</subsectionHeader>
<bodyText confidence="0.999972103448276">
In ROUGE and METEOR, PORTER-STEM and
WORD-NET are used to increase the chance of
the MT output words matching the references.
We use a different stochastic approach in SIA to
achieve the same purpose. The string alignment
has a good dynamic framework which allows the
stochastic word matching to be easily incorporated
into it. The stochastic string alignment can be im-
plemented by simply replacing the function COM-
PUTE SCORE with the function of Figure 5. The
function similarity(word1, word2) returns a ratio
which reflects how similar the two words are. Now
we consider how to compute the similarity ratio of
two words. Our method is motivated by the phrase
extraction method of Bannard and Callison-Burch
(2005), which computes the similarity ratio of two
words by looking at their relationship with words
in another language. Given a bilingual parallel
corpus with aligned sentences, say English and
French, the probability of an English word given
a French word can be computed by training word
alignment models such as IBM Model4. Then for
every English word e, we have a set of conditional
probabilities given each French word: p(ejf1),
p(ejf2), ... , p(ejfN). If we consider these proba-
bilities as a vector, the similarities of two English
words can be obtained by computing the dot prod-
uct of their corresponding vectors.2 The formula
is described below:
</bodyText>
<equation confidence="0.961378666666667">
N
similarity(ea7 ej) = p(eajfk)p(ejjfk) (3)
k=1
</equation>
<bodyText confidence="0.764232615384615">
Paraphrasing methods based on monolingual par-
allel corpora such as (Pang et al., 2003; Barzilay
and Lee, 2003) can also be used to compute the
similarity ratio of two words, but they don’t have
as rich training resources as the bilingual methods
do.
2Although the marginalized probability (over all French
words) of an English word given the other English word
(E&apos;1 p(eijfk)p(fkjef)) is a more intuitive way of measur-
ing the similarity, the dot product of the vectors p(ejf) de-
scribed above performed slightly better in our experiments.
function STO COMPUTE SCORE(mt, ref, i, j, n, p)
if mt[i] == ref [j] then
</bodyText>
<figure confidence="0.871786571428571">
-,/
return 1/ (i − n) x (j − p);
else
return similarity(mt[ib-f [i]);
\/(i−n) x (f−n)
end if
end function
</figure>
<figureCaption confidence="0.906301">
Figure 5: Compute Stochastic Word Matching
Score
</figureCaption>
<subsectionHeader confidence="0.991385">
3.3 Iterative Alignment Scheme
</subsectionHeader>
<bodyText confidence="0.9996264">
ROUGE-W, METEOR, and WER all score MT
output by first computing a score based on each
available reference, and then taking the highest
score as the final score for the MT output. This
scheme has the problem of not being able to use
multiple references simultaneously. The itera-
tive alignment scheme proposed here is meant to
alleviate this problem, by doing alignment be-
tween the MT output and one of the available ref-
erences until no more words in the MT output
can be found in the references. In each align-
ment round, the score based on each reference
is computed and the highest one is taken as the
score for the round. Then the words which have
been aligned in best alignment will not be con-
sidered in the next round. With the same num-
ber of aligned words, the MT output with fewer
alignment rounds should be considered better than
those requiring more rounds. For this reason, a
decay factor α is multiplied with the scores of
each round. The final score of the MT output is
then computed by summing the weighted scores
of each alignment round. The scheme is described
in Figure 6.
The function GET ALIGN SCORE 1 used
in GET ALIGN SCORE IN MULTIPLE REFS
is slightly different from GET ALIGN SCORE
described in the prior subsection. The dynamic
programming algorithm for getting the best
alignment is the same, except that it has two more
tables as input, which record the unavailable po-
sitions in the MT output and the reference. These
positions have already been used in the prior best
alignments and should not be considered in the
ongoing alignment. It also returns the aligned
positions of the best alignment. The pseudocode
for GET ALIGN SCORE 1 is shown in Figure 7.
The computation of the length penalty is similar
to BLEU: it is set to 1 if length of the MT output
is longer than the arithmetic mean of length of the
</bodyText>
<page confidence="0.987404">
542
</page>
<figure confidence="0.987690772727273">
function GET ALIGN SCORE IN MULTIPLE REFS(mt,
ref1, ..., refN, α)
� Iteratively Compute the Alignment Score Based on
Multiple References and the Decay Factor α
final score = 0;
while max score != 0 do
for i = 1, ..., N do
(score, align) =
GET ALIGN SCORE 1(mt, refi, mt table, ref tablei);
if score &gt; max score then
max score = score;
max align = align;
max ref = i;
end if
end for
final score += max score ×α;
α × = α;
Add the words in align to mt table and
ref tablemax ref;
end while
returnfinal score× length penalty;
end function
</figure>
<figureCaption confidence="0.999927">
Figure 6: Iterative Alignment Scheme
</figureCaption>
<bodyText confidence="0.9999752">
references, and otherwise is set to the ratio of the
two. Figure 8 shows how the iterative alignment
scheme works with an evaluation set containing
one MT output and two references. The selected
alignment in each round is shown, as well as the
unavailable positions in MT output and refer-
ences. With the iterative scheme, every common
word between the MT output and the reference
set can make a contribution to the metric, and
by such means SIA is able to make full use of
the word-level information. Furthermore, the
order (alignment round) in which the words are
aligned provides a way to weight them. In BLEU,
multiple references can be used simultaneously,
but the common n-grams are treated equally.
</bodyText>
<sectionHeader confidence="0.999738" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999920583333333">
Evaluation experiments were conducted to com-
pare the performance of different metrics includ-
ing BLEU, ROUGE, METEOR and SIA.3 The test
data for the experiments are from the MT evalu-
ation workshop at ACL05. There are seven sets
of MT outputs (E09 E11 E12 E14 E15 E17 E22),
all of which contain 919 English sentences. These
sentences are the translation of the same Chinese
input generated by seven different MT systems.
The fluency and adequacy of each sentence are
manually ranked from 1 to 5. For each MT output,
there are two sets of human scores available, and
</bodyText>
<footnote confidence="0.837072333333333">
3METEOR and ROUGE can be downloaded at
http://www.cs.cmu.edu/˜alavie/METEOR and
http://www.isi.edu/licensed-sw/see/rouge
</footnote>
<bodyText confidence="0.61365475">
function GET ALIGN SCORE1(mt, ref, mttable, reftable)
� Compute the alignment score of the MT output mt
with length M and the reference ref with length N, without
considering the positions in mttable and reftable
</bodyText>
<equation confidence="0.934493769230769">
M = |mt|; N = |ref|;
for i = 1; i ≤ M; i = i +1 do
forj= 1; j ≤ N; j = j +1 do
fork= 1; k ≤ i; k = k +1 do
form= 1; m ≤ j; m = m +1 do
scorei,j,k,m
= max{scorei_1,j,k,m, scorei,j_1,k,m};
end for
end for
if i is not in mttable and j is not in reftable then
scorei,j,i,j = n=1mM;pax=1,N {scorei,j,i,j,
,
scorei_1,j_1,n,p + COMPUTE SCORE(mt, ref ,i, j, n, p)};
</equation>
<tableCaption confidence="0.715606">
end if
end for
end for
</tableCaption>
<figure confidence="0.554041666666667">
return sc°reM,N,M,N and the corresponding alignment;
M
end function
</figure>
<figureCaption confidence="0.816803">
Figure 7: Alignment Algorithm Based on Gaps
Without Considering Aligned Positions
</figureCaption>
<reference confidence="0.58909">
m: England with France discussed this crisis in London
r1: Britain and France consulted about this crisis in London with each other
r2: England and France discussed the crisis in London
m: England with France discussed this crisis in London
r2: England and France discussed the crisis in London
r1: Britain and France consulted about this crisis in London with each other
m: England with France discussed this crisis in London
r1: Britain and France consulted about this crisis in London with each other
r2: England and France discussed the crisis in London
</reference>
<figureCaption confidence="0.954865">
Figure 8: Alignment Example for SIA
</figureCaption>
<page confidence="0.995731">
543
</page>
<bodyText confidence="0.999922307692308">
we randomly choose one as the score used in the
experiments. The human overall scores are calcu-
lated as the arithmetic means of the human fluency
scores and adequacy scores. There are four sets
of human translations (E01, E02, E03, E04) serv-
ing as references for those MT outputs. The MT
outputs and reference sentences are transformed to
lower case. Our experiments are carried out as fol-
lows: automatic metrics are used to evaluate the
MT outputs based on the four sets of references,
and the Pearson’s correlation coefficient of the au-
tomatic scores and the human scores is computed
to see how well they agree.
</bodyText>
<subsectionHeader confidence="0.998245">
4.1 N-gram vs. Loose Sequence
</subsectionHeader>
<bodyText confidence="0.999958066666667">
One of the problems addressed in this paper is
the different performance of n-gram based metrics
and loose-sequence-based metrics in sentence-
level evaluation. To see how they really differ
in experiments, we choose BLEU and ROUGE-
W as the representative metrics for the two types,
and used them to evaluate the 6433 sentences in
the 7 MT outputs. The Pearson correlation coeffi-
cients are then computed based on the 6433 sam-
ples. The experimental results are shown in Ta-
ble 1. BLEU-n denotes the BLEU metric with
the longest n-gram of length n. F denotes flu-
ency, A denotes adequacy, and O denotes overall.
We see that with the increase of n-gram length,
BLEU’s performance does not increase monoton-
ically. The best result in adequacy evaluation is
achieved at 2-gram and the best result in fluency is
achieved at 4-gram. Using n-grams longer than 2
doesn’t buy much improvement for BLEU in flu-
ency evaluation, and does not compensate for the
loss in adequacy evaluation. This confirms Liu and
Gildea (2005)’s finding that in sentence level eval-
uation, long n-grams in BLEU are not beneficial.
The loose-sequence-based ROUGE-W does much
better than BLEU in fluency evaluation, but it does
poorly in adequacy evaluation and doesn’t achieve
a significant improvement in overall evaluation.
We speculate that the reason is that ROUGE-W
doesn’t make full use of the available word-level
information.
</bodyText>
<subsectionHeader confidence="0.99661">
4.2 METEOR vs. SIA
</subsectionHeader>
<bodyText confidence="0.9869226">
SIA is designed to take the advantage of loose-
sequence-based metrics without losing word-level
information. To see how well it works, we choose
E09 as the development set and the sentences in
the other 6 sets as the test data. The decay fac-
</bodyText>
<table confidence="0.99886075">
B-3 R 1 R 2 M S
F 0.167 0.152 0.192 0.167 0.202
A 0.306 0.304 0.287 0.332 0.322
O 0.265 0.256 0.266 0.280 0.292
</table>
<tableCaption confidence="0.900809">
Table 2: Sentence level evaluation results of
BLEU, ROUGE, METEOR and SIA
</tableCaption>
<bodyText confidence="0.999792860465116">
tor in SIA is determined by optimizing the over-
all evaluation for E09, and then used with SIA
to evaluate the other 5514 sentences based on the
four sets of references. The similarity of English
words is computed by training IBM Model 4 in
an English-French parallel corpus which contains
seven hundred thousand sentence pairs. For every
English word, only the entries of the top 100 most
similar English words are kept and the similarity
ratios of them are then re-normalized. The words
outside the training corpus will be considered as
only having itself as its similar word. To com-
pare the performance of SIA with BLEU, ROUGE
and METEOR, the evaluation results based on
the same testing data is given in Table 2. B-
3 denotes BLEU-3; R 1 denotes the skipped bi-
gram based ROUGE metric which considers all
skip distances and uses PORTER-STEM; R 2 de-
notes ROUGE-W with PORTER-STEM; M de-
notes the METEOR metric using PORTER-STEM
and WORD-NET synonym; S denotes SIA.
We see that METEOR, as the other metric
sitting in the middle of n-gram based metrics
and loose sequence metrics, achieves improve-
ment over BLEU in both adequacy and fluency
evaluation. Though METEOR gets the best re-
sults in adequacy evaluation, in fluency evaluation,
it is worse than the loose-sequence-based metric
ROUGE-W-STEM. SIA is the only one among
the 5 metrics which does well in both fluency and
adequacy evaluation. It achieves the best results in
fluency evaluation and comparable results to ME-
TEOR in adequacy evaluation, and the balanced
performance leads to the best overall evaluation
results in the experiment. To estimate the signif-
icance of the correlations, bootstrap resampling
(Koehn, 2004) is used to randomly select 5514
sentences with replacement out of the whole test
set of 5514 sentences, and then the correlation co-
efficients are computed based on the selected sen-
tence set. The resampling is repeated 5000 times,
and the 95% confidence intervals are shown in Ta-
bles 3, 4, and 5. We can see that it is very diffi-
</bodyText>
<page confidence="0.995065">
544
</page>
<table confidence="0.999763">
BLEU-1 BLEU-2 BLEU-3 BLEU-4 BLEU-5 BLEU-6 ROUGE-W
F 0.147 0.162 0.166 0.168 0.165 0.164 0.191
A 0.288 0.296 0.291 0.285 0.279 0.274 0.268
O 0.243 0.256 0.255 0.251 0.247 0.244 0.254
</table>
<tableCaption confidence="0.995933">
Table 1: Sentence level evaluation results of BLEU and ROUGE-W
</tableCaption>
<table confidence="0.998408166666667">
low mean high WLS WLS WLS WLS
B-3 (-16.6%) 0.138 0.165 0.192 (+16.4%) PROB INCS PROB
R 1 (-17.8%) 0.124 0.151 0.177 (+17.3%) INCS
R 2 (-14.3%) 0.164 0.191 0.218 (+14.2%) F 0.189 0.202 0.188 0.202
M (-15.8%) 0.139 0.166 0.191 (+15.5%) A 0.295 0.310 0.311 0.322
S (-13.3%) 0.174 0.201 0.227 (+13.3%) O 0.270 0.285 0.278 0.292
</table>
<tableCaption confidence="0.996131">
Table 7: Results of different components in SIA
</tableCaption>
<table confidence="0.998306142857143">
low mean high WLS WLS WLS WLS
INCS INCS INCS INCS
B-3 (-08.2%) 0.280 0.306 0.330 (+08.1%) STEM WN STEM
R 1 (-08.5%) 0.278 0.304 0.329 (+08.4%) WN
R 2 (-09.2%) 0.259 0.285 0.312 (+09.5%) F 0.188 0.188 0.187 0.191
M (-07.3%) 0.307 0.332 0.355 (+07.0%) A 0.311 0.313 0.310 0.317
S (-07.9%) 0.295 0.321 0.346 (+07.8%) O 0.278 0.280 0.277 0.284
</table>
<tableCaption confidence="0.968657833333333">
Table 3: 95% significance intervals for sentence-
level fluency evaluation
Table 4: 95% significance intervals for sentence-
level adequacy evaluation
Table 8: Results of SIA working with Porter-Stem
and WordNet
</tableCaption>
<bodyText confidence="0.9968606">
cult for one metric to significantly outperform an-
other metric in sentence-level evaluation. The re-
sults show that the mean of the correlation factors
converges right to the value we computed based on
the whole testing set, and the confidence intervals
correlate with the means.
While sentence-level evaluation is useful if we
are interested in a confidence measure on MT out-
puts, syste-x level evaluation is more useful for
comparing MT systems and guiding their develop-
ment. Thus we also present the evaluation results
based on the 7 MT output sets in Table 6. SIA uses
the same decay factor as in the sentence-level eval-
uation. Its system-level score is computed as the
arithmetic mean of the sentence level scores, and
</bodyText>
<table confidence="0.993904">
low mean high
B-3 (-09.8%) 0.238 0.264 0.290 (+09.9%)
R 1 (-10.2%) 0.229 0.255 0.281 (+10.0%)
R 2 (-10.0%) 0.238 0.265 0.293 (+10.4%)
M (-09.0%) 0.254 0.279 0.304 (+08.8%)
S (-08.7%) 0.265 0.291 0.316 (+08.8%)
</table>
<tableCaption confidence="0.7692885">
Table 5: 95% significance intervals for sentence-
level overall evaluation
</tableCaption>
<bodyText confidence="0.998866166666667">
so are ROUGE, METEOR and the human judg-
ments. We can see that SIA achieves the best per-
formance in both fluency and adequacy evaluation
of the 7 systems. Though the 7-sample based re-
sults are not reliable, we can get a sense of how
well SIA works in the system-level evaluation.
</bodyText>
<subsectionHeader confidence="0.996451">
4.3 Components in SIA
</subsectionHeader>
<bodyText confidence="0.975638294117647">
To see how the three components in SIA con-
tribute to the final performance, we conduct exper-
iments where one or two components are removed
in SIA, shown in Table 7. The three components
are denoted as WLS (weighted loose sequence
alignment), PROB (stochastic word matching),
and INCS (iterative alignment scheme) respec-
tively. WLS without INCS does only one round
of alignment and chooses the best alignment score
as the final score. This scheme is similar to
ROUGE-W and METEOR. We can see that INCS,
as expected, improves the adequacy evaluation
without hurting the fluency evaluation. PROB
improves both adequacy and fluency evaluation
performance. The result that SIA works with
PORTER-STEM and WordNet is also shown in
Table 8. When PORTER-STEM and WordNet are
</bodyText>
<page confidence="0.987703">
545
</page>
<table confidence="0.99777975">
B-6 R 1 R 2 M S
F 0.514 0.466 0.458 0.378 0.532
A 0.876 0.900 0.906 0.875 0.928
O 0.794 0.790 0.792 0.741 0.835
</table>
<tableCaption confidence="0.999277">
Table 6: Results of BLEU, ROUGE, METEOR and SIA in system level evaluation
</tableCaption>
<bodyText confidence="0.954922166666667">
both used, PORTER-STEM is used first. We can
see that they are not as good as using the stochastic
word matching. Since INCS and PROB are inde-
pendent of WLS, we believe they can also be used
to improve other metrics such as ROUGE-W and
METEOR.
</bodyText>
<sectionHeader confidence="0.999703" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999464533333333">
This paper describes a new metric SIA for MT
evaluation, which achieves good performance by
combining the advantages of n-gram-based met-
rics and loose-sequence-based metrics. SIA uses
stochastic word mapping to allow soft or partial
matches between the MT hypotheses and the ref-
erences. This stochastic component is shown to
be better than PORTER-STEM and WordNet in
our experiments. We also analyzed the effect of
other components in SIA and speculate that they
can also be used in other metrics to improve their
performance.
Acknowledgments This work was supported
by NSF ITR IIS-09325646 and NSF ITR IIS-
0428020.
</bodyText>
<sectionHeader confidence="0.999507" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999340508474576">
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
correlation with human judegments. In Proceed-
ings of the ACL-04 workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, Ann Arbor, Michigan.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of the 43rd Annual Conference of the As-
sociation for Computational Linguistics (ACL-05).
Regina Barzilay and Lillian Lee. 2003. Learning
to paraphrase: An unsupervised approach using
multiple-sequence alignment. In Proceedings of the
2003 Meeting of the North American chapter of the
Association for Computational Linguistics (NAACL-
03), pages 16–23.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2003. Confidence es-
timation for machine translation. Technical report,
Center for Language and Speech Processing, Johns
Hopkins University, Baltimore. Summer Workshop
Final Report.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263–311.
G. Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In In HLT 2002, Human Lan-
guage Technology Conference, San Diego, CA.
Philipp Koehn. 2004. Statistical significance tests
for machine translation evaluation. In 2004 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 388–395, Barcelona,
Spain, July.
Chin-Yew Lin and Franz Josef Och. 2004. Auto-
matic evaluation of machine translation quality us-
ing longest common subsequence and skip-bigram
statistics. In Proceedings of the 42th Annual Confer-
ence of the Association for Computational Linguis-
tics (ACL-04), Barcelona, Spain.
Ding Liu and Daniel Gildea. 2005. Syntactic fea-
tures for evaluation of machine translation. In ACL
2005 Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for Machine Translation and/or Sum-
marization.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations:
Extracting paraphrases and generating new sen-
tences. In Proceedings of the 2003 Meeting of the
North American chapter of the Association for Com-
putational Linguistics (NAACL-03).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Conference of the Association for Com-
putational Linguistics (ACL-02), Philadelphia, PA.
</reference>
<page confidence="0.99851">
546
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.855037">
<title confidence="0.999763">Stochastic Iterative Alignment for Machine Translation Evaluation</title>
<author confidence="0.999899">Liu Gildea</author>
<affiliation confidence="0.99995">Department of Computer Science University of Rochester</affiliation>
<address confidence="0.999815">Rochester, NY 14627</address>
<abstract confidence="0.992344285714286">A number of metrics for automatic evaluation of machine translation have been proposed in recent years, with some metrics focusing on measuring the adequacy of MT output, and other metrics focusing on fluency. Adequacy-oriented metsuch as BLEU measure overlap of MT outputs and their references, but do not represent sentence-level information. In contrast, fluency-oriented metrics such as ROUGE-W compute longest common subsequences, but ignore words not aligned by the LCS. We propose a metric based on stochastic iterative string alignment (SIA), which aims to combine the strengths of both approaches. We compare SIA with existing metrics, and find that it outperforms them in overall evaluation, and works specially well in fluency evaluation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>m: England with France discussed this crisis in London r1: Britain and France consulted about this crisis in London with each other r2: England and France discussed the crisis in London m: England with France discussed this crisis in London r2: England and France discussed the crisis in London r1: Britain and France consulted about this crisis in London with each other m: England with France discussed this crisis in London r1: Britain and France consulted about this crisis in London with each other r2: England and France discussed the crisis in London</title>
<marker></marker>
<rawString>m: England with France discussed this crisis in London r1: Britain and France consulted about this crisis in London with each other r2: England and France discussed the crisis in London m: England with France discussed this crisis in London r2: England and France discussed the crisis in London r1: Britain and France consulted about this crisis in London with each other m: England with France discussed this crisis in London r1: Britain and France consulted about this crisis in London with each other r2: England and France discussed the crisis in London</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor: An automatic metric for mt evaluation with improved correlation with human judegments.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL-04 workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="2044" citStr="Banerjee and Lavie, 2005" startWordPosition="330" endWordPosition="333">with human judgments at the sentence level, even when they correlate well over large test sets (Blatz et al., 2003). Liu and Gildea (2005) also pointed out that due to the limited references for every MT output, using the overlapping ratio of n-grams longer than 2 did not improve sentence level evaluation performance of BLEU. The problem leads to an even worse result in BLEU’S fluency evaluation, which is supposed to rely on the long ngrams. In order to improve sentence-level evaluation performance, several metrics have been proposed, including ROUGE-W, ROUGE-S (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005). ROUGE-W differs from BLEU and NIST in that it doesn’t require the common sequence between MT output and the references to be consecutive, and thus longer common sequences can be found. There is a problem with loose-sequencebased metrics: the words outside the longest common sequence are not considered in the metric, even if they appear both in MT output and the reference. ROUGE-S is meant to alleviate this problem by computing the common skipped bigrams instead of the LCS. But the price ROUGES pays is falling back to the shorter sequences and losing the advantage of long common sequences. ME</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judegments. In Proceedings of the ACL-04 workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Conference of the Association for Computational Linguistics (ACL-05).</booktitle>
<contexts>
<context position="12643" citStr="Bannard and Callison-Burch (2005)" startWordPosition="2183" endWordPosition="2186">he MT output words matching the references. We use a different stochastic approach in SIA to achieve the same purpose. The string alignment has a good dynamic framework which allows the stochastic word matching to be easily incorporated into it. The stochastic string alignment can be implemented by simply replacing the function COMPUTE SCORE with the function of Figure 5. The function similarity(word1, word2) returns a ratio which reflects how similar the two words are. Now we consider how to compute the similarity ratio of two words. Our method is motivated by the phrase extraction method of Bannard and Callison-Burch (2005), which computes the similarity ratio of two words by looking at their relationship with words in another language. Given a bilingual parallel corpus with aligned sentences, say English and French, the probability of an English word given a French word can be computed by training word alignment models such as IBM Model4. Then for every English word e, we have a set of conditional probabilities given each French word: p(ejf1), p(ejf2), ... , p(ejfN). If we consider these probabilities as a vector, the similarities of two English words can be obtained by computing the dot product of their corres</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proceedings of the 43rd Annual Conference of the Association for Computational Linguistics (ACL-05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Learning to paraphrase: An unsupervised approach using multiple-sequence alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL03),</booktitle>
<pages>16--23</pages>
<contexts>
<context position="13451" citStr="Barzilay and Lee, 2003" startWordPosition="2315" endWordPosition="2318"> and French, the probability of an English word given a French word can be computed by training word alignment models such as IBM Model4. Then for every English word e, we have a set of conditional probabilities given each French word: p(ejf1), p(ejf2), ... , p(ejfN). If we consider these probabilities as a vector, the similarities of two English words can be obtained by computing the dot product of their corresponding vectors.2 The formula is described below: N similarity(ea7 ej) = p(eajfk)p(ejjfk) (3) k=1 Paraphrasing methods based on monolingual parallel corpora such as (Pang et al., 2003; Barzilay and Lee, 2003) can also be used to compute the similarity ratio of two words, but they don’t have as rich training resources as the bilingual methods do. 2Although the marginalized probability (over all French words) of an English word given the other English word (E&apos;1 p(eijfk)p(fkjef)) is a more intuitive way of measuring the similarity, the dot product of the vectors p(ejf) described above performed slightly better in our experiments. function STO COMPUTE SCORE(mt, ref, i, j, n, p) if mt[i] == ref [j] then -,/ return 1/ (i − n) x (j − p); else return similarity(mt[ib-f [i]); \/(i−n) x (f−n) end if end fun</context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase: An unsupervised approach using multiple-sequence alignment. In Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL03), pages 16–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blatz</author>
<author>Erin Fitzgerald</author>
<author>George Foster</author>
<author>Simona Gandrabur</author>
<author>Cyril Goutte</author>
<author>Alex Kulesza</author>
<author>Alberto Sanchis</author>
<author>Nicola Ueffing</author>
</authors>
<title>Confidence estimation for machine translation.</title>
<date>2003</date>
<tech>Technical report,</tech>
<institution>Center for Language and Speech Processing, Johns Hopkins University, Baltimore. Summer Workshop Final Report.</institution>
<contexts>
<context position="1534" citStr="Blatz et al., 2003" startWordPosition="244" endWordPosition="247"> in fluency evaluation. 1 Introduction Evaluation has long been a stumbling block in the development of machine translation systems, due to the simple fact that there are many correct translations for a given sentence. Human evaluation of system output is costly in both time and money, leading to the rise of automatic evaluation metrics in recent years. In the 2003 Johns Hopkins Workshop on Speech and Language Engineering, experiments on MT evaluation showed that BLEU and NIST do not correlate well with human judgments at the sentence level, even when they correlate well over large test sets (Blatz et al., 2003). Liu and Gildea (2005) also pointed out that due to the limited references for every MT output, using the overlapping ratio of n-grams longer than 2 did not improve sentence level evaluation performance of BLEU. The problem leads to an even worse result in BLEU’S fluency evaluation, which is supposed to rely on the long ngrams. In order to improve sentence-level evaluation performance, several metrics have been proposed, including ROUGE-W, ROUGE-S (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005). ROUGE-W differs from BLEU and NIST in that it doesn’t require the common sequence betwee</context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, Gandrabur, Goutte, Kulesza, Sanchis, Ueffing, 2003</marker>
<rawString>John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing. 2003. Confidence estimation for machine translation. Technical report, Center for Language and Speech Processing, Johns Hopkins University, Baltimore. Summer Workshop Final Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="4537" citStr="Brown et al., 1993" startWordPosition="749" endWordPosition="752">ence by giving more credits to the n-grams in the common sequence, our method is more flexible in that not only do the strict n-grams get more credits, but also the tighter sequences. • Stochastic word matching. For the purpose of increasing hitting chance of MT outputs in references, we use a stochastic word matching in the string alignment instead of WORDSTEM and WORD-NET used in METEOR and ROUGE. Instead of using exact matching, we use a soft matching based on the similarity between two words, which is trained in a bilingual corpus. The corpus is aligned in the word level using IBM Model4 (Brown et al., 1993). Stochastic word matching is a uniform replacement for both morphological processing and synonym matching. More importantly, it can be easily adapted for different kinds of languages, as long as there are bilingual parallel corpora available (which is always true for statistical machine translation). • Iterative alignment scheme. In this scheme, the string alignment will be continued until there are no more co-occuring words to be found between the MT output and any one of the references. In this way, every co-occuring word between the MT output and the references can be considered and contri</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram cooccurrence statistics.</title>
<date>2002</date>
<booktitle>In In HLT 2002, Human Language Technology Conference,</booktitle>
<location>San Diego, CA.</location>
<contexts>
<context position="5634" citStr="Doddington, 2002" startWordPosition="931" endWordPosition="932">he references. In this way, every co-occuring word between the MT output and the references can be considered and contribute to the final score, and multiple references can be used simultaneously. The remainder of the paper is organized as follows: section 2 gives a recap of BLEU, ROUGEW and METEOR; section 3 describes the three components of SIA; section 4 compares the performance of different metrics based on experimental results; section 5 presents our conclusion. 2 Recap of BLEU, ROUGE-W and METEOR The most commonly used automatic evaluation metrics, BLEU (Papineni et al., 2002) and NIST (Doddington, 2002), are based on the assumption that “The closer a machine translation is to a proref: Life is just like a box of tasty chocolate mt1: Life is like one nice chocolate in box ref: Life is just like a box of tasty chocolate mt2: Life is of one nice chocolate in box Figure 1: Alignment Example for ROUGE-W fessional human translation, the better it is” (Papineni et al., 2002). For every hypothesis, BLEU computes the fraction of n-grams which also appear in the reference sentences, as well as a brevity penalty. NIST uses a similar strategy to BLEU but further considers that n-grams with different fre</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>G. Doddington. 2002. Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. In In HLT 2002, Human Language Technology Conference, San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>388--395</pages>
<location>Barcelona, Spain,</location>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 388–395, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42th Annual Conference of the Association for Computational Linguistics (ACL-04),</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="2006" citStr="Lin and Och, 2004" startWordPosition="324" endWordPosition="327">and NIST do not correlate well with human judgments at the sentence level, even when they correlate well over large test sets (Blatz et al., 2003). Liu and Gildea (2005) also pointed out that due to the limited references for every MT output, using the overlapping ratio of n-grams longer than 2 did not improve sentence level evaluation performance of BLEU. The problem leads to an even worse result in BLEU’S fluency evaluation, which is supposed to rely on the long ngrams. In order to improve sentence-level evaluation performance, several metrics have been proposed, including ROUGE-W, ROUGE-S (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005). ROUGE-W differs from BLEU and NIST in that it doesn’t require the common sequence between MT output and the references to be consecutive, and thus longer common sequences can be found. There is a problem with loose-sequencebased metrics: the words outside the longest common sequence are not considered in the metric, even if they appear both in MT output and the reference. ROUGE-S is meant to alleviate this problem by computing the common skipped bigrams instead of the LCS. But the price ROUGES pays is falling back to the shorter sequences and losing the </context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och. 2004. Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. In Proceedings of the 42th Annual Conference of the Association for Computational Linguistics (ACL-04), Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Syntactic features for evaluation of machine translation.</title>
<date>2005</date>
<booktitle>In ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</booktitle>
<contexts>
<context position="1557" citStr="Liu and Gildea (2005)" startWordPosition="248" endWordPosition="251">n. 1 Introduction Evaluation has long been a stumbling block in the development of machine translation systems, due to the simple fact that there are many correct translations for a given sentence. Human evaluation of system output is costly in both time and money, leading to the rise of automatic evaluation metrics in recent years. In the 2003 Johns Hopkins Workshop on Speech and Language Engineering, experiments on MT evaluation showed that BLEU and NIST do not correlate well with human judgments at the sentence level, even when they correlate well over large test sets (Blatz et al., 2003). Liu and Gildea (2005) also pointed out that due to the limited references for every MT output, using the overlapping ratio of n-grams longer than 2 did not improve sentence level evaluation performance of BLEU. The problem leads to an even worse result in BLEU’S fluency evaluation, which is supposed to rely on the long ngrams. In order to improve sentence-level evaluation performance, several metrics have been proposed, including ROUGE-W, ROUGE-S (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005). ROUGE-W differs from BLEU and NIST in that it doesn’t require the common sequence between MT output and the ref</context>
</contexts>
<marker>Liu, Gildea, 2005</marker>
<rawString>Ding Liu and Daniel Gildea. 2005. Syntactic features for evaluation of machine translation. In ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-03).</booktitle>
<contexts>
<context position="13426" citStr="Pang et al., 2003" startWordPosition="2311" endWordPosition="2314">tences, say English and French, the probability of an English word given a French word can be computed by training word alignment models such as IBM Model4. Then for every English word e, we have a set of conditional probabilities given each French word: p(ejf1), p(ejf2), ... , p(ejfN). If we consider these probabilities as a vector, the similarities of two English words can be obtained by computing the dot product of their corresponding vectors.2 The formula is described below: N similarity(ea7 ej) = p(eajfk)p(ejjfk) (3) k=1 Paraphrasing methods based on monolingual parallel corpora such as (Pang et al., 2003; Barzilay and Lee, 2003) can also be used to compute the similarity ratio of two words, but they don’t have as rich training resources as the bilingual methods do. 2Although the marginalized probability (over all French words) of an English word given the other English word (E&apos;1 p(eijfk)p(fkjef)) is a more intuitive way of measuring the similarity, the dot product of the vectors p(ejf) described above performed slightly better in our experiments. function STO COMPUTE SCORE(mt, ref, i, j, n, p) if mt[i] == ref [j] then -,/ return 1/ (i − n) x (j − p); else return similarity(mt[ib-f [i]); \/(i−</context>
</contexts>
<marker>Pang, Knight, Marcu, 2003</marker>
<rawString>Bo Pang, Kevin Knight, and Daniel Marcu. 2003. Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences. In Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-03).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Conference of the Association for Computational Linguistics (ACL-02),</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="5606" citStr="Papineni et al., 2002" startWordPosition="925" endWordPosition="928">en the MT output and any one of the references. In this way, every co-occuring word between the MT output and the references can be considered and contribute to the final score, and multiple references can be used simultaneously. The remainder of the paper is organized as follows: section 2 gives a recap of BLEU, ROUGEW and METEOR; section 3 describes the three components of SIA; section 4 compares the performance of different metrics based on experimental results; section 5 presents our conclusion. 2 Recap of BLEU, ROUGE-W and METEOR The most commonly used automatic evaluation metrics, BLEU (Papineni et al., 2002) and NIST (Doddington, 2002), are based on the assumption that “The closer a machine translation is to a proref: Life is just like a box of tasty chocolate mt1: Life is like one nice chocolate in box ref: Life is just like a box of tasty chocolate mt2: Life is of one nice chocolate in box Figure 1: Alignment Example for ROUGE-W fessional human translation, the better it is” (Papineni et al., 2002). For every hypothesis, BLEU computes the fraction of n-grams which also appear in the reference sentences, as well as a brevity penalty. NIST uses a similar strategy to BLEU but further considers tha</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Conference of the Association for Computational Linguistics (ACL-02), Philadelphia, PA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>