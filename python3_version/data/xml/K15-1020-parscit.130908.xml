<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.929397">
Making the Most of Crowdsourced Document Annotations:
Confused Supervised LDA
</title>
<author confidence="0.996902">
Paul Felt
</author>
<affiliation confidence="0.9843375">
Dept. of Computer Science
Brigham Young University
</affiliation>
<email confidence="0.976917">
paul.lewis.felt@gmail.com
</email>
<author confidence="0.990773">
Jordan Boyd-Graber
</author>
<affiliation confidence="0.9998875">
Dept. of Computer Science
University of Colorado Boulder
</affiliation>
<email confidence="0.985555">
Jordan.Boyd.Graber@colorado.edu
</email>
<author confidence="0.991988">
Eric K. Ringger
</author>
<affiliation confidence="0.983774">
Dept. of Computer Science
Brigham Young University
</affiliation>
<email confidence="0.99327">
ringger@cs.byu.edu
</email>
<author confidence="0.9987">
Kevin Seppi
</author>
<affiliation confidence="0.9835555">
Dept. of Computer Science
Brigham Young University
</affiliation>
<email confidence="0.996751">
kseppi@byu.edu
</email>
<sectionHeader confidence="0.998591" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999770857142857">
Corpus labeling projects frequently use
low-cost workers from microtask market-
places; however, these workers are often
inexperienced or have misaligned incen-
tives. Crowdsourcing models must be ro-
bust to the resulting systematic and non-
systematic inaccuracies. We introduce a
novel crowdsourcing model that adapts the
discrete supervised topic model sLDA to
handle multiple corrupt, usually conflict-
ing (hence “confused”) supervision sig-
nals. Our model achieves significant gains
over previous work in the accuracy of de-
duced ground truth.
</bodyText>
<sectionHeader confidence="0.567832" genericHeader="method">
1 Modeling Annotators and Abilities
</sectionHeader>
<bodyText confidence="0.999838647058824">
Supervised machine learning requires labeled
training corpora, historically produced by labo-
rious and costly annotation projects. Microtask
markets such as Amazon’s Mechanical Turk and
Crowdflower have turned crowd labor into a com-
modity that can be purchased with relatively lit-
tle overhead. However, crowdsourced judgments
can suffer from high error rates. A common solu-
tion to this problem is to obtain multiple redundant
human judgments, or annotations,1 relying on the
observation that, in aggregate, non-experts often
rival or exceed experts by averaging over individ-
ual error patterns (Surowiecki, 2005; Snow et al.,
2008; Jurgens, 2013).
A crowdsourcing model harnesses the wisdom
of the crowd and infers labels based on the ev-
idence of the available annotations, imperfect
</bodyText>
<footnote confidence="0.9857055">
1In this paper, we call human judgments annotations to
distinguish them from gold standard class labels.
</footnote>
<bodyText confidence="0.999957923076923">
though they be. A common baseline crowd-
sourcing method aggregates annotations by ma-
jority vote, but this approach ignores important
information. For example, some annotators are
more reliable than others and their judgments
ought to be upweighted accordingly. State-of-
the-art crowdsourcing methods account for anno-
tator expertise, often through a probabilistic for-
malism. Compounding the challenge, assessing
unobserved annotator expertise is tangled with es-
timating ground truth from imperfect annotations;
thus, joint inference of these interrelated quantities
is necessary. State-of-the-art models also take the
data into account, because data features can help
ratify or veto human annotators.
We introduce a model that improves on state
of the art crowdsourcing algorithms by modeling
not only the annotations but also the features of
the data (e.g., words in a document). Section 2
identifies modeling deficiencies affecting previous
work and proposes a solution based on topic mod-
eling; Section 2.4 presents inference for the new
model. Experiments that contrast the proposed
model with select previous work on several text
classification datasets are presented in Section 3.
In Section 4 we highlight additional related work.
</bodyText>
<sectionHeader confidence="0.9971675" genericHeader="method">
2 Latent Representations that Reflect
Labels and Confusion
</sectionHeader>
<bodyText confidence="0.999596714285714">
Most crowdsourcing models extend the item-
response model of Dawid and Skene (1979). The
Bayesian version of this model, referred to here as
ITEMRESP, is depicted in Figure 1. In the gen-
erative story for this model, a confusion matrix
-y, is drawn for each human annotator j. Each
row -y,, of the confusion matrix -y, is drawn from
</bodyText>
<page confidence="0.98199">
194
</page>
<note confidence="0.992746">
Proceedings of the 19th Conference on Computational Language Learning, pages 194–203,
Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figureCaption confidence="0.9938654">
Figure 1: ITEMRESP as a plate diagram. Round
nodes are random variables. Rectangular nodes
are free parameters. Shaded nodes are observed.
D, J, C are the number of documents, annotators,
and classes, respectively.
</figureCaption>
<bodyText confidence="0.935195777777778">
a symmetric Dirichlet distribution Dir(b(γ)
jc ) and
encodes a categorical probability distribution over
label classes that annotator j is apt to choose when
presented with a document whose true label is c.
Then for each document d an unobserved docu-
ment label yd is drawn. Annotations are generated
as annotator j corrupts the true label yd according
to the categorical distribution Cat(γjyd).
</bodyText>
<subsectionHeader confidence="0.995649">
2.1 Leveraging Data
</subsectionHeader>
<bodyText confidence="0.992818866666667">
Some extensions to ITEMRESP model the features
of the data (e.g., words in a document). Many
data-aware crowdsourcing models condition the
labels on the data (Jin and Ghahramani, 2002;
Raykar et al., 2010; Liu et al., 2012; Yan et al.,
2014), possibly because discriminative classifiers
dominate supervised machine learning. Others
model the data generatively (Bragg et al., 2013;
Lam and Stork, 2005; Felt et al., 2014; Simp-
son and Roberts, 2015). Felt et al. (2015) argue
that generative models are better suited than condi-
tional models to crowdsourcing scenarios because
generative models often learn faster than their
conditional counterparts (Ng and Jordan, 2001)—
especially early in the learning curve. This advan-
tage is amplified by the annotation noise typical of
crowdsourcing scenarios.
Extensions to ITEMRESP that model document
features generatively tend to share a common
high-level architecture. After the document class
label yd is drawn for each document d, features are
drawn from class-conditional distributions. Felt et
al. (2015) identify the MOMRESP model, repro-
duced in Figure 2, as a strong representative of
generative crowdsourcing models. In MOMRESP,
Figure 2: MOMRESP as a plate diagram. IxdI =
V , the size of the vocabulary. Documents with
similar feature vectors x tend to share a common
label y. Reduces to mixture-of-multinomials clus-
tering when no annotations a are observed.
the feature vector xd for document d is drawn from
the multinomial distribution with parameter vector
φyd. This class-conditional multinomial model of
the data inherits many of the strengths and weak-
nesses of the naive Bayes model that it resem-
bles. Strengths include easy inference and a strong
inductive bias which helps the model be robust
to annotation noise and scarcity. Weaknesses in-
clude overly strict conditional independence as-
sumptions among features, leading to overconfi-
dence in the document model and thereby caus-
ing the model to overweight feature evidence and
underweight annotation evidence. This imbalance
can result in degraded performance in the presence
of high quality or many annotations.
</bodyText>
<subsectionHeader confidence="0.998717">
2.2 Confused Supervised LDA (CSLDA)
</subsectionHeader>
<bodyText confidence="0.999915529411765">
We solve the problem of imbalanced feature and
annotation evidence observed in MOMRESP by re-
placing the class-conditional structure of previous
generative crowdsourcing models with a richer
generative story where documents are drawn first
and class labels yd are obtained afterwards via a
log-linear mapping. This move towards condi-
tioning classes on documents content is sensible
because in many situations document content is
authored first, whereas label structure is not im-
posed until afterwards. It is plausible to assume
that there will exist some mapping from a latent
document structure to the desired document label
distinctions. Moreover, by jointly modeling top-
ics and the mapping to labels, we can learn the
latent document representations that best explain
how best to predict and correct annotator errors.
</bodyText>
<equation confidence="0.9867161">
γjc
c = 1 ... C
j = 1 ... J
yd
adj
j = 1 ... J
d = 1 ... D
b(θ)
θ
b(γ)
θ
b(θ)
b(γ)
yd
γjc
xd
adj
j = 1 ... J
d = 1 ... D
c = 1 ... C
j = 1 . . . J
b(φ)
c = 1 ... C
φc
195
t = 1 ... T
b(θ)
b(φ)
φt
n =
1 ... Nd
wdn
zdn
θd
yd
adj
j = 1 ... J
d = 1 ... D
c = 1 ... 0
c =
1 ... 0
j = 1 ... J
ηc
b(γ)
γjc
E
µ
Term Definition
Size of document d
P
n ✶(zdn = t)
P
d,n ✶(zdn = t)
P
d adjc, ✶(yd = c)
(Njc1 ··· NjcC)
P
dn ✶(wdn = v ∧ zdn = t)
P
dn ✶(zdn = t)
</equation>
<bodyText confidence="0.920932">
Count excludes variable being sampled
Vector where ¯zdt = Nd &amp; ✶ (zdn = t)
Excludes the zdn being sampled
</bodyText>
<figure confidence="0.9367749">
Nd
Ndt
Nt
Njcc,
Njc
Nvt
Nt
Nˆ
zd
ˆ¯zd
</figure>
<tableCaption confidence="0.6124115">
Table 1: Definition of counts and select notation.
✶(·) is the indicator function.
</tableCaption>
<bodyText confidence="0.9999519">
We call our model confused supervised LDA
(CSLDA, Figure 3), based on supervised topic
modeling. Latent Dirichlet Allocation (Blei et
al., 2003, LDA) models text documents as ad-
mixtures of word distributions, or topics. Al-
though pre-calculated LDA topics as features can
inform a crowdsourcing model (Levenberg et al.,
2014), supervised LDA (sLDA) provides a prin-
cipled way of incorporating document class la-
bels and topics into a single model, allowing topic
variables and response variables to co-inform one
another in joint inference. For example, when
sLDA is given movie reviews labeled with sen-
timent, inferred topics cluster around sentiment-
heavy words (Mcauliffe and Blei, 2007), which
may be quite different from the topics inferred by
unsupervised LDA. One way to view CSLDA is as
a discrete sLDA in settings with noisy supervision
from multiple, imprecise annotators.
The generative story for CSLDA is:
</bodyText>
<listItem confidence="0.9692185625">
1. Draw per-topic word distributions φt from
Dir(b(θ)).
2. Draw per-class regression parameters ηc from
Gauss(µ, E).
3. Draw per-annotator confusion matrices γj
with row γjc drawn from Dir(b(γ)
jc ).
4. For each document d,
(a) Draw topic vector θd from Dir(b(θ)).
(b) For each token position n, draw topic
zdn from Cat(θd) and word wdn from
Cat(φzdn).
(c) Draw class label yd with probability pro-
portional to exp[ηyd¯zd].
(d) For each annotator j draw annotation
vector adj from γjyd.
</listItem>
<figureCaption confidence="0.864922">
Figure 3: CSLDA as a plate diagram. D, J, C, T
</figureCaption>
<bodyText confidence="0.9295016">
are the number of documents, annotators, classes,
and topics, respectively. Nd is the size of docu-
ment d. |φt |= V , the size of the vocabulary. ηc
is a vector of regression parameters. Reduces to
LDA when no annotations a are observed.
</bodyText>
<subsectionHeader confidence="0.996392">
2.3 Stochastic EM
</subsectionHeader>
<bodyText confidence="0.9999959">
We use stochastic expectation maximization (EM)
for posterior inference in CSLDA, alternating be-
tween sampling values for topics z and document
class labels y (the E-step) and optimizing values
of regression parameters η (the M-step). To sam-
ple z and y efficiently, we derive the full condi-
tional distributions of z and y in a collapsed model
where θ, φ, and γ have been analytically integrated
out. Omitting multiplicative constants, the col-
lapsed model joint probability is
</bodyText>
<equation confidence="0.978394777777778">
p(z, w, y, a|η) =p(z)p(w|z)p(y|z, η)p(a|y) (1)
! !
Y
B(Nd+b(θ)) · B(Nt+b(φ)
t )
t
! ⎛ ⎞
exp(η� yd ¯zd) ⎝Y Y B(Njc+b�7))
Pc exp(ηczd) j c
</equation>
<bodyText confidence="0.9991375">
where B(·) is the Beta function (multivariate as
necessary), counts N and related symbols are de-
fined in Table 1, and M(a) = Qd,j M(adj) where
M(adj) is the multinomial coefficient.
Simplifying Equation 1 yields full conditionals
for each word zdn,
</bodyText>
<equation confidence="0.8867166">
� �
p(zdn = t|ˆz, w, y, a,η) a ˆNdt + b(θ) (2)
t
exp(ηydt
Nd )
Pc exp(ηct
Nd + ηC ˆ¯zd),
a M(a)· Y
d
Y·
d
ˆNwdnt + b(φ)
wdn
· ˆNt + |b(φ)|1
·
</equation>
<page confidence="0.983147">
196
</page>
<bodyText confidence="0.985359">
and similarly for document label yd:
</bodyText>
<equation confidence="0.98337975">
c
Q
0(ˆNjcc0 + b(γ))adjc0
~ P+ b(γ)jcc0) Ec0 adjc0
</equation>
<bodyText confidence="0.997909529411765">
where xk A x(x + 1) · · · (x + k − 1) is the rising
factorial. In Equation 2 the first and third terms
are independent of word n and can be cached at
the document level for efficiency.
For the M-step, we train the regression param-
eters η (containing one vector per class) by opti-
mizing the same objective function as for training
a logistic regression classifier, assuming that class
y is given:
We optimize the objective (Equation 4) using L-
BFGS and a regularizing Gaussian prior with µ =
0, σ2 = 1.
While EM is sensitive to initialization, CSLDA
is straightforward to initialize. Majority vote is
used to set initial y values ˜y. Corresponding initial
values for z and η are obtained by clamping y to y˜
and running stochastic EM on z and η.
</bodyText>
<subsectionHeader confidence="0.991102">
2.4 Hyperparameter Optimization
</subsectionHeader>
<bodyText confidence="0.99998575">
Ideally, we would test CSLDA performance under
all of the many algorithms available for inference
in such a model. Although that is not feasible,
Asuncion et al. (2009) demonstrate that hyperapa-
rameter optimization in LDA topic models helps
to bring the performance of alternative inference
algorithms into approximate agreement. Accord-
ingly, in Section 2.4 we implement hyperparame-
ter optimization for CSLDA to make our results as
general as possible.
Before moving on, however, we take a moment
to validate that the observation of Asuncion et al.
generalizes from LDA to the ITEMRESP model,
which, together with LDA, comprises CSLDA.
Figure 4 demonstrates that three ITEMRESP infer-
ence algorithms, Gibbs sampling (Gibbs), mean-
field variational inference (Var), and the iter-
ated conditional modes algorithm (ICM) (Besag,
1986), are brought into better agreement after opti-
mizing their hyperparameters via grid search. That
</bodyText>
<figure confidence="0.995670368421053">
NEWSGROUPS
0.8
algorithm
Gibbs
Var
ICM
0.2
of annotated instancs x 1,000
(a) Hyperparameters fixed
NEWSGROUPS
0.8
algorithm
Gibbs
Var
ICM
0.2
(b) Hyperparameters optimized via grid search on validation
Number of annotated instances x 1000
data
</figure>
<figureCaption confidence="0.66422325">
Figure 4: Differences among the inferred label ac-
curacy learning curves of three ITEMRESP infer-
ence algorithms are reduced when hyperparame-
ters are optimized.
</figureCaption>
<bodyText confidence="0.999721166666667">
is, the algorithms in Figure 4b are in better agree-
ment, particularly near the extremes, than the algo-
rithms in Figure 4a. This difference is subtle, but it
holds to an equal and greater extent in other simu-
lation conditions we tested (experiment details are
similar to those reported in Section 3).
</bodyText>
<subsectionHeader confidence="0.793348">
Fixed-point Hyperparameter Updates
</subsectionHeader>
<bodyText confidence="0.9997432">
Although a grid search is effective, it is not prac-
tical for a model with many hyperparameters such
as CSLDA. For efficiency, therefore, we use the
fixed-point updates of Minka (2000). Our up-
dates differ slightly from Minka’s since we tie hy-
perparameters, allowing them to be learned more
quickly from less data. In our implementation the
matrices of hyperparameters b(φ) and b(θ) over the
Dirichlet-multinomial distributions are completely
tied such that b(φ)
</bodyText>
<equation confidence="0.924767625">
tv = b(φ)∀t, v and b(θ) t= b(θ)∀t.
This leads to
P t,v[Ψ(Ntv+b(φ))]−TV Ψ(b(φ))
b(φ)←b(φ)· (5)
V [Ψ(Nt+V b(φ))−Ψ(V b(φ))]
and
b(θ)←b(θ)·Pd,t[Ψ(Ndt+b(θ))]−NTΨ(b(θ))(6)
T [Ψ(Nd+Tb(θ))−Ψ(Tb(θ))]
</equation>
<bodyText confidence="0.946136">
The updates for b(γ) are slightly more involved
since we choose to tie the diagonal entries b(γ)
d and
separately the off-diagonal entries b(γ)
</bodyText>
<equation confidence="0.933706615384615">
o , updating
each separately:
(7) (γ) Pj,c[Ψ(Njcc+bdγ))]−JCΨ(b(dγ)) 7
bd Abd ·Z(b(γ)) ( )
Y
j
Y exp(ηTc ¯zd) (4)
p(y = c|z, η) = (
d Pc0 exp zd)
exp(ηT c ¯zd)
p(yd = c|z, w, y, a, η) ∝
P
c0 exp(ηTc0 ¯zd) (3)
</equation>
<figure confidence="0.993435105263158">
Accuracy
0.6
0.4
Accuracy
0.6
0.4
197
b(γ)
o·
E
j,c,c,7�c
[T (Njcc,+b(γ)o)]−JC(C−1)T (b(γ))]
(C−1)Z(b(γ))
where
�
Z(b(γ)) =
j,c
[T(Njc + b(γ)
d + (C − 1)b(γ)
o )]
and b(γ)
o ,
WEATHER
0 5 10 15
Number of annotations x 1,000
algorithm
csLDA
MomResp
LogResp
ItemResp
Majority
1.0
0.9
Accuracy
0.8
0.7
0.6
(8)
</figure>
<equation confidence="0.417304666666667">
− JCT(b(γ)
d + (C − 1)b(γ)
o ).
</equation>
<bodyText confidence="0.9999749">
As in the work of Asuncion et al. (2009), we add
an algorithmic gamma prior (b(·) — G(α, Q)) for
smoothing by adding α−1 b(·)to the numerator and Q
to the denominator of Equations 5-8. Note that
these algorithmic gamma “priors” should not be
understood as first-class members of the CSLDA
model (Figure 3). Rather, they are regularization
terms that keep our hyperparameter search algo-
rithm from straying towards problematic values
such as 0 or oc.
</bodyText>
<sectionHeader confidence="0.999796" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.996602241379311">
For all experiments we set CSLDA’s number of
topics T to 1.5 times the number of classes in each
dataset. We found that model performance was
reasonably robust to this parameter. Only when
T drops below the number of label classes does
performance suffer. As per Section 2.3, z and q
values are initialized with 500 rounds of stochas-
tic EM, after which the full model is updated with
1000 additional rounds. Predictions are generated
by aggregating samples from the last 100 rounds
(the mode of the approximate marginal posterior).
We compare CSLDA with (1) a majority vote
baseline, (2) the ITEMRESP model, and rep-
resentatives of the two main classes of data-
aware crowdsourcing models, namely (3) data-
generative and (4) data-conditional. MOMRESP
represents a typical data-generative model (Bragg
et al., 2013; Felt et al., 2014; Lam and Stork, 2005;
Simpson and Roberts, 2015). Data-conditional ap-
proaches typically model data features condition-
ally using a log-linear model (Jin and Ghahramani,
2002; Raykar et al., 2010; Liu et al., 2012; Yan et
al., 2014). For the purposes of this paper, we re-
fer to this model as LOGRESP. For ITEMRESP,
MOMRESP, and LOGRESP we use the variational
inference methods presented by Felt et al. (2015).
Unlike that paper, in this work we have augmented
inference with the in-line hyperparameter updates
described in Section 2.4.
</bodyText>
<figureCaption confidence="0.9834365">
Figure 5: Inferred label accuracy of models on
sentiment-annotated weather tweets.
</figureCaption>
<subsectionHeader confidence="0.99023">
3.1 Human-generated Annotations
</subsectionHeader>
<bodyText confidence="0.999787205882353">
To gauge the effectiveness of data-aware crowd-
sourcing models, we use the sentiment-annotated
tweet dataset distributed by CrowdFlower as a
part of its “data for everyone” initiative.2 In the
“Weather Sentiment” task, 20 annotators judged
the sentiment of 1000 tweets as either positive,
negative, neutral, or unrelated to the weather.
In the secondary “Weather Sentiment Evaluated”
task, 10 additional annotators judged the correct-
ness of each consensus label. We construct a
gold standard from the consensus labels that were
judged to be correct by 9 of the 10 annotators in
the secondary task.
Figure 5 plots learning curves of the accuracy
of model-inferred labels as annotations are added
(ordered by timestamp). All methods, including
majority vote, converge to roughly the same accu-
racy when all 20,000 annotations are added. When
fewer annotations are available, statistical mod-
els beat majority vote, and CSLDA is consider-
ably more accurate than other approaches. Learn-
ing curves are bumpy because annotation order is
not random and because inferred label accuracy is
calculated only over documents with at least one
annotation. Learning curves collectively increase
when average annotation depth (the number of an-
notations per item) increases and decrease when
new documents are annotated and average anno-
tation depth decreases. CSLDA stands out by be-
ing more robust to these changes than other algo-
rithms, and also by maintaining a higher level of
accuracy across the board. This is important be-
cause high accuracy using fewer annotations trans-
lates to decreased annotations costs.
</bodyText>
<footnote confidence="0.955864">
2http://www.crowdflower.com/data-for-everyone
</footnote>
<page confidence="0.990204">
198
</page>
<table confidence="0.99862325">
D C V 1 d Nd
N �
20 News 16,995 20 22,851 111
WebKB 3,543 4 5,781 131
Reuters8 6,523 8 6,776 53
Reuters52 7,735 52 5,579 58
CADE12 34,801 12 41,628 110
Enron 3,854 32 14,069 431
</table>
<tableCaption confidence="0.6741644">
Table 2: Dataset statistics. D is number of doc-
uments, C is number of classes, V is number of
features, and N, Ed Nd is average document size.
Values are calculated after setting aside 15% as
validation data and doing feature selection.
</tableCaption>
<subsectionHeader confidence="0.998694">
3.2 Synthetic Annotations
</subsectionHeader>
<bodyText confidence="0.99958771875">
Datasets including both annotations and gold stan-
dard labels are in short supply. Although plenty
of text categorization datasets have been anno-
tated, common practice reflects that initial noisy
annotations be discarded and only consensus la-
bels be published. Consequently, we follow pre-
vious work in achieving broad validation by con-
structing synthetic annotators that corrupt known
gold standard labels. We base our experimen-
tal setup on the annotations gathered by Felt et
al. (2015),3 who paid CrowdFlower annotators to
relabel 1000 documents from the well-known 20
Newsgroups classification dataset. In that exper-
iment, 136 annotators contributed, each instance
was labeled an average of 6.9 times, and anno-
tator accuracies were distributed approximately
according to a Beta(3.6, 5.1) distribution. Ac-
cordingly we construct 100 synthetic annotators,
each parametrized by an accuracy drawn from
Beta(3.6, 5.1) and with errors drawn from a sym-
metric Dirichlet Dir(1). Datasets are annotated
by selecting an instance (at random without re-
placement) and then selecting K annotators (at
random without replacement) to annotate it before
moving on. We choose K = 7 to mirror the em-
pirical average in the CrowdFlower annotation set.
We evaluate on six text classification datasets,
summarized in Table 2. The 20 Newsgroups, We-
bKB, Cade12, Reuters8, and Reuters52 datasets
are described in more detail by Cardoso-Cachopo
(2007). The LDC-labeled Enron emails dataset is
described by Berry et al. (2001). Each dataset is
</bodyText>
<footnote confidence="0.945506">
3The dataset is available via git at git://nlp.cs.
byu.edu/plf1/crowdflower-newsgroups.git
</footnote>
<figure confidence="0.978997333333333">
NEWSGROUPS
0 10 20
nnotat
CADE12
algorithm
csLDA
MomResp
LogResp
ItemResp
Majority
0 50 100 150 200
Number of annotations x 1,000
</figure>
<figureCaption confidence="0.914383466666667">
Figure 6: Inferred label accuracy of models on
synthetic annotations. The first instance is anno-
tated 7 times, then the second, and so on.
preprocessed via Porter stemming and by removal
of the stopwords from MALLET’s stopword list
(McCallum, 2002). Features occurring fewer than
5 times in the corpus are discarded. In the case
of MOMRESP, features are fractionally scaled so
that each document is the same length, in keep-
ing with previous work in multinomial document
models (Nigam et al., 2006).
Figure 6 plots learning curves on three repre-
sentative datasets (Enron resembles Cade12, and
the Reuters datasets resemble WebKB). CSLDA
consistently outperforms LOGRESP, ITEMRESP,
</figureCaption>
<bodyText confidence="0.9979356">
and majority vote. The generative models
(CSLDA and MOMRESP) tend to excel in low-
annotation portions of the learning curve, par-
tially because generative models tend to converge
quickly and partially because generative models
naturally learn from unlabeled documents (i.e.,
semi-supervision). However, MOMRESP tends to
quickly reach a performance plateau after which
additional annotations do little good. The perfor-
mance of MOMRESP is also highly dataset de-
</bodyText>
<figure confidence="0.998630033333333">
WEBKB
algorithm
csLDA
MomResp
LogResp
ItemResp
Majority
Accuracy
0.95
0.90
0.85
0.80
1.00
0 25 50 75 100 125
algorithm
csLDA
MomResp
LogResp
ItemResp
Majority
Accuracy
0.9
0.8
0.7
0.6
Accuracy
0.9
0.8
0.7
0.6
</figure>
<page confidence="0.979527">
199
</page>
<table confidence="0.965181428571429">
95% Accuracy CSLDA MOMRESP LOGRESP ITEMRESP Majority
20 News 85 (5.0x) 150 (8.8x) 152 (8.9x) 168 (9.9x) 233 (13.7x)
WebKB 31 (8.8x) - 46 (13.0x) 46 (13.0x) -
Reuters8 25 (3.8x) - 73 (11.2x) 62 (9.5x) -
Reuters52 33 (4.3x) 73 (9.4x) 67.5 (8.7x) 60 (7.8x) 87 (11.2x)
CADE12 250 (7.2x) - 295 (8.5x) 290 (8.3x) 570 (16.4x)
Enron 31 (8.0x) - 40 (10.4x) 38 (9.9x) 47 (12.2x)
</table>
<tableCaption confidence="0.995885">
Table 3: The number of annotations ×1000 at which the algorithm reaches 95% inferred label accuracy
</tableCaption>
<bodyText confidence="0.977808923076923">
on the indicated dataset (average annotations per instance are in parenthesis). All instances are annotated
once, then twice, and so on. Empty entries (’-’) do not reach 95% even with 20 annotations per instance.
pendent: it is good on 20 Newsgroups, mediocre
on WebKB, and poor on CADE12. By contrast,
CSLDA is relatively stable across datasets.
To understand the different behavior of the two
generative models, recall that MOMRESP is iden-
tical to ITEMRESP save for its multinomial data
model. Indeed, the equations governing infer-
ence of label y in MOMRESP simply sum together
terms from an ITEMRESP model and terms from
a mixture of multinomials clustering model (and
for reasons explained in Section 2.1, the multino-
mial data model terms tend to dominate). There-
fore when MOMRESP diverges from ITEMRESP
it is because MOMRESP is attracted toward a y as-
signment that satisfies the multinomial data model,
grouping similar documents together. This can
both help and hurt. When data clusters and la-
bel classes are misaligned, MOMRESP falters (as
in the case of the Cade12 dataset). In contrast,
CSLDA’s flexible mapping from topics to labels
is less sensitive: topics can diverge from label
classes so long as there exists some linear trans-
formation from the topics to the labels.
Many corpus annotation projects are not com-
plete until the corpus achieves some target level of
quality. We repeat the experiment reported in Fig-
ure 6, but rather than simulating seven annotations
for each instance before moving on, we simulate
one annotation for each instance, then two, and so
on until each instance in the dataset is annotated
20 times. Table 3 reports the minimal number of
annotations before an algorithm’s inferred labels
reach an accuracy of 95%, a lofty goal that can re-
quire significant amounts of annotation when us-
ing poor quality annotations. CSLDA achieves
95% accuracy with fewer annotations, correspond-
ing to reduced annotation cost.
</bodyText>
<figure confidence="0.974301857142857">
NEWSGROUPS
algorithm
csLDA
csLDA−P
0.70
20 30 40 50 60
Number of annotations x 1,000
</figure>
<figureCaption confidence="0.9918175">
Figure 7: Joint inference for CSLDA vs pipeline
inference (CSLDA-P).
</figureCaption>
<subsectionHeader confidence="0.999432">
3.3 Joint vs Pipeline Inference
</subsectionHeader>
<bodyText confidence="0.9999067">
To isolate the effectiveness of joint inference in
CSLDA, we compare against the pipeline alterna-
tive where topics are inferred first and then held
constant during inference (Levenberg et al., 2014).
Joint inference yields modest but consistent bene-
fits over a pipeline approach. Figure 7 highlights
a portion of the learning curve on the Newsgroups
dataset (based on the experiments summarized in
Table 3). This trend holds across all of the datasets
that we examined.
</bodyText>
<subsectionHeader confidence="0.93978">
3.4 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999859071428571">
Class-conditional models like MOMRESP include
a feature that data-conditional models like CSLDA
lack: an explicit prior over class prevalence. Fig-
ure 8a shows that CSLDA performs poorly on the
CrowdFlower-annotated Newsgroups documents
described at the beginning of Section 3 (not the
synthetic annotations). Error analysis uncovers
that CSLDA lumps related classes together in this
dataset. This is because annotators could specify
up to 3 simultaneous labels for each annotation,
so that similar labels (e.g., “talk.politics.misc”
and “talk.politics.mideast”) are usually chosen in
blocks. Suppose each member of a set of doc-
uments with similar topical content is annotated
</bodyText>
<figure confidence="0.9848157">
0.90
Accuracy
0.85
0.80
0.75
200
CFGROUPS1000
0.7
0.4
(a) Original data
</figure>
<figureCaption confidence="0.942148">
Figure 8: An illustrative failure case. CSLDA,
lacking a class label prior, prefers to combine label
classes that are highly co-annotated.
</figureCaption>
<bodyText confidence="0.999706416666667">
with both label A and B. In this scenario it is ap-
parent that CSLDA will achieve its best fit by in-
ferring all documents to have the same label either
A or B. By contrast, MOMRESP’s uniform prior
distribution over θ leads it to prefer solutions with
a balance of A and B.
The hypothesis that class combination explains
CSLDA’s performance is supported by Figure 8b,
which shows that CSLDA recovers after com-
bining the classes that were most frequently co-
annotated. We greedily combine label class pairs
to maximize Krippendorf’s α until only 10 la-
bels were left: “alt.atheism,” religion, and poli-
tics classes were combined; also, “sci.electronics”
and the computing classes. The remaining eight
classes were unaltered. However, one could also
argue that the original behavior of CSLDA is in
some ways desirable. That is, if two classes of
documents are mostly the same both topically and
in terms of annotator decisions, perhaps those
classes ought to be collapsed. We are not overly
concerned that MOMRESP beats CSLDA in Fig-
ure 8, since this result is consistent with early rel-
ative performance in simulation.
</bodyText>
<sectionHeader confidence="0.997675" genericHeader="method">
4 Additional Related Work
</sectionHeader>
<bodyText confidence="0.99992116">
This section reviews related work not already dis-
cussed. A growing body of work extends the item-
response model to account for variables such as
item difficulty (Whitehill et al., 2009; Passonneau
and Carpenter, 2013; Zhou et al., 2012), anno-
tator trustworthiness (Hovy et al., 2013), corre-
lation among various combinations of these vari-
ables (Zhou et al., 2014), and change in annotator
behavior over time (Simpson and Roberts, 2015).
Welinder et al. (2010) carefully model the pro-
cess of annotating objects in images, including
variables for item difficulty, item class, and class-
conditional perception noise. In follow-up work,
Liu et al. (2012) demonstrate that similar levels
of performance can be achieved with the sim-
ple item-response model by using variational in-
ference rather than EM. Alternative inference al-
gorithms have been proposed for crowdsourcing
models (Dalvi et al., 2013; Ghosh et al., 2011;
Karger et al., 2013; Zhang et al., 2014). Some
crowdsourcing work regards labeled data not as an
end in itself, but rather as a means to train clas-
sifiers (Lin et al., 2014). The fact-finding litera-
ture assigns trust scores to assertions made by un-
trusted sources (Pasternack and Roth, 2010).
</bodyText>
<sectionHeader confidence="0.997384" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999827814814815">
We describe CSLDA, a generative, data-aware
crowdsourcing model that addresses important
modeling deficiencies identified in previous work.
In particular, CSLDA handles data in which the
natural document clusters are at odds with the
intended document labels. It also transitions
smoothly from situations in which few annotations
are available to those in which many annotations
are available. Because of the flexible mapping in
CSLDA to class labels, many structural variants
are possible in future work. For example, this
mapping could depend not just on inferred topi-
cal content but also directly on data features (c.f.
Nguyen et al. (2013)) or learned embedded feature
representations.
The large number of parameters in the learned
confusion matrices of crowdsourcing models
present difficulty at scale. This could be addressed
by modeling structure both inside of the annotators
and classes. Redundant annotations give unique
insights into both inter-annotator and inter-class
relationships and could be used to induce anno-
tator or label class hierarchies with parsimonious
representations. Simpson et al. (2013) identify an-
notator clusters using community detection algo-
rithms but do not address annotator hierarchy or
scalable confusion representations.
</bodyText>
<figure confidence="0.99987784">
0 2 4 6
(b) After combining frequently co-annotated label classes
algorithm
csLDA
MomResp
LogResp
ItemResp
Majority
Accuracy
0.95
0.90
0.85
0.80
0.75
CFSIMPLEGROUPS
Accuracy
0.6
0.5
0 2 4 6
algorithm
csLDA
MomResp
LogResp
ItemResp
Majority
</figure>
<page confidence="0.992857">
201
</page>
<bodyText confidence="0.996562625">
Acknowledgments This work was supported by
the collaborative NSF Grant IIS-1409739 (BYU)
and IIS-1409287 (UMD). Boyd-Graber is also
supported by NSF grants IIS-1320538 and NCSE-
1422492. Any opinions, findings, conclusions, or
recommendations expressed here are those of the
authors and do not necessarily reflect the view of
the sponsor.
</bodyText>
<sectionHeader confidence="0.998884" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999721854166667">
Arthur Asuncion, Max Welling, Padhraic Smyth, and
Yee Whye Teh. 2009. On smoothing and inference
for topic models. In Proceedings of Uncertainty in
Artificial Intelligence.
Michael W. Berry, Murray Browne, and Ben Signer.
2001. Topic annotated Enron email data set. Lin-
guistic Data Consortium.
Julian Besag. 1986. On the statistical analysis of dirty
pictures. Journal of the Royal Statistical Society,
48(3):259–302.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
Jonathan Bragg, Mausam, and Daniel S. Weld. 2013.
Crowdsourcing multi-label classification for taxon-
omy creation. In First AAAI Conference on Human
Computation and Crowdsourcing.
Ana Margarida de Jesus Cardoso-Cachopo. 2007. Im-
proving Methods for Single-label Text Categoriza-
tion. Ph.D. thesis, Universidade Tecnica de Lisboa.
Nilesh Dalvi, Anirban Dasgupta, Ravi Kumar, and Vib-
hor Rastogi. 2013. Aggregating crowdsourced bi-
nary ratings. In Proceedings of World Wide Web
Conference.
Alexander P. Dawid and Allan M. Skene. 1979. Max-
imum likelihood estimation of observer error-rates
using the EM algorithm. Applied Statistics, pages
20–28.
Paul Felt, Robbie Haertel, Eric Ringger, and Kevin
Seppi. 2014. MomResp: A Bayesian model for
multi-annotator document labeling. In International
Language Resources and Evaluation.
Paul Felt, Eric Ringger, Kevin Seppi, and Robbie Haer-
tel. 2015. Early gains matter: A case for preferring
generative over discriminative crowdsourcing mod-
els. In Conference of the North American Chapter
of the Association for Computational Linguistics.
Arpita Ghosh, Satyen Kale, and Preston McAfee.
2011. Who moderates the moderators?: crowd-
sourcing abuse detection in user-generated content.
In Proceedings of the 12th ACM conference on Elec-
tronic commerce.
Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,
and Eduard H. Hovy. 2013. Learning whom to trust
with MACE. In Conference of the North American
Chapter of the Association for Computational Lin-
guistics.
Rong Jin and Zoubin Ghahramani. 2002. Learning
with multiple labels. In Proceedings of Advances in
Neural Information Processing Systems, pages 897–
904.
David Jurgens. 2013. Embracing ambiguity: A
comparison of annotation methodologies for crowd-
sourcing word sense labels. In Proceedings of
NAACL-HLT, pages 556–562.
David R. Karger, Sewoong Oh, and Devavrat Shah.
2013. Efficient crowdsourcing for multi-class label-
ing. In ACM SIGMETRICS Performance Evaluation
Review, volume 41, pages 81–92. ACM.
Chuck P. Lam and David G. Stork. 2005. Toward
optimal labeling strategy under multiple unreliable
labelers. In AAAI Spring Symposium: Knowledge
Collection from Volunteer Contributors.
Abby Levenberg, Stephen Pulman, Karo Moilanen,
Edwin Simpson, and Stephen Roberts. 2014.
Predicting economic indicators from web text us-
ing sentiment composition. International Jour-
nal of Computer and Communication Engineering,
3(2):109–115.
Christopher H. Lin, Mausam, and Daniel S. Weld.
2014. To re (label), or not to re (label). In Sec-
ond AAAI Conference on Human Computation and
Crowdsourcing.
Qiang Liu, Jian Peng, and Alex T. Ihler. 2012. Varia-
tional inference for crowdsourcing. In Proceedings
of Advances in Neural Information Processing Sys-
tems.
Jon D. Mcauliffe and David Blei. 2007. Supervised
topic models. In Proceedings ofAdvances in Neural
Information Processing Systems.
Andrew McCallum. 2002. Mallet: A machine learning
for language toolkit. http://mallet.cs.umass.edu.
Thomas Minka. 2000. Estimating a Dirichlet distribu-
tion.
Andrew Y. Ng and Michael I. Jordan. 2001. On dis-
criminative vs. generative classifiers: A comparison
of logistic regression and Naive Bayes. Proceedings
of Advances in Neural Information Processing Sys-
tems.
Viet-An Nguyen, Jordan L. Boyd-Graber, and Philip
Resnik. 2013. Lexical and hierarchical topic regres-
sion. In Proceedings of Advances in Neural Infor-
mation Processing Systems.
Kamal Nigam, Andrew McCallum, and Tom Mitchell.
2006. Semi-supervised text classification using EM.
Semi-Supervised Learning, pages 33–56.
</reference>
<page confidence="0.97204">
202
</page>
<reference confidence="0.999500709090909">
Rebecca J. Passonneau and Bob Carpenter. 2013. The
benefits of a model of annotation. In Proceedings of
the 7th Linguistic Annotation Workshop and Inter-
operability with Discourse, pages 187–195.
Jeff Pasternack and Dan Roth. 2010. Knowing what
to believe (when you already know something). In
Proceedings of International Conference on Compu-
tational Linguistics.
Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Ger-
ardo Hermosillo Valadez, Charles Florin, Luca Bo-
goni, and Linda Moy. 2010. Learning from crowds.
Journal of Machine Learning Research, 11:1297–
1322.
Edwin Simpson and Stephen Roberts. 2015. Bayesian
methods for intelligent task assignment in crowd-
sourcing systems. In Decision Making: Uncer-
tainty, Imperfection, Deliberation and Scalability,
pages 1–32. Springer.
E. Simpson, S. Roberts, I. Psorakis, and A. Smith.
2013. Dynamic bayesian combination of multiple
imperfect classifiers. In Decision Making and Im-
perfection, pages 1–35. Springer.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast—but is it
good?: Evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of EMNLP. ACL.
James Surowiecki. 2005. The Wisdom of Crowds.
Random House LLC.
Peter Welinder, Steve Branson, Pietro Perona, and
Serge J. Belongie. 2010. The multidimensional wis-
dom of crowds. In NIPS, pages 2424–2432.
Jacob Whitehill, Ting-fan Wu, Jacob Bergsma, Javier R
Movellan, and Paul L. Ruvolo. 2009. Whose
vote should count more: Optimal integration of la-
bels from labelers of unknown expertise. NIPS,
22:2035–2043.
Yan Yan, R´omer Rosales, Glenn Fung, Ramanathan
Subramanian, and Jennifer Dy. 2014. Learn-
ing from multiple annotators with varying expertise.
Machine Learning, 95(3):291–327.
Yuchen Zhang, Xi Chen, Dengyong Zhou, and
Michael I. Jordan. 2014. Spectral methods meet
em: A provably optimal algorithm for crowd-
sourcing. In Advances in Neural Information Pro-
cessing Systems 27, pages 1260–1268. Curran As-
sociates, Inc.
Dengyong Zhou, Sumit Basu, Yi Mao, and John C.
Platt. 2012. Learning from the wisdom of crowds
by minimax entropy. In NIPS, volume 25, pages
2204–2212.
Dengyong Zhou, Qiang Liu, John Platt, and Christo-
pher Meek. 2014. Aggregating ordinal labels from
crowds by minimax conditional entropy. In Pro-
ceedings of the International Conference of Machine
Learning.
</reference>
<page confidence="0.999197">
203
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9980415">Making the Most of Crowdsourced Document Confused Supervised LDA</title>
<author confidence="0.985651">Paul</author>
<affiliation confidence="0.986714">Dept. of Computer</affiliation>
<author confidence="0.671105">Brigham Young</author>
<email confidence="0.999755">paul.lewis.felt@gmail.com</email>
<author confidence="0.987763">Jordan</author>
<affiliation confidence="0.9999265">Dept. of Computer University of Colorado</affiliation>
<email confidence="0.988255">Jordan.Boyd.Graber@colorado.edu</email>
<author confidence="0.999891">Eric K Ringger</author>
<affiliation confidence="0.997613">Dept. of Computer Science Brigham Young University</affiliation>
<email confidence="0.998674">ringger@cs.byu.edu</email>
<author confidence="0.954076">Kevin</author>
<affiliation confidence="0.996222">Dept. of Computer</affiliation>
<author confidence="0.860177">Brigham Young</author>
<email confidence="0.999874">kseppi@byu.edu</email>
<abstract confidence="0.99939244">Corpus labeling projects frequently use low-cost workers from microtask marketplaces; however, these workers are often inexperienced or have misaligned incentives. Crowdsourcing models must be robust to the resulting systematic and nonsystematic inaccuracies. We introduce a novel crowdsourcing model that adapts the discrete supervised topic model sLDA to handle multiple corrupt, usually conflicting (hence “confused”) supervision signals. Our model achieves significant gains over previous work in the accuracy of deduced ground truth. 1 Modeling Annotators and Abilities Supervised machine learning requires labeled training corpora, historically produced by laborious and costly annotation projects. Microtask markets such as Amazon’s Mechanical Turk and Crowdflower have turned crowd labor into a commodity that can be purchased with relatively little overhead. However, crowdsourced judgments can suffer from high error rates. A common solution to this problem is to obtain multiple redundant judgments, or on the observation that, in aggregate, non-experts often rival or exceed experts by averaging over individual error patterns (Surowiecki, 2005; Snow et al., 2008; Jurgens, 2013). model the wisdom of the crowd and infers labels based on the evidence of the available annotations, imperfect this paper, we call human judgments them from gold standard class though they be. A common baseline crowdmethod aggregates annotations by mabut this approach ignores important information. For example, some annotators are more reliable than others and their judgments ought to be upweighted accordingly. State-ofthe-art crowdsourcing methods account for annotator expertise, often through a probabilistic formalism. Compounding the challenge, assessing unobserved annotator expertise is tangled with estimating ground truth from imperfect annotations; thus, joint inference of these interrelated quantities is necessary. State-of-the-art models also take the data into account, because data features can help ratify or veto human annotators. We introduce a model that improves on state of the art crowdsourcing algorithms by modeling not only the annotations but also the features of the data (e.g., words in a document). Section 2 identifies modeling deficiencies affecting previous work and proposes a solution based on topic modeling; Section 2.4 presents inference for the new model. Experiments that contrast the proposed model with select previous work on several text classification datasets are presented in Section 3. In Section 4 we highlight additional related work. 2 Latent Representations that Reflect Labels and Confusion Most crowdsourcing models extend the itemresponse model of Dawid and Skene (1979). The Bayesian version of this model, referred to here as is depicted in Figure 1. In the generative story for this model, a confusion matrix drawn for each human annotator Each the confusion matrix drawn from 194 of the 19th Conference on Computational Language pages China, July 30-31, 2015. Association for Computational Linguistics 1: a plate diagram. Round nodes are random variables. Rectangular nodes are free parameters. Shaded nodes are observed. J, C the number of documents, annotators, and classes, respectively. symmetric Dirichlet distribution encodes a categorical probability distribution over classes that annotator apt to choose when with a document whose true label is for each document unobserved doculabel drawn. Annotations are generated annotator the true label the categorical distribution 2.1 Leveraging Data extensions to the features of the data (e.g., words in a document). Many data-aware crowdsourcing models condition the labels on the data (Jin and Ghahramani, 2002; Raykar et al., 2010; Liu et al., 2012; Yan et al., 2014), possibly because discriminative classifiers dominate supervised machine learning. Others model the data generatively (Bragg et al., 2013; Lam and Stork, 2005; Felt et al., 2014; Simpson and Roberts, 2015). Felt et al. (2015) argue that generative models are better suited than conditional models to crowdsourcing scenarios because generative models often learn faster than their conditional counterparts (Ng and Jordan, 2001)— especially early in the learning curve. This advantage is amplified by the annotation noise typical of crowdsourcing scenarios. to model document features generatively tend to share a common high-level architecture. After the document class drawn for each document features are drawn from class-conditional distributions. Felt et (2015) identify the reproduced in Figure 2, as a strong representative of crowdsourcing models. In 2: a plate diagram. the size of the vocabulary. Documents with feature vectors to share a common Reduces to mixture-of-multinomials cluswhen no annotations observed. feature vector document drawn from the multinomial distribution with parameter vector This class-conditional multinomial model of the data inherits many of the strengths and weaknesses of the naive Bayes model that it resembles. Strengths include easy inference and a strong inductive bias which helps the model be robust to annotation noise and scarcity. Weaknesses include overly strict conditional independence assumptions among features, leading to overconfidence in the document model and thereby causing the model to overweight feature evidence and underweight annotation evidence. This imbalance can result in degraded performance in the presence of high quality or many annotations. Confused Supervised LDA We solve the problem of imbalanced feature and evidence observed in replacing the class-conditional structure of previous generative crowdsourcing models with a richer generative story where documents are drawn first class labels obtained afterwards via a log-linear mapping. This move towards conditioning classes on documents content is sensible because in many situations document content is authored first, whereas label structure is not imposed until afterwards. It is plausible to assume that there will exist some mapping from a latent document structure to the desired document label distinctions. Moreover, by jointly modeling topics and the mapping to labels, we can learn the latent document representations that best explain how best to predict and correct annotator errors.</abstract>
<note confidence="0.956004894736842">1 C 1 J 1 J 1 D θ θ 1 J 1 D 1 C 1 . . J 1 C 195 1 T 1 J 1 D 1 0 0 1 J E</note>
<title confidence="0.895215875">µ Term Definition of document P P P P P</title>
<abstract confidence="0.991316252707582">Count excludes variable being sampled where the sampled Table 1: Definition of counts and select notation. the indicator function. call our model supervised LDA Figure 3), based on supervised topic modeling. Latent Dirichlet Allocation (Blei et al., 2003, LDA) models text documents as admixtures of word distributions, or topics. Although pre-calculated LDA topics as features can inform a crowdsourcing model (Levenberg et al., 2014), supervised LDA (sLDA) provides a principled way of incorporating document class labels and topics into a single model, allowing topic variables and response variables to co-inform one another in joint inference. For example, when sLDA is given movie reviews labeled with sentiment, inferred topics cluster around sentimentheavy words (Mcauliffe and Blei, 2007), which may be quite different from the topics inferred by LDA. One way to view is as a discrete sLDA in settings with noisy supervision from multiple, imprecise annotators. generative story for is: Draw per-topic word distributions Draw per-class regression parameters Draw per-annotator confusion matrices row from For each document Draw topic vector For each token position draw topic word Draw class label probability proto For each annotator annotation 3: a plate diagram. J, C, T are the number of documents, annotators, classes, topics, respectively. the size of docuthe size of the vocabulary. is a vector of regression parameters. Reduces to when no annotations observed. 2.3 Stochastic EM We use stochastic expectation maximization (EM) posterior inference in alternating besampling values for topics document labels E-step) and optimizing values regression parameters M-step). To samwe derive the full condidistributions of a collapsed model and been analytically integrated out. Omitting multiplicative constants, the collapsed model joint probability is w, y, ! ! Y t ! ⎛ ⎞ c the Beta function (multivariate as counts related symbols are dein Table 1, and = the multinomial coefficient. Simplifying Equation 1 yields full conditionals each word � � w, y, t d d · · 196 similarly for document label 1) · · the rising factorial. In Equation 2 the first and third terms independent of word can be cached at the document level for efficiency. For the M-step, we train the regression paramone vector per class) by optimizing the same objective function as for training a logistic regression classifier, assuming that class given: We optimize the objective (Equation 4) using Land a regularizing Gaussian prior with EM is sensitive to initialization, is straightforward to initialize. Majority vote is to set initial Corresponding initial for obtained by clamping running stochastic EM on 2.4 Hyperparameter Optimization we would test performance under all of the many algorithms available for inference in such a model. Although that is not feasible, Asuncion et al. (2009) demonstrate that hyperaparameter optimization in LDA topic models helps to bring the performance of alternative inference algorithms into approximate agreement. Accordingly, in Section 2.4 we implement hyperparameoptimization for to make our results as general as possible. Before moving on, however, we take a moment to validate that the observation of Asuncion et al. from LDA to the together with LDA, comprises 4 demonstrates that three inference algorithms, Gibbs sampling (Gibbs), meanfield variational inference (Var), and the iterated conditional modes algorithm (ICM) (Besag, 1986), are brought into better agreement after optimizing their hyperparameters via grid search. That NEWSGROUPS 0.8 algorithm Gibbs Var ICM 0.2 of annotated instancs x 1,000 fixed NEWSGROUPS 0.8 algorithm Gibbs Var ICM 0.2 (b) Hyperparameters optimized via grid search on validation Number of annotated instances x 1000 data Figure 4: Differences among the inferred label aclearning curves of three inference algorithms are reduced when hyperparameters are optimized. is, the algorithms in Figure 4b are in better agreement, particularly near the extremes, than the algorithms in Figure 4a. This difference is subtle, but it holds to an equal and greater extent in other simulation conditions we tested (experiment details are similar to those reported in Section 3). Fixed-point Hyperparameter Updates Although a grid search is effective, it is not practical for a model with many hyperparameters such For efficiency, therefore, we use the fixed-point updates of Minka (2000). Our updates differ slightly from Minka’s since we tie hyperparameters, allowing them to be learned more quickly from less data. In our implementation the of hyperparameters the Dirichlet-multinomial distributions are completely such that v This leads to and updates for slightly more involved we choose to tie the diagonal entries the off-diagonal entries updating each separately: ) Y j Y ( = d w, y, a, c0 exp(ηTc0 ¯zd) (3) Accuracy 0.6 0.4 Accuracy 0.6 0.4 197 E where � = j,c (C o )] o , WEATHER 0 5 10 15 Number of annotations x 1,000 algorithm csLDA MomResp LogResp ItemResp Majority 1.0 0.9 Accuracy 0.8 0.7 0.6 (8) (C o As in the work of Asuncion et al. (2009), we add algorithmic gamma prior for by adding the numerator and to the denominator of Equations 5-8. Note that these algorithmic gamma “priors” should not be as first-class members of the model (Figure 3). Rather, they are regularization terms that keep our hyperparameter search algorithm from straying towards problematic values as 0 or 3 Experiments all experiments we set number of 1.5 times the number of classes in each dataset. We found that model performance was reasonably robust to this parameter. Only when below the number of label classes does suffer. As per Section 2.3, values are initialized with 500 rounds of stochastic EM, after which the full model is updated with 1000 additional rounds. Predictions are generated by aggregating samples from the last 100 rounds (the mode of the approximate marginal posterior). compare with (1) a majority vote (2) the and representatives of the two main classes of dataaware crowdsourcing models, namely (3) dataand (4) data-conditional. represents a typical data-generative model (Bragg et al., 2013; Felt et al., 2014; Lam and Stork, 2005; Simpson and Roberts, 2015). Data-conditional approaches typically model data features conditionally using a log-linear model (Jin and Ghahramani, 2002; Raykar et al., 2010; Liu et al., 2012; Yan et al., 2014). For the purposes of this paper, we reto this model as For and use the variational inference methods presented by Felt et al. (2015). Unlike that paper, in this work we have augmented inference with the in-line hyperparameter updates described in Section 2.4. Figure 5: Inferred label accuracy of models on sentiment-annotated weather tweets. 3.1 Human-generated Annotations To gauge the effectiveness of data-aware crowdsourcing models, we use the sentiment-annotated tweet dataset distributed by CrowdFlower as a of its “data for everyone” the “Weather Sentiment” task, 20 annotators judged the sentiment of 1000 tweets as either positive, negative, neutral, or unrelated to the weather. In the secondary “Weather Sentiment Evaluated” task, 10 additional annotators judged the correctness of each consensus label. We construct a gold standard from the consensus labels that were judged to be correct by 9 of the 10 annotators in the secondary task. Figure 5 plots learning curves of the accuracy of model-inferred labels as annotations are added (ordered by timestamp). All methods, including majority vote, converge to roughly the same accuracy when all 20,000 annotations are added. When fewer annotations are available, statistical modbeat majority vote, and is considerably more accurate than other approaches. Learning curves are bumpy because annotation order is not random and because inferred label accuracy is calculated only over documents with at least one annotation. Learning curves collectively increase when average annotation depth (the number of annotations per item) increases and decrease when new documents are annotated and average annodepth decreases. stands out by being more robust to these changes than other algorithms, and also by maintaining a higher level of accuracy across the board. This is important because high accuracy using fewer annotations translates to decreased annotations costs.</abstract>
<date confidence="0.448908">198</date>
<affiliation confidence="0.640142">D C V</affiliation>
<address confidence="0.901605">20 News 16,995 20 22,851 111</address>
<phone confidence="0.5270898">WebKB 3,543 4 5,781 131 Reuters8 6,523 8 6,776 53 Reuters52 7,735 52 5,579 58 CADE12 34,801 12 41,628 110 Enron 3,854 32 14,069 431</phone>
<abstract confidence="0.83547022683706">2: Dataset statistics. number of docnumber of classes, number of and average document size. Values are calculated after setting aside 15% as validation data and doing feature selection. 3.2 Synthetic Annotations Datasets including both annotations and gold standard labels are in short supply. Although plenty of text categorization datasets have been annotated, common practice reflects that initial noisy annotations be discarded and only consensus labels be published. Consequently, we follow previous work in achieving broad validation by constructing synthetic annotators that corrupt known gold standard labels. We base our experimental setup on the annotations gathered by Felt et paid CrowdFlower annotators to relabel 1000 documents from the well-known 20 Newsgroups classification dataset. In that experiment, 136 annotators contributed, each instance was labeled an average of 6.9 times, and annotator accuracies were distributed approximately to a 5.1) Accordingly we construct 100 synthetic annotators, each parametrized by an accuracy drawn from 5.1) with errors drawn from a sym- Dirichlet Datasets are annotated by selecting an instance (at random without reand then selecting (at random without replacement) to annotate it before on. We choose = 7 mirror the empirical average in the CrowdFlower annotation set. We evaluate on six text classification datasets, summarized in Table 2. The 20 Newsgroups, WebKB, Cade12, Reuters8, and Reuters52 datasets are described in more detail by Cardoso-Cachopo (2007). The LDC-labeled Enron emails dataset is described by Berry et al. (2001). Each dataset is dataset is available via git at byu.edu/plf1/crowdflower-newsgroups.git NEWSGROUPS 0 10 20 CADE12 algorithm csLDA MomResp LogResp ItemResp Majority 0 50 100 150 200 Number of annotations x 1,000 Figure 6: Inferred label accuracy of models on synthetic annotations. The first instance is annotated 7 times, then the second, and so on. preprocessed via Porter stemming and by removal of the stopwords from MALLET’s stopword list (McCallum, 2002). Features occurring fewer than 5 times in the corpus are discarded. In the case features are fractionally scaled so that each document is the same length, in keeping with previous work in multinomial document models (Nigam et al., 2006). Figure 6 plots learning curves on three representative datasets (Enron resembles Cade12, and Reuters datasets resemble WebKB). outperforms and majority vote. The generative and tend to excel in lowannotation portions of the learning curve, partially because generative models tend to converge quickly and partially because generative models naturally learn from unlabeled documents (i.e., However, to quickly reach a performance plateau after which additional annotations do little good. The perforof also highly dataset de- WEBKB algorithm csLDA MomResp LogResp ItemResp Majority Accuracy 0.95 0.90 0.85 0.80 1.00 0 25 50 75 100 125 algorithm csLDA MomResp LogResp ItemResp Majority Accuracy 0.9 0.8 0.7 0.6 Accuracy 0.9 0.8 0.7 0.6 199 95% Accuracy Majority 20 News 85 (5.0x) 150 (8.8x) 152 (8.9x) 168 (9.9x) 233 (13.7x) WebKB 31 (8.8x) - 46 (13.0x) 46 (13.0x) - Reuters8 25 (3.8x) - 73 (11.2x) 62 (9.5x) - Reuters52 33 (4.3x) 73 (9.4x) 67.5 (8.7x) 60 (7.8x) 87 (11.2x) CADE12 250 (7.2x) - 295 (8.5x) 290 (8.3x) 570 (16.4x) Enron 31 (8.0x) - 40 (10.4x) 38 (9.9x) 47 (12.2x) 3: The number of annotations which the algorithm reaches 95% inferred label accuracy on the indicated dataset (average annotations per instance are in parenthesis). All instances are annotated once, then twice, and so on. Empty entries (’-’) do not reach 95% even with 20 annotations per instance. pendent: it is good on 20 Newsgroups, mediocre on WebKB, and poor on CADE12. By contrast, is relatively stable across datasets. To understand the different behavior of the two models, recall that idento for its multinomial data model. Indeed, the equations governing inferof label sum together from an and terms from a mixture of multinomials clustering model (and for reasons explained in Section 2.1, the multinomial data model terms tend to dominate). Therewhen from is because attracted toward a assignment that satisfies the multinomial data model, grouping similar documents together. This can both help and hurt. When data clusters and laclasses are misaligned, (as in the case of the Cade12 dataset). In contrast, flexible mapping from topics to labels is less sensitive: topics can diverge from label classes so long as there exists some linear transformation from the topics to the labels. Many corpus annotation projects are not complete until the corpus achieves some target level of quality. We repeat the experiment reported in Figure 6, but rather than simulating seven annotations for each instance before moving on, we simulate one annotation for each instance, then two, and so on until each instance in the dataset is annotated 20 times. Table 3 reports the minimal number of annotations before an algorithm’s inferred labels reach an accuracy of 95%, a lofty goal that can require significant amounts of annotation when uspoor quality annotations. achieves 95% accuracy with fewer annotations, corresponding to reduced annotation cost. NEWSGROUPS algorithm csLDA csLDA−P 0.70 20 30 40 50 60 Number of annotations x 1,000 7: Joint inference for vs pipeline 3.3 Joint vs Pipeline Inference To isolate the effectiveness of joint inference in we compare against the pipeline alternative where topics are inferred first and then held constant during inference (Levenberg et al., 2014). Joint inference yields modest but consistent benefits over a pipeline approach. Figure 7 highlights a portion of the learning curve on the Newsgroups dataset (based on the experiments summarized in Table 3). This trend holds across all of the datasets that we examined. 3.4 Error Analysis models like feature that data-conditional models like lack: an explicit prior over class prevalence. Fig- 8a shows that performs poorly on the CrowdFlower-annotated Newsgroups documents described at the beginning of Section 3 (not the synthetic annotations). Error analysis uncovers lumps related classes together in this dataset. This is because annotators could specify up to 3 simultaneous labels for each annotation, so that similar labels (e.g., “talk.politics.misc” and “talk.politics.mideast”) are usually chosen in blocks. Suppose each member of a set of documents with similar topical content is annotated 0.90 Accuracy 0.85 0.80 0.75 200 CFGROUPS1000 0.7 0.4 (a) Original data 8: An illustrative failure case. lacking a class label prior, prefers to combine label classes that are highly co-annotated. with both label A and B. In this scenario it is apthat will achieve its best fit by inferring all documents to have the same label either or B. By contrast, uniform prior over it to prefer solutions with a balance of A and B. The hypothesis that class combination explains performance is supported by Figure 8b, shows that recovers after combining the classes that were most frequently coannotated. We greedily combine label class pairs maximize Krippendorf’s only 10 labels were left: “alt.atheism,” religion, and politics classes were combined; also, “sci.electronics” and the computing classes. The remaining eight classes were unaltered. However, one could also that the original behavior of is in some ways desirable. That is, if two classes of documents are mostly the same both topically and in terms of annotator decisions, perhaps those classes ought to be collapsed. We are not overly that in Figure 8, since this result is consistent with early relative performance in simulation. 4 Additional Related Work This section reviews related work not already discussed. A growing body of work extends the itemresponse model to account for variables such as item difficulty (Whitehill et al., 2009; Passonneau and Carpenter, 2013; Zhou et al., 2012), annotator trustworthiness (Hovy et al., 2013), correlation among various combinations of these variables (Zhou et al., 2014), and change in annotator behavior over time (Simpson and Roberts, 2015). Welinder et al. (2010) carefully model the process of annotating objects in images, including variables for item difficulty, item class, and classconditional perception noise. In follow-up work, Liu et al. (2012) demonstrate that similar levels of performance can be achieved with the simple item-response model by using variational inference rather than EM. Alternative inference algorithms have been proposed for crowdsourcing models (Dalvi et al., 2013; Ghosh et al., 2011; Karger et al., 2013; Zhang et al., 2014). Some crowdsourcing work regards labeled data not as an end in itself, but rather as a means to train classifiers (Lin et al., 2014). The fact-finding literature assigns trust scores to assertions made by untrusted sources (Pasternack and Roth, 2010). 5 Conclusion and Future Work describe a generative, data-aware crowdsourcing model that addresses important modeling deficiencies identified in previous work. particular, handles data in which the natural document clusters are at odds with the intended document labels. It also transitions smoothly from situations in which few annotations are available to those in which many annotations are available. Because of the flexible mapping in to class labels, many structural variants are possible in future work. For example, this mapping could depend not just on inferred topical content but also directly on data features (c.f. Nguyen et al. (2013)) or learned embedded feature representations. The large number of parameters in the learned confusion matrices of crowdsourcing models present difficulty at scale. This could be addressed by modeling structure both inside of the annotators and classes. Redundant annotations give unique insights into both inter-annotator and inter-class relationships and could be used to induce annotator or label class hierarchies with parsimonious representations. Simpson et al. (2013) identify annotator clusters using community detection algorithms but do not address annotator hierarchy or scalable confusion representations. 0 2 4 6 (b) After combining frequently co-annotated label classes algorithm csLDA MomResp LogResp ItemResp Majority Accuracy 0.95 0.90 0.85 0.80 0.75 CFSIMPLEGROUPS Accuracy 0.6 0.5 0 2 4 6 algorithm csLDA MomResp LogResp ItemResp Majority 201 work was supported by collaborative Boyd-Graber is also by and 1422492. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsor.</abstract>
<note confidence="0.680156642857143">References Arthur Asuncion, Max Welling, Padhraic Smyth, and Yee Whye Teh. 2009. On smoothing and inference topic models. In of Uncertainty in Michael W. Berry, Murray Browne, and Ben Signer. Topic annotated Enron email data set. Lin- Data Julian Besag. 1986. On the statistical analysis of dirty of the Royal Statistical 48(3):259–302. David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent Dirichlet allocation. of Ma- Learning 3:993–1022. Jonathan Bragg, Mausam, and Daniel S. Weld. 2013.</note>
<title confidence="0.836312">Crowdsourcing multi-label classification for taxon-</title>
<author confidence="0.774649">In AAAI Conference on Human</author>
<affiliation confidence="0.566408">and</affiliation>
<note confidence="0.4894016">Margarida de Jesus Cardoso-Cachopo. 2007. Improving Methods for Single-label Text Categoriza- Ph.D. thesis, Universidade Tecnica de Lisboa. Nilesh Dalvi, Anirban Dasgupta, Ravi Kumar, and Vibhor Rastogi. 2013. Aggregating crowdsourced bi-</note>
<title confidence="0.880195">ratings. In of World Wide Web</title>
<author confidence="0.947934">Max-</author>
<abstract confidence="0.972702333333333">imum likelihood estimation of observer error-rates the EM algorithm. pages 20–28.</abstract>
<author confidence="0.749393">MomResp A Bayesian model for document labeling In</author>
<title confidence="0.861372">Resources and</title>
<author confidence="0.972533">Paul Felt</author>
<author confidence="0.972533">Eric Ringger</author>
<author confidence="0.972533">Kevin Seppi</author>
<author confidence="0.972533">Robbie Haer-</author>
<abstract confidence="0.901080142857143">tel. 2015. Early gains matter: A case for preferring generative over discriminative crowdsourcing mod- In of the North American Chapter the Association for Computational Arpita Ghosh, Satyen Kale, and Preston McAfee. 2011. Who moderates the moderators?: crowdsourcing abuse detection in user-generated content.</abstract>
<title confidence="0.473943">of the 12th ACM conference on Elec-</title>
<author confidence="0.848762666666667">Learning whom to trust MACE In of the North American</author>
<affiliation confidence="0.97886">Chapter of the Association for Computational Lin-</affiliation>
<address confidence="0.425521">Rong Jin and Zoubin Ghahramani. 2002. Learning</address>
<abstract confidence="0.611891380952381">multiple labels. In of Advances in Information Processing pages 897– 904. David Jurgens. 2013. Embracing ambiguity: A comparison of annotation methodologies for crowdword sense labels. In of pages 556–562. David R. Karger, Sewoong Oh, and Devavrat Shah. 2013. Efficient crowdsourcing for multi-class label- In SIGMETRICS Performance Evaluation volume 41, pages 81–92. ACM. Chuck P. Lam and David G. Stork. 2005. Toward optimal labeling strategy under multiple unreliable In Spring Symposium: Knowledge from Volunteer Abby Levenberg, Stephen Pulman, Karo Moilanen, Edwin Simpson, and Stephen Roberts. 2014. Predicting economic indicators from web text ussentiment composition. Jourof Computer and Communication 3(2):109–115.</abstract>
<author confidence="0.578454">Christopher H Lin</author>
<author confidence="0.578454">Mausam</author>
<author confidence="0.578454">Daniel S Weld</author>
<note confidence="0.87945675">To re (label), or not to re (label). In Second AAAI Conference on Human Computation and Qiang Liu, Jian Peng, and Alex T. Ihler. 2012. Variainference for crowdsourcing. In</note>
<title confidence="0.807115">of Advances in Neural Information Processing Sys-</title>
<author confidence="0.741693">Supervised</author>
<title confidence="0.6690095">models. In ofAdvances in Neural Processing</title>
<author confidence="0.445486">Mallet A machine learning</author>
<abstract confidence="0.964754166666667">for language toolkit. http://mallet.cs.umass.edu. Thomas Minka. 2000. Estimating a Dirichlet distribution. Andrew Y. Ng and Michael I. Jordan. 2001. On discriminative vs. generative classifiers: A comparison logistic regression and Naive Bayes.</abstract>
<note confidence="0.87999775">of Advances in Neural Information Processing Sys- Viet-An Nguyen, Jordan L. Boyd-Graber, and Philip Resnik. 2013. Lexical and hierarchical topic regres- In of Advances in Neural Infor- Processing Kamal Nigam, Andrew McCallum, and Tom Mitchell. 2006. Semi-supervised text classification using EM. pages 33–56. 202 Rebecca J. Passonneau and Bob Carpenter. 2013. The of a model of annotation. In of the 7th Linguistic Annotation Workshop and Interwith pages 187–195. Jeff Pasternack and Dan Roth. 2010. Knowing what to believe (when you already know something). In Proceedings of International Conference on Compu-</note>
<author confidence="0.42674">Vikas C Raykar</author>
<author confidence="0.42674">Shipeng Yu</author>
<author confidence="0.42674">Linda H Zhao</author>
<author confidence="0.42674">Gerardo Hermosillo Valadez</author>
<author confidence="0.42674">Charles Florin</author>
<author confidence="0.42674">Luca Bo-</author>
<abstract confidence="0.85211275">goni, and Linda Moy. 2010. Learning from crowds. of Machine Learning 11:1297– 1322. Edwin Simpson and Stephen Roberts. 2015. Bayesian methods for intelligent task assignment in crowdsystems. In Making: Uncer- Imperfection, Deliberation and pages 1–32. Springer. E. Simpson, S. Roberts, I. Psorakis, and A. Smith. 2013. Dynamic bayesian combination of multiple classifiers. In Making and Impages 1–35. Springer. Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast—but is it good?: Evaluating non-expert annotations for natulanguage tasks. In of ACL.</abstract>
<note confidence="0.7910805">Surowiecki. 2005. Wisdom of Random House LLC. Peter Welinder, Steve Branson, Pietro Perona, and Serge J. Belongie. 2010. The multidimensional wisof crowds. In pages 2424–2432. Jacob Whitehill, Ting-fan Wu, Jacob Bergsma, Javier R Movellan, and Paul L. Ruvolo. 2009. Whose vote should count more: Optimal integration of lafrom labelers of unknown expertise. 22:2035–2043. Yan Yan, R´omer Rosales, Glenn Fung, Ramanathan Subramanian, and Jennifer Dy. 2014. Learning from multiple annotators with varying expertise. 95(3):291–327. Yuchen Zhang, Xi Chen, Dengyong Zhou, and Michael I. Jordan. 2014. Spectral methods meet em: A provably optimal algorithm for crowd- In in Neural Information Pro- Systems pages 1260–1268. Curran Associates, Inc. Dengyong Zhou, Sumit Basu, Yi Mao, and John C. Platt. 2012. Learning from the wisdom of crowds minimax entropy. In volume 25, pages 2204–2212. Dengyong Zhou, Qiang Liu, John Platt, and Christopher Meek. 2014. Aggregating ordinal labels from</note>
<author confidence="0.61678">In Pro-</author>
<affiliation confidence="0.731047">ceedings of the International Conference of Machine</affiliation>
<address confidence="0.675685">203</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Arthur Asuncion</author>
<author>Max Welling</author>
<author>Padhraic Smyth</author>
<author>Yee Whye Teh</author>
</authors>
<title>On smoothing and inference for topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="11729" citStr="Asuncion et al. (2009)" startWordPosition="1939" endWordPosition="1942">raining a logistic regression classifier, assuming that class y is given: We optimize the objective (Equation 4) using LBFGS and a regularizing Gaussian prior with µ = 0, σ2 = 1. While EM is sensitive to initialization, CSLDA is straightforward to initialize. Majority vote is used to set initial y values ˜y. Corresponding initial values for z and η are obtained by clamping y to y˜ and running stochastic EM on z and η. 2.4 Hyperparameter Optimization Ideally, we would test CSLDA performance under all of the many algorithms available for inference in such a model. Although that is not feasible, Asuncion et al. (2009) demonstrate that hyperaparameter optimization in LDA topic models helps to bring the performance of alternative inference algorithms into approximate agreement. Accordingly, in Section 2.4 we implement hyperparameter optimization for CSLDA to make our results as general as possible. Before moving on, however, we take a moment to validate that the observation of Asuncion et al. generalizes from LDA to the ITEMRESP model, which, together with LDA, comprises CSLDA. Figure 4 demonstrates that three ITEMRESP inference algorithms, Gibbs sampling (Gibbs), meanfield variational inference (Var), and t</context>
<context position="14606" citStr="Asuncion et al. (2009)" startWordPosition="2409" endWordPosition="2412">tely the off-diagonal entries b(γ) o , updating each separately: (7) (γ) Pj,c[Ψ(Njcc+bdγ))]−JCΨ(b(dγ)) 7 bd Abd ·Z(b(γ)) ( ) Y j Y exp(ηTc ¯zd) (4) p(y = c|z, η) = ( d Pc0 exp zd) exp(ηT c ¯zd) p(yd = c|z, w, y, a, η) ∝ P c0 exp(ηTc0 ¯zd) (3) Accuracy 0.6 0.4 Accuracy 0.6 0.4 197 b(γ) o· E j,c,c,7�c [T (Njcc,+b(γ)o)]−JC(C−1)T (b(γ))] (C−1)Z(b(γ)) where � Z(b(γ)) = j,c [T(Njc + b(γ) d + (C − 1)b(γ) o )] and b(γ) o , WEATHER 0 5 10 15 Number of annotations x 1,000 algorithm csLDA MomResp LogResp ItemResp Majority 1.0 0.9 Accuracy 0.8 0.7 0.6 (8) − JCT(b(γ) d + (C − 1)b(γ) o ). As in the work of Asuncion et al. (2009), we add an algorithmic gamma prior (b(·) — G(α, Q)) for smoothing by adding α−1 b(·)to the numerator and Q to the denominator of Equations 5-8. Note that these algorithmic gamma “priors” should not be understood as first-class members of the CSLDA model (Figure 3). Rather, they are regularization terms that keep our hyperparameter search algorithm from straying towards problematic values such as 0 or oc. 3 Experiments For all experiments we set CSLDA’s number of topics T to 1.5 times the number of classes in each dataset. We found that model performance was reasonably robust to this parameter</context>
</contexts>
<marker>Asuncion, Welling, Smyth, Teh, 2009</marker>
<rawString>Arthur Asuncion, Max Welling, Padhraic Smyth, and Yee Whye Teh. 2009. On smoothing and inference for topic models. In Proceedings of Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael W Berry</author>
<author>Murray Browne</author>
<author>Ben Signer</author>
</authors>
<title>Topic annotated Enron email data set. Linguistic Data Consortium.</title>
<date>2001</date>
<contexts>
<context position="20089" citStr="Berry et al. (2001)" startWordPosition="3279" endWordPosition="3282">accuracy drawn from Beta(3.6, 5.1) and with errors drawn from a symmetric Dirichlet Dir(1). Datasets are annotated by selecting an instance (at random without replacement) and then selecting K annotators (at random without replacement) to annotate it before moving on. We choose K = 7 to mirror the empirical average in the CrowdFlower annotation set. We evaluate on six text classification datasets, summarized in Table 2. The 20 Newsgroups, WebKB, Cade12, Reuters8, and Reuters52 datasets are described in more detail by Cardoso-Cachopo (2007). The LDC-labeled Enron emails dataset is described by Berry et al. (2001). Each dataset is 3The dataset is available via git at git://nlp.cs. byu.edu/plf1/crowdflower-newsgroups.git NEWSGROUPS 0 10 20 nnotat CADE12 algorithm csLDA MomResp LogResp ItemResp Majority 0 50 100 150 200 Number of annotations x 1,000 Figure 6: Inferred label accuracy of models on synthetic annotations. The first instance is annotated 7 times, then the second, and so on. preprocessed via Porter stemming and by removal of the stopwords from MALLET’s stopword list (McCallum, 2002). Features occurring fewer than 5 times in the corpus are discarded. In the case of MOMRESP, features are fractio</context>
</contexts>
<marker>Berry, Browne, Signer, 2001</marker>
<rawString>Michael W. Berry, Murray Browne, and Ben Signer. 2001. Topic annotated Enron email data set. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Besag</author>
</authors>
<title>On the statistical analysis of dirty pictures.</title>
<date>1986</date>
<journal>Journal of the Royal Statistical Society,</journal>
<volume>48</volume>
<issue>3</issue>
<contexts>
<context position="12388" citStr="Besag, 1986" startWordPosition="2039" endWordPosition="2040">in LDA topic models helps to bring the performance of alternative inference algorithms into approximate agreement. Accordingly, in Section 2.4 we implement hyperparameter optimization for CSLDA to make our results as general as possible. Before moving on, however, we take a moment to validate that the observation of Asuncion et al. generalizes from LDA to the ITEMRESP model, which, together with LDA, comprises CSLDA. Figure 4 demonstrates that three ITEMRESP inference algorithms, Gibbs sampling (Gibbs), meanfield variational inference (Var), and the iterated conditional modes algorithm (ICM) (Besag, 1986), are brought into better agreement after optimizing their hyperparameters via grid search. That NEWSGROUPS 0.8 algorithm Gibbs Var ICM 0.2 of annotated instancs x 1,000 (a) Hyperparameters fixed NEWSGROUPS 0.8 algorithm Gibbs Var ICM 0.2 (b) Hyperparameters optimized via grid search on validation Number of annotated instances x 1000 data Figure 4: Differences among the inferred label accuracy learning curves of three ITEMRESP inference algorithms are reduced when hyperparameters are optimized. is, the algorithms in Figure 4b are in better agreement, particularly near the extremes, than the al</context>
</contexts>
<marker>Besag, 1986</marker>
<rawString>Julian Besag. 1986. On the statistical analysis of dirty pictures. Journal of the Royal Statistical Society, 48(3):259–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="8130" citStr="Blei et al., 2003" startWordPosition="1307" endWordPosition="1310"> ... Nd wdn zdn θd yd adj j = 1 ... J d = 1 ... D c = 1 ... 0 c = 1 ... 0 j = 1 ... J ηc b(γ) γjc E µ Term Definition Size of document d P n ✶(zdn = t) P d,n ✶(zdn = t) P d adjc, ✶(yd = c) (Njc1 ··· NjcC) P dn ✶(wdn = v ∧ zdn = t) P dn ✶(zdn = t) Count excludes variable being sampled Vector where ¯zdt = Nd &amp; ✶ (zdn = t) Excludes the zdn being sampled Nd Ndt Nt Njcc, Njc Nvt Nt Nˆ zd ˆ¯zd Table 1: Definition of counts and select notation. ✶(·) is the indicator function. We call our model confused supervised LDA (CSLDA, Figure 3), based on supervised topic modeling. Latent Dirichlet Allocation (Blei et al., 2003, LDA) models text documents as admixtures of word distributions, or topics. Although pre-calculated LDA topics as features can inform a crowdsourcing model (Levenberg et al., 2014), supervised LDA (sLDA) provides a principled way of incorporating document class labels and topics into a single model, allowing topic variables and response variables to co-inform one another in joint inference. For example, when sLDA is given movie reviews labeled with sentiment, inferred topics cluster around sentimentheavy words (Mcauliffe and Blei, 2007), which may be quite different from the topics inferred b</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Bragg</author>
<author>Mausam</author>
<author>Daniel S Weld</author>
</authors>
<title>Crowdsourcing multi-label classification for taxonomy creation.</title>
<date>2013</date>
<booktitle>In First AAAI Conference on Human Computation and Crowdsourcing.</booktitle>
<contexts>
<context position="4713" citStr="Bragg et al., 2013" startWordPosition="692" endWordPosition="695">ocument whose true label is c. Then for each document d an unobserved document label yd is drawn. Annotations are generated as annotator j corrupts the true label yd according to the categorical distribution Cat(γjyd). 2.1 Leveraging Data Some extensions to ITEMRESP model the features of the data (e.g., words in a document). Many data-aware crowdsourcing models condition the labels on the data (Jin and Ghahramani, 2002; Raykar et al., 2010; Liu et al., 2012; Yan et al., 2014), possibly because discriminative classifiers dominate supervised machine learning. Others model the data generatively (Bragg et al., 2013; Lam and Stork, 2005; Felt et al., 2014; Simpson and Roberts, 2015). Felt et al. (2015) argue that generative models are better suited than conditional models to crowdsourcing scenarios because generative models often learn faster than their conditional counterparts (Ng and Jordan, 2001)— especially early in the learning curve. This advantage is amplified by the annotation noise typical of crowdsourcing scenarios. Extensions to ITEMRESP that model document features generatively tend to share a common high-level architecture. After the document class label yd is drawn for each document d, feat</context>
<context position="15838" citStr="Bragg et al., 2013" startWordPosition="2611" endWordPosition="2614">drops below the number of label classes does performance suffer. As per Section 2.3, z and q values are initialized with 500 rounds of stochastic EM, after which the full model is updated with 1000 additional rounds. Predictions are generated by aggregating samples from the last 100 rounds (the mode of the approximate marginal posterior). We compare CSLDA with (1) a majority vote baseline, (2) the ITEMRESP model, and representatives of the two main classes of dataaware crowdsourcing models, namely (3) datagenerative and (4) data-conditional. MOMRESP represents a typical data-generative model (Bragg et al., 2013; Felt et al., 2014; Lam and Stork, 2005; Simpson and Roberts, 2015). Data-conditional approaches typically model data features conditionally using a log-linear model (Jin and Ghahramani, 2002; Raykar et al., 2010; Liu et al., 2012; Yan et al., 2014). For the purposes of this paper, we refer to this model as LOGRESP. For ITEMRESP, MOMRESP, and LOGRESP we use the variational inference methods presented by Felt et al. (2015). Unlike that paper, in this work we have augmented inference with the in-line hyperparameter updates described in Section 2.4. Figure 5: Inferred label accuracy of models on</context>
</contexts>
<marker>Bragg, Mausam, Weld, 2013</marker>
<rawString>Jonathan Bragg, Mausam, and Daniel S. Weld. 2013. Crowdsourcing multi-label classification for taxonomy creation. In First AAAI Conference on Human Computation and Crowdsourcing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana Margarida de Jesus Cardoso-Cachopo</author>
</authors>
<title>Improving Methods for Single-label Text Categorization.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Universidade Tecnica de Lisboa.</institution>
<contexts>
<context position="20015" citStr="Cardoso-Cachopo (2007)" startWordPosition="3269" endWordPosition="3270">. Accordingly we construct 100 synthetic annotators, each parametrized by an accuracy drawn from Beta(3.6, 5.1) and with errors drawn from a symmetric Dirichlet Dir(1). Datasets are annotated by selecting an instance (at random without replacement) and then selecting K annotators (at random without replacement) to annotate it before moving on. We choose K = 7 to mirror the empirical average in the CrowdFlower annotation set. We evaluate on six text classification datasets, summarized in Table 2. The 20 Newsgroups, WebKB, Cade12, Reuters8, and Reuters52 datasets are described in more detail by Cardoso-Cachopo (2007). The LDC-labeled Enron emails dataset is described by Berry et al. (2001). Each dataset is 3The dataset is available via git at git://nlp.cs. byu.edu/plf1/crowdflower-newsgroups.git NEWSGROUPS 0 10 20 nnotat CADE12 algorithm csLDA MomResp LogResp ItemResp Majority 0 50 100 150 200 Number of annotations x 1,000 Figure 6: Inferred label accuracy of models on synthetic annotations. The first instance is annotated 7 times, then the second, and so on. preprocessed via Porter stemming and by removal of the stopwords from MALLET’s stopword list (McCallum, 2002). Features occurring fewer than 5 times</context>
</contexts>
<marker>Cardoso-Cachopo, 2007</marker>
<rawString>Ana Margarida de Jesus Cardoso-Cachopo. 2007. Improving Methods for Single-label Text Categorization. Ph.D. thesis, Universidade Tecnica de Lisboa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nilesh Dalvi</author>
<author>Anirban Dasgupta</author>
<author>Ravi Kumar</author>
<author>Vibhor Rastogi</author>
</authors>
<title>Aggregating crowdsourced binary ratings.</title>
<date>2013</date>
<booktitle>In Proceedings of World Wide Web Conference.</booktitle>
<contexts>
<context position="27684" citStr="Dalvi et al., 2013" startWordPosition="4493" endWordPosition="4496">al., 2013), correlation among various combinations of these variables (Zhou et al., 2014), and change in annotator behavior over time (Simpson and Roberts, 2015). Welinder et al. (2010) carefully model the process of annotating objects in images, including variables for item difficulty, item class, and classconditional perception noise. In follow-up work, Liu et al. (2012) demonstrate that similar levels of performance can be achieved with the simple item-response model by using variational inference rather than EM. Alternative inference algorithms have been proposed for crowdsourcing models (Dalvi et al., 2013; Ghosh et al., 2011; Karger et al., 2013; Zhang et al., 2014). Some crowdsourcing work regards labeled data not as an end in itself, but rather as a means to train classifiers (Lin et al., 2014). The fact-finding literature assigns trust scores to assertions made by untrusted sources (Pasternack and Roth, 2010). 5 Conclusion and Future Work We describe CSLDA, a generative, data-aware crowdsourcing model that addresses important modeling deficiencies identified in previous work. In particular, CSLDA handles data in which the natural document clusters are at odds with the intended document labe</context>
</contexts>
<marker>Dalvi, Dasgupta, Kumar, Rastogi, 2013</marker>
<rawString>Nilesh Dalvi, Anirban Dasgupta, Ravi Kumar, and Vibhor Rastogi. 2013. Aggregating crowdsourced binary ratings. In Proceedings of World Wide Web Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander P Dawid</author>
<author>Allan M Skene</author>
</authors>
<title>Maximum likelihood estimation of observer error-rates using the EM algorithm. Applied Statistics,</title>
<date>1979</date>
<pages>20--28</pages>
<contexts>
<context position="3282" citStr="Dawid and Skene (1979)" startWordPosition="467" endWordPosition="470"> crowdsourcing algorithms by modeling not only the annotations but also the features of the data (e.g., words in a document). Section 2 identifies modeling deficiencies affecting previous work and proposes a solution based on topic modeling; Section 2.4 presents inference for the new model. Experiments that contrast the proposed model with select previous work on several text classification datasets are presented in Section 3. In Section 4 we highlight additional related work. 2 Latent Representations that Reflect Labels and Confusion Most crowdsourcing models extend the itemresponse model of Dawid and Skene (1979). The Bayesian version of this model, referred to here as ITEMRESP, is depicted in Figure 1. In the generative story for this model, a confusion matrix -y, is drawn for each human annotator j. Each row -y,, of the confusion matrix -y, is drawn from 194 Proceedings of the 19th Conference on Computational Language Learning, pages 194–203, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics Figure 1: ITEMRESP as a plate diagram. Round nodes are random variables. Rectangular nodes are free parameters. Shaded nodes are observed. D, J, C are the number of documents, an</context>
</contexts>
<marker>Dawid, Skene, 1979</marker>
<rawString>Alexander P. Dawid and Allan M. Skene. 1979. Maximum likelihood estimation of observer error-rates using the EM algorithm. Applied Statistics, pages 20–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Felt</author>
<author>Robbie Haertel</author>
<author>Eric Ringger</author>
<author>Kevin Seppi</author>
</authors>
<title>MomResp: A Bayesian model for multi-annotator document labeling.</title>
<date>2014</date>
<booktitle>In International Language Resources and Evaluation.</booktitle>
<contexts>
<context position="4753" citStr="Felt et al., 2014" startWordPosition="700" endWordPosition="703">ach document d an unobserved document label yd is drawn. Annotations are generated as annotator j corrupts the true label yd according to the categorical distribution Cat(γjyd). 2.1 Leveraging Data Some extensions to ITEMRESP model the features of the data (e.g., words in a document). Many data-aware crowdsourcing models condition the labels on the data (Jin and Ghahramani, 2002; Raykar et al., 2010; Liu et al., 2012; Yan et al., 2014), possibly because discriminative classifiers dominate supervised machine learning. Others model the data generatively (Bragg et al., 2013; Lam and Stork, 2005; Felt et al., 2014; Simpson and Roberts, 2015). Felt et al. (2015) argue that generative models are better suited than conditional models to crowdsourcing scenarios because generative models often learn faster than their conditional counterparts (Ng and Jordan, 2001)— especially early in the learning curve. This advantage is amplified by the annotation noise typical of crowdsourcing scenarios. Extensions to ITEMRESP that model document features generatively tend to share a common high-level architecture. After the document class label yd is drawn for each document d, features are drawn from class-conditional di</context>
<context position="15857" citStr="Felt et al., 2014" startWordPosition="2615" endWordPosition="2618">er of label classes does performance suffer. As per Section 2.3, z and q values are initialized with 500 rounds of stochastic EM, after which the full model is updated with 1000 additional rounds. Predictions are generated by aggregating samples from the last 100 rounds (the mode of the approximate marginal posterior). We compare CSLDA with (1) a majority vote baseline, (2) the ITEMRESP model, and representatives of the two main classes of dataaware crowdsourcing models, namely (3) datagenerative and (4) data-conditional. MOMRESP represents a typical data-generative model (Bragg et al., 2013; Felt et al., 2014; Lam and Stork, 2005; Simpson and Roberts, 2015). Data-conditional approaches typically model data features conditionally using a log-linear model (Jin and Ghahramani, 2002; Raykar et al., 2010; Liu et al., 2012; Yan et al., 2014). For the purposes of this paper, we refer to this model as LOGRESP. For ITEMRESP, MOMRESP, and LOGRESP we use the variational inference methods presented by Felt et al. (2015). Unlike that paper, in this work we have augmented inference with the in-line hyperparameter updates described in Section 2.4. Figure 5: Inferred label accuracy of models on sentiment-annotate</context>
</contexts>
<marker>Felt, Haertel, Ringger, Seppi, 2014</marker>
<rawString>Paul Felt, Robbie Haertel, Eric Ringger, and Kevin Seppi. 2014. MomResp: A Bayesian model for multi-annotator document labeling. In International Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Felt</author>
<author>Eric Ringger</author>
<author>Kevin Seppi</author>
<author>Robbie Haertel</author>
</authors>
<title>Early gains matter: A case for preferring generative over discriminative crowdsourcing models.</title>
<date>2015</date>
<booktitle>In Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4801" citStr="Felt et al. (2015)" startWordPosition="709" endWordPosition="712">is drawn. Annotations are generated as annotator j corrupts the true label yd according to the categorical distribution Cat(γjyd). 2.1 Leveraging Data Some extensions to ITEMRESP model the features of the data (e.g., words in a document). Many data-aware crowdsourcing models condition the labels on the data (Jin and Ghahramani, 2002; Raykar et al., 2010; Liu et al., 2012; Yan et al., 2014), possibly because discriminative classifiers dominate supervised machine learning. Others model the data generatively (Bragg et al., 2013; Lam and Stork, 2005; Felt et al., 2014; Simpson and Roberts, 2015). Felt et al. (2015) argue that generative models are better suited than conditional models to crowdsourcing scenarios because generative models often learn faster than their conditional counterparts (Ng and Jordan, 2001)— especially early in the learning curve. This advantage is amplified by the annotation noise typical of crowdsourcing scenarios. Extensions to ITEMRESP that model document features generatively tend to share a common high-level architecture. After the document class label yd is drawn for each document d, features are drawn from class-conditional distributions. Felt et al. (2015) identify the MOM</context>
<context position="16264" citStr="Felt et al. (2015)" startWordPosition="2683" endWordPosition="2686">sentatives of the two main classes of dataaware crowdsourcing models, namely (3) datagenerative and (4) data-conditional. MOMRESP represents a typical data-generative model (Bragg et al., 2013; Felt et al., 2014; Lam and Stork, 2005; Simpson and Roberts, 2015). Data-conditional approaches typically model data features conditionally using a log-linear model (Jin and Ghahramani, 2002; Raykar et al., 2010; Liu et al., 2012; Yan et al., 2014). For the purposes of this paper, we refer to this model as LOGRESP. For ITEMRESP, MOMRESP, and LOGRESP we use the variational inference methods presented by Felt et al. (2015). Unlike that paper, in this work we have augmented inference with the in-line hyperparameter updates described in Section 2.4. Figure 5: Inferred label accuracy of models on sentiment-annotated weather tweets. 3.1 Human-generated Annotations To gauge the effectiveness of data-aware crowdsourcing models, we use the sentiment-annotated tweet dataset distributed by CrowdFlower as a part of its “data for everyone” initiative.2 In the “Weather Sentiment” task, 20 annotators judged the sentiment of 1000 tweets as either positive, negative, neutral, or unrelated to the weather. In the secondary “Wea</context>
<context position="19077" citStr="Felt et al. (2015)" startWordPosition="3125" endWordPosition="3128">ument size. Values are calculated after setting aside 15% as validation data and doing feature selection. 3.2 Synthetic Annotations Datasets including both annotations and gold standard labels are in short supply. Although plenty of text categorization datasets have been annotated, common practice reflects that initial noisy annotations be discarded and only consensus labels be published. Consequently, we follow previous work in achieving broad validation by constructing synthetic annotators that corrupt known gold standard labels. We base our experimental setup on the annotations gathered by Felt et al. (2015),3 who paid CrowdFlower annotators to relabel 1000 documents from the well-known 20 Newsgroups classification dataset. In that experiment, 136 annotators contributed, each instance was labeled an average of 6.9 times, and annotator accuracies were distributed approximately according to a Beta(3.6, 5.1) distribution. Accordingly we construct 100 synthetic annotators, each parametrized by an accuracy drawn from Beta(3.6, 5.1) and with errors drawn from a symmetric Dirichlet Dir(1). Datasets are annotated by selecting an instance (at random without replacement) and then selecting K annotators (at</context>
</contexts>
<marker>Felt, Ringger, Seppi, Haertel, 2015</marker>
<rawString>Paul Felt, Eric Ringger, Kevin Seppi, and Robbie Haertel. 2015. Early gains matter: A case for preferring generative over discriminative crowdsourcing models. In Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arpita Ghosh</author>
<author>Satyen Kale</author>
<author>Preston McAfee</author>
</authors>
<title>Who moderates the moderators?: crowdsourcing abuse detection in user-generated content.</title>
<date>2011</date>
<booktitle>In Proceedings of the 12th ACM conference on Electronic commerce.</booktitle>
<contexts>
<context position="27704" citStr="Ghosh et al., 2011" startWordPosition="4497" endWordPosition="4500">ion among various combinations of these variables (Zhou et al., 2014), and change in annotator behavior over time (Simpson and Roberts, 2015). Welinder et al. (2010) carefully model the process of annotating objects in images, including variables for item difficulty, item class, and classconditional perception noise. In follow-up work, Liu et al. (2012) demonstrate that similar levels of performance can be achieved with the simple item-response model by using variational inference rather than EM. Alternative inference algorithms have been proposed for crowdsourcing models (Dalvi et al., 2013; Ghosh et al., 2011; Karger et al., 2013; Zhang et al., 2014). Some crowdsourcing work regards labeled data not as an end in itself, but rather as a means to train classifiers (Lin et al., 2014). The fact-finding literature assigns trust scores to assertions made by untrusted sources (Pasternack and Roth, 2010). 5 Conclusion and Future Work We describe CSLDA, a generative, data-aware crowdsourcing model that addresses important modeling deficiencies identified in previous work. In particular, CSLDA handles data in which the natural document clusters are at odds with the intended document labels. It also transiti</context>
</contexts>
<marker>Ghosh, Kale, McAfee, 2011</marker>
<rawString>Arpita Ghosh, Satyen Kale, and Preston McAfee. 2011. Who moderates the moderators?: crowdsourcing abuse detection in user-generated content. In Proceedings of the 12th ACM conference on Electronic commerce.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dirk Hovy</author>
<author>Taylor Berg-Kirkpatrick</author>
<author>Ashish Vaswani</author>
<author>Eduard H Hovy</author>
</authors>
<title>Learning whom to trust with MACE.</title>
<date>2013</date>
<booktitle>In Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="27076" citStr="Hovy et al., 2013" startWordPosition="4399" endWordPosition="4402">desirable. That is, if two classes of documents are mostly the same both topically and in terms of annotator decisions, perhaps those classes ought to be collapsed. We are not overly concerned that MOMRESP beats CSLDA in Figure 8, since this result is consistent with early relative performance in simulation. 4 Additional Related Work This section reviews related work not already discussed. A growing body of work extends the itemresponse model to account for variables such as item difficulty (Whitehill et al., 2009; Passonneau and Carpenter, 2013; Zhou et al., 2012), annotator trustworthiness (Hovy et al., 2013), correlation among various combinations of these variables (Zhou et al., 2014), and change in annotator behavior over time (Simpson and Roberts, 2015). Welinder et al. (2010) carefully model the process of annotating objects in images, including variables for item difficulty, item class, and classconditional perception noise. In follow-up work, Liu et al. (2012) demonstrate that similar levels of performance can be achieved with the simple item-response model by using variational inference rather than EM. Alternative inference algorithms have been proposed for crowdsourcing models (Dalvi et a</context>
</contexts>
<marker>Hovy, Berg-Kirkpatrick, Vaswani, Hovy, 2013</marker>
<rawString>Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani, and Eduard H. Hovy. 2013. Learning whom to trust with MACE. In Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong Jin</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Learning with multiple labels.</title>
<date>2002</date>
<booktitle>In Proceedings of Advances in Neural Information Processing Systems,</booktitle>
<pages>897--904</pages>
<contexts>
<context position="4517" citStr="Jin and Ghahramani, 2002" startWordPosition="663" endWordPosition="666">nd classes, respectively. a symmetric Dirichlet distribution Dir(b(γ) jc ) and encodes a categorical probability distribution over label classes that annotator j is apt to choose when presented with a document whose true label is c. Then for each document d an unobserved document label yd is drawn. Annotations are generated as annotator j corrupts the true label yd according to the categorical distribution Cat(γjyd). 2.1 Leveraging Data Some extensions to ITEMRESP model the features of the data (e.g., words in a document). Many data-aware crowdsourcing models condition the labels on the data (Jin and Ghahramani, 2002; Raykar et al., 2010; Liu et al., 2012; Yan et al., 2014), possibly because discriminative classifiers dominate supervised machine learning. Others model the data generatively (Bragg et al., 2013; Lam and Stork, 2005; Felt et al., 2014; Simpson and Roberts, 2015). Felt et al. (2015) argue that generative models are better suited than conditional models to crowdsourcing scenarios because generative models often learn faster than their conditional counterparts (Ng and Jordan, 2001)— especially early in the learning curve. This advantage is amplified by the annotation noise typical of crowdsourc</context>
<context position="16030" citStr="Jin and Ghahramani, 2002" startWordPosition="2640" endWordPosition="2643">d with 1000 additional rounds. Predictions are generated by aggregating samples from the last 100 rounds (the mode of the approximate marginal posterior). We compare CSLDA with (1) a majority vote baseline, (2) the ITEMRESP model, and representatives of the two main classes of dataaware crowdsourcing models, namely (3) datagenerative and (4) data-conditional. MOMRESP represents a typical data-generative model (Bragg et al., 2013; Felt et al., 2014; Lam and Stork, 2005; Simpson and Roberts, 2015). Data-conditional approaches typically model data features conditionally using a log-linear model (Jin and Ghahramani, 2002; Raykar et al., 2010; Liu et al., 2012; Yan et al., 2014). For the purposes of this paper, we refer to this model as LOGRESP. For ITEMRESP, MOMRESP, and LOGRESP we use the variational inference methods presented by Felt et al. (2015). Unlike that paper, in this work we have augmented inference with the in-line hyperparameter updates described in Section 2.4. Figure 5: Inferred label accuracy of models on sentiment-annotated weather tweets. 3.1 Human-generated Annotations To gauge the effectiveness of data-aware crowdsourcing models, we use the sentiment-annotated tweet dataset distributed by </context>
</contexts>
<marker>Jin, Ghahramani, 2002</marker>
<rawString>Rong Jin and Zoubin Ghahramani. 2002. Learning with multiple labels. In Proceedings of Advances in Neural Information Processing Systems, pages 897– 904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
</authors>
<title>Embracing ambiguity: A comparison of annotation methodologies for crowdsourcing word sense labels.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>556--562</pages>
<contexts>
<context position="1665" citStr="Jurgens, 2013" startWordPosition="229" endWordPosition="230">quires labeled training corpora, historically produced by laborious and costly annotation projects. Microtask markets such as Amazon’s Mechanical Turk and Crowdflower have turned crowd labor into a commodity that can be purchased with relatively little overhead. However, crowdsourced judgments can suffer from high error rates. A common solution to this problem is to obtain multiple redundant human judgments, or annotations,1 relying on the observation that, in aggregate, non-experts often rival or exceed experts by averaging over individual error patterns (Surowiecki, 2005; Snow et al., 2008; Jurgens, 2013). A crowdsourcing model harnesses the wisdom of the crowd and infers labels based on the evidence of the available annotations, imperfect 1In this paper, we call human judgments annotations to distinguish them from gold standard class labels. though they be. A common baseline crowdsourcing method aggregates annotations by majority vote, but this approach ignores important information. For example, some annotators are more reliable than others and their judgments ought to be upweighted accordingly. State-ofthe-art crowdsourcing methods account for annotator expertise, often through a probabilis</context>
</contexts>
<marker>Jurgens, 2013</marker>
<rawString>David Jurgens. 2013. Embracing ambiguity: A comparison of annotation methodologies for crowdsourcing word sense labels. In Proceedings of NAACL-HLT, pages 556–562.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Karger</author>
<author>Sewoong Oh</author>
<author>Devavrat Shah</author>
</authors>
<title>Efficient crowdsourcing for multi-class labeling.</title>
<date>2013</date>
<journal>In ACM SIGMETRICS Performance Evaluation Review,</journal>
<volume>41</volume>
<pages>81--92</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="27725" citStr="Karger et al., 2013" startWordPosition="4501" endWordPosition="4504">mbinations of these variables (Zhou et al., 2014), and change in annotator behavior over time (Simpson and Roberts, 2015). Welinder et al. (2010) carefully model the process of annotating objects in images, including variables for item difficulty, item class, and classconditional perception noise. In follow-up work, Liu et al. (2012) demonstrate that similar levels of performance can be achieved with the simple item-response model by using variational inference rather than EM. Alternative inference algorithms have been proposed for crowdsourcing models (Dalvi et al., 2013; Ghosh et al., 2011; Karger et al., 2013; Zhang et al., 2014). Some crowdsourcing work regards labeled data not as an end in itself, but rather as a means to train classifiers (Lin et al., 2014). The fact-finding literature assigns trust scores to assertions made by untrusted sources (Pasternack and Roth, 2010). 5 Conclusion and Future Work We describe CSLDA, a generative, data-aware crowdsourcing model that addresses important modeling deficiencies identified in previous work. In particular, CSLDA handles data in which the natural document clusters are at odds with the intended document labels. It also transitions smoothly from sit</context>
</contexts>
<marker>Karger, Oh, Shah, 2013</marker>
<rawString>David R. Karger, Sewoong Oh, and Devavrat Shah. 2013. Efficient crowdsourcing for multi-class labeling. In ACM SIGMETRICS Performance Evaluation Review, volume 41, pages 81–92. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chuck P Lam</author>
<author>David G Stork</author>
</authors>
<title>Toward optimal labeling strategy under multiple unreliable labelers. In AAAI Spring Symposium: Knowledge Collection from Volunteer Contributors.</title>
<date>2005</date>
<contexts>
<context position="4734" citStr="Lam and Stork, 2005" startWordPosition="696" endWordPosition="699">abel is c. Then for each document d an unobserved document label yd is drawn. Annotations are generated as annotator j corrupts the true label yd according to the categorical distribution Cat(γjyd). 2.1 Leveraging Data Some extensions to ITEMRESP model the features of the data (e.g., words in a document). Many data-aware crowdsourcing models condition the labels on the data (Jin and Ghahramani, 2002; Raykar et al., 2010; Liu et al., 2012; Yan et al., 2014), possibly because discriminative classifiers dominate supervised machine learning. Others model the data generatively (Bragg et al., 2013; Lam and Stork, 2005; Felt et al., 2014; Simpson and Roberts, 2015). Felt et al. (2015) argue that generative models are better suited than conditional models to crowdsourcing scenarios because generative models often learn faster than their conditional counterparts (Ng and Jordan, 2001)— especially early in the learning curve. This advantage is amplified by the annotation noise typical of crowdsourcing scenarios. Extensions to ITEMRESP that model document features generatively tend to share a common high-level architecture. After the document class label yd is drawn for each document d, features are drawn from c</context>
<context position="15878" citStr="Lam and Stork, 2005" startWordPosition="2619" endWordPosition="2622"> does performance suffer. As per Section 2.3, z and q values are initialized with 500 rounds of stochastic EM, after which the full model is updated with 1000 additional rounds. Predictions are generated by aggregating samples from the last 100 rounds (the mode of the approximate marginal posterior). We compare CSLDA with (1) a majority vote baseline, (2) the ITEMRESP model, and representatives of the two main classes of dataaware crowdsourcing models, namely (3) datagenerative and (4) data-conditional. MOMRESP represents a typical data-generative model (Bragg et al., 2013; Felt et al., 2014; Lam and Stork, 2005; Simpson and Roberts, 2015). Data-conditional approaches typically model data features conditionally using a log-linear model (Jin and Ghahramani, 2002; Raykar et al., 2010; Liu et al., 2012; Yan et al., 2014). For the purposes of this paper, we refer to this model as LOGRESP. For ITEMRESP, MOMRESP, and LOGRESP we use the variational inference methods presented by Felt et al. (2015). Unlike that paper, in this work we have augmented inference with the in-line hyperparameter updates described in Section 2.4. Figure 5: Inferred label accuracy of models on sentiment-annotated weather tweets. 3.1</context>
</contexts>
<marker>Lam, Stork, 2005</marker>
<rawString>Chuck P. Lam and David G. Stork. 2005. Toward optimal labeling strategy under multiple unreliable labelers. In AAAI Spring Symposium: Knowledge Collection from Volunteer Contributors.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abby Levenberg</author>
<author>Stephen Pulman</author>
<author>Karo Moilanen</author>
<author>Edwin Simpson</author>
<author>Stephen Roberts</author>
</authors>
<title>Predicting economic indicators from web text using sentiment composition.</title>
<date>2014</date>
<journal>International Journal of Computer and Communication Engineering,</journal>
<volume>3</volume>
<issue>2</issue>
<contexts>
<context position="8311" citStr="Levenberg et al., 2014" startWordPosition="1335" endWordPosition="1338">jc, ✶(yd = c) (Njc1 ··· NjcC) P dn ✶(wdn = v ∧ zdn = t) P dn ✶(zdn = t) Count excludes variable being sampled Vector where ¯zdt = Nd &amp; ✶ (zdn = t) Excludes the zdn being sampled Nd Ndt Nt Njcc, Njc Nvt Nt Nˆ zd ˆ¯zd Table 1: Definition of counts and select notation. ✶(·) is the indicator function. We call our model confused supervised LDA (CSLDA, Figure 3), based on supervised topic modeling. Latent Dirichlet Allocation (Blei et al., 2003, LDA) models text documents as admixtures of word distributions, or topics. Although pre-calculated LDA topics as features can inform a crowdsourcing model (Levenberg et al., 2014), supervised LDA (sLDA) provides a principled way of incorporating document class labels and topics into a single model, allowing topic variables and response variables to co-inform one another in joint inference. For example, when sLDA is given movie reviews labeled with sentiment, inferred topics cluster around sentimentheavy words (Mcauliffe and Blei, 2007), which may be quite different from the topics inferred by unsupervised LDA. One way to view CSLDA is as a discrete sLDA in settings with noisy supervision from multiple, imprecise annotators. The generative story for CSLDA is: 1. Draw pe</context>
<context position="24479" citStr="Levenberg et al., 2014" startWordPosition="3985" endWordPosition="3988">s inferred labels reach an accuracy of 95%, a lofty goal that can require significant amounts of annotation when using poor quality annotations. CSLDA achieves 95% accuracy with fewer annotations, corresponding to reduced annotation cost. NEWSGROUPS algorithm csLDA csLDA−P 0.70 20 30 40 50 60 Number of annotations x 1,000 Figure 7: Joint inference for CSLDA vs pipeline inference (CSLDA-P). 3.3 Joint vs Pipeline Inference To isolate the effectiveness of joint inference in CSLDA, we compare against the pipeline alternative where topics are inferred first and then held constant during inference (Levenberg et al., 2014). Joint inference yields modest but consistent benefits over a pipeline approach. Figure 7 highlights a portion of the learning curve on the Newsgroups dataset (based on the experiments summarized in Table 3). This trend holds across all of the datasets that we examined. 3.4 Error Analysis Class-conditional models like MOMRESP include a feature that data-conditional models like CSLDA lack: an explicit prior over class prevalence. Figure 8a shows that CSLDA performs poorly on the CrowdFlower-annotated Newsgroups documents described at the beginning of Section 3 (not the synthetic annotations). </context>
</contexts>
<marker>Levenberg, Pulman, Moilanen, Simpson, Roberts, 2014</marker>
<rawString>Abby Levenberg, Stephen Pulman, Karo Moilanen, Edwin Simpson, and Stephen Roberts. 2014. Predicting economic indicators from web text using sentiment composition. International Journal of Computer and Communication Engineering, 3(2):109–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher H Lin</author>
<author>Mausam</author>
<author>Daniel S Weld</author>
</authors>
<title>To re (label), or not to re (label).</title>
<date>2014</date>
<booktitle>In Second AAAI Conference on Human Computation and Crowdsourcing.</booktitle>
<contexts>
<context position="27879" citStr="Lin et al., 2014" startWordPosition="4530" endWordPosition="4533">model the process of annotating objects in images, including variables for item difficulty, item class, and classconditional perception noise. In follow-up work, Liu et al. (2012) demonstrate that similar levels of performance can be achieved with the simple item-response model by using variational inference rather than EM. Alternative inference algorithms have been proposed for crowdsourcing models (Dalvi et al., 2013; Ghosh et al., 2011; Karger et al., 2013; Zhang et al., 2014). Some crowdsourcing work regards labeled data not as an end in itself, but rather as a means to train classifiers (Lin et al., 2014). The fact-finding literature assigns trust scores to assertions made by untrusted sources (Pasternack and Roth, 2010). 5 Conclusion and Future Work We describe CSLDA, a generative, data-aware crowdsourcing model that addresses important modeling deficiencies identified in previous work. In particular, CSLDA handles data in which the natural document clusters are at odds with the intended document labels. It also transitions smoothly from situations in which few annotations are available to those in which many annotations are available. Because of the flexible mapping in CSLDA to class labels,</context>
</contexts>
<marker>Lin, Mausam, Weld, 2014</marker>
<rawString>Christopher H. Lin, Mausam, and Daniel S. Weld. 2014. To re (label), or not to re (label). In Second AAAI Conference on Human Computation and Crowdsourcing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiang Liu</author>
<author>Jian Peng</author>
<author>Alex T Ihler</author>
</authors>
<title>Variational inference for crowdsourcing.</title>
<date>2012</date>
<booktitle>In Proceedings of Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="4556" citStr="Liu et al., 2012" startWordPosition="671" endWordPosition="674"> distribution Dir(b(γ) jc ) and encodes a categorical probability distribution over label classes that annotator j is apt to choose when presented with a document whose true label is c. Then for each document d an unobserved document label yd is drawn. Annotations are generated as annotator j corrupts the true label yd according to the categorical distribution Cat(γjyd). 2.1 Leveraging Data Some extensions to ITEMRESP model the features of the data (e.g., words in a document). Many data-aware crowdsourcing models condition the labels on the data (Jin and Ghahramani, 2002; Raykar et al., 2010; Liu et al., 2012; Yan et al., 2014), possibly because discriminative classifiers dominate supervised machine learning. Others model the data generatively (Bragg et al., 2013; Lam and Stork, 2005; Felt et al., 2014; Simpson and Roberts, 2015). Felt et al. (2015) argue that generative models are better suited than conditional models to crowdsourcing scenarios because generative models often learn faster than their conditional counterparts (Ng and Jordan, 2001)— especially early in the learning curve. This advantage is amplified by the annotation noise typical of crowdsourcing scenarios. Extensions to ITEMRESP t</context>
<context position="16069" citStr="Liu et al., 2012" startWordPosition="2648" endWordPosition="2651">generated by aggregating samples from the last 100 rounds (the mode of the approximate marginal posterior). We compare CSLDA with (1) a majority vote baseline, (2) the ITEMRESP model, and representatives of the two main classes of dataaware crowdsourcing models, namely (3) datagenerative and (4) data-conditional. MOMRESP represents a typical data-generative model (Bragg et al., 2013; Felt et al., 2014; Lam and Stork, 2005; Simpson and Roberts, 2015). Data-conditional approaches typically model data features conditionally using a log-linear model (Jin and Ghahramani, 2002; Raykar et al., 2010; Liu et al., 2012; Yan et al., 2014). For the purposes of this paper, we refer to this model as LOGRESP. For ITEMRESP, MOMRESP, and LOGRESP we use the variational inference methods presented by Felt et al. (2015). Unlike that paper, in this work we have augmented inference with the in-line hyperparameter updates described in Section 2.4. Figure 5: Inferred label accuracy of models on sentiment-annotated weather tweets. 3.1 Human-generated Annotations To gauge the effectiveness of data-aware crowdsourcing models, we use the sentiment-annotated tweet dataset distributed by CrowdFlower as a part of its “data for </context>
<context position="27441" citStr="Liu et al. (2012)" startWordPosition="4456" endWordPosition="4459">ork not already discussed. A growing body of work extends the itemresponse model to account for variables such as item difficulty (Whitehill et al., 2009; Passonneau and Carpenter, 2013; Zhou et al., 2012), annotator trustworthiness (Hovy et al., 2013), correlation among various combinations of these variables (Zhou et al., 2014), and change in annotator behavior over time (Simpson and Roberts, 2015). Welinder et al. (2010) carefully model the process of annotating objects in images, including variables for item difficulty, item class, and classconditional perception noise. In follow-up work, Liu et al. (2012) demonstrate that similar levels of performance can be achieved with the simple item-response model by using variational inference rather than EM. Alternative inference algorithms have been proposed for crowdsourcing models (Dalvi et al., 2013; Ghosh et al., 2011; Karger et al., 2013; Zhang et al., 2014). Some crowdsourcing work regards labeled data not as an end in itself, but rather as a means to train classifiers (Lin et al., 2014). The fact-finding literature assigns trust scores to assertions made by untrusted sources (Pasternack and Roth, 2010). 5 Conclusion and Future Work We describe C</context>
</contexts>
<marker>Liu, Peng, Ihler, 2012</marker>
<rawString>Qiang Liu, Jian Peng, and Alex T. Ihler. 2012. Variational inference for crowdsourcing. In Proceedings of Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon D Mcauliffe</author>
<author>David Blei</author>
</authors>
<title>Supervised topic models.</title>
<date>2007</date>
<booktitle>In Proceedings ofAdvances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="8673" citStr="Mcauliffe and Blei, 2007" startWordPosition="1391" endWordPosition="1394">based on supervised topic modeling. Latent Dirichlet Allocation (Blei et al., 2003, LDA) models text documents as admixtures of word distributions, or topics. Although pre-calculated LDA topics as features can inform a crowdsourcing model (Levenberg et al., 2014), supervised LDA (sLDA) provides a principled way of incorporating document class labels and topics into a single model, allowing topic variables and response variables to co-inform one another in joint inference. For example, when sLDA is given movie reviews labeled with sentiment, inferred topics cluster around sentimentheavy words (Mcauliffe and Blei, 2007), which may be quite different from the topics inferred by unsupervised LDA. One way to view CSLDA is as a discrete sLDA in settings with noisy supervision from multiple, imprecise annotators. The generative story for CSLDA is: 1. Draw per-topic word distributions φt from Dir(b(θ)). 2. Draw per-class regression parameters ηc from Gauss(µ, E). 3. Draw per-annotator confusion matrices γj with row γjc drawn from Dir(b(γ) jc ). 4. For each document d, (a) Draw topic vector θd from Dir(b(θ)). (b) For each token position n, draw topic zdn from Cat(θd) and word wdn from Cat(φzdn). (c) Draw class labe</context>
</contexts>
<marker>Mcauliffe, Blei, 2007</marker>
<rawString>Jon D. Mcauliffe and David Blei. 2007. Supervised topic models. In Proceedings ofAdvances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="20576" citStr="McCallum, 2002" startWordPosition="3355" endWordPosition="3356">are described in more detail by Cardoso-Cachopo (2007). The LDC-labeled Enron emails dataset is described by Berry et al. (2001). Each dataset is 3The dataset is available via git at git://nlp.cs. byu.edu/plf1/crowdflower-newsgroups.git NEWSGROUPS 0 10 20 nnotat CADE12 algorithm csLDA MomResp LogResp ItemResp Majority 0 50 100 150 200 Number of annotations x 1,000 Figure 6: Inferred label accuracy of models on synthetic annotations. The first instance is annotated 7 times, then the second, and so on. preprocessed via Porter stemming and by removal of the stopwords from MALLET’s stopword list (McCallum, 2002). Features occurring fewer than 5 times in the corpus are discarded. In the case of MOMRESP, features are fractionally scaled so that each document is the same length, in keeping with previous work in multinomial document models (Nigam et al., 2006). Figure 6 plots learning curves on three representative datasets (Enron resembles Cade12, and the Reuters datasets resemble WebKB). CSLDA consistently outperforms LOGRESP, ITEMRESP, and majority vote. The generative models (CSLDA and MOMRESP) tend to excel in lowannotation portions of the learning curve, partially because generative models tend to </context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Minka</author>
</authors>
<title>Estimating a Dirichlet distribution.</title>
<date>2000</date>
<contexts>
<context position="13406" citStr="Minka (2000)" startWordPosition="2200" endWordPosition="2201">curves of three ITEMRESP inference algorithms are reduced when hyperparameters are optimized. is, the algorithms in Figure 4b are in better agreement, particularly near the extremes, than the algorithms in Figure 4a. This difference is subtle, but it holds to an equal and greater extent in other simulation conditions we tested (experiment details are similar to those reported in Section 3). Fixed-point Hyperparameter Updates Although a grid search is effective, it is not practical for a model with many hyperparameters such as CSLDA. For efficiency, therefore, we use the fixed-point updates of Minka (2000). Our updates differ slightly from Minka’s since we tie hyperparameters, allowing them to be learned more quickly from less data. In our implementation the matrices of hyperparameters b(φ) and b(θ) over the Dirichlet-multinomial distributions are completely tied such that b(φ) tv = b(φ)∀t, v and b(θ) t= b(θ)∀t. This leads to P t,v[Ψ(Ntv+b(φ))]−TV Ψ(b(φ)) b(φ)←b(φ)· (5) V [Ψ(Nt+V b(φ))−Ψ(V b(φ))] and b(θ)←b(θ)·Pd,t[Ψ(Ndt+b(θ))]−NTΨ(b(θ))(6) T [Ψ(Nd+Tb(θ))−Ψ(Tb(θ))] The updates for b(γ) are slightly more involved since we choose to tie the diagonal entries b(γ) d and separately the off-diagonal </context>
</contexts>
<marker>Minka, 2000</marker>
<rawString>Thomas Minka. 2000. Estimating a Dirichlet distribution.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>On discriminative vs. generative classifiers: A comparison of logistic regression and Naive Bayes.</title>
<date>2001</date>
<booktitle>Proceedings of Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="5002" citStr="Ng and Jordan, 2001" startWordPosition="737" endWordPosition="740">f the data (e.g., words in a document). Many data-aware crowdsourcing models condition the labels on the data (Jin and Ghahramani, 2002; Raykar et al., 2010; Liu et al., 2012; Yan et al., 2014), possibly because discriminative classifiers dominate supervised machine learning. Others model the data generatively (Bragg et al., 2013; Lam and Stork, 2005; Felt et al., 2014; Simpson and Roberts, 2015). Felt et al. (2015) argue that generative models are better suited than conditional models to crowdsourcing scenarios because generative models often learn faster than their conditional counterparts (Ng and Jordan, 2001)— especially early in the learning curve. This advantage is amplified by the annotation noise typical of crowdsourcing scenarios. Extensions to ITEMRESP that model document features generatively tend to share a common high-level architecture. After the document class label yd is drawn for each document d, features are drawn from class-conditional distributions. Felt et al. (2015) identify the MOMRESP model, reproduced in Figure 2, as a strong representative of generative crowdsourcing models. In MOMRESP, Figure 2: MOMRESP as a plate diagram. IxdI = V , the size of the vocabulary. Documents wit</context>
</contexts>
<marker>Ng, Jordan, 2001</marker>
<rawString>Andrew Y. Ng and Michael I. Jordan. 2001. On discriminative vs. generative classifiers: A comparison of logistic regression and Naive Bayes. Proceedings of Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Viet-An Nguyen</author>
<author>Jordan L Boyd-Graber</author>
<author>Philip Resnik</author>
</authors>
<title>Lexical and hierarchical topic regression.</title>
<date>2013</date>
<booktitle>In Proceedings of Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="28671" citStr="Nguyen et al. (2013)" startWordPosition="4651" endWordPosition="4654">nerative, data-aware crowdsourcing model that addresses important modeling deficiencies identified in previous work. In particular, CSLDA handles data in which the natural document clusters are at odds with the intended document labels. It also transitions smoothly from situations in which few annotations are available to those in which many annotations are available. Because of the flexible mapping in CSLDA to class labels, many structural variants are possible in future work. For example, this mapping could depend not just on inferred topical content but also directly on data features (c.f. Nguyen et al. (2013)) or learned embedded feature representations. The large number of parameters in the learned confusion matrices of crowdsourcing models present difficulty at scale. This could be addressed by modeling structure both inside of the annotators and classes. Redundant annotations give unique insights into both inter-annotator and inter-class relationships and could be used to induce annotator or label class hierarchies with parsimonious representations. Simpson et al. (2013) identify annotator clusters using community detection algorithms but do not address annotator hierarchy or scalable confusion</context>
</contexts>
<marker>Nguyen, Boyd-Graber, Resnik, 2013</marker>
<rawString>Viet-An Nguyen, Jordan L. Boyd-Graber, and Philip Resnik. 2013. Lexical and hierarchical topic regression. In Proceedings of Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>Andrew McCallum</author>
<author>Tom Mitchell</author>
</authors>
<title>Semi-supervised text classification using EM. Semi-Supervised Learning,</title>
<date>2006</date>
<pages>33--56</pages>
<contexts>
<context position="20825" citStr="Nigam et al., 2006" startWordPosition="3395" endWordPosition="3398">PS 0 10 20 nnotat CADE12 algorithm csLDA MomResp LogResp ItemResp Majority 0 50 100 150 200 Number of annotations x 1,000 Figure 6: Inferred label accuracy of models on synthetic annotations. The first instance is annotated 7 times, then the second, and so on. preprocessed via Porter stemming and by removal of the stopwords from MALLET’s stopword list (McCallum, 2002). Features occurring fewer than 5 times in the corpus are discarded. In the case of MOMRESP, features are fractionally scaled so that each document is the same length, in keeping with previous work in multinomial document models (Nigam et al., 2006). Figure 6 plots learning curves on three representative datasets (Enron resembles Cade12, and the Reuters datasets resemble WebKB). CSLDA consistently outperforms LOGRESP, ITEMRESP, and majority vote. The generative models (CSLDA and MOMRESP) tend to excel in lowannotation portions of the learning curve, partially because generative models tend to converge quickly and partially because generative models naturally learn from unlabeled documents (i.e., semi-supervision). However, MOMRESP tends to quickly reach a performance plateau after which additional annotations do little good. The performa</context>
</contexts>
<marker>Nigam, McCallum, Mitchell, 2006</marker>
<rawString>Kamal Nigam, Andrew McCallum, and Tom Mitchell. 2006. Semi-supervised text classification using EM. Semi-Supervised Learning, pages 33–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Bob Carpenter</author>
</authors>
<title>The benefits of a model of annotation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,</booktitle>
<pages>187--195</pages>
<contexts>
<context position="27009" citStr="Passonneau and Carpenter, 2013" startWordPosition="4388" endWordPosition="4391">ever, one could also argue that the original behavior of CSLDA is in some ways desirable. That is, if two classes of documents are mostly the same both topically and in terms of annotator decisions, perhaps those classes ought to be collapsed. We are not overly concerned that MOMRESP beats CSLDA in Figure 8, since this result is consistent with early relative performance in simulation. 4 Additional Related Work This section reviews related work not already discussed. A growing body of work extends the itemresponse model to account for variables such as item difficulty (Whitehill et al., 2009; Passonneau and Carpenter, 2013; Zhou et al., 2012), annotator trustworthiness (Hovy et al., 2013), correlation among various combinations of these variables (Zhou et al., 2014), and change in annotator behavior over time (Simpson and Roberts, 2015). Welinder et al. (2010) carefully model the process of annotating objects in images, including variables for item difficulty, item class, and classconditional perception noise. In follow-up work, Liu et al. (2012) demonstrate that similar levels of performance can be achieved with the simple item-response model by using variational inference rather than EM. Alternative inference</context>
</contexts>
<marker>Passonneau, Carpenter, 2013</marker>
<rawString>Rebecca J. Passonneau and Bob Carpenter. 2013. The benefits of a model of annotation. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 187–195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Pasternack</author>
<author>Dan Roth</author>
</authors>
<title>Knowing what to believe (when you already know something).</title>
<date>2010</date>
<booktitle>In Proceedings of International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="27997" citStr="Pasternack and Roth, 2010" startWordPosition="4548" endWordPosition="4551">lassconditional perception noise. In follow-up work, Liu et al. (2012) demonstrate that similar levels of performance can be achieved with the simple item-response model by using variational inference rather than EM. Alternative inference algorithms have been proposed for crowdsourcing models (Dalvi et al., 2013; Ghosh et al., 2011; Karger et al., 2013; Zhang et al., 2014). Some crowdsourcing work regards labeled data not as an end in itself, but rather as a means to train classifiers (Lin et al., 2014). The fact-finding literature assigns trust scores to assertions made by untrusted sources (Pasternack and Roth, 2010). 5 Conclusion and Future Work We describe CSLDA, a generative, data-aware crowdsourcing model that addresses important modeling deficiencies identified in previous work. In particular, CSLDA handles data in which the natural document clusters are at odds with the intended document labels. It also transitions smoothly from situations in which few annotations are available to those in which many annotations are available. Because of the flexible mapping in CSLDA to class labels, many structural variants are possible in future work. For example, this mapping could depend not just on inferred top</context>
</contexts>
<marker>Pasternack, Roth, 2010</marker>
<rawString>Jeff Pasternack and Dan Roth. 2010. Knowing what to believe (when you already know something). In Proceedings of International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vikas C Raykar</author>
<author>Shipeng Yu</author>
<author>Linda H Zhao</author>
<author>Gerardo Hermosillo Valadez</author>
<author>Charles Florin</author>
<author>Luca Bogoni</author>
<author>Linda Moy</author>
</authors>
<title>Learning from crowds.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>11</volume>
<pages>1322</pages>
<contexts>
<context position="4538" citStr="Raykar et al., 2010" startWordPosition="667" endWordPosition="670">a symmetric Dirichlet distribution Dir(b(γ) jc ) and encodes a categorical probability distribution over label classes that annotator j is apt to choose when presented with a document whose true label is c. Then for each document d an unobserved document label yd is drawn. Annotations are generated as annotator j corrupts the true label yd according to the categorical distribution Cat(γjyd). 2.1 Leveraging Data Some extensions to ITEMRESP model the features of the data (e.g., words in a document). Many data-aware crowdsourcing models condition the labels on the data (Jin and Ghahramani, 2002; Raykar et al., 2010; Liu et al., 2012; Yan et al., 2014), possibly because discriminative classifiers dominate supervised machine learning. Others model the data generatively (Bragg et al., 2013; Lam and Stork, 2005; Felt et al., 2014; Simpson and Roberts, 2015). Felt et al. (2015) argue that generative models are better suited than conditional models to crowdsourcing scenarios because generative models often learn faster than their conditional counterparts (Ng and Jordan, 2001)— especially early in the learning curve. This advantage is amplified by the annotation noise typical of crowdsourcing scenarios. Extens</context>
<context position="16051" citStr="Raykar et al., 2010" startWordPosition="2644" endWordPosition="2647">nds. Predictions are generated by aggregating samples from the last 100 rounds (the mode of the approximate marginal posterior). We compare CSLDA with (1) a majority vote baseline, (2) the ITEMRESP model, and representatives of the two main classes of dataaware crowdsourcing models, namely (3) datagenerative and (4) data-conditional. MOMRESP represents a typical data-generative model (Bragg et al., 2013; Felt et al., 2014; Lam and Stork, 2005; Simpson and Roberts, 2015). Data-conditional approaches typically model data features conditionally using a log-linear model (Jin and Ghahramani, 2002; Raykar et al., 2010; Liu et al., 2012; Yan et al., 2014). For the purposes of this paper, we refer to this model as LOGRESP. For ITEMRESP, MOMRESP, and LOGRESP we use the variational inference methods presented by Felt et al. (2015). Unlike that paper, in this work we have augmented inference with the in-line hyperparameter updates described in Section 2.4. Figure 5: Inferred label accuracy of models on sentiment-annotated weather tweets. 3.1 Human-generated Annotations To gauge the effectiveness of data-aware crowdsourcing models, we use the sentiment-annotated tweet dataset distributed by CrowdFlower as a part</context>
</contexts>
<marker>Raykar, Yu, Zhao, Valadez, Florin, Bogoni, Moy, 2010</marker>
<rawString>Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Gerardo Hermosillo Valadez, Charles Florin, Luca Bogoni, and Linda Moy. 2010. Learning from crowds. Journal of Machine Learning Research, 11:1297– 1322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edwin Simpson</author>
<author>Stephen Roberts</author>
</authors>
<title>Bayesian methods for intelligent task assignment in crowdsourcing systems.</title>
<date>2015</date>
<booktitle>In Decision Making: Uncertainty, Imperfection, Deliberation and Scalability,</booktitle>
<pages>1--32</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="4781" citStr="Simpson and Roberts, 2015" startWordPosition="704" endWordPosition="708">nobserved document label yd is drawn. Annotations are generated as annotator j corrupts the true label yd according to the categorical distribution Cat(γjyd). 2.1 Leveraging Data Some extensions to ITEMRESP model the features of the data (e.g., words in a document). Many data-aware crowdsourcing models condition the labels on the data (Jin and Ghahramani, 2002; Raykar et al., 2010; Liu et al., 2012; Yan et al., 2014), possibly because discriminative classifiers dominate supervised machine learning. Others model the data generatively (Bragg et al., 2013; Lam and Stork, 2005; Felt et al., 2014; Simpson and Roberts, 2015). Felt et al. (2015) argue that generative models are better suited than conditional models to crowdsourcing scenarios because generative models often learn faster than their conditional counterparts (Ng and Jordan, 2001)— especially early in the learning curve. This advantage is amplified by the annotation noise typical of crowdsourcing scenarios. Extensions to ITEMRESP that model document features generatively tend to share a common high-level architecture. After the document class label yd is drawn for each document d, features are drawn from class-conditional distributions. Felt et al. (20</context>
<context position="15906" citStr="Simpson and Roberts, 2015" startWordPosition="2623" endWordPosition="2626">fer. As per Section 2.3, z and q values are initialized with 500 rounds of stochastic EM, after which the full model is updated with 1000 additional rounds. Predictions are generated by aggregating samples from the last 100 rounds (the mode of the approximate marginal posterior). We compare CSLDA with (1) a majority vote baseline, (2) the ITEMRESP model, and representatives of the two main classes of dataaware crowdsourcing models, namely (3) datagenerative and (4) data-conditional. MOMRESP represents a typical data-generative model (Bragg et al., 2013; Felt et al., 2014; Lam and Stork, 2005; Simpson and Roberts, 2015). Data-conditional approaches typically model data features conditionally using a log-linear model (Jin and Ghahramani, 2002; Raykar et al., 2010; Liu et al., 2012; Yan et al., 2014). For the purposes of this paper, we refer to this model as LOGRESP. For ITEMRESP, MOMRESP, and LOGRESP we use the variational inference methods presented by Felt et al. (2015). Unlike that paper, in this work we have augmented inference with the in-line hyperparameter updates described in Section 2.4. Figure 5: Inferred label accuracy of models on sentiment-annotated weather tweets. 3.1 Human-generated Annotations</context>
<context position="27227" citStr="Simpson and Roberts, 2015" startWordPosition="4423" endWordPosition="4426">ught to be collapsed. We are not overly concerned that MOMRESP beats CSLDA in Figure 8, since this result is consistent with early relative performance in simulation. 4 Additional Related Work This section reviews related work not already discussed. A growing body of work extends the itemresponse model to account for variables such as item difficulty (Whitehill et al., 2009; Passonneau and Carpenter, 2013; Zhou et al., 2012), annotator trustworthiness (Hovy et al., 2013), correlation among various combinations of these variables (Zhou et al., 2014), and change in annotator behavior over time (Simpson and Roberts, 2015). Welinder et al. (2010) carefully model the process of annotating objects in images, including variables for item difficulty, item class, and classconditional perception noise. In follow-up work, Liu et al. (2012) demonstrate that similar levels of performance can be achieved with the simple item-response model by using variational inference rather than EM. Alternative inference algorithms have been proposed for crowdsourcing models (Dalvi et al., 2013; Ghosh et al., 2011; Karger et al., 2013; Zhang et al., 2014). Some crowdsourcing work regards labeled data not as an end in itself, but rathe</context>
</contexts>
<marker>Simpson, Roberts, 2015</marker>
<rawString>Edwin Simpson and Stephen Roberts. 2015. Bayesian methods for intelligent task assignment in crowdsourcing systems. In Decision Making: Uncertainty, Imperfection, Deliberation and Scalability, pages 1–32. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Simpson</author>
<author>S Roberts</author>
<author>I Psorakis</author>
<author>A Smith</author>
</authors>
<title>Dynamic bayesian combination of multiple imperfect classifiers.</title>
<date>2013</date>
<booktitle>In Decision Making and Imperfection,</booktitle>
<pages>1--35</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="29145" citStr="Simpson et al. (2013)" startWordPosition="4717" endWordPosition="4720">ure work. For example, this mapping could depend not just on inferred topical content but also directly on data features (c.f. Nguyen et al. (2013)) or learned embedded feature representations. The large number of parameters in the learned confusion matrices of crowdsourcing models present difficulty at scale. This could be addressed by modeling structure both inside of the annotators and classes. Redundant annotations give unique insights into both inter-annotator and inter-class relationships and could be used to induce annotator or label class hierarchies with parsimonious representations. Simpson et al. (2013) identify annotator clusters using community detection algorithms but do not address annotator hierarchy or scalable confusion representations. 0 2 4 6 (b) After combining frequently co-annotated label classes algorithm csLDA MomResp LogResp ItemResp Majority Accuracy 0.95 0.90 0.85 0.80 0.75 CFSIMPLEGROUPS Accuracy 0.6 0.5 0 2 4 6 algorithm csLDA MomResp LogResp ItemResp Majority 201 Acknowledgments This work was supported by the collaborative NSF Grant IIS-1409739 (BYU) and IIS-1409287 (UMD). Boyd-Graber is also supported by NSF grants IIS-1320538 and NCSE1422492. Any opinions, findings, con</context>
</contexts>
<marker>Simpson, Roberts, Psorakis, Smith, 2013</marker>
<rawString>E. Simpson, S. Roberts, I. Psorakis, and A. Smith. 2013. Dynamic bayesian combination of multiple imperfect classifiers. In Decision Making and Imperfection, pages 1–35. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast—but is it good?: Evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<publisher>ACL.</publisher>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast—but is it good?: Evaluating non-expert annotations for natural language tasks. In Proceedings of EMNLP. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Surowiecki</author>
</authors>
<title>The Wisdom of Crowds.</title>
<date>2005</date>
<publisher>Random House LLC.</publisher>
<contexts>
<context position="1630" citStr="Surowiecki, 2005" startWordPosition="223" endWordPosition="224">lities Supervised machine learning requires labeled training corpora, historically produced by laborious and costly annotation projects. Microtask markets such as Amazon’s Mechanical Turk and Crowdflower have turned crowd labor into a commodity that can be purchased with relatively little overhead. However, crowdsourced judgments can suffer from high error rates. A common solution to this problem is to obtain multiple redundant human judgments, or annotations,1 relying on the observation that, in aggregate, non-experts often rival or exceed experts by averaging over individual error patterns (Surowiecki, 2005; Snow et al., 2008; Jurgens, 2013). A crowdsourcing model harnesses the wisdom of the crowd and infers labels based on the evidence of the available annotations, imperfect 1In this paper, we call human judgments annotations to distinguish them from gold standard class labels. though they be. A common baseline crowdsourcing method aggregates annotations by majority vote, but this approach ignores important information. For example, some annotators are more reliable than others and their judgments ought to be upweighted accordingly. State-ofthe-art crowdsourcing methods account for annotator ex</context>
</contexts>
<marker>Surowiecki, 2005</marker>
<rawString>James Surowiecki. 2005. The Wisdom of Crowds. Random House LLC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Welinder</author>
<author>Steve Branson</author>
<author>Pietro Perona</author>
<author>Serge J Belongie</author>
</authors>
<title>The multidimensional wisdom of crowds.</title>
<date>2010</date>
<booktitle>In NIPS,</booktitle>
<pages>2424--2432</pages>
<contexts>
<context position="27251" citStr="Welinder et al. (2010)" startWordPosition="4427" endWordPosition="4430"> not overly concerned that MOMRESP beats CSLDA in Figure 8, since this result is consistent with early relative performance in simulation. 4 Additional Related Work This section reviews related work not already discussed. A growing body of work extends the itemresponse model to account for variables such as item difficulty (Whitehill et al., 2009; Passonneau and Carpenter, 2013; Zhou et al., 2012), annotator trustworthiness (Hovy et al., 2013), correlation among various combinations of these variables (Zhou et al., 2014), and change in annotator behavior over time (Simpson and Roberts, 2015). Welinder et al. (2010) carefully model the process of annotating objects in images, including variables for item difficulty, item class, and classconditional perception noise. In follow-up work, Liu et al. (2012) demonstrate that similar levels of performance can be achieved with the simple item-response model by using variational inference rather than EM. Alternative inference algorithms have been proposed for crowdsourcing models (Dalvi et al., 2013; Ghosh et al., 2011; Karger et al., 2013; Zhang et al., 2014). Some crowdsourcing work regards labeled data not as an end in itself, but rather as a means to train cl</context>
</contexts>
<marker>Welinder, Branson, Perona, Belongie, 2010</marker>
<rawString>Peter Welinder, Steve Branson, Pietro Perona, and Serge J. Belongie. 2010. The multidimensional wisdom of crowds. In NIPS, pages 2424–2432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Whitehill</author>
<author>Ting-fan Wu</author>
<author>Jacob Bergsma</author>
<author>Javier R Movellan</author>
<author>Paul L Ruvolo</author>
</authors>
<title>Whose vote should count more: Optimal integration of labels from labelers of unknown expertise.</title>
<date>2009</date>
<journal>NIPS,</journal>
<pages>22--2035</pages>
<contexts>
<context position="26977" citStr="Whitehill et al., 2009" startWordPosition="4384" endWordPosition="4387">sses were unaltered. However, one could also argue that the original behavior of CSLDA is in some ways desirable. That is, if two classes of documents are mostly the same both topically and in terms of annotator decisions, perhaps those classes ought to be collapsed. We are not overly concerned that MOMRESP beats CSLDA in Figure 8, since this result is consistent with early relative performance in simulation. 4 Additional Related Work This section reviews related work not already discussed. A growing body of work extends the itemresponse model to account for variables such as item difficulty (Whitehill et al., 2009; Passonneau and Carpenter, 2013; Zhou et al., 2012), annotator trustworthiness (Hovy et al., 2013), correlation among various combinations of these variables (Zhou et al., 2014), and change in annotator behavior over time (Simpson and Roberts, 2015). Welinder et al. (2010) carefully model the process of annotating objects in images, including variables for item difficulty, item class, and classconditional perception noise. In follow-up work, Liu et al. (2012) demonstrate that similar levels of performance can be achieved with the simple item-response model by using variational inference rathe</context>
</contexts>
<marker>Whitehill, Wu, Bergsma, Movellan, Ruvolo, 2009</marker>
<rawString>Jacob Whitehill, Ting-fan Wu, Jacob Bergsma, Javier R Movellan, and Paul L. Ruvolo. 2009. Whose vote should count more: Optimal integration of labels from labelers of unknown expertise. NIPS, 22:2035–2043.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yan Yan</author>
<author>R´omer Rosales</author>
<author>Glenn Fung</author>
<author>Ramanathan Subramanian</author>
<author>Jennifer Dy</author>
</authors>
<title>Learning from multiple annotators with varying expertise.</title>
<date>2014</date>
<booktitle>Machine Learning,</booktitle>
<volume>95</volume>
<issue>3</issue>
<contexts>
<context position="4575" citStr="Yan et al., 2014" startWordPosition="675" endWordPosition="678">b(γ) jc ) and encodes a categorical probability distribution over label classes that annotator j is apt to choose when presented with a document whose true label is c. Then for each document d an unobserved document label yd is drawn. Annotations are generated as annotator j corrupts the true label yd according to the categorical distribution Cat(γjyd). 2.1 Leveraging Data Some extensions to ITEMRESP model the features of the data (e.g., words in a document). Many data-aware crowdsourcing models condition the labels on the data (Jin and Ghahramani, 2002; Raykar et al., 2010; Liu et al., 2012; Yan et al., 2014), possibly because discriminative classifiers dominate supervised machine learning. Others model the data generatively (Bragg et al., 2013; Lam and Stork, 2005; Felt et al., 2014; Simpson and Roberts, 2015). Felt et al. (2015) argue that generative models are better suited than conditional models to crowdsourcing scenarios because generative models often learn faster than their conditional counterparts (Ng and Jordan, 2001)— especially early in the learning curve. This advantage is amplified by the annotation noise typical of crowdsourcing scenarios. Extensions to ITEMRESP that model document </context>
<context position="16088" citStr="Yan et al., 2014" startWordPosition="2652" endWordPosition="2655">gating samples from the last 100 rounds (the mode of the approximate marginal posterior). We compare CSLDA with (1) a majority vote baseline, (2) the ITEMRESP model, and representatives of the two main classes of dataaware crowdsourcing models, namely (3) datagenerative and (4) data-conditional. MOMRESP represents a typical data-generative model (Bragg et al., 2013; Felt et al., 2014; Lam and Stork, 2005; Simpson and Roberts, 2015). Data-conditional approaches typically model data features conditionally using a log-linear model (Jin and Ghahramani, 2002; Raykar et al., 2010; Liu et al., 2012; Yan et al., 2014). For the purposes of this paper, we refer to this model as LOGRESP. For ITEMRESP, MOMRESP, and LOGRESP we use the variational inference methods presented by Felt et al. (2015). Unlike that paper, in this work we have augmented inference with the in-line hyperparameter updates described in Section 2.4. Figure 5: Inferred label accuracy of models on sentiment-annotated weather tweets. 3.1 Human-generated Annotations To gauge the effectiveness of data-aware crowdsourcing models, we use the sentiment-annotated tweet dataset distributed by CrowdFlower as a part of its “data for everyone” initiativ</context>
</contexts>
<marker>Yan, Rosales, Fung, Subramanian, Dy, 2014</marker>
<rawString>Yan Yan, R´omer Rosales, Glenn Fung, Ramanathan Subramanian, and Jennifer Dy. 2014. Learning from multiple annotators with varying expertise. Machine Learning, 95(3):291–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuchen Zhang</author>
<author>Xi Chen</author>
<author>Dengyong Zhou</author>
<author>Michael I Jordan</author>
</authors>
<title>Spectral methods meet em: A provably optimal algorithm for crowdsourcing.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems 27,</booktitle>
<pages>1260--1268</pages>
<publisher>Curran Associates, Inc.</publisher>
<contexts>
<context position="27746" citStr="Zhang et al., 2014" startWordPosition="4505" endWordPosition="4508">ariables (Zhou et al., 2014), and change in annotator behavior over time (Simpson and Roberts, 2015). Welinder et al. (2010) carefully model the process of annotating objects in images, including variables for item difficulty, item class, and classconditional perception noise. In follow-up work, Liu et al. (2012) demonstrate that similar levels of performance can be achieved with the simple item-response model by using variational inference rather than EM. Alternative inference algorithms have been proposed for crowdsourcing models (Dalvi et al., 2013; Ghosh et al., 2011; Karger et al., 2013; Zhang et al., 2014). Some crowdsourcing work regards labeled data not as an end in itself, but rather as a means to train classifiers (Lin et al., 2014). The fact-finding literature assigns trust scores to assertions made by untrusted sources (Pasternack and Roth, 2010). 5 Conclusion and Future Work We describe CSLDA, a generative, data-aware crowdsourcing model that addresses important modeling deficiencies identified in previous work. In particular, CSLDA handles data in which the natural document clusters are at odds with the intended document labels. It also transitions smoothly from situations in which few </context>
</contexts>
<marker>Zhang, Chen, Zhou, Jordan, 2014</marker>
<rawString>Yuchen Zhang, Xi Chen, Dengyong Zhou, and Michael I. Jordan. 2014. Spectral methods meet em: A provably optimal algorithm for crowdsourcing. In Advances in Neural Information Processing Systems 27, pages 1260–1268. Curran Associates, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dengyong Zhou</author>
<author>Sumit Basu</author>
<author>Yi Mao</author>
<author>John C Platt</author>
</authors>
<title>Learning from the wisdom of crowds by minimax entropy.</title>
<date>2012</date>
<booktitle>In NIPS,</booktitle>
<volume>25</volume>
<pages>2204--2212</pages>
<contexts>
<context position="27029" citStr="Zhou et al., 2012" startWordPosition="4392" endWordPosition="4395">the original behavior of CSLDA is in some ways desirable. That is, if two classes of documents are mostly the same both topically and in terms of annotator decisions, perhaps those classes ought to be collapsed. We are not overly concerned that MOMRESP beats CSLDA in Figure 8, since this result is consistent with early relative performance in simulation. 4 Additional Related Work This section reviews related work not already discussed. A growing body of work extends the itemresponse model to account for variables such as item difficulty (Whitehill et al., 2009; Passonneau and Carpenter, 2013; Zhou et al., 2012), annotator trustworthiness (Hovy et al., 2013), correlation among various combinations of these variables (Zhou et al., 2014), and change in annotator behavior over time (Simpson and Roberts, 2015). Welinder et al. (2010) carefully model the process of annotating objects in images, including variables for item difficulty, item class, and classconditional perception noise. In follow-up work, Liu et al. (2012) demonstrate that similar levels of performance can be achieved with the simple item-response model by using variational inference rather than EM. Alternative inference algorithms have bee</context>
</contexts>
<marker>Zhou, Basu, Mao, Platt, 2012</marker>
<rawString>Dengyong Zhou, Sumit Basu, Yi Mao, and John C. Platt. 2012. Learning from the wisdom of crowds by minimax entropy. In NIPS, volume 25, pages 2204–2212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dengyong Zhou</author>
<author>Qiang Liu</author>
<author>John Platt</author>
<author>Christopher Meek</author>
</authors>
<title>Aggregating ordinal labels from crowds by minimax conditional entropy.</title>
<date>2014</date>
<booktitle>In Proceedings of the International Conference of Machine Learning.</booktitle>
<contexts>
<context position="27155" citStr="Zhou et al., 2014" startWordPosition="4412" endWordPosition="4415">lly and in terms of annotator decisions, perhaps those classes ought to be collapsed. We are not overly concerned that MOMRESP beats CSLDA in Figure 8, since this result is consistent with early relative performance in simulation. 4 Additional Related Work This section reviews related work not already discussed. A growing body of work extends the itemresponse model to account for variables such as item difficulty (Whitehill et al., 2009; Passonneau and Carpenter, 2013; Zhou et al., 2012), annotator trustworthiness (Hovy et al., 2013), correlation among various combinations of these variables (Zhou et al., 2014), and change in annotator behavior over time (Simpson and Roberts, 2015). Welinder et al. (2010) carefully model the process of annotating objects in images, including variables for item difficulty, item class, and classconditional perception noise. In follow-up work, Liu et al. (2012) demonstrate that similar levels of performance can be achieved with the simple item-response model by using variational inference rather than EM. Alternative inference algorithms have been proposed for crowdsourcing models (Dalvi et al., 2013; Ghosh et al., 2011; Karger et al., 2013; Zhang et al., 2014). Some cr</context>
</contexts>
<marker>Zhou, Liu, Platt, Meek, 2014</marker>
<rawString>Dengyong Zhou, Qiang Liu, John Platt, and Christopher Meek. 2014. Aggregating ordinal labels from crowds by minimax conditional entropy. In Proceedings of the International Conference of Machine Learning.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>