<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000010">
<title confidence="0.99547">
Minimum Risk Annealing for Training Log-Linear Models*
</title>
<author confidence="0.979921">
David A. Smith and Jason Eisner
</author>
<affiliation confidence="0.922688">
Department of Computer Science
Center for Language and Speech Processing
Johns Hopkins University
</affiliation>
<address confidence="0.827027">
Baltimore, MD 21218, USA
</address>
<email confidence="0.999263">
{dasmith,eisner}@jhu.edu
</email>
<sectionHeader confidence="0.993917" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999504055555556">
When training the parameters for a natural language system,
one would prefer to minimize 1-best loss (error) on an eval-
uation set. Since the error surface for many natural language
problems is piecewise constant and riddled with local min-
ima, many systems instead optimize log-likelihood, which is
conveniently differentiable and convex. We propose training
instead to minimize the expected loss, or risk. We define this
expectation using a probability distribution over hypotheses
that we gradually sharpen (anneal) to focus on the 1-best hy-
pothesis. Besides the linear loss functions used in previous
work, we also describe techniques for optimizing nonlinear
functions such as precision or the BLEU metric. We present
experiments training log-linear combinations of models for
dependency parsing and for machine translation. In machine
translation, annealed minimum risk training achieves signif-
icant improvements in BLEU over standard minimum error
training. We also show improvements in labeled dependency
parsing.
</bodyText>
<sectionHeader confidence="0.967715" genericHeader="method">
1 Direct Minimization of Error
</sectionHeader>
<bodyText confidence="0.9567971875">
Researchers in empirical natural language pro-
cessing have expended substantial ink and effort in
developing metrics to evaluate systems automati-
cally against gold-standard corpora. The ongoing
evaluation literature is perhaps most obvious in the
machine translation community’s efforts to better
BLEU (Papineni et al., 2002).
Despite this research, parsing or machine trans-
lation systems are often trained using the much
simpler and harsher metric of maximum likeli-
hood. One reason is that in supervised training,
the log-likelihood objective function is generally
convex, meaning that it has a single global max-
imum that can be easily found (indeed, for su-
pervised generative models, the parameters at this
maximum may even have a closed-form solution).
In contrast to the likelihood surface, the error sur-
face for discrete structured prediction is not only
riddled with local minima, but piecewise constant
This work was supported by an NSF graduate research
fellowship for the first author and by NSF ITR grant IIS-
0313193 and ONR grant N00014-01-1-0685. The views ex-
pressed are not necessarily endorsed by the sponsors. We
thank Sanjeev Khudanpur, Noah Smith, Markus Dreyer, and
the reviewers for helpful discussions and comments.
and not everywhere differentiable with respect to
the model parameters (Figure 1). Despite these
difficulties, some work has shown it worthwhile
to minimize error directly (Och, 2003; Bahl et al.,
1988).
We show improvements over previous work on
error minimization by minimizing the risk or ex-
pected error—a continuous function that can be
derived by combining the likelihood with any eval-
uation metric (§2). Seeking to avoid local min-
ima, deterministic annealing (Rose, 1998) gradu-
ally changes the objective function from a convex
entropy surface to the more complex risk surface
(§3). We also discuss regularizing the objective
function to prevent overfitting (§4). We explain
how to compute expected loss under some evalu-
ation metrics common in natural language tasks
(§5). We then apply this machinery to training
log-linear combinations of models for dependency
parsing and for machine translation (§6). Finally,
we note the connections of minimum risk training
to max-margin training and minimum Bayes risk
decoding (§7), and recapitulate our results (§8).
</bodyText>
<sectionHeader confidence="0.928892" genericHeader="method">
2 Training Log-Linear Models
</sectionHeader>
<bodyText confidence="0.9951099375">
In this work, we focus on rescoring with log-
linear models. In particular, our experiments con-
sider log-linear combinations of a relatively small
number of features over entire complex structures,
such as trees or translations, known in some pre-
vious work as products of experts (Hinton, 1999)
or logarithmic opinion pools (Smith et al., 2005).
A feature in the combined model might thus be
a log probability from an entire submodel. Giv-
ing this feature a small or negative weight can
discount a submodel that is foolishly structured,
badly trained, or redundant with the other features.
For each sentence xi in our training corpus S,
we are given Ki possible analyses yi,i, ... yi,K,.
(These may be all of the possible translations or
parse trees; or only the Ki most probable under
</bodyText>
<page confidence="0.951989">
787
</page>
<note confidence="0.969165">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 787–794,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<figureCaption confidence="0.977961">
Figure 1: The loss surface for a machine translation sys-
tem: while other parameters are held constant, we vary the
weights on the distortion and word penalty features. Note the
piecewise constant regions with several local maxima.
</figureCaption>
<bodyText confidence="0.999363">
some other model; or only a random sample of
size Ki.) Each analysis has a vector of real-valued
features (i.e., factors, or experts) denoted fi,k. The
score of the analysis yi,k is θ · fi,k, the dot prod-
uct of its features with a parameter vector θ. For
each sentence, we obtain a normalized probability
distribution over the Ki analyses as
</bodyText>
<equation confidence="0.981298666666667">
exp θ · fi,k
pθ(yi,k  |xi) = EKi
k��1 exp θ · fi,k&apos; (1)
</equation>
<bodyText confidence="0.9998955">
We wish to adjust this model’s parameters θ
to minimize the severity of the errors we make
when using it to choose among analyses. A loss
function Ly*(y) assesses a penalty for choosing
y when y∗ is correct. We will usually write this
simply as L(y) since y∗ is fixed and clear from
context. For clearer exposition, we assume below
that the total loss over some test corpus is the sum
of the losses on individual sentences, although we
will revisit that assumption in §5.
</bodyText>
<subsectionHeader confidence="0.99758">
2.1 Minimizing Loss or Expected Loss
</subsectionHeader>
<bodyText confidence="0.999957666666667">
One training criterion directly mimics test condi-
tions. It looks at the loss incurred if we choose the
best analysis of each xi according to the model:
</bodyText>
<equation confidence="0.996349">
pθ(yi  |xi)) (2)
</equation>
<bodyText confidence="0.99916125">
Since small changes in θ either do not change
the best analysis or else push a different analy-
sis to the top, this objective function is piecewise
constant, hence not amenable to gradient descent.
Och (2003) observed, however, that the piecewise-
constant property could be exploited to character-
ize the function exhaustively along any line in pa-
rameter space, and hence to minimize it globally
along that line. By calling this global line mini-
mization as a subroutine of multidimensional opti-
mization, he was able to minimize (2) well enough
to improve over likelihood maximization for train-
ing factored machine translation systems.
Instead of considering only the best hypothesis
for any θ, we can minimize risk, i.e., the expected
loss under pθ across all analyses yi:
</bodyText>
<equation confidence="0.9412176">
�
mθ de
inEpOL(yi,k) f Bin
i
(3)
</equation>
<bodyText confidence="0.999165166666667">
This “smoothed” objective is now continuous and
differentiable. However, it no longer exactly mim-
ics test conditions, and it typically remains non-
convex, so that gradient descent is still not guaran-
teed to find a global minimum. Och (2003) found
that such smoothing during training “gives almost
identical results” on translation metrics.
The simplest possible loss function is 0/1 loss,
where L(y) is 0 if y is the true analysis y∗i and
1 otherwise. This loss function does not at-
tempt to give partial credit. Even in this sim-
ple case, assuming P =6 NP, there exists no gen-
eral polynomial-time algorithm for even approx-
imating (2) to within any constant factor, even
for Ki = 2 (Hoffgen et al., 1995, from Theo-
rem 4.10.4).1 The same is true for for (3), since
for Ki = 2 it can be easily shown that the min 0/1
risk is between 50% and 100% of the min 0/1 loss.
</bodyText>
<subsectionHeader confidence="0.999909">
2.2 Maximizing Likelihood
</subsectionHeader>
<bodyText confidence="0.9996966">
Rather than minimizing a loss function suited to
the task, many systems (especially for language
modeling) choose simply to maximize the prob-
ability of the gold standard. The log of this likeli-
hood is a convex function of the parameters θ:
</bodyText>
<equation confidence="0.886787">
log pθ(y∗i  |xi) (4)
</equation>
<bodyText confidence="0.92664725">
where y∗i is the true analysis of sentence xi. The
only wrinkle is that pθ(y∗i  |xi) may be left unde-
fined by equation (1) if y∗i is not in our set of Ki
hypotheses. When maximizing likelihood, there-
fore, we will replace y∗i with the min-loss analy-
sis in the hypothesis set; if multiple analyses tie
1Known algorithms are exponential but only in the dimen-
sionality of the feature space (Johnson and Preparata, 1978).
</bodyText>
<figure confidence="0.950369">
� L(argmax
min yi
θ
i
� L(yi,k)pθ(yi,k  |xi)
k
�
max
θ
i
788
Bleu %
Translation model 1
</figure>
<figureCaption confidence="0.98741225">
Figure 2: Loss and expected loss as one translation model’s
weight varies: the gray line (-y = oo) shows true BLEU (to be
optimized in equation (2)). The black lines show the expected
BLEU as -y in equation (5) increases from 0.1 toward oo.
</figureCaption>
<bodyText confidence="0.999134666666667">
for this honor, we follow Charniak and Johnson
(2005) in summing their probabilities.2
Maximizing (4) is equivalent to minimizing an
upper bound on the expected 0/1 loss Ei(1 −
pθ(yi  |xi)). Though the log makes it tractable,
this remains a 0/1 objective that does not give par-
tial credit to wrong answers, such as imperfect but
useful translations. Most systems should be eval-
uated and preferably trained on less harsh metrics.
</bodyText>
<sectionHeader confidence="0.988669" genericHeader="method">
3 Deterministic Annealing
</sectionHeader>
<bodyText confidence="0.999889285714286">
To balance the advantages of direct loss minimiza-
tion, continuous risk minimization, and convex
optimization, deterministic annealing attempts
the solution of increasingly difficult optimization
problems (Rose, 1998). Adding a scale hyperpa-
rameter γ to equation (1), we have the following
family of distributions:
</bodyText>
<equation confidence="0.952484">
pγ,θ(yi,k  |xi) =K(expθ · fi,k)γ γ (5)
Ek&apos;- 1 (exp θ · fi k&apos;)
</equation>
<bodyText confidence="0.997837333333333">
When γ = 0, all yi,k are equally likely, giving
the uniform distribution; when γ = 1, we recover
the model in equation (1); and as γ —* oc, we
approach the winner-take-all Viterbi function that
assigns probability 1 to the top-scoring analysis.
For a fixed γ, deterministic annealing solves
</bodyText>
<equation confidence="0.6268315">
min Ep-Y,e[L(yi,k)] (6)
θ
</equation>
<bodyText confidence="0.983933358974359">
2An alternative would be to artificially add yz (e.g., the
reference translation(s)) to the hypothesis set during training.
We then increase γ according to some schedule
and optimize θ again. When γ is low, the smooth
objective might allow us to pass over local min-
ima that could open up at higher γ. Figure 3 shows
how the smoothing is gradually weakened to reach
the risk objective (3) as γ —* 1 and approach the
true error objective (2) as γ —* oc.
Our risk minimization most resembles the work
of Rao and Rose (2001), who trained an isolated-
word speech recognition system for expected
word-error rate. Deterministic annealing has also
been used to tackle non-convex likelihood sur-
faces in unsupervised learning with EM (Ueda and
Nakano, 1998; Smith and Eisner, 2004). Other
work on “generalized probabilistic descent” mini-
mizes a similar objective function but with γ held
constant (Katagiri et al., 1998).
Although the entropy is generally higher at
lower values of γ, it varies as the optimization
changes θ. In particular, a pure unregularized log-
linear model such as (5) is really a function of γ·θ,
so the optimizer could exactly compensate for in-
creased γ by decreasing the θ vector proportion-
ately!3 Most deterministic annealing procedures,
therefore, express a direct preference on the en-
tropy H, and choose γ and θ accordingly:
min Ep-Y,e[L(yi,k)] − T · H(pγ,θ) (7)
γ,θ
In place of a schedule for raising γ, we now use
a cooling schedule to lower T from oc to −oc,
thereby weakening the preference for high en-
tropy. The Lagrange multiplier T on entropy is
called “temperature” due to a satisfying connec-
tion to statistical mechanics. Once T is quite cool,
it is common in practice to switch to raising γ di-
rectly and rapidly (quenching) until some conver-
gence criterion is met (Rao and Rose, 2001).
</bodyText>
<sectionHeader confidence="0.996984" genericHeader="method">
4 Regularization
</sectionHeader>
<bodyText confidence="0.998032222222222">
Informally, high temperature or γ &lt; 1 smooths
our model during training toward higher-entropy
conditional distributions that are not so peaked at
the desired analyses y* . Another reason for such
smoothing is simply to prevent overfitting to these
training examples.
A typical way to control overfitting is to use a
quadratic regularizing term, ||θ||2 or more gener-
ally Ed θ2d/2σ2d. Keeping this small keeps weights
</bodyText>
<footnote confidence="0.8838765">
3For such models, -y merely aids the nonlinear optimizer
in its search, by making it easier to scale all of 0 at once.
</footnote>
<equation confidence="0.997485666666667">
−10 −5 0 5 10
17.5 18.0 18.5 19.0
γ = ∞
γ = 0.1
γ = 1
γ = 10
</equation>
<page confidence="0.98255">
789
</page>
<bodyText confidence="0.999904875">
low and entropy high. We may add this regularizer
to equation (6) or (7). In the maximum likelihood
framework, we may subtract it from equation (4),
which is equivalent to maximum a posteriori esti-
mation with a diagonal Gaussian prior (Chen and
Rosenfeld, 1999). The variance a2d may reflect a
prior belief about the potential usefulness of fea-
ture d, or may be tuned on heldout data.
Another simple regularization method is to stop
cooling before T reaches 0 (cf. Elidan and Fried-
man (2005)). If loss on heldout data begins to
increase, we may be starting to overfit. This
technique can be used along with annealing or
quadratic regularization and can achieve addi-
tional accuracy gains, which we report elsewhere
(Dreyer et al., 2006).
</bodyText>
<sectionHeader confidence="0.980233" genericHeader="method">
5 Computing Expected Loss
</sectionHeader>
<bodyText confidence="0.999922967741936">
At each temperature setting of deterministic an-
nealing, we need to minimize the expected loss on
the training corpus. We now discuss how this ex-
pectation is computed. When rescoring, we as-
sume that we simply wish to combine, in some
way, statistics of whole sentences4 to arrive at the
overall loss for the corpus. We consider evalua-
tion metrics for natural language tasks from two
broadly applicable classes: linear and nonlinear.
A linear metric is a sum (or other linear combi-
nation) of the loss or gain on individual sentences.
Accuracy—in dependency parsing, part-of-speech
tagging, and other labeling tasks—falls into this
class, as do recall, word error rate in ASR, and
the crossing-brackets metric in parsing. Thanks to
the linearity of expectation, we can easily compute
our expected loss in equation (6) by adding up the
expected loss on each sentence.
Some other metrics involve nonlinear combi-
nations over the sentences of the corpus. One
common example is precision, P def = Pi ci/Pi ai,
where ci is the number of correctly posited ele-
ments, and ai is the total number of posited ele-
ments, in the decoding of sentence i. (Depend-
ing on the task, the elements may be words, bi-
grams, labeled constituents, etc.) Our goal is to
maximize P, so during a step of deterministic an-
nealing, we need to maximize the expectation of
P when the sentences are decoded randomly ac-
cording to equation (5). Although this expectation
is continuous and differentiable as a function of
</bodyText>
<footnote confidence="0.671469333333333">
4Computing sentence xi’s statistics usually involves iter-
ating over hypotheses yi,i, ... yi,Ki. If these share substruc-
ture in a hypothesis lattice, dynamic programming may help.
</footnote>
<bodyText confidence="0.997290833333333">
0, unfortunately it seems hard to compute for any
given 0. We observe however that an equivalent
goal is to minimize − log P. Taking that as our
loss function instead, equation (6) now needs to
minimize the expectation of − log P,5 which de-
composes somewhat more nicely:
</bodyText>
<equation confidence="0.99385">
E[− log P] = E[log X Xai − log ci]
i i
</equation>
<bodyText confidence="0.995454125">
= E[log A] − E[log C] (8)
where the integer random variables A = Pi ai
and C = Pi ci count the number of posited and
correctly posited elements over the whole corpus.
To approximate E[g(A)], where g is any twice-
differentiable function (here g = log), we can ap-
proximate g locally by a quadratic, given by the
Taylor expansion of g about A’s mean µA = E[A]:
</bodyText>
<equation confidence="0.993028375">
E[g(A)] ,:; E[g(µA) + (A − µA)g0(µA)
+2(A − µA)2g00(µA)]
1
=g(µA) + E[A − µA]g0(µA)
+2E[(A − µA)2]g00(µA)
1
=g(µA) + 2σ2
1Ag00(µA)�
</equation>
<bodyText confidence="0.982664083333333">
Here µA = Pi µai and Q2A = Pi Q2ai, since A
is a sum of independent random variables ai (i.e.,
given the current model parameters 0, our ran-
domized decoder decodes each sentence indepen-
dently). In other words, given our quadratic ap-
proximation to g, E[g(A)] depends on the (true)
distribution of A only through the single-sentence
means µai and variances a2ai, which can be found
by enumerating the Ki decodings of sentence i.
The approximation becomes arbitrarily good as
we anneal -y —* oc, since then Q2A —* 0 and
E[g(A)] focuses on g near µA. For equation (8),
</bodyText>
<equation confidence="0.968343666666667">
2
A
E[g(A)] = E[log A] ,:; log(µA) − 2µ2 A
</equation>
<bodyText confidence="0.9995744">
and E[log C] is found similarly.
Similar techniques can be used to compute the
expected logarithms of some other non-linear met-
rics, such as F-measure (the harmonic mean of
precision and recall)6 and Papineni et al. (2002)’s
</bodyText>
<footnote confidence="0.977152">
5This changes the trajectory that DA takes through pa-
rameter space, but ultimately the objective is the same: as
-y --+ oo over the course of DA, minimizing E[− log P] be-
comes indistinguishable from maximizing E[P].
6R def = C/B; the count B of correct elements is known.
def
</footnote>
<equation confidence="0.5352335">
So log F = log 2PR/(P + R) = log 2R/(1 + R/P) =
log 2C/B − log(1 + A/B). Consider g(x) = log 1 + x/B.
</equation>
<page confidence="0.93708">
790
</page>
<bodyText confidence="0.987926666666667">
BLEU translation metric (the geometric mean of
several precisions). In particular, the expectation
of log BLEU distributes over its N + 1 summands:
</bodyText>
<equation confidence="0.890972666666667">
r
log BLEU = min(1 − ,0) +
A1
</equation>
<bodyText confidence="0.999971782608696">
where Pn is the precision of the n-gram elements
in the decoding.7 As is standard in MT research,
we take wn = 1/N and N = 4. The first term in
the BLEU score is the log brevity penalty, a con-
tinuous function of A1 (the total number of uni-
gram tokens in the decoded corpus) that fires only
if A1 &lt; r (the average word count of the reference
corpus). We again use a Taylor series to approxi-
mate the expected log brevity penalty.
We mention an alternative way to compute (say)
the expected precision C/A: integrate numerically
over the joint density of C and A. How can we
obtain this density? As (C, A) = Ei(ci, ai) is a
sum of independent random length-2 vectors, its
mean vector and 2 x 2 covariance matrix can be
respectively found by summing the means and co-
variance matrices of the (ci, ai), each exactly com-
puted from the distribution (5) over Ki hypothe-
ses. We can easily approximate (C, A) by the
(continuous) bivariate normal with that mean and
covariance matrix8—or else accumulate an exact
representation of its (discrete) probability mass
function by a sequence of numerical convolutions.
</bodyText>
<sectionHeader confidence="0.999618" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999929545454545">
We tested the above training methods on two
different tasks: dependency parsing and phrase-
based machine translation. Since the basic setup
was the same for both, we outline it here before
describing the tasks in detail.
In both cases, we start with 8 to 10 models
(the “experts”) already trained on separate training
data. To find the optimal coefficients 0 for a log-
linear combination of these experts, we use sepa-
rate development data, using the following proce-
dure due to Och (2003):
</bodyText>
<listItem confidence="0.906377666666667">
1. Initialization: Initialize 0 to the 0 vector. For
each development sentence xi, set its Ki-best
list to 0 (thus Ki = 0).
</listItem>
<footnote confidence="0.786107833333333">
7BLEU is careful when measuring ci on a particular de-
coding yi,k. It only counts the first two copies of the (e.g.) as
correct if the occurs at most twice in any reference translation
of xi. This “clipping” does not affect the rest of our method.
8Reasonable for a large corpus, by Lyapunov’s central
limit theorem (allows non-identically distributed summands).
</footnote>
<listItem confidence="0.998261153846154">
2. Decoding: For each development sentence
xi, use the current 0 to extract the 200 anal-
yses yi,k with the greatest scores exp 0 · fi,k.
Calcuate each analysis’s loss statistics (e.g.,
ci and ai), and add it to the Ki-best list if it is
not already there.
3. Convergence: If Ki has not increased for
any development sentence, or if we have
reached our limit of 20 iterations, stop: the
search has converged.
4. Optimization: Adjust 0 to improve our ob-
jective function over the whole development
corpus. Return to step 2.
</listItem>
<bodyText confidence="0.540594">
Our experiments simply compare three proce-
dures at step 4. We may either
</bodyText>
<listItem confidence="0.891278416666667">
• maximize log-likelihood (4), a convex func-
tion, at a given level of quadratic regulariza-
tion, by BFGS gradient descent;
• minimize error (2) by Och’s line search
method, which globally optimizes each com-
ponent of 0 while holding the others con-
stant;9 or
• minimize the same error (2) more effectively,
by raising -y —* oc while minimizing the an-
nealed risk (6), that is, cooling T —* −oc (or
-y —* oc) and at each value, locally minimiz-
ing equation (7) using BFGS.
</listItem>
<bodyText confidence="0.999925571428571">
Since these different optimization procedures
will usually find different 0 at step 4, their K-best
lists will diverge after the first iteration.
For final testing, we selected among several
variants of each procedure using a separate small
heldout set. Final results are reported for a larger,
disjoint test set.
</bodyText>
<subsectionHeader confidence="0.993026">
6.1 Machine Translation
</subsectionHeader>
<bodyText confidence="0.999923625">
For our machine translation experiments, we
trained phrase-based alignment template models
of Finnish-English, French-English, and German-
English, as follows. For each language pair, we
aligned 100,000 sentence pairs from European
Parliament transcripts using GIZA++. We then
used Philip Koehn’s phrase extraction software
to merge the GIZA++ alignments and to extract
</bodyText>
<footnote confidence="0.9906025">
9The component whose optimization achieved the lowest
loss is then updated. The process iterates until no lower loss
can be found. In contrast, Papineni (1999) proposed a linear
programming method that may search along diagonal lines.
</footnote>
<equation confidence="0.946680666666667">
N
wn log Pn
n=1
</equation>
<page confidence="0.982784">
791
</page>
<bodyText confidence="0.975155">
and score the alignment template model’s phrases
(Koehn et al., 2003).
The Pharaoh phrase-based decoder uses pre-
cisely the setup of this paper. It scores a candidate
translation (including its phrasal alignment to the
original text) as 0 • f, where f is a vector of the
following 8 features:
</bodyText>
<listItem confidence="0.9643656875">
1. the probability of the source phrase given the
target phrase
2. the probability of the target phrase given the
source phrase
3. the weighted lexical probability of the source
words given the target words
4. the weighted lexical probability of the target
words given the source words
5. a phrase penalty that fires for each template
in the translation
6. a distortion penalty that fires when phrases
translate out of order
7. a word penalty that fires for each English
word in the output
8. a trigram language model estimated on the
English side of the bitext
</listItem>
<bodyText confidence="0.999984666666667">
Our goal was to train the weights 0 of these 8
features. We used the method described above,
employing the Pharaoh decoder at step 2 to gener-
ate the 200-best translations according to the cur-
rent 0. As explained above, we compared three
procedures at step 4: maximum log-likelihood by
gradient ascent; minimum error using Och’s line-
search method; and annealed minimum risk. As
our development data for training 0, we used 200
sentence pairs for each language pair.
Since our methods can be tuned with hyperpa-
rameters, we used performance on a separate 200-
sentence held-out set to choose the best hyper-
parameter values. The hyperparameter levels for
each method were
</bodyText>
<listItem confidence="0.9858525">
• maximum likelihood: a Gaussian prior with
all ad at 0.25, 0.5, 1, or oc
• minimum error: 1, 5, or 10 different ran-
dom starting points, drawn from a uniform
</listItem>
<table confidence="0.9994408">
Optimization Finnish- French- German-
Procedure English English English
Max. like. 5.02 5.31 7.43
Min. error 10.27 26.16 20.94
Ann. min. risk 16.43 27.31 21.30
</table>
<tableCaption confidence="0.674142714285714">
Table 1: BLEU 4n1 percentage on translating 2000-
sentence test corpora, after training the 8 experts on 100,000
sentence pairs and fitting their weights 0 on 200 more, using
settings tuned on a further 200. The current minimum risk an-
nealing method achieved significant improvements over min-
imum error and maximum likelihood at or below the 0.001
level, using a permutation test with 1000 replications.
</tableCaption>
<bodyText confidence="0.9937275">
distribution on [−1, 1] x [−1, 1] x • • • , when
optimizing 0 at an iteration of step 4.10
</bodyText>
<listItem confidence="0.97682625">
• annealed minimum risk: with explicit en-
tropy constraints, starting temperature T E
1100, 200,1000}; stopping temperature T E
10.01, 0.0011. The temperature was cooled
</listItem>
<bodyText confidence="0.945226875">
by half at each step; then we quenched by
doubling -y at each step. (We also ran exper-
iments with quadratic regularization with all
Qd at 0.5, 1, or 2 (§4) in addition to the en-
tropy constraint. Also, instead of the entropy
constraint, we simply annealed on -y while
adding a quadratic regularization term. None
of these regularized models beat the best set-
ting of standard deterministic annealing on
heldout or test data.)
Final results on a separate 2000-sentence test set
are shown in table 1. We evaluated translation us-
ing BLEU with one reference translation and n-
grams up to 4. The minimum risk annealing pro-
cedure significantly outperformed maximum like-
lihood and minimum error training in all three lan-
guage pairs (p &lt; 0.001, paired-sample permuta-
tion test with 1000 replications).
Minimum risk annealing generally outper-
formed minimum error training on the held-out
set, regardless of the starting temperature T. How-
ever, higher starting temperatures do give better
performance and a more monotonic learning curve
(Figure 3), a pattern that held up on test data.
(In the same way, for minimum error training,
10That is, we run step 4 from several starting points, finish-
ing at several different points; we pick the finishing point with
lowest development error (2). This reduces the sensitivity of
this method to the starting value of 0. Maximum likelihood
is not sensitive to the starting value of 0 because it has only a
global optimum; annealed minimum risk is not sensitive to it
either, because initially -y Pz� 0, making equation (6) flat.
</bodyText>
<page confidence="0.988031">
792
</page>
<figure confidence="0.9823185">
Bleu
Iteration
</figure>
<figureCaption confidence="0.9918615">
Figure 3: Iterative change in BLEU on German-English de-
velopment (upper) and held-out (lower), under annealed min-
imum risk training with different starting temperatures, ver-
sus minimum error training with 10 random restarts.
</figureCaption>
<figure confidence="0.851677">
Iteration
</figure>
<figureCaption confidence="0.99981">
Figure 4: Iterative change in BLEU on German-English
development (upper) and held-out (lower), using 10 random
restarts vs. only 1.
</figureCaption>
<bodyText confidence="0.9995616">
more random restarts give better performance and
a more monotonic learning curve—see Figure 4.)
Minimum risk annealing did not always win on
the training set, suggesting that its advantage is
not superior minimization but rather superior gen-
eralization: under the risk criterion, multiple low-
loss hypotheses per sentence can help guide the
learner to the right part of parameter space.
Although the components of the translation and
language models interact in complex ways, the im-
provement on Finnish-English may be due in part
to the higher weight that minimum risk annealing
found for the word penalty. That system is there-
fore more likely to produce shorter output like i
have taken note of your remarks and i also agree
with that . than like this longer output from the
minimum-error-trained system: i have taken note
ofyour remarks and i shall also agree with all that
the union.
We annealed using our novel expected-BLEU
approximation from §5. We found this to perform
significantly better on BLEU evaluation than if we
trained with a “linearized” BLEU that summed
per-sentence BLEU scores (as used in minimum
Bayes risk decoding by Kumar and Byrne (2004)).
</bodyText>
<subsectionHeader confidence="0.996628">
6.2 Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.999732628571428">
We trained dependency parsers for three different
languages: Bulgarian, Dutch, and Slovenian.11 In-
put sentences to the parser were already tagged for
parts of speech. Each parser employed 10 experts,
each parameterized as a globally normalized log-
linear model (Lafferty et al., 2001). For example,
the 9th component of the feature vector fz�k (which
described the kth parse of the ith sentence) was the
log of that parse’s normalized probability accord-
ing to the 9th expert.
Each expert was trained separately to maximize
the conditional probability of the correct parse
given the sentence. We used 10 iterations of gradi-
ent ascent. To speed training, for each of the first
9 iterations, the gradient was estimated on a (dif-
ferent) sample of only 1000 training sentences.
We then trained the vector 0, used to combine
the experts, to minimize the number of labeled de-
pendency attachment errors on a 200-sentence de-
velopment set. Optimization proceeded over lists
of the 200-best parses of each sentence produced
by a joint decoder using the 10 experts.
Evaluating on labeled dependency accuracy on
200 test sentences for each language, we see that
minimum error and annealed minimum risk train-
ing are much closer than for MT. For Bulgarian
and Dutch, they are statistically indistinguishable
using a paired-sample permutations test with 1000
replications. Indeed, on Dutch, all three opti-
mization procedures produce indistinguishable re-
sults. On Slovenian, annealed minimum risk train-
ing does show a significant improvement over the
other two methods. Overall, however, the results
for this task are mediocre. We are still working on
improving the underlying experts.
</bodyText>
<sectionHeader confidence="0.999965" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.809067428571429">
We have seen that annealed minimum risk train-
ing provides a useful alternative to maximum like-
lihood and minimum error training. In our ex-
periments, it never performed significantly worse
11For information on these corpora, see the CoNLL-X
shared task on multilingual dependency parsing: http:
//nextens.uvt.nl/~conll/.
</bodyText>
<figure confidence="0.999471909090909">
5 10 15 20
16 18 20 22
T=1000
T=200
T=100
Min. error
5 10 15 20
Bleu
5 10 15 20
10 restarts
1 restart
</figure>
<page confidence="0.994104">
793
</page>
<table confidence="0.9996636">
Optimization labeled dependency acc. [%]
Procedure Slovenian Bulgarian Dutch
Max. like. 27.78 47.23 36.78
Min. error 22.52 54.72 36.78
Ann. min. risk 31.16 54.66 36.71
</table>
<tableCaption confidence="0.658573555555556">
Table 2: Labeled dependency accuracy on parsing 200-
sentence test corpora, after training 10 experts on 1000 sen-
tences and fitting their weights 0 on 200 more. For Slove-
nian, minimum risk annealing is significantly better than the
other training methods, while minimum error is significantly
worse. For Bulgarian, both minimum error and annealed min-
imum risk training achieve significant gains over maximum
likelihood, but are indistinguishable from each other. For
Dutch, the three methods are indistinguishable.
</tableCaption>
<bodyText confidence="0.999667652173913">
than either and in some cases significantly helped.
Note, however, that annealed minimum risk train-
ing results in a deterministic classifier just as these
other training procedures do. The orthogonal
technique of minimum Bayes risk decoding has
achieved gains on parsing (Goodman, 1996) and
machine translation (Kumar and Byrne, 2004). In
speech recognition, researchers have improved de-
coding by smoothing probability estimates numer-
ically on heldout data in a manner reminiscent of
annealing (Goel and Byrne, 2000). We are inter-
ested in applying our techniques for approximat-
ing nonlinear loss functions to MBR by perform-
ing the risk minimization inside the dynamic pro-
gramming or other decoder.
Another training approach that incorporates ar-
bitrary loss functions is found in the structured
prediction literature in the margin-based-learning
community (Taskar et al., 2004; Crammer et al.,
2004). Like other max-margin techniques, these
attempt to make the best hypothesis far away from
the inferior ones. The distinction is in using a loss
function to calculate the required margins.
</bodyText>
<sectionHeader confidence="0.999353" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.988893428571429">
Despite the challenging shape of the error sur-
face, we have seen that it is practical to opti-
mize task-specific error measures rather than op-
timizing likelihood—it produces lower-error sys-
tems. Different methods can be used to attempt
this global, non-convex optimization. We showed
that for MT, and sometimes for dependency pars-
ing, an annealed minimum risk approach to opti-
mization performs significantly better than a pre-
vious line-search method that does not smooth the
error surface. It never does significantly worse.
With such improved methods for minimizing er-
ror, we can hope to make better use of task-specific
training criteria in NLP.
</bodyText>
<sectionHeader confidence="0.958566" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999966421875">
L. R. Bahl, P. F. Brown, P. V. de Souza, and R. L. Mer-
cer. 1988. A new algorithm for the estimation of hidden
Markov model parameters. In ICASSP, pages 493–496.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best
parsing and maxent discriminative reranking. In ACL,
pages 173–180.
S. F. Chen and R. Rosenfeld. 1999. A gaussian prior for
smoothing maximum entropy models. Technical report,
CS Dept., Carnegie Mellon University.
K. Crammer, R. McDonald, and F. Pereira. 2004. New large
margin algorithms for structured prediction. In Learning
with Structured Outputs (NIPS).
M. Dreyer, D. A. Smith, and N. A. Smith. 2006. Vine parsing
and minimum risk reranking for speed and precision. In
CoNLL.
G. Elidan and N. Friedman. 2005. Learning hidden variable
networks: The information bottleneck approach. JMLR,
6:81–127.
V. Goel and W. J. Byrne. 2000. Minimum Bayes-Risk au-
tomatic speech recognition. Computer Speech and Lan-
guage, 14(2):115–135.
J. T. Goodman. 1996. Parsing algorithms and metrics. In
ACL, pages 177–183.
G. Hinton. 1999. Products of experts. In Proc. of ICANN,
volume 1, pages 1–6.
K.-U. Hoffgen, H.-U. Simon, and K. S. Van Horn. 1995.
Robust trainability of single neurons. J. of Computer and
System Sciences, 50(1):114–125.
D. S. Johnson and F. P. Preparata. 1978. The densest hemi-
sphere problem. Theoretical Comp. Sci., 6(93–107).
S. Katagiri, B.-H. Juang, and C.-H. Lee. 1998. Pattern recog-
nition using a family of design algorithms based upon the
generalized probabilistic descent method. Proc. IEEE,
86(11):2345–2373, November.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In HLT-NAACL, pages 48–54.
S. Kumar and W. Byrne. 2004. Minimum bayes-risk decod-
ing for statistical machine translation. In HLT-NAACL.
J. Lafferty, A. McCallum, and F. C. N. Pereira. 2001. Condi-
tional random fields: Probabilistic models for segmenting
and labeling sequence data. In ICML.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In ACL, pages 160–167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: A method for automatic evaluation of machine
translation. In ACL, pages 311–318.
K. A. Papineni. 1999. Discriminative training via linear
programming. In ICASSP.
A. Rao and K. Rose. 2001. Deterministically annealed de-
sign of Hidden Markov Model speech recognizers. IEEE
Trans. on Speech and Audio Processing, 9(2):111–126.
K. Rose. 1998. Deterministic annealing for clustering, com-
pression, classification, regression, and related optimiza-
tion problems. Proc. IEEE, 86(11):2210–2239.
N. A. Smith and J. Eisner. 2004. Annealing techniques for
unsupervised statistical language learning. In ACL, pages
486–493.
A. Smith, T. Cohn, and M. Osborne. 2005. Logarithmic
opinion pools for conditional random fields. In ACL, pages
18–25.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Manning.
2004. Max-margin parsing. In EMNLP, pages 1–8.
N. Ueda and R. Nakano. 1998. Deterministic annealing EM
algorithm. Neural Networks, 11(2):271–282.
</reference>
<page confidence="0.998378">
794
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000019">
<title confidence="0.994004">Risk Annealing for Training Log-Linear</title>
<author confidence="0.999618">A Smith Eisner</author>
<affiliation confidence="0.998491666666667">Department of Computer Science Center for Language and Speech Processing Johns Hopkins University</affiliation>
<address confidence="0.999957">Baltimore, MD 21218, USA</address>
<abstract confidence="0.996319103937009">When training the parameters for a natural language system, would prefer to minimize 1-best on an evaluation set. Since the error surface for many natural language problems is piecewise constant and riddled with local minima, many systems instead optimize log-likelihood, which is conveniently differentiable and convex. We propose training to minimize the or We define this expectation using a probability distribution over hypotheses that we gradually sharpen (anneal) to focus on the 1-best hy- Besides the functions used in previous we also describe techniques for optimizing such as precision or the We present experiments training log-linear combinations of models for dependency parsing and for machine translation. In machine translation, annealed minimum risk training achieves signifimprovements in standard minimum error training. We also show improvements in labeled dependency parsing. 1 Direct Minimization of Error Researchers in empirical natural language processing have expended substantial ink and effort in developing metrics to evaluate systems automatically against gold-standard corpora. The ongoing evaluation literature is perhaps most obvious in the machine translation community’s efforts to better et al., 2002). Despite this research, parsing or machine translation systems are often trained using the much simpler and harsher metric of maximum likelihood. One reason is that in supervised training, the log-likelihood objective function is generally convex, meaning that it has a single global maximum that can be easily found (indeed, for supervised generative models, the parameters at this maximum may even have a closed-form solution). contrast to the likelihood surface, the surface for discrete structured prediction is not only riddled with local minima, but piecewise constant work was supported by an NSF graduate research fellowship for the first author and by NSF ITR grant IIS- 0313193 and ONR grant N00014-01-1-0685. The views expressed are not necessarily endorsed by the sponsors. We thank Sanjeev Khudanpur, Noah Smith, Markus Dreyer, and the reviewers for helpful discussions and comments. and not everywhere differentiable with respect to the model parameters (Figure 1). Despite these difficulties, some work has shown it worthwhile to minimize error directly (Och, 2003; Bahl et al., 1988). We show improvements over previous work on minimization by minimizing the excontinuous function that can be derived by combining the likelihood with any evalmetric Seeking to avoid local minima, deterministic annealing (Rose, 1998) gradually changes the objective function from a convex entropy surface to the more complex risk surface We also discuss regularizing the objective to prevent overfitting We explain how to compute expected loss under some evaluation metrics common in natural language tasks We then apply this machinery to training log-linear combinations of models for dependency and for machine translation Finally, note the connections of minimum risk to max-margin training and minimum Bayes risk and recapitulate our results 2 Training Log-Linear Models In this work, we focus on rescoring with loglinear models. In particular, our experiments consider log-linear combinations of a relatively small number of features over entire complex structures, such as trees or translations, known in some prework as of experts 1999) opinion pools et al., 2005). A feature in the combined model might thus be a log probability from an entire submodel. Giving this feature a small or negative weight can discount a submodel that is foolishly structured, badly trained, or redundant with the other features. each sentence our training corpus are given analyses ... (These may be all of the possible translations or trees; or only the probable under 787 of the COLING/ACL 2006 Main Conference Poster pages 787–794, July 2006. Association for Computational Linguistics 1: loss surface for a machine translation system: while other parameters are held constant, we vary the weights on the distortion and word penalty features. Note the piecewise constant regions with several local maxima. some other model; or only a random sample of Each analysis has a vector of real-valued (i.e., factors, or experts) denoted The of the analysis the dot prodof its features with a parameter vector For each sentence, we obtain a normalized probability over the as = wish to adjust this model’s parameters to minimize the severity of the errors we make using it to choose among analyses. A a penalty for choosing is correct. We will usually write this as is fixed and clear from context. For clearer exposition, we assume below that the total loss over some test corpus is the sum of the losses on individual sentences, although we revisit that assumption in 2.1 Minimizing Loss or Expected Loss One training criterion directly mimics test conditions. It looks at the loss incurred if we choose the analysis of each to the model: small changes in do not change the best analysis or else push a different analysis to the top, this objective function is piecewise constant, hence not amenable to gradient descent. Och (2003) observed, however, that the piecewiseconstant property could be exploited to characterize the function exhaustively along any line in parameter space, and hence to minimize it globally along that line. By calling this global line minimization as a subroutine of multidimensional optimization, he was able to minimize (2) well enough to improve over likelihood maximization for training factored machine translation systems. Instead of considering only the best hypothesis any we can minimize i.e., the all analyses � i (3) This “smoothed” objective is now continuous and differentiable. However, it no longer exactly mimics test conditions, and it typically remains nonconvex, so that gradient descent is still not guaranteed to find a global minimum. Och (2003) found that such smoothing during training “gives almost identical results” on translation metrics. The simplest possible loss function is 0/1 loss, 0 if the true analysis and 1 otherwise. This loss function does not attempt to give partial credit. Even in this simcase, assuming P there exists no general polynomial-time algorithm for even approximating (2) to within any constant factor, even 2 et al., 1995, from Theo- The same is true for for (3), since 2 can be easily shown that the min 0/1 risk is between 50% and 100% of the min 0/1 loss. 2.2 Maximizing Likelihood Rather than minimizing a loss function suited to the task, many systems (especially for language modeling) choose simply to maximize the probof the gold standard. The log of this likelia convex function of the parameters | is the true analysis of sentence The wrinkle is that  |be left undeby equation (1) if is not in our set of hypotheses. When maximizing likelihood, therewe will replace with the min-loss analysis in the hypothesis set; if multiple analyses tie algorithms are exponential but only in the dimensionality of the feature space (Johnson and Preparata, 1978). � min θ i k � max θ i 788 Bleu % Translation model 1 2: and expected loss as one translation model’s varies: the gray line shows true be optimized in equation (2)). The black lines show the expected equation (5) increases from 0.1 toward for this honor, we follow Charniak and Johnson in summing their Maximizing (4) is equivalent to minimizing an bound on the expected 0/1 loss − the it tractable, this remains a 0/1 objective that does not give partial credit to wrong answers, such as imperfect but useful translations. Most systems should be evaluated and preferably trained on less harsh metrics. 3 Deterministic Annealing To balance the advantages of direct loss minimization, continuous risk minimization, and convex annealing the solution of increasingly difficult optimization problems (Rose, 1998). Adding a scale hyperpaequation (1), we have the following family of distributions: 0, equally likely, giving uniform distribution; when 1, recover model in equation (1); and as we approach the winner-take-all Viterbi function that assigns probability 1 to the top-scoring analysis. a fixed deterministic annealing solves θ alternative would be to artificially add the reference translation(s)) to the hypothesis set during training. then increase to some optimize When low, the smooth objective might allow us to pass over local minthat could open up at higher Figure 3 shows how the smoothing is gradually weakened to reach risk objective (3) as approach the error objective (2) as Our risk minimization most resembles the work of Rao and Rose (2001), who trained an isolatedword speech recognition system for expected word-error rate. Deterministic annealing has also been used to tackle non-convex likelihood surfaces in unsupervised learning with EM (Ueda and Nakano, 1998; Smith and Eisner, 2004). Other work on “generalized probabilistic descent” minia similar objective function but with constant (Katagiri et al., 1998). Although the entropy is generally higher at values of it varies as the optimization In particular, a pure unregularized logmodel such as (5) is really a function of so the optimizer could exactly compensate for indecreasing the proportiondeterministic annealing procedures, therefore, express a direct preference on the enand choose min γ,θ − place of a schedule for raising we now use to lower thereby weakening the preference for high en- The Lagrange multiplier entropy is called “temperature” due to a satisfying connecto statistical mechanics. Once quite cool, is common in practice to switch to raising diand rapidly until some convergence criterion is met (Rao and Rose, 2001). 4 Regularization high temperature or &lt; our model during training toward higher-entropy conditional distributions that are not so peaked at desired analyses Another reason for such smoothing is simply to prevent overfitting to these training examples. A typical way to control overfitting is to use a regularizing term, more gener- Keeping this small keeps weights such models, aids the nonlinear optimizer its search, by making it easier to scale all of once. −10 −5 0 5 10 17.5 18.0 18.5 19.0 γ = ∞ = = = 789 low and entropy high. We may add this regularizer to equation (6) or (7). In the maximum likelihood framework, we may subtract it from equation (4), is equivalent to maximum posteriori estimation with a diagonal Gaussian prior (Chen and 1999). The variance reflect a prior belief about the potential usefulness of feaor may be tuned on heldout data. Another simple regularization method is to stop before 0 (cf. Elidan and Friedman (2005)). If loss on heldout data begins to increase, we may be starting to overfit. This technique can be used along with annealing or quadratic regularization and can achieve additional accuracy gains, which we report elsewhere (Dreyer et al., 2006). 5 Computing Expected Loss At each temperature setting of deterministic annealing, we need to minimize the expected loss on the training corpus. We now discuss how this expectation is computed. When rescoring, we assume that we simply wish to combine, in some statistics of whole arrive at the overall loss for the corpus. We consider evaluation metrics for natural language tasks from two applicable classes: is a sum (or other linear combination) of the loss or gain on individual sentences. Accuracy—in dependency parsing, part-of-speech tagging, and other labeling tasks—falls into this class, as do recall, word error rate in ASR, and the crossing-brackets metric in parsing. Thanks to the linearity of expectation, we can easily compute in equation (6) by adding up the expected loss on each sentence. other metrics involve combinations over the sentences of the corpus. One example is precision, the number of correctly posited eleand the total number of posited elein the decoding of sentence (Depending on the task, the elements may be words, bigrams, labeled constituents, etc.) Our goal is to so during a step of deterministic anwe need to maximize the the sentences are decoded randomly according to equation (5). Although this expectation is continuous and differentiable as a function of sentence statistics usually involves iterover hypotheses ... If these share substructure in a hypothesis lattice, dynamic programming may help. unfortunately it seems hard to compute for any We observe however that an equivalent is to minimize Taking that as our loss function instead, equation (6) now needs to the expectation of decomposes somewhat more nicely: = i i (8) the integer random variables the number of posited and correctly posited elements over the whole corpus. approximate where any twicefunction (here we can apby a quadratic, given by the expansion of mean + − − 1 + − − 1 + since a sum of variables the current model parameters our randomized decoder decodes each sentence independently). In other words, given our quadratic apto on the (true) of through the single-sentence variances which can be found enumerating the of sentence The approximation becomes arbitrarily good as anneal —* since then on For equation (8), 2 A = found similarly. Similar techniques can be used to compute the expected logarithms of some other non-linear metrics, such as F-measure (the harmonic mean of and Papineni et al. (2002)’s changes the trajectory that DA takes through parameter space, but ultimately the objective is the same: as oo the course of DA, minimizing beindistinguishable from maximizing the count correct elements is def log = log + = + = log 1 + 790 metric (the geometric mean of several precisions). In particular, the expectation over its 1 r min(1 − ,0) + is the precision of the elements the As is standard in MT research, take = 4. first term in is the log a confunction of total number of unigram tokens in the decoded corpus) that fires only r average word count of the reference corpus). We again use a Taylor series to approximate the expected log brevity penalty. We mention an alternative way to compute (say) expected precision integrate numerically the of How can we this density? As = a of length-2 vectors, its vector and matrix can be respectively found by summing the means and comatrices of the exactly comfrom the distribution (5) over hypothe- We can easily approximate the (continuous) bivariate normal with that mean and else accumulate an exact representation of its (discrete) probability mass function by a sequence of numerical convolutions. 6 Experiments We tested the above training methods on two different tasks: dependency parsing and phrasebased machine translation. Since the basic setup was the same for both, we outline it here before describing the tasks in detail. In both cases, we start with 8 to 10 models (the “experts”) already trained on separate training To find the optimal coefficients a loglinear combination of these experts, we use separate development data, using the following procedure due to Och (2003): the 0 vector. For development sentence set its to careful when measuring a particular de- It only counts the first two copies of as if at most twice in any reference translation This “clipping” does not affect the rest of our method. for a large corpus, by Lyapunov’s central limit theorem (allows non-identically distributed summands). Decoding: each development sentence use the current extract the 200 analthe greatest scores · Calcuate each analysis’s loss statistics (e.g., and add it to the list if it is not already there. Convergence: not increased for any development sentence, or if we have reached our limit of 20 iterations, stop: the search has converged. Optimization: improve our objective function over the whole development corpus. Return to step 2. Our experiments simply compare three procedures at step 4. We may either maximize log-likelihood a convex function, at a given level of quadratic regularization, by BFGS gradient descent; minimize error (2) Och’s line search which each comof holding the others conor • minimize the same error (2) more effectively, raising —* oc the anrisk that is, cooling —* −oc —* and at each value, locally minimizing equation (7) using BFGS. Since these different optimization procedures usually find different step 4, their lists will diverge after the first iteration. For final testing, we selected among several variants of each procedure using a separate small heldout set. Final results are reported for a larger, disjoint test set. 6.1 Machine Translation For our machine translation experiments, we trained phrase-based alignment template models of Finnish-English, French-English, and German- English, as follows. For each language pair, we aligned 100,000 sentence pairs from European transcripts using We then used Philip Koehn’s phrase extraction software merge the alignments and to extract component whose optimization achieved the lowest loss is then updated. The process iterates until no lower loss can be found. In contrast, Papineni (1999) proposed a linear programming method that may search along diagonal lines. N log 791 and score the alignment template model’s phrases (Koehn et al., 2003). The Pharaoh phrase-based decoder uses precisely the setup of this paper. It scores a candidate translation (including its phrasal alignment to the text) as • where a vector of the following 8 features: 1. the probability of the source phrase given the target phrase 2. the probability of the target phrase given the source phrase 3. the weighted lexical probability of the source words given the target words 4. the weighted lexical probability of the target words given the source words 5. a phrase penalty that fires for each template in the translation 6. a distortion penalty that fires when phrases translate out of order 7. a word penalty that fires for each English word in the output 8. a trigram language model estimated on the English side of the bitext goal was to train the weights these 8 features. We used the method described above, employing the Pharaoh decoder at step 2 to generate the 200-best translations according to the cur- As explained above, we compared three procedures at step 4: maximum log-likelihood by gradient ascent; minimum error using Och’s linesearch method; and annealed minimum risk. As development data for training we used 200 sentence pairs for each language pair. Since our methods can be tuned with hyperparameters, we used performance on a separate 200sentence held-out set to choose the best hyperparameter values. The hyperparameter levels for each method were maximum likelihood: Gaussian prior with at 0.25, 0.5, 1, or minimum error: 5, or 10 different random starting points, drawn from a uniform Optimization Procedure Finnish- English French- English German-English Max. like. Min. error Ann. min. risk 5.02 5.31 7.43 10.27 26.16 20.94 16.43 27.31 21.30 1: percentage on translating 2000sentence test corpora, after training the 8 experts on 100,000 pairs and fitting their weights 200 more, using settings tuned on a further 200. The current minimum risk annealing method achieved significant improvements over minimum error and maximum likelihood at or below the 0.001 level, using a permutation test with 1000 replications. on 1] 1] • • • when an iteration of step annealed minimum risk: explicit enconstraints, starting temperature E stopping temperature E The temperature was cooled by half at each step; then we quenched by each step. (We also ran experiments with quadratic regularization with all 0.5, 1, or 2 in addition to the entropy constraint. Also, instead of the entropy we simply annealed on adding a quadratic regularization term. None of these regularized models beat the best setting of standard deterministic annealing on heldout or test data.) Final results on a separate 2000-sentence test set are shown in table 1. We evaluated translation usone reference translation and grams up to 4. The minimum risk annealing procedure significantly outperformed maximum likelihood and minimum error training in all three lanpairs &lt; permutation test with 1000 replications). Minimum risk annealing generally outperformed minimum error training on the held-out regardless of the starting temperature However, higher starting temperatures do give better performance and a more monotonic learning curve (Figure 3), a pattern that held up on test data. (In the same way, for minimum error training, is, we run step 4 from several starting points, finishing at several different points; we pick the finishing point with lowest development error (2). This reduces the sensitivity of method to the starting value of Maximum likelihood not sensitive to the starting value of it has only a global optimum; annealed minimum risk is not sensitive to it because initially Pz� making equation (6) flat. 792 Bleu Iteration 3: change in German-English development (upper) and held-out (lower), under annealed minrisk training with different versus minimum error training with 10 random restarts. Iteration 4: change in German-English development (upper) and held-out (lower), using 10 random restarts vs. only 1. more random restarts give better performance and a more monotonic learning curve—see Figure 4.) risk annealing did win on suggesting that its advantage is not superior minimization but rather superior genunder the risk criterion, lowloss hypotheses per sentence can help guide the learner to the right part of parameter space. Although the components of the translation and language models interact in complex ways, the improvement on Finnish-English may be due in part to the higher weight that minimum risk annealing found for the word penalty. That system is theremore likely to produce shorter output like have taken note of your remarks and i also agree that . like this longer output from the system: have taken note ofyour remarks and i shall also agree with all that the union. annealed using our novel from We found this to perform better on than if we with a “linearized” summed (as used in minimum Bayes risk decoding by Kumar and Byrne (2004)). 6.2 Dependency Parsing We trained dependency parsers for three different Bulgarian, Dutch, and Input sentences to the parser were already tagged for parts of speech. Each parser employed 10 experts, each parameterized as a globally normalized loglinear model (Lafferty et al., 2001). For example, component of the feature vector the of the was the log of that parse’s normalized probability accordto the Each expert was trained separately to maximize the conditional probability of the correct parse given the sentence. We used 10 iterations of gradient ascent. To speed training, for each of the first 9 iterations, the gradient was estimated on a (different) sample of only 1000 training sentences. then trained the vector used to combine the experts, to minimize the number of labeled dependency attachment errors on a 200-sentence development set. Optimization proceeded over lists of the 200-best parses of each sentence produced by a joint decoder using the 10 experts. Evaluating on labeled dependency accuracy on 200 test sentences for each language, we see that minimum error and annealed minimum risk training are much closer than for MT. For Bulgarian and Dutch, they are statistically indistinguishable using a paired-sample permutations test with 1000 replications. Indeed, on Dutch, all three optimization procedures produce indistinguishable results. On Slovenian, annealed minimum risk training does show a significant improvement over the other two methods. Overall, however, the results for this task are mediocre. We are still working on improving the underlying experts. 7 Related Work We have seen that annealed minimum risk training provides a useful alternative to maximum likelihood and minimum error training. In our experiments, it never performed significantly worse information on these corpora, see the CoNLL-X task on multilingual dependency parsing:</abstract>
<phone confidence="0.6637465">5 10 15 20 16 18 20 22</phone>
<pubnum confidence="0.556766333333333">T=1000 T=200 T=100</pubnum>
<author confidence="0.943183">error</author>
<phone confidence="0.819359">5 10 15 20</phone>
<email confidence="0.613452">Bleu</email>
<phone confidence="0.416467">5 10 15 20</phone>
<abstract confidence="0.815996732142857">10 restarts 1 restart 793 Optimization Procedure labeled dependency acc. [%] Slovenian Bulgarian Dutch Max. like. 27.78 47.23 36.78 Min. error 22.52 54.72 36.78 Ann. min. risk 31.16 54.66 36.71 2: dependency accuracy on parsing 200sentence test corpora, after training 10 experts on 1000 senand fitting their weights 200 more. For Slovenian, minimum risk annealing is significantly better than the other training methods, while minimum error is significantly worse. For Bulgarian, both minimum error and annealed minimum risk training achieve significant gains over maximum likelihood, but are indistinguishable from each other. For Dutch, the three methods are indistinguishable. than either and in some cases significantly helped. Note, however, that annealed minimum risk training results in a deterministic classifier just as these other training procedures do. The orthogonal of Bayes risk decoding achieved gains on parsing (Goodman, 1996) and machine translation (Kumar and Byrne, 2004). In speech recognition, researchers have improved decoding by smoothing probability estimates numerically on heldout data in a manner reminiscent of annealing (Goel and Byrne, 2000). We are interested in applying our techniques for approximating nonlinear loss functions to MBR by performing the risk minimization inside the dynamic programming or other decoder. Another training approach that incorporates arloss functions is found in the in the margin-based-learning community (Taskar et al., 2004; Crammer et al., 2004). Like other max-margin techniques, these attempt to make the best hypothesis far away from the inferior ones. The distinction is in using a loss function to calculate the required margins. 8 Conclusions Despite the challenging shape of the error surface, we have seen that it is practical to optimize task-specific error measures rather than optimizing likelihood—it produces lower-error systems. Different methods can be used to attempt this global, non-convex optimization. We showed that for MT, and sometimes for dependency parsing, an annealed minimum risk approach to optimization performs significantly better than a previous line-search method that does not smooth the error surface. It never does significantly worse. With such improved methods for minimizing error, we can hope to make better use of task-specific training criteria in NLP. References L. R. Bahl, P. F. Brown, P. V. de Souza, and R. L. Mercer. 1988. A new algorithm for the estimation of hidden model parameters. In pages 493–496. E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best and maxent discriminative reranking. In pages 173–180. S. F. Chen and R. Rosenfeld. 1999. A gaussian prior for smoothing maximum entropy models. Technical report, CS Dept., Carnegie Mellon University. K. Crammer, R. McDonald, and F. Pereira. 2004. New large algorithms for structured prediction. In Structured Outputs M. Dreyer, D. A. Smith, and N. A. Smith. 2006. Vine parsing and minimum risk reranking for speed and precision. In G. Elidan and N. Friedman. 2005. Learning hidden variable The information bottleneck approach. 6:81–127. V. Goel and W. J. Byrne. 2000. Minimum Bayes-Risk auspeech recognition. Speech and Lan- 14(2):115–135. J. T. Goodman. 1996. Parsing algorithms and metrics. In pages 177–183. Hinton. 1999. Products of experts. In of volume 1, pages 1–6. K.-U. Hoffgen, H.-U. Simon, and K. S. Van Horn. 1995. trainability of single neurons. of Computer and 50(1):114–125. D. S. Johnson and F. P. Preparata. 1978. The densest hemiproblem. Comp. 6(93–107). S. Katagiri, B.-H. Juang, and C.-H. Lee. 1998. Pattern recognition using a family of design algorithms based upon the probabilistic descent method. 86(11):2345–2373, November. P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrasetranslation. In pages 48–54. S. Kumar and W. Byrne. 2004. Minimum bayes-risk decodfor statistical machine translation. In J. Lafferty, A. McCallum, and F. C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting labeling sequence data. In F. J. Och. 2003. Minimum error rate training in statistical translation. In pages 160–167. K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. A method for automatic evaluation of machine In pages 311–318. K. A. Papineni. 1999. Discriminative training via linear In A. Rao and K. Rose. 2001. Deterministically annealed deof Hidden Markov Model speech recognizers. on Speech and Audio 9(2):111–126. K. Rose. 1998. Deterministic annealing for clustering, compression, classification, regression, and related optimizaproblems. 86(11):2210–2239. N. A. Smith and J. Eisner. 2004. Annealing techniques for statistical language learning. In pages 486–493.</abstract>
<note confidence="0.731715625">A. Smith, T. Cohn, and M. Osborne. 2005. Logarithmic pools for conditional random fields. In pages 18–25. B. Taskar, D. Klein, M. Collins, D. Koller, and C. Manning. Max-margin parsing. In pages 1–8. N. Ueda and R. Nakano. 1998. Deterministic annealing EM 11(2):271–282. 794</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L R Bahl</author>
<author>P F Brown</author>
<author>P V de Souza</author>
<author>R L Mercer</author>
</authors>
<title>A new algorithm for the estimation of hidden Markov model parameters. In</title>
<date>1988</date>
<booktitle>ICASSP,</booktitle>
<pages>493--496</pages>
<marker>Bahl, Brown, de Souza, Mercer, 1988</marker>
<rawString>L. R. Bahl, P. F. Brown, P. V. de Souza, and R. L. Mercer. 1988. A new algorithm for the estimation of hidden Markov model parameters. In ICASSP, pages 493–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In ACL,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="8613" citStr="Charniak and Johnson (2005)" startWordPosition="1409" endWordPosition="1412">maximizing likelihood, therefore, we will replace y∗i with the min-loss analysis in the hypothesis set; if multiple analyses tie 1Known algorithms are exponential but only in the dimensionality of the feature space (Johnson and Preparata, 1978). � L(argmax min yi θ i � L(yi,k)pθ(yi,k |xi) k � max θ i 788 Bleu % Translation model 1 Figure 2: Loss and expected loss as one translation model’s weight varies: the gray line (-y = oo) shows true BLEU (to be optimized in equation (2)). The black lines show the expected BLEU as -y in equation (5) increases from 0.1 toward oo. for this honor, we follow Charniak and Johnson (2005) in summing their probabilities.2 Maximizing (4) is equivalent to minimizing an upper bound on the expected 0/1 loss Ei(1 − pθ(yi |xi)). Though the log makes it tractable, this remains a 0/1 objective that does not give partial credit to wrong answers, such as imperfect but useful translations. Most systems should be evaluated and preferably trained on less harsh metrics. 3 Deterministic Annealing To balance the advantages of direct loss minimization, continuous risk minimization, and convex optimization, deterministic annealing attempts the solution of increasingly difficult optimization prob</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best parsing and maxent discriminative reranking. In ACL, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>R Rosenfeld</author>
</authors>
<title>A gaussian prior for smoothing maximum entropy models.</title>
<date>1999</date>
<tech>Technical report, CS</tech>
<institution>Dept., Carnegie Mellon University.</institution>
<contexts>
<context position="12396" citStr="Chen and Rosenfeld, 1999" startWordPosition="2053" endWordPosition="2056">ng to these training examples. A typical way to control overfitting is to use a quadratic regularizing term, ||θ||2 or more generally Ed θ2d/2σ2d. Keeping this small keeps weights 3For such models, -y merely aids the nonlinear optimizer in its search, by making it easier to scale all of 0 at once. −10 −5 0 5 10 17.5 18.0 18.5 19.0 γ = ∞ γ = 0.1 γ = 1 γ = 10 789 low and entropy high. We may add this regularizer to equation (6) or (7). In the maximum likelihood framework, we may subtract it from equation (4), which is equivalent to maximum a posteriori estimation with a diagonal Gaussian prior (Chen and Rosenfeld, 1999). The variance a2d may reflect a prior belief about the potential usefulness of feature d, or may be tuned on heldout data. Another simple regularization method is to stop cooling before T reaches 0 (cf. Elidan and Friedman (2005)). If loss on heldout data begins to increase, we may be starting to overfit. This technique can be used along with annealing or quadratic regularization and can achieve additional accuracy gains, which we report elsewhere (Dreyer et al., 2006). 5 Computing Expected Loss At each temperature setting of deterministic annealing, we need to minimize the expected loss on t</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>S. F. Chen and R. Rosenfeld. 1999. A gaussian prior for smoothing maximum entropy models. Technical report, CS Dept., Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>New large margin algorithms for structured prediction.</title>
<date>2004</date>
<booktitle>In Learning with Structured Outputs (NIPS).</booktitle>
<contexts>
<context position="30178" citStr="Crammer et al., 2004" startWordPosition="5045" endWordPosition="5048">an, 1996) and machine translation (Kumar and Byrne, 2004). In speech recognition, researchers have improved decoding by smoothing probability estimates numerically on heldout data in a manner reminiscent of annealing (Goel and Byrne, 2000). We are interested in applying our techniques for approximating nonlinear loss functions to MBR by performing the risk minimization inside the dynamic programming or other decoder. Another training approach that incorporates arbitrary loss functions is found in the structured prediction literature in the margin-based-learning community (Taskar et al., 2004; Crammer et al., 2004). Like other max-margin techniques, these attempt to make the best hypothesis far away from the inferior ones. The distinction is in using a loss function to calculate the required margins. 8 Conclusions Despite the challenging shape of the error surface, we have seen that it is practical to optimize task-specific error measures rather than optimizing likelihood—it produces lower-error systems. Different methods can be used to attempt this global, non-convex optimization. We showed that for MT, and sometimes for dependency parsing, an annealed minimum risk approach to optimization performs sig</context>
</contexts>
<marker>Crammer, McDonald, Pereira, 2004</marker>
<rawString>K. Crammer, R. McDonald, and F. Pereira. 2004. New large margin algorithms for structured prediction. In Learning with Structured Outputs (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dreyer</author>
<author>D A Smith</author>
<author>N A Smith</author>
</authors>
<title>Vine parsing and minimum risk reranking for speed and precision.</title>
<date>2006</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="12870" citStr="Dreyer et al., 2006" startWordPosition="2134" endWordPosition="2137">ay subtract it from equation (4), which is equivalent to maximum a posteriori estimation with a diagonal Gaussian prior (Chen and Rosenfeld, 1999). The variance a2d may reflect a prior belief about the potential usefulness of feature d, or may be tuned on heldout data. Another simple regularization method is to stop cooling before T reaches 0 (cf. Elidan and Friedman (2005)). If loss on heldout data begins to increase, we may be starting to overfit. This technique can be used along with annealing or quadratic regularization and can achieve additional accuracy gains, which we report elsewhere (Dreyer et al., 2006). 5 Computing Expected Loss At each temperature setting of deterministic annealing, we need to minimize the expected loss on the training corpus. We now discuss how this expectation is computed. When rescoring, we assume that we simply wish to combine, in some way, statistics of whole sentences4 to arrive at the overall loss for the corpus. We consider evaluation metrics for natural language tasks from two broadly applicable classes: linear and nonlinear. A linear metric is a sum (or other linear combination) of the loss or gain on individual sentences. Accuracy—in dependency parsing, part-of-</context>
</contexts>
<marker>Dreyer, Smith, Smith, 2006</marker>
<rawString>M. Dreyer, D. A. Smith, and N. A. Smith. 2006. Vine parsing and minimum risk reranking for speed and precision. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Elidan</author>
<author>N Friedman</author>
</authors>
<title>Learning hidden variable networks: The information bottleneck approach.</title>
<date>2005</date>
<journal>JMLR,</journal>
<pages>6--81</pages>
<contexts>
<context position="12626" citStr="Elidan and Friedman (2005)" startWordPosition="2093" endWordPosition="2097">timizer in its search, by making it easier to scale all of 0 at once. −10 −5 0 5 10 17.5 18.0 18.5 19.0 γ = ∞ γ = 0.1 γ = 1 γ = 10 789 low and entropy high. We may add this regularizer to equation (6) or (7). In the maximum likelihood framework, we may subtract it from equation (4), which is equivalent to maximum a posteriori estimation with a diagonal Gaussian prior (Chen and Rosenfeld, 1999). The variance a2d may reflect a prior belief about the potential usefulness of feature d, or may be tuned on heldout data. Another simple regularization method is to stop cooling before T reaches 0 (cf. Elidan and Friedman (2005)). If loss on heldout data begins to increase, we may be starting to overfit. This technique can be used along with annealing or quadratic regularization and can achieve additional accuracy gains, which we report elsewhere (Dreyer et al., 2006). 5 Computing Expected Loss At each temperature setting of deterministic annealing, we need to minimize the expected loss on the training corpus. We now discuss how this expectation is computed. When rescoring, we assume that we simply wish to combine, in some way, statistics of whole sentences4 to arrive at the overall loss for the corpus. We consider e</context>
</contexts>
<marker>Elidan, Friedman, 2005</marker>
<rawString>G. Elidan and N. Friedman. 2005. Learning hidden variable networks: The information bottleneck approach. JMLR, 6:81–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Goel</author>
<author>W J Byrne</author>
</authors>
<title>Minimum Bayes-Risk automatic speech recognition.</title>
<date>2000</date>
<journal>Computer Speech and Language,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="29796" citStr="Goel and Byrne, 2000" startWordPosition="4987" endWordPosition="4990">kelihood, but are indistinguishable from each other. For Dutch, the three methods are indistinguishable. than either and in some cases significantly helped. Note, however, that annealed minimum risk training results in a deterministic classifier just as these other training procedures do. The orthogonal technique of minimum Bayes risk decoding has achieved gains on parsing (Goodman, 1996) and machine translation (Kumar and Byrne, 2004). In speech recognition, researchers have improved decoding by smoothing probability estimates numerically on heldout data in a manner reminiscent of annealing (Goel and Byrne, 2000). We are interested in applying our techniques for approximating nonlinear loss functions to MBR by performing the risk minimization inside the dynamic programming or other decoder. Another training approach that incorporates arbitrary loss functions is found in the structured prediction literature in the margin-based-learning community (Taskar et al., 2004; Crammer et al., 2004). Like other max-margin techniques, these attempt to make the best hypothesis far away from the inferior ones. The distinction is in using a loss function to calculate the required margins. 8 Conclusions Despite the ch</context>
</contexts>
<marker>Goel, Byrne, 2000</marker>
<rawString>V. Goel and W. J. Byrne. 2000. Minimum Bayes-Risk automatic speech recognition. Computer Speech and Language, 14(2):115–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J T Goodman</author>
</authors>
<title>Parsing algorithms and metrics.</title>
<date>1996</date>
<booktitle>In ACL,</booktitle>
<pages>177--183</pages>
<contexts>
<context position="29566" citStr="Goodman, 1996" startWordPosition="4955" endWordPosition="4956">k annealing is significantly better than the other training methods, while minimum error is significantly worse. For Bulgarian, both minimum error and annealed minimum risk training achieve significant gains over maximum likelihood, but are indistinguishable from each other. For Dutch, the three methods are indistinguishable. than either and in some cases significantly helped. Note, however, that annealed minimum risk training results in a deterministic classifier just as these other training procedures do. The orthogonal technique of minimum Bayes risk decoding has achieved gains on parsing (Goodman, 1996) and machine translation (Kumar and Byrne, 2004). In speech recognition, researchers have improved decoding by smoothing probability estimates numerically on heldout data in a manner reminiscent of annealing (Goel and Byrne, 2000). We are interested in applying our techniques for approximating nonlinear loss functions to MBR by performing the risk minimization inside the dynamic programming or other decoder. Another training approach that incorporates arbitrary loss functions is found in the structured prediction literature in the margin-based-learning community (Taskar et al., 2004; Crammer e</context>
</contexts>
<marker>Goodman, 1996</marker>
<rawString>J. T. Goodman. 1996. Parsing algorithms and metrics. In ACL, pages 177–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hinton</author>
</authors>
<title>Products of experts.</title>
<date>1999</date>
<booktitle>In Proc. of ICANN,</booktitle>
<volume>1</volume>
<pages>1--6</pages>
<contexts>
<context position="3914" citStr="Hinton, 1999" startWordPosition="587" endWordPosition="588">(§5). We then apply this machinery to training log-linear combinations of models for dependency parsing and for machine translation (§6). Finally, we note the connections of minimum risk training to max-margin training and minimum Bayes risk decoding (§7), and recapitulate our results (§8). 2 Training Log-Linear Models In this work, we focus on rescoring with loglinear models. In particular, our experiments consider log-linear combinations of a relatively small number of features over entire complex structures, such as trees or translations, known in some previous work as products of experts (Hinton, 1999) or logarithmic opinion pools (Smith et al., 2005). A feature in the combined model might thus be a log probability from an entire submodel. Giving this feature a small or negative weight can discount a submodel that is foolishly structured, badly trained, or redundant with the other features. For each sentence xi in our training corpus S, we are given Ki possible analyses yi,i, ... yi,K,. (These may be all of the possible translations or parse trees; or only the Ki most probable under 787 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 787–794, Sydney, July 2006. c�2</context>
</contexts>
<marker>Hinton, 1999</marker>
<rawString>G. Hinton. 1999. Products of experts. In Proc. of ICANN, volume 1, pages 1–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K-U Hoffgen</author>
<author>H-U Simon</author>
<author>K S Van Horn</author>
</authors>
<title>Robust trainability of single neurons.</title>
<date>1995</date>
<journal>J. of Computer and System Sciences,</journal>
<volume>50</volume>
<issue>1</issue>
<marker>Hoffgen, Simon, Van Horn, 1995</marker>
<rawString>K.-U. Hoffgen, H.-U. Simon, and K. S. Van Horn. 1995. Robust trainability of single neurons. J. of Computer and System Sciences, 50(1):114–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D S Johnson</author>
<author>F P Preparata</author>
</authors>
<title>The densest hemisphere problem.</title>
<date>1978</date>
<booktitle>Theoretical Comp. Sci.,</booktitle>
<pages>6--93</pages>
<contexts>
<context position="8230" citStr="Johnson and Preparata, 1978" startWordPosition="1336" endWordPosition="1339">to the task, many systems (especially for language modeling) choose simply to maximize the probability of the gold standard. The log of this likelihood is a convex function of the parameters θ: log pθ(y∗i |xi) (4) where y∗i is the true analysis of sentence xi. The only wrinkle is that pθ(y∗i |xi) may be left undefined by equation (1) if y∗i is not in our set of Ki hypotheses. When maximizing likelihood, therefore, we will replace y∗i with the min-loss analysis in the hypothesis set; if multiple analyses tie 1Known algorithms are exponential but only in the dimensionality of the feature space (Johnson and Preparata, 1978). � L(argmax min yi θ i � L(yi,k)pθ(yi,k |xi) k � max θ i 788 Bleu % Translation model 1 Figure 2: Loss and expected loss as one translation model’s weight varies: the gray line (-y = oo) shows true BLEU (to be optimized in equation (2)). The black lines show the expected BLEU as -y in equation (5) increases from 0.1 toward oo. for this honor, we follow Charniak and Johnson (2005) in summing their probabilities.2 Maximizing (4) is equivalent to minimizing an upper bound on the expected 0/1 loss Ei(1 − pθ(yi |xi)). Though the log makes it tractable, this remains a 0/1 objective that does not gi</context>
</contexts>
<marker>Johnson, Preparata, 1978</marker>
<rawString>D. S. Johnson and F. P. Preparata. 1978. The densest hemisphere problem. Theoretical Comp. Sci., 6(93–107).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Katagiri</author>
<author>B-H Juang</author>
<author>C-H Lee</author>
</authors>
<title>Pattern recognition using a family of design algorithms based upon the generalized probabilistic descent method.</title>
<date>1998</date>
<booktitle>Proc. IEEE,</booktitle>
<volume>86</volume>
<issue>11</issue>
<contexts>
<context position="10615" citStr="Katagiri et al., 1998" startWordPosition="1738" endWordPosition="1741"> at higher γ. Figure 3 shows how the smoothing is gradually weakened to reach the risk objective (3) as γ —* 1 and approach the true error objective (2) as γ —* oc. Our risk minimization most resembles the work of Rao and Rose (2001), who trained an isolatedword speech recognition system for expected word-error rate. Deterministic annealing has also been used to tackle non-convex likelihood surfaces in unsupervised learning with EM (Ueda and Nakano, 1998; Smith and Eisner, 2004). Other work on “generalized probabilistic descent” minimizes a similar objective function but with γ held constant (Katagiri et al., 1998). Although the entropy is generally higher at lower values of γ, it varies as the optimization changes θ. In particular, a pure unregularized loglinear model such as (5) is really a function of γ·θ, so the optimizer could exactly compensate for increased γ by decreasing the θ vector proportionately!3 Most deterministic annealing procedures, therefore, express a direct preference on the entropy H, and choose γ and θ accordingly: min Ep-Y,e[L(yi,k)] − T · H(pγ,θ) (7) γ,θ In place of a schedule for raising γ, we now use a cooling schedule to lower T from oc to −oc, thereby weakening the preferenc</context>
</contexts>
<marker>Katagiri, Juang, Lee, 1998</marker>
<rawString>S. Katagiri, B.-H. Juang, and C.-H. Lee. 1998. Pattern recognition using a family of design algorithms based upon the generalized probabilistic descent method. Proc. IEEE, 86(11):2345–2373, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrasebased translation.</title>
<date>2003</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="20926" citStr="Koehn et al., 2003" startWordPosition="3541" endWordPosition="3544">t template models of Finnish-English, French-English, and GermanEnglish, as follows. For each language pair, we aligned 100,000 sentence pairs from European Parliament transcripts using GIZA++. We then used Philip Koehn’s phrase extraction software to merge the GIZA++ alignments and to extract 9The component whose optimization achieved the lowest loss is then updated. The process iterates until no lower loss can be found. In contrast, Papineni (1999) proposed a linear programming method that may search along diagonal lines. N wn log Pn n=1 791 and score the alignment template model’s phrases (Koehn et al., 2003). The Pharaoh phrase-based decoder uses precisely the setup of this paper. It scores a candidate translation (including its phrasal alignment to the original text) as 0 • f, where f is a vector of the following 8 features: 1. the probability of the source phrase given the target phrase 2. the probability of the target phrase given the source phrase 3. the weighted lexical probability of the source words given the target words 4. the weighted lexical probability of the target words given the source words 5. a phrase penalty that fires for each template in the translation 6. a distortion penalty</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrasebased translation. In HLT-NAACL, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kumar</author>
<author>W Byrne</author>
</authors>
<title>Minimum bayes-risk decoding for statistical machine translation.</title>
<date>2004</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="26462" citStr="Kumar and Byrne (2004)" startWordPosition="4465" endWordPosition="4468">er weight that minimum risk annealing found for the word penalty. That system is therefore more likely to produce shorter output like i have taken note of your remarks and i also agree with that . than like this longer output from the minimum-error-trained system: i have taken note ofyour remarks and i shall also agree with all that the union. We annealed using our novel expected-BLEU approximation from §5. We found this to perform significantly better on BLEU evaluation than if we trained with a “linearized” BLEU that summed per-sentence BLEU scores (as used in minimum Bayes risk decoding by Kumar and Byrne (2004)). 6.2 Dependency Parsing We trained dependency parsers for three different languages: Bulgarian, Dutch, and Slovenian.11 Input sentences to the parser were already tagged for parts of speech. Each parser employed 10 experts, each parameterized as a globally normalized loglinear model (Lafferty et al., 2001). For example, the 9th component of the feature vector fz�k (which described the kth parse of the ith sentence) was the log of that parse’s normalized probability according to the 9th expert. Each expert was trained separately to maximize the conditional probability of the correct parse giv</context>
<context position="29614" citStr="Kumar and Byrne, 2004" startWordPosition="4960" endWordPosition="4963"> the other training methods, while minimum error is significantly worse. For Bulgarian, both minimum error and annealed minimum risk training achieve significant gains over maximum likelihood, but are indistinguishable from each other. For Dutch, the three methods are indistinguishable. than either and in some cases significantly helped. Note, however, that annealed minimum risk training results in a deterministic classifier just as these other training procedures do. The orthogonal technique of minimum Bayes risk decoding has achieved gains on parsing (Goodman, 1996) and machine translation (Kumar and Byrne, 2004). In speech recognition, researchers have improved decoding by smoothing probability estimates numerically on heldout data in a manner reminiscent of annealing (Goel and Byrne, 2000). We are interested in applying our techniques for approximating nonlinear loss functions to MBR by performing the risk minimization inside the dynamic programming or other decoder. Another training approach that incorporates arbitrary loss functions is found in the structured prediction literature in the margin-based-learning community (Taskar et al., 2004; Crammer et al., 2004). Like other max-margin techniques, </context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>S. Kumar and W. Byrne. 2004. Minimum bayes-risk decoding for statistical machine translation. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="26771" citStr="Lafferty et al., 2001" startWordPosition="4511" endWordPosition="4514"> also agree with all that the union. We annealed using our novel expected-BLEU approximation from §5. We found this to perform significantly better on BLEU evaluation than if we trained with a “linearized” BLEU that summed per-sentence BLEU scores (as used in minimum Bayes risk decoding by Kumar and Byrne (2004)). 6.2 Dependency Parsing We trained dependency parsers for three different languages: Bulgarian, Dutch, and Slovenian.11 Input sentences to the parser were already tagged for parts of speech. Each parser employed 10 experts, each parameterized as a globally normalized loglinear model (Lafferty et al., 2001). For example, the 9th component of the feature vector fz�k (which described the kth parse of the ith sentence) was the log of that parse’s normalized probability according to the 9th expert. Each expert was trained separately to maximize the conditional probability of the correct parse given the sentence. We used 10 iterations of gradient ascent. To speed training, for each of the first 9 iterations, the gradient was estimated on a (different) sample of only 1000 training sentences. We then trained the vector 0, used to combine the experts, to minimize the number of labeled dependency attachm</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="2711" citStr="Och, 2003" startWordPosition="400" endWordPosition="401">face for discrete structured prediction is not only riddled with local minima, but piecewise constant This work was supported by an NSF graduate research fellowship for the first author and by NSF ITR grant IIS0313193 and ONR grant N00014-01-1-0685. The views expressed are not necessarily endorsed by the sponsors. We thank Sanjeev Khudanpur, Noah Smith, Markus Dreyer, and the reviewers for helpful discussions and comments. and not everywhere differentiable with respect to the model parameters (Figure 1). Despite these difficulties, some work has shown it worthwhile to minimize error directly (Och, 2003; Bahl et al., 1988). We show improvements over previous work on error minimization by minimizing the risk or expected error—a continuous function that can be derived by combining the likelihood with any evaluation metric (§2). Seeking to avoid local minima, deterministic annealing (Rose, 1998) gradually changes the objective function from a convex entropy surface to the more complex risk surface (§3). We also discuss regularizing the objective function to prevent overfitting (§4). We explain how to compute expected loss under some evaluation metrics common in natural language tasks (§5). We t</context>
<context position="6072" citStr="Och (2003)" startWordPosition="961" endWordPosition="962">ear from context. For clearer exposition, we assume below that the total loss over some test corpus is the sum of the losses on individual sentences, although we will revisit that assumption in §5. 2.1 Minimizing Loss or Expected Loss One training criterion directly mimics test conditions. It looks at the loss incurred if we choose the best analysis of each xi according to the model: pθ(yi |xi)) (2) Since small changes in θ either do not change the best analysis or else push a different analysis to the top, this objective function is piecewise constant, hence not amenable to gradient descent. Och (2003) observed, however, that the piecewiseconstant property could be exploited to characterize the function exhaustively along any line in parameter space, and hence to minimize it globally along that line. By calling this global line minimization as a subroutine of multidimensional optimization, he was able to minimize (2) well enough to improve over likelihood maximization for training factored machine translation systems. Instead of considering only the best hypothesis for any θ, we can minimize risk, i.e., the expected loss under pθ across all analyses yi: � mθ de inEpOL(yi,k) f Bin i (3) This</context>
<context position="18347" citStr="Och (2003)" startWordPosition="3114" endWordPosition="3115">e an exact representation of its (discrete) probability mass function by a sequence of numerical convolutions. 6 Experiments We tested the above training methods on two different tasks: dependency parsing and phrasebased machine translation. Since the basic setup was the same for both, we outline it here before describing the tasks in detail. In both cases, we start with 8 to 10 models (the “experts”) already trained on separate training data. To find the optimal coefficients 0 for a loglinear combination of these experts, we use separate development data, using the following procedure due to Och (2003): 1. Initialization: Initialize 0 to the 0 vector. For each development sentence xi, set its Ki-best list to 0 (thus Ki = 0). 7BLEU is careful when measuring ci on a particular decoding yi,k. It only counts the first two copies of the (e.g.) as correct if the occurs at most twice in any reference translation of xi. This “clipping” does not affect the rest of our method. 8Reasonable for a large corpus, by Lyapunov’s central limit theorem (allows non-identically distributed summands). 2. Decoding: For each development sentence xi, use the current 0 to extract the 200 analyses yi,k with the great</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training in statistical machine translation. In ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="1619" citStr="Papineni et al., 2002" startWordPosition="227" endWordPosition="230">combinations of models for dependency parsing and for machine translation. In machine translation, annealed minimum risk training achieves significant improvements in BLEU over standard minimum error training. We also show improvements in labeled dependency parsing. 1 Direct Minimization of Error Researchers in empirical natural language processing have expended substantial ink and effort in developing metrics to evaluate systems automatically against gold-standard corpora. The ongoing evaluation literature is perhaps most obvious in the machine translation community’s efforts to better BLEU (Papineni et al., 2002). Despite this research, parsing or machine translation systems are often trained using the much simpler and harsher metric of maximum likelihood. One reason is that in supervised training, the log-likelihood objective function is generally convex, meaning that it has a single global maximum that can be easily found (indeed, for supervised generative models, the parameters at this maximum may even have a closed-form solution). In contrast to the likelihood surface, the error surface for discrete structured prediction is not only riddled with local minima, but piecewise constant This work was s</context>
<context position="16185" citStr="Papineni et al. (2002)" startWordPosition="2719" endWordPosition="2722">, given our quadratic approximation to g, E[g(A)] depends on the (true) distribution of A only through the single-sentence means µai and variances a2ai, which can be found by enumerating the Ki decodings of sentence i. The approximation becomes arbitrarily good as we anneal -y —* oc, since then Q2A —* 0 and E[g(A)] focuses on g near µA. For equation (8), 2 A E[g(A)] = E[log A] ,:; log(µA) − 2µ2 A and E[log C] is found similarly. Similar techniques can be used to compute the expected logarithms of some other non-linear metrics, such as F-measure (the harmonic mean of precision and recall)6 and Papineni et al. (2002)’s 5This changes the trajectory that DA takes through parameter space, but ultimately the objective is the same: as -y --+ oo over the course of DA, minimizing E[− log P] becomes indistinguishable from maximizing E[P]. 6R def = C/B; the count B of correct elements is known. def So log F = log 2PR/(P + R) = log 2R/(1 + R/P) = log 2C/B − log(1 + A/B). Consider g(x) = log 1 + x/B. 790 BLEU translation metric (the geometric mean of several precisions). In particular, the expectation of log BLEU distributes over its N + 1 summands: r log BLEU = min(1 − ,0) + A1 where Pn is the precision of the n-gr</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K A Papineni</author>
</authors>
<title>Discriminative training via linear programming.</title>
<date>1999</date>
<booktitle>In ICASSP.</booktitle>
<contexts>
<context position="20761" citStr="Papineni (1999)" startWordPosition="3515" endWordPosition="3516">et. Final results are reported for a larger, disjoint test set. 6.1 Machine Translation For our machine translation experiments, we trained phrase-based alignment template models of Finnish-English, French-English, and GermanEnglish, as follows. For each language pair, we aligned 100,000 sentence pairs from European Parliament transcripts using GIZA++. We then used Philip Koehn’s phrase extraction software to merge the GIZA++ alignments and to extract 9The component whose optimization achieved the lowest loss is then updated. The process iterates until no lower loss can be found. In contrast, Papineni (1999) proposed a linear programming method that may search along diagonal lines. N wn log Pn n=1 791 and score the alignment template model’s phrases (Koehn et al., 2003). The Pharaoh phrase-based decoder uses precisely the setup of this paper. It scores a candidate translation (including its phrasal alignment to the original text) as 0 • f, where f is a vector of the following 8 features: 1. the probability of the source phrase given the target phrase 2. the probability of the target phrase given the source phrase 3. the weighted lexical probability of the source words given the target words 4. th</context>
</contexts>
<marker>Papineni, 1999</marker>
<rawString>K. A. Papineni. 1999. Discriminative training via linear programming. In ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rao</author>
<author>K Rose</author>
</authors>
<title>Deterministically annealed design of Hidden Markov Model speech recognizers.</title>
<date>2001</date>
<booktitle>IEEE Trans. on Speech and Audio Processing,</booktitle>
<pages>9--2</pages>
<contexts>
<context position="10226" citStr="Rao and Rose (2001)" startWordPosition="1680" endWordPosition="1683"> the top-scoring analysis. For a fixed γ, deterministic annealing solves min Ep-Y,e[L(yi,k)] (6) θ 2An alternative would be to artificially add yz (e.g., the reference translation(s)) to the hypothesis set during training. We then increase γ according to some schedule and optimize θ again. When γ is low, the smooth objective might allow us to pass over local minima that could open up at higher γ. Figure 3 shows how the smoothing is gradually weakened to reach the risk objective (3) as γ —* 1 and approach the true error objective (2) as γ —* oc. Our risk minimization most resembles the work of Rao and Rose (2001), who trained an isolatedword speech recognition system for expected word-error rate. Deterministic annealing has also been used to tackle non-convex likelihood surfaces in unsupervised learning with EM (Ueda and Nakano, 1998; Smith and Eisner, 2004). Other work on “generalized probabilistic descent” minimizes a similar objective function but with γ held constant (Katagiri et al., 1998). Although the entropy is generally higher at lower values of γ, it varies as the optimization changes θ. In particular, a pure unregularized loglinear model such as (5) is really a function of γ·θ, so the optim</context>
<context position="11516" citStr="Rao and Rose, 2001" startWordPosition="1896" endWordPosition="1899">r proportionately!3 Most deterministic annealing procedures, therefore, express a direct preference on the entropy H, and choose γ and θ accordingly: min Ep-Y,e[L(yi,k)] − T · H(pγ,θ) (7) γ,θ In place of a schedule for raising γ, we now use a cooling schedule to lower T from oc to −oc, thereby weakening the preference for high entropy. The Lagrange multiplier T on entropy is called “temperature” due to a satisfying connection to statistical mechanics. Once T is quite cool, it is common in practice to switch to raising γ directly and rapidly (quenching) until some convergence criterion is met (Rao and Rose, 2001). 4 Regularization Informally, high temperature or γ &lt; 1 smooths our model during training toward higher-entropy conditional distributions that are not so peaked at the desired analyses y* . Another reason for such smoothing is simply to prevent overfitting to these training examples. A typical way to control overfitting is to use a quadratic regularizing term, ||θ||2 or more generally Ed θ2d/2σ2d. Keeping this small keeps weights 3For such models, -y merely aids the nonlinear optimizer in its search, by making it easier to scale all of 0 at once. −10 −5 0 5 10 17.5 18.0 18.5 19.0 γ = ∞ γ = 0.</context>
</contexts>
<marker>Rao, Rose, 2001</marker>
<rawString>A. Rao and K. Rose. 2001. Deterministically annealed design of Hidden Markov Model speech recognizers. IEEE Trans. on Speech and Audio Processing, 9(2):111–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Rose</author>
</authors>
<title>Deterministic annealing for clustering, compression, classification, regression, and related optimization problems.</title>
<date>1998</date>
<booktitle>Proc. IEEE,</booktitle>
<volume>86</volume>
<issue>11</issue>
<contexts>
<context position="3006" citStr="Rose, 1998" startWordPosition="447" endWordPosition="448">orsed by the sponsors. We thank Sanjeev Khudanpur, Noah Smith, Markus Dreyer, and the reviewers for helpful discussions and comments. and not everywhere differentiable with respect to the model parameters (Figure 1). Despite these difficulties, some work has shown it worthwhile to minimize error directly (Och, 2003; Bahl et al., 1988). We show improvements over previous work on error minimization by minimizing the risk or expected error—a continuous function that can be derived by combining the likelihood with any evaluation metric (§2). Seeking to avoid local minima, deterministic annealing (Rose, 1998) gradually changes the objective function from a convex entropy surface to the more complex risk surface (§3). We also discuss regularizing the objective function to prevent overfitting (§4). We explain how to compute expected loss under some evaluation metrics common in natural language tasks (§5). We then apply this machinery to training log-linear combinations of models for dependency parsing and for machine translation (§6). Finally, we note the connections of minimum risk training to max-margin training and minimum Bayes risk decoding (§7), and recapitulate our results (§8). 2 Training Lo</context>
<context position="9230" citStr="Rose, 1998" startWordPosition="1503" endWordPosition="1504">mming their probabilities.2 Maximizing (4) is equivalent to minimizing an upper bound on the expected 0/1 loss Ei(1 − pθ(yi |xi)). Though the log makes it tractable, this remains a 0/1 objective that does not give partial credit to wrong answers, such as imperfect but useful translations. Most systems should be evaluated and preferably trained on less harsh metrics. 3 Deterministic Annealing To balance the advantages of direct loss minimization, continuous risk minimization, and convex optimization, deterministic annealing attempts the solution of increasingly difficult optimization problems (Rose, 1998). Adding a scale hyperparameter γ to equation (1), we have the following family of distributions: pγ,θ(yi,k |xi) =K(expθ · fi,k)γ γ (5) Ek&apos;- 1 (exp θ · fi k&apos;) When γ = 0, all yi,k are equally likely, giving the uniform distribution; when γ = 1, we recover the model in equation (1); and as γ —* oc, we approach the winner-take-all Viterbi function that assigns probability 1 to the top-scoring analysis. For a fixed γ, deterministic annealing solves min Ep-Y,e[L(yi,k)] (6) θ 2An alternative would be to artificially add yz (e.g., the reference translation(s)) to the hypothesis set during training. </context>
</contexts>
<marker>Rose, 1998</marker>
<rawString>K. Rose. 1998. Deterministic annealing for clustering, compression, classification, regression, and related optimization problems. Proc. IEEE, 86(11):2210–2239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N A Smith</author>
<author>J Eisner</author>
</authors>
<title>Annealing techniques for unsupervised statistical language learning.</title>
<date>2004</date>
<booktitle>In ACL,</booktitle>
<pages>486--493</pages>
<contexts>
<context position="10476" citStr="Smith and Eisner, 2004" startWordPosition="1717" endWordPosition="1720">rding to some schedule and optimize θ again. When γ is low, the smooth objective might allow us to pass over local minima that could open up at higher γ. Figure 3 shows how the smoothing is gradually weakened to reach the risk objective (3) as γ —* 1 and approach the true error objective (2) as γ —* oc. Our risk minimization most resembles the work of Rao and Rose (2001), who trained an isolatedword speech recognition system for expected word-error rate. Deterministic annealing has also been used to tackle non-convex likelihood surfaces in unsupervised learning with EM (Ueda and Nakano, 1998; Smith and Eisner, 2004). Other work on “generalized probabilistic descent” minimizes a similar objective function but with γ held constant (Katagiri et al., 1998). Although the entropy is generally higher at lower values of γ, it varies as the optimization changes θ. In particular, a pure unregularized loglinear model such as (5) is really a function of γ·θ, so the optimizer could exactly compensate for increased γ by decreasing the θ vector proportionately!3 Most deterministic annealing procedures, therefore, express a direct preference on the entropy H, and choose γ and θ accordingly: min Ep-Y,e[L(yi,k)] − T · H(p</context>
</contexts>
<marker>Smith, Eisner, 2004</marker>
<rawString>N. A. Smith and J. Eisner. 2004. Annealing techniques for unsupervised statistical language learning. In ACL, pages 486–493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Smith</author>
<author>T Cohn</author>
<author>M Osborne</author>
</authors>
<title>Logarithmic opinion pools for conditional random fields.</title>
<date>2005</date>
<booktitle>In ACL,</booktitle>
<pages>18--25</pages>
<contexts>
<context position="3964" citStr="Smith et al., 2005" startWordPosition="593" endWordPosition="596">ng log-linear combinations of models for dependency parsing and for machine translation (§6). Finally, we note the connections of minimum risk training to max-margin training and minimum Bayes risk decoding (§7), and recapitulate our results (§8). 2 Training Log-Linear Models In this work, we focus on rescoring with loglinear models. In particular, our experiments consider log-linear combinations of a relatively small number of features over entire complex structures, such as trees or translations, known in some previous work as products of experts (Hinton, 1999) or logarithmic opinion pools (Smith et al., 2005). A feature in the combined model might thus be a log probability from an entire submodel. Giving this feature a small or negative weight can discount a submodel that is foolishly structured, badly trained, or redundant with the other features. For each sentence xi in our training corpus S, we are given Ki possible analyses yi,i, ... yi,K,. (These may be all of the possible translations or parse trees; or only the Ki most probable under 787 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 787–794, Sydney, July 2006. c�2006 Association for Computational Linguistics Figu</context>
</contexts>
<marker>Smith, Cohn, Osborne, 2005</marker>
<rawString>A. Smith, T. Cohn, and M. Osborne. 2005. Logarithmic opinion pools for conditional random fields. In ACL, pages 18–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>D Klein</author>
<author>M Collins</author>
<author>D Koller</author>
<author>C Manning</author>
</authors>
<title>Max-margin parsing.</title>
<date>2004</date>
<booktitle>In EMNLP,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="30155" citStr="Taskar et al., 2004" startWordPosition="5041" endWordPosition="5044">ins on parsing (Goodman, 1996) and machine translation (Kumar and Byrne, 2004). In speech recognition, researchers have improved decoding by smoothing probability estimates numerically on heldout data in a manner reminiscent of annealing (Goel and Byrne, 2000). We are interested in applying our techniques for approximating nonlinear loss functions to MBR by performing the risk minimization inside the dynamic programming or other decoder. Another training approach that incorporates arbitrary loss functions is found in the structured prediction literature in the margin-based-learning community (Taskar et al., 2004; Crammer et al., 2004). Like other max-margin techniques, these attempt to make the best hypothesis far away from the inferior ones. The distinction is in using a loss function to calculate the required margins. 8 Conclusions Despite the challenging shape of the error surface, we have seen that it is practical to optimize task-specific error measures rather than optimizing likelihood—it produces lower-error systems. Different methods can be used to attempt this global, non-convex optimization. We showed that for MT, and sometimes for dependency parsing, an annealed minimum risk approach to op</context>
</contexts>
<marker>Taskar, Klein, Collins, Koller, Manning, 2004</marker>
<rawString>B. Taskar, D. Klein, M. Collins, D. Koller, and C. Manning. 2004. Max-margin parsing. In EMNLP, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ueda</author>
<author>R Nakano</author>
</authors>
<title>Deterministic annealing EM algorithm.</title>
<date>1998</date>
<journal>Neural Networks,</journal>
<volume>11</volume>
<issue>2</issue>
<contexts>
<context position="10451" citStr="Ueda and Nakano, 1998" startWordPosition="1713" endWordPosition="1716">We then increase γ according to some schedule and optimize θ again. When γ is low, the smooth objective might allow us to pass over local minima that could open up at higher γ. Figure 3 shows how the smoothing is gradually weakened to reach the risk objective (3) as γ —* 1 and approach the true error objective (2) as γ —* oc. Our risk minimization most resembles the work of Rao and Rose (2001), who trained an isolatedword speech recognition system for expected word-error rate. Deterministic annealing has also been used to tackle non-convex likelihood surfaces in unsupervised learning with EM (Ueda and Nakano, 1998; Smith and Eisner, 2004). Other work on “generalized probabilistic descent” minimizes a similar objective function but with γ held constant (Katagiri et al., 1998). Although the entropy is generally higher at lower values of γ, it varies as the optimization changes θ. In particular, a pure unregularized loglinear model such as (5) is really a function of γ·θ, so the optimizer could exactly compensate for increased γ by decreasing the θ vector proportionately!3 Most deterministic annealing procedures, therefore, express a direct preference on the entropy H, and choose γ and θ accordingly: min </context>
</contexts>
<marker>Ueda, Nakano, 1998</marker>
<rawString>N. Ueda and R. Nakano. 1998. Deterministic annealing EM algorithm. Neural Networks, 11(2):271–282.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>