<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.98837">
Iterative Scaling and Coordinate Descent Methods for Maximum Entropy
</title>
<author confidence="0.995185">
Fang-Lan Huang, Cho-Jui Hsieh, Kai-Wei Chang, and Chih-Jen Lin
</author>
<affiliation confidence="0.863706333333333">
Department of Computer Science
National Taiwan University
Taipei 106, Taiwan
</affiliation>
<email confidence="0.993249">
{d93011,b92085,b92084,cjlin}@csie.ntu.edu.tw
</email>
<sectionHeader confidence="0.994663" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998265">
Maximum entropy (Maxent) is useful in
many areas. Iterative scaling (IS) methods
are one of the most popular approaches to
solve Maxent. With many variants of IS
methods, it is difficult to understand them
and see the differences. In this paper, we
create a general and unified framework for
IS methods. This framework also connects
IS and coordinate descent (CD) methods.
Besides, we develop a CD method for
Maxent. Results show that it is faster than
existing iterative scaling methods1.
</bodyText>
<sectionHeader confidence="0.99842" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999629">
Maximum entropy (Maxent) is widely used in
many areas such as natural language processing
(NLP) and document classification. Maxent mod-
els the conditional probability as:
</bodyText>
<equation confidence="0.976440333333333">
Pw(y|x)-Sw(x,y)/Tw(x), (1)
Pt wtft(x,y), Tw(x)-P
Sw(x,y)-e y Sw(x,y),
</equation>
<bodyText confidence="0.983350428571429">
where x indicates a context, y is the label of the
context, and w E Rn is the weight vector. A
function ft(x, y) denotes the t-th feature extracted
from the context x and the label y.
Given an empirical probability distribution
P˜(x, y) obtained from training samples, Maxent
minimizes the following negative log-likelihood:
minw − Px,y P˜(x, y) log Pw(y|x)
= Px P˜(x) log Tw(x) − Pt wt
where P˜(x) = Py P˜(x, y) is the marginal prob-
ability of x, and P˜(ft) = Px,y P˜(x, y)ft(x, y) is
the expected value of ft(x, y). To avoid overfit-
ting the training samples, some add a regulariza-
tion term and solve:
</bodyText>
<equation confidence="0.981282">
P(ft) w2
+ 2Q2,
t
(3)
</equation>
<footnote confidence="0.969014333333333">
1A complete version of this work is at http:
//www.csie.ntu.edu.tw/˜cjlin/papers/
maxent_journal.pdf.
</footnote>
<bodyText confidence="0.999866916666667">
where σ is a regularization parameter. We focus
on (3) instead of (2) because (3) is strictly convex.
Iterative scaling (IS) methods are popular in
training Maxent models. They all share the same
property of solving a one-variable sub-problem
at a time. Existing IS methods include general-
ized iterative scaling (GIS) by Darroch and Rat-
cliff (1972), improved iterative scaling (IIS) by
Della Pietra et al. (1997), and sequential condi-
tional generalized iterative scaling (SCGIS) by
Goodman (2002). In optimization, coordinate de-
scent (CD) is a popular method which also solves
a one-variable sub-problem at a time. With these
many IS and CD methods, it is uneasy to see their
differences. In Section 2, we propose a unified
framework to describe IS and CD methods from
an optimization viewpoint. Using this framework,
we design a fast CD approach for Maxent in Sec-
tion 3. In Section 4, we compare the proposed
CD method with IS and LBFGS methods. Results
show that the CD method is more efficient.
Notation n is the number of features. The total
number of nonzeros in samples and the average
number of nonzeros per feature are respectively
</bodyText>
<equation confidence="0.8147005">
#nz - P P t:ft(x,y)�=0 1 and
x,y
</equation>
<sectionHeader confidence="0.949148" genericHeader="method">
2 A Framework for IS Methods
</sectionHeader>
<subsectionHeader confidence="0.955205">
2.1 The Framework
</subsectionHeader>
<bodyText confidence="0.999962076923077">
The one-variable sub-problem of IS methods is re-
lated to the function reduction L(w+zet)−L(w),
where et = [0, ... , 0, 1, 0, ... , 0]T. IS methods
differ in how they approximate the function reduc-
tion. They can also be categorized according to
whether w’s components are sequentially or par-
allely updated. In this section, we create a frame-
work in Figure 1 for these methods.
Sequential update For a sequential-update
algorithm, once a one-variable sub-problem is
solved, the corresponding element in w is up-
dated. The new w is then used to construct the
next sub-problem. The procedure is sketched in
</bodyText>
<equation confidence="0.9974599">
(2)
P˜ (ft),
min
w
wt
L(w)-P
x
P˜(x)logTw(x)−P
t
l - #nz/n.
</equation>
<page confidence="0.978229">
285
</page>
<note confidence="0.964036">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 285–288,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<figure confidence="0.996126888888889">
Iterative scaling
Sequential update
Find At(z) to approximate Let At(z) =
L(w + zet) − L(w) L(w+zet)−L(w)
SCGIS CD
Parallel update
Find a separable function A(z) to
approximate L(w + z) − L(w)
GIS, IIS
</figure>
<figureCaption confidence="0.973744">
Figure 1: An illustration of various iterative scaling methods.
</figureCaption>
<figure confidence="0.644761333333333">
Algorithm 1 A sequential-update IS method
While w is not optimal
Fort = 1,...,n
</figure>
<listItem confidence="0.749445428571429">
1. Find an approximate function At(z) sat-
isfying (4).
2. Approximately minz At(z) to get ¯zt.
3. wt ← wt + ¯zt.
Algorithm 1. If the t-th component is selected for
update, a sequential IS method solves the follow-
ing one-variable sub-problem:
</listItem>
<equation confidence="0.6434056">
minz At(z),
where At(z) bounds the function difference:
At(z) ≥ L(w + zet) − L(w)
(4)
(5)
</equation>
<bodyText confidence="0.9817152">
An approximate function At(z) satisfying (4) does
not ensure that the function value is strictly de-
creasing. That is, the new function value L(w +
zet) may be only the same as L(w). Therefore,
we can impose an additional condition
</bodyText>
<equation confidence="0.985172">
At(0) = 0 (6)
</equation>
<bodyText confidence="0.988585">
on the approximate function At(z). If A′t(0) =6 0
and assume ¯zt ≡ arg minz At(z) exists, with the
condition At(0) = 0, we have At(¯zt) &lt; 0. This in-
equality and (4) then imply L(w + ¯ztet)&lt;L(w).
If A′t(0) = ∇tL(w) = 0, the convexity of L(w)
implies that we cannot decrease the function value
by modifying wt. Then we should move on to
modify other components of w.
A CD method can be viewed as a sequential IS
method. It solves the following sub-problem:
minz ACD
</bodyText>
<equation confidence="0.706783">
t (z) = L(w + zet) − L(w)
</equation>
<bodyText confidence="0.683725111111111">
without any approximation. Existing IS methods
consider approximations as At(z) may be simpler
for minimization.
Parallel update A parallel IS method simul-
taneously constructs n independent one-variable
sub-problems. After (approximately) solving all
of them, the whole vector w is updated. Algo-
rithm 2 gives the procedure. The differentiable
function A(z), z ∈ Rn, is an approximation of
</bodyText>
<equation confidence="0.9920595">
L(w + z) − L(w) satisfying
A(z) ≥ L(w + z) − L(w), A(0) = 0, and
A(z) = P (7)
t At(zt).
</equation>
<bodyText confidence="0.911316">
Similar to (4) and (6), the first two conditions en-
</bodyText>
<listItem confidence="0.8816565">
Algorithm 2 A parallel-update IS method
While w is not optimal
1. Find approximate functions At(zt) ∀t satis-
fying (7).
2. Fort = 1,...,n
Approximately minzt At(zt) to get ¯zt.
3. Fort = 1,...,n
wt ← wt + ¯zt.
</listItem>
<bodyText confidence="0.989457583333333">
sure that the function value is strictly decreasing.
The last condition shows thatA(z)is separable, so
minz A(z) = Pt minzt At(zt).
That is,we can minimizeAt(zt),∀t simultaneously,
and then update wt ∀t together. A parallel-update
method possesses nice implementation properties.
However, since it less aggressively updates w, it
usually converges slower. If A(z) satisfies (7),
taking z = ztet implies that (4) and (6) hold for
any At(zt). A parallel method could thus be trans-
formed to a sequential method using the same ap-
proximate function, but not vice versa.
</bodyText>
<subsectionHeader confidence="0.999225">
2.2 Existing Iterative Scaling Methods
</subsectionHeader>
<bodyText confidence="0.99998125">
We introduce GIS, IIS and SCGIS via the pro-
posed framework. GIS and IIS use a parallel up-
date, but SCGIS is sequential. Their approximate
functions aim to bound the function reduction
</bodyText>
<equation confidence="0.9696825">
L(w+z)−L(w)=Px P˜(x) logTw+z(x)
Tw (x) +PtQt(zt),
</equation>
<bodyText confidence="0.9566085">
(8)
where Tw(x) and Qt(zt) are defined in (1) and (5),
respectively. Then GIS, IIS and SCGIS use simi-
lar inequalities to get approximate functions. They
apply log α ≤ α − 1 ∀α &gt; 0 to get
GIS defines
f# ≡ maxx,y f#(x, y), f#(x, y) ≡ Pt ft(x, y),
and adds a feature fn+1(x, y)≡f#−f#(x, y) with
zn+1 = 0. Assuming ft(x, y) ≥ 0, ∀t, x, y, and
using Jensen’s inequality
</bodyText>
<equation confidence="0.9873206875">
ePt=1 f# (ztf ) &lt; En+1 ft(x,y)eztf# and
n+1 ft(x,y) #
— t=1 f#
ePt ztft(x,y) ≤ Pt ft(x,y)eztf# + fn+1(x,y) (10)
we obtain n independent one-variable functions:
(ft).
AGISt(zt) =ezt f##1 Px,y P˜(x)Pw(y |x)ft(x, y)
+ Qt(zt).
and Qt(z)≡ 2wtz+z2
2σ2−z P˜
= Px
P˜(x) log Tw+zet(x)
Tw (x) + Qt(z)
(8)≤P P Qt(zt).
x,y P˜ (x)Pw(y|x)(e tztft(x,y)−1)+P (9)
t
</equation>
<page confidence="0.902564">
286
</page>
<bodyText confidence="0.538309">
IIS applies Jensen’s inequality
ePt ft(x,1 (ztf#(x,y)) Pt ft(x,y))eztf#(x,y)
on (9) to get the approximate function
</bodyText>
<equation confidence="0.922425">
AIIS(zt) = Xx,yP˜(x)Pw(y |x)ft(x, y) ez f#((x,y)
+ Qt(zt).
</equation>
<bodyText confidence="0.973397333333333">
SCGIS is a sequential-update method. It replaces
f# in GIS with f#t ≡ maxx,y ft(x, y). Using ztet
as z in (8), a derivation similar to (10) gives
</bodyText>
<equation confidence="0.937362285714286">
eztft(x,y) ≤ ft(x,y)
t eztf# t + f# t −ft(x,y)
f# f# t .
The approximate function of SCGIS is
ASCGISt(zt) =ez f#−1 Px,y P˜(x)Pw(y |x) ft (x, y)
t
+ Qt(zt).
</equation>
<bodyText confidence="0.910768142857143">
We prove the linear convergence of existing IS
methods (proof omitted):
Theorem 1 Assume each sub-problem Ast(zt) is
exactly minimized, where s is IIS, GIS, SCGIS, or
CD. The sequence {wk} generated by any of these
four methods linearly converges. That is, there is
a constant µ ∈ (0, 1) such that
</bodyText>
<equation confidence="0.750608">
L(wk+1)−L(w∗) ≤ (1−µ)(L(wk)−L(w∗)),∀k,
</equation>
<bodyText confidence="0.852396">
where w∗ is the global optimum of (3).
</bodyText>
<subsectionHeader confidence="0.999006">
2.3 Solving one-variable sub-problems
</subsectionHeader>
<bodyText confidence="0.999817">
Without the regularization term, by A′t(zt) = 0,
GIS and SCGIS both have a simple closed-form
solution of the sub-problem. With the regular-
ization term, the sub-problems no longer have a
closed-form solution. We discuss the cost of solv-
ing sub-problems by the Newton method, which
iteratively updates zt by
</bodyText>
<equation confidence="0.993689">
zt ← zt − As ′(zt)/As ′′(zt). (11)
t t
</equation>
<bodyText confidence="0.992836666666667">
Here s indicates an IS or a CD method.
Below we check the calculation of Ast′(zt) as
the cost of As
</bodyText>
<equation confidence="0.929386333333333">
′(zt)=P t ′′(zt) is similar. We have
As x,y P˜ (x)Pw(y|x)ft(x, y)eztfs(x,y)
t (12)
f# if s is GIS,
f#t if s is SCGIS,
f#(x, y) if s is IIS.
For CD, ˜
At D′(zt)=Q′t(zt)+Px,y P(x)Pw+ztet(y |x)ft(x, y).
(13)
</equation>
<bodyText confidence="0.924743666666667">
The main cost is on calculating Pw(y|x) ∀x, y,
whenever w is updated. Parallel-update ap-
proaches calculate Pw(y|x) once every n sub-
problems, but sequential-update methods evalu-
ates Pw(y|x) after every sub-problem. Consider
the situation of updating w to w+ztet. By (1),
</bodyText>
<tableCaption confidence="0.746409">
Table 1: Time for minimizing At(zt) by the New-
ton method CD GIS SCGIS IIS
1st Newton direction O( l) O( l) O( l) O( l)
</tableCaption>
<bodyText confidence="0.983607423076923">
Newton direction
Each subsequent O( l) O(1) O(1) O( l)
obtaining Pw+ztet(y|x) ∀x, y requires expensive
O(#nz) operations to evaluate Sw+ztet(x,y) and
Tw+ztet(x) ∀x, y. A trick to trade memory for
time is to store all Sw(x, y) and Tw(x),
Sw+ztet(x, y)=Sw(x, y)eztft(x,y),
Tw+ztet(x)=Tw(x)+PySw(x, y)(eztft(x,y)−1).
Since Sw+ztet(x, y) = Sw(x, y) if ft(x, y) =
0, this procedure reduces the the O(#nz) opera-
tions to O(#nz/n) = O(¯l). However, it needs
extra spaces to store all Sw(x, y) and Tw(x).
This trick for updating Pw(y|x) has been used
in SCGIS (Goodman, 2002). Thus, the first
Newton iteration of all methods discussed here
takes O(¯l) operations. For each subsequent
Newton iteration, CD needs O(¯l) as it calcu-
lates Pw+ztet(y|x) whenever zt is changed. For
GIS and SCGIS, if Px,y P˜(x)Pw(y|x)ft(x, y)
is stored at the first Newton iteration, then (12)
can be done in O(1) time. For IIS, because
f#(x, y) of (12) depends on x and y, we cannot
store Px,y P˜(x)Pw(y|x)ft(x, y) as in GIS and
SCGIS. Hence each Newton direction needs O( l).
We summarize the cost for solving sub-problems
in Table 1.
</bodyText>
<sectionHeader confidence="0.910855" genericHeader="method">
3 Comparison and a New CD Method
</sectionHeader>
<subsectionHeader confidence="0.999384">
3.1 Comparison of IS/CD methods
</subsectionHeader>
<bodyText confidence="0.9702088">
From the above discussion, an IS or a CD method
falls into a place between two extreme designs:
Easy to minimize At(zt) Hard to minimizeAt(zt)
At(zt) a loose bound At(zt) a tight bound
↔
There is a tradeoff between the tightness to bound
the function difference and the hardness to solve
the sub-problem. To check how IS and CD meth-
ods fit into this explanation, we obtain relation-
ships of their approximate functions:
</bodyText>
<equation confidence="0.996800571428571">
t (zt) ≤ ASCGIS
t (zt) ≤ AGIS
ACD t (zt), (14)
ACD
t (zt) ≤ AIIS
t (zt) ≤ AGIS
t (zt) ∀ zt.
</equation>
<bodyText confidence="0.91910525">
The derivation is omitted. From (14), CD con-
siders more accurate sub-problems than SCGIS
and GIS. However, to solve each sub-problem,
from Table 1, CD’s each Newton step takes more
time. The same situation occurs in comparing
IIS and GIS. Therefore, while a tight At(zt) can
+ Q′t(zt)
where
</bodyText>
<equation confidence="0.918873">
fs(x, y) ≡



</equation>
<page confidence="0.991343">
287
</page>
<bodyText confidence="0.999903">
give faster convergence by handling fewer sub-
problems, the total time may not be less due to
the higher cost of each sub-problem.
</bodyText>
<subsectionHeader confidence="0.998554">
3.2 A Fast CD Method
</subsectionHeader>
<bodyText confidence="0.96227175">
We develop a CD method which is cheaper in
solving each sub-problem but still enjoys fast fi-
nal convergence. This method is modified from
Chang et al. (2008), a CD approach for linear
SVM. We approximately minimize ACD
t (z) by ap-
plying only one Newton iteration. The Newton di-
rection at z = 0 is now
</bodyText>
<equation confidence="0.995152">
d = −ACD�(0)/ACD ��(0). (15)
t t
</equation>
<bodyText confidence="0.99999">
As taking the full Newton direction may not de-
crease the function value, we need a line search
procedure to find λ &gt; 0 such that z = λd satisfies
the following sufficient decrease condition:
</bodyText>
<equation confidence="0.968647">
ACD
t (z)−ACD
t (0) = ACD
t (z) c γzACD�(0), (16)
t
</equation>
<bodyText confidence="0.999113666666667">
where γ is a constant in (0,1/2). A simple
way to find λ is by sequentially checking λ =
1, β, β2, . . . ,where β E (0, 1). The line search
procedure is guaranteed to stop (proof omitted).
We can further prove that near the optimum two
results hold: First, the Newton direction (15) sat-
isfies the sufficient decrease condition (16) with
λ=1. Then the cost for each sub-problem is O( l),
similar to that for exactly solving sub-problems of
GIS or SCGIS. This result is important as other-
wise each trial of z = λd expensively costs O(¯l)
for calculating ACD
t (z). Second, taking one New-
ton direction of the tighter ACD
t (zt) reduces the
function L(w) more rapidly than exactly minimiz-
ing a loose At(zt) of GIS, IIS or SCGIS. These
two results show that the new CD method im-
proves upon the traditional CD by approximately
solving sub-problems, while still maintains fast
convergence.
</bodyText>
<sectionHeader confidence="0.999787" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9725836">
We apply Maxent models to part of
speech (POS) tagging for BROWN corpus
(http://www.nltk.org) and chunk-
ing tasks for CoNLL2000 (http://www.
cnts.ua.ac.be/conll2000/chunking).
We randomly split the BROWN corpus
to 4/5 training and 1/5 testing. Our im-
plementation is built upon OpenNLP
(http://maxent.sourceforge.net).
We implement CD (the new one in Section 3.2),
GIS, SCGIS, and LBFGS for comparisons. We
include LBFGS as Malouf (2002) reported that
it is better than other approaches including GIS
Figure 2: First row: time versus the relative func-
tion value difference (17). Second row: time ver-
sus testing accuracy/F1. Time is in seconds.
and IIS. We use σ2 = 10, and set β = 0.5 and
γ = 0.001 in (16).
We begin at checking time versus the relative
difference of the function value to the optimum:
L(w) − L(w*)/L(w*). (17)
Results are in the first row of Figure 2. We check
in the second row of Figure 2 about testing ac-
curacy/F1 versus training time. Among the three
IS/CD methods compared, the new CD approach
is the fastest. SCGIS comes the second, while
GIS is the last. This result is consistent with
the tightness of their approximation functions; see
(14). LBFGS has fast final convergence, but it
does not perform well in the beginning.
</bodyText>
<sectionHeader confidence="0.999429" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.99998775">
In summary, we create a general framework for
explaining IS methods. Based on this framework,
we develop a new CD method for Maxent. It is
more efficient than existing IS methods.
</bodyText>
<sectionHeader confidence="0.997889" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997255428571429">
K.-W. Chang, C.-J. Hsieh, and C.-J. Lin. 2008. Coor-
dinate descent method for large-scale L2-loss linear
SVM. JMLR, 9:1369–1398.
John N. Darroch and Douglas Ratcliff. 1972. Gener-
alized iterative scaling for log-linear models. Ann.
Math. Statist., 43(5):1470–1480.
Stephen Della Pietra, Vincent Della Pietra, and John
Lafferty. 1997. Inducing features of random fields.
IEEE PAMI, 19(4):380–393.
Joshua Goodman. 2002. Sequential conditional gener-
alized iterative scaling. In ACL, pages 9–16.
Robert Malouf. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In
CONLL.
</reference>
<figure confidence="0.999651">
(a) BROWN
(b) CoNLL2000
Training Time (s) Training Time (s)
(c) BROWN (d) CoNLL2000
Relative function value difference
101
100
10−1
10−2
0 500 1000 1500 2000
CD
SCGIS
GIS
LBFGS
Relative function value difference
102
101
100
10
10−2
0 50 100 150 200
−1
CD
SCGIS
GIS
LBFGS
Training Time (s)
Training Time (s)
Testing Accuracy
96.5
95.5
94.5
97
96
95
94
0
500 1000 1500 2000
CD
SCGIS
GIS
LBFGS
F1 measure
93.5
92.5
91.5
90.5
93
92
91
90
0 50 100 150 200
CD
SCGIS
GIS
LBFGS
</figure>
<page confidence="0.948163">
288
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.776392">
<title confidence="0.998673">Iterative Scaling and Coordinate Descent Methods for Maximum Entropy</title>
<author confidence="0.999665">Fang-Lan Huang</author>
<author confidence="0.999665">Cho-Jui Hsieh</author>
<author confidence="0.999665">Kai-Wei Chang</author>
<author confidence="0.999665">Chih-Jen Lin</author>
<affiliation confidence="0.999262">Department of Computer Science National Taiwan University</affiliation>
<address confidence="0.985357">Taipei 106, Taiwan</address>
<abstract confidence="0.978906923076923">Maximum entropy (Maxent) is useful in areas. Iterative scaling methods are one of the most popular approaches to Maxent. With many variants of methods, it is difficult to understand them and see the differences. In this paper, we create a general and unified framework for This framework also connects coordinate descent methods. we develop a for Maxent. Results show that it is faster than iterative scaling</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K-W Chang</author>
<author>C-J Hsieh</author>
<author>C-J Lin</author>
</authors>
<title>Coordinate descent method for large-scale L2-loss linear SVM.</title>
<date>2008</date>
<pages>9--1369</pages>
<publisher>JMLR,</publisher>
<contexts>
<context position="11689" citStr="Chang et al. (2008)" startWordPosition="2014" endWordPosition="2017"> derivation is omitted. From (14), CD considers more accurate sub-problems than SCGIS and GIS. However, to solve each sub-problem, from Table 1, CD’s each Newton step takes more time. The same situation occurs in comparing IIS and GIS. Therefore, while a tight At(zt) can + Q′t(zt) where fs(x, y) ≡    287 give faster convergence by handling fewer subproblems, the total time may not be less due to the higher cost of each sub-problem. 3.2 A Fast CD Method We develop a CD method which is cheaper in solving each sub-problem but still enjoys fast final convergence. This method is modified from Chang et al. (2008), a CD approach for linear SVM. We approximately minimize ACD t (z) by applying only one Newton iteration. The Newton direction at z = 0 is now d = −ACD�(0)/ACD ��(0). (15) t t As taking the full Newton direction may not decrease the function value, we need a line search procedure to find λ &gt; 0 such that z = λd satisfies the following sufficient decrease condition: ACD t (z)−ACD t (0) = ACD t (z) c γzACD�(0), (16) t where γ is a constant in (0,1/2). A simple way to find λ is by sequentially checking λ = 1, β, β2, . . . ,where β E (0, 1). The line search procedure is guaranteed to stop (proof o</context>
</contexts>
<marker>Chang, Hsieh, Lin, 2008</marker>
<rawString>K.-W. Chang, C.-J. Hsieh, and C.-J. Lin. 2008. Coordinate descent method for large-scale L2-loss linear SVM. JMLR, 9:1369–1398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John N Darroch</author>
<author>Douglas Ratcliff</author>
</authors>
<title>Generalized iterative scaling for log-linear models.</title>
<date>1972</date>
<journal>Ann. Math. Statist.,</journal>
<volume>43</volume>
<issue>5</issue>
<contexts>
<context position="2083" citStr="Darroch and Ratcliff (1972)" startWordPosition="325" endWordPosition="329">and P˜(ft) = Px,y P˜(x, y)ft(x, y) is the expected value of ft(x, y). To avoid overfitting the training samples, some add a regularization term and solve: P(ft) w2 + 2Q2, t (3) 1A complete version of this work is at http: //www.csie.ntu.edu.tw/˜cjlin/papers/ maxent_journal.pdf. where σ is a regularization parameter. We focus on (3) instead of (2) because (3) is strictly convex. Iterative scaling (IS) methods are popular in training Maxent models. They all share the same property of solving a one-variable sub-problem at a time. Existing IS methods include generalized iterative scaling (GIS) by Darroch and Ratcliff (1972), improved iterative scaling (IIS) by Della Pietra et al. (1997), and sequential conditional generalized iterative scaling (SCGIS) by Goodman (2002). In optimization, coordinate descent (CD) is a popular method which also solves a one-variable sub-problem at a time. With these many IS and CD methods, it is uneasy to see their differences. In Section 2, we propose a unified framework to describe IS and CD methods from an optimization viewpoint. Using this framework, we design a fast CD approach for Maxent in Section 3. In Section 4, we compare the proposed CD method with IS and LBFGS methods. R</context>
</contexts>
<marker>Darroch, Ratcliff, 1972</marker>
<rawString>John N. Darroch and Douglas Ratcliff. 1972. Generalized iterative scaling for log-linear models. Ann. Math. Statist., 43(5):1470–1480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>John Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1997</date>
<journal>IEEE PAMI,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="2147" citStr="Pietra et al. (1997)" startWordPosition="336" endWordPosition="339">o avoid overfitting the training samples, some add a regularization term and solve: P(ft) w2 + 2Q2, t (3) 1A complete version of this work is at http: //www.csie.ntu.edu.tw/˜cjlin/papers/ maxent_journal.pdf. where σ is a regularization parameter. We focus on (3) instead of (2) because (3) is strictly convex. Iterative scaling (IS) methods are popular in training Maxent models. They all share the same property of solving a one-variable sub-problem at a time. Existing IS methods include generalized iterative scaling (GIS) by Darroch and Ratcliff (1972), improved iterative scaling (IIS) by Della Pietra et al. (1997), and sequential conditional generalized iterative scaling (SCGIS) by Goodman (2002). In optimization, coordinate descent (CD) is a popular method which also solves a one-variable sub-problem at a time. With these many IS and CD methods, it is uneasy to see their differences. In Section 2, we propose a unified framework to describe IS and CD methods from an optimization viewpoint. Using this framework, we design a fast CD approach for Maxent in Section 3. In Section 4, we compare the proposed CD method with IS and LBFGS methods. Results show that the CD method is more efficient. Notation n is </context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1997</marker>
<rawString>Stephen Della Pietra, Vincent Della Pietra, and John Lafferty. 1997. Inducing features of random fields. IEEE PAMI, 19(4):380–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Sequential conditional generalized iterative scaling.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="2231" citStr="Goodman (2002)" startWordPosition="349" endWordPosition="350">w2 + 2Q2, t (3) 1A complete version of this work is at http: //www.csie.ntu.edu.tw/˜cjlin/papers/ maxent_journal.pdf. where σ is a regularization parameter. We focus on (3) instead of (2) because (3) is strictly convex. Iterative scaling (IS) methods are popular in training Maxent models. They all share the same property of solving a one-variable sub-problem at a time. Existing IS methods include generalized iterative scaling (GIS) by Darroch and Ratcliff (1972), improved iterative scaling (IIS) by Della Pietra et al. (1997), and sequential conditional generalized iterative scaling (SCGIS) by Goodman (2002). In optimization, coordinate descent (CD) is a popular method which also solves a one-variable sub-problem at a time. With these many IS and CD methods, it is uneasy to see their differences. In Section 2, we propose a unified framework to describe IS and CD methods from an optimization viewpoint. Using this framework, we design a fast CD approach for Maxent in Section 3. In Section 4, we compare the proposed CD method with IS and LBFGS methods. Results show that the CD method is more efficient. Notation n is the number of features. The total number of nonzeros in samples and the average numb</context>
<context position="9953" citStr="Goodman, 2002" startWordPosition="1701" endWordPosition="1702">IS 1st Newton direction O( l) O( l) O( l) O( l) Newton direction Each subsequent O( l) O(1) O(1) O( l) obtaining Pw+ztet(y|x) ∀x, y requires expensive O(#nz) operations to evaluate Sw+ztet(x,y) and Tw+ztet(x) ∀x, y. A trick to trade memory for time is to store all Sw(x, y) and Tw(x), Sw+ztet(x, y)=Sw(x, y)eztft(x,y), Tw+ztet(x)=Tw(x)+PySw(x, y)(eztft(x,y)−1). Since Sw+ztet(x, y) = Sw(x, y) if ft(x, y) = 0, this procedure reduces the the O(#nz) operations to O(#nz/n) = O(¯l). However, it needs extra spaces to store all Sw(x, y) and Tw(x). This trick for updating Pw(y|x) has been used in SCGIS (Goodman, 2002). Thus, the first Newton iteration of all methods discussed here takes O(¯l) operations. For each subsequent Newton iteration, CD needs O(¯l) as it calculates Pw+ztet(y|x) whenever zt is changed. For GIS and SCGIS, if Px,y P˜(x)Pw(y|x)ft(x, y) is stored at the first Newton iteration, then (12) can be done in O(1) time. For IIS, because f#(x, y) of (12) depends on x and y, we cannot store Px,y P˜(x)Pw(y|x)ft(x, y) as in GIS and SCGIS. Hence each Newton direction needs O( l). We summarize the cost for solving sub-problems in Table 1. 3 Comparison and a New CD Method 3.1 Comparison of IS/CD metho</context>
</contexts>
<marker>Goodman, 2002</marker>
<rawString>Joshua Goodman. 2002. Sequential conditional generalized iterative scaling. In ACL, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
</authors>
<title>A comparison of algorithms for maximum entropy parameter estimation.</title>
<date>2002</date>
<booktitle>In CONLL.</booktitle>
<contexts>
<context position="13441" citStr="Malouf (2002)" startWordPosition="2325" endWordPosition="2326">CGIS. These two results show that the new CD method improves upon the traditional CD by approximately solving sub-problems, while still maintains fast convergence. 4 Experiments We apply Maxent models to part of speech (POS) tagging for BROWN corpus (http://www.nltk.org) and chunking tasks for CoNLL2000 (http://www. cnts.ua.ac.be/conll2000/chunking). We randomly split the BROWN corpus to 4/5 training and 1/5 testing. Our implementation is built upon OpenNLP (http://maxent.sourceforge.net). We implement CD (the new one in Section 3.2), GIS, SCGIS, and LBFGS for comparisons. We include LBFGS as Malouf (2002) reported that it is better than other approaches including GIS Figure 2: First row: time versus the relative function value difference (17). Second row: time versus testing accuracy/F1. Time is in seconds. and IIS. We use σ2 = 10, and set β = 0.5 and γ = 0.001 in (16). We begin at checking time versus the relative difference of the function value to the optimum: L(w) − L(w*)/L(w*). (17) Results are in the first row of Figure 2. We check in the second row of Figure 2 about testing accuracy/F1 versus training time. Among the three IS/CD methods compared, the new CD approach is the fastest. SCGI</context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>Robert Malouf. 2002. A comparison of algorithms for maximum entropy parameter estimation. In CONLL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>