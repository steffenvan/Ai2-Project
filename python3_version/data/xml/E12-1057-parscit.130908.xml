<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000038">
<title confidence="0.96402">
The effect of domain and text type on text prediction quality
</title>
<author confidence="0.981911">
Suzan Verberne, Antal van den Bosch, Helmer Strik, Lou Boves
</author>
<affiliation confidence="0.9868">
Centre for Language Studies
Radboud University Nijmegen
</affiliation>
<email confidence="0.986983">
s.verberne@let.ru.nl
</email>
<sectionHeader confidence="0.993657" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998708">
Text prediction is the task of suggesting
text while the user is typing. Its main aim
is to reduce the number of keystrokes that
are needed to type a text. In this paper, we
address the influence of text type and do-
main differences on text prediction quality.
By training and testing our text predic-
tion algorithm on four different text types
(Wikipedia, Twitter, transcriptions of con-
versational speech and FAQ) with equal
corpus sizes, we found that there is a clear
effect of text type on text prediction qual-
ity: training and testing on the same text
type gave percentages of saved keystrokes
between 27 and 34%; training on a differ-
ent text type caused the scores to drop to
percentages between 16 and 28%.
In our case study, we compared a num-
ber of training corpora for a specific data
set for which training data is sparse: ques-
tions about neurological issues. We found
that both text type and topic domain play
a role in text prediction quality. The
best performing training corpus was a set
of medical pages from Wikipedia. The
second-best result was obtained by leave-
one-out experiments on the test questions,
even though this training corpus was much
smaller (2,672 words) than the other cor-
pora (1.5 Million words).
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.985305425531915">
Text prediction is the task of suggesting text while
the user is typing. Its main aim is to reduce the
number of keystrokes that are needed to type a
text, thereby saving time. Text prediction algo-
rithms have been implemented for mobile devices,
office software (Open Office Writer), search en-
gines (Google query completion), and in special-
needs software for writers who have difficulties
typing (Garay-Vitoria and Abascal, 2006). In most
applications, the scope of the prediction is the
completion of the current word; hence the often-
used term ‘word completion’.
The most basic method for word completion is
checking after each typed character whether the
prefix typed since the last whitespace is unique
according to a lexicon. If it is, the algorithm sug-
gests to complete the prefix with the lexicon en-
try. The algorithm may also suggest to complete a
prefix even before the word’s uniqueness point is
reached, using statistical information on the pre-
vious context. Moreover, it has been shown that
significantly better prediction results can be ob-
tained if not only the prefix of the current word
is included as previous context, but also previ-
ous words (Fazly and Hirst, 2003) or characters
(Van den Bosch and Bogers, 2008).
In the current paper, we follow up on this work
by addressing the influence of text type and do-
main differences on text prediction quality. Brief
messages on mobile devices (such as text mes-
sages, Twitter and Facebook updates) are of a dif-
ferent style and lexicon than documents typed in
office software (Westman and Freund, 2010). In
addition, the topic domain of the text also influ-
ences its content. These differences may cause an
algorithm trained on one text type or domain to
perform poorly on another.
The questions that we aim to answer in this pa-
per are (1) “What is the effect of text type dif-
ferences on the quality of a text prediction algo-
rithm?” and (2) “What is the best choice of train-
ing data if domain- and text type-specific data is
sparse?”. To answer these questions, we perform
three experiments:
1. A series of within-text type experiments on
four different types of Dutch text: Wikipedia
articles, Twitter data, transcriptions of con-
</bodyText>
<page confidence="0.965361">
561
</page>
<note confidence="0.978908">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 561–569,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<listItem confidence="0.881699692307692">
versational speech and web pages of Fre-
quently Asked Questions (FAQ).
2. A series of across-text type experiments in
which we train and test on different text
types;
3. A case study using texts from a specific do-
main and text type: questions about neuro-
logical issues. Training data for this combi-
nation of language (Dutch), text type (FAQ)
and domain (medical/neurological) is sparse.
Therefore, we search for the type of training
data that gives the best prediction results for
this corpus. We compare the following train-
ing corpora:
• The corpora that we compared in the
text type experiments: Wikipedia, Twit-
ter, Speech and FAQ, 1.5 Million words
per corpus.
• A 1.5 Million words training corpus that
is of the same domain as the target data:
medical pages from Wikipedia;
• The 359 questions from the neuro-QA
data themselves, evaluated in a leave-
one-out setting (359 times training on
358 questions and evaluating on the re-
maining questions).
</listItem>
<bodyText confidence="0.999900642857143">
The prospective application of the third series
of experiments is the development of a text predic-
tion algorithm in an online care platform: an on-
line community for patients seeking information
about their illness. In this specific case the target
group is patients with language disabilities due to
neurological disorders.
The remainder of this paper is organized as fol-
lows: In Section 2 we give a brief overview of text
prediction methods discussed in the literature. In
Section 3 we present our approach to text predic-
tion. Sections 4 and 5 describe the experiments
that we carried out and the results we obtained.
We phrase our conclusions in Section 6.
</bodyText>
<sectionHeader confidence="0.961984" genericHeader="method">
2 Text prediction methods
</sectionHeader>
<bodyText confidence="0.999889169491526">
Text prediction methods have been developed for
several different purposes. The older algorithms
were built as communicative devices for people
with disabilities, such as motor and speech impair-
ments. More recently, text prediction is developed
for writing with reduced keyboards, specifically
for writing (composing messages) on mobile de-
vices (Garay-Vitoria and Abascal, 2006).
All modern methods share the general idea that
previous context (which we will call the ‘buffer’)
can be used to predict the next block of charac-
ters (the ‘predictive unit’). If the user gets correct
suggestions for continuation of the text then the
number of keystrokes needed to type the text is
reduced. The unit to be predicted by a text pre-
diction algorithm can be anything ranging from a
single character (which actually does not save any
keystrokes) to multiple words. Single words are
the most widely used as prediction units because
they are recognizable at a low cognitive load for
the user, and word prediction gives good results
in terms of keystroke savings (Garay-Vitoria and
Abascal, 2006).
There is some variation among methods in the
size and type of buffer used. Most methods use
character n-grams as buffer, because they are pow-
erful and can be implemented independently of the
target language (Carlberger, 1997). In many al-
gorithms the buffer is cleared at the start of each
new word (making the buffer never larger than
the length of the current word). In the paper
by (Van den Bosch and Bogers, 2008), two ex-
tensions to the basic prefix-model are compared.
They found that an algorithm that uses the previ-
ous n characters as buffer, crossing word borders
without clearing the buffer, performs better than
both a prefix character model and an algorithm
that includes the full previous word as feature. In
addition to using the previously typed characters
and/or words in the buffer, word characteristics
such as frequency and recency could also be taken
into account (Garay-Vitoria and Abascal, 2006).
Possible evaluation measures for text predic-
tion are the proportion of words that are correctly
predicted, the percentage of keystrokes that could
maximally be saved (if the user would always
make the correct decision), and the time saved by
the use of the algorithm (Garay-Vitoria and Abas-
cal, 2006). The performance that can be obtained
by text prediction algorithms depends on the lan-
guage they are evaluated on. Lower results are ob-
tained for higher-inflected languages such as Ger-
man than for low-inflected languages such as En-
glish (Matiasek et al., 2002). In their overview of
text prediction systems, (Garay-Vitoria and Abas-
cal, 2006) report performance scores ranging from
29% to 56% of keystrokes saved.
An important factor that is known to influence
the quality of text prediction systems, is training
</bodyText>
<page confidence="0.995229">
562
</page>
<bodyText confidence="0.999859">
set size (Lesher et al., 1999; Van den Bosch,
2011). The paper by (Van den Bosch, 2011) shows
log-linear learning curves for word prediction (a
constant improvement each time the training cor-
pus size is doubled), when the training set size is
increased incrementally from 102 to 3∗107 words.
</bodyText>
<sectionHeader confidence="0.825569" genericHeader="method">
3 Our approach to text prediction
</sectionHeader>
<bodyText confidence="0.9999471">
We implement a text prediction algorithm for
Dutch, which is a productive compounding lan-
guage like German, but has a somewhat simpler
inflectional system. We do not focus on the effect
of training set size, but on the effect of text type
and topic domain differences.
Our approach to text prediction is largely in-
spired by (Van den Bosch and Bogers, 2008). We
experiment with two different buffer types that are
based on character n-grams:
</bodyText>
<listItem confidence="0.899130428571429">
• ‘Prefix of current word’ contains all char-
acters of only the word currently keyed in,
where the buffer shifts by one character posi-
tion with every new character.
• ‘Buffer15’ buffer also includes any other
characters keyed in belonging to previously
keyed-in words.
</listItem>
<bodyText confidence="0.99961056">
Modeling character history beyond the current
word can naturally be done with a buffer model in
which the buffer shifts by one position per charac-
ter, while a typical left-aligned prefix model (that
never shifts and fixes letters to their positional fea-
ture) would not be able to do this.
In the buffer, all characters from the text are
kept, including whitespace and punctuation. The
predictive unit is one token (word or punctuation
symbol). In both the buffer and the prediction la-
bel, any capitalization is kept. At each point in the
typing process, our algorithm gives one sugges-
tion: the word that is the most likely continuation
of the current buffer.
We save the training data as a classification data
set: each character in the buffer fills a feature slot
and the word that is to be predicted is the classi-
fication label. Figures 1 and 2 give examples of
each of the buffer types Prefix and Buffer15 that
we created for the text fragment “tot een niveau”
in the context “stelselmatig bij elke verkiezing tot
een niveau van’ ’(structurally with each election
to a level of). We use the implementation of the
IGTree decision tree algorithm in TiMBL (Daele-
mans et al., 1997) to train our models.
</bodyText>
<subsectionHeader confidence="0.99663">
3.1 Evaluation
</subsectionHeader>
<bodyText confidence="0.98304625">
We evaluate our algorithms on corpus data. This
means that we have to make assumptions about
user behaviour. We assume that the user confirms
a suggested word as soon as it is suggested cor-
rectly, not typing any additional characters before
confirming. We evaluate our text prediction al-
gorithms in terms of the percentage of keystrokes
saved K:
</bodyText>
<equation confidence="0.971028333333333">
o
K = �Z (Fi) − �Z 0(Wi) * 100 (1)
EZ 0(Fi)
</equation>
<bodyText confidence="0.999961714285714">
in which n is the number of words in the test
set, Wi is the number of keystrokes that have been
typed before the word i is correctly suggested
and Fi is the number of keystrokes that would be
needed to type the complete word i. For example,
our algorithm correctly predicts the word niveau
after the context i n g t o t e e n n i
v in the test set. Assuming that the user confirms
the word niveau at this point, three keystrokes
were needed for the prefix niv. So, Wi = 3 and
Fi = 6. The number of keystrokes needed for
whitespace and punctuation are unchanged: these
have to be typed anyway, independently of the
support by a text prediction algorithm.
</bodyText>
<sectionHeader confidence="0.928644" genericHeader="method">
4 Text type experiments
</sectionHeader>
<bodyText confidence="0.9999065">
In this section, we describe the first and second se-
ries of experiments. The case study on questions
from the neurological domain is described in Sec-
tion 5.
</bodyText>
<subsectionHeader confidence="0.989804">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999968571428571">
In the text type experiments, we evaluate our text
prediction algorithm on four different types of
Dutch text: Wikipedia, Twitter data, transcriptions
of conversational speech, and web pages of Fre-
quently Asked Questions (FAQ). The Wikipedia
corpus that we use is part of the Lassy cor-
pus (Van Noord, 2009); we obtained a version
from the summer of 2010.1 The Twitter data
are collected continuously and automatically fil-
tered for language by Erik Tjong Kim Sang (Tjong
Kim Sang, 2011). We used the tweets from all
users that posted at least 19 tweets (excluding
retweets) during one day in June 2011. This is
a set of 1 Million Twitter messages from 30,000
</bodyText>
<footnote confidence="0.99771">
1http://www.let.rug.nl/vannoord/trees/Treebank/Machine/
NLWIKI20100826/COMPACT/
</footnote>
<page confidence="0.97797">
563
</page>
<equation confidence="0.818424058823529">
n i v e a u
n i v e a
n i v e
t o t
e e n
n i v
t o
e e
n i
t
e
n
tot
tot
tot
een
een
</equation>
<bodyText confidence="0.408629714285714">
een
niveau
niveau
niveau
niveau
niveau
niveau
</bodyText>
<figureCaption confidence="0.9972735">
Figure 1: Example of buffer type ‘Prefix’ for the text fragment “(elke verkiezing) tot een niveau”. Un-
derscores represent whitespaces.
</figureCaption>
<bodyText confidence="0.9655858">
l k e v e r k i e z i n g tot
k e v e r k i e z i n g t tot
e v e r k i e z i n g t o tot
v e r k i e z i n g t o t tot
v e r k i e z i n g t o t een
e r k i e z i n g t o t e een
r k i e z i n g t o t e e een
k i e z i n g t o t e e n een
i e z i n g t o t e e n niveau
e z i n g t o t e e n n niveau
z i n g t o t e e n n i niveau
i n g t o t e e n n i v niveau
n g t o t e e n n i v e niveau
g t o t e e n n i v e a niveau
t o t e e n n i v e a u niveau
</bodyText>
<figureCaption confidence="0.988616">
Figure 2: Example of buffer type ‘Buffer15’ for the text fragment “(elke verkiezing) tot een niveau”.
Underscores represent whitespaces.
</figureCaption>
<bodyText confidence="0.9931066">
different users. The transcriptions of conversa-
tional speech are from the Spoken Dutch Corpus
(CGN) (Oostdijk, 2000); for our experiments, we
only use the category ‘spontaneous speech’. We
obtained the FAQ data by downloading the first
1,000 pages that Google returns for the query ‘faq’
with the language restriction Dutch. After clean-
ing the pages from HTML and other coding, the
resulting corpus contained approximately 1.7 Mil-
lion words of questions and answers.
</bodyText>
<subsectionHeader confidence="0.998609">
4.2 Within-text type experiments
</subsectionHeader>
<bodyText confidence="0.9999294">
For each of the four text types, we compare the
buffer types ‘Prefix’ and ‘Buffer15’. In each ex-
periment, we use 1.5 Million words from the cor-
pus to train the algorithm and 100,000 words to
test it. The results are in Table 1.
</bodyText>
<subsectionHeader confidence="0.999759">
4.3 Across-text type experiments
</subsectionHeader>
<bodyText confidence="0.9849105">
We investigate the importance of text type differ-
ences for text prediction with a series of experi-
ments in which we train and test our algorithm on
texts of different text types. We keep the size of
the train and test sets the same: 1.5 Million words
and 100,000 words respectively. The results are in
</bodyText>
<tableCaption confidence="0.583457">
Table 2.
</tableCaption>
<subsectionHeader confidence="0.997507">
4.4 Discussion of the results
</subsectionHeader>
<bodyText confidence="0.9975785">
Table 1 shows that for all text types, the buffer
of 15 characters that crosses word borders gives
better results than the prefix of the current word
only. We get a relative improvement of 35% (for
FAQ) to 62% (for Speech) of Buffer15 compared
to Prefix-only.
Table 2 shows that text type differences have
an influence on text prediction quality: all across-
text type experiments lead to lower results than
the within-text type experiments. From the re-
sults in Table 2, we can deduce that of the four
text types, speech and Twitter language resem-
ble each other more than they resemble the other
two, and Wikipedia and FAQ resemble each other
more. Twitter and Wikipedia data are the least
similar: training on Wikipedia data makes the text
prediction score for Twitter data drop from 29.2 to
16.5%.2
</bodyText>
<footnote confidence="0.803353">
2Note that the results are not symmetric. For example,
</footnote>
<page confidence="0.996319">
564
</page>
<tableCaption confidence="0.808758666666667">
Table 1: Results from the within-text type experiments in terms of percentages of saved keystrokes.
Prefix means: ‘use the previous characters of the current word as features’. Buffer 15 means ‘use a buffer
of the previous 15 characters as features’.
</tableCaption>
<table confidence="0.9998406">
Prefix Buffer15
Wikipedia 22.2% 30.5%
Twitter 21.3% 29.2%
Speech 20.7% 33.4%
FAQ 20.2% 27.2%
</table>
<tableCaption confidence="0.974296">
Table 2: Results from the across-text type experiments in terms of percentages of saved keystrokes, using
the best-scoring configuration from the within-text type experiments: a buffer of 15 characters
</tableCaption>
<table confidence="0.9980112">
Trained on Tested on Wikipedia Tested on Twitter Tested on Speech Tested on FAQ
Wikipedia 30.5% 16.5% 22.3% 24.9%
Twitter 17.9% 29.2% 27.9% 20.7%
Speech 19.7% 22.5% 33.4% 21.0%
FAQ 22.6% 18.2% 22.9% 27.2%
</table>
<sectionHeader confidence="0.563958" genericHeader="method">
5 Case study: questions about
neurological issues
</sectionHeader>
<bodyText confidence="0.999494833333333">
Online care platforms aim to bring together pa-
tients and experts. Through this medium, patients
can find information about their illness, and get in
contact with fellow-sufferers. Patients who suffer
from neurological damage may have communica-
tive disabilities because their speaking and writ-
ing skills are impaired. For these patients, existing
online care platforms are often not easily accessi-
ble. Aphasia, for example, hampers the exchange
of information because the patient has problems
with word finding.
In the project ‘Communicatie en revalidatie
DigiPoli’ (ComPoli), language and speech tech-
nologies are implemented in the infrastructure of
an existing online care platform in order to fa-
cilitate communication for patients suffering from
neurological damage. Part of the online care plat-
form is a list of frequently asked questions about
neurological diseases with answers. A user can
browse through the questions using a chat-by-click
interface (Geuze et al., 2008). Besides reading the
listed questions and answers, the user has the op-
tion to submit a question that is not yet included in
training on Wikipedia, testing on Twitter gives a different re-
sult from training on Twitter, testing on Wikipedia. This is
due to the size and domain of the vocabularies in both data
sets and the richness of the contexts (in order for the algo-
rithm to predict a word, it has to have seen it in the train set).
If the test set has a larger vocabulary than the train set, a lower
proportion of words can be predicted than when it is the other
way around.
the list. The newly submitted questions are sent to
an expert who answers them and adds both ques-
tion and answer to the chat-by-click database. In
typing the question to be submitted, the user will
be supported by a text prediction application.
The aim of this section is to find the best train-
ing corpus for newly formulated questions in the
neurological domain. We realize that questions
formulated by users of a web interface are dif-
ferent from questions formulated by experts for
the purpose of a FAQ-list. Therefore, we plan to
gather real user data once we have a first version
of the user interface running online. For develop-
ing the text prediction algorithm that is behind the
initial version of the application, we aim to find
the best training corpus using the questions from
the chat-by-click data as training set.
</bodyText>
<subsectionHeader confidence="0.977269">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.9996229">
The chat-by-click data set on neurological issues
consists of 639 questions with corresponding an-
swers. A small sample of the data (translated to
English) is shown in Table 3. In order to create the
test data for our experiments, we removed dupli-
cate questions from the chat-by-click data, leaving
a set of 359 questions.3
In the previous sections, we used corpora of
100,000 words as test collections and we calcu-
lated the percentage of saved keystrokes over the
</bodyText>
<footnote confidence="0.966455666666667">
3Some questions and answers are repeated several times
in the chat-by-click data because they are located at different
places in the chat-by-click hierarchy.
</footnote>
<page confidence="0.99873">
565
</page>
<tableCaption confidence="0.999578">
Table 3: A sample of the neuro-QA data, translated to English.
</tableCaption>
<bodyText confidence="0.992468222222222">
question 0 505 Can (P)LS be cured?
answer 0 505 Unfortunately, a real cure is not possible. However, things can be done to combat the effects of the
diseases, mainly relieving symptoms such as stiffness and spasticity. The phisical therapist and reha-
bilitation specialist can play a major role in symptom relief. Moreover, there are medications that can
reduce spasticity.
question 0 508 How is (P)LS diagnosed?
answer 0 508 The diagnosis PLS is difficult to establish, especially because the symptoms strongly resemble HSP
symptoms (Strumpell’s disease). Apart from blood and muscle research, several neurological examina-
tions will be carried out.
</bodyText>
<tableCaption confidence="0.66083575">
Table 4: Results for the neuro-QA questions only in terms of percentages of saved keystrokes, using
different training sets. The text prediction configuration used in all settings is Buffer15. The test samples
are 359 questions with an average length of 7.5 words. The percentages of saved keystrokes are means
over the 359 questions.
</tableCaption>
<table confidence="0.998797875">
Training corpus # words Mean % of saved keystrokes in OOV-rate
neuro-QA questions (stdev)
Twitter 1.5 Million 13.3% (12.5) 28.5%
Speech 1.5 Million 14.1% (13.2) 26.6%
Wikipedia 1.5 Million 16.1% (13.1) 19.4%
FAQ 1.5 Million 19.4% (15.6) 20.0%
Medical Wikipedia 1.5 Million 28.1% (16.5) 7.0%
Neuro-QA questions (leave-one-out) 2,672 26.5% (19.9) 17.8%
</table>
<bodyText confidence="0.999095857142857">
complete test corpus. In the reality of our case
study however, users will type only brief frag-
ments of text: the length of the question they want
to submit. This means that there is potentially a
large deviation in the effectiveness of the text pre-
diction algorithm per user, depending on the con-
tent of the small text they are typing. Therefore,
we decided to evaluate our training corpora sepa-
rately on each of the 359 unique questions, so that
we can report both mean and standard deviation
of the text prediction scores on small (realistically
sized) samples. The average number of words per
question is 7.5; the total size of the neuro-QA cor-
pus is 2,672 words.
</bodyText>
<subsectionHeader confidence="0.993103">
5.2 Experiments
</subsectionHeader>
<bodyText confidence="0.999996666666667">
We aim to find the training set that gives the best
text prediction result for the neuro-QA questions.
We compare the following training corpora:
</bodyText>
<listItem confidence="0.989291666666667">
• The corpora that we compared in the text type
experiments: Wikipedia, Twitter, Speech and
FAQ, 1.5 Million words per corpus.
• A 1.5 Million words training corpus that is
of the same topic domain as the target data:
Wikipedia articles from the medical domain;
• The 359 questions from the neuro-QA data
themselves, evaluated in a leave-one-out set-
ting (359 times training on 358 questions and
</listItem>
<bodyText confidence="0.996029157894737">
evaluating on the remaining questions).
In order to create the ‘medical Wikipedia’ cor-
pus, we consulted the category structure of the
Wikipedia corpus. The Wikipedia category ‘Ge-
neeskunde’ (Medicine) contains 69,898 pages and
in the deeper nodes of the hierarchy we see many
non-medical pages, such as trappist beers (or-
dered under beer, booze, alcohol, Psychoactive
drug, drug, and then medicine). If we remove all
pages that are more than five levels under the ‘Ge-
neeskunde’ category root, 21,071 pages are left,
which contain fairly over the 1.5 Million words
that we need. We used the first 1.5 Million words
of the corpus in our experiments.
The text prediction results for the different cor-
pora are in Table 4. For each corpus, the out-of-
vocabulary rate is given: the percentage of words
in the Neuro-QA questions that do not occur in the
corpus.4
</bodyText>
<subsectionHeader confidence="0.999515">
5.3 Discussion of the results
</subsectionHeader>
<bodyText confidence="0.99848575">
We measured the statistical significance of the
mean differences between all text prediction
scores using a Wilcoxon Signed Rank test on
paired results for the 359 questions. We found that
</bodyText>
<footnote confidence="0.962175333333333">
4The OOV-rate for the Neuro-QA corpus itself is the av-
erage of the OOV-rate of each leave-one-out experiment: the
proportion of words that only occur in one question.
</footnote>
<page confidence="0.997014">
566
</page>
<tableCaption confidence="0.4728295">
ECDFs for text prediction scores on Neuro−QA questions
using six different training corpora
</tableCaption>
<figure confidence="0.570038">
Text prediction scores
</figure>
<figureCaption confidence="0.9883305">
Figure 3: Empirical CDFs for text prediction scores on Neuro-QA data. Note that the curves that are at
the bottom-right side represent the better-performing settings.
</figureCaption>
<figure confidence="0.996157111111111">
0 10 20 30 40 50 60
Cumulative Percent of test corpus
0.0 0.2 0.4 0.6 0.8 1.0
Twitter
Speech
Wikipedia
FAQ
Neuro−QA (leave−one−out)
Medical Wikipedia
</figure>
<bodyText confidence="0.9999311">
the difference between the Twitter and Speech cor-
pora on the task is not significant (P = 0.18).
The difference between Neuro-QA and Medical
Wikipedia is significant with P = 0.02; all other
differences are significant with P &lt; 0.01.
The Medical Wikipedia corpus and the leave-
one-out experiments on the Neuro-QA data give
better text prediction scores than the other corpora.
The Medical Wikipedia even scores slightly better
than the Neuro-QA data itself. Twitter and Speech
are the least-suited training corpora for the Neuro-
QA questions, and FAQ data gives a bit better re-
sults than a general Wikipedia corpus.
These results suggest that both text type and
topic domain play a role in text prediction qual-
ity, but the high scores for the Medical Wikipedia
corpus shows that topic domain is even more im-
portant than text type.5 The column ‘OOV-rate’
shows that this is probably due to the high cover-
age of terms in the Neuro-QA data by the Medical
</bodyText>
<footnote confidence="0.53792025">
5We should note here that we did not control for domain
differences between the four different text types. They are
intended to be ‘general domain’ but Wikipedia articles will
naturally be of different topics than conversational speech.
</footnote>
<bodyText confidence="0.999742833333333">
Wikipedia corpus.
Table 4 also shows that the standard devia-
tion among the 359 samples is relatively large.
For some questions, we 0% of the keystrokes are
saved, while for other, scores of over 80% are ob-
tained (by the Neuro-QA and Medical Wikipedia
training corpora). We further analyzed the differ-
ences between the training sets by plotting the Em-
pirical Cumulative Distribution Function (ECDF)
for each experiment. An ECDF shows the devel-
opment of text prediction scores (shown on the X-
axis) by walking through the test set in 359 steps
(shown on the Y-axis).
The ECDFs for our training corpora are in Fig-
ure 3. Note that the curves that are at the bottom-
right side represent the better-performing settings
(they get to a higher maximum after having seen
a smaller portion of the samples). From Figure 3,
it is again clear that the Neuro-QA and Medical
Wikipedia corpora outperform the other training
corpora, and that of the other four, FAQ is the best-
performing corpus. Figure 3 also shows a large
difference in the sizes of the starting percentiles:
The proportion of samples with a text prediction
</bodyText>
<page confidence="0.97805">
567
</page>
<figure confidence="0.940782833333333">
Histogram of text prediction scores for the Neuro−QA Histogram of text prediction scores for leave−one−out
questions trained on Medical Wikipedia experiments on Neuro−QA questions
Frequency Frequency
0 20 40 60 80 0 20 40 60 80
0 20 40 60 80
percentage of keystrokes saved
</figure>
<figureCaption confidence="0.916990666666667">
Figure 4: Histogram of text prediction scores
for the Neuro-QA questions trained on Medical
Wikipedia. Each bin represents 36 questions.
</figureCaption>
<figure confidence="0.998018">
0 20 40 60 80
percentage of keystrokes saved
</figure>
<figureCaption confidence="0.932026">
Figure 5: Histogram of text prediction scores
for leave-one-out experiments on Neuro-QA ques-
tions. Each bin represents 36 questions.
</figureCaption>
<bodyText confidence="0.999882777777778">
score of 0% is less than 10% for the Medical
Wikipedia up to more than 30% for Speech.
We inspected the questions that get a text pre-
diction score of 0%. We see many medical terms
in these questions, and many of the utterances are
not even questions, but multi-word terms repre-
senting topical headers in the chat-by-click data.
Seven samples get a zero-score in the output of all
six training corpora, e.g.:
</bodyText>
<listItem confidence="0.999635">
• glycogenose III.
• potassium-aggrevated myotonias.
</listItem>
<bodyText confidence="0.97868275">
26 samples get a zero-score in the output of all
training corpora except for Medical Wikipedia and
Neuro-QA itself. These are mainly short headings
with domain-specific terms such as:
</bodyText>
<listItem confidence="0.999090666666667">
• idiopatische neuralgische amyotrofie.
• Markesbery-Griggs distale myopathie.
• oculopharyngeale spierdystrofie.
</listItem>
<bodyText confidence="0.999975928571429">
Interestingly, the ECDFs show that the Med-
ical Wikipedia and Neuro-QA corpora cross at
around percentile 70 (around the point of 40%
saved keystrokes). This indicates that although the
means of the two result samples are close to each
other, the distribution the scores for the individ-
ual questions is different. The histograms of both
distributions (Figures 4 and 5) confirm this: the
algorithm trained on the Medical Wikipedia cor-
pus leads a larger number of samples with scores
around the mean, while the leave-one-out exper-
iments lead to a larger number of samples with
low prediction scores and a larger number of sam-
ples with high prediction scores. This is also re-
flected by the higher standard deviation for Neuro-
QA than for Medical Wikipedia.
Since both the leave-one-out training on the
Neuro-QA questions and the Medical Wikipedia
led to good results but behave differently for dif-
ferent portions of the test data, we also evaluated a
combination of both corpora on our test set: We
created training corpora consisting of the Medi-
cal Wikipedia corpus, complemented by 90% of
the Neuro-QA questions, testing on the remaining
10% of the Neuro-QA questions. This led to mean
percentage of saved keystrokes of 28.6%, not sig-
nificantly higher than just the Medical Wikipedia
corpus.
</bodyText>
<sectionHeader confidence="0.999499" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999399727272727">
In Section 1, we asked two questions: (1) “What
is the effect of text type differences on the quality
of a text prediction algorithm?” and (2) “What is
the best choice of training data if domain- and text
type-specific data is sparse?”
By training and testing our text prediction al-
gorithm on four different text types (Wikipedia,
Twitter, transcriptions of conversational speech
and FAQ) with equal corpus sizes, we found that
there is a clear effect of text type on text prediction
quality: training and testing on the same text type
</bodyText>
<page confidence="0.991062">
568
</page>
<bodyText confidence="0.999971535714286">
gave percentages of saved keystrokes between 27
and 34%; training on a different text type caused
the scores to drop to percentages between 16 and
28%.
In our case study, we compared a number of
training corpora for a specific data set for which
training data is sparse: questions about neuro-
logical issues. We found significant differences
between the text prediction scores obtained with
the six training corpora: the Twitter and Speech
corpora were the least suited, followed by the
Wikipedia and FAQ corpus. The highest scores
were obtained by training the algorithm on the
medical pages from Wikipedia, immediately fol-
lowed by leave-one-out experiments on the 359
neurological questions. The large differences be-
tween the lexical coverage of the medical domain
played a central role in the scores for the different
training corpora.
Because we obtained good results by both
the Medical Wikipedia corpus and the neuro-QA
questions themselves, we opted for a combination
of both data types as training corpus in the initial
version of the online text prediction application.
Currently, a demonstration version of the appli-
cation is running for ComPoli-users. We hope to
collect questions from these users to re-train our
algorithm with more representative examples.
</bodyText>
<sectionHeader confidence="0.998226" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9996746">
This work is part of the research programme
‘Communicatie en revalidatie digiPoli’ (Com-
Poli6), which is funded by ZonMW, the Nether-
lands organisation for health research and devel-
opment.
</bodyText>
<sectionHeader confidence="0.999276" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997937618181818">
J. Carlberger. 1997. Design and Implementation of a
Probabilistic Word Prediciton Program. Master the-
sis, Royal Institute of Technology (KTH), Sweden.
W. Daelemans, A. Van Den Bosch, and T. Weijters.
1997. IGTree: Using trees for compression and clas-
sification in lazy learning algorithms. Artificial In-
telligence Review, 11(1):407–423.
A. Fazly and G. Hirst. 2003. Testing the efficacy of
part-of-speech information in word completion. In
Proceedings of the 2003 EACL Workshop on Lan-
guage Modeling for Text Entry Methods, pages 9–
16.
6http://lands.let.ru.nl/˜strik/research/ComPoli/
N. Garay-Vitoria and J. Abascal. 2006. Text prediction
systems: a survey. Universal Access in the Informa-
tion Society, 4(3):188–203.
J. Geuze, P. Desain, and J. Ringelberg. 2008. Re-
phrase: chat-by-click: a fundamental new mode of
human communication over the internet. In CHI’08
extended abstracts on Human factors in computing
systems, pages 3345–3350. ACM.
G.W. Lesher, B.J. Moulton, D.J. Higginbotham, et al.
1999. Effects of ngram order and training text size
on word prediction. In Proceedings of the RESNA
’99 Annual Conference, pages 52–54.
Johannes Matiasek, Marco Baroni, and Harald Trost.
2002. FASTY - A Multi-lingual Approach to Text
Prediction. In Klaus Miesenberger, Joachim Klaus,
and Wolfgang Zagler, editors, Computers Helping
People with Special Needs, volume 2398 of Lec-
ture Notes in Computer Science, pages 165–176.
Springer Berlin / Heidelberg.
N. Oostdijk. 2000. The spoken Dutch corpus:
overview and first evaluation. In Proceedings of
LREC-2000, Athens, volume 2, pages 887–894.
Erik Tjong Kim Sang. 2011. Het gebruik van Twit-
ter voor Taalkundig Onderzoek. In TABU: Bulletin
voor Taalwetenschap, volume 39, pages 62–72. In
Dutch.
A. Van den Bosch and T. Bogers. 2008. Efficient
context-sensitive word completion for mobile de-
vices. In Proceedings of the 10th international con-
ference on Human computer interaction with mobile
devices and services, pages 465–470. ACM.
A. Van den Bosch. 2011. Effects of context and re-
cency in scaled word completion. Computational
Linguistics in the Netherlands Journal, 1:79–94,
12/2011.
G. Van Noord. 2009. Huge parsed corpora in LASSY.
In Proceedings of The 7th International Workshop
on Treebanks and Linguistic Theories (TLT7).
S. Westman and L. Freund. 2010. Information Interac-
tion in 140 Characters or Less: Genres on Twitter. In
Proceedings of the third symposium on Information
Interaction in Context (IIiX), pages 323–328. ACM.
</reference>
<page confidence="0.998589">
569
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.527493">
<title confidence="0.997755">The effect of domain and text type on text prediction quality</title>
<author confidence="0.995805">Suzan Verberne</author>
<author confidence="0.995805">Antal van_den_Bosch</author>
<author confidence="0.995805">Helmer Strik</author>
<author confidence="0.995805">Lou</author>
<affiliation confidence="0.9989585">Centre for Language Radboud University</affiliation>
<email confidence="0.979422">s.verberne@let.ru.nl</email>
<abstract confidence="0.984473612903226">Text prediction is the task of suggesting text while the user is typing. Its main aim is to reduce the number of keystrokes that are needed to type a text. In this paper, we address the influence of text type and domain differences on text prediction quality. By training and testing our text prediction algorithm on four different text types (Wikipedia, Twitter, transcriptions of conversational speech and FAQ) with equal corpus sizes, we found that there is a clear effect of text type on text prediction quality: training and testing on the same text type gave percentages of saved keystrokes between 27 and 34%; training on a different text type caused the scores to drop to percentages between 16 and 28%. In our case study, we compared a number of training corpora for a specific data set for which training data is sparse: questions about neurological issues. We found that both text type and topic domain play a role in text prediction quality. The best performing training corpus was a set of medical pages from Wikipedia. The second-best result was obtained by leaveone-out experiments on the test questions, even though this training corpus was much smaller (2,672 words) than the other corpora (1.5 Million words).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Carlberger</author>
</authors>
<title>Design and Implementation of a Probabilistic Word Prediciton Program. Master thesis,</title>
<date>1997</date>
<institution>Royal Institute of Technology (KTH), Sweden.</institution>
<contexts>
<context position="6799" citStr="Carlberger, 1997" startWordPosition="1128" endWordPosition="1129">t to be predicted by a text prediction algorithm can be anything ranging from a single character (which actually does not save any keystrokes) to multiple words. Single words are the most widely used as prediction units because they are recognizable at a low cognitive load for the user, and word prediction gives good results in terms of keystroke savings (Garay-Vitoria and Abascal, 2006). There is some variation among methods in the size and type of buffer used. Most methods use character n-grams as buffer, because they are powerful and can be implemented independently of the target language (Carlberger, 1997). In many algorithms the buffer is cleared at the start of each new word (making the buffer never larger than the length of the current word). In the paper by (Van den Bosch and Bogers, 2008), two extensions to the basic prefix-model are compared. They found that an algorithm that uses the previous n characters as buffer, crossing word borders without clearing the buffer, performs better than both a prefix character model and an algorithm that includes the full previous word as feature. In addition to using the previously typed characters and/or words in the buffer, word characteristics such a</context>
</contexts>
<marker>Carlberger, 1997</marker>
<rawString>J. Carlberger. 1997. Design and Implementation of a Probabilistic Word Prediciton Program. Master thesis, Royal Institute of Technology (KTH), Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>A Van Den Bosch</author>
<author>T Weijters</author>
</authors>
<title>IGTree: Using trees for compression and classification in lazy learning algorithms.</title>
<date>1997</date>
<journal>Artificial Intelligence Review,</journal>
<volume>11</volume>
<issue>1</issue>
<marker>Daelemans, Van Den Bosch, Weijters, 1997</marker>
<rawString>W. Daelemans, A. Van Den Bosch, and T. Weijters. 1997. IGTree: Using trees for compression and classification in lazy learning algorithms. Artificial Intelligence Review, 11(1):407–423.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fazly</author>
<author>G Hirst</author>
</authors>
<title>Testing the efficacy of part-of-speech information in word completion.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 EACL Workshop on Language Modeling for Text Entry Methods,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="2633" citStr="Fazly and Hirst, 2003" startWordPosition="438" endWordPosition="441">on’. The most basic method for word completion is checking after each typed character whether the prefix typed since the last whitespace is unique according to a lexicon. If it is, the algorithm suggests to complete the prefix with the lexicon entry. The algorithm may also suggest to complete a prefix even before the word’s uniqueness point is reached, using statistical information on the previous context. Moreover, it has been shown that significantly better prediction results can be obtained if not only the prefix of the current word is included as previous context, but also previous words (Fazly and Hirst, 2003) or characters (Van den Bosch and Bogers, 2008). In the current paper, we follow up on this work by addressing the influence of text type and domain differences on text prediction quality. Brief messages on mobile devices (such as text messages, Twitter and Facebook updates) are of a different style and lexicon than documents typed in office software (Westman and Freund, 2010). In addition, the topic domain of the text also influences its content. These differences may cause an algorithm trained on one text type or domain to perform poorly on another. The questions that we aim to answer in thi</context>
</contexts>
<marker>Fazly, Hirst, 2003</marker>
<rawString>A. Fazly and G. Hirst. 2003. Testing the efficacy of part-of-speech information in word completion. In Proceedings of the 2003 EACL Workshop on Language Modeling for Text Entry Methods, pages 9– 16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Garay-Vitoria</author>
<author>J Abascal</author>
</authors>
<title>Text prediction systems: a survey.</title>
<date>2006</date>
<journal>Universal Access in the Information Society,</journal>
<volume>4</volume>
<issue>3</issue>
<contexts>
<context position="1881" citStr="Garay-Vitoria and Abascal, 2006" startWordPosition="311" endWordPosition="314">ond-best result was obtained by leaveone-out experiments on the test questions, even though this training corpus was much smaller (2,672 words) than the other corpora (1.5 Million words). 1 Introduction Text prediction is the task of suggesting text while the user is typing. Its main aim is to reduce the number of keystrokes that are needed to type a text, thereby saving time. Text prediction algorithms have been implemented for mobile devices, office software (Open Office Writer), search engines (Google query completion), and in specialneeds software for writers who have difficulties typing (Garay-Vitoria and Abascal, 2006). In most applications, the scope of the prediction is the completion of the current word; hence the oftenused term ‘word completion’. The most basic method for word completion is checking after each typed character whether the prefix typed since the last whitespace is unique according to a lexicon. If it is, the algorithm suggests to complete the prefix with the lexicon entry. The algorithm may also suggest to complete a prefix even before the word’s uniqueness point is reached, using statistical information on the previous context. Moreover, it has been shown that significantly better predic</context>
<context position="5866" citStr="Garay-Vitoria and Abascal, 2006" startWordPosition="970" endWordPosition="973">on methods discussed in the literature. In Section 3 we present our approach to text prediction. Sections 4 and 5 describe the experiments that we carried out and the results we obtained. We phrase our conclusions in Section 6. 2 Text prediction methods Text prediction methods have been developed for several different purposes. The older algorithms were built as communicative devices for people with disabilities, such as motor and speech impairments. More recently, text prediction is developed for writing with reduced keyboards, specifically for writing (composing messages) on mobile devices (Garay-Vitoria and Abascal, 2006). All modern methods share the general idea that previous context (which we will call the ‘buffer’) can be used to predict the next block of characters (the ‘predictive unit’). If the user gets correct suggestions for continuation of the text then the number of keystrokes needed to type the text is reduced. The unit to be predicted by a text prediction algorithm can be anything ranging from a single character (which actually does not save any keystrokes) to multiple words. Single words are the most widely used as prediction units because they are recognizable at a low cognitive load for the us</context>
<context position="7489" citStr="Garay-Vitoria and Abascal, 2006" startWordPosition="1242" endWordPosition="1245"> each new word (making the buffer never larger than the length of the current word). In the paper by (Van den Bosch and Bogers, 2008), two extensions to the basic prefix-model are compared. They found that an algorithm that uses the previous n characters as buffer, crossing word borders without clearing the buffer, performs better than both a prefix character model and an algorithm that includes the full previous word as feature. In addition to using the previously typed characters and/or words in the buffer, word characteristics such as frequency and recency could also be taken into account (Garay-Vitoria and Abascal, 2006). Possible evaluation measures for text prediction are the proportion of words that are correctly predicted, the percentage of keystrokes that could maximally be saved (if the user would always make the correct decision), and the time saved by the use of the algorithm (Garay-Vitoria and Abascal, 2006). The performance that can be obtained by text prediction algorithms depends on the language they are evaluated on. Lower results are obtained for higher-inflected languages such as German than for low-inflected languages such as English (Matiasek et al., 2002). In their overview of text predictio</context>
</contexts>
<marker>Garay-Vitoria, Abascal, 2006</marker>
<rawString>N. Garay-Vitoria and J. Abascal. 2006. Text prediction systems: a survey. Universal Access in the Information Society, 4(3):188–203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Geuze</author>
<author>P Desain</author>
<author>J Ringelberg</author>
</authors>
<title>Rephrase: chat-by-click: a fundamental new mode of human communication over the internet. In CHI’08 extended abstracts on Human factors in computing systems,</title>
<date>2008</date>
<pages>3345--3350</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="17135" citStr="Geuze et al., 2008" startWordPosition="2998" endWordPosition="3001"> platforms are often not easily accessible. Aphasia, for example, hampers the exchange of information because the patient has problems with word finding. In the project ‘Communicatie en revalidatie DigiPoli’ (ComPoli), language and speech technologies are implemented in the infrastructure of an existing online care platform in order to facilitate communication for patients suffering from neurological damage. Part of the online care platform is a list of frequently asked questions about neurological diseases with answers. A user can browse through the questions using a chat-by-click interface (Geuze et al., 2008). Besides reading the listed questions and answers, the user has the option to submit a question that is not yet included in training on Wikipedia, testing on Twitter gives a different result from training on Twitter, testing on Wikipedia. This is due to the size and domain of the vocabularies in both data sets and the richness of the contexts (in order for the algorithm to predict a word, it has to have seen it in the train set). If the test set has a larger vocabulary than the train set, a lower proportion of words can be predicted than when it is the other way around. the list. The newly su</context>
</contexts>
<marker>Geuze, Desain, Ringelberg, 2008</marker>
<rawString>J. Geuze, P. Desain, and J. Ringelberg. 2008. Rephrase: chat-by-click: a fundamental new mode of human communication over the internet. In CHI’08 extended abstracts on Human factors in computing systems, pages 3345–3350. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G W Lesher</author>
<author>B J Moulton</author>
<author>D J Higginbotham</author>
</authors>
<title>Effects of ngram order and training text size on word prediction.</title>
<date>1999</date>
<booktitle>In Proceedings of the RESNA ’99 Annual Conference,</booktitle>
<pages>52--54</pages>
<contexts>
<context position="8337" citStr="Lesher et al., 1999" startWordPosition="1380" endWordPosition="1383"> time saved by the use of the algorithm (Garay-Vitoria and Abascal, 2006). The performance that can be obtained by text prediction algorithms depends on the language they are evaluated on. Lower results are obtained for higher-inflected languages such as German than for low-inflected languages such as English (Matiasek et al., 2002). In their overview of text prediction systems, (Garay-Vitoria and Abascal, 2006) report performance scores ranging from 29% to 56% of keystrokes saved. An important factor that is known to influence the quality of text prediction systems, is training 562 set size (Lesher et al., 1999; Van den Bosch, 2011). The paper by (Van den Bosch, 2011) shows log-linear learning curves for word prediction (a constant improvement each time the training corpus size is doubled), when the training set size is increased incrementally from 102 to 3∗107 words. 3 Our approach to text prediction We implement a text prediction algorithm for Dutch, which is a productive compounding language like German, but has a somewhat simpler inflectional system. We do not focus on the effect of training set size, but on the effect of text type and topic domain differences. Our approach to text prediction is</context>
</contexts>
<marker>Lesher, Moulton, Higginbotham, 1999</marker>
<rawString>G.W. Lesher, B.J. Moulton, D.J. Higginbotham, et al. 1999. Effects of ngram order and training text size on word prediction. In Proceedings of the RESNA ’99 Annual Conference, pages 52–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Matiasek</author>
<author>Marco Baroni</author>
<author>Harald Trost</author>
</authors>
<title>FASTY - A Multi-lingual Approach to Text Prediction.</title>
<date>2002</date>
<journal>Computers Helping People with Special Needs,</journal>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>2398</volume>
<pages>165--176</pages>
<editor>In Klaus Miesenberger, Joachim Klaus, and Wolfgang Zagler, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin / Heidelberg.</location>
<contexts>
<context position="8052" citStr="Matiasek et al., 2002" startWordPosition="1334" endWordPosition="1337">lso be taken into account (Garay-Vitoria and Abascal, 2006). Possible evaluation measures for text prediction are the proportion of words that are correctly predicted, the percentage of keystrokes that could maximally be saved (if the user would always make the correct decision), and the time saved by the use of the algorithm (Garay-Vitoria and Abascal, 2006). The performance that can be obtained by text prediction algorithms depends on the language they are evaluated on. Lower results are obtained for higher-inflected languages such as German than for low-inflected languages such as English (Matiasek et al., 2002). In their overview of text prediction systems, (Garay-Vitoria and Abascal, 2006) report performance scores ranging from 29% to 56% of keystrokes saved. An important factor that is known to influence the quality of text prediction systems, is training 562 set size (Lesher et al., 1999; Van den Bosch, 2011). The paper by (Van den Bosch, 2011) shows log-linear learning curves for word prediction (a constant improvement each time the training corpus size is doubled), when the training set size is increased incrementally from 102 to 3∗107 words. 3 Our approach to text prediction We implement a tex</context>
</contexts>
<marker>Matiasek, Baroni, Trost, 2002</marker>
<rawString>Johannes Matiasek, Marco Baroni, and Harald Trost. 2002. FASTY - A Multi-lingual Approach to Text Prediction. In Klaus Miesenberger, Joachim Klaus, and Wolfgang Zagler, editors, Computers Helping People with Special Needs, volume 2398 of Lecture Notes in Computer Science, pages 165–176. Springer Berlin / Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Oostdijk</author>
</authors>
<title>The spoken Dutch corpus: overview and first evaluation.</title>
<date>2000</date>
<booktitle>In Proceedings of LREC-2000, Athens,</booktitle>
<volume>2</volume>
<pages>887--894</pages>
<contexts>
<context position="13514" citStr="Oostdijk, 2000" startWordPosition="2412" endWordPosition="2413"> k i e z i n g t o t tot v e r k i e z i n g t o t een e r k i e z i n g t o t e een r k i e z i n g t o t e e een k i e z i n g t o t e e n een i e z i n g t o t e e n niveau e z i n g t o t e e n n niveau z i n g t o t e e n n i niveau i n g t o t e e n n i v niveau n g t o t e e n n i v e niveau g t o t e e n n i v e a niveau t o t e e n n i v e a u niveau Figure 2: Example of buffer type ‘Buffer15’ for the text fragment “(elke verkiezing) tot een niveau”. Underscores represent whitespaces. different users. The transcriptions of conversational speech are from the Spoken Dutch Corpus (CGN) (Oostdijk, 2000); for our experiments, we only use the category ‘spontaneous speech’. We obtained the FAQ data by downloading the first 1,000 pages that Google returns for the query ‘faq’ with the language restriction Dutch. After cleaning the pages from HTML and other coding, the resulting corpus contained approximately 1.7 Million words of questions and answers. 4.2 Within-text type experiments For each of the four text types, we compare the buffer types ‘Prefix’ and ‘Buffer15’. In each experiment, we use 1.5 Million words from the corpus to train the algorithm and 100,000 words to test it. The results are </context>
</contexts>
<marker>Oostdijk, 2000</marker>
<rawString>N. Oostdijk. 2000. The spoken Dutch corpus: overview and first evaluation. In Proceedings of LREC-2000, Athens, volume 2, pages 887–894.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Tjong Kim Sang</author>
</authors>
<title>Het gebruik van Twitter voor Taalkundig Onderzoek.</title>
<date>2011</date>
<booktitle>In TABU: Bulletin voor Taalwetenschap,</booktitle>
<volume>39</volume>
<pages>62--72</pages>
<note>In Dutch.</note>
<contexts>
<context position="12280" citStr="Sang, 2011" startWordPosition="2073" endWordPosition="2074">nd second series of experiments. The case study on questions from the neurological domain is described in Section 5. 4.1 Data In the text type experiments, we evaluate our text prediction algorithm on four different types of Dutch text: Wikipedia, Twitter data, transcriptions of conversational speech, and web pages of Frequently Asked Questions (FAQ). The Wikipedia corpus that we use is part of the Lassy corpus (Van Noord, 2009); we obtained a version from the summer of 2010.1 The Twitter data are collected continuously and automatically filtered for language by Erik Tjong Kim Sang (Tjong Kim Sang, 2011). We used the tweets from all users that posted at least 19 tweets (excluding retweets) during one day in June 2011. This is a set of 1 Million Twitter messages from 30,000 1http://www.let.rug.nl/vannoord/trees/Treebank/Machine/ NLWIKI20100826/COMPACT/ 563 n i v e a u n i v e a n i v e t o t e e n n i v t o e e n i t e n tot tot tot een een een niveau niveau niveau niveau niveau niveau Figure 1: Example of buffer type ‘Prefix’ for the text fragment “(elke verkiezing) tot een niveau”. Underscores represent whitespaces. l k e v e r k i e z i n g tot k e v e r k i e z i n g t tot e v e r k i e z </context>
</contexts>
<marker>Sang, 2011</marker>
<rawString>Erik Tjong Kim Sang. 2011. Het gebruik van Twitter voor Taalkundig Onderzoek. In TABU: Bulletin voor Taalwetenschap, volume 39, pages 62–72. In Dutch.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Van den Bosch</author>
<author>T Bogers</author>
</authors>
<title>Efficient context-sensitive word completion for mobile devices.</title>
<date>2008</date>
<booktitle>In Proceedings of the 10th international conference on Human computer interaction with mobile devices and services,</booktitle>
<pages>465--470</pages>
<publisher>ACM.</publisher>
<marker>Van den Bosch, Bogers, 2008</marker>
<rawString>A. Van den Bosch and T. Bogers. 2008. Efficient context-sensitive word completion for mobile devices. In Proceedings of the 10th international conference on Human computer interaction with mobile devices and services, pages 465–470. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Van den Bosch</author>
</authors>
<title>Effects of context and recency in scaled word completion.</title>
<date>2011</date>
<journal>Computational Linguistics in the Netherlands Journal,</journal>
<volume>1</volume>
<pages>12--2011</pages>
<marker>Van den Bosch, 2011</marker>
<rawString>A. Van den Bosch. 2011. Effects of context and recency in scaled word completion. Computational Linguistics in the Netherlands Journal, 1:79–94, 12/2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Van Noord</author>
</authors>
<title>Huge parsed corpora in LASSY.</title>
<date>2009</date>
<booktitle>In Proceedings of The 7th International Workshop on Treebanks and Linguistic Theories (TLT7).</booktitle>
<marker>Van Noord, 2009</marker>
<rawString>G. Van Noord. 2009. Huge parsed corpora in LASSY. In Proceedings of The 7th International Workshop on Treebanks and Linguistic Theories (TLT7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Westman</author>
<author>L Freund</author>
</authors>
<title>Information Interaction in 140 Characters or Less: Genres on Twitter.</title>
<date>2010</date>
<booktitle>In Proceedings of the third symposium on Information Interaction in Context (IIiX),</booktitle>
<pages>323--328</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3012" citStr="Westman and Freund, 2010" startWordPosition="503" endWordPosition="506">ormation on the previous context. Moreover, it has been shown that significantly better prediction results can be obtained if not only the prefix of the current word is included as previous context, but also previous words (Fazly and Hirst, 2003) or characters (Van den Bosch and Bogers, 2008). In the current paper, we follow up on this work by addressing the influence of text type and domain differences on text prediction quality. Brief messages on mobile devices (such as text messages, Twitter and Facebook updates) are of a different style and lexicon than documents typed in office software (Westman and Freund, 2010). In addition, the topic domain of the text also influences its content. These differences may cause an algorithm trained on one text type or domain to perform poorly on another. The questions that we aim to answer in this paper are (1) “What is the effect of text type differences on the quality of a text prediction algorithm?” and (2) “What is the best choice of training data if domain- and text type-specific data is sparse?”. To answer these questions, we perform three experiments: 1. A series of within-text type experiments on four different types of Dutch text: Wikipedia articles, Twitter </context>
</contexts>
<marker>Westman, Freund, 2010</marker>
<rawString>S. Westman and L. Freund. 2010. Information Interaction in 140 Characters or Less: Genres on Twitter. In Proceedings of the third symposium on Information Interaction in Context (IIiX), pages 323–328. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>