<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000100">
<title confidence="0.983525">
Parsing Noun Phrase Structure with CCG
</title>
<author confidence="0.99747">
David Vadas and James R. Curran
</author>
<affiliation confidence="0.99904">
School of Information Technologies
University of Sydney
</affiliation>
<address confidence="0.460531">
NSW 2006, Australia
</address>
<email confidence="0.995239">
{dvadas1, james}@it.usyd.edu.au
</email>
<sectionHeader confidence="0.998598" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99818225">
Statistical parsing of noun phrase (NP) struc-
ture has been hampered by a lack of gold-
standard data. This is a significant problem for
CCGbank, where binary branching NP deriva-
tions are often incorrect, a result of the auto-
matic conversion from the Penn Treebank.
We correct these errors in CCGbank using a
gold-standard corpus of NP structure, result-
ing in a much more accurate corpus. We also
implement novel NER features that generalise
the lexical information needed to parse NPs
and provide important semantic information.
Finally, evaluating against DepBank demon-
strates the effectiveness of our modified cor-
pus and novel features, with an increase in
parser performance of 1.51%.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9656202">
Internal noun phrase (NP) structure is not recovered
by a number of widely-used parsers, e.g. Collins
(2003). This is because their training data, the Penn
Treebank (Marcus et al., 1993), does not fully anno-
tate NP structure. The flat structure described by the
Penn Treebank can be seen in this example:
(NP (NN lung) (NN cancer) (NNS deaths))
CCGbank (Hockenmaier and Steedman, 2007) is
the primary English corpus for Combinatory Cate-
gorial Grammar (CCG) (Steedman, 2000) and was
created by a semi-automatic conversion from the
Penn Treebank. However, CCG is a binary branch-
ing grammar, and as such, cannot leave NP structure
underspecified. Instead, all NPs were made right-
branching, as shown in this example:
</bodyText>
<page confidence="0.921051">
335
</page>
<equation confidence="0.95542125">
(N
(N/N lung)
(N
(N/N cancer) (N deaths) ) )
</equation>
<bodyText confidence="0.9996254">
This structure is correct for most English NPs and
is the best solution that doesn’t require manual re-
annotation. However, the resulting derivations often
contain errors. This can be seen in the previous ex-
ample, where lung cancer should form a con-
stituent, but does not.
The first contribution of this paper is to correct
these CCGbank errors. We apply an automatic con-
version process using the gold-standard NP data an-
notated by Vadas and Curran (2007a). Over a quar-
ter of the sentences in CCGbank need to be altered,
demonstrating the magnitude of the NP problem and
how important it is that these errors are fixed.
We then run a number of parsing experiments us-
ing our new version of the CCGbank corpus. In
particular, we implement new features using NER
tags from the BBN Entity Type Corpus (Weischedel
and Brunstein, 2005). These features are targeted at
improving the recovery of NP structure, increasing
parser performance by 0.64% F-score.
Finally, we evaluate against DepBank (King et al.,
2003). This corpus annotates internal NP structure,
and so is particularly relevant for the changes we
have made to CCGbank. The CCG parser now recov-
ers additional structure learnt from our NP corrected
corpus, increasing performance by 0.92%. Applying
the NER features results in a total increase of 1.51%.
This work allows parsers trained on CCGbank to
model NP structure accurately, and then pass this
crucial information on to downstream systems.
</bodyText>
<note confidence="0.818135">
Proceedings of ACL-08: HLT, pages 335–343,
</note>
<page confidence="0.582217">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<figure confidence="0.999547684210526">
(a) (b)
N N
N
N
N /N N
acetate fibers
N /N
cotton conj
and
N /N
cotton
N /N
conj
and
N /N [conj]
N /N
acetate
N
fibers
</figure>
<figureCaption confidence="0.999541">
Figure 1: (a) Incorrect CCG derivation from Hockenmaier and Steedman (2007) (b) The correct derivation
</figureCaption>
<sectionHeader confidence="0.989848" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999636">
Parsing of NPs is typically framed as NP bracketing,
where the task is limited to discriminating between
left and right-branching NPs of three nouns only:
</bodyText>
<listItem confidence="0.999708">
• (crude oil) prices – left-branching
• world (oil prices) – right-branching
</listItem>
<bodyText confidence="0.999883652173913">
Lauer (1995) presents two models to solve this prob-
lem: the adjacency model, which compares the as-
sociation strength between words 1–2 to words 2–3;
and the dependency model, which compares words
1–2 to words 1–3. Lauer (1995) experiments with a
data set of 244 NPs, and finds that the dependency
model is superior, achieving 80.7% accuracy.
Most NP bracketing research has used Lauer’s
data set. Because it is a very small corpus, most
approaches have been unsupervised, measuring as-
sociation strength with counts from a separate large
corpus. Nakov and Hearst (2005) use search engine
hit counts and extend the query set with typographi-
cal markers. This results in 89.3% accuracy.
Recently, Vadas and Curran (2007a) annotated in-
ternal NP structure for the entire Penn Treebank, pro-
viding a large gold-standard corpus for NP bracket-
ing. Vadas and Curran (2007b) carry out supervised
experiments using this data set of 36,584 NPs, out-
performing the Collins (2003) parser.
The Vadas and Curran (2007a) annotation scheme
inserts NML and JJP brackets to describe the correct
NP structure, as shown below:
</bodyText>
<equation confidence="0.797412">
(NP (NML (NN lung) (NN cancer) )
(NNS deaths) )
</equation>
<bodyText confidence="0.998768">
We use these brackets to determine new gold-
standard CCG derivations in Section 3.
</bodyText>
<subsectionHeader confidence="0.929599">
2.1 Combinatory Categorial Grammar
</subsectionHeader>
<bodyText confidence="0.998733285714286">
Combinatory Categorial Grammar (CCG) (Steed-
man, 2000) is a type-driven, lexicalised theory of
grammar. Lexical categories (also called supertags)
are made up of basic atoms such as S (Sentence)
and NP (Noun Phrase), which can be combined to
form complex categories. For example, a transitive
verb such as bought (as in IBM bought the
company) would have the category: (S\NP)/NP.
The slashes indicate the directionality of arguments,
here two arguments are expected: an NP subject on
the left; and an NP object on the right. Once these
arguments are filled, a sentence is produced.
Categories are combined using combinatory rules
such as forward and backward application:
</bodyText>
<equation confidence="0.9999825">
X/Y Y X (&gt;) (1)
Y X\Y X (&lt;) (2)
</equation>
<bodyText confidence="0.999575">
Other rules such as composition and type-raising are
used to analyse some linguistic constructions, while
retaining the canonical categories for each word.
This is an advantage of CCG, allowing it to recover
long-range dependencies without the need for post-
processing, as is the case for many other parsers.
In Section 1, we described the incorrect NP struc-
tures in CCGbank, but a further problem that high-
lights the need to improve NP derivations is shown
in Figure 1. When a conjunction occurs in an NP, a
non-CCG rule is required in order to reach a parse:
</bodyText>
<equation confidence="0.644672">
conj N N (3)
</equation>
<bodyText confidence="0.998912727272727">
This rule treats the conjunction in the same manner
as a modifier, and results in the incorrect derivation
shown in Figure 1(a). Our work creates the correct
CCG derivation, shown in Figure 1(b), and removes
the need for the grammar rule in (3).
Honnibal and Curran (2007) have also made
changes to CCGbank, aimed at better differentiat-
ing between complements and adjuncts. PropBank
(Palmer et al., 2005) is used as a gold-standard to in-
form these decisions, similar to the way that we use
the Vadas and Curran (2007a) data.
</bodyText>
<page confidence="0.996036">
336
</page>
<figure confidence="0.978154363636364">
(a) (b) (c)
N N N
N
N /N
lung
N /N
���
deaths
���
N /N
N
deaths
N
cancer deaths
N /N
(N/N)/(N/N)
lung
cancer
���
lung
cancer
���
</figure>
<figureCaption confidence="0.996245">
Figure 2: (a) Original right-branching CCGbank (b) Left-branching (c) Left-branching with new supertags
</figureCaption>
<subsectionHeader confidence="0.999302">
2.2 CCG parsing
</subsectionHeader>
<bodyText confidence="0.99998225">
The C&amp;C CCG parser (Clark and Curran, 2007b) is
used to perform our experiments, and to evaluate
the effect of the changes to CCGbank. The parser
uses a two-stage system, first employing a supertag-
ger (Bangalore and Joshi, 1999) to propose lexi-
cal categories for each word, and then applying the
CKY chart parsing algorithm. A log-linear model is
used to identify the most probable derivation, which
makes it possible to add the novel features we de-
scribe in Section 4, unlike a PCFG.
The C&amp;C parser is evaluated on predicate-
argument dependencies derived from CCGbank.
These dependencies are represented as 5-tuples:
(hf, f, s, ha, l), where hf is the head of the predi-
cate; f is the supertag of hf; s describes which ar-
gument of f is being filled; ha is the head of the
argument; and l encodes whether the dependency is
local or long-range. For example, the dependency
encoding company as the object of bought (as in
IBM bought the company) is represented by:
</bodyText>
<equation confidence="0.898411">
(bought, (S\NP1)/NP2, 2, company,−) (4)
</equation>
<bodyText confidence="0.9928115">
This is a local dependency, where company is fill-
ing the second argument slot, the object.
</bodyText>
<sectionHeader confidence="0.998256" genericHeader="method">
3 Conversion Process
</sectionHeader>
<bodyText confidence="0.999488714285714">
This section describes the process of converting the
Vadas and Curran (2007a) data to CCG derivations.
The tokens dominated by NML and JJP brackets in
the source data are formed into constituents in the
corresponding CCGbank sentence. We generate the
two forms of output that CCGbank contains: AUTO
files, which represent the tree structure of each sen-
tence; and PARG files, which list the word–word de-
pendencies (Hockenmaier and Steedman, 2005).
We apply one preprocessing step on the Penn
Treebank data, where if multiple tokens are enclosed
by brackets, then a NML node is placed around those
tokens. For example, we would insert the NML
bracket shown below:
</bodyText>
<equation confidence="0.901952333333333">
(NP (DT a) (-LRB- -LRB-)
(NML (RB very) (JJ negative) )
(-RRB- -RRB-) (NN reaction) )
</equation>
<bodyText confidence="0.99993025">
This simple heuristic captures NP structure not ex-
plicitly annotated by Vadas and Curran (2007a).
The conversion algorithm applies the following
steps for each NML or JJP bracket:
</bodyText>
<listItem confidence="0.999139222222222">
1. Identify the CCGbank lowest spanning node,
the lowest constituent that covers all of the
words in the NML or JJP bracket;
2. flatten the lowest spanning node, to remove the
right-branching structure;
3. insert new left-branching structure;
4. identify heads;
5. assign supertags;
6. generate new dependencies.
</listItem>
<bodyText confidence="0.991922">
As an example, we will follow the conversion pro-
cess for the NML bracket below:
</bodyText>
<equation confidence="0.826968">
(NP (NML (NN lung) (NN cancer) )
(NNS deaths) )
</equation>
<bodyText confidence="0.999947066666667">
The corresponding lowest spanning node, which
incorrectly has cancer deaths as a constituent,
is shown in Figure 2(a). To flatten the node, we re-
cursively remove brackets that partially overlap the
NML bracket. Nodes that don’t overlap at all are left
intact. This process results in a list of nodes (which
may or may not be leaves), which in our example is
[lung, cancer, deaths]. We then insert the cor-
rect left-branching structure, shown in Figure 2(b).
At this stage, the supertags are still incomplete.
Heads are then assigned using heuristics adapted
from Hockenmaier and Steedman (2007). Since we
are applying these to CCGbank NP structures rather
than the Penn Treebank, the POS tag based heuristics
are sufficient to determine heads accurately.
</bodyText>
<page confidence="0.991155">
337
</page>
<bodyText confidence="0.9999978">
Finally, we assign supertags to the new structure.
We want to make the minimal number of changes
to the entire sentence derivation, and so the supertag
of the dominating node is fixed. Categories are then
propagated recursively down the tree. For a node
with category X, its head child is also given the cat-
egory X. The non-head child is always treated as
an adjunct, and given the category X/X or X \X as
appropriate. Figure 2(c) shows the final result of this
step for our example.
</bodyText>
<subsectionHeader confidence="0.998611">
3.1 Dependency generation
</subsectionHeader>
<bodyText confidence="0.967023625">
The changes described so far have generated the new
tree structure, but the last step is to generate new de-
pendencies. We recursively traverse the tree, at each
level creating a dependency between the heads of
the left and right children. These dependencies are
never long-range, and therefore easy to deal with.
We may also need to change dependencies reaching
from inside to outside the NP, if the head(s) of the
NP have changed. In these cases we simply replace
the old head(s) with the new one(s) in the relevant
dependencies. The number of heads may change be-
cause we now analyse conjunctions correctly.
In our example, the original dependencies were:
(lung, N /N1,1, deaths, −) (5)
(cancer, N /N1,1, deaths, −) (6)
while after the conversion process, (5) becomes:
</bodyText>
<equation confidence="0.56291">
(lung, (N/N1)/(N/N)2, 2, cancer, −) (7)
</equation>
<bodyText confidence="0.9999836">
To determine that the conversion process worked
correctly, we manually inspected its output for
unique tree structures in Sections 00–07. This iden-
tified problem cases to correct, such as those de-
scribed in the following section.
</bodyText>
<subsectionHeader confidence="0.997303">
3.2 Exceptional cases
</subsectionHeader>
<bodyText confidence="0.999946375">
Firstly, when the lowest spanning node covers the
NML or JJP bracket exactly, no changes need to be
made to CCGbank. These cases occur when CCG-
bank already received the correct structure during
the original conversion process. For example, brack-
ets separating a possessive from its possessor were
detected automatically.
A more complex case is conjunctions, which do
not follow the simple head/adjunct method of as-
signing supertags. Instead, conjuncts are identified
during the head-finding stage, and then assigned the
supertag dominating the entire coordination. Inter-
vening non-conjunct nodes are given the same cate-
gory with the conj feature, resulting in a derivation
that can be parsed with the standard CCGbank bi-
nary coordination rules:
</bodyText>
<equation confidence="0.999925">
conj X X[conj] (8)
X X[conj] X (9)
</equation>
<bodyText confidence="0.974814388888889">
The derivation in Figure 1(b) is produced by these
corrections to coordination derivations. As a result,
applications of the non-CCG rule shown in (3) have
been reduced from 1378 to 145 cases.
Some POS tags require special behaviour. De-
terminers and possessive pronouns are both usually
given the supertag NP[nb]/N, and this should not
be changed by the conversion process. Accordingly,
we do not alter tokens with POS tags of DT and PRP$.
Instead, their sibling node is given the category N
and their parent node is made the head. The parent’s
sibling is then assigned the appropriate adjunct cat-
egory (usually NP\NP). Tokens with punctuation
POS tags1 do not have their supertag changed either.
Finally, there are cases where the lowest span-
ning node covers a constituent that should not be
changed. For example, in the following NP:
(NP
</bodyText>
<equation confidence="0.946659">
(NML (NN lower) (NN court) )
(JJ final) (NN ruling) )
with the original CCGbank lowest spanning node:
(N (N/N lower)
(N (N/N court)
(N (N/N final) (N ruling) ) ) )
</equation>
<bodyText confidence="0.998675857142857">
the final ruling node should not be altered.
It may seem trivial to process in this case, but
consider a similarly structured NP: lower court
ruling that the U.S. can bar the use
of... Our minimalist approach avoids reanalysing
the many linguistic constructions that can be dom-
inated by NPs, as this would reinvent the creation
of CCGbank. As a result, we only flatten those
constituents that partially overlap the NML or JJP
bracket. The existing structure and dependencies of
other constituents are retained. Note that we are still
converting every NML and JJP bracket, as even in
the subordinate clause example, only the structure
around lower court needs to be altered.
</bodyText>
<footnote confidence="0.446733">
1period, comma, colon, and left and right bracket.
</footnote>
<page confidence="0.977415">
338
</page>
<figure confidence="0.997607111111111">
the world ’s largest aid donor
NP[nb]/N N /N N NP\NP NP\NP NP\NP
&gt;
&gt;
NP &lt;
NP
N
the world ’s largest aid donor
NP[nb]/N N (NP[nb]/N )\NP N /N N /N N
&gt; &gt;
NP N
&lt; &gt;
NP[nb]/N N
NP &gt;
&lt;
NP &lt;
NP
(a) (b)
</figure>
<figureCaption confidence="0.998602">
Figure 3: CCGbank derivations for possessives
</figureCaption>
<table confidence="0.999705454545455">
# %
Possessive 224 43.75
Left child contains DT/PRP$ 87 16.99
Couldn’t assign to non-leaf 66 12.89
Conjunction 35 6.84
Automatic conversion was correct 26 5.08
Entity with internal brackets 23 4.49
DT 22 4.30
NML/JJP bracket is an error 12 2.34
Other 17 3.32
Total 512 100.00
</table>
<tableCaption confidence="0.999577">
Table 1: Manual analysis
</tableCaption>
<subsectionHeader confidence="0.99713">
3.3 Manual annotation
</subsectionHeader>
<bodyText confidence="0.999987186046512">
A handful of problems that occurred during the con-
version process were corrected manually. The first
indicator of a problem was the presence of a pos-
sessive. This is unexpected, because possessives
were already bracketed properly when CCGbank
was originally created (Hockenmaier, 2003, §3.6.4).
Secondly, a non-flattened node should not be as-
signed a supertag that it did not already have. This
is because, as described previously, a non-leaf node
could dominate any kind of structure. Finally, we
expect the lowest spanning node to cover only the
NML or JJP bracket and one more constituent to the
right. If it doesn’t, because of unusual punctuation
or an incorrect bracket, then it may be an error. In
all these cases, which occur throughout the corpus,
we manually analysed the derivation and fixed any
errors that were observed.
512 cases were flagged by this approach, or
1.90% of the 26,993 brackets converted to CCG. Ta-
ble 1 shows the causes of these problems. The most
common cause of errors was possessives, as the con-
version process highlighted a number of instances
where the original CCGbank analysis was incorrect.
An example of this error can be seen in Figure 3(a),
where the possessive doesn’t take any arguments.
Instead, largest aid donor incorrectly modifies the
NP one word at a time. The correct derivation after
manual analysis is in (b).
The second-most common cause occurs when
there is apposition inside the NP. This can be seen
in Figure 4. As there is no punctuation on which
to coordinate (which is how CCGbank treats most
appositions) the best derivation we can obtain is to
have Victor Borge modify the preceding NP.
The final step in the conversion process was
to validate the corpus against the CCG grammar,
first by those productions used in the existing
CCGbank, and then against those actually licensed
by CCG (with pre-existing ungrammaticalities re-
moved). Sixteen errors were identified by this pro-
cess and subsequently corrected by manual analysis.
In total, we have altered 12,475 CCGbank sen-
tences (25.5%) and 20,409 dependencies (1.95%).
</bodyText>
<sectionHeader confidence="0.998604" genericHeader="method">
4 NER features
</sectionHeader>
<bodyText confidence="0.999925416666667">
Named entity recognition (NER) provides informa-
tion that is particularly relevant for NP parsing, sim-
ply because entities are nouns. For example, know-
ing that Air Force is an entity tells us that Air
Force contract is aleft-branching NP.
Vadas and Curran (2007a) describe using NE tags
during the annotation process, suggesting that NER-
based features will be helpful in a statistical model.
There has also been recent work combining NER and
parsing in the biomedical field. Lewin (2007) exper-
iments with detecting base-NPs using NER informa-
tion, while Buyko et al. (2007) use a CRF to identify
</bodyText>
<page confidence="0.99303">
339
</page>
<figure confidence="0.998389">
a guest comedian Victor Borge a guest comedian Victor Borge
NP[nb]/N N /N N /N N /N N NP[nb]/N N /N N (NP\NP)/(NP\NP) NP\NP
&gt; &gt; &gt;
N N NP\NP
NP &gt;
&gt; &lt;
N NP
&gt;
NP
(a) (b)
</figure>
<figureCaption confidence="0.999451">
Figure 4: CCGbank derivations for apposition with DT
</figureCaption>
<bodyText confidence="0.954345853658537">
N
&gt;
coordinate structure in biological named entities.
We draw NE tags from the BBN Entity Type
Corpus (Weischedel and Brunstein, 2005), which
describes 28 different entity types. These in-
clude the standard person, location and organization
classes, as well person descriptions (generally occu-
pations), NORP (National, Other, Religious or Po-
litical groups), and works of art. Some classes also
have finer-grained subtypes, although we use only
the coarse tags in our experiments.
Clark and Curran (2007b) has a full description
of the C&amp;C parser’s pre-existing features, to which
we have added a number of novel NER-based fea-
tures. Many of these features generalise the head
words and/or POS tags that are already part of the
feature set. The results of applying these features
are described in Sections 5.3 and 6.
The first feature is a simple lexical feature, de-
scribing the NE tag of each token in the sentence.
This feature, and all others that we describe here,
are not active when the NE tag(s) are O, as there is no
NER information from tokens that are not entities.
The next group of features is based on the lo-
cal tree (a parent and two child nodes) formed by
every grammar rule application. We add a fea-
ture where the rule being applied is combined with
the parent’s NE tag. For example, when joining
two constituents2: (five, CD, CARD, N /N) and
(Europeans, NNPS, NORP, N ), the feature is:
N —* N /N N + NORP
as the head of the constituent is Europeans.
In the same way, we implement features that com-
bine the grammar rule with the child nodes. There
are already features in the model describing each
combination of the children’s head words and POS
tags, which we extend to include combinations with
2These 4-tuples are the node’s head, POS, NE, and supertag.
the NE tags. Using the same example as above, one
of the new features would be:
</bodyText>
<equation confidence="0.498906">
N —* N /N N + CARD + NORP
</equation>
<bodyText confidence="0.999692888888889">
The last group of features is based on the NE
category spanned by each constituent. We iden-
tify constituents that dominate tokens that all have
the same NE tag, as these nodes will not cause a
“crossing bracket” with the named entity. For ex-
ample, the constituent Force contract, in the
NP Air Force contract, spans two different
NE tags, and should be penalised by the model. Air
Force, on the other hand, only spans ORG tags, and
should be preferred accordingly.
We also take into account whether the constituent
spans the entire named entity. Combining these
nodes with others of different NE tags should not
be penalised by the model, as the NE must combine
with the rest of the sentence at some point.
These NE spanning features are implemented as
the grammar rule in combination with the parent
node or the child nodes. For the former, one fea-
ture is active when the node spans the entire entity,
and another is active in other cases. Similarly, there
are four features for the child nodes, depending on
whether neither, the left, the right or both nodes span
the entire NE. As an example, if the Air Force
constituent were being joined with contract, then
the child feature would be:
N —* N /N N + LEFT + ORG + O
assuming that there are more O tags to the right.
</bodyText>
<sectionHeader confidence="0.999275" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9981468">
Our experiments are run with the C&amp;C CCG parser
(Clark and Curran, 2007b), and will evaluate the
changes made to CCGbank, as well as the effective-
ness of the NER features. We train on Sections 02-
21, and test on Section 00.
</bodyText>
<page confidence="0.991896">
340
</page>
<table confidence="0.998422666666667">
PREC RECALL F-SCORE
Original 91.85 92.67 92.26
NP corrected 91.22 92.08 91.65
</table>
<tableCaption confidence="0.816025">
Table 2: Supertagging results
</tableCaption>
<table confidence="0.999894">
PREC RECALL F-SCORE
Original 85.34 84.55 84.94
NP corrected 85.08 84.17 84.63
</table>
<tableCaption confidence="0.999714">
Table 3: Parsing results with gold-standard POS tags
</tableCaption>
<subsectionHeader confidence="0.996124">
5.1 Supertagging
</subsectionHeader>
<bodyText confidence="0.999948454545454">
Before we begin full parsing experiments, we eval-
uate on the supertagger alone. The supertagger is
an important stage of the CCG parsing process, its
results will affect performance in later experiments.
Table 2 shows that F-score has dropped by 0.61%.
This is not surprising, as the conversion process has
increased the ambiguity of supertags in NPs. Previ-
ously, a bare NP could only have a sequence of N IN
tags followed by a final N . There are now more
complex possibilities, equal to the Catalan number
of the length of the NP.
</bodyText>
<subsectionHeader confidence="0.999028">
5.2 Initial parsing results
</subsectionHeader>
<bodyText confidence="0.999136142857143">
We now compare parser performance on our NP cor-
rected version of the corpus to that on original CCG-
bank. We are using the normal-form parser model
and report labelled precision, recall and F-score for
all dependencies. The results are shown in Table 3.
The F-score drops by 0.31% in our new version of
the corpus. However, this comparison is not entirely
fair, as the original CCGbank test data does not in-
clude the NP structure that the NP corrected model is
being evaluated on. Vadas and Curran (2007a) expe-
rienced a similar drop in performance on Penn Tree-
bank data, and noted that the F-score for NML and
JJP brackets was about 20% lower than the overall
figure. We suspect that a similar effect is causing the
drop in performance here.
Unfortunately, there are no explicit NML and JJP
brackets to evaluate on in the CCG corpus, and so an
NP structure only figure is difficult to compute. Re-
call can be calculated by marking those dependen-
cies altered in the conversion process, and evaluating
only on them. Precision cannot be measured in this
</bodyText>
<table confidence="0.995860333333333">
PREC RECALL F-SCORE
Original 83.65 82.81 83.23
NP corrected 83.31 82.33 82.82
</table>
<tableCaption confidence="0.988032">
Table 4: Parsing results with automatic POS tags
</tableCaption>
<table confidence="0.999872">
PREC RECALL F-SCORE
Original 86.00 85.15 85.58
NP corrected 85.71 84.83 85.27
</table>
<tableCaption confidence="0.99976">
Table 5: Parsing results with NER features
</tableCaption>
<bodyText confidence="0.999927142857143">
way, as NP dependencies remain undifferentiated in
parser output. The result is a recall of 77.03%, which
is noticeably lower than the overall figure.
We have also experimented with using automat-
ically assigned POS tags. These tags are accurate
with an F-score of 96.34%, with precision 96.20%
and recall 96.49%. Table 4 shows that, unsur-
prisingly, performance is lower without the gold-
standard data. The NP corrected model drops an ad-
ditional 0.1% F-score over the original model, sug-
gesting that POS tags are particularly important for
recovering internal NP structure. Evaluating NP de-
pendencies only, in the same manner as before, re-
sults in a recall figure of 75.21%.
</bodyText>
<subsectionHeader confidence="0.941144">
5.3 NER features results
</subsectionHeader>
<bodyText confidence="0.9999566875">
Table 5 shows the results of adding the NER fea-
tures we described in Section 4. Performance has
increased by 0.64% on both versions of the corpora.
It is surprising that the NP corrected increase is not
larger, as we would expect the features to be less
effective on the original CCGbank. This is because
incorrect right-branching NPs such as Air Force con-
tract would introduce noise to the NER features.
Table 6 presents the results of using automati-
cally assigned POS and NE tags, i.e. parsing raw
text. The NER tagger achieves 84.45% F-score on
all non-0 classes, with precision being 78.35% and
recall 91.57%. We can see that parsing F-score
has dropped by about 2% compared to using gold-
standard POS and NER data, however, the NER fea-
tures still improve performance by about 0.3%.
</bodyText>
<page confidence="0.993632">
341
</page>
<table confidence="0.996764666666667">
PREC RECALL F-SCORE
Original 83.92 83.06 83.49
NP corrected 83.62 82.65 83.14
</table>
<tableCaption confidence="0.997244">
Table 6: Parsing results with automatic POS and NE tags
</tableCaption>
<sectionHeader confidence="0.993215" genericHeader="method">
6 DepBank evaluation
</sectionHeader>
<bodyText confidence="0.999779974358974">
One problem with the evaluation in the previous sec-
tion, is that the original CCGbank is not expected to
recover internal NP structure, making its task eas-
ier and inflating its performance. To remove this
variable, we carry out a second evaluation against
the Briscoe and Carroll (2006) reannotation of Dep-
Bank (King et al., 2003), as described in Clark and
Curran (2007a). Parser output is made similar to the
grammatical relations (GRs) of the Briscoe and Car-
roll (2006) data, however, the conversion remains
complex. Clark and Curran (2007a) report an upper
bound on performance, using gold-standard CCG-
bank dependencies, of 84.76% F-score.
This evaluation is particularly relevant for NPs, as
the Briscoe and Carroll (2006) corpus has been an-
notated for internal NP structure. With our new ver-
sion of CCGbank, the parser will be able to recover
these GRs correctly, where before this was unlikely.
Firstly, we show the figures achieved using gold-
standard CCGbank derivations in Table 7. In the NP
corrected version of the corpus, performance has in-
creased by 1.02% F-score. This is a reversal of the
results in Section 5, and demonstrates that correct
NP structure improves parsing performance, rather
than reduces it. Because of this increase to the up-
per bound of performance, we are now even closer
to a true formalism-independent evaluation.
We now move to evaluating the C&amp;C parser it-
self and the improvement gained by the NER fea-
tures. Table 8 show our results, with the NP cor-
rected version outperforming original CCGbank by
0.92%. Using the NER features has also caused an
increase in F-score, giving a total improvement of
1.51%. These results demonstrate how successful
the correcting of NPs in CCGbank has been.
Furthermore, the performance increase of 0.59%
on the NP corrected corpus is more than the 0.25%
increase on the original. This demonstrates that NER
features are particularly helpful for NP structure.
</bodyText>
<table confidence="0.994374">
PREC RECALL F-SCORE
Original 86.86 81.61 84.15
NP corrected 87.97 82.54 85.17
</table>
<tableCaption confidence="0.886351">
Table 7: DepBank gold-standard evaluation
</tableCaption>
<table confidence="0.9998882">
PREC RECALL F-SCORE
Original 82.57 81.29 81.92
NP corrected 83.53 82.15 82.84
Original, NER 82.87 81.49 82.17
NP corrected, NER 84.12 82.75 83.43
</table>
<tableCaption confidence="0.994219">
Table 8: DepBank evaluation results
</tableCaption>
<sectionHeader confidence="0.998357" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99992425">
The first contribution of this paper is the application
of the Vadas and Curran (2007a) data to Combina-
tory Categorial Grammar. Our experimental results
have shown that this more accurate representation
of CCGbank’s NP structure increases parser perfor-
mance. Our second major contribution is the intro-
duction of novel NER features, a source of semantic
information previously unused in parsing.
As a result of this work, internal NP structure is
now recoverable by the C&amp;C parser, a result demon-
strated by our total performance increase of 1.51%
F-score. Even when parsing raw text, without gold
standard POS and NER tags, our approach has re-
sulted in performance gains.
In addition, we have made possible further in-
creases to NP structure accuracy. New features can
now be implemented and evaluated in a CCG pars-
ing context. For example, bigram counts from a very
large corpus have already been used in NP bracket-
ing, and could easily be applied to parsing. Sim-
ilarly, additional supertagging features can now be
created to deal with the increased ambiguity in NPs.
Downstream NLP components can now exploit the
crucial information in NP structure.
</bodyText>
<sectionHeader confidence="0.998713" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.988847166666667">
We would like to thank Mark Steedman and
Matthew Honnibal for help with converting the NP
data to CCG; and the anonymous reviewers for their
helpful feedback. This work has been supported by
the Australian Research Council under Discovery
Project DP0665973.
</bodyText>
<page confidence="0.997325">
342
</page>
<sectionHeader confidence="0.998333" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999689487804878">
Srinivas Bangalore and Aravind Joshi. 1999. Supertag-
ging: An approach to almost parsing. Computational
Linguistics, 25(2):237–265.
Ted Briscoe and John Carroll. 2006. Evaluating the accu-
racy of an unlexicalized statistical parser on the PARC
DepBank. In Proceedings of the COLING/ACL 2006
Main Conference Poster Sessions, pages 41–48. Syd-
ney, Australia.
Ekaterina Buyko, Katrin Tomanek, and Udo Hahn.
2007. Resolution of coordination ellipses in biological
named entities with conditional random fields. In Pro-
ceedings of the 10th Conference of the Pacific Associa-
tion for Computational Linguistics (PACLING-2007),
pages 163–171. Melbourne, Australia.
Stephen Clark and James R. Curran. 2007a. Formalism-
independent parser evaluation with CCG and Dep-
Bank. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL-
07), pages 248–255. Prague, Czech Republic.
Stephen Clark and James R. Curran. 2007b. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics,
33(4):493–552.
Michael Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguistics,
29(4):589–637.
Julia Hockenmaier. 2003. Data and Models for Statis-
tical Parsing with Combinatory Categorial Grammar.
Ph.D. thesis, University of Edinburgh.
Julia Hockenmaier and Mark Steedman. 2005. CCGbank
manual. Technical Report MS-CIS-05-09, Department
of Computer and Information Science, University of
Pennsylvania.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Compu-
tational Linguistics, 33(3):355–396.
Matthew Honnibal and James R. Curran. 2007. Im-
proving the complement/adjunct distinction in CCG-
bank. In Proceedings of the 10th Conference of
the Pacific Association for Computational Linguistics
(PACLING-07), pages 210–217. Melbourne, Australia.
Tracy Holloway King, Richard Crouch, Stefan Riezler,
Mary Dalrymple, and Ronald M. Kaplan. 2003. The
PARC700 dependency bank. In Proceedings of the 4th
International Workshop on Linguistically Interpreted
Corpora (LINC-03). Budapest, Hungary.
Mark Lauer. 1995. Corpus statistics meet the compound
noun: Some empirical results. In Proceedings of the
33rd Annual Meeting of the Association for Computa-
tional Linguistics, pages 47–54. Cambridge, MA.
Ian Lewin. 2007. BaseNPs that contain gene names: do-
main specificity and genericity. In Biological, trans-
lational, and clinical language processing workshop,
pages 163–170. Prague, Czech Republic.
Mitchell Marcus, Beatrice Santorini, and Mary
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313–330.
Preslav Nakov and Marti Hearst. 2005. Search engine
statistics beyond the n-gram: Application to noun
compound bracketing. In Proceedings of the 9th Con-
ference on Computational Natural Language Learning
(CoNLL-05), pages 17–24. Ann Arbor, MI.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005.
The proposition bank: An annotated corpus of seman-
tic roles. Computational Linguistics, 31(1):71–106.
Mark Steedman. 2000. The Syntactic Process. MIT Press,
Cambridge, MA.
David Vadas and James R. Curran. 2007a. Adding noun
phrase structure to the Penn Treebank. In Proceed-
ings of the 45th Annual Meeting of the Association of
Computational Linguistics (ACL-07), pages 240–247.
Prague, Czech Republic.
David Vadas and James R. Curran. 2007b. Large-scale
supervised models for noun phrase bracketing. In Pro-
ceedings of the 10th Conference of the Pacific Associa-
tion for Computational Linguistics (PACLING-2007),
pages 104–112. Melbourne, Australia.
Ralph Weischedel and Ada Brunstein. 2005. BBN pro-
noun coreference and entity type corpus. Technical
report.
</reference>
<page confidence="0.999366">
343
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.892766">
<title confidence="0.998789">Noun Phrase Structure with</title>
<author confidence="0.999978">Vadas R Curran</author>
<affiliation confidence="0.999708">School of Information Technologies University of Sydney</affiliation>
<address confidence="0.983872">NSW 2006, Australia</address>
<abstract confidence="0.993945470588235">parsing of noun phrase structure has been hampered by a lack of goldstandard data. This is a significant problem for where binary branching derivations are often incorrect, a result of the automatic conversion from the Penn Treebank. We correct these errors in CCGbank using a corpus of resulting in a much more accurate corpus. We also novel that generalise lexical information needed to parse and provide important semantic information. Finally, evaluating against DepBank demonstrates the effectiveness of our modified corpus and novel features, with an increase in parser performance of 1.51%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind Joshi</author>
</authors>
<title>Supertagging: An approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="7242" citStr="Bangalore and Joshi, 1999" startWordPosition="1199" endWordPosition="1202">nk (Palmer et al., 2005) is used as a gold-standard to inform these decisions, similar to the way that we use the Vadas and Curran (2007a) data. 336 (a) (b) (c) N N N N N /N lung N /N ��� deaths ��� N /N N deaths N cancer deaths N /N (N/N)/(N/N) lung cancer ��� lung cancer ��� Figure 2: (a) Original right-branching CCGbank (b) Left-branching (c) Left-branching with new supertags 2.2 CCG parsing The C&amp;C CCG parser (Clark and Curran, 2007b) is used to perform our experiments, and to evaluate the effect of the changes to CCGbank. The parser uses a two-stage system, first employing a supertagger (Bangalore and Joshi, 1999) to propose lexical categories for each word, and then applying the CKY chart parsing algorithm. A log-linear model is used to identify the most probable derivation, which makes it possible to add the novel features we describe in Section 4, unlike a PCFG. The C&amp;C parser is evaluated on predicateargument dependencies derived from CCGbank. These dependencies are represented as 5-tuples: (hf, f, s, ha, l), where hf is the head of the predicate; f is the supertag of hf; s describes which argument of f is being filled; ha is the head of the argument; and l encodes whether the dependency is local o</context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Srinivas Bangalore and Aravind Joshi. 1999. Supertagging: An approach to almost parsing. Computational Linguistics, 25(2):237–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Evaluating the accuracy of an unlexicalized statistical parser on the PARC DepBank.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions,</booktitle>
<pages>41--48</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="25251" citStr="Briscoe and Carroll (2006)" startWordPosition="4279" endWordPosition="4282"> 91.57%. We can see that parsing F-score has dropped by about 2% compared to using goldstandard POS and NER data, however, the NER features still improve performance by about 0.3%. 341 PREC RECALL F-SCORE Original 83.92 83.06 83.49 NP corrected 83.62 82.65 83.14 Table 6: Parsing results with automatic POS and NE tags 6 DepBank evaluation One problem with the evaluation in the previous section, is that the original CCGbank is not expected to recover internal NP structure, making its task easier and inflating its performance. To remove this variable, we carry out a second evaluation against the Briscoe and Carroll (2006) reannotation of DepBank (King et al., 2003), as described in Clark and Curran (2007a). Parser output is made similar to the grammatical relations (GRs) of the Briscoe and Carroll (2006) data, however, the conversion remains complex. Clark and Curran (2007a) report an upper bound on performance, using gold-standard CCGbank dependencies, of 84.76% F-score. This evaluation is particularly relevant for NPs, as the Briscoe and Carroll (2006) corpus has been annotated for internal NP structure. With our new version of CCGbank, the parser will be able to recover these GRs correctly, where before thi</context>
</contexts>
<marker>Briscoe, Carroll, 2006</marker>
<rawString>Ted Briscoe and John Carroll. 2006. Evaluating the accuracy of an unlexicalized statistical parser on the PARC DepBank. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 41–48. Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Buyko</author>
<author>Katrin Tomanek</author>
<author>Udo Hahn</author>
</authors>
<title>Resolution of coordination ellipses in biological named entities with conditional random fields.</title>
<date>2007</date>
<booktitle>In Proceedings of the 10th Conference of the Pacific Association for Computational Linguistics (PACLING-2007),</booktitle>
<pages>163--171</pages>
<location>Melbourne, Australia.</location>
<contexts>
<context position="17569" citStr="Buyko et al. (2007)" startWordPosition="2931" endWordPosition="2934">and 20,409 dependencies (1.95%). 4 NER features Named entity recognition (NER) provides information that is particularly relevant for NP parsing, simply because entities are nouns. For example, knowing that Air Force is an entity tells us that Air Force contract is aleft-branching NP. Vadas and Curran (2007a) describe using NE tags during the annotation process, suggesting that NERbased features will be helpful in a statistical model. There has also been recent work combining NER and parsing in the biomedical field. Lewin (2007) experiments with detecting base-NPs using NER information, while Buyko et al. (2007) use a CRF to identify 339 a guest comedian Victor Borge a guest comedian Victor Borge NP[nb]/N N /N N /N N /N N NP[nb]/N N /N N (NP\NP)/(NP\NP) NP\NP &gt; &gt; &gt; N N NP\NP NP &gt; &gt; &lt; N NP &gt; NP (a) (b) Figure 4: CCGbank derivations for apposition with DT N &gt; coordinate structure in biological named entities. We draw NE tags from the BBN Entity Type Corpus (Weischedel and Brunstein, 2005), which describes 28 different entity types. These include the standard person, location and organization classes, as well person descriptions (generally occupations), NORP (National, Other, Religious or Political grou</context>
</contexts>
<marker>Buyko, Tomanek, Hahn, 2007</marker>
<rawString>Ekaterina Buyko, Katrin Tomanek, and Udo Hahn. 2007. Resolution of coordination ellipses in biological named entities with conditional random fields. In Proceedings of the 10th Conference of the Pacific Association for Computational Linguistics (PACLING-2007), pages 163–171. Melbourne, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Formalismindependent parser evaluation with CCG and DepBank.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL07),</booktitle>
<pages>248--255</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="7056" citStr="Clark and Curran, 2007" startWordPosition="1168" endWordPosition="1171">d removes the need for the grammar rule in (3). Honnibal and Curran (2007) have also made changes to CCGbank, aimed at better differentiating between complements and adjuncts. PropBank (Palmer et al., 2005) is used as a gold-standard to inform these decisions, similar to the way that we use the Vadas and Curran (2007a) data. 336 (a) (b) (c) N N N N N /N lung N /N ��� deaths ��� N /N N deaths N cancer deaths N /N (N/N)/(N/N) lung cancer ��� lung cancer ��� Figure 2: (a) Original right-branching CCGbank (b) Left-branching (c) Left-branching with new supertags 2.2 CCG parsing The C&amp;C CCG parser (Clark and Curran, 2007b) is used to perform our experiments, and to evaluate the effect of the changes to CCGbank. The parser uses a two-stage system, first employing a supertagger (Bangalore and Joshi, 1999) to propose lexical categories for each word, and then applying the CKY chart parsing algorithm. A log-linear model is used to identify the most probable derivation, which makes it possible to add the novel features we describe in Section 4, unlike a PCFG. The C&amp;C parser is evaluated on predicateargument dependencies derived from CCGbank. These dependencies are represented as 5-tuples: (hf, f, s, ha, l), where </context>
<context position="18318" citStr="Clark and Curran (2007" startWordPosition="3062" endWordPosition="3065">N (NP\NP)/(NP\NP) NP\NP &gt; &gt; &gt; N N NP\NP NP &gt; &gt; &lt; N NP &gt; NP (a) (b) Figure 4: CCGbank derivations for apposition with DT N &gt; coordinate structure in biological named entities. We draw NE tags from the BBN Entity Type Corpus (Weischedel and Brunstein, 2005), which describes 28 different entity types. These include the standard person, location and organization classes, as well person descriptions (generally occupations), NORP (National, Other, Religious or Political groups), and works of art. Some classes also have finer-grained subtypes, although we use only the coarse tags in our experiments. Clark and Curran (2007b) has a full description of the C&amp;C parser’s pre-existing features, to which we have added a number of novel NER-based features. Many of these features generalise the head words and/or POS tags that are already part of the feature set. The results of applying these features are described in Sections 5.3 and 6. The first feature is a simple lexical feature, describing the NE tag of each token in the sentence. This feature, and all others that we describe here, are not active when the NE tag(s) are O, as there is no NER information from tokens that are not entities. The next group of features i</context>
<context position="21050" citStr="Clark and Curran, 2007" startWordPosition="3564" endWordPosition="3567"> are implemented as the grammar rule in combination with the parent node or the child nodes. For the former, one feature is active when the node spans the entire entity, and another is active in other cases. Similarly, there are four features for the child nodes, depending on whether neither, the left, the right or both nodes span the entire NE. As an example, if the Air Force constituent were being joined with contract, then the child feature would be: N —* N /N N + LEFT + ORG + O assuming that there are more O tags to the right. 5 Experiments Our experiments are run with the C&amp;C CCG parser (Clark and Curran, 2007b), and will evaluate the changes made to CCGbank, as well as the effectiveness of the NER features. We train on Sections 02- 21, and test on Section 00. 340 PREC RECALL F-SCORE Original 91.85 92.67 92.26 NP corrected 91.22 92.08 91.65 Table 2: Supertagging results PREC RECALL F-SCORE Original 85.34 84.55 84.94 NP corrected 85.08 84.17 84.63 Table 3: Parsing results with gold-standard POS tags 5.1 Supertagging Before we begin full parsing experiments, we evaluate on the supertagger alone. The supertagger is an important stage of the CCG parsing process, its results will affect performance in l</context>
<context position="25335" citStr="Clark and Curran (2007" startWordPosition="4294" endWordPosition="4297">tandard POS and NER data, however, the NER features still improve performance by about 0.3%. 341 PREC RECALL F-SCORE Original 83.92 83.06 83.49 NP corrected 83.62 82.65 83.14 Table 6: Parsing results with automatic POS and NE tags 6 DepBank evaluation One problem with the evaluation in the previous section, is that the original CCGbank is not expected to recover internal NP structure, making its task easier and inflating its performance. To remove this variable, we carry out a second evaluation against the Briscoe and Carroll (2006) reannotation of DepBank (King et al., 2003), as described in Clark and Curran (2007a). Parser output is made similar to the grammatical relations (GRs) of the Briscoe and Carroll (2006) data, however, the conversion remains complex. Clark and Curran (2007a) report an upper bound on performance, using gold-standard CCGbank dependencies, of 84.76% F-score. This evaluation is particularly relevant for NPs, as the Briscoe and Carroll (2006) corpus has been annotated for internal NP structure. With our new version of CCGbank, the parser will be able to recover these GRs correctly, where before this was unlikely. Firstly, we show the figures achieved using goldstandard CCGbank der</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007a. Formalismindependent parser evaluation with CCG and DepBank. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL07), pages 248–255. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Widecoverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="7056" citStr="Clark and Curran, 2007" startWordPosition="1168" endWordPosition="1171">d removes the need for the grammar rule in (3). Honnibal and Curran (2007) have also made changes to CCGbank, aimed at better differentiating between complements and adjuncts. PropBank (Palmer et al., 2005) is used as a gold-standard to inform these decisions, similar to the way that we use the Vadas and Curran (2007a) data. 336 (a) (b) (c) N N N N N /N lung N /N ��� deaths ��� N /N N deaths N cancer deaths N /N (N/N)/(N/N) lung cancer ��� lung cancer ��� Figure 2: (a) Original right-branching CCGbank (b) Left-branching (c) Left-branching with new supertags 2.2 CCG parsing The C&amp;C CCG parser (Clark and Curran, 2007b) is used to perform our experiments, and to evaluate the effect of the changes to CCGbank. The parser uses a two-stage system, first employing a supertagger (Bangalore and Joshi, 1999) to propose lexical categories for each word, and then applying the CKY chart parsing algorithm. A log-linear model is used to identify the most probable derivation, which makes it possible to add the novel features we describe in Section 4, unlike a PCFG. The C&amp;C parser is evaluated on predicateargument dependencies derived from CCGbank. These dependencies are represented as 5-tuples: (hf, f, s, ha, l), where </context>
<context position="18318" citStr="Clark and Curran (2007" startWordPosition="3062" endWordPosition="3065">N (NP\NP)/(NP\NP) NP\NP &gt; &gt; &gt; N N NP\NP NP &gt; &gt; &lt; N NP &gt; NP (a) (b) Figure 4: CCGbank derivations for apposition with DT N &gt; coordinate structure in biological named entities. We draw NE tags from the BBN Entity Type Corpus (Weischedel and Brunstein, 2005), which describes 28 different entity types. These include the standard person, location and organization classes, as well person descriptions (generally occupations), NORP (National, Other, Religious or Political groups), and works of art. Some classes also have finer-grained subtypes, although we use only the coarse tags in our experiments. Clark and Curran (2007b) has a full description of the C&amp;C parser’s pre-existing features, to which we have added a number of novel NER-based features. Many of these features generalise the head words and/or POS tags that are already part of the feature set. The results of applying these features are described in Sections 5.3 and 6. The first feature is a simple lexical feature, describing the NE tag of each token in the sentence. This feature, and all others that we describe here, are not active when the NE tag(s) are O, as there is no NER information from tokens that are not entities. The next group of features i</context>
<context position="21050" citStr="Clark and Curran, 2007" startWordPosition="3564" endWordPosition="3567"> are implemented as the grammar rule in combination with the parent node or the child nodes. For the former, one feature is active when the node spans the entire entity, and another is active in other cases. Similarly, there are four features for the child nodes, depending on whether neither, the left, the right or both nodes span the entire NE. As an example, if the Air Force constituent were being joined with contract, then the child feature would be: N —* N /N N + LEFT + ORG + O assuming that there are more O tags to the right. 5 Experiments Our experiments are run with the C&amp;C CCG parser (Clark and Curran, 2007b), and will evaluate the changes made to CCGbank, as well as the effectiveness of the NER features. We train on Sections 02- 21, and test on Section 00. 340 PREC RECALL F-SCORE Original 91.85 92.67 92.26 NP corrected 91.22 92.08 91.65 Table 2: Supertagging results PREC RECALL F-SCORE Original 85.34 84.55 84.94 NP corrected 85.08 84.17 84.63 Table 3: Parsing results with gold-standard POS tags 5.1 Supertagging Before we begin full parsing experiments, we evaluate on the supertagger alone. The supertagger is an important stage of the CCG parsing process, its results will affect performance in l</context>
<context position="25335" citStr="Clark and Curran (2007" startWordPosition="4294" endWordPosition="4297">tandard POS and NER data, however, the NER features still improve performance by about 0.3%. 341 PREC RECALL F-SCORE Original 83.92 83.06 83.49 NP corrected 83.62 82.65 83.14 Table 6: Parsing results with automatic POS and NE tags 6 DepBank evaluation One problem with the evaluation in the previous section, is that the original CCGbank is not expected to recover internal NP structure, making its task easier and inflating its performance. To remove this variable, we carry out a second evaluation against the Briscoe and Carroll (2006) reannotation of DepBank (King et al., 2003), as described in Clark and Curran (2007a). Parser output is made similar to the grammatical relations (GRs) of the Briscoe and Carroll (2006) data, however, the conversion remains complex. Clark and Curran (2007a) report an upper bound on performance, using gold-standard CCGbank dependencies, of 84.76% F-score. This evaluation is particularly relevant for NPs, as the Briscoe and Carroll (2006) corpus has been annotated for internal NP structure. With our new version of CCGbank, the parser will be able to recover these GRs correctly, where before this was unlikely. Firstly, we show the figures achieved using goldstandard CCGbank der</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007b. Widecoverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="997" citStr="Collins (2003)" startWordPosition="152" endWordPosition="153">rrect, a result of the automatic conversion from the Penn Treebank. We correct these errors in CCGbank using a gold-standard corpus of NP structure, resulting in a much more accurate corpus. We also implement novel NER features that generalise the lexical information needed to parse NPs and provide important semantic information. Finally, evaluating against DepBank demonstrates the effectiveness of our modified corpus and novel features, with an increase in parser performance of 1.51%. 1 Introduction Internal noun phrase (NP) structure is not recovered by a number of widely-used parsers, e.g. Collins (2003). This is because their training data, the Penn Treebank (Marcus et al., 1993), does not fully annotate NP structure. The flat structure described by the Penn Treebank can be seen in this example: (NP (NN lung) (NN cancer) (NNS deaths)) CCGbank (Hockenmaier and Steedman, 2007) is the primary English corpus for Combinatory Categorial Grammar (CCG) (Steedman, 2000) and was created by a semi-automatic conversion from the Penn Treebank. However, CCG is a binary branching grammar, and as such, cannot leave NP structure underspecified. Instead, all NPs were made rightbranching, as shown in this exam</context>
<context position="4656" citStr="Collins (2003)" startWordPosition="758" endWordPosition="759"> bracketing research has used Lauer’s data set. Because it is a very small corpus, most approaches have been unsupervised, measuring association strength with counts from a separate large corpus. Nakov and Hearst (2005) use search engine hit counts and extend the query set with typographical markers. This results in 89.3% accuracy. Recently, Vadas and Curran (2007a) annotated internal NP structure for the entire Penn Treebank, providing a large gold-standard corpus for NP bracketing. Vadas and Curran (2007b) carry out supervised experiments using this data set of 36,584 NPs, outperforming the Collins (2003) parser. The Vadas and Curran (2007a) annotation scheme inserts NML and JJP brackets to describe the correct NP structure, as shown below: (NP (NML (NN lung) (NN cancer) ) (NNS deaths) ) We use these brackets to determine new goldstandard CCG derivations in Section 3. 2.1 Combinatory Categorial Grammar Combinatory Categorial Grammar (CCG) (Steedman, 2000) is a type-driven, lexicalised theory of grammar. Lexical categories (also called supertags) are made up of basic atoms such as S (Sentence) and NP (Noun Phrase), which can be combined to form complex categories. For example, a transitive verb</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Michael Collins. 2003. Head-driven statistical models for natural language parsing. Computational Linguistics, 29(4):589–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
</authors>
<title>Data and Models for Statistical Parsing with Combinatory Categorial Grammar.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="15187" citStr="Hockenmaier, 2003" startWordPosition="2538" endWordPosition="2539">ssessives # % Possessive 224 43.75 Left child contains DT/PRP$ 87 16.99 Couldn’t assign to non-leaf 66 12.89 Conjunction 35 6.84 Automatic conversion was correct 26 5.08 Entity with internal brackets 23 4.49 DT 22 4.30 NML/JJP bracket is an error 12 2.34 Other 17 3.32 Total 512 100.00 Table 1: Manual analysis 3.3 Manual annotation A handful of problems that occurred during the conversion process were corrected manually. The first indicator of a problem was the presence of a possessive. This is unexpected, because possessives were already bracketed properly when CCGbank was originally created (Hockenmaier, 2003, §3.6.4). Secondly, a non-flattened node should not be assigned a supertag that it did not already have. This is because, as described previously, a non-leaf node could dominate any kind of structure. Finally, we expect the lowest spanning node to cover only the NML or JJP bracket and one more constituent to the right. If it doesn’t, because of unusual punctuation or an incorrect bracket, then it may be an error. In all these cases, which occur throughout the corpus, we manually analysed the derivation and fixed any errors that were observed. 512 cases were flagged by this approach, or 1.90% </context>
</contexts>
<marker>Hockenmaier, 2003</marker>
<rawString>Julia Hockenmaier. 2003. Data and Models for Statistical Parsing with Combinatory Categorial Grammar. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank manual.</title>
<date>2005</date>
<tech>Technical Report MS-CIS-05-09,</tech>
<institution>Department of Computer and Information Science, University of Pennsylvania.</institution>
<contexts>
<context position="8572" citStr="Hockenmaier and Steedman, 2005" startWordPosition="1424" endWordPosition="1427">t the company) is represented by: (bought, (S\NP1)/NP2, 2, company,−) (4) This is a local dependency, where company is filling the second argument slot, the object. 3 Conversion Process This section describes the process of converting the Vadas and Curran (2007a) data to CCG derivations. The tokens dominated by NML and JJP brackets in the source data are formed into constituents in the corresponding CCGbank sentence. We generate the two forms of output that CCGbank contains: AUTO files, which represent the tree structure of each sentence; and PARG files, which list the word–word dependencies (Hockenmaier and Steedman, 2005). We apply one preprocessing step on the Penn Treebank data, where if multiple tokens are enclosed by brackets, then a NML node is placed around those tokens. For example, we would insert the NML bracket shown below: (NP (DT a) (-LRB- -LRB-) (NML (RB very) (JJ negative) ) (-RRB- -RRB-) (NN reaction) ) This simple heuristic captures NP structure not explicitly annotated by Vadas and Curran (2007a). The conversion algorithm applies the following steps for each NML or JJP bracket: 1. Identify the CCGbank lowest spanning node, the lowest constituent that covers all of the words in the NML or JJP b</context>
</contexts>
<marker>Hockenmaier, Steedman, 2005</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2005. CCGbank manual. Technical Report MS-CIS-05-09, Department of Computer and Information Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: a corpus of CCG derivations and dependency structures extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="1274" citStr="Hockenmaier and Steedman, 2007" startWordPosition="196" endWordPosition="199">rmation needed to parse NPs and provide important semantic information. Finally, evaluating against DepBank demonstrates the effectiveness of our modified corpus and novel features, with an increase in parser performance of 1.51%. 1 Introduction Internal noun phrase (NP) structure is not recovered by a number of widely-used parsers, e.g. Collins (2003). This is because their training data, the Penn Treebank (Marcus et al., 1993), does not fully annotate NP structure. The flat structure described by the Penn Treebank can be seen in this example: (NP (NN lung) (NN cancer) (NNS deaths)) CCGbank (Hockenmaier and Steedman, 2007) is the primary English corpus for Combinatory Categorial Grammar (CCG) (Steedman, 2000) and was created by a semi-automatic conversion from the Penn Treebank. However, CCG is a binary branching grammar, and as such, cannot leave NP structure underspecified. Instead, all NPs were made rightbranching, as shown in this example: 335 (N (N/N lung) (N (N/N cancer) (N deaths) ) ) This structure is correct for most English NPs and is the best solution that doesn’t require manual reannotation. However, the resulting derivations often contain errors. This can be seen in the previous example, where lung</context>
<context position="3420" citStr="Hockenmaier and Steedman (2007)" startWordPosition="556" endWordPosition="559">r now recovers additional structure learnt from our NP corrected corpus, increasing performance by 0.92%. Applying the NER features results in a total increase of 1.51%. This work allows parsers trained on CCGbank to model NP structure accurately, and then pass this crucial information on to downstream systems. Proceedings of ACL-08: HLT, pages 335–343, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics (a) (b) N N N N N /N N acetate fibers N /N cotton conj and N /N cotton N /N conj and N /N [conj] N /N acetate N fibers Figure 1: (a) Incorrect CCG derivation from Hockenmaier and Steedman (2007) (b) The correct derivation 2 Background Parsing of NPs is typically framed as NP bracketing, where the task is limited to discriminating between left and right-branching NPs of three nouns only: • (crude oil) prices – left-branching • world (oil prices) – right-branching Lauer (1995) presents two models to solve this problem: the adjacency model, which compares the association strength between words 1–2 to words 2–3; and the dependency model, which compares words 1–2 to words 1–3. Lauer (1995) experiments with a data set of 244 NPs, and finds that the dependency model is superior, achieving 8</context>
<context position="10089" citStr="Hockenmaier and Steedman (2007)" startWordPosition="1675" endWordPosition="1678">NN lung) (NN cancer) ) (NNS deaths) ) The corresponding lowest spanning node, which incorrectly has cancer deaths as a constituent, is shown in Figure 2(a). To flatten the node, we recursively remove brackets that partially overlap the NML bracket. Nodes that don’t overlap at all are left intact. This process results in a list of nodes (which may or may not be leaves), which in our example is [lung, cancer, deaths]. We then insert the correct left-branching structure, shown in Figure 2(b). At this stage, the supertags are still incomplete. Heads are then assigned using heuristics adapted from Hockenmaier and Steedman (2007). Since we are applying these to CCGbank NP structures rather than the Penn Treebank, the POS tag based heuristics are sufficient to determine heads accurately. 337 Finally, we assign supertags to the new structure. We want to make the minimal number of changes to the entire sentence derivation, and so the supertag of the dominating node is fixed. Categories are then propagated recursively down the tree. For a node with category X, its head child is also given the category X. The non-head child is always treated as an adjunct, and given the category X/X or X \X as appropriate. Figure 2(c) show</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: a corpus of CCG derivations and dependency structures extracted from the Penn Treebank. Computational Linguistics, 33(3):355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Honnibal</author>
<author>James R Curran</author>
</authors>
<title>Improving the complement/adjunct distinction in CCGbank.</title>
<date>2007</date>
<booktitle>In Proceedings of the 10th Conference of the Pacific Association for Computational Linguistics (PACLING-07),</booktitle>
<pages>210--217</pages>
<location>Melbourne, Australia.</location>
<contexts>
<context position="6508" citStr="Honnibal and Curran (2007)" startWordPosition="1069" endWordPosition="1072">dependencies without the need for postprocessing, as is the case for many other parsers. In Section 1, we described the incorrect NP structures in CCGbank, but a further problem that highlights the need to improve NP derivations is shown in Figure 1. When a conjunction occurs in an NP, a non-CCG rule is required in order to reach a parse: conj N N (3) This rule treats the conjunction in the same manner as a modifier, and results in the incorrect derivation shown in Figure 1(a). Our work creates the correct CCG derivation, shown in Figure 1(b), and removes the need for the grammar rule in (3). Honnibal and Curran (2007) have also made changes to CCGbank, aimed at better differentiating between complements and adjuncts. PropBank (Palmer et al., 2005) is used as a gold-standard to inform these decisions, similar to the way that we use the Vadas and Curran (2007a) data. 336 (a) (b) (c) N N N N N /N lung N /N ��� deaths ��� N /N N deaths N cancer deaths N /N (N/N)/(N/N) lung cancer ��� lung cancer ��� Figure 2: (a) Original right-branching CCGbank (b) Left-branching (c) Left-branching with new supertags 2.2 CCG parsing The C&amp;C CCG parser (Clark and Curran, 2007b) is used to perform our experiments, and to evalua</context>
</contexts>
<marker>Honnibal, Curran, 2007</marker>
<rawString>Matthew Honnibal and James R. Curran. 2007. Improving the complement/adjunct distinction in CCGbank. In Proceedings of the 10th Conference of the Pacific Association for Computational Linguistics (PACLING-07), pages 210–217. Melbourne, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tracy Holloway King</author>
<author>Richard Crouch</author>
<author>Stefan Riezler</author>
<author>Mary Dalrymple</author>
<author>Ronald M Kaplan</author>
</authors>
<title>The PARC700 dependency bank.</title>
<date>2003</date>
<booktitle>In Proceedings of the 4th International Workshop on Linguistically Interpreted Corpora (LINC-03).</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="2656" citStr="King et al., 2003" startWordPosition="428" endWordPosition="431">sing the gold-standard NP data annotated by Vadas and Curran (2007a). Over a quarter of the sentences in CCGbank need to be altered, demonstrating the magnitude of the NP problem and how important it is that these errors are fixed. We then run a number of parsing experiments using our new version of the CCGbank corpus. In particular, we implement new features using NER tags from the BBN Entity Type Corpus (Weischedel and Brunstein, 2005). These features are targeted at improving the recovery of NP structure, increasing parser performance by 0.64% F-score. Finally, we evaluate against DepBank (King et al., 2003). This corpus annotates internal NP structure, and so is particularly relevant for the changes we have made to CCGbank. The CCG parser now recovers additional structure learnt from our NP corrected corpus, increasing performance by 0.92%. Applying the NER features results in a total increase of 1.51%. This work allows parsers trained on CCGbank to model NP structure accurately, and then pass this crucial information on to downstream systems. Proceedings of ACL-08: HLT, pages 335–343, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics (a) (b) N N N N N /N N acetate</context>
<context position="25295" citStr="King et al., 2003" startWordPosition="4287" endWordPosition="4290"> by about 2% compared to using goldstandard POS and NER data, however, the NER features still improve performance by about 0.3%. 341 PREC RECALL F-SCORE Original 83.92 83.06 83.49 NP corrected 83.62 82.65 83.14 Table 6: Parsing results with automatic POS and NE tags 6 DepBank evaluation One problem with the evaluation in the previous section, is that the original CCGbank is not expected to recover internal NP structure, making its task easier and inflating its performance. To remove this variable, we carry out a second evaluation against the Briscoe and Carroll (2006) reannotation of DepBank (King et al., 2003), as described in Clark and Curran (2007a). Parser output is made similar to the grammatical relations (GRs) of the Briscoe and Carroll (2006) data, however, the conversion remains complex. Clark and Curran (2007a) report an upper bound on performance, using gold-standard CCGbank dependencies, of 84.76% F-score. This evaluation is particularly relevant for NPs, as the Briscoe and Carroll (2006) corpus has been annotated for internal NP structure. With our new version of CCGbank, the parser will be able to recover these GRs correctly, where before this was unlikely. Firstly, we show the figures</context>
</contexts>
<marker>King, Crouch, Riezler, Dalrymple, Kaplan, 2003</marker>
<rawString>Tracy Holloway King, Richard Crouch, Stefan Riezler, Mary Dalrymple, and Ronald M. Kaplan. 2003. The PARC700 dependency bank. In Proceedings of the 4th International Workshop on Linguistically Interpreted Corpora (LINC-03). Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Lauer</author>
</authors>
<title>Corpus statistics meet the compound noun: Some empirical results.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>47--54</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="3705" citStr="Lauer (1995)" startWordPosition="603" endWordPosition="604">systems. Proceedings of ACL-08: HLT, pages 335–343, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics (a) (b) N N N N N /N N acetate fibers N /N cotton conj and N /N cotton N /N conj and N /N [conj] N /N acetate N fibers Figure 1: (a) Incorrect CCG derivation from Hockenmaier and Steedman (2007) (b) The correct derivation 2 Background Parsing of NPs is typically framed as NP bracketing, where the task is limited to discriminating between left and right-branching NPs of three nouns only: • (crude oil) prices – left-branching • world (oil prices) – right-branching Lauer (1995) presents two models to solve this problem: the adjacency model, which compares the association strength between words 1–2 to words 2–3; and the dependency model, which compares words 1–2 to words 1–3. Lauer (1995) experiments with a data set of 244 NPs, and finds that the dependency model is superior, achieving 80.7% accuracy. Most NP bracketing research has used Lauer’s data set. Because it is a very small corpus, most approaches have been unsupervised, measuring association strength with counts from a separate large corpus. Nakov and Hearst (2005) use search engine hit counts and extend the</context>
</contexts>
<marker>Lauer, 1995</marker>
<rawString>Mark Lauer. 1995. Corpus statistics meet the compound noun: Some empirical results. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 47–54. Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Lewin</author>
</authors>
<title>BaseNPs that contain gene names: domain specificity and genericity. In Biological, translational, and clinical language processing workshop,</title>
<date>2007</date>
<pages>163--170</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="17484" citStr="Lewin (2007)" startWordPosition="2919" endWordPosition="2920">y manual analysis. In total, we have altered 12,475 CCGbank sentences (25.5%) and 20,409 dependencies (1.95%). 4 NER features Named entity recognition (NER) provides information that is particularly relevant for NP parsing, simply because entities are nouns. For example, knowing that Air Force is an entity tells us that Air Force contract is aleft-branching NP. Vadas and Curran (2007a) describe using NE tags during the annotation process, suggesting that NERbased features will be helpful in a statistical model. There has also been recent work combining NER and parsing in the biomedical field. Lewin (2007) experiments with detecting base-NPs using NER information, while Buyko et al. (2007) use a CRF to identify 339 a guest comedian Victor Borge a guest comedian Victor Borge NP[nb]/N N /N N /N N /N N NP[nb]/N N /N N (NP\NP)/(NP\NP) NP\NP &gt; &gt; &gt; N N NP\NP NP &gt; &gt; &lt; N NP &gt; NP (a) (b) Figure 4: CCGbank derivations for apposition with DT N &gt; coordinate structure in biological named entities. We draw NE tags from the BBN Entity Type Corpus (Weischedel and Brunstein, 2005), which describes 28 different entity types. These include the standard person, location and organization classes, as well person des</context>
</contexts>
<marker>Lewin, 2007</marker>
<rawString>Ian Lewin. 2007. BaseNPs that contain gene names: domain specificity and genericity. In Biological, translational, and clinical language processing workshop, pages 163–170. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="1075" citStr="Marcus et al., 1993" startWordPosition="163" endWordPosition="166">orrect these errors in CCGbank using a gold-standard corpus of NP structure, resulting in a much more accurate corpus. We also implement novel NER features that generalise the lexical information needed to parse NPs and provide important semantic information. Finally, evaluating against DepBank demonstrates the effectiveness of our modified corpus and novel features, with an increase in parser performance of 1.51%. 1 Introduction Internal noun phrase (NP) structure is not recovered by a number of widely-used parsers, e.g. Collins (2003). This is because their training data, the Penn Treebank (Marcus et al., 1993), does not fully annotate NP structure. The flat structure described by the Penn Treebank can be seen in this example: (NP (NN lung) (NN cancer) (NNS deaths)) CCGbank (Hockenmaier and Steedman, 2007) is the primary English corpus for Combinatory Categorial Grammar (CCG) (Steedman, 2000) and was created by a semi-automatic conversion from the Penn Treebank. However, CCG is a binary branching grammar, and as such, cannot leave NP structure underspecified. Instead, all NPs were made rightbranching, as shown in this example: 335 (N (N/N lung) (N (N/N cancer) (N deaths) ) ) This structure is correc</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell Marcus, Beatrice Santorini, and Mary Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Marti Hearst</author>
</authors>
<title>Search engine statistics beyond the n-gram: Application to noun compound bracketing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL-05),</booktitle>
<pages>17--24</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="4261" citStr="Nakov and Hearst (2005)" startWordPosition="692" endWordPosition="695"> left-branching • world (oil prices) – right-branching Lauer (1995) presents two models to solve this problem: the adjacency model, which compares the association strength between words 1–2 to words 2–3; and the dependency model, which compares words 1–2 to words 1–3. Lauer (1995) experiments with a data set of 244 NPs, and finds that the dependency model is superior, achieving 80.7% accuracy. Most NP bracketing research has used Lauer’s data set. Because it is a very small corpus, most approaches have been unsupervised, measuring association strength with counts from a separate large corpus. Nakov and Hearst (2005) use search engine hit counts and extend the query set with typographical markers. This results in 89.3% accuracy. Recently, Vadas and Curran (2007a) annotated internal NP structure for the entire Penn Treebank, providing a large gold-standard corpus for NP bracketing. Vadas and Curran (2007b) carry out supervised experiments using this data set of 36,584 NPs, outperforming the Collins (2003) parser. The Vadas and Curran (2007a) annotation scheme inserts NML and JJP brackets to describe the correct NP structure, as shown below: (NP (NML (NN lung) (NN cancer) ) (NNS deaths) ) We use these brack</context>
</contexts>
<marker>Nakov, Hearst, 2005</marker>
<rawString>Preslav Nakov and Marti Hearst. 2005. Search engine statistics beyond the n-gram: Application to noun compound bracketing. In Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL-05), pages 17–24. Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="6640" citStr="Palmer et al., 2005" startWordPosition="1089" endWordPosition="1092">tures in CCGbank, but a further problem that highlights the need to improve NP derivations is shown in Figure 1. When a conjunction occurs in an NP, a non-CCG rule is required in order to reach a parse: conj N N (3) This rule treats the conjunction in the same manner as a modifier, and results in the incorrect derivation shown in Figure 1(a). Our work creates the correct CCG derivation, shown in Figure 1(b), and removes the need for the grammar rule in (3). Honnibal and Curran (2007) have also made changes to CCGbank, aimed at better differentiating between complements and adjuncts. PropBank (Palmer et al., 2005) is used as a gold-standard to inform these decisions, similar to the way that we use the Vadas and Curran (2007a) data. 336 (a) (b) (c) N N N N N /N lung N /N ��� deaths ��� N /N N deaths N cancer deaths N /N (N/N)/(N/N) lung cancer ��� lung cancer ��� Figure 2: (a) Original right-branching CCGbank (b) Left-branching (c) Left-branching with new supertags 2.2 CCG parsing The C&amp;C CCG parser (Clark and Curran, 2007b) is used to perform our experiments, and to evaluate the effect of the changes to CCGbank. The parser uses a two-stage system, first employing a supertagger (Bangalore and Joshi, 199</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1362" citStr="Steedman, 2000" startWordPosition="211" endWordPosition="212">ank demonstrates the effectiveness of our modified corpus and novel features, with an increase in parser performance of 1.51%. 1 Introduction Internal noun phrase (NP) structure is not recovered by a number of widely-used parsers, e.g. Collins (2003). This is because their training data, the Penn Treebank (Marcus et al., 1993), does not fully annotate NP structure. The flat structure described by the Penn Treebank can be seen in this example: (NP (NN lung) (NN cancer) (NNS deaths)) CCGbank (Hockenmaier and Steedman, 2007) is the primary English corpus for Combinatory Categorial Grammar (CCG) (Steedman, 2000) and was created by a semi-automatic conversion from the Penn Treebank. However, CCG is a binary branching grammar, and as such, cannot leave NP structure underspecified. Instead, all NPs were made rightbranching, as shown in this example: 335 (N (N/N lung) (N (N/N cancer) (N deaths) ) ) This structure is correct for most English NPs and is the best solution that doesn’t require manual reannotation. However, the resulting derivations often contain errors. This can be seen in the previous example, where lung cancer should form a constituent, but does not. The first contribution of this paper is</context>
<context position="5013" citStr="Steedman, 2000" startWordPosition="814" endWordPosition="816">rran (2007a) annotated internal NP structure for the entire Penn Treebank, providing a large gold-standard corpus for NP bracketing. Vadas and Curran (2007b) carry out supervised experiments using this data set of 36,584 NPs, outperforming the Collins (2003) parser. The Vadas and Curran (2007a) annotation scheme inserts NML and JJP brackets to describe the correct NP structure, as shown below: (NP (NML (NN lung) (NN cancer) ) (NNS deaths) ) We use these brackets to determine new goldstandard CCG derivations in Section 3. 2.1 Combinatory Categorial Grammar Combinatory Categorial Grammar (CCG) (Steedman, 2000) is a type-driven, lexicalised theory of grammar. Lexical categories (also called supertags) are made up of basic atoms such as S (Sentence) and NP (Noun Phrase), which can be combined to form complex categories. For example, a transitive verb such as bought (as in IBM bought the company) would have the category: (S\NP)/NP. The slashes indicate the directionality of arguments, here two arguments are expected: an NP subject on the left; and an NP object on the right. Once these arguments are filled, a sentence is produced. Categories are combined using combinatory rules such as forward and back</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vadas</author>
<author>James R Curran</author>
</authors>
<title>Adding noun phrase structure to the Penn Treebank.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL-07),</booktitle>
<pages>240--247</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2104" citStr="Vadas and Curran (2007" startWordPosition="336" endWordPosition="339">as such, cannot leave NP structure underspecified. Instead, all NPs were made rightbranching, as shown in this example: 335 (N (N/N lung) (N (N/N cancer) (N deaths) ) ) This structure is correct for most English NPs and is the best solution that doesn’t require manual reannotation. However, the resulting derivations often contain errors. This can be seen in the previous example, where lung cancer should form a constituent, but does not. The first contribution of this paper is to correct these CCGbank errors. We apply an automatic conversion process using the gold-standard NP data annotated by Vadas and Curran (2007a). Over a quarter of the sentences in CCGbank need to be altered, demonstrating the magnitude of the NP problem and how important it is that these errors are fixed. We then run a number of parsing experiments using our new version of the CCGbank corpus. In particular, we implement new features using NER tags from the BBN Entity Type Corpus (Weischedel and Brunstein, 2005). These features are targeted at improving the recovery of NP structure, increasing parser performance by 0.64% F-score. Finally, we evaluate against DepBank (King et al., 2003). This corpus annotates internal NP structure, a</context>
<context position="4408" citStr="Vadas and Curran (2007" startWordPosition="716" endWordPosition="719">he association strength between words 1–2 to words 2–3; and the dependency model, which compares words 1–2 to words 1–3. Lauer (1995) experiments with a data set of 244 NPs, and finds that the dependency model is superior, achieving 80.7% accuracy. Most NP bracketing research has used Lauer’s data set. Because it is a very small corpus, most approaches have been unsupervised, measuring association strength with counts from a separate large corpus. Nakov and Hearst (2005) use search engine hit counts and extend the query set with typographical markers. This results in 89.3% accuracy. Recently, Vadas and Curran (2007a) annotated internal NP structure for the entire Penn Treebank, providing a large gold-standard corpus for NP bracketing. Vadas and Curran (2007b) carry out supervised experiments using this data set of 36,584 NPs, outperforming the Collins (2003) parser. The Vadas and Curran (2007a) annotation scheme inserts NML and JJP brackets to describe the correct NP structure, as shown below: (NP (NML (NN lung) (NN cancer) ) (NNS deaths) ) We use these brackets to determine new goldstandard CCG derivations in Section 3. 2.1 Combinatory Categorial Grammar Combinatory Categorial Grammar (CCG) (Steedman, </context>
<context position="6752" citStr="Vadas and Curran (2007" startWordPosition="1111" endWordPosition="1114">1. When a conjunction occurs in an NP, a non-CCG rule is required in order to reach a parse: conj N N (3) This rule treats the conjunction in the same manner as a modifier, and results in the incorrect derivation shown in Figure 1(a). Our work creates the correct CCG derivation, shown in Figure 1(b), and removes the need for the grammar rule in (3). Honnibal and Curran (2007) have also made changes to CCGbank, aimed at better differentiating between complements and adjuncts. PropBank (Palmer et al., 2005) is used as a gold-standard to inform these decisions, similar to the way that we use the Vadas and Curran (2007a) data. 336 (a) (b) (c) N N N N N /N lung N /N ��� deaths ��� N /N N deaths N cancer deaths N /N (N/N)/(N/N) lung cancer ��� lung cancer ��� Figure 2: (a) Original right-branching CCGbank (b) Left-branching (c) Left-branching with new supertags 2.2 CCG parsing The C&amp;C CCG parser (Clark and Curran, 2007b) is used to perform our experiments, and to evaluate the effect of the changes to CCGbank. The parser uses a two-stage system, first employing a supertagger (Bangalore and Joshi, 1999) to propose lexical categories for each word, and then applying the CKY chart parsing algorithm. A log-linear </context>
<context position="8202" citStr="Vadas and Curran (2007" startWordPosition="1365" endWordPosition="1368">k. These dependencies are represented as 5-tuples: (hf, f, s, ha, l), where hf is the head of the predicate; f is the supertag of hf; s describes which argument of f is being filled; ha is the head of the argument; and l encodes whether the dependency is local or long-range. For example, the dependency encoding company as the object of bought (as in IBM bought the company) is represented by: (bought, (S\NP1)/NP2, 2, company,−) (4) This is a local dependency, where company is filling the second argument slot, the object. 3 Conversion Process This section describes the process of converting the Vadas and Curran (2007a) data to CCG derivations. The tokens dominated by NML and JJP brackets in the source data are formed into constituents in the corresponding CCGbank sentence. We generate the two forms of output that CCGbank contains: AUTO files, which represent the tree structure of each sentence; and PARG files, which list the word–word dependencies (Hockenmaier and Steedman, 2005). We apply one preprocessing step on the Penn Treebank data, where if multiple tokens are enclosed by brackets, then a NML node is placed around those tokens. For example, we would insert the NML bracket shown below: (NP (DT a) (-</context>
<context position="17258" citStr="Vadas and Curran (2007" startWordPosition="2881" endWordPosition="2884">rammar, first by those productions used in the existing CCGbank, and then against those actually licensed by CCG (with pre-existing ungrammaticalities removed). Sixteen errors were identified by this process and subsequently corrected by manual analysis. In total, we have altered 12,475 CCGbank sentences (25.5%) and 20,409 dependencies (1.95%). 4 NER features Named entity recognition (NER) provides information that is particularly relevant for NP parsing, simply because entities are nouns. For example, knowing that Air Force is an entity tells us that Air Force contract is aleft-branching NP. Vadas and Curran (2007a) describe using NE tags during the annotation process, suggesting that NERbased features will be helpful in a statistical model. There has also been recent work combining NER and parsing in the biomedical field. Lewin (2007) experiments with detecting base-NPs using NER information, while Buyko et al. (2007) use a CRF to identify 339 a guest comedian Victor Borge a guest comedian Victor Borge NP[nb]/N N /N N /N N /N N NP[nb]/N N /N N (NP\NP)/(NP\NP) NP\NP &gt; &gt; &gt; N N NP\NP NP &gt; &gt; &lt; N NP &gt; NP (a) (b) Figure 4: CCGbank derivations for apposition with DT N &gt; coordinate structure in biological nam</context>
<context position="22527" citStr="Vadas and Curran (2007" startWordPosition="3819" endWordPosition="3822">re are now more complex possibilities, equal to the Catalan number of the length of the NP. 5.2 Initial parsing results We now compare parser performance on our NP corrected version of the corpus to that on original CCGbank. We are using the normal-form parser model and report labelled precision, recall and F-score for all dependencies. The results are shown in Table 3. The F-score drops by 0.31% in our new version of the corpus. However, this comparison is not entirely fair, as the original CCGbank test data does not include the NP structure that the NP corrected model is being evaluated on. Vadas and Curran (2007a) experienced a similar drop in performance on Penn Treebank data, and noted that the F-score for NML and JJP brackets was about 20% lower than the overall figure. We suspect that a similar effect is causing the drop in performance here. Unfortunately, there are no explicit NML and JJP brackets to evaluate on in the CCG corpus, and so an NP structure only figure is difficult to compute. Recall can be calculated by marking those dependencies altered in the conversion process, and evaluating only on them. Precision cannot be measured in this PREC RECALL F-SCORE Original 83.65 82.81 83.23 NP cor</context>
<context position="27294" citStr="Vadas and Curran (2007" startWordPosition="4610" endWordPosition="4613">n CCGbank has been. Furthermore, the performance increase of 0.59% on the NP corrected corpus is more than the 0.25% increase on the original. This demonstrates that NER features are particularly helpful for NP structure. PREC RECALL F-SCORE Original 86.86 81.61 84.15 NP corrected 87.97 82.54 85.17 Table 7: DepBank gold-standard evaluation PREC RECALL F-SCORE Original 82.57 81.29 81.92 NP corrected 83.53 82.15 82.84 Original, NER 82.87 81.49 82.17 NP corrected, NER 84.12 82.75 83.43 Table 8: DepBank evaluation results 7 Conclusion The first contribution of this paper is the application of the Vadas and Curran (2007a) data to Combinatory Categorial Grammar. Our experimental results have shown that this more accurate representation of CCGbank’s NP structure increases parser performance. Our second major contribution is the introduction of novel NER features, a source of semantic information previously unused in parsing. As a result of this work, internal NP structure is now recoverable by the C&amp;C parser, a result demonstrated by our total performance increase of 1.51% F-score. Even when parsing raw text, without gold standard POS and NER tags, our approach has resulted in performance gains. In addition, w</context>
</contexts>
<marker>Vadas, Curran, 2007</marker>
<rawString>David Vadas and James R. Curran. 2007a. Adding noun phrase structure to the Penn Treebank. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL-07), pages 240–247. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vadas</author>
<author>James R Curran</author>
</authors>
<title>Large-scale supervised models for noun phrase bracketing.</title>
<date>2007</date>
<booktitle>In Proceedings of the 10th Conference of the Pacific Association for Computational Linguistics (PACLING-2007),</booktitle>
<pages>104--112</pages>
<location>Melbourne, Australia.</location>
<contexts>
<context position="2104" citStr="Vadas and Curran (2007" startWordPosition="336" endWordPosition="339">as such, cannot leave NP structure underspecified. Instead, all NPs were made rightbranching, as shown in this example: 335 (N (N/N lung) (N (N/N cancer) (N deaths) ) ) This structure is correct for most English NPs and is the best solution that doesn’t require manual reannotation. However, the resulting derivations often contain errors. This can be seen in the previous example, where lung cancer should form a constituent, but does not. The first contribution of this paper is to correct these CCGbank errors. We apply an automatic conversion process using the gold-standard NP data annotated by Vadas and Curran (2007a). Over a quarter of the sentences in CCGbank need to be altered, demonstrating the magnitude of the NP problem and how important it is that these errors are fixed. We then run a number of parsing experiments using our new version of the CCGbank corpus. In particular, we implement new features using NER tags from the BBN Entity Type Corpus (Weischedel and Brunstein, 2005). These features are targeted at improving the recovery of NP structure, increasing parser performance by 0.64% F-score. Finally, we evaluate against DepBank (King et al., 2003). This corpus annotates internal NP structure, a</context>
<context position="4408" citStr="Vadas and Curran (2007" startWordPosition="716" endWordPosition="719">he association strength between words 1–2 to words 2–3; and the dependency model, which compares words 1–2 to words 1–3. Lauer (1995) experiments with a data set of 244 NPs, and finds that the dependency model is superior, achieving 80.7% accuracy. Most NP bracketing research has used Lauer’s data set. Because it is a very small corpus, most approaches have been unsupervised, measuring association strength with counts from a separate large corpus. Nakov and Hearst (2005) use search engine hit counts and extend the query set with typographical markers. This results in 89.3% accuracy. Recently, Vadas and Curran (2007a) annotated internal NP structure for the entire Penn Treebank, providing a large gold-standard corpus for NP bracketing. Vadas and Curran (2007b) carry out supervised experiments using this data set of 36,584 NPs, outperforming the Collins (2003) parser. The Vadas and Curran (2007a) annotation scheme inserts NML and JJP brackets to describe the correct NP structure, as shown below: (NP (NML (NN lung) (NN cancer) ) (NNS deaths) ) We use these brackets to determine new goldstandard CCG derivations in Section 3. 2.1 Combinatory Categorial Grammar Combinatory Categorial Grammar (CCG) (Steedman, </context>
<context position="6752" citStr="Vadas and Curran (2007" startWordPosition="1111" endWordPosition="1114">1. When a conjunction occurs in an NP, a non-CCG rule is required in order to reach a parse: conj N N (3) This rule treats the conjunction in the same manner as a modifier, and results in the incorrect derivation shown in Figure 1(a). Our work creates the correct CCG derivation, shown in Figure 1(b), and removes the need for the grammar rule in (3). Honnibal and Curran (2007) have also made changes to CCGbank, aimed at better differentiating between complements and adjuncts. PropBank (Palmer et al., 2005) is used as a gold-standard to inform these decisions, similar to the way that we use the Vadas and Curran (2007a) data. 336 (a) (b) (c) N N N N N /N lung N /N ��� deaths ��� N /N N deaths N cancer deaths N /N (N/N)/(N/N) lung cancer ��� lung cancer ��� Figure 2: (a) Original right-branching CCGbank (b) Left-branching (c) Left-branching with new supertags 2.2 CCG parsing The C&amp;C CCG parser (Clark and Curran, 2007b) is used to perform our experiments, and to evaluate the effect of the changes to CCGbank. The parser uses a two-stage system, first employing a supertagger (Bangalore and Joshi, 1999) to propose lexical categories for each word, and then applying the CKY chart parsing algorithm. A log-linear </context>
<context position="8202" citStr="Vadas and Curran (2007" startWordPosition="1365" endWordPosition="1368">k. These dependencies are represented as 5-tuples: (hf, f, s, ha, l), where hf is the head of the predicate; f is the supertag of hf; s describes which argument of f is being filled; ha is the head of the argument; and l encodes whether the dependency is local or long-range. For example, the dependency encoding company as the object of bought (as in IBM bought the company) is represented by: (bought, (S\NP1)/NP2, 2, company,−) (4) This is a local dependency, where company is filling the second argument slot, the object. 3 Conversion Process This section describes the process of converting the Vadas and Curran (2007a) data to CCG derivations. The tokens dominated by NML and JJP brackets in the source data are formed into constituents in the corresponding CCGbank sentence. We generate the two forms of output that CCGbank contains: AUTO files, which represent the tree structure of each sentence; and PARG files, which list the word–word dependencies (Hockenmaier and Steedman, 2005). We apply one preprocessing step on the Penn Treebank data, where if multiple tokens are enclosed by brackets, then a NML node is placed around those tokens. For example, we would insert the NML bracket shown below: (NP (DT a) (-</context>
<context position="17258" citStr="Vadas and Curran (2007" startWordPosition="2881" endWordPosition="2884">rammar, first by those productions used in the existing CCGbank, and then against those actually licensed by CCG (with pre-existing ungrammaticalities removed). Sixteen errors were identified by this process and subsequently corrected by manual analysis. In total, we have altered 12,475 CCGbank sentences (25.5%) and 20,409 dependencies (1.95%). 4 NER features Named entity recognition (NER) provides information that is particularly relevant for NP parsing, simply because entities are nouns. For example, knowing that Air Force is an entity tells us that Air Force contract is aleft-branching NP. Vadas and Curran (2007a) describe using NE tags during the annotation process, suggesting that NERbased features will be helpful in a statistical model. There has also been recent work combining NER and parsing in the biomedical field. Lewin (2007) experiments with detecting base-NPs using NER information, while Buyko et al. (2007) use a CRF to identify 339 a guest comedian Victor Borge a guest comedian Victor Borge NP[nb]/N N /N N /N N /N N NP[nb]/N N /N N (NP\NP)/(NP\NP) NP\NP &gt; &gt; &gt; N N NP\NP NP &gt; &gt; &lt; N NP &gt; NP (a) (b) Figure 4: CCGbank derivations for apposition with DT N &gt; coordinate structure in biological nam</context>
<context position="22527" citStr="Vadas and Curran (2007" startWordPosition="3819" endWordPosition="3822">re are now more complex possibilities, equal to the Catalan number of the length of the NP. 5.2 Initial parsing results We now compare parser performance on our NP corrected version of the corpus to that on original CCGbank. We are using the normal-form parser model and report labelled precision, recall and F-score for all dependencies. The results are shown in Table 3. The F-score drops by 0.31% in our new version of the corpus. However, this comparison is not entirely fair, as the original CCGbank test data does not include the NP structure that the NP corrected model is being evaluated on. Vadas and Curran (2007a) experienced a similar drop in performance on Penn Treebank data, and noted that the F-score for NML and JJP brackets was about 20% lower than the overall figure. We suspect that a similar effect is causing the drop in performance here. Unfortunately, there are no explicit NML and JJP brackets to evaluate on in the CCG corpus, and so an NP structure only figure is difficult to compute. Recall can be calculated by marking those dependencies altered in the conversion process, and evaluating only on them. Precision cannot be measured in this PREC RECALL F-SCORE Original 83.65 82.81 83.23 NP cor</context>
<context position="27294" citStr="Vadas and Curran (2007" startWordPosition="4610" endWordPosition="4613">n CCGbank has been. Furthermore, the performance increase of 0.59% on the NP corrected corpus is more than the 0.25% increase on the original. This demonstrates that NER features are particularly helpful for NP structure. PREC RECALL F-SCORE Original 86.86 81.61 84.15 NP corrected 87.97 82.54 85.17 Table 7: DepBank gold-standard evaluation PREC RECALL F-SCORE Original 82.57 81.29 81.92 NP corrected 83.53 82.15 82.84 Original, NER 82.87 81.49 82.17 NP corrected, NER 84.12 82.75 83.43 Table 8: DepBank evaluation results 7 Conclusion The first contribution of this paper is the application of the Vadas and Curran (2007a) data to Combinatory Categorial Grammar. Our experimental results have shown that this more accurate representation of CCGbank’s NP structure increases parser performance. Our second major contribution is the introduction of novel NER features, a source of semantic information previously unused in parsing. As a result of this work, internal NP structure is now recoverable by the C&amp;C parser, a result demonstrated by our total performance increase of 1.51% F-score. Even when parsing raw text, without gold standard POS and NER tags, our approach has resulted in performance gains. In addition, w</context>
</contexts>
<marker>Vadas, Curran, 2007</marker>
<rawString>David Vadas and James R. Curran. 2007b. Large-scale supervised models for noun phrase bracketing. In Proceedings of the 10th Conference of the Pacific Association for Computational Linguistics (PACLING-2007), pages 104–112. Melbourne, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Ada Brunstein</author>
</authors>
<title>BBN pronoun coreference and entity type corpus.</title>
<date>2005</date>
<tech>Technical report.</tech>
<contexts>
<context position="2479" citStr="Weischedel and Brunstein, 2005" startWordPosition="402" endWordPosition="405">evious example, where lung cancer should form a constituent, but does not. The first contribution of this paper is to correct these CCGbank errors. We apply an automatic conversion process using the gold-standard NP data annotated by Vadas and Curran (2007a). Over a quarter of the sentences in CCGbank need to be altered, demonstrating the magnitude of the NP problem and how important it is that these errors are fixed. We then run a number of parsing experiments using our new version of the CCGbank corpus. In particular, we implement new features using NER tags from the BBN Entity Type Corpus (Weischedel and Brunstein, 2005). These features are targeted at improving the recovery of NP structure, increasing parser performance by 0.64% F-score. Finally, we evaluate against DepBank (King et al., 2003). This corpus annotates internal NP structure, and so is particularly relevant for the changes we have made to CCGbank. The CCG parser now recovers additional structure learnt from our NP corrected corpus, increasing performance by 0.92%. Applying the NER features results in a total increase of 1.51%. This work allows parsers trained on CCGbank to model NP structure accurately, and then pass this crucial information on </context>
<context position="17951" citStr="Weischedel and Brunstein, 2005" startWordPosition="3007" endWordPosition="3010">sting that NERbased features will be helpful in a statistical model. There has also been recent work combining NER and parsing in the biomedical field. Lewin (2007) experiments with detecting base-NPs using NER information, while Buyko et al. (2007) use a CRF to identify 339 a guest comedian Victor Borge a guest comedian Victor Borge NP[nb]/N N /N N /N N /N N NP[nb]/N N /N N (NP\NP)/(NP\NP) NP\NP &gt; &gt; &gt; N N NP\NP NP &gt; &gt; &lt; N NP &gt; NP (a) (b) Figure 4: CCGbank derivations for apposition with DT N &gt; coordinate structure in biological named entities. We draw NE tags from the BBN Entity Type Corpus (Weischedel and Brunstein, 2005), which describes 28 different entity types. These include the standard person, location and organization classes, as well person descriptions (generally occupations), NORP (National, Other, Religious or Political groups), and works of art. Some classes also have finer-grained subtypes, although we use only the coarse tags in our experiments. Clark and Curran (2007b) has a full description of the C&amp;C parser’s pre-existing features, to which we have added a number of novel NER-based features. Many of these features generalise the head words and/or POS tags that are already part of the feature s</context>
</contexts>
<marker>Weischedel, Brunstein, 2005</marker>
<rawString>Ralph Weischedel and Ada Brunstein. 2005. BBN pronoun coreference and entity type corpus. Technical report.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>