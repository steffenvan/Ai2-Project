<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002629">
<title confidence="0.985369">
Practical Grammar-Based NLG from Examples
</title>
<author confidence="0.481357">
David DeVault and David Traum and Ron Artstein
</author>
<affiliation confidence="0.25963">
USC Institute for Creative Technologies
</affiliation>
<address confidence="0.41208">
13274 Fiji Way
Marina del Rey, CA 90292
</address>
<email confidence="0.998399">
{devault,traum,artstein}@ict.usc.edu
</email>
<sectionHeader confidence="0.995636" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999722352941176">
We present a technique that opens up
grammar-based generation to a wider range
of practical applications by dramatically re-
ducing the development costs and linguis-
tic expertise that are required. Our method
infers the grammatical resources needed for
generation from a set of declarative exam-
ples that link surface expressions directly to
the application’s available semantic represen-
tations. The same examples further serve to
optimize a run-time search strategy that gener-
ates the best output that can be found within an
application-specific time frame. Our method
offers substantially lower development costs
than hand-crafted grammars for application-
specific NLG, while maintaining high output
quality and diversity.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9977941875">
This paper presents a new example-based genera-
tion technique designed to reduce the development
costs and linguistic expertise needed to integrate a
grammar-based generation component into an ex-
isting application. We believe this approach will
broaden the class of applications in which grammar-
based generation may feasibly be deployed.
In principle, grammar-based generation offers
significant advantages for many applications, when
compared with simpler template-based or canned
text output solutions, by providing productive cov-
erage and greater output variety. However, realiz-
ing these advantages can require significant devel-
opment costs (Busemann and Horacek, 1998).
One possible strategy is to exploit a wide-
coverage realizer that aims for applicability in mul-
tiple application domains (White et al., 2007; Cahill
and van Genabith, 2006; Zhong and Stent, 2005;
Langkilde-Geary, 2002; Langkilde and Knight,
1998; Elhadad, 1991). These realizers provide
a sound wide-coverage grammar (or robust wide-
coverage language model) for free, but demand a
specific input format that is otherwise foreign to
an existing application. Unfortunately, the devel-
opment burden of implementing the translation be-
tween the system’s available semantic representa-
tions and the required input format can be quite sub-
stantial (Busemann and Horacek, 1998). Indeed, im-
plementing the translation might require as much ef-
fort as would be required to build a simple custom
generator; cf. (Callaway, 2003). Thus, there cur-
rently are many applications where using a wide-
coverage generator remains impractical.
Another strategy is for system builders to hand
craft an application-specific grammar for genera-
tion. This approach can be initially attractive to
system builders because it allows syntactic cover-
age and semantic modeling to be tailored directly
to application needs. However, writing grammati-
cal rules by hand ultimately requires a painstaking,
time-consuming effort by a developer who has de-
tailed linguistic knowledge as well as detailed appli-
cation knowledge. Further, the resulting coverage is
inevitably limited to the set of linguistic construc-
tions that have been selected for careful modeling.
A third strategy is to use an example-based ap-
proach (Wong and Mooney, 2007; Stone, 2003;
Varges and Mellish, 2001) in which the connection
</bodyText>
<page confidence="0.997382">
77
</page>
<bodyText confidence="0.99989482051282">
between available application semantic representa-
tions and desired output utterances is specified by
example. Example-based approaches aim to allow
system builders to specify a productive generation
capacity while leaving the representations and rea-
soning that underlie that productive capacity mostly
implicit in a set of training examples. This method-
ology insulates system builders from the detailed ex-
pertise and technical infrastructure needed to imple-
ment the productive capacity directly, and has made
example-based approaches attractive not only in text
generation but also in related areas such as concate-
native speech synthesis and motion capture based
animation; see, e.g., (Stone et al., 2004).
The technique we present in this paper is a new
example-based approach to specifying application-
specific text generation. As in other hand-crafted
and example-based approaches, our technique al-
lows syntactic coverage and semantic modeling to
follow the needs and available semantic representa-
tions in an application. One contribution of our tech-
nique is to relieve the generation content author of
the burden of manual syntactic modeling by lever-
aging an off-the-shelf parser; defects in the syntax
provided by the parser are effectively overcome us-
ing a machine learning technique. Additionally, our
technique organizes the authoring task in a way that
relieves the generation author of carefully modeling
the connections between particular syntactic con-
structions and available semantic representations.
Together, we argue, these features dramatically
reduce the linguistic expertise and other develop-
ment costs that are required to integrate a grammar-
based generation component into an existing system.
In a case study application, we show that our ap-
proach allows an application developer who lacks
detailed linguistic knowledge to extend grammatical
coverage at an expense of less than one minute per
additional lexical entry.
</bodyText>
<sectionHeader confidence="0.935376" genericHeader="method">
2 Case Study: Doctor Perez
</sectionHeader>
<bodyText confidence="0.999792425925926">
Our approach has been tested as a replacement for
the generation component of interactive virtual hu-
mans used for social training purposes (Swartout et
al., 2006). Virtual humans are embodied conversa-
tional agents that play the role of people in simula-
tions or games. The case study we present in this
paper is the generation of output utterances for a
particular virtual human, Doctor Perez, who is de-
signed to teach negotiation skills in a multi-modal,
multi-party, non-team dialogue setting (Traum et al.,
2008). The human trainee who talks to the doctor
plays the role of a U.S. Army captain named Cap-
tain Kirk. The design goals for Doctor Perez create
a number of requirements for a practical NLG com-
ponent. We briefly summarize these requirements
here; see (DeVault et al., 2008) for more details.
Doctor Perez has a relatively rich internal mental
state including beliefs, goals, plans, and emotions.
He uses an attribute-value matrix (AVM) semantic
representation to describe an utterance as a set of
core speech acts and other dialogue acts. Speech
acts generally have semantic contents that describe
propositions and questions about states and actions
in the domain. To facilitate interprocess communi-
cation, and statistical processing, this AVM structure
is linearized into a “frame” of key values in which
each non-recursive terminal value is paired with a
path from the root to the final attribute. Figure 1
shows a typical frame. See (Traum, 2003) for addi-
tional details and examples of this representation.
While only hundreds of frames currently arise in
actual dialogues, the number of potential frames is
orders of magnitude larger, and it is difficult to pre-
dict in advance which frames might occur. The ut-
terances that realize these frames need to take a va-
riety of syntactic forms, including simple declar-
ative sentences, various modal constructions relat-
ing to hypothetical actions or plans, yes/no and wh-
questions, and abbreviated dialogue forms such as
elliptical clarification and repair requests, ground-
ing, and turn-taking utterances. Highly fluent out-
put is not a necessity for this character, since Doc-
tor Perez is designed to simulate a non-native En-
glish speaker. However, in order to support com-
pelling real-time conversational interaction and ef-
fective training, the generation module must be able
to identify an utterance for Doctor Perez to use
within approximately 200ms on modern hardware.
Finally, the development team for Doctor Perez’s
language capabilities includes approximately 10
programmers, testers, linguists, and computational
linguists. Wherever possible, it is better if any de-
veloper can improve any aspect of Doctor Perez’s
language processing; e.g., if a programmer discov-
</bodyText>
<page confidence="0.996809">
78
</page>
<bodyText confidence="0.982902333333333">
ers a bug or disfluency in the NLG output, it is better
if she can fix it directly rather than requiring a (com-
putational) linguist to do so.
</bodyText>
<sectionHeader confidence="0.965756" genericHeader="method">
3 Technical Approach
</sectionHeader>
<bodyText confidence="0.999974689655172">
Our approach builds on recently developed tech-
niques in statistical parsing, lexicalized syntax mod-
eling, generation with lexicalized grammars, and
search optimization to automatically construct all
the resources needed for a high-quality run-time
generation component. In particular, we leverage the
increasing availability of off-the-shelf parsers such
as (Charniak, 2001; Charniak, 2005) to automati-
cally (or semi-automatically) assign syntactic anal-
yses to a set of suggested output sentences. We
then draw on lexicalization techniques for statistical
language models (Magerman, 1995; Collins, 1999;
Chiang, 2000; Chiang, 2003) to induce a probabilis-
tic, lexicalized tree-adjoining grammar that supports
the derivation of all the suggested output sentences,
and many others besides.
The final step is to use the training examples to
learn an effective search policy so that our run-time
generation component can find good output sen-
tences in a reasonable time frame. In particular, we
use variants of existing search optimization (Daumé
and Marcu, 2005) and ranking algorithms (Collins
and Koo, 2005) to train our run-time component to
find good outputs within a specified time window;
see also (Stent et al., 2004; Walker et al., 2001). The
result is a run-time component that treats generation
as an anytime search problem, and is thus suitable
for applications in which a time/performance trade-
off is necessary (such as real-time dialogue).
</bodyText>
<subsectionHeader confidence="0.999948">
3.1 Specification of Training Examples
</subsectionHeader>
<bodyText confidence="0.997972117647059">
Each training example in our approach speci-
fies a target output utterance (string), its syn-
tax, and a set of links between substrings within
the utterance and system semantic representa-
tions. Formally, a training example takes the form
(u, syntax(u), semantics(u)). We will illustrate
this format using the training example in Figure 1. In
this example, the generation content author suggests
the output utterance u = we don’t have medical
supplies here captain. Each utterance u is accom-
panied by syntax(u), a syntactic analysis in Penn
Treebank format (Marcus et al., 1994). In the fig-
ure, we show two alternative syntactic analyses that
might be specified: one is the uncorrected output of
the Charniak parser on this sentence, and the other
a hand-corrected version of that parse; we evaluate
the utility of this hand correction in Section 4.
To represent the meaning of utterances, our ap-
proach assumes that the system provides some set
M = {m1, ..., mj} of semantic representations.
The meaning of any individual utterance is then
identified with some subset of M. For Doctor Perez,
M comprises the 232 distinct key-value pairs that
appear in the system’s various generation frames. In
this example, the utterance’s meaning is captured by
the 8 key-value pairs indicated in the figure.
Our approach requires the generation content
author to link these 8 key-value pairs to con-
tiguous surface expressions within the utterance.
The technique is flexible about which surface ex-
pressions are chosen (e.g. they need not corre-
spond to constituent boundaries); however, they do
need to be compatible with the way the syntactic
analysis tokenizes the utterance, as follows. Let
</bodyText>
<equation confidence="0.98683">
t(u) = (t1,..., tn) be the terminals in the syn-
tactic analysis, in left-to-right order. Formally,
semantics(u) = {(s1, M1), ..., (sk, Mk)}, where
t(u) = s1@ · · · @sk (with @ denoting concatena-
</equation>
<bodyText confidence="0.999934">
tion), and where MZ C M for all i E 1..k. In this
example, the surface expression we don’t, which to-
kenizes as (we, do, n′t), is connected to key-values
that indicate a negative polarity assertion.
This training example format has two features that
are crucial to our approach. First, the semantics of
an utterance is specified independently of its syntax.
This greatly reduces the amount of linguistic exper-
tise a generation content author needs to have. It
also allows making changes to the underlying syn-
tax without having to re-author the semantic links.
Second, the assignment of semantic representa-
tions to surface expressions must span the entire ut-
terance. No words or expressions can be viewed as
“meaningless”. This is essential because, otherwise,
the semantically motivated search algorithm used in
generation has no basis on which to include those
particular expressions when it constructs its output
utterance. Many systems, including Doctor Perez,
lack some of the internal representations that would
be necessary to specify semantics down to the lex-
</bodyText>
<page confidence="0.985815">
79
</page>
<figure confidence="0.99232556060606">
Utterance we don’t have medical supplies here captain
cat: NP
cat: SA
cat: S
cat: S
pos: NN
captain
cat: NP
cat: VP
pos: PRP
we
pos: AUX
do
pos: RB
n’t
cat: VP
pos: AUX
have
pos: JJ
medical
pos: NNS
supplies
cat: NP
cat: SA
cat: S
cat: NP
pos: PRP
we
pos: AUX
do
cat: VP
pos: RB
n’t
pos:AUX
have
pos: JJ
medical
pos: NNS
supplies
cat: ADVP
pos: RB
here
cat: VP
cat: NP
Syntax
cat: ADVP
pos: RB
here
pos: VBP
captain
cat: VP
(corrected Charniak parse) or (uncorrected Charniak parse)
semantic frame
speech-act.action = assert
speech-act.content.polarity = negative
speech-act.content.attribute = resourceAttribute
speech-act.content.value = medical-supplies
speech-act.content.object-id = market
addressee = captain-kirk
dialogue-act.addressee = captain-kirk
speech-act.addressee = captain-kirk
we do n’t ....... {
have............11l.
Semantics medical supplies . .
here
captain ........ I
</figure>
<figureCaption confidence="0.9930805">
Figure 1: A generation training example for Doctor Perez. If uncorrected syntax is used, the generation content author
only writes the utterance and the links to the semantic frame.
</figureCaption>
<bodyText confidence="0.99842625">
ical level. An important feature of our approach is
that it allows an arbitrary semantic granularity to be
employed, by mapping the representations available
in the system to appropriate multi-word chunks.
</bodyText>
<subsectionHeader confidence="0.999463">
3.2 Automatic Grammar Induction
</subsectionHeader>
<bodyText confidence="0.996605642857143">
We adopt essentially the probabilistic tree-adjoining
grammar (PTAG) formalism and grammar induc-
tion technique of (Chiang, 2003). Our approach
makes three modifications, however. First, while
Chiang’s model includes both full adjunction and
sister adjunction operations, our grammar has only
sister adjunction (left and right), exactly as in the
TAGLET grammar formalism of (Stone, 2002). Sec-
ond, to support lexicalization at an arbitrary gran-
ularity, we allow Chiang’s tree templates to be as-
sociated with more than one lexical anchor. Third,
to unify syntactic and semantic reasoning in search,
we augment lexical anchors with semantic informa-
tion. Formally, wherever Chiang’s model has a lex-
ical anchor w, ours has a pair ((wi, ..., w,,,), M′),
where M′ ⊆ M is connected to lexical anchors
(wi, ..., w,,,) by the generation content author, as in
Figure 1. The result is that the derivation probabili-
ties the grammar assigns depend not only on the im-
plicated syntactic structures and lexical anchors but
also on the senses of those lexical anchors in appli-
cation terms.
We induce our grammar from training exam-
ples such as Figure 1 using heuristics to assign
derivations to the examples, exactly as in (Chiang,
2003). The process proceeds in two stages. In
the first stage, a collection of rules is used to au-
tomatically “decorate” the training syntax with a
number of features. These include deciding the
lexical anchor(s) for each non-terminal constituent
and assigning complement/adjunct status for non-
terminals which are not on their parent’s lexical-
ization path; see (Magerman, 1995; Chiang, 2003;
Collins, 1999). In addition, we deterministically add
features to improve several grammatical aspects, in-
cluding (1) enforcing verb inflectional agreement in
derived trees, (2) enforcing consistency in the finite-
ness of VP and S complements, and (3) restricting
subject/direct object/indirect object complements to
play the same grammatical role in derived trees.
In the second stage, the complements and ad-
juncts in the decorated trees are incrementally re-
</bodyText>
<page confidence="0.985878">
80
</page>
<table confidence="0.8855034">
fin:other, cat: S
cat: SA
cat: NP, apr: VBP,
apn: other
pos: PRP
we
apn: other, pos:VBP
do
fin: yes, cat: VP,
gra: obj1
syntax:
pos: RB
n’t
fin: yes, cat: VP
fin:yes, cat: VP,
gra: obj1
pos: VBP
have
cat: NP, gra: obj1
operations: initial tree comp
semantics: speech-act.action = assert speech-act.content.attribute = resourceAttribute
speech-act.content.polarity = negative
cat: NP, apr: VBP,
gra: obj1, apn: other
cat:ADVP, gra: adj
pos: RB
here
cat: NP, apr: VBZ,
gra: adj, apn: 3ps
pos: NN
captain
pos: JJ
medical
pos: NNS
supplies
syntax:
operations: comp left/right adjunction left/right adjunction
semantics: speech-act.content.value = speech-act.content.object-id = addressee = captain-kirk
medical-supplies market dialogue-act.addressee = captain-kirk
speech-act.addressee = captain-kirk
</table>
<figureCaption confidence="0.998991">
Figure 2: The linguistic resources inferred from the training example in Figure 1.
</figureCaption>
<bodyText confidence="0.999360615384615">
moved, yielding the reusable linguistic resources in
the grammar, as illustrated in Figure 2, as well as
the maximum likelihood estimates needed to com-
pute operation probabilities.
Our approach uses this induced grammar to treat
generation as a search problem: given a desired se-
mantic representation M′ ⊆ M, use the grammar
to incrementally construct an output utterance u that
expresses M′. We treat generation as anytime search
by accruing multiple goal states up until a specified
timeout (for Doctor Perez: 200ms) and returning a
list of alternative outputs ranked by their derivation
probabilities.
</bodyText>
<subsectionHeader confidence="0.999699">
3.3 Optimizing the Search Strategy
</subsectionHeader>
<bodyText confidence="0.9999693">
The search space created by a grammar induced in
this way is too large to be searched exhaustively in
most applications. The solution we have developed
is a beam search strategy that uses weighted features
to rank alternative grammatical expansions at each
step. In particular, the beam size and structure is op-
timized so that, with high probability, the beam can
be searched exhaustively before the timeout.1 The
second step of automated processing, then, is a train-
ing problem of finding weighted features such that
</bodyText>
<footnote confidence="0.95772825">
1For Doctor Perez, we use a wider beam for initial trees,
since the Doctor’s semantic representation is particularly im-
poverished at the level of main verbs. At search depths &gt; 1, we
use beam size 1 (i.e. greedy search).
</footnote>
<bodyText confidence="0.999942190476191">
for every training problem, nodes that lead to good
generation output are ranked highly enough by those
features to make it into the beam.
We use domain-independent rules to automati-
cally define a set of features that could be heuris-
tically useful for a given induced grammar. These
include features for various syntactic structures and
operations, numbers of undesired and desired mean-
ings of different types added by an expansion,
derivation probabilities, etc. (For Doctor Perez,
this yields about 600 features.) Our training algo-
rithm is based on the search optimization algorithm
of (Daumé and Marcu, 2005), which updates fea-
ture weights when mistakes are made during search
on training examples. For the weight update step,
we use the boosting approach of (Collins and Koo,
2005), which performs feature selection and iden-
tifies weight values that improve the ranking of al-
ternative derivation steps when mistakes are made
during search. We discuss the resulting success rate
and quality in the next section.
</bodyText>
<sectionHeader confidence="0.997704" genericHeader="method">
4 Cost/Benefit Analysis
</sectionHeader>
<bodyText confidence="0.9998086">
The motivation that underlies our technical approach
is to reduce the development costs and linguistic ex-
pertise needed to develop a grammar-based genera-
tion component for an existing system. In this sec-
tion, we assess the progress we have made by ana-
</bodyText>
<page confidence="0.996928">
81
</page>
<bodyText confidence="0.971663645833333">
lyzing the use of our approach for Doctor Perez.
Method. We began with a sample of 220 in-
stances of frames that Doctor Perez’s dialogue man-
ager had requested of the generation component in
previous dialogues with users. Each frame was as-
sociated with a hand-authored target output utter-
ance. We then constructed two alternative training
examples, in the format specified in Section 3.1, for
each frame. One example had uncorrected output
of the Charniak parser for the syntax, and another
had hand-corrected parser output (see Figure 1). The
connections between surface expressions and frame
key-value pairs were identical in both uncorrected
and corrected training sets.
We then built two generators using the two sets
of training examples. We used 90% of the data for
training and held out 10% for testing. The genera-
tors sometimes failed to find a successful utterance
within the 200ms timeout. For example, the success
rate of the version of our generator trained on uncor-
rected syntax was 96.0% for training examples and
81.8% for test examples.
Quality of generated output. To assess output
quality, 5 system developers rated each of 494 utter-
ances, in relation to the specific frame for which it
was produced, on a single 1 (“very bad”) to 5 (“very
good”) scale. The 494 utterances included all of the
hand-authored (suggested) utterances in the training
examples. They also included all the top-ranked ut-
terances that were successfully generated by the two
generators. We asked our judges to make an over-
all assessment of output quality, incorporating both
accuracy and fluency, for the Doctor Perez charac-
ter. Judges were blind to the conditions under which
utterances were produced. We discuss additional de-
tails of this rating task in (DeVault et al., 2008).
The judges achieved a reliability of α = 0.708
(Krippendorff, 1980); this value shows that agree-
ment is well above chance, and allows for tentative
conclusions. We ran a small number of planned
comparisons on these ratings. Surprisingly, we
found no significant difference between generated
output trained on corrected and uncorrected syntax
(t(29) = 0.056,p &gt; 0.9 on test items, t(498) =
−1.1,p &gt; 0.2 on all items).2 However, we did
2The distribution of ratings across utterances is not normal;
to validate our results we accompanied each t-test by a non-
parametric Wilcoxon rank sum test, and significance always fell
</bodyText>
<figure confidence="0.962249090909091">
Hand-authored (N = 1099)
Generated:
Training input (N = 949)
Test input (N = 90)
40
30
20
10
0
1 2 3 4 5
Rating
</figure>
<figureCaption confidence="0.995374">
Figure 3: Observed rating frequencies for hand-authored
vs. generated utterances (uncorrected syntax).
</figureCaption>
<bodyText confidence="0.989135428571429">
find that hand-authored utterances (mean rating 4.4)
are significantly better (t(388) = 5.9,p &lt; 0.001)
than generated utterances (mean rating 3.8 for un-
corrected syntax). These ratings are depicted in Fig-
ure 3. While the figure suggests a slight reduction in
quality for generated output for test frames vs. train-
ing frames, we did not find a significant difference
between the two (t(19) = 1.4228,p &gt; 0.15).
Variety of generated output. In general our any-
time algorithm delivers a ranked list of alternative
outputs. While in this initial study our judges rated
only the highest ranked output generated for each
frame, we observed that many of the lower ranked
outputs are of relatively high quality. For example,
Figure 4 shows a variety of alternative outputs that
were generated for two of Doctor Perez’s training
examples. Many of these outputs are not present as
hand-authored utterances (for any frame); this illus-
trates the potential of our approach to provide a va-
riety of alternative outputs or paraphrases, which in
some applications may be useful even for meanings
for which an example utterance is hand-authored.
Figure 5 shows the overall distribution in the number
of outputs returned for Doctor Perez.
Development costs. The development costs in-
cluded implementation of the approach and specifi-
cation of Doctor Perez’s training set. Implementa-
in the same general range.
</bodyText>
<figure confidence="0.889040071428572">
60
50
Frequency (%)
82
Rank Time (ms) Novel?
1 16 no the clinic is up to standard captain
2 94 no the clinic is acceptable captain (hand-authored for this input)
3 78 yes the clinic should be in acceptable condition captain
4 16 yes the clinic downtown is currently acceptable captain
5 78 yes the clinic should agree in an acceptable condition captain
1 94 no there are no medical supplies downtown captain
2 172 no we don’t have medical supplies downtown captain
3 125 yes well captain i do not think downtown there are medical supplies
4 16 yes i do not think there are medical supplies downtown captain
</figure>
<figureCaption confidence="0.9989195">
Figure 4: All the utterances generated (uncorrected syntax) for two examples. Rank is determined by derivation
probability. Outputs marked as novel are different from any suggested output for any training example.
</figureCaption>
<figure confidence="0.997295333333333">
30
20 Frequency (%)
10
0
0 1 2 3 4 5 6 7 8 9
Number of successful outputs
</figure>
<figureCaption confidence="0.999949">
Figure 5: Variety of outputs for each input.
</figureCaption>
<bodyText confidence="0.999976722222222">
tion required an effort of approximately six person
months. The developer who carried out the imple-
mentation initially had no familiarity with the Doc-
tor Perez domain, so part of this time was spent un-
derstanding Doctor Perez and his available seman-
tic representations. The bulk of the development
time was spent implementing the grammar induction
and training processes. Grammar induction included
implementing the probabilistic grammar model and
writing about 40 rules that are used to extract gram-
matical entries from the training examples. Of these
40 rules, only 3 are specific to Doctor Perez.3 The
remainder are broadly applicable to syntactic anal-
yses in Penn Treebank format, and thus we expect
they would transfer to applications of our approach
in other domains. Similarly, the training algorithms
are entirely domain neutral and could be expected to
transfer well to additional domains.
</bodyText>
<subsectionHeader confidence="0.866559">
Specification of Doctor Perez’s training data took
</subsectionHeader>
<footnote confidence="0.851963">
3These 3 rules compensate for frequent errors in Charniak
parser output for the words captain, elder, and imam, which are
often used to signal the addressee of Doctor Perez’s utterances.
</footnote>
<bodyText confidence="0.99817534375">
about 6 hours, or about 1.6 minutes per training ex-
ample. This time included hand correction of syn-
tactic analyses generated by the Charniak parser and
definition of semantic links between surface expres-
sions and frame key-value pairs. Since we found
that hand-correcting syntax does not improve out-
put quality, this 1.6 minutes/example figure over-
estimates the authoring time required by our ap-
proach. The remaining work lies in defining the se-
mantic links. For Doctor Perez, approximately half
of the semantic links were automatically assigned
with simple ad hoc scripts.4 The semantic linking
process might be further sped up through a stream-
lined authoring interface offering additional automa-
tion, or even using a machine learning approach to
suggest appropriate links.
Linguistic expertise required. Since we found
that hand-correcting syntax does not improve output
quality, a developer who wishes to exploit our ap-
proach may use the Charniak parser to supply the
syntactic model for the domain. Thus, while one
developer with linguistic expertise is required to im-
plement the approach, anybody on the application
team can contribute by hand authoring additional ut-
terances and defining semantic links. The benefit of
this authoring effort is the ability to generate high
quality output for many novel semantic inputs.
Cost/benefit. The grammar induced from the 198
training examples (with uncorrected syntax) con-
tains 426 lexical entries of the type depicted in Fig-
ure 2. These 426 lexical entries were produced auto-
matically from about 6 hours worth of authoring ef-
</bodyText>
<footnote confidence="0.882091">
4Time to compose these scripts is included in the 1.6 min-
utes/example.
</footnote>
<page confidence="0.998737">
83
</page>
<bodyText confidence="0.999952222222222">
fort together with domain-neutral algorithms. This
translates to a rate of grammar expansion of less
than 1 minute per lexical entry, on average, for this
small application-specific grammar. This constitutes
a dramatic improvement over our previous experi-
ence hand-crafting grammars. It would be challeng-
ing for an expert to specify a lexical entry such as
those in Figure 2 in under one minute (and probably
impossible for someone lacking detailed linguistic
knowledge). In our experience, however, the bulk
of development lies in additional time spent con-
sidering and investigating possible interactions be-
tween lexical entries in generation. Our technique
helps with both problems: the grammar induction
streamlines the specification of lexical entries, and
the training removes the need for a developer to
manually trace through the various complex inter-
actions between lexical entries during generation.
</bodyText>
<sectionHeader confidence="0.998057" genericHeader="method">
5 Limitations
</sectionHeader>
<bodyText confidence="0.99898625">
Currently, we do not support semantic links from
non-contiguous expressions, which means a desired
output like “we rely heavily on medical supplies”
would be difficult to annotate if rely...on corresponds
to a single semantic representation. This is not an in-
trinsic limitation to our general approach, but rather
a simplification in our initial implementation.
As discussed in Section 3.2, our grammar induc-
tion process adds syntactic features related to verb
inflection, finiteness, and grammatical role to the in-
ferred lexical entries. Such features improve the flu-
ency and accuracy of output derived with the gram-
mar. While we believe such features can always be
assigned using domain-independent rules, develop-
ing these rules requires linguistic expertise, and it
is likely that additional rules and features (not yet
implemented) would improve coverage of linguistic
phenomena such as control verbs, various kinds of
coordination, and relative clauses, inter alia.
A more entrenched limitation of our approach
is its assumption that the generator does not need
context as a separate input. This means, for ex-
ample, that our approach cannot generate referring
expressions (by selecting disambiguating semantic
properties); rather, all semantic properties must be
pre-selected and included in the generation request.
Generation of anaphoric expressions is also limited,
since contextual ambiguities are not considered.
</bodyText>
<sectionHeader confidence="0.999916" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999995590909091">
To our knowledge, this is the first implemented
generation technique that does all three of the fol-
lowing: directly interfaces to existing application
semantic representations, infers a phrase structure
grammar from examples, and does not require hand-
authored syntax as input. (Varges and Mellish,
2001) also aims to reduce the authoring burden of
domain-specific generation; however, they seem to
use a special purpose semantic annotation rather
than pre-existing application semantics, and their
task is defined in terms of the Penn Treebank, so
hand-authored syntax is used as input. (Wong and
Mooney, 2007) also interfaces to existing applica-
tion semantics, and does not require hand-authored
syntax as input. Their technique infers a syn-
chronous grammar in which the hierarchical linguis-
tic analysis is isomorphic to the hierarchy in the ap-
plication semantics, and differs from phrase struc-
ture. It would be interesting to compare their out-
put quality with ours; their automated alignment of
words to semantics might also provide a way to fur-
ther reduce the authoring burden of our approach.
</bodyText>
<sectionHeader confidence="0.987885" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999996">
We have presented a new example-based approach
to specifying text generation for an existing appli-
cation. We have used a cost/benefit analysis to ar-
gue that our approach offers productive coverage
and high-quality output with less linguistic expertise
and lower development costs than building a hand-
crafted grammar. In future work, we will evaluate
our approach in additional application settings, and
study the performance of our approach as the size
and scope of the training set grows.
</bodyText>
<sectionHeader confidence="0.998302" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996365375">
Thanks to our anonymous reviewers, Arno Hartholt,
Susan Robinson, Thomas Russ, Chung-chieh Shan,
and Matthew Stone. This work was sponsored by the
U.S. Army Research, Development, and Engineer-
ing Command (RDECOM), and the content does not
necessarily reflect the position or the policy of the
Government, and no official endorsement should be
inferred.
</bodyText>
<page confidence="0.997813">
84
</page>
<sectionHeader confidence="0.990362" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999905836538462">
S. Busemann and H. Horacek. 1998. A flexible shallow
approach to text generation. In Proceedings of INLG,
pages 238–247.
Aoife Cahill and Josef van Genabith. 2006. Robust
PCFG-based generation using automatically acquired
LFG approximations. In ACL, pages 1033–1040.
C. B. Callaway. 2003. Evaluating coverage for large
symbolic NLG grammars. Proceedings of IJCAI.
E. Charniak. 2001. Immediate-head parsing for lan-
guage models. In ACL, pages 124–131, Morristown,
NJ, USA. Association for Computational Linguistics.
E. Charniak. 2005. ftp://ftp.cs.brown.edu/pub/nlparser/
parser05Aug16.tar.gz.
D. Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
ACL ’00: Proceedings of the 38th Annual Meeting
on Association for Computational Linguistics, pages
456–463, Morristown, NJ, USA. Association for Com-
putational Linguistics.
D. Chiang. 2003. Statistical parsing with an automat-
ically extracted tree adjoining grammar. In R. Bod,
R. Scha, and K. Sima’an, editors, Data Oriented Pars-
ing, pages 299–316. CSLI Publications, Stanford.
M. Collins and T. Koo. 2005. Discriminative reranking
for natural language parsing. Computational Linguis-
tics, 31(1):25–70.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. dissertation, Uni-
versity of Pennsylvania.
H. Daumé and D. Marcu. 2005. Learning as search
optimization: approximate large margin methods for
structured prediction. In ICML ’05: Proceedings of
the 22nd international conference on Machine learn-
ing, pages 169–176, New York, NY, USA. ACM.
David DeVault, David Traum, and Ron Artstein. 2008.
Making grammar-based generation easier to deploy in
dialogue systems. In Ninth SIGdial Workshop on Dis-
course and Dialogue (SIGdial).
M. Elhadad. 1991. FUF: the universal unifier user man-
ual version 5.0. Technical Report CUCS-038-91.
K. Krippendorff, 1980. Content Analysis: An Introduc-
tion to Its Methodology, chapter 12, pages 129–154.
Sage, Beverly Hills, CA.
I. Langkilde and K. Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
COLING-ACL, pages 704–710.
I. Langkilde-Geary. 2002. An empirical verification of
coverage and correctness for a general-purpose sen-
tence generator.
D. M. Magerman. 1995. Statistical decision-tree mod-
els for parsing. In Proceedings of the 33rd annual
meeting on Association for Computational Linguistics,
pages 276–283, Morristown, NJ, USA. Association for
Computational Linguistics.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguistics,
19(2):313–330.
A. Stent, R. Prasad, and M. Walker. 2004. Trainable sen-
tence planning for complex information presentation
in spoken dialog systems. In ACL.
Matthew Stone, Doug DeCarlo, Insuk Oh, Christian Ro-
driguez, Adrian Stere, Alyssa Lees, and Chris Bregler.
2004. Speaking with hands: creating animated con-
versational characters from recordings of human per-
formance. ACM Trans. Graph., 23(3):506–513.
M. Stone. 2002. Lexicalized grammar 101. In ACL
Workshop on Tools and Methodologies for Teaching
Natural Language Processing.
Matthew Stone. 2003. Specifying generation of referring
expressions by example. In AAAI Spring Symposium
on Natural Language Generation in Spoken and Writ-
ten Dialogue, pages 133–140.
W. Swartout, J. Gratch, R. W. Hill, E. Hovy, S. Marsella,
J. Rickel, and D. Traum. 2006. Toward virtual hu-
mans. AI Mag., 27(2):96–108.
D. R. Traum, W. Swartout, J. Gratch, and S. Marsella.
2008. A virtual human dialogue model for non-team
interaction. In L. Dybkjaer and W. Minker, editors,
Recent Trends in Discourse and Dialogue. Springer.
D. Traum. 2003. Semantics and pragmatics of questions
and answers for dialogue agents. In proceedings of the
International Workshop on Computational Semantics,
pages 380–394, January.
Sebastian Varges and Chris Mellish. 2001. Instance-
based natural language generation. In NAACL, pages
1–8.
M. Walker, O. Rambow, and M. Rogati. 2001. Spot:
A trainable sentence planner. In Proceedings of the
North American Meeting of the Association for Com-
putational Linguistics.
M. White, R. Rajkumar, and S. Martin. 2007. To-
wards broad coverage surface realization with CCG.
In Proc. of the Workshop on Using Corpora for NLG:
Language Generation and Machine Translation (UC-
NLG+MT).
Yuk Wah Wong and Raymond Mooney. 2007. Genera-
tion by inverting a semantic parser that uses statistical
machine translation. In Proceedings of NAACL-HLT,
pages 172–179.
H. Zhong and A. Stent. 2005. Building surface realiz-
ers automatically from corpora using general-purpose
tools. In Proc. Corpus Linguistics ’05 Workshop on
Using Corpora for Natural Language Generation.
</reference>
<page confidence="0.999699">
85
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.867723">
<title confidence="0.999992">Practical Grammar-Based NLG from Examples</title>
<author confidence="0.998635">David DeVault</author>
<author confidence="0.998635">David Traum</author>
<author confidence="0.998635">Ron</author>
<affiliation confidence="0.99837">USC Institute for Creative</affiliation>
<address confidence="0.947272">13274 Fiji</address>
<author confidence="0.938638">Marina del Rey</author>
<author confidence="0.938638">CA</author>
<email confidence="0.998978">devault@ict.usc.edu</email>
<email confidence="0.998978">traum@ict.usc.edu</email>
<email confidence="0.998978">artstein@ict.usc.edu</email>
<abstract confidence="0.998628055555555">We present a technique that opens up grammar-based generation to a wider range of practical applications by dramatically reducing the development costs and linguistic expertise that are required. Our method infers the grammatical resources needed for generation from a set of declarative examples that link surface expressions directly to the application’s available semantic representations. The same examples further serve to optimize a run-time search strategy that generates the best output that can be found within an application-specific time frame. Our method offers substantially lower development costs than hand-crafted grammars for applicationspecific NLG, while maintaining high output quality and diversity.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Busemann</author>
<author>H Horacek</author>
</authors>
<title>A flexible shallow approach to text generation.</title>
<date>1998</date>
<booktitle>In Proceedings of INLG,</booktitle>
<pages>238--247</pages>
<contexts>
<context position="1622" citStr="Busemann and Horacek, 1998" startWordPosition="224" endWordPosition="227">ration technique designed to reduce the development costs and linguistic expertise needed to integrate a grammar-based generation component into an existing application. We believe this approach will broaden the class of applications in which grammarbased generation may feasibly be deployed. In principle, grammar-based generation offers significant advantages for many applications, when compared with simpler template-based or canned text output solutions, by providing productive coverage and greater output variety. However, realizing these advantages can require significant development costs (Busemann and Horacek, 1998). One possible strategy is to exploit a widecoverage realizer that aims for applicability in multiple application domains (White et al., 2007; Cahill and van Genabith, 2006; Zhong and Stent, 2005; Langkilde-Geary, 2002; Langkilde and Knight, 1998; Elhadad, 1991). These realizers provide a sound wide-coverage grammar (or robust widecoverage language model) for free, but demand a specific input format that is otherwise foreign to an existing application. Unfortunately, the development burden of implementing the translation between the system’s available semantic representations and the required </context>
</contexts>
<marker>Busemann, Horacek, 1998</marker>
<rawString>S. Busemann and H. Horacek. 1998. A flexible shallow approach to text generation. In Proceedings of INLG, pages 238–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Josef van Genabith</author>
</authors>
<title>Robust PCFG-based generation using automatically acquired LFG approximations.</title>
<date>2006</date>
<booktitle>In ACL,</booktitle>
<pages>1033--1040</pages>
<marker>Cahill, van Genabith, 2006</marker>
<rawString>Aoife Cahill and Josef van Genabith. 2006. Robust PCFG-based generation using automatically acquired LFG approximations. In ACL, pages 1033–1040.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C B Callaway</author>
</authors>
<title>Evaluating coverage for large symbolic NLG grammars.</title>
<date>2003</date>
<booktitle>Proceedings of IJCAI.</booktitle>
<contexts>
<context position="2433" citStr="Callaway, 2003" startWordPosition="350" endWordPosition="351"> Langkilde-Geary, 2002; Langkilde and Knight, 1998; Elhadad, 1991). These realizers provide a sound wide-coverage grammar (or robust widecoverage language model) for free, but demand a specific input format that is otherwise foreign to an existing application. Unfortunately, the development burden of implementing the translation between the system’s available semantic representations and the required input format can be quite substantial (Busemann and Horacek, 1998). Indeed, implementing the translation might require as much effort as would be required to build a simple custom generator; cf. (Callaway, 2003). Thus, there currently are many applications where using a widecoverage generator remains impractical. Another strategy is for system builders to hand craft an application-specific grammar for generation. This approach can be initially attractive to system builders because it allows syntactic coverage and semantic modeling to be tailored directly to application needs. However, writing grammatical rules by hand ultimately requires a painstaking, time-consuming effort by a developer who has detailed linguistic knowledge as well as detailed application knowledge. Further, the resulting coverage </context>
</contexts>
<marker>Callaway, 2003</marker>
<rawString>C. B. Callaway. 2003. Evaluating coverage for large symbolic NLG grammars. Proceedings of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Immediate-head parsing for language models. In</title>
<date>2001</date>
<booktitle>ACL,</booktitle>
<pages>124--131</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="8513" citStr="Charniak, 2001" startWordPosition="1281" endWordPosition="1282"> aspect of Doctor Perez’s language processing; e.g., if a programmer discov78 ers a bug or disfluency in the NLG output, it is better if she can fix it directly rather than requiring a (computational) linguist to do so. 3 Technical Approach Our approach builds on recently developed techniques in statistical parsing, lexicalized syntax modeling, generation with lexicalized grammars, and search optimization to automatically construct all the resources needed for a high-quality run-time generation component. In particular, we leverage the increasing availability of off-the-shelf parsers such as (Charniak, 2001; Charniak, 2005) to automatically (or semi-automatically) assign syntactic analyses to a set of suggested output sentences. We then draw on lexicalization techniques for statistical language models (Magerman, 1995; Collins, 1999; Chiang, 2000; Chiang, 2003) to induce a probabilistic, lexicalized tree-adjoining grammar that supports the derivation of all the suggested output sentences, and many others besides. The final step is to use the training examples to learn an effective search policy so that our run-time generation component can find good output sentences in a reasonable time frame. In</context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>E. Charniak. 2001. Immediate-head parsing for language models. In ACL, pages 124–131, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<date>2005</date>
<pages>05--16</pages>
<contexts>
<context position="8530" citStr="Charniak, 2005" startWordPosition="1283" endWordPosition="1284">r Perez’s language processing; e.g., if a programmer discov78 ers a bug or disfluency in the NLG output, it is better if she can fix it directly rather than requiring a (computational) linguist to do so. 3 Technical Approach Our approach builds on recently developed techniques in statistical parsing, lexicalized syntax modeling, generation with lexicalized grammars, and search optimization to automatically construct all the resources needed for a high-quality run-time generation component. In particular, we leverage the increasing availability of off-the-shelf parsers such as (Charniak, 2001; Charniak, 2005) to automatically (or semi-automatically) assign syntactic analyses to a set of suggested output sentences. We then draw on lexicalization techniques for statistical language models (Magerman, 1995; Collins, 1999; Chiang, 2000; Chiang, 2003) to induce a probabilistic, lexicalized tree-adjoining grammar that supports the derivation of all the suggested output sentences, and many others besides. The final step is to use the training examples to learn an effective search policy so that our run-time generation component can find good output sentences in a reasonable time frame. In particular, we u</context>
</contexts>
<marker>Charniak, 2005</marker>
<rawString>E. Charniak. 2005. ftp://ftp.cs.brown.edu/pub/nlparser/ parser05Aug16.tar.gz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Statistical parsing with an automatically-extracted tree adjoining grammar.</title>
<date>2000</date>
<booktitle>In ACL ’00: Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>456--463</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="8756" citStr="Chiang, 2000" startWordPosition="1315" endWordPosition="1316">approach builds on recently developed techniques in statistical parsing, lexicalized syntax modeling, generation with lexicalized grammars, and search optimization to automatically construct all the resources needed for a high-quality run-time generation component. In particular, we leverage the increasing availability of off-the-shelf parsers such as (Charniak, 2001; Charniak, 2005) to automatically (or semi-automatically) assign syntactic analyses to a set of suggested output sentences. We then draw on lexicalization techniques for statistical language models (Magerman, 1995; Collins, 1999; Chiang, 2000; Chiang, 2003) to induce a probabilistic, lexicalized tree-adjoining grammar that supports the derivation of all the suggested output sentences, and many others besides. The final step is to use the training examples to learn an effective search policy so that our run-time generation component can find good output sentences in a reasonable time frame. In particular, we use variants of existing search optimization (Daumé and Marcu, 2005) and ranking algorithms (Collins and Koo, 2005) to train our run-time component to find good outputs within a specified time window; see also (Stent et al., 20</context>
</contexts>
<marker>Chiang, 2000</marker>
<rawString>D. Chiang. 2000. Statistical parsing with an automatically-extracted tree adjoining grammar. In ACL ’00: Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pages 456–463, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Statistical parsing with an automatically extracted tree adjoining grammar. In</title>
<date>2003</date>
<booktitle>Data Oriented Parsing,</booktitle>
<pages>299--316</pages>
<editor>R. Bod, R. Scha, and K. Sima’an, editors,</editor>
<publisher>CSLI Publications, Stanford.</publisher>
<contexts>
<context position="8771" citStr="Chiang, 2003" startWordPosition="1317" endWordPosition="1318">s on recently developed techniques in statistical parsing, lexicalized syntax modeling, generation with lexicalized grammars, and search optimization to automatically construct all the resources needed for a high-quality run-time generation component. In particular, we leverage the increasing availability of off-the-shelf parsers such as (Charniak, 2001; Charniak, 2005) to automatically (or semi-automatically) assign syntactic analyses to a set of suggested output sentences. We then draw on lexicalization techniques for statistical language models (Magerman, 1995; Collins, 1999; Chiang, 2000; Chiang, 2003) to induce a probabilistic, lexicalized tree-adjoining grammar that supports the derivation of all the suggested output sentences, and many others besides. The final step is to use the training examples to learn an effective search policy so that our run-time generation component can find good output sentences in a reasonable time frame. In particular, we use variants of existing search optimization (Daumé and Marcu, 2005) and ranking algorithms (Collins and Koo, 2005) to train our run-time component to find good outputs within a specified time window; see also (Stent et al., 2004; Walker et a</context>
<context position="14002" citStr="Chiang, 2003" startWordPosition="2148" endWordPosition="2149">.....11l. Semantics medical supplies . . here captain ........ I Figure 1: A generation training example for Doctor Perez. If uncorrected syntax is used, the generation content author only writes the utterance and the links to the semantic frame. ical level. An important feature of our approach is that it allows an arbitrary semantic granularity to be employed, by mapping the representations available in the system to appropriate multi-word chunks. 3.2 Automatic Grammar Induction We adopt essentially the probabilistic tree-adjoining grammar (PTAG) formalism and grammar induction technique of (Chiang, 2003). Our approach makes three modifications, however. First, while Chiang’s model includes both full adjunction and sister adjunction operations, our grammar has only sister adjunction (left and right), exactly as in the TAGLET grammar formalism of (Stone, 2002). Second, to support lexicalization at an arbitrary granularity, we allow Chiang’s tree templates to be associated with more than one lexical anchor. Third, to unify syntactic and semantic reasoning in search, we augment lexical anchors with semantic information. Formally, wherever Chiang’s model has a lexical anchor w, ours has a pair ((w</context>
<context position="15477" citStr="Chiang, 2003" startWordPosition="2386" endWordPosition="2387">al anchors but also on the senses of those lexical anchors in application terms. We induce our grammar from training examples such as Figure 1 using heuristics to assign derivations to the examples, exactly as in (Chiang, 2003). The process proceeds in two stages. In the first stage, a collection of rules is used to automatically “decorate” the training syntax with a number of features. These include deciding the lexical anchor(s) for each non-terminal constituent and assigning complement/adjunct status for nonterminals which are not on their parent’s lexicalization path; see (Magerman, 1995; Chiang, 2003; Collins, 1999). In addition, we deterministically add features to improve several grammatical aspects, including (1) enforcing verb inflectional agreement in derived trees, (2) enforcing consistency in the finiteness of VP and S complements, and (3) restricting subject/direct object/indirect object complements to play the same grammatical role in derived trees. In the second stage, the complements and adjuncts in the decorated trees are incrementally re80 fin:other, cat: S cat: SA cat: NP, apr: VBP, apn: other pos: PRP we apn: other, pos:VBP do fin: yes, cat: VP, gra: obj1 syntax: pos: RB n’</context>
</contexts>
<marker>Chiang, 2003</marker>
<rawString>D. Chiang. 2003. Statistical parsing with an automatically extracted tree adjoining grammar. In R. Bod, R. Scha, and K. Sima’an, editors, Data Oriented Parsing, pages 299–316. CSLI Publications, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>T Koo</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="9244" citStr="Collins and Koo, 2005" startWordPosition="1389" endWordPosition="1392">tput sentences. We then draw on lexicalization techniques for statistical language models (Magerman, 1995; Collins, 1999; Chiang, 2000; Chiang, 2003) to induce a probabilistic, lexicalized tree-adjoining grammar that supports the derivation of all the suggested output sentences, and many others besides. The final step is to use the training examples to learn an effective search policy so that our run-time generation component can find good output sentences in a reasonable time frame. In particular, we use variants of existing search optimization (Daumé and Marcu, 2005) and ranking algorithms (Collins and Koo, 2005) to train our run-time component to find good outputs within a specified time window; see also (Stent et al., 2004; Walker et al., 2001). The result is a run-time component that treats generation as an anytime search problem, and is thus suitable for applications in which a time/performance tradeoff is necessary (such as real-time dialogue). 3.1 Specification of Training Examples Each training example in our approach specifies a target output utterance (string), its syntax, and a set of links between substrings within the utterance and system semantic representations. Formally, a training exam</context>
<context position="18983" citStr="Collins and Koo, 2005" startWordPosition="2926" endWordPosition="2929">t rules to automatically define a set of features that could be heuristically useful for a given induced grammar. These include features for various syntactic structures and operations, numbers of undesired and desired meanings of different types added by an expansion, derivation probabilities, etc. (For Doctor Perez, this yields about 600 features.) Our training algorithm is based on the search optimization algorithm of (Daumé and Marcu, 2005), which updates feature weights when mistakes are made during search on training examples. For the weight update step, we use the boosting approach of (Collins and Koo, 2005), which performs feature selection and identifies weight values that improve the ranking of alternative derivation steps when mistakes are made during search. We discuss the resulting success rate and quality in the next section. 4 Cost/Benefit Analysis The motivation that underlies our technical approach is to reduce the development costs and linguistic expertise needed to develop a grammar-based generation component for an existing system. In this section, we assess the progress we have made by ana81 lyzing the use of our approach for Doctor Perez. Method. We began with a sample of 220 insta</context>
</contexts>
<marker>Collins, Koo, 2005</marker>
<rawString>M. Collins and T. Koo. 2005. Discriminative reranking for natural language parsing. Computational Linguistics, 31(1):25–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<institution>University of Pennsylvania.</institution>
<note>Ph.D. dissertation,</note>
<contexts>
<context position="8742" citStr="Collins, 1999" startWordPosition="1313" endWordPosition="1314">l Approach Our approach builds on recently developed techniques in statistical parsing, lexicalized syntax modeling, generation with lexicalized grammars, and search optimization to automatically construct all the resources needed for a high-quality run-time generation component. In particular, we leverage the increasing availability of off-the-shelf parsers such as (Charniak, 2001; Charniak, 2005) to automatically (or semi-automatically) assign syntactic analyses to a set of suggested output sentences. We then draw on lexicalization techniques for statistical language models (Magerman, 1995; Collins, 1999; Chiang, 2000; Chiang, 2003) to induce a probabilistic, lexicalized tree-adjoining grammar that supports the derivation of all the suggested output sentences, and many others besides. The final step is to use the training examples to learn an effective search policy so that our run-time generation component can find good output sentences in a reasonable time frame. In particular, we use variants of existing search optimization (Daumé and Marcu, 2005) and ranking algorithms (Collins and Koo, 2005) to train our run-time component to find good outputs within a specified time window; see also (St</context>
<context position="15493" citStr="Collins, 1999" startWordPosition="2388" endWordPosition="2389"> also on the senses of those lexical anchors in application terms. We induce our grammar from training examples such as Figure 1 using heuristics to assign derivations to the examples, exactly as in (Chiang, 2003). The process proceeds in two stages. In the first stage, a collection of rules is used to automatically “decorate” the training syntax with a number of features. These include deciding the lexical anchor(s) for each non-terminal constituent and assigning complement/adjunct status for nonterminals which are not on their parent’s lexicalization path; see (Magerman, 1995; Chiang, 2003; Collins, 1999). In addition, we deterministically add features to improve several grammatical aspects, including (1) enforcing verb inflectional agreement in derived trees, (2) enforcing consistency in the finiteness of VP and S complements, and (3) restricting subject/direct object/indirect object complements to play the same grammatical role in derived trees. In the second stage, the complements and adjuncts in the decorated trees are incrementally re80 fin:other, cat: S cat: SA cat: NP, apr: VBP, apn: other pos: PRP we apn: other, pos:VBP do fin: yes, cat: VP, gra: obj1 syntax: pos: RB n’t fin: yes, cat:</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. dissertation, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daumé</author>
<author>D Marcu</author>
</authors>
<title>Learning as search optimization: approximate large margin methods for structured prediction.</title>
<date>2005</date>
<booktitle>In ICML ’05: Proceedings of the 22nd international conference on Machine learning,</booktitle>
<pages>169--176</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="9197" citStr="Daumé and Marcu, 2005" startWordPosition="1382" endWordPosition="1385">ign syntactic analyses to a set of suggested output sentences. We then draw on lexicalization techniques for statistical language models (Magerman, 1995; Collins, 1999; Chiang, 2000; Chiang, 2003) to induce a probabilistic, lexicalized tree-adjoining grammar that supports the derivation of all the suggested output sentences, and many others besides. The final step is to use the training examples to learn an effective search policy so that our run-time generation component can find good output sentences in a reasonable time frame. In particular, we use variants of existing search optimization (Daumé and Marcu, 2005) and ranking algorithms (Collins and Koo, 2005) to train our run-time component to find good outputs within a specified time window; see also (Stent et al., 2004; Walker et al., 2001). The result is a run-time component that treats generation as an anytime search problem, and is thus suitable for applications in which a time/performance tradeoff is necessary (such as real-time dialogue). 3.1 Specification of Training Examples Each training example in our approach specifies a target output utterance (string), its syntax, and a set of links between substrings within the utterance and system sema</context>
<context position="18809" citStr="Daumé and Marcu, 2005" startWordPosition="2897" endWordPosition="2900">y search). for every training problem, nodes that lead to good generation output are ranked highly enough by those features to make it into the beam. We use domain-independent rules to automatically define a set of features that could be heuristically useful for a given induced grammar. These include features for various syntactic structures and operations, numbers of undesired and desired meanings of different types added by an expansion, derivation probabilities, etc. (For Doctor Perez, this yields about 600 features.) Our training algorithm is based on the search optimization algorithm of (Daumé and Marcu, 2005), which updates feature weights when mistakes are made during search on training examples. For the weight update step, we use the boosting approach of (Collins and Koo, 2005), which performs feature selection and identifies weight values that improve the ranking of alternative derivation steps when mistakes are made during search. We discuss the resulting success rate and quality in the next section. 4 Cost/Benefit Analysis The motivation that underlies our technical approach is to reduce the development costs and linguistic expertise needed to develop a grammar-based generation component for </context>
</contexts>
<marker>Daumé, Marcu, 2005</marker>
<rawString>H. Daumé and D. Marcu. 2005. Learning as search optimization: approximate large margin methods for structured prediction. In ICML ’05: Proceedings of the 22nd international conference on Machine learning, pages 169–176, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David DeVault</author>
<author>David Traum</author>
<author>Ron Artstein</author>
</authors>
<title>Making grammar-based generation easier to deploy in dialogue systems.</title>
<date>2008</date>
<booktitle>In Ninth SIGdial Workshop on Discourse and Dialogue (SIGdial).</booktitle>
<contexts>
<context position="6034" citStr="DeVault et al., 2008" startWordPosition="900" endWordPosition="903">Virtual humans are embodied conversational agents that play the role of people in simulations or games. The case study we present in this paper is the generation of output utterances for a particular virtual human, Doctor Perez, who is designed to teach negotiation skills in a multi-modal, multi-party, non-team dialogue setting (Traum et al., 2008). The human trainee who talks to the doctor plays the role of a U.S. Army captain named Captain Kirk. The design goals for Doctor Perez create a number of requirements for a practical NLG component. We briefly summarize these requirements here; see (DeVault et al., 2008) for more details. Doctor Perez has a relatively rich internal mental state including beliefs, goals, plans, and emotions. He uses an attribute-value matrix (AVM) semantic representation to describe an utterance as a set of core speech acts and other dialogue acts. Speech acts generally have semantic contents that describe propositions and questions about states and actions in the domain. To facilitate interprocess communication, and statistical processing, this AVM structure is linearized into a “frame” of key values in which each non-recursive terminal value is paired with a path from the ro</context>
<context position="21259" citStr="DeVault et al., 2008" startWordPosition="3298" endWordPosition="3301">erances, in relation to the specific frame for which it was produced, on a single 1 (“very bad”) to 5 (“very good”) scale. The 494 utterances included all of the hand-authored (suggested) utterances in the training examples. They also included all the top-ranked utterances that were successfully generated by the two generators. We asked our judges to make an overall assessment of output quality, incorporating both accuracy and fluency, for the Doctor Perez character. Judges were blind to the conditions under which utterances were produced. We discuss additional details of this rating task in (DeVault et al., 2008). The judges achieved a reliability of α = 0.708 (Krippendorff, 1980); this value shows that agreement is well above chance, and allows for tentative conclusions. We ran a small number of planned comparisons on these ratings. Surprisingly, we found no significant difference between generated output trained on corrected and uncorrected syntax (t(29) = 0.056,p &gt; 0.9 on test items, t(498) = −1.1,p &gt; 0.2 on all items).2 However, we did 2The distribution of ratings across utterances is not normal; to validate our results we accompanied each t-test by a nonparametric Wilcoxon rank sum test, and sign</context>
</contexts>
<marker>DeVault, Traum, Artstein, 2008</marker>
<rawString>David DeVault, David Traum, and Ron Artstein. 2008. Making grammar-based generation easier to deploy in dialogue systems. In Ninth SIGdial Workshop on Discourse and Dialogue (SIGdial).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Elhadad</author>
</authors>
<title>FUF: the universal unifier user manual version 5.0.</title>
<date>1991</date>
<tech>Technical Report CUCS-038-91.</tech>
<contexts>
<context position="1884" citStr="Elhadad, 1991" startWordPosition="266" endWordPosition="267">ibly be deployed. In principle, grammar-based generation offers significant advantages for many applications, when compared with simpler template-based or canned text output solutions, by providing productive coverage and greater output variety. However, realizing these advantages can require significant development costs (Busemann and Horacek, 1998). One possible strategy is to exploit a widecoverage realizer that aims for applicability in multiple application domains (White et al., 2007; Cahill and van Genabith, 2006; Zhong and Stent, 2005; Langkilde-Geary, 2002; Langkilde and Knight, 1998; Elhadad, 1991). These realizers provide a sound wide-coverage grammar (or robust widecoverage language model) for free, but demand a specific input format that is otherwise foreign to an existing application. Unfortunately, the development burden of implementing the translation between the system’s available semantic representations and the required input format can be quite substantial (Busemann and Horacek, 1998). Indeed, implementing the translation might require as much effort as would be required to build a simple custom generator; cf. (Callaway, 2003). Thus, there currently are many applications where</context>
</contexts>
<marker>Elhadad, 1991</marker>
<rawString>M. Elhadad. 1991. FUF: the universal unifier user manual version 5.0. Technical Report CUCS-038-91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology, chapter 12,</title>
<date>1980</date>
<pages>129--154</pages>
<location>Sage, Beverly Hills, CA.</location>
<contexts>
<context position="21328" citStr="Krippendorff, 1980" startWordPosition="3311" endWordPosition="3312">n a single 1 (“very bad”) to 5 (“very good”) scale. The 494 utterances included all of the hand-authored (suggested) utterances in the training examples. They also included all the top-ranked utterances that were successfully generated by the two generators. We asked our judges to make an overall assessment of output quality, incorporating both accuracy and fluency, for the Doctor Perez character. Judges were blind to the conditions under which utterances were produced. We discuss additional details of this rating task in (DeVault et al., 2008). The judges achieved a reliability of α = 0.708 (Krippendorff, 1980); this value shows that agreement is well above chance, and allows for tentative conclusions. We ran a small number of planned comparisons on these ratings. Surprisingly, we found no significant difference between generated output trained on corrected and uncorrected syntax (t(29) = 0.056,p &gt; 0.9 on test items, t(498) = −1.1,p &gt; 0.2 on all items).2 However, we did 2The distribution of ratings across utterances is not normal; to validate our results we accompanied each t-test by a nonparametric Wilcoxon rank sum test, and significance always fell Hand-authored (N = 1099) Generated: Training inp</context>
</contexts>
<marker>Krippendorff, 1980</marker>
<rawString>K. Krippendorff, 1980. Content Analysis: An Introduction to Its Methodology, chapter 12, pages 129–154. Sage, Beverly Hills, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
<author>K Knight</author>
</authors>
<title>Generation that exploits corpus-based statistical knowledge.</title>
<date>1998</date>
<booktitle>In COLING-ACL,</booktitle>
<pages>704--710</pages>
<contexts>
<context position="1868" citStr="Langkilde and Knight, 1998" startWordPosition="262" endWordPosition="265">marbased generation may feasibly be deployed. In principle, grammar-based generation offers significant advantages for many applications, when compared with simpler template-based or canned text output solutions, by providing productive coverage and greater output variety. However, realizing these advantages can require significant development costs (Busemann and Horacek, 1998). One possible strategy is to exploit a widecoverage realizer that aims for applicability in multiple application domains (White et al., 2007; Cahill and van Genabith, 2006; Zhong and Stent, 2005; Langkilde-Geary, 2002; Langkilde and Knight, 1998; Elhadad, 1991). These realizers provide a sound wide-coverage grammar (or robust widecoverage language model) for free, but demand a specific input format that is otherwise foreign to an existing application. Unfortunately, the development burden of implementing the translation between the system’s available semantic representations and the required input format can be quite substantial (Busemann and Horacek, 1998). Indeed, implementing the translation might require as much effort as would be required to build a simple custom generator; cf. (Callaway, 2003). Thus, there currently are many ap</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>I. Langkilde and K. Knight. 1998. Generation that exploits corpus-based statistical knowledge. In COLING-ACL, pages 704–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde-Geary</author>
</authors>
<title>An empirical verification of coverage and correctness for a general-purpose sentence generator.</title>
<date>2002</date>
<contexts>
<context position="1840" citStr="Langkilde-Geary, 2002" startWordPosition="260" endWordPosition="261">lications in which grammarbased generation may feasibly be deployed. In principle, grammar-based generation offers significant advantages for many applications, when compared with simpler template-based or canned text output solutions, by providing productive coverage and greater output variety. However, realizing these advantages can require significant development costs (Busemann and Horacek, 1998). One possible strategy is to exploit a widecoverage realizer that aims for applicability in multiple application domains (White et al., 2007; Cahill and van Genabith, 2006; Zhong and Stent, 2005; Langkilde-Geary, 2002; Langkilde and Knight, 1998; Elhadad, 1991). These realizers provide a sound wide-coverage grammar (or robust widecoverage language model) for free, but demand a specific input format that is otherwise foreign to an existing application. Unfortunately, the development burden of implementing the translation between the system’s available semantic representations and the required input format can be quite substantial (Busemann and Horacek, 1998). Indeed, implementing the translation might require as much effort as would be required to build a simple custom generator; cf. (Callaway, 2003). Thus,</context>
</contexts>
<marker>Langkilde-Geary, 2002</marker>
<rawString>I. Langkilde-Geary. 2002. An empirical verification of coverage and correctness for a general-purpose sentence generator.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Magerman</author>
</authors>
<title>Statistical decision-tree models for parsing.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd annual meeting on Association for Computational Linguistics,</booktitle>
<pages>276--283</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="8727" citStr="Magerman, 1995" startWordPosition="1311" endWordPosition="1312">o so. 3 Technical Approach Our approach builds on recently developed techniques in statistical parsing, lexicalized syntax modeling, generation with lexicalized grammars, and search optimization to automatically construct all the resources needed for a high-quality run-time generation component. In particular, we leverage the increasing availability of off-the-shelf parsers such as (Charniak, 2001; Charniak, 2005) to automatically (or semi-automatically) assign syntactic analyses to a set of suggested output sentences. We then draw on lexicalization techniques for statistical language models (Magerman, 1995; Collins, 1999; Chiang, 2000; Chiang, 2003) to induce a probabilistic, lexicalized tree-adjoining grammar that supports the derivation of all the suggested output sentences, and many others besides. The final step is to use the training examples to learn an effective search policy so that our run-time generation component can find good output sentences in a reasonable time frame. In particular, we use variants of existing search optimization (Daumé and Marcu, 2005) and ranking algorithms (Collins and Koo, 2005) to train our run-time component to find good outputs within a specified time windo</context>
<context position="15463" citStr="Magerman, 1995" startWordPosition="2384" endWordPosition="2385">ctures and lexical anchors but also on the senses of those lexical anchors in application terms. We induce our grammar from training examples such as Figure 1 using heuristics to assign derivations to the examples, exactly as in (Chiang, 2003). The process proceeds in two stages. In the first stage, a collection of rules is used to automatically “decorate” the training syntax with a number of features. These include deciding the lexical anchor(s) for each non-terminal constituent and assigning complement/adjunct status for nonterminals which are not on their parent’s lexicalization path; see (Magerman, 1995; Chiang, 2003; Collins, 1999). In addition, we deterministically add features to improve several grammatical aspects, including (1) enforcing verb inflectional agreement in derived trees, (2) enforcing consistency in the finiteness of VP and S complements, and (3) restricting subject/direct object/indirect object complements to play the same grammatical role in derived trees. In the second stage, the complements and adjuncts in the decorated trees are incrementally re80 fin:other, cat: S cat: SA cat: NP, apr: VBP, apn: other pos: PRP we apn: other, pos:VBP do fin: yes, cat: VP, gra: obj1 synt</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>D. M. Magerman. 1995. Statistical decision-tree models for parsing. In Proceedings of the 33rd annual meeting on Association for Computational Linguistics, pages 276–283, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="10202" citStr="Marcus et al., 1994" startWordPosition="1542" endWordPosition="1545">gue). 3.1 Specification of Training Examples Each training example in our approach specifies a target output utterance (string), its syntax, and a set of links between substrings within the utterance and system semantic representations. Formally, a training example takes the form (u, syntax(u), semantics(u)). We will illustrate this format using the training example in Figure 1. In this example, the generation content author suggests the output utterance u = we don’t have medical supplies here captain. Each utterance u is accompanied by syntax(u), a syntactic analysis in Penn Treebank format (Marcus et al., 1994). In the figure, we show two alternative syntactic analyses that might be specified: one is the uncorrected output of the Charniak parser on this sentence, and the other a hand-corrected version of that parse; we evaluate the utility of this hand correction in Section 4. To represent the meaning of utterances, our approach assumes that the system provides some set M = {m1, ..., mj} of semantic representations. The meaning of any individual utterance is then identified with some subset of M. For Doctor Perez, M comprises the 232 distinct key-value pairs that appear in the system’s various gener</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1994. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stent</author>
<author>R Prasad</author>
<author>M Walker</author>
</authors>
<title>Trainable sentence planning for complex information presentation in spoken dialog systems.</title>
<date>2004</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="9358" citStr="Stent et al., 2004" startWordPosition="1409" endWordPosition="1412">99; Chiang, 2000; Chiang, 2003) to induce a probabilistic, lexicalized tree-adjoining grammar that supports the derivation of all the suggested output sentences, and many others besides. The final step is to use the training examples to learn an effective search policy so that our run-time generation component can find good output sentences in a reasonable time frame. In particular, we use variants of existing search optimization (Daumé and Marcu, 2005) and ranking algorithms (Collins and Koo, 2005) to train our run-time component to find good outputs within a specified time window; see also (Stent et al., 2004; Walker et al., 2001). The result is a run-time component that treats generation as an anytime search problem, and is thus suitable for applications in which a time/performance tradeoff is necessary (such as real-time dialogue). 3.1 Specification of Training Examples Each training example in our approach specifies a target output utterance (string), its syntax, and a set of links between substrings within the utterance and system semantic representations. Formally, a training example takes the form (u, syntax(u), semantics(u)). We will illustrate this format using the training example in Figu</context>
</contexts>
<marker>Stent, Prasad, Walker, 2004</marker>
<rawString>A. Stent, R. Prasad, and M. Walker. 2004. Trainable sentence planning for complex information presentation in spoken dialog systems. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Stone</author>
<author>Doug DeCarlo</author>
<author>Insuk Oh</author>
<author>Christian Rodriguez</author>
<author>Adrian Stere</author>
<author>Alyssa Lees</author>
<author>Chris Bregler</author>
</authors>
<title>Speaking with hands: creating animated conversational characters from recordings of human performance.</title>
<date>2004</date>
<journal>ACM Trans. Graph.,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="3988" citStr="Stone et al., 2004" startWordPosition="580" endWordPosition="583">s specified by example. Example-based approaches aim to allow system builders to specify a productive generation capacity while leaving the representations and reasoning that underlie that productive capacity mostly implicit in a set of training examples. This methodology insulates system builders from the detailed expertise and technical infrastructure needed to implement the productive capacity directly, and has made example-based approaches attractive not only in text generation but also in related areas such as concatenative speech synthesis and motion capture based animation; see, e.g., (Stone et al., 2004). The technique we present in this paper is a new example-based approach to specifying applicationspecific text generation. As in other hand-crafted and example-based approaches, our technique allows syntactic coverage and semantic modeling to follow the needs and available semantic representations in an application. One contribution of our technique is to relieve the generation content author of the burden of manual syntactic modeling by leveraging an off-the-shelf parser; defects in the syntax provided by the parser are effectively overcome using a machine learning technique. Additionally, o</context>
</contexts>
<marker>Stone, DeCarlo, Oh, Rodriguez, Stere, Lees, Bregler, 2004</marker>
<rawString>Matthew Stone, Doug DeCarlo, Insuk Oh, Christian Rodriguez, Adrian Stere, Alyssa Lees, and Chris Bregler. 2004. Speaking with hands: creating animated conversational characters from recordings of human performance. ACM Trans. Graph., 23(3):506–513.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stone</author>
</authors>
<title>Lexicalized grammar 101.</title>
<date>2002</date>
<booktitle>In ACL Workshop on Tools and Methodologies for Teaching Natural Language Processing.</booktitle>
<contexts>
<context position="14261" citStr="Stone, 2002" startWordPosition="2185" endWordPosition="2186">n important feature of our approach is that it allows an arbitrary semantic granularity to be employed, by mapping the representations available in the system to appropriate multi-word chunks. 3.2 Automatic Grammar Induction We adopt essentially the probabilistic tree-adjoining grammar (PTAG) formalism and grammar induction technique of (Chiang, 2003). Our approach makes three modifications, however. First, while Chiang’s model includes both full adjunction and sister adjunction operations, our grammar has only sister adjunction (left and right), exactly as in the TAGLET grammar formalism of (Stone, 2002). Second, to support lexicalization at an arbitrary granularity, we allow Chiang’s tree templates to be associated with more than one lexical anchor. Third, to unify syntactic and semantic reasoning in search, we augment lexical anchors with semantic information. Formally, wherever Chiang’s model has a lexical anchor w, ours has a pair ((wi, ..., w,,,), M′), where M′ ⊆ M is connected to lexical anchors (wi, ..., w,,,) by the generation content author, as in Figure 1. The result is that the derivation probabilities the grammar assigns depend not only on the implicated syntactic structures and l</context>
</contexts>
<marker>Stone, 2002</marker>
<rawString>M. Stone. 2002. Lexicalized grammar 101. In ACL Workshop on Tools and Methodologies for Teaching Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Stone</author>
</authors>
<title>Specifying generation of referring expressions by example.</title>
<date>2003</date>
<booktitle>In AAAI Spring Symposium on Natural Language Generation in Spoken and Written Dialogue,</booktitle>
<pages>133--140</pages>
<contexts>
<context position="3228" citStr="Stone, 2003" startWordPosition="471" endWordPosition="472">ammar for generation. This approach can be initially attractive to system builders because it allows syntactic coverage and semantic modeling to be tailored directly to application needs. However, writing grammatical rules by hand ultimately requires a painstaking, time-consuming effort by a developer who has detailed linguistic knowledge as well as detailed application knowledge. Further, the resulting coverage is inevitably limited to the set of linguistic constructions that have been selected for careful modeling. A third strategy is to use an example-based approach (Wong and Mooney, 2007; Stone, 2003; Varges and Mellish, 2001) in which the connection 77 between available application semantic representations and desired output utterances is specified by example. Example-based approaches aim to allow system builders to specify a productive generation capacity while leaving the representations and reasoning that underlie that productive capacity mostly implicit in a set of training examples. This methodology insulates system builders from the detailed expertise and technical infrastructure needed to implement the productive capacity directly, and has made example-based approaches attractive </context>
</contexts>
<marker>Stone, 2003</marker>
<rawString>Matthew Stone. 2003. Specifying generation of referring expressions by example. In AAAI Spring Symposium on Natural Language Generation in Spoken and Written Dialogue, pages 133–140.</rawString>
</citation>
<citation valid="false">
<authors>
<author>W Swartout</author>
<author>J Gratch</author>
<author>R W Hill</author>
<author>E Hovy</author>
<author>S Marsella</author>
</authors>
<marker>Swartout, Gratch, Hill, Hovy, Marsella, </marker>
<rawString>W. Swartout, J. Gratch, R. W. Hill, E. Hovy, S. Marsella,</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rickel</author>
<author>D Traum</author>
</authors>
<title>Toward virtual humans.</title>
<date>2006</date>
<journal>AI Mag.,</journal>
<volume>27</volume>
<issue>2</issue>
<marker>Rickel, Traum, 2006</marker>
<rawString>J. Rickel, and D. Traum. 2006. Toward virtual humans. AI Mag., 27(2):96–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Traum</author>
<author>W Swartout</author>
<author>J Gratch</author>
<author>S Marsella</author>
</authors>
<title>A virtual human dialogue model for non-team interaction.</title>
<date>2008</date>
<booktitle>Recent Trends in Discourse and Dialogue.</booktitle>
<editor>In L. Dybkjaer and W. Minker, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="5763" citStr="Traum et al., 2008" startWordPosition="852" endWordPosition="855"> coverage at an expense of less than one minute per additional lexical entry. 2 Case Study: Doctor Perez Our approach has been tested as a replacement for the generation component of interactive virtual humans used for social training purposes (Swartout et al., 2006). Virtual humans are embodied conversational agents that play the role of people in simulations or games. The case study we present in this paper is the generation of output utterances for a particular virtual human, Doctor Perez, who is designed to teach negotiation skills in a multi-modal, multi-party, non-team dialogue setting (Traum et al., 2008). The human trainee who talks to the doctor plays the role of a U.S. Army captain named Captain Kirk. The design goals for Doctor Perez create a number of requirements for a practical NLG component. We briefly summarize these requirements here; see (DeVault et al., 2008) for more details. Doctor Perez has a relatively rich internal mental state including beliefs, goals, plans, and emotions. He uses an attribute-value matrix (AVM) semantic representation to describe an utterance as a set of core speech acts and other dialogue acts. Speech acts generally have semantic contents that describe prop</context>
</contexts>
<marker>Traum, Swartout, Gratch, Marsella, 2008</marker>
<rawString>D. R. Traum, W. Swartout, J. Gratch, and S. Marsella. 2008. A virtual human dialogue model for non-team interaction. In L. Dybkjaer and W. Minker, editors, Recent Trends in Discourse and Dialogue. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Traum</author>
</authors>
<title>Semantics and pragmatics of questions and answers for dialogue agents.</title>
<date>2003</date>
<booktitle>In proceedings of the International Workshop on Computational Semantics,</booktitle>
<pages>380--394</pages>
<contexts>
<context position="6710" citStr="Traum, 2003" startWordPosition="1007" endWordPosition="1008">ental state including beliefs, goals, plans, and emotions. He uses an attribute-value matrix (AVM) semantic representation to describe an utterance as a set of core speech acts and other dialogue acts. Speech acts generally have semantic contents that describe propositions and questions about states and actions in the domain. To facilitate interprocess communication, and statistical processing, this AVM structure is linearized into a “frame” of key values in which each non-recursive terminal value is paired with a path from the root to the final attribute. Figure 1 shows a typical frame. See (Traum, 2003) for additional details and examples of this representation. While only hundreds of frames currently arise in actual dialogues, the number of potential frames is orders of magnitude larger, and it is difficult to predict in advance which frames might occur. The utterances that realize these frames need to take a variety of syntactic forms, including simple declarative sentences, various modal constructions relating to hypothetical actions or plans, yes/no and whquestions, and abbreviated dialogue forms such as elliptical clarification and repair requests, grounding, and turn-taking utterances.</context>
</contexts>
<marker>Traum, 2003</marker>
<rawString>D. Traum. 2003. Semantics and pragmatics of questions and answers for dialogue agents. In proceedings of the International Workshop on Computational Semantics, pages 380–394, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Varges</author>
<author>Chris Mellish</author>
</authors>
<title>Instancebased natural language generation.</title>
<date>2001</date>
<booktitle>In NAACL,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="3255" citStr="Varges and Mellish, 2001" startWordPosition="473" endWordPosition="476">eration. This approach can be initially attractive to system builders because it allows syntactic coverage and semantic modeling to be tailored directly to application needs. However, writing grammatical rules by hand ultimately requires a painstaking, time-consuming effort by a developer who has detailed linguistic knowledge as well as detailed application knowledge. Further, the resulting coverage is inevitably limited to the set of linguistic constructions that have been selected for careful modeling. A third strategy is to use an example-based approach (Wong and Mooney, 2007; Stone, 2003; Varges and Mellish, 2001) in which the connection 77 between available application semantic representations and desired output utterances is specified by example. Example-based approaches aim to allow system builders to specify a productive generation capacity while leaving the representations and reasoning that underlie that productive capacity mostly implicit in a set of training examples. This methodology insulates system builders from the detailed expertise and technical infrastructure needed to implement the productive capacity directly, and has made example-based approaches attractive not only in text generation</context>
<context position="29870" citStr="Varges and Mellish, 2001" startWordPosition="4663" endWordPosition="4666">ple, that our approach cannot generate referring expressions (by selecting disambiguating semantic properties); rather, all semantic properties must be pre-selected and included in the generation request. Generation of anaphoric expressions is also limited, since contextual ambiguities are not considered. 6 Related Work To our knowledge, this is the first implemented generation technique that does all three of the following: directly interfaces to existing application semantic representations, infers a phrase structure grammar from examples, and does not require handauthored syntax as input. (Varges and Mellish, 2001) also aims to reduce the authoring burden of domain-specific generation; however, they seem to use a special purpose semantic annotation rather than pre-existing application semantics, and their task is defined in terms of the Penn Treebank, so hand-authored syntax is used as input. (Wong and Mooney, 2007) also interfaces to existing application semantics, and does not require hand-authored syntax as input. Their technique infers a synchronous grammar in which the hierarchical linguistic analysis is isomorphic to the hierarchy in the application semantics, and differs from phrase structure. It</context>
</contexts>
<marker>Varges, Mellish, 2001</marker>
<rawString>Sebastian Varges and Chris Mellish. 2001. Instancebased natural language generation. In NAACL, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>O Rambow</author>
<author>M Rogati</author>
</authors>
<title>Spot: A trainable sentence planner.</title>
<date>2001</date>
<booktitle>In Proceedings of the North American Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9380" citStr="Walker et al., 2001" startWordPosition="1413" endWordPosition="1416">iang, 2003) to induce a probabilistic, lexicalized tree-adjoining grammar that supports the derivation of all the suggested output sentences, and many others besides. The final step is to use the training examples to learn an effective search policy so that our run-time generation component can find good output sentences in a reasonable time frame. In particular, we use variants of existing search optimization (Daumé and Marcu, 2005) and ranking algorithms (Collins and Koo, 2005) to train our run-time component to find good outputs within a specified time window; see also (Stent et al., 2004; Walker et al., 2001). The result is a run-time component that treats generation as an anytime search problem, and is thus suitable for applications in which a time/performance tradeoff is necessary (such as real-time dialogue). 3.1 Specification of Training Examples Each training example in our approach specifies a target output utterance (string), its syntax, and a set of links between substrings within the utterance and system semantic representations. Formally, a training example takes the form (u, syntax(u), semantics(u)). We will illustrate this format using the training example in Figure 1. In this example,</context>
</contexts>
<marker>Walker, Rambow, Rogati, 2001</marker>
<rawString>M. Walker, O. Rambow, and M. Rogati. 2001. Spot: A trainable sentence planner. In Proceedings of the North American Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M White</author>
<author>R Rajkumar</author>
<author>S Martin</author>
</authors>
<title>Towards broad coverage surface realization with CCG.</title>
<date>2007</date>
<booktitle>In Proc. of the Workshop on Using Corpora for NLG: Language Generation and Machine Translation (UCNLG+MT).</booktitle>
<contexts>
<context position="1763" citStr="White et al., 2007" startWordPosition="247" endWordPosition="250">isting application. We believe this approach will broaden the class of applications in which grammarbased generation may feasibly be deployed. In principle, grammar-based generation offers significant advantages for many applications, when compared with simpler template-based or canned text output solutions, by providing productive coverage and greater output variety. However, realizing these advantages can require significant development costs (Busemann and Horacek, 1998). One possible strategy is to exploit a widecoverage realizer that aims for applicability in multiple application domains (White et al., 2007; Cahill and van Genabith, 2006; Zhong and Stent, 2005; Langkilde-Geary, 2002; Langkilde and Knight, 1998; Elhadad, 1991). These realizers provide a sound wide-coverage grammar (or robust widecoverage language model) for free, but demand a specific input format that is otherwise foreign to an existing application. Unfortunately, the development burden of implementing the translation between the system’s available semantic representations and the required input format can be quite substantial (Busemann and Horacek, 1998). Indeed, implementing the translation might require as much effort as woul</context>
</contexts>
<marker>White, Rajkumar, Martin, 2007</marker>
<rawString>M. White, R. Rajkumar, and S. Martin. 2007. Towards broad coverage surface realization with CCG. In Proc. of the Workshop on Using Corpora for NLG: Language Generation and Machine Translation (UCNLG+MT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond Mooney</author>
</authors>
<title>Generation by inverting a semantic parser that uses statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>172--179</pages>
<contexts>
<context position="3215" citStr="Wong and Mooney, 2007" startWordPosition="467" endWordPosition="470">application-specific grammar for generation. This approach can be initially attractive to system builders because it allows syntactic coverage and semantic modeling to be tailored directly to application needs. However, writing grammatical rules by hand ultimately requires a painstaking, time-consuming effort by a developer who has detailed linguistic knowledge as well as detailed application knowledge. Further, the resulting coverage is inevitably limited to the set of linguistic constructions that have been selected for careful modeling. A third strategy is to use an example-based approach (Wong and Mooney, 2007; Stone, 2003; Varges and Mellish, 2001) in which the connection 77 between available application semantic representations and desired output utterances is specified by example. Example-based approaches aim to allow system builders to specify a productive generation capacity while leaving the representations and reasoning that underlie that productive capacity mostly implicit in a set of training examples. This methodology insulates system builders from the detailed expertise and technical infrastructure needed to implement the productive capacity directly, and has made example-based approache</context>
<context position="30177" citStr="Wong and Mooney, 2007" startWordPosition="4710" endWordPosition="4713">elated Work To our knowledge, this is the first implemented generation technique that does all three of the following: directly interfaces to existing application semantic representations, infers a phrase structure grammar from examples, and does not require handauthored syntax as input. (Varges and Mellish, 2001) also aims to reduce the authoring burden of domain-specific generation; however, they seem to use a special purpose semantic annotation rather than pre-existing application semantics, and their task is defined in terms of the Penn Treebank, so hand-authored syntax is used as input. (Wong and Mooney, 2007) also interfaces to existing application semantics, and does not require hand-authored syntax as input. Their technique infers a synchronous grammar in which the hierarchical linguistic analysis is isomorphic to the hierarchy in the application semantics, and differs from phrase structure. It would be interesting to compare their output quality with ours; their automated alignment of words to semantics might also provide a way to further reduce the authoring burden of our approach. 7 Conclusion and Future Work We have presented a new example-based approach to specifying text generation for an </context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Yuk Wah Wong and Raymond Mooney. 2007. Generation by inverting a semantic parser that uses statistical machine translation. In Proceedings of NAACL-HLT, pages 172–179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhong</author>
<author>A Stent</author>
</authors>
<title>Building surface realizers automatically from corpora using general-purpose tools.</title>
<date>2005</date>
<booktitle>In Proc. Corpus Linguistics ’05 Workshop on Using Corpora for Natural Language Generation.</booktitle>
<contexts>
<context position="1817" citStr="Zhong and Stent, 2005" startWordPosition="256" endWordPosition="259">roaden the class of applications in which grammarbased generation may feasibly be deployed. In principle, grammar-based generation offers significant advantages for many applications, when compared with simpler template-based or canned text output solutions, by providing productive coverage and greater output variety. However, realizing these advantages can require significant development costs (Busemann and Horacek, 1998). One possible strategy is to exploit a widecoverage realizer that aims for applicability in multiple application domains (White et al., 2007; Cahill and van Genabith, 2006; Zhong and Stent, 2005; Langkilde-Geary, 2002; Langkilde and Knight, 1998; Elhadad, 1991). These realizers provide a sound wide-coverage grammar (or robust widecoverage language model) for free, but demand a specific input format that is otherwise foreign to an existing application. Unfortunately, the development burden of implementing the translation between the system’s available semantic representations and the required input format can be quite substantial (Busemann and Horacek, 1998). Indeed, implementing the translation might require as much effort as would be required to build a simple custom generator; cf. </context>
</contexts>
<marker>Zhong, Stent, 2005</marker>
<rawString>H. Zhong and A. Stent. 2005. Building surface realizers automatically from corpora using general-purpose tools. In Proc. Corpus Linguistics ’05 Workshop on Using Corpora for Natural Language Generation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>