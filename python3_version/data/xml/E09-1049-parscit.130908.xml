<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000167">
<title confidence="0.9958635">
N-gram-based Statistical Machine Translation versus Syntax Augmented
Machine Translation: comparison and system combination
</title>
<author confidence="0.851535">
Maxim Khalilov and José A.R. Fonollosa
</author>
<affiliation confidence="0.541888">
Universitat Politècnica de Catalunya
</affiliation>
<address confidence="0.9669455">
Campus Nord UPC, 08034
Barcelona, Spain
</address>
<email confidence="0.999507">
{khalilov,adrian}@talp.upc.edu
</email>
<sectionHeader confidence="0.99396" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99948236">
In this paper we compare and contrast
two approaches to Machine Translation
(MT): the CMU-UKA Syntax Augmented
Machine Translation system (SAMT) and
UPC-TALP N-gram-based Statistical Ma-
chine Translation (SMT). SAMT is a hier-
archical syntax-driven translation system
underlain by a phrase-based model and a
target part parse tree. In N-gram-based
SMT, the translation process is based on
bilingual units related to word-to-word
alignment and statistical modeling of the
bilingual context following a maximum-
entropy framework. We provide a step-
by-step comparison of the systems and re-
port results in terms of automatic evalu-
ation metrics and required computational
resources for a smaller Arabic-to-English
translation task (1.5M tokens in the train-
ing corpus). Human error analysis clari-
fies advantages and disadvantages of the
systems under consideration. Finally, we
combine the output of both systems to
yield significant improvements in transla-
tion quality.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999014818181818">
There is an ongoing controversy regarding
whether or not information about the syntax of
language can benefit MT or contribute to a hybrid
system.
Classical IBM word-based models were re-
cently augmented with a phrase translation ca-
pability, as shown in Koehn et al. (2003), or in
more recent implementation, the MOSES MT sys-
tem1 (Koehn et al., 2007). In parallel to the phrase-
based approach, the N-gram-based approach ap-
peared (Mariño et al., 2006). It stemms from
</bodyText>
<footnote confidence="0.845583">
1www.statmt.org/moses/
</footnote>
<bodyText confidence="0.999821666666667">
the Finite-State Transducers paradigm, and is ex-
tended to the log-linear modeling framework, as
shown in (Mariño et al., 2006). A system follow-
ing this approach deals with bilingual units, called
tuples, which are composed of one or more words
from the source language and zero or more words
from the target one. The N-gram-based systems
allow for linguistically motivated word reordering
by implementing word order monotonization.
Prior to the SMT revolution, a major part
of MT systems was developed using rule-based
algorithms; however, starting from the 1990’s,
syntax-driven systems based on phrase hierar-
chy have gained popularity. A representative
sample of modern syntax-based systems includes
models based on bilingual synchronous grammar
(Melamed, 2004), parse tree-to-string translation
models (Yamada and Knight, 2001) and non-
isomorphic tree-to-tree mappings (Eisner, 2003).
The orthodox phrase-based model was en-
hanced in Chiang (2005), where a hierarchical
phrase model allowing for multiple generaliza-
tions within each phrase was introduced. The
open-source toolkit SAMT2 (Zollmann and Venu-
gopal, 2006) is a further evolution of this ap-
proach, in which syntactic categories extracted
from the target side parse tree are directly assigned
to the hierarchically structured phrases.
Several publications discovering similarities
and differences between distinct translation mod-
els have been written over the last few years. In
Crego et al. (2005b), the N-gram-based system
is contrasted with a state-of-the-art phrase-based
framework, while in DeNeefe et al. (2007), the
authors seek to estimate the advantages, weak-
est points and possible overlap between syntax-
based MT and phrase-based SMT. In Zollmann et
al. (2008) the comparison of phrase-based, &amp;quot;Chi-
ang’s style&amp;quot; hirearchical system and SAMT is pro-
</bodyText>
<footnote confidence="0.971196">
2www.cs.cmu.edu/∼zollmann/samt
</footnote>
<note confidence="0.9618375">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 424–432,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.998303">
424
</page>
<bodyText confidence="0.996833333333333">
vided.
In this study, we intend to compare the differ-
ences and similarities of the statistical N-gram-
based SMT approach and the SAMT system. The
comparison is performed on a small Arabic-to-
English translation task from the news domain.
</bodyText>
<sectionHeader confidence="0.926827" genericHeader="method">
2 SAMT system
</sectionHeader>
<bodyText confidence="0.999969333333333">
A criticism of phrase-based models is data sparse-
ness. This problem is even more serious when the
source, the target, or both languages are inflec-
tional and rich in morphology. Moreover, phrase-
based models are unable to cope with global re-
ordering because the distortion model is based
on movement distance, which may face computa-
tional resource limitations (Och and Ney, 2004).
This problem was successfully addressed when
the MT system based on generalized hierarchi-
cally structured phrases was introduced and dis-
cussed in Chiang (2005). It operates with only two
markers (a substantial phrase category and &amp;quot;a glue
marker&amp;quot;). Moreover, a recent work (Zollmann and
Venugopal, 2006) reports significant improvement
in terms of translation quality if complete or par-
tial syntactic categories (derived from the target
side parse tree) are assigned to the phrases.
</bodyText>
<subsectionHeader confidence="0.994033">
2.1 Modeling
</subsectionHeader>
<bodyText confidence="0.9959765">
A formalism for Syntax Augmented Translation
is probabilistic synchronous context-free grammar
(PSynCFG), which is defined in terms of source
and target terminal sets and a set of non-terminals:
X ) (-y, α, —, w)
where X is a non-terminal, -y is a sequence of
source-side terminals and non-terminals, α is a se-
quence of target-side terminals and non-terminals,
— is a one-to-one mapping from non-terminal to-
kens space in -y to non-terminal space in α, and w
is a non-negative weight assigned to the rule.
The non-terminal set is generated from the syn-
tactic categories corresponding to the target-side
Penn Treebank set, a set of glue rules and a spe-
cial marker representing the &amp;quot;Chiang-style&amp;quot; rules,
which do not span the parse tree. Consequently, all
lexical mapping rules are covered by the phrases
mapping table.
</bodyText>
<subsectionHeader confidence="0.9206335">
2.2 Rules annotation, generalization and
pruning
</subsectionHeader>
<bodyText confidence="0.99984932">
The SAMT system is based on a purely lexi-
cal phrase table, which is identified as shown in
Koehn et al. (2003), and word alignment, which is
generated by the grow-diag-final-and method (ex-
panding the alignment by adding directly neigh-
boring alignment points and alignment points in
the diagonal neighborhood) (Och and Ney, 2003).
Meanwhile, the target of the training corpus is
parsed with Charniak’s parser (Charniak, 2000),
and each phrase is annotated with the constituent
that spans the target side of the rules. The set of
non-terminals is extended by means of conditional
and additive categories according to Combinatory
Categorical Grammar (CCG) (Steedman, 1999).
Under this approach, new rules can be formed. For
example, RB+VB, can represent an additive con-
stituent consisting of two synthetically generated
adjacent categories 3, i.e., an adverb and a verb.
Furthermore, DT�NP can indicate an incomplete
noun phrase with a missing determiner to the left.
The rule recursive generalization procedure co-
incides with the one proposed in Chiang (2005),
but violates the restrictions introduced for single-
category grammar; for example, rules that contain
adjacent generalized elements are not discarded.
</bodyText>
<equation confidence="0.8498815">
Thus, each rule
N ) f1 ... fm/e1 ... en
can be extended by another existing rule
M ) fi ... fu/ej ... ev
</equation>
<bodyText confidence="0.9999285">
where 1 &lt; i &lt; u &lt; m and 1 &lt; j &lt; v &lt; n, to
obtain a new rule
</bodyText>
<equation confidence="0.9245985">
N ) f1 ... fi−1Mkfu+1 ... fm/
e1 ... ej−1Mkev+1 ... en
</equation>
<bodyText confidence="0.999547">
where k is an index for the non-terminal M that in-
dicates a one-to-one correspondence between the
new M tokens on the two sides.
Figure 1 shows an example of initial rules ex-
traction, which can be further extended using the
hierarchical model, as shown in Figure 2 (conse-
quently involving more general elements in rule
description).
Rules pruning is necessary because the set of
generalized rules can be huge. Pruning is per-
formed according to the relative frequency and
the nature of the rules: non-lexical rules that
have been seen only once are discarded; source-
conditioned rules with a relative frequency of ap-
pearance below a threshold are also eliminated.
</bodyText>
<footnote confidence="0.99694475">
3Adjacent generalized elements are not allowed in Chi-
ang’s work because of over-generation. However, over-
generation is not an issue within the SAMT framework due
to restrictions introduced by target-side syntax
</footnote>
<page confidence="0.99891">
425
</page>
<bodyText confidence="0.9463">
Rules that do not contain non-terminals are not
pruned.
</bodyText>
<subsectionHeader confidence="0.998779">
2.3 Decoding and feature functions
</subsectionHeader>
<bodyText confidence="0.999717">
The decoding process is accomplished using a top-
down log-linear model. The source sentence is de-
coded and enriched with the PSynCFG in such a
way that translation quality is represented by a set
of feature functions for each rule, i.e.:
</bodyText>
<listItem confidence="0.977706444444445">
• rule conditional probabilities, given a source,
a target or a left-hand-side category;
• lexical weights features, as described in
Koehn et al. (2003);
• counters of target words and rule applica-
tions;
• binary features reflecting rule context (purely
lexical and purely abstract, among others);
• rule rareness and unbalancedness penalties.
</listItem>
<bodyText confidence="0.9999408">
The decoding process can be represented as
a search through the space of neg log probabil-
ity of the target language terminals. The set of
feature functions is combined with a finite-state
target-side n-gram language model (LM), which
is used to derive the target language sequence dur-
ing a parsing decoding. The feature weights are
optimized according to the highest BLEU score.
For more details refer to Zollmann and Venu-
gopal (2006).
</bodyText>
<sectionHeader confidence="0.971365" genericHeader="method">
3 UPC n-gram SMT system
</sectionHeader>
<bodyText confidence="0.998328777777778">
A description of the UPC-TALP N-gram transla-
tion system can be found in Mariño et al. (2006).
SMT is based on the principle of translating a
source sentence (f) into a sentence in the target
language (e). The problem is formulated in terms
of source and target languages; it is defined ac-
cording to equation (1) and can be reformulated as
selecting a translation with the highest probability
from a set of target sentences (2):
</bodyText>
<figureCaption confidence="0.999992">
Figure 1: Example of SAMT and N-gram elements extraction.
Figure 2: Example of SAMT generalized rules.
</figureCaption>
<page confidence="0.946403">
426
</page>
<equation confidence="0.999296714285714">
eI1 = arg
max {1
p(eI1  |fJ1)
eI
1
{ 1
p(fJ 1  |eI 1) � p(eI 1) (2)
</equation>
<listItem confidence="0.99718625">
• a target LM of Part-of-Speech tags;
• a word penalty model that is used to compen-
sate for the system’s preference for short out-
put sentences;
</listItem>
<equation confidence="0.862313">
= arg max
el
1
</equation>
<bodyText confidence="0.999954818181818">
where I and J represent the number of words in
the target and source languages, respectively.
Modern state-of-the-art SMT systems operate
with the bilingual units extracted from the parallel
corpus based on word-to-word alignment. They
are enhanced by the maximum entropy approach
and the posterior probability is calculated as a log-
linear combination of a set of feature functions
(Och and Ney, 2002). Using this technique, the
additional models are combined to determine the
translation hypothesis, as shown in (3):
</bodyText>
<equation confidence="0.9973255">
I
EM �
�eI 1 = arg max amhm(eI 1, fJ 1 ) (3)
eI1 m=1
</equation>
<bodyText confidence="0.999967">
where the feature functions hm refer to the system
models and the set of am refers to the weights cor-
responding to these models.
</bodyText>
<subsectionHeader confidence="0.999656">
3.1 N-gram-based translation system
</subsectionHeader>
<bodyText confidence="0.999997333333333">
The N-gram approach to SMT is considered to
be an alternative to the phrase-based translation,
where a given source word sequence is decom-
posed into monolingual phrases that are then trans-
lated one by one (Marcu and Wong, 2002).
The N-gram-based approach regards transla-
tion as a stochastic process that maximizes the
joint probability p(f, e), leading to a decomposi-
tion based on bilingual n-grams. The core part of
the system constructed in this way is a translation
model (TM), which is based on bilingual units,
called tuples, that are extracted from a word align-
ment (performed with GIZA++ tool4) according to
certain constraints. A bilingual TM actually con-
stitutes an n-gram LM of tuples, which approxi-
mates the joint probability between the languages
under consideration and can be seen here as a LM,
where the language is composed of tuples.
</bodyText>
<subsectionHeader confidence="0.999606">
3.2 Additional features
</subsectionHeader>
<bodyText confidence="0.9960135">
The N-gram translation system implements a log-
linear combination of five additional models:
</bodyText>
<listItem confidence="0.998911">
• an n-gram target LM;
</listItem>
<footnote confidence="0.497865">
4http://code.google.com/p/giza-pp/
</footnote>
<listItem confidence="0.9389465">
• source-to-target and target-to-source lexicon
models as shown in Och and Ney (2004)).
</listItem>
<subsectionHeader confidence="0.997363">
3.3 Extended word reordering
</subsectionHeader>
<bodyText confidence="0.999846625">
An extended monotone distortion model based
on the automatically learned reordering rules was
implemented as described in Crego and Mariño
(2006). Based on the word-to-word alignment, tu-
ples were extracted by an unfolding technique. As
a result, the tuples were broken into smaller tuples,
and these were sequenced in the order of the target
words. An example of unfolding tuple extraction,
contrasted with the SAMT chunk-based rules con-
struction, is presented in Figure 1.
The reordering strategy is additionally sup-
ported by a 4-gram LM of reordered source POS
tags. In training, POS tags are reordered according
to the extracted reordering patterns and word-to-
word links. The resulting sequence of source POS
tags is used to train the n-gram LM.
</bodyText>
<subsectionHeader confidence="0.993997">
3.4 Decoding and optimization
</subsectionHeader>
<bodyText confidence="0.999975909090909">
The open-source MARIE5 decoder was used as a
search engine for the translation system. Details
can be found in Crego et al. (2005a). The de-
coder implements a beam-search algorithm with
pruning capabilities. All the additional fea-
ture models were taken into account during the
decoding process. Given the development set
and references, the log-linear combination of
weights was adjusted using a simplex optimization
method and an n-best re-ranking as described in
http://www.statmt.org/jhuws/.
</bodyText>
<sectionHeader confidence="0.999935" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.995354">
4.1 Evaluation framework
</subsectionHeader>
<bodyText confidence="0.999948375">
As training corpus, we used the 50K first-lines ex-
traction from the Arabic-English corpus that was
provided to the NIST’086 evaluation campaign
and belongs to the news domain. The corpus
statistics can be found in Table 1. The develop-
ment and test sets were provided with 4 reference
translations, belong to the same domain and con-
tain 663 and 500 sentences, respectively.
</bodyText>
<footnote confidence="0.9999595">
5http://gps-tsc.upc.es/veu/soft/soft/marie/
6www.nist.gov/speech/tests/mt/2008/
</footnote>
<page confidence="0.973501">
427
</page>
<table confidence="0.999824">
Sentences 50 K 50 K
Words 1.41 M 1.57 K
Average sentence length 28.15 31.22
Vocabulary 51.10 K 31.51 K
</table>
<tableCaption confidence="0.999861">
Table 1: Basic statistics of the training corpus.
</tableCaption>
<bodyText confidence="0.999533363636364">
Evaluation conditions were case-insensitive and
sensitive to tokenization. The word alignment is
automatically computed by using GIZA++ (Och
and Ney, 2004) in both directions, which are made
symmetric by using the grow-diag-final-and oper-
ation.
The experiments were done on a dual-processor
Pentium IV Intel Xeon Quad Core X5355 2.66
GHz machine with 24 G of RAM. All computa-
tional times and memory size results are approxi-
mated.
</bodyText>
<subsectionHeader confidence="0.990825">
4.2 Arabic data preprocessing
</subsectionHeader>
<bodyText confidence="0.999902214285714">
Arabic is a VSO (SVO in some cases) pro-
drop language with rich templatic morphology,
where words are made up of roots and affixes
and clitics agglutinate to words. For prepro-
cessing, a similar approach to that shown in
Habash and Sadat (2006) was employed, and the
MADA+TOKAN system for disambiguation and
tokenization was used. For disambiguation, only
diacritic unigram statistics were employed. For to-
kenization, the D3 scheme with -TAGBIES option
was used. The scheme splits the following set of
clitics: w+, f+, b+, k+, l+, Al+ and pronominal cl-
itics. The -TAGBIES option produces Bies POS
tags on all taggable tokens.
</bodyText>
<subsectionHeader confidence="0.994038">
4.3 SAMT experiments
</subsectionHeader>
<bodyText confidence="0.999524789473684">
The SAMT guideline was used to perform
the experiments and is available on-line:
http://www.cs.cmu.edu/∼zollmann/samt/.
Moses MT script was used to create the
grow — diag — final word alignment and
extract purely lexical phrases, which are then used
to induce the SAMT grammar. The target side
(English) of the training corpus was parsed with
the Charniak’s parser (Charniak, 2000).
Rule extraction and filtering procedures were
restricted to the concatenation of the development
and test sets, allowing for rules with a maximal
length of 12 elements in the source side and with a
zero minimum occurrence criterion for both non-
lexical and purely lexical rules.
Moses-style phrases extracted with a phrase-
based system were 4.8M, while a number of gen-
eralized rules representing the hierarchical model
grew dramatically to 22.9M. 10.8M of them were
pruned out on the filtering step.
The vocabulary of the English Penn Treebank
elementary non-terminals is 72, while a number of
generalized elements, including additive and trun-
cated categories, is 35.7K.
The FastTranslateChart beam-search de-
coder was used as an engine of MER training aim-
ing to tune the feature weight coefficients and pro-
duce final n-best and 1-best translations by com-
bining the intensive search with a standard 4-gram
LM as shown in Venugopal et al. (2007). The it-
eration limit was set to 10 with 1000-best list and
the highest BLEU score as optimization criteria.
We did not use completely abstract rules (with-
out any source-side lexical utterance), since these
rules significantly slow down the decoding process
(noAllowAbstractRules option).
Table 2 shows a summary of computational time
and RAM needed at each step of the translation.
</bodyText>
<table confidence="0.933321166666667">
Step Time Memory
Parsing 1.5h 80Mb
Rules extraction 10h 3.5Gb
Filtering&amp;merging 3h 4.0Gb
Weights tuning 40h 3Gb
Testing 2h 3Gb
</table>
<tableCaption confidence="0.9955935">
Table 2: SAMT: Computational resources.
Evaluation scores including results of system
combination (see subsection 4.6) are reported in
Table 3.
</tableCaption>
<subsectionHeader confidence="0.997641">
4.4 N-gram system experiments
</subsectionHeader>
<bodyText confidence="0.964643444444444">
The core model of the N-gram-based system is a
4-gram LM of bilingual units containing: 184.345
1-grams7, 552.838 2-grams, 179.466 3-grams and
176.2214-grams.
Along with this model, an N-gram SMT sys-
tem implements a log-linear combination of a 5-
gram target LM estimated on the English portion
of the parallel corpus, as well as supporting 4-
gram source and target models of POS tags. Bies
</bodyText>
<footnote confidence="0.9747025">
7This number also corresponds to the bilingual model vo-
cabulary.
</footnote>
<table confidence="0.868969375">
Arabic English
428
BLEU NIST mPER mWER METEOR
SAMT 43.20 9.26 36.89 49.45 58.50
N-gram-based SMT 46.39 10.06 32.98 48.47 62.36
System combination 48.00 10.15 33.20 47.54 62.27
MOSES Factored System 44.73 9.62 33.92 47.23 59.84
Oracle 61.90 11.41 28.84 41.52 66.19
</table>
<tableCaption confidence="0.999939">
Table 3: Test set evaluation results
</tableCaption>
<bodyText confidence="0.9994298">
POS tags were used for the Arabic portion, as
shown in subsection 4.2; a TnT tool was used for
English POS tagging (Brants, 2000).
The number of non-unique initially extracted
tuples is 1.1M, which were pruned according to
the maximum number of translation options per
tuple on the source side (30). Tuples with a NULL
on the source side were attached to either the pre-
vious or the next unit (Mariño et al., 2006). The
feature models weights were optimized according
to the same optimization criteria as in the SAMT
experiments (the highest BLEU score).
Stage-by-stage RAM and time requirements are
presented in Table 4, while translation quality
evaluation results can be found in Table 3.
</bodyText>
<table confidence="0.9210022">
Step Time Memory
Models estimation 0.2h 1.9Gb
Reordering 1h —
Weights tuning 15h 120Mb
Testing 2h 120Mb
</table>
<tableCaption confidence="0.95583">
Table 4: Tuple-based SMT: Computational re-
sources.
</tableCaption>
<subsectionHeader confidence="0.99691">
4.5 Statistical significance
</subsectionHeader>
<bodyText confidence="0.999970714285714">
A statistical significance test based on a bootstrap
resampling method, as shown in Koehn (2004),
was performed. For the 98% confidence interval
and 1000 set resamples, translations generated by
SAMT and N-gram system are significantly dif-
ferent according to BLEU (43.20±1.69 for SAMT
vs. 46.42 ± 1.61 for tuple-based system).
</bodyText>
<subsectionHeader confidence="0.98885">
4.6 System combination
</subsectionHeader>
<bodyText confidence="0.999976904761905">
Many MT systems generate very different trans-
lations of similar quality, even if the models
involved into translation process are analogous.
Thus, the outputs of syntax-driven and purely sta-
tistical MT systems were combined at the sentence
level using 1000-best lists of the most probable
translations produced by the both systems.
For system combination, we followed a Mini-
mum Bayes-risk algorithm, as introduced in Ku-
mar and Byrne (2004). Table 3 shows the results
of the system combination experiments on the test
set, which are contrasted with the oracle transla-
tion results, performed as a selection of the transla-
tions with the highest BLEU score from the union
of two 1000-best lists generated by SAMT and N-
gram SMT.
We also analyzed the percentage contribution of
each system to the system combination: 55-60%
of best translations come from the tuples-based
system 1000-best list, both for system combina-
tion and oracle experiments on the test set.
</bodyText>
<subsectionHeader confidence="0.99717">
4.7 Phrase-based reference system
</subsectionHeader>
<bodyText confidence="0.999061">
In order to understand the obtained results com-
pared to the state-of-the-art SMT, a reference
phrase-based factored SMT system was trained
and tested on the same data using the MOSES
toolkit. Surface forms of words (factor “0“), POS
(factor “1“) and canonical forms of the words
(lemmata) (factor “2“) were used as English fac-
tors, and surface forms and POS were the Arabic
factors.
Word alignment was performed according to
the grow-diag-final algorithm with the GIZA++
tool, a msd-bidirectional-fe conditional reordering
model was trained; the system had access to the
target-side 4-gram LMs of words and POS. The 0-
0,1+0-1,2+0-1 scheme was used on the translation
step and 1,2-0,1+1-0,1 to create generation tables.
A detailed description of the model training can
be found on the MOSES tutorial web-page8. The
results may be seen in Table 3.
</bodyText>
<sectionHeader confidence="0.987601" genericHeader="method">
5 Error analysis
</sectionHeader>
<bodyText confidence="0.8791905">
To understand the strong and weak points of both
systems under consideration, a human analysis of
</bodyText>
<footnote confidence="0.978972">
8http://www.statmt.org/moses/
</footnote>
<page confidence="0.998607">
429
</page>
<bodyText confidence="0.990673588235294">
the typical translation errors generated by each
system was performed following the framework
proposed in Vilar et al. (2006) and contrasting the
systems output with four reference translations.
Human evaluation of translation output is a time-
consuming process, thus a set of 100 randomly
chosen sentences was picked out from the corre-
sponding system output and was considered as a
representative sample of the automatically gener-
ated translation of the test corpus. According to
the proposed error topology, some classes of errors
can overlap (for example, an unknown word can
lead to a reordering problem), but it allows finding
the most prominent source of errors in a reliable
way (Vilar et al., 2006; Povovic et al., 2006). Ta-
ble 5 presents the comparative statistics of errors
generated by the SAMT and the N-gram-based
SMT systems. The average length of the generated
translations is 32.09 words for the SAMT transla-
tion and 35.30 for the N-gram-based system.
Apart from unknown words, the most important
sources of errors of the SAMT system are missing
content words and extra words generated by the
translation system, causing 17.22 % and 10.60 %
of errors, respectively. A high number of missing
content words is a serious problem affecting the
translation accuracy. In some cases, the system
is able to construct a grammatically correct
translation, but omitting an important content
word leads to a significant reduction in translation
accuracy:
SAMT translation: the ministers of arab
environment for the closure of the Israeli dymwnp
reactor.
</bodyText>
<figureCaption confidence="0.989023125">
Ref 1: arab environment ministers demand the
closure of the Israeli daemona nuclear reactor.
Ref 2: arab environment ministers demand the
closure of Israeli dimona reactor.
Ref 3: arab environment ministers call for Israeli
nuclear reactor at dimona to be shut down.
Ref 4: arab environmental ministers call for the
shutdown of the Israeli dimona reactor.
</figureCaption>
<bodyText confidence="0.9664835">
Extra words embedded into the correctly trans-
lated phrases are a well-known problem of MT
systems based on hierarchical models operating on
the small corpora. For example, in many cases
the Arabic expression AlbHr Almyt is trans-
lated into English as dead sea side and not
as dead sea, since the bilingual instances con-
tain only the whole English phrase, like following:
AlbHr Almyt#the dead sea side#@NP
The N-gram-based system handles miss-
ing words more correctly – only 9.40 % of
the errors come from the missing content
</bodyText>
<table confidence="0.999955473684211">
Type Sub-type SAMT N-gram
Missing words Content words 152 (25.17 %) 92 (15.44 %)
Filler words 104 (17.22 %) 56 (9.40 %)
48 (7.95 %) 36 (6.04 %)
Word order 96 (15.89 %) 140 (23.49 %)
Local word order 20 (3.31 %) 68 (11.41 %)
Local phrase order 20 (3.31 %) 20 (3.36 %)
Long range word order 32 (5.30 %) 48 (8.05 %)
Long range phrase order 24 (3.97 %) 4 (0.67 %)
Incorrect words 164 (27.15 %) 204 (34.23 %)
Sense: wrong lexical choice 24 (3.97 %) 60 (10.07 %)
Sense: incorrect disambiguation 16 (2.65 %) 8 (1.34 %)
Incorrect form 24 (3.97 %) 56 (9.40 %)
Extra words 64 (10.60 %) 56 (9.40 %)
Style 28 (4.64 %) 20 (3.36 %)
Idioms 4 (0.07 %) 4 (0.67 %)
Unknown words 132 (21.85 %) 104 (17.45 %)
Punctuation 60 (9.93 %) 56 (9.40 %)
Total 604 596
</table>
<tableCaption confidence="0.999751">
Table 5: Human made error statistics for a representative test set.
</tableCaption>
<page confidence="0.996841">
430
</page>
<bodyText confidence="0.9875422">
words; however, it does not handle local and
long-term reordering, thus the main problem
is phrase reordering (11.41 % and 8.05 %
of errors). In the example below, the un-
derlined block (Circumstantial Complement:
from local officials in the tour-
ism sector) is embedded between the verb
and the direct object, while in correct translation
it must be placed in the end of the sentence.
N-gram translation: the winner received
</bodyText>
<figureCaption confidence="0.9023473">
from local officials in the tourism sector three
gold medals.
Ref 1: the winner received three gold medals
from local officials from the tourism sector.
Ref 2: the winner received three gold medals
from the local tourism officials.
Ref 3: the winner received his prize of 3 gold
medals from local officials in the tourist industry.
Ref 4: the winner received three gold medals
from local officials in the tourist sector.
</figureCaption>
<bodyText confidence="0.999969107142857">
Along with inserting extra words and wrong
lexical choice, another prominent source of
incorrect translation, generated by the N-
gram system, is an erroneous grammatical
form selection, i.e., a situation when the sys-
tem is able to find the correct translation but
cannot choose the correct form. For example,
arab environment minister call for
closing dymwnp Israeli reactor,
where the verb-preposition combination
call for was correctly translated on the
stem level, but the system was not able to generate
a third person conjugation calls for. In spite
of the fact that English is a language with nearly
no inflection, 9.40 % of errors stem from poor
word form modeling. This is an example of the
weakest point of the SMT systems having access
to a small training material; the decoder does not
use syntactic information about the subject of
the sentence (singular) and makes a choice only
concerning the tuple probability.
The difference in total number of errors is neg-
ligible, however a subjective evaluation of the sys-
tems output shows that the translation generated
by the N-gram system is more understandable
than the SAMT one, since more content words are
translated correctly and the meaning of the sen-
tence is still preserved.
</bodyText>
<sectionHeader confidence="0.987485" genericHeader="discussions">
6 Discussion and conclusions
</sectionHeader>
<bodyText confidence="0.999989022222222">
In this study two systems are compared: the UPC-
TALP N-gram-based and the CMU-UKA SAMT
systems, originating from the ideas of Finite-State
Transducers and hierarchical phrase translation,
respectively. The comparison was created to be as
fair as possible, using the same training material
and the same tools on the preprocessing, word-
to-word alignment and language modeling steps.
The obtained results were also contrasted with the
state-of-the-art phrase-based SMT.
Analyzing the automatic evaluation scores, the
N-gram-based approach shows good performance
for the small Arabic-to-English task and signifi-
cantly outperforms the SAMT system. The results
shown by the modern phrase-based SMT (factored
MOSES) lie between the two systems under con-
sideration. Considering memory size and compu-
tational time, the tuple-based system has obtained
significantly better results than SAMT, primarily
because of its smaller search space.
Interesting results were obtained for the PER
and WER metrics: according to the PER,
the UPC-TALP system outperforms the SAMT
by 10%, while the WER improvement hardly
achieves a 2% difference. The N-gram-based
SMT can translate the context better, but pro-
duces more reordering errors than SAMT. This
may be explained by the fact that Arabic and En-
glish are languages with high disparity in word
order, and the N-gram system deals worse with
long-distance reordering because it attempts to use
shorter units. However, by means of introducing
the word context into the TM, short-distance bilin-
gual dependencies can be captured effectively.
The main conclusion that can be made from
the human evaluation analysis is that the systems
commit a comparable number of errors, but they
are distributed dissimilarly. In case of the SAMT
system, the frequent errors are caused by missing
or incorrectly inserted extra words, while the N-
gram-based system suffers from reordering prob-
lems and wrong words/word form choice
Significant improvement in translation quality
was achieved by combining the outputs of the two
systems based on different translating principles.
</bodyText>
<sectionHeader confidence="0.998644" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.911450666666667">
This work has been funded by the Spanish Gov-
ernment under grant TEC2006-13964-C03 (AVI-
VAVOZ project).
</bodyText>
<page confidence="0.996669">
431
</page>
<bodyText confidence="0.3379738">
J. B. Mariño, R. E. Banchs, J. M. Crego, A. de Gispert,
P. Lambert, J. A. R. Fonollosa, and M. R. Costa-
jussà. 2006. N-gram based machine translation.
Computational Linguistics, 32(4):527–549, Decem-
ber.
</bodyText>
<sectionHeader confidence="0.758035" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995604372093023">
T. Brants. 2000. TnT – a statistical part-of-speech tag-
ger. In Proceedings of the 6th Applied Natural Lan-
guage Processing (ANLP-2000).
E. Charniak. 2000. A maximum entropy-inspired
parser. In Proceedings of NAACL 2000, pages 132–
139.
D. Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL 2005, pages 263–270.
J. M. Crego and J. B. Mariño. 2006. Improving statis-
tical MT by coupling reordering and decoding. Ma-
chine Translation, 20(3):199–215.
J. M. Crego, J. Mariño, and A. de Gispert. 2005a. An
Ngram-based Statistical Machine Translation De-
coder. In Proceedings of INTERSPEECH05, pages
3185–3188.
J.M. Crego, M.R. Costa-jussà, J.B. Mariño, and J.A.R.
Fonollosa. 2005b. Ngram-based versus phrase-
based statistical machine translation. In Proc. of the
IWSLT 2005, pages 177–184.
S. DeNeefe, K. Knight, W. Wang, and D. Marcu. 2007.
What can syntax-based MT learn from phrase-based
MT? In Proceedings of EMNLP-CoNLL 2007,
pages 755–763.
J. Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of
ACL 2003 (companion volume), pages 205–208.
N. Habash and F. Sadat. 2006. Arabic preprocessing
schemes for statistical machine translation. In Pro-
ceedings of HLT/NAACL 2006, pages 49–52.
Ph. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based machine translation. In Proceedings of
HLT-NAACL 2003, pages 48–54.
Ph. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: open-source toolkit
for statistical machine translation. In Proceedings
of ACL 2007, pages 177–180.
P. Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388–395.
S. Kumar and W. Byrne. 2004. Minimum bayes-risk
decoding for statistical machine translation. In Pro-
ceedings of HLT/NAACL 2004.
D. Marcu and W. Wong. 2002. A Phrase-based, Joint
Probability Model for Statistical Machine Transla-
tion. In Proceedings of EMNLP02, pages 133–139.
I.D. Melamed. 2004. Statistical machine translation by
parsing. In Proceedings of ACL 2004, pages 111–
114.
F. J. Och and H. Ney. 2002. Discriminative Train-
ing and Maximum Entropy Models for Statistical
Machine Translation. In Proceedings of ACL 2002,
pages 295–302.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19–51.
F. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417–449.
M. Povovic, A. de Gispert, D. Gupta, P. Lambert, J.B.
Mariño, M. Federico, H. Ney, and R. Banchs. 2006.
Morpho-syntactic information for automatic error
analysis of statistic machine translation output. In
In Proceeding of the HLT-NAACL Workshop on Sta-
tistical Machine Translation, pages 1–6.
M. Steedman. 1999. Alternative quantifier scope in
ccg. In Proceedings of ACL 1999, pages 301–308.
A. Venugopal, A. Zollmann, and S. Vogel. 2007.
An Efficient Two-Pass Approach to Synchronous-
CFG Driven Statistical MT. In Proceedings of
HLT/NAACL 2007, pages 500–507.
D. Vilar, J. Xu, L. F. D’Haro, and H. Ney. 2006. Error
Analysis of Machine Translation Output. In Pro-
ceedings of LREC’06, pages 697–702.
K. Yamada and K. Knight. 2001. A syntax-based sta-
tistical translation model. In Proceedings of ACL
2001, pages 523–530.
A. Zollmann and A. Venugopal. 2006. Syntax aug-
mented machine translation via chart parsing. In
Proceedings of NAACL 2006.
A. Zollmann, A. Venugopal, F. Och, and J. Ponte.
2008. Systematic comparison of Phrase-based, Hi-
erarchical and Syntax-Augmented Statistical mt. In
Proceedings of Coling 2008, pages 1145–1152.
</reference>
<page confidence="0.998553">
432
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.775869">
<title confidence="0.9913675">N-gram-based Statistical Machine Translation versus Syntax Augmented Machine Translation: comparison and system combination</title>
<author confidence="0.99973">Khalilov A R Fonollosa</author>
<affiliation confidence="0.999717">Universitat Politècnica de Catalunya</affiliation>
<address confidence="0.9123495">Campus Nord UPC, 08034 Barcelona, Spain</address>
<email confidence="0.99977">khalilov@talp.upc.edu</email>
<email confidence="0.99977">adrian@talp.upc.edu</email>
<abstract confidence="0.998174653846154">In this paper we compare and contrast two approaches to Machine Translation (MT): the CMU-UKA Syntax Augmented Machine Translation system (SAMT) and UPC-TALP N-gram-based Statistical Machine Translation (SMT). SAMT is a hierarchical syntax-driven translation system underlain by a phrase-based model and a target part parse tree. In N-gram-based SMT, the translation process is based on bilingual units related to word-to-word alignment and statistical modeling of the bilingual context following a maximumentropy framework. We provide a stepby-step comparison of the systems and report results in terms of automatic evaluation metrics and required computational resources for a smaller Arabic-to-English task in the training corpus). Human error analysis clarifies advantages and disadvantages of the systems under consideration. Finally, we combine the output of both systems to yield significant improvements in translation quality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Brants</author>
</authors>
<title>TnT – a statistical part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th Applied Natural Language Processing (ANLP-2000).</booktitle>
<contexts>
<context position="17758" citStr="Brants, 2000" startWordPosition="2818" endWordPosition="2819"> the English portion of the parallel corpus, as well as supporting 4- gram source and target models of POS tags. Bies 7This number also corresponds to the bilingual model vocabulary. Arabic English 428 BLEU NIST mPER mWER METEOR SAMT 43.20 9.26 36.89 49.45 58.50 N-gram-based SMT 46.39 10.06 32.98 48.47 62.36 System combination 48.00 10.15 33.20 47.54 62.27 MOSES Factored System 44.73 9.62 33.92 47.23 59.84 Oracle 61.90 11.41 28.84 41.52 66.19 Table 3: Test set evaluation results POS tags were used for the Arabic portion, as shown in subsection 4.2; a TnT tool was used for English POS tagging (Brants, 2000). The number of non-unique initially extracted tuples is 1.1M, which were pruned according to the maximum number of translation options per tuple on the source side (30). Tuples with a NULL on the source side were attached to either the previous or the next unit (Mariño et al., 2006). The feature models weights were optimized according to the same optimization criteria as in the SAMT experiments (the highest BLEU score). Stage-by-stage RAM and time requirements are presented in Table 4, while translation quality evaluation results can be found in Table 3. Step Time Memory Models estimation 0.2</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>T. Brants. 2000. TnT – a statistical part-of-speech tagger. In Proceedings of the 6th Applied Natural Language Processing (ANLP-2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL 2000,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="6184" citStr="Charniak, 2000" startWordPosition="936" endWordPosition="937">the &amp;quot;Chiang-style&amp;quot; rules, which do not span the parse tree. Consequently, all lexical mapping rules are covered by the phrases mapping table. 2.2 Rules annotation, generalization and pruning The SAMT system is based on a purely lexical phrase table, which is identified as shown in Koehn et al. (2003), and word alignment, which is generated by the grow-diag-final-and method (expanding the alignment by adding directly neighboring alignment points and alignment points in the diagonal neighborhood) (Och and Ney, 2003). Meanwhile, the target of the training corpus is parsed with Charniak’s parser (Charniak, 2000), and each phrase is annotated with the constituent that spans the target side of the rules. The set of non-terminals is extended by means of conditional and additive categories according to Combinatory Categorical Grammar (CCG) (Steedman, 1999). Under this approach, new rules can be formed. For example, RB+VB, can represent an additive constituent consisting of two synthetically generated adjacent categories 3, i.e., an adverb and a verb. Furthermore, DT�NP can indicate an incomplete noun phrase with a missing determiner to the left. The rule recursive generalization procedure coincides with </context>
<context position="15244" citStr="Charniak, 2000" startWordPosition="2415" endWordPosition="2416">ion, the D3 scheme with -TAGBIES option was used. The scheme splits the following set of clitics: w+, f+, b+, k+, l+, Al+ and pronominal clitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. 4.3 SAMT experiments The SAMT guideline was used to perform the experiments and is available on-line: http://www.cs.cmu.edu/∼zollmann/samt/. Moses MT script was used to create the grow — diag — final word alignment and extract purely lexical phrases, which are then used to induce the SAMT grammar. The target side (English) of the training corpus was parsed with the Charniak’s parser (Charniak, 2000). Rule extraction and filtering procedures were restricted to the concatenation of the development and test sets, allowing for rules with a maximal length of 12 elements in the source side and with a zero minimum occurrence criterion for both nonlexical and purely lexical rules. Moses-style phrases extracted with a phrasebased system were 4.8M, while a number of generalized rules representing the hierarchical model grew dramatically to 22.9M. 10.8M of them were pruned out on the filtering step. The vocabulary of the English Penn Treebank elementary non-terminals is 72, while a number of genera</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>E. Charniak. 2000. A maximum entropy-inspired parser. In Proceedings of NAACL 2000, pages 132– 139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>263--270</pages>
<contexts>
<context position="2692" citStr="Chiang (2005)" startWordPosition="390" endWordPosition="391">ms allow for linguistically motivated word reordering by implementing word order monotonization. Prior to the SMT revolution, a major part of MT systems was developed using rule-based algorithms; however, starting from the 1990’s, syntax-driven systems based on phrase hierarchy have gained popularity. A representative sample of modern syntax-based systems includes models based on bilingual synchronous grammar (Melamed, 2004), parse tree-to-string translation models (Yamada and Knight, 2001) and nonisomorphic tree-to-tree mappings (Eisner, 2003). The orthodox phrase-based model was enhanced in Chiang (2005), where a hierarchical phrase model allowing for multiple generalizations within each phrase was introduced. The open-source toolkit SAMT2 (Zollmann and Venugopal, 2006) is a further evolution of this approach, in which syntactic categories extracted from the target side parse tree are directly assigned to the hierarchically structured phrases. Several publications discovering similarities and differences between distinct translation models have been written over the last few years. In Crego et al. (2005b), the N-gram-based system is contrasted with a state-of-the-art phrase-based framework, w</context>
<context position="4558" citStr="Chiang (2005)" startWordPosition="676" endWordPosition="677">mall Arabic-toEnglish translation task from the news domain. 2 SAMT system A criticism of phrase-based models is data sparseness. This problem is even more serious when the source, the target, or both languages are inflectional and rich in morphology. Moreover, phrasebased models are unable to cope with global reordering because the distortion model is based on movement distance, which may face computational resource limitations (Och and Ney, 2004). This problem was successfully addressed when the MT system based on generalized hierarchically structured phrases was introduced and discussed in Chiang (2005). It operates with only two markers (a substantial phrase category and &amp;quot;a glue marker&amp;quot;). Moreover, a recent work (Zollmann and Venugopal, 2006) reports significant improvement in terms of translation quality if complete or partial syntactic categories (derived from the target side parse tree) are assigned to the phrases. 2.1 Modeling A formalism for Syntax Augmented Translation is probabilistic synchronous context-free grammar (PSynCFG), which is defined in terms of source and target terminal sets and a set of non-terminals: X ) (-y, α, —, w) where X is a non-terminal, -y is a sequence of sour</context>
<context position="6817" citStr="Chiang (2005)" startWordPosition="1033" endWordPosition="1034"> annotated with the constituent that spans the target side of the rules. The set of non-terminals is extended by means of conditional and additive categories according to Combinatory Categorical Grammar (CCG) (Steedman, 1999). Under this approach, new rules can be formed. For example, RB+VB, can represent an additive constituent consisting of two synthetically generated adjacent categories 3, i.e., an adverb and a verb. Furthermore, DT�NP can indicate an incomplete noun phrase with a missing determiner to the left. The rule recursive generalization procedure coincides with the one proposed in Chiang (2005), but violates the restrictions introduced for singlecategory grammar; for example, rules that contain adjacent generalized elements are not discarded. Thus, each rule N ) f1 ... fm/e1 ... en can be extended by another existing rule M ) fi ... fu/ej ... ev where 1 &lt; i &lt; u &lt; m and 1 &lt; j &lt; v &lt; n, to obtain a new rule N ) f1 ... fi−1Mkfu+1 ... fm/ e1 ... ej−1Mkev+1 ... en where k is an index for the non-terminal M that indicates a one-to-one correspondence between the new M tokens on the two sides. Figure 1 shows an example of initial rules extraction, which can be further extended using the hier</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>D. Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of ACL 2005, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Crego</author>
<author>J B Mariño</author>
</authors>
<title>Improving statistical MT by coupling reordering and decoding.</title>
<date>2006</date>
<journal>Machine Translation,</journal>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="11981" citStr="Crego and Mariño (2006)" startWordPosition="1903" endWordPosition="1906">titutes an n-gram LM of tuples, which approximates the joint probability between the languages under consideration and can be seen here as a LM, where the language is composed of tuples. 3.2 Additional features The N-gram translation system implements a loglinear combination of five additional models: • an n-gram target LM; 4http://code.google.com/p/giza-pp/ • source-to-target and target-to-source lexicon models as shown in Och and Ney (2004)). 3.3 Extended word reordering An extended monotone distortion model based on the automatically learned reordering rules was implemented as described in Crego and Mariño (2006). Based on the word-to-word alignment, tuples were extracted by an unfolding technique. As a result, the tuples were broken into smaller tuples, and these were sequenced in the order of the target words. An example of unfolding tuple extraction, contrasted with the SAMT chunk-based rules construction, is presented in Figure 1. The reordering strategy is additionally supported by a 4-gram LM of reordered source POS tags. In training, POS tags are reordered according to the extracted reordering patterns and word-toword links. The resulting sequence of source POS tags is used to train the n-gram </context>
</contexts>
<marker>Crego, Mariño, 2006</marker>
<rawString>J. M. Crego and J. B. Mariño. 2006. Improving statistical MT by coupling reordering and decoding. Machine Translation, 20(3):199–215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Crego</author>
<author>J Mariño</author>
<author>A de Gispert</author>
</authors>
<title>An Ngram-based Statistical Machine Translation Decoder.</title>
<date>2005</date>
<booktitle>In Proceedings of INTERSPEECH05,</booktitle>
<pages>3185--3188</pages>
<marker>Crego, Mariño, de Gispert, 2005</marker>
<rawString>J. M. Crego, J. Mariño, and A. de Gispert. 2005a. An Ngram-based Statistical Machine Translation Decoder. In Proceedings of INTERSPEECH05, pages 3185–3188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Crego</author>
<author>M R Costa-jussà</author>
<author>J B Mariño</author>
<author>J A R Fonollosa</author>
</authors>
<title>Ngram-based versus phrasebased statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of the IWSLT</booktitle>
<pages>177--184</pages>
<contexts>
<context position="3201" citStr="Crego et al. (2005" startWordPosition="463" endWordPosition="466">somorphic tree-to-tree mappings (Eisner, 2003). The orthodox phrase-based model was enhanced in Chiang (2005), where a hierarchical phrase model allowing for multiple generalizations within each phrase was introduced. The open-source toolkit SAMT2 (Zollmann and Venugopal, 2006) is a further evolution of this approach, in which syntactic categories extracted from the target side parse tree are directly assigned to the hierarchically structured phrases. Several publications discovering similarities and differences between distinct translation models have been written over the last few years. In Crego et al. (2005b), the N-gram-based system is contrasted with a state-of-the-art phrase-based framework, while in DeNeefe et al. (2007), the authors seek to estimate the advantages, weakest points and possible overlap between syntaxbased MT and phrase-based SMT. In Zollmann et al. (2008) the comparison of phrase-based, &amp;quot;Chiang’s style&amp;quot; hirearchical system and SAMT is pro2www.cs.cmu.edu/∼zollmann/samt Proceedings of the 12th Conference of the European Chapter of the ACL, pages 424–432, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 424 vided. In this study, we intend</context>
<context position="12744" citStr="Crego et al. (2005" startWordPosition="2029" endWordPosition="2032">and these were sequenced in the order of the target words. An example of unfolding tuple extraction, contrasted with the SAMT chunk-based rules construction, is presented in Figure 1. The reordering strategy is additionally supported by a 4-gram LM of reordered source POS tags. In training, POS tags are reordered according to the extracted reordering patterns and word-toword links. The resulting sequence of source POS tags is used to train the n-gram LM. 3.4 Decoding and optimization The open-source MARIE5 decoder was used as a search engine for the translation system. Details can be found in Crego et al. (2005a). The decoder implements a beam-search algorithm with pruning capabilities. All the additional feature models were taken into account during the decoding process. Given the development set and references, the log-linear combination of weights was adjusted using a simplex optimization method and an n-best re-ranking as described in http://www.statmt.org/jhuws/. 4 Experiments 4.1 Evaluation framework As training corpus, we used the 50K first-lines extraction from the Arabic-English corpus that was provided to the NIST’086 evaluation campaign and belongs to the news domain. The corpus statistic</context>
</contexts>
<marker>Crego, Costa-jussà, Mariño, Fonollosa, 2005</marker>
<rawString>J.M. Crego, M.R. Costa-jussà, J.B. Mariño, and J.A.R. Fonollosa. 2005b. Ngram-based versus phrasebased statistical machine translation. In Proc. of the IWSLT 2005, pages 177–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S DeNeefe</author>
<author>K Knight</author>
<author>W Wang</author>
<author>D Marcu</author>
</authors>
<title>What can syntax-based MT learn from phrase-based MT?</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL</booktitle>
<pages>755--763</pages>
<contexts>
<context position="3321" citStr="DeNeefe et al. (2007)" startWordPosition="479" endWordPosition="482">a hierarchical phrase model allowing for multiple generalizations within each phrase was introduced. The open-source toolkit SAMT2 (Zollmann and Venugopal, 2006) is a further evolution of this approach, in which syntactic categories extracted from the target side parse tree are directly assigned to the hierarchically structured phrases. Several publications discovering similarities and differences between distinct translation models have been written over the last few years. In Crego et al. (2005b), the N-gram-based system is contrasted with a state-of-the-art phrase-based framework, while in DeNeefe et al. (2007), the authors seek to estimate the advantages, weakest points and possible overlap between syntaxbased MT and phrase-based SMT. In Zollmann et al. (2008) the comparison of phrase-based, &amp;quot;Chiang’s style&amp;quot; hirearchical system and SAMT is pro2www.cs.cmu.edu/∼zollmann/samt Proceedings of the 12th Conference of the European Chapter of the ACL, pages 424–432, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 424 vided. In this study, we intend to compare the differences and similarities of the statistical N-grambased SMT approach and the SAMT system. The compar</context>
</contexts>
<marker>DeNeefe, Knight, Wang, Marcu, 2007</marker>
<rawString>S. DeNeefe, K. Knight, W. Wang, and D. Marcu. 2007. What can syntax-based MT learn from phrase-based MT? In Proceedings of EMNLP-CoNLL 2007, pages 755–763.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Learning non-isomorphic tree mappings for machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>205--208</pages>
<contexts>
<context position="2629" citStr="Eisner, 2003" startWordPosition="380" endWordPosition="381"> zero or more words from the target one. The N-gram-based systems allow for linguistically motivated word reordering by implementing word order monotonization. Prior to the SMT revolution, a major part of MT systems was developed using rule-based algorithms; however, starting from the 1990’s, syntax-driven systems based on phrase hierarchy have gained popularity. A representative sample of modern syntax-based systems includes models based on bilingual synchronous grammar (Melamed, 2004), parse tree-to-string translation models (Yamada and Knight, 2001) and nonisomorphic tree-to-tree mappings (Eisner, 2003). The orthodox phrase-based model was enhanced in Chiang (2005), where a hierarchical phrase model allowing for multiple generalizations within each phrase was introduced. The open-source toolkit SAMT2 (Zollmann and Venugopal, 2006) is a further evolution of this approach, in which syntactic categories extracted from the target side parse tree are directly assigned to the hierarchically structured phrases. Several publications discovering similarities and differences between distinct translation models have been written over the last few years. In Crego et al. (2005b), the N-gram-based system </context>
</contexts>
<marker>Eisner, 2003</marker>
<rawString>J. Eisner. 2003. Learning non-isomorphic tree mappings for machine translation. In Proceedings of ACL 2003 (companion volume), pages 205–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
<author>F Sadat</author>
</authors>
<title>Arabic preprocessing schemes for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT/NAACL</booktitle>
<pages>49--52</pages>
<contexts>
<context position="14460" citStr="Habash and Sadat (2006)" startWordPosition="2291" endWordPosition="2294">word alignment is automatically computed by using GIZA++ (Och and Ney, 2004) in both directions, which are made symmetric by using the grow-diag-final-and operation. The experiments were done on a dual-processor Pentium IV Intel Xeon Quad Core X5355 2.66 GHz machine with 24 G of RAM. All computational times and memory size results are approximated. 4.2 Arabic data preprocessing Arabic is a VSO (SVO in some cases) prodrop language with rich templatic morphology, where words are made up of roots and affixes and clitics agglutinate to words. For preprocessing, a similar approach to that shown in Habash and Sadat (2006) was employed, and the MADA+TOKAN system for disambiguation and tokenization was used. For disambiguation, only diacritic unigram statistics were employed. For tokenization, the D3 scheme with -TAGBIES option was used. The scheme splits the following set of clitics: w+, f+, b+, k+, l+, Al+ and pronominal clitics. The -TAGBIES option produces Bies POS tags on all taggable tokens. 4.3 SAMT experiments The SAMT guideline was used to perform the experiments and is available on-line: http://www.cs.cmu.edu/∼zollmann/samt/. Moses MT script was used to create the grow — diag — final word alignment and</context>
</contexts>
<marker>Habash, Sadat, 2006</marker>
<rawString>N. Habash and F. Sadat. 2006. Arabic preprocessing schemes for statistical machine translation. In Proceedings of HLT/NAACL 2006, pages 49–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och Koehn</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<pages>48--54</pages>
<marker>Koehn, Marcu, 2003</marker>
<rawString>Ph. Koehn, F.J. Och, and D. Marcu. 2003. Statistical phrase-based machine translation. In Proceedings of HLT-NAACL 2003, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hoang Koehn</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: open-source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>177--180</pages>
<contexts>
<context position="1605" citStr="Koehn et al., 2007" startWordPosition="229" endWordPosition="232">ranslation task (1.5M tokens in the training corpus). Human error analysis clarifies advantages and disadvantages of the systems under consideration. Finally, we combine the output of both systems to yield significant improvements in translation quality. 1 Introduction There is an ongoing controversy regarding whether or not information about the syntax of language can benefit MT or contribute to a hybrid system. Classical IBM word-based models were recently augmented with a phrase translation capability, as shown in Koehn et al. (2003), or in more recent implementation, the MOSES MT system1 (Koehn et al., 2007). In parallel to the phrasebased approach, the N-gram-based approach appeared (Mariño et al., 2006). It stemms from 1www.statmt.org/moses/ the Finite-State Transducers paradigm, and is extended to the log-linear modeling framework, as shown in (Mariño et al., 2006). A system following this approach deals with bilingual units, called tuples, which are composed of one or more words from the source language and zero or more words from the target one. The N-gram-based systems allow for linguistically motivated word reordering by implementing word order monotonization. Prior to the SMT revolution, </context>
</contexts>
<marker>Koehn, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Ph. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: open-source toolkit for statistical machine translation. In Proceedings of ACL 2007, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>388--395</pages>
<contexts>
<context position="18600" citStr="Koehn (2004)" startWordPosition="2952" endWordPosition="2953">er the previous or the next unit (Mariño et al., 2006). The feature models weights were optimized according to the same optimization criteria as in the SAMT experiments (the highest BLEU score). Stage-by-stage RAM and time requirements are presented in Table 4, while translation quality evaluation results can be found in Table 3. Step Time Memory Models estimation 0.2h 1.9Gb Reordering 1h — Weights tuning 15h 120Mb Testing 2h 120Mb Table 4: Tuple-based SMT: Computational resources. 4.5 Statistical significance A statistical significance test based on a bootstrap resampling method, as shown in Koehn (2004), was performed. For the 98% confidence interval and 1000 set resamples, translations generated by SAMT and N-gram system are significantly different according to BLEU (43.20±1.69 for SAMT vs. 46.42 ± 1.61 for tuple-based system). 4.6 System combination Many MT systems generate very different translations of similar quality, even if the models involved into translation process are analogous. Thus, the outputs of syntax-driven and purely statistical MT systems were combined at the sentence level using 1000-best lists of the most probable translations produced by the both systems. For system com</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP 2004, pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kumar</author>
<author>W Byrne</author>
</authors>
<title>Minimum bayes-risk decoding for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT/NAACL</booktitle>
<contexts>
<context position="19293" citStr="Kumar and Byrne (2004)" startWordPosition="3056" endWordPosition="3060">, translations generated by SAMT and N-gram system are significantly different according to BLEU (43.20±1.69 for SAMT vs. 46.42 ± 1.61 for tuple-based system). 4.6 System combination Many MT systems generate very different translations of similar quality, even if the models involved into translation process are analogous. Thus, the outputs of syntax-driven and purely statistical MT systems were combined at the sentence level using 1000-best lists of the most probable translations produced by the both systems. For system combination, we followed a Minimum Bayes-risk algorithm, as introduced in Kumar and Byrne (2004). Table 3 shows the results of the system combination experiments on the test set, which are contrasted with the oracle translation results, performed as a selection of the translations with the highest BLEU score from the union of two 1000-best lists generated by SAMT and Ngram SMT. We also analyzed the percentage contribution of each system to the system combination: 55-60% of best translations come from the tuples-based system 1000-best list, both for system combination and oracle experiments on the test set. 4.7 Phrase-based reference system In order to understand the obtained results comp</context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>S. Kumar and W. Byrne. 2004. Minimum bayes-risk decoding for statistical machine translation. In Proceedings of HLT/NAACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>W Wong</author>
</authors>
<title>A Phrase-based, Joint Probability Model for Statistical Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP02,</booktitle>
<pages>133--139</pages>
<contexts>
<context position="10921" citStr="Marcu and Wong, 2002" startWordPosition="1741" endWordPosition="1744">mbination of a set of feature functions (Och and Ney, 2002). Using this technique, the additional models are combined to determine the translation hypothesis, as shown in (3): I EM � �eI 1 = arg max amhm(eI 1, fJ 1 ) (3) eI1 m=1 where the feature functions hm refer to the system models and the set of am refers to the weights corresponding to these models. 3.1 N-gram-based translation system The N-gram approach to SMT is considered to be an alternative to the phrase-based translation, where a given source word sequence is decomposed into monolingual phrases that are then translated one by one (Marcu and Wong, 2002). The N-gram-based approach regards translation as a stochastic process that maximizes the joint probability p(f, e), leading to a decomposition based on bilingual n-grams. The core part of the system constructed in this way is a translation model (TM), which is based on bilingual units, called tuples, that are extracted from a word alignment (performed with GIZA++ tool4) according to certain constraints. A bilingual TM actually constitutes an n-gram LM of tuples, which approximates the joint probability between the languages under consideration and can be seen here as a LM, where the language</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>D. Marcu and W. Wong. 2002. A Phrase-based, Joint Probability Model for Statistical Machine Translation. In Proceedings of EMNLP02, pages 133–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I D Melamed</author>
</authors>
<title>Statistical machine translation by parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL 2004,</booktitle>
<pages>111--114</pages>
<contexts>
<context position="2507" citStr="Melamed, 2004" startWordPosition="365" endWordPosition="366">is approach deals with bilingual units, called tuples, which are composed of one or more words from the source language and zero or more words from the target one. The N-gram-based systems allow for linguistically motivated word reordering by implementing word order monotonization. Prior to the SMT revolution, a major part of MT systems was developed using rule-based algorithms; however, starting from the 1990’s, syntax-driven systems based on phrase hierarchy have gained popularity. A representative sample of modern syntax-based systems includes models based on bilingual synchronous grammar (Melamed, 2004), parse tree-to-string translation models (Yamada and Knight, 2001) and nonisomorphic tree-to-tree mappings (Eisner, 2003). The orthodox phrase-based model was enhanced in Chiang (2005), where a hierarchical phrase model allowing for multiple generalizations within each phrase was introduced. The open-source toolkit SAMT2 (Zollmann and Venugopal, 2006) is a further evolution of this approach, in which syntactic categories extracted from the target side parse tree are directly assigned to the hierarchically structured phrases. Several publications discovering similarities and differences betwee</context>
</contexts>
<marker>Melamed, 2004</marker>
<rawString>I.D. Melamed. 2004. Statistical machine translation by parsing. In Proceedings of ACL 2004, pages 111– 114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Discriminative Training and Maximum Entropy Models for Statistical Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>295--302</pages>
<contexts>
<context position="10359" citStr="Och and Ney, 2002" startWordPosition="1641" endWordPosition="1644">g max {1 p(eI1 |fJ1) eI 1 { 1 p(fJ 1 |eI 1) � p(eI 1) (2) • a target LM of Part-of-Speech tags; • a word penalty model that is used to compensate for the system’s preference for short output sentences; = arg max el 1 where I and J represent the number of words in the target and source languages, respectively. Modern state-of-the-art SMT systems operate with the bilingual units extracted from the parallel corpus based on word-to-word alignment. They are enhanced by the maximum entropy approach and the posterior probability is calculated as a loglinear combination of a set of feature functions (Och and Ney, 2002). Using this technique, the additional models are combined to determine the translation hypothesis, as shown in (3): I EM � �eI 1 = arg max amhm(eI 1, fJ 1 ) (3) eI1 m=1 where the feature functions hm refer to the system models and the set of am refers to the weights corresponding to these models. 3.1 N-gram-based translation system The N-gram approach to SMT is considered to be an alternative to the phrase-based translation, where a given source word sequence is decomposed into monolingual phrases that are then translated one by one (Marcu and Wong, 2002). The N-gram-based approach regards tr</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>F. J. Och and H. Ney. 2002. Discriminative Training and Maximum Entropy Models for Statistical Machine Translation. In Proceedings of ACL 2002, pages 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="6088" citStr="Och and Ney, 2003" startWordPosition="920" endWordPosition="923">onding to the target-side Penn Treebank set, a set of glue rules and a special marker representing the &amp;quot;Chiang-style&amp;quot; rules, which do not span the parse tree. Consequently, all lexical mapping rules are covered by the phrases mapping table. 2.2 Rules annotation, generalization and pruning The SAMT system is based on a purely lexical phrase table, which is identified as shown in Koehn et al. (2003), and word alignment, which is generated by the grow-diag-final-and method (expanding the alignment by adding directly neighboring alignment points and alignment points in the diagonal neighborhood) (Och and Ney, 2003). Meanwhile, the target of the training corpus is parsed with Charniak’s parser (Charniak, 2000), and each phrase is annotated with the constituent that spans the target side of the rules. The set of non-terminals is extended by means of conditional and additive categories according to Combinatory Categorical Grammar (CCG) (Steedman, 1999). Under this approach, new rules can be formed. For example, RB+VB, can represent an additive constituent consisting of two synthetically generated adjacent categories 3, i.e., an adverb and a verb. Furthermore, DT�NP can indicate an incomplete noun phrase wi</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="4397" citStr="Och and Ney, 2004" startWordPosition="650" endWordPosition="653"> this study, we intend to compare the differences and similarities of the statistical N-grambased SMT approach and the SAMT system. The comparison is performed on a small Arabic-toEnglish translation task from the news domain. 2 SAMT system A criticism of phrase-based models is data sparseness. This problem is even more serious when the source, the target, or both languages are inflectional and rich in morphology. Moreover, phrasebased models are unable to cope with global reordering because the distortion model is based on movement distance, which may face computational resource limitations (Och and Ney, 2004). This problem was successfully addressed when the MT system based on generalized hierarchically structured phrases was introduced and discussed in Chiang (2005). It operates with only two markers (a substantial phrase category and &amp;quot;a glue marker&amp;quot;). Moreover, a recent work (Zollmann and Venugopal, 2006) reports significant improvement in terms of translation quality if complete or partial syntactic categories (derived from the target side parse tree) are assigned to the phrases. 2.1 Modeling A formalism for Syntax Augmented Translation is probabilistic synchronous context-free grammar (PSynCFG</context>
<context position="11804" citStr="Och and Ney (2004)" startWordPosition="1878" endWordPosition="1881">sed on bilingual units, called tuples, that are extracted from a word alignment (performed with GIZA++ tool4) according to certain constraints. A bilingual TM actually constitutes an n-gram LM of tuples, which approximates the joint probability between the languages under consideration and can be seen here as a LM, where the language is composed of tuples. 3.2 Additional features The N-gram translation system implements a loglinear combination of five additional models: • an n-gram target LM; 4http://code.google.com/p/giza-pp/ • source-to-target and target-to-source lexicon models as shown in Och and Ney (2004)). 3.3 Extended word reordering An extended monotone distortion model based on the automatically learned reordering rules was implemented as described in Crego and Mariño (2006). Based on the word-to-word alignment, tuples were extracted by an unfolding technique. As a result, the tuples were broken into smaller tuples, and these were sequenced in the order of the target words. An example of unfolding tuple extraction, contrasted with the SAMT chunk-based rules construction, is presented in Figure 1. The reordering strategy is additionally supported by a 4-gram LM of reordered source POS tags.</context>
<context position="13913" citStr="Och and Ney, 2004" startWordPosition="2198" endWordPosition="2201"> belongs to the news domain. The corpus statistics can be found in Table 1. The development and test sets were provided with 4 reference translations, belong to the same domain and contain 663 and 500 sentences, respectively. 5http://gps-tsc.upc.es/veu/soft/soft/marie/ 6www.nist.gov/speech/tests/mt/2008/ 427 Sentences 50 K 50 K Words 1.41 M 1.57 K Average sentence length 28.15 31.22 Vocabulary 51.10 K 31.51 K Table 1: Basic statistics of the training corpus. Evaluation conditions were case-insensitive and sensitive to tokenization. The word alignment is automatically computed by using GIZA++ (Och and Ney, 2004) in both directions, which are made symmetric by using the grow-diag-final-and operation. The experiments were done on a dual-processor Pentium IV Intel Xeon Quad Core X5355 2.66 GHz machine with 24 G of RAM. All computational times and memory size results are approximated. 4.2 Arabic data preprocessing Arabic is a VSO (SVO in some cases) prodrop language with rich templatic morphology, where words are made up of roots and affixes and clitics agglutinate to words. For preprocessing, a similar approach to that shown in Habash and Sadat (2006) was employed, and the MADA+TOKAN system for disambig</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>F. Och and H. Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Povovic</author>
<author>A de Gispert</author>
<author>D Gupta</author>
<author>P Lambert</author>
<author>J B Mariño</author>
<author>M Federico</author>
<author>H Ney</author>
<author>R Banchs</author>
</authors>
<title>Morpho-syntactic information for automatic error analysis of statistic machine translation output.</title>
<date>2006</date>
<booktitle>In In Proceeding of the HLT-NAACL Workshop on Statistical Machine Translation,</booktitle>
<pages>1--6</pages>
<marker>Povovic, de Gispert, Gupta, Lambert, Mariño, Federico, Ney, Banchs, 2006</marker>
<rawString>M. Povovic, A. de Gispert, D. Gupta, P. Lambert, J.B. Mariño, M. Federico, H. Ney, and R. Banchs. 2006. Morpho-syntactic information for automatic error analysis of statistic machine translation output. In In Proceeding of the HLT-NAACL Workshop on Statistical Machine Translation, pages 1–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>Alternative quantifier scope in ccg.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>301--308</pages>
<contexts>
<context position="6429" citStr="Steedman, 1999" startWordPosition="973" endWordPosition="974">ble, which is identified as shown in Koehn et al. (2003), and word alignment, which is generated by the grow-diag-final-and method (expanding the alignment by adding directly neighboring alignment points and alignment points in the diagonal neighborhood) (Och and Ney, 2003). Meanwhile, the target of the training corpus is parsed with Charniak’s parser (Charniak, 2000), and each phrase is annotated with the constituent that spans the target side of the rules. The set of non-terminals is extended by means of conditional and additive categories according to Combinatory Categorical Grammar (CCG) (Steedman, 1999). Under this approach, new rules can be formed. For example, RB+VB, can represent an additive constituent consisting of two synthetically generated adjacent categories 3, i.e., an adverb and a verb. Furthermore, DT�NP can indicate an incomplete noun phrase with a missing determiner to the left. The rule recursive generalization procedure coincides with the one proposed in Chiang (2005), but violates the restrictions introduced for singlecategory grammar; for example, rules that contain adjacent generalized elements are not discarded. Thus, each rule N ) f1 ... fm/e1 ... en can be extended by a</context>
</contexts>
<marker>Steedman, 1999</marker>
<rawString>M. Steedman. 1999. Alternative quantifier scope in ccg. In Proceedings of ACL 1999, pages 301–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Venugopal</author>
<author>A Zollmann</author>
<author>S Vogel</author>
</authors>
<title>An Efficient Two-Pass Approach to SynchronousCFG Driven Statistical MT.</title>
<date>2007</date>
<booktitle>In Proceedings of HLT/NAACL</booktitle>
<pages>500--507</pages>
<contexts>
<context position="16187" citStr="Venugopal et al. (2007)" startWordPosition="2565" endWordPosition="2568">asebased system were 4.8M, while a number of generalized rules representing the hierarchical model grew dramatically to 22.9M. 10.8M of them were pruned out on the filtering step. The vocabulary of the English Penn Treebank elementary non-terminals is 72, while a number of generalized elements, including additive and truncated categories, is 35.7K. The FastTranslateChart beam-search decoder was used as an engine of MER training aiming to tune the feature weight coefficients and produce final n-best and 1-best translations by combining the intensive search with a standard 4-gram LM as shown in Venugopal et al. (2007). The iteration limit was set to 10 with 1000-best list and the highest BLEU score as optimization criteria. We did not use completely abstract rules (without any source-side lexical utterance), since these rules significantly slow down the decoding process (noAllowAbstractRules option). Table 2 shows a summary of computational time and RAM needed at each step of the translation. Step Time Memory Parsing 1.5h 80Mb Rules extraction 10h 3.5Gb Filtering&amp;merging 3h 4.0Gb Weights tuning 40h 3Gb Testing 2h 3Gb Table 2: SAMT: Computational resources. Evaluation scores including results of system comb</context>
</contexts>
<marker>Venugopal, Zollmann, Vogel, 2007</marker>
<rawString>A. Venugopal, A. Zollmann, and S. Vogel. 2007. An Efficient Two-Pass Approach to SynchronousCFG Driven Statistical MT. In Proceedings of HLT/NAACL 2007, pages 500–507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Vilar</author>
<author>J Xu</author>
<author>L F D’Haro</author>
<author>H Ney</author>
</authors>
<title>Error Analysis of Machine Translation Output.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC’06,</booktitle>
<pages>697--702</pages>
<marker>Vilar, Xu, D’Haro, Ney, 2006</marker>
<rawString>D. Vilar, J. Xu, L. F. D’Haro, and H. Ney. 2006. Error Analysis of Machine Translation Output. In Proceedings of LREC’06, pages 697–702.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yamada</author>
<author>K Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>523--530</pages>
<contexts>
<context position="2574" citStr="Yamada and Knight, 2001" startWordPosition="371" endWordPosition="374">ich are composed of one or more words from the source language and zero or more words from the target one. The N-gram-based systems allow for linguistically motivated word reordering by implementing word order monotonization. Prior to the SMT revolution, a major part of MT systems was developed using rule-based algorithms; however, starting from the 1990’s, syntax-driven systems based on phrase hierarchy have gained popularity. A representative sample of modern syntax-based systems includes models based on bilingual synchronous grammar (Melamed, 2004), parse tree-to-string translation models (Yamada and Knight, 2001) and nonisomorphic tree-to-tree mappings (Eisner, 2003). The orthodox phrase-based model was enhanced in Chiang (2005), where a hierarchical phrase model allowing for multiple generalizations within each phrase was introduced. The open-source toolkit SAMT2 (Zollmann and Venugopal, 2006) is a further evolution of this approach, in which syntactic categories extracted from the target side parse tree are directly assigned to the hierarchically structured phrases. Several publications discovering similarities and differences between distinct translation models have been written over the last few y</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>K. Yamada and K. Knight. 2001. A syntax-based statistical translation model. In Proceedings of ACL 2001, pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Zollmann</author>
<author>A Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL</booktitle>
<contexts>
<context position="2861" citStr="Zollmann and Venugopal, 2006" startWordPosition="411" endWordPosition="415">was developed using rule-based algorithms; however, starting from the 1990’s, syntax-driven systems based on phrase hierarchy have gained popularity. A representative sample of modern syntax-based systems includes models based on bilingual synchronous grammar (Melamed, 2004), parse tree-to-string translation models (Yamada and Knight, 2001) and nonisomorphic tree-to-tree mappings (Eisner, 2003). The orthodox phrase-based model was enhanced in Chiang (2005), where a hierarchical phrase model allowing for multiple generalizations within each phrase was introduced. The open-source toolkit SAMT2 (Zollmann and Venugopal, 2006) is a further evolution of this approach, in which syntactic categories extracted from the target side parse tree are directly assigned to the hierarchically structured phrases. Several publications discovering similarities and differences between distinct translation models have been written over the last few years. In Crego et al. (2005b), the N-gram-based system is contrasted with a state-of-the-art phrase-based framework, while in DeNeefe et al. (2007), the authors seek to estimate the advantages, weakest points and possible overlap between syntaxbased MT and phrase-based SMT. In Zollmann </context>
<context position="4701" citStr="Zollmann and Venugopal, 2006" startWordPosition="696" endWordPosition="699">s. This problem is even more serious when the source, the target, or both languages are inflectional and rich in morphology. Moreover, phrasebased models are unable to cope with global reordering because the distortion model is based on movement distance, which may face computational resource limitations (Och and Ney, 2004). This problem was successfully addressed when the MT system based on generalized hierarchically structured phrases was introduced and discussed in Chiang (2005). It operates with only two markers (a substantial phrase category and &amp;quot;a glue marker&amp;quot;). Moreover, a recent work (Zollmann and Venugopal, 2006) reports significant improvement in terms of translation quality if complete or partial syntactic categories (derived from the target side parse tree) are assigned to the phrases. 2.1 Modeling A formalism for Syntax Augmented Translation is probabilistic synchronous context-free grammar (PSynCFG), which is defined in terms of source and target terminal sets and a set of non-terminals: X ) (-y, α, —, w) where X is a non-terminal, -y is a sequence of source-side terminals and non-terminals, α is a sequence of target-side terminals and non-terminals, — is a one-to-one mapping from non-terminal to</context>
<context position="9172" citStr="Zollmann and Venugopal (2006)" startWordPosition="1429" endWordPosition="1433">03); • counters of target words and rule applications; • binary features reflecting rule context (purely lexical and purely abstract, among others); • rule rareness and unbalancedness penalties. The decoding process can be represented as a search through the space of neg log probability of the target language terminals. The set of feature functions is combined with a finite-state target-side n-gram language model (LM), which is used to derive the target language sequence during a parsing decoding. The feature weights are optimized according to the highest BLEU score. For more details refer to Zollmann and Venugopal (2006). 3 UPC n-gram SMT system A description of the UPC-TALP N-gram translation system can be found in Mariño et al. (2006). SMT is based on the principle of translating a source sentence (f) into a sentence in the target language (e). The problem is formulated in terms of source and target languages; it is defined according to equation (1) and can be reformulated as selecting a translation with the highest probability from a set of target sentences (2): Figure 1: Example of SAMT and N-gram elements extraction. Figure 2: Example of SAMT generalized rules. 426 eI1 = arg max {1 p(eI1 |fJ1) eI 1 { 1 p</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>A. Zollmann and A. Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings of NAACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Zollmann</author>
<author>A Venugopal</author>
<author>F Och</author>
<author>J Ponte</author>
</authors>
<title>Systematic comparison of Phrase-based, Hierarchical and Syntax-Augmented Statistical mt.</title>
<date>2008</date>
<booktitle>In Proceedings of Coling</booktitle>
<pages>1145--1152</pages>
<contexts>
<context position="3474" citStr="Zollmann et al. (2008)" startWordPosition="504" endWordPosition="507">al, 2006) is a further evolution of this approach, in which syntactic categories extracted from the target side parse tree are directly assigned to the hierarchically structured phrases. Several publications discovering similarities and differences between distinct translation models have been written over the last few years. In Crego et al. (2005b), the N-gram-based system is contrasted with a state-of-the-art phrase-based framework, while in DeNeefe et al. (2007), the authors seek to estimate the advantages, weakest points and possible overlap between syntaxbased MT and phrase-based SMT. In Zollmann et al. (2008) the comparison of phrase-based, &amp;quot;Chiang’s style&amp;quot; hirearchical system and SAMT is pro2www.cs.cmu.edu/∼zollmann/samt Proceedings of the 12th Conference of the European Chapter of the ACL, pages 424–432, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 424 vided. In this study, we intend to compare the differences and similarities of the statistical N-grambased SMT approach and the SAMT system. The comparison is performed on a small Arabic-toEnglish translation task from the news domain. 2 SAMT system A criticism of phrase-based models is data sparseness.</context>
</contexts>
<marker>Zollmann, Venugopal, Och, Ponte, 2008</marker>
<rawString>A. Zollmann, A. Venugopal, F. Och, and J. Ponte. 2008. Systematic comparison of Phrase-based, Hierarchical and Syntax-Augmented Statistical mt. In Proceedings of Coling 2008, pages 1145–1152.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>