<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.988723666666667">
Efficient Minimum Error Rate Training and
Minimum Bayes-Risk Decoding for
Translation Hypergraphs and Lattices
</title>
<author confidence="0.99424">
Shankar Kumar1 and Wolfgang Macherey1 and Chris Dyer2 and Franz Och1
</author>
<affiliation confidence="0.945414">
1Google Inc.
</affiliation>
<address confidence="0.8191725">
1600 Amphitheatre Pkwy.
Mountain View, CA 94043, USA
</address>
<email confidence="0.99904">
{shankarkumar,wmach,och}@google.com
</email>
<sectionHeader confidence="0.997365" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999029090909091">
Minimum Error Rate Training (MERT)
and Minimum Bayes-Risk (MBR) decod-
ing are used in most current state-of-the-
art Statistical Machine Translation (SMT)
systems. The algorithms were originally
developed to work with N-best lists of
translations, and recently extended to lat-
tices that encode many more hypotheses
than typical N-best lists. We here extend
lattice-based MERT and MBR algorithms
to work with hypergraphs that encode a
vast number of translations produced by
MT systems based on Synchronous Con-
text Free Grammars. These algorithms
are more efficient than the lattice-based
versions presented earlier. We show how
MERT can be employed to optimize pa-
rameters for MBR decoding. Our exper-
iments show speedups from MERT and
MBR as well as performance improve-
ments from MBR decoding on several lan-
guage pairs.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9995965625">
Statistical Machine Translation (SMT) systems
have improved considerably by directly using the
error criterion in both training and decoding. By
doing so, the system can be optimized for the
translation task instead of a criterion such as like-
lihood that is unrelated to the evaluation met-
ric. Two popular techniques that incorporate the
error criterion are Minimum Error Rate Train-
ing (MERT) (Och, 2003) and Minimum Bayes-
Risk (MBR) decoding (Kumar and Byrne, 2004).
These two techniques were originally developed
for N-best lists of translation hypotheses and re-
cently extended to translation lattices (Macherey
et al., 2008; Tromble et al., 2008) generated by a
phrase-based SMT system (Och and Ney, 2004).
Translation lattices contain a significantly higher
</bodyText>
<affiliation confidence="0.9927915">
2Department of Linguistics
University of Maryland
</affiliation>
<address confidence="0.736739">
College Park, MD 20742, USA
</address>
<email confidence="0.991602">
redpony@umd.edu
</email>
<bodyText confidence="0.999855696969697">
number of translation alternatives relative to N-
best lists. The extension to lattices reduces the
runtimes for both MERT and MBR, and gives per-
formance improvements from MBR decoding.
SMT systems based on synchronous context
free grammars (SCFG) (Chiang, 2007; Zollmann
and Venugopal, 2006; Galley et al., 2006) have
recently been shown to give competitive perfor-
mance relative to phrase-based SMT. For these
systems, a hypergraph or packed forest provides a
compact representation for encoding a huge num-
ber of translation hypotheses (Huang, 2008).
In this paper, we extend MERT and MBR
decoding to work on hypergraphs produced by
SCFG-based MT systems. We present algorithms
that are more efficient relative to the lattice al-
gorithms presented in Macherey et al. (2008;
Tromble et al. (2008). Lattice MBR decoding uses
a linear approximation to the BLEU score (Pap-
ineni et al., 2001); the weights in this linear loss
are set heuristically by assuming that n-gram pre-
cisions decay exponentially with n. However, this
may not be optimal in practice. We employ MERT
to select these weights by optimizing BLEU score
on a development set.
A related MBR-inspired approach for hyper-
graphs was developed by Zhang and Gildea
(2008). In this work, hypergraphs were rescored to
maximize the expected count of synchronous con-
stituents in the translation. In contrast, our MBR
algorithm directly selects the hypothesis in the
hypergraph with the maximum expected approx-
imate corpus BLEU score (Tromble et al., 2008).
</bodyText>
<figureCaption confidence="0.996367">
Figure 1: An example hypergraph.
</figureCaption>
<figure confidence="0.622395583333333">
will soon announce
Suzuki
X1 announces
its future in
X1 X2
X1 its future in the
X1 its future in the
Rally World Championship
X1 X2
X1 X2
X1 X2
X1 X2
</figure>
<page confidence="0.986379">
163
</page>
<note confidence="0.999617">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 163–171,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<sectionHeader confidence="0.965107" genericHeader="method">
2 Translation Hypergraphs
</sectionHeader>
<bodyText confidence="0.976349148148148">
A translation lattice compactly encodes a large
number of hypotheses produced by a phrase-based
SMT system. The corresponding representation
for an SMT system based on SCFGs (e.g. Chi-
ang (2007), Zollmann and Venugopal (2006), Mi
et al. (2008)) is a directed hypergraph or a packed
forest (Huang, 2008).
Formally, a hypergraph is a pair 7-1 = (V, £)
consisting of a vertex set V and a set of hyperedges
£ C_ V* x V. Each hyperedge e E £ connects a
head vertex h(e) with a sequence of tail vertices
T(e) = {v1, ..., v,,,}. The number of tail vertices
is called the arity (|e|) of the hyperedge. If the ar-
ity of a hyperedge is zero, h(e) is called a source
vertex. The arity of a hypergraph is the maximum
arity of its hyperedges. A hyperedge of arity 1 is a
regular edge, and a hypergraph of arity 1 is a regu-
lar graph (lattice). Each hyperedge is labeled with
a rule re from the SCFG. The number of nontermi-
nals on the right-hand side of re corresponds with
the arity of e. An example without scores is shown
in Figure 1. A path in a translation hypergraph in-
duces a translation hypothesis E along with its se-
quence of SCFG rules D = r1, r2, ..., rK which,
if applied to the start symbol, derives E. The se-
quence of SCFG rules induced by a path is also
called a derivation tree for E.
</bodyText>
<sectionHeader confidence="0.98094" genericHeader="method">
3 Minimum Error Rate Training
</sectionHeader>
<bodyText confidence="0.9938196">
Given a set of source sentences F1� with corre-
sponding reference translations RS , the objective
of MERT is to find a parameter set �λm which min-
imizes an automated evaluation criterion under a
linear model:
</bodyText>
<equation confidence="0.997662142857143">
S
λM = arg min { Err`Rs, E(Fs; λM1 )´ i
al l s=1 111
 XS ff
ˆE(Fs; λM 1 ) = arg max λmhm(E, Fs) .
E
s=1
</equation>
<bodyText confidence="0.99994575">
In the context of statistical machine translation,
the optimization procedure was first described in
Och (2003) for N-best lists and later extended to
phrase-lattices in Macherey et al. (2008). The al-
gorithm is based on the insight that, under a log-
linear model, the cost function of any candidate
translation can be represented as a line in the plane
if the initial parameter set λM is shifted along a
direction dM . Let C = {E1, ..., EK} denote a set
of candidate translations, then computing the best
scoring translation hypothesis E� out of C results in
the following optimization problem:
</bodyText>
<equation confidence="0.843906555555556">
n o
ˆE(F; γ) = arg max (λM 1 + γ · dM 1 )&gt; · hM 1 (E, F)
E∈C
= arg max ˘a(E, F) + γ · b(E, F)
{z }
(∗)
ff
Hence, the total score (*) for each candidate trans-
lation E E C can be described as a line with
</equation>
<bodyText confidence="0.949555">
as the independent variable. For any particu-
</bodyText>
<subsectionHeader confidence="0.682811">
lar choice of
</subsectionHeader>
<bodyText confidence="0.957631333333333">
the decoder seeks that translation
which yields the largest score and therefore corre-
sponds to the topmost line segment. If
</bodyText>
<equation confidence="0.4225265">
is shifted
from
</equation>
<bodyText confidence="0.900388210526316">
to +oo, other translation hypotheses
may at some point constitute the topmost line seg-
ments and thus change the decision made by the
decoder. The entire sequence of topmost line seg-
ments is called upper envelope and provides an ex-
haustive representation of all possible outcomes
that the decoder may yield if
is shifted along
the chosen direction. Both the translations and
their corresponding line segments can efficiently
be computed without incorporating any error crite-
rion. Once the envelope has been determined, the
translation candidates of its constituent line seg-
ments are projected onto their corresponding error
counts, thus yielding the exact and unsmoothed er-
ror surface for all candidate translations encoded
in C. The error surface can now easily be traversed
in order to find that
under which the new param-
</bodyText>
<equation confidence="0.909454">
eter set
M +
</equation>
<bodyText confidence="0.93696412">
M minimizes the global error.
In this section, we present an extension of the
algorithm described in Macherey et al. (2008)
that allows us to efficiently compute and repre-
sent upper envelopes over all candidate transla-
tions encoded in hypergraphs. Conceptually, the
algorithm works by propagating (initially empty)
envelopes from the
source nodes
bottom-up to its unique root node, thereby ex-
panding the envelopes by applying SCFG rules to
the partial candidate translations that are associ-
ated with the
constituent line segments.
To recombine envelopes, we need two operators:
the sum and the maximum over convex polygons.
To illustrate which operator is applied when, we
transform 7-1 = (V, £) into a regular graph with
typed nodes by (1) marking all vertices v E V with
the symbol V and (2) replacing each hyperedge
e E £,
1, with a small subgraph consisting
of a new vertex
whose incoming and out-
going edges connect the same head an
</bodyText>
<equation confidence="0.977913464285714">
γ
γ,
γ
−oo
γ
γ�
λ
γ�·d
hypergraph’s
envelope’s
|e |&gt;
v∧(e)
d tail nodes
X
= arg max
λmhm(E, F)
E∈C
m
 |{z }
=a(E,F)
+ γ · X dmhm(E, F)
m
 |{z }
=b(E,F)
∈
E
C
¯
</equation>
<page confidence="0.950627">
164
</page>
<construct confidence="0.419725">
Algorithm 1 n-operation (Sum)
</construct>
<bodyText confidence="0.9218635">
input: associative map a: V --+ Env(V), hyperarc e
output: Minkowski sum of envelopes over T(e)
</bodyText>
<equation confidence="0.9539488">
for (i = 0; i &lt; |T(e)|; ++i) {
v = Ti(e);
pq.enqueue(( v, i, 0));
}
L = O;
D = ( e, ε1 ··· ε|e|)
while (!pq.empty()) {
( v, i, j) = pq.dequeue();
` = A[v][j];
D[i+1] = `.D;
if (L.empty() V L.back().x &lt; `.x) {
if (0 &lt; j) {
`.y += L.back().y - A[v][j-1].y;
`.m += L.back().m - A[v][j-1].m;
}
L.push_back(`);
L.back().D = D;
} else {
L.back().y += `.y;
L.back().m += `.m;
L.back().D[i+1] = `.D;
if (0 &lt; j) {
L.back().y -= A[v][j-1].y;
L.back().m -= A[v][j-1].m;
}
}
if (++j &lt; A[v].size())
pq.enqueue(( v, i, j));
}
return L;
</equation>
<bodyText confidence="0.969499296296296">
in the transformed graph as were connected by e
in the original graph. The unique outgoing edge
of v∧(e) is associated with the rule re; incoming
edges are not linked to any rule. Figure 2 illus-
trates the transformation for a hyperedge with ar-
ity 3. The graph transformation is isomorphic.
The rules associated with every hyperedge spec-
ify how line segments in the envelopes of a hyper-
edge’s tail nodes can be combined. Suppose we
have a hyperedge e with rule re : X —* aX1bX2c
and T(e) = {v1, v2}. Then we substitute X1 and
X2 in the rule with candidate translations associ-
ated with line segments in envelopes Env(v1) and
Env(v2) respectively.
To derive the algorithm, we consider the gen-
eral case of a hyperedge e with rule re : X —*
w1X1w2...wnXnwn+1. Because the right-hand
side of re has n nonterminals, the arity of e is
|e |= n. Let T(e) = {v1, ..., vn} denote the
tail nodes of e. We now assume that each tail
node vi E T(e) is associated with the upper en-
velope over all candidate translations that are in-
duced by derivations of the corresponding nonter-
minal symbol Xi. These envelopes shall be de-
Algorithm 2 V-operation (Max)
input: array L[0..K-1] containing line objects
output: upper envelope of L
</bodyText>
<equation confidence="0.9602742">
Sort(L:m);
j = 0; K = size(L);
for (i = 0; i &lt; K; ++i) {
` = L[i];
`.x = -00;
</equation>
<construct confidence="0.981309666666667">
if (0 &lt; j) {
if (L[j-1].m == `.m) {
if (`.y &lt;= L[j-1].y) continue;
</construct>
<equation confidence="0.8769625">
--j;
}
while (0 &lt; j) {
`.x = (`.y - L[j-1].y)/
(L[j-1].m - `.m);
if (L[j-1].x &lt; `.x) break;
--j;
}
if (0 == j) `.x = -00;
L[j++] = `;
} else L[j++] = `;
}
L.resize(j);
return L;
</equation>
<bodyText confidence="0.9998058">
noted by Env(vi). To decompose the problem of
computing and propagating the tail envelopes over
the hyperedge e to its head node, we now define
two operations, one for either node type, to spec-
ify how envelopes associated with the tail vertices
are propagated to the head vertex.
Nodes of Type “∧”: For a type n node, the
resulting envelope is the Minkowski sum over
the envelopes of the incoming edges (Berg et
al., 2008). Since the envelopes of the incoming
edges are convex hulls, the Minkowski sum pro-
vides an upper bound to the number of line seg-
ments that constitute the resulting envelope: the
bound is the sum over the number of line seg-
ments in the envelopes of the incoming edges, i.e.:
</bodyText>
<equation confidence="0.987511">
��Env(v∧(e))I G E ��Env(vv)��.
v�ET(e)
</equation>
<bodyText confidence="0.996843066666667">
Algorithm 1 shows the pseudo code for comput-
ing the Minkowski sum over multiple envelopes.
The line objects E used in this algorithm are
encoded as 4-tuples, each consisting of the x-
intercept with E’s left-adjacent line stored as E.x,
the slope E.m, the y-intercept E.y, and the (partial)
derivation tree E.D. At the beginning, the leftmost
line segment of each envelope is inserted into a
priority queue pq. The priority is defined in terms
of a line’s x-intercept such that lower values imply
higher priority. Hence, the priority queue enumer-
ates all line segments from left to right in ascend-
ing order of their x-intercepts, which is the order
needed to compute the Minkowski sum.
Nodes of Type “V”: The operation performed
</bodyText>
<page confidence="0.996566">
165
</page>
<figureCaption confidence="0.958225333333333">
Figure 2: Transformation of a hypergraph into
a factor graph and bottom-up propagation of en-
velopes.
</figureCaption>
<bodyText confidence="0.999656034482759">
at nodes of type “V” computes the convex hull
over the union of the envelopes propagated over
the incoming edges. This operation is a “max”
operation and it is identical to the algorithm de-
scribed in (Macherey et al., 2008) for phrase lat-
tices. Algorithm 2 contains the pseudo code.
The complete algorithm then works as follows:
Traversing all nodes in x bottom-up in topolog-
ical order, we proceed for each node v E V over
its incoming hyperedges and combine in each such
hyperedge e the envelopes associated with the tail
nodes T(e) by computing their sum according to
Algorithm 1 (n-operation). For each incoming
hyperedge e, the resulting envelope is then ex-
panded by applying the rule re to its constituent
line segments. The envelopes associated with dif-
ferent incoming hyperedges of node v are then
combined and reduced according to Algorithm 2
(V-operation). By construction, the envelope at
the root node is the convex hull over the line seg-
ments of all candidate translations that can be de-
rived from the hypergraph.
The suggested algorithm has similar properties
as the algorithm presented in (Macherey et al.,
2008). In particular, it has the same upper bound
on the number of line segments that constitute the
envelope at the root node, i.e, the size of this enve-
lope is guaranteed to be no larger than the number
of edges in the transformed hypergraph.
</bodyText>
<sectionHeader confidence="0.996258" genericHeader="method">
4 Minimum Bayes-Risk Decoding
</sectionHeader>
<bodyText confidence="0.962237285714286">
We first review Minimum Bayes-Risk (MBR) de-
coding for statistical MT. An MBR decoder seeks
the hypothesis with the least expected loss under a
probability model (Bickel and Doksum, 1977). If
we think of statistical MT as a classifier that maps
a source sentence F to a target sentence E, the
MBR decoder can be expressed as follows:
</bodyText>
<equation confidence="0.998616333333333">
E
E� = argmin L(E, E&apos;)P(E|F), (1)
E&apos;E9 EE9
</equation>
<bodyText confidence="0.999853888888889">
where L(E, E&apos;) is the loss between any two hy-
potheses E and E&apos;, P(E|F) is the probability
model, and 9 is the space of translations (N-best
list, lattice, or a hypergraph).
MBR decoding for translation can be performed
by reranking an N-best list of hypotheses gener-
ated by an MT system (Kumar and Byrne, 2004).
This reranking can be done for any sentence-
level loss function such as BLEU (Papineni et al.,
2001), Word Error Rate, or Position-independent
Error Rate.
Recently, Tromble et al. (2008) extended
MBR decoding to translation lattices under an
approximate BLEU score. They approximated
log(BLEU) score by a linear function of n-gram
matches and candidate length. If E and E&apos; are the
reference and the candidate translations respec-
tively, this linear function is given by:
</bodyText>
<equation confidence="0.993342">
G(E, E&apos;) = θ0|E&apos; |+ E θjwj#w(E&apos;)δw(E), (2)
w
</equation>
<bodyText confidence="0.999924523809524">
where w is an n-gram present in either E or E&apos;,
and θ0,θ1,..., θN are weights which are deter-
mined empirically, where N is the maximum n-
gram order.
Under such a linear decomposition, the MBR
decoder (Equation 1) can be written as
Tromble et al. (2008) implement the MBR
decoder using Weighted Finite State Automata
(WFSA) operations. First, the set of n-grams
is extracted from the lattice. Next, the posterior
probability of each n-gram is computed. A new
automaton is then created by intersecting each n-
gram with weight (from Equation 2) to an un-
weighted lattice. Finally, the MBR hypothesis is
extracted as the best path in the automaton. We
will refer to this procedure as FSAMBR.
The above steps are carried out one n-gram at
a time. For a moderately large lattice, there can
be several thousands of n-grams and the proce-
dure becomes expensive. We now present an alter-
nate approximate procedure which can avoid this
</bodyText>
<equation confidence="0.9875188">
= max
=E
E = argmax E θjwj#w(E&apos;)p(w|9), (3)
E&apos;E9 θ0|E&apos; |+
w
</equation>
<bodyText confidence="0.99964">
where the posterior probability of an n-gram in the
lattice is given by
</bodyText>
<equation confidence="0.9885945">
p(w|9) = E 1w(E)P(E|F). (4)
EE9
</equation>
<page confidence="0.986249">
166
</page>
<bodyText confidence="0.999251">
enumeration making the resulting algorithm much
faster than FSAMBR.
</bodyText>
<subsectionHeader confidence="0.976119">
4.1 Efficient MBR for lattices
</subsectionHeader>
<bodyText confidence="0.953787666666667">
The key idea behind this new algorithm is to
rewrite the n-gram posterior probability (Equa-
tion 4) as follows:
</bodyText>
<equation confidence="0.991164666666667">
E
p(w|G) =
EE9
</equation>
<bodyText confidence="0.923785842105263">
where f(e, w, E) is a score assigned to edge e on
path E containing n-gram w:
{ 1 w ∈ e,p(e|G) &gt; p(e&apos;|G),
e&apos; precedes e on E
0 otherwise
In other words, for each path E, we count the edge
that contributes n-gram w and has the highest edge
posterior probability relative to its predecessors on
the path E; there is exactly one such edge on each
lattice path E.
We note that f(e, w, E) relies on the full path
E which means that it cannot be computed based
on local statistics. We therefore approximate the
quantity f(e, w, E) with f*(e, w,G) that counts
the edge e with n-gram w that has the highest arc
posterior probability relative to predecessors in the
entire lattice G. f*(e, w,G) can be computed lo-
cally, and the n-gram posterior probability based
on f* can be determined as follows:
</bodyText>
<equation confidence="0.98284125">
f*(e, w, G)P(E|F) (7)
*/
1wEef \e, w, G) E
EE9
</equation>
<bodyText confidence="0.675092">
Algorithm 3 MBR Decoding on Lattices
</bodyText>
<listItem confidence="0.988013875">
1: Sort the lattice nodes topologically.
2: Compute backward probabilities of each node.
3: Compute posterior prob. of each n-gram:
4: for each edge e do
5: Compute edge posterior probability P(e|G).
6: Compute n-gram posterior probs. P(w|G):
7: for each n-gram w introduced by e do
8: Propagate n − 1 gram suffix to he.
</listItem>
<figure confidence="0.888250818181818">
9: if p(e|G) &gt; Score(w, T(e)) then
10: Update posterior probs. and scores:
p(w|G) += p(e|G) − Score(w, T(e)).
Score(w, he) = p(e|G).
11: else
12: Score(w, he) = Score(w, T(e)).
13: end if
14: end for
15: end for
16: Assign scores to edges (given by Equation 3).
17: Find best path in the lattice (Equation 3).
</figure>
<bodyText confidence="0.9985440625">
(Algorithm 3). However, there are important dif-
ferences when computing the n-gram posterior
probabilities (Step 3). In this inside pass, we now
maintain both n-gram prefixes and suffixes (up to
the maximum order −1) on each hypergraph node.
This is necessary because unlike a lattice, new n-
grams may be created at subsequent nodes by con-
catenating words both to the left and the right side
of the n-gram. When the arity of the edge is 2,
a rule has the general form aX1bX2c, where X1
and X2 are sequences from tail nodes. As a result,
we need to consider all new sequences which can
be created by the cross-product of the n-grams on
the two tail nodes. E.g. if X1 = {c, cd, d} and
X2 = {f, g}, then a total of six sequences will
result. In practice, such a cross-product is not pro-
</bodyText>
<equation confidence="0.998620357142857">
E f(e, w, E)P(E|F) (5)
eEE
f(e, w, E) =
(6)
E
p(w|G) =
EE9
E
eEE
E=
eES
1E(e)P(E|F)
E= 1wEef*(e, w, G)P(e|G),
eES
</equation>
<bodyText confidence="0.999937181818182">
where P(e|G) is the posterior probability of a lat-
tice edge. The algorithm to perform Lattice MBR
is given in Algorithm 3. For each node t in the lat-
tice, we maintain a quantity Score(w, t) for each
n-gram w that lies on a path from the source node
to t. Score(w, t) is the highest posterior probabil-
ity among all edges on the paths that terminate on t
and contain n-gram w. The forward pass requires
computing the n-grams introduced by each edge;
to do this, we propagate n-grams (up to maximum
order −1) terminating on each node.
</bodyText>
<subsectionHeader confidence="0.989582">
4.2 Extension to Hypergraphs
</subsectionHeader>
<bodyText confidence="0.973584">
We next extend the Lattice MBR decoding algo-
rithm (Algorithm 3) to rescore hypergraphs pro-
duced by a SCFG based MT system. Algorithm 4
is an extension to the MBR decoder on lattices
Algorithm 4 MBR Decoding on Hypergraphs
</bodyText>
<listItem confidence="0.997330363636364">
1: Sort the hypergraph nodes topologically.
2: Compute inside probabilities of each node.
3: Compute posterior prob. of each hyperedge P(e|G).
4: Compute posterior prob. of each n-gram:
5: for each hyperedge e do
6: Merge the n-grams on the tail nodes T(e). If the
same n-gram is present on multiple tail nodes, keep
the highest score.
7: Apply the rule on e to the n-grams on T(e).
8: Propagate n − 1 gram prefixes/suffixes to he.
9: for each n-gram w introduced by this hyperedge do
</listItem>
<figure confidence="0.416262">
10: if p(e|G) &gt; Score(w, T(e)) then
11: p(w|G) += p(e|G) − Score(w, T(e))
Score(w, he) = p(e|G)
12: else
13: Score(w, he) = Score(w, T(e))
14: end if
15: end for
16: end for
</figure>
<page confidence="0.882114333333333">
17: Assign scores to hyperedges (Equation 3).
18: Find best path in the hypergraph (Equation 3).
167
</page>
<bodyText confidence="0.9999876">
hibitive when the maximum n-gram order in MBR
does not exceed the order of the n-gram language
model used in creating the hypergraph. In the lat-
ter case, we will have a small set of unique prefixes
and suffixes on the tail nodes.
</bodyText>
<sectionHeader confidence="0.990024" genericHeader="method">
5 MERT for MBR Parameter
Optimization
</sectionHeader>
<bodyText confidence="0.997756111111111">
Lattice MBR Decoding (Equation 3) assumes a
linear form for the gain function (Equation 2).
This linear function contains n + 1 parameters
B0, B1, ..., BN, where N is the maximum order of
the n-grams involved. Tromble et al. (2008) ob-
tained these factors as a function of n-gram preci-
sions derived from multiple training runs. How-
ever, this does not guarantee that the resulting
linear score (Equation 2) is close to the corpus
BLEU. We now describe how MERT can be used
to estimate these factors to achieve a better ap-
proximation to the corpus BLEU.
We recall that MERT selects weights in a lin-
ear model to optimize an error criterion (e.g. cor-
pus BLEU) on a training set. The lattice MBR
decoder (Equation 3) can be written as a lin-
ear model: E� = argmaxE,Eg �Ni=0 Bigi(E&apos;, F),
where g0(E&apos;,F) = IE&apos;I and gi(E&apos;, F) =
</bodyText>
<equation confidence="0.9089085">
E
w:|w|=i #w(E&apos;)p(w|9).
</equation>
<bodyText confidence="0.999978666666667">
The linear approximation to BLEU may not
hold in practice for unseen test sets or language-
pairs. Therefore, we would like to allow the de-
coder to backoff to the MAP translation in such
cases. To do that, we introduce an additional fea-
ture function gN+1(E, F) equal to the original de-
coder cost for this sentence. A weight assignment
of 1.0 for this feature function and zeros for the
other feature functions would imply that the MAP
translation is chosen. We now have a total of N+2
feature functions which we optimize using MERT
to obtain highest BLEU score on a training set.
</bodyText>
<sectionHeader confidence="0.998984" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999845">
We now describe our experiments to evaluate
MERT and MBR on lattices and hypergraphs, and
show how MERT can be used to tune MBR pa-
rameters.
</bodyText>
<subsectionHeader confidence="0.998879">
6.1 Translation Tasks
</subsectionHeader>
<bodyText confidence="0.99897425">
We report results on two tasks. The first one is
the constrained data track of the NIST Arabic-
to-English (aren) and Chinese-to-English (zhen)
translation task1. On this task, the parallel and the
</bodyText>
<footnote confidence="0.981416">
1http://www.nist.gov/speech/tests/mt
</footnote>
<table confidence="0.8297612">
Dataset # of sentences
aren zhen
dev 1797 1664
nist02 1043 878
nist03 663 919
</table>
<tableCaption confidence="0.999827">
Table 1: Statistics over the NIST dev/test sets.
</tableCaption>
<bodyText confidence="0.999131571428572">
monolingual data included all the allowed train-
ing sets for the constrained track. Table 1 reports
statistics computed over these data sets. Our de-
velopment set (dev) consists of the NIST 2005 eval
set; we use this set for optimizing MBR parame-
ters. We report results on NIST 2002 and NIST
2003 evaluation sets.
The second task consists of systems for 39
language-pairs with English as the target language
and trained on at most 300M word tokens mined
from the web and other published sources. The de-
velopment and test sets for this task are randomly
selected sentences from the web, and contain 5000
and 1000 sentences respectively.
</bodyText>
<subsectionHeader confidence="0.999362">
6.2 MT System Description
</subsectionHeader>
<bodyText confidence="0.999987944444445">
Our phrase-based statistical MT system is simi-
lar to the alignment template system described in
(Och and Ney, 2004; Tromble et al., 2008). Trans-
lation is performed using a standard dynamic pro-
gramming beam-search decoder (Och and Ney,
2004) using two decoding passes. The first de-
coder pass generates either a lattice or an N-best
list. MBR decoding is performed in the second
pass.
We also train two SCFG-based MT systems:
a hierarchical phrase-based SMT (Chiang, 2007)
system and a syntax augmented machine transla-
tion (SAMT) system using the approach described
in Zollmann and Venugopal (2006). Both systems
are built on top of our phrase-based systems. In
these systems, the decoder generates an initial hy-
pergraph or an N-best list, which are then rescored
using MBR decoding.
</bodyText>
<subsectionHeader confidence="0.998841">
6.3 MERT Results
</subsectionHeader>
<bodyText confidence="0.9999655">
Table 2 shows runtime experiments for the hyper-
graph MERT implementation in comparison with
the phrase-lattice implementation on both the aren
and the zhen system. The first two columns show
the average amount of time in msecs that either
algorithm requires to compute the upper envelope
when applied to phrase lattices. Compared to the
algorithm described in (Macherey et al., 2008)
which is optimized for phrase lattices, the hyper-
graph implementation causes a small increase in
</bodyText>
<page confidence="0.995218">
168
</page>
<table confidence="0.9997364">
Avg. Runtime/sent [msec]
(Macherey 2008) Suggested Alg.
aren zhen aren zhen
phrase lattice 8.57 7.91 10.30 8.65
hypergraph – – 8.19 8.11
</table>
<tableCaption confidence="0.99935">
Table 2: Average time for computing envelopes.
</tableCaption>
<bodyText confidence="0.999977909090909">
running time. This increase is mainly due to the
representation of line segments; while the phrase-
lattice implementation stores a single backpointer,
the hypergraph version stores a vector of back-
pointers.
The last two columns show the average amount
of time that is required to compute the upper en-
velope on hypergraphs. For comparison, we prune
hypergraphs to the same density (# of edges per
edge on the best path) and achieve identical run-
ning times for computing the error surface.
</bodyText>
<subsectionHeader confidence="0.994397">
6.4 MBR Results
</subsectionHeader>
<bodyText confidence="0.99995019047619">
We first compare the new lattice MBR (Algo-
rithm 3) with MBR decoding on 1000-best lists
and FSAMBR (Tromble et al., 2008) on lattices
generated by the phrase-based systems; evaluation
is done using both BLEU and average run-time per
sentence (Table 3). Note that N-best MBR uses
a sentence BLEU loss function. The new lattice
MBR algorithm gives about the same performance
as FSAMBR while yielding a 20X speedup.
We next report the performance of MBR on hy-
pergraphs generated by Hiero/SAMT systems. Ta-
ble 4 compares Hypergraph MBR (HGMBR) with
MAP and MBR decoding on 1000 best lists. On
some systems such as the Arabic-English SAMT,
the gains from Hypergraph MBR over 1000-best
MBR are significant. In other cases, Hypergraph
MBR performs at least as well as N-best MBR.
In all cases, we observe a 7X speedup in run-
time. This shows the usefulness of Hypergraph
MBR decoding as an efficient alternative to N-
best MBR.
</bodyText>
<subsectionHeader confidence="0.980026">
6.5 MBR Parameter Tuning with MERT
</subsectionHeader>
<bodyText confidence="0.9989911">
We now describe the results by tuning MBR n-
gram parameters (Equation 2) using MERT. We
first compute N + 1 MBR feature functions on
each edge of the lattice/hypergraph. We also in-
clude the total decoder cost on the edge as as addi-
tional feature function. MERT is then performed
to optimize the BLEU score on a development set;
For MERT, we use 40 random initial parameters as
well as parameters computed using corpus based
statistics (Tromble et al., 2008).
</bodyText>
<table confidence="0.999682375">
BLEU (%) Avg.
aren zhen time
nist03 nist02 nist03 nist02 (ms.)
MAP 54.2 64.2 40.1 39.0 -
N-best MBR 54.3 64.5 40.2 39.2 3.7
Lattice MBR
FSAMBR 54.9 65.2 40.6 39.5 3.7
LatMBR 54.8 65.2 40.7 39.4 0.2
</table>
<tableCaption confidence="0.987687">
Table 3: Lattice MBR for a phrase-based system.
</tableCaption>
<table confidence="0.999977090909091">
BLEU (%) Avg.
aren zhen time
nist03 nist02 nist03 nist02 (ms.)
Hiero
MAP 52.8 62.9 41.0 39.8 -
N-best MBR 53.2 63.0 41.0 40.1 3.7
HGMBR 53.3 63.1 41.0 40.2 0.5
SAMT
MAP 53.4 63.9 41.3 40.3 -
N-best MBR 53.8 64.3 41.7 41.1 3.7
HGMBR 54.0 64.6 41.8 41.1 0.5
</table>
<tableCaption confidence="0.999803">
Table 4: Hypergraph MBR for Hiero/SAMT systems.
</tableCaption>
<bodyText confidence="0.99931340625">
Table 5 shows results for NIST systems. We
report results on nist03 set and present three sys-
tems for each language pair: phrase-based (pb),
hierarchical (hier), and SAMT; Lattice MBR is
done for the phrase-based system while HGMBR
is used for the other two. We select the MBR
scaling factor (Tromble et al., 2008) based on the
development set; it is set to 0.1, 0.01, 0.5, 0.2, 0.5
and 1.0 for the aren-phrase, aren-hier, aren-samt,
zhen-phrase zhen-hier and zhen-samt systems re-
spectively. For the multi-language case, we train
phrase-based systems and perform lattice MBR
for all language pairs. We use a scaling factor of
0.7 for all pairs. Additional gains can be obtained
by tuning this factor; however, we do not explore
that dimension in this paper. In all cases, we prune
the lattices/hypergraphs to a density of 30 using
forward-backward pruning (Sixtus and Ortmanns,
1999).
We consider a BLEU score difference to be a)
gain if is at least 0.2 points, b) drop if it is at most
-0.2 points, and c) no change otherwise. The re-
sults are shown in Table 6. In both tables, the fol-
lowing results are reported: Lattice/HGMBR with
default parameters (−5,1.5, 2, 3, 4) computed us-
ing corpus statistics (Tromble et al., 2008),
Lattice/HGMBR with parameters derived from
MERT both without/with the baseline model cost
feature (mert−b, mert+b). For multi-language
systems, we only show the # of language-pairs
with gains/no-changes/drops for each MBR vari-
ant with respect to the MAP translation.
</bodyText>
<page confidence="0.996297">
169
</page>
<bodyText confidence="0.999968813953489">
We observed in the NIST systems that MERT
resulted in short translations relative to MAP on
the unseen test set. To prevent this behavior,
we modify the MERT error criterion to include
a sentence-level brevity scorer with parameter α:
BLEU+brevity(α). This brevity scorer penalizes
each candidate translation that is shorter than the
average length over its reference translations, us-
ing a penalty term which is linear in the difference
between either length. We tune α on the develop-
ment set so that the brevity score of MBR transla-
tion is close to that of the MAP translation.
In the NIST systems, MERT yields small im-
provements on top of MBR with default param-
eters. This is the case for Arabic-English Hi-
ero/SAMT. In all other cases, we see no change
or even a slight degradation due to MERT.
We hypothesize that the default MBR parame-
ters (Tromble et al., 2008) are well tuned. There-
fore there is little gain by additional tuning using
MERT.
In the multi-language systems, the results show
a different trend. We observe that MBR with de-
fault parameters results in gains on 18 pairs, no
differences on 9 pairs, and losses on 12 pairs.
When we optimize MBR features with MERT, the
number of language pairs with gains/no changes/-
drops is 22/5/12. Thus, MERT has a bigger impact
here than in the NIST systems. We hypothesize
that the default MBR parameters are sub-optimal
for some language pairs and that MERT helps to
find better parameter settings. In particular, MERT
avoids the need for manually tuning these param-
eters by language pair.
Finally, when baseline model costs are added
as an extra feature (mert+b), the number of pairs
with gains/no changes/drops is 26/8/5. This shows
that this feature can allow MBR decoding to back-
off to the MAP translation. When MBR does not
produce a higher BLEU score relative to MAP
on the development set, MERT assigns a higher
weight to this feature function. We see such an
effect for 4 systems.
</bodyText>
<sectionHeader confidence="0.999869" genericHeader="conclusions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.718647625">
We have presented efficient algorithms
which extend previous work on lattice-based
MERT (Macherey et al., 2008) and MBR de-
coding (Tromble et al., 2008) to work with
hypergraphs. Our new MERT algorithm can work
with both lattices and hypergraphs. On lattices, it
achieves similar run-times as the implementation
System BLEU (%)
MAP MBR
default mert-b mert+b
aren.pb 54.2 54.8 54.8 54.9
aren.hier 52.8 53.3 53.5 53.7
aren.samt 53.4 54.0 54.4 54.0
zhen.pb 40.1 40.7 40.7 40.9
zhen.hier 41.0 41.0 41.0 41.0
zhen.samt 41.3 41.8 41.6 41.7
</bodyText>
<tableCaption confidence="0.985179">
Table 5: MBR Parameter Tuning on NIST systems
</tableCaption>
<table confidence="0.998738">
MBR wrt. MAP default mert-b mert+b
# of gains 18 22 26
# of no-changes 9 5 8
# of drops 12 12 5
</table>
<tableCaption confidence="0.998796">
Table 6: MBR on Multi-language systems.
</tableCaption>
<bodyText confidence="0.999767852941176">
described in Macherey et al. (2008). The new
Lattice MBR decoder achieves a 20X speedup
relative to either FSAMBR implementation
described in Tromble et al. (2008) or MBR on
1000-best lists. The algorithm gives comparable
results relative to FSAMBR. On hypergraphs
produced by Hierarchical and Syntax Augmented
MT systems, our MBR algorithm gives a 7X
speedup relative to 1000-best MBR while giving
comparable or even better performance.
Lattice MBR decoding is obtained under a lin-
ear approximation to BLEU, where the weights
are obtained using n-gram precisions derived from
development data. This may not be optimal in
practice for unseen test sets and language pairs,
and the resulting linear loss may be quite differ-
ent from the corpus level BLEU. In this paper, we
have described how MERT can be employed to
estimate the weights for the linear loss function
to maximize BLEU on a development set. On an
experiment with 40 language pairs, we obtain im-
provements on 26 pairs, no difference on 8 pairs
and drops on 5 pairs. This was achieved with-
out any need for manual tuning for each language
pair. The baseline model cost feature helps the al-
gorithm effectively back off to the MAP transla-
tion in language pairs where MBR features alone
would not have helped.
MERT and MBR decoding are popular tech-
niques for incorporating the final evaluation met-
ric into the development of SMT systems. We be-
lieve that our efficient algorithms will make them
more widely applicable in both SCFG-based and
phrase-based MT systems.
</bodyText>
<page confidence="0.994667">
170
</page>
<sectionHeader confidence="0.998338" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999879489361702">
M. Berg, O. Cheong, M. Krefeld, and M. Overmars,
2008. Computational Geometry: Algorithms and
Applications, chapter 13, pages 290–296. Springer-
Verlag, 3rd edition.
P. J. Bickel and K. A. Doksum. 1977. Mathematical
Statistics: Basic Ideas and Selected topics. Holden-
Day Inc., Oakland, CA, USA.
D. Chiang. 2007. Hierarchical phrase based transla-
tion . Computational Linguistics, 33(2):201 – 228.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable Inference
and Training of Context-Rich Syntactic Translation
Models.. In COLING/ACL, Sydney, Australia.
L. Huang. 2008. Advanced Dynamic Programming
in Semiring and Hypergraph Frameworks. In COL-
ING, Manchester, UK.
S. Kumar and W. Byrne. 2004. Minimum Bayes-
Risk Decoding for Statistical Machine Translation.
In HLT-NAACL, Boston, MA, USA.
W. Macherey, F. Och, I. Thayer, and J. Uszkoreit.
2008. Lattice-based Minimum Error Rate Train-
ing for Statistical Machine Translation. In EMNLP,
Honolulu, Hawaii, USA.
H. Mi, L. Huang, and Q. Liu. 2008. Forest-Based
Translation. In ACL, Columbus, OH, USA.
F. Och and H. Ney. 2004. The Alignment Template
Approach to Statistical Machine Translation. Com-
putational Linguistics, 30(4):417 – 449.
F. Och. 2003. Minimum Error Rate Training in Statis-
tical Machine Translation. In ACL, Sapporo, Japan.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001.
Bleu: a Method for Automatic Evaluation of Ma-
chine Translation. Technical Report RC22176
(W0109-022), IBM Research Division.
A. Sixtus and S. Ortmanns. 1999. High Quality
Word Graphs Using Forward-Backward Pruning. In
ICASSP, Phoenix, AZ, USA.
R. Tromble, S. Kumar, F. Och, and W. Macherey. 2008.
Lattice Minimum Bayes-Risk Decoding for Statis-
tical Machine Translation. In EMNLP, Honolulu,
Hawaii.
H. Zhang and D. Gildea. 2008. Efficient Multi-pass
Decoding for Synchronous Context Free Grammars.
In ACL, Columbus, OH, USA.
A. Zollmann and A. Venugopal. 2006. Syntax Aug-
mented Machine Translation via Chart Parsing. In
HLT-NAACL, New York, NY, USA.
</reference>
<page confidence="0.998205">
171
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.395654">
<title confidence="0.8748235">Efficient Minimum Error Rate Training and Minimum Bayes-Risk Decoding for Translation Hypergraphs and Lattices and and and</title>
<affiliation confidence="0.873552">Inc.</affiliation>
<address confidence="0.968243">1600 Amphitheatre Pkwy. Mountain View, CA 94043, USA</address>
<abstract confidence="0.997991826086956">Minimum Error Rate Training (MERT) and Minimum Bayes-Risk (MBR) decoding are used in most current state-of-theart Statistical Machine Translation (SMT) systems. The algorithms were originally to work with lists of translations, and recently extended to lattices that encode many more hypotheses typical lists. We here extend lattice-based MERT and MBR algorithms to work with hypergraphs that encode a vast number of translations produced by MT systems based on Synchronous Context Free Grammars. These algorithms are more efficient than the lattice-based versions presented earlier. We show how MERT can be employed to optimize parameters for MBR decoding. Our experiments show speedups from MERT and MBR as well as performance improvements from MBR decoding on several language pairs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Berg</author>
<author>O Cheong</author>
<author>M Krefeld</author>
<author>M Overmars</author>
</authors>
<date>2008</date>
<booktitle>Computational Geometry: Algorithms and Applications, chapter 13,</booktitle>
<pages>290--296</pages>
<publisher>SpringerVerlag,</publisher>
<note>3rd edition.</note>
<contexts>
<context position="11011" citStr="Berg et al., 2008" startWordPosition="1939" endWordPosition="1942">j-1].y) continue; --j; } while (0 &lt; j) { `.x = (`.y - L[j-1].y)/ (L[j-1].m - `.m); if (L[j-1].x &lt; `.x) break; --j; } if (0 == j) `.x = -00; L[j++] = `; } else L[j++] = `; } L.resize(j); return L; noted by Env(vi). To decompose the problem of computing and propagating the tail envelopes over the hyperedge e to its head node, we now define two operations, one for either node type, to specify how envelopes associated with the tail vertices are propagated to the head vertex. Nodes of Type “∧”: For a type n node, the resulting envelope is the Minkowski sum over the envelopes of the incoming edges (Berg et al., 2008). Since the envelopes of the incoming edges are convex hulls, the Minkowski sum provides an upper bound to the number of line segments that constitute the resulting envelope: the bound is the sum over the number of line segments in the envelopes of the incoming edges, i.e.: ��Env(v∧(e))I G E ��Env(vv)��. v�ET(e) Algorithm 1 shows the pseudo code for computing the Minkowski sum over multiple envelopes. The line objects E used in this algorithm are encoded as 4-tuples, each consisting of the xintercept with E’s left-adjacent line stored as E.x, the slope E.m, the y-intercept E.y, and the (partia</context>
</contexts>
<marker>Berg, Cheong, Krefeld, Overmars, 2008</marker>
<rawString>M. Berg, O. Cheong, M. Krefeld, and M. Overmars, 2008. Computational Geometry: Algorithms and Applications, chapter 13, pages 290–296. SpringerVerlag, 3rd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Bickel</author>
<author>K A Doksum</author>
</authors>
<title>Mathematical Statistics: Basic Ideas and Selected topics. HoldenDay Inc.,</title>
<date>1977</date>
<location>Oakland, CA, USA.</location>
<contexts>
<context position="13739" citStr="Bickel and Doksum, 1977" startWordPosition="2402" endWordPosition="2405">all candidate translations that can be derived from the hypergraph. The suggested algorithm has similar properties as the algorithm presented in (Macherey et al., 2008). In particular, it has the same upper bound on the number of line segments that constitute the envelope at the root node, i.e, the size of this envelope is guaranteed to be no larger than the number of edges in the transformed hypergraph. 4 Minimum Bayes-Risk Decoding We first review Minimum Bayes-Risk (MBR) decoding for statistical MT. An MBR decoder seeks the hypothesis with the least expected loss under a probability model (Bickel and Doksum, 1977). If we think of statistical MT as a classifier that maps a source sentence F to a target sentence E, the MBR decoder can be expressed as follows: E E� = argmin L(E, E&apos;)P(E|F), (1) E&apos;E9 EE9 where L(E, E&apos;) is the loss between any two hypotheses E and E&apos;, P(E|F) is the probability model, and 9 is the space of translations (N-best list, lattice, or a hypergraph). MBR decoding for translation can be performed by reranking an N-best list of hypotheses generated by an MT system (Kumar and Byrne, 2004). This reranking can be done for any sentencelevel loss function such as BLEU (Papineni et al., 2001</context>
</contexts>
<marker>Bickel, Doksum, 1977</marker>
<rawString>P. J. Bickel and K. A. Doksum. 1977. Mathematical Statistics: Basic Ideas and Selected topics. HoldenDay Inc., Oakland, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Hierarchical phrase based translation .</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<pages>228</pages>
<contexts>
<context position="2236" citStr="Chiang, 2007" startWordPosition="334" endWordPosition="335">y developed for N-best lists of translation hypotheses and recently extended to translation lattices (Macherey et al., 2008; Tromble et al., 2008) generated by a phrase-based SMT system (Och and Ney, 2004). Translation lattices contain a significantly higher 2Department of Linguistics University of Maryland College Park, MD 20742, USA redpony@umd.edu number of translation alternatives relative to Nbest lists. The extension to lattices reduces the runtimes for both MERT and MBR, and gives performance improvements from MBR decoding. SMT systems based on synchronous context free grammars (SCFG) (Chiang, 2007; Zollmann and Venugopal, 2006; Galley et al., 2006) have recently been shown to give competitive performance relative to phrase-based SMT. For these systems, a hypergraph or packed forest provides a compact representation for encoding a huge number of translation hypotheses (Huang, 2008). In this paper, we extend MERT and MBR decoding to work on hypergraphs produced by SCFG-based MT systems. We present algorithms that are more efficient relative to the lattice algorithms presented in Macherey et al. (2008; Tromble et al. (2008). Lattice MBR decoding uses a linear approximation to the BLEU sco</context>
<context position="4044" citStr="Chiang (2007)" startWordPosition="631" endWordPosition="633">proximate corpus BLEU score (Tromble et al., 2008). Figure 1: An example hypergraph. will soon announce Suzuki X1 announces its future in X1 X2 X1 its future in the X1 its future in the Rally World Championship X1 X2 X1 X2 X1 X2 X1 X2 163 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 163–171, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP 2 Translation Hypergraphs A translation lattice compactly encodes a large number of hypotheses produced by a phrase-based SMT system. The corresponding representation for an SMT system based on SCFGs (e.g. Chiang (2007), Zollmann and Venugopal (2006), Mi et al. (2008)) is a directed hypergraph or a packed forest (Huang, 2008). Formally, a hypergraph is a pair 7-1 = (V, £) consisting of a vertex set V and a set of hyperedges £ C_ V* x V. Each hyperedge e E £ connects a head vertex h(e) with a sequence of tail vertices T(e) = {v1, ..., v,,,}. The number of tail vertices is called the arity (|e|) of the hyperedge. If the arity of a hyperedge is zero, h(e) is called a source vertex. The arity of a hypergraph is the maximum arity of its hyperedges. A hyperedge of arity 1 is a regular edge, and a hypergraph of ari</context>
<context position="23366" citStr="Chiang, 2007" startWordPosition="4124" endWordPosition="4125">t sets for this task are randomly selected sentences from the web, and contain 5000 and 1000 sentences respectively. 6.2 MT System Description Our phrase-based statistical MT system is similar to the alignment template system described in (Och and Ney, 2004; Tromble et al., 2008). Translation is performed using a standard dynamic programming beam-search decoder (Och and Ney, 2004) using two decoding passes. The first decoder pass generates either a lattice or an N-best list. MBR decoding is performed in the second pass. We also train two SCFG-based MT systems: a hierarchical phrase-based SMT (Chiang, 2007) system and a syntax augmented machine translation (SAMT) system using the approach described in Zollmann and Venugopal (2006). Both systems are built on top of our phrase-based systems. In these systems, the decoder generates an initial hypergraph or an N-best list, which are then rescored using MBR decoding. 6.3 MERT Results Table 2 shows runtime experiments for the hypergraph MERT implementation in comparison with the phrase-lattice implementation on both the aren and the zhen system. The first two columns show the average amount of time in msecs that either algorithm requires to compute th</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>D. Chiang. 2007. Hierarchical phrase based translation . Computational Linguistics, 33(2):201 – 228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>J Graehl</author>
<author>K Knight</author>
<author>D Marcu</author>
<author>S DeNeefe</author>
<author>W Wang</author>
<author>I Thayer</author>
</authors>
<title>Scalable Inference and Training of Context-Rich Syntactic Translation Models.. In COLING/ACL,</title>
<date>2006</date>
<location>Sydney, Australia.</location>
<contexts>
<context position="2288" citStr="Galley et al., 2006" startWordPosition="340" endWordPosition="343">hypotheses and recently extended to translation lattices (Macherey et al., 2008; Tromble et al., 2008) generated by a phrase-based SMT system (Och and Ney, 2004). Translation lattices contain a significantly higher 2Department of Linguistics University of Maryland College Park, MD 20742, USA redpony@umd.edu number of translation alternatives relative to Nbest lists. The extension to lattices reduces the runtimes for both MERT and MBR, and gives performance improvements from MBR decoding. SMT systems based on synchronous context free grammars (SCFG) (Chiang, 2007; Zollmann and Venugopal, 2006; Galley et al., 2006) have recently been shown to give competitive performance relative to phrase-based SMT. For these systems, a hypergraph or packed forest provides a compact representation for encoding a huge number of translation hypotheses (Huang, 2008). In this paper, we extend MERT and MBR decoding to work on hypergraphs produced by SCFG-based MT systems. We present algorithms that are more efficient relative to the lattice algorithms presented in Macherey et al. (2008; Tromble et al. (2008). Lattice MBR decoding uses a linear approximation to the BLEU score (Papineni et al., 2001); the weights in this line</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe, W. Wang, and I. Thayer. 2006. Scalable Inference and Training of Context-Rich Syntactic Translation Models.. In COLING/ACL, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
</authors>
<title>Advanced Dynamic Programming in Semiring and Hypergraph Frameworks. In COLING,</title>
<date>2008</date>
<location>Manchester, UK.</location>
<contexts>
<context position="2525" citStr="Huang, 2008" startWordPosition="378" endWordPosition="379">versity of Maryland College Park, MD 20742, USA redpony@umd.edu number of translation alternatives relative to Nbest lists. The extension to lattices reduces the runtimes for both MERT and MBR, and gives performance improvements from MBR decoding. SMT systems based on synchronous context free grammars (SCFG) (Chiang, 2007; Zollmann and Venugopal, 2006; Galley et al., 2006) have recently been shown to give competitive performance relative to phrase-based SMT. For these systems, a hypergraph or packed forest provides a compact representation for encoding a huge number of translation hypotheses (Huang, 2008). In this paper, we extend MERT and MBR decoding to work on hypergraphs produced by SCFG-based MT systems. We present algorithms that are more efficient relative to the lattice algorithms presented in Macherey et al. (2008; Tromble et al. (2008). Lattice MBR decoding uses a linear approximation to the BLEU score (Papineni et al., 2001); the weights in this linear loss are set heuristically by assuming that n-gram precisions decay exponentially with n. However, this may not be optimal in practice. We employ MERT to select these weights by optimizing BLEU score on a development set. A related MB</context>
<context position="4152" citStr="Huang, 2008" startWordPosition="650" endWordPosition="651">i X1 announces its future in X1 X2 X1 its future in the X1 its future in the Rally World Championship X1 X2 X1 X2 X1 X2 X1 X2 163 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 163–171, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP 2 Translation Hypergraphs A translation lattice compactly encodes a large number of hypotheses produced by a phrase-based SMT system. The corresponding representation for an SMT system based on SCFGs (e.g. Chiang (2007), Zollmann and Venugopal (2006), Mi et al. (2008)) is a directed hypergraph or a packed forest (Huang, 2008). Formally, a hypergraph is a pair 7-1 = (V, £) consisting of a vertex set V and a set of hyperedges £ C_ V* x V. Each hyperedge e E £ connects a head vertex h(e) with a sequence of tail vertices T(e) = {v1, ..., v,,,}. The number of tail vertices is called the arity (|e|) of the hyperedge. If the arity of a hyperedge is zero, h(e) is called a source vertex. The arity of a hypergraph is the maximum arity of its hyperedges. A hyperedge of arity 1 is a regular edge, and a hypergraph of arity 1 is a regular graph (lattice). Each hyperedge is labeled with a rule re from the SCFG. The number of non</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>L. Huang. 2008. Advanced Dynamic Programming in Semiring and Hypergraph Frameworks. In COLING, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kumar</author>
<author>W Byrne</author>
</authors>
<title>Minimum BayesRisk Decoding for Statistical Machine Translation.</title>
<date>2004</date>
<booktitle>In HLT-NAACL,</booktitle>
<location>Boston, MA, USA.</location>
<contexts>
<context position="1587" citStr="Kumar and Byrne, 2004" startWordPosition="237" endWordPosition="240">BR decoding. Our experiments show speedups from MERT and MBR as well as performance improvements from MBR decoding on several language pairs. 1 Introduction Statistical Machine Translation (SMT) systems have improved considerably by directly using the error criterion in both training and decoding. By doing so, the system can be optimized for the translation task instead of a criterion such as likelihood that is unrelated to the evaluation metric. Two popular techniques that incorporate the error criterion are Minimum Error Rate Training (MERT) (Och, 2003) and Minimum BayesRisk (MBR) decoding (Kumar and Byrne, 2004). These two techniques were originally developed for N-best lists of translation hypotheses and recently extended to translation lattices (Macherey et al., 2008; Tromble et al., 2008) generated by a phrase-based SMT system (Och and Ney, 2004). Translation lattices contain a significantly higher 2Department of Linguistics University of Maryland College Park, MD 20742, USA redpony@umd.edu number of translation alternatives relative to Nbest lists. The extension to lattices reduces the runtimes for both MERT and MBR, and gives performance improvements from MBR decoding. SMT systems based on synch</context>
<context position="14239" citStr="Kumar and Byrne, 2004" startWordPosition="2495" endWordPosition="2498">l MT. An MBR decoder seeks the hypothesis with the least expected loss under a probability model (Bickel and Doksum, 1977). If we think of statistical MT as a classifier that maps a source sentence F to a target sentence E, the MBR decoder can be expressed as follows: E E� = argmin L(E, E&apos;)P(E|F), (1) E&apos;E9 EE9 where L(E, E&apos;) is the loss between any two hypotheses E and E&apos;, P(E|F) is the probability model, and 9 is the space of translations (N-best list, lattice, or a hypergraph). MBR decoding for translation can be performed by reranking an N-best list of hypotheses generated by an MT system (Kumar and Byrne, 2004). This reranking can be done for any sentencelevel loss function such as BLEU (Papineni et al., 2001), Word Error Rate, or Position-independent Error Rate. Recently, Tromble et al. (2008) extended MBR decoding to translation lattices under an approximate BLEU score. They approximated log(BLEU) score by a linear function of n-gram matches and candidate length. If E and E&apos; are the reference and the candidate translations respectively, this linear function is given by: G(E, E&apos;) = θ0|E&apos; |+ E θjwj#w(E&apos;)δw(E), (2) w where w is an n-gram present in either E or E&apos;, and θ0,θ1,..., θN are weights which </context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>S. Kumar and W. Byrne. 2004. Minimum BayesRisk Decoding for Statistical Machine Translation. In HLT-NAACL, Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Macherey</author>
<author>F Och</author>
<author>I Thayer</author>
<author>J Uszkoreit</author>
</authors>
<title>Lattice-based Minimum Error Rate Training for Statistical Machine Translation. In EMNLP,</title>
<date>2008</date>
<location>Honolulu, Hawaii, USA.</location>
<contexts>
<context position="1747" citStr="Macherey et al., 2008" startWordPosition="260" endWordPosition="263">istical Machine Translation (SMT) systems have improved considerably by directly using the error criterion in both training and decoding. By doing so, the system can be optimized for the translation task instead of a criterion such as likelihood that is unrelated to the evaluation metric. Two popular techniques that incorporate the error criterion are Minimum Error Rate Training (MERT) (Och, 2003) and Minimum BayesRisk (MBR) decoding (Kumar and Byrne, 2004). These two techniques were originally developed for N-best lists of translation hypotheses and recently extended to translation lattices (Macherey et al., 2008; Tromble et al., 2008) generated by a phrase-based SMT system (Och and Ney, 2004). Translation lattices contain a significantly higher 2Department of Linguistics University of Maryland College Park, MD 20742, USA redpony@umd.edu number of translation alternatives relative to Nbest lists. The extension to lattices reduces the runtimes for both MERT and MBR, and gives performance improvements from MBR decoding. SMT systems based on synchronous context free grammars (SCFG) (Chiang, 2007; Zollmann and Venugopal, 2006; Galley et al., 2006) have recently been shown to give competitive performance r</context>
<context position="5670" citStr="Macherey et al. (2008)" startWordPosition="946" endWordPosition="949">. The sequence of SCFG rules induced by a path is also called a derivation tree for E. 3 Minimum Error Rate Training Given a set of source sentences F1� with corresponding reference translations RS , the objective of MERT is to find a parameter set �λm which minimizes an automated evaluation criterion under a linear model: S λM = arg min { Err`Rs, E(Fs; λM1 )´ i al l s=1 111  XS ff ˆE(Fs; λM 1 ) = arg max λmhm(E, Fs) . E s=1 In the context of statistical machine translation, the optimization procedure was first described in Och (2003) for N-best lists and later extended to phrase-lattices in Macherey et al. (2008). The algorithm is based on the insight that, under a loglinear model, the cost function of any candidate translation can be represented as a line in the plane if the initial parameter set λM is shifted along a direction dM . Let C = {E1, ..., EK} denote a set of candidate translations, then computing the best scoring translation hypothesis E� out of C results in the following optimization problem: n o ˆE(F; γ) = arg max (λM 1 + γ · dM 1 )&gt; · hM 1 (E, F) E∈C = arg max ˘a(E, F) + γ · b(E, F) {z } (∗) ff Hence, the total score (*) for each candidate translation E E C can be described as a line w</context>
<context position="7430" citStr="Macherey et al. (2008)" startWordPosition="1263" endWordPosition="1266"> direction. Both the translations and their corresponding line segments can efficiently be computed without incorporating any error criterion. Once the envelope has been determined, the translation candidates of its constituent line segments are projected onto their corresponding error counts, thus yielding the exact and unsmoothed error surface for all candidate translations encoded in C. The error surface can now easily be traversed in order to find that under which the new parameter set M + M minimizes the global error. In this section, we present an extension of the algorithm described in Macherey et al. (2008) that allows us to efficiently compute and represent upper envelopes over all candidate translations encoded in hypergraphs. Conceptually, the algorithm works by propagating (initially empty) envelopes from the source nodes bottom-up to its unique root node, thereby expanding the envelopes by applying SCFG rules to the partial candidate translations that are associated with the constituent line segments. To recombine envelopes, we need two operators: the sum and the maximum over convex polygons. To illustrate which operator is applied when, we transform 7-1 = (V, £) into a regular graph with t</context>
<context position="12380" citStr="Macherey et al., 2008" startWordPosition="2174" endWordPosition="2177">in terms of a line’s x-intercept such that lower values imply higher priority. Hence, the priority queue enumerates all line segments from left to right in ascending order of their x-intercepts, which is the order needed to compute the Minkowski sum. Nodes of Type “V”: The operation performed 165 Figure 2: Transformation of a hypergraph into a factor graph and bottom-up propagation of envelopes. at nodes of type “V” computes the convex hull over the union of the envelopes propagated over the incoming edges. This operation is a “max” operation and it is identical to the algorithm described in (Macherey et al., 2008) for phrase lattices. Algorithm 2 contains the pseudo code. The complete algorithm then works as follows: Traversing all nodes in x bottom-up in topological order, we proceed for each node v E V over its incoming hyperedges and combine in each such hyperedge e the envelopes associated with the tail nodes T(e) by computing their sum according to Algorithm 1 (n-operation). For each incoming hyperedge e, the resulting envelope is then expanded by applying the rule re to its constituent line segments. The envelopes associated with different incoming hyperedges of node v are then combined and reduc</context>
<context position="24078" citStr="Macherey et al., 2008" startWordPosition="4236" endWordPosition="4239">bed in Zollmann and Venugopal (2006). Both systems are built on top of our phrase-based systems. In these systems, the decoder generates an initial hypergraph or an N-best list, which are then rescored using MBR decoding. 6.3 MERT Results Table 2 shows runtime experiments for the hypergraph MERT implementation in comparison with the phrase-lattice implementation on both the aren and the zhen system. The first two columns show the average amount of time in msecs that either algorithm requires to compute the upper envelope when applied to phrase lattices. Compared to the algorithm described in (Macherey et al., 2008) which is optimized for phrase lattices, the hypergraph implementation causes a small increase in 168 Avg. Runtime/sent [msec] (Macherey 2008) Suggested Alg. aren zhen aren zhen phrase lattice 8.57 7.91 10.30 8.65 hypergraph – – 8.19 8.11 Table 2: Average time for computing envelopes. running time. This increase is mainly due to the representation of line segments; while the phraselattice implementation stores a single backpointer, the hypergraph version stores a vector of backpointers. The last two columns show the average amount of time that is required to compute the upper envelope on hyper</context>
<context position="30394" citStr="Macherey et al., 2008" startWordPosition="5315" endWordPosition="5318">ttings. In particular, MERT avoids the need for manually tuning these parameters by language pair. Finally, when baseline model costs are added as an extra feature (mert+b), the number of pairs with gains/no changes/drops is 26/8/5. This shows that this feature can allow MBR decoding to backoff to the MAP translation. When MBR does not produce a higher BLEU score relative to MAP on the development set, MERT assigns a higher weight to this feature function. We see such an effect for 4 systems. 7 Discussion We have presented efficient algorithms which extend previous work on lattice-based MERT (Macherey et al., 2008) and MBR decoding (Tromble et al., 2008) to work with hypergraphs. Our new MERT algorithm can work with both lattices and hypergraphs. On lattices, it achieves similar run-times as the implementation System BLEU (%) MAP MBR default mert-b mert+b aren.pb 54.2 54.8 54.8 54.9 aren.hier 52.8 53.3 53.5 53.7 aren.samt 53.4 54.0 54.4 54.0 zhen.pb 40.1 40.7 40.7 40.9 zhen.hier 41.0 41.0 41.0 41.0 zhen.samt 41.3 41.8 41.6 41.7 Table 5: MBR Parameter Tuning on NIST systems MBR wrt. MAP default mert-b mert+b # of gains 18 22 26 # of no-changes 9 5 8 # of drops 12 12 5 Table 6: MBR on Multi-language syste</context>
</contexts>
<marker>Macherey, Och, Thayer, Uszkoreit, 2008</marker>
<rawString>W. Macherey, F. Och, I. Thayer, and J. Uszkoreit. 2008. Lattice-based Minimum Error Rate Training for Statistical Machine Translation. In EMNLP, Honolulu, Hawaii, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Mi</author>
<author>L Huang</author>
<author>Q Liu</author>
</authors>
<title>Forest-Based Translation.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<location>Columbus, OH, USA.</location>
<contexts>
<context position="4093" citStr="Mi et al. (2008)" startWordPosition="638" endWordPosition="641">008). Figure 1: An example hypergraph. will soon announce Suzuki X1 announces its future in X1 X2 X1 its future in the X1 its future in the Rally World Championship X1 X2 X1 X2 X1 X2 X1 X2 163 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 163–171, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP 2 Translation Hypergraphs A translation lattice compactly encodes a large number of hypotheses produced by a phrase-based SMT system. The corresponding representation for an SMT system based on SCFGs (e.g. Chiang (2007), Zollmann and Venugopal (2006), Mi et al. (2008)) is a directed hypergraph or a packed forest (Huang, 2008). Formally, a hypergraph is a pair 7-1 = (V, £) consisting of a vertex set V and a set of hyperedges £ C_ V* x V. Each hyperedge e E £ connects a head vertex h(e) with a sequence of tail vertices T(e) = {v1, ..., v,,,}. The number of tail vertices is called the arity (|e|) of the hyperedge. If the arity of a hyperedge is zero, h(e) is called a source vertex. The arity of a hypergraph is the maximum arity of its hyperedges. A hyperedge of arity 1 is a regular edge, and a hypergraph of arity 1 is a regular graph (lattice). Each hyperedge</context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>H. Mi, L. Huang, and Q. Liu. 2008. Forest-Based Translation. In ACL, Columbus, OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>The Alignment Template Approach to Statistical Machine Translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<pages>449</pages>
<contexts>
<context position="1829" citStr="Och and Ney, 2004" startWordPosition="274" endWordPosition="277">g the error criterion in both training and decoding. By doing so, the system can be optimized for the translation task instead of a criterion such as likelihood that is unrelated to the evaluation metric. Two popular techniques that incorporate the error criterion are Minimum Error Rate Training (MERT) (Och, 2003) and Minimum BayesRisk (MBR) decoding (Kumar and Byrne, 2004). These two techniques were originally developed for N-best lists of translation hypotheses and recently extended to translation lattices (Macherey et al., 2008; Tromble et al., 2008) generated by a phrase-based SMT system (Och and Ney, 2004). Translation lattices contain a significantly higher 2Department of Linguistics University of Maryland College Park, MD 20742, USA redpony@umd.edu number of translation alternatives relative to Nbest lists. The extension to lattices reduces the runtimes for both MERT and MBR, and gives performance improvements from MBR decoding. SMT systems based on synchronous context free grammars (SCFG) (Chiang, 2007; Zollmann and Venugopal, 2006; Galley et al., 2006) have recently been shown to give competitive performance relative to phrase-based SMT. For these systems, a hypergraph or packed forest prov</context>
<context position="23010" citStr="Och and Ney, 2004" startWordPosition="4064" endWordPosition="4067">set (dev) consists of the NIST 2005 eval set; we use this set for optimizing MBR parameters. We report results on NIST 2002 and NIST 2003 evaluation sets. The second task consists of systems for 39 language-pairs with English as the target language and trained on at most 300M word tokens mined from the web and other published sources. The development and test sets for this task are randomly selected sentences from the web, and contain 5000 and 1000 sentences respectively. 6.2 MT System Description Our phrase-based statistical MT system is similar to the alignment template system described in (Och and Ney, 2004; Tromble et al., 2008). Translation is performed using a standard dynamic programming beam-search decoder (Och and Ney, 2004) using two decoding passes. The first decoder pass generates either a lattice or an N-best list. MBR decoding is performed in the second pass. We also train two SCFG-based MT systems: a hierarchical phrase-based SMT (Chiang, 2007) system and a syntax augmented machine translation (SAMT) system using the approach described in Zollmann and Venugopal (2006). Both systems are built on top of our phrase-based systems. In these systems, the decoder generates an initial hyperg</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>F. Och and H. Ney. 2004. The Alignment Template Approach to Statistical Machine Translation. Computational Linguistics, 30(4):417 – 449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation. In ACL,</title>
<date>2003</date>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1526" citStr="Och, 2003" startWordPosition="229" endWordPosition="230">MERT can be employed to optimize parameters for MBR decoding. Our experiments show speedups from MERT and MBR as well as performance improvements from MBR decoding on several language pairs. 1 Introduction Statistical Machine Translation (SMT) systems have improved considerably by directly using the error criterion in both training and decoding. By doing so, the system can be optimized for the translation task instead of a criterion such as likelihood that is unrelated to the evaluation metric. Two popular techniques that incorporate the error criterion are Minimum Error Rate Training (MERT) (Och, 2003) and Minimum BayesRisk (MBR) decoding (Kumar and Byrne, 2004). These two techniques were originally developed for N-best lists of translation hypotheses and recently extended to translation lattices (Macherey et al., 2008; Tromble et al., 2008) generated by a phrase-based SMT system (Och and Ney, 2004). Translation lattices contain a significantly higher 2Department of Linguistics University of Maryland College Park, MD 20742, USA redpony@umd.edu number of translation alternatives relative to Nbest lists. The extension to lattices reduces the runtimes for both MERT and MBR, and gives performan</context>
<context position="5589" citStr="Och (2003)" startWordPosition="935" endWordPosition="936"> D = r1, r2, ..., rK which, if applied to the start symbol, derives E. The sequence of SCFG rules induced by a path is also called a derivation tree for E. 3 Minimum Error Rate Training Given a set of source sentences F1� with corresponding reference translations RS , the objective of MERT is to find a parameter set �λm which minimizes an automated evaluation criterion under a linear model: S λM = arg min { Err`Rs, E(Fs; λM1 )´ i al l s=1 111  XS ff ˆE(Fs; λM 1 ) = arg max λmhm(E, Fs) . E s=1 In the context of statistical machine translation, the optimization procedure was first described in Och (2003) for N-best lists and later extended to phrase-lattices in Macherey et al. (2008). The algorithm is based on the insight that, under a loglinear model, the cost function of any candidate translation can be represented as a line in the plane if the initial parameter set λM is shifted along a direction dM . Let C = {E1, ..., EK} denote a set of candidate translations, then computing the best scoring translation hypothesis E� out of C results in the following optimization problem: n o ˆE(F; γ) = arg max (λM 1 + γ · dM 1 )&gt; · hM 1 (E, F) E∈C = arg max ˘a(E, F) + γ · b(E, F) {z } (∗) ff Hence, the </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In ACL, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>Bleu: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2001</date>
<tech>Technical Report RC22176 (W0109-022),</tech>
<institution>IBM Research Division.</institution>
<contexts>
<context position="2862" citStr="Papineni et al., 2001" startWordPosition="432" endWordPosition="436">llmann and Venugopal, 2006; Galley et al., 2006) have recently been shown to give competitive performance relative to phrase-based SMT. For these systems, a hypergraph or packed forest provides a compact representation for encoding a huge number of translation hypotheses (Huang, 2008). In this paper, we extend MERT and MBR decoding to work on hypergraphs produced by SCFG-based MT systems. We present algorithms that are more efficient relative to the lattice algorithms presented in Macherey et al. (2008; Tromble et al. (2008). Lattice MBR decoding uses a linear approximation to the BLEU score (Papineni et al., 2001); the weights in this linear loss are set heuristically by assuming that n-gram precisions decay exponentially with n. However, this may not be optimal in practice. We employ MERT to select these weights by optimizing BLEU score on a development set. A related MBR-inspired approach for hypergraphs was developed by Zhang and Gildea (2008). In this work, hypergraphs were rescored to maximize the expected count of synchronous constituents in the translation. In contrast, our MBR algorithm directly selects the hypothesis in the hypergraph with the maximum expected approximate corpus BLEU score (Tr</context>
<context position="14340" citStr="Papineni et al., 2001" startWordPosition="2513" endWordPosition="2516">kel and Doksum, 1977). If we think of statistical MT as a classifier that maps a source sentence F to a target sentence E, the MBR decoder can be expressed as follows: E E� = argmin L(E, E&apos;)P(E|F), (1) E&apos;E9 EE9 where L(E, E&apos;) is the loss between any two hypotheses E and E&apos;, P(E|F) is the probability model, and 9 is the space of translations (N-best list, lattice, or a hypergraph). MBR decoding for translation can be performed by reranking an N-best list of hypotheses generated by an MT system (Kumar and Byrne, 2004). This reranking can be done for any sentencelevel loss function such as BLEU (Papineni et al., 2001), Word Error Rate, or Position-independent Error Rate. Recently, Tromble et al. (2008) extended MBR decoding to translation lattices under an approximate BLEU score. They approximated log(BLEU) score by a linear function of n-gram matches and candidate length. If E and E&apos; are the reference and the candidate translations respectively, this linear function is given by: G(E, E&apos;) = θ0|E&apos; |+ E θjwj#w(E&apos;)δw(E), (2) w where w is an n-gram present in either E or E&apos;, and θ0,θ1,..., θN are weights which are determined empirically, where N is the maximum ngram order. Under such a linear decomposition, th</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001. Bleu: a Method for Automatic Evaluation of Machine Translation. Technical Report RC22176 (W0109-022), IBM Research Division.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sixtus</author>
<author>S Ortmanns</author>
</authors>
<title>High Quality Word Graphs Using Forward-Backward Pruning.</title>
<date>1999</date>
<booktitle>In ICASSP,</booktitle>
<location>Phoenix, AZ, USA.</location>
<contexts>
<context position="27710" citStr="Sixtus and Ortmanns, 1999" startWordPosition="4859" endWordPosition="4862">. We select the MBR scaling factor (Tromble et al., 2008) based on the development set; it is set to 0.1, 0.01, 0.5, 0.2, 0.5 and 1.0 for the aren-phrase, aren-hier, aren-samt, zhen-phrase zhen-hier and zhen-samt systems respectively. For the multi-language case, we train phrase-based systems and perform lattice MBR for all language pairs. We use a scaling factor of 0.7 for all pairs. Additional gains can be obtained by tuning this factor; however, we do not explore that dimension in this paper. In all cases, we prune the lattices/hypergraphs to a density of 30 using forward-backward pruning (Sixtus and Ortmanns, 1999). We consider a BLEU score difference to be a) gain if is at least 0.2 points, b) drop if it is at most -0.2 points, and c) no change otherwise. The results are shown in Table 6. In both tables, the following results are reported: Lattice/HGMBR with default parameters (−5,1.5, 2, 3, 4) computed using corpus statistics (Tromble et al., 2008), Lattice/HGMBR with parameters derived from MERT both without/with the baseline model cost feature (mert−b, mert+b). For multi-language systems, we only show the # of language-pairs with gains/no-changes/drops for each MBR variant with respect to the MAP tr</context>
</contexts>
<marker>Sixtus, Ortmanns, 1999</marker>
<rawString>A. Sixtus and S. Ortmanns. 1999. High Quality Word Graphs Using Forward-Backward Pruning. In ICASSP, Phoenix, AZ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Tromble</author>
<author>S Kumar</author>
<author>F Och</author>
<author>W Macherey</author>
</authors>
<title>Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation. In EMNLP,</title>
<date>2008</date>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="1770" citStr="Tromble et al., 2008" startWordPosition="264" endWordPosition="267">tion (SMT) systems have improved considerably by directly using the error criterion in both training and decoding. By doing so, the system can be optimized for the translation task instead of a criterion such as likelihood that is unrelated to the evaluation metric. Two popular techniques that incorporate the error criterion are Minimum Error Rate Training (MERT) (Och, 2003) and Minimum BayesRisk (MBR) decoding (Kumar and Byrne, 2004). These two techniques were originally developed for N-best lists of translation hypotheses and recently extended to translation lattices (Macherey et al., 2008; Tromble et al., 2008) generated by a phrase-based SMT system (Och and Ney, 2004). Translation lattices contain a significantly higher 2Department of Linguistics University of Maryland College Park, MD 20742, USA redpony@umd.edu number of translation alternatives relative to Nbest lists. The extension to lattices reduces the runtimes for both MERT and MBR, and gives performance improvements from MBR decoding. SMT systems based on synchronous context free grammars (SCFG) (Chiang, 2007; Zollmann and Venugopal, 2006; Galley et al., 2006) have recently been shown to give competitive performance relative to phrase-based</context>
<context position="3481" citStr="Tromble et al., 2008" startWordPosition="532" endWordPosition="535">1); the weights in this linear loss are set heuristically by assuming that n-gram precisions decay exponentially with n. However, this may not be optimal in practice. We employ MERT to select these weights by optimizing BLEU score on a development set. A related MBR-inspired approach for hypergraphs was developed by Zhang and Gildea (2008). In this work, hypergraphs were rescored to maximize the expected count of synchronous constituents in the translation. In contrast, our MBR algorithm directly selects the hypothesis in the hypergraph with the maximum expected approximate corpus BLEU score (Tromble et al., 2008). Figure 1: An example hypergraph. will soon announce Suzuki X1 announces its future in X1 X2 X1 its future in the X1 its future in the Rally World Championship X1 X2 X1 X2 X1 X2 X1 X2 163 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 163–171, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP 2 Translation Hypergraphs A translation lattice compactly encodes a large number of hypotheses produced by a phrase-based SMT system. The corresponding representation for an SMT system based on SCFGs (e.g. Chiang (2007), Zollmann and Venugopal (2006), Mi e</context>
<context position="14426" citStr="Tromble et al. (2008)" startWordPosition="2525" endWordPosition="2528"> sentence F to a target sentence E, the MBR decoder can be expressed as follows: E E� = argmin L(E, E&apos;)P(E|F), (1) E&apos;E9 EE9 where L(E, E&apos;) is the loss between any two hypotheses E and E&apos;, P(E|F) is the probability model, and 9 is the space of translations (N-best list, lattice, or a hypergraph). MBR decoding for translation can be performed by reranking an N-best list of hypotheses generated by an MT system (Kumar and Byrne, 2004). This reranking can be done for any sentencelevel loss function such as BLEU (Papineni et al., 2001), Word Error Rate, or Position-independent Error Rate. Recently, Tromble et al. (2008) extended MBR decoding to translation lattices under an approximate BLEU score. They approximated log(BLEU) score by a linear function of n-gram matches and candidate length. If E and E&apos; are the reference and the candidate translations respectively, this linear function is given by: G(E, E&apos;) = θ0|E&apos; |+ E θjwj#w(E&apos;)δw(E), (2) w where w is an n-gram present in either E or E&apos;, and θ0,θ1,..., θN are weights which are determined empirically, where N is the maximum ngram order. Under such a linear decomposition, the MBR decoder (Equation 1) can be written as Tromble et al. (2008) implement the MBR d</context>
<context position="20511" citStr="Tromble et al. (2008)" startWordPosition="3630" endWordPosition="3633"> end for 16: end for 17: Assign scores to hyperedges (Equation 3). 18: Find best path in the hypergraph (Equation 3). 167 hibitive when the maximum n-gram order in MBR does not exceed the order of the n-gram language model used in creating the hypergraph. In the latter case, we will have a small set of unique prefixes and suffixes on the tail nodes. 5 MERT for MBR Parameter Optimization Lattice MBR Decoding (Equation 3) assumes a linear form for the gain function (Equation 2). This linear function contains n + 1 parameters B0, B1, ..., BN, where N is the maximum order of the n-grams involved. Tromble et al. (2008) obtained these factors as a function of n-gram precisions derived from multiple training runs. However, this does not guarantee that the resulting linear score (Equation 2) is close to the corpus BLEU. We now describe how MERT can be used to estimate these factors to achieve a better approximation to the corpus BLEU. We recall that MERT selects weights in a linear model to optimize an error criterion (e.g. corpus BLEU) on a training set. The lattice MBR decoder (Equation 3) can be written as a linear model: E� = argmaxE,Eg �Ni=0 Bigi(E&apos;, F), where g0(E&apos;,F) = IE&apos;I and gi(E&apos;, F) = E w:|w|=i #w(</context>
<context position="23033" citStr="Tromble et al., 2008" startWordPosition="4068" endWordPosition="4071">of the NIST 2005 eval set; we use this set for optimizing MBR parameters. We report results on NIST 2002 and NIST 2003 evaluation sets. The second task consists of systems for 39 language-pairs with English as the target language and trained on at most 300M word tokens mined from the web and other published sources. The development and test sets for this task are randomly selected sentences from the web, and contain 5000 and 1000 sentences respectively. 6.2 MT System Description Our phrase-based statistical MT system is similar to the alignment template system described in (Och and Ney, 2004; Tromble et al., 2008). Translation is performed using a standard dynamic programming beam-search decoder (Och and Ney, 2004) using two decoding passes. The first decoder pass generates either a lattice or an N-best list. MBR decoding is performed in the second pass. We also train two SCFG-based MT systems: a hierarchical phrase-based SMT (Chiang, 2007) system and a syntax augmented machine translation (SAMT) system using the approach described in Zollmann and Venugopal (2006). Both systems are built on top of our phrase-based systems. In these systems, the decoder generates an initial hypergraph or an N-best list,</context>
<context position="24988" citStr="Tromble et al., 2008" startWordPosition="4387" endWordPosition="4390">ning time. This increase is mainly due to the representation of line segments; while the phraselattice implementation stores a single backpointer, the hypergraph version stores a vector of backpointers. The last two columns show the average amount of time that is required to compute the upper envelope on hypergraphs. For comparison, we prune hypergraphs to the same density (# of edges per edge on the best path) and achieve identical running times for computing the error surface. 6.4 MBR Results We first compare the new lattice MBR (Algorithm 3) with MBR decoding on 1000-best lists and FSAMBR (Tromble et al., 2008) on lattices generated by the phrase-based systems; evaluation is done using both BLEU and average run-time per sentence (Table 3). Note that N-best MBR uses a sentence BLEU loss function. The new lattice MBR algorithm gives about the same performance as FSAMBR while yielding a 20X speedup. We next report the performance of MBR on hypergraphs generated by Hiero/SAMT systems. Table 4 compares Hypergraph MBR (HGMBR) with MAP and MBR decoding on 1000 best lists. On some systems such as the Arabic-English SAMT, the gains from Hypergraph MBR over 1000-best MBR are significant. In other cases, Hyper</context>
<context position="26275" citStr="Tromble et al., 2008" startWordPosition="4609" endWordPosition="4612">e observe a 7X speedup in runtime. This shows the usefulness of Hypergraph MBR decoding as an efficient alternative to Nbest MBR. 6.5 MBR Parameter Tuning with MERT We now describe the results by tuning MBR ngram parameters (Equation 2) using MERT. We first compute N + 1 MBR feature functions on each edge of the lattice/hypergraph. We also include the total decoder cost on the edge as as additional feature function. MERT is then performed to optimize the BLEU score on a development set; For MERT, we use 40 random initial parameters as well as parameters computed using corpus based statistics (Tromble et al., 2008). BLEU (%) Avg. aren zhen time nist03 nist02 nist03 nist02 (ms.) MAP 54.2 64.2 40.1 39.0 - N-best MBR 54.3 64.5 40.2 39.2 3.7 Lattice MBR FSAMBR 54.9 65.2 40.6 39.5 3.7 LatMBR 54.8 65.2 40.7 39.4 0.2 Table 3: Lattice MBR for a phrase-based system. BLEU (%) Avg. aren zhen time nist03 nist02 nist03 nist02 (ms.) Hiero MAP 52.8 62.9 41.0 39.8 - N-best MBR 53.2 63.0 41.0 40.1 3.7 HGMBR 53.3 63.1 41.0 40.2 0.5 SAMT MAP 53.4 63.9 41.3 40.3 - N-best MBR 53.8 64.3 41.7 41.1 3.7 HGMBR 54.0 64.6 41.8 41.1 0.5 Table 4: Hypergraph MBR for Hiero/SAMT systems. Table 5 shows results for NIST systems. We repor</context>
<context position="28052" citStr="Tromble et al., 2008" startWordPosition="4923" endWordPosition="4926">e a scaling factor of 0.7 for all pairs. Additional gains can be obtained by tuning this factor; however, we do not explore that dimension in this paper. In all cases, we prune the lattices/hypergraphs to a density of 30 using forward-backward pruning (Sixtus and Ortmanns, 1999). We consider a BLEU score difference to be a) gain if is at least 0.2 points, b) drop if it is at most -0.2 points, and c) no change otherwise. The results are shown in Table 6. In both tables, the following results are reported: Lattice/HGMBR with default parameters (−5,1.5, 2, 3, 4) computed using corpus statistics (Tromble et al., 2008), Lattice/HGMBR with parameters derived from MERT both without/with the baseline model cost feature (mert−b, mert+b). For multi-language systems, we only show the # of language-pairs with gains/no-changes/drops for each MBR variant with respect to the MAP translation. 169 We observed in the NIST systems that MERT resulted in short translations relative to MAP on the unseen test set. To prevent this behavior, we modify the MERT error criterion to include a sentence-level brevity scorer with parameter α: BLEU+brevity(α). This brevity scorer penalizes each candidate translation that is shorter th</context>
<context position="30434" citStr="Tromble et al., 2008" startWordPosition="5323" endWordPosition="5326">ed for manually tuning these parameters by language pair. Finally, when baseline model costs are added as an extra feature (mert+b), the number of pairs with gains/no changes/drops is 26/8/5. This shows that this feature can allow MBR decoding to backoff to the MAP translation. When MBR does not produce a higher BLEU score relative to MAP on the development set, MERT assigns a higher weight to this feature function. We see such an effect for 4 systems. 7 Discussion We have presented efficient algorithms which extend previous work on lattice-based MERT (Macherey et al., 2008) and MBR decoding (Tromble et al., 2008) to work with hypergraphs. Our new MERT algorithm can work with both lattices and hypergraphs. On lattices, it achieves similar run-times as the implementation System BLEU (%) MAP MBR default mert-b mert+b aren.pb 54.2 54.8 54.8 54.9 aren.hier 52.8 53.3 53.5 53.7 aren.samt 53.4 54.0 54.4 54.0 zhen.pb 40.1 40.7 40.7 40.9 zhen.hier 41.0 41.0 41.0 41.0 zhen.samt 41.3 41.8 41.6 41.7 Table 5: MBR Parameter Tuning on NIST systems MBR wrt. MAP default mert-b mert+b # of gains 18 22 26 # of no-changes 9 5 8 # of drops 12 12 5 Table 6: MBR on Multi-language systems. described in Macherey et al. (2008).</context>
</contexts>
<marker>Tromble, Kumar, Och, Macherey, 2008</marker>
<rawString>R. Tromble, S. Kumar, F. Och, and W. Macherey. 2008. Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation. In EMNLP, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhang</author>
<author>D Gildea</author>
</authors>
<title>Efficient Multi-pass Decoding for Synchronous Context Free Grammars.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<location>Columbus, OH, USA.</location>
<contexts>
<context position="3201" citStr="Zhang and Gildea (2008)" startWordPosition="489" endWordPosition="492">ork on hypergraphs produced by SCFG-based MT systems. We present algorithms that are more efficient relative to the lattice algorithms presented in Macherey et al. (2008; Tromble et al. (2008). Lattice MBR decoding uses a linear approximation to the BLEU score (Papineni et al., 2001); the weights in this linear loss are set heuristically by assuming that n-gram precisions decay exponentially with n. However, this may not be optimal in practice. We employ MERT to select these weights by optimizing BLEU score on a development set. A related MBR-inspired approach for hypergraphs was developed by Zhang and Gildea (2008). In this work, hypergraphs were rescored to maximize the expected count of synchronous constituents in the translation. In contrast, our MBR algorithm directly selects the hypothesis in the hypergraph with the maximum expected approximate corpus BLEU score (Tromble et al., 2008). Figure 1: An example hypergraph. will soon announce Suzuki X1 announces its future in X1 X2 X1 its future in the X1 its future in the Rally World Championship X1 X2 X1 X2 X1 X2 X1 X2 163 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 163–171, Suntec, Singapore, 2-7 August 200</context>
</contexts>
<marker>Zhang, Gildea, 2008</marker>
<rawString>H. Zhang and D. Gildea. 2008. Efficient Multi-pass Decoding for Synchronous Context Free Grammars. In ACL, Columbus, OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Zollmann</author>
<author>A Venugopal</author>
</authors>
<title>Syntax Augmented Machine Translation via Chart Parsing. In HLT-NAACL,</title>
<date>2006</date>
<location>New York, NY, USA.</location>
<contexts>
<context position="2266" citStr="Zollmann and Venugopal, 2006" startWordPosition="336" endWordPosition="339">r N-best lists of translation hypotheses and recently extended to translation lattices (Macherey et al., 2008; Tromble et al., 2008) generated by a phrase-based SMT system (Och and Ney, 2004). Translation lattices contain a significantly higher 2Department of Linguistics University of Maryland College Park, MD 20742, USA redpony@umd.edu number of translation alternatives relative to Nbest lists. The extension to lattices reduces the runtimes for both MERT and MBR, and gives performance improvements from MBR decoding. SMT systems based on synchronous context free grammars (SCFG) (Chiang, 2007; Zollmann and Venugopal, 2006; Galley et al., 2006) have recently been shown to give competitive performance relative to phrase-based SMT. For these systems, a hypergraph or packed forest provides a compact representation for encoding a huge number of translation hypotheses (Huang, 2008). In this paper, we extend MERT and MBR decoding to work on hypergraphs produced by SCFG-based MT systems. We present algorithms that are more efficient relative to the lattice algorithms presented in Macherey et al. (2008; Tromble et al. (2008). Lattice MBR decoding uses a linear approximation to the BLEU score (Papineni et al., 2001); th</context>
<context position="4075" citStr="Zollmann and Venugopal (2006)" startWordPosition="634" endWordPosition="637">s BLEU score (Tromble et al., 2008). Figure 1: An example hypergraph. will soon announce Suzuki X1 announces its future in X1 X2 X1 its future in the X1 its future in the Rally World Championship X1 X2 X1 X2 X1 X2 X1 X2 163 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 163–171, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP 2 Translation Hypergraphs A translation lattice compactly encodes a large number of hypotheses produced by a phrase-based SMT system. The corresponding representation for an SMT system based on SCFGs (e.g. Chiang (2007), Zollmann and Venugopal (2006), Mi et al. (2008)) is a directed hypergraph or a packed forest (Huang, 2008). Formally, a hypergraph is a pair 7-1 = (V, £) consisting of a vertex set V and a set of hyperedges £ C_ V* x V. Each hyperedge e E £ connects a head vertex h(e) with a sequence of tail vertices T(e) = {v1, ..., v,,,}. The number of tail vertices is called the arity (|e|) of the hyperedge. If the arity of a hyperedge is zero, h(e) is called a source vertex. The arity of a hypergraph is the maximum arity of its hyperedges. A hyperedge of arity 1 is a regular edge, and a hypergraph of arity 1 is a regular graph (lattic</context>
<context position="23492" citStr="Zollmann and Venugopal (2006)" startWordPosition="4141" endWordPosition="4144">ively. 6.2 MT System Description Our phrase-based statistical MT system is similar to the alignment template system described in (Och and Ney, 2004; Tromble et al., 2008). Translation is performed using a standard dynamic programming beam-search decoder (Och and Ney, 2004) using two decoding passes. The first decoder pass generates either a lattice or an N-best list. MBR decoding is performed in the second pass. We also train two SCFG-based MT systems: a hierarchical phrase-based SMT (Chiang, 2007) system and a syntax augmented machine translation (SAMT) system using the approach described in Zollmann and Venugopal (2006). Both systems are built on top of our phrase-based systems. In these systems, the decoder generates an initial hypergraph or an N-best list, which are then rescored using MBR decoding. 6.3 MERT Results Table 2 shows runtime experiments for the hypergraph MERT implementation in comparison with the phrase-lattice implementation on both the aren and the zhen system. The first two columns show the average amount of time in msecs that either algorithm requires to compute the upper envelope when applied to phrase lattices. Compared to the algorithm described in (Macherey et al., 2008) which is opti</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>A. Zollmann and A. Venugopal. 2006. Syntax Augmented Machine Translation via Chart Parsing. In HLT-NAACL, New York, NY, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>