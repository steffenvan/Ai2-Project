<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002888">
<title confidence="0.808063">
Building a Robust Dialogue System with Limited Data *
</title>
<note confidence="0.7039002">
Sharon J. Goldwater, Elizabeth Owen Bratt, Jean Mark Gawron, and John Dowdingt
SRI International
333 Ravenswood Avenue
Menlo Park, CA 94025
{goldwater, owen, gawron, dowding} gai.sri.com
</note>
<sectionHeader confidence="0.989954" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999878307692308">
We describe robustness techniques used in the Com-
mandTalk system at the recognition level, the pars-
ing level, and the dialogue level, and how these were
influenced by the lack of domain data. We used
interviews with subject matter experts (SME&apos;s) to
develop a single grammar for recognition, under-
standing, and generation, thus eliminating the need
for a robust parser. We broadened the coverage of
the recognition grammar by allowing word insertions
and deletions, and we implemented clarification and
correction subdialogues to increase robustness at the
dialogue level. We discuss the applicability of these
techniques to other domains.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998162">
Three types of robustness must be considered when
designing a dialogue system. First, there is robust-
ness at the recognition level. When plentiful data
is available, a robust n..-gram language model can be
produced, but when data is limited, producing a ro-
bust language model for recognition can be prob-
lematic. Second, there is robustness at the level
of the parser. Robust parsing is often achieved by
combining a full parser with a partial parser and
fragment-combining rules, but even then some utter-
ances may be correctly recognized, only to be parsed
incorrectly or not at all. Finally, there is robustness
at the dialogue level. Utterances may be uninter-
pretable within the context of the dialogue due to
errors on the past of either the system or the user,
and the dialogue manager should be able to handle
such problems gracefully.
Our CommandTalk dialogue system was designed
for a highly specialized domain with little available
data, so finding ways to build a robust system with
</bodyText>
<listItem confidence="0.920513222222222">
• This research was supported by the Defense Advanced Re-
search Projects Agency under Contract N66001.94-C-6046
with the Space and Naval Warfare Systems Center. The views
and conclusions contained in this document are those of the
authors and should not be interpreted as necessarily repre-
senting the official policies, either express or implied, of the
Defense Advanced Research Projects Agency of the U.S. Gov-
ernment,
t Currently affiliated with GO.corn
</listItem>
<bodyText confidence="0.999522416666667">
limited data was a major concern. In this paper,
we discuss our methods and their applicability to
other domains. Section 2 gives a brief overview of
the CommandTalk system. In Section 3, we discuss
the approach we took to building recognition, under-
standing, and generation models for CommandTalk,
and how it relates to the first two types of robustness
mentioned. Section 4 discusses additional robust-
ness techniques at the recognizes level, and Section 5
describes dialogue-level robustness techniques. Sec-
tion 6 discusses the applicability of our methods to
other domains.
</bodyText>
<sectionHeader confidence="0.992409" genericHeader="introduction">
2 CommandTalk
</sectionHeader>
<bodyText confidence="0.99902">
CommandTalk is a spoken-language interface to the
ModSAF (Modular Semi-Automated Forces) battle-
field simulator, developed with the goal of allow-
ing military commanders to interact with simulated
forces in a manner as similar as possible to the way
they would command actual forces. CommandTalk
allows the use of ordinary English commands and
mouse gestures to
</bodyText>
<listItem confidence="0.99063325">
• Create forces and control measures (points and
lines)
• Assign missions to forces
• Modify missions during execution
• Control ModSAF system functions, such as the
map display
• Get information about the state of the simula-
tion
</listItem>
<bodyText confidence="0.910480636363636">
CommandTalk consists of a number of indepen-
dent, cooperating agents interacting through SRI&apos;s
Open Agent Architecture (OAA) (Martin et al.,
1998). OAA uses a facilitator agent that plans and
coordinates interactions among agents during dis-
tributed computation. An introduction to the basic
CommandTalk agents can be found in Moore et al.
(1997). CommandTalk&apos;s dialogue component is de-
scribed in detail in Stent et al, (1999), and its use
of linguistic and situational context is described in
Dowding et al. (1999).
</bodyText>
<page confidence="0.999476">
61
</page>
<sectionHeader confidence="0.946933" genericHeader="method">
3 The One-Grammar Approach
</sectionHeader>
<bodyText confidence="0.999901151515152">
In a domain with limited data, the inability to col-
lect a sufficient corpus for training a statistical lan-
guage model can be a significant problem. For
CommandTalk, we did not create a statistical lan-
guage model. Instead, with information gathered
from interviews of subject matter experts (SME&apos;s),
we developed a handwritten grammar using Gemini
(Dowding et al., 1993), a unification-based gram-
mar formalism. We used this unification grammar
for both natural language understanding and gener-
ation, and, using a grammar compiler we developed,
compiled it into a context-free form suitable for the
speech recognizer as well.
The effects of_this single-grammar approach on
the robustne,ss of the CommandTalk system were
twofold. On the negative side, we presumably ended
up with a recognition language model with less cov-
erage than a statistical model would have had. Our
attempts to deal with this are discussed in the next
section. On the positive side, we eliminated the
usual discrepancy in coverage between the recognizer
and the natural language parser. This was advanta-
geous, since no fragment-combining or other parsing
robustness techniques were needed.
Our approach had other advantages as well. Any
changes we made to the understanding grammar
were automatically reflected in the recognition and
generation grammars, making additions and modifi-
cations efficient. Also, anecdotal evidence suggests
that the language used by the system often influ-
ences the language used by speakers, so maintaining
consistency between the input and output of the sys-
tem is desirable.
</bodyText>
<sectionHeader confidence="0.996022" genericHeader="method">
4 Utterance-Level Robustness
</sectionHeader>
<bodyText confidence="0.999936523809524">
It is difficult to write a grammar that is constrained
enough to be useful without excluding some rea-
sonable user utterances. To alleviate this prob-
lem, we modified the speech recognition grammar
and natural language parser to allow certain &amp;quot;close-
to-grammar&amp;quot; utterances. Utterances with inserted
words, such as Center on Checkpoint I now or zoom
way out (where Center on Checkpoint 1 and zoom
out are grammatical) were permitted by allowing
the recognizer to skip unknown words, We also al-
lowed utterances with deleted words, as long as those
words did not contribute to the semantics of the ut-
terance as determined by the Gemini semantic rifles
constraining logical forms. For example, a user could
say, Set speed, 40 kph rather than Set speed to 40 kph.
The idea behind these modifications was to allow ut-
terances with a slightly broader range of wordings
than those in the grammar, but with essentially the
same meanings.
We began by testing the effects of these modi-
fications on in-grammar utterances, to ensure that
</bodyText>
<table confidence="0.9986526">
Non-Robust Robust
Time, CPURT 0.664 &apos; 1.05
SRR 2.56% • 1.70%
AWER 1.68% 2.94%
SER 10.00% . 12.07%
</table>
<tableCaption confidence="0.999586">
Table 1: In-Grammar Recognition Results
</tableCaption>
<bodyText confidence="0.99886875">
they did not significantly deciedse recognition per-
formance. We used a small test corpus of approxi-
mately 800 utterances read by SRI employees. We
collected four measures of performance:
</bodyText>
<listItem confidence="0.994080928571429">
• Recognition time, measured, in multiples of
CPU real time (CPURT). A recognition time
of 1xCPURT means that on:our CPU (a Sun
Ultra2), recognition took exactly as, long as the
duration of the utterance.
• Sentence reject rate (SRR). The percentage of
sentences that the recognizer rejects.
• Adjusted word error rate (AWER). The per-
centage of words in non-rejected sentences that
are misrecognized.
• Sentence error rate (SER). The percentage of
sentences in which some sort of error occurred,
either a complete rejection or misrecognized
word.
</listItem>
<bodyText confidence="0.9998285">
Several parameters affected the results, most no-
tably the numerical penalties assigned for inserting
or deleting words, and the pruning threshold of the
recognizer. Raising the pruning threshold caused
both reject and error rates to go down, but slowed
recognition. Lowering the penalties caused rejection
rates to go down, but word and sentence error rates
to go up, since some sentences which had been re-
jected were now recognized partially correctly, and
some sentences which had been recognized correctly
now included some errors. Lowering the penalties
also led to slower recognition.
Table 1 shows recognition results for the non-
robust and robust versions of the recognition gram-
mar on in-grammar utterances. The pruning thresh-
old is the same for both versions and the insertion
and deletion penalties are set to intermediate val-
ues. Recognition times for the robust grammar are
about 60% slower than those of the control gram-
mar, but still at acceptable levels. Reject and error
rates are fairly close for the two grammars. Overall,
adding robustness to the recognition grammar did
not severely penalize in-grammar recognition per-
formance.
We had very little out-of-grammar data for Corn-
mandTalk, and finding subjects in this highly spe-
cialized domain would have been difficult and ex-
pensive. To test our robustness techniques on out-
</bodyText>
<page confidence="0.996321">
62
</page>
<bodyText confidence="0.9999618">
of-grammar utterances, we decided to port them
to another domain with easily accessible users and
data; namely, the ATIS air travel domain. We wrote
a small grammar covering part of the ATIS data
and compiled it into a recognition grammar using
the same techniques as in CommandTalk. Unfortu-
nately, we were unable to carry out any experiments,
because the recognition grammar we derived yielded
recognition times that were so slow as to be imprac-
tical. We discuss these results further in Section 6.
</bodyText>
<sectionHeader confidence="0.998497" genericHeader="method">
5 Dialogue-Level Robustness
</sectionHeader>
<bodyText confidence="0.999921166666667">
To be considered robust at the dialogue level, a sys-
tem must be able to deal with situations where an
utterance is recognized and parsed, but cannot be in-
terpreted withirt-the current system state or dialogue
context. In addition,-it must be easy for the user to
correct faulty interpretations on the part of the sys-
tem. Contextual interpretation problems may occur
for a variety of reasons, including misrecognitions,
incorrect reference resolution, and confusion or in-
completeness on the part of the user.
The CommandTalk dialogue manager maintains
a stack to keep track of the current discourse con-
text and uses small finite-state machines to represent
different types of subdialogues. Below we illustrate
some types of subdialogues and other techniques
which provide robustness at the dialogue level. Note
that for each utterance, we write what the system
recognizes, not what the user actually says.
</bodyText>
<subsectionHeader confidence="0.445018">
5.1 Correction Sub dialogues
</subsectionHeader>
<equation confidence="0.954252">
Ex. 1:
U 1 Create a CEV at 76 53
S 2 0
U 3 Put Objective Golf here &lt;click&gt;
S 4 0 I will locate Objective Golf at FQ
658 583
U 5 I said Objective Alpha
S 6 0 I will locate Objective Alpha at FQ
658 853
</equation>
<bodyText confidence="0.999952266666667">
Allowing the user to correct full or partial utterances
can remedy interpretation problems caused by mis-
recognitions, incorrect reference resolution, or user
error.
In Example 1, the system responds to the user&apos;s
first utterance by producing a rising tone, illustrated
by the 0 symbol, to indicate successful interpreta-
tion and execution of the command, in this case cre-
ation of a CEV, a type of vehicle. (Unsuccessful
interpretation is indicated by a falling tone, illus-
trated by the 0 symbol.) In utterances 3 through
6, a misrecognition causes the system to perform
the wrong behavior. The user initiates a correction
subdialogue, and the system goes on to correctly re-
interpret the full utterance.
</bodyText>
<figure confidence="0.456984">
5.2 Implicit Confirmation
Ex. 2;
U 7 CEV, advance to Checkpoint 1
S 8 ® 100A11 will proceed to Checkpoint
1
</figure>
<bodyText confidence="0.999593307692308">
Use of implicit confirmation in combination with
correction subdialogues makes it easy to correct
faulty interpretations as soon as possible by alert-
ing the user to possible sources of error.
In utterances 7 and 8, the system must resolve
the user&apos;s reference, &amp;quot;CEV&amp;quot;, to a particular unit.
It therefore echoes the user&apos;s command using the
CEV&apos;s unique call sign. This makes explicit the sys-
tem&apos;s interpretation of the user&apos;s utterance, giving
the user a chance to correct the system if necessary.
Note that utterance 4 also contains an implicit
confirmation, since the system has resolved the
user&apos;s gesture to a set of coordinates.
</bodyText>
<subsectionHeader confidence="0.703423">
5.3 Clarification Subdialogues
</subsectionHeader>
<equation confidence="0.932774333333333">
Ex. 3:
U 9 Create a CEV at 72 69
S 10 0
U 11 CEV, conduct crater breach facing
south
S 12 There are two CEV&apos;s. Do you mean
100A11 or 100Al2?
U 13 Al2
S 14 c3 Which point should 100Al2 conduct
</equation>
<bodyText confidence="0.953397095238095">
crater breach oriented south at?
U 15 Checkpoint 2
S 16 0 There is no Checkpoint 2. Which
point should 100Al2 conduct crater
breach oriented south at?
U 17 I meant Checkpoint 1
S 18 0 100Al2 will conduct crater breach
oriented south at Checkpoint 1
Clarification subdialogues are generally initiated by
the system as a result of errors or incomplete com-
mands on the part of the user.
Example 3 illustrates three different types of prob-
lems that can be corrected by system questions.
First, the user&apos;s reference to &amp;quot;CEV&amp;quot; in utterance
11 is ambiguous, so the system asks a question to
determine which CEV the user is referring to. Next,
the system asks the user to supply a missing piece
of information that is required to carry out the com-
mand. Finally, when the user makes an error by
referring to a point that doesn&apos;t exist, the system
prompts for a, correction.
</bodyText>
<sectionHeader confidence="0.792863" genericHeader="conclusions">
6 Discussion and Conclusions
</sectionHeader>
<bodyText confidence="0.9559695">
CommandTalk is an example of a successful and ro-
bust dialogue system in a domain with limited ac-
</bodyText>
<page confidence="0.998785">
63
</page>
<bodyText confidence="0.999968844155845">
cess to both data and subjects. The pre-dialogue
version of CommandTalk was used in the STOW
(Synthetic Theater of War) &apos;97 ACTD (Advanced
Concept Technology Demonstration) exercise, an in-
tensive 48-hour continuous military simulation by
all four U.S. military services, and received high
praise. The dialogue portion of the system has in-
creased CommandTalk&apos;s usefulness and robustness.
Nevertheless, several questions remain, not the least
of which is whether the robustness techniques used
for CommandTalk can be successfully transferred to
other domains.
We have no doubt that our methods for adding ro-
bustness at the dialogue level can and should be im-
plemented in other domains, but this is not as clear
for our parsilig and recognition robustness methods.
The one-grammar approach is key to our elimi-
nating the necessity for robust parsing, renders a
large corpus for generating a recognition model un-
necessary, and has other advantages as well. Yet
our experience in the ATIS domain suggests that
further research into this approach is needed. Our
ATIS grammar is based on a grammar of general
English and has a very different structure from that
of CommandTalk&apos;s semantic grammar, but we were
unable to isolate the factor or factors responsible for
its poor recognition performance. Recent research
(Rayner et al., 2000) suggests that it may be pos-
sible to compile a useful recognition model from a
general English unification grammar if the gram-
mar is constructed carefully and a few compromises
are made. We also believe that using an appropri-
ate grammar approximation algorithm to reduce the
complexity of the recognition model may prove fruit-
ful. This would reintroduce some discrepancy be-
tween the recognition and understanding language
models, but maintain the other advantages of the
one-grammar approach.
In either case, the effectiveness of our recognition
robustness techniques remains an open question. We
know they have no significant negative impact on in-
grammar recognition, but whether they are helpful
in recognizing and, more importantly, interpreting
out-of-grammar utterances is unknown. We have
been unable to evaluate them so far in the Corn-
mandTalk or any other domain, although we hope
to do so in the future.
Another possible solution to the problem of
producing a workable robust recognition grammar
would return to a statistical approach rather than
using word insertions and deletions. Stolcke and
Segal (1994) describe a method for combining a
context-free grammar with an n-gram model gen-
erated from a small corpus of a few hundred utter-
ances to create a more accurate n-gram model. This
method would provide a robust recognition model
based on the context-free grammar compiled from
our unification grammar. We would still have to
write only one grammar for the system, it would still
influence the recognition model, and we could still
be sure that the system would never say anything it
couldn&apos;t recognize. This approach would require us-
ing robust parsing methods, but might be the best
solution for other domains if compiling a practical
recognition grammar proves too difficult.
Despite the success of the CommandTalk system,
it is clear that more investigation is called for to
determine how best, to develop dialogue systems in
domains with limited data. Researchers must de-
termine which types of unification grammars can be
compiled into practical recognition grammars using
existing technology, whether grammar approxima-
tions or other techniques can produce good results
for a broader range of grammars, whether allow-
ing word insertions and deletions is an effective ro-
bustness technique, or whether we should use other
methods altogether.
</bodyText>
<sectionHeader confidence="0.998434" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999680571428571">
J. Dowding, J. Gawron, D. Appelt, L. Cherny,
R. Moore, and D. Moran. 1993. Gemini: A Natu-
ral Language System for Spoken Language Under-
standing. In Proceedings of the Thirty-First An-
nual Meeting of the ACL, Columbus, OH. Associ-
ation for Computational Linguistics.
J. Dowding, E. Owen Bratt, and S. Goldwater.
1999. Interpreting Language in Context in Corn-
mandTalk. In Communicative Agents: The Use
of Natural Language in Embodied Systems, pages
63-67.
D. Martin, A. Cheyer, and D. Moran. 1998. Build-
ing Distributed Software Systems with the Open
Agent Architecture. In Proceedings of the Third
international Conference on the Practical Appli-
cation of Intelligent Agents and Multi-Agent Tech-
nology, Blackpool, Lancashire, UR. The Practical
Application Company Ltd.
R. Moore, J. Dowding, H. Bratt, J. Gawron,
Y. Gorfu, and A. Cheyer. 1997. CommandTalk:
A Spoken-Language Interface for Battlefield Sim-
ulations. In Proceedings of the Fifth Conference
on Applied Natural Language Processing, pages
1-7, Washington, DC. Association for Computa-
tional Linguistics.
M. Rayner, B. A. Hockey, F. James, E. Owen Bratt,
S. Goldwater, and J. M. Gawron. 2000. Compil-
ing Language Models from a Linguistically Moti-
vated Unification Grammar. Submitted to COL-
ING &apos;00.
A. Stent, J. Dowding, J. Gawron, E. Owen Bratt,
and R. Moore. 1999. The CommandTalk Spoken
Dialogue System. In Proceedings of the 37th An-
nual Meeting of the ACL. Association of Compu-
tational Linguistics.
</reference>
<page confidence="0.986369">
64
</page>
<reference confidence="0.9981484">
A. Stolcke and J. Segal. 1994. Precise N-Gram
Probabilities from Stochastic Context-free Gram-
mar. In Proceedings of the 3.2nd Annual Meeting
of the A ssociation for Computational Linguistics,
pages 74-79,
</reference>
<page confidence="0.999613">
65
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.916254">
<title confidence="0.999894">Building a Robust Dialogue System with Limited Data *</title>
<author confidence="0.999885">Sharon J Goldwater</author>
<author confidence="0.999885">Elizabeth Owen Bratt</author>
<author confidence="0.999885">Jean Mark Gawron</author>
<author confidence="0.999885">John</author>
<affiliation confidence="0.979602">SRI</affiliation>
<address confidence="0.973337">333 Ravenswood Menlo Park, CA</address>
<email confidence="0.99015">goldwatergai.sri.com</email>
<email confidence="0.99015">owengai.sri.com</email>
<email confidence="0.99015">gawrongai.sri.com</email>
<email confidence="0.99015">dowdinggai.sri.com</email>
<abstract confidence="0.999352142857143">We describe robustness techniques used in the CommandTalk system at the recognition level, the parsing level, and the dialogue level, and how these were influenced by the lack of domain data. We used interviews with subject matter experts (SME&apos;s) to develop a single grammar for recognition, understanding, and generation, thus eliminating the need for a robust parser. We broadened the coverage of the recognition grammar by allowing word insertions and deletions, and we implemented clarification and correction subdialogues to increase robustness at the dialogue level. We discuss the applicability of these techniques to other domains.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Dowding</author>
<author>J Gawron</author>
<author>D Appelt</author>
<author>L Cherny</author>
<author>R Moore</author>
<author>D Moran</author>
</authors>
<title>Gemini: A Natural Language System for Spoken Language Understanding.</title>
<date>1993</date>
<booktitle>In Proceedings of the Thirty-First Annual Meeting of the ACL,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, OH.</location>
<contexts>
<context position="4447" citStr="Dowding et al., 1993" startWordPosition="697" endWordPosition="700">mandTalk agents can be found in Moore et al. (1997). CommandTalk&apos;s dialogue component is described in detail in Stent et al, (1999), and its use of linguistic and situational context is described in Dowding et al. (1999). 61 3 The One-Grammar Approach In a domain with limited data, the inability to collect a sufficient corpus for training a statistical language model can be a significant problem. For CommandTalk, we did not create a statistical language model. Instead, with information gathered from interviews of subject matter experts (SME&apos;s), we developed a handwritten grammar using Gemini (Dowding et al., 1993), a unification-based grammar formalism. We used this unification grammar for both natural language understanding and generation, and, using a grammar compiler we developed, compiled it into a context-free form suitable for the speech recognizer as well. The effects of_this single-grammar approach on the robustne,ss of the CommandTalk system were twofold. On the negative side, we presumably ended up with a recognition language model with less coverage than a statistical model would have had. Our attempts to deal with this are discussed in the next section. On the positive side, we eliminated t</context>
</contexts>
<marker>Dowding, Gawron, Appelt, Cherny, Moore, Moran, 1993</marker>
<rawString>J. Dowding, J. Gawron, D. Appelt, L. Cherny, R. Moore, and D. Moran. 1993. Gemini: A Natural Language System for Spoken Language Understanding. In Proceedings of the Thirty-First Annual Meeting of the ACL, Columbus, OH. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Dowding</author>
<author>E Owen Bratt</author>
<author>S Goldwater</author>
</authors>
<title>Interpreting Language in Context in CornmandTalk. In Communicative Agents: The Use of Natural Language in Embodied Systems,</title>
<date>1999</date>
<pages>63--67</pages>
<contexts>
<context position="4046" citStr="Dowding et al. (1999)" startWordPosition="633" endWordPosition="636">ontrol ModSAF system functions, such as the map display • Get information about the state of the simulation CommandTalk consists of a number of independent, cooperating agents interacting through SRI&apos;s Open Agent Architecture (OAA) (Martin et al., 1998). OAA uses a facilitator agent that plans and coordinates interactions among agents during distributed computation. An introduction to the basic CommandTalk agents can be found in Moore et al. (1997). CommandTalk&apos;s dialogue component is described in detail in Stent et al, (1999), and its use of linguistic and situational context is described in Dowding et al. (1999). 61 3 The One-Grammar Approach In a domain with limited data, the inability to collect a sufficient corpus for training a statistical language model can be a significant problem. For CommandTalk, we did not create a statistical language model. Instead, with information gathered from interviews of subject matter experts (SME&apos;s), we developed a handwritten grammar using Gemini (Dowding et al., 1993), a unification-based grammar formalism. We used this unification grammar for both natural language understanding and generation, and, using a grammar compiler we developed, compiled it into a contex</context>
</contexts>
<marker>Dowding, Bratt, Goldwater, 1999</marker>
<rawString>J. Dowding, E. Owen Bratt, and S. Goldwater. 1999. Interpreting Language in Context in CornmandTalk. In Communicative Agents: The Use of Natural Language in Embodied Systems, pages 63-67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Martin</author>
<author>A Cheyer</author>
<author>D Moran</author>
</authors>
<title>Building Distributed Software Systems with the Open Agent Architecture.</title>
<date>1998</date>
<booktitle>In Proceedings of the Third international Conference on the Practical Application of Intelligent Agents and Multi-Agent Technology,</booktitle>
<publisher>The Practical Application Company Ltd.</publisher>
<location>Blackpool, Lancashire, UR.</location>
<contexts>
<context position="3678" citStr="Martin et al., 1998" startWordPosition="574" endWordPosition="577">eloped with the goal of allowing military commanders to interact with simulated forces in a manner as similar as possible to the way they would command actual forces. CommandTalk allows the use of ordinary English commands and mouse gestures to • Create forces and control measures (points and lines) • Assign missions to forces • Modify missions during execution • Control ModSAF system functions, such as the map display • Get information about the state of the simulation CommandTalk consists of a number of independent, cooperating agents interacting through SRI&apos;s Open Agent Architecture (OAA) (Martin et al., 1998). OAA uses a facilitator agent that plans and coordinates interactions among agents during distributed computation. An introduction to the basic CommandTalk agents can be found in Moore et al. (1997). CommandTalk&apos;s dialogue component is described in detail in Stent et al, (1999), and its use of linguistic and situational context is described in Dowding et al. (1999). 61 3 The One-Grammar Approach In a domain with limited data, the inability to collect a sufficient corpus for training a statistical language model can be a significant problem. For CommandTalk, we did not create a statistical lan</context>
</contexts>
<marker>Martin, Cheyer, Moran, 1998</marker>
<rawString>D. Martin, A. Cheyer, and D. Moran. 1998. Building Distributed Software Systems with the Open Agent Architecture. In Proceedings of the Third international Conference on the Practical Application of Intelligent Agents and Multi-Agent Technology, Blackpool, Lancashire, UR. The Practical Application Company Ltd.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Moore</author>
<author>J Dowding</author>
<author>H Bratt</author>
<author>J Gawron</author>
<author>Y Gorfu</author>
<author>A Cheyer</author>
</authors>
<title>CommandTalk: A Spoken-Language Interface for Battlefield Simulations.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>1--7</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Washington, DC.</location>
<contexts>
<context position="3877" citStr="Moore et al. (1997)" startWordPosition="605" endWordPosition="608">inary English commands and mouse gestures to • Create forces and control measures (points and lines) • Assign missions to forces • Modify missions during execution • Control ModSAF system functions, such as the map display • Get information about the state of the simulation CommandTalk consists of a number of independent, cooperating agents interacting through SRI&apos;s Open Agent Architecture (OAA) (Martin et al., 1998). OAA uses a facilitator agent that plans and coordinates interactions among agents during distributed computation. An introduction to the basic CommandTalk agents can be found in Moore et al. (1997). CommandTalk&apos;s dialogue component is described in detail in Stent et al, (1999), and its use of linguistic and situational context is described in Dowding et al. (1999). 61 3 The One-Grammar Approach In a domain with limited data, the inability to collect a sufficient corpus for training a statistical language model can be a significant problem. For CommandTalk, we did not create a statistical language model. Instead, with information gathered from interviews of subject matter experts (SME&apos;s), we developed a handwritten grammar using Gemini (Dowding et al., 1993), a unification-based grammar </context>
</contexts>
<marker>Moore, Dowding, Bratt, Gawron, Gorfu, Cheyer, 1997</marker>
<rawString>R. Moore, J. Dowding, H. Bratt, J. Gawron, Y. Gorfu, and A. Cheyer. 1997. CommandTalk: A Spoken-Language Interface for Battlefield Simulations. In Proceedings of the Fifth Conference on Applied Natural Language Processing, pages 1-7, Washington, DC. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M Rayner</author>
<author>B A Hockey</author>
<author>F James</author>
<author>E Owen Bratt</author>
</authors>
<marker>Rayner, Hockey, James, Bratt, </marker>
<rawString>M. Rayner, B. A. Hockey, F. James, E. Owen Bratt,</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>J M Gawron</author>
</authors>
<title>Compiling Language Models from a Linguistically Motivated Unification Grammar.</title>
<date>2000</date>
<note>Submitted to COLING &apos;00.</note>
<marker>Goldwater, Gawron, 2000</marker>
<rawString>S. Goldwater, and J. M. Gawron. 2000. Compiling Language Models from a Linguistically Motivated Unification Grammar. Submitted to COLING &apos;00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stent</author>
<author>J Dowding</author>
<author>J Gawron</author>
<author>E Owen Bratt</author>
<author>R Moore</author>
</authors>
<title>The CommandTalk Spoken Dialogue System.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the ACL. Association of Computational Linguistics.</booktitle>
<contexts>
<context position="3957" citStr="Stent et al, (1999)" startWordPosition="618" endWordPosition="621">s (points and lines) • Assign missions to forces • Modify missions during execution • Control ModSAF system functions, such as the map display • Get information about the state of the simulation CommandTalk consists of a number of independent, cooperating agents interacting through SRI&apos;s Open Agent Architecture (OAA) (Martin et al., 1998). OAA uses a facilitator agent that plans and coordinates interactions among agents during distributed computation. An introduction to the basic CommandTalk agents can be found in Moore et al. (1997). CommandTalk&apos;s dialogue component is described in detail in Stent et al, (1999), and its use of linguistic and situational context is described in Dowding et al. (1999). 61 3 The One-Grammar Approach In a domain with limited data, the inability to collect a sufficient corpus for training a statistical language model can be a significant problem. For CommandTalk, we did not create a statistical language model. Instead, with information gathered from interviews of subject matter experts (SME&apos;s), we developed a handwritten grammar using Gemini (Dowding et al., 1993), a unification-based grammar formalism. We used this unification grammar for both natural language understand</context>
</contexts>
<marker>Stent, Dowding, Gawron, Bratt, Moore, 1999</marker>
<rawString>A. Stent, J. Dowding, J. Gawron, E. Owen Bratt, and R. Moore. 1999. The CommandTalk Spoken Dialogue System. In Proceedings of the 37th Annual Meeting of the ACL. Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>J Segal</author>
</authors>
<title>Precise N-Gram Probabilities from Stochastic Context-free Grammar.</title>
<date>1994</date>
<booktitle>In Proceedings of the 3.2nd Annual Meeting of the A ssociation for Computational Linguistics,</booktitle>
<pages>74--79</pages>
<contexts>
<context position="15684" citStr="Stolcke and Segal (1994)" startWordPosition="2536" endWordPosition="2539">In either case, the effectiveness of our recognition robustness techniques remains an open question. We know they have no significant negative impact on ingrammar recognition, but whether they are helpful in recognizing and, more importantly, interpreting out-of-grammar utterances is unknown. We have been unable to evaluate them so far in the CornmandTalk or any other domain, although we hope to do so in the future. Another possible solution to the problem of producing a workable robust recognition grammar would return to a statistical approach rather than using word insertions and deletions. Stolcke and Segal (1994) describe a method for combining a context-free grammar with an n-gram model generated from a small corpus of a few hundred utterances to create a more accurate n-gram model. This method would provide a robust recognition model based on the context-free grammar compiled from our unification grammar. We would still have to write only one grammar for the system, it would still influence the recognition model, and we could still be sure that the system would never say anything it couldn&apos;t recognize. This approach would require using robust parsing methods, but might be the best solution for other</context>
</contexts>
<marker>Stolcke, Segal, 1994</marker>
<rawString>A. Stolcke and J. Segal. 1994. Precise N-Gram Probabilities from Stochastic Context-free Grammar. In Proceedings of the 3.2nd Annual Meeting of the A ssociation for Computational Linguistics, pages 74-79,</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>