<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000362">
<title confidence="0.882707">
Bayesian Unsupervised Topic Segmentation
</title>
<author confidence="0.993173">
Jacob Eisenstein and Regina Barzilay
</author>
<affiliation confidence="0.9985325">
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
</affiliation>
<address confidence="0.64887">
77 Massachusetts Ave., Cambridge MA 02139
</address>
<email confidence="0.99968">
{jacobe,regina}@csail.mit.edu
</email>
<sectionHeader confidence="0.994821" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999559384615385">
This paper describes a novel Bayesian ap-
proach to unsupervised topic segmentation.
Unsupervised systems for this task are driven
by lexical cohesion: the tendency of well-
formed segments to induce a compact and
consistent lexical distribution. We show that
lexical cohesion can be placed in a Bayesian
context by modeling the words in each topic
segment as draws from a multinomial lan-
guage model associated with the segment;
maximizing the observation likelihood in such
a model yields a lexically-cohesive segmenta-
tion. This contrasts with previous approaches,
which relied on hand-crafted cohesion met-
rics. The Bayesian framework provides a prin-
cipled way to incorporate additional features
such as cue phrases, a powerful indicator of
discourse structure that has not been previ-
ously used in unsupervised segmentation sys-
tems. Our model yields consistent improve-
ments over an array of state-of-the-art systems
on both text and speech datasets. We also
show that both an entropy-based analysis and
a well-known previous technique can be de-
rived as special cases of the Bayesian frame-
work.1
</bodyText>
<sectionHeader confidence="0.999161" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998316">
Topic segmentation is one of the fundamental prob-
lems in discourse analysis, where the task is to
divide a text into a linear sequence of topically-
coherent segments. Hearst’s TEXTTILING (1994)
introduced the idea that unsupervised segmentation
</bodyText>
<footnote confidence="0.939337666666667">
1Code and materials for this work are available at
http://groups.csail.mit.edu/rbg/code/
bayesseg/.
</footnote>
<bodyText confidence="0.999888138888889">
can be driven by lexical cohesion, as high-quality
segmentations feature homogeneous lexical distri-
butions within each topic segment. Lexical cohesion
has provided the inspiration for several successful
systems (e.g., Utiyama and Isahara, 2001; Galley et
al.2003; Malioutov and Barzilay, 2006), and is cur-
rently the dominant approach to unsupervised topic
segmentation.
But despite the effectiveness of lexical cohesion
for unsupervised topic segmentation, it is clear that
there are other important indicators that are ignored
by the current generation of unsupervised systems.
For example, consider cue phrases, which are ex-
plicit discourse markers such as “now” or “how-
ever” (Grosz and Sidner, 1986; Hirschberg and Lit-
man, 1993; Knott, 1996). Cue phrases have been
shown to be a useful feature for supervised topic
segmentation (Passonneau and Litman, 1993; Gal-
ley et al., 2003), but cannot be incorporated by
current unsupervised models. One reason for this
is that existing unsupervised methods use arbitrary,
hand-crafted metrics for quantifying lexical cohe-
sion, such as weighted cosine similarity (Hearst,
1994; Malioutov and Barzilay, 2006). Without su-
pervision, it is not possible to combine such met-
rics with additional sources of information. More-
over, such hand-crafted metrics may not general-
ize well across multiple datasets, and often include
parameters which must be tuned on development
sets (Malioutov and Barzilay, 2006; Galley et al.,
2003).
In this paper, we situate lexical cohesion in a
Bayesian framework, allowing other sources of in-
formation to be incorporated without the need for
labeled data. We formalize lexical cohesion in a
generative model in which the text for each seg-
</bodyText>
<page confidence="0.982833">
334
</page>
<note confidence="0.962076">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 334–343,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999697">
ment is produced by a distinct lexical distribution.
Lexically-consistent segments are favored by this
model because probability mass is conserved for
a narrow subset of words. Thus, lexical cohesion
arises naturally through the generative process, and
other sources of information – such as cue words
– can easily be incorporated as emissions from the
segment boundaries.
More formally, we treat the words in each sen-
tence as draws from a language model associated
with the topic segment. This is related to topic-
modeling methods such as latent Dirichlet allocation
(LDA; Blei et al. 2003), but here the induced topics
are tied to a linear discourse structure. This property
enables a dynamic programming solution to find the
exact maximum-likelihood segmentation. We con-
sider two approaches to handling the language mod-
els: estimating them explicitly, and integrating them
out, using the Dirichlet Compound Multinomial dis-
tribution (also known as the multivariate Polya dis-
tribution).
We model cue phrases as generated from a sep-
arate multinomial that is shared across all topics
and documents in the dataset; a high-likelihood
model will obtain a compact set of cue phrases.
The addition of cue phrases renders our dynamic
programming-based inference inapplicable, so we
design a sampling-based inference technique. This
algorithm can learn in a completely unsupervised
fashion, but it also provides a principled mechanism
to improve search through the addition of declara-
tive linguistic knowledge. This is achieved by bias-
ing the selection of samples towards boundaries with
known cue phrases; this does not change the under-
lying probabilistic model, but guides search in the
direction of linguistically-plausible segmentations.
We evaluate our algorithm on corpora of spoken
and written language, including the benchmark ICSI
meeting dataset (Janin et al., 2003) and a new tex-
tual corpus constructed from the contents of a med-
ical textbook. In both cases our model achieves per-
formance surpassing multiple state-of-the-art base-
lines. Moreover, we demonstrate that the addition of
cue phrases can further improve segmentation per-
formance over cohesion-based methods.
In addition to the practical advantages demon-
strated by these experimental results, our model re-
veals interesting theoretical properties. Other re-
searchers have observed relationships between dis-
course structure and entropy (e.g., Genzel and Char-
niak, 2002). We show that in a special case of
our model, the segmentation objective is equal to
a weighted sum of the negative entropies for each
topic segment. This finding demonstrates that a re-
lationship between discourse segmentation and en-
tropy is a natural consequence of modeling topic
structure in a generative Bayesian framework. In
addition, we show that the benchmark segmentation
system of Utiyama and Isahara (2001) can be viewed
as another special case of our Bayesian model.
</bodyText>
<sectionHeader confidence="0.999785" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999914181818182">
Existing unsupervised cohesion-based approaches
can be characterized in terms of the metric used to
quantify cohesion and the search technique. Galley
et al. (2003) characterize cohesion in terms of lexical
chains – repetitions of a given lexical item over some
fixed-length window of sentences. In their unsu-
pervised model, inference is performed by selecting
segmentation points at the local maxima of the cohe-
sion function. Malioutov and Barzilay (2006) opti-
mize a normalized minimum-cut criteria based on a
variation of the cosine similarity between sentences.
Most similar to our work is the approach of Utiyama
and Isahara (2001), who search for segmentations
with compact language models; as shown in Sec-
tion 3.1.1, this can be viewed as a special case of our
model. Both of these last two systems use dynamic
programming to search the space of segmentations.
An alternative Bayesian approach to segmentation
was proposed by Purver et al. (2006). They assume a
set of documents that is characterized by some num-
ber of hidden topics that are shared across multiple
documents. They then build a linear segmentation
by adding a switching variable to indicate whether
the topic distribution for each sentence is identical
to that of its predecessor. Unlike Purver et al., we
do not assume a dataset in which topics are shared
across multiple documents; indeed, our model can
be applied to single documents individually. Addi-
tionally, the inference procedure of Purver et al. re-
quires sampling multiple layers of hidden variables.
In contrast, our inference procedure leverages the
nature of linear segmentation to search only in the
space of segmentation points.
</bodyText>
<page confidence="0.998522">
335
</page>
<bodyText confidence="0.999966333333333">
The relationship between discourse structure and
cue phrases has been studied extensively; for an
early example of computational work on this topic,
see (Grosz, 1977). Passonneau and Litman (1993)
were the first to investigate the relationship between
cue phrases and linear segmentation. More recently,
cue phrases have been applied to topic segmentation
in the supervised setting. In a supervised system that
is distinct from the unsupervised model described
above, Galley et al. (2003) automatically identify
candidate cue phrases by mining labeled data for
words that are especially likely to appear at segment
boundaries; the presence of cue phrases is then used
as a feature in a rule-based classifier for linear topic
segmentation. Elsner and Charniak (2008) specify
a list of cue phrases by hand; the cue phrases are
used as a feature in a maximum-entropy classifier
for conversation disentanglement. Unlike these ap-
proaches, we identify candidate cue phrases auto-
matically from unlabeled data and incorporate them
in the topic segmentation task without supervision.
</bodyText>
<sectionHeader confidence="0.8947125" genericHeader="method">
3 Lexical Cohesion in a Bayesian
Framework
</sectionHeader>
<bodyText confidence="0.997612944444445">
The core idea of lexical cohesion is that topically-
coherent segments demonstrate compact and con-
sistent lexical distributions (Halliday and Hasan,
1976). Lexical cohesion can be placed in a prob-
abilistic context by modeling the words in each
topic segment as draws from a multinomial language
model associated with the segment. Formally, if sen-
tence t is in segment j, then the bag of words xt
is drawn from the multinomial language model θj.
This is similar in spirit to hidden topic models such
as latent Dirichlet allocation (Blei et al., 2003), but
rather than assigning a hidden topic to each word,
we constrain the topics to yield a linear segmenta-
tion of the document.
We will assume that topic breaks occur at sen-
tence boundaries, and write zt to indicate the topic
assignment for sentence t. The observation likeli-
hood is,
</bodyText>
<equation confidence="0.996443333333333">
T
p(X|z, Θ) = ri p(xt|θzt), (1)
t
</equation>
<bodyText confidence="0.999605318181818">
where X is the set of all T sentences, z is the vector
of segment assignments for each sentence, and Θ is
the set of all K language models.2 A linear segmen-
tation is ensured by the additional constraint that zt
must be equal to either zt−1 (the previous sentence’s
segment) or zt−1 + 1 (the next segment).
To obtain a high likelihood, the language mod-
els associated with each segment should concentrate
their probability mass on a compact subset of words.
Language models that spread their probability mass
over a broad set of words will induce a lower likeli-
hood. This is consistent with the principle of lexical
cohesion.
Thus far, we have described a segmentation in
terms of two parameters: the segment indices z, and
the set of language models Θ. For the task of seg-
menting documents, we are interested only in the
segment indices, and would prefer not to have to
search in the space of language models as well. We
consider two alternatives: taking point estimates of
the language models (Section 3.1), and analytically
marginalizing them out (Section 3.2).
</bodyText>
<subsectionHeader confidence="0.795221">
3.1 Setting the language model to the posterior
expectation
</subsectionHeader>
<bodyText confidence="0.999991583333333">
One way to handle the language models is to choose
a single point estimate for each set of segmenta-
tion points z. Suppose that each language model
is drawn from a symmetric Dirichlet prior: θj —
Dir(θ0). Let nj be a vector in which each element is
the sum of the lexical counts over all the sentences
in segment j: nj,i = E{t:zt=j} mt,i, where mt,i is
the count of word i in sentence t. Assuming that
each xt — θj, then the posterior distribution for θj
is Dirichlet with vector parameter nj +θ0 (Bernardo
and Smith, 2000). The expected value of this distri-
bution is the multinomial distribution ˆθj, where,
</bodyText>
<equation confidence="0.998106666666667">
ˆθj,i =nj,i + θ0 (2)
�W .
i nj,i + Wθ0
</equation>
<bodyText confidence="0.952347111111111">
In this equation, W indicates the number of words
in the vocabulary. Having obtained an estimate for
the language model ˆθj, the observed data likelihood
for segment j is a product over each sentence in the
segment,
2Our experiments will assume that the number of topics K
is known. This is common practice for this task, as the desired
number of segments may be determined by the user (Malioutov
and Barzilay, 2006).
</bodyText>
<page confidence="0.929119">
336
</page>
<equation confidence="0.993637285714286">
Y
p({xt : zt = j}|�Bj) =
{t:zt=j}
Y=
{t:zt=j}
�Bnj,i
j,i �(5)
</equation>
<bodyText confidence="0.99985025">
By viewing the likelihood as a product over all
terms in the vocabulary, we observe interesting con-
nections with prior work on segmentation and infor-
mation theory.
</bodyText>
<subsectionHeader confidence="0.559619">
3.1.1 Connection to previous work
</subsectionHeader>
<bodyText confidence="0.995385391304348">
In this section, we explain how our model gen-
eralizes the well-known method of Utiyama and
Isahara (2001; hereafter U&amp;I). As in our work,
Utiyama and Isahara propose a probabilistic frame-
work based on maximizing the compactness of the
language models induced for each segment. Their
likelihood equation is identical to our equations 3-5.
They then define the language models for each seg-
ment as �Bj,i = nj,iW1 , without rigorous justifi-
W+Ei nj,i
cation. This form is equivalent to Laplacian smooth-
ing (Manning and Sch¨utze, 1999), and is a special
case of our equation 2, with B0 = 1. Thus, the lan-
guage models in U&amp;I can be viewed as the expec-
tation of the posterior distribution p(Bj|{xt : zt =
j}, B0), in the special case that B0 = 1. Our ap-
proach generalizes U&amp;I and provides a Bayesian
justification for the language models that they ap-
ply. The remainder of the paper further extends this
work by marginalizing out the language model, and
by adding cue phrases. We empirically demonstrate
that these extensions substantially improve perfor-
mance.
</bodyText>
<subsectionHeader confidence="0.760782">
3.1.2 Connection to entropy
</subsectionHeader>
<bodyText confidence="0.999979333333333">
Our model also has a connection to entropy,
and situates entropy-based segmentation within a
Bayesian framework. Equation 1 defines the objec-
tive function as a product across sentences; using
equations 3-5 we can decompose this across seg-
ments instead. Working in logarithms,
</bodyText>
<equation confidence="0.96078">
log p(X|z, 6) =
</equation>
<bodyText confidence="0.9998528">
The last line substitutes in the logarithm of equa-
tion 5. Setting B0 = 0 and rearranging equation 2,
we obtain nj,i = Nj�Bj,i, with Nj = PW i nj,i, the
total number of words in segment j. Substituting
this into equation 6, we obtain
</bodyText>
<equation confidence="0.942176666666667">
log p(X|z,
XK
j
</equation>
<bodyText confidence="0.999817777777778">
where H(Bj) is the negative entropy of the multino-
mial Bj. Thus, with B0 = 0, the log conditional prob-
ability in equation 6 is optimized by a segmentation
that minimizes the weighted sum of entropies per
segment, where the weights are equal to the segment
lengths. This result suggests intriguing connections
with prior work on the relationship between entropy
and discourse structure (e.g., Genzel and Charniak,
2002; Sporleder and Lapata, 2006).
</bodyText>
<subsectionHeader confidence="0.999791">
3.2 Marginalizing the language model
</subsectionHeader>
<bodyText confidence="0.999991166666667">
The previous subsection uses point estimates of
the language models to reveal connections to en-
tropy and prior work on segmentation. However,
point estimates are theoretically unsatisfying from
a Bayesian perspective, and better performance may
be obtained by marginalizing over all possible lan-
</bodyText>
<equation confidence="0.9101246">
Y Bj,i (3)
iExt
�Bmt,i (4)
j,i
W
Y
i
W
Y
i
XT log p(xt|�Bzt)
t
XK X log p(xt|�Bj)
j {t:zt=j}
nj,i log Bj,i (6)
</equation>
<figure confidence="0.865772666666667">
XK
j
XW
i
6) = XK X ki log ki
j Nj
i
Bj),
NjH(
</figure>
<page confidence="0.686812">
337
</page>
<equation confidence="0.8612166">
guage models:
K
p(X|z, θ0) = Y Y p(xt|θ0)
j {t:zt=j1
YK Z Y
dθj p(xt|θj)p(θj|θ0)
j {t:zt=j1
K
= Y pdcm({xt : zt = j}|θ0), (7)
j
</equation>
<bodyText confidence="0.976522352941177">
each segment, so the overall likelihood for the point-
estimate version also decomposes across segments.
Any objective function that can be decomposed
into a product across segments can be maximized
using dynamic programming. We define B(t) as the
value of the objective function for the optimal seg-
mentation up to sentence t. The contribution to the
objective function from a single segment between
sentences t&apos; and t is written,
b(t&apos;, t) = p({xt, ... xt}|zt-...t = j)
where pdcm refers to the Dirichlet compound multi-
nomial distribution (DCM), also known as the multi-
variate Polya distribution (Johnson et al., 1997). The
DCM distribution expresses the expectation over all
multinomial language models, when conditioning
on the Dirichlet prior θ0. When θ0 is a symmetric
Dirichlet prior,
</bodyText>
<equation confidence="0.9992915">
pdcm({xt : zt = j}|θ0)
F(Wθ0)
=
F(Nj + Wθ0)
</equation>
<bodyText confidence="0.999903111111111">
where nj,i is the count of word i in segment j, and
Nj = PWi nj,i, the total number of words in the
segment. The symbol F refers to the Gamma func-
tion, an extension of the factorial function to real
numbers. Using the DCM distribution, we can com-
pute the data likelihood for each segment from the
lexical counts over the entire segment. The overall
observation likelihood is a product across the likeli-
hoods for each segment.
</bodyText>
<subsectionHeader confidence="0.999617">
3.3 Objective function and inference
</subsectionHeader>
<bodyText confidence="0.9884528">
The optimal segmentation maximizes the joint prob-
ability,
p(X, z|θ0) = p(X|z, θ0)p(z).
We assume that p(z) is a uniform distribution over
valid segmentations, and assigns no probability
mass to invalid segmentations. The data likelihood
is defined for point estimate language models in
equation 5 and for marginalized language models
in equation 7. Note that equation 7 is written as a
product over segments. The point estimates for the
language models depend only on the counts within
The maximum value of the objective function
is then given by the recurrence relation, B(t) =
maxt,&lt;t B(t&apos;)b(t&apos;+1, t), with the base case B(0) =
1. These values can be stored in a table of size T
(equal to the number of sentences); this admits a dy-
namic program that performs inference in polyno-
mial time.3 If the number of segments is specified
in advance, the dynamic program is slightly more
complex, with a table of size TK.
</bodyText>
<subsectionHeader confidence="0.959054">
3.4 Priors
</subsectionHeader>
<bodyText confidence="0.9999308">
The Dirichlet compound multinomial integrates
over language models, but we must still set the
prior θ0. We can re-estimate this prior based on
the observed data by interleaving gradient-based
search in a Viterbi expectation-maximization frame-
work (Gauvain and Lee, 1994). In the E-step, we
estimate a segmentation z� of the dataset, as de-
scribed in Section 3.3. In the M-step, we maxi-
mize p(θ0|X, z) ∝ p(X|θ0, z)p(θ0). Assuming a
non-informative hyperprior p(θ0), we maximize the
likelihood in Equation 7 across all documents. The
maximization is performed using a gradient-based
search; the gradients are dervied by Minka (2003).
This procedure is iterated until convergence or a
maximum of twenty iterations.
</bodyText>
<sectionHeader confidence="0.99128" genericHeader="method">
4 Cue Phrases
</sectionHeader>
<bodyText confidence="0.999851333333333">
One of the key advantages of a Bayesian framework
for topic segmentation is that it permits the prin-
cipled combination of multiple data sources, even
</bodyText>
<footnote confidence="0.99788375">
3This assumes that the objective function for individual seg-
ments can also be computed efficiently. In our case, we need
only keep vectors of counts for each segment, and evaluate
probability density functions over the counts.
</footnote>
<equation confidence="0.9655832">
F(nj,i + Wθ0)
F(θ0) ,
W
Y
i
</equation>
<page confidence="0.972624">
338
</page>
<bodyText confidence="0.985328821428572">
without labeled data. We are especially interested
in cue phrases, which are explicit markers for dis-
course structure, such as “now” or “first” (Grosz
and Sidner, 1986; Hirschberg and Litman, 1993;
Knott, 1996). Cue phrases have previously been
used in supervised topic segmentation (e.g., Gal-
ley et al. 2003); we show how they can be used in
an unsupervised setting.
The previous section modeled lexical cohesion by
treating the bag of words in each sentence as a se-
ries of draws from a multinomial language model
indexed by the topic segment. To incorporate cue
phrases, this generative model is modified to reflect
the idea that some of the text will be topic-specific,
but other terms will be topic-neutral cue phrases
that express discourse structure. This idea is imple-
mented by drawing the text at each topic boundary
from a special language model φ, which is shared
across all topics and all documents in the dataset.
For sentences that are not at segment bound-
aries, the likelihood is as before: p(xt|z, o, φ) =
Q
i∈xt θzt,i. For sentences that immediately follow
segment boundaries, we draw the first ` words from
φ instead. Writing x�`)
t for the ` cue words in xt,
and Rt for the remaining words, the likelihood for a
segment-initial sentence is,
</bodyText>
<equation confidence="0.997622333333333">
Yp(xt|zt =� zt−1, o, φ) =
i∈x��)
t
</equation>
<bodyText confidence="0.9998976">
We draw φ from a symmetric Dirichlet prior φ0. Fol-
lowing prior work (Galley et al., 2003; Litman and
Passonneau, 1995), we consider only the first word
of each sentence as a potential cue phrase; thus, we
set ` = 1 in all experiments.
</bodyText>
<subsectionHeader confidence="0.853361">
4.1 Inference
</subsectionHeader>
<bodyText confidence="0.999945575757576">
To estimate or marginalize the language models o
and φ, it is necessary to maintain lexical counts for
each segment and for the segment boundaries. The
counts for φ are summed across every segment in
the entire dataset, so shifting a boundary will af-
fect the probability of every segment, not only the
adjacent segments as before. Thus, the factoriza-
tion that enabled dynamic programming inference
in Section 3.3 is no longer applicable. Instead, we
must resort to approximate inference.
Sampling-based inference is frequently used in
related Bayesian models. Such approaches build
a stationary Markov chain by repeatedly sampling
among the hidden variables in the model. The most
commonly-used sampling-based technique is Gibbs
sampling, which iteratively samples from the condi-
tional distribution of each hidden variable (Bishop,
2006). However, Gibbs sampling is slow to con-
verge to a stationary distribution when the hidden
variables are tightly coupled. This is the case in
linear topic segmentation, due to the constraint that
zt E {zt−1, zt−1 + 11 (see Section 3).
For this reason, we apply the more general
Metropolis-Hastings algorithm, which permits sam-
pling arbitrary transformations of the latent vari-
ables. In our framework, such transformations cor-
respond to moves through the space of possible seg-
mentations. A new segmentation z0 is drawn from
the previous hypothesized segmentation z based on
a proposal distribution q(z0|z).4 The probability of
accepting a proposed transformation depends on the
ratio of the joint probabilities and a correction term
for asymmetries in the proposal distribution:
</bodyText>
<equation confidence="0.998689">
paccept(z z0) = min �
1, p(X, z0|θ0, φ0) q(z|z0) .
p(X, z|θ0, φ0) q(z0|z)
</equation>
<bodyText confidence="0.999851">
The Metropolis-Hastings algorithm guarantees
that by accepting samples at this ratio, our sampling
procedure will converge to the stationary distribu-
tion for the hidden variables z. When cue phrases
are included, the observation likelihood is written:
</bodyText>
<equation confidence="0.9853186">
Y
p(X|z, o, φ) =
{t:zt6=zt−1}
YX
{t:zt=zt−1}
</equation>
<bodyText confidence="0.999944">
As in Section 3.2, we can marginalize over the
language models. We obtain a product of DCM dis-
tributions: one for each segment, and one for all cue
phrases in the dataset.
</bodyText>
<subsectionHeader confidence="0.99376">
4.2 Proposal distribution
</subsectionHeader>
<bodyText confidence="0.7952965">
Metropolis-Hastings requires a proposal distribution
to sample new configurations. The proposal distri-
4Because the cue phrase language model 0 is used across
the entire dataset, transformations affect the likelihood of all
documents in the corpus. For clarity, our exposition will focus
on the single-document case.
</bodyText>
<equation confidence="0.7916184">
Yφi θzt,i.
i∈�xt
Yφi θzt,i
i∈�xt
Y
i∈x��)
t
Y
i∈xt
θzt,i.
</equation>
<page confidence="0.994992">
339
</page>
<bodyText confidence="0.999934277777778">
bution does not affect the underlying probabilistic
model – Metropolis-Hastings will converge to the
same underlying distribution for any non-degenerate
proposal. However, a well-chosen proposal distribu-
tion can substantially speed convergence.
Our basic proposal distribution selects an existing
segmentation point with uniform probability, and
considers a set of local moves. The proposal is con-
structed so that no probability mass is allocated to
moves that change the order of segment boundaries,
or merge two segments; one consequence of this re-
striction is that moves cannot add or remove seg-
ments.5 We set the proposal distribution to decrease
exponentially with the move distance, thus favoring
incremental transformations to the segmentation.
More formally, let d(z —* z&apos;) &gt; 0 equal the dis-
tance that the selected segmentation point is moved
when we transform the segmentation from z to z&apos;.
We can write the proposal distribution q(z&apos;  |z) a
c(z —* z&apos;)d(z —* z&apos;)A, where A &lt; 0 sets the rate
of exponential decay and c is an indicator function
enforcing the constraint that the moves do not reach
or cross existing segmentation points.6
We can also incorporate declarative linguistic
knowledge by biasing the proposal distribution in
favor of moves that place boundaries near known
cue phrase markers. We multiply the unnormalized
chance of proposing a move to location z —* z&apos; by a
term equal to one plus the number of candidate cue
phrases in the segment-initial sentences in the new
configuration z&apos;, written num-cue(z&apos;). Formally,
qling(z&apos;  |z&apos;) a (1 + num-cue(z&apos;))q(z&apos;  |z). We
use a list of cue phrases identified by Hirschberg and
Litman (1993). We evaluate our model with both the
basic and linguistically-enhanced proposal distribu-
tions.
</bodyText>
<subsectionHeader confidence="0.999019">
4.3 Priors
</subsectionHeader>
<bodyText confidence="0.999807333333333">
As in section 3.4, we set the priors 00 and 00 us-
ing gradient-based search. In this case, we perform
gradient-based optimization after epochs of 1000
</bodyText>
<footnote confidence="0.754128666666667">
5Permitting moves to change the number of segments would
substantially complicate inference.
6We set a = − &apos;
</footnote>
<bodyText confidence="0.9991512">
max-move, where max-move is the maximum
move-length, set to 5 in our experiments. These parameters af-
fect the rate of convergence but are unrelated to the underly-
ing probability model. In the limit of enough samples, all non-
pathological settings will yield the same segmentation results.
Metropolis-Hasting steps. Interleaving sampling-
based inference with direct optimization of param-
eters can be considered a form of Monte Carlo
Expectation-Maximization (MCEM; Wei and Tan-
ner, 1990).
</bodyText>
<sectionHeader confidence="0.989582" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.998008184210526">
Corpora We evaluate our approach on corpora
from two different domains: transcribed meetings
and written text.
For multi-speaker meetings, we use the ICSI cor-
pus of meeting transcripts (Janin et al., 2003), which
is becoming a standard for speech segmentation
(e.g., Galley et al. 2003; Purver et al. 2006). This
dataset includes transcripts of 75 multi-party meet-
ings, of which 25 are annotated for segment bound-
aries.
For text, we introduce a dataset in which each
document is a chapter selected from a medical text-
book (Walker et al., 1990).7 The task is to divide
each chapter into the sections indicated by the au-
thor. This dataset contains 227 chapters, with 1136
sections (an average of 5.00 per chapter). Each
chapter contains an average of 140 sentences, giv-
ing an average of 28 sentences per segment.
Metrics All experiments are evaluated in terms
of the commonly-used Pk (Beeferman et al., 1999)
and WindowDiff (WD) (Pevzner and Hearst, 2002)
scores. Both metrics pass a window through the
document, and assess whether the sentences on the
edges of the window are properly segmented with
respect to each other. WindowDiff is stricter in
that it requires that the number of intervening seg-
ments between the two sentences be identical in
the hypothesized and the reference segmentations,
while Pk only asks whether the two sentences are in
the same segment or not. Pk and WindowDiff are
penalties, so lower values indicate better segmenta-
tions. We use the evaluation source code provided
by Malioutov and Barzilay (2006).
System configuration We evaluate our Bayesian
approach both with and without cue phrases. With-
out cue phrases, we use the dynamic programming
inference described in section 3.3. This system is
referred to as BAYESSEG in Table 1. When adding
</bodyText>
<footnote confidence="0.996787">
7The full text of this book is available for free download at
http://onlinebooks.library.upenn.edu.
</footnote>
<page confidence="0.99705">
340
</page>
<bodyText confidence="0.993837936170213">
cue phrases, we use the Metropolis-Hastings model
described in 4.1. Both basic and linguistically-
motivated proposal distributions are evaluated (see
Section 4.2); these are referred to as BAYESSEG-
CUE and BAYESSEG-CUE-PROP in the table.
For the sampling-based systems, results are av-
eraged over five runs. The initial configuration is
obtained from the dynamic programming inference,
and then 100,000 sampling iterations are performed.
The final segmentation is obtained by annealing the
last 25,000 iterations to a temperature of zero. The
use of annealing to obtain a maximum a posteri-
ori (MAP) configuration from sampling-based in-
ference is common (e.g., Finkel 2005; Goldwater
2007). The total running time of our system is on the
order of three minutes per document. Due to mem-
ory constraints, we divide the textbook dataset into
ten parts, and perform inference in each part sepa-
rately. We may achieve better results by performing
inference over the entire dataset simultaneously, due
to pooling counts for cue phrases across all docu-
ments.
Baselines We compare against three com-
petitive alternative systems from the literature:
U&amp;I (Utiyama and Isahara, 2001); LCSEG (Galley
et al., 2003); MCS (Malioutov and Barzilay, 2006).
All three systems are described in the related work
(Section 2). In all cases, we use the publicly avail-
able executables provided by the authors.
Parameter settings For LCSEG, we use the pa-
rameter values specified in the paper (Galley et al.,
2003). MCS requires parameter settings to be tuned
on a development set. Our corpora do not include
development sets, so tuning was performed using the
lecture transcript corpus described by Malioutov and
Barzilay (2006). Our system does not require pa-
rameter tuning; priors are re-estimated as described
in Sections 3.4 and 4.3. U&amp;I requires no parameter
tuning, and is used “out of the box.” In all exper-
iments, we assume that the number of desired seg-
ments is provided.
Preprocessing Standard preprocessing techniques
are applied to the text for all comparisons. The
Porter (1980) stemming algorithm is applied to
group equivalent lexical items. A set of stop-words
is also removed, using the same list originally em-
ployed by several competitive systems (Choi, 2000;
</bodyText>
<table confidence="0.999777428571429">
Textbook Pk WD
U&amp;I .370 .376
MCS .368 .382
LCSEG .370 .385
BAYESSEG .339 .353
BAYESSEG-CUE .339 .353
BAYESSEG-CUE-PROP .343 .355
Meetings Pk WD
U&amp;I .297 .347
MCS .370 .411
LCSEG .309 .322
BAYESSEG .264 .319
BAYESSEG-CUE .261 .316
BAYESSEG-CUE-PROP .258 .312
</table>
<tableCaption confidence="0.8820785">
Table 1: Comparison of segmentation algorithms. Both
metrics are penalties, so lower scores indicate bet-
</tableCaption>
<bodyText confidence="0.839707571428572">
ter performance. BAYESSEG is the cohesion-only
Bayesian system with marginalized language mod-
els. BAYESSEG-CUE is the Bayesian system with cue
phrases. BAYESSEG-CUE-PROP adds the linguistically-
motivated proposal distribution.
Utiyama and Isahara, 2001; Malioutov and Barzilay,
2006).
</bodyText>
<sectionHeader confidence="0.999968" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.9999735">
Table 1 presents the performance results for three
instantiations of our Bayesian framework and three
competitive alternative systems. As shown in the ta-
ble, the Bayesian models achieve the best results on
both metrics for both corpora. On the medical text-
book corpus, the Bayesian systems achieve a raw
performance gain of 2-3% with respect to all base-
lines on both metrics. On the ICSI meeting corpus,
the Bayesian systems perform 4-5% better than the
best baseline on the Pk metric, and achieve smaller
improvement on the WindowDiff metric. The results
on the meeting corpus also compare favorably with
the topic-modeling method of Purver et al. (2006),
who report a Pk of .289 and a WindowDiff of .329.
Another observation from Table 1 is that the con-
tribution of cue phrases depends on the dataset. Cue
phrases improve performance on the meeting cor-
pus, but not on the textbook corpus. The effective-
ness of cue phrases as a feature depends on whether
the writer or speaker uses them consistently. At the
</bodyText>
<page confidence="0.993637">
341
</page>
<table confidence="0.998513818181818">
Meetings Textbook
okay* 234.4 the 1345.9
I 212.6 this 14.3
so* 113.4 it 4.1
um 91.7 these 4.1
and* 67.3 a 2.9
yeah 10.5 on 2.1
but* 9.4 most 2.0
uh 4.8 heart 1.8
right 2.4 creating 1.8
agenda 1.3 hundred 1.8
</table>
<tableCaption confidence="0.975805">
Table 2: Cue phrases selected by our unsupervised
model, sorted by chi-squared. Boldface indicates that the
chi-squared value is significant at the level of p &lt; .01.
Asterisks indicate cue phrases that were extracted by the
supervised procedure of Galley et al. (2003).
</tableCaption>
<bodyText confidence="0.999981464285714">
same time, the addition of cue phrases prevents the
use of exact inference techniques, which may ex-
plain the decline in results for the meetings dataset.
To investigate the quality of the cue phrases that
our model extracts, we list its top ten cue phrases
for each dataset in Table 2. Cue phrases are ranked
by their chi-squared value, which is computed based
on the number of occurrences for each word at the
beginning of a hypothesized segment, as compared
to the expectation. For cue phrases listed in bold,
the chi-squared value is statistically significant at
the level of p &lt; .01, indicating that the frequency
with which the cue phrase appears at the beginning
of segments is unlikely to be a chance phenomenon.
As shown in the left column of the table, our
model has identified several strong cue phrases from
the meeting dataset which appear to be linguistically
plausible. Galley et al. (2003) performed a simi-
lar chi-squared analysis, but used the true segment
boundaries in the labeled data; this can be thought
of as a sort of ground truth. Four of the ten cue
phrases identified by our system overlap with their
analysis; these are indicated with asterisks. In con-
trast to our model’s success at extracting cue phrases
from the meeting dataset, only very common words
are selected for the textbook dataset. This may help
to explain why cue phrases improve performance for
meeting transcripts, but not for the textbook.
</bodyText>
<sectionHeader confidence="0.998261" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999992875">
This paper presents a novel Bayesian approach to
unsupervised topic segmentation. Our algorithm is
capable of incorporating both lexical cohesion and
cue phrase features in a principled manner, and out-
performs state-of-the-art baselines on text and tran-
scribed speech corpora. We have developed exact
and sampling-based inference techniques, both of
which search only over the space of segmentations
and marginalize out the associated language mod-
els. Finally, we have shown that our model provides
a theoretical framework with connections to infor-
mation theory, while also generalizing and justify-
ing prior work. In the future, we hope to explore the
use of similar Bayesian techniques for hierarchical
segmentation, and to incorporate additional features
such as prosody and speaker change information.
</bodyText>
<sectionHeader confidence="0.999024" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998593230769231">
The authors acknowledge the support of the Na-
tional Science Foundation (CAREER grant IIS-
0448168) and the Microsoft Research Faculty Fel-
lowship. Thanks to Aaron Adler, S. R. K. Branavan,
Harr Chen, Michael Collins, Randall Davis, Dan
Roy, David Sontag and the anonymous reviewers for
helpful comments and suggestions. We also thank
Michel Galley, Igor Malioutov, and Masao Utiyama
for making their topic segmentation code publically
available. Any opinions, findings, and conclusions
or recommendations expressed above are those of
the authors and do not necessarily reflect the views
of the NSF.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999841692307692">
Doug Beeferman, Adam Berger, and John D. Lafferty.
1999. Statistical models for text segmentation. Ma-
chine Learning, 34(1-3):177–210.
Jos´e M. Bernardo and Adrian F. M. Smith. 2000.
Bayesian Theory. Wiley.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Springer.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Machine
Learning Research, 3:993–1022.
Freddy Y. Y. Choi. 2000. Advances in domain inde-
pendent linear text segmentation. In Proceedings of
NAACL, pages 26–33.
</reference>
<page confidence="0.980437">
342
</page>
<reference confidence="0.999079741176471">
Micha Elsner and Eugene Charniak. 2008. You Talk-
ing to Me? A Corpus and Algorithm for Conversation
Disentanglement. In Proceedings of ACL.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of ACL, pages 363–370.
Michel Galley, Kathleen R. McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse seg-
mentation of multi-party conversation. Proceedings of
ACL, pages 562–569.
Jean-Luc Gauvain and Chin-Hui Lee. 1994. Maximum
a posteriori estimation for multivariate Gaussian mix-
ture observations of Markov chains. IEEE Transac-
tions on Speech and Audio Processing, 2(2):291–298.
Dmitriy Genzel and Eugene Charniak. 2002. Entropy
rate constancy in text. In Proceedings of ACL, pages
199–206.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of ACL, pages 744–751.
Barbara Grosz and Candace Sidner. 1986. Attention,
intentions, and the structure of discourse. Computa-
tional Linguistics, 12(3):175–204.
Barbara Grosz. 1977. The representation and use of fo-
cus in dialogue understanding. Technical Report 151,
Artificial Intelligence Center, SRI International.
M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. Longman.
Marti A. Hearst. 1994. Multi-paragraph segmentation of
expository text. In Proceedings of ACL, pages 9–16.
Julia Hirschberg and Diane Litman. 1993. Empirical
studies on the disambiguation of cue phrases. Com-
putational Linguistics, 19(3):501–530.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, et al. 2003. The ICSI Meeting Corpus. Acous-
tics, Speech, and Signal Processing, 2003. Proceed-
ings.(ICASSP’03). 2003 IEEE International Confer-
ence on, 1.
Norman L. Johnson, Samuel Kotz, and N. Balakrishnan.
1997. Discrete Multivariate Distributions. Wiley.
Alistair Knott. 1996. A Data-Driven Methodology for
Motivating a Set of Coherence Relations. Ph.D. thesis,
University of Edinburgh.
Diane J. Litman and Rebecca J. Passonneau. 1995. Com-
bining multiple knowledge sources for discourse seg-
mentation. In Proceedings of the ACL, pages 108–115.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Pro-
ceedings of ACL, pages 25–32.
Christopher D. Manning and Hinrich Sch¨utze. 1999.
Foundations of Statistical Natural Language Process-
ing. The MIT Press.
Thomas P. Minka. 2003. Estimating a dirichlet distri-
bution. Technical report, Massachusetts Institute of
Technology.
Rebecca Passonneau and Diane Litman. 1993. Intention-
based segmentation: Human reliability and correlation
with linguistic cues. In Proceedings of ACL, pages
148–155.
Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Computational Linguistics, 28(1):19–36.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 14:130–137.
M. Purver, T.L. Griffiths, K.P. K¨ording, and J.B. Tenen-
baum. 2006. Unsupervised topic modelling for multi-
party spoken discourse. In Proceedings of ACL, pages
17–24.
Caroline Sporleder and Mirella Lapata. 2006. Broad
coverage paragraph segmentation across languages
and domains. ACM Transactions on Speech and Lan-
guage Processing, 3(2):1–35.
Masao Utiyama and Hitoshi Isahara. 2001. A statistical
model for domain-independent text segmentation. In
Proceedings of ACL, pages 491–498.
H. Kenneth Walker, W. Dallas Hall, and J. Willis Hurst,
editors. 1990. Clinical Methods: The History, Physi-
cal, and Laboratory Examinations. Butterworths.
Greg C. G. Wei and Martin A. Tanner. 1990. A
monte carlo implementation of the EM algorithm and
the poor man’s data augmentation algorithms. Jour-
nal of the American Statistical Association, 85(411),
September.
</reference>
<page confidence="0.999352">
343
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.411193">
<title confidence="0.999915">Bayesian Unsupervised Topic Segmentation</title>
<author confidence="0.973743">Eisenstein</author>
<affiliation confidence="0.9792645">Computer Science and Artificial Intelligence Massachusetts Institute of</affiliation>
<address confidence="0.831032">77 Massachusetts Ave., Cambridge MA</address>
<abstract confidence="0.99925144">This paper describes a novel Bayesian approach to unsupervised topic segmentation. Unsupervised systems for this task are driven the tendency of wellformed segments to induce a compact and consistent lexical distribution. We show that lexical cohesion can be placed in a Bayesian context by modeling the words in each topic segment as draws from a multinomial language model associated with the segment; maximizing the observation likelihood in such a model yields a lexically-cohesive segmentation. This contrasts with previous approaches, which relied on hand-crafted cohesion metrics. The Bayesian framework provides a principled way to incorporate additional features such as cue phrases, a powerful indicator of discourse structure that has not been previously used in unsupervised segmentation systems. Our model yields consistent improvements over an array of state-of-the-art systems on both text and speech datasets. We also show that both an entropy-based analysis and a well-known previous technique can be de-</abstract>
<note confidence="0.529739">rived as special cases of the Bayesian frame-</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Doug Beeferman</author>
<author>Adam Berger</author>
<author>John D Lafferty</author>
</authors>
<title>Statistical models for text segmentation.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="26201" citStr="Beeferman et al., 1999" startWordPosition="4241" endWordPosition="4244">03; Purver et al. 2006). This dataset includes transcripts of 75 multi-party meetings, of which 25 are annotated for segment boundaries. For text, we introduce a dataset in which each document is a chapter selected from a medical textbook (Walker et al., 1990).7 The task is to divide each chapter into the sections indicated by the author. This dataset contains 227 chapters, with 1136 sections (an average of 5.00 per chapter). Each chapter contains an average of 140 sentences, giving an average of 28 sentences per segment. Metrics All experiments are evaluated in terms of the commonly-used Pk (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002) scores. Both metrics pass a window through the document, and assess whether the sentences on the edges of the window are properly segmented with respect to each other. WindowDiff is stricter in that it requires that the number of intervening segments between the two sentences be identical in the hypothesized and the reference segmentations, while Pk only asks whether the two sentences are in the same segment or not. Pk and WindowDiff are penalties, so lower values indicate better segmentations. We use the evaluation source code provided by Maliou</context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1999</marker>
<rawString>Doug Beeferman, Adam Berger, and John D. Lafferty. 1999. Statistical models for text segmentation. Machine Learning, 34(1-3):177–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e M Bernardo</author>
<author>Adrian F M Smith</author>
</authors>
<title>Bayesian Theory.</title>
<date>2000</date>
<publisher>Wiley.</publisher>
<contexts>
<context position="11810" citStr="Bernardo and Smith, 2000" startWordPosition="1850" endWordPosition="1853">ally marginalizing them out (Section 3.2). 3.1 Setting the language model to the posterior expectation One way to handle the language models is to choose a single point estimate for each set of segmentation points z. Suppose that each language model is drawn from a symmetric Dirichlet prior: θj — Dir(θ0). Let nj be a vector in which each element is the sum of the lexical counts over all the sentences in segment j: nj,i = E{t:zt=j} mt,i, where mt,i is the count of word i in sentence t. Assuming that each xt — θj, then the posterior distribution for θj is Dirichlet with vector parameter nj +θ0 (Bernardo and Smith, 2000). The expected value of this distribution is the multinomial distribution ˆθj, where, ˆθj,i =nj,i + θ0 (2) �W . i nj,i + Wθ0 In this equation, W indicates the number of words in the vocabulary. Having obtained an estimate for the language model ˆθj, the observed data likelihood for segment j is a product over each sentence in the segment, 2Our experiments will assume that the number of topics K is known. This is common practice for this task, as the desired number of segments may be determined by the user (Malioutov and Barzilay, 2006). 336 Y p({xt : zt = j}|�Bj) = {t:zt=j} Y= {t:zt=j} �Bnj,i </context>
</contexts>
<marker>Bernardo, Smith, 2000</marker>
<rawString>Jos´e M. Bernardo and Adrian F. M. Smith. 2000. Bayesian Theory. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<date>2006</date>
<booktitle>Pattern Recognition and Machine Learning.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="21042" citStr="Bishop, 2006" startWordPosition="3420" endWordPosition="3421">hifting a boundary will affect the probability of every segment, not only the adjacent segments as before. Thus, the factorization that enabled dynamic programming inference in Section 3.3 is no longer applicable. Instead, we must resort to approximate inference. Sampling-based inference is frequently used in related Bayesian models. Such approaches build a stationary Markov chain by repeatedly sampling among the hidden variables in the model. The most commonly-used sampling-based technique is Gibbs sampling, which iteratively samples from the conditional distribution of each hidden variable (Bishop, 2006). However, Gibbs sampling is slow to converge to a stationary distribution when the hidden variables are tightly coupled. This is the case in linear topic segmentation, due to the constraint that zt E {zt−1, zt−1 + 11 (see Section 3). For this reason, we apply the more general Metropolis-Hastings algorithm, which permits sampling arbitrary transformations of the latent variables. In our framework, such transformations correspond to moves through the space of possible segmentations. A new segmentation z0 is drawn from the previous hypothesized segmentation z based on a proposal distribution q(z</context>
</contexts>
<marker>Bishop, 2006</marker>
<rawString>Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="4178" citStr="Blei et al. 2003" startWordPosition="618" endWordPosition="621"> Computational Linguistics ment is produced by a distinct lexical distribution. Lexically-consistent segments are favored by this model because probability mass is conserved for a narrow subset of words. Thus, lexical cohesion arises naturally through the generative process, and other sources of information – such as cue words – can easily be incorporated as emissions from the segment boundaries. More formally, we treat the words in each sentence as draws from a language model associated with the topic segment. This is related to topicmodeling methods such as latent Dirichlet allocation (LDA; Blei et al. 2003), but here the induced topics are tied to a linear discourse structure. This property enables a dynamic programming solution to find the exact maximum-likelihood segmentation. We consider two approaches to handling the language models: estimating them explicitly, and integrating them out, using the Dirichlet Compound Multinomial distribution (also known as the multivariate Polya distribution). We model cue phrases as generated from a separate multinomial that is shared across all topics and documents in the dataset; a high-likelihood model will obtain a compact set of cue phrases. The addition</context>
<context position="9846" citStr="Blei et al., 2003" startWordPosition="1501" endWordPosition="1504">n task without supervision. 3 Lexical Cohesion in a Bayesian Framework The core idea of lexical cohesion is that topicallycoherent segments demonstrate compact and consistent lexical distributions (Halliday and Hasan, 1976). Lexical cohesion can be placed in a probabilistic context by modeling the words in each topic segment as draws from a multinomial language model associated with the segment. Formally, if sentence t is in segment j, then the bag of words xt is drawn from the multinomial language model θj. This is similar in spirit to hidden topic models such as latent Dirichlet allocation (Blei et al., 2003), but rather than assigning a hidden topic to each word, we constrain the topics to yield a linear segmentation of the document. We will assume that topic breaks occur at sentence boundaries, and write zt to indicate the topic assignment for sentence t. The observation likelihood is, T p(X|z, Θ) = ri p(xt|θzt), (1) t where X is the set of all T sentences, z is the vector of segment assignments for each sentence, and Θ is the set of all K language models.2 A linear segmentation is ensured by the additional constraint that zt must be equal to either zt−1 (the previous sentence’s segment) or zt−1</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Freddy Y Y Choi</author>
</authors>
<title>Advances in domain independent linear text segmentation.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>26--33</pages>
<contexts>
<context position="29414" citStr="Choi, 2000" startWordPosition="4752" endWordPosition="4753">e transcript corpus described by Malioutov and Barzilay (2006). Our system does not require parameter tuning; priors are re-estimated as described in Sections 3.4 and 4.3. U&amp;I requires no parameter tuning, and is used “out of the box.” In all experiments, we assume that the number of desired segments is provided. Preprocessing Standard preprocessing techniques are applied to the text for all comparisons. The Porter (1980) stemming algorithm is applied to group equivalent lexical items. A set of stop-words is also removed, using the same list originally employed by several competitive systems (Choi, 2000; Textbook Pk WD U&amp;I .370 .376 MCS .368 .382 LCSEG .370 .385 BAYESSEG .339 .353 BAYESSEG-CUE .339 .353 BAYESSEG-CUE-PROP .343 .355 Meetings Pk WD U&amp;I .297 .347 MCS .370 .411 LCSEG .309 .322 BAYESSEG .264 .319 BAYESSEG-CUE .261 .316 BAYESSEG-CUE-PROP .258 .312 Table 1: Comparison of segmentation algorithms. Both metrics are penalties, so lower scores indicate better performance. BAYESSEG is the cohesion-only Bayesian system with marginalized language models. BAYESSEG-CUE is the Bayesian system with cue phrases. BAYESSEG-CUE-PROP adds the linguisticallymotivated proposal distribution. Utiyama an</context>
</contexts>
<marker>Choi, 2000</marker>
<rawString>Freddy Y. Y. Choi. 2000. Advances in domain independent linear text segmentation. In Proceedings of NAACL, pages 26–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Eugene Charniak</author>
</authors>
<title>You Talking to Me? A Corpus and Algorithm for Conversation Disentanglement.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="8946" citStr="Elsner and Charniak (2008)" startWordPosition="1355" endWordPosition="1358">is topic, see (Grosz, 1977). Passonneau and Litman (1993) were the first to investigate the relationship between cue phrases and linear segmentation. More recently, cue phrases have been applied to topic segmentation in the supervised setting. In a supervised system that is distinct from the unsupervised model described above, Galley et al. (2003) automatically identify candidate cue phrases by mining labeled data for words that are especially likely to appear at segment boundaries; the presence of cue phrases is then used as a feature in a rule-based classifier for linear topic segmentation. Elsner and Charniak (2008) specify a list of cue phrases by hand; the cue phrases are used as a feature in a maximum-entropy classifier for conversation disentanglement. Unlike these approaches, we identify candidate cue phrases automatically from unlabeled data and incorporate them in the topic segmentation task without supervision. 3 Lexical Cohesion in a Bayesian Framework The core idea of lexical cohesion is that topicallycoherent segments demonstrate compact and consistent lexical distributions (Halliday and Hasan, 1976). Lexical cohesion can be placed in a probabilistic context by modeling the words in each topic</context>
</contexts>
<marker>Elsner, Charniak, 2008</marker>
<rawString>Micha Elsner and Eugene Charniak. 2008. You Talking to Me? A Corpus and Algorithm for Conversation Disentanglement. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>363--370</pages>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of ACL, pages 363–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Eric FoslerLussier, and Hongyan Jing.</title>
<date>2003</date>
<booktitle>Proceedings of ACL,</booktitle>
<pages>562--569</pages>
<marker>Galley, McKeown, 2003</marker>
<rawString>Michel Galley, Kathleen R. McKeown, Eric FoslerLussier, and Hongyan Jing. 2003. Discourse segmentation of multi-party conversation. Proceedings of ACL, pages 562–569.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Luc Gauvain</author>
<author>Chin-Hui Lee</author>
</authors>
<date>1994</date>
<note>Maximum</note>
<contexts>
<context position="17798" citStr="Gauvain and Lee, 1994" startWordPosition="2878" endWordPosition="2881">(t) = maxt,&lt;t B(t&apos;)b(t&apos;+1, t), with the base case B(0) = 1. These values can be stored in a table of size T (equal to the number of sentences); this admits a dynamic program that performs inference in polynomial time.3 If the number of segments is specified in advance, the dynamic program is slightly more complex, with a table of size TK. 3.4 Priors The Dirichlet compound multinomial integrates over language models, but we must still set the prior θ0. We can re-estimate this prior based on the observed data by interleaving gradient-based search in a Viterbi expectation-maximization framework (Gauvain and Lee, 1994). In the E-step, we estimate a segmentation z� of the dataset, as described in Section 3.3. In the M-step, we maximize p(θ0|X, z) ∝ p(X|θ0, z)p(θ0). Assuming a non-informative hyperprior p(θ0), we maximize the likelihood in Equation 7 across all documents. The maximization is performed using a gradient-based search; the gradients are dervied by Minka (2003). This procedure is iterated until convergence or a maximum of twenty iterations. 4 Cue Phrases One of the key advantages of a Bayesian framework for topic segmentation is that it permits the principled combination of multiple data sources, </context>
</contexts>
<marker>Gauvain, Lee, 1994</marker>
<rawString>Jean-Luc Gauvain and Chin-Hui Lee. 1994. Maximum</rawString>
</citation>
<citation valid="false">
<title>a posteriori estimation for multivariate Gaussian mixture observations of Markov chains.</title>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>2</volume>
<issue>2</issue>
<marker></marker>
<rawString>a posteriori estimation for multivariate Gaussian mixture observations of Markov chains. IEEE Transactions on Speech and Audio Processing, 2(2):291–298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitriy Genzel</author>
<author>Eugene Charniak</author>
</authors>
<title>Entropy rate constancy in text.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>199--206</pages>
<contexts>
<context position="6016" citStr="Genzel and Charniak, 2002" startWordPosition="891" endWordPosition="895">including the benchmark ICSI meeting dataset (Janin et al., 2003) and a new textual corpus constructed from the contents of a medical textbook. In both cases our model achieves performance surpassing multiple state-of-the-art baselines. Moreover, we demonstrate that the addition of cue phrases can further improve segmentation performance over cohesion-based methods. In addition to the practical advantages demonstrated by these experimental results, our model reveals interesting theoretical properties. Other researchers have observed relationships between discourse structure and entropy (e.g., Genzel and Charniak, 2002). We show that in a special case of our model, the segmentation objective is equal to a weighted sum of the negative entropies for each topic segment. This finding demonstrates that a relationship between discourse segmentation and entropy is a natural consequence of modeling topic structure in a generative Bayesian framework. In addition, we show that the benchmark segmentation system of Utiyama and Isahara (2001) can be viewed as another special case of our Bayesian model. 2 Related Work Existing unsupervised cohesion-based approaches can be characterized in terms of the metric used to quant</context>
<context position="14655" citStr="Genzel and Charniak, 2002" startWordPosition="2339" endWordPosition="2342">m of equation 5. Setting B0 = 0 and rearranging equation 2, we obtain nj,i = Nj�Bj,i, with Nj = PW i nj,i, the total number of words in segment j. Substituting this into equation 6, we obtain log p(X|z, XK j where H(Bj) is the negative entropy of the multinomial Bj. Thus, with B0 = 0, the log conditional probability in equation 6 is optimized by a segmentation that minimizes the weighted sum of entropies per segment, where the weights are equal to the segment lengths. This result suggests intriguing connections with prior work on the relationship between entropy and discourse structure (e.g., Genzel and Charniak, 2002; Sporleder and Lapata, 2006). 3.2 Marginalizing the language model The previous subsection uses point estimates of the language models to reveal connections to entropy and prior work on segmentation. However, point estimates are theoretically unsatisfying from a Bayesian perspective, and better performance may be obtained by marginalizing over all possible lanY Bj,i (3) iExt �Bmt,i (4) j,i W Y i W Y i XT log p(xt|�Bzt) t XK X log p(xt|�Bj) j {t:zt=j} nj,i log Bj,i (6) XK j XW i 6) = XK X ki log ki j Nj i Bj), NjH( 337 guage models: K p(X|z, θ0) = Y Y p(xt|θ0) j {t:zt=j1 YK Z Y dθj p(xt|θj)p(θ</context>
</contexts>
<marker>Genzel, Charniak, 2002</marker>
<rawString>Dmitriy Genzel and Eugene Charniak. 2002. Entropy rate constancy in text. In Proceedings of ACL, pages 199–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Tom Griffiths</author>
</authors>
<title>A fully bayesian approach to unsupervised part-of-speech tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>744--751</pages>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Sharon Goldwater and Tom Griffiths. 2007. A fully bayesian approach to unsupervised part-of-speech tagging. In Proceedings of ACL, pages 744–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Grosz</author>
<author>Candace Sidner</author>
</authors>
<title>Attention, intentions, and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="2405" citStr="Grosz and Sidner, 1986" startWordPosition="345" endWordPosition="348">neous lexical distributions within each topic segment. Lexical cohesion has provided the inspiration for several successful systems (e.g., Utiyama and Isahara, 2001; Galley et al.2003; Malioutov and Barzilay, 2006), and is currently the dominant approach to unsupervised topic segmentation. But despite the effectiveness of lexical cohesion for unsupervised topic segmentation, it is clear that there are other important indicators that are ignored by the current generation of unsupervised systems. For example, consider cue phrases, which are explicit discourse markers such as “now” or “however” (Grosz and Sidner, 1986; Hirschberg and Litman, 1993; Knott, 1996). Cue phrases have been shown to be a useful feature for supervised topic segmentation (Passonneau and Litman, 1993; Galley et al., 2003), but cannot be incorporated by current unsupervised models. One reason for this is that existing unsupervised methods use arbitrary, hand-crafted metrics for quantifying lexical cohesion, such as weighted cosine similarity (Hearst, 1994; Malioutov and Barzilay, 2006). Without supervision, it is not possible to combine such metrics with additional sources of information. Moreover, such hand-crafted metrics may not ge</context>
<context position="18829" citStr="Grosz and Sidner, 1986" startWordPosition="3048" endWordPosition="3051">a maximum of twenty iterations. 4 Cue Phrases One of the key advantages of a Bayesian framework for topic segmentation is that it permits the principled combination of multiple data sources, even 3This assumes that the objective function for individual segments can also be computed efficiently. In our case, we need only keep vectors of counts for each segment, and evaluate probability density functions over the counts. F(nj,i + Wθ0) F(θ0) , W Y i 338 without labeled data. We are especially interested in cue phrases, which are explicit markers for discourse structure, such as “now” or “first” (Grosz and Sidner, 1986; Hirschberg and Litman, 1993; Knott, 1996). Cue phrases have previously been used in supervised topic segmentation (e.g., Galley et al. 2003); we show how they can be used in an unsupervised setting. The previous section modeled lexical cohesion by treating the bag of words in each sentence as a series of draws from a multinomial language model indexed by the topic segment. To incorporate cue phrases, this generative model is modified to reflect the idea that some of the text will be topic-specific, but other terms will be topic-neutral cue phrases that express discourse structure. This idea </context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Barbara Grosz and Candace Sidner. 1986. Attention, intentions, and the structure of discourse. Computational Linguistics, 12(3):175–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Grosz</author>
</authors>
<title>The representation and use of focus in dialogue understanding.</title>
<date>1977</date>
<tech>Technical Report 151,</tech>
<publisher>Longman.</publisher>
<institution>Artificial Intelligence Center, SRI</institution>
<contexts>
<context position="8347" citStr="Grosz, 1977" startWordPosition="1266" endWordPosition="1267"> to that of its predecessor. Unlike Purver et al., we do not assume a dataset in which topics are shared across multiple documents; indeed, our model can be applied to single documents individually. Additionally, the inference procedure of Purver et al. requires sampling multiple layers of hidden variables. In contrast, our inference procedure leverages the nature of linear segmentation to search only in the space of segmentation points. 335 The relationship between discourse structure and cue phrases has been studied extensively; for an early example of computational work on this topic, see (Grosz, 1977). Passonneau and Litman (1993) were the first to investigate the relationship between cue phrases and linear segmentation. More recently, cue phrases have been applied to topic segmentation in the supervised setting. In a supervised system that is distinct from the unsupervised model described above, Galley et al. (2003) automatically identify candidate cue phrases by mining labeled data for words that are especially likely to appear at segment boundaries; the presence of cue phrases is then used as a feature in a rule-based classifier for linear topic segmentation. Elsner and Charniak (2008) </context>
</contexts>
<marker>Grosz, 1977</marker>
<rawString>Barbara Grosz. 1977. The representation and use of focus in dialogue understanding. Technical Report 151, Artificial Intelligence Center, SRI International. M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion in English. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Multi-paragraph segmentation of expository text.</title>
<date>1994</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="2822" citStr="Hearst, 1994" startWordPosition="410" endWordPosition="411">tors that are ignored by the current generation of unsupervised systems. For example, consider cue phrases, which are explicit discourse markers such as “now” or “however” (Grosz and Sidner, 1986; Hirschberg and Litman, 1993; Knott, 1996). Cue phrases have been shown to be a useful feature for supervised topic segmentation (Passonneau and Litman, 1993; Galley et al., 2003), but cannot be incorporated by current unsupervised models. One reason for this is that existing unsupervised methods use arbitrary, hand-crafted metrics for quantifying lexical cohesion, such as weighted cosine similarity (Hearst, 1994; Malioutov and Barzilay, 2006). Without supervision, it is not possible to combine such metrics with additional sources of information. Moreover, such hand-crafted metrics may not generalize well across multiple datasets, and often include parameters which must be tuned on development sets (Malioutov and Barzilay, 2006; Galley et al., 2003). In this paper, we situate lexical cohesion in a Bayesian framework, allowing other sources of information to be incorporated without the need for labeled data. We formalize lexical cohesion in a generative model in which the text for each seg334 Proceedin</context>
</contexts>
<marker>Hearst, 1994</marker>
<rawString>Marti A. Hearst. 1994. Multi-paragraph segmentation of expository text. In Proceedings of ACL, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hirschberg</author>
<author>Diane Litman</author>
</authors>
<title>Empirical studies on the disambiguation of cue phrases.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>3</issue>
<contexts>
<context position="2434" citStr="Hirschberg and Litman, 1993" startWordPosition="349" endWordPosition="353">ons within each topic segment. Lexical cohesion has provided the inspiration for several successful systems (e.g., Utiyama and Isahara, 2001; Galley et al.2003; Malioutov and Barzilay, 2006), and is currently the dominant approach to unsupervised topic segmentation. But despite the effectiveness of lexical cohesion for unsupervised topic segmentation, it is clear that there are other important indicators that are ignored by the current generation of unsupervised systems. For example, consider cue phrases, which are explicit discourse markers such as “now” or “however” (Grosz and Sidner, 1986; Hirschberg and Litman, 1993; Knott, 1996). Cue phrases have been shown to be a useful feature for supervised topic segmentation (Passonneau and Litman, 1993; Galley et al., 2003), but cannot be incorporated by current unsupervised models. One reason for this is that existing unsupervised methods use arbitrary, hand-crafted metrics for quantifying lexical cohesion, such as weighted cosine similarity (Hearst, 1994; Malioutov and Barzilay, 2006). Without supervision, it is not possible to combine such metrics with additional sources of information. Moreover, such hand-crafted metrics may not generalize well across multiple</context>
<context position="18858" citStr="Hirschberg and Litman, 1993" startWordPosition="3052" endWordPosition="3055">ations. 4 Cue Phrases One of the key advantages of a Bayesian framework for topic segmentation is that it permits the principled combination of multiple data sources, even 3This assumes that the objective function for individual segments can also be computed efficiently. In our case, we need only keep vectors of counts for each segment, and evaluate probability density functions over the counts. F(nj,i + Wθ0) F(θ0) , W Y i 338 without labeled data. We are especially interested in cue phrases, which are explicit markers for discourse structure, such as “now” or “first” (Grosz and Sidner, 1986; Hirschberg and Litman, 1993; Knott, 1996). Cue phrases have previously been used in supervised topic segmentation (e.g., Galley et al. 2003); we show how they can be used in an unsupervised setting. The previous section modeled lexical cohesion by treating the bag of words in each sentence as a series of draws from a multinomial language model indexed by the topic segment. To incorporate cue phrases, this generative model is modified to reflect the idea that some of the text will be topic-specific, but other terms will be topic-neutral cue phrases that express discourse structure. This idea is implemented by drawing the</context>
<context position="24423" citStr="Hirschberg and Litman (1993)" startWordPosition="3955" endWordPosition="3958"> an indicator function enforcing the constraint that the moves do not reach or cross existing segmentation points.6 We can also incorporate declarative linguistic knowledge by biasing the proposal distribution in favor of moves that place boundaries near known cue phrase markers. We multiply the unnormalized chance of proposing a move to location z —* z&apos; by a term equal to one plus the number of candidate cue phrases in the segment-initial sentences in the new configuration z&apos;, written num-cue(z&apos;). Formally, qling(z&apos; |z&apos;) a (1 + num-cue(z&apos;))q(z&apos; |z). We use a list of cue phrases identified by Hirschberg and Litman (1993). We evaluate our model with both the basic and linguistically-enhanced proposal distributions. 4.3 Priors As in section 3.4, we set the priors 00 and 00 using gradient-based search. In this case, we perform gradient-based optimization after epochs of 1000 5Permitting moves to change the number of segments would substantially complicate inference. 6We set a = − &apos; max-move, where max-move is the maximum move-length, set to 5 in our experiments. These parameters affect the rate of convergence but are unrelated to the underlying probability model. In the limit of enough samples, all nonpathologic</context>
</contexts>
<marker>Hirschberg, Litman, 1993</marker>
<rawString>Julia Hirschberg and Diane Litman. 1993. Empirical studies on the disambiguation of cue phrases. Computational Linguistics, 19(3):501–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Janin</author>
<author>D Baron</author>
<author>J Edwards</author>
<author>D Ellis</author>
<author>D Gelbart</author>
<author>N Morgan</author>
<author>B Peskin</author>
<author>T Pfau</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
</authors>
<date>2003</date>
<booktitle>The ICSI Meeting Corpus. Acoustics, Speech, and Signal Processing, 2003. Proceedings.(ICASSP’03). 2003 IEEE International Conference on,</booktitle>
<pages>1</pages>
<contexts>
<context position="5455" citStr="Janin et al., 2003" startWordPosition="809" endWordPosition="812">rence inapplicable, so we design a sampling-based inference technique. This algorithm can learn in a completely unsupervised fashion, but it also provides a principled mechanism to improve search through the addition of declarative linguistic knowledge. This is achieved by biasing the selection of samples towards boundaries with known cue phrases; this does not change the underlying probabilistic model, but guides search in the direction of linguistically-plausible segmentations. We evaluate our algorithm on corpora of spoken and written language, including the benchmark ICSI meeting dataset (Janin et al., 2003) and a new textual corpus constructed from the contents of a medical textbook. In both cases our model achieves performance surpassing multiple state-of-the-art baselines. Moreover, we demonstrate that the addition of cue phrases can further improve segmentation performance over cohesion-based methods. In addition to the practical advantages demonstrated by these experimental results, our model reveals interesting theoretical properties. Other researchers have observed relationships between discourse structure and entropy (e.g., Genzel and Charniak, 2002). We show that in a special case of our</context>
<context position="25500" citStr="Janin et al., 2003" startWordPosition="4122" endWordPosition="4125">meters affect the rate of convergence but are unrelated to the underlying probability model. In the limit of enough samples, all nonpathological settings will yield the same segmentation results. Metropolis-Hasting steps. Interleaving samplingbased inference with direct optimization of parameters can be considered a form of Monte Carlo Expectation-Maximization (MCEM; Wei and Tanner, 1990). 5 Experimental Setup Corpora We evaluate our approach on corpora from two different domains: transcribed meetings and written text. For multi-speaker meetings, we use the ICSI corpus of meeting transcripts (Janin et al., 2003), which is becoming a standard for speech segmentation (e.g., Galley et al. 2003; Purver et al. 2006). This dataset includes transcripts of 75 multi-party meetings, of which 25 are annotated for segment boundaries. For text, we introduce a dataset in which each document is a chapter selected from a medical textbook (Walker et al., 1990).7 The task is to divide each chapter into the sections indicated by the author. This dataset contains 227 chapters, with 1136 sections (an average of 5.00 per chapter). Each chapter contains an average of 140 sentences, giving an average of 28 sentences per seg</context>
</contexts>
<marker>Janin, Baron, Edwards, Ellis, Gelbart, Morgan, Peskin, Pfau, Shriberg, Stolcke, 2003</marker>
<rawString>A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, et al. 2003. The ICSI Meeting Corpus. Acoustics, Speech, and Signal Processing, 2003. Proceedings.(ICASSP’03). 2003 IEEE International Conference on, 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norman L Johnson</author>
<author>Samuel Kotz</author>
<author>N Balakrishnan</author>
</authors>
<title>Discrete Multivariate Distributions.</title>
<date>1997</date>
<publisher>Wiley.</publisher>
<contexts>
<context position="15924" citStr="Johnson et al., 1997" startWordPosition="2563" endWordPosition="2566">7) j each segment, so the overall likelihood for the pointestimate version also decomposes across segments. Any objective function that can be decomposed into a product across segments can be maximized using dynamic programming. We define B(t) as the value of the objective function for the optimal segmentation up to sentence t. The contribution to the objective function from a single segment between sentences t&apos; and t is written, b(t&apos;, t) = p({xt, ... xt}|zt-...t = j) where pdcm refers to the Dirichlet compound multinomial distribution (DCM), also known as the multivariate Polya distribution (Johnson et al., 1997). The DCM distribution expresses the expectation over all multinomial language models, when conditioning on the Dirichlet prior θ0. When θ0 is a symmetric Dirichlet prior, pdcm({xt : zt = j}|θ0) F(Wθ0) = F(Nj + Wθ0) where nj,i is the count of word i in segment j, and Nj = PWi nj,i, the total number of words in the segment. The symbol F refers to the Gamma function, an extension of the factorial function to real numbers. Using the DCM distribution, we can compute the data likelihood for each segment from the lexical counts over the entire segment. The overall observation likelihood is a product</context>
</contexts>
<marker>Johnson, Kotz, Balakrishnan, 1997</marker>
<rawString>Norman L. Johnson, Samuel Kotz, and N. Balakrishnan. 1997. Discrete Multivariate Distributions. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alistair Knott</author>
</authors>
<title>A Data-Driven Methodology for Motivating a Set of Coherence Relations.</title>
<date>1996</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="2448" citStr="Knott, 1996" startWordPosition="354" endWordPosition="355">. Lexical cohesion has provided the inspiration for several successful systems (e.g., Utiyama and Isahara, 2001; Galley et al.2003; Malioutov and Barzilay, 2006), and is currently the dominant approach to unsupervised topic segmentation. But despite the effectiveness of lexical cohesion for unsupervised topic segmentation, it is clear that there are other important indicators that are ignored by the current generation of unsupervised systems. For example, consider cue phrases, which are explicit discourse markers such as “now” or “however” (Grosz and Sidner, 1986; Hirschberg and Litman, 1993; Knott, 1996). Cue phrases have been shown to be a useful feature for supervised topic segmentation (Passonneau and Litman, 1993; Galley et al., 2003), but cannot be incorporated by current unsupervised models. One reason for this is that existing unsupervised methods use arbitrary, hand-crafted metrics for quantifying lexical cohesion, such as weighted cosine similarity (Hearst, 1994; Malioutov and Barzilay, 2006). Without supervision, it is not possible to combine such metrics with additional sources of information. Moreover, such hand-crafted metrics may not generalize well across multiple datasets, and</context>
<context position="18872" citStr="Knott, 1996" startWordPosition="3056" endWordPosition="3057">the key advantages of a Bayesian framework for topic segmentation is that it permits the principled combination of multiple data sources, even 3This assumes that the objective function for individual segments can also be computed efficiently. In our case, we need only keep vectors of counts for each segment, and evaluate probability density functions over the counts. F(nj,i + Wθ0) F(θ0) , W Y i 338 without labeled data. We are especially interested in cue phrases, which are explicit markers for discourse structure, such as “now” or “first” (Grosz and Sidner, 1986; Hirschberg and Litman, 1993; Knott, 1996). Cue phrases have previously been used in supervised topic segmentation (e.g., Galley et al. 2003); we show how they can be used in an unsupervised setting. The previous section modeled lexical cohesion by treating the bag of words in each sentence as a series of draws from a multinomial language model indexed by the topic segment. To incorporate cue phrases, this generative model is modified to reflect the idea that some of the text will be topic-specific, but other terms will be topic-neutral cue phrases that express discourse structure. This idea is implemented by drawing the text at each </context>
</contexts>
<marker>Knott, 1996</marker>
<rawString>Alistair Knott. 1996. A Data-Driven Methodology for Motivating a Set of Coherence Relations. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane J Litman</author>
<author>Rebecca J Passonneau</author>
</authors>
<title>Combining multiple knowledge sources for discourse segmentation.</title>
<date>1995</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>108--115</pages>
<contexts>
<context position="20074" citStr="Litman and Passonneau, 1995" startWordPosition="3265" endWordPosition="3268"> drawing the text at each topic boundary from a special language model φ, which is shared across all topics and all documents in the dataset. For sentences that are not at segment boundaries, the likelihood is as before: p(xt|z, o, φ) = Q i∈xt θzt,i. For sentences that immediately follow segment boundaries, we draw the first ` words from φ instead. Writing x�`) t for the ` cue words in xt, and Rt for the remaining words, the likelihood for a segment-initial sentence is, Yp(xt|zt =� zt−1, o, φ) = i∈x��) t We draw φ from a symmetric Dirichlet prior φ0. Following prior work (Galley et al., 2003; Litman and Passonneau, 1995), we consider only the first word of each sentence as a potential cue phrase; thus, we set ` = 1 in all experiments. 4.1 Inference To estimate or marginalize the language models o and φ, it is necessary to maintain lexical counts for each segment and for the segment boundaries. The counts for φ are summed across every segment in the entire dataset, so shifting a boundary will affect the probability of every segment, not only the adjacent segments as before. Thus, the factorization that enabled dynamic programming inference in Section 3.3 is no longer applicable. Instead, we must resort to appr</context>
</contexts>
<marker>Litman, Passonneau, 1995</marker>
<rawString>Diane J. Litman and Rebecca J. Passonneau. 1995. Combining multiple knowledge sources for discourse segmentation. In Proceedings of the ACL, pages 108–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Malioutov</author>
<author>Regina Barzilay</author>
</authors>
<title>Minimum cut model for spoken lecture segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="1997" citStr="Malioutov and Barzilay, 2006" startWordPosition="283" endWordPosition="286">gmentation is one of the fundamental problems in discourse analysis, where the task is to divide a text into a linear sequence of topicallycoherent segments. Hearst’s TEXTTILING (1994) introduced the idea that unsupervised segmentation 1Code and materials for this work are available at http://groups.csail.mit.edu/rbg/code/ bayesseg/. can be driven by lexical cohesion, as high-quality segmentations feature homogeneous lexical distributions within each topic segment. Lexical cohesion has provided the inspiration for several successful systems (e.g., Utiyama and Isahara, 2001; Galley et al.2003; Malioutov and Barzilay, 2006), and is currently the dominant approach to unsupervised topic segmentation. But despite the effectiveness of lexical cohesion for unsupervised topic segmentation, it is clear that there are other important indicators that are ignored by the current generation of unsupervised systems. For example, consider cue phrases, which are explicit discourse markers such as “now” or “however” (Grosz and Sidner, 1986; Hirschberg and Litman, 1993; Knott, 1996). Cue phrases have been shown to be a useful feature for supervised topic segmentation (Passonneau and Litman, 1993; Galley et al., 2003), but cannot</context>
<context position="6967" citStr="Malioutov and Barzilay (2006)" startWordPosition="1042" endWordPosition="1045">ework. In addition, we show that the benchmark segmentation system of Utiyama and Isahara (2001) can be viewed as another special case of our Bayesian model. 2 Related Work Existing unsupervised cohesion-based approaches can be characterized in terms of the metric used to quantify cohesion and the search technique. Galley et al. (2003) characterize cohesion in terms of lexical chains – repetitions of a given lexical item over some fixed-length window of sentences. In their unsupervised model, inference is performed by selecting segmentation points at the local maxima of the cohesion function. Malioutov and Barzilay (2006) optimize a normalized minimum-cut criteria based on a variation of the cosine similarity between sentences. Most similar to our work is the approach of Utiyama and Isahara (2001), who search for segmentations with compact language models; as shown in Section 3.1.1, this can be viewed as a special case of our model. Both of these last two systems use dynamic programming to search the space of segmentations. An alternative Bayesian approach to segmentation was proposed by Purver et al. (2006). They assume a set of documents that is characterized by some number of hidden topics that are shared a</context>
<context position="12351" citStr="Malioutov and Barzilay, 2006" startWordPosition="1946" endWordPosition="1949">distribution for θj is Dirichlet with vector parameter nj +θ0 (Bernardo and Smith, 2000). The expected value of this distribution is the multinomial distribution ˆθj, where, ˆθj,i =nj,i + θ0 (2) �W . i nj,i + Wθ0 In this equation, W indicates the number of words in the vocabulary. Having obtained an estimate for the language model ˆθj, the observed data likelihood for segment j is a product over each sentence in the segment, 2Our experiments will assume that the number of topics K is known. This is common practice for this task, as the desired number of segments may be determined by the user (Malioutov and Barzilay, 2006). 336 Y p({xt : zt = j}|�Bj) = {t:zt=j} Y= {t:zt=j} �Bnj,i j,i �(5) By viewing the likelihood as a product over all terms in the vocabulary, we observe interesting connections with prior work on segmentation and information theory. 3.1.1 Connection to previous work In this section, we explain how our model generalizes the well-known method of Utiyama and Isahara (2001; hereafter U&amp;I). As in our work, Utiyama and Isahara propose a probabilistic framework based on maximizing the compactness of the language models induced for each segment. Their likelihood equation is identical to our equations 3</context>
<context position="26824" citStr="Malioutov and Barzilay (2006)" startWordPosition="4342" endWordPosition="4345"> 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002) scores. Both metrics pass a window through the document, and assess whether the sentences on the edges of the window are properly segmented with respect to each other. WindowDiff is stricter in that it requires that the number of intervening segments between the two sentences be identical in the hypothesized and the reference segmentations, while Pk only asks whether the two sentences are in the same segment or not. Pk and WindowDiff are penalties, so lower values indicate better segmentations. We use the evaluation source code provided by Malioutov and Barzilay (2006). System configuration We evaluate our Bayesian approach both with and without cue phrases. Without cue phrases, we use the dynamic programming inference described in section 3.3. This system is referred to as BAYESSEG in Table 1. When adding 7The full text of this book is available for free download at http://onlinebooks.library.upenn.edu. 340 cue phrases, we use the Metropolis-Hastings model described in 4.1. Both basic and linguisticallymotivated proposal distributions are evaluated (see Section 4.2); these are referred to as BAYESSEGCUE and BAYESSEG-CUE-PROP in the table. For the sampling-</context>
<context position="28401" citStr="Malioutov and Barzilay, 2006" startWordPosition="4585" endWordPosition="4588">guration from sampling-based inference is common (e.g., Finkel 2005; Goldwater 2007). The total running time of our system is on the order of three minutes per document. Due to memory constraints, we divide the textbook dataset into ten parts, and perform inference in each part separately. We may achieve better results by performing inference over the entire dataset simultaneously, due to pooling counts for cue phrases across all documents. Baselines We compare against three competitive alternative systems from the literature: U&amp;I (Utiyama and Isahara, 2001); LCSEG (Galley et al., 2003); MCS (Malioutov and Barzilay, 2006). All three systems are described in the related work (Section 2). In all cases, we use the publicly available executables provided by the authors. Parameter settings For LCSEG, we use the parameter values specified in the paper (Galley et al., 2003). MCS requires parameter settings to be tuned on a development set. Our corpora do not include development sets, so tuning was performed using the lecture transcript corpus described by Malioutov and Barzilay (2006). Our system does not require parameter tuning; priors are re-estimated as described in Sections 3.4 and 4.3. U&amp;I requires no parameter</context>
<context position="30060" citStr="Malioutov and Barzilay, 2006" startWordPosition="4843" endWordPosition="4846">U&amp;I .370 .376 MCS .368 .382 LCSEG .370 .385 BAYESSEG .339 .353 BAYESSEG-CUE .339 .353 BAYESSEG-CUE-PROP .343 .355 Meetings Pk WD U&amp;I .297 .347 MCS .370 .411 LCSEG .309 .322 BAYESSEG .264 .319 BAYESSEG-CUE .261 .316 BAYESSEG-CUE-PROP .258 .312 Table 1: Comparison of segmentation algorithms. Both metrics are penalties, so lower scores indicate better performance. BAYESSEG is the cohesion-only Bayesian system with marginalized language models. BAYESSEG-CUE is the Bayesian system with cue phrases. BAYESSEG-CUE-PROP adds the linguisticallymotivated proposal distribution. Utiyama and Isahara, 2001; Malioutov and Barzilay, 2006). 6 Results Table 1 presents the performance results for three instantiations of our Bayesian framework and three competitive alternative systems. As shown in the table, the Bayesian models achieve the best results on both metrics for both corpora. On the medical textbook corpus, the Bayesian systems achieve a raw performance gain of 2-3% with respect to all baselines on both metrics. On the ICSI meeting corpus, the Bayesian systems perform 4-5% better than the best baseline on the Pk metric, and achieve smaller improvement on the WindowDiff metric. The results on the meeting corpus also compa</context>
</contexts>
<marker>Malioutov, Barzilay, 2006</marker>
<rawString>Igor Malioutov and Regina Barzilay. 2006. Minimum cut model for spoken lecture segmentation. In Proceedings of ACL, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>The MIT Press.</publisher>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas P Minka</author>
</authors>
<title>Estimating a dirichlet distribution.</title>
<date>2003</date>
<tech>Technical report,</tech>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="18157" citStr="Minka (2003)" startWordPosition="2938" endWordPosition="2939">pound multinomial integrates over language models, but we must still set the prior θ0. We can re-estimate this prior based on the observed data by interleaving gradient-based search in a Viterbi expectation-maximization framework (Gauvain and Lee, 1994). In the E-step, we estimate a segmentation z� of the dataset, as described in Section 3.3. In the M-step, we maximize p(θ0|X, z) ∝ p(X|θ0, z)p(θ0). Assuming a non-informative hyperprior p(θ0), we maximize the likelihood in Equation 7 across all documents. The maximization is performed using a gradient-based search; the gradients are dervied by Minka (2003). This procedure is iterated until convergence or a maximum of twenty iterations. 4 Cue Phrases One of the key advantages of a Bayesian framework for topic segmentation is that it permits the principled combination of multiple data sources, even 3This assumes that the objective function for individual segments can also be computed efficiently. In our case, we need only keep vectors of counts for each segment, and evaluate probability density functions over the counts. F(nj,i + Wθ0) F(θ0) , W Y i 338 without labeled data. We are especially interested in cue phrases, which are explicit markers f</context>
</contexts>
<marker>Minka, 2003</marker>
<rawString>Thomas P. Minka. 2003. Estimating a dirichlet distribution. Technical report, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Passonneau</author>
<author>Diane Litman</author>
</authors>
<title>Intentionbased segmentation: Human reliability and correlation with linguistic cues.</title>
<date>1993</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>148--155</pages>
<contexts>
<context position="2563" citStr="Passonneau and Litman, 1993" startWordPosition="370" endWordPosition="373">ahara, 2001; Galley et al.2003; Malioutov and Barzilay, 2006), and is currently the dominant approach to unsupervised topic segmentation. But despite the effectiveness of lexical cohesion for unsupervised topic segmentation, it is clear that there are other important indicators that are ignored by the current generation of unsupervised systems. For example, consider cue phrases, which are explicit discourse markers such as “now” or “however” (Grosz and Sidner, 1986; Hirschberg and Litman, 1993; Knott, 1996). Cue phrases have been shown to be a useful feature for supervised topic segmentation (Passonneau and Litman, 1993; Galley et al., 2003), but cannot be incorporated by current unsupervised models. One reason for this is that existing unsupervised methods use arbitrary, hand-crafted metrics for quantifying lexical cohesion, such as weighted cosine similarity (Hearst, 1994; Malioutov and Barzilay, 2006). Without supervision, it is not possible to combine such metrics with additional sources of information. Moreover, such hand-crafted metrics may not generalize well across multiple datasets, and often include parameters which must be tuned on development sets (Malioutov and Barzilay, 2006; Galley et al., 200</context>
<context position="8377" citStr="Passonneau and Litman (1993)" startWordPosition="1268" endWordPosition="1271">s predecessor. Unlike Purver et al., we do not assume a dataset in which topics are shared across multiple documents; indeed, our model can be applied to single documents individually. Additionally, the inference procedure of Purver et al. requires sampling multiple layers of hidden variables. In contrast, our inference procedure leverages the nature of linear segmentation to search only in the space of segmentation points. 335 The relationship between discourse structure and cue phrases has been studied extensively; for an early example of computational work on this topic, see (Grosz, 1977). Passonneau and Litman (1993) were the first to investigate the relationship between cue phrases and linear segmentation. More recently, cue phrases have been applied to topic segmentation in the supervised setting. In a supervised system that is distinct from the unsupervised model described above, Galley et al. (2003) automatically identify candidate cue phrases by mining labeled data for words that are especially likely to appear at segment boundaries; the presence of cue phrases is then used as a feature in a rule-based classifier for linear topic segmentation. Elsner and Charniak (2008) specify a list of cue phrases </context>
</contexts>
<marker>Passonneau, Litman, 1993</marker>
<rawString>Rebecca Passonneau and Diane Litman. 1993. Intentionbased segmentation: Human reliability and correlation with linguistic cues. In Proceedings of ACL, pages 148–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Pevzner</author>
<author>Marti A Hearst</author>
</authors>
<title>A critique and improvement of an evaluation metric for text segmentation.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="26248" citStr="Pevzner and Hearst, 2002" startWordPosition="4248" endWordPosition="4251">s transcripts of 75 multi-party meetings, of which 25 are annotated for segment boundaries. For text, we introduce a dataset in which each document is a chapter selected from a medical textbook (Walker et al., 1990).7 The task is to divide each chapter into the sections indicated by the author. This dataset contains 227 chapters, with 1136 sections (an average of 5.00 per chapter). Each chapter contains an average of 140 sentences, giving an average of 28 sentences per segment. Metrics All experiments are evaluated in terms of the commonly-used Pk (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002) scores. Both metrics pass a window through the document, and assess whether the sentences on the edges of the window are properly segmented with respect to each other. WindowDiff is stricter in that it requires that the number of intervening segments between the two sentences be identical in the hypothesized and the reference segmentations, while Pk only asks whether the two sentences are in the same segment or not. Pk and WindowDiff are penalties, so lower values indicate better segmentations. We use the evaluation source code provided by Malioutov and Barzilay (2006). System configuration W</context>
</contexts>
<marker>Pevzner, Hearst, 2002</marker>
<rawString>Lev Pevzner and Marti A. Hearst. 2002. A critique and improvement of an evaluation metric for text segmentation. Computational Linguistics, 28(1):19–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<tech>Program,</tech>
<pages>14--130</pages>
<contexts>
<context position="29229" citStr="Porter (1980)" startWordPosition="4723" endWordPosition="4724"> in the paper (Galley et al., 2003). MCS requires parameter settings to be tuned on a development set. Our corpora do not include development sets, so tuning was performed using the lecture transcript corpus described by Malioutov and Barzilay (2006). Our system does not require parameter tuning; priors are re-estimated as described in Sections 3.4 and 4.3. U&amp;I requires no parameter tuning, and is used “out of the box.” In all experiments, we assume that the number of desired segments is provided. Preprocessing Standard preprocessing techniques are applied to the text for all comparisons. The Porter (1980) stemming algorithm is applied to group equivalent lexical items. A set of stop-words is also removed, using the same list originally employed by several competitive systems (Choi, 2000; Textbook Pk WD U&amp;I .370 .376 MCS .368 .382 LCSEG .370 .385 BAYESSEG .339 .353 BAYESSEG-CUE .339 .353 BAYESSEG-CUE-PROP .343 .355 Meetings Pk WD U&amp;I .297 .347 MCS .370 .411 LCSEG .309 .322 BAYESSEG .264 .319 BAYESSEG-CUE .261 .316 BAYESSEG-CUE-PROP .258 .312 Table 1: Comparison of segmentation algorithms. Both metrics are penalties, so lower scores indicate better performance. BAYESSEG is the cohesion-only Baye</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>M. F. Porter. 1980. An algorithm for suffix stripping. Program, 14:130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Purver</author>
<author>T L Griffiths</author>
<author>K P K¨ording</author>
<author>J B Tenenbaum</author>
</authors>
<title>Unsupervised topic modelling for multiparty spoken discourse.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>17--24</pages>
<marker>Purver, Griffiths, K¨ording, Tenenbaum, 2006</marker>
<rawString>M. Purver, T.L. Griffiths, K.P. K¨ording, and J.B. Tenenbaum. 2006. Unsupervised topic modelling for multiparty spoken discourse. In Proceedings of ACL, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Sporleder</author>
<author>Mirella Lapata</author>
</authors>
<title>Broad coverage paragraph segmentation across languages and domains.</title>
<date>2006</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>3</volume>
<issue>2</issue>
<contexts>
<context position="14684" citStr="Sporleder and Lapata, 2006" startWordPosition="2343" endWordPosition="2346"> = 0 and rearranging equation 2, we obtain nj,i = Nj�Bj,i, with Nj = PW i nj,i, the total number of words in segment j. Substituting this into equation 6, we obtain log p(X|z, XK j where H(Bj) is the negative entropy of the multinomial Bj. Thus, with B0 = 0, the log conditional probability in equation 6 is optimized by a segmentation that minimizes the weighted sum of entropies per segment, where the weights are equal to the segment lengths. This result suggests intriguing connections with prior work on the relationship between entropy and discourse structure (e.g., Genzel and Charniak, 2002; Sporleder and Lapata, 2006). 3.2 Marginalizing the language model The previous subsection uses point estimates of the language models to reveal connections to entropy and prior work on segmentation. However, point estimates are theoretically unsatisfying from a Bayesian perspective, and better performance may be obtained by marginalizing over all possible lanY Bj,i (3) iExt �Bmt,i (4) j,i W Y i W Y i XT log p(xt|�Bzt) t XK X log p(xt|�Bj) j {t:zt=j} nj,i log Bj,i (6) XK j XW i 6) = XK X ki log ki j Nj i Bj), NjH( 337 guage models: K p(X|z, θ0) = Y Y p(xt|θ0) j {t:zt=j1 YK Z Y dθj p(xt|θj)p(θj|θ0) j {t:zt=j1 K = Y pdcm({</context>
</contexts>
<marker>Sporleder, Lapata, 2006</marker>
<rawString>Caroline Sporleder and Mirella Lapata. 2006. Broad coverage paragraph segmentation across languages and domains. ACM Transactions on Speech and Language Processing, 3(2):1–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masao Utiyama</author>
<author>Hitoshi Isahara</author>
</authors>
<title>A statistical model for domain-independent text segmentation.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>491--498</pages>
<contexts>
<context position="1947" citStr="Utiyama and Isahara, 2001" startWordPosition="276" endWordPosition="279">e Bayesian framework.1 1 Introduction Topic segmentation is one of the fundamental problems in discourse analysis, where the task is to divide a text into a linear sequence of topicallycoherent segments. Hearst’s TEXTTILING (1994) introduced the idea that unsupervised segmentation 1Code and materials for this work are available at http://groups.csail.mit.edu/rbg/code/ bayesseg/. can be driven by lexical cohesion, as high-quality segmentations feature homogeneous lexical distributions within each topic segment. Lexical cohesion has provided the inspiration for several successful systems (e.g., Utiyama and Isahara, 2001; Galley et al.2003; Malioutov and Barzilay, 2006), and is currently the dominant approach to unsupervised topic segmentation. But despite the effectiveness of lexical cohesion for unsupervised topic segmentation, it is clear that there are other important indicators that are ignored by the current generation of unsupervised systems. For example, consider cue phrases, which are explicit discourse markers such as “now” or “however” (Grosz and Sidner, 1986; Hirschberg and Litman, 1993; Knott, 1996). Cue phrases have been shown to be a useful feature for supervised topic segmentation (Passonneau </context>
<context position="6434" citStr="Utiyama and Isahara (2001)" startWordPosition="959" endWordPosition="962">ted by these experimental results, our model reveals interesting theoretical properties. Other researchers have observed relationships between discourse structure and entropy (e.g., Genzel and Charniak, 2002). We show that in a special case of our model, the segmentation objective is equal to a weighted sum of the negative entropies for each topic segment. This finding demonstrates that a relationship between discourse segmentation and entropy is a natural consequence of modeling topic structure in a generative Bayesian framework. In addition, we show that the benchmark segmentation system of Utiyama and Isahara (2001) can be viewed as another special case of our Bayesian model. 2 Related Work Existing unsupervised cohesion-based approaches can be characterized in terms of the metric used to quantify cohesion and the search technique. Galley et al. (2003) characterize cohesion in terms of lexical chains – repetitions of a given lexical item over some fixed-length window of sentences. In their unsupervised model, inference is performed by selecting segmentation points at the local maxima of the cohesion function. Malioutov and Barzilay (2006) optimize a normalized minimum-cut criteria based on a variation of</context>
<context position="12721" citStr="Utiyama and Isahara (2001" startWordPosition="2010" endWordPosition="2013">ment j is a product over each sentence in the segment, 2Our experiments will assume that the number of topics K is known. This is common practice for this task, as the desired number of segments may be determined by the user (Malioutov and Barzilay, 2006). 336 Y p({xt : zt = j}|�Bj) = {t:zt=j} Y= {t:zt=j} �Bnj,i j,i �(5) By viewing the likelihood as a product over all terms in the vocabulary, we observe interesting connections with prior work on segmentation and information theory. 3.1.1 Connection to previous work In this section, we explain how our model generalizes the well-known method of Utiyama and Isahara (2001; hereafter U&amp;I). As in our work, Utiyama and Isahara propose a probabilistic framework based on maximizing the compactness of the language models induced for each segment. Their likelihood equation is identical to our equations 3-5. They then define the language models for each segment as �Bj,i = nj,iW1 , without rigorous justifiW+Ei nj,i cation. This form is equivalent to Laplacian smoothing (Manning and Sch¨utze, 1999), and is a special case of our equation 2, with B0 = 1. Thus, the language models in U&amp;I can be viewed as the expectation of the posterior distribution p(Bj|{xt : zt = j}, B0)</context>
<context position="28336" citStr="Utiyama and Isahara, 2001" startWordPosition="4575" endWordPosition="4578"> use of annealing to obtain a maximum a posteriori (MAP) configuration from sampling-based inference is common (e.g., Finkel 2005; Goldwater 2007). The total running time of our system is on the order of three minutes per document. Due to memory constraints, we divide the textbook dataset into ten parts, and perform inference in each part separately. We may achieve better results by performing inference over the entire dataset simultaneously, due to pooling counts for cue phrases across all documents. Baselines We compare against three competitive alternative systems from the literature: U&amp;I (Utiyama and Isahara, 2001); LCSEG (Galley et al., 2003); MCS (Malioutov and Barzilay, 2006). All three systems are described in the related work (Section 2). In all cases, we use the publicly available executables provided by the authors. Parameter settings For LCSEG, we use the parameter values specified in the paper (Galley et al., 2003). MCS requires parameter settings to be tuned on a development set. Our corpora do not include development sets, so tuning was performed using the lecture transcript corpus described by Malioutov and Barzilay (2006). Our system does not require parameter tuning; priors are re-estimate</context>
<context position="30029" citStr="Utiyama and Isahara, 2001" startWordPosition="4839" endWordPosition="4842">Choi, 2000; Textbook Pk WD U&amp;I .370 .376 MCS .368 .382 LCSEG .370 .385 BAYESSEG .339 .353 BAYESSEG-CUE .339 .353 BAYESSEG-CUE-PROP .343 .355 Meetings Pk WD U&amp;I .297 .347 MCS .370 .411 LCSEG .309 .322 BAYESSEG .264 .319 BAYESSEG-CUE .261 .316 BAYESSEG-CUE-PROP .258 .312 Table 1: Comparison of segmentation algorithms. Both metrics are penalties, so lower scores indicate better performance. BAYESSEG is the cohesion-only Bayesian system with marginalized language models. BAYESSEG-CUE is the Bayesian system with cue phrases. BAYESSEG-CUE-PROP adds the linguisticallymotivated proposal distribution. Utiyama and Isahara, 2001; Malioutov and Barzilay, 2006). 6 Results Table 1 presents the performance results for three instantiations of our Bayesian framework and three competitive alternative systems. As shown in the table, the Bayesian models achieve the best results on both metrics for both corpora. On the medical textbook corpus, the Bayesian systems achieve a raw performance gain of 2-3% with respect to all baselines on both metrics. On the ICSI meeting corpus, the Bayesian systems perform 4-5% better than the best baseline on the Pk metric, and achieve smaller improvement on the WindowDiff metric. The results o</context>
</contexts>
<marker>Utiyama, Isahara, 2001</marker>
<rawString>Masao Utiyama and Hitoshi Isahara. 2001. A statistical model for domain-independent text segmentation. In Proceedings of ACL, pages 491–498.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kenneth Walker</author>
<author>W Dallas Hall</author>
</authors>
<date>1990</date>
<booktitle>Clinical Methods: The History, Physical, and Laboratory Examinations.</booktitle>
<editor>and J. Willis Hurst, editors.</editor>
<publisher>Butterworths.</publisher>
<marker>Walker, Hall, 1990</marker>
<rawString>H. Kenneth Walker, W. Dallas Hall, and J. Willis Hurst, editors. 1990. Clinical Methods: The History, Physical, and Laboratory Examinations. Butterworths.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg C G Wei</author>
<author>Martin A Tanner</author>
</authors>
<title>A monte carlo implementation of the EM algorithm and the poor man’s data augmentation algorithms.</title>
<date>1990</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>85</volume>
<issue>411</issue>
<contexts>
<context position="25272" citStr="Wei and Tanner, 1990" startWordPosition="4086" endWordPosition="4090">ization after epochs of 1000 5Permitting moves to change the number of segments would substantially complicate inference. 6We set a = − &apos; max-move, where max-move is the maximum move-length, set to 5 in our experiments. These parameters affect the rate of convergence but are unrelated to the underlying probability model. In the limit of enough samples, all nonpathological settings will yield the same segmentation results. Metropolis-Hasting steps. Interleaving samplingbased inference with direct optimization of parameters can be considered a form of Monte Carlo Expectation-Maximization (MCEM; Wei and Tanner, 1990). 5 Experimental Setup Corpora We evaluate our approach on corpora from two different domains: transcribed meetings and written text. For multi-speaker meetings, we use the ICSI corpus of meeting transcripts (Janin et al., 2003), which is becoming a standard for speech segmentation (e.g., Galley et al. 2003; Purver et al. 2006). This dataset includes transcripts of 75 multi-party meetings, of which 25 are annotated for segment boundaries. For text, we introduce a dataset in which each document is a chapter selected from a medical textbook (Walker et al., 1990).7 The task is to divide each chap</context>
</contexts>
<marker>Wei, Tanner, 1990</marker>
<rawString>Greg C. G. Wei and Martin A. Tanner. 1990. A monte carlo implementation of the EM algorithm and the poor man’s data augmentation algorithms. Journal of the American Statistical Association, 85(411), September.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>