<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9962525">
Minimum Imputed Risk: Unsupervised Discriminative Training for
Machine Translation
</title>
<author confidence="0.975806">
Zhifei Li∗
</author>
<affiliation confidence="0.436361">
Google Research
</affiliation>
<address confidence="0.565845">
Mountain View, CA 94043, USA
</address>
<email confidence="0.95274">
zhifei.work@gmail.com
</email>
<author confidence="0.985558">
Ziyuan Wang, Sanjeev Khudanpur
</author>
<affiliation confidence="0.96645">
Johns Hopkins University
</affiliation>
<address confidence="0.828361">
Baltimore, MD 21218, USA
</address>
<email confidence="0.9991">
zwang40,khudanpur@jhu.edu
</email>
<sectionHeader confidence="0.995638" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999694736842105">
Discriminative training for machine transla-
tion has been well studied in the recent past.
A limitation of the work to date is that it relies
on the availability of high-quality in-domain
bilingual text for supervised training. We
present an unsupervised discriminative train-
ing framework to incorporate the usually plen-
tiful target-language monolingual data by us-
ing a rough “reverse” translation system. Intu-
itively, our method strives to ensure that prob-
abilistic “round-trip” translation from a target-
language sentence to the source-language and
back will have low expected loss. Theoret-
ically, this may be justified as (discrimina-
tively) minimizing an imputed empirical risk.
Empirically, we demonstrate that augment-
ing supervised training with unsupervised data
improves translation performance over the su-
pervised case for both IWSLT and NIST tasks.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.874569166666667">
Missing data is a common problem in statistics when
fitting the parameters 0 of a model. A common strat-
egy is to attempt to impute, or “fill in,” the missing
data (Little and Rubin, 1987), as typified by the EM
algorithm. In this paper we develop imputation tech-
niques when 0 is to be trained discriminatively.
We focus on machine translation (MT) as our ex-
ample application. A Chinese-to-English machine
translation system is given a Chinese sentence x and
∗ Zhifei Li is currently working at Google Research, and
this work was done while he was a PHD student at Johns Hop-
kins University.
</bodyText>
<note confidence="0.649377333333333">
Jason Eisner
Johns Hopkins University
Baltimore, MD 21218, USA
</note>
<email confidence="0.905684">
eisner@jhu.edu
</email>
<author confidence="0.962807">
Brian Roark
</author>
<affiliation confidence="0.7771785">
Oregon Health &amp; Science University
Beaverton, Oregon 97006, USA
</affiliation>
<email confidence="0.990609">
roark@cslu.ogi.edu
</email>
<bodyText confidence="0.999859125">
asked to predict its English translation y. This sys-
tem employs statistical models pe(y I x) whose pa-
rameters 0 are discriminatively trained using bilin-
gual sentence pairs (x, y). But bilingual data for
such supervised training may be relatively scarce for
a particular language pair (e.g., Urdu-English), es-
pecially for some topics (e.g., technical manuals) or
genres (e.g., blogs). So systems seek to exploit ad-
ditional monolingual data, i.e., a corpus of English
sentences y with no corresponding source-language
sentences x, to improve estimation of 0. This is our
missing data scenario.1
Discriminative training of the parameters 0 of
pe(y I x) using monolingual English data is a cu-
rious idea, since there is no Chinese input x to trans-
late. We propose an unsupervised training approach,
called minimum imputed risk training, which is con-
ceptually straightforward: First guess x (probabilis-
tically) from the observed y using a reverse English-
to-Chinese translation model po(x I y). Then train
the discriminative Chinese-to-English model pe(y
x) to do a good job at translating this imputed x
back to y, as measured by a given performance met-
ric. Intuitively, our method strives to ensure that
probabilistic “round-trip” translation from a target-
language sentence to the source-language and back
again will have low expected loss.
Our approach can be applied in an application
scenario where we have (1) enough out-of-domain
bilingual data to build two baseline translation sys-
tems, with parameters 0 for the forward direction,
and φ for the reverse direction; (2) a small amount
</bodyText>
<footnote confidence="0.987926">
1Contrast this with traditional semi-supervised training that
looks to exploit “unlabeled” inputs x, with missing outputs y.
</footnote>
<page confidence="0.864414">
920
</page>
<note confidence="0.958281">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 920–929,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999779689655173">
of in-domain bilingual development data to discrim-
inatively tune a small number of parameters in φ;
and (3) a large amount of in-domain English mono-
lingual data.
The novelty here is to exploit (3) to discrimina-
tively tune the parameters θ of all translation model
components,2 pθ(y|x) and pθ(y), not merely train a
generative language model pθ(y), as is the norm.
Following the theoretical development below, the
empirical effectiveness of our approach is demon-
strated by replacing a key supervised discriminative
training step in the development of large MT sys-
tems — learning the log-linear combination of sev-
eral component model scores (viewed as features) to
optimize a performance metric (e.g. BLEU) on a set
of (x, y) pairs — with our unsupervised discrimina-
tive training using only y. One may hence contrast
our approach with the traditional supervised meth-
ods applied to the MT task such as minimum error
rate training (Och, 2003; Macherey et al., 2008), the
averaged Perceptron (Liang et al., 2006), maximum
conditional likelihood (Blunsom et al., 2008), min-
imum risk (Smith and Eisner, 2006; Li and Eisner,
2009), and MIRA (Watanabe et al., 2007; Chiang et
al., 2009).
We perform experiments using the open-source
MT toolkit Joshua (Li et al., 2009a), and show that
adding unsupervised data to the traditional super-
vised training setup improves performance.
</bodyText>
<sectionHeader confidence="0.952932" genericHeader="introduction">
2 Supervised Discriminative Training via
</sectionHeader>
<subsectionHeader confidence="0.855292">
Minimization of Empirical Risk
</subsectionHeader>
<bodyText confidence="0.98512665">
Let us first review discriminative training in the su-
pervised setting—as used in MERT (Och, 2003) and
subsequent work.
One wishes to tune the parameters θ of some
complex translation system δθ(x). The function δθ,
which translates Chinese x to English y = δθ(x)
need not be probabilistic. For example, θ may be
the parameters of a scoring function used by δ, along
with pruning and decoding heuristics, for extracting
a high-scoring translation of x.
The goal of discriminative training is to mini-
mize the expected loss of δθ(·), under a given task-
specific loss function L(y&apos;, y) that measures how
2Note that the extra monolingual data is used only for tuning
the model weights, but not for inducing new phrases or rules.
bad it would be to output y&apos; when the correct output
is y. For an MT system that is judged by the BLEU
metric (Papineni et al., 2001), for instance, L(y&apos;, y)
may be the negated BLEU score of y&apos; w.r.t. y. To be
precise, the goal3 is to find θ with low Bayes risk,
</bodyText>
<equation confidence="0.958087">
�θ* = argmin p(x, y) L(δθ(x), y) (1)
θ x,y
</equation>
<bodyText confidence="0.88699">
where p(x, y) is the joint distribution of the input-
output pairs.4
The true p(x, y) is, of course, not known and,
in practice, one typically minimizes empirical risk
by replacing p(x, y) above with the empirical dis-
tribution ˜p(x, y) given by a supervised training set
{(xi, yi), i = 1, ... , N}. Therefore,
</bodyText>
<equation confidence="0.7420955">
�θ* = argmin ˜p(x, y) L(δθ(x), y)
θ x,y
= argmin 1 N L(δθ(xi),yi). (2)
θ N i=1
</equation>
<bodyText confidence="0.9950715">
The search for θ* typically requires the use of nu-
merical methods and some regularization.5
</bodyText>
<sectionHeader confidence="0.993126" genericHeader="method">
3 Unsupervised Discriminative Training
with Missing Inputs
</sectionHeader>
<subsectionHeader confidence="0.999989">
3.1 Minimization of Imputed Risk
</subsectionHeader>
<bodyText confidence="0.959804090909091">
We now turn to the unsupervised case, where we
have training examples {yi} but not their corre-
sponding inputs {xi}. We cannot compute the sum-
mand L(δθ(xi), yi) for such i in (2), since δθ(xi)
requires to know xi. So we propose to replace
3This goal is different from the minimum risk training of
Li and Eisner (2009) in a subtle but important way. In both
cases, 0∗ minimizes risk or expected loss, but the expectation
is w.r.t. different distributions: the expectation in Li and Eisner
(2009) is under the conditional distribution p(y I x), while the
expectation in (1) is under the joint distribution p(x, y).
</bodyText>
<footnote confidence="0.9890496">
4In the terminology of statistical decision theory, p(x, y) is
a distribution over states of nature. We seek a decision rule
Sθ(x) that will incur low expected loss on observations x that
are generated from unseen states of nature.
5To compensate for the shortcut of using the unsmoothed
empirical distribution rather than a posterior estimate of p(x, y)
(Minka, 2000), it is common to add a regularization term ||0||2 2
in the objective of (2). The regularization term can prevent over-
fitting to a training set that is not large enough to learn all pa-
rameters.
</footnote>
<page confidence="0.958064">
921
</page>
<equation confidence="0.993455">
L(δθ(xi), yi) with the expectation
E
pφ(x  |yi) L(δθ(x), yi), (3)
x
</equation>
<bodyText confidence="0.99960975">
where pφ(·  |·) is a “reverse prediction model” that
attempts to impute the missing xi data. We call the
resulting variant of (2) the minimization of imputed
empirical risk, and say that
</bodyText>
<equation confidence="0.992884666666667">
E
pφ(x |yi) L(δθ(x),yi) (4)
x
</equation>
<bodyText confidence="0.999787">
is the estimate with the minimum imputed risk6.
The minimum imputed risk objective of (4) could
be evaluated by brute force as follows.
</bodyText>
<listItem confidence="0.967684714285714">
1. For each unsupervised example yi, use the re-
verse prediction model pφ(·  |yi) to impute pos-
sible reverse translations Xi = Jxi1, xi2, ...1,
and add each (xij, yi) pair, weighted by
pφ(xij  |yi) G 1, to an imputed training set.
2. Perform the supervised training of (2) on the
imputed and weighted training data.
</listItem>
<bodyText confidence="0.999943705882353">
The second step means that we must use δθ to
forward-translate each imputed xij, evaluate the loss
of the translations y&apos;ij against the corresponding true
translation yi, and choose the θ that minimizes the
weighted sum of these losses (i.e., the empirical risk
when the empirical distribution ˜p(x, y) is derived
from the imputed training set). Specific to our MT
task, this tries to ensure that probabilistic “round-
trip” translation, from the target-language sentence
yi to the source-language and back again, will have
a low expected loss.7
The trouble with this method is that the reverse
model pφ generates a weighted lattice or hyper-
graph Xi encoding exponentially many translations
of yi, and it is computationally infeasible to forward-
translate each xij E Xi. We therefore investigate
several approximations to (4) in Section 3.4.
</bodyText>
<footnote confidence="0.987844875">
6One may exploit both supervised data {(xi, yi)} and unsu-
pervised data {yj} to perform semi-supervised training via an
interpolation of (2) and (4). We will do so in our experiments.
7Our approach may be applied to other tasks as well. For
example, in a speech recognition task, SB is a speech recognizer
that produces text, whereas po is a speech synthesizer that must
produce a distribution over audio (or at least over acoustic fea-
tures or phone sequences) (Huang et al., 2010).
</footnote>
<subsectionHeader confidence="0.9995">
3.2 The Reverse Prediction Model pφ
</subsectionHeader>
<bodyText confidence="0.999909043478261">
A crucial ingredient in (4) is the reverse prediction
model pφ(·|·) that attempts to impute the missing xi.
We will train this model in advance, doing the best
job we can from available data, including any out-
of-domain bilingual data as well as any in-domain
monolingual data$ x.
In the MT setting, δθ and pφ may have similar pa-
rameterization. One translates Chinese to English;
the other translates English to Chinese.
Yet the setup is not quite symmetric. Whereas δθ
is a translation system that aims to produce a single,
low-loss translation, the reverse version pφ is rather
a probabilistic model. It is supposed to give an accu-
rate probability distribution over possible values xij
of the missing input sentence xi. All of these val-
ues are taken into account in (4), regardless of the
loss that they would incur if they were evaluated for
translation quality relative to the missing xi.
Thus, φ does not need to be trained to minimize
the risk itself (so there is no circularity). Ideally,
it should be trained to match the underlying condi-
tional distribution of x given y, by achieving a low
conditional cross-entropy
</bodyText>
<equation confidence="0.8900515">
H(X  |Y ) = − E p(x, y) log pφ(x  |y). (5)
x,y
</equation>
<bodyText confidence="0.999407555555556">
In practice, φ is trained by (empirically) minimiz-
ing − M �� 1 log pφ (xj |yj) + 2ar �φ�22 on some
bilingual data, with the regularization coefficient σ2
tuned on held out data.
It may be tolerable for pφ to impute mediocre
translations xij. All that is necessary is that the (for-
ward) translations generated from the imputed xij
“simulate” the competing hypotheses that we would
see when translating the correct Chinese input xi.
</bodyText>
<subsectionHeader confidence="0.9939715">
3.3 The Forward Translation System δθ and
The Loss Function L(δθ(xi), yi)
</subsectionHeader>
<bodyText confidence="0.997412125">
The minimum empirical risk objective of (2) is
quite general and various popular supervised train-
ing methods (Lafferty et al., 2001; Collins, 2002;
Och, 2003; Crammer et al., 2006; Smith and Eisner,
$In a translation task from x to y, one usually does not make
use of in-domain monolingual data x. But we can exploit x to
train a language model po(x) for the reverse translation system,
which will make the imputed xij look like true Chinese inputs.
</bodyText>
<equation confidence="0.9966835">
θ∗ = argmin
θ
1 N
N i=1
</equation>
<page confidence="0.968459">
922
</page>
<bodyText confidence="0.9935122">
2006) can be formalized in this framework by choos-
ing different functions for δB and L(δB(xi), yi). The
generality of (2) extends to our minimum imputed
risk objective of (4). Below, we specify the δB and
L(δB(xi), yi) we considered in our investigation.
</bodyText>
<subsectionHeader confidence="0.884099">
3.3.1 Deterministic Decoding
</subsectionHeader>
<bodyText confidence="0.998395">
A simple translation rule would define
</bodyText>
<equation confidence="0.982378">
δB(x) = argmax pB(y  |x) (6)
y
</equation>
<bodyText confidence="0.999890909090909">
If this δB(x) is used together with a loss function
L(δB(xi), yi) that is the negated BLEU score9, our
minimum imputed risk objective of (4) is equivalent
to MERT (Och, 2003) on the imputed training data.
However, this would not yield a differentiable ob-
jective function. Infinitesimal changes to θ could re-
sult in discrete changes to the winning output string
δB(x) in (6), and hence to the loss L(δB(x), yi). Och
(2003) developed a specialized line search to per-
form the optimization, which is not scalable when
the number of model parameters θ is large.
</bodyText>
<subsectionHeader confidence="0.925326">
3.3.2 Randomized Decoding
</subsectionHeader>
<bodyText confidence="0.993002">
Instead of using the argmax of (6), we assume
during training that δB(x) is itself random, i.e. the
MT system randomly outputs a translation y with
probability pB(y  |x). As a result, we will modify
our objective function of (4) to take yet another ex-
pectation over the unknown y. Specifically, we will
replace L(δB(x), yi) in (4) with
</bodyText>
<equation confidence="0.9755895">
E pB(y  |x) L(y, yi). (7)
y
</equation>
<bodyText confidence="0.94360828">
Now, the minimum imputed empirical risk objective
of (4) becomes
po(x  |yi) pB(y  |x) L(y, yi)
(8)
If the loss function L(y, yi) is a negated BLEU, this
is equivalent to performing minimum-risk training
described by (Smith and Eisner, 2006; Li and Eisner,
2009) on the imputed data.10
9One can manipulate the loss function to support other
methods that use deterministic decoding, such as Perceptron
(Collins, 2002) and MIRA (Crammer et al., 2006).
10Again, one may manipulate the loss function to support
other probabilistic methods that use randomized decoding, such
as CRFs (Lafferty et al., 2001).
The objective function in (8) is now differentiable,
since each coefficient pB(y  |x) is a differentiable
function of θ, and thus amenable to optimization
by gradient-based methods; we use the L-BFGS al-
gorithm (Liu et al., 1989) in our experiments. We
perform experiments with the syntax-based MT sys-
tem Joshua (Li et al., 2009a), which implements
dynamic programming algorithms for second-order
expectation semirings (Li and Eisner, 2009) to effi-
ciently compute the gradients needed for optimizing
(8).
</bodyText>
<subsectionHeader confidence="0.999667">
3.4 Approximating po(x  |yi)
</subsectionHeader>
<bodyText confidence="0.9996916">
As mentioned at the end of Section 3.1, it is com-
putationally infeasible to forward-translate each of
the imputed reverse translations xis. We propose
four approximations that are computationally feasi-
ble. Each may be regarded as a different approxima-
tion of po(x  |yi) in equations (4) or (8).
k-best. For each yi, add to the imputed training set
only the k most probable translations {xi1, ... xik}
according to po(x  |yi). (These can be extracted
from Xi using standard algorithms (Huang and Chi-
ang, 2005).) Rescale their probabilities to sum to 1.
Sampling. For each yi, add to the training set k in-
dependent samples {xi1, ... xik} from the distribu-
tion po(x  |yi), each with weight 1/k. (These can be
sampled from Xi using standard algorithms (John-
son et al., 2007).) This method is known in the liter-
ature as multiple imputation (Rubin, 1987).
Lattice. 11 Under certain special cases it is be pos-
sible to compute the expected loss in (3) exactly
via dynamic programming. Although Xi does con-
tain exponentially many translations, it may use a
“packed” representation in which these translations
share structure. This representation may further-
more enable sharing work in forward-translation, so
as to efficiently translate the entire set Xi and ob-
tain a distribution over translations y. Finally, the
expected loss under that distribution, as required by
equation (3), may also be efficiently computable.
All this turns out to be possible if (a) the poste-
rior distribution po(x  |yi) is represented by an un-
</bodyText>
<footnote confidence="0.89620125">
11The lattice approximation is presented here as a theoreti-
cal contribution, and we do not empirically evaluate it since its
implementation requires extensive engineering effort that is be-
yond the main scope of this paper.
</footnote>
<equation confidence="0.990920125">
θ∗ = argmin
B
1
N
N
i=1
L
x,y
</equation>
<page confidence="0.985194">
923
</page>
<bodyText confidence="0.999876891891892">
ambiguous weighted finite-state automaton Xi, (b)
the forward translation system δθ is structured in a
certain way as a weighted synchronous context-free
grammar, and (c) the loss function decomposes in a
certain way. We omit the details of the construction
as beyond the scope of this paper.
In our experimental setting described below, (b) is
true (using Joshua), and (c) is true (since we use a
loss function presented by Tromble et al. (2008) that
is an approximation to BLEU and is decomposable).
While (a) is not true in our setting because Xi is a
hypergraph (which is ambiguous), Li et al. (2009b)
show how to approximate a hypergraph representa-
tion of pφ(x  |yi) by an unambiguous WFSA. One
could then apply the construction to this WFSA12,
obtaining an approximation to (3).
Rule-level Composition. Intuitively, the reason
why the structure-sharing in the hypergraph Xi (gen-
erated by the reverse system) cannot be exploited
during forward translating is that when the forward
Hiero system translates a string xi E Xi, it must
parse it into recursive phrases.
But the structure-sharing within the hypergraph of
Xi has already parsed xi into recursive phrases, in a
way determined by the reverse Hiero system; each
translation phrase (or rule) corresponding to a hy-
peredge. To exploit structure-sharing, we can use
a forward translation system that decomposes ac-
cording to that existing parse of xi. We can do that
by considering only forward translations that respect
the hypergraph structure of Xi. The simplest way to
do this is to require complete isomorphism of the
SCFG trees used for the reverse and forward trans-
lations. In other words, this does round-trip impu-
tation (i.e., from y to x, and then to y&apos;) at the rule
level. This is essentially the approach taken by Li et
al. (2010).
</bodyText>
<subsectionHeader confidence="0.874037">
3.5 The Log-Linear Model pθ
</subsectionHeader>
<bodyText confidence="0.9997175">
We have not yet specified the form of pθ. Following
much work in MT, we begin with a linear model
</bodyText>
<equation confidence="0.9843745">
Xscore(x, y) = θ · f(x, y) = θkfk(x, y) (9)
k
</equation>
<bodyText confidence="0.958098142857143">
where f(x, y) is a feature vector indexed by k. Our
deterministic test-time translation system δθ simply
12Note that the forward translation of a WFSA is tractable by
using a lattice-based decoder such as that by Dyer et al. (2008).
outputs the highest-scoring y for fixed x. At training
time, our randomized decoder (Section 3.3.2) uses
the Boltzmann distribution (here a log-linear model)
</bodyText>
<equation confidence="0.9991395">
eγ·score(x,y) eγ·score(x,y)
Z(x) =Py, eγ·score(x,y,) (10)
</equation>
<bodyText confidence="0.999980611111111">
The scaling factor γ controls the sharpness of the
training-time distribution, i.e., the degree to which
the randomized decoder favors the highest-scoring
y. For large γ, our training objective approaches
the imputed risk of the deterministic test-time sys-
tem while remaining differentiable.
In a task like MT, in addition to the input x and
output y, we often need to introduce a latent variable
d to represent the hidden derivation that relates x to
y. A derivation d represents a particular phrase seg-
mentation in a phrase-based MT system (Koehn et
al., 2003) and a derivation tree in a typical syntax-
based system (Galley et al., 2006; Chiang, 2007).
We change our model to assign scores not to an
(x, y) pair but to the detailed derivation d; in partic-
ular, now the function f that extracts a feature vector
can look at all of d. We replace y by d in (9)–(10),
and finally define pθ(y|x) by marginalizing out d,
</bodyText>
<equation confidence="0.9902845">
pθ(y |x) = X pθ(d|x) (11)
dED(x,y)
</equation>
<bodyText confidence="0.992022">
where D(x, y) represents the set of derivations that
yield x and y.
</bodyText>
<sectionHeader confidence="0.99125" genericHeader="method">
4 Minimum Imputed Risk vs. EM
</sectionHeader>
<bodyText confidence="0.953695">
The notion of imputing missing data is familiar
from other settings (Little and Rubin, 1987), particu-
larly the expectation maximization (EM) algorithm,
a widely used generative approach. So it is instruc-
tive to compare EM with minimum imputed risk.
One can estimate θ by maximizing the log-
likelihood of the data {(xi, yi), i = 1, ... , N} as
log pθ(xi, yi). (12)
If the xi’s are missing, EM tries to iteratively maxi-
mize the marginal probability:
</bodyText>
<equation confidence="0.864610545454546">
pθ(x,yi). (13)
pθ(y  |x) =
1
N
argmax
θ
XN
i=1
argmax 1 XN X
θ N i=1 log
x
</equation>
<page confidence="0.994274">
924
</page>
<bodyText confidence="0.9998318">
The E-step of each iteration comprises comput-
ing &amp;x pθt(x  |yi) log pθ(x, yi), the expected log-
likelihood of the complete data, where pθt(x  |yi) is
the conditional part of pθt(x, yi) under the current
iterate 0t, and the M-step comprises maximizing it:
</bodyText>
<equation confidence="0.51203375">
0t+1 = argmax 1 N E
θ N i=1 pθt(x  |yi) log pθ(x, yi).
x
(14)
</equation>
<bodyText confidence="0.997263444444444">
Notice that if we replace pθt(x|yi) with pφ(x  |yi)
in the equation above, and admit negated log-
likelihood as a loss function, then the EM update
(14) becomes identical to (4). In other words, the
minimum imputed risk approach of Section 3.1 dif-
fers from EM in (i) using an externally-provided and
static pφ, instead of refining it at each iteration based
on the current pθt, and (ii) using a specific loss func-
tion, namely negated log-likelihood.
So why not simply use the maximum-likelihood
(EM) training procedure for MT? One reason is
that it is not discriminative: the loss function (e.g.
negated BLEU) is ignored during training.
A second reason is that training good joint models
pθ(x, y) is computationally expensive. Contempo-
rary MT makes heavy use of log-linear probability
models, which allow the system designer to inject
phrase tables, linguistic intuitions, or prior knowl-
edge through a careful choice of features. Comput-
ing the objective function of (14) in closed form is
difficult if pθ is an arbitrary log-linear model, be-
cause the joint probability pθ(xi, yi) is then defined
as a ratio whose denominator Zθ involves a sum over
all possible sentence pairs (x, y) of any length.
By contrast, our discriminative framework will
only require us to work with conditional models.
While conditional probabilities such as pφ(x  |y) and
pθ(y  |x) are also ratios, computing their denomina-
tors only requires us to sum over a packed forest of
possible translations of a given y or x.13
In summary, EM would impute missing data us-
ing pθ(x  |y) and predict outputs using pθ(y  |x),
both being conditional forms of the same joint
model pθ(x, y). Our minimum imputed risk train-
ing method is similar, but it instead uses a pair of
13Analogously, discriminative CRFs have become more pop-
ular than generative HMMs because they permit efficient train-
ing even with a wide variety of log-linear features (Lafferty et
al., 2001).
separately parameterized, separately trained mod-
els pφ(x  |y) and pθ(y  |x). By sticking to condi-
tional models, we can efficiently use more sophis-
ticated model features, and we can incorporate the
loss function when we train 0, which should improve
both efficiency and accuracy at test time.
</bodyText>
<sectionHeader confidence="0.996168" genericHeader="method">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.999887666666667">
We report results on Chinese-to-English translation
tasks using Joshua (Li et al., 2009a), an open-source
implementation of Hiero (Chiang, 2007).
</bodyText>
<subsectionHeader confidence="0.5702165">
5.1 Baseline Systems
5.1.1 IWSLT Task
</subsectionHeader>
<bodyText confidence="0.999808545454545">
We train both reverse and forward baseline sys-
tems. The translation models are built using the cor-
pus for the IWSLT 2005 Chinese to English trans-
lation task (Eck and Hori, 2005), which comprises
40,000 pairs of transcribed utterances in the travel
domain. We use a 5-gram language model with
modified Kneser-Ney smoothing (Chen and Good-
man, 1998), trained on the English (resp. Chi-
nese) side of the bitext. We use a standard train-
ing pipeline and pruning settings recommended by
(Chiang, 2007).
</bodyText>
<subsubsectionHeader confidence="0.738119">
5.1.2 NIST Task
</subsubsectionHeader>
<bodyText confidence="0.999764625">
For the NIST task, the TM is trained on about 1M
parallel sentence pairs (about 28M words in each
language), which are sub-sampled from corpora dis-
tributed by LDC for the NIST MT evaluation using a
sampling method implemented in Joshua. We also
used a 5-gram language model, trained on a data set
consisting of a 130M words in English Gigaword
(LDC2007T07) and the bitext’s English side.
</bodyText>
<subsectionHeader confidence="0.998754">
5.2 Feature Functions
</subsectionHeader>
<bodyText confidence="0.999929">
We use two classes of features fk for discriminative
training of pθ as defined in (9).
</bodyText>
<subsectionHeader confidence="0.681036">
5.2.1 Regular Hiero Features
</subsectionHeader>
<bodyText confidence="0.999904">
We include ten features that are standard in Hi-
ero (Chiang, 2007). In particular, these include
one baseline language model feature, three baseline
translation models, one word penalty feature, three
features to count how many rules with an arity of
</bodyText>
<page confidence="0.996822">
925
</page>
<bodyText confidence="0.994866">
zero/one/two are used in a derivation, and two fea-
tures to count how many times the unary and binary
glue rules in Hiero are used in a derivation.
</bodyText>
<subsectionHeader confidence="0.907022">
5.2.2 Target-rule Bigram Features
</subsectionHeader>
<bodyText confidence="0.999969111111111">
In this paper, we do not attempt to discrimina-
tively tune a separate parameter for each bilingual
rule in the Hiero grammar. Instead, we train several
hundred features that generalize across these rules.
For each bilingual rule, we extract bigram fea-
tures over the target-side symbols (including non-
terminals and terminals). For example, if a bilingual
rule’s target-side is “on the X1 issue of X2” where
X1 and X2 are non-terminals (with a position in-
dex), we extract the bigram features on the, the X,
X issue, issue of, and of X. (Note that the posi-
tion index of a non-terminal is ignored in the fea-
ture.) Moreover, for the terminal symbols, we will
use their dominant POS tags (instead of the sym-
bol itself). For example, the feature the X becomes
DT X. We use 541 such bigram features for IWSLT
task (and 1023 such features for NIST task) that fire
frequently.
</bodyText>
<subsectionHeader confidence="0.99557">
5.3 Data Sets for Discriminative Training
5.3.1 IWSLT Task
</subsectionHeader>
<bodyText confidence="0.918144863636364">
In addition to the 40,000 sentence pairs used to
train the baseline generative models (which are used
to compute the features fk), we use three bilingual
data sets listed in Table 1, also from IWSLT, for dis-
criminative training: one to train the reverse model
po (which uses only the 10 standard Hiero features
as described in Section 5.2.1),14 one to train the for-
ward model δθ (which uses both classes of features
described in Section 5.2, i.e., 551 features in total),
and one for test.
Note that the reverse model φ is always trained us-
ing the supervised data of Dev φ, while the forward
model θ may be trained in a supervised or semi-
supervised manner, as we will show below.
In all three data sets, each Chinese sentence xi
has 16 English reference translations, so each yi is
actually a set of 16 translations. When we impute
data from yi (in the semi-supervised scenario), we
14Ideally, we should train 0 to minimize the conditional
cross-entropy (5) as suggested in section 3.2. In the present
results, we trained 0 discriminatively to minimize risk, purely
for ease of implementation using well versed steps.
</bodyText>
<table confidence="0.9965822">
Data set Purpose # of sentences
Chinese English
Dev φ training φ 503 503×16
Dev θ training θ 503∗ 503×16
Eval θ testing 506 506×16
</table>
<tableCaption confidence="0.999488">
Table 1: IWSLT Data sets used for discriminative
</tableCaption>
<bodyText confidence="0.986716272727273">
training/test. Dev φ is used for discriminatively training
of the reverse model φ, Dev θ is for the forward model,
and Eval θ is for testing. The star ∗ for Dev θ empha-
sizes that some of its Chinese side will not be used in the
training (see Table 2 for details).
actually impute 16 different values of xi, by using
po to separately reverse translate each sentence in
yi. This effectively adds 16 pairs of the form (xi, yi)
to the training set (see section 3.4), where each xi
is a different input sentence (imputed) in each case,
but yi is always the original set of 16 references.
</bodyText>
<subsectionHeader confidence="0.826187">
5.3.2 NIST Task
</subsectionHeader>
<bodyText confidence="0.999413555555555">
For the NIST task, we use MT03 set (having 919
sentences) to tune the component parameters in both
the forward and reverse baseline systems. Addition-
ally, we use the English side of MT04 (having 1788
sentences) to perform semi-supervised tuning of the
forward model. The test sets are MT05 and MT06
(having 1082 and 1099 sentences, respectively). In
all the data sets, each source sentence has four refer-
ence translations.
</bodyText>
<subsectionHeader confidence="0.998904">
5.4 Main Results
</subsectionHeader>
<bodyText confidence="0.999663916666667">
We compare two training scenarios: supervised and
semi-supervised. The supervised system (“Sup”)
carries out discriminative training on a bilingual data
set. The semi-supervised system (“+Unsup”) addi-
tionally uses some monolingual English text for dis-
criminative training (where we impute one Chinese
translation per English sentence).
Tables 2 and 3 report the results for the two tasks
under two training scenarios. Clearly, adding unsu-
pervised data improves over the supervised case, by
at least 1.3 BLEU points in IWSLT and 0.5 BLEU in
NIST.
</bodyText>
<sectionHeader confidence="0.861665" genericHeader="method">
5.5 Results for Analysis Purposes
</sectionHeader>
<bodyText confidence="0.998603">
Below, we will present more results on the IWSLT
data set to help us understand the behavior of the
</bodyText>
<page confidence="0.995157">
926
</page>
<table confidence="0.938465333333333">
Training scenario Test BLEU
Sup, (200, 200x16) 47.6
+Unsup, 101x16 Eng sentences 49.0
+Unsup, 202x16 Eng sentences 48.9
+Unsup, 303x16 Eng sentences 49.7*
Data size
</table>
<figure confidence="0.862128615384615">
101
202
303
Imputed-CN BLEU
WLM NLM
11.8 3.0
11.7 3.2
13.4 3.5
Test-EN BLEU
WLM NLM
48.5 46.7
48.9 47.6
48.8 47.9
</figure>
<tableCaption confidence="0.482274">
Table 2: BLEU scores for semi-supervised training for
IWSLT task. The supervised system (“Sup”) is trained
on a subset of Dev 0 containing 200 Chinese sentences
and 200x16 English translations. “+Unsup” means that
we include additional (monolingual) English sentences
from Dev 0 for semi-supervised training; for each En-
glish sentence, we impute the 1-best Chinese translation.
A star * indicates a result that is signicantly better than
the “Sup” baseline (paired permutation test, p &lt; 0.05).
Table 3: BLEU scores for semi-supervised training for
</tableCaption>
<bodyText confidence="0.971987888888889">
NIST task. The “Sup” system is trained on MT03, while
the “+Unsup” system is trained with additional 1788 En-
glish sentences from MT04. (Note that while MT04 has
1788x4 English sentences as it has four sets of refer-
ences, we only use one such set, for computational ef-
ficiency of discriminative training.) A star * indicates a
result that is signicantly better than the “Sup” baseline
(paired permutation test, p &lt; 0.05).
methods proposed in this paper.
</bodyText>
<sectionHeader confidence="0.6599465" genericHeader="method">
5.5.1 Imputation with Different Reverse
Models
</sectionHeader>
<bodyText confidence="0.8886253">
A critical component of our unsupervised method
is the reverse translation model pφ(x  |y). We
wonder how the performance of our unsupervised
method changes when the quality of the reverse sys-
tem varies. To study this question, we used two dif-
ferent reverse translation systems, one with a lan-
guage model trained on the Chinese side of the bi-
text (“WLM”), and the other one without using such
a Chinese LM (“NLM”). Table 4 (in the fully unsu-
pervised case) shows that the imputed Chinese trans-
lations have a far lower BLEU score without the lan-
guage model,15 and that this costs us about 1 English
15The BLEU scores are low even with the language model
because only one Chinese reference is available for scoring.
Table 4: BLEU scores for unsupervised training
with/without using a language model in the reverse
system. A data size of 101 means that we use only
the English sentences from a subset of Dev 0 containing
101 Chinese sentences and 101x16 English translations;
for each English sentence we impute the 1-best Chinese
translation. “WLM” means a Chinese language model
is used in the reverse system, while “NLM” means no
Chinese language model is used. In addition to reporting
the BLEU score on Eval 0, we also report “Imputed-CN
BLEU”, the BLEU score of the imputed Chinese sentences
against their corresponding Chinese reference sentences.
BLEU point in the forward translations. Still, even
with the worse imputation (in the case of “NLM”),
our forward translations improve as we add more
monolingual data.
</bodyText>
<subsectionHeader confidence="0.75487">
5.5.2 Imputation with Different k-best Sizes
</subsectionHeader>
<bodyText confidence="0.999982166666667">
In all the experiments so far, we used the reverse
translation system to impute only a single Chinese
translation for each English monolingual sentence.
This is the 1-best approximation of section 3.4.
Table 5 shows (in the fully unsupervised case)
that the performance does not change much as k in-
creases.16 This may be because that the 5-best sen-
tences are likely to be quite similar to one another
(May and Knight, 2006). Imputing a longer k-best
list, a sample, or a lattice for xi (see section 3.4)
might achieve more diversity in the training inputs,
which might make the system more robust.
</bodyText>
<sectionHeader confidence="0.999499" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9801606">
In this paper, we present an unsupervised discrimi-
native training method that works with missing in-
puts. The key idea in our method is to use a re-
verse model to impute the missing input from the ob-
served output. The training will then forward trans-
late the imputed input, and choose the parameters of
the forward model such that the imputed risk (i.e.,
16In the present experiments, however, we simply weighted
all k imputed translations equally, rather than in proportion to
their posterior probabilities as suggested in Section 3.4.
</bodyText>
<figure confidence="0.983379571428571">
Training scenario
Test BLEU
MT05 MT06
Sup, (919, 919x4)
32.4 30.6
+Unsup, 1788 Eng sentences
33.0* 31.1*
</figure>
<page confidence="0.975762">
927
</page>
<tableCaption confidence="0.993118">
Table 5: BLEU scores for unsupervised training with
</tableCaption>
<bodyText confidence="0.953225538461538">
different k-best sizes. We use 101×16 monolingual En-
glish sentences, and for each English sentence we impute
the k-best Chinese translations using the reverse system.
the expected loss of the forward translations with
respect to the observed output) is minimized. This
matches the intuition that the probabilistic “round-
trip” translation from the target-language sentence
to the source-language and back should have low ex-
pected loss.
We applied our method to two Chinese to English
machine translation tasks (i.e. IWSLT and NIST).
We showed that augmenting supervised data with
unsupervised data improved performance over the
supervised case (for both tasks).
Our discriminative model used only a small
amount of training data and relatively few features.
In future work, we plan to test our method in settings
where there are large amounts of monolingual train-
ing data (enabling many discriminative features).
Also, our experiments here were performed on a lan-
guage pair (i.e., Chinese to English) that has quite
rich bilingual resources in the domain of the test
data. In future work, we plan to consider low-
resource test domains and language pairs like Urdu-
English, where bilingual data for novel domains is
sparse.
</bodyText>
<sectionHeader confidence="0.994948" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999188">
This work was partially supported by NSF Grants
No IIS-0963898 and No IIS-0964102 and the
DARPA GALE Program. The authors thank Markus
Dreyer, Damianos Karakos and Jason Smith for in-
sightful discussions.
</bodyText>
<sectionHeader confidence="0.999474" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999721553571429">
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In ACL, pages 200–208.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical report.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In NAACL, pages 218–226.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: theory and experiments
with perceptron algorithms. In EMNLP, pages 1–8.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. J. Mach. Learn. Res., 7:551–
585.
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In ACL,
pages 1012–1020.
Matthias Eck and Chiori Hori. 2005. Overview of the
iwslt 2005 evaluation campaign. In In IWSLT.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In ACL,
pages 961–968.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In IWPT, pages 53–64.
Jui-Ting Huang, Xiao Li, and Alex Acero. 2010. Dis-
criminative training methods for language models us-
ing conditional entropy criteria. In ICASSP.
Mark Johnson, Thomas Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In NAACL, pages 139–146.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In NAACL,
pages 48–54.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In ICML.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In EMNLP, pages
40–51.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar. Zaidan. 2009a.
Joshua: An open source toolkit for parsing-based ma-
chine translation. In WMT09, pages 26–30.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009b.
Variational decoding for statistical machine transla-
tion. In ACL, pages 593–601.
Zhifei Li, Ziyuan Wang, Sanjeev Khudanpur, and Jason
Eisner. 2010. Unsupervised discriminative language
</reference>
<figure confidence="0.833489666666667">
Training scenario
Test BLEU
Unsup, k=1
Unsup, k=2
Unsup, k=3
Unsup, k=4
Unsup, k=5
48.5
48.4
48.9
48.5
48.4
</figure>
<page confidence="0.978189">
928
</page>
<reference confidence="0.999007225">
model training for machine translation using simulated
confusion sets. In COLING, pages 556–664.
Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In ACL, pages 761–
768.
R. J. A. Little and D. B. Rubin. 1987. Statistical Analysis
with Missing Data. J. Wiley &amp; Sons, New York.
Dong C. Liu, Jorge Nocedal, and Dong C. 1989. On the
limited memory bfgs method for large scale optimiza-
tion. Mathematical Programming, 45:503–528.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum er-
ror rate training for statistical machine translation. In
EMNLP, pages 725–734.
Jonathan May and Kevin Knight. 2006. A better n-best
list: practical determinization of weighted finite tree
automata. In NAACL, pages 351–358.
Thomas Minka. 2000. Empirical risk minimization is
an incomplete inductive principle. In MIT Media Lab
note.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL, pages 160–
167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A method for automatic eval-
uation of machine translation. In ACL, pages 311–318.
D. B. Rubin. 1987. Multiple Imputation for Nonresponse
in Surveys. J. Wiley &amp; Sons, New York.
David A. Smith and Jason Eisner. 2006. Minimum
risk annealing for training log-linear models. In ACL,
pages 787–794.
Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang
Macherey. 2008. Lattice minimum-Bayes-risk de-
coding for statistical machine translation. In EMNLP,
pages 620–629.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In EMNLP-CoNLL, pages
764–773.
</reference>
<page confidence="0.998558">
929
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.476121">
<title confidence="0.958599">Minimum Imputed Risk: Unsupervised Discriminative Training Machine Translation Google</title>
<address confidence="0.933902">Mountain View, CA 94043,</address>
<email confidence="0.99919">zhifei.work@gmail.com</email>
<author confidence="0.990493">Ziyuan Wang</author>
<author confidence="0.990493">Sanjeev</author>
<affiliation confidence="0.650436">Johns Hopkins</affiliation>
<address confidence="0.935261">Baltimore, MD 21218,</address>
<email confidence="0.998205">zwang40,khudanpur@jhu.edu</email>
<abstract confidence="0.9962001">Discriminative training for machine translation has been well studied in the recent past. A limitation of the work to date is that it relies on the availability of high-quality in-domain bilingual text for supervised training. We present an unsupervised discriminative training framework to incorporate the usually plentiful target-language monolingual data by using a rough “reverse” translation system. Intuitively, our method strives to ensure that probabilistic “round-trip” translation from a targetlanguage sentence to the source-language and back will have low expected loss. Theoretically, this may be justified as (discriminaminimizing an empirical Empirically, we demonstrate that augmenting supervised training with unsupervised data improves translation performance over the supervised case for both IWSLT and NIST tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>A discriminative latent variable model for statistical machine translation.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>200--208</pages>
<contexts>
<context position="4882" citStr="Blunsom et al., 2008" startWordPosition="751" endWordPosition="754">ur approach is demonstrated by replacing a key supervised discriminative training step in the development of large MT systems — learning the log-linear combination of several component model scores (viewed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs — with our unsupervised discriminative training using only y. One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training (Och, 2003; Macherey et al., 2008), the averaged Perceptron (Liang et al., 2006), maximum conditional likelihood (Blunsom et al., 2008), minimum risk (Smith and Eisner, 2006; Li and Eisner, 2009), and MIRA (Watanabe et al., 2007; Chiang et al., 2009). We perform experiments using the open-source MT toolkit Joshua (Li et al., 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. 2 Supervised Discriminative Training via Minimization of Empirical Risk Let us first review discriminative training in the supervised setting—as used in MERT (Och, 2003) and subsequent work. One wishes to tune the parameters θ of some complex translation system δθ(x). The function δθ, which tr</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. A discriminative latent variable model for statistical machine translation. In ACL, pages 200–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical report.</tech>
<contexts>
<context position="23544" citStr="Chen and Goodman, 1998" startWordPosition="3938" endWordPosition="3942">in 0, which should improve both efficiency and accuracy at test time. 5 Experimental Results We report results on Chinese-to-English translation tasks using Joshua (Li et al., 2009a), an open-source implementation of Hiero (Chiang, 2007). 5.1 Baseline Systems 5.1.1 IWSLT Task We train both reverse and forward baseline systems. The translation models are built using the corpus for the IWSLT 2005 Chinese to English translation task (Eck and Hori, 2005), which comprises 40,000 pairs of transcribed utterances in the travel domain. We use a 5-gram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1998), trained on the English (resp. Chinese) side of the bitext. We use a standard training pipeline and pruning settings recommended by (Chiang, 2007). 5.1.2 NIST Task For the NIST task, the TM is trained on about 1M parallel sentence pairs (about 28M words in each language), which are sub-sampled from corpora distributed by LDC for the NIST MT evaluation using a sampling method implemented in Joshua. We also used a 5-gram language model, trained on a data set consisting of a 130M words in English Gigaword (LDC2007T07) and the bitext’s English side. 5.2 Feature Functions We use two classes of fea</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In NAACL,</booktitle>
<pages>218--226</pages>
<contexts>
<context position="4997" citStr="Chiang et al., 2009" startWordPosition="772" endWordPosition="775">T systems — learning the log-linear combination of several component model scores (viewed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs — with our unsupervised discriminative training using only y. One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training (Och, 2003; Macherey et al., 2008), the averaged Perceptron (Liang et al., 2006), maximum conditional likelihood (Blunsom et al., 2008), minimum risk (Smith and Eisner, 2006; Li and Eisner, 2009), and MIRA (Watanabe et al., 2007; Chiang et al., 2009). We perform experiments using the open-source MT toolkit Joshua (Li et al., 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. 2 Supervised Discriminative Training via Minimization of Empirical Risk Let us first review discriminative training in the supervised setting—as used in MERT (Och, 2003) and subsequent work. One wishes to tune the parameters θ of some complex translation system δθ(x). The function δθ, which translates Chinese x to English y = δθ(x) need not be probabilistic. For example, θ may be the parameters of a scorin</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 new features for statistical machine translation. In NAACL, pages 218–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="19544" citStr="Chiang, 2007" startWordPosition="3249" endWordPosition="3250">ness of the training-time distribution, i.e., the degree to which the randomized decoder favors the highest-scoring y. For large γ, our training objective approaches the imputed risk of the deterministic test-time system while remaining differentiable. In a task like MT, in addition to the input x and output y, we often need to introduce a latent variable d to represent the hidden derivation that relates x to y. A derivation d represents a particular phrase segmentation in a phrase-based MT system (Koehn et al., 2003) and a derivation tree in a typical syntaxbased system (Galley et al., 2006; Chiang, 2007). We change our model to assign scores not to an (x, y) pair but to the detailed derivation d; in particular, now the function f that extracts a feature vector can look at all of d. We replace y by d in (9)–(10), and finally define pθ(y|x) by marginalizing out d, pθ(y |x) = X pθ(d|x) (11) dED(x,y) where D(x, y) represents the set of derivations that yield x and y. 4 Minimum Imputed Risk vs. EM The notion of imputing missing data is familiar from other settings (Little and Rubin, 1987), particularly the expectation maximization (EM) algorithm, a widely used generative approach. So it is instruc</context>
<context position="23158" citStr="Chiang, 2007" startWordPosition="3877" endWordPosition="3878">ve become more popular than generative HMMs because they permit efficient training even with a wide variety of log-linear features (Lafferty et al., 2001). separately parameterized, separately trained models pφ(x |y) and pθ(y |x). By sticking to conditional models, we can efficiently use more sophisticated model features, and we can incorporate the loss function when we train 0, which should improve both efficiency and accuracy at test time. 5 Experimental Results We report results on Chinese-to-English translation tasks using Joshua (Li et al., 2009a), an open-source implementation of Hiero (Chiang, 2007). 5.1 Baseline Systems 5.1.1 IWSLT Task We train both reverse and forward baseline systems. The translation models are built using the corpus for the IWSLT 2005 Chinese to English translation task (Eck and Hori, 2005), which comprises 40,000 pairs of transcribed utterances in the travel domain. We use a 5-gram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1998), trained on the English (resp. Chinese) side of the bitext. We use a standard training pipeline and pruning settings recommended by (Chiang, 2007). 5.1.2 NIST Task For the NIST task, the TM is trained on about 1M </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In EMNLP,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="11946" citStr="Collins, 2002" startWordPosition="1971" endWordPosition="1972">lly) minimizing − M �� 1 log pφ (xj |yj) + 2ar �φ�22 on some bilingual data, with the regularization coefficient σ2 tuned on held out data. It may be tolerable for pφ to impute mediocre translations xij. All that is necessary is that the (forward) translations generated from the imputed xij “simulate” the competing hypotheses that we would see when translating the correct Chinese input xi. 3.3 The Forward Translation System δθ and The Loss Function L(δθ(xi), yi) The minimum empirical risk objective of (2) is quite general and various popular supervised training methods (Lafferty et al., 2001; Collins, 2002; Och, 2003; Crammer et al., 2006; Smith and Eisner, $In a translation task from x to y, one usually does not make use of in-domain monolingual data x. But we can exploit x to train a language model po(x) for the reverse translation system, which will make the imputed xij look like true Chinese inputs. θ∗ = argmin θ 1 N N i=1 922 2006) can be formalized in this framework by choosing different functions for δB and L(δB(xi), yi). The generality of (2) extends to our minimum imputed risk objective of (4). Below, we specify the δB and L(δB(xi), yi) we considered in our investigation. 3.3.1 Determi</context>
<context position="13989" citStr="Collins, 2002" startWordPosition="2326" endWordPosition="2327">B(y |x). As a result, we will modify our objective function of (4) to take yet another expectation over the unknown y. Specifically, we will replace L(δB(x), yi) in (4) with E pB(y |x) L(y, yi). (7) y Now, the minimum imputed empirical risk objective of (4) becomes po(x |yi) pB(y |x) L(y, yi) (8) If the loss function L(y, yi) is a negated BLEU, this is equivalent to performing minimum-risk training described by (Smith and Eisner, 2006; Li and Eisner, 2009) on the imputed data.10 9One can manipulate the loss function to support other methods that use deterministic decoding, such as Perceptron (Collins, 2002) and MIRA (Crammer et al., 2006). 10Again, one may manipulate the loss function to support other probabilistic methods that use randomized decoding, such as CRFs (Lafferty et al., 2001). The objective function in (8) is now differentiable, since each coefficient pB(y |x) is a differentiable function of θ, and thus amenable to optimization by gradient-based methods; we use the L-BFGS algorithm (Liu et al., 1989) in our experiments. We perform experiments with the syntax-based MT system Joshua (Li et al., 2009a), which implements dynamic programming algorithms for second-order expectation semiri</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms. In EMNLP, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai ShalevShwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passiveaggressive algorithms.</title>
<date>2006</date>
<journal>J. Mach. Learn. Res.,</journal>
<volume>7</volume>
<pages>585</pages>
<contexts>
<context position="11979" citStr="Crammer et al., 2006" startWordPosition="1975" endWordPosition="1978">og pφ (xj |yj) + 2ar �φ�22 on some bilingual data, with the regularization coefficient σ2 tuned on held out data. It may be tolerable for pφ to impute mediocre translations xij. All that is necessary is that the (forward) translations generated from the imputed xij “simulate” the competing hypotheses that we would see when translating the correct Chinese input xi. 3.3 The Forward Translation System δθ and The Loss Function L(δθ(xi), yi) The minimum empirical risk objective of (2) is quite general and various popular supervised training methods (Lafferty et al., 2001; Collins, 2002; Och, 2003; Crammer et al., 2006; Smith and Eisner, $In a translation task from x to y, one usually does not make use of in-domain monolingual data x. But we can exploit x to train a language model po(x) for the reverse translation system, which will make the imputed xij look like true Chinese inputs. θ∗ = argmin θ 1 N N i=1 922 2006) can be formalized in this framework by choosing different functions for δB and L(δB(xi), yi). The generality of (2) extends to our minimum imputed risk objective of (4). Below, we specify the δB and L(δB(xi), yi) we considered in our investigation. 3.3.1 Deterministic Decoding A simple translat</context>
<context position="14021" citStr="Crammer et al., 2006" startWordPosition="2330" endWordPosition="2333">will modify our objective function of (4) to take yet another expectation over the unknown y. Specifically, we will replace L(δB(x), yi) in (4) with E pB(y |x) L(y, yi). (7) y Now, the minimum imputed empirical risk objective of (4) becomes po(x |yi) pB(y |x) L(y, yi) (8) If the loss function L(y, yi) is a negated BLEU, this is equivalent to performing minimum-risk training described by (Smith and Eisner, 2006; Li and Eisner, 2009) on the imputed data.10 9One can manipulate the loss function to support other methods that use deterministic decoding, such as Perceptron (Collins, 2002) and MIRA (Crammer et al., 2006). 10Again, one may manipulate the loss function to support other probabilistic methods that use randomized decoding, such as CRFs (Lafferty et al., 2001). The objective function in (8) is now differentiable, since each coefficient pB(y |x) is a differentiable function of θ, and thus amenable to optimization by gradient-based methods; we use the L-BFGS algorithm (Liu et al., 1989) in our experiments. We perform experiments with the syntax-based MT system Joshua (Li et al., 2009a), which implements dynamic programming algorithms for second-order expectation semirings (Li and Eisner, 2009) to eff</context>
</contexts>
<marker>Crammer, Dekel, Keshet, ShalevShwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai ShalevShwartz, and Yoram Singer. 2006. Online passiveaggressive algorithms. J. Mach. Learn. Res., 7:551– 585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Dyer</author>
<author>Smaranda Muresan</author>
<author>Philip Resnik</author>
</authors>
<title>Generalizing word lattice translation.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>1012--1020</pages>
<contexts>
<context position="18674" citStr="Dyer et al. (2008)" startWordPosition="3109" endWordPosition="3112">rees used for the reverse and forward translations. In other words, this does round-trip imputation (i.e., from y to x, and then to y&apos;) at the rule level. This is essentially the approach taken by Li et al. (2010). 3.5 The Log-Linear Model pθ We have not yet specified the form of pθ. Following much work in MT, we begin with a linear model Xscore(x, y) = θ · f(x, y) = θkfk(x, y) (9) k where f(x, y) is a feature vector indexed by k. Our deterministic test-time translation system δθ simply 12Note that the forward translation of a WFSA is tractable by using a lattice-based decoder such as that by Dyer et al. (2008). outputs the highest-scoring y for fixed x. At training time, our randomized decoder (Section 3.3.2) uses the Boltzmann distribution (here a log-linear model) eγ·score(x,y) eγ·score(x,y) Z(x) =Py, eγ·score(x,y,) (10) The scaling factor γ controls the sharpness of the training-time distribution, i.e., the degree to which the randomized decoder favors the highest-scoring y. For large γ, our training objective approaches the imputed risk of the deterministic test-time system while remaining differentiable. In a task like MT, in addition to the input x and output y, we often need to introduce a l</context>
</contexts>
<marker>Dyer, Muresan, Resnik, 2008</marker>
<rawString>Christopher Dyer, Smaranda Muresan, and Philip Resnik. 2008. Generalizing word lattice translation. In ACL, pages 1012–1020.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Eck</author>
<author>Chiori Hori</author>
</authors>
<title>Overview of the iwslt 2005 evaluation campaign. In</title>
<date>2005</date>
<booktitle>In IWSLT.</booktitle>
<contexts>
<context position="23375" citStr="Eck and Hori, 2005" startWordPosition="3913" endWordPosition="3916">x |y) and pθ(y |x). By sticking to conditional models, we can efficiently use more sophisticated model features, and we can incorporate the loss function when we train 0, which should improve both efficiency and accuracy at test time. 5 Experimental Results We report results on Chinese-to-English translation tasks using Joshua (Li et al., 2009a), an open-source implementation of Hiero (Chiang, 2007). 5.1 Baseline Systems 5.1.1 IWSLT Task We train both reverse and forward baseline systems. The translation models are built using the corpus for the IWSLT 2005 Chinese to English translation task (Eck and Hori, 2005), which comprises 40,000 pairs of transcribed utterances in the travel domain. We use a 5-gram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1998), trained on the English (resp. Chinese) side of the bitext. We use a standard training pipeline and pruning settings recommended by (Chiang, 2007). 5.1.2 NIST Task For the NIST task, the TM is trained on about 1M parallel sentence pairs (about 28M words in each language), which are sub-sampled from corpora distributed by LDC for the NIST MT evaluation using a sampling method implemented in Joshua. We also used a 5-gram languag</context>
</contexts>
<marker>Eck, Hori, 2005</marker>
<rawString>Matthias Eck and Chiori Hori. 2005. Overview of the iwslt 2005 evaluation campaign. In In IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In ACL,</booktitle>
<pages>961--968</pages>
<contexts>
<context position="19529" citStr="Galley et al., 2006" startWordPosition="3245" endWordPosition="3248"> γ controls the sharpness of the training-time distribution, i.e., the degree to which the randomized decoder favors the highest-scoring y. For large γ, our training objective approaches the imputed risk of the deterministic test-time system while remaining differentiable. In a task like MT, in addition to the input x and output y, we often need to introduce a latent variable d to represent the hidden derivation that relates x to y. A derivation d represents a particular phrase segmentation in a phrase-based MT system (Koehn et al., 2003) and a derivation tree in a typical syntaxbased system (Galley et al., 2006; Chiang, 2007). We change our model to assign scores not to an (x, y) pair but to the detailed derivation d; in particular, now the function f that extracts a feature vector can look at all of d. We replace y by d in (9)–(10), and finally define pθ(y|x) by marginalizing out d, pθ(y |x) = X pθ(d|x) (11) dED(x,y) where D(x, y) represents the set of derivations that yield x and y. 4 Minimum Imputed Risk vs. EM The notion of imputing missing data is familiar from other settings (Little and Rubin, 1987), particularly the expectation maximization (EM) algorithm, a widely used generative approach. S</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In ACL, pages 961–968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In IWPT,</booktitle>
<pages>53--64</pages>
<contexts>
<context position="15213" citStr="Huang and Chiang, 2005" startWordPosition="2518" endWordPosition="2522">gs (Li and Eisner, 2009) to efficiently compute the gradients needed for optimizing (8). 3.4 Approximating po(x |yi) As mentioned at the end of Section 3.1, it is computationally infeasible to forward-translate each of the imputed reverse translations xis. We propose four approximations that are computationally feasible. Each may be regarded as a different approximation of po(x |yi) in equations (4) or (8). k-best. For each yi, add to the imputed training set only the k most probable translations {xi1, ... xik} according to po(x |yi). (These can be extracted from Xi using standard algorithms (Huang and Chiang, 2005).) Rescale their probabilities to sum to 1. Sampling. For each yi, add to the training set k independent samples {xi1, ... xik} from the distribution po(x |yi), each with weight 1/k. (These can be sampled from Xi using standard algorithms (Johnson et al., 2007).) This method is known in the literature as multiple imputation (Rubin, 1987). Lattice. 11 Under certain special cases it is be possible to compute the expected loss in (3) exactly via dynamic programming. Although Xi does contain exponentially many translations, it may use a “packed” representation in which these translations share str</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In IWPT, pages 53–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jui-Ting Huang</author>
<author>Xiao Li</author>
<author>Alex Acero</author>
</authors>
<title>Discriminative training methods for language models using conditional entropy criteria.</title>
<date>2010</date>
<booktitle>In ICASSP.</booktitle>
<contexts>
<context position="10088" citStr="Huang et al., 2010" startWordPosition="1648" endWordPosition="1651">it is computationally infeasible to forwardtranslate each xij E Xi. We therefore investigate several approximations to (4) in Section 3.4. 6One may exploit both supervised data {(xi, yi)} and unsupervised data {yj} to perform semi-supervised training via an interpolation of (2) and (4). We will do so in our experiments. 7Our approach may be applied to other tasks as well. For example, in a speech recognition task, SB is a speech recognizer that produces text, whereas po is a speech synthesizer that must produce a distribution over audio (or at least over acoustic features or phone sequences) (Huang et al., 2010). 3.2 The Reverse Prediction Model pφ A crucial ingredient in (4) is the reverse prediction model pφ(·|·) that attempts to impute the missing xi. We will train this model in advance, doing the best job we can from available data, including any outof-domain bilingual data as well as any in-domain monolingual data$ x. In the MT setting, δθ and pφ may have similar parameterization. One translates Chinese to English; the other translates English to Chinese. Yet the setup is not quite symmetric. Whereas δθ is a translation system that aims to produce a single, low-loss translation, the reverse vers</context>
</contexts>
<marker>Huang, Li, Acero, 2010</marker>
<rawString>Jui-Ting Huang, Xiao Li, and Alex Acero. 2010. Discriminative training methods for language models using conditional entropy criteria. In ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Bayesian inference for PCFGs via Markov chain Monte Carlo. In</title>
<date>2007</date>
<booktitle>NAACL,</booktitle>
<pages>139--146</pages>
<contexts>
<context position="15474" citStr="Johnson et al., 2007" startWordPosition="2565" endWordPosition="2569">pose four approximations that are computationally feasible. Each may be regarded as a different approximation of po(x |yi) in equations (4) or (8). k-best. For each yi, add to the imputed training set only the k most probable translations {xi1, ... xik} according to po(x |yi). (These can be extracted from Xi using standard algorithms (Huang and Chiang, 2005).) Rescale their probabilities to sum to 1. Sampling. For each yi, add to the training set k independent samples {xi1, ... xik} from the distribution po(x |yi), each with weight 1/k. (These can be sampled from Xi using standard algorithms (Johnson et al., 2007).) This method is known in the literature as multiple imputation (Rubin, 1987). Lattice. 11 Under certain special cases it is be possible to compute the expected loss in (3) exactly via dynamic programming. Although Xi does contain exponentially many translations, it may use a “packed” representation in which these translations share structure. This representation may furthermore enable sharing work in forward-translation, so as to efficiently translate the entire set Xi and obtain a distribution over translations y. Finally, the expected loss under that distribution, as required by equation (</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas Griffiths, and Sharon Goldwater. 2007. Bayesian inference for PCFGs via Markov chain Monte Carlo. In NAACL, pages 139–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In NAACL,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="19454" citStr="Koehn et al., 2003" startWordPosition="3231" endWordPosition="3234">γ·score(x,y) eγ·score(x,y) Z(x) =Py, eγ·score(x,y,) (10) The scaling factor γ controls the sharpness of the training-time distribution, i.e., the degree to which the randomized decoder favors the highest-scoring y. For large γ, our training objective approaches the imputed risk of the deterministic test-time system while remaining differentiable. In a task like MT, in addition to the input x and output y, we often need to introduce a latent variable d to represent the hidden derivation that relates x to y. A derivation d represents a particular phrase segmentation in a phrase-based MT system (Koehn et al., 2003) and a derivation tree in a typical syntaxbased system (Galley et al., 2006; Chiang, 2007). We change our model to assign scores not to an (x, y) pair but to the detailed derivation d; in particular, now the function f that extracts a feature vector can look at all of d. We replace y by d in (9)–(10), and finally define pθ(y|x) by marginalizing out d, pθ(y |x) = X pθ(d|x) (11) dED(x,y) where D(x, y) represents the set of derivations that yield x and y. 4 Minimum Imputed Risk vs. EM The notion of imputing missing data is familiar from other settings (Little and Rubin, 1987), particularly the ex</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In NAACL, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="11931" citStr="Lafferty et al., 2001" startWordPosition="1967" endWordPosition="1970">is trained by (empirically) minimizing − M �� 1 log pφ (xj |yj) + 2ar �φ�22 on some bilingual data, with the regularization coefficient σ2 tuned on held out data. It may be tolerable for pφ to impute mediocre translations xij. All that is necessary is that the (forward) translations generated from the imputed xij “simulate” the competing hypotheses that we would see when translating the correct Chinese input xi. 3.3 The Forward Translation System δθ and The Loss Function L(δθ(xi), yi) The minimum empirical risk objective of (2) is quite general and various popular supervised training methods (Lafferty et al., 2001; Collins, 2002; Och, 2003; Crammer et al., 2006; Smith and Eisner, $In a translation task from x to y, one usually does not make use of in-domain monolingual data x. But we can exploit x to train a language model po(x) for the reverse translation system, which will make the imputed xij look like true Chinese inputs. θ∗ = argmin θ 1 N N i=1 922 2006) can be formalized in this framework by choosing different functions for δB and L(δB(xi), yi). The generality of (2) extends to our minimum imputed risk objective of (4). Below, we specify the δB and L(δB(xi), yi) we considered in our investigation</context>
<context position="14174" citStr="Lafferty et al., 2001" startWordPosition="2353" endWordPosition="2356">B(y |x) L(y, yi). (7) y Now, the minimum imputed empirical risk objective of (4) becomes po(x |yi) pB(y |x) L(y, yi) (8) If the loss function L(y, yi) is a negated BLEU, this is equivalent to performing minimum-risk training described by (Smith and Eisner, 2006; Li and Eisner, 2009) on the imputed data.10 9One can manipulate the loss function to support other methods that use deterministic decoding, such as Perceptron (Collins, 2002) and MIRA (Crammer et al., 2006). 10Again, one may manipulate the loss function to support other probabilistic methods that use randomized decoding, such as CRFs (Lafferty et al., 2001). The objective function in (8) is now differentiable, since each coefficient pB(y |x) is a differentiable function of θ, and thus amenable to optimization by gradient-based methods; we use the L-BFGS algorithm (Liu et al., 1989) in our experiments. We perform experiments with the syntax-based MT system Joshua (Li et al., 2009a), which implements dynamic programming algorithms for second-order expectation semirings (Li and Eisner, 2009) to efficiently compute the gradients needed for optimizing (8). 3.4 Approximating po(x |yi) As mentioned at the end of Section 3.1, it is computationally infea</context>
<context position="22699" citStr="Lafferty et al., 2001" startWordPosition="3805" endWordPosition="3808">conditional probabilities such as pφ(x |y) and pθ(y |x) are also ratios, computing their denominators only requires us to sum over a packed forest of possible translations of a given y or x.13 In summary, EM would impute missing data using pθ(x |y) and predict outputs using pθ(y |x), both being conditional forms of the same joint model pθ(x, y). Our minimum imputed risk training method is similar, but it instead uses a pair of 13Analogously, discriminative CRFs have become more popular than generative HMMs because they permit efficient training even with a wide variety of log-linear features (Lafferty et al., 2001). separately parameterized, separately trained models pφ(x |y) and pθ(y |x). By sticking to conditional models, we can efficiently use more sophisticated model features, and we can incorporate the loss function when we train 0, which should improve both efficiency and accuracy at test time. 5 Experimental Results We report results on Chinese-to-English translation tasks using Joshua (Li et al., 2009a), an open-source implementation of Hiero (Chiang, 2007). 5.1 Baseline Systems 5.1.1 IWSLT Task We train both reverse and forward baseline systems. The translation models are built using the corpus</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Jason Eisner</author>
</authors>
<title>First- and second-order expectation semirings with applications to minimumrisk training on translation forests.</title>
<date>2009</date>
<booktitle>In EMNLP,</booktitle>
<pages>40--51</pages>
<contexts>
<context position="4942" citStr="Li and Eisner, 2009" startWordPosition="762" endWordPosition="765">criminative training step in the development of large MT systems — learning the log-linear combination of several component model scores (viewed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs — with our unsupervised discriminative training using only y. One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training (Och, 2003; Macherey et al., 2008), the averaged Perceptron (Liang et al., 2006), maximum conditional likelihood (Blunsom et al., 2008), minimum risk (Smith and Eisner, 2006; Li and Eisner, 2009), and MIRA (Watanabe et al., 2007; Chiang et al., 2009). We perform experiments using the open-source MT toolkit Joshua (Li et al., 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. 2 Supervised Discriminative Training via Minimization of Empirical Risk Let us first review discriminative training in the supervised setting—as used in MERT (Och, 2003) and subsequent work. One wishes to tune the parameters θ of some complex translation system δθ(x). The function δθ, which translates Chinese x to English y = δθ(x) need not be probabil</context>
<context position="7175" citStr="Li and Eisner (2009)" startWordPosition="1156" endWordPosition="1159">t {(xi, yi), i = 1, ... , N}. Therefore, �θ* = argmin ˜p(x, y) L(δθ(x), y) θ x,y = argmin 1 N L(δθ(xi),yi). (2) θ N i=1 The search for θ* typically requires the use of numerical methods and some regularization.5 3 Unsupervised Discriminative Training with Missing Inputs 3.1 Minimization of Imputed Risk We now turn to the unsupervised case, where we have training examples {yi} but not their corresponding inputs {xi}. We cannot compute the summand L(δθ(xi), yi) for such i in (2), since δθ(xi) requires to know xi. So we propose to replace 3This goal is different from the minimum risk training of Li and Eisner (2009) in a subtle but important way. In both cases, 0∗ minimizes risk or expected loss, but the expectation is w.r.t. different distributions: the expectation in Li and Eisner (2009) is under the conditional distribution p(y I x), while the expectation in (1) is under the joint distribution p(x, y). 4In the terminology of statistical decision theory, p(x, y) is a distribution over states of nature. We seek a decision rule Sθ(x) that will incur low expected loss on observations x that are generated from unseen states of nature. 5To compensate for the shortcut of using the unsmoothed empirical distri</context>
<context position="13835" citStr="Li and Eisner, 2009" startWordPosition="2301" endWordPosition="2304">nstead of using the argmax of (6), we assume during training that δB(x) is itself random, i.e. the MT system randomly outputs a translation y with probability pB(y |x). As a result, we will modify our objective function of (4) to take yet another expectation over the unknown y. Specifically, we will replace L(δB(x), yi) in (4) with E pB(y |x) L(y, yi). (7) y Now, the minimum imputed empirical risk objective of (4) becomes po(x |yi) pB(y |x) L(y, yi) (8) If the loss function L(y, yi) is a negated BLEU, this is equivalent to performing minimum-risk training described by (Smith and Eisner, 2006; Li and Eisner, 2009) on the imputed data.10 9One can manipulate the loss function to support other methods that use deterministic decoding, such as Perceptron (Collins, 2002) and MIRA (Crammer et al., 2006). 10Again, one may manipulate the loss function to support other probabilistic methods that use randomized decoding, such as CRFs (Lafferty et al., 2001). The objective function in (8) is now differentiable, since each coefficient pB(y |x) is a differentiable function of θ, and thus amenable to optimization by gradient-based methods; we use the L-BFGS algorithm (Liu et al., 1989) in our experiments. We perform </context>
</contexts>
<marker>Li, Eisner, 2009</marker>
<rawString>Zhifei Li and Jason Eisner. 2009. First- and second-order expectation semirings with applications to minimumrisk training on translation forests. In EMNLP, pages 40–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zaidan</author>
</authors>
<title>Joshua: An open source toolkit for parsing-based machine translation. In</title>
<date>2009</date>
<booktitle>WMT09,</booktitle>
<pages>26--30</pages>
<marker>Zaidan, 2009</marker>
<rawString>Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren Thornton, Jonathan Weese, and Omar. Zaidan. 2009a. Joshua: An open source toolkit for parsing-based machine translation. In WMT09, pages 26–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Jason Eisner</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Variational decoding for statistical machine translation.</title>
<date>2009</date>
<booktitle>In ACL,</booktitle>
<pages>593--601</pages>
<contexts>
<context position="5078" citStr="Li et al., 2009" startWordPosition="785" endWordPosition="788">wed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs — with our unsupervised discriminative training using only y. One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training (Och, 2003; Macherey et al., 2008), the averaged Perceptron (Liang et al., 2006), maximum conditional likelihood (Blunsom et al., 2008), minimum risk (Smith and Eisner, 2006; Li and Eisner, 2009), and MIRA (Watanabe et al., 2007; Chiang et al., 2009). We perform experiments using the open-source MT toolkit Joshua (Li et al., 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. 2 Supervised Discriminative Training via Minimization of Empirical Risk Let us first review discriminative training in the supervised setting—as used in MERT (Och, 2003) and subsequent work. One wishes to tune the parameters θ of some complex translation system δθ(x). The function δθ, which translates Chinese x to English y = δθ(x) need not be probabilistic. For example, θ may be the parameters of a scoring function used by δ, along with pruning and decoding heuristics, for extracting </context>
<context position="14502" citStr="Li et al., 2009" startWordPosition="2407" endWordPosition="2410"> function to support other methods that use deterministic decoding, such as Perceptron (Collins, 2002) and MIRA (Crammer et al., 2006). 10Again, one may manipulate the loss function to support other probabilistic methods that use randomized decoding, such as CRFs (Lafferty et al., 2001). The objective function in (8) is now differentiable, since each coefficient pB(y |x) is a differentiable function of θ, and thus amenable to optimization by gradient-based methods; we use the L-BFGS algorithm (Liu et al., 1989) in our experiments. We perform experiments with the syntax-based MT system Joshua (Li et al., 2009a), which implements dynamic programming algorithms for second-order expectation semirings (Li and Eisner, 2009) to efficiently compute the gradients needed for optimizing (8). 3.4 Approximating po(x |yi) As mentioned at the end of Section 3.1, it is computationally infeasible to forward-translate each of the imputed reverse translations xis. We propose four approximations that are computationally feasible. Each may be regarded as a different approximation of po(x |yi) in equations (4) or (8). k-best. For each yi, add to the imputed training set only the k most probable translations {xi1, ... </context>
<context position="17075" citStr="Li et al. (2009" startWordPosition="2834" endWordPosition="2837">1 L x,y 923 ambiguous weighted finite-state automaton Xi, (b) the forward translation system δθ is structured in a certain way as a weighted synchronous context-free grammar, and (c) the loss function decomposes in a certain way. We omit the details of the construction as beyond the scope of this paper. In our experimental setting described below, (b) is true (using Joshua), and (c) is true (since we use a loss function presented by Tromble et al. (2008) that is an approximation to BLEU and is decomposable). While (a) is not true in our setting because Xi is a hypergraph (which is ambiguous), Li et al. (2009b) show how to approximate a hypergraph representation of pφ(x |yi) by an unambiguous WFSA. One could then apply the construction to this WFSA12, obtaining an approximation to (3). Rule-level Composition. Intuitively, the reason why the structure-sharing in the hypergraph Xi (generated by the reverse system) cannot be exploited during forward translating is that when the forward Hiero system translates a string xi E Xi, it must parse it into recursive phrases. But the structure-sharing within the hypergraph of Xi has already parsed xi into recursive phrases, in a way determined by the reverse </context>
<context position="23101" citStr="Li et al., 2009" startWordPosition="3868" endWordPosition="3871">nstead uses a pair of 13Analogously, discriminative CRFs have become more popular than generative HMMs because they permit efficient training even with a wide variety of log-linear features (Lafferty et al., 2001). separately parameterized, separately trained models pφ(x |y) and pθ(y |x). By sticking to conditional models, we can efficiently use more sophisticated model features, and we can incorporate the loss function when we train 0, which should improve both efficiency and accuracy at test time. 5 Experimental Results We report results on Chinese-to-English translation tasks using Joshua (Li et al., 2009a), an open-source implementation of Hiero (Chiang, 2007). 5.1 Baseline Systems 5.1.1 IWSLT Task We train both reverse and forward baseline systems. The translation models are built using the corpus for the IWSLT 2005 Chinese to English translation task (Eck and Hori, 2005), which comprises 40,000 pairs of transcribed utterances in the travel domain. We use a 5-gram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1998), trained on the English (resp. Chinese) side of the bitext. We use a standard training pipeline and pruning settings recommended by (Chiang, 2007). 5.1.2 NI</context>
</contexts>
<marker>Li, Eisner, Khudanpur, 2009</marker>
<rawString>Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009b. Variational decoding for statistical machine translation. In ACL, pages 593–601.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Ziyuan Wang</author>
<author>Sanjeev Khudanpur</author>
<author>Jason Eisner</author>
</authors>
<title>Unsupervised discriminative language model training for machine translation using simulated confusion sets.</title>
<date>2010</date>
<booktitle>In COLING,</booktitle>
<pages>556--664</pages>
<contexts>
<context position="18269" citStr="Li et al. (2010)" startWordPosition="3032" endWordPosition="3035">rmined by the reverse Hiero system; each translation phrase (or rule) corresponding to a hyperedge. To exploit structure-sharing, we can use a forward translation system that decomposes according to that existing parse of xi. We can do that by considering only forward translations that respect the hypergraph structure of Xi. The simplest way to do this is to require complete isomorphism of the SCFG trees used for the reverse and forward translations. In other words, this does round-trip imputation (i.e., from y to x, and then to y&apos;) at the rule level. This is essentially the approach taken by Li et al. (2010). 3.5 The Log-Linear Model pθ We have not yet specified the form of pθ. Following much work in MT, we begin with a linear model Xscore(x, y) = θ · f(x, y) = θkfk(x, y) (9) k where f(x, y) is a feature vector indexed by k. Our deterministic test-time translation system δθ simply 12Note that the forward translation of a WFSA is tractable by using a lattice-based decoder such as that by Dyer et al. (2008). outputs the highest-scoring y for fixed x. At training time, our randomized decoder (Section 3.3.2) uses the Boltzmann distribution (here a log-linear model) eγ·score(x,y) eγ·score(x,y) Z(x) =P</context>
</contexts>
<marker>Li, Wang, Khudanpur, Eisner, 2010</marker>
<rawString>Zhifei Li, Ziyuan Wang, Sanjeev Khudanpur, and Jason Eisner. 2010. Unsupervised discriminative language model training for machine translation using simulated confusion sets. In COLING, pages 556–664.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
<author>Ben Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In ACL,</booktitle>
<pages>761--768</pages>
<marker>Liang, Bouchard-Cˆot´e, Klein, Taskar, 2006</marker>
<rawString>Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and Ben Taskar. 2006. An end-to-end discriminative approach to machine translation. In ACL, pages 761– 768.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J A Little</author>
<author>D B Rubin</author>
</authors>
<title>Statistical Analysis with Missing Data.</title>
<date>1987</date>
<publisher>J. Wiley &amp; Sons,</publisher>
<location>New York.</location>
<contexts>
<context position="1335" citStr="Little and Rubin, 1987" startWordPosition="192" endWordPosition="195">s to ensure that probabilistic “round-trip” translation from a targetlanguage sentence to the source-language and back will have low expected loss. Theoretically, this may be justified as (discriminatively) minimizing an imputed empirical risk. Empirically, we demonstrate that augmenting supervised training with unsupervised data improves translation performance over the supervised case for both IWSLT and NIST tasks. 1 Introduction Missing data is a common problem in statistics when fitting the parameters 0 of a model. A common strategy is to attempt to impute, or “fill in,” the missing data (Little and Rubin, 1987), as typified by the EM algorithm. In this paper we develop imputation techniques when 0 is to be trained discriminatively. We focus on machine translation (MT) as our example application. A Chinese-to-English machine translation system is given a Chinese sentence x and ∗ Zhifei Li is currently working at Google Research, and this work was done while he was a PHD student at Johns Hopkins University. Jason Eisner Johns Hopkins University Baltimore, MD 21218, USA eisner@jhu.edu Brian Roark Oregon Health &amp; Science University Beaverton, Oregon 97006, USA roark@cslu.ogi.edu asked to predict its Eng</context>
<context position="20033" citStr="Little and Rubin, 1987" startWordPosition="3340" endWordPosition="3343"> phrase-based MT system (Koehn et al., 2003) and a derivation tree in a typical syntaxbased system (Galley et al., 2006; Chiang, 2007). We change our model to assign scores not to an (x, y) pair but to the detailed derivation d; in particular, now the function f that extracts a feature vector can look at all of d. We replace y by d in (9)–(10), and finally define pθ(y|x) by marginalizing out d, pθ(y |x) = X pθ(d|x) (11) dED(x,y) where D(x, y) represents the set of derivations that yield x and y. 4 Minimum Imputed Risk vs. EM The notion of imputing missing data is familiar from other settings (Little and Rubin, 1987), particularly the expectation maximization (EM) algorithm, a widely used generative approach. So it is instructive to compare EM with minimum imputed risk. One can estimate θ by maximizing the loglikelihood of the data {(xi, yi), i = 1, ... , N} as log pθ(xi, yi). (12) If the xi’s are missing, EM tries to iteratively maximize the marginal probability: pθ(x,yi). (13) pθ(y |x) = 1 N argmax θ XN i=1 argmax 1 XN X θ N i=1 log x 924 The E-step of each iteration comprises computing &amp;x pθt(x |yi) log pθ(x, yi), the expected loglikelihood of the complete data, where pθt(x |yi) is the conditional part</context>
</contexts>
<marker>Little, Rubin, 1987</marker>
<rawString>R. J. A. Little and D. B. Rubin. 1987. Statistical Analysis with Missing Data. J. Wiley &amp; Sons, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
<author>C Dong</author>
</authors>
<title>On the limited memory bfgs method for large scale optimization.</title>
<date>1989</date>
<booktitle>Mathematical Programming,</booktitle>
<pages>45--503</pages>
<contexts>
<context position="14403" citStr="Liu et al., 1989" startWordPosition="2390" endWordPosition="2393"> by (Smith and Eisner, 2006; Li and Eisner, 2009) on the imputed data.10 9One can manipulate the loss function to support other methods that use deterministic decoding, such as Perceptron (Collins, 2002) and MIRA (Crammer et al., 2006). 10Again, one may manipulate the loss function to support other probabilistic methods that use randomized decoding, such as CRFs (Lafferty et al., 2001). The objective function in (8) is now differentiable, since each coefficient pB(y |x) is a differentiable function of θ, and thus amenable to optimization by gradient-based methods; we use the L-BFGS algorithm (Liu et al., 1989) in our experiments. We perform experiments with the syntax-based MT system Joshua (Li et al., 2009a), which implements dynamic programming algorithms for second-order expectation semirings (Li and Eisner, 2009) to efficiently compute the gradients needed for optimizing (8). 3.4 Approximating po(x |yi) As mentioned at the end of Section 3.1, it is computationally infeasible to forward-translate each of the imputed reverse translations xis. We propose four approximations that are computationally feasible. Each may be regarded as a different approximation of po(x |yi) in equations (4) or (8). k-</context>
</contexts>
<marker>Liu, Nocedal, Dong, 1989</marker>
<rawString>Dong C. Liu, Jorge Nocedal, and Dong C. 1989. On the limited memory bfgs method for large scale optimization. Mathematical Programming, 45:503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Macherey</author>
<author>Franz Och</author>
<author>Ignacio Thayer</author>
<author>Jakob Uszkoreit</author>
</authors>
<title>Lattice-based minimum error rate training for statistical machine translation.</title>
<date>2008</date>
<booktitle>In EMNLP,</booktitle>
<pages>725--734</pages>
<contexts>
<context position="4781" citStr="Macherey et al., 2008" startWordPosition="737" endWordPosition="740">l pθ(y), as is the norm. Following the theoretical development below, the empirical effectiveness of our approach is demonstrated by replacing a key supervised discriminative training step in the development of large MT systems — learning the log-linear combination of several component model scores (viewed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs — with our unsupervised discriminative training using only y. One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training (Och, 2003; Macherey et al., 2008), the averaged Perceptron (Liang et al., 2006), maximum conditional likelihood (Blunsom et al., 2008), minimum risk (Smith and Eisner, 2006; Li and Eisner, 2009), and MIRA (Watanabe et al., 2007; Chiang et al., 2009). We perform experiments using the open-source MT toolkit Joshua (Li et al., 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. 2 Supervised Discriminative Training via Minimization of Empirical Risk Let us first review discriminative training in the supervised setting—as used in MERT (Och, 2003) and subsequent work. On</context>
</contexts>
<marker>Macherey, Och, Thayer, Uszkoreit, 2008</marker>
<rawString>Wolfgang Macherey, Franz Och, Ignacio Thayer, and Jakob Uszkoreit. 2008. Lattice-based minimum error rate training for statistical machine translation. In EMNLP, pages 725–734.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan May</author>
<author>Kevin Knight</author>
</authors>
<title>A better n-best list: practical determinization of weighted finite tree automata.</title>
<date>2006</date>
<booktitle>In NAACL,</booktitle>
<pages>351--358</pages>
<contexts>
<context position="31929" citStr="May and Knight, 2006" startWordPosition="5361" endWordPosition="5364">he forward translations. Still, even with the worse imputation (in the case of “NLM”), our forward translations improve as we add more monolingual data. 5.5.2 Imputation with Different k-best Sizes In all the experiments so far, we used the reverse translation system to impute only a single Chinese translation for each English monolingual sentence. This is the 1-best approximation of section 3.4. Table 5 shows (in the fully unsupervised case) that the performance does not change much as k increases.16 This may be because that the 5-best sentences are likely to be quite similar to one another (May and Knight, 2006). Imputing a longer k-best list, a sample, or a lattice for xi (see section 3.4) might achieve more diversity in the training inputs, which might make the system more robust. 6 Conclusions In this paper, we present an unsupervised discriminative training method that works with missing inputs. The key idea in our method is to use a reverse model to impute the missing input from the observed output. The training will then forward translate the imputed input, and choose the parameters of the forward model such that the imputed risk (i.e., 16In the present experiments, however, we simply weighted </context>
</contexts>
<marker>May, Knight, 2006</marker>
<rawString>Jonathan May and Kevin Knight. 2006. A better n-best list: practical determinization of weighted finite tree automata. In NAACL, pages 351–358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Minka</author>
</authors>
<title>Empirical risk minimization is an incomplete inductive principle.</title>
<date>2000</date>
<booktitle>In MIT Media Lab note.</booktitle>
<contexts>
<context position="7839" citStr="Minka, 2000" startWordPosition="1267" endWordPosition="1268">nimizes risk or expected loss, but the expectation is w.r.t. different distributions: the expectation in Li and Eisner (2009) is under the conditional distribution p(y I x), while the expectation in (1) is under the joint distribution p(x, y). 4In the terminology of statistical decision theory, p(x, y) is a distribution over states of nature. We seek a decision rule Sθ(x) that will incur low expected loss on observations x that are generated from unseen states of nature. 5To compensate for the shortcut of using the unsmoothed empirical distribution rather than a posterior estimate of p(x, y) (Minka, 2000), it is common to add a regularization term ||0||2 2 in the objective of (2). The regularization term can prevent overfitting to a training set that is not large enough to learn all parameters. 921 L(δθ(xi), yi) with the expectation E pφ(x |yi) L(δθ(x), yi), (3) x where pφ(· |·) is a “reverse prediction model” that attempts to impute the missing xi data. We call the resulting variant of (2) the minimization of imputed empirical risk, and say that E pφ(x |yi) L(δθ(x),yi) (4) x is the estimate with the minimum imputed risk6. The minimum imputed risk objective of (4) could be evaluated by brute f</context>
</contexts>
<marker>Minka, 2000</marker>
<rawString>Thomas Minka. 2000. Empirical risk minimization is an incomplete inductive principle. In MIT Media Lab note.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="4757" citStr="Och, 2003" startWordPosition="735" endWordPosition="736">nguage model pθ(y), as is the norm. Following the theoretical development below, the empirical effectiveness of our approach is demonstrated by replacing a key supervised discriminative training step in the development of large MT systems — learning the log-linear combination of several component model scores (viewed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs — with our unsupervised discriminative training using only y. One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training (Och, 2003; Macherey et al., 2008), the averaged Perceptron (Liang et al., 2006), maximum conditional likelihood (Blunsom et al., 2008), minimum risk (Smith and Eisner, 2006; Li and Eisner, 2009), and MIRA (Watanabe et al., 2007; Chiang et al., 2009). We perform experiments using the open-source MT toolkit Joshua (Li et al., 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. 2 Supervised Discriminative Training via Minimization of Empirical Risk Let us first review discriminative training in the supervised setting—as used in MERT (Och, 2003)</context>
<context position="11957" citStr="Och, 2003" startWordPosition="1973" endWordPosition="1974"> − M �� 1 log pφ (xj |yj) + 2ar �φ�22 on some bilingual data, with the regularization coefficient σ2 tuned on held out data. It may be tolerable for pφ to impute mediocre translations xij. All that is necessary is that the (forward) translations generated from the imputed xij “simulate” the competing hypotheses that we would see when translating the correct Chinese input xi. 3.3 The Forward Translation System δθ and The Loss Function L(δθ(xi), yi) The minimum empirical risk objective of (2) is quite general and various popular supervised training methods (Lafferty et al., 2001; Collins, 2002; Och, 2003; Crammer et al., 2006; Smith and Eisner, $In a translation task from x to y, one usually does not make use of in-domain monolingual data x. But we can exploit x to train a language model po(x) for the reverse translation system, which will make the imputed xij look like true Chinese inputs. θ∗ = argmin θ 1 N N i=1 922 2006) can be formalized in this framework by choosing different functions for δB and L(δB(xi), yi). The generality of (2) extends to our minimum imputed risk objective of (4). Below, we specify the δB and L(δB(xi), yi) we considered in our investigation. 3.3.1 Deterministic Deco</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In ACL, pages 160– 167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2001</date>
<booktitle>In ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="6115" citStr="Papineni et al., 2001" startWordPosition="960" endWordPosition="963">inese x to English y = δθ(x) need not be probabilistic. For example, θ may be the parameters of a scoring function used by δ, along with pruning and decoding heuristics, for extracting a high-scoring translation of x. The goal of discriminative training is to minimize the expected loss of δθ(·), under a given taskspecific loss function L(y&apos;, y) that measures how 2Note that the extra monolingual data is used only for tuning the model weights, but not for inducing new phrases or rules. bad it would be to output y&apos; when the correct output is y. For an MT system that is judged by the BLEU metric (Papineni et al., 2001), for instance, L(y&apos;, y) may be the negated BLEU score of y&apos; w.r.t. y. To be precise, the goal3 is to find θ with low Bayes risk, �θ* = argmin p(x, y) L(δθ(x), y) (1) θ x,y where p(x, y) is the joint distribution of the inputoutput pairs.4 The true p(x, y) is, of course, not known and, in practice, one typically minimizes empirical risk by replacing p(x, y) above with the empirical distribution ˜p(x, y) given by a supervised training set {(xi, yi), i = 1, ... , N}. Therefore, �θ* = argmin ˜p(x, y) L(δθ(x), y) θ x,y = argmin 1 N L(δθ(xi),yi). (2) θ N i=1 The search for θ* typically requires the</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. BLEU: A method for automatic evaluation of machine translation. In ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D B Rubin</author>
</authors>
<title>Multiple Imputation for Nonresponse in Surveys.</title>
<date>1987</date>
<publisher>J. Wiley &amp; Sons,</publisher>
<location>New York.</location>
<contexts>
<context position="1335" citStr="Rubin, 1987" startWordPosition="194" endWordPosition="195"> that probabilistic “round-trip” translation from a targetlanguage sentence to the source-language and back will have low expected loss. Theoretically, this may be justified as (discriminatively) minimizing an imputed empirical risk. Empirically, we demonstrate that augmenting supervised training with unsupervised data improves translation performance over the supervised case for both IWSLT and NIST tasks. 1 Introduction Missing data is a common problem in statistics when fitting the parameters 0 of a model. A common strategy is to attempt to impute, or “fill in,” the missing data (Little and Rubin, 1987), as typified by the EM algorithm. In this paper we develop imputation techniques when 0 is to be trained discriminatively. We focus on machine translation (MT) as our example application. A Chinese-to-English machine translation system is given a Chinese sentence x and ∗ Zhifei Li is currently working at Google Research, and this work was done while he was a PHD student at Johns Hopkins University. Jason Eisner Johns Hopkins University Baltimore, MD 21218, USA eisner@jhu.edu Brian Roark Oregon Health &amp; Science University Beaverton, Oregon 97006, USA roark@cslu.ogi.edu asked to predict its Eng</context>
<context position="15552" citStr="Rubin, 1987" startWordPosition="2581" endWordPosition="2582">ifferent approximation of po(x |yi) in equations (4) or (8). k-best. For each yi, add to the imputed training set only the k most probable translations {xi1, ... xik} according to po(x |yi). (These can be extracted from Xi using standard algorithms (Huang and Chiang, 2005).) Rescale their probabilities to sum to 1. Sampling. For each yi, add to the training set k independent samples {xi1, ... xik} from the distribution po(x |yi), each with weight 1/k. (These can be sampled from Xi using standard algorithms (Johnson et al., 2007).) This method is known in the literature as multiple imputation (Rubin, 1987). Lattice. 11 Under certain special cases it is be possible to compute the expected loss in (3) exactly via dynamic programming. Although Xi does contain exponentially many translations, it may use a “packed” representation in which these translations share structure. This representation may furthermore enable sharing work in forward-translation, so as to efficiently translate the entire set Xi and obtain a distribution over translations y. Finally, the expected loss under that distribution, as required by equation (3), may also be efficiently computable. All this turns out to be possible if (</context>
<context position="20033" citStr="Rubin, 1987" startWordPosition="3342" endWordPosition="3343">ed MT system (Koehn et al., 2003) and a derivation tree in a typical syntaxbased system (Galley et al., 2006; Chiang, 2007). We change our model to assign scores not to an (x, y) pair but to the detailed derivation d; in particular, now the function f that extracts a feature vector can look at all of d. We replace y by d in (9)–(10), and finally define pθ(y|x) by marginalizing out d, pθ(y |x) = X pθ(d|x) (11) dED(x,y) where D(x, y) represents the set of derivations that yield x and y. 4 Minimum Imputed Risk vs. EM The notion of imputing missing data is familiar from other settings (Little and Rubin, 1987), particularly the expectation maximization (EM) algorithm, a widely used generative approach. So it is instructive to compare EM with minimum imputed risk. One can estimate θ by maximizing the loglikelihood of the data {(xi, yi), i = 1, ... , N} as log pθ(xi, yi). (12) If the xi’s are missing, EM tries to iteratively maximize the marginal probability: pθ(x,yi). (13) pθ(y |x) = 1 N argmax θ XN i=1 argmax 1 XN X θ N i=1 log x 924 The E-step of each iteration comprises computing &amp;x pθt(x |yi) log pθ(x, yi), the expected loglikelihood of the complete data, where pθt(x |yi) is the conditional part</context>
</contexts>
<marker>Rubin, 1987</marker>
<rawString>D. B. Rubin. 1987. Multiple Imputation for Nonresponse in Surveys. J. Wiley &amp; Sons, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Minimum risk annealing for training log-linear models.</title>
<date>2006</date>
<booktitle>In ACL,</booktitle>
<pages>787--794</pages>
<contexts>
<context position="4920" citStr="Smith and Eisner, 2006" startWordPosition="758" endWordPosition="761">ing a key supervised discriminative training step in the development of large MT systems — learning the log-linear combination of several component model scores (viewed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs — with our unsupervised discriminative training using only y. One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training (Och, 2003; Macherey et al., 2008), the averaged Perceptron (Liang et al., 2006), maximum conditional likelihood (Blunsom et al., 2008), minimum risk (Smith and Eisner, 2006; Li and Eisner, 2009), and MIRA (Watanabe et al., 2007; Chiang et al., 2009). We perform experiments using the open-source MT toolkit Joshua (Li et al., 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. 2 Supervised Discriminative Training via Minimization of Empirical Risk Let us first review discriminative training in the supervised setting—as used in MERT (Och, 2003) and subsequent work. One wishes to tune the parameters θ of some complex translation system δθ(x). The function δθ, which translates Chinese x to English y = δθ(x</context>
<context position="13813" citStr="Smith and Eisner, 2006" startWordPosition="2297" endWordPosition="2300">.2 Randomized Decoding Instead of using the argmax of (6), we assume during training that δB(x) is itself random, i.e. the MT system randomly outputs a translation y with probability pB(y |x). As a result, we will modify our objective function of (4) to take yet another expectation over the unknown y. Specifically, we will replace L(δB(x), yi) in (4) with E pB(y |x) L(y, yi). (7) y Now, the minimum imputed empirical risk objective of (4) becomes po(x |yi) pB(y |x) L(y, yi) (8) If the loss function L(y, yi) is a negated BLEU, this is equivalent to performing minimum-risk training described by (Smith and Eisner, 2006; Li and Eisner, 2009) on the imputed data.10 9One can manipulate the loss function to support other methods that use deterministic decoding, such as Perceptron (Collins, 2002) and MIRA (Crammer et al., 2006). 10Again, one may manipulate the loss function to support other probabilistic methods that use randomized decoding, such as CRFs (Lafferty et al., 2001). The objective function in (8) is now differentiable, since each coefficient pB(y |x) is a differentiable function of θ, and thus amenable to optimization by gradient-based methods; we use the L-BFGS algorithm (Liu et al., 1989) in our ex</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>David A. Smith and Jason Eisner. 2006. Minimum risk annealing for training log-linear models. In ACL, pages 787–794.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Tromble</author>
<author>Shankar Kumar</author>
<author>Franz Och</author>
<author>Wolfgang Macherey</author>
</authors>
<title>Lattice minimum-Bayes-risk decoding for statistical machine translation.</title>
<date>2008</date>
<booktitle>In EMNLP,</booktitle>
<pages>620--629</pages>
<contexts>
<context position="16918" citStr="Tromble et al. (2008)" startWordPosition="2805" endWordPosition="2808">e do not empirically evaluate it since its implementation requires extensive engineering effort that is beyond the main scope of this paper. θ∗ = argmin B 1 N N i=1 L x,y 923 ambiguous weighted finite-state automaton Xi, (b) the forward translation system δθ is structured in a certain way as a weighted synchronous context-free grammar, and (c) the loss function decomposes in a certain way. We omit the details of the construction as beyond the scope of this paper. In our experimental setting described below, (b) is true (using Joshua), and (c) is true (since we use a loss function presented by Tromble et al. (2008) that is an approximation to BLEU and is decomposable). While (a) is not true in our setting because Xi is a hypergraph (which is ambiguous), Li et al. (2009b) show how to approximate a hypergraph representation of pφ(x |yi) by an unambiguous WFSA. One could then apply the construction to this WFSA12, obtaining an approximation to (3). Rule-level Composition. Intuitively, the reason why the structure-sharing in the hypergraph Xi (generated by the reverse system) cannot be exploited during forward translating is that when the forward Hiero system translates a string xi E Xi, it must parse it in</context>
</contexts>
<marker>Tromble, Kumar, Och, Macherey, 2008</marker>
<rawString>Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang Macherey. 2008. Lattice minimum-Bayes-risk decoding for statistical machine translation. In EMNLP, pages 620–629.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Jun Suzuki</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Online large-margin training for statistical machine translation. In EMNLP-CoNLL,</title>
<date>2007</date>
<pages>764--773</pages>
<contexts>
<context position="4975" citStr="Watanabe et al., 2007" startWordPosition="768" endWordPosition="771"> development of large MT systems — learning the log-linear combination of several component model scores (viewed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs — with our unsupervised discriminative training using only y. One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training (Och, 2003; Macherey et al., 2008), the averaged Perceptron (Liang et al., 2006), maximum conditional likelihood (Blunsom et al., 2008), minimum risk (Smith and Eisner, 2006; Li and Eisner, 2009), and MIRA (Watanabe et al., 2007; Chiang et al., 2009). We perform experiments using the open-source MT toolkit Joshua (Li et al., 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. 2 Supervised Discriminative Training via Minimization of Empirical Risk Let us first review discriminative training in the supervised setting—as used in MERT (Och, 2003) and subsequent work. One wishes to tune the parameters θ of some complex translation system δθ(x). The function δθ, which translates Chinese x to English y = δθ(x) need not be probabilistic. For example, θ may be the </context>
</contexts>
<marker>Watanabe, Suzuki, Tsukada, Isozaki, 2007</marker>
<rawString>Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. 2007. Online large-margin training for statistical machine translation. In EMNLP-CoNLL, pages 764–773.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>