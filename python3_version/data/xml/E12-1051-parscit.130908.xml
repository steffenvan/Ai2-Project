<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9978165">
Instance-Driven Attachment of Semantic Annotations
over Conceptual Hierarchies
</title>
<author confidence="0.987771">
Janara Christensen*
</author>
<affiliation confidence="0.995104">
University of Washington
</affiliation>
<address confidence="0.848012">
Seattle, Washington 98195
</address>
<email confidence="0.99878">
janara@cs.washington.edu
</email>
<author confidence="0.850918">
Marius Pas¸ca
</author>
<affiliation confidence="0.805787">
Google Inc.
</affiliation>
<address confidence="0.883147">
Mountain View, California 94043
</address>
<email confidence="0.998808">
mars@google.com
</email>
<sectionHeader confidence="0.995633" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999905">
Whether automatically extracted or human
generated, open-domain factual knowledge
is often available in the form of semantic
annotations (e.g., composed-by) that take
one or more specific instances (e.g., rhap-
sody in blue, george gershwin) as their ar-
guments. This paper introduces a method
for converting flat sets of instance-level
annotations into hierarchically organized,
concept-level annotations, which capture
not only the broad semantics of the desired
arguments (e.g., ‘People’ rather than ‘Loca-
tions’), but also the correct level of general-
ity (e.g., ‘Composers’ rather than ‘People’,
or ‘Jazz Composers’). The method refrains
from encoding features specific to a partic-
ular domain or annotation, to ensure imme-
diate applicability to new, previously un-
seen annotations. Over a gold standard of
semantic annotations and concepts that best
capture their arguments, the method sub-
stantially outperforms three baselines, on
average, computing concepts that are less
than one step in the hierarchy away from
the corresponding gold standard concepts.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.991925551020409">
Background: Knowledge about the world can
be thought of as semantic assertions or anno-
tations, at two levels of granularity: instance
level (e.g., rhapsody in blue, tristan und isolde,
george gershwin, richard wagner) and concept
level (e.g., ‘Musical Compositions’, ‘Works of
Art’, ‘Composers’). Instance-level annotations
correspond to factual knowledge that can be
found in repositories extracted automatically from
text (Banko et al., 2007; Wu and Weld, 2010)
*Contributions made during an internship at Google.
or manually created within encyclopedic re-
sources (Remy, 2002). Such facts could state, for
instance, that rhapsody in blue was composed-
by george gershwin, or that tristan und isolde
was composed-by richard wagner. In compar-
ison, concept-level annotations more concisely
and effectively capture the underlying semantics
of the annotations by identifying the concepts cor-
responding to the arguments, e.g., ‘Musical Com-
positions’ are composed-by ‘Composers’.
The frequent occurrence of instances, relative
to more abstract concepts, in Web documents and
popular Web search queries (Barr et al., 2008;
Li, 2010), is both an asset and a liability from
the point of view of knowledge acquisition. On
one hand, it makes instance-level annotations rel-
atively easy to find, either from manually created
resources (Remy, 2002; Bollacker et al., 2008),
or extracted automatically from text (Banko et
al., 2007). On the other hand, it makes concept-
level annotations more difficult to acquire di-
rectly. While “Rhapsody in Blue was composed
by George Gershwin [..]” may occur in some
form within Web documents, the more abstract
“Musical compositions are composed by musi-
cians [..]” is unlikely to occur. A more practical
approach to collecting concept-level annotations
is to indirectly derive them from already plenti-
ful instance-level annotations, effectively distill-
ing factual knowledge into more abstract, concise
and generalizable knowledge.
Contributions: This paper introduces a method
for converting flat sets of specific, instance-
level annotations into hierarchically organized,
concept-level annotations. As illustrated in Fig-
ure 1, the resulting annotations must capture not
just the broad semantics of the desired arguments
(e.g., ‘People’ rather than ‘Locations’ or ‘Prod-
</bodyText>
<page confidence="0.981366">
503
</page>
<note confidence="0.858705">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 503–513,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
Annotations
</note>
<figureCaption confidence="0.782071333333333">
Figure 1: Hierarchical Semantic Annotations: The
attachment of semantic annotations (e.g., composed-
by) into a conceptual hierarchy, a portion of which is
</figureCaption>
<bodyText confidence="0.966106166666667">
shown in the diagram, requires the identification of the
correct concept at the correct level of generality (e.g.,
‘Composers’ rather than ‘Jazz Composers’ or ‘Peo-
ple’, for the right argument of composed-by).
ucts’, as the right argument of the annotation
composed-by), but actually identify the concepts
at the correct level of generality/specificity (e.g.,
‘Composers’ rather than ‘Artists’ or ‘Jazz Com-
posers’) in the underlying conceptual hierarchy.
To ensure portability to new, previously unseen
annotations, the proposed method avoids encod-
ing features specific to a particular domain or an-
notation. In particular, the use of annotations’ la-
bels (composed-by) as lexical features might be
tempting, but would anchor the annotation model
to that particular annotation. Instead, the method
relies only on features that generalize across an-
notations. Over a gold standard of semantic anno-
tations and concepts that best capture their argu-
ments, the method substantially outperforms three
baseline methods. On average, the method com-
putes concepts that are less than one step in the
hierarchy away from the corresponding gold stan-
dard concepts of the various annotations.
</bodyText>
<sectionHeader confidence="0.977818" genericHeader="method">
2 Hierarchical Semantic Annotations
</sectionHeader>
<subsectionHeader confidence="0.820971">
2.1 Task Description
</subsectionHeader>
<bodyText confidence="0.713802666666667">
Data Sources: The computation of hierarchical
semantic annotations relies on the following data
sources:
</bodyText>
<listItem confidence="0.9864096">
• a target annotation r (e.g., acted-in) that takes
M arguments;
• N annotations I={&lt;iij, ... , imj&gt;} j�1 of
r at instance level, e.g., {&lt;leonardo dicaprio,
inception&gt;, &lt;milla jovovich, fifth element&gt;} (in
this example, M=2);
• mappings {i→c} from instances to con-
cepts to which they belong, e.g., milla jovovich
→ ‘American Actors’, milla jovovich → ‘People
from Kiev’, milla jovovich → ‘Models’;
• mappings {cs→cg} from more specific con-
cepts to more general concepts, as encoded in a
hierarchy H, e.g., ‘American Actors’→‘Actors’,
‘People from Kiev’→‘People from Ukraine’,
‘Actors’→‘Entertainers’.
</listItem>
<bodyText confidence="0.999832510638298">
Thus, the main inputs are the conceptual hi-
erarchy H, and the instance-level annotations I.
The hierarchy contains instance-to-concept map-
pings, as well as specific-to-general concept map-
pings. Via transitivity, instances (milla jovovich)
and concepts (‘American Actors’) may be im-
mediate children of more general concepts (‘Ac-
tors’), or transitive descendants of more general
concepts (‘Entertainers’). The hierarchy is not re-
quired to be a tree; in particular, a concept may
have multiple parent concepts. The instance-level
annotations may be created collaboratively by hu-
man contributors, or extracted automatically from
Web documents or some other data source.
Goal: Given the data sources, the goal is to de-
termine to which concept c in the hierarchy H the
arguments of the target concept-level annotation
r should be attached. While the left argument of
acted-in could attach to ‘American Actors’, ‘Peo-
ple from Kiev’, ‘Entertainers’ or ‘People’, it is
best attached to the concept ‘Actors’. The goal
is to select the concept c that most appropriately
generalizes across the instances. Over the set I
of instance-level annotations, selecting a method
for this goal can be thought of as a minimization
problem. The metric to be minimized is the sum
of the distances between each predicted concept c
and the correct concept cgold, where the distance
is the number of edges between c and cgold in H.
Intuitions and Challenges: Given instances such
as milla jovovich that instantiate an argument of
an annotation like acted-in, the conceptual hierar-
chy can be used to propagate the annotation up-
wards, from instances to their concepts, then in
turn further upwards to more general concepts.
The best concept would be one of the many can-
didate concepts reached during propagation. In-
tuitively, when compared to other candidate con-
cepts, a higher proportion of the descendant in-
stances of the best concept should instantiate (or
match) the annotation. At the same time, rela-
tive to other candidate concepts, the best concept
should have more descendant instances.
While the intuitions seem clear, their inclu-
sion in a working method faces a series of prac-
tical challenges. First, the data sources may be
noisy. One form of noise is missing or erroneous
</bodyText>
<figure confidence="0.829541625">
composed−by lives−in instrument−played sung−by
Conceptual hierarchy
Baroque Composers
Composers by genre
Composers
People
Jazz Composers
Cellists Singers
Musicians
504
Conceptual hierarchy
Entities
Locations People
Singers Actors
American Actors English Actors
Instance-level annotations
acted-in(leonardo dicaprio, inception)
acted-in(milla jovovich, fifth element)
acted-in(judy dench, casino royale)
acted-in(colin firth, the king’s speech)
Instance to concept mappings
leonardo dicaprio: American Actors
milla jovovich: American Actors
judy dench: English Actors
colin firth: English Actors
Candidate concepts
Entities
People
Actors
American Actors
English Actors
Raw statistics
</figure>
<construct confidence="0.571639333333333">
Entities, 4, 0.01 ...
People, 3, 0.1 ...
Actors, 2, 0.7 ...
</construct>
<table confidence="0.8706582">
American Actors, 1, 0.9 ...
English Actors, 1, 0.8 ...
Training/testing data
People-Actors, 3/2, 0.1/0.7 ...
Actors-People, 2/3, 0.7/0.1 ...
Actors-American Actors, 2/1, 0.7/0.9 ...
American Actors-Actors, 1/2, 0.9/0.7 ...
.
.
.
Classified data
0, People-Actors, 3/2, 0.1/0.7 ...
1, Actors-People, 2/3, 0.7/0.1 ...
1, Actors-American Actors, 2/1, 0.7/0.9 ...
0, American Actors-Actors, 1/2, 0.9/0.7 ...
</table>
<figure confidence="0.944820235294118">
.
..
Ranked data (for Concept-level annotations)
4, Actors
3, People
2, American Actors
1, English Actors
0, Entities
Concept-level annotations
acted-in(Actors, ?)
Features Depth, Instance Percent ...
Query logs
fifth element actors
fifth element costumes
inception quotes
out of africa actors
the king’s speech oscars
</figure>
<figureCaption confidence="0.999909">
Figure 2: Method Overview: Inferring concept-level annotations from instance-level annotations.
</figureCaption>
<bodyText confidence="0.999993302325581">
instance-level annotations, which may artificially
skew the distribution of matching instances to-
wards a less than optimal region in the hierarchy.
If the input annotations for acted-in are available
almost exhaustively for all descendant instances
of ‘American Actors’, and are available for only a
few of the descendant instances of ‘Belgian Ac-
tors’, ‘Italian Actors’ etc., then the distribution
over the hierarchy may incorrectly suggest that
the left argument of acted-in is ‘American Actors’
rather than the more general ‘Actors’. In another
example, if virtually all instances that instantiate
the left argument of the annotation won-award are
mapped to the concept ‘Award Winning Actors’,
then it would be difficult to distinguish ‘Award
Winning Actors’ from the more general ‘Actors’
or ‘People’, as best concept to be computed for
the annotation. Another type of noise is missing
or erroneous edges in the hierarchy, which could
artificially direct propagation towards irrelevant
regions of the hierarchy, or prevent propagation
from even reaching relevant regions of the hier-
archy. For example, if the hierarchy incorrectly
maps ‘Actors’ to ‘Entertainment’, then ‘Entertain-
ment’ and its ancestor concepts incorrectly be-
come candidate concepts during propagation for
the left argument of acted-in. Conversely, if miss-
ing edges caused ‘Actors’ to not have any children
in the hierarchy, then ‘Actors’ would not even be
reached and considered as a candidate concept
during propagation.
Second, to apply evidence collected from some
annotations to a new annotation, the evidence
must generalize across annotations. However,
collected evidence or statistics may vary widely
across annotations. Observing that 90% of all de-
scendant instances of the concept ‘Actors’ match
an annotation acted-in constitutes strong evidence
that ‘Actors’ is a good concept for acted-in. In
contrast, observing that only 0.09% of all descen-
dant instances of the concept ‘Football Teams’
match won-super-bowl should not be as strong
negative evidence as the percentage suggests.
</bodyText>
<subsectionHeader confidence="0.997286">
2.2 Inferring Concept-Level Annotations
</subsectionHeader>
<bodyText confidence="0.984744764705883">
Determining Candidate Concepts: As illus-
trated in the left part of Figure 2, the first step to-
wards inferring concept-level from instance-level
annotations is to propagate the instances that in-
stantiate a particular argument of the annota-
tion, upwards in the hierarchy. Starting from the
left arguments of the annotation acted-in, namely
leonardo dicaprio, milla jovovich etc., the prop-
agation reaches their parent concepts ‘American
Actors’, ‘English Actors’, then their parent and
ancestor concepts ‘Actors’, ‘People’, ‘Entities’
etc. The concepts reached during upward prop-
agation become candidate concepts. In subse-
quent steps, the candidates are modeled, scored
and ranked such that ideally the best concept is
ranked at the top.
Ranking Candidate Concepts: The identifica-
</bodyText>
<page confidence="0.989954">
505
</page>
<bodyText confidence="0.99987615942029">
tion of a ranking function is cast as a semi-
supervised learning problem. Given the cor-
rect (gold) concept of an annotation, it would be
tempting to employ binary classification directly,
by marking the correct concept as a positive ex-
ample, and all other candidate concepts as nega-
tive examples. Unfortunately, this would produce
a highly imbalanced training set, with thousands
of negative examples and, more importantly, with
only one positive example. Another disadvan-
tage of using binary classification directly is that
it is difficult to capture the preference for concepts
closer in the hierarchy to the correct concept, over
concepts many edges away. Finally, the absolute
values of the features that might be employed may
be comparable within an annotation, but incompa-
rable across annotations, which reduces the porta-
bility of the resulting model to new annotations.
To address the above issues, the ranking func-
tion proposed does not construct training exam-
ples from raw features collected for each indi-
vidual candidate concept. Instead, it constructs
training examples from pairwise comparisons of
a candidate concept with another candidate con-
cept. Concretely, a pairwise comparison is la-
beled as a positive example if the first concept is
closer to the correct concept than the second, or as
negative otherwise. The pairwise formulation has
three immediate advantages. First, it accomodates
the preference for concepts closer to the gold con-
cept. Second, the pairwise formulation produces
a larger, more balanced training set. Third, deci-
sions of whether the first concept being compared
is more relevant than the second are more likely to
generalize across annotations, than absolute deci-
sions of whether (and how much) a particular con-
cept is relevant for a given annotation.
Compiling Ranking Features: The features are
grouped into four categories: (A) annotation co-
occurrence features, (B) concept features, (C) ar-
gument co-occurrence features, and (D) combina-
tion features, as described below.
(A) Annotation Co-occurrence Features: The
annotation co-occurrence features emphasize how
well an annotation applies to a concept. These
features include (1) MATCHED INSTANCES the
number of descendant instances of the concept
that appear with the annotation, (2) INSTANCE
PERCENT the percentage of matched instances in
the concept, (3) MORE THAN THREE MATCHING
INSTANCES and (4) MORE THAN TEN MATCH-
ING INSTANCES, which indicate when the match-
ing descendant instances might be noise.
Also in this category are features that relay in-
formation about the candidate concept’s children
concepts. These features include (1) MATCHED
CHILDREN the number of child concepts con-
taining at least one matching instance, (2) CHIL-
DREN PERCENT the percentage of child concepts
with at least one matching instance, (3) AVG IN-
STANCE PERCENT CHILDREN the average per-
centage of matching descendant instances of the
child concepts, and (4) INSTANCE PERCENT TO
INSTANCE PERCENT CHILDREN the ratio be-
tween INSTANCE PERCENT and AVERAGE IN-
STANCE PERCENT OF CHILDREN. The last fea-
ture is meant to capture dramatic changes in per-
centages when moving in the hierarchy from child
concepts to the candidate concept in question.
</bodyText>
<listItem confidence="0.6791858">
(B) Concept Features: Concept features ap-
proximate the generality of the concepts: (1)
NUM INSTANCES the number of descendant in-
stances of the concept, (2) NUM CHILDREN the
number of child concepts, and (3) DEPTH the dis-
tance to the concept’s farthest descendant.
(C) Argument Co-occurrence Features: The ar-
gument co-occurrence features model the likeli-
hood that an annotation applies to a concept by
looking at co-occurrences with another argument
of the same annotation. Intuitively, if a con-
cept representing one argument has a high co-
occurrence with an instance that is some other ar-
gument, a relationship more likely exists between
members of the concept and the instance. For ex-
ample, given acted-in, ‘Actors’ is likely to have a
higher co-occurrence with casablanca than ‘Peo-
ple’ is. These features are generated from a set of
Web queries. Therefore, the collected values are
likely to be affected by different noise than that
present in the original dataset. For every concept
and instance pair from the arguments of a given
annotation, they feature the number of times each
of the tokens in the concept appears in the same
query with each of the tokens in the instance,
normalizing to the respective number of tokens.
The procedure generates, for each candidate con-
cept, an average co-occurrence score (AVG CO-
OCCURRENCE) and a total co-occurrence score
(TOTAL CO-OCCURRENCE) over all instances the
concept is paired with.
(D) Combination Features: The last group
of features are combinations of the above fea-
tures: (1) DEPTH, INSTANCE PERCENT which is
DEPTH multiplied by INSTANCE PERCENT, and
</listItem>
<page confidence="0.969211">
506
</page>
<table confidence="0.9999722">
Concept Distance Match Total Match Total AvgInst Depth Avg Total
ToCorrect Inst Inst Child Child PercOfChild Cooccur Cooccur
People 4 36512 879423 22 29 4% 14 0.67 33506
Actors 0 29101 54420 6 10 32% 6 2.08 99971
English Actors 2 3091 5922 3 4 37% 3 2.75 28378
Labeled Concept Pair Annotation Co-occurrence Concept Arg Co-occurrence Combination
Features Features Features Features
Concept Label Match Inst Match Child AvgInst Num Num Depth Avg Total Depth DepthInst
Pair Inst Perc Child Perc PercChild Inst Child Cooccur Cooccur InstPerc PercChild
People-Actors 0 1.25 0.08 3.67 1.26 0.13 1.25 3.67 2.33 0.32 0.34 0.18 0.66
Actors-People 1 0.8 12.88 0.27 0.79 7.65 0.8 0.27 0.43 3.11 2.98 5.52 1.51
Actors-English Actors 1 9.41 1.02 2.0 0.8 0.87 9.41 2.0 2.0 0.76 3.52 2.05 4.1
English Actors-Actors 0 0.11 0.98 0.5 1.25 1.15 0.11 0.5 0.5 1.32 0.28 0.49 0.24
English Actors-People 1 0.08 12.57 0.14 0.99 8.82 0.08 0.14 0.21 4.12 0.85 2.69 0.37
People-English Actors 0 11.81 0.08 7.33 1.01 0.11 11.81 7.33 4.67 0.24 1.18 0.37 2.72
</table>
<tableCaption confidence="0.867737333333333">
Table 1: Training/Testing Examples: The top table shows examples of raw statistics gathered for three candidate
concepts for the left argument of the annotation acted-in. The second table shows the training/testing examples
generated from these concepts and statistics. Each example represents a pair of concepts which is labeled positive
</tableCaption>
<construct confidence="0.336961333333333">
if the first concept is closer to the correct concept than the second concept. Features shown here are the ratio
between a statistic for the first concept and a statistic for the second (e.g. DEPTH for Actors-English Actors is 2
as ‘Actors’ has depth of 6 and ‘English Actors’ has depth of 3). Some features omitted due to space constraints.
</construct>
<bodyText confidence="0.990443076923077">
(2) DEPTH, INSTANCE PERCENT, CHILDREN,
which is the DEPTH multipled by the INSTANCE
PERCENT multiplied by MATCHED CHILDREN.
Both these features seek to balance the perceived
relevance of an annotation to a candidate concept,
with the generality of the candidate concept.
Generating Learning Examples: For a given
annotation, the ranking features described so far
are computed for each candidate concept (e.g.,
‘Movie Actors’, ‘Models’, ‘Actors’). However,
the actual training and testing examples are gener-
ated for pairs of candidate concepts (e.g., &lt;‘Film
Actors’, ‘Models’&gt;, &lt;‘Film Actors’, ‘Actors’&gt;,
&lt;‘Models, ‘Actors’&gt;). A training example rep-
resents a comparison between two candidate con-
cepts, and specifies which of the two is more rele-
vant. To create training and testing examples, the
values of the features of the first concept in the
pair are respectively combined with the values of
the features of the second concept in the pair to
produce values corresponding to the entire pair.
Following classification of testing examples,
concepts are ranked according to the number of
other concepts which they are classified as more
relevant than. Table 1 shows examples of train-
ing/testing data.
</bodyText>
<sectionHeader confidence="0.997766" genericHeader="method">
3 Experimental Setting
</sectionHeader>
<subsectionHeader confidence="0.988657">
3.1 Data Sources
</subsectionHeader>
<bodyText confidence="0.99987196969697">
Conceptual Hierarchy: The experiments com-
pute concept-level annotations relative to a con-
ceptual hierarchy derived automatically from the
Wikipedia (Remy, 2002) category network, as de-
scribed in (Ponzetto and Navigli, 2009). The hi-
erarchy filters out edges (e.g., from ‘British Film
Actors’ to ‘Cinema of the United Kingdom’) from
the Wikipedia category network that do not corre-
spond to IsA relations. A concept in the hierarchy
is a Wikipedia category (e.g., ‘English Film Ac-
tors’) that has zero or more Wikipedia categories
as child concepts, and zero or more Wikipedia
categories (e.g., ‘English People by Occupation’,
‘British Film Actors’) as parent concepts. Each
concept in the hierarchy has zero or more in-
stances, which are the Wikipedia articles listed (in
Wikipedia) under the respective categories (e.g.,
colin firth is an instance of ‘English Actors’).
Instance-Level Annotations: The experiments
exploit a set of binary instance-level annotations
(e.g., acted-in, composed) among Wikipedia in-
stances, as available in Freebase (Bollacker et
al., 2008). The annotation is a Freebase prop-
erty (e.g., /music/composition/composer). Inter-
nally, the left and right arguments are Freebase
topic identifiers mapped to their corresponding
Wikipedia articles (e.g., /m/03f4k mapped to the
Wikipedia article on george gershwin). In this pa-
per, the derived annotations and instances are dis-
played in a shorter, more readable form for con-
ciseness and clarity. As features do not use the
label of the annotation, labels are never used in
the experiments and evaluation.
</bodyText>
<page confidence="0.983786">
507
</page>
<bodyText confidence="0.993466">
Web Search Queries: The argument co-
occurrence features described above are com-
puted over a set of around 100 million
anonymized Web search queries from 2010.
</bodyText>
<subsectionHeader confidence="0.996397">
3.2 Experimental Runs
</subsectionHeader>
<bodyText confidence="0.999696">
The experimental runs exploit ranking features
described in the previous section, employing:
</bodyText>
<listItem confidence="0.999001666666667">
• one of three learning algorithms: naive Bayes
(NAIVEBAYES), maximum entropy (MAXENT),
or perceptron (PERCEPTRON) (Mitchell, 1997),
chosen for their scalability to larger datasets via
distributed implementations.
• one of three ways of combining the values
</listItem>
<bodyText confidence="0.982117230769231">
of features collected for individual candidate con-
cepts into values of features for pairs of candidate
concepts: the raw ratio of the values of the re-
spective features of the two concepts (0 when the
denominator is 0); the ratio scaled to the interval
[0, 1]; or a binary value indicating which of the
values is larger.
For completeness, the experiments include
three additional, baseline runs. Each baseline
computes scores for all candidate concepts based
on the respective metric; then candidate concepts
are ranked in decreasing order of their scores. The
baselines metrics are:
</bodyText>
<listItem confidence="0.940542636363636">
• INSTPERCENT ranks candidate concepts by
the percentage of matched instances that are de-
scendants of the concept. It emphasizes concepts
which are “proven” to belong to the annotation;
• ENTROPY ranks candidate concepts by the
entropy (Shannon, 1948) of the proportion of
matched descendant instances of the concept;
• AVGDEPTH ranks candidate concepts by
their distances to half of the maximum hierarchy
height, emphasizing a balance of generality and
specificity.
</listItem>
<subsectionHeader confidence="0.989202">
3.3 Evaluation Procedure
</subsectionHeader>
<bodyText confidence="0.993006796296296">
Gold Standard of Concept-Level Annotations:
A random, weighted sample of 200 annotation la-
bels (e.g., corresponding to composed-by, play-
instrument) is selected, out of the set of labels
of all instance-level annotations collected from
Freebase. During sampling, the weights are the
counts of distinct instance-level annotations (e.g.,
&lt;rhapsody in blue, george gershwin&gt;) avail-
able for the label. The arguments of the anno-
tation labels are then manually annotated with
a gold concept, which is the category from the
Wikipedia hierarchy that best captures their se-
mantics. The manual annotation is carried out
independently by two human judges, who then
verify each other’s work and discard inconsisten-
cies. For example, the gold concept of the left
argument of composed-by is annotated to be the
Wikipedia category ‘Musical Compositions’. In
the process, some annotation labels are discarded,
when (a) it is not clear what concept captures an
argument (e.g., for the right argument offunction-
of-building), or (b) more than 5000 candidate con-
cepts are available via propagation for one of the
arguments, which would cause too many train-
ing or testing examples to be generated via con-
cept pairs, and slow down the experiments. The
retained 139 annotation labels, whose arguments
have been labeled with their respective gold con-
cepts, form the gold standard for the experiments.
More precisely, an entry in the resulting gold stan-
dard consists of an annotation label, one of its
arguments being considered (left or right), and
a gold concept that best captures that argument.
The set of annotation labels from the gold stan-
dard is quite diverse and covers many domains of
potential interest, e.g., has-company(‘Industries’,
‘Companies’), written-by(‘Films’, ‘Screenwrit-
ers’), member-of(‘Politicians’,‘Political Parties’),
or part-of-movement(‘Artists’, ‘Art Movements’).
Evaluation Metric: Following previous work
on selectional preferences (Kozareva and Hovy,
2010; Ritter et al., 2010), each entry in the gold
standard, (i.e., each argument for a given annota-
tion) is evaluated separately. Experimental runs
compute a ranked list of candidate concepts for
each entry in the gold standard. In theory, a com-
puted candidate concept is better if it is closer
semantically to the gold concept. In practice,
the accuracy of a ranked list of candidate con-
cepts, relative to the gold concept of the anno-
tation label, is measured by two scoring metrics
that correspond to the mean reciprocal rank score
(MRR) (Voorhees and Tice, 2000) and a modifi-
cation of it (DRR) (Pas¸ca and Alfonseca, 2009):
</bodyText>
<equation confidence="0.9978015">
MRR = N N
i=1
</equation>
<bodyText confidence="0.999892">
N is the number of annotations and ranki is the
rank of the gold concept in the returned list for
MRR. An annotation ai receives no credit for
MRR if the gold concept does not appear in the
corresponding ranked list.
</bodyText>
<equation confidence="0.994599666666667">
DRR = �
1 N
i=1
</equation>
<bodyText confidence="0.998259">
For DRR, ranki is the rank of a candidate con-
cept in the returned list and Len is the length of
</bodyText>
<figure confidence="0.946077">
1
max
rank
ranki
1
ranki x (1 + Len)
max
rank
</figure>
<page confidence="0.9872">
508
</page>
<table confidence="0.99985184375">
Annotation (Number of Candidate Concepts) Examples of Instances Top Ranked Concepts
Composers compose Musical Compositions (3038) aaron copland; black sabbath Music by Nationality; Composers; Classical
Composers
Musical Compositions composed-by Composers (1734) we are the champions; yor- Musical Compositions; Compositions by
ckscher marsch Composer; Classical Music
Foods contain Nutrients (1112) acca sellowiana; lasagna Foods; Edible Plants; Food Ingredients
Organizations has-boardmember People (3401) conocophillips; spence school Companies by Stock Exchange; Companies
Listed on the NYSE; Companies
Educational Organizations has-graduate Alumni (4072) air force institute of technology; Education by Country; Schools by Country;
deering high school Universities and Colleges by Country
Television Actors guest-role Fictional Characters (4823) melanie griffith; patti laBelle Television Actors by Nationality; Actors;
American Actors
Musical Groups has-member Musicians (2287) steroid maximus; u2 Musical Groups; Musical Groups by Genre;
Musical Groups by Nationality
Record Labels represent Musician (920) columbia records; vandit Record Labels; Record Labels by Country;
Record Labels by Genre
Awards awarded-to People (458) academy award for best original Film Awards; Awards; Grammy Awards
song; erasmus prize
Foods contain Nutrients (177) lycopene; glutamic acid Carboxylic Acids; Acids; Essential Nutrients
Architects design Buildings and Structures (4811) 20 times square; berkeley build- Buildings and Structures; Buildings and Struc-
ing tures by Architect; Houses by Country
People died-from Causes of Death (577) malaria; skiing Diseases; Infectious Diseases; Causes of
Death
Art Directors direct Films (1265) batman begins; the lion king Films; Films by Director; Film
Episodes guest-star Television Actors (1067) amy poehler; david caruso Television Actors by Nationality; Actors;
American Actors
Television Network has-tv-show Television Series (2492) george of the jungle; great expec- Television Series by Network; Television Se-
tations ries; Television Series by Genre
Musicians play Musical Instruments (423) accordion; tubular bell Musical Instruments; Musical Instruments by
Nationality; Percussion Instruments
Politicians member-of Political Parties (938) independent moralizing front; Political Parties; Political Parties by Country;
national coalition party Political Parties by Ideology
</table>
<tableCaption confidence="0.9391725">
Table 2: Concepts Computed for Gold-Standard Annotations: Examples of entries from the gold standard and
counts of candidate concepts (Wikipedia categories) reached from upward propagation of instances (Wikipedia
instances). The target gold concept is shown in bold. Also shown are examples of Wikipedia instances, and the
top concepts computed by the best-performing learning algorithm for the respective gold concepts.
</tableCaption>
<bodyText confidence="0.99898309375">
the minimum path in the hierarchy between the
concept and the gold concept. Len is minimum
(0) if the candidate concept is the same as the gold
standard concept. A given annotation ai receives
no credit for DRR if no path is found between the
returned concepts and the gold concept.
As an illustration, for a single annotation, the
right argument of composed-by, the ranked list
of concepts returned by an experimental may
be [‘Symphonies by Anton Bruckner’, ‘Sym-
phonies by Joseph Haydn’, ‘Symphonies by Gus-
tav Mahler’, ‘Musical Compositions’, ..], with the
gold concept being ‘Musical Compositions’. The
length of the path between ‘Symphonies by An-
ton Bruckner’ etc. and ‘Musical Compositions’ is
2 (via ‘Symphonies’). Therefore, the MRR score
would be 0.25 (given by the fourth element of
the ranked list), whereas the DRR score would be
0.33 (given by the first element of the ranked list).
MRR and DRR are computed in five-fold cross
validation. Concretely, the gold standard is split
into five folds such that the sets of annotation la-
bels in each fold are disjoint. Thus, none of
the annotation labels in testing appears in train-
ing. This restriction makes the evaluation more
rigurous and conservative as it actually assesses
the extent the models learned are applicable to
new, previously unseen annotation labels. If
this restriction were relaxed, the baselines would
preform equivalently as they do not depend on
the training data, but the learned methods would
likely do better.
</bodyText>
<sectionHeader confidence="0.961652" genericHeader="method">
4 Evaluation Results
</sectionHeader>
<subsectionHeader confidence="0.998317">
4.1 Quantitative Results
</subsectionHeader>
<bodyText confidence="0.999740090909091">
Conceptual Hierarchy: The conceptual hierar-
chy contains 108,810 Wikipedia categories, and
its maximum depth, measured as the distance
from a concept to its farthest descendant, is 16.
Candidate Concepts: On average, for the gold
standard, the method propagates a given annota-
tion from instances to 1,525 candidate concepts,
from which the single best concept must be deter-
mined. The left part of Table 2 illustrates the num-
ber of candidate concepts reached during propa-
gation for a sample of annotations.
</bodyText>
<page confidence="0.994244">
509
</page>
<table confidence="0.999867">
Experimental Run Accuracy
N=1 N=20
MRR DRR MRR DRR
� With raw-ratio features:
NAIVEBAYES 0.021 0.180 0.054 0.222
MAXENT 0.029 0.168 0.045 0.208
PERCEPTRON 0.029 0.176 0.045 0.216
� With scaled-ratio features:
NAIVEBAYES 0.050 0.170 0.112 0.243
MAXENT 0.245 0.456 0.430 0.513
PERCEPTRON 0.245 0.391 0.367 0.461
� With binary features:
NAIVEBAYES 0.115 0.297 0.224 0.361
MAXENT 0.165 0.390 0.293 0.441
PERCEPTRON 0.180 0.332 0.330 0.429
� For baselines:
INSTPERCENT 0.029 0.173 0.045 0.224
ENTROPY 0.000 0.110 0.007 0.136
AVGDEPTH 0.007 0.018 0.028 0.045
</table>
<tableCaption confidence="0.999829">
Table 3: Precision Results: Accuracy of ranked lists
</tableCaption>
<bodyText confidence="0.73224475">
of concepts (Wikipedia categories) computed by var-
ious runs, as an average over the gold standard of
concept-level annotations, considering the top N can-
didate concepts computed for each gold standard entry.
</bodyText>
<subsectionHeader confidence="0.998057">
4.2 Qualitative Results
</subsectionHeader>
<bodyText confidence="0.999978036363636">
Precision: Table 3 compares the precision of the
ranked lists of candidate concepts produced by the
experimental runs. The MRR and DRR scores in
the table consider either at most 20 of the concepts
in the ranked list computed by a given experimen-
tal run, or only the first, top ranked computed con-
cept. Note that, in the latter case, the MRR and
DRR scores are equivalent to precision@1 scores.
Several conclusions can be drawn from the re-
sults. First, as expected by definition of the
scoring metrics, DRR scores are higher than the
stricter MRR scores, as they give partial credit
to concepts that, while not identical to the gold
concepts, are still close approximations. This is
particularly noticeable for the runs MAXENT and
PERCEPTRON with raw-ratio features (4.6 and
4.8 times higher respectively). Second, among
the baselines, INSTPERCENT is the most accu-
rate, with the computed concepts identifying the
gold concept strictly at rank 22 on average (for
an MRR score 0.045), and loosely at an aver-
age of 4 steps away from the gold concept (for
a DRR score of 0.224). Third, the accuracy of
the learning algorithms varies with how the pair-
wise feature values are combined. Overall, raw-
ratio feature values perform the worst, and scaled-
ratio the best, with binary in-between. Fourth,
the scores of the best experimental run, MAXENT
with scaled-ratio features, are 0.430 (MRR) and
0.513 (DRR) over the top 20 computed concepts,
and 0.245 (MRR) and 0.456 (DRR) when consid-
ering only the first concept. These scores corre-
spond to the ranked list being less than one step
away in the hierarchy. The very first computed
concept exactly matches the gold concept in about
one in four cases, and is slightly more than one
step away from it. In comparison, the very first
concept computed by the best baseline matches
the gold concept in about one in 35 cases (0.029
MRR), and is about 6 steps away (0.173 DRR).
The accuracies of the various learning algorithms
(not shown) were also measured and correlated
roughly with the MRR and DRR scores.
Discussion: The baseline runs INSTPERCENT
and ENTROPY produce categories that are far
too specific. For the gold annotation composed-
by(‘Composers’, ‘Musical Compositions’), INST-
PERCENT produces ‘Scottish Flautists’ for the left
argument and ‘Operas by Ernest Reyer’ for the
right. AVGDEPTH does not suffer from over-
specification, but often produces concepts that
have been reached via propagation, yet are not
close to the gold concept. For composed-by,
AVGDEPTH produces ‘Film’ for the left argument
and ‘History by Region’ for the right.
</bodyText>
<subsectionHeader confidence="0.969189">
4.3 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999973416666667">
The right part of Table 2 provides a more de-
tailed view into the best performing experimental
run, showing actual ranked lists of concepts pro-
duced for a sample of the gold standard entries
by MAXENT with scaled-ratio. A separate analy-
sis of the results indicates that the most common
cause of errors is noise in the conceptual hier-
archy, in the form of unbalanced instance-level
annotations and missing hierarchy edges. Un-
balanced annotations are annotations where cer-
tain subtrees of the hierarchy are artificially more
populated than other subtrees. For the left argu-
ment of the annotation has-profession, 0.05% of
‘New York Politicians’ are matched but 70% of
‘Bushrangers’ are matched. Such imbalances may
be inherent to how annotations are added to Free-
base: different human contributors may add new
annotations to particular portions of Freebase, but
miss other relevant portions.
The results are also affected by missing edges
in the hierarchy. Of the more than 100K con-
cepts in the hierarchy, 3479 are roots of subhier-
archies that are mutually disconnected. Exam-
ples are ‘People by Region’, ‘Shades of Red’, and
</bodyText>
<page confidence="0.980411">
510
</page>
<bodyText confidence="0.999947666666667">
‘Members of the Parliament of Northern Ireland’,
all of which should have parents in the hierarchy.
If a few edges are missing in a particular region
of the hierarchy, the method can recover, but if so
many edges are missing that a gold concept has
very few descendants, then propagation can be
substantially affected. In the worst case, the gold
concept becomes disconnected, and thus will be
missing from the set of candidate concepts com-
piled during propagation. For example, for the
annotation team-color(‘Sports Clubs’, ‘Colors’),
the only descendant concept of ‘Colors’ in the hi-
erarchy is ‘Horse Coat Colors’, meaning that the
gold concept ‘Colors’ is not reached during prop-
agation from instances upwards in the hierarchy.
</bodyText>
<sectionHeader confidence="0.999968" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999989283018868">
Similar to the task of attaching a semantic anno-
tation to the concept in a hierarchy that has the
best level of generality is the task of finding se-
lectional preferences for relations. Most relevant
to this paper is work that seeks to find the appro-
priate concept in a hierarchy for an argument of
a specific relation (Ribas, 1995; McCarthy, 1997;
Li and Abe, 1998). Li and Abe (1998) address
this problem by attempting to identify the best tree
cut in a hierarchy for an argument of a given verb.
They use the minimum description length princi-
ple to select a set of concepts from a hierarchy to
represent the selectional preferences. This work
makes several limiting assumptions including that
the hierarchy is a tree, and every instance belongs
to just one concept. Clark and Weir (2002) inves-
tigate the task of generalizing a single relation-
concept pair. A relation is propagated up a hier-
archy until a chi-square test determines the differ-
ence between the probability of the child and par-
ent concepts to be significant where the probabili-
ties are relation-concept frequencies. This method
has no direct translation to the task discussed here;
it is unclear how to choose the correct concept if
instances generalize to different concepts.
In other research on selectional preferences,
Pantel et al. (2007), Kozareva and Hovy (2010)
and Ritter et al. (2010) focus on generating ad-
missible arguments for relations, and Erk (2007)
and Bergsma et al. (2008) investigate classifying
a relation-instance pair as plausible or not.
Important to this paper is the Wikipedia cate-
gory network (Remy, 2002) and work on refin-
ing it. Ponzetto and Navigli (2009) disambiguate
Wikipedia categories by using WordNet synsets
and use this semantic information to construct a
taxonomy. The resulting taxonomy is the concep-
tual hierarchy used in the evaluation.
Another related area of work is the discovery of
relations between concepts. Nastase and Strube
(2008) use Wikipedia category names and cate-
gory structure to generate a set of relations be-
tween concepts. Yan et al. (2009) discover re-
lations between Wikipedia concepts via deep lin-
guistic information and Web frequency informa-
tion. Mohamed et al. (2011) generate candi-
date relations by coclustering text contexts for ev-
ery pair of concepts in a hierarchy. In a sense,
this area of research is complementary to that dis-
cussed in this paper. These methods induce new
relations, and the proposed method can be used
to find appropriate levels of generalization for the
arguments of any given relation.
</bodyText>
<sectionHeader confidence="0.999747" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999986424242424">
This paper introduces a method to convert flat sets
of instance-level annotations to hierarchically or-
ganized, concept-level annotations. The method
determines the appropriate concept for a given se-
mantic annotation in three stages. First, it propa-
gates annotations upwards in the hierarchy, form-
ing a set of candidate concepts. Second, it classi-
fies each candidate concept as more or less appro-
priate than each other candidate concept within an
annotation. Third, it ranks candidate concepts by
the number of other concepts relative to which it
is classified as more appropriate. Because the fea-
tures are comparisons between concepts within a
single semantic annotation, rather than consider-
ations of individual concepts, the method is able
to generalize across annotations, and can thus be
applied to new, previously unseen annotations.
Experiments demonstrate that, on average, the
method is able to identify the concept of a given
annotation’s argument within one hierarchy edge
of the gold concept.
The proposed method can take advantage of
existing work on open-domain information ex-
traction. The output of such work is usually
instance-level annotations, although often at sur-
face level (non-disambiguated arguments) rather
than semantic level (disambiguated arguments).
After argument disambiguation (e.g., (Dredze et
al., 2010)), the annotations can be used as input
to determining concept-level annotations. Thus,
the method has the potential to generalize any
existing database of instance-level annotations to
concept-level annotations.
</bodyText>
<page confidence="0.996066">
511
</page>
<sectionHeader confidence="0.995873" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999943027027027">
Michele Banko, Michael Cafarella, Stephen Soder-
land, Matt Broadhead, and Oren Etzioni. 2007.
Open information extraction from the Web. In Pro-
ceedings of the 20th International Joint Conference
on Artificial Intelligence (IJCAI-07), pages 2670–
2676, Hyderabad, India.
Cory Barr, Rosie Jones, and Moira Regelson. 2008.
The linguistic structure of English Web-search
queries. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP-08), pages 1021–1030, Honolulu,
Hawaii.
Shane Bergsma, Dekang Lin, and Randy Goebel.
2008. Discriminative learning of selectional pref-
erence from unlabeled text. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing (EMNLP-08), pages 59–68,
Honolulu, Hawaii.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A
collaboratively created graph database for struc-
turing human knowledge. In Proceedings of the
2008 International Conference on Management of
Data (SIGMOD-08), pages 1247–1250, Vancouver,
Canada.
Stephen Clark and David Weir. 2002. Class-based
probability estimation using a semantic hierarchy.
Computational Linguistics, 28(2):187–206.
Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-
ber, and Tim Finin. 2010. Entity disambiguation
for knowledge base population. In Proceedings
of the 23rd International Conference on Compu-
tational Linguistics (COLING-10), pages 277–285,
Beijing, China.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In Proceedings of the
45th Annual Meeting of the Association for Com-
putational Linguistics (ACL-07), pages 216–223,
Prague, Czech Republic.
Zornitsa Kozareva and Eduard Hovy. 2010. Learning
arguments and supertypes of semantic relations us-
ing recursive patterns. In Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-10), pages 1482–1491, Up-
psala, Sweden.
Hang Li and Naoki Abe. 1998. Generalizing case
frames using a thesaurus and the mdl principle. In
Proceedings of the ECAI-2000 Workshop on Ontol-
ogy Learning, pages 217–244, Berlin, Germany.
Xiao Li. 2010. Understanding the semantic struc-
ture of noun phrase queries. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics (ACL-10), pages 1337–1345,
Uppsala, Sweden.
Diana McCarthy. 1997. Word sense disambiguation
for acquisition of selectional preferences. In Pro-
ceedings of the ACL/EACL Workshop on Automatic
Information Extraction and Building of Lexical Se-
mantic Resources for NLP Applications, pages 52–
60, Madrid, Spain.
Tom Mitchell. 1997. Machine Learing. McGraw Hill.
Thahir Mohamed, Estevam Hruschka, and Tom
Mitchell. 2011. Discovering relations between
noun categories. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-11), pages 1447–1455, Edin-
burgh, United Kingdom.
Vivi Nastase and Michael Strube. 2008. Decoding
Wikipedia categories for knowledge acquisition. In
Proceedings of the 23rd National Conference on
Artificial Intelligence (AAAI-08), pages 1219–1224,
Chicago, Illinois.
M. Pas¸ca and E. Alfonseca. 2009. Web-derived
resources for Web Information Retrieval: From
conceptual hierarchies to attribute hierarchies. In
Proceedings of the 32nd International Conference
on Research and Development in Information Re-
trieval (SIGIR-09), pages 596–603, Boston, Mas-
sachusetts.
Patrick Pantel, Rahul Bhagat, Timothy Chklovski, and
Eduard Hovy. 2007. ISP: Learning inferential se-
lectional preferences. In Proceedings of the Annual
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL-07),
pages 564–571, Rochester, New York.
Simone Paolo Ponzetto and Roberto Navigli. 2009.
Large-scale taxonomy mapping for restructuring
and integrating Wikipedia. In Proceedings of
the 21st International Joint Conference on Ar-
tifical Intelligence (IJCAI-09), pages 2083–2088,
Barcelona, Spain.
Melanie Remy. 2002. Wikipedia: The free encyclope-
dia. Online Information Review, 26(6):434.
Francesc Ribas. 1995. On learning more appropriate
selectional restrictions. In Proceedings of the 7th
Conference of the European Chapter of the Asso-
ciation for Computational Linguistics (EACL-97),
pages 112–118, Madrid, Spain.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A la-
tent dirichlet allocation method for selectional pref-
erences. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL-10), pages 424–434, Uppsala, Sweden.
Claude Shannon. 1948. A mathematical theory of
communication. Bell System Technical Journal,
27:379–423,623–656.
Ellen Voorhees and Dawn Tice. 2000. Building a
question-answering test collection. In Proceedings
of the 23rd International Conference on Research
and Development in Information Retrieval (SIGIR-
00), pages 200–207, Athens, Greece.
</reference>
<page confidence="0.96902">
512
</page>
<reference confidence="0.998637538461538">
Fei Wu and Daniel S. Weld. 2010. Open information
extraction using wikipedia. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics (ACL-10), pages 118–127, Up-
psala, Sweden.
Yulan Yan, Naoaki Okazaki, Yutaka Matsuo, Zhenglu
Yang, and Mitsuru Ishizuka. 2009. Unsupervised
relation extraction by mining Wikipedia texts using
information from the Web. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP (ACL-
IJCNLP-09), pages 1021–1029, Suntec, Singapore.
</reference>
<page confidence="0.998855">
513
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.170295">
<title confidence="0.82692">Instance-Driven Attachment of Semantic</title>
<author confidence="0.401821">over Conceptual Hierarchies</author>
<affiliation confidence="0.991696">University of</affiliation>
<address confidence="0.963676">Seattle, Washington</address>
<email confidence="0.999783">janara@cs.washington.edu</email>
<affiliation confidence="0.48797">Google</affiliation>
<address confidence="0.760968">Mountain View, California</address>
<email confidence="0.99973">mars@google.com</email>
<abstract confidence="0.997151192307692">Whether automatically extracted or human generated, open-domain factual knowledge is often available in the form of semantic (e.g., that take or more specific instances (e.g., rhapin as their arguments. This paper introduces a method for converting flat sets of instance-level annotations into hierarchically organized, concept-level annotations, which capture not only the broad semantics of the desired arguments (e.g., ‘People’ rather than ‘Locations’), but also the correct level of generality (e.g., ‘Composers’ rather than ‘People’, or ‘Jazz Composers’). The method refrains from encoding features specific to a particular domain or annotation, to ensure immediate applicability to new, previously unseen annotations. Over a gold standard of semantic annotations and concepts that best capture their arguments, the method substantially outperforms three baselines, on average, computing concepts that are less than one step in the hierarchy away from the corresponding gold standard concepts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael Cafarella</author>
<author>Stephen Soderland</author>
<author>Matt Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Open information extraction from the Web.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI-07),</booktitle>
<pages>2670--2676</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="1772" citStr="Banko et al., 2007" startWordPosition="242" endWordPosition="245">ly outperforms three baselines, on average, computing concepts that are less than one step in the hierarchy away from the corresponding gold standard concepts. 1 Introduction Background: Knowledge about the world can be thought of as semantic assertions or annotations, at two levels of granularity: instance level (e.g., rhapsody in blue, tristan und isolde, george gershwin, richard wagner) and concept level (e.g., ‘Musical Compositions’, ‘Works of Art’, ‘Composers’). Instance-level annotations correspond to factual knowledge that can be found in repositories extracted automatically from text (Banko et al., 2007; Wu and Weld, 2010) *Contributions made during an internship at Google. or manually created within encyclopedic resources (Remy, 2002). Such facts could state, for instance, that rhapsody in blue was composedby george gershwin, or that tristan und isolde was composed-by richard wagner. In comparison, concept-level annotations more concisely and effectively capture the underlying semantics of the annotations by identifying the concepts corresponding to the arguments, e.g., ‘Musical Compositions’ are composed-by ‘Composers’. The frequent occurrence of instances, relative to more abstract concep</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael Cafarella, Stephen Soderland, Matt Broadhead, and Oren Etzioni. 2007. Open information extraction from the Web. In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI-07), pages 2670– 2676, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cory Barr</author>
<author>Rosie Jones</author>
<author>Moira Regelson</author>
</authors>
<title>The linguistic structure of English Web-search queries.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP-08),</booktitle>
<pages>1021--1030</pages>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="2442" citStr="Barr et al., 2008" startWordPosition="340" endWordPosition="343">ternship at Google. or manually created within encyclopedic resources (Remy, 2002). Such facts could state, for instance, that rhapsody in blue was composedby george gershwin, or that tristan und isolde was composed-by richard wagner. In comparison, concept-level annotations more concisely and effectively capture the underlying semantics of the annotations by identifying the concepts corresponding to the arguments, e.g., ‘Musical Compositions’ are composed-by ‘Composers’. The frequent occurrence of instances, relative to more abstract concepts, in Web documents and popular Web search queries (Barr et al., 2008; Li, 2010), is both an asset and a liability from the point of view of knowledge acquisition. On one hand, it makes instance-level annotations relatively easy to find, either from manually created resources (Remy, 2002; Bollacker et al., 2008), or extracted automatically from text (Banko et al., 2007). On the other hand, it makes conceptlevel annotations more difficult to acquire directly. While “Rhapsody in Blue was composed by George Gershwin [..]” may occur in some form within Web documents, the more abstract “Musical compositions are composed by musicians [..]” is unlikely to occur. A mor</context>
</contexts>
<marker>Barr, Jones, Regelson, 2008</marker>
<rawString>Cory Barr, Rosie Jones, and Moira Regelson. 2008. The linguistic structure of English Web-search queries. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP-08), pages 1021–1030, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Dekang Lin</author>
<author>Randy Goebel</author>
</authors>
<title>Discriminative learning of selectional preference from unlabeled text.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP-08),</booktitle>
<pages>59--68</pages>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="38208" citStr="Bergsma et al. (2008)" startWordPosition="5869" endWordPosition="5872"> relationconcept pair. A relation is propagated up a hierarchy until a chi-square test determines the difference between the probability of the child and parent concepts to be significant where the probabilities are relation-concept frequencies. This method has no direct translation to the task discussed here; it is unclear how to choose the correct concept if instances generalize to different concepts. In other research on selectional preferences, Pantel et al. (2007), Kozareva and Hovy (2010) and Ritter et al. (2010) focus on generating admissible arguments for relations, and Erk (2007) and Bergsma et al. (2008) investigate classifying a relation-instance pair as plausible or not. Important to this paper is the Wikipedia category network (Remy, 2002) and work on refining it. Ponzetto and Navigli (2009) disambiguate Wikipedia categories by using WordNet synsets and use this semantic information to construct a taxonomy. The resulting taxonomy is the conceptual hierarchy used in the evaluation. Another related area of work is the discovery of relations between concepts. Nastase and Strube (2008) use Wikipedia category names and category structure to generate a set of relations between concepts. Yan et a</context>
</contexts>
<marker>Bergsma, Lin, Goebel, 2008</marker>
<rawString>Shane Bergsma, Dekang Lin, and Randy Goebel. 2008. Discriminative learning of selectional preference from unlabeled text. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP-08), pages 59–68, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Tim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: A collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 International Conference on Management of Data (SIGMOD-08),</booktitle>
<pages>1247--1250</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="2686" citStr="Bollacker et al., 2008" startWordPosition="380" endWordPosition="383">comparison, concept-level annotations more concisely and effectively capture the underlying semantics of the annotations by identifying the concepts corresponding to the arguments, e.g., ‘Musical Compositions’ are composed-by ‘Composers’. The frequent occurrence of instances, relative to more abstract concepts, in Web documents and popular Web search queries (Barr et al., 2008; Li, 2010), is both an asset and a liability from the point of view of knowledge acquisition. On one hand, it makes instance-level annotations relatively easy to find, either from manually created resources (Remy, 2002; Bollacker et al., 2008), or extracted automatically from text (Banko et al., 2007). On the other hand, it makes conceptlevel annotations more difficult to acquire directly. While “Rhapsody in Blue was composed by George Gershwin [..]” may occur in some form within Web documents, the more abstract “Musical compositions are composed by musicians [..]” is unlikely to occur. A more practical approach to collecting concept-level annotations is to indirectly derive them from already plentiful instance-level annotations, effectively distilling factual knowledge into more abstract, concise and generalizable knowledge. Contr</context>
<context position="21468" citStr="Bollacker et al., 2008" startWordPosition="3249" endWordPosition="3252">ia category (e.g., ‘English Film Actors’) that has zero or more Wikipedia categories as child concepts, and zero or more Wikipedia categories (e.g., ‘English People by Occupation’, ‘British Film Actors’) as parent concepts. Each concept in the hierarchy has zero or more instances, which are the Wikipedia articles listed (in Wikipedia) under the respective categories (e.g., colin firth is an instance of ‘English Actors’). Instance-Level Annotations: The experiments exploit a set of binary instance-level annotations (e.g., acted-in, composed) among Wikipedia instances, as available in Freebase (Bollacker et al., 2008). The annotation is a Freebase property (e.g., /music/composition/composer). Internally, the left and right arguments are Freebase topic identifiers mapped to their corresponding Wikipedia articles (e.g., /m/03f4k mapped to the Wikipedia article on george gershwin). In this paper, the derived annotations and instances are displayed in a shorter, more readable form for conciseness and clarity. As features do not use the label of the annotation, labels are never used in the experiments and evaluation. 507 Web Search Queries: The argument cooccurrence features described above are computed over a </context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: A collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 International Conference on Management of Data (SIGMOD-08), pages 1247–1250, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>David Weir</author>
</authors>
<title>Class-based probability estimation using a semantic hierarchy.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>2</issue>
<contexts>
<context position="37541" citStr="Clark and Weir (2002)" startWordPosition="5761" endWordPosition="5764">s for relations. Most relevant to this paper is work that seeks to find the appropriate concept in a hierarchy for an argument of a specific relation (Ribas, 1995; McCarthy, 1997; Li and Abe, 1998). Li and Abe (1998) address this problem by attempting to identify the best tree cut in a hierarchy for an argument of a given verb. They use the minimum description length principle to select a set of concepts from a hierarchy to represent the selectional preferences. This work makes several limiting assumptions including that the hierarchy is a tree, and every instance belongs to just one concept. Clark and Weir (2002) investigate the task of generalizing a single relationconcept pair. A relation is propagated up a hierarchy until a chi-square test determines the difference between the probability of the child and parent concepts to be significant where the probabilities are relation-concept frequencies. This method has no direct translation to the task discussed here; it is unclear how to choose the correct concept if instances generalize to different concepts. In other research on selectional preferences, Pantel et al. (2007), Kozareva and Hovy (2010) and Ritter et al. (2010) focus on generating admissibl</context>
</contexts>
<marker>Clark, Weir, 2002</marker>
<rawString>Stephen Clark and David Weir. 2002. Class-based probability estimation using a semantic hierarchy. Computational Linguistics, 28(2):187–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Paul McNamee</author>
<author>Delip Rao</author>
<author>Adam Gerber</author>
<author>Tim Finin</author>
</authors>
<title>Entity disambiguation for knowledge base population.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING-10),</booktitle>
<pages>277--285</pages>
<location>Beijing, China.</location>
<marker>Dredze, McNamee, Rao, Gerber, Finin, 2010</marker>
<rawString>Mark Dredze, Paul McNamee, Delip Rao, Adam Gerber, and Tim Finin. 2010. Entity disambiguation for knowledge base population. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING-10), pages 277–285, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>A simple, similarity-based model for selectional preferences.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL-07),</booktitle>
<pages>216--223</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="38182" citStr="Erk (2007)" startWordPosition="5866" endWordPosition="5867">lizing a single relationconcept pair. A relation is propagated up a hierarchy until a chi-square test determines the difference between the probability of the child and parent concepts to be significant where the probabilities are relation-concept frequencies. This method has no direct translation to the task discussed here; it is unclear how to choose the correct concept if instances generalize to different concepts. In other research on selectional preferences, Pantel et al. (2007), Kozareva and Hovy (2010) and Ritter et al. (2010) focus on generating admissible arguments for relations, and Erk (2007) and Bergsma et al. (2008) investigate classifying a relation-instance pair as plausible or not. Important to this paper is the Wikipedia category network (Remy, 2002) and work on refining it. Ponzetto and Navigli (2009) disambiguate Wikipedia categories by using WordNet synsets and use this semantic information to construct a taxonomy. The resulting taxonomy is the conceptual hierarchy used in the evaluation. Another related area of work is the discovery of relations between concepts. Nastase and Strube (2008) use Wikipedia category names and category structure to generate a set of relations </context>
</contexts>
<marker>Erk, 2007</marker>
<rawString>Katrin Erk. 2007. A simple, similarity-based model for selectional preferences. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL-07), pages 216–223, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning arguments and supertypes of semantic relations using recursive patterns.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10),</booktitle>
<pages>1482--1491</pages>
<location>Uppsala,</location>
<contexts>
<context position="25545" citStr="Kozareva and Hovy, 2010" startWordPosition="3865" endWordPosition="3868">e gold standard for the experiments. More precisely, an entry in the resulting gold standard consists of an annotation label, one of its arguments being considered (left or right), and a gold concept that best captures that argument. The set of annotation labels from the gold standard is quite diverse and covers many domains of potential interest, e.g., has-company(‘Industries’, ‘Companies’), written-by(‘Films’, ‘Screenwriters’), member-of(‘Politicians’,‘Political Parties’), or part-of-movement(‘Artists’, ‘Art Movements’). Evaluation Metric: Following previous work on selectional preferences (Kozareva and Hovy, 2010; Ritter et al., 2010), each entry in the gold standard, (i.e., each argument for a given annotation) is evaluated separately. Experimental runs compute a ranked list of candidate concepts for each entry in the gold standard. In theory, a computed candidate concept is better if it is closer semantically to the gold concept. In practice, the accuracy of a ranked list of candidate concepts, relative to the gold concept of the annotation label, is measured by two scoring metrics that correspond to the mean reciprocal rank score (MRR) (Voorhees and Tice, 2000) and a modification of it (DRR) (Pas¸c</context>
<context position="38086" citStr="Kozareva and Hovy (2010)" startWordPosition="5848" endWordPosition="5851">s a tree, and every instance belongs to just one concept. Clark and Weir (2002) investigate the task of generalizing a single relationconcept pair. A relation is propagated up a hierarchy until a chi-square test determines the difference between the probability of the child and parent concepts to be significant where the probabilities are relation-concept frequencies. This method has no direct translation to the task discussed here; it is unclear how to choose the correct concept if instances generalize to different concepts. In other research on selectional preferences, Pantel et al. (2007), Kozareva and Hovy (2010) and Ritter et al. (2010) focus on generating admissible arguments for relations, and Erk (2007) and Bergsma et al. (2008) investigate classifying a relation-instance pair as plausible or not. Important to this paper is the Wikipedia category network (Remy, 2002) and work on refining it. Ponzetto and Navigli (2009) disambiguate Wikipedia categories by using WordNet synsets and use this semantic information to construct a taxonomy. The resulting taxonomy is the conceptual hierarchy used in the evaluation. Another related area of work is the discovery of relations between concepts. Nastase and S</context>
</contexts>
<marker>Kozareva, Hovy, 2010</marker>
<rawString>Zornitsa Kozareva and Eduard Hovy. 2010. Learning arguments and supertypes of semantic relations using recursive patterns. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10), pages 1482–1491, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
<author>Naoki Abe</author>
</authors>
<title>Generalizing case frames using a thesaurus and the mdl principle.</title>
<date>1998</date>
<booktitle>In Proceedings of the ECAI-2000 Workshop on Ontology Learning,</booktitle>
<pages>217--244</pages>
<location>Berlin, Germany.</location>
<contexts>
<context position="37117" citStr="Li and Abe, 1998" startWordPosition="5689" endWordPosition="5692">ion team-color(‘Sports Clubs’, ‘Colors’), the only descendant concept of ‘Colors’ in the hierarchy is ‘Horse Coat Colors’, meaning that the gold concept ‘Colors’ is not reached during propagation from instances upwards in the hierarchy. 5 Related Work Similar to the task of attaching a semantic annotation to the concept in a hierarchy that has the best level of generality is the task of finding selectional preferences for relations. Most relevant to this paper is work that seeks to find the appropriate concept in a hierarchy for an argument of a specific relation (Ribas, 1995; McCarthy, 1997; Li and Abe, 1998). Li and Abe (1998) address this problem by attempting to identify the best tree cut in a hierarchy for an argument of a given verb. They use the minimum description length principle to select a set of concepts from a hierarchy to represent the selectional preferences. This work makes several limiting assumptions including that the hierarchy is a tree, and every instance belongs to just one concept. Clark and Weir (2002) investigate the task of generalizing a single relationconcept pair. A relation is propagated up a hierarchy until a chi-square test determines the difference between the proba</context>
</contexts>
<marker>Li, Abe, 1998</marker>
<rawString>Hang Li and Naoki Abe. 1998. Generalizing case frames using a thesaurus and the mdl principle. In Proceedings of the ECAI-2000 Workshop on Ontology Learning, pages 217–244, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Li</author>
</authors>
<title>Understanding the semantic structure of noun phrase queries.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10),</booktitle>
<pages>1337--1345</pages>
<location>Uppsala,</location>
<contexts>
<context position="2453" citStr="Li, 2010" startWordPosition="344" endWordPosition="345"> or manually created within encyclopedic resources (Remy, 2002). Such facts could state, for instance, that rhapsody in blue was composedby george gershwin, or that tristan und isolde was composed-by richard wagner. In comparison, concept-level annotations more concisely and effectively capture the underlying semantics of the annotations by identifying the concepts corresponding to the arguments, e.g., ‘Musical Compositions’ are composed-by ‘Composers’. The frequent occurrence of instances, relative to more abstract concepts, in Web documents and popular Web search queries (Barr et al., 2008; Li, 2010), is both an asset and a liability from the point of view of knowledge acquisition. On one hand, it makes instance-level annotations relatively easy to find, either from manually created resources (Remy, 2002; Bollacker et al., 2008), or extracted automatically from text (Banko et al., 2007). On the other hand, it makes conceptlevel annotations more difficult to acquire directly. While “Rhapsody in Blue was composed by George Gershwin [..]” may occur in some form within Web documents, the more abstract “Musical compositions are composed by musicians [..]” is unlikely to occur. A more practical</context>
</contexts>
<marker>Li, 2010</marker>
<rawString>Xiao Li. 2010. Understanding the semantic structure of noun phrase queries. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10), pages 1337–1345, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
</authors>
<title>Word sense disambiguation for acquisition of selectional preferences.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL/EACL Workshop on Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications,</booktitle>
<pages>52--60</pages>
<location>Madrid,</location>
<contexts>
<context position="37098" citStr="McCarthy, 1997" startWordPosition="5687" endWordPosition="5688"> for the annotation team-color(‘Sports Clubs’, ‘Colors’), the only descendant concept of ‘Colors’ in the hierarchy is ‘Horse Coat Colors’, meaning that the gold concept ‘Colors’ is not reached during propagation from instances upwards in the hierarchy. 5 Related Work Similar to the task of attaching a semantic annotation to the concept in a hierarchy that has the best level of generality is the task of finding selectional preferences for relations. Most relevant to this paper is work that seeks to find the appropriate concept in a hierarchy for an argument of a specific relation (Ribas, 1995; McCarthy, 1997; Li and Abe, 1998). Li and Abe (1998) address this problem by attempting to identify the best tree cut in a hierarchy for an argument of a given verb. They use the minimum description length principle to select a set of concepts from a hierarchy to represent the selectional preferences. This work makes several limiting assumptions including that the hierarchy is a tree, and every instance belongs to just one concept. Clark and Weir (2002) investigate the task of generalizing a single relationconcept pair. A relation is propagated up a hierarchy until a chi-square test determines the differenc</context>
</contexts>
<marker>McCarthy, 1997</marker>
<rawString>Diana McCarthy. 1997. Word sense disambiguation for acquisition of selectional preferences. In Proceedings of the ACL/EACL Workshop on Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications, pages 52– 60, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Mitchell</author>
</authors>
<title>Machine Learing.</title>
<date>1997</date>
<publisher>McGraw Hill.</publisher>
<contexts>
<context position="22381" citStr="Mitchell, 1997" startWordPosition="3388" endWordPosition="3389">tations and instances are displayed in a shorter, more readable form for conciseness and clarity. As features do not use the label of the annotation, labels are never used in the experiments and evaluation. 507 Web Search Queries: The argument cooccurrence features described above are computed over a set of around 100 million anonymized Web search queries from 2010. 3.2 Experimental Runs The experimental runs exploit ranking features described in the previous section, employing: • one of three learning algorithms: naive Bayes (NAIVEBAYES), maximum entropy (MAXENT), or perceptron (PERCEPTRON) (Mitchell, 1997), chosen for their scalability to larger datasets via distributed implementations. • one of three ways of combining the values of features collected for individual candidate concepts into values of features for pairs of candidate concepts: the raw ratio of the values of the respective features of the two concepts (0 when the denominator is 0); the ratio scaled to the interval [0, 1]; or a binary value indicating which of the values is larger. For completeness, the experiments include three additional, baseline runs. Each baseline computes scores for all candidate concepts based on the respecti</context>
</contexts>
<marker>Mitchell, 1997</marker>
<rawString>Tom Mitchell. 1997. Machine Learing. McGraw Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thahir Mohamed</author>
<author>Estevam Hruschka</author>
<author>Tom Mitchell</author>
</authors>
<title>Discovering relations between noun categories.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP-11),</booktitle>
<pages>1447--1455</pages>
<location>Edinburgh, United Kingdom.</location>
<contexts>
<context position="38948" citStr="Mohamed et al. (2011)" startWordPosition="5985" endWordPosition="5988">y network (Remy, 2002) and work on refining it. Ponzetto and Navigli (2009) disambiguate Wikipedia categories by using WordNet synsets and use this semantic information to construct a taxonomy. The resulting taxonomy is the conceptual hierarchy used in the evaluation. Another related area of work is the discovery of relations between concepts. Nastase and Strube (2008) use Wikipedia category names and category structure to generate a set of relations between concepts. Yan et al. (2009) discover relations between Wikipedia concepts via deep linguistic information and Web frequency information. Mohamed et al. (2011) generate candidate relations by coclustering text contexts for every pair of concepts in a hierarchy. In a sense, this area of research is complementary to that discussed in this paper. These methods induce new relations, and the proposed method can be used to find appropriate levels of generalization for the arguments of any given relation. 6 Conclusions This paper introduces a method to convert flat sets of instance-level annotations to hierarchically organized, concept-level annotations. The method determines the appropriate concept for a given semantic annotation in three stages. First, i</context>
</contexts>
<marker>Mohamed, Hruschka, Mitchell, 2011</marker>
<rawString>Thahir Mohamed, Estevam Hruschka, and Tom Mitchell. 2011. Discovering relations between noun categories. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP-11), pages 1447–1455, Edinburgh, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivi Nastase</author>
<author>Michael Strube</author>
</authors>
<title>Decoding Wikipedia categories for knowledge acquisition.</title>
<date>2008</date>
<booktitle>In Proceedings of the 23rd National Conference on Artificial Intelligence (AAAI-08),</booktitle>
<pages>1219--1224</pages>
<location>Chicago, Illinois.</location>
<contexts>
<context position="38698" citStr="Nastase and Strube (2008)" startWordPosition="5944" endWordPosition="5947">d Hovy (2010) and Ritter et al. (2010) focus on generating admissible arguments for relations, and Erk (2007) and Bergsma et al. (2008) investigate classifying a relation-instance pair as plausible or not. Important to this paper is the Wikipedia category network (Remy, 2002) and work on refining it. Ponzetto and Navigli (2009) disambiguate Wikipedia categories by using WordNet synsets and use this semantic information to construct a taxonomy. The resulting taxonomy is the conceptual hierarchy used in the evaluation. Another related area of work is the discovery of relations between concepts. Nastase and Strube (2008) use Wikipedia category names and category structure to generate a set of relations between concepts. Yan et al. (2009) discover relations between Wikipedia concepts via deep linguistic information and Web frequency information. Mohamed et al. (2011) generate candidate relations by coclustering text contexts for every pair of concepts in a hierarchy. In a sense, this area of research is complementary to that discussed in this paper. These methods induce new relations, and the proposed method can be used to find appropriate levels of generalization for the arguments of any given relation. 6 Con</context>
</contexts>
<marker>Nastase, Strube, 2008</marker>
<rawString>Vivi Nastase and Michael Strube. 2008. Decoding Wikipedia categories for knowledge acquisition. In Proceedings of the 23rd National Conference on Artificial Intelligence (AAAI-08), pages 1219–1224, Chicago, Illinois.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pas¸ca</author>
<author>E Alfonseca</author>
</authors>
<title>Web-derived resources for Web Information Retrieval: From conceptual hierarchies to attribute hierarchies.</title>
<date>2009</date>
<booktitle>In Proceedings of the 32nd International Conference on Research and Development in Information Retrieval (SIGIR-09),</booktitle>
<pages>596--603</pages>
<location>Boston, Massachusetts.</location>
<marker>Pas¸ca, Alfonseca, 2009</marker>
<rawString>M. Pas¸ca and E. Alfonseca. 2009. Web-derived resources for Web Information Retrieval: From conceptual hierarchies to attribute hierarchies. In Proceedings of the 32nd International Conference on Research and Development in Information Retrieval (SIGIR-09), pages 596–603, Boston, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Rahul Bhagat</author>
<author>Timothy Chklovski</author>
<author>Eduard Hovy</author>
</authors>
<title>ISP: Learning inferential selectional preferences.</title>
<date>2007</date>
<booktitle>In Proceedings of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-07),</booktitle>
<pages>564--571</pages>
<location>Rochester, New York.</location>
<contexts>
<context position="38060" citStr="Pantel et al. (2007)" startWordPosition="5844" endWordPosition="5847">g that the hierarchy is a tree, and every instance belongs to just one concept. Clark and Weir (2002) investigate the task of generalizing a single relationconcept pair. A relation is propagated up a hierarchy until a chi-square test determines the difference between the probability of the child and parent concepts to be significant where the probabilities are relation-concept frequencies. This method has no direct translation to the task discussed here; it is unclear how to choose the correct concept if instances generalize to different concepts. In other research on selectional preferences, Pantel et al. (2007), Kozareva and Hovy (2010) and Ritter et al. (2010) focus on generating admissible arguments for relations, and Erk (2007) and Bergsma et al. (2008) investigate classifying a relation-instance pair as plausible or not. Important to this paper is the Wikipedia category network (Remy, 2002) and work on refining it. Ponzetto and Navigli (2009) disambiguate Wikipedia categories by using WordNet synsets and use this semantic information to construct a taxonomy. The resulting taxonomy is the conceptual hierarchy used in the evaluation. Another related area of work is the discovery of relations betwe</context>
</contexts>
<marker>Pantel, Bhagat, Chklovski, Hovy, 2007</marker>
<rawString>Patrick Pantel, Rahul Bhagat, Timothy Chklovski, and Eduard Hovy. 2007. ISP: Learning inferential selectional preferences. In Proceedings of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-07), pages 564–571, Rochester, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Roberto Navigli</author>
</authors>
<title>Large-scale taxonomy mapping for restructuring and integrating Wikipedia.</title>
<date>2009</date>
<booktitle>In Proceedings of the 21st International Joint Conference on Artifical Intelligence (IJCAI-09),</booktitle>
<pages>2083--2088</pages>
<location>Barcelona,</location>
<contexts>
<context position="20626" citStr="Ponzetto and Navigli, 2009" startWordPosition="3120" endWordPosition="3123">t in the pair are respectively combined with the values of the features of the second concept in the pair to produce values corresponding to the entire pair. Following classification of testing examples, concepts are ranked according to the number of other concepts which they are classified as more relevant than. Table 1 shows examples of training/testing data. 3 Experimental Setting 3.1 Data Sources Conceptual Hierarchy: The experiments compute concept-level annotations relative to a conceptual hierarchy derived automatically from the Wikipedia (Remy, 2002) category network, as described in (Ponzetto and Navigli, 2009). The hierarchy filters out edges (e.g., from ‘British Film Actors’ to ‘Cinema of the United Kingdom’) from the Wikipedia category network that do not correspond to IsA relations. A concept in the hierarchy is a Wikipedia category (e.g., ‘English Film Actors’) that has zero or more Wikipedia categories as child concepts, and zero or more Wikipedia categories (e.g., ‘English People by Occupation’, ‘British Film Actors’) as parent concepts. Each concept in the hierarchy has zero or more instances, which are the Wikipedia articles listed (in Wikipedia) under the respective categories (e.g., colin</context>
<context position="38402" citStr="Ponzetto and Navigli (2009)" startWordPosition="5900" endWordPosition="5903">where the probabilities are relation-concept frequencies. This method has no direct translation to the task discussed here; it is unclear how to choose the correct concept if instances generalize to different concepts. In other research on selectional preferences, Pantel et al. (2007), Kozareva and Hovy (2010) and Ritter et al. (2010) focus on generating admissible arguments for relations, and Erk (2007) and Bergsma et al. (2008) investigate classifying a relation-instance pair as plausible or not. Important to this paper is the Wikipedia category network (Remy, 2002) and work on refining it. Ponzetto and Navigli (2009) disambiguate Wikipedia categories by using WordNet synsets and use this semantic information to construct a taxonomy. The resulting taxonomy is the conceptual hierarchy used in the evaluation. Another related area of work is the discovery of relations between concepts. Nastase and Strube (2008) use Wikipedia category names and category structure to generate a set of relations between concepts. Yan et al. (2009) discover relations between Wikipedia concepts via deep linguistic information and Web frequency information. Mohamed et al. (2011) generate candidate relations by coclustering text con</context>
</contexts>
<marker>Ponzetto, Navigli, 2009</marker>
<rawString>Simone Paolo Ponzetto and Roberto Navigli. 2009. Large-scale taxonomy mapping for restructuring and integrating Wikipedia. In Proceedings of the 21st International Joint Conference on Artifical Intelligence (IJCAI-09), pages 2083–2088, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Melanie Remy</author>
</authors>
<title>Wikipedia: The free encyclopedia.</title>
<date>2002</date>
<journal>Online Information Review,</journal>
<volume>26</volume>
<issue>6</issue>
<contexts>
<context position="1907" citStr="Remy, 2002" startWordPosition="264" endWordPosition="265">tandard concepts. 1 Introduction Background: Knowledge about the world can be thought of as semantic assertions or annotations, at two levels of granularity: instance level (e.g., rhapsody in blue, tristan und isolde, george gershwin, richard wagner) and concept level (e.g., ‘Musical Compositions’, ‘Works of Art’, ‘Composers’). Instance-level annotations correspond to factual knowledge that can be found in repositories extracted automatically from text (Banko et al., 2007; Wu and Weld, 2010) *Contributions made during an internship at Google. or manually created within encyclopedic resources (Remy, 2002). Such facts could state, for instance, that rhapsody in blue was composedby george gershwin, or that tristan und isolde was composed-by richard wagner. In comparison, concept-level annotations more concisely and effectively capture the underlying semantics of the annotations by identifying the concepts corresponding to the arguments, e.g., ‘Musical Compositions’ are composed-by ‘Composers’. The frequent occurrence of instances, relative to more abstract concepts, in Web documents and popular Web search queries (Barr et al., 2008; Li, 2010), is both an asset and a liability from the point of v</context>
<context position="20563" citStr="Remy, 2002" startWordPosition="3112" endWordPosition="3113"> the values of the features of the first concept in the pair are respectively combined with the values of the features of the second concept in the pair to produce values corresponding to the entire pair. Following classification of testing examples, concepts are ranked according to the number of other concepts which they are classified as more relevant than. Table 1 shows examples of training/testing data. 3 Experimental Setting 3.1 Data Sources Conceptual Hierarchy: The experiments compute concept-level annotations relative to a conceptual hierarchy derived automatically from the Wikipedia (Remy, 2002) category network, as described in (Ponzetto and Navigli, 2009). The hierarchy filters out edges (e.g., from ‘British Film Actors’ to ‘Cinema of the United Kingdom’) from the Wikipedia category network that do not correspond to IsA relations. A concept in the hierarchy is a Wikipedia category (e.g., ‘English Film Actors’) that has zero or more Wikipedia categories as child concepts, and zero or more Wikipedia categories (e.g., ‘English People by Occupation’, ‘British Film Actors’) as parent concepts. Each concept in the hierarchy has zero or more instances, which are the Wikipedia articles lis</context>
<context position="38349" citStr="Remy, 2002" startWordPosition="5892" endWordPosition="5893">nd parent concepts to be significant where the probabilities are relation-concept frequencies. This method has no direct translation to the task discussed here; it is unclear how to choose the correct concept if instances generalize to different concepts. In other research on selectional preferences, Pantel et al. (2007), Kozareva and Hovy (2010) and Ritter et al. (2010) focus on generating admissible arguments for relations, and Erk (2007) and Bergsma et al. (2008) investigate classifying a relation-instance pair as plausible or not. Important to this paper is the Wikipedia category network (Remy, 2002) and work on refining it. Ponzetto and Navigli (2009) disambiguate Wikipedia categories by using WordNet synsets and use this semantic information to construct a taxonomy. The resulting taxonomy is the conceptual hierarchy used in the evaluation. Another related area of work is the discovery of relations between concepts. Nastase and Strube (2008) use Wikipedia category names and category structure to generate a set of relations between concepts. Yan et al. (2009) discover relations between Wikipedia concepts via deep linguistic information and Web frequency information. Mohamed et al. (2011) </context>
</contexts>
<marker>Remy, 2002</marker>
<rawString>Melanie Remy. 2002. Wikipedia: The free encyclopedia. Online Information Review, 26(6):434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francesc Ribas</author>
</authors>
<title>On learning more appropriate selectional restrictions.</title>
<date>1995</date>
<booktitle>In Proceedings of the 7th Conference of the European Chapter of the Association for Computational Linguistics (EACL-97),</booktitle>
<pages>112--118</pages>
<location>Madrid,</location>
<contexts>
<context position="37082" citStr="Ribas, 1995" startWordPosition="5685" endWordPosition="5686"> For example, for the annotation team-color(‘Sports Clubs’, ‘Colors’), the only descendant concept of ‘Colors’ in the hierarchy is ‘Horse Coat Colors’, meaning that the gold concept ‘Colors’ is not reached during propagation from instances upwards in the hierarchy. 5 Related Work Similar to the task of attaching a semantic annotation to the concept in a hierarchy that has the best level of generality is the task of finding selectional preferences for relations. Most relevant to this paper is work that seeks to find the appropriate concept in a hierarchy for an argument of a specific relation (Ribas, 1995; McCarthy, 1997; Li and Abe, 1998). Li and Abe (1998) address this problem by attempting to identify the best tree cut in a hierarchy for an argument of a given verb. They use the minimum description length principle to select a set of concepts from a hierarchy to represent the selectional preferences. This work makes several limiting assumptions including that the hierarchy is a tree, and every instance belongs to just one concept. Clark and Weir (2002) investigate the task of generalizing a single relationconcept pair. A relation is propagated up a hierarchy until a chi-square test determin</context>
</contexts>
<marker>Ribas, 1995</marker>
<rawString>Francesc Ribas. 1995. On learning more appropriate selectional restrictions. In Proceedings of the 7th Conference of the European Chapter of the Association for Computational Linguistics (EACL-97), pages 112–118, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>A latent dirichlet allocation method for selectional preferences.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10),</booktitle>
<pages>424--434</pages>
<location>Uppsala,</location>
<contexts>
<context position="25567" citStr="Ritter et al., 2010" startWordPosition="3869" endWordPosition="3872">xperiments. More precisely, an entry in the resulting gold standard consists of an annotation label, one of its arguments being considered (left or right), and a gold concept that best captures that argument. The set of annotation labels from the gold standard is quite diverse and covers many domains of potential interest, e.g., has-company(‘Industries’, ‘Companies’), written-by(‘Films’, ‘Screenwriters’), member-of(‘Politicians’,‘Political Parties’), or part-of-movement(‘Artists’, ‘Art Movements’). Evaluation Metric: Following previous work on selectional preferences (Kozareva and Hovy, 2010; Ritter et al., 2010), each entry in the gold standard, (i.e., each argument for a given annotation) is evaluated separately. Experimental runs compute a ranked list of candidate concepts for each entry in the gold standard. In theory, a computed candidate concept is better if it is closer semantically to the gold concept. In practice, the accuracy of a ranked list of candidate concepts, relative to the gold concept of the annotation label, is measured by two scoring metrics that correspond to the mean reciprocal rank score (MRR) (Voorhees and Tice, 2000) and a modification of it (DRR) (Pas¸ca and Alfonseca, 2009)</context>
<context position="38111" citStr="Ritter et al. (2010)" startWordPosition="5853" endWordPosition="5856">belongs to just one concept. Clark and Weir (2002) investigate the task of generalizing a single relationconcept pair. A relation is propagated up a hierarchy until a chi-square test determines the difference between the probability of the child and parent concepts to be significant where the probabilities are relation-concept frequencies. This method has no direct translation to the task discussed here; it is unclear how to choose the correct concept if instances generalize to different concepts. In other research on selectional preferences, Pantel et al. (2007), Kozareva and Hovy (2010) and Ritter et al. (2010) focus on generating admissible arguments for relations, and Erk (2007) and Bergsma et al. (2008) investigate classifying a relation-instance pair as plausible or not. Important to this paper is the Wikipedia category network (Remy, 2002) and work on refining it. Ponzetto and Navigli (2009) disambiguate Wikipedia categories by using WordNet synsets and use this semantic information to construct a taxonomy. The resulting taxonomy is the conceptual hierarchy used in the evaluation. Another related area of work is the discovery of relations between concepts. Nastase and Strube (2008) use Wikipedi</context>
</contexts>
<marker>Ritter, Mausam, Etzioni, 2010</marker>
<rawString>Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent dirichlet allocation method for selectional preferences. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10), pages 424–434, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claude Shannon</author>
</authors>
<title>A mathematical theory of communication.</title>
<date>1948</date>
<journal>Bell System Technical Journal,</journal>
<pages>27--379</pages>
<contexts>
<context position="23342" citStr="Shannon, 1948" startWordPosition="3540" endWordPosition="3541">he ratio scaled to the interval [0, 1]; or a binary value indicating which of the values is larger. For completeness, the experiments include three additional, baseline runs. Each baseline computes scores for all candidate concepts based on the respective metric; then candidate concepts are ranked in decreasing order of their scores. The baselines metrics are: • INSTPERCENT ranks candidate concepts by the percentage of matched instances that are descendants of the concept. It emphasizes concepts which are “proven” to belong to the annotation; • ENTROPY ranks candidate concepts by the entropy (Shannon, 1948) of the proportion of matched descendant instances of the concept; • AVGDEPTH ranks candidate concepts by their distances to half of the maximum hierarchy height, emphasizing a balance of generality and specificity. 3.3 Evaluation Procedure Gold Standard of Concept-Level Annotations: A random, weighted sample of 200 annotation labels (e.g., corresponding to composed-by, playinstrument) is selected, out of the set of labels of all instance-level annotations collected from Freebase. During sampling, the weights are the counts of distinct instance-level annotations (e.g., &lt;rhapsody in blue, georg</context>
</contexts>
<marker>Shannon, 1948</marker>
<rawString>Claude Shannon. 1948. A mathematical theory of communication. Bell System Technical Journal, 27:379–423,623–656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Voorhees</author>
<author>Dawn Tice</author>
</authors>
<title>Building a question-answering test collection.</title>
<date>2000</date>
<booktitle>In Proceedings of the 23rd International Conference on Research and Development in Information Retrieval (SIGIR00),</booktitle>
<pages>200--207</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="26107" citStr="Voorhees and Tice, 2000" startWordPosition="3961" endWordPosition="3964">vious work on selectional preferences (Kozareva and Hovy, 2010; Ritter et al., 2010), each entry in the gold standard, (i.e., each argument for a given annotation) is evaluated separately. Experimental runs compute a ranked list of candidate concepts for each entry in the gold standard. In theory, a computed candidate concept is better if it is closer semantically to the gold concept. In practice, the accuracy of a ranked list of candidate concepts, relative to the gold concept of the annotation label, is measured by two scoring metrics that correspond to the mean reciprocal rank score (MRR) (Voorhees and Tice, 2000) and a modification of it (DRR) (Pas¸ca and Alfonseca, 2009): MRR = N N i=1 N is the number of annotations and ranki is the rank of the gold concept in the returned list for MRR. An annotation ai receives no credit for MRR if the gold concept does not appear in the corresponding ranked list. DRR = � 1 N i=1 For DRR, ranki is the rank of a candidate concept in the returned list and Len is the length of 1 max rank ranki 1 ranki x (1 + Len) max rank 508 Annotation (Number of Candidate Concepts) Examples of Instances Top Ranked Concepts Composers compose Musical Compositions (3038) aaron copland; </context>
</contexts>
<marker>Voorhees, Tice, 2000</marker>
<rawString>Ellen Voorhees and Dawn Tice. 2000. Building a question-answering test collection. In Proceedings of the 23rd International Conference on Research and Development in Information Retrieval (SIGIR00), pages 200–207, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Open information extraction using wikipedia.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10),</booktitle>
<pages>118--127</pages>
<location>Uppsala,</location>
<contexts>
<context position="1792" citStr="Wu and Weld, 2010" startWordPosition="246" endWordPosition="249"> baselines, on average, computing concepts that are less than one step in the hierarchy away from the corresponding gold standard concepts. 1 Introduction Background: Knowledge about the world can be thought of as semantic assertions or annotations, at two levels of granularity: instance level (e.g., rhapsody in blue, tristan und isolde, george gershwin, richard wagner) and concept level (e.g., ‘Musical Compositions’, ‘Works of Art’, ‘Composers’). Instance-level annotations correspond to factual knowledge that can be found in repositories extracted automatically from text (Banko et al., 2007; Wu and Weld, 2010) *Contributions made during an internship at Google. or manually created within encyclopedic resources (Remy, 2002). Such facts could state, for instance, that rhapsody in blue was composedby george gershwin, or that tristan und isolde was composed-by richard wagner. In comparison, concept-level annotations more concisely and effectively capture the underlying semantics of the annotations by identifying the concepts corresponding to the arguments, e.g., ‘Musical Compositions’ are composed-by ‘Composers’. The frequent occurrence of instances, relative to more abstract concepts, in Web documents</context>
</contexts>
<marker>Wu, Weld, 2010</marker>
<rawString>Fei Wu and Daniel S. Weld. 2010. Open information extraction using wikipedia. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10), pages 118–127, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulan Yan</author>
<author>Naoaki Okazaki</author>
<author>Yutaka Matsuo</author>
<author>Zhenglu Yang</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Unsupervised relation extraction by mining Wikipedia texts using information from the Web.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACLIJCNLP-09),</booktitle>
<pages>1021--1029</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="38817" citStr="Yan et al. (2009)" startWordPosition="5965" endWordPosition="5968">. (2008) investigate classifying a relation-instance pair as plausible or not. Important to this paper is the Wikipedia category network (Remy, 2002) and work on refining it. Ponzetto and Navigli (2009) disambiguate Wikipedia categories by using WordNet synsets and use this semantic information to construct a taxonomy. The resulting taxonomy is the conceptual hierarchy used in the evaluation. Another related area of work is the discovery of relations between concepts. Nastase and Strube (2008) use Wikipedia category names and category structure to generate a set of relations between concepts. Yan et al. (2009) discover relations between Wikipedia concepts via deep linguistic information and Web frequency information. Mohamed et al. (2011) generate candidate relations by coclustering text contexts for every pair of concepts in a hierarchy. In a sense, this area of research is complementary to that discussed in this paper. These methods induce new relations, and the proposed method can be used to find appropriate levels of generalization for the arguments of any given relation. 6 Conclusions This paper introduces a method to convert flat sets of instance-level annotations to hierarchically organized,</context>
</contexts>
<marker>Yan, Okazaki, Matsuo, Yang, Ishizuka, 2009</marker>
<rawString>Yulan Yan, Naoaki Okazaki, Yutaka Matsuo, Zhenglu Yang, and Mitsuru Ishizuka. 2009. Unsupervised relation extraction by mining Wikipedia texts using information from the Web. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACLIJCNLP-09), pages 1021–1029, Suntec, Singapore.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>