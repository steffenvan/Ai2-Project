<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.9952815">
Manual and Automatic Evaluation of Machine Translation
between European Languages
</title>
<author confidence="0.997444">
Philipp Koehn
</author>
<affiliation confidence="0.999123">
School of Informatics
University of Edinburgh
</affiliation>
<email confidence="0.97886">
pkoehn@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.997209" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999818823529412">
We evaluated machine translation perfor-
mance for six European language pairs
that participated in a shared task: translat-
ing French, German, Spanish texts to En-
glish and back. Evaluation was done auto-
matically using the BLEU score and man-
ually on fluency and adequacy.
For the 2006 NAACL/HLT Workshop on Ma-
chine Translation, we organized a shared task to
evaluate machine translation performance. 14 teams
from 11 institutions participated, ranging from com-
mercial companies, industrial research labs to indi-
vidual graduate students.
The motivation for such a competition is to estab-
lish baseline performance numbers for defined train-
ing scenarios and test sets. We assembled various
forms of data and resources: a baseline MT system,
language models, prepared training and test sets,
resulting in actual machine translation output from
several state-of-the-art systems and manual evalua-
tions. All this is available at the workshop website1.
The shared task is a follow-up to the one we orga-
nized in the previous year, at a similar venue (Koehn
and Monz, 2005). As then, we concentrated on the
translation of European languages and the use of the
Europarl corpus for training. Again, most systems
that participated could be categorized as statistical
phrase-based systems. While there is now a num-
ber of competitions — DARPA/NIST (Li, 2005),
IWSLT (Eck and Hori, 2005), TC-Star — this one
focuses on text translation between various Euro-
pean languages.
This year’s shared task changed in some aspects
from last year’s:
</bodyText>
<listItem confidence="0.975745">
• We carried out a manual evaluation in addition
to the automatic scoring. Manual evaluation
</listItem>
<footnote confidence="0.943991">
1http://www.statmt.org/wmt06/
</footnote>
<author confidence="0.871561">
Christof Monz
</author>
<affiliation confidence="0.9920995">
Department of Computer Science
Queen Mary, University of London
</affiliation>
<email confidence="0.91395">
christof@dcs.qmul.ac.uk
</email>
<bodyText confidence="0.971055181818182">
was done by the participants. This revealed
interesting clues about the properties of auto-
matic and manual scoring.
• We evaluated translation from English, in ad-
dition to into English. English was again
paired with German, French, and Spanish.
We dropped, however, one of the languages,
Finnish, partly to keep the number of tracks
manageable, partly because we assumed that it
would be hard to find enough Finnish speakers
for the manual evaluation.
</bodyText>
<listItem confidence="0.988656333333333">
• We included an out-of-domain test set. This al-
lows us to compare machine translation perfor-
mance in-domain and out-of-domain.
</listItem>
<sectionHeader confidence="0.992675" genericHeader="keywords">
1 Evaluation Framework
</sectionHeader>
<bodyText confidence="0.99983725">
The evaluation framework for the shared task is sim-
ilar to the one used in last year’s shared task. Train-
ing and testing is based on the Europarl corpus. Fig-
ure 1 provides some statistics about this corpus.
</bodyText>
<subsectionHeader confidence="0.993883">
1.1 Baseline system
</subsectionHeader>
<bodyText confidence="0.999591333333333">
To lower the barrier of entrance to the competition,
we provided a complete baseline MT system, along
with data resources. To summarize, we provided:
</bodyText>
<listItem confidence="0.9999416">
• sentence-aligned, tokenized training corpus
• a development and development test set
• trained language models for each language
• the phrase-based MT decoder Pharaoh
• a training script to build models for Pharaoh
</listItem>
<bodyText confidence="0.9999002">
The performance of the baseline system is simi-
lar to the best submissions in last year’s shared task.
We are currently working on a complete open source
implementation of a training and decoding system,
which should become available over the summer.
</bodyText>
<page confidence="0.980458">
102
</page>
<note confidence="0.94983">
Proceedings of the Workshop on Statistical Machine Translation, pages 102–121,
New York City, June 2006. c�2006 Association for Computational Linguistics
</note>
<table confidence="0.976109592592593">
Training corpus
Spanish H English French H English German H English
Sentences 730,740 688,031 751,088
Foreign words 15,676,710 15,323,737 15,256,793
English words 15,222,105 13,808,104 16,052,269
Distinct foreign words 102,886 80,349 195,291
Distinct English words 64,123 61,627 65,889
Language model data
English Spanish French German
Sentence 1,003,349 1,070,305 1,066,974 1,078,141
Words 27,493,499 29,129,720 31,604,879 26,562,167
In-domain test set
English Spanish French German
Sentences 2,000
Words 59,307 61,824 66,783 55,533
Unseen words 141 206 164 387
Ratio of unseen words 0.23% 0.40% 0.24% 0.70%
Distinct words 6,031 7,719 7,230 8,812
Distinct unseen words 139 203 163 385
Out-of-domain test set
English Spanish French German
Sentences 1,064
Words 25,919 29,826 31,937 26,818
Unseen words 464 368 839 913
Ratio of unseen words 1.79% 1.23% 2.62% 3.40%
Distinct words 5,166 5,689 5,728 6,594
Distinct unseen words 340 267 375 637
</table>
<figureCaption confidence="0.985767">
Figure 1: Properties of the training and test sets used in the shared task. The training data is the Europarl cor-
</figureCaption>
<bodyText confidence="0.967837333333333">
pus, from which also the in-domain test set is taken. There is twice as much language modelling data, since
training data for the machine translation system is filtered against sentences of length larger than 40 words.
Out-of-domain test data is from the Project Syndicate web site, a compendium of political commentary.
</bodyText>
<page confidence="0.998329">
103
</page>
<affiliation confidence="0.896735933333333">
ID Participant
cmu Carnegie Mellon University, USA (Zollmann and Venugopal, 2006)
lcc Language Computer Corporation, USA (Olteanu et al., 2006b)
ms Microsoft, USA (Menezes et al., 2006)
nrc National Research Council, Canada (Johnson et al., 2006)
ntt Nippon Telegraph and Telephone, Japan (Watanabe et al., 2006)
rali RALI, University of Montreal, Canada (Patry et al., 2006)
systran Systran, France
uedin-birch University of Edinburgh, UK — Alexandra Birch (Birch et al., 2006)
uedin-phi University of Edinburgh, UK — Philipp Koehn (Birch et al., 2006)
upc-jg University of Catalonia, Spain — Jes´us Gim´enez (Gim´enez and M`arquez, 2006)
upc-jmc University of Catalonia, Spain — Josep Maria Crego (Crego et al., 2006)
upc-mr University of Catalonia, Spain — Marta Ruiz Costa-juss`a (Costa-juss`a et al., 2006)
upv University of Valencia, Spain (S´anchez and Benedi, 2006)
utd University of Texas at Dallas, USA (Olteanu et al., 2006a)
</affiliation>
<figureCaption confidence="0.9972">
Figure 2: Participants in the shared task. Not all groups participated in all translation directions.
</figureCaption>
<subsectionHeader confidence="0.994339">
1.2 Test Data
</subsectionHeader>
<bodyText confidence="0.9988464">
The test data was again drawn from a segment of
the Europarl corpus from the fourth quarter of 2000,
which is excluded from the training data. Partici-
pants were also provided with two sets of 2,000 sen-
tences of parallel text to be used for system develop-
ment and tuning.
In addition to the Europarl test set, we also col-
lected 29 editorials from the Project Syndicate web-
site2, which are published in all the four languages
of the shared task. We aligned the texts at a sen-
tence level across all four languages, resulting in
1064 sentence per language. For statistics on this
test set, refer to Figure 1.
The out-of-domain test set differs from the Eu-
roparl data in various ways. The text type are edi-
torials instead of speech transcripts. The domain is
general politics, economics and science. However, it
is also mostly political content (even if not focused
on the internal workings of the European Union) and
opinion.
</bodyText>
<subsectionHeader confidence="0.845011">
1.3 Participants
</subsectionHeader>
<bodyText confidence="0.9998265">
We received submissions from 14 groups from 11
institutions, as listed in Figure 2. Most of these
groups follow a phrase-based statistical approach to
machine translation. Microsoft’s approach uses de-
</bodyText>
<footnote confidence="0.814243">
2http://www.project-syndicate.com/
</footnote>
<bodyText confidence="0.998055117647059">
pendency trees, others use hierarchical phrase mod-
els. Systran submitted their commercial rule-based
system that was not tuned to the Europarl corpus.
About half of the participants of last year’s shared
task participated again. The other half was replaced
by other participants, so we ended up with roughly
the same number. Compared to last year’s shared
task, the participants represent more long-term re-
search efforts. This may be the sign of a maturing
research environment.
While building a machine translation system is
a serious undertaking, in future we hope to attract
more newcomers to the field by keeping the barrier
of entry as low as possible.
For more on the participating systems, please re-
fer to the respective system description in the pro-
ceedings of the workshop.
</bodyText>
<sectionHeader confidence="0.992017" genericHeader="introduction">
2 Automatic Evaluation
</sectionHeader>
<bodyText confidence="0.9981034">
For the automatic evaluation, we used BLEU, since it
is the most established metric in the field. The BLEU
metric, as all currently proposed automatic metrics,
is occasionally suspected to be biased towards sta-
tistical systems, especially the phrase-based systems
currently in use. It rewards matches of n-gram se-
quences, but measures only at most indirectly over-
all grammatical coherence.
The BLEU score has been shown to correlate
well with human judgement, when statistical ma-
</bodyText>
<page confidence="0.998402">
104
</page>
<bodyText confidence="0.9998736875">
chine translation systems are compared (Dodding-
ton, 2002; Przybocki, 2004; Li, 2005). However, a
recent study (Callison-Burch et al., 2006), pointed
out that this correlation may not always be strong.
They demonstrated this with the comparison of sta-
tistical systems against (a) manually post-edited MT
output, and (b) a rule-based commercial system.
The development of automatic scoring methods is
an open field of research. It was our hope that this
competition, which included the manual and auto-
matic evaluation of statistical systems and one rule-
based commercial system, will give further insight
into the relation between automatic and manual eval-
uation. At the very least, we are creating a data re-
source (the manual annotations) that may the basis
of future research in evaluation metrics.
</bodyText>
<subsectionHeader confidence="0.986392">
2.1 Computing BLEU Scores
</subsectionHeader>
<bodyText confidence="0.99997775">
We computed BLEU scores for each submission with
a single reference translation. For each sentence,
we counted how many n-grams in the system output
also occurred in the reference translation. By taking
the ratio of matching n-grams to the total number of
n-grams in the system output, we obtain the preci-
sion pn for each n-gram order n. These values for
n-gram precision are combined into a BLEU score:
</bodyText>
<equation confidence="0.98921375">
4
BLEU = BP · exp( X log pn) (1)
n=1
BP = min(1, e1−r/c) (2)
</equation>
<bodyText confidence="0.999973375">
The formula for the BLEU metric also includes a
brevity penalty for too short output, which is based
on the total number of words in the system output c
and in the reference r.
BLEU is sensitive to tokenization. Because of
this, we retokenized and lowercased submitted out-
put with our own tokenizer, which was also used to
prepare the training and test data.
</bodyText>
<subsectionHeader confidence="0.999588">
2.2 Statistical Significance
</subsectionHeader>
<bodyText confidence="0.99899911627907">
Confidence Interval: Since BLEU scores are not
computed on the sentence level, traditional methods
to compute statistical significance and confidence
intervals do not apply. Hence, we use the bootstrap
resampling method described by Koehn (2004).
Following this method, we repeatedly — say,
1000 times — sample sets of sentences from the out-
put of each system, measure their BLEU score, and
use these 1000 BLEU scores as basis for estimating
a confidence interval. When dropping the top and
bottom 2.5% the remaining BLEU scores define the
range of the confidence interval.
Pairwise comparison: We can use the same method
to assess the statistical significance of one system
outperforming another. If two systems’ scores are
close, this may simply be a random effect in the test
data. To check for this, we do pairwise bootstrap re-
sampling: Again, we repeatedly sample sets of sen-
tences, this time from both systems, and compare
their BLEU scores on these sets. If one system is bet-
ter in 95% of the sample sets, we conclude that its
higher BLEU score is statistically significantly bet-
ter.
The bootstrap method has been critized by Riezler
and Maxwell (2005) and Collins et al. (2005), as be-
ing too optimistic in deciding for statistical signifi-
cant difference between systems. We are therefore
applying a different method, which has been used at
the 2005 DARPA/NIST evaluation.
We divide up each test set into blocks of 20 sen-
tences (100 blocks for the in-domain test set, 53
blocks for the out-of-domain test set), check for each
block, if one system has a higher BLEU score than
the other, and then use the sign test.
The sign test checks, how likely a sample of better
and worse BLEU scores would have been generated
by two systems of equal performance.
Let say, if we find one system doing better on 20
of the blocks, and worse on 80 of the blocks, is it
significantly worse? We check, how likely only up
to k = 20 better scores out of n = 100 would have
been generated by two equal systems, using the bi-
nomial distribution:
</bodyText>
<equation confidence="0.996291333333333">
Xk �� �
n
i=0
</equation>
<bodyText confidence="0.923682666666667">
If p(0..k; n, p) &lt; 0.05, or p(0..k; n, p) &gt; 0.95
then we have a statistically significant difference be-
tween the systems.
</bodyText>
<equation confidence="0.9337144">
p(0..k; n, p) = Xk i n−i
Gn)p p
i=0
(3)
= 0.5n
</equation>
<page confidence="0.995091">
105
</page>
<figureCaption confidence="0.958835333333333">
Figure 3: Annotation tool for manual judgement of adequacy and fluency of the system output. Translations
from 5 randomly selected systems for a randomly selected sentence is presented. No additional information
beyond the instructions on this page are given to the judges. The tool tracks and reports annotation speed.
</figureCaption>
<sectionHeader confidence="0.983436" genericHeader="method">
3 Manual Evaluation
</sectionHeader>
<bodyText confidence="0.9999525">
While automatic measures are an invaluable tool
for the day-to-day development of machine trans-
lation systems, they are only a imperfect substitute
for human assessment of translation quality, or as
the acronym BLEU puts it, a bilingual evaluation
understudy.
Many human evaluation metrics have been pro-
posed. Also, the argument has been made that ma-
chine translation performance should be evaluated
via task-based evaluation metrics, i.e. how much it
assists performing a useful task, such as supporting
human translators or aiding the analysis of texts.
The main disadvantage of manual evaluation is
that it is time-consuming and thus too expensive to
do frequently. In this shared task, we were also con-
fronted with this problem, and since we had no fund-
ing for paying human judgements, we asked partic-
ipants in the evaluation to share the burden. Par-
ticipants and other volunteers contributed about 180
hours of labor in the manual evaluation.
</bodyText>
<subsectionHeader confidence="0.999721">
3.1 Collecting Human Judgements
</subsectionHeader>
<bodyText confidence="0.999752238095238">
We asked participants to each judge 200–300 sen-
tences in terms of fluency and adequacy, the most
commonly used manual evaluation metrics. We set-
tled on contrastive evaluations of 5 system outputs
for a single test sentence. See Figure 3 for a screen-
shot of the evaluation tool.
Presenting the output of several system allows
the human judge to make more informed judge-
ments, contrasting the quality of the different sys-
tems. The judgements tend to be done more in form
of a ranking of the different systems. We assumed
that such a contrastive assessment would be benefi-
cial for an evaluation that essentially pits different
systems against each other.
While we had up to 11 submissions for a trans-
lation direction, we did decide against presenting
all 11 system outputs to the human judge. Our ini-
tial experimentation with the evaluation tool showed
that this is often too overwhelming.
Making the ten judgements (2 types for 5 sys-
tems) takes on average 2 minutes. Typically, judges
</bodyText>
<page confidence="0.997351">
106
</page>
<bodyText confidence="0.999991466666667">
initially spent about 3 minutes per sentence, but then
accelerate with experience. Judges where excluded
from assessing the quality of MT systems that were
submitted by their institution. Sentences and sys-
tems were randomly selected and randomly shuffled
for presentation.
We collected around 300–400 judgements per
judgement type (adequacy or fluency), per system,
per language pair. This is less than the 694 judge-
ments 2004 DARPA/NIST evaluation, or the 532
judgements in the 2005 DARPA/NIST evaluation.
This decreases the statistical significance of our re-
sults compared to those studies. The number of
judgements is additionally fragmented by our break-
up of sentences into in-domain and out-of-domain.
</bodyText>
<subsectionHeader confidence="0.999593">
3.2 Normalizing the judgements
</subsectionHeader>
<bodyText confidence="0.998824666666667">
The human judges were presented with the follow-
ing definition of adequacy and fluency, but no addi-
tional instructions:
</bodyText>
<sectionHeader confidence="0.959508166666667" genericHeader="method">
Adequacy Fluency
5 All Meaning Flawless English
4 Most Meaning Good English
3 Much Meaning Non-native English
2 Little Meaning Disfluent English
1 None Incomprehensible
</sectionHeader>
<bodyText confidence="0.999941404761905">
Judges varied in the average score they handed
out. The average fluency judgement per judge
ranged from 2.33 to 3.67, the average adequacy
judgement ranged from 2.56 to 4.13. Since different
judges judged different systems (recall that judges
were excluded to judge system output from their
own institution), we normalized the scores.
The normalized judgement per judge is the raw
judgement plus (3 minus average raw judgement for
this judge). In words, the judgements are normal-
ized, so that the average normalized judgement per
judge is 3.
Another way to view the judgements is that they
are less quality judgements of machine translation
systems per se, but rankings of machine translation
systems. In fact, it is very difficult to maintain con-
sistent standards, on what (say) an adequacy judge-
ment of 3 means even for a specific language pair.
The way judgements are collected, human judges
tend to use the scores to rank systems against each
other. If one system is perfect, another has slight
flaws and the third more flaws, a judge is inclined
to hand out judgements of 5, 4, and 3. On the other
hand, when all systems produce muddled output, but
one is better, and one is worse, but not completely
wrong, a judge is inclined to hand out judgements of
4, 3, and 2. The judgement of 4 in the first case will
go to a vastly better system output than in the second
case.
We therefore also normalized judgements on a
per-sentence basis. The normalized judgement per
sentence is the raw judgement plus (0 minus average
raw judgement for this judge on this sentence).
Systems that generally do better than others will
receive a positive average normalizedjudgement per
sentence. Systems that generally do worse than oth-
ers will receive a negative one.
One may argue with these efforts on normaliza-
tion, and ultimately their value should be assessed
by assessing their impact on inter-annotator agree-
ment. Given the limited number of judgements we
received, we did not try to evaluate this.
</bodyText>
<subsectionHeader confidence="0.999724">
3.3 Statistical Significance
</subsectionHeader>
<bodyText confidence="0.999824">
Confidence Interval: To estimate confidence inter-
vals for the average mean scores for the systems, we
use standard significance testing.
Given a set of n sentences, we can compute the
sample mean x� and sample variance s2 of the indi-
vidual sentence judgements xi:
</bodyText>
<equation confidence="0.9789298">
x� = 1 n xi (4)
n i=1
1
s2 =
n − 1
</equation>
<bodyText confidence="0.9968265">
The extend of the confidence interval [x−d, x+df
can be computed by
</bodyText>
<subsectionHeader confidence="0.519257">
s
</subsectionHeader>
<bodyText confidence="0.95722475">
d = 1.96 ·�n (6)
Pairwise Comparison: As for the automatic evalu-
ation metric, we want to be able to rank different sys-
tems against each other, for which we need assess-
ments of statistical significance on the differences
between a pair of systems.
Unfortunately, we have much less data to work
with than with the automatic scores. The way we
</bodyText>
<equation confidence="0.988361">
n
(xi − x)2 (5)
i=1
</equation>
<page confidence="0.995025">
107
</page>
<table confidence="0.9908672">
Basis Diff. Ratio
Sign test on BLEU 331 75%
Bootstrap on BLEU 348 78%
Sign test on Fluency 224 50%
Sign test on Adequacy 225 51%
</table>
<figureCaption confidence="0.833354">
Figure 4: Number and ratio of statistically signifi-
</figureCaption>
<bodyText confidence="0.920996363636364">
cant distinction between system performance. Au-
tomatic scores are computed on a larger tested than
manual scores (3064 sentences vs. 300–400 sen-
tences).
collected manual judgements, we do not necessar-
ily have the same sentence judged for both systems
(judges evaluate 5 systems out of the 8–10 partici-
pating systems).
Still, for about good number of sentences, we do
have this direct comparison, which allows us to ap-
ply the sign test, as described in Section 2.2.
</bodyText>
<sectionHeader confidence="0.999468" genericHeader="evaluation">
4 Results and Analysis
</sectionHeader>
<bodyText confidence="0.999983">
The results of the manual and automatic evaluation
of the participating system translations is detailed in
the figures at the end of this paper. The scores and
confidence intervals are detailed first in the Figures
7–10 in table form (including ranks), and then in
graphical form in Figures 11–16. In the graphs, sys-
tem scores are indicated by a point, the confidence
intervals by shaded areas around the point.
In all figures, we present the per-sentence normal-
ized judgements. The normalization on a per-judge
basis gave very similar ranking, only slightly less
consistent with the ranking from the pairwise com-
parisons.
The confidence intervals are computed by boot-
strap resampling for BLEU, and by standard signif-
icance testing for the manual scores, as described
earlier in the paper.
Pairwise comparison is done using the sign test.
Often, two systems can not be distinguished with
a confidence of over 95%, so there are ranked the
same. This actually happens quite frequently (more
below), so that the rankings are broad estimates. For
instance: if 10 systems participate, and one system
does better than 3 others, worse then 2, and is not
significant different from the remaining 4, its rank is
in the interval 3–7.
</bodyText>
<table confidence="0.846954">
Domain BLEU Fluency Adequacy
in-domain 26.63 3.17 3.58
out-of-domain 20.37 2.74 3.08
</table>
<figureCaption confidence="0.789606">
Figure 5: Evaluation scores for in-domain and out-
of-domain test sets, averaged over all systems
</figureCaption>
<subsectionHeader confidence="0.998299">
4.1 Close results
</subsectionHeader>
<bodyText confidence="0.999888545454546">
At first glance, we quickly recognize that many sys-
tems are scored very similar, both in terms of man-
ual judgement and BLEU. There may be occasion-
ally a system clearly at the top or at the bottom, but
most systems are so close that it is hard to distin-
guish them.
In Figure 4, we displayed the number of system
comparisons, for which we concluded statistical sig-
nificance. For the automatic scoring method BLEU,
we can distinguish three quarters of the systems.
While the Bootstrap method is slightly more sensi-
tive, it is very much in line with the sign test on text
blocks.
For the manual scoring, we can distinguish only
half of the systems, both in terms of fluency and ad-
equacy. More judgements would have enabled us
to make better distinctions, but it is not clear what
the upper limit is. We can check, what the conse-
quences of less manual annotation of results would
have been: With half the number of manual judge-
ments, we can distinguish about 40% of the systems,
10% less.
</bodyText>
<subsectionHeader confidence="0.977905">
4.2 In-domain vs. out-of-domain
</subsectionHeader>
<bodyText confidence="0.999585857142857">
The test set included 2000 sentences from the
Europarl corpus, but also 1064 sentences out-of-
domain test data. Since the inclusion of out-of-
domain test data was a very late decision, the par-
ticipants were not informed of this. So, this was a
surprise element due to practical reasons, not mal-
ice.
All systems (except for Systran, which was not
tuned to Europarl) did considerably worse on out-
of-domain training data. This is demonstrated by
average scores over all systems, in terms of BLEU,
fluency and adequacy, as displayed in Figure 5.
The manual scores are averages over the raw un-
normalized scores.
</bodyText>
<page confidence="0.994432">
108
</page>
<table confidence="0.998059142857143">
Language Pair BLEU Fluency Adequacy
French-English 26.09 3.25 3.61
Spanish-English 28.18 3.19 3.71
German-English 21.17 2.87 3.10
English-French 28.33 2.86 3.16
English-Spanish 27.49 2.86 3.34
English-German 14.01 3.15 3.65
</table>
<figureCaption confidence="0.975670333333333">
Figure 6: Average scores for different language
pairs. Manual scoring is done by different judges,
resulting in a not very meaningful comparison.
</figureCaption>
<subsectionHeader confidence="0.998656">
4.3 Language pairs
</subsectionHeader>
<bodyText confidence="0.999933470588235">
It is well know that language pairs such as English-
German pose more challenges to machine transla-
tion systems than language pairs such as French-
English. Different sentence structure and rich target
language morphology are two reasons for this.
Again, we can compute average scores for all sys-
tems for the different language pairs (Figure 6). The
differences in difficulty are better reflected in the
BLEU scores than in the raw un-normalized man-
ual judgements. The easiest language pair according
to BLEU (English-French: 28.33) received worse
manual scores than the hardest (English-German:
14.01). This is because different judges focused on
different language pairs. Hence, the different av-
erages of manual scores for the different language
pairs reflect the behaviour of the judges, not the
quality of the systems on different language pairs.
</bodyText>
<subsectionHeader confidence="0.973209">
4.4 Manual judgement vs. BLEU
</subsectionHeader>
<bodyText confidence="0.999722884615384">
Given the closeness of most systems and the wide
over-lapping confidence intervals it is hard to make
strong statements about the correlation between hu-
man judgements and automatic scoring methods
such as BLEU.
We confirm the finding by Callison-Burch et al.
(2006) that the rule-based system of Systran is not
adequately appreciated by BLEU. In-domain Sys-
tran scores on this metric are lower than all statistical
systems, even the ones that have much worse human
scores. Surprisingly, this effect is much less obvious
for out-of-domain test data. For instance, for out-of-
domain English-French, Systran has the best BLEU
and manual scores.
Our suspicion is that BLEU is very sensitive to
jargon, to selecting exactly the right words, and
not synonyms that human judges may appreciate
as equally good. This is can not be the only ex-
planation, since the discrepancy still holds, for in-
stance, for out-of-domain French-English, where
Systran receives among the best adequacy and flu-
ency scores, but a worse BLEU score than all but
one statistical system.
This data set of manual judgements should pro-
vide a fruitful resource for research on better auto-
matic scoring methods.
</bodyText>
<subsectionHeader confidence="0.998447">
4.5 Best systems
</subsectionHeader>
<bodyText confidence="0.999992388888889">
So, who won the competition? The best answer
to this is: many research labs have very competi-
tive systems whose performance is hard to tell apart.
This is not completely surprising, since all systems
use very similar technology.
For some language pairs (such as German-
English) system performance is more divergent than
for others (such as English-French), at least as mea-
sured by BLEU.
The statistical systems seem to still lag be-
hind the commercial rule-based competition when
translating into morphological rich languages, as
demonstrated by the results for English-German and
English-French.
The predominate focus of building systems that
translate into English has ignored so far the difficult
issues of generating rich morphology which may not
be determined solely by local context.
</bodyText>
<subsectionHeader confidence="0.994085">
4.6 Comments on Manual Evaluation
</subsectionHeader>
<bodyText confidence="0.9999618">
This is the first time that we organized a large-scale
manual evaluation. While we used the standard met-
rics of the community, the we way presented trans-
lations and prompted for assessment differed from
other evaluation campaigns. For instance, in the
recent IWSLT evaluation, first fluency annotations
were solicited (while withholding the source sen-
tence), and then adequacy annotations.
Almost all annotators reported difficulties in
maintaining a consistent standard for fluency and ad-
equacy judgements, but nevertheless most did not
explicitly move towards a ranking-based evaluation.
Almost all annotators expressed their preference to
move to a ranking-based evaluation in the future. A
few pointed out that adequacy should be broken up
</bodyText>
<page confidence="0.996477">
109
</page>
<bodyText confidence="0.999821444444444">
into two criteria: (a) are all source words covered?
(b) does the translation have the same meaning, in-
cluding connotations?
Annotators suggested that long sentences are al-
most impossible to judge. Since all long sen-
tence translation are somewhat muddled, even a con-
trastive evaluation between systems was difficult. A
few annotators suggested to break up long sentences
into clauses and evaluate these separately.
Not every annotator was fluent in both the source
and the target language. While it is essential to be
fluent in the target language, it is not strictly nec-
essary to know the source language, if a reference
translation was given. However, ince we extracted
the test corpus automatically from web sources, the
reference translation was not always accurate — due
to sentence alignment errors, or because translators
did not adhere to a strict sentence-by-sentence trans-
lation (say, using pronouns when referring to enti-
ties mentioned in the previous sentence). Lack of
correct reference translations was pointed out as a
short-coming of our evaluation. One annotator sug-
gested that this was the case for as much as 10% of
our test sentences. Annotators argued for the impor-
tance of having correct and even multiple references.
It was also proposed to allow annotators to skip
sentences that they are unable to judge.
</bodyText>
<sectionHeader confidence="0.999493" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999956388888889">
We carried out an extensive manual and automatic
evaluation of machine translation performance on
European language pairs. While many systems had
similar performance, the results offer interesting in-
sights, especially about the relative performance of
statistical and rule-based systems.
Due to many similarly performing systems, we
are not able to draw strong conclusions on the ques-
tion of correlation of manual and automatic evalua-
tion metrics. The bias of automatic methods in favor
of statistical systems seems to be less pronounced on
out-of-domain test data.
The manual evaluation of scoring translation on
a graded scale from 1–5 seems to be very hard to
perform. Replacing this with an ranked evalua-
tion seems to be more suitable. Human judges also
pointed out difficulties with the evaluation of long
sentences.
</bodyText>
<sectionHeader confidence="0.997898" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<reference confidence="0.930064214285714">
The manual evaluation would not have been possible
without the contributions of the manual annotators:
Jesus Andres Ferrer, Abhishek Arun, Amittai Axel-
rod, Alexandra Birch, Chris Callison-Burch, Jorge
Civera, Marta Ruiz Costa-juss`a, Josep Maria Crego,
Elsa Cubel, Chris Irwin Davis, Loic Dugast, Chris
Dyer, Andreas Eisele, Cameron Fordyce, Jes´us
Gim´enez, Fabrizio Gotti, Hieu Hoang, Eric Joanis
Howard Johnson, Philipp Koehn, Beata Kouchnir,
Roland Kuhn, Elliott Macklovitch, Arul Menezes,
Marian Olteanu, Chris Quirk, Reinhard Rapp, Fatiha
Sadat, Joan Andreu S`anchez, Germ´an Sanchis,
Michel Simard, Ashish Venugopal, and Taro Watan-
abe.
</reference>
<bodyText confidence="0.985588333333333">
This work was supported in part under the GALE
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-06-C-0022.
</bodyText>
<sectionHeader confidence="0.967266" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.991999307692308">
Birch, A., Callison-Burch, C., Osborne, M., and
Koehn, P. (2006). Constraining the phrase-based,
joint probability statistical translation model. In
Proceedings on the Workshop on Statistical Ma-
chine Translation, pages 154–157, New York
City. Association for Computational Linguistics.
Callison-Burch, C., Osborne, M., and Koehn, P.
(2006). Re-evaluating the role of BLEU in ma-
chine translation research. In Proceedings of
EACL.
Collins, M., Koehn, P., and Kucerova, I. (2005).
Clause restructuring for statistical machine trans-
lation. In Proceedings ofACL.
Costa-juss`a, M. R., Crego, J. M., de Gispert, A.,
Lambert, P., Khalilov, M., Mari˜no, J. B., Fonol-
losa, J. A. R., and Banchs, R. (2006). Talp phrase-
based statistical translation system for european
language pairs. In Proceedings on the Workshop
on Statistical Machine Translation, pages 142–
145, New York City. Association for Computa-
tional Linguistics.
Crego, J. M., de Gispert, A., Lambert, P., Costa-
juss`a, M. R., Khalilov, M., Banchs, R., Mari˜no,
J. B., and Fonollosa, J. A. R. (2006). N-gram-
based smt system enhanced with reordering pat-
terns. In Proceedings on the Workshop on Statis-
</bodyText>
<page confidence="0.989979">
110
</page>
<bodyText confidence="0.996101898876404">
tical Machine Translation, pages 162–165, New
York City. Association for Computational Lin-
guistics.
Doddington, G. (2002). The NIST automated mea-
sure and its relation to IBM’s BLEU. In Proceed-
ings of LREC-2002 Workshopon Machine Trans-
lation Evaluation: Human Evaluators Meet Auto-
mated Metrics, Gran Canaria, Spain.
Eck, M. and Hori, C. (2005). Overview of the iwslt
2005 evaluation campaign. In Proc. of the Inter-
national Workshop on Spoken Language Transla-
tion.
Gim´enez, J. and M`arquez, L. (2006). The ldv-
combo system for smt. In Proceedings on
the Workshop on Statistical Machine Translation,
pages 166–169, New York City. Association for
Computational Linguistics.
Johnson, H., Sadat, F., Foster, G., Kuhn, R., Simard,
M., Joanis, E., and Larkin, S. (2006). Portage:
with smoothed phrase tables and segment choice
models. In Proceedings on the Workshop on
Statistical Machine Translation, pages 134–137,
New York City. Association for Computational
Linguistics.
Koehn, P. (2004). Statistical significance tests for
machine translation evaluation. In Lin, D. and
Wu, D., editors, Proceedings of EMNLP 2004,
pages 388–395, Barcelona, Spain. Association for
Computational Linguistics.
Koehn, P. and Monz, C. (2005). Shared task: Statis-
tical machine translation between European lan-
guages. In Proceedings of the ACL Workshop
on Building and Using Parallel Texts, pages 119–
124, Ann Arbor, Michigan. Association for Com-
putational Linguistics.
Li, A. (2005). Results of the 2005 NIST machine
translation evaluation. In Machine Translation
Workshop.
Menezes, A., Toutanova, K., and Quirk, C. (2006).
Microsoft research treelet translation system:
Naacl 2006 europarl evaluation. In Proceedings
on the Workshop on Statistical Machine Transla-
tion, pages 158–161, New York City. Association
for Computational Linguistics.
Olteanu, M., Davis, C., Volosen, I., and Moldovan,
D. (2006a). Phramer - an open source statisti-
cal phrase-based translator. In Proceedings on
the Workshop on Statistical Machine Translation,
pages 146–149, New York City. Association for
Computational Linguistics.
Olteanu, M., Suriyentrakorn, P., and Moldovan, D.
(2006b). Language models and reranking for ma-
chine translation. In Proceedings on the Workshop
on Statistical Machine Translation, pages 150–
153, New York City. Association for Computa-
tional Linguistics.
Patry, A., Gotti, F., and Langlais, P. (2006). Mood at
work: Ramses versus pharaoh. In Proceedings on
the Workshop on Statistical Machine Translation,
pages 126–129, New York City. Association for
Computational Linguistics.
Przybocki, M. (2004). NIST machine translation
2004 evaluation – summary of results. In Machine
Translation Evaluation Workshop.
Riezler, S. and Maxwell, J. T. (2005). On some pit-
falls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures
for Machine Translation and/or Summarization,
pages 57–64, Ann Arbor, Michigan. Association
for Computational Linguistics.
S´anchez, J. A. and Benedi, J. M. (2006). Stochas-
tic inversion transduction grammars for obtaining
word phrases for phrase-based statistical machine
translation. In Proceedings on the Workshop on
Statistical Machine Translation, pages 130–133,
New York City. Association for Computational
Linguistics.
Watanabe, T., Tsukada, H., and Isozaki, H. (2006).
Ntt system description for the wmt2006 shared
task. In Proceedings on the Workshop on Statis-
tical Machine Translation, pages 122–125, New
York City. Association for Computational Lin-
guistics.
Zollmann, A. and Venugopal, A. (2006). Syntax
augmented machine translation via chart parsing.
In Proceedings on the Workshop on Statistical
Machine Translation, pages 138–141, New York
City. Association for Computational Linguistics.
</bodyText>
<page confidence="0.997979">
111
</page>
<table confidence="0.921856305555556">
French-English (In Domain)
Adequacy (rank) Fluency (rank) BLEU (rank)
upc-jmc +0.19±0.08 (1-7) +0.09±0.08 (1-8) 30.42±0.86 (1-6)
lcc +0.14±0.07 (1-6) +0.13±0.06 (1-7) 30.81±0.85 (1-4)
utd +0.13±0.08 (1-7) +0.14±0.07 (1-6) 30.53±0.87 (2-7)
upc-mr +0.13±0.08 (1-8) +0.13±0.07 (1-6) 30.33±0.88 (1-7)
nrc +0.12±0.10 (1-7) +0.06±0.11 (2-6) 29.62±0.84 (8)
ntt +0.11±0.08 (1-8) +0.14±0.08 (2-8) 30.72±0.87 (1-7)
cmu +0.10±0.08 (3-7) +0.05±0.07 (4-8) 30.18±0.80 (2-7)
rali -0.02±0.08 (5-8) +0.00±0.08 (3-9) 30.39±0.91 (3-7)
systran -0.08±0.09 (9) -0.17±0.09 (8-9) 21.44±0.65 (10)
upv -0.76±0.09 (10) -0.52±0.09 (10) 24.10±0.89 (9)
Spanish-English (In Domain)
Adequacy (rank) Fluency (rank) BLEU (rank)
upc-jmc +0.15±0.08 (1-7) +0.18±0.08 (1-6) 31.01±0.97 (1-5)
ntt +0.10±0.08 (1-7) +0.10±0.08 (1-8) 31.29±0.88 (1-5)
lcc +0.08±0.07 (1-8) +0.04±0.06 (2-8) 31.46±0.87 (1-4)
utd +0.08±0.06 (1-8) +0.08±0.07 (2-7) 31.10±0.89 (1-5)
nrc +0.06±0.10 (2-8) +0.08±0.07 (1-9) 30.04±0.79 (6)
upc-mr +0.06±0.07 (1-8) +0.08±0.07 (1-6) 29.43±0.83 (7)
uedin-birch +0.03±0.11 (1-8) -0.07±0.15 (2-10) 29.01±0.81 (8)
rali +0.00±0.07 (3-9) -0.02±0.07 (3-9) 30.80±0.87 (2-5)
upc-jg -0.10±0.07 (7-9) -0.11±0.07 (6-9) 28.03±0.83 (9)
upv -0.45±0.10 (10) -0.41±0.10 (9-10) 23.91±0.83 (10)
German-English (In Domain)
Adequacy (rank) Fluency (rank) BLEU (rank)
uedin-phi +0.30±0.09 (1-2) +0.33±0.08 (1) 27.30±0.86 (1)
lcc +0.15±0.07 (2-7) +0.12±0.07 (2-7) 25.97±0.81 (2)
nrc +0.12±0.07 (2-7) +0.14±0.07 (2-6) 24.54±0.80 (5-7)
utd +0.08±0.07 (3-7) +0.01±0.08 (2-8) 25.44±0.85 (3-4)
ntt +0.07±0.08 (2-9) +0.06±0.09 (2-8) 25.64±0.83 (3-4)
upc-mr +0.00±0.09 (3-9) -0.21±0.09 (6-9) 23.68±0.79 (8)
rali -0.01±0.06 (4-9) +0.00±0.07 (3-9) 24.60±0.80 (5-7)
upc-jmc -0.02±0.09 (2-9) -0.04±0.09 (3-9) 24.43±0.86 (5-7)
systran -0.05±0.10 (3-9) -0.05±0.09 (3-9) 15.86±0.59 (10)
upv -0.55±0.09 (10) -0.38±0.08 (10) 18.08±0.77 (9)
</table>
<figureCaption confidence="0.999121">
Figure 7: Evaluation of translation to English on in-domain test data
</figureCaption>
<page confidence="0.971858">
112
</page>
<table confidence="0.807046666666667">
English-French (In Domain)
Adequacy (rank) Fluency (rank) BLEU (rank)
nrc +0.08±0.09 (1-5) +0.09±0.09 (1-5) 31.75±0.83 (1-6)
upc-mr +0.08±0.08 (1-4) +0.04±0.07 (1-5) 31.50±0.76 (1-6)
upc-jmc +0.03±0.09 (1-6) +0.02±0.08 (1-6) 31.75±0.78 (1-5)
systran -0.01±0.12 (2-7) +0.06±0.12 (1-6) 25.07±0.71 (7)
utd -0.03±0.07 (3-7) -0.05±0.07 (3-7) 31.42±0.85 (3-6)
rali -0.08±0.09 (1-7) -0.09±0.09 (2-7) 31.79±0.85 (1-6)
ntt -0.09±0.09 (4-7) -0.06±0.08 (4-7) 31.92±0.84 (1-5)
English-Spanish (In Domain)
Adequacy (rank) Fluency (rank) BLEU (rank)
ms +0.23±0.09 (1-5) +0.13±0.09 (1-7) 29.76±0.82 (7-8)
upc-mr +0.20±0.09 (1-4) +0.17±0.09 (1-5) 31.06±0.86 (1-4)
utd +0.18±0.08 (1-5) +0.15±0.08 (1-6) 30.73±0.90 (1-4)
nrc +0.12±0.09 (2-7) +0.17±0.08 (1-6) 29.97±0.86 (5-6)
ntt +0.10±0.09 (3-7) +0.14±0.08 (1-6) 30.93±0.85 (1-4)
upc-jmc +0.04±0.10 (2-7) +0.01±0.08 (2-7) 30.44±0.86 (1-4)
rali -0.05±0.08 (5-8) -0.03±0.08 (6-8) 29.38±0.85 (5-6)
uedin-birch -0.18±0.14 (6-9) -0.17±0.13 (6-10) 28.49±0.87 (7-8)
upc-jg -0.32±0.11 (9) -0.37±0.09 (8-10) 27.46±0.78 (9)
upv -0.83±0.15 (9-10) -0.59±0.15 (8-10) 23.17±0.73 (10)
English-German (In Domain)
Adequacy (rank) Fluency (rank) BLEU (rank)
upc-mr +0.28±0.08 (1-3) +0.14±0.08 (1-5) 17.24±0.81 (3-5)
ntt +0.19±0.08 (1-5) +0.09±0.06 (2-6) 18.15±0.89 (1-3)
upc-jmc +0.17±0.08 (1-5) +0.13±0.08 (1-4) 17.73±0.81 (1-3)
nrc +0.17±0.08 (2-4) +0.11±0.08 (1-5) 17.52±0.78 (4-5)
rali +0.08±0.10 (3-6) +0.03±0.09 (2-6) 17.93±0.85 (1-4)
systran -0.08±0.11 (5-6) +0.00±0.10 (3-6) 9.84±0.52 (7)
upv -0.84±0.12 (7) -0.51±0.10 (7) 13.37±0.78 (6)
</table>
<figureCaption confidence="0.998111">
Figure 8: Evaluation of translation from English on in-domain test data
</figureCaption>
<page confidence="0.995396">
113
</page>
<table confidence="0.936002666666667">
French-English (Out of Domain)
Adequacy (rank) Fluency (rank) BLEU (rank)
upc-jmc +0.23±0.09 (1-5) +0.13±0.11 (1-8) 21.79±0.92 (1-4)
cmu +0.22±0.11 (1-8) +0.13±0.09 (1-9) 21.15±0.86 (4-7)
systran +0.19±0.15 (1-8) +0.15±0.14 (1-7) 19.42±0.82 (9)
lcc +0.13±0.12 (1-9) +0.11±0.11 (1-9) 21.77±0.88 (1-5)
upc-mr +0.12±0.12 (2-8) +0.11±0.10 (1-7) 21.95±0.94 (1-3)
utd +0.04±0.10 (1-9) +0.01±0.10 (1-8) 21.39±0.94 (3-7)
ntt -0.02±0.12 (3-9) +0.08±0.11 (1-9) 21.34±0.85 (3-7)
nrc -0.03±0.14 (3-8) +0.00±0.11 (3-9) 21.15±0.86 (3-7)
rali -0.09±0.12 (4-9) -0.10±0.11 (5-9) 20.17±0.85 (8)
upv -0.76±0.16 (10) -0.58±0.14 (10) 15.55±0.79 (10)
</table>
<subsectionHeader confidence="0.248572">
Spanish-English (Out of Domain)
</subsectionHeader>
<bodyText confidence="0.363172454545455">
Adequacy (rank) Fluency (rank) BLEU (rank)
upc-jmc +0.28±0.10 (1-2) +0.17±0.10 (1-6) 27.92±0.94 (1-3)
uedin-birch +0.25±0.16 (1-7) +0.18±0.19 (1-6) 25.20±0.91 (5-8)
nrc +0.18±0.16 (2-8) +0.09±0.09 (1-8) 25.40±0.94 (5-7)
ntt +0.11±0.10 (2-7) +0.17±0.10 (2-6) 26.85±0.89 (3-4)
upc-mr +0.08±0.11 (2-8) +0.10±0.10 (1-7) 25.62±0.87 (5-8)
lcc +0.04±0.10 (4-9) +0.07±0.11 (3-7) 27.18±0.92 (1-4)
utd +0.03±0.11 (2-9) +0.03±0.10 (2-8) 27.41±0.96 (1-3)
upc-jg -0.09±0.11 (4-9) -0.09±0.09 (7-9) 23.42±0.87 (9)
rali -0.09±0.11 (4-9) -0.15±0.11 (6-9) 25.03±0.91 (6-8)
upv -0.63±0.14 (10) -0.47±0.11 (10) 19.17±0.78 (10)
</bodyText>
<subsectionHeader confidence="0.461214">
German-English (Out of Domain)
</subsectionHeader>
<bodyText confidence="0.717500909090909">
Adequacy (rank) Fluency (rank) BLEU (rank)
systran +0.30±0.12 (1-4) +0.21±0.12 (1-4) 15.56±0.71 (7-9)
uedin-phi +0.22±0.09 (1-6) +0.21±0.10 (1-7) 18.87±0.84 (1)
lcc +0.18±0.10 (1-6) +0.20±0.10 (1-7) 17.96±0.79 (2-3)
utd +0.08±0.09 (2-7) +0.07±0.08 (2-6) 16.97±0.76 (4-6)
ntt +0.07±0.12 (1-9) +0.21±0.13 (1-7) 17.37±0.76 (3-5)
nrc +0.04±0.10 (3-8) +0.04±0.09 (2-8) 15.93±0.76 (7-8)
upc-mr +0.02±0.10 (4-8) -0.11±0.09 (6-8) 16.89±0.79 (4-6)
upc-jmc -0.01±0.10 (4-8) -0.04±0.11 (3-9) 17.57±0.80 (2-5)
rali -0.14±0.08 (8-9) -0.14±0.08 (8-9) 15.22±0.69 (8-9)
upv -0.64±0.11 (10) -0.54±0.09 (10) 11.78±0.71 (10)
</bodyText>
<figureCaption confidence="0.998723">
Figure 9: Evaluation of translation to English on out-of-domain test data
</figureCaption>
<page confidence="0.989199">
114
</page>
<table confidence="0.8791054">
English-French (Out of Domain)
Adequacy (rank) Fluency (rank) BLEU (rank)
systran +0.50±0.20 (1) +0.41±0.18 (1) 25.31±0.88 (1)
upc-jmc +0.09±0.11 (2-5) +0.09±0.11 (2-4) 23.30±0.75 (2-6)
upc-mr +0.09±0.11 (2-4) +0.04±0.09 (2-4) 23.21±0.75 (2-6)
utd -0.02±0.11 (2-6) -0.05±0.09 (2-6) 22.79±0.86 (7)
rali -0.12±0.12 (4-7) -0.17±0.12 (5-7) 23.34±0.89 (2-6)
nrc -0.13±0.13 (4-7) -0.16±0.10 (4-7) 23.66±0.91 (2-5)
ntt -0.23±0.12 (4-7) -0.06±0.10 (4-7) 22.99±0.96 (3-6)
English-Spanish (Out of Domain)
Adequacy (rank) Fluency (rank) BLEU (rank)
upc-mr +0.35±0.11 (1-3) +0.19±0.10 (1-6) 26.62±0.92 (1-2)
ms +0.33±0.16 (1-7) +0.15±0.13 (1-8) 26.15±0.88 (6-7)
utd +0.21±0.13 (2-6) +0.13±0.11 (1-7) 25.26±0.78 (3-5)
nrc +0.18±0.12 (1-6) +0.07±0.11 (2-7) 25.58±0.85 (3-5)
upc-jmc +0.17±0.15 (2-7) +0.24±0.12 (1-6) 25.59±0.95 (3-5)
ntt +0.12±0.13 (2-7) +0.12±0.13 (1-7) 26.52±0.90 (1-2)
rali -0.17±0.16 (6-8) -0.05±0.13 (4-8) 24.03±0.83 (6-8)
uedin-birch -0.36±0.24 (6-10) -0.16±0.16 (5-9) 23.18±0.88 (7-8)
upc-jg -0.45±0.13 (8-9) -0.42±0.10 (9-10) 22.04±0.84 (9)
upv -1.09±0.21 (9) -0.64±0.19 (8-9) 16.83±0.72 (10)
English-German (Out of Domain)
Adequacy (rank) Fluency (rank) BLEU (rank)
systran +0.47±0.15 (1) +0.39±0.15 (1-2) 10.78±0.69 (1-6)
upc-mr +0.31±0.13 (2-3) +0.21±0.11 (1-3) 10.96±0.70 (1-5)
upc-jmc +0.22±0.14 (2-3) +0.01±0.10 (3-6) 10.64±0.66 (1-6)
rali +0.13±0.12 (4-6) -0.06±0.10 (4-6) 10.57±0.65 (1-6)
nrc +0.00±0.11 (4-6) +0.05±0.09 (2-6) 10.64±0.65 (2-6)
ntt -0.03±0.12 (4-6) +0.08±0.11 (3-5) 10.51±0.64 (1-6)
upv -0.94±0.13 (7) -0.57±0.10 (7) 6.55±0.53 (7)
</table>
<figureCaption confidence="0.939218">
Figure 10: Evaluation of translation from English on out-of-domain test data
</figureCaption>
<page confidence="0.949378">
115
</page>
<figure confidence="0.999348698924731">
French-English
In domain Out of Domain
Adequacy
Adequacy
0.3
0.3
cmu• •upc-jmc
systran•
lcc&amp;•upc-mr
utd• •
0.2
0.2
0.1
0.1
nrc••ntt
-0.0
-0.0
rali•
-0.1
-0.1
-0.2
-0.2
-0.3
-0.3
-0.4
-0.4
-0.5
-0.5
-0.6
-0.6
-0.7
-0.7
•upv
-0.8
BLEU
-0.8
21 22 23 24 25 26 27 28 29 30 31
15 16 17 18 19 20 21 22
•upv
BLEU
•systran
upc-
ntt
•
rali
upc-jmc
•.utd
cc
nrc••l
mr&amp;• •
cmu% • •-
Fluency
systran•
0.2
0.1
-0.0
-0.1
-0.2
-0.3
-0.4
•upv
BLEU
-0.5
•systran
•upv
BLEU
upc -jmc
cmu•••upc-mr
•-lcc
nrc•
ntt• •utd
rali•
Fluency
0.2
0.1
-0.0
-0.1
-0.2
-0.3
-0.4
-0.5
-0.6
mr&amp;.u
upc-jmc• • • nt
•-l
nrc% • •cmu
td
t
cc
upc-
•
rali
21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22
</figure>
<figureCaption confidence="0.985111">
Figure 11: Correlation between manual and automatic scores for French-English
</figureCaption>
<page confidence="0.955906">
116
</page>
<figure confidence="0.792551">
Spanish-English
</figure>
<figureCaption confidence="0.807984">
Figure 12: Correlation between manual and automatic scores for Spanish-English
</figureCaption>
<figure confidence="0.999521020408163">
-0.3
-0.4
0.2
0.1
-0.0
-0.1
-0.2
-0.3
-0.4
-0.5
•upv
BLEU
-0.4
•upv
-0.3
In Domain
c-uedin-bi
rch•n•rc
•upc-jg
Adequacy
0.3
0.2
0.1
-0.0
-0.1
-0.2
Out of Domain
•upc-jmc
uedin-birch•
•nrc
•ntt
lcc••utd
upc-jg•
Adequacy
upc-jmc
•
ntt•
•�utd
•lcc
•
rali
•
upc-mr•
•rali
BLEU
-0.7
-0.5
-0.6
•upv
23 24 25 26 27 28 29 30 31 32
19 20 21 22 23 24 25 26 27 28
upc-mr• i c.ntt
utd// •lcc
•
•rali
upc-jg•
Fluency
0.2
0.1
-0.0
-0.1
-0.2
ntt
uedin-birch• • •upc-jmc
upc-mr
•lcc
•utd
•upc-jg
•rali
Fluency
0.2
0.1
-0.0
-0.1
-0.2
-0.3
-0.4
•upc-jmc
•
uedin-birch
nrc••
-0.5
BLEU
-0.5
•upv
BLEU
23 24 25 26 27 28 29 30 31 32
19 20 21 22 23 24 25 26 27 28
117
In Domain
Out of Domain
Adequacy
Adequacy
German-English
15 16 17 18 19 20 21 22 23 24 25 26 27
0.4
0.3
0.2
0.1
-0.0
-0.1
-0.2
-0.3
-0.4
-0.5
-0.6
uedin-phi•
lcc
•
utd••ntt
upc-mr• ••rali
upc-jmc
BLEU
nrc•
•systran
•upv
Fluency
BLEU
uedin-phi•
nrc•
. \10 •
upc jmc•
•ula
•upc-mr
•lcc
15 16 17 18 19 20 21 22 23 24 25 26 27
0.4
0.3
0.2
0.1
-0.0
-0.1
-0.2
-0.3
-0.4
•systran
•upv
•uedin-phi
upc-mr/ •upc -jmc
•rali
•systran
lcc•
utd�•
nrc• •ntt
-0.3
-0.4
-0.5
-0.6
•upv
BLEU
12 13 14 15 16 17 18 19 20
0.4
0.3
0.2
0.1
-0.0
-0.1
-0.2
Fluency
uedin-phi
systran• ntt•
•�lcc
•
•utd
•upc-jmc
•upc-mr
0.4
nrc•
•rali
-0.3
-0.4
-0.5
•upv
BLEU
12 13 14 15 16 17 18 19 20
0.3
0.2
0.1
-0.0
-0.1
-0.2
English-French
In Domain Out of Domain
Adequacy
Adequacy
BLEU
upc-mr••nrc .
ud *upc-jmc
raliftntt
0.2
0.1
0.0
-0.1
25 26 27 28 29 30 31 32
-0.2
-0.3
•systran
•
ntt
BLEU
systran•
upc-mr •upc-jmc
utd•
rali••nrc
0.5
0.4
0.3
0.2
0.1
0.0
-0.1
-0.2
-0.3
20 21 22 23 24 25 26
Fluency
Fluency
BLEU
•systran
•nrc
upc-mr• •upc-jmc
utd•••ntt
rali
25 26 27 28 29 30 31 32
0.2
0.1
0.0
-0.1
-0.2
-0.3
BLEU
systran•
c-
upc-mr••u me p �
utd••ntt
rali••nrc
20 21 22 23 24 25 26
0.5
0.4
0.3
0.2
0.1
0.0
-0.1
-0.2
-0.3
</figure>
<figureCaption confidence="0.968702">
Figure 14: Correlation between manual and automatic scores for English-French
</figureCaption>
<page confidence="0.848601">
119
</page>
<figure confidence="0.99977847">
In Domain
Out of Domain
•upv
BLEU
Adequacy
BLEU
-0.9
0.3
0.2
0.1
-0.0
-0.1
-0.2
-0.3
-0.4
-0.5
-0.6
-0.7
-0.8
•upv
23 24 25 26 27 28 29 30 31 32
upc-jg•
•upc-mr
•utd
•upc-jmc
•uedin-birch
rali•
ms•
nrc•
•ntt
•rali
•uedin-birch
upc-jg•
16 17 18 19 20 21 22 23 24 25 26 27
Adequacy
•upc-mr
ms•
utd`,•&apos;/nrc
upc-jmc • •ntt
0.4
0.3
0.2
0.1
-0.0
-0.1
-0.2
-0.3
-0.4
-0.5
-0.6
-0.7
-0.8
-0.9
-1.0
-1.1
English-Spanish
Fluency
upc-jmc• •upc-mr
•ntt
•nrc
•rali
•uedin-birch
-0.2
-0.3
upc-jg•
-0.5
•upv
BLEU
16 17 18 19 20 21 22 23 24 25 26 27
-0.4
utd•ms•
nr
ms•
•
rali
uedin-birch•
upc-jg•
Fluency
-0.4
•upc-mr
utd
•upc-jmc
-0.5
-0.6
•upv
BLEU
23 24 25 26 27 28 29 30 31 32
0.2
0.1
-0.0
-0.1
-0.2
-0.3
0.3
0.2
0.1
-0.0
-0.1
-0.6
-0.7
</figure>
<figureCaption confidence="0.98086">
Figure 15: Correlation between manual and automatic scores for English-Spanish
</figureCaption>
<page confidence="0.579967">
120
</page>
<figure confidence="0.999630202380952">
English-German
In Domain Out of Domain
Adequacy
Adequacy
0.3
0.2
0.1
-0.0
-0.1
-0.2
-0.3
-0.4
-0.5
-0.6
-0.7
-0.8
•upv
-0.2
-0.3
-0.4
-0.5
-0.6
-0.7
-0.8
-0.9
•upv
0.5
0.4
•systran
•upc-mr
upc-jmc�
nrc• •
•rali
0.3
•ntt
0.2
0.1
-0.0
-0.1
•systran
•upc-mr
upc-jmc•
rali•
ntt••nrc
BLEU
BLEU -1.0
-0.9
9 10 11 12 13 14 15 16 17 18 19
6 7 8 9 10 11
Fluency
0.2
0.1
-0.0
-0.1
-0.2
-0.3
-0.4
•upv
BLEU
-0.5
•upv
BLEU
•systran
•upc-mr
ntt •nrc
rali• • upc-jmc
Fluency
0.4
0.3
0.2
0.1
-0.0
-0.1
-0.2
-0.3
-0.4
-0.5
-0.6
•systran
upc-mr••upc-jmc
•ntt
rali•
nrc•
9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11
</figure>
<figureCaption confidence="0.995998">
Figure 16: Correlation between manual and automatic scores for English-German
</figureCaption>
<page confidence="0.993095">
121
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.990219">Manual and Automatic Evaluation of Machine between European Languages</title>
<author confidence="0.98858">Philipp</author>
<affiliation confidence="0.9988525">School of University of</affiliation>
<email confidence="0.986386">pkoehn@inf.ed.ac.uk</email>
<abstract confidence="0.999791606060606">We evaluated machine translation performance for six European language pairs that participated in a shared task: translating French, German, Spanish texts to English and back. Evaluation was done autousing the and manon For the 2006 NAACL/HLT Workshop on Machine Translation, we organized a shared task to evaluate machine translation performance. 14 teams from 11 institutions participated, ranging from commercial companies, industrial research labs to individual graduate students. The motivation for such a competition is to establish baseline performance numbers for defined training scenarios and test sets. We assembled various forms of data and resources: a baseline MT system, language models, prepared training and test sets, resulting in actual machine translation output from several state-of-the-art systems and manual evalua- All this is available at the workshop The shared task is a follow-up to the one we organized in the previous year, at a similar venue (Koehn and Monz, 2005). As then, we concentrated on the translation of European languages and the use of the Europarl corpus for training. Again, most systems that participated could be categorized as statistical phrase-based systems. While there is now a number of competitions — DARPA/NIST (Li, 2005), IWSLT (Eck and Hori, 2005), TC-Star — this one focuses on text translation between various European languages.</abstract>
<note confidence="0.413604">This year’s shared task changed in some aspects from last year’s:</note>
<title confidence="0.846551">We carried out a manual evaluation in addition to the automatic scoring. Manual evaluation</title>
<author confidence="0.996899">Christof Monz</author>
<affiliation confidence="0.9976615">Department of Computer Science Queen Mary, University of London</affiliation>
<email confidence="0.815238">christof@dcs.qmul.ac.uk</email>
<abstract confidence="0.999194393939394">was done by the participants. This revealed interesting clues about the properties of automatic and manual scoring. We evaluated translation in adto English was again paired with German, French, and Spanish. We dropped, however, one of the languages, Finnish, partly to keep the number of tracks manageable, partly because we assumed that it would be hard to find enough Finnish speakers for the manual evaluation. • We included an out-of-domain test set. This allows us to compare machine translation performance in-domain and out-of-domain. 1 Evaluation Framework The evaluation framework for the shared task is similar to the one used in last year’s shared task. Training and testing is based on the Europarl corpus. Figure 1 provides some statistics about this corpus. 1.1 Baseline system To lower the barrier of entrance to the competition, we provided a complete baseline MT system, along with data resources. To summarize, we provided: • sentence-aligned, tokenized training corpus • a development and development test set • trained language models for each language • the phrase-based MT decoder Pharaoh • a training script to build models for Pharaoh The performance of the baseline system is similar to the best submissions in last year’s shared task. We are currently working on a complete open source implementation of a training and decoding system, which should become available over the summer.</abstract>
<note confidence="0.909055222222222">102 of the Workshop on Statistical Machine pages 102–121, York City, June 2006. Association for Computational Linguistics Training corpus Sentences 730,740 688,031 751,088 Foreign words 15,676,710 15,323,737 15,256,793 English words 15,222,105 13,808,104 16,052,269 Distinct foreign words 102,886 80,349 195,291 Distinct English words 64,123 61,627 65,889</note>
<title confidence="0.87321">Language model data</title>
<author confidence="0.943807">English Spanish French German</author>
<note confidence="0.4023875">Sentence 1,003,349 1,070,305 1,066,974 1,078,141 Words 27,493,499 29,129,720 31,604,879 26,562,167</note>
<title confidence="0.647708">In-domain test set</title>
<author confidence="0.754866">English Spanish French German</author>
<note confidence="0.969530833333333">Sentences 2,000 Words 59,307 61,824 66,783 55,533 Unseen words 141 206 164 387 Ratio of unseen words 0.23% 0.40% 0.24% 0.70% Distinct words 6,031 7,719 7,230 8,812 Distinct unseen words 139 203 163 385</note>
<title confidence="0.898189">Out-of-domain test set</title>
<author confidence="0.906393">English Spanish French German</author>
<note confidence="0.959773142857143">Sentences 1,064 Words 25,919 29,826 31,937 26,818 Unseen words 464 368 839 913 Ratio of unseen words 1.79% 1.23% 2.62% 3.40% Distinct words 5,166 5,689 5,728 6,594 Distinct unseen words 340 267 375 637 Figure 1: Properties of the training and test sets used in the shared task. The training data is the Europarl cor-</note>
<abstract confidence="0.96350978508772">pus, from which also the in-domain test set is taken. There is twice as much language modelling data, since training data for the machine translation system is filtered against sentences of length larger than 40 words. Out-of-domain test data is from the Project Syndicate web site, a compendium of political commentary. 103 ID Participant cmu Carnegie Mellon University, USA (Zollmann and Venugopal, 2006) lcc Language Computer Corporation, USA (Olteanu et al., 2006b) ms Microsoft, USA (Menezes et al., 2006) nrc National Research Council, Canada (Johnson et al., 2006) ntt Nippon Telegraph and Telephone, Japan (Watanabe et al., 2006) rali RALI, University of Montreal, Canada (Patry et al., 2006) systran Systran, France uedin-birch University of Edinburgh, UK — Alexandra Birch (Birch et al., 2006) uedin-phi University of Edinburgh, UK — Philipp Koehn (Birch et al., 2006) upc-jg University of Catalonia, Spain — Jes´us Gim´enez (Gim´enez and M`arquez, 2006) upc-jmc University of Catalonia, Spain — Josep Maria Crego (Crego et al., 2006) upc-mr University of Catalonia, Spain — Marta Ruiz Costa-juss`a (Costa-juss`a et al., 2006) upv University of Valencia, Spain (S´anchez and Benedi, 2006) utd University of Texas at Dallas, USA (Olteanu et al., 2006a) Figure 2: Participants in the shared task. Not all groups participated in all translation directions. 1.2 Test Data The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000, which is excluded from the training data. Participants were also provided with two sets of 2,000 sentences of parallel text to be used for system development and tuning. In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate webwhich are published in all the four languages of the shared task. We aligned the texts at a sentence level across all four languages, resulting in 1064 sentence per language. For statistics on this test set, refer to Figure 1. The out-of-domain test set differs from the Europarl data in various ways. The text type are editorials instead of speech transcripts. The domain is general politics, economics and science. However, it is also mostly political content (even if not focused on the internal workings of the European Union) and opinion. 1.3 Participants We received submissions from 14 groups from 11 institutions, as listed in Figure 2. Most of these groups follow a phrase-based statistical approach to translation. Microsoft’s approach uses dependency trees, others use hierarchical phrase models. Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus. About half of the participants of last year’s shared task participated again. The other half was replaced by other participants, so we ended up with roughly the same number. Compared to last year’s shared task, the participants represent more long-term research efforts. This may be the sign of a maturing research environment. While building a machine translation system is a serious undertaking, in future we hope to attract more newcomers to the field by keeping the barrier of entry as low as possible. For more on the participating systems, please refer to the respective system description in the proceedings of the workshop. 2 Automatic Evaluation the automatic evaluation, we used since it the most established metric in the field. The metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use. It rewards matches of n-gram sequences, but measures only at most indirectly overall grammatical coherence. has been shown to correlate with human judgement, when statistical ma- 104 chine translation systems are compared (Doddington, 2002; Przybocki, 2004; Li, 2005). However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong. They demonstrated this with the comparison of statistical systems against (a) manually post-edited MT output, and (b) a rule-based commercial system. The development of automatic scoring methods is an open field of research. It was our hope that this competition, which included the manual and automatic evaluation of statistical systems and one rulebased commercial system, will give further insight into the relation between automatic and manual evaluation. At the very least, we are creating a data resource (the manual annotations) that may the basis of future research in evaluation metrics. Computing computed for each submission with a single reference translation. For each sentence, we counted how many n-grams in the system output also occurred in the reference translation. By taking the ratio of matching n-grams to the total number of n-grams in the system output, we obtain the precifor each n-gram order These values for precision are combined into a 4 X formula for the also includes a brevity penalty for too short output, which is based the total number of words in the system output in the reference sensitive to tokenization. Because of this, we retokenized and lowercased submitted output with our own tokenizer, which was also used to prepare the training and test data. 2.2 Statistical Significance Interval: are not computed on the sentence level, traditional methods to compute statistical significance and confidence intervals do not apply. Hence, we use the bootstrap resampling method described by Koehn (2004). Following this method, we repeatedly — say, 1000 times — sample sets of sentences from the outof each system, measure their and these 1000 as basis for estimating a confidence interval. When dropping the top and 2.5% the remaining define the range of the confidence interval. comparison: can use the same method to assess the statistical significance of one system outperforming another. If two systems’ scores are close, this may simply be a random effect in the test data. To check for this, we do pairwise bootstrap resampling: Again, we repeatedly sample sets of sentences, this time from both systems, and compare on these sets. If one system is better in 95% of the sample sets, we conclude that its is statistically significantly better. The bootstrap method has been critized by Riezler and Maxwell (2005) and Collins et al. (2005), as being too optimistic in deciding for statistical significant difference between systems. We are therefore applying a different method, which has been used at the 2005 DARPA/NIST evaluation. We divide up each test set into blocks of 20 sentences (100 blocks for the in-domain test set, 53 blocks for the out-of-domain test set), check for each if one system has a higher than the other, and then use the sign test. The sign test checks, how likely a sample of better worse would have been generated by two systems of equal performance. Let say, if we find one system doing better on 20 of the blocks, and worse on 80 of the blocks, is it significantly worse? We check, how likely only up 20 scores out of 100 have been generated by two equal systems, using the binomial distribution: n or then we have a statistically significant difference between the systems. = i p (3) 105 3: Annotation tool for manual judgement of the system output. Translations from 5 randomly selected systems for a randomly selected sentence is presented. No additional information beyond the instructions on this page are given to the judges. The tool tracks and reports annotation speed. 3 Manual Evaluation While automatic measures are an invaluable tool for the day-to-day development of machine translation systems, they are only a imperfect substitute for human assessment of translation quality, or as acronym it, a bilingual evaluation Many human evaluation metrics have been proposed. Also, the argument has been made that machine translation performance should be evaluated via task-based evaluation metrics, i.e. how much it assists performing a useful task, such as supporting human translators or aiding the analysis of texts. The main disadvantage of manual evaluation is that it is time-consuming and thus too expensive to do frequently. In this shared task, we were also confronted with this problem, and since we had no funding for paying human judgements, we asked participants in the evaluation to share the burden. Participants and other volunteers contributed about 180 hours of labor in the manual evaluation. 3.1 Collecting Human Judgements We asked participants to each judge 200–300 sentences in terms of fluency and adequacy, the most commonly used manual evaluation metrics. We settled on contrastive evaluations of 5 system outputs for a single test sentence. See Figure 3 for a screenshot of the evaluation tool. Presenting the output of several system allows the human judge to make more informed judgements, contrasting the quality of the different systems. The judgements tend to be done more in form of a ranking of the different systems. We assumed that such a contrastive assessment would be beneficial for an evaluation that essentially pits different systems against each other. While we had up to 11 submissions for a translation direction, we did decide against presenting all 11 system outputs to the human judge. Our initial experimentation with the evaluation tool showed that this is often too overwhelming. Making the ten judgements (2 types for 5 systems) takes on average 2 minutes. Typically, judges 106 initially spent about 3 minutes per sentence, but then accelerate with experience. Judges where excluded from assessing the quality of MT systems that were submitted by their institution. Sentences and systems were randomly selected and randomly shuffled for presentation. We collected around 300–400 judgements per judgement type (adequacy or fluency), per system, per language pair. This is less than the 694 judgements 2004 DARPA/NIST evaluation, or the 532 judgements in the 2005 DARPA/NIST evaluation. This decreases the statistical significance of our results compared to those studies. The number of judgements is additionally fragmented by our breakup of sentences into in-domain and out-of-domain. 3.2 Normalizing the judgements The human judges were presented with the followdefinition of but no additional instructions: Adequacy Fluency</abstract>
<address confidence="0.767194333333333">5 All Meaning Flawless English 4 Most Meaning Good English 3 Much Meaning Non-native English</address>
<abstract confidence="0.984960547619047">2 Little Meaning Disfluent English 1 None Incomprehensible Judges varied in the average score they handed out. The average fluency judgement per judge ranged from 2.33 to 3.67, the average adequacy judgement ranged from 2.56 to 4.13. Since different judges judged different systems (recall that judges were excluded to judge system output from their own institution), we normalized the scores. judgement per judge the raw judgement plus (3 minus average raw judgement for this judge). In words, the judgements are normalso that the average judgement per 3. Another way to view the judgements is that they are less quality judgements of machine translation systems per se, but rankings of machine translation systems. In fact, it is very difficult to maintain consistent standards, on what (say) an adequacy judgement of 3 means even for a specific language pair. The way judgements are collected, human judges tend to use the scores to rank systems against each other. If one system is perfect, another has slight flaws and the third more flaws, a judge is inclined to hand out judgements of 5, 4, and 3. On the other hand, when all systems produce muddled output, but one is better, and one is worse, but not completely wrong, a judge is inclined to hand out judgements of 4, 3, and 2. The judgement of 4 in the first case will go to a vastly better system output than in the second case. We therefore also normalized judgements on a basis. The judgement per the raw judgement plus (0 minus average raw judgement for this judge on this sentence). Systems that generally do better than others will a positive average per Systems that generally do worse than others will receive a negative one. One may argue with these efforts on normalization, and ultimately their value should be assessed by assessing their impact on inter-annotator agreement. Given the limited number of judgements we received, we did not try to evaluate this. 3.3 Statistical Significance Interval: estimate confidence intervals for the average mean scores for the systems, we use standard significance testing. a set of we can compute the mean sample variance of the indisentence judgements = 1 n n (4) 1 extend of the confidence interval can be computed by s 1.96 (6) Comparison: for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems. Unfortunately, we have much less data to work with than with the automatic scores. The way we n 107 Basis Diff. Ratio Sign test on BLEU 331 75% Bootstrap on BLEU 348 78% Sign test on Fluency 224 50% Sign test on Adequacy 225 51% Figure 4: Number and ratio of statistically significant distinction between system performance. Automatic scores are computed on a larger tested than manual scores (3064 sentences vs. 300–400 sentences). collected manual judgements, we do not necessarily have the same sentence judged for both systems (judges evaluate 5 systems out of the 8–10 participating systems). Still, for about good number of sentences, we do have this direct comparison, which allows us to apply the sign test, as described in Section 2.2. 4 Results and Analysis The results of the manual and automatic evaluation of the participating system translations is detailed in the figures at the end of this paper. The scores and confidence intervals are detailed first in the Figures 7–10 in table form (including ranks), and then in graphical form in Figures 11–16. In the graphs, system scores are indicated by a point, the confidence intervals by shaded areas around the point. In all figures, we present the per-sentence normalized judgements. The normalization on a per-judge basis gave very similar ranking, only slightly less consistent with the ranking from the pairwise comparisons. The confidence intervals are computed by bootresampling for and by standard significance testing for the manual scores, as described earlier in the paper. Pairwise comparison is done using the sign test. Often, two systems can not be distinguished with a confidence of over 95%, so there are ranked the same. This actually happens quite frequently (more below), so that the rankings are broad estimates. For instance: if 10 systems participate, and one system does better than 3 others, worse then 2, and is not significant different from the remaining 4, its rank is in the interval 3–7. Domain Fluency Adequacy in-domain 26.63 3.17 3.58 out-of-domain 20.37 2.74 3.08 Figure 5: Evaluation scores for in-domain and outof-domain test sets, averaged over all systems 4.1 Close results At first glance, we quickly recognize that many systems are scored very similar, both in terms of manjudgement and There may be occasionally a system clearly at the top or at the bottom, but most systems are so close that it is hard to distinguish them. In Figure 4, we displayed the number of system comparisons, for which we concluded statistical sig- For the automatic scoring method we can distinguish three quarters of the systems. While the Bootstrap method is slightly more sensitive, it is very much in line with the sign test on text blocks. For the manual scoring, we can distinguish only half of the systems, both in terms of fluency and adequacy. More judgements would have enabled us to make better distinctions, but it is not clear what the upper limit is. We can check, what the consequences of less manual annotation of results would have been: With half the number of manual judgements, we can distinguish about 40% of the systems, 10% less. 4.2 In-domain vs. out-of-domain The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data. Since the inclusion of out-ofdomain test data was a very late decision, the participants were not informed of this. So, this was a surprise element due to practical reasons, not malice. All systems (except for Systran, which was not tuned to Europarl) did considerably worse on outof-domain training data. This is demonstrated by scores over all systems, in terms of as displayed in Figure 5. The manual scores are averages over the raw unnormalized scores. 108 Language Pair Fluency Adequacy French-English 26.09 3.25 3.61 Spanish-English 28.18 3.19 3.71 German-English 21.17 2.87 3.10 English-French 28.33 2.86 3.16 English-Spanish 27.49 2.86 3.34 English-German 14.01 3.15 3.65 Figure 6: Average scores for different language pairs. Manual scoring is done by different judges, resulting in a not very meaningful comparison. 4.3 Language pairs It is well know that language pairs such as English- German pose more challenges to machine translation systems than language pairs such as French- English. Different sentence structure and rich target language morphology are two reasons for this. Again, we can compute average scores for all systems for the different language pairs (Figure 6). The differences in difficulty are better reflected in the than in the raw un-normalized manual judgements. The easiest language pair according 28.33) received worse manual scores than the hardest (English-German: 14.01). This is because different judges focused on different language pairs. Hence, the different averages of manual scores for the different language pairs reflect the behaviour of the judges, not the quality of the systems on different language pairs. Manual judgement vs. Given the closeness of most systems and the wide over-lapping confidence intervals it is hard to make strong statements about the correlation between human judgements and automatic scoring methods as We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not appreciated by In-domain Systran scores on this metric are lower than all statistical systems, even the ones that have much worse human scores. Surprisingly, this effect is much less obvious for out-of-domain test data. For instance, for out-of- English-French, Systran has the best and manual scores. suspicion is that very sensitive to jargon, to selecting exactly the right words, and not synonyms that human judges may appreciate as equally good. This is can not be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluscores, but a worse than all but one statistical system. This data set of manual judgements should provide a fruitful resource for research on better automatic scoring methods. 4.5 Best systems So, who won the competition? The best answer to this is: many research labs have very competitive systems whose performance is hard to tell apart. This is not completely surprising, since all systems use very similar technology. For some language pairs (such as German- English) system performance is more divergent than for others (such as English-French), at least as meaby The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages, as demonstrated by the results for English-German and English-French. The predominate focus of building systems that translate into English has ignored so far the difficult issues of generating rich morphology which may not be determined solely by local context. 4.6 Comments on Manual Evaluation This is the first time that we organized a large-scale manual evaluation. While we used the standard metrics of the community, the we way presented translations and prompted for assessment differed from other evaluation campaigns. For instance, in the recent IWSLT evaluation, first fluency annotations were solicited (while withholding the source sentence), and then adequacy annotations. Almost all annotators reported difficulties in maintaining a consistent standard for fluency and adequacy judgements, but nevertheless most did not explicitly move towards a ranking-based evaluation. Almost all annotators expressed their preference to move to a ranking-based evaluation in the future. A few pointed out that adequacy should be broken up 109 into two criteria: (a) are all source words covered? (b) does the translation have the same meaning, including connotations? Annotators suggested that long sentences are almost impossible to judge. Since all long sentranslation are somewhat even a contrastive evaluation between systems was difficult. A few annotators suggested to break up long sentences into clauses and evaluate these separately. Not every annotator was fluent in both the source and the target language. While it is essential to be fluent in the target language, it is not strictly necessary to know the source language, if a reference translation was given. However, ince we extracted the test corpus automatically from web sources, the reference translation was not always accurate — due to sentence alignment errors, or because translators did not adhere to a strict sentence-by-sentence translation (say, using pronouns when referring to entities mentioned in the previous sentence). Lack of correct reference translations was pointed out as a short-coming of our evaluation. One annotator suggested that this was the case for as much as 10% of our test sentences. Annotators argued for the importance of having correct and even multiple references. It was also proposed to allow annotators to skip sentences that they are unable to judge. 5 Conclusions We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs. While many systems had similar performance, the results offer interesting insights, especially about the relative performance of statistical and rule-based systems. Due to many similarly performing systems, we are not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics. The bias of automatic methods in favor of statistical systems seems to be less pronounced on out-of-domain test data. The manual evaluation of scoring translation on a graded scale from 1–5 seems to be very hard to perform. Replacing this with an ranked evaluation seems to be more suitable. Human judges also pointed out difficulties with the evaluation of long sentences. Acknowledgements The manual evaluation would not have been possible without the contributions of the manual annotators:</abstract>
<degree confidence="0.469968166666667">Jesus Andres Ferrer, Abhishek Arun, Amittai Axelrod, Alexandra Birch, Chris Callison-Burch, Jorge Civera, Marta Ruiz Costa-juss`a, Josep Maria Crego, Elsa Cubel, Chris Irwin Davis, Loic Dugast, Chris Dyer, Andreas Eisele, Cameron Fordyce, Jes´us Gim´enez, Fabrizio Gotti, Hieu Hoang, Eric Joanis</degree>
<author confidence="0.949403">Howard Johnson</author>
<author confidence="0.949403">Philipp Koehn</author>
<author confidence="0.949403">Beata Kouchnir</author>
<author confidence="0.949403">Roland Kuhn</author>
<author confidence="0.949403">Elliott Macklovitch</author>
<author confidence="0.949403">Arul Menezes</author>
<author confidence="0.949403">Marian Olteanu</author>
<author confidence="0.949403">Chris Quirk</author>
<author confidence="0.949403">Reinhard Rapp</author>
<author confidence="0.949403">Fatiha Sadat</author>
<author confidence="0.949403">Joan Andreu S`anchez</author>
<author confidence="0.949403">Germ´an Sanchis</author>
<author confidence="0.949403">Michel Simard</author>
<author confidence="0.949403">Ashish Venugopal</author>
<author confidence="0.949403">Taro Watan-</author>
<email confidence="0.537045">abe.</email>
<note confidence="0.9334589">This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency, Contract No. HR0011-06-C-0022. References Birch, A., Callison-Burch, C., Osborne, M., and Koehn, P. (2006). Constraining the phrase-based, joint probability statistical translation model. In Proceedings on the Workshop on Statistical Mapages 154–157, New York City. Association for Computational Linguistics. Callison-Burch, C., Osborne, M., and Koehn, P. (2006). Re-evaluating the role of BLEU in matranslation research. In of Collins, M., Koehn, P., and Kucerova, I. (2005). Clause restructuring for statistical machine trans- In Costa-juss`a, M. R., Crego, J. M., de Gispert, A., P., Khalilov, M., J. B., Fonollosa, J. A. R., and Banchs, R. (2006). Talp phrasebased statistical translation system for european pairs. In on the Workshop Statistical Machine pages 142– 145, New York City. Association for Computational Linguistics. Crego, J. M., de Gispert, A., Lambert, P., Costa- M. R., Khalilov, M., Banchs, R., J. B., and Fonollosa, J. A. R. (2006). N-grambased smt system enhanced with reordering pat- In on the Workshop on Statis- 110 Machine pages 162–165, New York City. Association for Computational Linguistics. Doddington, G. (2002). The NIST automated meaand its relation to IBM’s BLEU. In Proceedings of LREC-2002 Workshopon Machine Translation Evaluation: Human Evaluators Meet Auto- Gran Canaria, Spain. Eck, M. and Hori, C. (2005). Overview of the iwslt evaluation campaign. In of the International Workshop on Spoken Language Transla- Gim´enez, J. and M`arquez, L. (2006). The ldvsystem for smt. In on Workshop on Statistical Machine pages 166–169, New York City. Association for Computational Linguistics. Johnson, H., Sadat, F., Foster, G., Kuhn, R., Simard, M., Joanis, E., and Larkin, S. (2006). Portage: with smoothed phrase tables and segment choice In on the Workshop on Machine pages 134–137, New York City. Association for Computational Linguistics. Koehn, P. (2004). Statistical significance tests for machine translation evaluation. In Lin, D. and D., editors, of EMNLP pages 388–395, Barcelona, Spain. Association for Computational Linguistics. Koehn, P. and Monz, C. (2005). Shared task: Statistical machine translation between European lan- In of the ACL Workshop Building and Using Parallel pages 119– 124, Ann Arbor, Michigan. Association for Computational Linguistics. Li, A. (2005). Results of the 2005 NIST machine evaluation. In Translation Menezes, A., Toutanova, K., and Quirk, C. (2006). Microsoft research treelet translation system: 2006 europarl evaluation. In on the Workshop on Statistical Machine Translapages 158–161, New York City. Association for Computational Linguistics. Olteanu, M., Davis, C., Volosen, I., and Moldovan, D. (2006a). Phramer an open source statistiphrase-based translator. In on Workshop on Statistical Machine pages 146–149, New York City. Association for Computational Linguistics. Olteanu, M., Suriyentrakorn, P., and Moldovan, D. (2006b). Language models and reranking for matranslation. In on the Workshop Statistical Machine pages 150– 153, New York City. Association for Computational Linguistics. Patry, A., Gotti, F., and Langlais, P. (2006). Mood at Ramses versus pharaoh. In on Workshop on Statistical Machine pages 126–129, New York City. Association for Computational Linguistics. Przybocki, M. (2004). NIST machine translation</note>
<abstract confidence="0.888605571428572">evaluation – summary of results. In Evaluation Riezler, S. and Maxwell, J. T. (2005). On some pitfalls in automatic evaluation and significance testfor MT. In of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures Machine Translation and/or pages 57–64, Ann Arbor, Michigan. Association for Computational Linguistics. S´anchez, J. A. and Benedi, J. M. (2006). Stochastic inversion transduction grammars for obtaining word phrases for phrase-based statistical machine In on the Workshop on Machine pages 130–133,</abstract>
<note confidence="0.733522642857143">New York City. Association for Computational Linguistics. Watanabe, T., Tsukada, H., and Isozaki, H. (2006). Ntt system description for the wmt2006 shared In on the Workshop on Statis- Machine pages 122–125, New York City. Association for Computational Linguistics. Zollmann, A. and Venugopal, A. (2006). Syntax augmented machine translation via chart parsing. on the Workshop on Statistical pages 138–141, New York City. Association for Computational Linguistics. 111</note>
<title confidence="0.449024">French-English (In Domain)</title>
<abstract confidence="0.914915153846154">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy . 0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt</abstract>
<phone confidence="0.445107">9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11</phone>
<note confidence="0.8332555">Figure 16: Correlation between manual and automatic scores for English-German 121</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Joan Andreu S`anchez Sadat</author>
<author>Germ´an Sanchis</author>
<author>Michel Simard</author>
<author>Ashish Venugopal</author>
<author>Taro Watanabe</author>
</authors>
<title>The manual evaluation would not have been possible without the contributions of the manual annotators: Jesus Andres Ferrer, Abhishek Arun, Amittai Axelrod,</title>
<location>Alexandra Birch, Chris Callison-Burch, Jorge Civera, Marta Ruiz Costa-juss`a, Josep Maria Crego, Elsa Cubel, Chris Irwin Davis, Loic Dugast, Chris Dyer, Andreas Eisele, Cameron Fordyce, Jes´us Gim´enez, Fabrizio Gotti, Hieu Hoang, Eric Joanis Howard Johnson, Philipp Koehn, Beata Kouchnir, Roland Kuhn, Elliott Macklovitch, Arul Menezes, Marian Olteanu, Chris Quirk, Reinhard Rapp, Fatiha</location>
<marker>Sadat, Sanchis, Simard, Venugopal, Watanabe, </marker>
<rawString>The manual evaluation would not have been possible without the contributions of the manual annotators: Jesus Andres Ferrer, Abhishek Arun, Amittai Axelrod, Alexandra Birch, Chris Callison-Burch, Jorge Civera, Marta Ruiz Costa-juss`a, Josep Maria Crego, Elsa Cubel, Chris Irwin Davis, Loic Dugast, Chris Dyer, Andreas Eisele, Cameron Fordyce, Jes´us Gim´enez, Fabrizio Gotti, Hieu Hoang, Eric Joanis Howard Johnson, Philipp Koehn, Beata Kouchnir, Roland Kuhn, Elliott Macklovitch, Arul Menezes, Marian Olteanu, Chris Quirk, Reinhard Rapp, Fatiha Sadat, Joan Andreu S`anchez, Germ´an Sanchis, Michel Simard, Ashish Venugopal, and Taro Watanabe.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>