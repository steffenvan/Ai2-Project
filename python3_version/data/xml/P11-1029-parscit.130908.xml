<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.076530">
<title confidence="0.9869515">
Creative Language Retrieval:
A Robust Hybrid of Information Retrieval and Linguistic Creativity
</title>
<author confidence="0.998772">
Tony Veale
</author>
<affiliation confidence="0.9787695">
School of Computer Science and Informatics,
University College Dublin,
</affiliation>
<address confidence="0.939205">
Belfield, Dublin D4, Ireland.
</address>
<email confidence="0.996205">
Tony.Veale@UCD.ie
</email>
<sectionHeader confidence="0.998576" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999925782608696">
Information retrieval (IR) and figurative
language processing (FLP) could scarcely
be more different in their treatment of lan-
guage and meaning. IR views language as
an open-ended set of mostly stable signs
with which texts can be indexed and re-
trieved, focusing more on a text’s potential
relevance than its potential meaning. In
contrast, FLP views language as a system
of unstable signs that can be used to talk
about the world in creative new ways.
There is another key difference: IR is prac-
tical, scalable and robust, and in daily use
by millions of casual users. FLP is neither
scalable nor robust, and not yet practical
enough to migrate beyond the lab. This pa-
per thus presents a mutually beneficial hy-
brid of IR and FLP, one that enriches IR
with new operators to enable the non-literal
retrieval of creative expressions, and which
also transplants FLP into a robust, scalable
framework in which practical applications
of linguistic creativity can be implemented.
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999798731707317">
Words should not always be taken at face value.
Figurative devices like metaphor can communicate
far richer meanings than are evident from a super-
ficial – and perhaps literally nonsensical – reading.
Figurative Language Processing (FLP) thus uses a
variety of special mechanisms and representations,
to assign non-literal meanings not just to meta-
phors, but to similes, analogies, epithets, puns and
other creative uses of language (see Martin, 1990;
Fass, 1991; Way, 1991; Indurkhya, 1992; Fass,
1997; Barnden, 2006; Veale and Butnariu, 2010).
Computationalists have explored heterodox
solutions to the procedural and representational
challenges of metaphor, and FLP more generally,
ranging from flexible representations (e.g. the
preference semantics of Wilks (1978) and the col-
lative semantics of Fass (1991, 1997)) to processes
of cross-domain structure alignment (e.g. structure
mapping theory; see Gentner (1983) and Falken-
hainer et al. 1989) and even structural inversion
(Veale, 2006). Though thematically related, each
approach to FLP is broadly distinct, giving com-
putational form to different cognitive demands of
creative language: thus, some focus on inter-
domain mappings (e.g. Gentner, 1983) while oth-
ers focus more on intra-domain inference (e.g. Ba-
rnden, 2006). However, while computationally
interesting, none has yet achieved the scalability or
robustness needed to make a significant practical
impact outside the laboratory. Moreover, such
systems tend to be developed in isolation, and are
rarely designed to cohere as part of a larger frame-
work of creative reasoning (e.g. Boden, 1994).
In contrast, Information Retrieval (IR) is both
scalable and robust, and its results translate easily
from the laboratory into practical applications (e.g.
see Salton, 1968; Van Rijsbergen, 1979). Whereas
FLP derives its utility and its fragility from its at-
tempts to identify deeper meanings beneath the
surface, the widespread applicability of IR stems
directly from its superficial treatment of language
</bodyText>
<page confidence="0.959153">
278
</page>
<note confidence="0.9797355">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 278–287,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999415052631579">
and meaning. IR does not distinguish between
creative and conventional uses of language, or
between literal and non-literal meanings. IR is also
remarkably modular: its components are designed
to work together interchangeably, from stemmers
and indexers to heuristics for query expansion and
document ranking. Yet, because IR treats all lan-
guage as literal language, it relies on literal
matching between queries and the texts that they
retrieve. Documents are retrieved precisely be-
cause they contain stretches of text that literally
resemble the query. This works well in the main,
but it means that IR falls flat when the goal of re-
trieval is not to identify relevant documents but to
retrieve new and creative ways of expressing a
given idea. To retrieve creative language, and to be
potentially surprised or inspired by the results, one
needs to facilitate a non-literal relationship be-
tween queries and the texts that they match.
The complementarity of FLP and IR suggests a
productive hybrid of both paradigms. If the most
robust elements of FLP are used to provide new
non-literal query operators for IR, then IR can be
used to retrieve potentially new and creative ways
of speaking about a topic from a large text collec-
tion. In return, IR can provide a stable, robust and
extensible platform on which to use these opera-
tors to build FLP systems that exhibit linguistic
creativity. In the next section we consider the re-
lated work on which the current realization of
these ideas is founded, before presenting a specific
trio of new semantic query operators in section 3.
We describe three simple but practical applications
of this creative IR paradigm in section 4. Empirical
support for the FLP intuitions that underpin our
new operators is provided in section 5. The paper
concludes with some closing observations about
future goals and developments in section 6.
</bodyText>
<sectionHeader confidence="0.998984" genericHeader="related work">
2 Related Work and Ideas
</sectionHeader>
<bodyText confidence="0.999989983333333">
IR works on the premise that a user can turn an
information need into an effective query by antici-
pating the language that is used to talk about a
given topic in a target collection. If the collection
uses creative language in speaking about a topic,
then a query must also contain the seeds of this
creative language. Veale (2004) introduces the idea
of creative information retrieval to explore how an
IR system can itself provide a degree of creative
anticipation, acting as a mediator between the lit-
eral specification of a meaning and the retrieval of
creative articulations of this meaning. This antici-
pation ranges from simple re-articulation (e.g. a
text may implicitly evoke “Qur’an” even if it only
contains “Muslim bible”) to playful allusions and
epithets (e.g. the CEO of a rubber company may be
punningly described as a “rubber baron”). A crea-
tive IR system may even anticipate out-of-
dictionary words, like chocoholic and sexoholic.
Conventional IR systems use a range of query
expansion techniques to automatically bolster a
user’s query with additional keywords or weights,
to permit the retrieval of relevant texts it might not
otherwise match (e.g. Vernimb, 1977; Voorhees,
1994). Techniques vary, from the use of stemmers
and morphological analysis to the use of thesauri
(such as WordNet; see Fellbaum, 1998; Voorhees,
1998) to pad a query with synonyms, to the use of
statistical analysis to identify more appropriate
context-sensitive associations and near-synonyms
(e.g. Xu and Croft, 1996). While some techniques
may suggest conventional metaphors that have be-
come lexicalized in a language, they are unlikely to
identify relatively novel expressions. Crucially,
expansion improves recall at the expense of overall
precision, making automatic techniques even more
dangerous when the goal is to retrieve results that
are creative and relevant. Creative IR must balance
a need for fine user control with the statistical
breadth and convenience of automatic expansion.
Fortunately, statistical corpus analysis is an ob-
vious area of overlap for IR and FLP. Distribu-
tional analyses of large corpora have been shown
to produce nuanced models of lexical similarity
(e.g. Weeds and Weir, 2005) as well as context-
sensitive thesauri for a given domain (Lin, 1998).
Hearst (1992) shows how a pattern like “Xs and
other Ys” can be used to construct more fluid,
context-specific taxonomies than those provided
by WordNet (e.g. “athletes and other celebrities”
suggests a context in which athletes are viewed as
stars). Mason (2004) shows how statistical analysis
can automatically detect and extract conventional
metaphors from corpora, though creative meta-
phors still remain a tantalizing challenge. Hanks
(2005) shows how the “Xs like A, B and C” con-
struction allows us to derive flexible ad-hoc cate-
gories from corpora, while Hanks (2006) argues
for a gradable conception of metaphoricity based
on word-sense distributions in corpora.
</bodyText>
<page confidence="0.996584">
279
</page>
<bodyText confidence="0.999898928571429">
Veale and Hao (2007) exploit the simile frame
“as X as Y” to harvest a great many common
similes and their underlying stereotypes from the
web (e.g. “as hot as an oven”), while Veale and
Hao (2010) show that the pattern “about as X as Y”
retrieves an equally large collection of creative (if
mostly ironic) comparisons. These authors demon-
strate that a large vocabulary of stereotypical ideas
(over 4000 nouns) and their salient properties (over
2000 adjectives) can be harvested from the web.
We now build on these results to develop a set
of new semantic operators, that use corpus-derived
knowledge to support finely controlled non-literal
matching and automatic query expansion.
</bodyText>
<sectionHeader confidence="0.997044" genericHeader="method">
3 Creative Text Retrieval
</sectionHeader>
<bodyText confidence="0.999980930232558">
In language, creativity is always a matter of con-
strual. While conventional IR queries articulate a
need for information, creative IR queries articulate
a need for expressions to convey the same meaning
in a fresh or unusual way. A query and a matching
phrase can be figuratively construed to have the
same meaning if there is a non-literal mapping
between the elements of the query and the ele-
ments of the phrase. In creative IR, this non-literal
mapping is facilitated by the query’s explicit use of
semantic wildcards (e.g. see Mihalcea, 2002).
The wildcard * is a boon for power-users of the
Google search engine, precisely because it allows
users to focus on the retrieval of matching phrases
rather than relevant documents. For instance, * can
be used to find alternate ways of instantiating a
culturally-established linguistic pattern, or “snow-
clone”: thus, the Google queries “In * no one can
hear you scream” (from Alien), “Reader, I * him”
(from Jane Eyre) and “This is your brain on *”
(from a famous TV advert) find new ways in
which old patterns have been instantiated for hu-
morous effect on the Web. On a larger scale, Veale
and Hao (2007) used the * wildcard to harvest web
similes, but reported that harvesting cultural data
with wildcards is not a straightforward process.
Google and other engines are designed to maxi-
mize document relevance and to rank results ac-
cordingly. They are not designed to maximize the
diversity of results, or to find the largest set of
wildcard bindings. Nor are they designed to find
the most commonplace bindings for wildcards.
Following Guilford’s (1950) pioneering work,
diversity is widely considered a key component in
the psychology of creativity. By focusing on the
phrase level rather than the document level, and by
returning phrase sets rather than document sets,
creative IR maximizes diversity by finding as
many bindings for its wildcards as a text collection
will support. But we need more flexible and pre-
cise wildcards than *. We now consider three va-
rieties of semantic wildcards that build on insights
from corpus-linguistic approaches to FLP.
</bodyText>
<subsectionHeader confidence="0.999013">
3.1 The Neighborhood Wildcard ?X
</subsectionHeader>
<bodyText confidence="0.999781473684211">
Semantic query expansion replaces a query term X
with a set {X, X1, X2, ..., Xn} where each Xi is
related to X by a prescribed lexico-semantic rela-
tionship, such as synonymy, hyponymy or
meronymy. A generic, lightweight resource like
WordNet can provide these relations, or a richer
ontology can be used if one is available (e.g. see
Navigli and Velardi, 2003). Intuitively, each query
term suggests other terms from its semantic neigh-
borhood, yet there are practical limits to this intui-
tion. Xi may not be an obvious or natural substitute
for X. A neighborhood can be drawn too small,
impacting recall, or too large, impacting precision.
Corpus analysis suggests an approach that is
both semantic and pragmatic. As noted in Hanks
(2005), languages provide constructions for build-
ing ad-hoc sets of items that can be considered
comparable in a given context. For instance, a co-
ordination of bare plurals suggests that two ideas
are related at a generic level, as in “priests and
imams” or “mosques and synagogues”. More gen-
erally, consider the pattern “X and Y”, where X and
Y are proper-names (e.g., “Zeus and Hera”), or X
and Y are inflected nouns or verbs with the same
inflection (e.g., the plurals “cats and dogs” or the
verb forms “kicking and screaming”). Millions of
matches for this pattern can be found in the Google
3-grams (Brants and Franz, 2006), allowing us to
build a map of comparable terms by linking the
root-forms of X and Y with a similarity score ob-
tained via a WordNet-based measure (e.g. see Bu-
danitsky and Hirst (2006) for a good selection).
The pragmatic neighborhood of a term X can be
defined as {X, X1, X2, ..., Xn}, so that for each
Xi, the Google 3-grams contain “X+inf and
Xi+inf” or “X+inf and Xi+inf”. The boundaries of
neighborhoods are thus set by usage patterns: if ?X
denotes the neighborhood of X, then ?artist
</bodyText>
<page confidence="0.961108">
280
</page>
<bodyText confidence="0.999011058823529">
matches not just artist, composer and poet, but
studio, portfolio and gallery, and many other
terms that are semantically dissimilar but prag-
matically linked to artist. Since each Xi ∈ ?X is
ranked by similarity to X, query matches can also
be ranked by similarity.
When X is an adjective, then ?X matches any
element of {X, Xi, X2, ..., Xn}, where each Xi
pragmatically reinforces X, and X pragmatically
reinforces each Xi. To ensure X and Xi really are
mutually reinforcing adjectives, we use the double-
ground simile pattern “as X and Xi as” to harvest
{X1, ..., Xn} for each X. Moreover, to maximize
recall, we use the Google API (rather than Google
ngrams) to harvest suitable bindings for X and Xi
from the web. For example, @witty = {charming,
clever, intelligent, entertaining, ..., edgy, fun}.
</bodyText>
<subsectionHeader confidence="0.999099">
3.2 The Cultural Stereotype Wildcard @X
</subsectionHeader>
<bodyText confidence="0.998768393939394">
Dickens claims in A Christmas Carol that “the
wisdom of a people is in the simile”. Similes ex-
ploit familiar stereotypes to describe a less familiar
concept, so one can learn a great deal about a cul-
ture and its language from the similes that have the
most currency (Taylor, 1954). The wildcard @X
builds on the results of Veale and Hao (2007) to
allow creative IR queries to retrieve matches on
the basis of cultural expectations. This foundation
provides a large set of adjectival features (over
2000) for a larger set of nouns (over 4000) denot-
ing stereotypes for which these features are salient.
If N is a noun, then @N matches any element
of the set {A1, A2, ..., An}, where each Ai is an
adjective denoting a stereotypical property of N.
For example, @diamond matches any element of
{transparent, immutable, beautiful, tough, expen-
sive, valuable, shiny, bright, lasting, desirable,
strong, ..., hard} . If A is an adjective, then @A
matches any element of the set {N1, N2, ..., Nn},
where each Ni is a noun denoting a stereotype for
which A is a culturally established property. For
example, @tall matches any element of {giraffe,
skyscraper, tree, redwood, tower, sunflower, light-
house, beanstalk, rocket, ..., supermodel}.
Stereotypes crystallize in a language as clichés,
so one can argue that stereotypes and clichés are
little or no use to a creative IR system. Yet, as
demonstrated in Fishlov (1992), creative language
is replete with stereotypes, not in their cliched
guises, but in novel and often incongruous combi-
nations. The creative value of a stereotype lies in
how it is used, as we’ll show later in section 4.
</bodyText>
<subsectionHeader confidence="0.998136">
3.3 The Ad-Hoc Category Wildcard ^X
</subsectionHeader>
<bodyText confidence="0.999975048780488">
Barsalou (1983) introduced the notion of an ad-
hoc category, a cross-cutting collection of often
disparate elements that cohere in the context of a
specific task or goal. The ad-hoc nature of these
categories is reflected in the difficulty we have in
naming them concisely: the cumbersome “things to
take on a camping trip” is Barsalou’s most cited
example. But ad-hoc categories do not replace
natural kinds; rather, they supplement an existing
system of more-or-less rigid categories, such as the
categories found in WordNet.
The semantic wildcard ^C matches C and any
element of {C1, C2, ..., Cn}, where each Ci is a
member of the category named by C. ^C can de-
note a fixed category in a resource like WordNet or
even Wikipedia; thus, ^fruit matches any member
of {apple, orange, pear, ..., lemon} and ^animal
any member of {dog, cat, mouse, ..., deer, fox}.
Ad-hoc categories arise in creative IR when the
results of a query – or more specifically, the bind-
ings for a query wildcard – are funneled into a new
user-defined category. For instance, the query
“^fruit juice” matches any phrase in a text collec-
tion that denotes a named fruit juice, from “lemon
juice” to “pawpaw juice”. A user can now funnel
the bindings for ^fruit in this query into an ad-hoc
category juicefruit, to gather together those fruits
that are used for their juice. Elements of ^juicefruit
are ranked by the corpus frequencies discovered by
the original query; low-frequency juicefruit mem-
bers in the Google ngrams include coffee, raisin,
almond, carob and soybean. Ad-hoc categories
allow users of IR to remake a category system in
their own image, and create a new vocabulary of
categories to serve their own goals and interests, as
when “^food pizza” is used to suggest disparate
members for the ad-hoc category pizzatopping.
The more subtle a query, the more disparate the
elements it can funnel into an ad-hoc category. We
now consider how basic semantic wildcards can be
combined to generate even more diverse results.
</bodyText>
<subsectionHeader confidence="0.924994">
3.4 Compound Operators
</subsectionHeader>
<bodyText confidence="0.979987">
Each wildcard maps a query term onto a set of ex-
</bodyText>
<page confidence="0.987048">
281
</page>
<bodyText confidence="0.904638326086956">
pansion terms. The compositional semantics of a
wildcard combination can thus be understood in
set-theoretic terms. The most obvious and useful
combinations of ?, @ and ^ are described below:
?? Neighbor-of-a-neighbor: if ?X matches any
element of {X, X1, X2, ..., Xn} then ??X matches
any of ?X U ?X1 U ... U ?Xn, where the ranking
of Xij in ??X is a function of the ranking of Xi in
?X and the ranking of Xij in ?Xi. Thus, ??artist
matches far more terms than ?artist, yielding more
diversity, more noise, and more creative potential.
@@ Stereotype-of-a-stereotype: if @X matches
any element of {X1, X2, ..., Xn} then @@X
matches any of @X1 U @X2 U ... U @Xn. For
instance, @@diamond matches any stereotype
that shares a salient property with diamond, and
@@sharp matches any salient property of any
noun for which sharp is a stereotypical property.
?@ Neighborhood-of-a-stereotype: if @X matches
any element of {X1, X2, ..., Xn} then ? @ X
matches any of ?X1 U ?X2 U ... U ?Xn. Thus,
?@cunning matches any term in the pragmatic
neighborhood of a stereotype for cunning, while
?@knife matches any property that mutually rein-
forces any stereotypical property of knife
@? Stereotypes-in-a-neighborhood: if ?X matches
any of {X, X1, X2, ..., Xn} then @?X matches any
of @X U @X1 U ... U @Xn. Thus, @?corpse
matches any salient property of any stereotype in
the neighborhood of corpse, while @?fast matches
any stereotype noun with a salient property that is
similar to, and reinforced by, fast.
?^ Neighborhood-of-a-category: if ^C matches
any of {C, C1, C2, ..., Cn} then ?^C matches any
of ?C U ?C1 U ... U ?Cn.
^? Categories-in-a-neighborhood: if ?X matches
any of {X, X1, X2, ..., Xn} then ^?X matches any
of ^X U ^X1 U ... U ^Xn.
@^ Stereotypes-in-a-category: if ^C matches any
of {C, C1, C2, ..., Cn} then @^C matches any of
@C U @C1 U ... U @Cn.
^@ Members-of-a-stereotype-category: if @X
matches any element of {X1, X2, ..., Xn} then
^@X matches any of ^X1 U ^X2 U ... U ^Xn.
So ^@strong matches any member of a category
(such as warrior) that is stereotypically strong.
</bodyText>
<sectionHeader confidence="0.920729" genericHeader="method">
4 Applications of Creative Retrieval
</sectionHeader>
<bodyText confidence="0.999971756097561">
The Google ngrams comprise a vast array of ex-
tracts from English web texts, of 1 to 5 words in
length (Brants and Franz, 2006). Many extracts are
well-formed phrases that give lexical form to many
different ideas. But an even greater number of
ngrams are not linguistically well-formed. The
Google ngrams can be seen as a lexicalized idea
space, embedded within a larger sea of noise.
Creative IR can be used to explore this idea space.
Each creative query is a jumping off point in a
space of lexicalized ideas that is implied by a large
corpus, with each successive match leading the
user deeper into the space. By turning matches into
queries, a user can perform a creative exploration
of the space of phrases and ideas (see Boden,
1994) while purposefully sidestepping the noise of
the Google ngrams. Consider the pleonastic query
“Catholic ?pope”. Retrieved phrases include, in
descending order of lexical similarity, “Catholic
president”, “Catholic politician”, “Catholic king”,
“Catholic emperor” and “Catholic patriarch”.
Suppose a user selects “Catholic king”: the new
query “Catholic ?king” now retrieves “Catholic
queen”, “Catholic court”, “Catholic knight”,
“Catholic kingdom” and “Catholic throne”. The
subsequent query “Catholic ?kingdom” in turn
retrieves “Catholic dynasty” and “Catholic army”,
among others. In this way, creative IR allows a
user to explore the text-supported ramifications of
a metaphor like Popes are Kings (e.g., if popes are
kings, they too might have queens, command ar-
mies, found dynasties, or sit on thrones).
Creative IR gives users the tools to conduct
their own explorations of language. The more
wildcards a query contains, the more degrees of
freedom it offers to the explorer. Thus, the query
“?scientist ‘s ?laboratory” uncovers a plethora of
analogies for the relationship between scientists
and their labs: matches in the Google 3-grams in-
clude “technician’s workshop”, “artist’s studio”,
“chef’s kitchen” and “gardener’s greenhouse”.
</bodyText>
<page confidence="0.990845">
282
</page>
<subsectionHeader confidence="0.997219">
4.1 Metaphors with Aristotle
</subsectionHeader>
<bodyText confidence="0.998539548387097">
For a term X, the wildcard ?X suggests those other
terms that writers have considered to be compara-
ble to X, while ??X extrapolates beyond the cor-
pus evidence to suggest an even larger space of
potential comparisons. A meaningful metaphor can
be constructed for X by framing X with any
stereotype to which it is pragmatically comparable,
that is, any stereotype in ?X. Collectively, these
stereotypes can impart the properties @?X to X.
Suppose one wants to metaphorically ascribe
the property P to X. The set @P contains those
stereotypes for which P is culturally salient. Thus,
close metaphors for X (what MacCormac (1985)
dubs epiphors) in the context of P are suggested by
?X ∩ @P. More distant metaphors (MacCormac
dubs these diaphors) are suggested by ??X ∩ @P.
For instance, to describe a scholar as wise, one can
use poet, yogi, philosopher or rabbi as compari-
sons. Yet even a simple metaphor will impart other
features to a topic. If ^PS denotes the ad-hoc set
of additional properties that may be inferred for X
when a stereotype S is used to convey property P,
then ^PS = ?P ∩ @@P. The query “^PS X” now
finds corpus-attested elements of ^PS that can
meaningfully be used to modify X.
These IR formulations are used by Aristotle, an
online metaphor generator, to generate targeted
metaphors that highlight a property P in a topic X.
Aristotle uses the Google ngrams to supply values
for ?X, ??X, ?P and ^PS. The system can be ac-
cessed at: www.educatedinsolence.com/aristotle
</bodyText>
<subsectionHeader confidence="0.957072">
4.2 Expressing Attitude with Idiom Savant
</subsectionHeader>
<bodyText confidence="0.999631708333333">
Our retrieval goals in IR are often affective in na-
ture: we want to find a way of speaking about a
topic that expresses a particular sentiment and car-
ries a certain tone. However, affective categories
are amongst the most cross-cutting structures in
language. Words for disparate ideas are grouped
according to the sentiments in which they are gen-
erally held. We respect judges but dislike critics;
we respect heroes but dislike killers; we respect
sharpshooters but dislike snipers; and respect re-
bels but dislike insurgents. It seems therefore that
the particulars of sentiment are best captured by a
set of culture-specific ad-hoc categories.
We thus construct two ad-hoc categories,
^posword and ^negword, to hold the most obvi-
ously positive or negative words in Whissell’s
(1989) Dictionary of Affect. We then grow these
categories to include additional reinforcing ele-
ments from their pragmatic neighborhoods,
?^posword and ?^negword. As these categories
grow, so too do their neighborhoods, allowing a
simple semi-automated bootstrapping process to
significantly grow the categories over several it-
erations. We construct two phrasal equivalents of
these categories, ^posphrase and ^negphrase,
using the queries “^posword - ^pastpart” (e.g.,
matching “high-minded” and “sharp-eyed”) and
“^negword - ^pastpart” (e.g., matching “flat-
footed” and “dead-eyed”) to mine affective phrases
from the Google 3-grams. The resulting ad-hoc
categories (of ~600 elements each) are manually
edited to fix any obvious mis-categorizations.
Idiom Savant is a web application that uses
^posphrase and ^negphrase to suggest flattering
and insulting epithets for a given topic. The query
“^posphrase ?X” retrieves phrases for a topic X
that put a positive spin on a related topic to which
X is sometimes compared, while “^negphrase
?X” conversely imparts a negative spin. Thus, for
politician, the Google 4-grams provide the flatter-
ing epithets “much-needed leader”, “awe-inspiring
leader”, “hands-on boss” and “far-sighted states-
man”, as well as insults like “power-mad leader”,
“back-stabbing boss”, “ice-cold technocrat” and
“self-promoting hack”. Riskier diaphors can be
retrieved via “^posphrase ??X” and “^negphrase
??X”. Idiom Savant is accessible online at:
www.educatedinsolence.com/idiom-savant/
</bodyText>
<subsectionHeader confidence="0.991813">
4.3 Poetic Similes with The Jigsaw Bard
</subsectionHeader>
<bodyText confidence="0.9998535">
The well-formed phrases of a large corpus can be
viewed as the linguistic equivalent of objets trou-
ves in art: readymade or “found” objects that might
take on fresh meanings in a creative context. The
phrase “robot fish”, for instance, denotes a more-
or-less literal object in the context of autonomous
robotic submersibles, but can also be used to con-
vey a figurative meaning as part of a creative com-
parison (e.g., “he was as cold as a robot fish”).
Fishlov (1992) argues that poetic comparisons
are most resonant when they combine mutually-
reinforcing (if distant) ideas, to create memorable
images and evoke nuanced feelings. Building on
Fishlov’s argument, creative IR can be used to turn
</bodyText>
<page confidence="0.995938">
283
</page>
<bodyText confidence="0.999838742857143">
the readymade phrases of the Google ngrams into
vehicles for creative comparison. For a topic X and
a property P, simple similes of the form “X is as P
as S” are easily generated, where S E @P n ??X.
Fishlov would dub these non-poetic similes
(NPS). However, the query “?P @P” will retrieve
corpus-attested elaborations of stereotypes in @P
to suggest similes of the form “X is as P as Pl S”,
where Pl E ?P. These similes exhibit elements of
what Fishlov dubs poetic similes (PS). Why say
“as cold as a fish” when you can say “as cold as a
wet fish”, “a dead haddock”, “a wet January”, “a
frozen corpse”, or “a heartless robot”? Complex
queries can retrieve more creative combinations, so
“@P @P” (e.g. “robot fish” or “snow storm” for
cold), “?P @P @P” (e.g. “creamy chocolate
mousse” for rich) and “@P - ^pastpart @P” (e.g.
“snow-covered graveyard” and “bullet-riddled
corpse” for cold) each retrieve ngrams that blend
two different but overlapping stereotypes.
Blended properties also make for nuanced
similes of the form “as P and ?P as S”, where S E
@P n @?P. While one can be “as rich as a fat
king”, something can be “as rich and enticing as a
chocolate truffle”, “a chocolate brownie”, “a
chocolate fruitcake”, and even “a chocolate king”.
The Jigsaw Bard is a web application that
harnesses the readymades of the Google ngrams to
formulate novel similes from existing phrases. By
mapping blended properties to ngram phrases that
combine multiple stereotypes, the Bard expands its
generative scope considerably, allowing this appli-
cation to generate hundreds of thousands of evoca-
tive comparisons. The Bard can be accessed online
at: www.educatedinsolence.com/jigsaw/
</bodyText>
<sectionHeader confidence="0.985897" genericHeader="method">
5 Empirical Evaluation
</sectionHeader>
<bodyText confidence="0.999940112903226">
Though ^ is the most overtly categorical of our
wildcards, all three wildcards – ?, @ and ^ – are
categorical in nature. Each has a semantic or
pragmatic membership function that maps a term
onto an expansion set of related members. The
membership functions for specific uses of ^ are
created in an ad-hoc fashion by the users that ex-
ploit it; in contrast, the membership functions for
uses of @ and ? are derived automatically, via
pattern-matching and corpus analysis. Nonetheless,
ad-hoc categories in creative IR are often popu-
lated with the bindings produced by uses of @ and
? and combinations thereof. In a sense, ?X and
@X and their variations are themselves ad-hoc
categories. But how well do they serve as catego-
ries? Are they large, but noisy? Or too small, with
limited coverage? We can evaluate the effective-
ness of ? and @, and indirectly that of ^ too, by
comparing the use of ? and @ as category builders
to a hand-crafted gold standard like WordNet.
Other researchers have likewise used WordNet
as a gold standard for categorization experiments,
and we replicate here the experimental set-up of
Almuhareb and Poesio (2004, 2005), which is de-
signed to measure the effectiveness of web-
acquired conceptual descriptions. Almuhareb and
Poesio choose 214 English nouns from 13 of
WordNet’s upper-level semantic categories, and
proceed to harvest property values for these con-
cepts from the web using the Hearst-like pattern
“a|an|the * C is|was”. This pattern yields a com-
bined total of 51,045 values for all 214 nouns;
these values are primarily adjectives, such as hot
and black for coffee, but noun-modifiers of C are
also allowed, such as fruit for cake. They also har-
vest 8934 attribute nouns, such as temperature and
color, using the query “the * of the C is|was”.
These values and attributes are then used as the
basis of a clustering algorithm to partition the 214
nouns back into their original 13 categories. Com-
paring these clusters with the original WordNet-
based groupings, Almuhareb and Poesio report a
cluster accuracy of 71.96% using just values like
hot and black (51,045 values), an accuracy of
64.02% using just attributes like temperature and
color (8,934 attributes), and an accuracy of 85.5%
using both together (a combined 59,979 features).
How concisely and accurately does @X de-
scribe a noun X for purposes of categorization? Let
^AP denote the set of 214 WordNet nouns used by
Almuhareb and Poesio. Then @^AP denotes a set
of 2,209 adjectival properties; this should be con-
trasted with the space of 51,045 adjectival values
used by Almuhareb and Poesio. Using the same
clustering algorithm over this feature set, @ X
achieves a clustering accuracy (as measured via
cluster purity) of 70.2%, compared to 71.96% for
Almuhareb and Poesio. However, when @X is
used to harvest a further set of attribute nouns for
X, via web queries of the form “the P * of X”
(where P E @X), then @X augmented with this
additional set of attributes (like hands for surgeon)
</bodyText>
<page confidence="0.994288">
284
</page>
<bodyText confidence="0.99993953125">
produces a larger space of 7,183 features. This in
turn yields a cluster accuracy of 90.2% which
contrasts with Almuhareb and Poesio’s 85.5% for
59,979 features. In either case, @X produces com-
parable clustering quality to Almuhareb and Poe-
sio, with just a small fraction of the features.
So how concisely and accurately does ?X de-
scribe a noun X for purposes of categorization?
While @X denotes a set of salient adjectives, ?X
denotes a set of comparable nouns. So this time,
?^AP denotes a set of 8,300 nouns in total, to act
as a feature space for the 214 nouns of Almuhareb
and Poesio. Remember, the contents of each ?X,
and of ?^AP overall, are determined entirely by
the contents of the Google 3-grams; the elements
of ?X are not ranked in any way, and all are treated
as equals. When the 8,300 features in ?^AP are
clustered into 13 categories, the resulting clusters
have a purity of 93.4% relative to WordNet. The
pragmatic neighborhood of X, ?X, appears to be an
accurate and concise proxy for the meaning of X.
What about adjectives? Almuhareb and Poe-
sio’s set of 214 words does not contain adjectives,
and besides, WordNet does not impose a category
structure on its adjectives. In any case, the role of
adjectives in the applications of section 4 is largely
an affective one: if X is a noun, then one must
have confidence that the adjectives in @X are con-
sonant with our understanding of X, and if P is a
property, that the adjectives in ?P evoke much the
same mood and sentiment as P. Our evaluation of
@X and ?P should thus be an affective one.
So how well do the properties in @X capture
our sentiments about a noun X? Well enough to
estimate the pleasantness of X from the adjectives
in @X, perhaps? Whissell’s (1989) dictionary of
affect provides pleasantness ratings for a sizeable
number of adjectives and nouns (over 8,000 words
in total), allowing us to estimate the pleasantness
of X as a weighted average of the pleasantness of
each Xi in @X (the weights here are web frequen-
cies for the similes that underpin @ in section 3.2).
We thus estimate the affect of all stereotype nouns
for which Whissell also records a score. A two-
tailed Pearson test (p &lt; 0.05) shows a positive cor-
relation of 0.5 between these estimates and the
pleasantness scores assigned by Whissell. In con-
trast, estimates based on the pleasantness of adjec-
tives found in corresponding WordNet glosses
show a positive correlation of just 0.278.
How well do the elements of ?P capture our
sentiments toward an adjective P? After all, we
hypothesize that the adjectives in ?P are highly
suggestive of P, and vice versa. Aristotle and the
Jigsaw Bard each rely on ?P to suggest adjectives
that evoke an unstated property in a metaphor or
simile, or to suggest coherent blends of properties.
When we estimate the pleasantness of each adjec-
tive P in Whissell’s dictionary via the weighted
mean of the pleasantness of adjectives in ?P (again
using web frequencies as weights), a two-tailed
Pearson test (p &lt; 0.05) shows a correlation of 0.7
between estimates and actual scores. It seems ?P
does a rather good job of capturing the feel of P.
</bodyText>
<sectionHeader confidence="0.99436" genericHeader="method">
6 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999990071428571">
Creative information retrieval is not a single appli-
cation, but a paradigm that allows us to conceive
of many different kinds of application for crea-
tively manipulating text. It is also a tool-kit for
implementing such an application, as shown here
in the cases of Aristotle, Idiom Savant and Jigsaw
Bard.
The wildcards @, ? and ^ allow users to for-
mulate their own task-specific ontologies of ad-hoc
categories. In a fully automated application, they
provide developers with a simple but powerful vo-
cabulary for describing the range and relationships
of the words, phrases and ideas to be manipulated.
The @, ? and ^ wildcards are just a start. We
expect other aspects of figurative language to be
incorporated into the framework whenever they
prove robust enough for use in an IR context. In
this respect, we aim to position Creative IR as an
open, modular platform in which diverse results in
FLP, from diverse researchers, can be meaning-
fully integrated. One can imagine wildcards for
matching potential puns, portmanteau words and
other novel forms, as well as wildcards for figura-
tive processes like metonymy, synecdoche, hyper-
bolae and even irony. Ultimately, it is hoped that
creative IR can serve as a textual bridge between
high-level creativity and the low-level creative
potentials that are implicit in a large corpus.
</bodyText>
<sectionHeader confidence="0.998815" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.995957333333333">
This work was funded in part by Science Founda-
tion Ireland (SFI), via the Centre for Next Genera-
tion Localization. (CNGL).
</bodyText>
<page confidence="0.997139">
285
</page>
<sectionHeader confidence="0.99632" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99978055319149">
Almuhareb, A. and Poesio, M. (2004). Attribute-Based
and Value-Based Clustering: An Evaluation. In Proc.
of EMNLP 2004. Barcelona.
Almuhareb, A. and Poesio, M. (2005). Concept Learn-
ing and Categorization from the Web. In Proc. of the
27th Annual meeting of the Cognitive Science Society.
Barnden, J. A. (2006). Artificial Intelligence, figurative
language and cognitive linguistics. In: G. Kristian-
sen, M. Achard, R. Dirven, and F. J. Ruiz de Men-
doza Ibanez (Eds.), Cognitive Linguistics: Current
Application and Future Perspectives, 431-459. Ber-
lin: Mouton de Gruyter.
Barsalou, L. W. (1983). Ad hoc categories. Memory and
Cognition, 11:211–227.
Boden, M. (1994). Creativity: A Framework for Re-
search, Behavioural &amp; Brain Sciences 17(3):558-
568.
Brants, T. and Franz, A. (2006). Web 1T 5-gram Ver. 1.
Linguistic Data Consortium.
Budanitsky, A. and Hirst, G. (2006). Evaluating Word-
Net-based Measures of Lexical Semantic Related-
ness. Computational Linguistics, 32(1):13-47.
Falkenhainer, B., Forbus, K. and Gentner, D. (1989).
Structure-Mapping Engine: Algorithm and Exam-
ples. Artificial Intelligence, 41:1-63.
Fass, D. (1991). Met*: a method for discriminating
metonymy and metaphor by computer. Computa-
tional Linguistics, 17(1):49-90.
Fass, D. (1997). Processing Metonymy and Metaphor.
Contemporary Studies in Cognitive Science &amp; Tech-
nology. New York: Ablex.
Fellbaum, C. (1998). WordNet: An Electronic Lexical
Database. MIT Press, Cambridge.
Fishlov, D. (1992). Poetic and Non-Poetic Simile:
Structure, Semantics, Rhetoric. Poetics Today, 14(1),
1-23.
Gentner, D. (1983), Structure-mapping: A Theoretical
Framework. Cognitive Science 7:155–170.
Guilford, J.P. (1950) Creativity, American Psychologist
5(9):444–454.
Hanks, P. (2005). Similes and Sets: The English Prepo-
sition ‘like’. In: Blatn‡, R. and Petkevic, V. (Eds.),
Languages and Linguistics: Festschrift for Fr. Cer-
mak. Charles University, Prague.
Hanks, P. (2006). Metaphoricity is gradable. In: Anatol
Stefanowitsch and Stefan Th. Gries (Eds.), Corpus-
Based Approaches to Metaphor and Metonymy,. 17-
35. Berlin: Mouton de Gruyter.
Hearst, M. (1992). Automatic acquisition of hyponyms
from large text corpora. In Proc. of the 14th Int. Conf.
on Computational Linguistics, pp 539–545.
Indurkhya, B. (1992). Metaphor and Cognition: Studies
in Cognitive Systems. Kluwer Academic Publishers,
Dordrecht: The Netherlands.
Lin, D. (1998). Automatic retrieval and clustering of
similar words. In Proc. of the 17th international con-
ference on Computational linguistics, 768-774.
MacCormac, E. R. (1985). A Cognitive Theory of Meta-
phor. MIT Press.
Martin, J. H. (1990). A Computational Model of Meta-
phor Interpretation. New York: Academic Press.
Mason, Z. J. (2004). CorMet: A Computational, Cor-
pus-Based Conventional Metaphor Extraction Sys-
tem, Computational Linguistics, 30(1):23-44.
Mihalcea, R. (2002). The Semantic Wildcard. In Proc.
of the LREC Workshop on Creating and Using Se-
mantics for Information Retrieval and Filtering. Ca-
nary Islands, Spain, May 2002.
Navigli, R. and Velardi, P. (2003). An Analysis of On-
tology-based Query Expansion Strategies. In Proc. of
the workshop on Adaptive Text Extraction and Min-
ing (ATEM 2003), at ECML 2003, the 14th European
Conf. on Machine Learning, 42–49
Salton, G. (1968). Automatic Information Organization
and Retrieval. New York: McGraw-Hill.
Taylor, A. (1954). Proverbial Comparisons and Similes
from California. Folklore Studies 3. Berkeley: Uni-
versity of California Press.
Van Rijsbergen, C. J. (1979). Information Retrieval.
Oxford: Butterworth-Heinemann.
Veale, T. (2004). The Challenge of Creative Informa-
tion Retrieval. Computational Linguistics and Intelli-
gent Text Processing: Lecture Notes in Computer
Science, Volume 2945/2004, 457-467.
Veale, T. (2006). Re-Representation and Creative Anal-
ogy: A Lexico-Semantic Perspective. New Genera-
tion Computing 24, pp 223-240.
Veale, T. and Hao, Y. (2007). Making Lexical Ontolo-
gies Functional and Context-Sensitive. In Proc. of
the 46th Annual Meeting of the Assoc. of Computa-
tional Linguistics.
Veale, T. and Hao, Y. (2010). Detecting Ironic Intent in
Creative Comparisons. In Proc. of ECAIÕ2010, the
19th European Conference on Artificial Intelligence.
</reference>
<page confidence="0.976908">
286
</page>
<reference confidence="0.999330133333333">
Veale, T. and Butnariu, C. (2010). Harvesting and Un-
derstanding On-line Neologisms. In: Onysko, A. and
Michel, S. (Eds.), Cognitive Perspectives on Word
Formation. 393-416. Mouton De Gruyter.
Vernimb, C. (1977). Automatic Query Adjustment in
Document Retrieval. Information Processing &amp;
Management. 13(6):339-353.
Voorhees, E. M. (1994). Query Expansion Using Lexi-
cal-Semantic Relations. In the proc. of SIGIR 94, the
17th International Conference on Research and De-
velopment in Information Retrieval. Berlin: Springer-
Verlag, 61-69.
Voorhees, E. M. (1998). Using WordNet for text re-
trieval. WordNet, An Electronic Lexical Database,
285–303. The MIT Press.
Way, E. C. (1991). Knowledge Representation and
Metaphor. Studies in Cognitive systems. Holland:
Kluwer.
Weeds, J. and Weir, D. (2005). Co-occurrence retrieval:
A flexible framework for lexical distributional simi-
larity. Computational Linguistics, 31(4):433–475.
Whissell, C. (1989). The dictionary of affect in lan-
guage. R. Plutchnik &amp; H. Kellerman (Eds.) Emotion:
Theory and research. NY: Harcourt Brace, 113-131.
Wilks, Y. (1978). Making Preferences More Active,
Artificial Intelligence 11.
Xu, J. and Croft, B. W. (1996). Query expansion using
local and global document analysis. In Proc. of the
19th annual international ACM SIGIR conference on
Research and development in information retrieval.
</reference>
<page confidence="0.99737">
287
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.597127">
<title confidence="0.9993395">Creative Language Retrieval: A Robust Hybrid of Information Retrieval and Linguistic Creativity</title>
<author confidence="0.999841">Tony</author>
<affiliation confidence="0.999907">School of Computer Science and University College</affiliation>
<address confidence="0.702766">Belfield, Dublin D4,</address>
<email confidence="0.797002">Tony.Veale@UCD.ie</email>
<abstract confidence="0.998581541666666">Information retrieval (IR) and figurative language processing (FLP) could scarcely be more different in their treatment of language and meaning. IR views language as an open-ended set of mostly stable signs with which texts can be indexed and retrieved, focusing more on a text’s potential relevance than its potential meaning. In contrast, FLP views language as a system that can be used to talk about the world in creative new ways. There is another key difference: IR is practical, scalable and robust, and in daily use by millions of casual users. FLP is neither scalable nor robust, and not yet practical enough to migrate beyond the lab. This paper thus presents a mutually beneficial hybrid of IR and FLP, one that enriches IR with new operators to enable the non-literal retrieval of creative expressions, and which also transplants FLP into a robust, scalable framework in which practical applications of linguistic creativity can be implemented.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Almuhareb</author>
<author>M Poesio</author>
</authors>
<title>Attribute-Based and Value-Based Clustering: An Evaluation.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP 2004.</booktitle>
<location>Barcelona.</location>
<contexts>
<context position="29106" citStr="Almuhareb and Poesio (2004" startWordPosition="4759" endWordPosition="4762">e IR are often populated with the bindings produced by uses of @ and ? and combinations thereof. In a sense, ?X and @X and their variations are themselves ad-hoc categories. But how well do they serve as categories? Are they large, but noisy? Or too small, with limited coverage? We can evaluate the effectiveness of ? and @, and indirectly that of ^ too, by comparing the use of ? and @ as category builders to a hand-crafted gold standard like WordNet. Other researchers have likewise used WordNet as a gold standard for categorization experiments, and we replicate here the experimental set-up of Almuhareb and Poesio (2004, 2005), which is designed to measure the effectiveness of webacquired conceptual descriptions. Almuhareb and Poesio choose 214 English nouns from 13 of WordNet’s upper-level semantic categories, and proceed to harvest property values for these concepts from the web using the Hearst-like pattern “a|an|the * C is|was”. This pattern yields a combined total of 51,045 values for all 214 nouns; these values are primarily adjectives, such as hot and black for coffee, but noun-modifiers of C are also allowed, such as fruit for cake. They also harvest 8934 attribute nouns, such as temperature and colo</context>
</contexts>
<marker>Almuhareb, Poesio, 2004</marker>
<rawString>Almuhareb, A. and Poesio, M. (2004). Attribute-Based and Value-Based Clustering: An Evaluation. In Proc. of EMNLP 2004. Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Almuhareb</author>
<author>M Poesio</author>
</authors>
<title>Concept Learning and Categorization from the Web.</title>
<date>2005</date>
<booktitle>In Proc. of the 27th Annual meeting of the Cognitive Science Society.</booktitle>
<marker>Almuhareb, Poesio, 2005</marker>
<rawString>Almuhareb, A. and Poesio, M. (2005). Concept Learning and Categorization from the Web. In Proc. of the 27th Annual meeting of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Barnden</author>
</authors>
<title>Artificial Intelligence, figurative language and cognitive linguistics. In:</title>
<date>2006</date>
<pages>431--459</pages>
<location>Berlin: Mouton</location>
<note>de Gruyter.</note>
<contexts>
<context position="1740" citStr="Barnden, 2006" startWordPosition="270" endWordPosition="271">work in which practical applications of linguistic creativity can be implemented. 1 Introduction Words should not always be taken at face value. Figurative devices like metaphor can communicate far richer meanings than are evident from a superficial – and perhaps literally nonsensical – reading. Figurative Language Processing (FLP) thus uses a variety of special mechanisms and representations, to assign non-literal meanings not just to metaphors, but to similes, analogies, epithets, puns and other creative uses of language (see Martin, 1990; Fass, 1991; Way, 1991; Indurkhya, 1992; Fass, 1997; Barnden, 2006; Veale and Butnariu, 2010). Computationalists have explored heterodox solutions to the procedural and representational challenges of metaphor, and FLP more generally, ranging from flexible representations (e.g. the preference semantics of Wilks (1978) and the collative semantics of Fass (1991, 1997)) to processes of cross-domain structure alignment (e.g. structure mapping theory; see Gentner (1983) and Falkenhainer et al. 1989) and even structural inversion (Veale, 2006). Though thematically related, each approach to FLP is broadly distinct, giving computational form to different cognitive de</context>
</contexts>
<marker>Barnden, 2006</marker>
<rawString>Barnden, J. A. (2006). Artificial Intelligence, figurative language and cognitive linguistics. In: G. Kristiansen, M. Achard, R. Dirven, and F. J. Ruiz de Mendoza Ibanez (Eds.), Cognitive Linguistics: Current Application and Future Perspectives, 431-459. Berlin: Mouton de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L W Barsalou</author>
</authors>
<title>Ad hoc categories. Memory and Cognition,</title>
<date>1983</date>
<pages>11--211</pages>
<contexts>
<context position="15516" citStr="Barsalou (1983)" startWordPosition="2518" endWordPosition="2519">blished property. For example, @tall matches any element of {giraffe, skyscraper, tree, redwood, tower, sunflower, lighthouse, beanstalk, rocket, ..., supermodel}. Stereotypes crystallize in a language as clichés, so one can argue that stereotypes and clichés are little or no use to a creative IR system. Yet, as demonstrated in Fishlov (1992), creative language is replete with stereotypes, not in their cliched guises, but in novel and often incongruous combinations. The creative value of a stereotype lies in how it is used, as we’ll show later in section 4. 3.3 The Ad-Hoc Category Wildcard ^X Barsalou (1983) introduced the notion of an adhoc category, a cross-cutting collection of often disparate elements that cohere in the context of a specific task or goal. The ad-hoc nature of these categories is reflected in the difficulty we have in naming them concisely: the cumbersome “things to take on a camping trip” is Barsalou’s most cited example. But ad-hoc categories do not replace natural kinds; rather, they supplement an existing system of more-or-less rigid categories, such as the categories found in WordNet. The semantic wildcard ^C matches C and any element of {C1, C2, ..., Cn}, where each Ci i</context>
</contexts>
<marker>Barsalou, 1983</marker>
<rawString>Barsalou, L. W. (1983). Ad hoc categories. Memory and Cognition, 11:211–227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Boden</author>
</authors>
<title>Creativity: A Framework for</title>
<date>1994</date>
<journal>Research, Behavioural &amp; Brain Sciences</journal>
<pages>17--3</pages>
<contexts>
<context position="2830" citStr="Boden, 1994" startWordPosition="429" endWordPosition="430">Though thematically related, each approach to FLP is broadly distinct, giving computational form to different cognitive demands of creative language: thus, some focus on interdomain mappings (e.g. Gentner, 1983) while others focus more on intra-domain inference (e.g. Barnden, 2006). However, while computationally interesting, none has yet achieved the scalability or robustness needed to make a significant practical impact outside the laboratory. Moreover, such systems tend to be developed in isolation, and are rarely designed to cohere as part of a larger framework of creative reasoning (e.g. Boden, 1994). In contrast, Information Retrieval (IR) is both scalable and robust, and its results translate easily from the laboratory into practical applications (e.g. see Salton, 1968; Van Rijsbergen, 1979). Whereas FLP derives its utility and its fragility from its attempts to identify deeper meanings beneath the surface, the widespread applicability of IR stems directly from its superficial treatment of language 278 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 278–287, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistic</context>
<context position="20435" citStr="Boden, 1994" startWordPosition="3375" endWordPosition="3376">Many extracts are well-formed phrases that give lexical form to many different ideas. But an even greater number of ngrams are not linguistically well-formed. The Google ngrams can be seen as a lexicalized idea space, embedded within a larger sea of noise. Creative IR can be used to explore this idea space. Each creative query is a jumping off point in a space of lexicalized ideas that is implied by a large corpus, with each successive match leading the user deeper into the space. By turning matches into queries, a user can perform a creative exploration of the space of phrases and ideas (see Boden, 1994) while purposefully sidestepping the noise of the Google ngrams. Consider the pleonastic query “Catholic ?pope”. Retrieved phrases include, in descending order of lexical similarity, “Catholic president”, “Catholic politician”, “Catholic king”, “Catholic emperor” and “Catholic patriarch”. Suppose a user selects “Catholic king”: the new query “Catholic ?king” now retrieves “Catholic queen”, “Catholic court”, “Catholic knight”, “Catholic kingdom” and “Catholic throne”. The subsequent query “Catholic ?kingdom” in turn retrieves “Catholic dynasty” and “Catholic army”, among others. In this way, cr</context>
</contexts>
<marker>Boden, 1994</marker>
<rawString>Boden, M. (1994). Creativity: A Framework for Research, Behavioural &amp; Brain Sciences 17(3):558-568.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
<author>A Franz</author>
</authors>
<date>2006</date>
<booktitle>Web 1T 5-gram Ver. 1. Linguistic Data Consortium.</booktitle>
<contexts>
<context position="12496" citStr="Brants and Franz, 2006" startWordPosition="1996" endWordPosition="1999">), languages provide constructions for building ad-hoc sets of items that can be considered comparable in a given context. For instance, a coordination of bare plurals suggests that two ideas are related at a generic level, as in “priests and imams” or “mosques and synagogues”. More generally, consider the pattern “X and Y”, where X and Y are proper-names (e.g., “Zeus and Hera”), or X and Y are inflected nouns or verbs with the same inflection (e.g., the plurals “cats and dogs” or the verb forms “kicking and screaming”). Millions of matches for this pattern can be found in the Google 3-grams (Brants and Franz, 2006), allowing us to build a map of comparable terms by linking the root-forms of X and Y with a similarity score obtained via a WordNet-based measure (e.g. see Budanitsky and Hirst (2006) for a good selection). The pragmatic neighborhood of a term X can be defined as {X, X1, X2, ..., Xn}, so that for each Xi, the Google 3-grams contain “X+inf and Xi+inf” or “X+inf and Xi+inf”. The boundaries of neighborhoods are thus set by usage patterns: if ?X denotes the neighborhood of X, then ?artist 280 matches not just artist, composer and poet, but studio, portfolio and gallery, and many other terms that </context>
<context position="19821" citStr="Brants and Franz, 2006" startWordPosition="3267" endWordPosition="3270">tegories-in-a-neighborhood: if ?X matches any of {X, X1, X2, ..., Xn} then ^?X matches any of ^X U ^X1 U ... U ^Xn. @^ Stereotypes-in-a-category: if ^C matches any of {C, C1, C2, ..., Cn} then @^C matches any of @C U @C1 U ... U @Cn. ^@ Members-of-a-stereotype-category: if @X matches any element of {X1, X2, ..., Xn} then ^@X matches any of ^X1 U ^X2 U ... U ^Xn. So ^@strong matches any member of a category (such as warrior) that is stereotypically strong. 4 Applications of Creative Retrieval The Google ngrams comprise a vast array of extracts from English web texts, of 1 to 5 words in length (Brants and Franz, 2006). Many extracts are well-formed phrases that give lexical form to many different ideas. But an even greater number of ngrams are not linguistically well-formed. The Google ngrams can be seen as a lexicalized idea space, embedded within a larger sea of noise. Creative IR can be used to explore this idea space. Each creative query is a jumping off point in a space of lexicalized ideas that is implied by a large corpus, with each successive match leading the user deeper into the space. By turning matches into queries, a user can perform a creative exploration of the space of phrases and ideas (se</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Brants, T. and Franz, A. (2006). Web 1T 5-gram Ver. 1. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Budanitsky</author>
<author>G Hirst</author>
</authors>
<title>Evaluating WordNet-based Measures of Lexical Semantic Relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<pages>32--1</pages>
<contexts>
<context position="12680" citStr="Budanitsky and Hirst (2006)" startWordPosition="2029" endWordPosition="2033">t two ideas are related at a generic level, as in “priests and imams” or “mosques and synagogues”. More generally, consider the pattern “X and Y”, where X and Y are proper-names (e.g., “Zeus and Hera”), or X and Y are inflected nouns or verbs with the same inflection (e.g., the plurals “cats and dogs” or the verb forms “kicking and screaming”). Millions of matches for this pattern can be found in the Google 3-grams (Brants and Franz, 2006), allowing us to build a map of comparable terms by linking the root-forms of X and Y with a similarity score obtained via a WordNet-based measure (e.g. see Budanitsky and Hirst (2006) for a good selection). The pragmatic neighborhood of a term X can be defined as {X, X1, X2, ..., Xn}, so that for each Xi, the Google 3-grams contain “X+inf and Xi+inf” or “X+inf and Xi+inf”. The boundaries of neighborhoods are thus set by usage patterns: if ?X denotes the neighborhood of X, then ?artist 280 matches not just artist, composer and poet, but studio, portfolio and gallery, and many other terms that are semantically dissimilar but pragmatically linked to artist. Since each Xi ∈ ?X is ranked by similarity to X, query matches can also be ranked by similarity. When X is an adjective,</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>Budanitsky, A. and Hirst, G. (2006). Evaluating WordNet-based Measures of Lexical Semantic Relatedness. Computational Linguistics, 32(1):13-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Falkenhainer</author>
<author>K Forbus</author>
<author>D Gentner</author>
</authors>
<title>Structure-Mapping Engine: Algorithm and Examples.</title>
<date>1989</date>
<journal>Artificial Intelligence,</journal>
<pages>41--1</pages>
<contexts>
<context position="2172" citStr="Falkenhainer et al. 1989" startWordPosition="327" endWordPosition="331">eanings not just to metaphors, but to similes, analogies, epithets, puns and other creative uses of language (see Martin, 1990; Fass, 1991; Way, 1991; Indurkhya, 1992; Fass, 1997; Barnden, 2006; Veale and Butnariu, 2010). Computationalists have explored heterodox solutions to the procedural and representational challenges of metaphor, and FLP more generally, ranging from flexible representations (e.g. the preference semantics of Wilks (1978) and the collative semantics of Fass (1991, 1997)) to processes of cross-domain structure alignment (e.g. structure mapping theory; see Gentner (1983) and Falkenhainer et al. 1989) and even structural inversion (Veale, 2006). Though thematically related, each approach to FLP is broadly distinct, giving computational form to different cognitive demands of creative language: thus, some focus on interdomain mappings (e.g. Gentner, 1983) while others focus more on intra-domain inference (e.g. Barnden, 2006). However, while computationally interesting, none has yet achieved the scalability or robustness needed to make a significant practical impact outside the laboratory. Moreover, such systems tend to be developed in isolation, and are rarely designed to cohere as part of a</context>
</contexts>
<marker>Falkenhainer, Forbus, Gentner, 1989</marker>
<rawString>Falkenhainer, B., Forbus, K. and Gentner, D. (1989). Structure-Mapping Engine: Algorithm and Examples. Artificial Intelligence, 41:1-63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Fass</author>
</authors>
<title>Met*: a method for discriminating metonymy and metaphor by computer.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<pages>17--1</pages>
<contexts>
<context position="1685" citStr="Fass, 1991" startWordPosition="262" endWordPosition="263">h also transplants FLP into a robust, scalable framework in which practical applications of linguistic creativity can be implemented. 1 Introduction Words should not always be taken at face value. Figurative devices like metaphor can communicate far richer meanings than are evident from a superficial – and perhaps literally nonsensical – reading. Figurative Language Processing (FLP) thus uses a variety of special mechanisms and representations, to assign non-literal meanings not just to metaphors, but to similes, analogies, epithets, puns and other creative uses of language (see Martin, 1990; Fass, 1991; Way, 1991; Indurkhya, 1992; Fass, 1997; Barnden, 2006; Veale and Butnariu, 2010). Computationalists have explored heterodox solutions to the procedural and representational challenges of metaphor, and FLP more generally, ranging from flexible representations (e.g. the preference semantics of Wilks (1978) and the collative semantics of Fass (1991, 1997)) to processes of cross-domain structure alignment (e.g. structure mapping theory; see Gentner (1983) and Falkenhainer et al. 1989) and even structural inversion (Veale, 2006). Though thematically related, each approach to FLP is broadly distin</context>
</contexts>
<marker>Fass, 1991</marker>
<rawString>Fass, D. (1991). Met*: a method for discriminating metonymy and metaphor by computer. Computational Linguistics, 17(1):49-90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Fass</author>
</authors>
<date>1997</date>
<booktitle>Processing Metonymy and Metaphor. Contemporary Studies in Cognitive Science &amp; Technology.</booktitle>
<publisher>Ablex.</publisher>
<location>New York:</location>
<contexts>
<context position="1725" citStr="Fass, 1997" startWordPosition="268" endWordPosition="269">alable framework in which practical applications of linguistic creativity can be implemented. 1 Introduction Words should not always be taken at face value. Figurative devices like metaphor can communicate far richer meanings than are evident from a superficial – and perhaps literally nonsensical – reading. Figurative Language Processing (FLP) thus uses a variety of special mechanisms and representations, to assign non-literal meanings not just to metaphors, but to similes, analogies, epithets, puns and other creative uses of language (see Martin, 1990; Fass, 1991; Way, 1991; Indurkhya, 1992; Fass, 1997; Barnden, 2006; Veale and Butnariu, 2010). Computationalists have explored heterodox solutions to the procedural and representational challenges of metaphor, and FLP more generally, ranging from flexible representations (e.g. the preference semantics of Wilks (1978) and the collative semantics of Fass (1991, 1997)) to processes of cross-domain structure alignment (e.g. structure mapping theory; see Gentner (1983) and Falkenhainer et al. 1989) and even structural inversion (Veale, 2006). Though thematically related, each approach to FLP is broadly distinct, giving computational form to differe</context>
</contexts>
<marker>Fass, 1997</marker>
<rawString>Fass, D. (1997). Processing Metonymy and Metaphor. Contemporary Studies in Cognitive Science &amp; Technology. New York: Ablex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="6659" citStr="Fellbaum, 1998" startWordPosition="1044" endWordPosition="1045">ains “Muslim bible”) to playful allusions and epithets (e.g. the CEO of a rubber company may be punningly described as a “rubber baron”). A creative IR system may even anticipate out-ofdictionary words, like chocoholic and sexoholic. Conventional IR systems use a range of query expansion techniques to automatically bolster a user’s query with additional keywords or weights, to permit the retrieval of relevant texts it might not otherwise match (e.g. Vernimb, 1977; Voorhees, 1994). Techniques vary, from the use of stemmers and morphological analysis to the use of thesauri (such as WordNet; see Fellbaum, 1998; Voorhees, 1998) to pad a query with synonyms, to the use of statistical analysis to identify more appropriate context-sensitive associations and near-synonyms (e.g. Xu and Croft, 1996). While some techniques may suggest conventional metaphors that have become lexicalized in a language, they are unlikely to identify relatively novel expressions. Crucially, expansion improves recall at the expense of overall precision, making automatic techniques even more dangerous when the goal is to retrieve results that are creative and relevant. Creative IR must balance a need for fine user control with t</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Fellbaum, C. (1998). WordNet: An Electronic Lexical Database. MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Fishlov</author>
</authors>
<title>Poetic and Non-Poetic Simile: Structure,</title>
<date>1992</date>
<journal>Semantics, Rhetoric. Poetics Today,</journal>
<volume>14</volume>
<issue>1</issue>
<pages>1--23</pages>
<contexts>
<context position="15245" citStr="Fishlov (1992)" startWordPosition="2472" endWordPosition="2473">rent, immutable, beautiful, tough, expensive, valuable, shiny, bright, lasting, desirable, strong, ..., hard} . If A is an adjective, then @A matches any element of the set {N1, N2, ..., Nn}, where each Ni is a noun denoting a stereotype for which A is a culturally established property. For example, @tall matches any element of {giraffe, skyscraper, tree, redwood, tower, sunflower, lighthouse, beanstalk, rocket, ..., supermodel}. Stereotypes crystallize in a language as clichés, so one can argue that stereotypes and clichés are little or no use to a creative IR system. Yet, as demonstrated in Fishlov (1992), creative language is replete with stereotypes, not in their cliched guises, but in novel and often incongruous combinations. The creative value of a stereotype lies in how it is used, as we’ll show later in section 4. 3.3 The Ad-Hoc Category Wildcard ^X Barsalou (1983) introduced the notion of an adhoc category, a cross-cutting collection of often disparate elements that cohere in the context of a specific task or goal. The ad-hoc nature of these categories is reflected in the difficulty we have in naming them concisely: the cumbersome “things to take on a camping trip” is Barsalou’s most ci</context>
<context position="26038" citStr="Fishlov (1992)" startWordPosition="4243" endWordPosition="4244">ase ??X” and “^negphrase ??X”. Idiom Savant is accessible online at: www.educatedinsolence.com/idiom-savant/ 4.3 Poetic Similes with The Jigsaw Bard The well-formed phrases of a large corpus can be viewed as the linguistic equivalent of objets trouves in art: readymade or “found” objects that might take on fresh meanings in a creative context. The phrase “robot fish”, for instance, denotes a moreor-less literal object in the context of autonomous robotic submersibles, but can also be used to convey a figurative meaning as part of a creative comparison (e.g., “he was as cold as a robot fish”). Fishlov (1992) argues that poetic comparisons are most resonant when they combine mutuallyreinforcing (if distant) ideas, to create memorable images and evoke nuanced feelings. Building on Fishlov’s argument, creative IR can be used to turn 283 the readymade phrases of the Google ngrams into vehicles for creative comparison. For a topic X and a property P, simple similes of the form “X is as P as S” are easily generated, where S E @P n ??X. Fishlov would dub these non-poetic similes (NPS). However, the query “?P @P” will retrieve corpus-attested elaborations of stereotypes in @P to suggest similes of the fo</context>
</contexts>
<marker>Fishlov, 1992</marker>
<rawString>Fishlov, D. (1992). Poetic and Non-Poetic Simile: Structure, Semantics, Rhetoric. Poetics Today, 14(1), 1-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gentner</author>
</authors>
<title>Structure-mapping: A Theoretical Framework.</title>
<date>1983</date>
<journal>Cognitive Science</journal>
<pages>7--155</pages>
<contexts>
<context position="2142" citStr="Gentner (1983)" startWordPosition="324" endWordPosition="325">ssign non-literal meanings not just to metaphors, but to similes, analogies, epithets, puns and other creative uses of language (see Martin, 1990; Fass, 1991; Way, 1991; Indurkhya, 1992; Fass, 1997; Barnden, 2006; Veale and Butnariu, 2010). Computationalists have explored heterodox solutions to the procedural and representational challenges of metaphor, and FLP more generally, ranging from flexible representations (e.g. the preference semantics of Wilks (1978) and the collative semantics of Fass (1991, 1997)) to processes of cross-domain structure alignment (e.g. structure mapping theory; see Gentner (1983) and Falkenhainer et al. 1989) and even structural inversion (Veale, 2006). Though thematically related, each approach to FLP is broadly distinct, giving computational form to different cognitive demands of creative language: thus, some focus on interdomain mappings (e.g. Gentner, 1983) while others focus more on intra-domain inference (e.g. Barnden, 2006). However, while computationally interesting, none has yet achieved the scalability or robustness needed to make a significant practical impact outside the laboratory. Moreover, such systems tend to be developed in isolation, and are rarely d</context>
</contexts>
<marker>Gentner, 1983</marker>
<rawString>Gentner, D. (1983), Structure-mapping: A Theoretical Framework. Cognitive Science 7:155–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J P Guilford</author>
</authors>
<date>1950</date>
<journal>Creativity, American Psychologist</journal>
<volume>5</volume>
<issue>9</issue>
<marker>Guilford, 1950</marker>
<rawString>Guilford, J.P. (1950) Creativity, American Psychologist 5(9):444–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hanks</author>
</authors>
<title>Similes and Sets: The English Preposition</title>
<date>2005</date>
<institution>Cermak. Charles University,</institution>
<location>Prague.</location>
<contexts>
<context position="8056" citStr="Hanks (2005)" startWordPosition="1255" endWordPosition="1256">rpora have been shown to produce nuanced models of lexical similarity (e.g. Weeds and Weir, 2005) as well as contextsensitive thesauri for a given domain (Lin, 1998). Hearst (1992) shows how a pattern like “Xs and other Ys” can be used to construct more fluid, context-specific taxonomies than those provided by WordNet (e.g. “athletes and other celebrities” suggests a context in which athletes are viewed as stars). Mason (2004) shows how statistical analysis can automatically detect and extract conventional metaphors from corpora, though creative metaphors still remain a tantalizing challenge. Hanks (2005) shows how the “Xs like A, B and C” construction allows us to derive flexible ad-hoc categories from corpora, while Hanks (2006) argues for a gradable conception of metaphoricity based on word-sense distributions in corpora. 279 Veale and Hao (2007) exploit the simile frame “as X as Y” to harvest a great many common similes and their underlying stereotypes from the web (e.g. “as hot as an oven”), while Veale and Hao (2010) show that the pattern “about as X as Y” retrieves an equally large collection of creative (if mostly ironic) comparisons. These authors demonstrate that a large vocabulary o</context>
<context position="11874" citStr="Hanks (2005)" startWordPosition="1890" endWordPosition="1891"> lexico-semantic relationship, such as synonymy, hyponymy or meronymy. A generic, lightweight resource like WordNet can provide these relations, or a richer ontology can be used if one is available (e.g. see Navigli and Velardi, 2003). Intuitively, each query term suggests other terms from its semantic neighborhood, yet there are practical limits to this intuition. Xi may not be an obvious or natural substitute for X. A neighborhood can be drawn too small, impacting recall, or too large, impacting precision. Corpus analysis suggests an approach that is both semantic and pragmatic. As noted in Hanks (2005), languages provide constructions for building ad-hoc sets of items that can be considered comparable in a given context. For instance, a coordination of bare plurals suggests that two ideas are related at a generic level, as in “priests and imams” or “mosques and synagogues”. More generally, consider the pattern “X and Y”, where X and Y are proper-names (e.g., “Zeus and Hera”), or X and Y are inflected nouns or verbs with the same inflection (e.g., the plurals “cats and dogs” or the verb forms “kicking and screaming”). Millions of matches for this pattern can be found in the Google 3-grams (B</context>
</contexts>
<marker>Hanks, 2005</marker>
<rawString>Hanks, P. (2005). Similes and Sets: The English Preposition ‘like’. In: Blatn‡, R. and Petkevic, V. (Eds.), Languages and Linguistics: Festschrift for Fr. Cermak. Charles University, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hanks</author>
</authors>
<title>Metaphoricity is gradable. In: Anatol Stefanowitsch and Stefan Th. Gries (Eds.), CorpusBased Approaches to Metaphor and Metonymy,.</title>
<date>2006</date>
<pages>17--35</pages>
<location>Berlin: Mouton</location>
<note>de Gruyter.</note>
<contexts>
<context position="8184" citStr="Hanks (2006)" startWordPosition="1279" endWordPosition="1280">esauri for a given domain (Lin, 1998). Hearst (1992) shows how a pattern like “Xs and other Ys” can be used to construct more fluid, context-specific taxonomies than those provided by WordNet (e.g. “athletes and other celebrities” suggests a context in which athletes are viewed as stars). Mason (2004) shows how statistical analysis can automatically detect and extract conventional metaphors from corpora, though creative metaphors still remain a tantalizing challenge. Hanks (2005) shows how the “Xs like A, B and C” construction allows us to derive flexible ad-hoc categories from corpora, while Hanks (2006) argues for a gradable conception of metaphoricity based on word-sense distributions in corpora. 279 Veale and Hao (2007) exploit the simile frame “as X as Y” to harvest a great many common similes and their underlying stereotypes from the web (e.g. “as hot as an oven”), while Veale and Hao (2010) show that the pattern “about as X as Y” retrieves an equally large collection of creative (if mostly ironic) comparisons. These authors demonstrate that a large vocabulary of stereotypical ideas (over 4000 nouns) and their salient properties (over 2000 adjectives) can be harvested from the web. We no</context>
</contexts>
<marker>Hanks, 2006</marker>
<rawString>Hanks, P. (2006). Metaphoricity is gradable. In: Anatol Stefanowitsch and Stefan Th. Gries (Eds.), CorpusBased Approaches to Metaphor and Metonymy,. 17-35. Berlin: Mouton de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proc. of the 14th Int. Conf. on Computational Linguistics,</booktitle>
<pages>539--545</pages>
<contexts>
<context position="7624" citStr="Hearst (1992)" startWordPosition="1191" endWordPosition="1192">cially, expansion improves recall at the expense of overall precision, making automatic techniques even more dangerous when the goal is to retrieve results that are creative and relevant. Creative IR must balance a need for fine user control with the statistical breadth and convenience of automatic expansion. Fortunately, statistical corpus analysis is an obvious area of overlap for IR and FLP. Distributional analyses of large corpora have been shown to produce nuanced models of lexical similarity (e.g. Weeds and Weir, 2005) as well as contextsensitive thesauri for a given domain (Lin, 1998). Hearst (1992) shows how a pattern like “Xs and other Ys” can be used to construct more fluid, context-specific taxonomies than those provided by WordNet (e.g. “athletes and other celebrities” suggests a context in which athletes are viewed as stars). Mason (2004) shows how statistical analysis can automatically detect and extract conventional metaphors from corpora, though creative metaphors still remain a tantalizing challenge. Hanks (2005) shows how the “Xs like A, B and C” construction allows us to derive flexible ad-hoc categories from corpora, while Hanks (2006) argues for a gradable conception of met</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Hearst, M. (1992). Automatic acquisition of hyponyms from large text corpora. In Proc. of the 14th Int. Conf. on Computational Linguistics, pp 539–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Indurkhya</author>
</authors>
<title>Metaphor and Cognition: Studies in Cognitive Systems.</title>
<date>1992</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht: The Netherlands.</location>
<contexts>
<context position="1713" citStr="Indurkhya, 1992" startWordPosition="266" endWordPosition="267">into a robust, scalable framework in which practical applications of linguistic creativity can be implemented. 1 Introduction Words should not always be taken at face value. Figurative devices like metaphor can communicate far richer meanings than are evident from a superficial – and perhaps literally nonsensical – reading. Figurative Language Processing (FLP) thus uses a variety of special mechanisms and representations, to assign non-literal meanings not just to metaphors, but to similes, analogies, epithets, puns and other creative uses of language (see Martin, 1990; Fass, 1991; Way, 1991; Indurkhya, 1992; Fass, 1997; Barnden, 2006; Veale and Butnariu, 2010). Computationalists have explored heterodox solutions to the procedural and representational challenges of metaphor, and FLP more generally, ranging from flexible representations (e.g. the preference semantics of Wilks (1978) and the collative semantics of Fass (1991, 1997)) to processes of cross-domain structure alignment (e.g. structure mapping theory; see Gentner (1983) and Falkenhainer et al. 1989) and even structural inversion (Veale, 2006). Though thematically related, each approach to FLP is broadly distinct, giving computational for</context>
</contexts>
<marker>Indurkhya, 1992</marker>
<rawString>Indurkhya, B. (1992). Metaphor and Cognition: Studies in Cognitive Systems. Kluwer Academic Publishers, Dordrecht: The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proc. of the 17th international conference on Computational linguistics,</booktitle>
<pages>768--774</pages>
<contexts>
<context position="7609" citStr="Lin, 1998" startWordPosition="1189" endWordPosition="1190">essions. Crucially, expansion improves recall at the expense of overall precision, making automatic techniques even more dangerous when the goal is to retrieve results that are creative and relevant. Creative IR must balance a need for fine user control with the statistical breadth and convenience of automatic expansion. Fortunately, statistical corpus analysis is an obvious area of overlap for IR and FLP. Distributional analyses of large corpora have been shown to produce nuanced models of lexical similarity (e.g. Weeds and Weir, 2005) as well as contextsensitive thesauri for a given domain (Lin, 1998). Hearst (1992) shows how a pattern like “Xs and other Ys” can be used to construct more fluid, context-specific taxonomies than those provided by WordNet (e.g. “athletes and other celebrities” suggests a context in which athletes are viewed as stars). Mason (2004) shows how statistical analysis can automatically detect and extract conventional metaphors from corpora, though creative metaphors still remain a tantalizing challenge. Hanks (2005) shows how the “Xs like A, B and C” construction allows us to derive flexible ad-hoc categories from corpora, while Hanks (2006) argues for a gradable co</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, D. (1998). Automatic retrieval and clustering of similar words. In Proc. of the 17th international conference on Computational linguistics, 768-774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E R MacCormac</author>
</authors>
<title>A Cognitive Theory of Metaphor.</title>
<date>1985</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="22338" citStr="MacCormac (1985)" startWordPosition="3662" endWordPosition="3663">m X, the wildcard ?X suggests those other terms that writers have considered to be comparable to X, while ??X extrapolates beyond the corpus evidence to suggest an even larger space of potential comparisons. A meaningful metaphor can be constructed for X by framing X with any stereotype to which it is pragmatically comparable, that is, any stereotype in ?X. Collectively, these stereotypes can impart the properties @?X to X. Suppose one wants to metaphorically ascribe the property P to X. The set @P contains those stereotypes for which P is culturally salient. Thus, close metaphors for X (what MacCormac (1985) dubs epiphors) in the context of P are suggested by ?X ∩ @P. More distant metaphors (MacCormac dubs these diaphors) are suggested by ??X ∩ @P. For instance, to describe a scholar as wise, one can use poet, yogi, philosopher or rabbi as comparisons. Yet even a simple metaphor will impart other features to a topic. If ^PS denotes the ad-hoc set of additional properties that may be inferred for X when a stereotype S is used to convey property P, then ^PS = ?P ∩ @@P. The query “^PS X” now finds corpus-attested elements of ^PS that can meaningfully be used to modify X. These IR formulations are us</context>
</contexts>
<marker>MacCormac, 1985</marker>
<rawString>MacCormac, E. R. (1985). A Cognitive Theory of Metaphor. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Martin</author>
</authors>
<title>A Computational Model of Metaphor Interpretation.</title>
<date>1990</date>
<publisher>Academic Press.</publisher>
<location>New York:</location>
<contexts>
<context position="1673" citStr="Martin, 1990" startWordPosition="260" endWordPosition="261">ions, and which also transplants FLP into a robust, scalable framework in which practical applications of linguistic creativity can be implemented. 1 Introduction Words should not always be taken at face value. Figurative devices like metaphor can communicate far richer meanings than are evident from a superficial – and perhaps literally nonsensical – reading. Figurative Language Processing (FLP) thus uses a variety of special mechanisms and representations, to assign non-literal meanings not just to metaphors, but to similes, analogies, epithets, puns and other creative uses of language (see Martin, 1990; Fass, 1991; Way, 1991; Indurkhya, 1992; Fass, 1997; Barnden, 2006; Veale and Butnariu, 2010). Computationalists have explored heterodox solutions to the procedural and representational challenges of metaphor, and FLP more generally, ranging from flexible representations (e.g. the preference semantics of Wilks (1978) and the collative semantics of Fass (1991, 1997)) to processes of cross-domain structure alignment (e.g. structure mapping theory; see Gentner (1983) and Falkenhainer et al. 1989) and even structural inversion (Veale, 2006). Though thematically related, each approach to FLP is br</context>
</contexts>
<marker>Martin, 1990</marker>
<rawString>Martin, J. H. (1990). A Computational Model of Metaphor Interpretation. New York: Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z J Mason</author>
</authors>
<date>2004</date>
<journal>CorMet: A Computational, Corpus-Based Conventional Metaphor Extraction System, Computational Linguistics,</journal>
<pages>30--1</pages>
<contexts>
<context position="7874" citStr="Mason (2004)" startWordPosition="1231" endWordPosition="1232">statistical breadth and convenience of automatic expansion. Fortunately, statistical corpus analysis is an obvious area of overlap for IR and FLP. Distributional analyses of large corpora have been shown to produce nuanced models of lexical similarity (e.g. Weeds and Weir, 2005) as well as contextsensitive thesauri for a given domain (Lin, 1998). Hearst (1992) shows how a pattern like “Xs and other Ys” can be used to construct more fluid, context-specific taxonomies than those provided by WordNet (e.g. “athletes and other celebrities” suggests a context in which athletes are viewed as stars). Mason (2004) shows how statistical analysis can automatically detect and extract conventional metaphors from corpora, though creative metaphors still remain a tantalizing challenge. Hanks (2005) shows how the “Xs like A, B and C” construction allows us to derive flexible ad-hoc categories from corpora, while Hanks (2006) argues for a gradable conception of metaphoricity based on word-sense distributions in corpora. 279 Veale and Hao (2007) exploit the simile frame “as X as Y” to harvest a great many common similes and their underlying stereotypes from the web (e.g. “as hot as an oven”), while Veale and Ha</context>
</contexts>
<marker>Mason, 2004</marker>
<rawString>Mason, Z. J. (2004). CorMet: A Computational, Corpus-Based Conventional Metaphor Extraction System, Computational Linguistics, 30(1):23-44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
</authors>
<title>The Semantic Wildcard.</title>
<date>2002</date>
<booktitle>In Proc. of the LREC Workshop on Creating and Using Semantics for Information Retrieval and Filtering. Canary Islands,</booktitle>
<location>Spain,</location>
<contexts>
<context position="9540" citStr="Mihalcea, 2002" startWordPosition="1501" endWordPosition="1502">al matching and automatic query expansion. 3 Creative Text Retrieval In language, creativity is always a matter of construal. While conventional IR queries articulate a need for information, creative IR queries articulate a need for expressions to convey the same meaning in a fresh or unusual way. A query and a matching phrase can be figuratively construed to have the same meaning if there is a non-literal mapping between the elements of the query and the elements of the phrase. In creative IR, this non-literal mapping is facilitated by the query’s explicit use of semantic wildcards (e.g. see Mihalcea, 2002). The wildcard * is a boon for power-users of the Google search engine, precisely because it allows users to focus on the retrieval of matching phrases rather than relevant documents. For instance, * can be used to find alternate ways of instantiating a culturally-established linguistic pattern, or “snowclone”: thus, the Google queries “In * no one can hear you scream” (from Alien), “Reader, I * him” (from Jane Eyre) and “This is your brain on *” (from a famous TV advert) find new ways in which old patterns have been instantiated for humorous effect on the Web. On a larger scale, Veale and Hao</context>
</contexts>
<marker>Mihalcea, 2002</marker>
<rawString>Mihalcea, R. (2002). The Semantic Wildcard. In Proc. of the LREC Workshop on Creating and Using Semantics for Information Retrieval and Filtering. Canary Islands, Spain, May 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
<author>P Velardi</author>
</authors>
<title>An Analysis of Ontology-based Query Expansion Strategies.</title>
<date>2003</date>
<booktitle>In Proc. of the workshop on Adaptive Text Extraction and Mining (ATEM 2003), at ECML 2003, the 14th European Conf. on Machine Learning,</booktitle>
<pages>42--49</pages>
<contexts>
<context position="11496" citStr="Navigli and Velardi, 2003" startWordPosition="1826" endWordPosition="1829">ings for its wildcards as a text collection will support. But we need more flexible and precise wildcards than *. We now consider three varieties of semantic wildcards that build on insights from corpus-linguistic approaches to FLP. 3.1 The Neighborhood Wildcard ?X Semantic query expansion replaces a query term X with a set {X, X1, X2, ..., Xn} where each Xi is related to X by a prescribed lexico-semantic relationship, such as synonymy, hyponymy or meronymy. A generic, lightweight resource like WordNet can provide these relations, or a richer ontology can be used if one is available (e.g. see Navigli and Velardi, 2003). Intuitively, each query term suggests other terms from its semantic neighborhood, yet there are practical limits to this intuition. Xi may not be an obvious or natural substitute for X. A neighborhood can be drawn too small, impacting recall, or too large, impacting precision. Corpus analysis suggests an approach that is both semantic and pragmatic. As noted in Hanks (2005), languages provide constructions for building ad-hoc sets of items that can be considered comparable in a given context. For instance, a coordination of bare plurals suggests that two ideas are related at a generic level,</context>
</contexts>
<marker>Navigli, Velardi, 2003</marker>
<rawString>Navigli, R. and Velardi, P. (2003). An Analysis of Ontology-based Query Expansion Strategies. In Proc. of the workshop on Adaptive Text Extraction and Mining (ATEM 2003), at ECML 2003, the 14th European Conf. on Machine Learning, 42–49</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
</authors>
<title>Automatic Information Organization and Retrieval.</title>
<date>1968</date>
<publisher>McGraw-Hill.</publisher>
<location>New York:</location>
<contexts>
<context position="3004" citStr="Salton, 1968" startWordPosition="454" endWordPosition="455">rdomain mappings (e.g. Gentner, 1983) while others focus more on intra-domain inference (e.g. Barnden, 2006). However, while computationally interesting, none has yet achieved the scalability or robustness needed to make a significant practical impact outside the laboratory. Moreover, such systems tend to be developed in isolation, and are rarely designed to cohere as part of a larger framework of creative reasoning (e.g. Boden, 1994). In contrast, Information Retrieval (IR) is both scalable and robust, and its results translate easily from the laboratory into practical applications (e.g. see Salton, 1968; Van Rijsbergen, 1979). Whereas FLP derives its utility and its fragility from its attempts to identify deeper meanings beneath the surface, the widespread applicability of IR stems directly from its superficial treatment of language 278 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 278–287, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics and meaning. IR does not distinguish between creative and conventional uses of language, or between literal and non-literal meanings. IR is also remarkably modular: its com</context>
</contexts>
<marker>Salton, 1968</marker>
<rawString>Salton, G. (1968). Automatic Information Organization and Retrieval. New York: McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Taylor</author>
</authors>
<date>1954</date>
<booktitle>Proverbial Comparisons and Similes from California. Folklore Studies 3.</booktitle>
<publisher>University of California Press.</publisher>
<location>Berkeley:</location>
<contexts>
<context position="14113" citStr="Taylor, 1954" startWordPosition="2283" endWordPosition="2284">nd simile pattern “as X and Xi as” to harvest {X1, ..., Xn} for each X. Moreover, to maximize recall, we use the Google API (rather than Google ngrams) to harvest suitable bindings for X and Xi from the web. For example, @witty = {charming, clever, intelligent, entertaining, ..., edgy, fun}. 3.2 The Cultural Stereotype Wildcard @X Dickens claims in A Christmas Carol that “the wisdom of a people is in the simile”. Similes exploit familiar stereotypes to describe a less familiar concept, so one can learn a great deal about a culture and its language from the similes that have the most currency (Taylor, 1954). The wildcard @X builds on the results of Veale and Hao (2007) to allow creative IR queries to retrieve matches on the basis of cultural expectations. This foundation provides a large set of adjectival features (over 2000) for a larger set of nouns (over 4000) denoting stereotypes for which these features are salient. If N is a noun, then @N matches any element of the set {A1, A2, ..., An}, where each Ai is an adjective denoting a stereotypical property of N. For example, @diamond matches any element of {transparent, immutable, beautiful, tough, expensive, valuable, shiny, bright, lasting, de</context>
</contexts>
<marker>Taylor, 1954</marker>
<rawString>Taylor, A. (1954). Proverbial Comparisons and Similes from California. Folklore Studies 3. Berkeley: University of California Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Van Rijsbergen</author>
</authors>
<title>Information Retrieval.</title>
<date>1979</date>
<publisher>Butterworth-Heinemann.</publisher>
<location>Oxford:</location>
<marker>Van Rijsbergen, 1979</marker>
<rawString>Van Rijsbergen, C. J. (1979). Information Retrieval. Oxford: Butterworth-Heinemann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Veale</author>
</authors>
<title>The Challenge of Creative Information Retrieval.</title>
<date>2004</date>
<booktitle>Computational Linguistics and Intelligent Text Processing: Lecture Notes in Computer Science,</booktitle>
<volume>2945</volume>
<pages>457--467</pages>
<contexts>
<context position="5663" citStr="Veale (2004)" startWordPosition="887" endWordPosition="888">applications of this creative IR paradigm in section 4. Empirical support for the FLP intuitions that underpin our new operators is provided in section 5. The paper concludes with some closing observations about future goals and developments in section 6. 2 Related Work and Ideas IR works on the premise that a user can turn an information need into an effective query by anticipating the language that is used to talk about a given topic in a target collection. If the collection uses creative language in speaking about a topic, then a query must also contain the seeds of this creative language. Veale (2004) introduces the idea of creative information retrieval to explore how an IR system can itself provide a degree of creative anticipation, acting as a mediator between the literal specification of a meaning and the retrieval of creative articulations of this meaning. This anticipation ranges from simple re-articulation (e.g. a text may implicitly evoke “Qur’an” even if it only contains “Muslim bible”) to playful allusions and epithets (e.g. the CEO of a rubber company may be punningly described as a “rubber baron”). A creative IR system may even anticipate out-ofdictionary words, like chocoholic</context>
</contexts>
<marker>Veale, 2004</marker>
<rawString>Veale, T. (2004). The Challenge of Creative Information Retrieval. Computational Linguistics and Intelligent Text Processing: Lecture Notes in Computer Science, Volume 2945/2004, 457-467.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Veale</author>
</authors>
<title>Re-Representation and Creative Analogy: A Lexico-Semantic Perspective.</title>
<date>2006</date>
<journal>New Generation Computing</journal>
<volume>24</volume>
<pages>223--240</pages>
<contexts>
<context position="2216" citStr="Veale, 2006" startWordPosition="336" endWordPosition="337"> epithets, puns and other creative uses of language (see Martin, 1990; Fass, 1991; Way, 1991; Indurkhya, 1992; Fass, 1997; Barnden, 2006; Veale and Butnariu, 2010). Computationalists have explored heterodox solutions to the procedural and representational challenges of metaphor, and FLP more generally, ranging from flexible representations (e.g. the preference semantics of Wilks (1978) and the collative semantics of Fass (1991, 1997)) to processes of cross-domain structure alignment (e.g. structure mapping theory; see Gentner (1983) and Falkenhainer et al. 1989) and even structural inversion (Veale, 2006). Though thematically related, each approach to FLP is broadly distinct, giving computational form to different cognitive demands of creative language: thus, some focus on interdomain mappings (e.g. Gentner, 1983) while others focus more on intra-domain inference (e.g. Barnden, 2006). However, while computationally interesting, none has yet achieved the scalability or robustness needed to make a significant practical impact outside the laboratory. Moreover, such systems tend to be developed in isolation, and are rarely designed to cohere as part of a larger framework of creative reasoning (e.g</context>
</contexts>
<marker>Veale, 2006</marker>
<rawString>Veale, T. (2006). Re-Representation and Creative Analogy: A Lexico-Semantic Perspective. New Generation Computing 24, pp 223-240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Veale</author>
<author>Y Hao</author>
</authors>
<title>Making Lexical Ontologies Functional and Context-Sensitive.</title>
<date>2007</date>
<booktitle>In Proc. of the 46th Annual Meeting of the Assoc. of Computational Linguistics.</booktitle>
<contexts>
<context position="8305" citStr="Veale and Hao (2007)" startWordPosition="1295" endWordPosition="1298">nstruct more fluid, context-specific taxonomies than those provided by WordNet (e.g. “athletes and other celebrities” suggests a context in which athletes are viewed as stars). Mason (2004) shows how statistical analysis can automatically detect and extract conventional metaphors from corpora, though creative metaphors still remain a tantalizing challenge. Hanks (2005) shows how the “Xs like A, B and C” construction allows us to derive flexible ad-hoc categories from corpora, while Hanks (2006) argues for a gradable conception of metaphoricity based on word-sense distributions in corpora. 279 Veale and Hao (2007) exploit the simile frame “as X as Y” to harvest a great many common similes and their underlying stereotypes from the web (e.g. “as hot as an oven”), while Veale and Hao (2010) show that the pattern “about as X as Y” retrieves an equally large collection of creative (if mostly ironic) comparisons. These authors demonstrate that a large vocabulary of stereotypical ideas (over 4000 nouns) and their salient properties (over 2000 adjectives) can be harvested from the web. We now build on these results to develop a set of new semantic operators, that use corpus-derived knowledge to support finely </context>
<context position="10147" citStr="Veale and Hao (2007)" startWordPosition="1606" endWordPosition="1609">halcea, 2002). The wildcard * is a boon for power-users of the Google search engine, precisely because it allows users to focus on the retrieval of matching phrases rather than relevant documents. For instance, * can be used to find alternate ways of instantiating a culturally-established linguistic pattern, or “snowclone”: thus, the Google queries “In * no one can hear you scream” (from Alien), “Reader, I * him” (from Jane Eyre) and “This is your brain on *” (from a famous TV advert) find new ways in which old patterns have been instantiated for humorous effect on the Web. On a larger scale, Veale and Hao (2007) used the * wildcard to harvest web similes, but reported that harvesting cultural data with wildcards is not a straightforward process. Google and other engines are designed to maximize document relevance and to rank results accordingly. They are not designed to maximize the diversity of results, or to find the largest set of wildcard bindings. Nor are they designed to find the most commonplace bindings for wildcards. Following Guilford’s (1950) pioneering work, diversity is widely considered a key component in the psychology of creativity. By focusing on the phrase level rather than the docu</context>
<context position="14176" citStr="Veale and Hao (2007)" startWordPosition="2293" endWordPosition="2296">Xn} for each X. Moreover, to maximize recall, we use the Google API (rather than Google ngrams) to harvest suitable bindings for X and Xi from the web. For example, @witty = {charming, clever, intelligent, entertaining, ..., edgy, fun}. 3.2 The Cultural Stereotype Wildcard @X Dickens claims in A Christmas Carol that “the wisdom of a people is in the simile”. Similes exploit familiar stereotypes to describe a less familiar concept, so one can learn a great deal about a culture and its language from the similes that have the most currency (Taylor, 1954). The wildcard @X builds on the results of Veale and Hao (2007) to allow creative IR queries to retrieve matches on the basis of cultural expectations. This foundation provides a large set of adjectival features (over 2000) for a larger set of nouns (over 4000) denoting stereotypes for which these features are salient. If N is a noun, then @N matches any element of the set {A1, A2, ..., An}, where each Ai is an adjective denoting a stereotypical property of N. For example, @diamond matches any element of {transparent, immutable, beautiful, tough, expensive, valuable, shiny, bright, lasting, desirable, strong, ..., hard} . If A is an adjective, then @A mat</context>
</contexts>
<marker>Veale, Hao, 2007</marker>
<rawString>Veale, T. and Hao, Y. (2007). Making Lexical Ontologies Functional and Context-Sensitive. In Proc. of the 46th Annual Meeting of the Assoc. of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Veale</author>
<author>Y Hao</author>
</authors>
<title>Detecting Ironic Intent in Creative Comparisons.</title>
<date>2010</date>
<booktitle>In Proc. of ECAIÕ2010, the 19th European Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="8482" citStr="Veale and Hao (2010)" startWordPosition="1328" endWordPosition="1331">Mason (2004) shows how statistical analysis can automatically detect and extract conventional metaphors from corpora, though creative metaphors still remain a tantalizing challenge. Hanks (2005) shows how the “Xs like A, B and C” construction allows us to derive flexible ad-hoc categories from corpora, while Hanks (2006) argues for a gradable conception of metaphoricity based on word-sense distributions in corpora. 279 Veale and Hao (2007) exploit the simile frame “as X as Y” to harvest a great many common similes and their underlying stereotypes from the web (e.g. “as hot as an oven”), while Veale and Hao (2010) show that the pattern “about as X as Y” retrieves an equally large collection of creative (if mostly ironic) comparisons. These authors demonstrate that a large vocabulary of stereotypical ideas (over 4000 nouns) and their salient properties (over 2000 adjectives) can be harvested from the web. We now build on these results to develop a set of new semantic operators, that use corpus-derived knowledge to support finely controlled non-literal matching and automatic query expansion. 3 Creative Text Retrieval In language, creativity is always a matter of construal. While conventional IR queries a</context>
</contexts>
<marker>Veale, Hao, 2010</marker>
<rawString>Veale, T. and Hao, Y. (2010). Detecting Ironic Intent in Creative Comparisons. In Proc. of ECAIÕ2010, the 19th European Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Veale</author>
<author>C Butnariu</author>
</authors>
<title>Harvesting and Understanding On-line Neologisms. In: Onysko,</title>
<date>2010</date>
<pages>393--416</pages>
<institution>Mouton De Gruyter.</institution>
<contexts>
<context position="1767" citStr="Veale and Butnariu, 2010" startWordPosition="272" endWordPosition="275">ractical applications of linguistic creativity can be implemented. 1 Introduction Words should not always be taken at face value. Figurative devices like metaphor can communicate far richer meanings than are evident from a superficial – and perhaps literally nonsensical – reading. Figurative Language Processing (FLP) thus uses a variety of special mechanisms and representations, to assign non-literal meanings not just to metaphors, but to similes, analogies, epithets, puns and other creative uses of language (see Martin, 1990; Fass, 1991; Way, 1991; Indurkhya, 1992; Fass, 1997; Barnden, 2006; Veale and Butnariu, 2010). Computationalists have explored heterodox solutions to the procedural and representational challenges of metaphor, and FLP more generally, ranging from flexible representations (e.g. the preference semantics of Wilks (1978) and the collative semantics of Fass (1991, 1997)) to processes of cross-domain structure alignment (e.g. structure mapping theory; see Gentner (1983) and Falkenhainer et al. 1989) and even structural inversion (Veale, 2006). Though thematically related, each approach to FLP is broadly distinct, giving computational form to different cognitive demands of creative language:</context>
</contexts>
<marker>Veale, Butnariu, 2010</marker>
<rawString>Veale, T. and Butnariu, C. (2010). Harvesting and Understanding On-line Neologisms. In: Onysko, A. and Michel, S. (Eds.), Cognitive Perspectives on Word Formation. 393-416. Mouton De Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Vernimb</author>
</authors>
<title>Automatic Query Adjustment</title>
<date>1977</date>
<booktitle>in Document Retrieval. Information Processing &amp; Management.</booktitle>
<pages>13--6</pages>
<contexts>
<context position="6512" citStr="Vernimb, 1977" startWordPosition="1021" endWordPosition="1022">rticulations of this meaning. This anticipation ranges from simple re-articulation (e.g. a text may implicitly evoke “Qur’an” even if it only contains “Muslim bible”) to playful allusions and epithets (e.g. the CEO of a rubber company may be punningly described as a “rubber baron”). A creative IR system may even anticipate out-ofdictionary words, like chocoholic and sexoholic. Conventional IR systems use a range of query expansion techniques to automatically bolster a user’s query with additional keywords or weights, to permit the retrieval of relevant texts it might not otherwise match (e.g. Vernimb, 1977; Voorhees, 1994). Techniques vary, from the use of stemmers and morphological analysis to the use of thesauri (such as WordNet; see Fellbaum, 1998; Voorhees, 1998) to pad a query with synonyms, to the use of statistical analysis to identify more appropriate context-sensitive associations and near-synonyms (e.g. Xu and Croft, 1996). While some techniques may suggest conventional metaphors that have become lexicalized in a language, they are unlikely to identify relatively novel expressions. Crucially, expansion improves recall at the expense of overall precision, making automatic techniques ev</context>
</contexts>
<marker>Vernimb, 1977</marker>
<rawString>Vernimb, C. (1977). Automatic Query Adjustment in Document Retrieval. Information Processing &amp; Management. 13(6):339-353.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>Query Expansion Using Lexical-Semantic Relations.</title>
<date>1994</date>
<booktitle>In the proc. of SIGIR 94, the 17th International Conference on Research and Development in Information Retrieval.</booktitle>
<pages>61--69</pages>
<publisher>SpringerVerlag,</publisher>
<location>Berlin:</location>
<contexts>
<context position="6529" citStr="Voorhees, 1994" startWordPosition="1023" endWordPosition="1024"> this meaning. This anticipation ranges from simple re-articulation (e.g. a text may implicitly evoke “Qur’an” even if it only contains “Muslim bible”) to playful allusions and epithets (e.g. the CEO of a rubber company may be punningly described as a “rubber baron”). A creative IR system may even anticipate out-ofdictionary words, like chocoholic and sexoholic. Conventional IR systems use a range of query expansion techniques to automatically bolster a user’s query with additional keywords or weights, to permit the retrieval of relevant texts it might not otherwise match (e.g. Vernimb, 1977; Voorhees, 1994). Techniques vary, from the use of stemmers and morphological analysis to the use of thesauri (such as WordNet; see Fellbaum, 1998; Voorhees, 1998) to pad a query with synonyms, to the use of statistical analysis to identify more appropriate context-sensitive associations and near-synonyms (e.g. Xu and Croft, 1996). While some techniques may suggest conventional metaphors that have become lexicalized in a language, they are unlikely to identify relatively novel expressions. Crucially, expansion improves recall at the expense of overall precision, making automatic techniques even more dangerous</context>
</contexts>
<marker>Voorhees, 1994</marker>
<rawString>Voorhees, E. M. (1994). Query Expansion Using Lexical-Semantic Relations. In the proc. of SIGIR 94, the 17th International Conference on Research and Development in Information Retrieval. Berlin: SpringerVerlag, 61-69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>Using WordNet for text retrieval. WordNet, An Electronic Lexical Database,</title>
<date>1998</date>
<pages>285--303</pages>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="6676" citStr="Voorhees, 1998" startWordPosition="1046" endWordPosition="1047">le”) to playful allusions and epithets (e.g. the CEO of a rubber company may be punningly described as a “rubber baron”). A creative IR system may even anticipate out-ofdictionary words, like chocoholic and sexoholic. Conventional IR systems use a range of query expansion techniques to automatically bolster a user’s query with additional keywords or weights, to permit the retrieval of relevant texts it might not otherwise match (e.g. Vernimb, 1977; Voorhees, 1994). Techniques vary, from the use of stemmers and morphological analysis to the use of thesauri (such as WordNet; see Fellbaum, 1998; Voorhees, 1998) to pad a query with synonyms, to the use of statistical analysis to identify more appropriate context-sensitive associations and near-synonyms (e.g. Xu and Croft, 1996). While some techniques may suggest conventional metaphors that have become lexicalized in a language, they are unlikely to identify relatively novel expressions. Crucially, expansion improves recall at the expense of overall precision, making automatic techniques even more dangerous when the goal is to retrieve results that are creative and relevant. Creative IR must balance a need for fine user control with the statistical br</context>
</contexts>
<marker>Voorhees, 1998</marker>
<rawString>Voorhees, E. M. (1998). Using WordNet for text retrieval. WordNet, An Electronic Lexical Database, 285–303. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E C Way</author>
</authors>
<title>Knowledge Representation and Metaphor. Studies in Cognitive systems.</title>
<date>1991</date>
<publisher>Kluwer.</publisher>
<location>Holland:</location>
<contexts>
<context position="1696" citStr="Way, 1991" startWordPosition="264" endWordPosition="265">plants FLP into a robust, scalable framework in which practical applications of linguistic creativity can be implemented. 1 Introduction Words should not always be taken at face value. Figurative devices like metaphor can communicate far richer meanings than are evident from a superficial – and perhaps literally nonsensical – reading. Figurative Language Processing (FLP) thus uses a variety of special mechanisms and representations, to assign non-literal meanings not just to metaphors, but to similes, analogies, epithets, puns and other creative uses of language (see Martin, 1990; Fass, 1991; Way, 1991; Indurkhya, 1992; Fass, 1997; Barnden, 2006; Veale and Butnariu, 2010). Computationalists have explored heterodox solutions to the procedural and representational challenges of metaphor, and FLP more generally, ranging from flexible representations (e.g. the preference semantics of Wilks (1978) and the collative semantics of Fass (1991, 1997)) to processes of cross-domain structure alignment (e.g. structure mapping theory; see Gentner (1983) and Falkenhainer et al. 1989) and even structural inversion (Veale, 2006). Though thematically related, each approach to FLP is broadly distinct, giving </context>
</contexts>
<marker>Way, 1991</marker>
<rawString>Way, E. C. (1991). Knowledge Representation and Metaphor. Studies in Cognitive systems. Holland: Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Weeds</author>
<author>D Weir</author>
</authors>
<title>Co-occurrence retrieval: A flexible framework for lexical distributional similarity.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>4</issue>
<contexts>
<context position="7541" citStr="Weeds and Weir, 2005" startWordPosition="1175" endWordPosition="1178"> lexicalized in a language, they are unlikely to identify relatively novel expressions. Crucially, expansion improves recall at the expense of overall precision, making automatic techniques even more dangerous when the goal is to retrieve results that are creative and relevant. Creative IR must balance a need for fine user control with the statistical breadth and convenience of automatic expansion. Fortunately, statistical corpus analysis is an obvious area of overlap for IR and FLP. Distributional analyses of large corpora have been shown to produce nuanced models of lexical similarity (e.g. Weeds and Weir, 2005) as well as contextsensitive thesauri for a given domain (Lin, 1998). Hearst (1992) shows how a pattern like “Xs and other Ys” can be used to construct more fluid, context-specific taxonomies than those provided by WordNet (e.g. “athletes and other celebrities” suggests a context in which athletes are viewed as stars). Mason (2004) shows how statistical analysis can automatically detect and extract conventional metaphors from corpora, though creative metaphors still remain a tantalizing challenge. Hanks (2005) shows how the “Xs like A, B and C” construction allows us to derive flexible ad-hoc </context>
</contexts>
<marker>Weeds, Weir, 2005</marker>
<rawString>Weeds, J. and Weir, D. (2005). Co-occurrence retrieval: A flexible framework for lexical distributional similarity. Computational Linguistics, 31(4):433–475.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Whissell</author>
</authors>
<title>The dictionary of affect in</title>
<date>1989</date>
<pages>113--131</pages>
<publisher>NY: Harcourt Brace,</publisher>
<marker>Whissell, 1989</marker>
<rawString>Whissell, C. (1989). The dictionary of affect in language. R. Plutchnik &amp; H. Kellerman (Eds.) Emotion: Theory and research. NY: Harcourt Brace, 113-131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wilks</author>
</authors>
<date>1978</date>
<journal>Making Preferences More Active, Artificial Intelligence</journal>
<volume>11</volume>
<contexts>
<context position="1992" citStr="Wilks (1978)" startWordPosition="302" endWordPosition="303">nd perhaps literally nonsensical – reading. Figurative Language Processing (FLP) thus uses a variety of special mechanisms and representations, to assign non-literal meanings not just to metaphors, but to similes, analogies, epithets, puns and other creative uses of language (see Martin, 1990; Fass, 1991; Way, 1991; Indurkhya, 1992; Fass, 1997; Barnden, 2006; Veale and Butnariu, 2010). Computationalists have explored heterodox solutions to the procedural and representational challenges of metaphor, and FLP more generally, ranging from flexible representations (e.g. the preference semantics of Wilks (1978) and the collative semantics of Fass (1991, 1997)) to processes of cross-domain structure alignment (e.g. structure mapping theory; see Gentner (1983) and Falkenhainer et al. 1989) and even structural inversion (Veale, 2006). Though thematically related, each approach to FLP is broadly distinct, giving computational form to different cognitive demands of creative language: thus, some focus on interdomain mappings (e.g. Gentner, 1983) while others focus more on intra-domain inference (e.g. Barnden, 2006). However, while computationally interesting, none has yet achieved the scalability or robus</context>
</contexts>
<marker>Wilks, 1978</marker>
<rawString>Wilks, Y. (1978). Making Preferences More Active, Artificial Intelligence 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xu</author>
<author>B W Croft</author>
</authors>
<title>Query expansion using local and global document analysis.</title>
<date>1996</date>
<booktitle>In Proc. of the 19th annual international ACM SIGIR conference on Research and development in information retrieval.</booktitle>
<contexts>
<context position="6845" citStr="Xu and Croft, 1996" startWordPosition="1069" endWordPosition="1072">t-ofdictionary words, like chocoholic and sexoholic. Conventional IR systems use a range of query expansion techniques to automatically bolster a user’s query with additional keywords or weights, to permit the retrieval of relevant texts it might not otherwise match (e.g. Vernimb, 1977; Voorhees, 1994). Techniques vary, from the use of stemmers and morphological analysis to the use of thesauri (such as WordNet; see Fellbaum, 1998; Voorhees, 1998) to pad a query with synonyms, to the use of statistical analysis to identify more appropriate context-sensitive associations and near-synonyms (e.g. Xu and Croft, 1996). While some techniques may suggest conventional metaphors that have become lexicalized in a language, they are unlikely to identify relatively novel expressions. Crucially, expansion improves recall at the expense of overall precision, making automatic techniques even more dangerous when the goal is to retrieve results that are creative and relevant. Creative IR must balance a need for fine user control with the statistical breadth and convenience of automatic expansion. Fortunately, statistical corpus analysis is an obvious area of overlap for IR and FLP. Distributional analyses of large cor</context>
</contexts>
<marker>Xu, Croft, 1996</marker>
<rawString>Xu, J. and Croft, B. W. (1996). Query expansion using local and global document analysis. In Proc. of the 19th annual international ACM SIGIR conference on Research and development in information retrieval.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>