<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.528122">
RECOGNIZING AND RESPONDING TO PLAN-ORIENTED MISCONCEPTIONS
</title>
<author confidence="0.74698">
Alex Quilici
</author>
<title confidence="0.594549">
Michael G. Dyer
</title>
<author confidence="0.554801">
Margot Flowers
</author>
<affiliation confidence="0.620087">
Artificial Intelligence Laboratory
Computer Science Department
University of California
</affiliation>
<address confidence="0.483776">
Los Angeles, CA 90024
</address>
<bodyText confidence="0.998446090909091">
This paper discusses the problem of recognizing and responding to plan-oriented misconceptions in
advice-seeking dialogs, concentrating on the problems of novice computer users. A cooperative response
is one that not only corrects the user&apos;s mistaken belief, but also addresses the missing or mistaken user
beliefs that led to it. Responding appropriately to a potentially incorrect user belief is presented as a
process of 1. checking whether the advisor holds the user&apos;s belief; 2. confirming the belief as a
misconception by finding an explanation for why the advisor does not hold this belief; 3. detecting the
mistaken beliefs underlying the misconception by trying to explain why the user holds the incorrect
belief, and 4. providing these explanations to the user. An explanation is shown to correspond to a set
of advisor beliefs, and searching for an explanation to proving whether various abstract configurations
of advisor beliefs hold. A taxonomy of domain-independent explanations for potential user misconcep-
tions involving plan applicability conditions, preconditions, and effects is presented.
</bodyText>
<sectionHeader confidence="0.999839" genericHeader="abstract">
1 INTRODUCTION
</sectionHeader>
<bodyText confidence="0.961998622222222">
A novice computer user having trouble performing a
task often describes his problem to an expert, who is
then expected to explain its cause and provide its
solution. Consider, for example, the following advice-
seeking dialog between a novice UNIX user and his
more experienced counterpart.
User: I tried to remove a file with the &amp;quot;rm&amp;quot; com-
mand. But the file was not removed and the error
message was permission denied. I checked and I
own the file. What&apos;s wrong?
Advisor: To remove a file, you need to be able to
write into the directory containing it. You do not
need to own the file.
User: How do I make that directory writeable?
Advisor: To make a directory writeable, use the
&amp;quot;chmod +w&amp;quot; command.
User: Wait a second while I try it. No, &amp;quot;chmod +w&amp;quot;
prints the error message &amp;quot;permission denied&amp;quot;.
Advisor: To make a directory writeable, you need to
own it.
User: So to remove a file, I have to own the directory
that contains it.
Advisor: No, to remove a file, you need to have write
permission on the directory that contains it. You
do not need to own the directory that contains it.
You need to own that directory when you do not
already have write permission on it.
User: So how do I remove the file?
Advisor: Send mail to whomever has write permis-
sion on the directory, asking him to remove the file
for you.
Participating as the advisor in such a dialog requires the
ability to recognize and respond to missing or mistaken
user beliefs about plan applicability conditions, precon-
ditions, and effects. The advisor above recognizes two
user misconceptions. The user first incorrectly believes
that owning a file is a precondition to removing it, and
then incorrectly believes that the precondition is owning
the directory containing it. This advisor also notices
several gaps in the user&apos;s knowledge. The user has no
plan for making a directory writeable, does not know
why the advisor&apos;s plan for doing so failed, and has no
plan for removing a file when the directory in which the
file resides is not writeable.
A cooperative advisor response to a missing user
</bodyText>
<footnote confidence="0.82852225">
Copyright 1988 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided
that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page. To
copy otherwise, or to republish, requires a fee and/or specific permission.
0362-613X/ 88 /thoo■-•$133.0o
</footnote>
<page confidence="0.867154">
38 Computational Linguistics, Volume 14, Number 3, September 1988
</page>
<note confidence="0.946553">
Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptions
</note>
<bodyText confidence="0.999881210526316">
belief simply provides that belief. The advisor above
provides the missing beliefs that the plan for making a
directory writeable is to use the &amp;quot;chmod +w&amp;quot; com-
mand, that owning a directory is a precondition to
making it writeable, and that the plan for removing a file
when all else fails is to send mail requesting its removal
to some person who has write permission on the direc-
tory in which it resides.
However, for a mistaken belief the advisor must not
only point out that the belief is incorrect and provide a
correction, but must also address the missing or mis-
taken beliefs that are the source of this misconception.
Above, this is done by pointing out that the actual
precondition is being able to write into the directory that
contains it, and by explaining that owning that directory
is necessary only if it is not already writeable.
In this paper we examine the problem of detecting
and responding to plan-oriented user misconceptions.
This problem can be broken into several subproblems:
</bodyText>
<listItem confidence="0.998530666666667">
1. Mapping the user&apos;s natural language problem de-
scription into a set of user beliefs.
2. Determining which of these beliefs are incorrect.
3. Inferring the missing or mistaken user beliefs that
might have led to these incorrect beliefs.
4. Selecting the advisor beliefs to present to the user as
a conversationally cooperative response.
5. Mapping these advisor beliefs into a natural language
response.
</listItem>
<bodyText confidence="0.999868">
Here we provide a computational model of (2), (3), and
(4). The input is a set of potentially incorrect user
beliefs. The output is a set of advisor beliefs to present
to the user which correct any mistaken user beliefs and
address the missing or mistaken user beliefs that may
have led to them. We consider three types of user
beliefs: those involving plan applicability conditions
(whether a particular plan should be used to achieve a
goal), enablements (whether a particular state must
exist before a plan can achieve a goal), and effects
(whether a state will exist as a result of a plan&apos;s
execution).
</bodyText>
<sectionHeader confidence="0.978206" genericHeader="keywords">
2 AN EXPLANATION-BASED APPROACH
</sectionHeader>
<bodyText confidence="0.999000048387097">
How can an advisor determine whether a particular user
belief is mistaken and understand how the user came to
believe it? And furthermore, how can the advisor de-
termine the contents of a cooperative response to the
user&apos;s misconception?
An advisor presented with a user belief must do
several things. He must first determine whether he
shares the user&apos;s belief. If he does, it is clearly not a
misconception. Assuming that he does not share that
belief, the advisor must confirm that it is, in fact, a
misconception, and then decide which user beliefs led
to it. (If the advisor cannot confirm that the user&apos;s belief
is mistaken, it could become a new advisor belief.)
We suggest an explanation-based approach to ac-
complish these tasks. To confirm that the user&apos;s belief is
a misconception, the advisor tries to find an explanation
for why he does not hold the user&apos;s belief. To infer the
problematic user beliefs underlying the user&apos;s mistaken
belief, the advisor tries to find an explanation for why
the user does hold this belief. These two explanations
constitute the advisor&apos;s response to the user.
We illustrate our approach by showing how the
advisor arrives at the response found in this exchange
from our introductory dialog.
User: So to remove a file, I have to own the directory
that contains it.
Advisor: No, to remove a file, you need to have write
permission on the directory that contains it. You
do not need to own the directory that contains it.
You need to own that directory when you do not
already have write permission on it.
Here the user&apos;s belief is that a file cannot be removed
without owning the directory in which it resides.
The advisor first tries to verify that he holds the
user&apos;s belief. In this case he cannot, so he must now try
to determine the reason why he does not hold this
belief. The explanation the advisor finds is that the
user&apos;s belief is contradicted by his belief that a file can
be removed without owning the directory in which it
resides, and that the user&apos;s belief can be replaced by his
belief that a file cannot be removed without write
permission on that directory.
At this point the advisor has confirmed that the
user&apos;s belief is a misconception. Now he must try to
discover which user beliefs led to this error. To do so,
the advisor tries to understand why the user holds this
erroneous belief. His explanation is that the user is
unaware that to remove a file it is really only necessary
to have write permission on the directory containing it,
and that owning that directory is necessary only if one
does not already have write permission on it. In other
words, the user is unaware that owning a directory is
not a precondition for removing a file, but a precondi-
tion for achieving one of its preconditions.
Once the advisor finds these explanations, he pres-
ents them to the user as the response to his misconcep-
tion. The response corrects the user&apos;s misconception by
pointing out that the user&apos;s claimed precondition for
removing a file is incorrect, by providing the actual
precondition for removing a file, and by providing the
missing user beliefs that led to the user&apos;s misconcep-
tion.
</bodyText>
<sectionHeader confidence="0.89293" genericHeader="introduction">
2.1 OTHER WORK IN EXPLANATION-BASED
UNDERSTANDING
</sectionHeader>
<bodyText confidence="0.999845428571429">
Our approach derives from work in explanation-based
story understanding (Schank 1986, Dyer 1983, Wilensky
1983, 1978, Cullingford 1978, Schank and Abelson
1977). The basic idea is that to understand a particular
input, such as a person&apos;s action, we have to explain why
it has occurred. One way to find an explanation for a
person&apos;s action is to relate it to known goals the person
</bodyText>
<note confidence="0.6851505">
Computational Linguistics, Volume 14, Number 3, September 1988 39
Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptions
</note>
<bodyText confidence="0.999935117647059">
is trying to achieve. Suppose, for example, that a story
understander reads that a hungry character bought a
restaurant guidebook (Wilensky 1983). One explanation
for this action is that hungry people want to eat, to eat
you have to be near food, to be near food you have
know where it is and then go there. The guidebook says
where the food is. This explanation can be constructed
either by using rules to build a reasoning chain or by
applying pre-existing schemas that capture the relation-
ship.
Our task may be thought of as trying to understand
why an actor (either the user or advisor) does or does
not hold a particular belief, a task similar to that faced
by explanation-based story understanders. Because of
the similarities in our tasks, we use the same approach,
trying to construct a potential explanation for the beliefs
we are trying to understand.
</bodyText>
<subsectionHeader confidence="0.996063">
2.2 THE REST OF THE PAPER
</subsectionHeader>
<bodyText confidence="0.9998985">
Subsequent sections of the paper present our approach
in more detail. First, we describe the representation we
use to represent plan-oriented user and advisor beliefs.
Then, we examine the process by which the necessary
explanations are found and provide a taxonomy of
explanations for the types of beliefs we consider. Fi-
nally, we show how our approach compares with other
work in detecting and correcting user misconceptions.
</bodyText>
<sectionHeader confidence="0.994815" genericHeader="method">
3 REPRESENTING USER AND ADVISOR BELIEFS
</sectionHeader>
<bodyText confidence="0.968755972972973">
The mistaken user beliefs that we consider involve plan
applicability conditions, enablements, and effects. In
this section we describe how these beliefs are repre-
sented. In essence, we make use of existing frameworks
for representing planning knowledge, except that we are
careful to distinguish between user and advisor beliefs.
Traditional planning systems (Fikes and Nilsson
1974, Sacerdoti 1974) represent an agent&apos;s planning
knowledge as a data base of operators associated with
applicability conditions, preconditions, and effects.
Since these systems have only one agent, the planner,
the entries in the data base are implicitly assumed to
represent that agent&apos;s beliefs. However, because user
misconceptions occur when the user&apos;s planning knowl-
edge differs from the advisor&apos;s, systems that deal with
user misconceptions must explicitly distinguish be-
tween advisor beliefs about what the user knows and
advisor beliefs about what the advisor knows.
Our representation for beliefs (Abelson 1973, 1979) is
similar to that used by existing systems that keep track
of the possibly contradictory knowledge of multiple
participants (Alvarado 1987; Alvarado, Dyer, and Flow-
ers 1986; Flowers, McGuire, and Birnbaum 1982; Pol-
lack 1986). A belief relation represents an advisor&apos;s
belief that an actor maintains that a particular plan
applicability condition, precondition, or effect holds.
The actor is either the user or the advisor.
belief(user, R) Advisor believes that user
maintains R
belief(advisor, R) Advisor believes that advisor
maintains R
In this paper we do not discuss beliefs involving other
relationships, such as a belief that an object has a
particular property. In addition, for readability we do
not use the belief predicate here, but instead precede a
list of planning relationships with either &amp;quot;the user
believes&amp;quot; or &amp;quot;the advisor believes&amp;quot;.
</bodyText>
<subsectionHeader confidence="0.998161">
3.1 REPRESENTING PLANNING RELATIONSHIPS
</subsectionHeader>
<bodyText confidence="0.997801857142857">
The planning relation can be one of the relations be-
tween actions and states shown below. Here A denotes
an action, which is either a primitive operator whose
execution results in a set of state changes, or a plan,
which is a sequence of these operators. S, Si, and S2
denote states, which are descriptions of properties of
objects.
</bodyText>
<listItem confidence="0.977561">
causes(A,S) Executing A has an effect S
!causes(A,S) Executing A does not have effect
</listItem>
<equation confidence="0.894226166666667">
enables(S1,A,S2) 51 is necessary for A to have S2
as an effect
!enables(S1,A,S2) S1 is unnecessary for A to have
S2 as an effect
applies(A,S) A is a correct or normal plan for
achieving goal state S
lapplies(A,S) A is not a plan for achieving S
precludes(S1,S2) S1 and S2 cannot exist
simultaneously
!precludes(S1,S2) S 1 and S2 can exist
simultaneously
goal(A,S) Actor A wants to achieve S
</equation>
<bodyText confidence="0.9997455">
These relationships are derived from existing represen-
tations. SPIRIT&apos;s (Pollack 1986) representation for
planning knowledge uses gen to represent a state result-
ing in an action and cgen to represent a state resulting in
an action only if some other state exists. Causes and
enables are identical in semantics to gen and cgen.
Applies, which has no analog in SPIRIT, is similar to the
intends relation in BORIS (Dyer 1983). The difference
between causes and applies is in whether the action is
intended to cause the state that results from its execu-
tion to exist. &amp;quot;Causes&amp;quot; represents cause-effect rela-
tions which are nonintentional, while &amp;quot;applies&amp;quot; repre-
sents a cause-effect relation between an action
(sequence) or plan which is intended to achieve a
desired state (a goal). An action causes a state whenever
the state results from its execution. An action applies to
a state when an actor believes the action will cause the
desired state to occur.
</bodyText>
<page confidence="0.976951">
40 Computational Linguistics, Volume 14, Number 3, September 1988
</page>
<note confidence="0.705671">
Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptions
</note>
<bodyText confidence="0.998328750000001">
To see why this distinction is necessary, consider
two actions that can be used by a user who wants to
remove one of his files: typing &amp;quot;rm&amp;quot; followed by the
file&apos;s name, and typing &amp;quot;rm *&amp;quot;. Both have removing the
file as one of their effects, but the latter also causes all
other files to be removed as well, an effect that is not the
user&apos;s goal. Only &amp;quot;rm file&amp;quot; applies to removing a file,
although both actions have an effect that causes the file
to be removed.
To further illustrate the semantics of these relations,
we show how they can be used to represent the first
exchange in our example dialog.
User: I tried to remove a file with the &amp;quot;rm&amp;quot; com-
mand. But the file was not removed and the error
message was permission denied. I checked and I
own the file. What&apos;s wrong?
Advisor: To remove a file, you need to be able to
write into the directory containing it. You do not
need to own the file.
Three of the user&apos;s beliefs in this exchange are: 1. the
&amp;quot;rm&amp;quot; command is used when one wants to remove a
file; 2. one has to own a file to remove it, and 3. an error
message resulted when the plan was executed. In terms
of these planning relations, the user&apos;s beliefs are:
applies(using &amp;quot;rm file&amp;quot;, the file&apos;s removal)
enables(owning the file, using &amp;quot;rm file&amp;quot;,
the file&apos;s removal)
causes(using &amp;quot;rm&amp;quot; on the user&apos;s file,
an error message)
The advisor holds several similar beliefs, except that he
believes that to remove a file it is necessary to have
write permission on the directory containing it. In terms
of the planning relationships, the advisor&apos;s beliefs are:
applies(using &amp;quot;rm file&amp;quot;, the file&apos;s removal)
enables(directory write permission, using &amp;quot;rm&amp;quot;,
the file&apos;s removal)
causes(using &amp;quot;rm&amp;quot; on the user&apos;s file,
an error message)
(The paper is not concerned with representing notions
such as &amp;quot;the file&apos;s removal&amp;quot; or &amp;quot;write permission on
the directory containing the file&amp;quot;. The details of the
representation for such things may be found in Quilici
(1985).)
The user and advisor in this exchange share one
belief that we have not represented. This belief is that
using &amp;quot;rm&amp;quot; did not cause the user&apos;s file to be removed.
To represent beliefs that a state did not result from an
action, that a plan is not applicable to a goal, or that a
state is not an enablement condition of an action having
another state as a result, we use !causes, !applies, and
!enables, respectively. The belief above is represented
with !causes, a belief that &amp;quot;mkdir&amp;quot; is not used to
remove a file is represented with !applies, and a belief
that &amp;quot;rm&amp;quot; does not require owning the directory con-
taining the file is represented with !enables.
!causes(using &amp;quot;rm&amp;quot; on the user&apos;s file,
the file&apos;s removal)
!applies(using &amp;quot;mkdir file&amp;quot;, the file&apos;s removal)
!enables(owning the directory, using &amp;quot;rm&amp;quot;,
the file&apos;s removal)
It is also necessary to be able to represent the notion
that a state&apos;s existence caused a planning failure. Con-
sider the following exchange:
User: I accidentally hit the up arrow key and it
deleted 20 unanswered mail messages. How can I
get them back?
Advisor: Hitting the up arrow does not delete your
messages, but does result in your being discon-
nected from the etherplexer. You could not access
your mail messages because they were moved to
&amp;quot;mbox&amp;quot;. The mail program requires that your
mail messages be in &amp;quot;mailbox&amp;quot;.
Here the advisor believes that the user&apos;s mail messages
are inaccessible because they are not in the location the
mail program expects them to be. The belief that the
mail program requires the mail messages to be in the file
&amp;quot;mailbox&amp;quot; can be represented using &amp;quot;enables&amp;quot;. The
advisor&apos;s belief that the mail messages being in the file
&amp;quot;mbox&amp;quot; prevents the mail program from accessing is
represented with precludes, which captures the notion
that two states are mutually exclusive.
enables(messages in &amp;quot;mailbox&amp;quot;, use &amp;quot;mail&amp;quot;,
display messages)
precludes(messages in &amp;quot;mbox&amp;quot;,
messages in &amp;quot;mailbox&amp;quot;)
&amp;quot;precludes&amp;quot; and &amp;quot;!precludes&amp;quot; relations between states
can be inferred using rules such as &amp;quot;an object cannot be
in two places at once.&amp;quot;
The one other relation we find useful is goal, which is
used in representing a belief that an actor wants to
achieve a particular state. In the example above, the
advisor believes that one goal of the user is accessing
his mail messages. The advisor&apos;s belief is:
goal(user, access user&apos;s mail messages)
Most user modeling systems use a similar relation to
explicitly represent that a state is a user&apos;s goal.
</bodyText>
<subsectionHeader confidence="0.998786">
3.2 SUMMARY OF THE REPRESENTATION
</subsectionHeader>
<bodyText confidence="0.9904424">
The main focus of our work is in trying to detect and
respond to user misconceptions. To do so, it is neces-
sary to have some representation for user and advisor
planning knowledge. Our representation is based on
that used by traditional planning systems. The most
important difference is that we take care to distinguish
between things the advisor believes and things the
advisor thinks the user believes. We also distinguish
Computational Linguistics, Volume 14, Number 3, September 1988 41
Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptions
between actions that are intended to achieve a state and
actions that happen to have a particular state as one of
their effects. And we find it necessary to represent
beliefs that two states cannot exist at the same time and
that achieving a particular state is a goal of the user.
</bodyText>
<sectionHeader confidence="0.9876265" genericHeader="method">
4 EXPLANATION-BASED MISCONCEPTION RECOGNITION
AND RESPONSE
</sectionHeader>
<bodyText confidence="0.9999929375">
Our approach to recognizing and responding to a poten-
tially incorrect user belief revolves around the advisor
trying to do several things. First, the advisor tries to
verify that he does not share the user&apos;s belief. Next, the
advisor tries to confirm that the user&apos;s belief is a
misconception. The advisor does so by finding an
explanation for why he does not share the user&apos;s belief.
After the belief is confirmed as a misconception, the
advisor tries to detect its source. He does this by finding
a potential explanation for why the user holds that
incorrect belief, based on a taxonomy of abstract expla-
nation classes. Finally, the advisor presents these ex-
planations to the user as a cooperative response.
But what exactly is an explanation? And what knowl-
edge does the advisor need to find one? And finally,
how is an explanation found?
</bodyText>
<subsectionHeader confidence="0.951827">
4.1 EXPLANATIONS AS SETS OF BELIEFS
</subsectionHeader>
<bodyText confidence="0.999543780487805">
An explanation is a set of advisor beliefs that accounts
for why a particular belief is or is not held. An advisor,
presented with a potentially incorrect user belief, has to
find two explanations.
The first explanation confirms that the user&apos;s belief is
a misconception. To find this explanation the advisor
tries to find a set of advisor beliefs that justify his not
holding the user&apos;s belief. For instance, the user in our
earlier example had an incorrect belief that owning a
directory is a precondition for removing a file.
enables(own directory, use &amp;quot;rm file&amp;quot;,
the file&apos;s removal)
Two advisor beliefs constitute an explanation for why
the advisor does not hold this belief. The first is the
advisor&apos;s contradictory belief that owning a directory is
not a precondition for removing a file. The other is his
belief that the actual precondition is write permission on
the directory containing the file.
!enables(own directory, use &amp;quot;rm file&amp;quot;,
the file&apos;s removal)
enables(writeable directory, use &amp;quot;rm file&amp;quot;,
the file&apos;s removal)
These two beliefs confirmed that the user&apos;s belief was
mistaken.
The other explanation explains why the user holds
this incorrect belief. To find this explanation the advisor
tries to find a set of advisor beliefs that capture the
source of the user&apos;s misconception. Two advisor beliefs
provide a possible explanation for the incorrect user
belief above. The first is that one has to own a directory
to make it writeable. The other is that having a writeable
directory is the precondition to removing a file.
enables(own directory, use &amp;quot;chmod&amp;quot;,
obtain writeable directory)
enables(writeable directory, use &amp;quot;rm&amp;quot;,
the file&apos;s removal)
The user&apos;s not sharing these advisor beliefs explains the
user&apos;s misconception, which is that the user does not
realize that owning a directory is merely a precondition
to obtaining write permission on the directory, which is
the actual precondition to removing the file.
</bodyText>
<subsectionHeader confidence="0.980342">
4.2 REQUIRED ADVISOR KNOWLEDGE
</subsectionHeader>
<bodyText confidence="0.999967071428572">
To find an explanation the advisor must have three
types of knowledge: 1. a set of domain-specific beliefs;
2. a set of rules for inferring additional beliefs, and 3. a
set of abstract explanation patterns. All of these must
come from past advisor experience or past advisor
interaction with users. However, here we simply as-
sume their existence and leave understanding how they
are obtained for future research.
The first type of required knowledge is a set of
domain-specific beliefs about plan applicability condi-
tions, preconditions, and effects. Examples of these
include beliefs that &amp;quot;rm&amp;quot; is used to remove a file, and
that it is necessary to have write permission on the
directory containing the file. Without these types of
beliefs it would be impossible for the advisor to correct
user misconceptions about the preconditions for remov-
ing a file. This category of knowledge includes beliefs
such as a belief that &amp;quot;rm&amp;quot; is not used to remove a
directory. These negated beliefs—!applies, !enables,
!causes, and so on—are especially useful in detecting
misconceptions. An advisor, with the explicit belief that
&amp;quot;rm&amp;quot; is not applicable to removing a directory, can
trivially detect that a user belief that &amp;quot;rm&amp;quot; is applicable
to removing a directory is incorrect.
These domain-specific beliefs are assumed to derive
from past advisor experiences. An advisor who success-
fully uses &amp;quot;rm&amp;quot; to remove a file will believe that using
&amp;quot;rm&amp;quot; is applicable to the goal of removing a file. An
advisor who uses &amp;quot;rm&amp;quot; to try to remove a directory and
has it fail will believe that &amp;quot;rm&amp;quot; is not applicable to
removing a directory. The negated beliefs correspond to
the bug lists kept by many tutoring and planning sys-
tems (Anderson, Boyle, and Yost 1985; Brown and
Burton 1978; Burton 1982; Stevens, Collins, and Goldin
1982).
The second type of advisor knowledge is a set of
rules that help infer negated domain-specific beliefs,
such as a belief that a particular action does not result in
a particular state, or that a given plan is not useful for a
particular goal. These rules are needed because the
advisor cannot be expected to have a complete set of
these beliefs. One such rule, for example, suggests that
</bodyText>
<page confidence="0.962416">
42 Computational Linguistics, Volume 14, Number 3, September 1988
</page>
<note confidence="0.638599">
Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptions
</note>
<bodyText confidence="0.999510666666667">
&amp;quot;if a state S is not among the known states that result
from an action A&apos;s execution, assume that A is not
applicable to achieving S.&amp;quot; There are similar rules for
the other types of beliefs.
The third and final type of knowledge is a taxonomy
of potential explanations for why an actor might or
might not hold a belief. Each type of planning relation—
applies, enables, and effects—is associated with two
sets of potential explanations. One set provides reasons
why an actor might hold a particular belief involving
that planning relation. The other set provides reasons
why an actor might not.
The inference rules and potential explanations differ
for each type of planning relation. Associated with each
type of planning relation is:
</bodyText>
<listItem confidence="0.994906142857143">
1. a set of rules for inferring its negation (which prove
useful in finding explanations for why the belief is or
is not held),
2. a potential explanation for why an actor does not
hold a belief involving that planning relationship, and
3. a set of potential explanations for why an actor does
hold a belief involving that planning relationship.
</listItem>
<bodyText confidence="0.9998168">
For example, applies is associated with a set of rules for
inferring that an actor holds a particular !applies belief,
a potential explanation for why an actor does not hold a
given applies belief, and a set of potential explanations
for why an actor does hold a given applies belief.
</bodyText>
<sectionHeader confidence="0.995079" genericHeader="method">
5 POTENTIAL EXPLANATIONS
</sectionHeader>
<bodyText confidence="0.999935866666667">
The advisor must be able to find a reason for why a
particular belief is or is not held. One way to do so is
1. classify the belief, and 2. try to verify one of the
potential explanations associated with that class of
belief. A potential explanation is an abstract pattern of
planning relationships. The idea is that to verify a
potential explanation, the advisor tries to prove, either
by memory search or by deductive reasoning, that each
of these planning relationships hold.
There are two types of potential explanations. The
first explains why an actor does not hold a belief. The
other explains why an actor does. In this section we
describe the potential explanations associated with the
planning relationships we have examined. The following
section discusses in detail how they are used.
</bodyText>
<subsectionHeader confidence="0.8899995">
5.1 POTENTIAL EXPLANATIONS FOR NOT HOLDING
A BELIEF
</subsectionHeader>
<bodyText confidence="0.998258222222222">
The potential explanations for why the advisor does not
hold an instance of one the plan-oriented beliefs are
shown below. Each of these potential explanations
suggests that to confirm that a user&apos;s belief is a miscon-
ception, the advisor must try to verify that one of his
beliefs contradicts the user&apos;s belief, and that one of his
beliefs can replace it. The only difference between the
potential explanations is in the type of belief being
contradicted or replaced.
</bodyText>
<table confidence="0.607969071428571">
Unshared Potential English
User Belief Explanation Description
applies(Ap, Sg) !applies(Ap,Sg) Plan is not used to
achieve Goal
applies(A,Sg) Other plan is used to
achieve Goal
enables(Sp,Ap,Sg) !enables(Sp,Ap,Sg) State is not precondition
of Action
enables(S,Ap,Sg) Other state is
precondition of Action
causes(Ap,Sp) !causes(Ap,Sp) Action does not cause
state
causes(A,Sp) Other action does cause
state
</table>
<bodyText confidence="0.977095361702128">
Consider our earlier example in which the user&apos;s belief
is that a precondition of removing a file is owning the
directory containing it. The potential explanation sug-
gests trying to prove that the advisor holds two beliefs:
that owning a directory is not a precondition of remov-
ing a file, and that some other state is. Here, the advisor
finds that he believes that owning a directory is not a
precondition of removing a file (either by finding that
relationship in his knowledge base or by deducing it).
The advisor also finds that directory write permission is
a precondition of removing a file. These beliefs explain
why the advisor does not hold the user&apos;s belief, con-
firming it as a misconception.
A similar process is used to confirm that the advisor
does not hold a user&apos;s applies or causes belief. Consider
the following exchange:
User: I tried to display my file with the &amp;quot;Is&amp;quot; com-
mand but it just printed the file&apos;s name.
Advisor: The &amp;quot;Is&amp;quot; command is not used to display
the contents of files, the &amp;quot;more&amp;quot; command is.
&amp;quot;Is&amp;quot; is used to list the names of your files.
The user&apos;s potentially incorrect belief is that &amp;quot;Is&amp;quot; is
applicable to achieving the goal of displaying a file&apos;s
contents. The potential explanation for why an advisor
does not hold this belief is that the advisor does not
believe that using &amp;quot;Is&amp;quot; is applicable to this user&apos;s goal,
and that using &amp;quot;Is&amp;quot; is applicable to some other goal. So
the advisor tries to verify (again, by either search or
deduction) that &amp;quot;Is&amp;quot; is not applicable to displaying the
file&apos;s contents, and he tries to verify that some other
plan does. Here the advisor finds that &amp;quot;more&amp;quot; is used
instead.
Finally, consider the following exchange:
User: I deleted a file by typing &amp;quot;remove&amp;quot;.
Advisor: No, typing &amp;quot;remove&amp;quot; did not delete your
file. Typing &amp;quot;rm&amp;quot; deleted it. Typing &amp;quot;remove&amp;quot;
cleans up your old mail messages.
The user&apos;s potentially mistaken belief is that typing
remove results in a file being deleted. The potential
explanation for why the advisor does not share this
belief is that the advisor instead believes that typing
&amp;quot;remove&amp;quot; does not result in a file being deleted and that
some other action does. The advisor verifies that typing
&amp;quot;remove&amp;quot; does not cause a file to be deleted and that
&amp;quot;rm&amp;quot; is an action that does.
Computational Linguistics, Volume 14, Number 3, September 1988 43
Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptions
</bodyText>
<subsectionHeader confidence="0.719666">
5.2 EXPLANATIONS FOR HOLDING A BELIEF
</subsectionHeader>
<bodyText confidence="0.999888166666667">
The potential explanations we have examined so far
explain why an actor does not hold a particular belief.
There are also potential explanations for why an actor
does hold an incorrect belief. We now present a taxon-
omy of these explanations for each of the three types of
beliefs.
</bodyText>
<sectionHeader confidence="0.836362" genericHeader="method">
5.2.1 EXPLANATIONS FOR INCORRECT APPLIES
</sectionHeader>
<bodyText confidence="0.998960285714286">
There are four potential explanations for why a user
holds an incorrect applies belief of the form applies(Ap,
Sp). Recall that to recognize that this type of user belief
is incorrect the advisor found two beliefs of the form
!applies(Ap, Sp) and applies(A, Sp). Here are the po-
tential explanations along with English descriptions for
each.
</bodyText>
<table confidence="0.9556255625">
Class Potential English
Of Mistake Explanation Description
Plan !causes(Ap,Sp) No effect achieves goal
Achieves
Different applies(Ap,S) Plan applies to other goal
Goal
Plan Missing !causes(Ap,S) User plan does not an effect
Effect causes(A,S) that other plan has
Unachievable enables(S,Ap,Sp) Some state enables Plan
Plan
Enablement !causes(A,Sp) No action achieves this state
Plan Thwarts causes(Ap,S) User plan has effect
User Goal precludes(S, Sp) that thwarts the user&apos;s goal
goal(user, Sp)
!causes(Ap,S) Advisor plan does not have
that effect
</table>
<bodyText confidence="0.997277125">
The first, &amp;quot;Plan Achieves Different Goal&amp;quot;, explains
one of our earlier examples. The explanation is that the
user is unaware that his plan does not have an effect that
achieves his goal, and that his plan is, in fact, used to
achieve some other goal.
User: I tried to display my file with the &amp;quot;Is&amp;quot; com-
mand but it just printed the file&apos;s name.
Advisor: The &amp;quot;Is&amp;quot; command is not used to display
the contents of files, the &amp;quot;more&amp;quot; command is.
&amp;quot;ls&amp;quot; is used to list the names of your files.
The user&apos;s incorrect belief that using &amp;quot;Is&amp;quot; displays a file
arises because the user is unaware of two things. The
first is that using &amp;quot;ls&amp;quot; does not display the contents of
files; the other is that &amp;quot;Is&amp;quot; is applicable to listing the
names of files.
The second, &amp;quot;Plan Missing Effect&amp;quot;, suggests that the
user is unaware that his plan P1 does not have one of the
effects that the plan P2 (that achieves his goal) has.
User: I tried to remove my directory and I got an
error message &amp;quot;directory not empty&amp;quot;. But &amp;quot;Is&amp;quot;
didn&apos;t list any files.
Advisor: Use &amp;quot;ls -a&amp;quot; to list all of your files. &amp;quot;ls&amp;quot;
cannot be used to list all of your files because &amp;quot;ls&amp;quot;
does not list those files whose names begin with a
period.
The user&apos;s mistaken belief is that &amp;quot;ls&amp;quot; should be used to
list all file names. This belief arises because the user is
unaware that &amp;quot;Is&amp;quot; does not have an effect that causes it
to list flies whose names begin with a period, an effect
that the correct plan (ls -a) has.
The third, &amp;quot;Unachievable Plan Enablement&amp;quot;, sug-
gests that the user is unaware his plan will not work
because there is no plan to achieve one of its enable-
ments .
User: So to read Margot&apos;s mail, all I have to do is
&amp;quot;more flowers/mail&amp;quot;.
Advisor: No, only &amp;quot;flowers&amp;quot; can read her mail.
The user mistakenly believes that his plan of using
&amp;quot;more&amp;quot; to examine Margot&apos;s mail file will allow him to
read her mail. The advisor believes that &amp;quot;more&amp;quot; has an
effect of displaying a user&apos;s mail, that one of its enable-
ments is that you have to be that particular user, and
that no plan has an effect that achieves this enablement.
The last, &amp;quot;Plan Thwarts User Goal&amp;quot;, suggests that
the user is unaware that another plan achieves the
user&apos;s goal without an additional effect that the user&apos;s
plan has.
User: To list files whose names begin with a number,
1 pipe &amp;quot;ls&amp;quot; to &amp;quot;grep [0-9]&amp;quot;.
Advisor: Use &amp;quot;Is [0-9]*&amp;quot; instead. It is more efficient.
The user&apos;s mistaken belief is that piping &amp;quot;Is&amp;quot; to &amp;quot;grep&amp;quot;
is the most appropriate plan for listing files whose
names begin with a digit. The user&apos;s misconception
arises because he is unaware that the plan of using
&amp;quot;ls[0-9]*&amp;quot; not only achieves his goal, but also does not
thwart his other goal of using his time efficiently.
</bodyText>
<sectionHeader confidence="0.73298" genericHeader="method">
5.2.2 EXPLANATIONS FOR AN INCORRECT ENABLES
</sectionHeader>
<bodyText confidence="0.9998365">
Just as there are several different sources of user
misconceptions about a plan&apos;s applicability to a goal,
there are also several different sources of user miscon-
ceptions about whether a state is a precondition to a
plan achieving a goal: that is, a user belief of the form
enables(Se, Ap, Sp). Recall that to recognize that this
type of belief is incorrect the advisor found two beliefs
of the form !enables(Se, Ap, Sp) and enables(S, Ap,
Sp). Here are the potential explanations along with
English descriptions for each.
</bodyText>
<table confidence="0.863122444444445">
Class Potential English
Of Mistake Explanation Description
Enablement For enables(Se,A,S) State enables actual
Subgoal enablement
Enablement For causes(A,Sp) Plan achieves user&apos;s goal
Only One Plan !enables(Se,A,Sp) without claimed
enablement
Enablement Too causes(A1,Se) User enablement results
Specific causes(A1,S) from action that achieves
</table>
<footnote confidence="0.66536">
causes(A2,S) real enablement
!causes(A2,Se) Other action achieves real
enablement without user&apos;s
enablement as effect
</footnote>
<bodyText confidence="0.6648005">
The first, &amp;quot;Enablement For Subgoal&amp;quot;, explains the
user&apos;s mistake in our introductory exchange. The ex-
</bodyText>
<page confidence="0.965664">
44 Computational Linguistics, Volume 14, Number 3, September 1988
</page>
<note confidence="0.848743">
Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptions
</note>
<bodyText confidence="0.9791206">
planation is that the user is unaware that his precondi-
tion is not a precondition of the goal itself, but of one of
its preconditions.
User: So to remove a file, I have to own the directory
that contains it.
Advisor: No, to remove a file, you need to have write
permission on the directory that contains it. You
do not need to own the directory that contains it.
You need to own that directory when you do not
already have write permission on it.
This user is unaware that owning a directory is a
precondition for achieving write permission on it, and
that having write permission is a precondition for re-
moving a file.
The second, &amp;quot;Enablement for One Plan&amp;quot;, suggests
that the user is unaware that a plan without his claimed
precondition achieves his goal.
User: So I can only edit files when I&apos;m on a smart
terminal?
Advisor: Only if you edit with &amp;quot;re&amp;quot;. &amp;quot;vi&amp;quot; works fine
on a dumb terminal.
The user&apos;s incorrect belief is that it is necessary to have
a smart terminal to edit a file. This belief arises because
the user is unaware that only one plan, using &amp;quot;vi&amp;quot;,
requires a smart terminal, and that there are other plans
that do not.
The last, &amp;quot;Enablement Too Specific&amp;quot;, suggests that
the user is unaware that his precondition is less general
than the actual precondition for achieving his goal.
User: So I have to remove a file to create a file?
Advisor: You do not have to remove a file to create
a file. You must have enough free space. Remov-
ing a file is only one way to obtain it. You could
also ask the system administrator for more space.
The user mistakenly believes that it is necessary to
remove an existing file before a new file can be created.
The advisor believes that the precondition is sufficient
space for the new file, which can be achieved either by
executing a plan for removing a file or by executing the
plan of requesting more space.
</bodyText>
<sectionHeader confidence="0.751236" genericHeader="method">
5.2.3 EXPLANATIONS FOR INCORRECT CAUSES
</sectionHeader>
<bodyText confidence="0.999706285714286">
One final class of user misconception is an incorrect
belief that a particular state results from a plan&apos;s exe-
cution; that is, a user belief of the form causes(Ap, Sp).
Recall that to recognize that this type of belief is
incorrect the advisor found beliefs of the form !cau-
ses(Ap, Sp) and causes(A, Sp). There are three potential
explanations for this type of mistaken belief.
</bodyText>
<table confidence="0.9693229">
Class Potential English
Of Mistake Explanation Description
Plan has Other applies(Ap,So) Action used to cause other
Effect enables(S,Ap,Sp) effect
Effect Requires !causes(A,S) State required for Action
Enablement and no way to achieve State
Effect Inferred causes(Ap,So) Action causes other effect
From Other precludes(So, S) That effect precludes a state
Effect precludes(Sp, S) that is precluded by user&apos;s
effect
</table>
<bodyText confidence="0.998917261904762">
The first, &amp;quot;Effect From Another Plan&amp;quot;, accounts for an
earlier example. The explanation is that the user is
unaware that the user&apos;s action actually has a different
effect.
User: I deleted a file by typing &amp;quot;remove&amp;quot;.
Advisor: No, typing &amp;quot;remove&amp;quot; did not delete your
file. Typing &amp;quot;rm&amp;quot; deleted it. Typing &amp;quot;remove&amp;quot;
deletes a mail message from the mail program.
The user&apos;s mistaken belief is typing &amp;quot;remove&amp;quot; deletes a
file. The user is unaware that typing &amp;quot;remove&amp;quot; actually
throws away old mail messages.
The second, &amp;quot;Effect Requires Unfulfilled Enable-
ment&amp;quot;, suggests that the user is unaware that a partic-
ular state is required for the plan to have the claimed
effect.
User: I was cleaning out my account when I acciden-
tally deleted all the command files by typing
Advisor: You can&apos;t delete the command files with
&amp;quot;rm&amp;quot; unless you are the system administrator.
The user incorrectly believes that typing &amp;quot;rm&amp;quot; resulted
in the removal of various system files. The advisor
believes that it is necessary for the user to be the system
administrator for this effect to occur.
The last, &amp;quot;Effect Inferred From Other Effect&amp;quot;, ac-
counts for another one of earlier examples. It suggests
that the user is unaware that one effect of his plan has
incorrectly led him to believe what was another effect of
the plan.
User: I accidentally hit the up arrow key and it
deleted 20 unanswered mail messages. How can I
get them back?
Advisor: Hitting the up arrow does not delete your
messages, but does result in your being discon-
nected from the etherplexer. You could not access
your mail messages because they were moved to
&amp;quot;mbox&amp;quot;. The mail program requires that your
mail messages be in &amp;quot;mailbox&amp;quot;.
The user incorrectly believes that one effect of hitting
uparrow was that his mail messages were deleted. This
belief occurs because the user is unaware that one effect
of hitting uparrow is that files are moved to a different
location, which makes them seem inaccessible.
</bodyText>
<sectionHeader confidence="0.993435" genericHeader="method">
6 A DETAILED PROCESS MODEL
</sectionHeader>
<bodyText confidence="0.970292523809524">
We have presented three sets of potential explanations
and briefly sketched how they are used. In this section
we provide a more detailed view of the process by
which an explanation is found.
An advisor presented with a user belief has three
goals. First, he wants to know whether he shares the
user&apos;s belief. Second, he wants to confirm that the
Computational Linguistics, Volume 14, Number 3, September 1988 45
Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptions
user&apos;s belief is indeed a misconception. Third, he wants
to infer the reasons behind the user&apos;s mistake.
The advisor accomplishes the first by trying to verify
that he holds the user&apos;s belief. He accomplishes the
second by trying to find an explanation for why he does
not hold the user&apos;s belief. He accomplishes the third by
trying to find an explanation for why the user does hold
that belief.
Two questions need to be answered. How does the
advisor verify that he holds a particular belief? And how
does the advisor explain why he does not hold a belief,
or why the user does?
</bodyText>
<subsectionHeader confidence="0.995219">
6.1 VERIFYING AN ADVISOR BELIEF
</subsectionHeader>
<bodyText confidence="0.999991666666667">
Verifying whether or not the advisor believes that a
particular planning relationship holds takes two steps.
First, the advisor searches his memory for the desired
piece of planning knowledge. Then, if it is not found, the
advisor applies the set of rules associated with that
planning relationship to try and prove that it holds.
Once the advisor has proved that the planning relation-
ship holds, either by search or by reasoning, that piece
of knowledge is noted to be an advisor belief.
Consider, for example, the process of verifying that
the advisor holds a belief that owning a directory is not
a precondition of removing a file. If this fact is already
known from past experience, the advisor will recognize
it during memory search. If not, the advisor can try to
deduce it. One rule that applies here says that &amp;quot;if a state
S is not one of the known states that are preconditions
to an action A for achieving a goal state, then assume
that S is not a precondition.&amp;quot; Here, this means that if
owning a directory is not among the known precondi-
tions for removing a file, assume it is not a precondition
for removing a file.
</bodyText>
<subsectionHeader confidence="0.98856">
6.2 FINDING AN EXPLANATION
</subsectionHeader>
<bodyText confidence="0.935977285714286">
The advisor must be able to explain why an actor does
or does not hold a particular belief. Finding an expla-
nation is accomplished by hypothesizing one associated
with the given class of belief and then trying to confirm
it. The advisor:
I. Classifies the belief according to its type: applies,
enables, or effects.
</bodyText>
<listItem confidence="0.974181230769231">
2. Selects one of the potential explanations associated
with that class of belief. The potential explanation is
an abstract configuration of planning relationships.
3. Instantiates this potential explanation with informa-
tion from the user&apos;s belief.
4. Tries to verify each of the planning relationships
within the potential explanation. If all can be veri-
fied, this potential explanation is the desired expla-
nation.
5. Repeats the process until one of the potential expla-
nations associated with this beliefs type is verified
or all potential explanations have been tried and have
failed.
</listItem>
<bodyText confidence="0.998598">
The result of the process of finding an explanation is
that the advisor has verified that he holds a particular
set of beliefs. These beliefs constitute the desired ex-
planation.
</bodyText>
<subsectionHeader confidence="0.998777">
6.3 AN EXAMPLE
</subsectionHeader>
<bodyText confidence="0.9962035625">
This section is a detailed look at the advisor&apos;s process-
ing of the user belief that owning a directory is a
precondition of removing a file.
enables(user own directory, use &amp;quot;rm&amp;quot;, the file&apos;s
removal)
First, the advisor tries to verify that he holds the user&apos;s
belief. He cannot.
Next, the advisor tries to confirm that the user&apos;s
belief is, in fact, a misconception. He does this by trying
to explain why he does not hold this user belief. He
notes that it can be classified as a belief that some state
Sp (owning the directory) is a precondition to achieving
some other state Sg (removing a file). The potential
explanation for why the advisor does not hold this type
of belief is that he believes that Sp is not a precondition
of achieving Sg, and that some other state S is a
precondition of Sg. By instantiating this potential expla-
nation, the advisor determines that he must check
whether he holds beliefs that:
!enables(owning a directory, use &amp;quot;rm file&amp;quot;, the file&apos;s
removal)
enables(S, use &amp;quot;rm file&amp;quot;, removing a file)
The advisor finds that he believes that owning a direc-
tory is not a precondition of removing a file (either by
finding that relationship in memory or by deducing it).
The advisor also finds that write permission on a
directory is a precondition of removing a file (that is,
that S can be instantiated with write permission on a
directory). These matching beliefs confirm that the
user&apos;s belief is a misconception.
Now, the advisor has to try to find an explanation for
why the user holds this mistaken belief. One potential
explanation is that the user is unaware that Sp is
actually a precondition of achieving a state S, which is
a precondition to achieving Sg. In this case, instantiat-
ing Sp and Sg leads to the advisor to try and verify that
he holds two beliefs:
enables(S, use &amp;quot;rm file&amp;quot;, the file&apos;s removal)
enables(owning a directory, A, S)
These beliefs are verified when the advisor finds that
having written permission on a directory is a precondi-
tion to removing a file, and that owning a directory is a
precondition to obtaining written permission on the
directory. The potential explanation suggests that the
user&apos;s misconception resulted from his being unaware
of these two advisor beliefs.
Finally, the advisor presents the resulting beliefs to
the user. The user is informed of the beliefs used to
</bodyText>
<page confidence="0.987953">
46 Computational Linguistics, Volume 14, Number 3, September 1988
</page>
<note confidence="0.891556">
Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptions
</note>
<bodyText confidence="0.9762395">
confirm the user&apos;s misconception and the beliefs used to
explain its source.
</bodyText>
<subsectionHeader confidence="0.987319">
6.4 THE POINT OF POTENTIAL EXPLANATIONS
</subsectionHeader>
<bodyText confidence="0.999978833333333">
Having a taxonomy of potential explanations lessens
the amount of reasoning the advisor must do to detect
and respond to the user&apos;s misconceptions.
To see why, consider an advisor trying to understand
how the user arrived at the mistaken belief that a
precondition of removing a file is owning the directory
containing it. The advisor is trying to find some connec-
tion between the user&apos;s enablement and removing a file.
The potential explanations suggest how to find specific,
likely-to-be-useful connections. For example, the po-
tential explanation &amp;quot;Enablement for Subgoal&amp;quot; suggests
examining whether achieving any of the preconditions
of removing a file requires owning a directory.
Without a set of potential explanations, it becomes
necessary to reason from a set of rules that describe
likely differences between user and advisor beliefs. One
rule might be that a user may incorrectly attribute an
enablement of one action to another action. Another
rule might be that a user may incorrectly attribute the
result of one action to another action. From a set of
such rules the advisor must somehow deduce the cause
of the user&apos;s mistake. By using potential explanations
the problem becomes instead one of guided memory
search rather than reasoning from first principles.
</bodyText>
<sectionHeader confidence="0.996972" genericHeader="related work">
7 RELATED WORK
</sectionHeader>
<bodyText confidence="0.999969692307692">
Two approaches have been used to detect and correct
misconceptions. The first approach is used by many
intelligent tutoring systems (Anderson, Boyle, and Yost
1985; Brown and Burton 1978; Burton 1982; Stevens,
Collins, and Goldin 1982). These systems locate mis-
taken beliefs in a data base of error-explanation pairs
and provide the associated explanation. A basic prob-
lem with this approach is that, because there is no
information about the underlying causes of the errors,
these systems can handle only those misconceptions
known in advance.
The other approach avoids the difficulty inherent in
enumerating all possible misconceptions within a do-
main by using strategies that address an entire class of
misconceptions. The user&apos;s misconception is classified
according to the abstract reasoning error likely to have
led to it. This approach shares many features with
recognizing abstract thematic situations (such as irony)
in narratives, where such situations are defined in terms
of abstract planning errors made by the narrative char-
acters (Dyer 1983; Dyer, Flowers, and Reeves 1987;
Dolan and Dyer 1986). Once an appropriate strategy is
found, it can be used to generate advice (in narratives,
this advice may be in the form of adages). In advisory
systems, this approach has been applied to both object-
and plan-oriented misconceptions.
</bodyText>
<subsectionHeader confidence="0.983103">
7.1 OBJECT-ORIENTED MISCONCEPTIONS
</subsectionHeader>
<bodyText confidence="0.999867926829268">
ROMPER (McCoy 1985, and this issue) corrects user
misconceptions dealing with whether an object is an
instance of a particular class of objects or possesses a
particular property.
User: I thought whales were fish.
ROMPER: No, they are mammals. You may have
thought they were fish because they are fin-
bearing and live in the water. However, they are
mammals since, while fish have gills, whales
breathe through lungs and feed their young with
milk.
ROMPER classifies a user&apos;s misconception as either a
misclassification or misattribution and then selects one
of several strategies associated with each class of mis-
conception to generate a response. Each strategy ad-
dresses a different type of reasoning error, and is
selected based on ROMPER&apos;s own beliefs about objects
and its model of the user&apos;s relevant beliefs. One such
strategy is useful when the advisor believes that X isa Z,
the user mistakenly believes that X isa Y, and the
advisor believes that X and Y share certain attributes.
The strategy suggests presenting these shared attributes
as a possible reason for the misclassification, and point-
ing out the unshared attributes that lead the advisor to
believe that X isa Z.
Despite dealing with a very different class of miscon-
ceptions, ROMPER&apos;s approach is similar to ours. The
major difference is that our explanation-based approach
separates the beliefs needed to confirm the user&apos;s belief
as a misconception from those needed to understand
why the user holds it. The strategy above divides into
two explanations. The first confirms that a user belief
that X isa Y is incorrect if the advisor believes that X isa
Z because X and Z share certain attributes. The other
suggests that the user may hold this belief because X
and Y share certain attributes. The advantage to our
approach is that the information regarding the beliefs
that confirm that the user has a misconception can be
separated from the explanations for why the user holds
the belief, and unnecessary duplication of tests is
avoided.
</bodyText>
<subsectionHeader confidence="0.99231">
7.2 PLAN-ORIENTED MISCONCEPTIONS
</subsectionHeader>
<bodyText confidence="0.932898055555556">
Two efforts have examined detecting and responding to
plan-oriented misconceptions.
Joshi, Webber, and Weishedel (1984) suggest using a
strategy-based approach to provide cooperative re-
sponses to problematic planning requests. They con-
sider &amp;quot;How do I do X?&amp;quot; questions in which X can be
inferred to be a subgoal of a more important goal Y.
User: How can I drop cs577?
System: It is too late in the quarter to drop it. But you
can avoid failing by taking an incomplete and
finishing your work next quarter.
They provide several strategies, listed below, for select-
ing the contents of a reasonable response, with strategy
selection based on the advisor&apos;s beliefs about which
Computational Linguistics, Volume 14, Number 3, September 1988 47
Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptions
plans achieve a particular goal and the achievability of
their preconditions.
</bodyText>
<figure confidence="0.959798727272727">
Situation Response
1. Unachievable Precondition E of X Provide E
(a) Plan P achieves Y Provide P
(b) No plan to achieve Y Point this out
2. X doesn&apos;t help achieve Y Point this out
(a) Plan P achieves Y Provide P
(b) No plan to achieve Y Point this out
3. Plan P better way to achieve Y Provide P
4. X only way to achieve Y Point this out
5. Plan P involving uncontrollable Provide P
event E achieves Y
</figure>
<bodyText confidence="0.999523777777778">
One such strategy, useful when the advisor believes that
X cannot be achieved because of an impossible-to-
achieve precondition, is to point out the troublesome
precondition and suggest an alternate plan that achieves
Y.
Our work differs from theirs in several respects. The
main difference is that they focus on correcting the
user&apos;s misconception instead of trying to explain why it
occurred. Only one strategy above is concerned with
providing an explanation that addresses the source of a
user misconception (in this case, an inappropriate plan).
The other strategies describe situations in which achiev-
ing X is inappropriate and an alternate plan for Y exists
and should be presented to the user as a correction. In
addition, they did not consider responding to incorrect
beliefs about plan preconditions or effects.
The other effort, SPIRIT (Pollack 1986), tries to
detect the inappropriate plans underlying queries made
by users of a computer mail program and the mistaken
user beliefs underlying those plans.
User: I want to prevent Tom from reading my file.
How can I set the permissions on it to faculty-read
only?
System: You can make the file readable by faculty
only using &amp;quot;set permission&amp;quot;. However, Tom can
still read it because he&apos;s the system administrator.
User misconceptions about the applicability and execu-
tability of plans are detected by reasoning about the
likely differences between the advisor&apos;s beliefs and the
user&apos;s, with various rules used to infer these differ-
ences. One such rule, used to detect the source of the
misconception above, states that an advisor who be-
lieves that an act has a particular result under certain
conditions can infer that the user has a similar belief
missing one of the required conditions.
SPIRIT has a task similar to ours but takes a very
different approach, trying to determine the cause of the
user&apos;s error through reasoning from first principles
rather than memory search. In addition, SPIRIT cannot
detect or respond to mistakes involving plan applicabil-
ity conditions or preconditions. Finally, SPIRIT does
not specify how knowledge of the cause of the user&apos;s
mistaken belief affects the information to be included in
a cooperative response, something that falls naturally
out of our model.
</bodyText>
<subsectionHeader confidence="0.990784">
7.3 UNIX ADVISORS
</subsectionHeader>
<bodyText confidence="0.997907409090909">
Finally, there are two other related research efforts, UC
(Wilensky et al. 1986, Wilensky, Arens, and Chin 1984)
and SC (Kemke 1986), that address providing advice to
novice UNIX users. Neither system, however, detects
or responds to misconceptions. Instead, both are con-
cerned with tailoring a response to a question to reflect
the user&apos;s level of expertise. UC&apos;s user modeling com-
ponent, KNOME (Chin 1986), analyzes a user&apos;s ques-
tions to determine which stereotypical class the user
belongs to and then uses this information to provide
more details and possibly more examples to less expe-
rienced users.
Novice: What does the &amp;quot;rwho&amp;quot; command do?
UC: Rwho lists all users on the network, their tty,
their login time, and their idle time.
Expert: What does the &amp;quot;rwho&amp;quot; command do?
UC: Rwho is like who, except rwho lists all users on
the network.
SC&apos;s user modeling component, SCUM (Nessen 1987),
takes an approach similar to UC&apos;s, also using stereo-
typical information. These approaches are complemen-
tary to ours.
</bodyText>
<sectionHeader confidence="0.999314" genericHeader="method">
8 IMPLEMENTATION DETAILS
</sectionHeader>
<bodyText confidence="0.999991761904762">
The theory discussed in this paper is embodied in
AQUA, a computer program currently under develop-
ment at UCLA. The current version of AQUA is
implemented in T (Rees, Adams, and Meehan 1984),
using RHAPSODY (Turner and Reeves 1987), a graph-
ical Al tools environment with Prolog-like unification
and backtracking capabilities, and runs on an Apollo
DN460 workstation. Given a set of user beliefs involv-
ing plan applicability conditions, preconditions, or ef-
fects, AQUA determines which of these user beliefs are
incorrect and what missing or mistaken user beliefs are
likely to have led to them, and then produces a set of
advisor beliefs that capture the content of the advisor&apos;s
response. AQUA&apos;s domain of expertise is in the basic
plans used to manipulate and access files, directories,
and electronic mail. It has been used to detect and
respond to at least two different incorrect user beliefs in
each class of misconception that we have identified.
More detailed descriptions of the program&apos;s implemen-
tation can be found in Quilici, Flowers, and Dyer (1986),
and in Quilici (1985).
</bodyText>
<sectionHeader confidence="0.966366" genericHeader="evaluation">
9 LIMITATIONS AND FUTURE WORK
</sectionHeader>
<bodyText confidence="0.999895875">
Our approach to determining why an actor does or does
not hold a particular belief has been to let potential
explanations direct the search for the advisor beliefs
that serve as an appropriate explanation. Our focus has
been on discovering and representing these explana-
tions. The limitations of our approach arise in areas we
have ignored, each of which is an interesting area of
research.
</bodyText>
<page confidence="0.970025">
48 Computational Linguistics, Volume 14, Number 3, September 1988
</page>
<note confidence="0.938391">
Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptions
</note>
<subsectionHeader confidence="0.656552">
9.1 INFERRING THE SET OF USER BELIEFS
</subsectionHeader>
<bodyText confidence="0.999976229166667">
Our model assumes that the user&apos;s problem description
has somehow been parsed into a set of beliefs. How-
ever, users rarely explicitly state their beliefs, leaving
the advisor to the difficult task of inferring them.
Consider our introductory exchange.
User: I tried to remove a file with the &amp;quot;rm&amp;quot; com-
mand. But the file was not removed and the error
message was permission denied. I checked and I
own the file. What&apos;s wrong?
Advisor: To remove a file, you need to be able to
write into the directory containing it. You do not
need to own the file.
Here the advisor must infer the user&apos;s beliefs that 1.
using &amp;quot;rm&amp;quot; is applicable to removing a file; that 2. using
&amp;quot;rm&amp;quot; did not cause the file&apos;s removal; that 3. using
&amp;quot;rm&amp;quot; resulted in an error message, and that 4. owning
a file is a precondition to removing it.
Inferring the first belief requires a rule such as &amp;quot;if the
user tries to achieve a state with a particular action,
assume the user believes that action achieves that
state.&amp;quot; The second belief can be inferred from the rule
that &amp;quot;if an utterance describes the nonexistence of a
state that is a believed result of an action, assume that
the user believes that the action did not cause the
state.&amp;quot; A similar rule can be used to infer the third
belief.
Inferring the final belief, that owning a file is a
precondition to its removal, is a difficult task. Because
there are a potentially-infinite number of incorrect user
beliefs about the preconditions of removing a file, the
advisor cannot simply match owning a file against a list
of incorrect preconditions. Because the user may have
been discussing other plans and other goals the advisor
cannot simply assume that any utterance after a plan&apos;s
failure refers to its preconditions. Instead, the advisor
needs to infer this user belief from the knowledge that
the user did some sort of verify-action, the knowledge
that one plan for dealing with a plan failure is to try to
verify that the enablements of the plan have been
achieved, and the knowledge that both owning the file
and having write permission are different instantiations
of having sufficient permission.
Inferring beliefs like these, that involve the user&apos;s
plans and goals and the relationships between, even
when they differ from the advisor&apos;s, is currently an
active area of research (Carberry, this issue; Kautz and
Allen 1986, Goodman 1986, Wilensky et al. 1986, Quilici
1985).
</bodyText>
<subsectionHeader confidence="0.958825">
9.2 RETRIEVING ADVISOR BELIEFS
</subsectionHeader>
<bodyText confidence="0.999980636363636">
Our potential explanations suggest patterns of beliefs
that the advisor should search for. However, we have
not specified how this search of the advisor&apos;s memory is
actually carried out, how a belief in memory can be
retrieved efficiently, or how the beliefs are actually
acquired through experience. AQUA&apos;s organization of
plan-oriented beliefs is discussed in Quilici (1988, 1985).
It is based on earlier work (Kolodner 1985, Schank
1982) in taking experiences and indexing them appro-
priately for efficient search and retrieval, especially that
involving indexing memory around various planning
failures (Kolodner and Cullingford 1986, Quilici 1985,
Hammond 1984, Dyer 1983).
Because the advisor may need to verify a belief that
is not stored directly in memory, memory search may
not be sufficient. Suppose the advisor is trying to verify
that owning a directory is not required to remove a file.
The advisor may be able to deduce this belief from a
past experience in which he removed a file from /tmp, a
directory owned by the system administrator. Similarly,
the advisor may be able to deduce that write permission
is needed to remove a file from his beliefs that write
permission is needed to make changes on objects and
that removing a file involves making a change to a
directory. This requires more powerful reasoning capa-
bilities than AQUA&apos;s simple rules for inferring negated
beliefs.
Finally, AQUA assumes the existence of a taxonomy
of planning failures. We have left the automatic creation
of this taxonomy from advisor experiences to future
research. Initial work in recognizing and indexing ab-
stract configurations of planning relations is discussed
in Dolan and Dyer (1985, 1986).
</bodyText>
<subsectionHeader confidence="0.967947">
9.3 OTHER CLASSES OF MISCONCEPTIONS
</subsectionHeader>
<bodyText confidence="0.999693695652174">
We are currently studying how well the classes of
misconceptions described here account for responses to
misconceptions in domains other than the problems of
novice computer users, such as the domain of simple
day-to-day planning. In addition, we are examining
other classes of planning misconceptions. For example,
to respond to an incorrect user belief such as &amp;quot;rm&amp;quot;
cannot be used to remove a file, the advisor needs
potential explanations for why an action does not apply
to a particular goal state.
We do not yet know whether our approach is suitable
for generating responses to misconceptions that are not
directly related to plan-goal interactions, such as mis-
takes in referring to an object. Consider the following
exchange:
User: Diana is up, but I cannot access my file.
Advisor: Your files are on Rhea, not Diana. They
moved your files yesterday because your file sys-
tem was full.
Here the user&apos;s problem is that he is incorrectly using
&amp;quot;Diana&amp;quot; to refer to the machine his files are on. We are
examining whether our approach is extendable to re-
spond to these types of user misconceptions.
</bodyText>
<subsectionHeader confidence="0.978188">
9.4 RESPONSE GENERATION
</subsectionHeader>
<bodyText confidence="0.993464517241379">
The response we provide is a set of advisor beliefs that
is as complete as possible. We make no attempt to use
knowledge about other user beliefs to modify our re-
sponse to provide only the most relevant beliefs. How-
ever, if the advisor can infer that a user knows that his
Computational Linguistics, Volume 14, Number 3, September 1988 49
Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptions
plan has failed (perhaps because of the error message a
command produces), he need not inform the user that
his plan is incorrect. One straightforward way to extend
our model is to have the advisor filter out those beliefs
he can infer the user has.
The advisor should use information about the user to
tailor their response based on the user&apos;s level of exper-
tise. Recall the following exchange:
User: I tried to remove my directory and I got an
error message &amp;quot;directory not empty&amp;quot;. But &amp;quot;ls&amp;quot;
didn&apos;t list any files.
Advisor: Use &amp;quot;ls -a&amp;quot; to list all of your files. &amp;quot;ls&amp;quot;
cannot be used to list all of your because &amp;quot;ls&amp;quot; does
not list those files whose names begin with a
period.
An advisor who knows the user is a novice might want
to augment his response with an explanation that &amp;quot;-a&amp;quot;
is a command option and that command options cause
changes in the normal behavior of commands. Several
researchers are working on tailoring the response to the
user based on knowledge about the user&apos;s expertise
(Paris, this issue; Chin 1986).
</bodyText>
<sectionHeader confidence="0.999779" genericHeader="conclusions">
10 CONCLUSIONS
</sectionHeader>
<bodyText confidence="0.999990366666667">
We have presented an explanation-based approach to
the problem of recognizing and responding to user
misconceptions. The advisor confirms that a user&apos;s
belief is a misconception by finding an explanation for
why he does not hold the user&apos;s belief. The advisor
infers its source by finding an explanation for the
mistaken belief. The process of finding an explanation
was presented as one of hypothesizing and trying to
verify a small set of potential explanations associated
with each type of user belief. In essence, the model uses
information about likely sources of different classes of
user misconceptions to recognize user mistakes and
infer their underlying causes.
This approach is attractive for several reasons. First,
because it has information about classes of abstract
misconceptions, it can handle misconceptions of which
it has no prior knowledge, as long as they fall into one
of these classes. A smaller set of potential explanations
can account for a large number of specific user mis-
takes. Second, the model makes use of knowledge of
the types of misconceptions users are likely to make to
circumvent the need for general deductive reasoning.
The model can easily be augmented to first check
whether it has knowledge of specific misconceptions, as
do tutoring systems, and to use general reasoning when
it is confronted with a misconception that cannot be
explained by any of its potential explanations, as does
SPIRIT. Finally, our approach leads to responses sim-
ilar (and sometimes more informative) than those of the
UNIX advisors we have observed.
</bodyText>
<sectionHeader confidence="0.994117" genericHeader="acknowledgments">
ACKNOWLEDGMENTS
</sectionHeader>
<bodyText confidence="0.999252333333333">
The work reported here was supported in part by a grant from the
Lockheed Software Technology Center (Austin, TX). Special thanks
go to Mike Gasser, John Reeves, and Ron Sumida for fighting their
way through several earlier incoherent and uninteresting versions of
this paper. Comments by two anonymous reviewers have also greatly
improved its organization and content.
</bodyText>
<sectionHeader confidence="0.995914" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.991349525">
Abelson, R. 1973 The Structure of Belief Systems. In Schank R.C.
and Colby K.M. (eds.), Computer Models of Thought and Lan-
guage. Freeman, San Francisco, CA.
Abelson, R. 1979 Differences between Beliefs and Knowledge Sys-
tems. Cognitive Science 3: 355-366.
Alvarado, S. 1987 Understanding Editorial Text: A computer model of
reasoning comprehension. Ph.D. thesis. University of California,
Los Angeles, CA.
Alvarado, S., Dyer, M., and Flowers, M. 1986 Editorial Comprehen-
sion in OpED through Argument Units. In Proceedings of the 1986
National Conference on Artificial Intelligence, Philadelphia, PA:
250-256.
Anderson, J.R., Boyle, C.F., and Yost, G. 1985 The Geometry Tutor.
In Proceedings of the 1985 Joint Conference on Artificial Intelli-
gence, Los Angeles, CA: 1-7.
Brown, J.S. and Burton, R.R. 1978 Diagnostic models for procedural
bugs in basic mathematical skills. Cognitive Science 2: 155-192.
Burton, R.R. 1982 Diagnosing bugs in a simple procedural skill.
Intelligent Tutoring Systems, Academic Press, London, England:
157-183.
Carberry, S. Plan Recognition and User Modeling, this issue.
Chin, D. 1986 User modeling in UC, the UNIX consultant. In
Proceedings of the CHI-86 Conference, Boston, MA.
Cullingford, S. 1978 Script Application: Computer Understanding of
Newspaper Stories, Ph.D. thesis. Yale University, New Haven,
CT.
Dolan, C. and Dyer, M. 1985 Learning Planning Heuristics through
Observation. In Proceedings of the 1985 Joint Conference on
Artificial Intelligence, Los Angeles, CA: 600-602.
Dolan, C. and Dyer, M. 1986 Encoding Planning Knowledge for
Recognition, Construction, and Learning. In Proceedings of the
8th Annual Cognitive Science Society: 488-499.
Dyer, M. 1983 In-Depth Understanding: A Computer Model of
Narrative Comprehension, MIT Press, Cambridge, MA.
Dyer, M., Flowers, M., and Reeves, J.F. 1987 Recognizing Situa-
tional Ironies: A Computer Model of Irony Recognition and
Narrative Understanding. In Advances in Computing and the
Humanities: forthcoming.
Fikes, R.E. and Nilsson, N.J. 1971 STRIPS: A New Approach to the
Application of Theorem Proving to Problem Solving. Artificial
Intelligence 2: 189-208.
Flowers. M., McGuire, R., and Birnbaum, L. 1982 Adversary Argu-
ments and the Logic of Personal Attacks. In Strategies for Natural
Language Processing. Lawrence Erlbaum, Hillsdale, NJ: 275-
294.
Goodman, B. 1986 Miscommunication and Plan Recognition. Unpub-
lished manuscript from UM86. International Workshop on User
Modeling, Maria Laach, West Germany.
Hammond, K. 1984 Indexing and Causality: The Organization of
Plans and Strategies in Memory. Technical Report 351, Yale
University, New Haven, CT.
Kautz, H. and Allen, J. 1986 Generalized Plan Recognition. In
Proceedings of the 1986 National Conference on Artificial Intelli-
gence, Los Angeles, CA: 423-427.
Kemke, C. 1986 The SINIX Consultant: Requirements, Design and
Implementation of an Intelligent Help System for a UNIX Deny-
50 Computational Linguistics, Volume 14, Number 3, September 1988
Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptions
ative. Report No. 11, Al Laboratory, Dept. of Computer Science,
Univ. of Saarbriicken, W. Germany.
Joshi, A., Webber, B., Weishedel, R. 1984 Living up to expectations:
computing expert responses. In Proceedings of the 1984 National
Conference on Artificial Intelligence, Dallas, TX: 169-175.
Kolodner, J.L. 1984 Retrieval and Organizational Strategies in Con-
ceptual Memory, Lawrence Erlbaum, Hillsdale, NJ.
Kolodner, J.L. and Cullingford, R.E. 1986 Towards a Memory
Architecture that Supports Reminding. In Proceedings of the 8th
Annual Cognitive Science Society: 467-477.
McCoy, K. Reasoning on a highlighted user model to respond to
misconceptions. this issue.
McCoy, K. 1985 Responding to Object-Oriented Misconceptions.
Ph.D. Thesis. University of Pennsylvania, Philadelphia, PA.
Nessen, E. 1987 SCUM: User modeling in the SINIX consultant.
Memo No. 18, Al Laboratory, Dept. of Computer Science,
University of Saarbriicken, Saarbriicken, West Germany.
Paris, C. Tailoring object descriptions to the user&apos;s level of expertise.
this issue.
Pollack, M. 1986 A model of plan inference that distinguishes between
the beliefs of actors and observers. In Proceedings of 24th meeting
of the Association of Computational Linguistics, New York, NY.
Pollack, M. 1986 Inferring domain plans in question-answering.
Ph.D. Thesis. University of Pennsylvania, Philadelphia, PA.
Quilici, A. 1988 AQUA: A system that detects and responds to user
misconceptions. In Kobsa Alfred and Wahlster Wolfgang (eds.),
User Modeling and Dialog Systems, Springer Verlag, Berlin, New
York.
Quilici, A., Dyer, M., and Flowers, M. 1986 AQUA: An intelligent
UNIX advisor. In Proceedings of the 1986 European Conference
on Artificial Intelligence, Brighton, England: 33-38.
Quilici, A. 1985 Human problem understanding and advice giving: A
computer model. Technical Report No. 85-00069, Computer Sci-
ence Department, University of California, Los Angeles, CA.
Rees, J.A., Adams, N.L., and Meehan, J.R. 1984 The T manual. Yale
University, New Haven, CT.
Sacerdoti, E. 1974 Planning in a hierarchy of abstraction spaces.
Artificial Intelligence 5(2): 115-135.
Schank, R.C. 1982 Dynamic Memory. Cambridge University Press,
Cambridge, MA.
Schank, R.C., and Abelson, R. 1977 Scripts, Plans, Goals, and
Understanding, Lawrence Erlbaum, Hillsdale, NJ.
Schank, R.C. 1986 Explanation Patterns: Understanding Mechani-
cally and Creatively, Lawrence Erlbaum, Hillsdale, NJ.
Stevens, A., Collins, A., and Goldin, S.E. 1982 Misconceptions in
students&apos; understanding. Intelligent Tutoring Systems, Academic
Press, London, England: 13-24.
Turner, S.R. and Reeves, J.F. 1987 The Rhapsody User&apos;s Manual.
Technical Note UCLA-AI-86-10, Artificial Intelligence Labora-
tory, University of California, Los Angeles, CA.
Wilensky, R., Mayfield, J., Albert, A., Chin, D., Cox, C., Luria, M.,
Martin, J., and Wu, D. 1986 UC: A Progress Report. Technical
Report UCB/CSB 87/303, Computer Science Division (EECS),
University of California, Berkeley, CA.
Wilensky, R., Arens, Y., and Chin, D. 1984 Talking to UNIX in
English: An Overview of UC, Communications of the ACM: 574-
593.
Wilensky, R. 1983 Planning and Understanding, Addison Wesley,
Reading, MA.
Wilensky, R. 1978 Understanding Goal-Based Stories. Ph.D. thesis.
Technical Report 140, Yale University, New Haven, CT.
Computational Linguistics, Volume 14, Number 3, September 1988 51
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.368243">
<title confidence="0.999711">AND RESPONDING MISCONCEPTIONS</title>
<author confidence="0.999945">Alex Quilici</author>
<affiliation confidence="0.452154">Dyer</affiliation>
<author confidence="0.977592">Margot Flowers</author>
<affiliation confidence="0.999169">Artificial Intelligence Computer Science University of</affiliation>
<address confidence="0.989792">Los Angeles, CA 90024</address>
<abstract confidence="0.987204545454546">This paper discusses the problem of recognizing and responding to plan-oriented misconceptions in advice-seeking dialogs, concentrating on the problems of novice computer users. A cooperative response is one that not only corrects the user&apos;s mistaken belief, but also addresses the missing or mistaken user beliefs that led to it. Responding appropriately to a potentially incorrect user belief is presented as a process of 1. checking whether the advisor holds the user&apos;s belief; 2. confirming the belief as a misconception by finding an explanation for why the advisor does not hold this belief; 3. detecting the mistaken beliefs underlying the misconception by trying to explain why the user holds the incorrect belief, and 4. providing these explanations to the user. An explanation is shown to correspond to a set of advisor beliefs, and searching for an explanation to proving whether various abstract configurations of advisor beliefs hold. A taxonomy of domain-independent explanations for potential user misconceptions involving plan applicability conditions, preconditions, and effects is presented.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Abelson</author>
</authors>
<title>The Structure of Belief Systems.</title>
<date>1973</date>
<booktitle>Computer Models of Thought and Language. Freeman,</booktitle>
<editor>In Schank R.C. and Colby K.M. (eds.),</editor>
<location>San Francisco, CA.</location>
<contexts>
<context position="12080" citStr="Abelson 1973" startWordPosition="1991" endWordPosition="1992">rdoti 1974) represent an agent&apos;s planning knowledge as a data base of operators associated with applicability conditions, preconditions, and effects. Since these systems have only one agent, the planner, the entries in the data base are implicitly assumed to represent that agent&apos;s beliefs. However, because user misconceptions occur when the user&apos;s planning knowledge differs from the advisor&apos;s, systems that deal with user misconceptions must explicitly distinguish between advisor beliefs about what the user knows and advisor beliefs about what the advisor knows. Our representation for beliefs (Abelson 1973, 1979) is similar to that used by existing systems that keep track of the possibly contradictory knowledge of multiple participants (Alvarado 1987; Alvarado, Dyer, and Flowers 1986; Flowers, McGuire, and Birnbaum 1982; Pollack 1986). A belief relation represents an advisor&apos;s belief that an actor maintains that a particular plan applicability condition, precondition, or effect holds. The actor is either the user or the advisor. belief(user, R) Advisor believes that user maintains R belief(advisor, R) Advisor believes that advisor maintains R In this paper we do not discuss beliefs involving ot</context>
</contexts>
<marker>Abelson, 1973</marker>
<rawString>Abelson, R. 1973 The Structure of Belief Systems. In Schank R.C. and Colby K.M. (eds.), Computer Models of Thought and Language. Freeman, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Abelson</author>
</authors>
<title>Differences between Beliefs and Knowledge Systems.</title>
<date>1979</date>
<journal>Cognitive Science</journal>
<volume>3</volume>
<pages>355--366</pages>
<marker>Abelson, 1979</marker>
<rawString>Abelson, R. 1979 Differences between Beliefs and Knowledge Systems. Cognitive Science 3: 355-366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Alvarado</author>
</authors>
<title>Understanding Editorial Text: A computer model of reasoning comprehension.</title>
<date>1987</date>
<tech>Ph.D. thesis.</tech>
<institution>University of California,</institution>
<location>Los Angeles, CA.</location>
<contexts>
<context position="12227" citStr="Alvarado 1987" startWordPosition="2013" endWordPosition="2014">ts. Since these systems have only one agent, the planner, the entries in the data base are implicitly assumed to represent that agent&apos;s beliefs. However, because user misconceptions occur when the user&apos;s planning knowledge differs from the advisor&apos;s, systems that deal with user misconceptions must explicitly distinguish between advisor beliefs about what the user knows and advisor beliefs about what the advisor knows. Our representation for beliefs (Abelson 1973, 1979) is similar to that used by existing systems that keep track of the possibly contradictory knowledge of multiple participants (Alvarado 1987; Alvarado, Dyer, and Flowers 1986; Flowers, McGuire, and Birnbaum 1982; Pollack 1986). A belief relation represents an advisor&apos;s belief that an actor maintains that a particular plan applicability condition, precondition, or effect holds. The actor is either the user or the advisor. belief(user, R) Advisor believes that user maintains R belief(advisor, R) Advisor believes that advisor maintains R In this paper we do not discuss beliefs involving other relationships, such as a belief that an object has a particular property. In addition, for readability we do not use the belief predicate here,</context>
</contexts>
<marker>Alvarado, 1987</marker>
<rawString>Alvarado, S. 1987 Understanding Editorial Text: A computer model of reasoning comprehension. Ph.D. thesis. University of California, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Alvarado</author>
<author>M Dyer</author>
<author>M Flowers</author>
</authors>
<title>Editorial Comprehension in OpED through Argument Units.</title>
<date>1986</date>
<booktitle>In Proceedings of the 1986 National Conference on Artificial Intelligence,</booktitle>
<pages>250--256</pages>
<location>Philadelphia, PA:</location>
<marker>Alvarado, Dyer, Flowers, 1986</marker>
<rawString>Alvarado, S., Dyer, M., and Flowers, M. 1986 Editorial Comprehension in OpED through Argument Units. In Proceedings of the 1986 National Conference on Artificial Intelligence, Philadelphia, PA: 250-256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Anderson</author>
<author>C F Boyle</author>
<author>G Yost</author>
</authors>
<title>The Geometry Tutor.</title>
<date>1985</date>
<booktitle>In Proceedings of the 1985 Joint Conference on Artificial Intelligence,</booktitle>
<pages>1--7</pages>
<location>Los Angeles, CA:</location>
<marker>Anderson, Boyle, Yost, 1985</marker>
<rawString>Anderson, J.R., Boyle, C.F., and Yost, G. 1985 The Geometry Tutor. In Proceedings of the 1985 Joint Conference on Artificial Intelligence, Los Angeles, CA: 1-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Brown</author>
<author>R R Burton</author>
</authors>
<title>Diagnostic models for procedural bugs in basic mathematical skills.</title>
<date>1978</date>
<journal>Cognitive Science</journal>
<volume>2</volume>
<pages>155--192</pages>
<contexts>
<context position="24882" citStr="Brown and Burton 1978" startWordPosition="4106" endWordPosition="4109">to removing a directory, can trivially detect that a user belief that &amp;quot;rm&amp;quot; is applicable to removing a directory is incorrect. These domain-specific beliefs are assumed to derive from past advisor experiences. An advisor who successfully uses &amp;quot;rm&amp;quot; to remove a file will believe that using &amp;quot;rm&amp;quot; is applicable to the goal of removing a file. An advisor who uses &amp;quot;rm&amp;quot; to try to remove a directory and has it fail will believe that &amp;quot;rm&amp;quot; is not applicable to removing a directory. The negated beliefs correspond to the bug lists kept by many tutoring and planning systems (Anderson, Boyle, and Yost 1985; Brown and Burton 1978; Burton 1982; Stevens, Collins, and Goldin 1982). The second type of advisor knowledge is a set of rules that help infer negated domain-specific beliefs, such as a belief that a particular action does not result in a particular state, or that a given plan is not useful for a particular goal. These rules are needed because the advisor cannot be expected to have a complete set of these beliefs. One such rule, for example, suggests that 42 Computational Linguistics, Volume 14, Number 3, September 1988 Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptions &amp;quot;if a sta</context>
<context position="48412" citStr="Brown and Burton 1978" startWordPosition="8043" endWordPosition="8046">ight be that a user may incorrectly attribute an enablement of one action to another action. Another rule might be that a user may incorrectly attribute the result of one action to another action. From a set of such rules the advisor must somehow deduce the cause of the user&apos;s mistake. By using potential explanations the problem becomes instead one of guided memory search rather than reasoning from first principles. 7 RELATED WORK Two approaches have been used to detect and correct misconceptions. The first approach is used by many intelligent tutoring systems (Anderson, Boyle, and Yost 1985; Brown and Burton 1978; Burton 1982; Stevens, Collins, and Goldin 1982). These systems locate mistaken beliefs in a data base of error-explanation pairs and provide the associated explanation. A basic problem with this approach is that, because there is no information about the underlying causes of the errors, these systems can handle only those misconceptions known in advance. The other approach avoids the difficulty inherent in enumerating all possible misconceptions within a domain by using strategies that address an entire class of misconceptions. The user&apos;s misconception is classified according to the abstract</context>
</contexts>
<marker>Brown, Burton, 1978</marker>
<rawString>Brown, J.S. and Burton, R.R. 1978 Diagnostic models for procedural bugs in basic mathematical skills. Cognitive Science 2: 155-192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R R Burton</author>
</authors>
<title>Diagnosing bugs in a simple procedural skill. Intelligent Tutoring Systems,</title>
<date>1982</date>
<pages>157--183</pages>
<publisher>Academic Press,</publisher>
<location>London, England:</location>
<contexts>
<context position="24895" citStr="Burton 1982" startWordPosition="4110" endWordPosition="4111">, can trivially detect that a user belief that &amp;quot;rm&amp;quot; is applicable to removing a directory is incorrect. These domain-specific beliefs are assumed to derive from past advisor experiences. An advisor who successfully uses &amp;quot;rm&amp;quot; to remove a file will believe that using &amp;quot;rm&amp;quot; is applicable to the goal of removing a file. An advisor who uses &amp;quot;rm&amp;quot; to try to remove a directory and has it fail will believe that &amp;quot;rm&amp;quot; is not applicable to removing a directory. The negated beliefs correspond to the bug lists kept by many tutoring and planning systems (Anderson, Boyle, and Yost 1985; Brown and Burton 1978; Burton 1982; Stevens, Collins, and Goldin 1982). The second type of advisor knowledge is a set of rules that help infer negated domain-specific beliefs, such as a belief that a particular action does not result in a particular state, or that a given plan is not useful for a particular goal. These rules are needed because the advisor cannot be expected to have a complete set of these beliefs. One such rule, for example, suggests that 42 Computational Linguistics, Volume 14, Number 3, September 1988 Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptions &amp;quot;if a state S is not a</context>
<context position="48425" citStr="Burton 1982" startWordPosition="8047" endWordPosition="8048"> incorrectly attribute an enablement of one action to another action. Another rule might be that a user may incorrectly attribute the result of one action to another action. From a set of such rules the advisor must somehow deduce the cause of the user&apos;s mistake. By using potential explanations the problem becomes instead one of guided memory search rather than reasoning from first principles. 7 RELATED WORK Two approaches have been used to detect and correct misconceptions. The first approach is used by many intelligent tutoring systems (Anderson, Boyle, and Yost 1985; Brown and Burton 1978; Burton 1982; Stevens, Collins, and Goldin 1982). These systems locate mistaken beliefs in a data base of error-explanation pairs and provide the associated explanation. A basic problem with this approach is that, because there is no information about the underlying causes of the errors, these systems can handle only those misconceptions known in advance. The other approach avoids the difficulty inherent in enumerating all possible misconceptions within a domain by using strategies that address an entire class of misconceptions. The user&apos;s misconception is classified according to the abstract reasoning er</context>
</contexts>
<marker>Burton, 1982</marker>
<rawString>Burton, R.R. 1982 Diagnosing bugs in a simple procedural skill. Intelligent Tutoring Systems, Academic Press, London, England: 157-183.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S Carberry</author>
</authors>
<title>Plan Recognition and User Modeling, this issue.</title>
<marker>Carberry, </marker>
<rawString>Carberry, S. Plan Recognition and User Modeling, this issue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chin</author>
</authors>
<title>User modeling in UC, the UNIX consultant.</title>
<date>1986</date>
<booktitle>In Proceedings of the CHI-86 Conference,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="55632" citStr="Chin 1986" startWordPosition="9218" endWordPosition="9219"> not specify how knowledge of the cause of the user&apos;s mistaken belief affects the information to be included in a cooperative response, something that falls naturally out of our model. 7.3 UNIX ADVISORS Finally, there are two other related research efforts, UC (Wilensky et al. 1986, Wilensky, Arens, and Chin 1984) and SC (Kemke 1986), that address providing advice to novice UNIX users. Neither system, however, detects or responds to misconceptions. Instead, both are concerned with tailoring a response to a question to reflect the user&apos;s level of expertise. UC&apos;s user modeling component, KNOME (Chin 1986), analyzes a user&apos;s questions to determine which stereotypical class the user belongs to and then uses this information to provide more details and possibly more examples to less experienced users. Novice: What does the &amp;quot;rwho&amp;quot; command do? UC: Rwho lists all users on the network, their tty, their login time, and their idle time. Expert: What does the &amp;quot;rwho&amp;quot; command do? UC: Rwho is like who, except rwho lists all users on the network. SC&apos;s user modeling component, SCUM (Nessen 1987), takes an approach similar to UC&apos;s, also using stereotypical information. These approaches are complementary to ou</context>
<context position="64700" citStr="Chin 1986" startWordPosition="10728" endWordPosition="10729">remove my directory and I got an error message &amp;quot;directory not empty&amp;quot;. But &amp;quot;ls&amp;quot; didn&apos;t list any files. Advisor: Use &amp;quot;ls -a&amp;quot; to list all of your files. &amp;quot;ls&amp;quot; cannot be used to list all of your because &amp;quot;ls&amp;quot; does not list those files whose names begin with a period. An advisor who knows the user is a novice might want to augment his response with an explanation that &amp;quot;-a&amp;quot; is a command option and that command options cause changes in the normal behavior of commands. Several researchers are working on tailoring the response to the user based on knowledge about the user&apos;s expertise (Paris, this issue; Chin 1986). 10 CONCLUSIONS We have presented an explanation-based approach to the problem of recognizing and responding to user misconceptions. The advisor confirms that a user&apos;s belief is a misconception by finding an explanation for why he does not hold the user&apos;s belief. The advisor infers its source by finding an explanation for the mistaken belief. The process of finding an explanation was presented as one of hypothesizing and trying to verify a small set of potential explanations associated with each type of user belief. In essence, the model uses information about likely sources of different clas</context>
</contexts>
<marker>Chin, 1986</marker>
<rawString>Chin, D. 1986 User modeling in UC, the UNIX consultant. In Proceedings of the CHI-86 Conference, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cullingford</author>
</authors>
<title>Script Application: Computer Understanding of Newspaper Stories,</title>
<date>1978</date>
<tech>Ph.D. thesis.</tech>
<institution>Yale University,</institution>
<location>New Haven, CT.</location>
<contexts>
<context position="9324" citStr="Cullingford 1978" startWordPosition="1552" endWordPosition="1553">condition for achieving one of its preconditions. Once the advisor finds these explanations, he presents them to the user as the response to his misconception. The response corrects the user&apos;s misconception by pointing out that the user&apos;s claimed precondition for removing a file is incorrect, by providing the actual precondition for removing a file, and by providing the missing user beliefs that led to the user&apos;s misconception. 2.1 OTHER WORK IN EXPLANATION-BASED UNDERSTANDING Our approach derives from work in explanation-based story understanding (Schank 1986, Dyer 1983, Wilensky 1983, 1978, Cullingford 1978, Schank and Abelson 1977). The basic idea is that to understand a particular input, such as a person&apos;s action, we have to explain why it has occurred. One way to find an explanation for a person&apos;s action is to relate it to known goals the person Computational Linguistics, Volume 14, Number 3, September 1988 39 Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptions is trying to achieve. Suppose, for example, that a story understander reads that a hungry character bought a restaurant guidebook (Wilensky 1983). One explanation for this action is that hungry people </context>
</contexts>
<marker>Cullingford, 1978</marker>
<rawString>Cullingford, S. 1978 Script Application: Computer Understanding of Newspaper Stories, Ph.D. thesis. Yale University, New Haven, CT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Dolan</author>
<author>M Dyer</author>
</authors>
<title>Learning Planning Heuristics through Observation.</title>
<date>1985</date>
<booktitle>In Proceedings of the 1985 Joint Conference on Artificial Intelligence,</booktitle>
<pages>600--602</pages>
<location>Los Angeles, CA:</location>
<contexts>
<context position="62084" citStr="Dolan and Dyer (1985" startWordPosition="10280" endWordPosition="10283">dvisor may be able to deduce that write permission is needed to remove a file from his beliefs that write permission is needed to make changes on objects and that removing a file involves making a change to a directory. This requires more powerful reasoning capabilities than AQUA&apos;s simple rules for inferring negated beliefs. Finally, AQUA assumes the existence of a taxonomy of planning failures. We have left the automatic creation of this taxonomy from advisor experiences to future research. Initial work in recognizing and indexing abstract configurations of planning relations is discussed in Dolan and Dyer (1985, 1986). 9.3 OTHER CLASSES OF MISCONCEPTIONS We are currently studying how well the classes of misconceptions described here account for responses to misconceptions in domains other than the problems of novice computer users, such as the domain of simple day-to-day planning. In addition, we are examining other classes of planning misconceptions. For example, to respond to an incorrect user belief such as &amp;quot;rm&amp;quot; cannot be used to remove a file, the advisor needs potential explanations for why an action does not apply to a particular goal state. We do not yet know whether our approach is suitable </context>
</contexts>
<marker>Dolan, Dyer, 1985</marker>
<rawString>Dolan, C. and Dyer, M. 1985 Learning Planning Heuristics through Observation. In Proceedings of the 1985 Joint Conference on Artificial Intelligence, Los Angeles, CA: 600-602.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Dolan</author>
<author>M Dyer</author>
</authors>
<title>Encoding Planning Knowledge for Recognition, Construction, and Learning.</title>
<date>1986</date>
<booktitle>In Proceedings of the 8th Annual Cognitive Science Society:</booktitle>
<pages>488--499</pages>
<contexts>
<context position="49335" citStr="Dolan and Dyer 1986" startWordPosition="8185" endWordPosition="8188"> can handle only those misconceptions known in advance. The other approach avoids the difficulty inherent in enumerating all possible misconceptions within a domain by using strategies that address an entire class of misconceptions. The user&apos;s misconception is classified according to the abstract reasoning error likely to have led to it. This approach shares many features with recognizing abstract thematic situations (such as irony) in narratives, where such situations are defined in terms of abstract planning errors made by the narrative characters (Dyer 1983; Dyer, Flowers, and Reeves 1987; Dolan and Dyer 1986). Once an appropriate strategy is found, it can be used to generate advice (in narratives, this advice may be in the form of adages). In advisory systems, this approach has been applied to both objectand plan-oriented misconceptions. 7.1 OBJECT-ORIENTED MISCONCEPTIONS ROMPER (McCoy 1985, and this issue) corrects user misconceptions dealing with whether an object is an instance of a particular class of objects or possesses a particular property. User: I thought whales were fish. ROMPER: No, they are mammals. You may have thought they were fish because they are finbearing and live in the water. </context>
</contexts>
<marker>Dolan, Dyer, 1986</marker>
<rawString>Dolan, C. and Dyer, M. 1986 Encoding Planning Knowledge for Recognition, Construction, and Learning. In Proceedings of the 8th Annual Cognitive Science Society: 488-499.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dyer</author>
</authors>
<title>In-Depth Understanding: A Computer Model of Narrative Comprehension,</title>
<date>1983</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="9285" citStr="Dyer 1983" startWordPosition="1547" endWordPosition="1548">n for removing a file, but a precondition for achieving one of its preconditions. Once the advisor finds these explanations, he presents them to the user as the response to his misconception. The response corrects the user&apos;s misconception by pointing out that the user&apos;s claimed precondition for removing a file is incorrect, by providing the actual precondition for removing a file, and by providing the missing user beliefs that led to the user&apos;s misconception. 2.1 OTHER WORK IN EXPLANATION-BASED UNDERSTANDING Our approach derives from work in explanation-based story understanding (Schank 1986, Dyer 1983, Wilensky 1983, 1978, Cullingford 1978, Schank and Abelson 1977). The basic idea is that to understand a particular input, such as a person&apos;s action, we have to explain why it has occurred. One way to find an explanation for a person&apos;s action is to relate it to known goals the person Computational Linguistics, Volume 14, Number 3, September 1988 39 Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptions is trying to achieve. Suppose, for example, that a story understander reads that a hungry character bought a restaurant guidebook (Wilensky 1983). One explanation</context>
<context position="14212" citStr="Dyer 1983" startWordPosition="2339" endWordPosition="2340">lies(A,S) A is not a plan for achieving S precludes(S1,S2) S1 and S2 cannot exist simultaneously !precludes(S1,S2) S 1 and S2 can exist simultaneously goal(A,S) Actor A wants to achieve S These relationships are derived from existing representations. SPIRIT&apos;s (Pollack 1986) representation for planning knowledge uses gen to represent a state resulting in an action and cgen to represent a state resulting in an action only if some other state exists. Causes and enables are identical in semantics to gen and cgen. Applies, which has no analog in SPIRIT, is similar to the intends relation in BORIS (Dyer 1983). The difference between causes and applies is in whether the action is intended to cause the state that results from its execution to exist. &amp;quot;Causes&amp;quot; represents cause-effect relations which are nonintentional, while &amp;quot;applies&amp;quot; represents a cause-effect relation between an action (sequence) or plan which is intended to achieve a desired state (a goal). An action causes a state whenever the state results from its execution. An action applies to a state when an actor believes the action will cause the desired state to occur. 40 Computational Linguistics, Volume 14, Number 3, September 1988 Quilic</context>
<context position="49281" citStr="Dyer 1983" startWordPosition="8178" endWordPosition="8179">erlying causes of the errors, these systems can handle only those misconceptions known in advance. The other approach avoids the difficulty inherent in enumerating all possible misconceptions within a domain by using strategies that address an entire class of misconceptions. The user&apos;s misconception is classified according to the abstract reasoning error likely to have led to it. This approach shares many features with recognizing abstract thematic situations (such as irony) in narratives, where such situations are defined in terms of abstract planning errors made by the narrative characters (Dyer 1983; Dyer, Flowers, and Reeves 1987; Dolan and Dyer 1986). Once an appropriate strategy is found, it can be used to generate advice (in narratives, this advice may be in the form of adages). In advisory systems, this approach has been applied to both objectand plan-oriented misconceptions. 7.1 OBJECT-ORIENTED MISCONCEPTIONS ROMPER (McCoy 1985, and this issue) corrects user misconceptions dealing with whether an object is an instance of a particular class of objects or possesses a particular property. User: I thought whales were fish. ROMPER: No, they are mammals. You may have thought they were fi</context>
<context position="61070" citStr="Dyer 1983" startWordPosition="10112" endWordPosition="10113">visor should search for. However, we have not specified how this search of the advisor&apos;s memory is actually carried out, how a belief in memory can be retrieved efficiently, or how the beliefs are actually acquired through experience. AQUA&apos;s organization of plan-oriented beliefs is discussed in Quilici (1988, 1985). It is based on earlier work (Kolodner 1985, Schank 1982) in taking experiences and indexing them appropriately for efficient search and retrieval, especially that involving indexing memory around various planning failures (Kolodner and Cullingford 1986, Quilici 1985, Hammond 1984, Dyer 1983). Because the advisor may need to verify a belief that is not stored directly in memory, memory search may not be sufficient. Suppose the advisor is trying to verify that owning a directory is not required to remove a file. The advisor may be able to deduce this belief from a past experience in which he removed a file from /tmp, a directory owned by the system administrator. Similarly, the advisor may be able to deduce that write permission is needed to remove a file from his beliefs that write permission is needed to make changes on objects and that removing a file involves making a change to</context>
</contexts>
<marker>Dyer, 1983</marker>
<rawString>Dyer, M. 1983 In-Depth Understanding: A Computer Model of Narrative Comprehension, MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dyer</author>
<author>M Flowers</author>
<author>J F Reeves</author>
</authors>
<title>Recognizing Situational Ironies: A Computer Model of Irony Recognition and Narrative Understanding.</title>
<date>1987</date>
<booktitle>In Advances in Computing and the Humanities: forthcoming.</booktitle>
<marker>Dyer, Flowers, Reeves, 1987</marker>
<rawString>Dyer, M., Flowers, M., and Reeves, J.F. 1987 Recognizing Situational Ironies: A Computer Model of Irony Recognition and Narrative Understanding. In Advances in Computing and the Humanities: forthcoming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Fikes</author>
<author>N J Nilsson</author>
</authors>
<title>STRIPS: A New Approach to the Application of Theorem Proving to Problem Solving.</title>
<date>1971</date>
<journal>Artificial Intelligence</journal>
<volume>2</volume>
<pages>189--208</pages>
<marker>Fikes, Nilsson, 1971</marker>
<rawString>Fikes, R.E. and Nilsson, N.J. 1971 STRIPS: A New Approach to the Application of Theorem Proving to Problem Solving. Artificial Intelligence 2: 189-208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M McGuire</author>
<author>R</author>
<author>L Birnbaum</author>
</authors>
<title>Adversary Arguments and the Logic of Personal Attacks.</title>
<date>1982</date>
<booktitle>In Strategies for Natural Language Processing. Lawrence Erlbaum,</booktitle>
<pages>275--294</pages>
<location>Hillsdale, NJ:</location>
<marker>McGuire, R, Birnbaum, 1982</marker>
<rawString>Flowers. M., McGuire, R., and Birnbaum, L. 1982 Adversary Arguments and the Logic of Personal Attacks. In Strategies for Natural Language Processing. Lawrence Erlbaum, Hillsdale, NJ: 275-294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Goodman</author>
</authors>
<title>Miscommunication and Plan Recognition. Unpublished manuscript from</title>
<date>1986</date>
<booktitle>UM86. International Workshop on User Modeling,</booktitle>
<location>Maria Laach, West</location>
<contexts>
<context position="60324" citStr="Goodman 1986" startWordPosition="10002" endWordPosition="10003">advisor needs to infer this user belief from the knowledge that the user did some sort of verify-action, the knowledge that one plan for dealing with a plan failure is to try to verify that the enablements of the plan have been achieved, and the knowledge that both owning the file and having write permission are different instantiations of having sufficient permission. Inferring beliefs like these, that involve the user&apos;s plans and goals and the relationships between, even when they differ from the advisor&apos;s, is currently an active area of research (Carberry, this issue; Kautz and Allen 1986, Goodman 1986, Wilensky et al. 1986, Quilici 1985). 9.2 RETRIEVING ADVISOR BELIEFS Our potential explanations suggest patterns of beliefs that the advisor should search for. However, we have not specified how this search of the advisor&apos;s memory is actually carried out, how a belief in memory can be retrieved efficiently, or how the beliefs are actually acquired through experience. AQUA&apos;s organization of plan-oriented beliefs is discussed in Quilici (1988, 1985). It is based on earlier work (Kolodner 1985, Schank 1982) in taking experiences and indexing them appropriately for efficient search and retrieval,</context>
</contexts>
<marker>Goodman, 1986</marker>
<rawString>Goodman, B. 1986 Miscommunication and Plan Recognition. Unpublished manuscript from UM86. International Workshop on User Modeling, Maria Laach, West Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hammond</author>
</authors>
<title>Indexing and Causality: The Organization of Plans and Strategies in Memory.</title>
<date>1984</date>
<tech>Technical Report 351,</tech>
<institution>Yale University,</institution>
<location>New Haven, CT.</location>
<contexts>
<context position="61058" citStr="Hammond 1984" startWordPosition="10110" endWordPosition="10111">fs that the advisor should search for. However, we have not specified how this search of the advisor&apos;s memory is actually carried out, how a belief in memory can be retrieved efficiently, or how the beliefs are actually acquired through experience. AQUA&apos;s organization of plan-oriented beliefs is discussed in Quilici (1988, 1985). It is based on earlier work (Kolodner 1985, Schank 1982) in taking experiences and indexing them appropriately for efficient search and retrieval, especially that involving indexing memory around various planning failures (Kolodner and Cullingford 1986, Quilici 1985, Hammond 1984, Dyer 1983). Because the advisor may need to verify a belief that is not stored directly in memory, memory search may not be sufficient. Suppose the advisor is trying to verify that owning a directory is not required to remove a file. The advisor may be able to deduce this belief from a past experience in which he removed a file from /tmp, a directory owned by the system administrator. Similarly, the advisor may be able to deduce that write permission is needed to remove a file from his beliefs that write permission is needed to make changes on objects and that removing a file involves making</context>
</contexts>
<marker>Hammond, 1984</marker>
<rawString>Hammond, K. 1984 Indexing and Causality: The Organization of Plans and Strategies in Memory. Technical Report 351, Yale University, New Haven, CT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kautz</author>
<author>J Allen</author>
</authors>
<title>Generalized Plan Recognition.</title>
<date>1986</date>
<booktitle>In Proceedings of the 1986 National Conference on Artificial Intelligence,</booktitle>
<pages>423--427</pages>
<location>Los Angeles, CA:</location>
<contexts>
<context position="60310" citStr="Kautz and Allen 1986" startWordPosition="9998" endWordPosition="10001">ditions. Instead, the advisor needs to infer this user belief from the knowledge that the user did some sort of verify-action, the knowledge that one plan for dealing with a plan failure is to try to verify that the enablements of the plan have been achieved, and the knowledge that both owning the file and having write permission are different instantiations of having sufficient permission. Inferring beliefs like these, that involve the user&apos;s plans and goals and the relationships between, even when they differ from the advisor&apos;s, is currently an active area of research (Carberry, this issue; Kautz and Allen 1986, Goodman 1986, Wilensky et al. 1986, Quilici 1985). 9.2 RETRIEVING ADVISOR BELIEFS Our potential explanations suggest patterns of beliefs that the advisor should search for. However, we have not specified how this search of the advisor&apos;s memory is actually carried out, how a belief in memory can be retrieved efficiently, or how the beliefs are actually acquired through experience. AQUA&apos;s organization of plan-oriented beliefs is discussed in Quilici (1988, 1985). It is based on earlier work (Kolodner 1985, Schank 1982) in taking experiences and indexing them appropriately for efficient search </context>
</contexts>
<marker>Kautz, Allen, 1986</marker>
<rawString>Kautz, H. and Allen, J. 1986 Generalized Plan Recognition. In Proceedings of the 1986 National Conference on Artificial Intelligence, Los Angeles, CA: 423-427.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Kemke</author>
</authors>
<title>The SINIX Consultant: Requirements, Design and Implementation of an Intelligent Help System for a UNIX</title>
<date>1986</date>
<booktitle>Deny50 Computational Linguistics, Volume 14, Number 3,</booktitle>
<contexts>
<context position="55357" citStr="Kemke 1986" startWordPosition="9175" endWordPosition="9176">ery different approach, trying to determine the cause of the user&apos;s error through reasoning from first principles rather than memory search. In addition, SPIRIT cannot detect or respond to mistakes involving plan applicability conditions or preconditions. Finally, SPIRIT does not specify how knowledge of the cause of the user&apos;s mistaken belief affects the information to be included in a cooperative response, something that falls naturally out of our model. 7.3 UNIX ADVISORS Finally, there are two other related research efforts, UC (Wilensky et al. 1986, Wilensky, Arens, and Chin 1984) and SC (Kemke 1986), that address providing advice to novice UNIX users. Neither system, however, detects or responds to misconceptions. Instead, both are concerned with tailoring a response to a question to reflect the user&apos;s level of expertise. UC&apos;s user modeling component, KNOME (Chin 1986), analyzes a user&apos;s questions to determine which stereotypical class the user belongs to and then uses this information to provide more details and possibly more examples to less experienced users. Novice: What does the &amp;quot;rwho&amp;quot; command do? UC: Rwho lists all users on the network, their tty, their login time, and their idle t</context>
</contexts>
<marker>Kemke, 1986</marker>
<rawString>Kemke, C. 1986 The SINIX Consultant: Requirements, Design and Implementation of an Intelligent Help System for a UNIX Deny50 Computational Linguistics, Volume 14, Number 3, September 1988</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dyer Quilici</author>
</authors>
<title>and Flowers Recognizing and Responding to Plan-Oriented Misconceptions ative.</title>
<date></date>
<tech>Report No. 11,</tech>
<institution>Al Laboratory, Dept. of Computer Science, Univ. of Saarbriicken, W.</institution>
<marker>Quilici, </marker>
<rawString>Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptions ative. Report No. 11, Al Laboratory, Dept. of Computer Science, Univ. of Saarbriicken, W. Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Joshi</author>
<author>B Webber</author>
<author>R Weishedel</author>
</authors>
<title>Living up to expectations: computing expert responses.</title>
<date>1984</date>
<booktitle>In Proceedings of the 1984 National Conference on Artificial Intelligence,</booktitle>
<pages>169--175</pages>
<location>Dallas, TX:</location>
<marker>Joshi, Webber, Weishedel, 1984</marker>
<rawString>Joshi, A., Webber, B., Weishedel, R. 1984 Living up to expectations: computing expert responses. In Proceedings of the 1984 National Conference on Artificial Intelligence, Dallas, TX: 169-175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Kolodner</author>
</authors>
<title>Retrieval and Organizational Strategies in Conceptual Memory, Lawrence Erlbaum,</title>
<date>1984</date>
<location>Hillsdale, NJ.</location>
<marker>Kolodner, 1984</marker>
<rawString>Kolodner, J.L. 1984 Retrieval and Organizational Strategies in Conceptual Memory, Lawrence Erlbaum, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Kolodner</author>
<author>R E Cullingford</author>
</authors>
<title>Towards a Memory Architecture that Supports Reminding.</title>
<date>1986</date>
<booktitle>In Proceedings of the 8th Annual Cognitive Science Society:</booktitle>
<pages>467--477</pages>
<contexts>
<context position="61030" citStr="Kolodner and Cullingford 1986" startWordPosition="10104" endWordPosition="10107">ential explanations suggest patterns of beliefs that the advisor should search for. However, we have not specified how this search of the advisor&apos;s memory is actually carried out, how a belief in memory can be retrieved efficiently, or how the beliefs are actually acquired through experience. AQUA&apos;s organization of plan-oriented beliefs is discussed in Quilici (1988, 1985). It is based on earlier work (Kolodner 1985, Schank 1982) in taking experiences and indexing them appropriately for efficient search and retrieval, especially that involving indexing memory around various planning failures (Kolodner and Cullingford 1986, Quilici 1985, Hammond 1984, Dyer 1983). Because the advisor may need to verify a belief that is not stored directly in memory, memory search may not be sufficient. Suppose the advisor is trying to verify that owning a directory is not required to remove a file. The advisor may be able to deduce this belief from a past experience in which he removed a file from /tmp, a directory owned by the system administrator. Similarly, the advisor may be able to deduce that write permission is needed to remove a file from his beliefs that write permission is needed to make changes on objects and that rem</context>
</contexts>
<marker>Kolodner, Cullingford, 1986</marker>
<rawString>Kolodner, J.L. and Cullingford, R.E. 1986 Towards a Memory Architecture that Supports Reminding. In Proceedings of the 8th Annual Cognitive Science Society: 467-477.</rawString>
</citation>
<citation valid="false">
<authors>
<author>K McCoy</author>
</authors>
<title>Reasoning on a highlighted user model to respond to misconceptions. this issue.</title>
<marker>McCoy, </marker>
<rawString>McCoy, K. Reasoning on a highlighted user model to respond to misconceptions. this issue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McCoy</author>
</authors>
<title>Responding to Object-Oriented Misconceptions.</title>
<date>1985</date>
<tech>Ph.D. Thesis.</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="49622" citStr="McCoy 1985" startWordPosition="8231" endWordPosition="8232"> reasoning error likely to have led to it. This approach shares many features with recognizing abstract thematic situations (such as irony) in narratives, where such situations are defined in terms of abstract planning errors made by the narrative characters (Dyer 1983; Dyer, Flowers, and Reeves 1987; Dolan and Dyer 1986). Once an appropriate strategy is found, it can be used to generate advice (in narratives, this advice may be in the form of adages). In advisory systems, this approach has been applied to both objectand plan-oriented misconceptions. 7.1 OBJECT-ORIENTED MISCONCEPTIONS ROMPER (McCoy 1985, and this issue) corrects user misconceptions dealing with whether an object is an instance of a particular class of objects or possesses a particular property. User: I thought whales were fish. ROMPER: No, they are mammals. You may have thought they were fish because they are finbearing and live in the water. However, they are mammals since, while fish have gills, whales breathe through lungs and feed their young with milk. ROMPER classifies a user&apos;s misconception as either a misclassification or misattribution and then selects one of several strategies associated with each class of misconce</context>
</contexts>
<marker>McCoy, 1985</marker>
<rawString>McCoy, K. 1985 Responding to Object-Oriented Misconceptions. Ph.D. Thesis. University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Nessen</author>
</authors>
<title>SCUM: User modeling in the SINIX consultant.</title>
<date>1987</date>
<journal>Memo</journal>
<volume>18</volume>
<institution>Al Laboratory, Dept. of Computer Science, University of Saarbriicken,</institution>
<location>Saarbriicken, West</location>
<contexts>
<context position="56117" citStr="Nessen 1987" startWordPosition="9301" endWordPosition="9302">th tailoring a response to a question to reflect the user&apos;s level of expertise. UC&apos;s user modeling component, KNOME (Chin 1986), analyzes a user&apos;s questions to determine which stereotypical class the user belongs to and then uses this information to provide more details and possibly more examples to less experienced users. Novice: What does the &amp;quot;rwho&amp;quot; command do? UC: Rwho lists all users on the network, their tty, their login time, and their idle time. Expert: What does the &amp;quot;rwho&amp;quot; command do? UC: Rwho is like who, except rwho lists all users on the network. SC&apos;s user modeling component, SCUM (Nessen 1987), takes an approach similar to UC&apos;s, also using stereotypical information. These approaches are complementary to ours. 8 IMPLEMENTATION DETAILS The theory discussed in this paper is embodied in AQUA, a computer program currently under development at UCLA. The current version of AQUA is implemented in T (Rees, Adams, and Meehan 1984), using RHAPSODY (Turner and Reeves 1987), a graphical Al tools environment with Prolog-like unification and backtracking capabilities, and runs on an Apollo DN460 workstation. Given a set of user beliefs involving plan applicability conditions, preconditions, or ef</context>
</contexts>
<marker>Nessen, 1987</marker>
<rawString>Nessen, E. 1987 SCUM: User modeling in the SINIX consultant. Memo No. 18, Al Laboratory, Dept. of Computer Science, University of Saarbriicken, Saarbriicken, West Germany.</rawString>
</citation>
<citation valid="false">
<authors>
<author>C Paris</author>
</authors>
<title>Tailoring object descriptions to the user&apos;s level of expertise. this issue.</title>
<marker>Paris, </marker>
<rawString>Paris, C. Tailoring object descriptions to the user&apos;s level of expertise. this issue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pollack</author>
</authors>
<title>A model of plan inference that distinguishes between the beliefs of actors and observers.</title>
<date>1986</date>
<booktitle>In Proceedings of 24th meeting of the Association of Computational Linguistics,</booktitle>
<location>New York, NY.</location>
<contexts>
<context position="12313" citStr="Pollack 1986" startWordPosition="2026" endWordPosition="2028"> are implicitly assumed to represent that agent&apos;s beliefs. However, because user misconceptions occur when the user&apos;s planning knowledge differs from the advisor&apos;s, systems that deal with user misconceptions must explicitly distinguish between advisor beliefs about what the user knows and advisor beliefs about what the advisor knows. Our representation for beliefs (Abelson 1973, 1979) is similar to that used by existing systems that keep track of the possibly contradictory knowledge of multiple participants (Alvarado 1987; Alvarado, Dyer, and Flowers 1986; Flowers, McGuire, and Birnbaum 1982; Pollack 1986). A belief relation represents an advisor&apos;s belief that an actor maintains that a particular plan applicability condition, precondition, or effect holds. The actor is either the user or the advisor. belief(user, R) Advisor believes that user maintains R belief(advisor, R) Advisor believes that advisor maintains R In this paper we do not discuss beliefs involving other relationships, such as a belief that an object has a particular property. In addition, for readability we do not use the belief predicate here, but instead precede a list of planning relationships with either &amp;quot;the user believes&amp;quot; </context>
<context position="13876" citStr="Pollack 1986" startWordPosition="2280" endWordPosition="2281"> which are descriptions of properties of objects. causes(A,S) Executing A has an effect S !causes(A,S) Executing A does not have effect enables(S1,A,S2) 51 is necessary for A to have S2 as an effect !enables(S1,A,S2) S1 is unnecessary for A to have S2 as an effect applies(A,S) A is a correct or normal plan for achieving goal state S lapplies(A,S) A is not a plan for achieving S precludes(S1,S2) S1 and S2 cannot exist simultaneously !precludes(S1,S2) S 1 and S2 can exist simultaneously goal(A,S) Actor A wants to achieve S These relationships are derived from existing representations. SPIRIT&apos;s (Pollack 1986) representation for planning knowledge uses gen to represent a state resulting in an action and cgen to represent a state resulting in an action only if some other state exists. Causes and enables are identical in semantics to gen and cgen. Applies, which has no analog in SPIRIT, is similar to the intends relation in BORIS (Dyer 1983). The difference between causes and applies is in whether the action is intended to cause the state that results from its execution to exist. &amp;quot;Causes&amp;quot; represents cause-effect relations which are nonintentional, while &amp;quot;applies&amp;quot; represents a cause-effect relation be</context>
<context position="53811" citStr="Pollack 1986" startWordPosition="8923" endWordPosition="8924">s in several respects. The main difference is that they focus on correcting the user&apos;s misconception instead of trying to explain why it occurred. Only one strategy above is concerned with providing an explanation that addresses the source of a user misconception (in this case, an inappropriate plan). The other strategies describe situations in which achieving X is inappropriate and an alternate plan for Y exists and should be presented to the user as a correction. In addition, they did not consider responding to incorrect beliefs about plan preconditions or effects. The other effort, SPIRIT (Pollack 1986), tries to detect the inappropriate plans underlying queries made by users of a computer mail program and the mistaken user beliefs underlying those plans. User: I want to prevent Tom from reading my file. How can I set the permissions on it to faculty-read only? System: You can make the file readable by faculty only using &amp;quot;set permission&amp;quot;. However, Tom can still read it because he&apos;s the system administrator. User misconceptions about the applicability and executability of plans are detected by reasoning about the likely differences between the advisor&apos;s beliefs and the user&apos;s, with various ru</context>
</contexts>
<marker>Pollack, 1986</marker>
<rawString>Pollack, M. 1986 A model of plan inference that distinguishes between the beliefs of actors and observers. In Proceedings of 24th meeting of the Association of Computational Linguistics, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pollack</author>
</authors>
<title>Inferring domain plans in question-answering.</title>
<date>1986</date>
<tech>Ph.D. Thesis.</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="12313" citStr="Pollack 1986" startWordPosition="2026" endWordPosition="2028"> are implicitly assumed to represent that agent&apos;s beliefs. However, because user misconceptions occur when the user&apos;s planning knowledge differs from the advisor&apos;s, systems that deal with user misconceptions must explicitly distinguish between advisor beliefs about what the user knows and advisor beliefs about what the advisor knows. Our representation for beliefs (Abelson 1973, 1979) is similar to that used by existing systems that keep track of the possibly contradictory knowledge of multiple participants (Alvarado 1987; Alvarado, Dyer, and Flowers 1986; Flowers, McGuire, and Birnbaum 1982; Pollack 1986). A belief relation represents an advisor&apos;s belief that an actor maintains that a particular plan applicability condition, precondition, or effect holds. The actor is either the user or the advisor. belief(user, R) Advisor believes that user maintains R belief(advisor, R) Advisor believes that advisor maintains R In this paper we do not discuss beliefs involving other relationships, such as a belief that an object has a particular property. In addition, for readability we do not use the belief predicate here, but instead precede a list of planning relationships with either &amp;quot;the user believes&amp;quot; </context>
<context position="13876" citStr="Pollack 1986" startWordPosition="2280" endWordPosition="2281"> which are descriptions of properties of objects. causes(A,S) Executing A has an effect S !causes(A,S) Executing A does not have effect enables(S1,A,S2) 51 is necessary for A to have S2 as an effect !enables(S1,A,S2) S1 is unnecessary for A to have S2 as an effect applies(A,S) A is a correct or normal plan for achieving goal state S lapplies(A,S) A is not a plan for achieving S precludes(S1,S2) S1 and S2 cannot exist simultaneously !precludes(S1,S2) S 1 and S2 can exist simultaneously goal(A,S) Actor A wants to achieve S These relationships are derived from existing representations. SPIRIT&apos;s (Pollack 1986) representation for planning knowledge uses gen to represent a state resulting in an action and cgen to represent a state resulting in an action only if some other state exists. Causes and enables are identical in semantics to gen and cgen. Applies, which has no analog in SPIRIT, is similar to the intends relation in BORIS (Dyer 1983). The difference between causes and applies is in whether the action is intended to cause the state that results from its execution to exist. &amp;quot;Causes&amp;quot; represents cause-effect relations which are nonintentional, while &amp;quot;applies&amp;quot; represents a cause-effect relation be</context>
<context position="53811" citStr="Pollack 1986" startWordPosition="8923" endWordPosition="8924">s in several respects. The main difference is that they focus on correcting the user&apos;s misconception instead of trying to explain why it occurred. Only one strategy above is concerned with providing an explanation that addresses the source of a user misconception (in this case, an inappropriate plan). The other strategies describe situations in which achieving X is inappropriate and an alternate plan for Y exists and should be presented to the user as a correction. In addition, they did not consider responding to incorrect beliefs about plan preconditions or effects. The other effort, SPIRIT (Pollack 1986), tries to detect the inappropriate plans underlying queries made by users of a computer mail program and the mistaken user beliefs underlying those plans. User: I want to prevent Tom from reading my file. How can I set the permissions on it to faculty-read only? System: You can make the file readable by faculty only using &amp;quot;set permission&amp;quot;. However, Tom can still read it because he&apos;s the system administrator. User misconceptions about the applicability and executability of plans are detected by reasoning about the likely differences between the advisor&apos;s beliefs and the user&apos;s, with various ru</context>
</contexts>
<marker>Pollack, 1986</marker>
<rawString>Pollack, M. 1986 Inferring domain plans in question-answering. Ph.D. Thesis. University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Quilici</author>
</authors>
<title>AQUA: A system that detects and responds to user misconceptions.</title>
<date>1988</date>
<booktitle>In Kobsa Alfred and Wahlster Wolfgang (eds.), User Modeling and Dialog Systems,</booktitle>
<publisher>Springer Verlag,</publisher>
<location>Berlin, New York.</location>
<contexts>
<context position="60769" citStr="Quilici (1988" startWordPosition="10069" endWordPosition="10070">d the relationships between, even when they differ from the advisor&apos;s, is currently an active area of research (Carberry, this issue; Kautz and Allen 1986, Goodman 1986, Wilensky et al. 1986, Quilici 1985). 9.2 RETRIEVING ADVISOR BELIEFS Our potential explanations suggest patterns of beliefs that the advisor should search for. However, we have not specified how this search of the advisor&apos;s memory is actually carried out, how a belief in memory can be retrieved efficiently, or how the beliefs are actually acquired through experience. AQUA&apos;s organization of plan-oriented beliefs is discussed in Quilici (1988, 1985). It is based on earlier work (Kolodner 1985, Schank 1982) in taking experiences and indexing them appropriately for efficient search and retrieval, especially that involving indexing memory around various planning failures (Kolodner and Cullingford 1986, Quilici 1985, Hammond 1984, Dyer 1983). Because the advisor may need to verify a belief that is not stored directly in memory, memory search may not be sufficient. Suppose the advisor is trying to verify that owning a directory is not required to remove a file. The advisor may be able to deduce this belief from a past experience in whi</context>
</contexts>
<marker>Quilici, 1988</marker>
<rawString>Quilici, A. 1988 AQUA: A system that detects and responds to user misconceptions. In Kobsa Alfred and Wahlster Wolfgang (eds.), User Modeling and Dialog Systems, Springer Verlag, Berlin, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Quilici</author>
<author>M Dyer</author>
<author>M Flowers</author>
</authors>
<title>AQUA: An intelligent UNIX advisor.</title>
<date>1986</date>
<booktitle>In Proceedings of the 1986 European Conference on Artificial Intelligence,</booktitle>
<pages>33--38</pages>
<location>Brighton, England:</location>
<marker>Quilici, Dyer, Flowers, 1986</marker>
<rawString>Quilici, A., Dyer, M., and Flowers, M. 1986 AQUA: An intelligent UNIX advisor. In Proceedings of the 1986 European Conference on Artificial Intelligence, Brighton, England: 33-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Quilici</author>
</authors>
<title>Human problem understanding and advice giving: A computer model.</title>
<date>1985</date>
<tech>Technical Report No. 85-00069,</tech>
<institution>Computer Science Department, University of California,</institution>
<location>Los Angeles, CA.</location>
<contexts>
<context position="16846" citStr="Quilici (1985)" startWordPosition="2789" endWordPosition="2790">isor holds several similar beliefs, except that he believes that to remove a file it is necessary to have write permission on the directory containing it. In terms of the planning relationships, the advisor&apos;s beliefs are: applies(using &amp;quot;rm file&amp;quot;, the file&apos;s removal) enables(directory write permission, using &amp;quot;rm&amp;quot;, the file&apos;s removal) causes(using &amp;quot;rm&amp;quot; on the user&apos;s file, an error message) (The paper is not concerned with representing notions such as &amp;quot;the file&apos;s removal&amp;quot; or &amp;quot;write permission on the directory containing the file&amp;quot;. The details of the representation for such things may be found in Quilici (1985).) The user and advisor in this exchange share one belief that we have not represented. This belief is that using &amp;quot;rm&amp;quot; did not cause the user&apos;s file to be removed. To represent beliefs that a state did not result from an action, that a plan is not applicable to a goal, or that a state is not an enablement condition of an action having another state as a result, we use !causes, !applies, and !enables, respectively. The belief above is represented with !causes, a belief that &amp;quot;mkdir&amp;quot; is not used to remove a file is represented with !applies, and a belief that &amp;quot;rm&amp;quot; does not require owning the dire</context>
<context position="57346" citStr="Quilici (1985)" startWordPosition="9499" endWordPosition="9500">ermines which of these user beliefs are incorrect and what missing or mistaken user beliefs are likely to have led to them, and then produces a set of advisor beliefs that capture the content of the advisor&apos;s response. AQUA&apos;s domain of expertise is in the basic plans used to manipulate and access files, directories, and electronic mail. It has been used to detect and respond to at least two different incorrect user beliefs in each class of misconception that we have identified. More detailed descriptions of the program&apos;s implementation can be found in Quilici, Flowers, and Dyer (1986), and in Quilici (1985). 9 LIMITATIONS AND FUTURE WORK Our approach to determining why an actor does or does not hold a particular belief has been to let potential explanations direct the search for the advisor beliefs that serve as an appropriate explanation. Our focus has been on discovering and representing these explanations. The limitations of our approach arise in areas we have ignored, each of which is an interesting area of research. 48 Computational Linguistics, Volume 14, Number 3, September 1988 Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptions 9.1 INFERRING THE SET OF </context>
<context position="60361" citStr="Quilici 1985" startWordPosition="10008" endWordPosition="10009">ief from the knowledge that the user did some sort of verify-action, the knowledge that one plan for dealing with a plan failure is to try to verify that the enablements of the plan have been achieved, and the knowledge that both owning the file and having write permission are different instantiations of having sufficient permission. Inferring beliefs like these, that involve the user&apos;s plans and goals and the relationships between, even when they differ from the advisor&apos;s, is currently an active area of research (Carberry, this issue; Kautz and Allen 1986, Goodman 1986, Wilensky et al. 1986, Quilici 1985). 9.2 RETRIEVING ADVISOR BELIEFS Our potential explanations suggest patterns of beliefs that the advisor should search for. However, we have not specified how this search of the advisor&apos;s memory is actually carried out, how a belief in memory can be retrieved efficiently, or how the beliefs are actually acquired through experience. AQUA&apos;s organization of plan-oriented beliefs is discussed in Quilici (1988, 1985). It is based on earlier work (Kolodner 1985, Schank 1982) in taking experiences and indexing them appropriately for efficient search and retrieval, especially that involving indexing m</context>
</contexts>
<marker>Quilici, 1985</marker>
<rawString>Quilici, A. 1985 Human problem understanding and advice giving: A computer model. Technical Report No. 85-00069, Computer Science Department, University of California, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Rees</author>
<author>N L Adams</author>
<author>J R Meehan</author>
</authors>
<title>The T manual. Yale University,</title>
<date>1984</date>
<location>New Haven, CT.</location>
<marker>Rees, Adams, Meehan, 1984</marker>
<rawString>Rees, J.A., Adams, N.L., and Meehan, J.R. 1984 The T manual. Yale University, New Haven, CT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Sacerdoti</author>
</authors>
<title>Planning in a hierarchy of abstraction spaces.</title>
<date>1974</date>
<journal>Artificial Intelligence</journal>
<volume>5</volume>
<issue>2</issue>
<pages>115--135</pages>
<contexts>
<context position="11479" citStr="Sacerdoti 1974" startWordPosition="1903" endWordPosition="1904">e a taxonomy of explanations for the types of beliefs we consider. Finally, we show how our approach compares with other work in detecting and correcting user misconceptions. 3 REPRESENTING USER AND ADVISOR BELIEFS The mistaken user beliefs that we consider involve plan applicability conditions, enablements, and effects. In this section we describe how these beliefs are represented. In essence, we make use of existing frameworks for representing planning knowledge, except that we are careful to distinguish between user and advisor beliefs. Traditional planning systems (Fikes and Nilsson 1974, Sacerdoti 1974) represent an agent&apos;s planning knowledge as a data base of operators associated with applicability conditions, preconditions, and effects. Since these systems have only one agent, the planner, the entries in the data base are implicitly assumed to represent that agent&apos;s beliefs. However, because user misconceptions occur when the user&apos;s planning knowledge differs from the advisor&apos;s, systems that deal with user misconceptions must explicitly distinguish between advisor beliefs about what the user knows and advisor beliefs about what the advisor knows. Our representation for beliefs (Abelson 197</context>
</contexts>
<marker>Sacerdoti, 1974</marker>
<rawString>Sacerdoti, E. 1974 Planning in a hierarchy of abstraction spaces. Artificial Intelligence 5(2): 115-135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Schank</author>
</authors>
<title>Dynamic Memory.</title>
<date>1982</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="60834" citStr="Schank 1982" startWordPosition="10080" endWordPosition="10081">r&apos;s, is currently an active area of research (Carberry, this issue; Kautz and Allen 1986, Goodman 1986, Wilensky et al. 1986, Quilici 1985). 9.2 RETRIEVING ADVISOR BELIEFS Our potential explanations suggest patterns of beliefs that the advisor should search for. However, we have not specified how this search of the advisor&apos;s memory is actually carried out, how a belief in memory can be retrieved efficiently, or how the beliefs are actually acquired through experience. AQUA&apos;s organization of plan-oriented beliefs is discussed in Quilici (1988, 1985). It is based on earlier work (Kolodner 1985, Schank 1982) in taking experiences and indexing them appropriately for efficient search and retrieval, especially that involving indexing memory around various planning failures (Kolodner and Cullingford 1986, Quilici 1985, Hammond 1984, Dyer 1983). Because the advisor may need to verify a belief that is not stored directly in memory, memory search may not be sufficient. Suppose the advisor is trying to verify that owning a directory is not required to remove a file. The advisor may be able to deduce this belief from a past experience in which he removed a file from /tmp, a directory owned by the system a</context>
</contexts>
<marker>Schank, 1982</marker>
<rawString>Schank, R.C. 1982 Dynamic Memory. Cambridge University Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Schank</author>
<author>R Abelson</author>
</authors>
<title>Scripts, Plans, Goals, and Understanding, Lawrence Erlbaum,</title>
<date>1977</date>
<location>Hillsdale, NJ.</location>
<contexts>
<context position="9350" citStr="Schank and Abelson 1977" startWordPosition="1554" endWordPosition="1557">eving one of its preconditions. Once the advisor finds these explanations, he presents them to the user as the response to his misconception. The response corrects the user&apos;s misconception by pointing out that the user&apos;s claimed precondition for removing a file is incorrect, by providing the actual precondition for removing a file, and by providing the missing user beliefs that led to the user&apos;s misconception. 2.1 OTHER WORK IN EXPLANATION-BASED UNDERSTANDING Our approach derives from work in explanation-based story understanding (Schank 1986, Dyer 1983, Wilensky 1983, 1978, Cullingford 1978, Schank and Abelson 1977). The basic idea is that to understand a particular input, such as a person&apos;s action, we have to explain why it has occurred. One way to find an explanation for a person&apos;s action is to relate it to known goals the person Computational Linguistics, Volume 14, Number 3, September 1988 39 Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptions is trying to achieve. Suppose, for example, that a story understander reads that a hungry character bought a restaurant guidebook (Wilensky 1983). One explanation for this action is that hungry people want to eat, to eat you ha</context>
</contexts>
<marker>Schank, Abelson, 1977</marker>
<rawString>Schank, R.C., and Abelson, R. 1977 Scripts, Plans, Goals, and Understanding, Lawrence Erlbaum, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Schank</author>
</authors>
<title>Explanation Patterns: Understanding Mechanically and Creatively,</title>
<date>1986</date>
<location>Lawrence Erlbaum, Hillsdale, NJ.</location>
<contexts>
<context position="9274" citStr="Schank 1986" startWordPosition="1545" endWordPosition="1546">a precondition for removing a file, but a precondition for achieving one of its preconditions. Once the advisor finds these explanations, he presents them to the user as the response to his misconception. The response corrects the user&apos;s misconception by pointing out that the user&apos;s claimed precondition for removing a file is incorrect, by providing the actual precondition for removing a file, and by providing the missing user beliefs that led to the user&apos;s misconception. 2.1 OTHER WORK IN EXPLANATION-BASED UNDERSTANDING Our approach derives from work in explanation-based story understanding (Schank 1986, Dyer 1983, Wilensky 1983, 1978, Cullingford 1978, Schank and Abelson 1977). The basic idea is that to understand a particular input, such as a person&apos;s action, we have to explain why it has occurred. One way to find an explanation for a person&apos;s action is to relate it to known goals the person Computational Linguistics, Volume 14, Number 3, September 1988 39 Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptions is trying to achieve. Suppose, for example, that a story understander reads that a hungry character bought a restaurant guidebook (Wilensky 1983). One </context>
</contexts>
<marker>Schank, 1986</marker>
<rawString>Schank, R.C. 1986 Explanation Patterns: Understanding Mechanically and Creatively, Lawrence Erlbaum, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stevens</author>
<author>A Collins</author>
<author>S E Goldin</author>
</authors>
<title>Misconceptions in students&apos; understanding. Intelligent Tutoring Systems,</title>
<date>1982</date>
<pages>13--24</pages>
<publisher>Academic Press,</publisher>
<location>London, England:</location>
<marker>Stevens, Collins, Goldin, 1982</marker>
<rawString>Stevens, A., Collins, A., and Goldin, S.E. 1982 Misconceptions in students&apos; understanding. Intelligent Tutoring Systems, Academic Press, London, England: 13-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R Turner</author>
<author>J F Reeves</author>
</authors>
<title>The Rhapsody User&apos;s Manual.</title>
<date>1987</date>
<tech>Technical Note UCLA-AI-86-10,</tech>
<institution>Artificial Intelligence Laboratory, University of California,</institution>
<location>Los Angeles, CA.</location>
<contexts>
<context position="56492" citStr="Turner and Reeves 1987" startWordPosition="9359" endWordPosition="9362">? UC: Rwho lists all users on the network, their tty, their login time, and their idle time. Expert: What does the &amp;quot;rwho&amp;quot; command do? UC: Rwho is like who, except rwho lists all users on the network. SC&apos;s user modeling component, SCUM (Nessen 1987), takes an approach similar to UC&apos;s, also using stereotypical information. These approaches are complementary to ours. 8 IMPLEMENTATION DETAILS The theory discussed in this paper is embodied in AQUA, a computer program currently under development at UCLA. The current version of AQUA is implemented in T (Rees, Adams, and Meehan 1984), using RHAPSODY (Turner and Reeves 1987), a graphical Al tools environment with Prolog-like unification and backtracking capabilities, and runs on an Apollo DN460 workstation. Given a set of user beliefs involving plan applicability conditions, preconditions, or effects, AQUA determines which of these user beliefs are incorrect and what missing or mistaken user beliefs are likely to have led to them, and then produces a set of advisor beliefs that capture the content of the advisor&apos;s response. AQUA&apos;s domain of expertise is in the basic plans used to manipulate and access files, directories, and electronic mail. It has been used to d</context>
</contexts>
<marker>Turner, Reeves, 1987</marker>
<rawString>Turner, S.R. and Reeves, J.F. 1987 The Rhapsody User&apos;s Manual. Technical Note UCLA-AI-86-10, Artificial Intelligence Laboratory, University of California, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Wilensky</author>
<author>J Mayfield</author>
<author>A Albert</author>
<author>D Chin</author>
<author>C Cox</author>
<author>M Luria</author>
<author>J Martin</author>
<author>D Wu</author>
</authors>
<title>UC: A Progress Report.</title>
<date>1986</date>
<tech>Technical Report UCB/CSB 87/303,</tech>
<institution>Computer Science Division (EECS), University of California,</institution>
<location>Berkeley, CA.</location>
<contexts>
<context position="55304" citStr="Wilensky et al. 1986" startWordPosition="9164" endWordPosition="9167">ed conditions. SPIRIT has a task similar to ours but takes a very different approach, trying to determine the cause of the user&apos;s error through reasoning from first principles rather than memory search. In addition, SPIRIT cannot detect or respond to mistakes involving plan applicability conditions or preconditions. Finally, SPIRIT does not specify how knowledge of the cause of the user&apos;s mistaken belief affects the information to be included in a cooperative response, something that falls naturally out of our model. 7.3 UNIX ADVISORS Finally, there are two other related research efforts, UC (Wilensky et al. 1986, Wilensky, Arens, and Chin 1984) and SC (Kemke 1986), that address providing advice to novice UNIX users. Neither system, however, detects or responds to misconceptions. Instead, both are concerned with tailoring a response to a question to reflect the user&apos;s level of expertise. UC&apos;s user modeling component, KNOME (Chin 1986), analyzes a user&apos;s questions to determine which stereotypical class the user belongs to and then uses this information to provide more details and possibly more examples to less experienced users. Novice: What does the &amp;quot;rwho&amp;quot; command do? UC: Rwho lists all users on the n</context>
<context position="60346" citStr="Wilensky et al. 1986" startWordPosition="10004" endWordPosition="10007">to infer this user belief from the knowledge that the user did some sort of verify-action, the knowledge that one plan for dealing with a plan failure is to try to verify that the enablements of the plan have been achieved, and the knowledge that both owning the file and having write permission are different instantiations of having sufficient permission. Inferring beliefs like these, that involve the user&apos;s plans and goals and the relationships between, even when they differ from the advisor&apos;s, is currently an active area of research (Carberry, this issue; Kautz and Allen 1986, Goodman 1986, Wilensky et al. 1986, Quilici 1985). 9.2 RETRIEVING ADVISOR BELIEFS Our potential explanations suggest patterns of beliefs that the advisor should search for. However, we have not specified how this search of the advisor&apos;s memory is actually carried out, how a belief in memory can be retrieved efficiently, or how the beliefs are actually acquired through experience. AQUA&apos;s organization of plan-oriented beliefs is discussed in Quilici (1988, 1985). It is based on earlier work (Kolodner 1985, Schank 1982) in taking experiences and indexing them appropriately for efficient search and retrieval, especially that invol</context>
</contexts>
<marker>Wilensky, Mayfield, Albert, Chin, Cox, Luria, Martin, Wu, 1986</marker>
<rawString>Wilensky, R., Mayfield, J., Albert, A., Chin, D., Cox, C., Luria, M., Martin, J., and Wu, D. 1986 UC: A Progress Report. Technical Report UCB/CSB 87/303, Computer Science Division (EECS), University of California, Berkeley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Wilensky</author>
<author>Y Arens</author>
<author>D Chin</author>
</authors>
<title>Talking to UNIX in English: An Overview of UC,</title>
<date>1984</date>
<journal>Communications of the ACM:</journal>
<pages>574--593</pages>
<marker>Wilensky, Arens, Chin, 1984</marker>
<rawString>Wilensky, R., Arens, Y., and Chin, D. 1984 Talking to UNIX in English: An Overview of UC, Communications of the ACM: 574-593.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Wilensky</author>
</authors>
<title>Planning and Understanding,</title>
<date>1983</date>
<publisher>Addison Wesley,</publisher>
<location>Reading, MA.</location>
<contexts>
<context position="9300" citStr="Wilensky 1983" startWordPosition="1549" endWordPosition="1550">ing a file, but a precondition for achieving one of its preconditions. Once the advisor finds these explanations, he presents them to the user as the response to his misconception. The response corrects the user&apos;s misconception by pointing out that the user&apos;s claimed precondition for removing a file is incorrect, by providing the actual precondition for removing a file, and by providing the missing user beliefs that led to the user&apos;s misconception. 2.1 OTHER WORK IN EXPLANATION-BASED UNDERSTANDING Our approach derives from work in explanation-based story understanding (Schank 1986, Dyer 1983, Wilensky 1983, 1978, Cullingford 1978, Schank and Abelson 1977). The basic idea is that to understand a particular input, such as a person&apos;s action, we have to explain why it has occurred. One way to find an explanation for a person&apos;s action is to relate it to known goals the person Computational Linguistics, Volume 14, Number 3, September 1988 39 Quilici, Dyer, and Flowers Recognizing and Responding to Plan-Oriented Misconceptions is trying to achieve. Suppose, for example, that a story understander reads that a hungry character bought a restaurant guidebook (Wilensky 1983). One explanation for this actio</context>
</contexts>
<marker>Wilensky, 1983</marker>
<rawString>Wilensky, R. 1983 Planning and Understanding, Addison Wesley, Reading, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Wilensky</author>
</authors>
<title>Understanding Goal-Based Stories.</title>
<date>1978</date>
<tech>Ph.D. thesis. Technical Report 140,</tech>
<institution>Yale University,</institution>
<location>New Haven, CT.</location>
<marker>Wilensky, 1978</marker>
<rawString>Wilensky, R. 1978 Understanding Goal-Based Stories. Ph.D. thesis. Technical Report 140, Yale University, New Haven, CT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Computational Linguistics</author>
</authors>
<date>1988</date>
<volume>14</volume>
<pages>51</pages>
<marker>Linguistics, 1988</marker>
<rawString>Computational Linguistics, Volume 14, Number 3, September 1988 51</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>