<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.025076">
<title confidence="0.990068">
Feature-based Pronunciation Modeling for Speech Recognition
</title>
<author confidence="0.855455">
Karen Livescu and James Glass
</author>
<affiliation confidence="0.568814">
MIT Computer Science and Artificial Intelligence Laboratory
</affiliation>
<address confidence="0.676061">
Cambridge, MA 02139, USA
</address>
<email confidence="0.99852">
{klivescu, glass}@csail.mit.edu
</email>
<sectionHeader confidence="0.99563" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999683647058824">
We present an approach to pronunciation mod-
eling in which the evolution of multiple lin-
guistic feature streams is explicitly represented.
This differs from phone-based models in that
pronunciation variation is viewed as the result
of feature asynchrony and changes in feature
values, rather than phone substitutions, inser-
tions, and deletions. We have implemented a
flexible feature-based pronunciation model us-
ing dynamic Bayesian networks. In this paper,
we describe our approach and report on a pilot
experiment using phonetic transcriptions of ut-
terances from the Switchboard corpus. The ex-
perimental results, as well as the model’s quali-
tative behavior, suggest that this is a promising
way of accounting for the types of pronuncia-
tion variation often seen in spontaneous speech.
</bodyText>
<sectionHeader confidence="0.999117" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999706952380953">
Pronunciation variation in spontaneous speech has been
cited as a serious obstacle for automatic speech recog-
nition (McAllester et al., 1998). Typical pronunciation
models approach this problem by augmenting a phone-
mic dictionary with additional pronunciations, often re-
sulting from the application of phone substitution, inser-
tion, and deletion rules. By carefully constructing a rule
set (Hazen et al., 2002), or by deriving rules or variants
from data (Riley and Ljolje, 1996), many phenomena can
be accounted for. However, the recognition improvement
over a phonemic dictionary is typically modest, and some
types of variation remain awkward to represent.
These observations have motivated approaches to
speech recognition based on multiple streams of linguis-
tic features rather than a single stream of phones (e.g.,
King et al. (1998); Metze and Waibel (2002); Livescu et
al. (2003)). Most of this work, however, has focused on
acoustic modeling, i.e. the mapping between the features
and acoustic observations. The pronunciation model is
typically still phone-based, limiting the feature values to
the target configurations of phones and forcing them to
behave as a synchronous “bundle”. Some approaches
have begun to relax these constraints. For example, Deng
et al. (1997) and Richardson et al. (2000) model asyn-
chronous feature trajectories using hidden Markov mod-
els (HMMs), with each state corresponding to a vector
of feature values. This approach is powerful, but it can-
not represent independencies between features. Kirch-
hoff (1996), in contrast, models the feature streams as
independent, except for a requirement that they synchro-
nize at syllable boundaries. As pointed out by Osten-
dorf (2000), such independence assumptions may allow
for too much variability.
In this paper, we propose a more general feature-
based pronunciation model implemented using dynamic
Bayesian networks (Dean and Kanazawa, 1989), which
allow us to take advantage of inter-feature independen-
cies while avoiding overly strong independence assump-
tions. In the following sections, we describe the model
and present proof-of-concept experiments using phonetic
transcriptions of utterances from the Switchboard conver-
sational speech corpus (Greenberg et al., 1996).
</bodyText>
<sectionHeader confidence="0.904718" genericHeader="method">
2 Serval [sic] examples
</sectionHeader>
<bodyText confidence="0.979281633333333">
To help ground the discussion, we first present several
examples of pronunciation variation. One common phe-
nomenon is the nasalization of vowels preceding nasal
consonants. This is a result of asynchrony: The velum is
lowered before the oral closure is made. In more extreme
cases, the nasal consonant is entirely absent, leaving only
a nasalized vowel, as in can’t —&gt; [ k ae n t ] 1. All of the
feature values are still correct, although phonetically, this
would be described as a deletion.
Another example, taken from the Switchboard corpus,
is several —&gt; [s eh r v ax l]. In this case, the tongue
and lips have desynchronized to the point that the tongue
&apos;Here and throughout, we use the ARPAbet phonetic symbol
set with additional diacritics, such as “ n” for nasalization.
retroflexion for [r] starts and ends before the lip narrow-
ing gesture for [v]. Again, all of the feature streams
are produced correctly, but there is an apparent exchange
of two phones, which cannot be represented via single-
phone confusions conditioned on phonemic context.
A final example from Switchboard is everybody —&gt; [eh
r uw ay]. It is difficult to imagine a set of phonetic trans-
formations that would predict this pronunciation without
allowing a host of other impossible pronunciations. How-
ever, when viewed in terms of features, the transforma-
tion from [eh v r iy bcl b ah dx iy] to [eh r uw ay] is
fairly simple. The tongue and lips desynchronize, caus-
ing the lips to start to close for the [bcl] during the previ-
ous vowel. In addition, the lip constrictions for [bcl] and
[v], and the tongue tip gesture for [dx], are reduced. We
will return to this example in the sections below.
</bodyText>
<sectionHeader confidence="0.997151" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.985686170731708">
A feature-based pronunciation model is one that explic-
itly models the evolution of multiple underlying linguis-
tic feature streams to predict the allowed realizations of
a word and their probabilities. Our approach begins with
the usual assumption that each word has one or more tar-
get phonemic pronunciations, or baseforms. Each base-
form is converted to a table of underlying feature values.
Table 1 shows what part of this table might look like for
the word everybody. The table may include “unspecified”
values (‘*’ in the table). More generally, each table en-
try can be a distribution over the range of feature values.
For now, we assume that all of the features go through the
same sequence of indices (and therefore the same number
of targets) in a given word; e.g., in Table 1, LIP-OPEN
goes through the same indices as TT-LOC, although it
has the same target value for indices 2 and 3. In the first
time frame of speech, all of the features begin in index
0; in subsequent frames, each feature can either stay in
the same index or transition to the next one with some
probability.
The surface feature values—i.e., the ones that are ac-
tually produced by the speaker—can stray from the un-
derlying pronunciation in two ways, typically because of
articulatory inertia: substitution, in which a feature fails
to reach its target underlying value; and asynchrony, in
which different features proceed through their sequences
of indices at different rates. We define the degree of asyn-
chrony between two sets of features as the difference be-
tween the average index of one set relative to the average
index of the second. The degree of asynchrony is con-
strained: More “synchronous” configurations are more
probable (soft constraints), and we make the further sim-
plifying assumption that there is an upper bound on the
degree of asynchrony (hard constraints).
A natural framework for such a model is provided by
dynamic Bayesian networks (DBNs), because of their
index 0 1 2 3 ...
phoneme eh v r iy ...
LIP-OPEN wide critical wide wide ...
TT-LOC alv. * ret. alv. ...
... ... ... ... ... ...
</bodyText>
<tableCaption confidence="0.746135">
Table 1: Part of a target pronunciation for everybody.
</tableCaption>
<figureCaption confidence="0.895007555555556">
In this feature set, LIP-OPEN is the lip opening degree;
TT-LOC is the location along the palate to which the
tongue tip is closest (alv. = alveolar; ret. = retroflex).
Figure 1: One frame of a DBN for recognition with
a feature-based pronunciation model. Nodes represent
variables; shaded nodes are observed. Edges repre-
sent dependencies between variables. Edges withoutpar-
ents/children point from/to variables in adjacent frames
(see text).
</figureCaption>
<bodyText confidence="0.999705217391304">
ability to efficiently implement factored state representa-
tions. Figure 1 shows one frame of the type of DBN used
in our model (simplified somewhat for clarity of presenta-
tion). This example DBN assumes a feature set with three
features. The variables at time frame t are as follows:
lexEntryt – entry in the lexicon corresponding to the cur-
rent word and baseform. Words with multiple base-
forms have one entry per baseform. lexEntryt’s par-
ents are lexEntryt_1 and wdTrt_1
indjt – index of feature j into the underlying pronun-
ciation, as in Table 1. indj0 = 0; in subsequent
frames indjt is conditioned on lexEntryt_1, indjt_1,
and wdTrt_1 (defined below).
Ujt – underlying value of feature j. Its distribution
p(Ujt |lexEntryt, indjt) is determined by the target
feature table of lexEntryt.
Sjt – observed surface value of feature j. p(Sjt |Ujt ) en-
codes allowed feature substitutions.
wdTrt – binary variable indicating whether this is the last
frame of the current word.
syncA;B t – binary variable that enforces a synchrony con-
straint between subsets A and B of the feature set.
It is observed with value 1; its distribution is con-
</bodyText>
<equation confidence="0.996002875">
1t 2 3
ind ind ind
t t
U 1 U 2
t U 3
t
S1 2 3
t St St
1,2;3
sync =1
1;2 sync =1
t t
lexEntryt
t
wdTr
t
</equation>
<bodyText confidence="0.9998485">
structed in such a way as to force its parent ind vari-
ables to obey the desired constraint. For example,
to enforce a constraint between the average index of
features 1 and 2 and the index of feature 3, we would
</bodyText>
<equation confidence="0.941758666666667">
have P(sync1,2;3
t = 1|ind1t , ind2t , ind3t) = 0 when-
ever ind1t, ind2t, indt3 violate the constraint.
</equation>
<bodyText confidence="0.9997828">
In an end-to-end recognizer, the acoustic observations
would depend on the Sit, which would be unobserved.
However, to facilitate quick experimentation and isolate
the pronunciation model, we begin by testing how well
we can do when given observed surface feature values.
</bodyText>
<sectionHeader confidence="0.999583" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.994332073529412">
We have performed a pilot experiment using the follow-
ing feature set, based on the vocal tract variables of artic-
ulatory phonology (Browman and Goldstein, 1992): de-
gree of lip opening; tongue tip location and opening de-
gree; tongue body location and opening degree; velum
state; and glottal (voicing) state. We imposed the follow-
ing synchrony constraints: (1) All four tongue features
are completely synchronized; (2) the lips can desynchro-
nize from the tongue by up to one index; and (3) the glot-
tis and velum are synchronized, and their index must be
within 2 of the mean index of the tongue and lips.
We used the Graphical Models Toolkit (Bilmes and
Zweig, 2002) to implement the model. The distri-
butions p(Sit |Uit ) were constructed by hand based on
linguistic considerations, e.g. that features tend to go
from more “constricted” values to less constricted ones,
but not vice versa. p(Uit |lexEntryt, indit) was de-
rived from manually-constructed phoneme-to-feature-
probability mappings. For these experiments, no param-
eter learning has been done.
The task was to recognize an isolated word, given a set
of observed surface feature sequences Sit. To create the
observations, we used the detailed phonetic transcriptions
created at ICSI for the Switchboard corpus (Greenberg et
al., 1996). For each word, we converted its transcrip-
tion to a sequence of feature vectors, one vector per 10
ms frame. For this purpose, we divided diphthongs and
stops into pairs of feature configurations. Given the input
feature sequences, we computed a Viterbi score for each
lexical entry in a 3000+-word (5500+-lexEntry) vocabu-
lary, by “observing” the lexEntry variable and finding
the most likely settings of all remaining variables. The
most likely variable settings can be thought of as a mul-
tistream alignment between the surface and underlying
feature streams. Finally, we output the word correspond-
ing to the highest-scoring lexical entry.
We performed this procedure on a development set of
165 word transcriptions, which was used to tune settings
such as synchronization constraints, and a test set of 236
transcriptions 2. We compared the performance of sev-
eral models, measured in terms of word error rate (WER)
and failure rate (FR), the percentage of inputs that had no
Viterbi alignment with the correct word. To get a sense of
the effect of feature asynchrony, we compared our asyn-
chronous model with a version in which all features are
forced to be synchronized, so that only feature substitu-
tion is allowed. This uses the same DBN, but with de-
generate distributions for the synchronization variables.
Also, since the Si values are derived from phonetic tran-
scriptions, and are therefore constant over several frames
at a time, we also built a variant of the DBN in which Si
is allowed to change value with non-zero probability only
when indi changes (by adding parents indit, indit−1, Sit−1
to Sit); we refer to this DBN as “segment-based”, and to
the original as “frame-based”. We compared four vari-
ants, differing along the “synchronous vs. asynchronous”
and “frame-based vs. segment-based” dimensions. The
variant which is both synchronous and segment-based is
similar to a phone-based pronunciation model with only
context-independent phone substitutions.
model dev set test set
WER FR WER FR
baseforms only 63.6 61.2 69.5 66.9
phonological rules 50.3 47.9 59.7 55.5
sync. seg.-based 38.2 24.8 43.2 35.2
sync. fr.-based 35.2 23.0 46.2 31.4
async. seg.-based 32.7 19.4 41.1 31.4
async. fr.-based 29.7 16.4 42.7 26.3
</bodyText>
<tableCaption confidence="0.972038">
Table 2: Results of Switchboard ranking experiment.
</tableCaption>
<bodyText confidence="0.999293578947368">
Table 2 shows the performance of these four models,
as well as of two “baseline” models: one allowing only
the baseform pronunciations (on average 1.7 per word),
and another including all pronunciations produced by
an extensive set of context-dependent phonological rules
(about 4 per word), with no feature substitutions or asyn-
chrony in either case. The phonological rules are the “full
rule set” described in Hazen et al. (2002). We note that
they were not designed with Switchboard in mind.
The models that allow asynchrony outperform the ones
that do not, in terms of both WER and FR. Looking more
closely at the performance on the development set, the
inputs on which the synchronous models failed but the
asynchronous models succeeded were in fact the kinds of
pronunciations that we expect to arise from feature asyn-
chrony, including: nasals replaced by nasalization on a
preceding vowel; a /t r/ sequence realized as /ch/; and
everybody —&gt; [eh r uw ay]. The relative merits of the
frame-based and segment-based models is less clear, as
</bodyText>
<footnote confidence="0.701612">
2We required that words in the development and test sets
have phonemic pronunciations with at least 4 phonemes, so as
to limit context effects from adjacent words.
</footnote>
<bodyText confidence="0.9996535">
they have opposite relative performance on the develop-
ment and test sets. For 27 (16.4%) development utter-
ances, none of the models was able to find an alignment
with the correct word. Most of these were due to apparent
gesture deletions and context-dependent feature changes,
which are not yet included in the model.
Figure 2 shows a part of the Viterbi alignment of ev-
erybody with [eh r uw ay], produced by the segment-
based, asynchronous model. Using this model, everybody
was the top-ranked word. As expected, the asynchrony is
manifested in the [uw] region, and the lips do not close
but reach only a narrow (glide-like) configuration.
</bodyText>
<figureCaption confidence="0.90184">
Figure 2: Spectrogram, transcription, and partial Viterbi
alignment, including the lip opening and tongue tip loca-
tion variables. Indices are relative to the underlying pro-
</figureCaption>
<bodyText confidence="0.99217875">
nunciation /eh v r iy bcl b ah dx iy/. Adjacentframes with
equal values have been mergedfor easier viewing. WI =
wide; NA = narrow; CR = critical; CL = closed; ALV
= alveolar; P-A = palato-alveolar; RET = retroflex.
</bodyText>
<sectionHeader confidence="0.999675" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999985366666667">
We have motivated our pronunciation model as part of
an overall strategy of feature-based speech recognition.
One way in which this model could fit into a complete
recognizer is, as mentioned above, by adding a variable
A representing the acoustic observations, with the SJ as
its parents. The modeling of p(A|S1, ... , SM) (where M
is the number of features) is a significant problem in its
own right. Alternatively, as this study suggests, there may
be some benefit to this type of model even if the acoustic
model is phone-based. One possible setup would be to
use a phonetic recognizer to produce a phone lattice, then
convert the phones into features and proceed as in our
Switchboard experiments.
Thus far we have not trained the variable distribu-
tions. With the exception of the sync variables, these
can be trained from feature transcriptions (i.e. SJ obser-
vations) using the Expectation-Maximization (EM) algo-
rithm (Dempster et al., 1977). In the absence of actual
feature transcriptions, they can be approximated by con-
verting detailed phonetic transcriptions, as we have done
in our decoding experiments above. The sync distribu-
tions cannot be trained via EM, since they are always
observed with value 1. They can either be treated as ex-
perimental parameters or trained discriminatively. We are
currently working on a new formulation in which the syn-
chronization constraints can be trained via EM.
In addition, we are currently investigating extensions
to the model, including context-dependent feature substi-
tutions. We also plan to extend this study to a larger data
set and to multi-word utterances.
</bodyText>
<sectionHeader confidence="0.99919" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999619204545455">
J. Bilmes and G. Zweig, “The Graphical Models Toolkit: An
open source software system for speech and time-series pro-
cessing,” ICASSP, Orlando, 2002.
C. P. Browman and L. Goldstein, “Articulatory phonology: An
overview,” Phonetica, 49:155–180, 1992.
T. Dean and K. Kanazawa, “A model for reasoning about per-
sistence and causation,” Computational Intelligence, 5:142–
150, 1989.
A. P. Dempster, N. M. Laird, and D. B. Rubin, “Maximum Like-
lihood from Incomplete Data via the EM Algorithm,” Jour-
nal of the Royal Statistical Society, 39:1–38,1977.
L. Deng, G. Ramsay, and D. Sun, “Production models as a struc-
tural basis for automatic speech recognition,” Speech Com-
munication, 33:93–111, 1997.
S. Greenberg, J. Hollenback, and D. Ellis, “Insights into spoken
language gleaned from phonetic transcription of the Switch-
board corpus,” ICSLP, Philadelphia, 1996.
T. J. Hazen, I. L. Hetherington, H. Shu, and K. Livescu, “Pro-
nunciation modeling using a finite-state transducer represen-
tation,” ITRW PMLA, Estes Park, CO, 2002.
S. King, T. Stephenson, S. Isard, P. Taylor, and A. Strachan,
“Speech recognition via phonetically featured syllables,” IC-
SLP, Sydney, 1998.
K. Kirchhoff, “Syllable-level desynchronisation of phonetic
features for speech recognition,” ICSLP, Philadelphia, 1996.
K. Livescu, J. Glass, and J. Bilmes, “Hidden feature models
for speech recognition using dynamic Bayesian networks,”
Eurospeech, Geneva, 2003.
D. McAllester, L. Gillick, F. Scattone, and M. Newman, “Fab-
ricating conversational speech data with acoustic models: A
program to examine model-data mismatch,” ICSLP, Sydney,
1998.
F. Metze and A. Waibel, “A flexible stream architecture for ASR
using articulatory features,” ICSLP, Denver, 2002.
M. Ostendorf, “Incorporating linguistic theories of pronuncia-
tion variation into speech-recognition models,” Phil. Trans.
R. Soc. Lond. A, 358:1325-1338, 2000.
M. Richardson, J. Bilmes, and C. Diorio, “Hidden-articulator
Markov models for speech recognition,” ITRW ASR2000,
Paris, 2000.
M. D. Riley and A. Ljolje, “Automatic generation of detailed
pronunciation lexicons,” in C.-H. Lee, F. K. Soong, and K.
K. Paliwal (eds.), Automatic Speech and Speaker Recogni-
tion, Kluwer Academic Publishers, Boston, 1996.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.956551">
<title confidence="0.999388">Feature-based Pronunciation Modeling for Speech Recognition</title>
<author confidence="0.995582">Karen Livescu</author>
<author confidence="0.995582">James</author>
<affiliation confidence="0.999579">MIT Computer Science and Artificial Intelligence</affiliation>
<address confidence="0.999907">Cambridge, MA 02139,</address>
<abstract confidence="0.997868944444444">We present an approach to pronunciation modeling in which the evolution of multiple linguistic feature streams is explicitly represented. This differs from phone-based models in that pronunciation variation is viewed as the result of feature asynchrony and changes in feature values, rather than phone substitutions, insertions, and deletions. We have implemented a flexible feature-based pronunciation model using dynamic Bayesian networks. In this paper, we describe our approach and report on a pilot experiment using phonetic transcriptions of utterances from the Switchboard corpus. The experimental results, as well as the model’s qualitative behavior, suggest that this is a promising way of accounting for the types of pronunciation variation often seen in spontaneous speech.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Bilmes</author>
<author>G Zweig</author>
</authors>
<title>The Graphical Models Toolkit: An open source software system for speech and time-series processing,” ICASSP,</title>
<date>2002</date>
<location>Orlando,</location>
<contexts>
<context position="10064" citStr="Bilmes and Zweig, 2002" startWordPosition="1655" endWordPosition="1658"> the following feature set, based on the vocal tract variables of articulatory phonology (Browman and Goldstein, 1992): degree of lip opening; tongue tip location and opening degree; tongue body location and opening degree; velum state; and glottal (voicing) state. We imposed the following synchrony constraints: (1) All four tongue features are completely synchronized; (2) the lips can desynchronize from the tongue by up to one index; and (3) the glottis and velum are synchronized, and their index must be within 2 of the mean index of the tongue and lips. We used the Graphical Models Toolkit (Bilmes and Zweig, 2002) to implement the model. The distributions p(Sit |Uit ) were constructed by hand based on linguistic considerations, e.g. that features tend to go from more “constricted” values to less constricted ones, but not vice versa. p(Uit |lexEntryt, indit) was derived from manually-constructed phoneme-to-featureprobability mappings. For these experiments, no parameter learning has been done. The task was to recognize an isolated word, given a set of observed surface feature sequences Sit. To create the observations, we used the detailed phonetic transcriptions created at ICSI for the Switchboard corpu</context>
</contexts>
<marker>Bilmes, Zweig, 2002</marker>
<rawString>J. Bilmes and G. Zweig, “The Graphical Models Toolkit: An open source software system for speech and time-series processing,” ICASSP, Orlando, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C P Browman</author>
<author>L Goldstein</author>
</authors>
<title>Articulatory phonology: An overview,”</title>
<date>1992</date>
<journal>Phonetica,</journal>
<volume>49</volume>
<contexts>
<context position="9559" citStr="Browman and Goldstein, 1992" startWordPosition="1567" endWordPosition="1570">een the average index of features 1 and 2 and the index of feature 3, we would have P(sync1,2;3 t = 1|ind1t , ind2t , ind3t) = 0 whenever ind1t, ind2t, indt3 violate the constraint. In an end-to-end recognizer, the acoustic observations would depend on the Sit, which would be unobserved. However, to facilitate quick experimentation and isolate the pronunciation model, we begin by testing how well we can do when given observed surface feature values. 4 Experiments We have performed a pilot experiment using the following feature set, based on the vocal tract variables of articulatory phonology (Browman and Goldstein, 1992): degree of lip opening; tongue tip location and opening degree; tongue body location and opening degree; velum state; and glottal (voicing) state. We imposed the following synchrony constraints: (1) All four tongue features are completely synchronized; (2) the lips can desynchronize from the tongue by up to one index; and (3) the glottis and velum are synchronized, and their index must be within 2 of the mean index of the tongue and lips. We used the Graphical Models Toolkit (Bilmes and Zweig, 2002) to implement the model. The distributions p(Sit |Uit ) were constructed by hand based on lingu</context>
</contexts>
<marker>Browman, Goldstein, 1992</marker>
<rawString>C. P. Browman and L. Goldstein, “Articulatory phonology: An overview,” Phonetica, 49:155–180, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dean</author>
<author>K Kanazawa</author>
</authors>
<title>A model for reasoning about persistence and causation,”</title>
<date>1989</date>
<journal>Computational Intelligence,</journal>
<volume>5</volume>
<pages>150</pages>
<contexts>
<context position="2936" citStr="Dean and Kanazawa, 1989" startWordPosition="433" endWordPosition="436">et al. (2000) model asynchronous feature trajectories using hidden Markov models (HMMs), with each state corresponding to a vector of feature values. This approach is powerful, but it cannot represent independencies between features. Kirchhoff (1996), in contrast, models the feature streams as independent, except for a requirement that they synchronize at syllable boundaries. As pointed out by Ostendorf (2000), such independence assumptions may allow for too much variability. In this paper, we propose a more general featurebased pronunciation model implemented using dynamic Bayesian networks (Dean and Kanazawa, 1989), which allow us to take advantage of inter-feature independencies while avoiding overly strong independence assumptions. In the following sections, we describe the model and present proof-of-concept experiments using phonetic transcriptions of utterances from the Switchboard conversational speech corpus (Greenberg et al., 1996). 2 Serval [sic] examples To help ground the discussion, we first present several examples of pronunciation variation. One common phenomenon is the nasalization of vowels preceding nasal consonants. This is a result of asynchrony: The velum is lowered before the oral cl</context>
</contexts>
<marker>Dean, Kanazawa, 1989</marker>
<rawString>T. Dean and K. Kanazawa, “A model for reasoning about persistence and causation,” Computational Intelligence, 5:142– 150, 1989.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum Likelihood from Incomplete Data via the EM Algorithm,”</title>
<journal>Journal of the Royal Statistical Society,</journal>
<pages>39--1</pages>
<marker>Dempster, Laird, Rubin, </marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin, “Maximum Likelihood from Incomplete Data via the EM Algorithm,” Journal of the Royal Statistical Society, 39:1–38,1977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Deng</author>
<author>G Ramsay</author>
<author>D Sun</author>
</authors>
<title>Production models as a structural basis for automatic speech recognition,”</title>
<date>1997</date>
<journal>Speech Communication,</journal>
<volume>33</volume>
<contexts>
<context position="2296" citStr="Deng et al. (1997)" startWordPosition="336" endWordPosition="339">ese observations have motivated approaches to speech recognition based on multiple streams of linguistic features rather than a single stream of phones (e.g., King et al. (1998); Metze and Waibel (2002); Livescu et al. (2003)). Most of this work, however, has focused on acoustic modeling, i.e. the mapping between the features and acoustic observations. The pronunciation model is typically still phone-based, limiting the feature values to the target configurations of phones and forcing them to behave as a synchronous “bundle”. Some approaches have begun to relax these constraints. For example, Deng et al. (1997) and Richardson et al. (2000) model asynchronous feature trajectories using hidden Markov models (HMMs), with each state corresponding to a vector of feature values. This approach is powerful, but it cannot represent independencies between features. Kirchhoff (1996), in contrast, models the feature streams as independent, except for a requirement that they synchronize at syllable boundaries. As pointed out by Ostendorf (2000), such independence assumptions may allow for too much variability. In this paper, we propose a more general featurebased pronunciation model implemented using dynamic Bay</context>
</contexts>
<marker>Deng, Ramsay, Sun, 1997</marker>
<rawString>L. Deng, G. Ramsay, and D. Sun, “Production models as a structural basis for automatic speech recognition,” Speech Communication, 33:93–111, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Greenberg</author>
<author>J Hollenback</author>
<author>D Ellis</author>
</authors>
<title>Insights into spoken language gleaned from phonetic transcription of the Switchboard corpus,” ICSLP,</title>
<date>1996</date>
<location>Philadelphia,</location>
<contexts>
<context position="3266" citStr="Greenberg et al., 1996" startWordPosition="478" endWordPosition="481">t that they synchronize at syllable boundaries. As pointed out by Ostendorf (2000), such independence assumptions may allow for too much variability. In this paper, we propose a more general featurebased pronunciation model implemented using dynamic Bayesian networks (Dean and Kanazawa, 1989), which allow us to take advantage of inter-feature independencies while avoiding overly strong independence assumptions. In the following sections, we describe the model and present proof-of-concept experiments using phonetic transcriptions of utterances from the Switchboard conversational speech corpus (Greenberg et al., 1996). 2 Serval [sic] examples To help ground the discussion, we first present several examples of pronunciation variation. One common phenomenon is the nasalization of vowels preceding nasal consonants. This is a result of asynchrony: The velum is lowered before the oral closure is made. In more extreme cases, the nasal consonant is entirely absent, leaving only a nasalized vowel, as in can’t —&gt; [ k ae n t ] 1. All of the feature values are still correct, although phonetically, this would be described as a deletion. Another example, taken from the Switchboard corpus, is several —&gt; [s eh r v ax l].</context>
<context position="10690" citStr="Greenberg et al., 1996" startWordPosition="1750" endWordPosition="1753"> implement the model. The distributions p(Sit |Uit ) were constructed by hand based on linguistic considerations, e.g. that features tend to go from more “constricted” values to less constricted ones, but not vice versa. p(Uit |lexEntryt, indit) was derived from manually-constructed phoneme-to-featureprobability mappings. For these experiments, no parameter learning has been done. The task was to recognize an isolated word, given a set of observed surface feature sequences Sit. To create the observations, we used the detailed phonetic transcriptions created at ICSI for the Switchboard corpus (Greenberg et al., 1996). For each word, we converted its transcription to a sequence of feature vectors, one vector per 10 ms frame. For this purpose, we divided diphthongs and stops into pairs of feature configurations. Given the input feature sequences, we computed a Viterbi score for each lexical entry in a 3000+-word (5500+-lexEntry) vocabulary, by “observing” the lexEntry variable and finding the most likely settings of all remaining variables. The most likely variable settings can be thought of as a multistream alignment between the surface and underlying feature streams. Finally, we output the word correspond</context>
</contexts>
<marker>Greenberg, Hollenback, Ellis, 1996</marker>
<rawString>S. Greenberg, J. Hollenback, and D. Ellis, “Insights into spoken language gleaned from phonetic transcription of the Switchboard corpus,” ICSLP, Philadelphia, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T J Hazen</author>
<author>I L Hetherington</author>
<author>H Shu</author>
<author>K Livescu</author>
</authors>
<title>Pronunciation modeling using a finite-state transducer representation,” ITRW PMLA,</title>
<date>2002</date>
<location>Estes Park, CO,</location>
<contexts>
<context position="1426" citStr="Hazen et al., 2002" startWordPosition="203" endWordPosition="206"> experimental results, as well as the model’s qualitative behavior, suggest that this is a promising way of accounting for the types of pronunciation variation often seen in spontaneous speech. 1 Introduction Pronunciation variation in spontaneous speech has been cited as a serious obstacle for automatic speech recognition (McAllester et al., 1998). Typical pronunciation models approach this problem by augmenting a phonemic dictionary with additional pronunciations, often resulting from the application of phone substitution, insertion, and deletion rules. By carefully constructing a rule set (Hazen et al., 2002), or by deriving rules or variants from data (Riley and Ljolje, 1996), many phenomena can be accounted for. However, the recognition improvement over a phonemic dictionary is typically modest, and some types of variation remain awkward to represent. These observations have motivated approaches to speech recognition based on multiple streams of linguistic features rather than a single stream of phones (e.g., King et al. (1998); Metze and Waibel (2002); Livescu et al. (2003)). Most of this work, however, has focused on acoustic modeling, i.e. the mapping between the features and acoustic observa</context>
<context position="13404" citStr="Hazen et al. (2002)" startWordPosition="2186" endWordPosition="2189">ased 38.2 24.8 43.2 35.2 sync. fr.-based 35.2 23.0 46.2 31.4 async. seg.-based 32.7 19.4 41.1 31.4 async. fr.-based 29.7 16.4 42.7 26.3 Table 2: Results of Switchboard ranking experiment. Table 2 shows the performance of these four models, as well as of two “baseline” models: one allowing only the baseform pronunciations (on average 1.7 per word), and another including all pronunciations produced by an extensive set of context-dependent phonological rules (about 4 per word), with no feature substitutions or asynchrony in either case. The phonological rules are the “full rule set” described in Hazen et al. (2002). We note that they were not designed with Switchboard in mind. The models that allow asynchrony outperform the ones that do not, in terms of both WER and FR. Looking more closely at the performance on the development set, the inputs on which the synchronous models failed but the asynchronous models succeeded were in fact the kinds of pronunciations that we expect to arise from feature asynchrony, including: nasals replaced by nasalization on a preceding vowel; a /t r/ sequence realized as /ch/; and everybody —&gt; [eh r uw ay]. The relative merits of the frame-based and segment-based models is l</context>
</contexts>
<marker>Hazen, Hetherington, Shu, Livescu, 2002</marker>
<rawString>T. J. Hazen, I. L. Hetherington, H. Shu, and K. Livescu, “Pronunciation modeling using a finite-state transducer representation,” ITRW PMLA, Estes Park, CO, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S King</author>
<author>T Stephenson</author>
<author>S Isard</author>
<author>P Taylor</author>
<author>A Strachan</author>
</authors>
<title>Speech recognition via phonetically featured syllables,” ICSLP,</title>
<date>1998</date>
<location>Sydney,</location>
<contexts>
<context position="1855" citStr="King et al. (1998)" startWordPosition="269" endWordPosition="272">ctionary with additional pronunciations, often resulting from the application of phone substitution, insertion, and deletion rules. By carefully constructing a rule set (Hazen et al., 2002), or by deriving rules or variants from data (Riley and Ljolje, 1996), many phenomena can be accounted for. However, the recognition improvement over a phonemic dictionary is typically modest, and some types of variation remain awkward to represent. These observations have motivated approaches to speech recognition based on multiple streams of linguistic features rather than a single stream of phones (e.g., King et al. (1998); Metze and Waibel (2002); Livescu et al. (2003)). Most of this work, however, has focused on acoustic modeling, i.e. the mapping between the features and acoustic observations. The pronunciation model is typically still phone-based, limiting the feature values to the target configurations of phones and forcing them to behave as a synchronous “bundle”. Some approaches have begun to relax these constraints. For example, Deng et al. (1997) and Richardson et al. (2000) model asynchronous feature trajectories using hidden Markov models (HMMs), with each state corresponding to a vector of feature v</context>
</contexts>
<marker>King, Stephenson, Isard, Taylor, Strachan, 1998</marker>
<rawString>S. King, T. Stephenson, S. Isard, P. Taylor, and A. Strachan, “Speech recognition via phonetically featured syllables,” ICSLP, Sydney, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kirchhoff</author>
</authors>
<title>Syllable-level desynchronisation of phonetic features for speech recognition,” ICSLP,</title>
<date>1996</date>
<location>Philadelphia,</location>
<contexts>
<context position="2562" citStr="Kirchhoff (1996)" startWordPosition="378" endWordPosition="380">on acoustic modeling, i.e. the mapping between the features and acoustic observations. The pronunciation model is typically still phone-based, limiting the feature values to the target configurations of phones and forcing them to behave as a synchronous “bundle”. Some approaches have begun to relax these constraints. For example, Deng et al. (1997) and Richardson et al. (2000) model asynchronous feature trajectories using hidden Markov models (HMMs), with each state corresponding to a vector of feature values. This approach is powerful, but it cannot represent independencies between features. Kirchhoff (1996), in contrast, models the feature streams as independent, except for a requirement that they synchronize at syllable boundaries. As pointed out by Ostendorf (2000), such independence assumptions may allow for too much variability. In this paper, we propose a more general featurebased pronunciation model implemented using dynamic Bayesian networks (Dean and Kanazawa, 1989), which allow us to take advantage of inter-feature independencies while avoiding overly strong independence assumptions. In the following sections, we describe the model and present proof-of-concept experiments using phonetic</context>
</contexts>
<marker>Kirchhoff, 1996</marker>
<rawString>K. Kirchhoff, “Syllable-level desynchronisation of phonetic features for speech recognition,” ICSLP, Philadelphia, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Livescu</author>
<author>J Glass</author>
<author>J Bilmes</author>
</authors>
<title>Hidden feature models for speech recognition using dynamic Bayesian networks,”</title>
<date>2003</date>
<location>Eurospeech, Geneva,</location>
<contexts>
<context position="1903" citStr="Livescu et al. (2003)" startWordPosition="277" endWordPosition="280">n resulting from the application of phone substitution, insertion, and deletion rules. By carefully constructing a rule set (Hazen et al., 2002), or by deriving rules or variants from data (Riley and Ljolje, 1996), many phenomena can be accounted for. However, the recognition improvement over a phonemic dictionary is typically modest, and some types of variation remain awkward to represent. These observations have motivated approaches to speech recognition based on multiple streams of linguistic features rather than a single stream of phones (e.g., King et al. (1998); Metze and Waibel (2002); Livescu et al. (2003)). Most of this work, however, has focused on acoustic modeling, i.e. the mapping between the features and acoustic observations. The pronunciation model is typically still phone-based, limiting the feature values to the target configurations of phones and forcing them to behave as a synchronous “bundle”. Some approaches have begun to relax these constraints. For example, Deng et al. (1997) and Richardson et al. (2000) model asynchronous feature trajectories using hidden Markov models (HMMs), with each state corresponding to a vector of feature values. This approach is powerful, but it cannot </context>
</contexts>
<marker>Livescu, Glass, Bilmes, 2003</marker>
<rawString>K. Livescu, J. Glass, and J. Bilmes, “Hidden feature models for speech recognition using dynamic Bayesian networks,” Eurospeech, Geneva, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McAllester</author>
<author>L Gillick</author>
<author>F Scattone</author>
<author>M Newman</author>
</authors>
<title>Fabricating conversational speech data with acoustic models: A program to examine model-data mismatch,” ICSLP,</title>
<date>1998</date>
<location>Sydney,</location>
<contexts>
<context position="1157" citStr="McAllester et al., 1998" startWordPosition="164" endWordPosition="167">insertions, and deletions. We have implemented a flexible feature-based pronunciation model using dynamic Bayesian networks. In this paper, we describe our approach and report on a pilot experiment using phonetic transcriptions of utterances from the Switchboard corpus. The experimental results, as well as the model’s qualitative behavior, suggest that this is a promising way of accounting for the types of pronunciation variation often seen in spontaneous speech. 1 Introduction Pronunciation variation in spontaneous speech has been cited as a serious obstacle for automatic speech recognition (McAllester et al., 1998). Typical pronunciation models approach this problem by augmenting a phonemic dictionary with additional pronunciations, often resulting from the application of phone substitution, insertion, and deletion rules. By carefully constructing a rule set (Hazen et al., 2002), or by deriving rules or variants from data (Riley and Ljolje, 1996), many phenomena can be accounted for. However, the recognition improvement over a phonemic dictionary is typically modest, and some types of variation remain awkward to represent. These observations have motivated approaches to speech recognition based on multi</context>
</contexts>
<marker>McAllester, Gillick, Scattone, Newman, 1998</marker>
<rawString>D. McAllester, L. Gillick, F. Scattone, and M. Newman, “Fabricating conversational speech data with acoustic models: A program to examine model-data mismatch,” ICSLP, Sydney, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Metze</author>
<author>A Waibel</author>
</authors>
<title>A flexible stream architecture for ASR using articulatory features,” ICSLP,</title>
<date>2002</date>
<location>Denver,</location>
<contexts>
<context position="1880" citStr="Metze and Waibel (2002)" startWordPosition="273" endWordPosition="276">onal pronunciations, often resulting from the application of phone substitution, insertion, and deletion rules. By carefully constructing a rule set (Hazen et al., 2002), or by deriving rules or variants from data (Riley and Ljolje, 1996), many phenomena can be accounted for. However, the recognition improvement over a phonemic dictionary is typically modest, and some types of variation remain awkward to represent. These observations have motivated approaches to speech recognition based on multiple streams of linguistic features rather than a single stream of phones (e.g., King et al. (1998); Metze and Waibel (2002); Livescu et al. (2003)). Most of this work, however, has focused on acoustic modeling, i.e. the mapping between the features and acoustic observations. The pronunciation model is typically still phone-based, limiting the feature values to the target configurations of phones and forcing them to behave as a synchronous “bundle”. Some approaches have begun to relax these constraints. For example, Deng et al. (1997) and Richardson et al. (2000) model asynchronous feature trajectories using hidden Markov models (HMMs), with each state corresponding to a vector of feature values. This approach is p</context>
</contexts>
<marker>Metze, Waibel, 2002</marker>
<rawString>F. Metze and A. Waibel, “A flexible stream architecture for ASR using articulatory features,” ICSLP, Denver, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ostendorf</author>
</authors>
<title>Incorporating linguistic theories of pronunciation variation into speech-recognition models,”</title>
<date>2000</date>
<journal>Phil. Trans. R. Soc. Lond. A,</journal>
<pages>358--1325</pages>
<contexts>
<context position="2725" citStr="Ostendorf (2000)" startWordPosition="404" endWordPosition="406"> values to the target configurations of phones and forcing them to behave as a synchronous “bundle”. Some approaches have begun to relax these constraints. For example, Deng et al. (1997) and Richardson et al. (2000) model asynchronous feature trajectories using hidden Markov models (HMMs), with each state corresponding to a vector of feature values. This approach is powerful, but it cannot represent independencies between features. Kirchhoff (1996), in contrast, models the feature streams as independent, except for a requirement that they synchronize at syllable boundaries. As pointed out by Ostendorf (2000), such independence assumptions may allow for too much variability. In this paper, we propose a more general featurebased pronunciation model implemented using dynamic Bayesian networks (Dean and Kanazawa, 1989), which allow us to take advantage of inter-feature independencies while avoiding overly strong independence assumptions. In the following sections, we describe the model and present proof-of-concept experiments using phonetic transcriptions of utterances from the Switchboard conversational speech corpus (Greenberg et al., 1996). 2 Serval [sic] examples To help ground the discussion, we</context>
</contexts>
<marker>Ostendorf, 2000</marker>
<rawString>M. Ostendorf, “Incorporating linguistic theories of pronunciation variation into speech-recognition models,” Phil. Trans. R. Soc. Lond. A, 358:1325-1338, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Richardson</author>
<author>J Bilmes</author>
<author>C Diorio</author>
</authors>
<title>Hidden-articulator Markov models for speech recognition,” ITRW ASR2000,</title>
<date>2000</date>
<location>Paris,</location>
<contexts>
<context position="2325" citStr="Richardson et al. (2000)" startWordPosition="341" endWordPosition="344">otivated approaches to speech recognition based on multiple streams of linguistic features rather than a single stream of phones (e.g., King et al. (1998); Metze and Waibel (2002); Livescu et al. (2003)). Most of this work, however, has focused on acoustic modeling, i.e. the mapping between the features and acoustic observations. The pronunciation model is typically still phone-based, limiting the feature values to the target configurations of phones and forcing them to behave as a synchronous “bundle”. Some approaches have begun to relax these constraints. For example, Deng et al. (1997) and Richardson et al. (2000) model asynchronous feature trajectories using hidden Markov models (HMMs), with each state corresponding to a vector of feature values. This approach is powerful, but it cannot represent independencies between features. Kirchhoff (1996), in contrast, models the feature streams as independent, except for a requirement that they synchronize at syllable boundaries. As pointed out by Ostendorf (2000), such independence assumptions may allow for too much variability. In this paper, we propose a more general featurebased pronunciation model implemented using dynamic Bayesian networks (Dean and Kana</context>
</contexts>
<marker>Richardson, Bilmes, Diorio, 2000</marker>
<rawString>M. Richardson, J. Bilmes, and C. Diorio, “Hidden-articulator Markov models for speech recognition,” ITRW ASR2000, Paris, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M D Riley</author>
<author>A Ljolje</author>
</authors>
<title>Automatic generation of detailed pronunciation lexicons,”</title>
<date>1996</date>
<booktitle>Automatic Speech and Speaker Recognition,</booktitle>
<editor>in C.-H. Lee, F. K. Soong, and K. K. Paliwal (eds.),</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Boston,</location>
<contexts>
<context position="1495" citStr="Riley and Ljolje, 1996" startWordPosition="215" endWordPosition="218">r, suggest that this is a promising way of accounting for the types of pronunciation variation often seen in spontaneous speech. 1 Introduction Pronunciation variation in spontaneous speech has been cited as a serious obstacle for automatic speech recognition (McAllester et al., 1998). Typical pronunciation models approach this problem by augmenting a phonemic dictionary with additional pronunciations, often resulting from the application of phone substitution, insertion, and deletion rules. By carefully constructing a rule set (Hazen et al., 2002), or by deriving rules or variants from data (Riley and Ljolje, 1996), many phenomena can be accounted for. However, the recognition improvement over a phonemic dictionary is typically modest, and some types of variation remain awkward to represent. These observations have motivated approaches to speech recognition based on multiple streams of linguistic features rather than a single stream of phones (e.g., King et al. (1998); Metze and Waibel (2002); Livescu et al. (2003)). Most of this work, however, has focused on acoustic modeling, i.e. the mapping between the features and acoustic observations. The pronunciation model is typically still phone-based, limiti</context>
</contexts>
<marker>Riley, Ljolje, 1996</marker>
<rawString>M. D. Riley and A. Ljolje, “Automatic generation of detailed pronunciation lexicons,” in C.-H. Lee, F. K. Soong, and K. K. Paliwal (eds.), Automatic Speech and Speaker Recognition, Kluwer Academic Publishers, Boston, 1996.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>