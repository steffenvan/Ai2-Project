<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000008">
<author confidence="0.212436">
Book Reviews Generating Natural Language under Pragmatic Constraints
</author>
<bodyText confidence="0.99985735483871">
the book more of a recommended reading is the renewed
empiricism in the field, largely promoted by the very practi-
cal need to scale up natural language systems, and largely
due to the realization that linguistic information about
words could be derived from massive on-line text resources.
Whether these resources come in the shape of on-line
dictionaries or text corpora is immaterial here. By reading
Looking Up, one becomes acutely aware of the richness of
lexical information available in (and distributed over) mil-
lions of words of text. One also understands that careful
inspection of a dictionary entry (or a set of related entries)
is likely to reveal considerably more in terms of lexical
properties of the word (or class of words) than is apparently
visible. The nature of lexical information in, and its extract-
ability from, such text resources has been much discussed
recently in the computational linguistics and computa-
tional lexicography literature; in particular, the statement
that there is a wealth of implicit information available in
on-line dictionaries and corpora has been made over and
over again recently (see, for instance, Atkins et al. 1988;
Boguraev and Briscoe 1989; Hindle 1989; Church and
Hanks 1989). However, there is a world of difference
between &amp;quot;retro-engineering,&amp;quot; by whatever means, methods
and rules for inferring lexical information from dictionary
entries, and being told in advance the kinds of lexical
regularities, generalizations, and properties encoded in these
entries. For that reason alone, and particularly given that
the COBUILD dictionary is available in machine-readable
form, Looking Up is a book that should not be ignored by
researchers interested in computational lexicography, lexi-
cal semantics, or simply the nature of word meaning.
</bodyText>
<sectionHeader confidence="0.993248" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.509615692307692">
Atkins, B. T., Kegl, J. and Levin, B. 1988 Anatomy of a Verb Entry.
International Journal of Lexicographyl(2):84-126.
Boguraev, B. K., and Briscoe, E. J. 1989 Computational Lexicography for
Natural Language Processing. Longman, London and New York.
Church, K. W. and Hanks, P. 1989 Word Association Norms, Mutual
Information, and Lexicography. Proceedings of the 27th Annual Meet-
ing of the Associaton for Computational Linguistics, Vancouver, B.C.
Fillmore, C. J. 1989 Two Dictionaries. International Journal of Lexicog-
raphy 2(1):57-83.
Hindle, D. 1989 Acquiring Disambiguation Rules from Text. Proceedings
of the 27th Annual Meeting of the Association for Computational
Linguistics, Vancouver, B.C.
Landau, S. I. 1989 Dictionaries: The Art and Craft of Lexicography.
</reference>
<affiliation confidence="0.745728">
Cambridge University Press, Cambridge, U.K.
</affiliation>
<footnote confidence="0.852902125">
Branimir Boguraev is a Research Staff Member at the IBM T.J.
Watson Research Center, where he is also in charge of the Lexical
Systems Project. He holds a Ph.D. degree from the University of
Cambridge, where he subsequently worked on a number of projects
in natural language processing. His recent work has been in the
areas of computational lexicography and lexicology. Boguraev&apos;s
address is: IBM Research, P.O. Box 704, Yorktown Heights, New
York 10598. E-mail: bkb@ibm.com
</footnote>
<note confidence="0.6797035">
GENERATING NATURAL LANGUAGE UNDER
PRAGMATIC CONSTRAINTS
</note>
<subsectionHeader confidence="0.469301">
Eduard H. Hovy
</subsectionHeader>
<bodyText confidence="0.540924333333333">
(Information Sciences Institute, University of Southern
California)
Hillsdale, NJ: Lawrence Erlbaum Associates, 1988, xiii +
214 pp
Hardbound, ISBN 0-8058-0248-7, $29.95
Paperbound, ISBN 0-8058-0249-5, $19.95
</bodyText>
<figure confidence="0.696747">
Reviewed by
Wolfgang Hoeppner
University of Koblenz
</figure>
<bodyText confidence="0.999691071428571">
This book is a revised version of the author&apos;s dissertation,
which was submitted to Yale University in February 1987
and published as a research report (Hovy 1987). One
shouldn&apos;t be too surprised to learn from the preface that
Roger C. Schank, Drew McDermott, and Bob Abelson
were the honorable members of the thesis committee. Vari-
ous well-known Al researchers—not exclusively from
Yale--have given a hand while the thesis was on its way
and are therefore mentioned in the acknowledgments. The
acknowledgments, by the way, give a first example of how
stylistic features affect the generation of text: Hovy switches
between several styles (formality, verbosity, gratefulness,
haste) while expressing his thanks to different classes of
persons. The preface stresses that the book is not only
useful to computational linguists, but also to theoretical
linguists, especially those working in generation. Thus, the
last section of every chapter deals with implementation and
might be skipped by readers not interested in the computa-
tional issues.
The book describes the generation system PAULINE, which
was developed and implemented by the author. The sys-
tem&apos;s name is an acronym: Planning And Uttering Lan-
guage In Natural Environments. (It is also the name of
Hov:y&apos;s sister).
Chapter 1 (11 pp.) introduces the specific research area:
how do pragmatic and stylistic issues influence the genera-
tion of natural language texts? Starting with real-world
descriptions of an event at Yale, the destruction of a
shantytown by university authorities, it is demonstrated
how different viewpoints of the respective authors affect
texts. The same event is then described by PAULINE in
different pragmatic adjustments. The event itself is repre-
sented in a network of about 120 elements (presumably
some version of conceptual dependency), and it is claimed
that the system produces over 100 different texts. The
second example is a description of a fictitious primary
election between Carter and Kennedy as Democratic presi-
dential candidates. &amp;quot;Well, so Carter lost the primary to
Kennedy by 1335 votes&amp;quot; is one example of a very condensed
description. This terseness is beaten only by example num-
ber 12, which consists of nothing but blanks: &amp;quot;The program
didn&apos;t find any topics that it liked and the hearer also liked,
</bodyText>
<page confidence="0.925926">
186 Computational Linguistics Volume 16, Number 3, September 1990
</page>
<subsectionHeader confidence="0.566686">
Book Reviews Generating Natural Language under Pragmatic Constraints
</subsectionHeader>
<bodyText confidence="0.9993198">
and it didn&apos;t have time to search the story representation
for other topics or to find ways of mitigating the unpleasant
ones&amp;quot; (p. 8). The third event deals with a fight between
Mike and Jim, resulting in the death of the latter. All three
events are used throughout the book. I didn&apos;t count the
different versions of the texts generated by the system and
included as examples in the book, but one gets a little bored
when reading, e.g., the nth version of the shantytown story.
The programmatic essence of Hovy&apos;s approach can best
be characterized by the following quotation:
Current artificial intelligence work in natural language process-
ing places far too little emphasis on the role of pragmatics. This
is a mistake. . . . The time is ripe to start examining what kinds
of goals are relevant, what strategies achieve them, and what
planners and realizers must be like to operate under their
control. We must face the fact that, in order to have real,
flexible text, we simply cannot do without recourse to the airy
world of pragmatics. We need to experiment! (p. 11)
True enough!
Chapter 2 (23 pp.) is briefly called &amp;quot;Pragmatics,&amp;quot; and
contains the theoretical underpinnings of PAULINE. Start-
ing with a short discussion on what pragmatics is all about
and what relation to semantics might be established, Hovy
identifies three basic categories that are relevant to his
system (p. 17):
</bodyText>
<listItem confidence="0.956905">
• Interlocutor&apos;s personal characteristics: factual knowl-
edge, opinions, emotional states, interpersonal relation-
ship, etc.
• The speaker&apos;s goals with respect to the hearer: effects on
future behavior, opinions, relative status, etc.
• The conversational atmosphere: tone, time, physical set-
ting, etc.
</listItem>
<bodyText confidence="0.992162706185567">
Each of these categories is illustrated with several examples
in subsequent sections. This leads to a set of 13 goals, which
are a first approximation to the underlying pragmatic
features necessary for generation. Some examples of these
goals are: increase knowledge; make the topic seem good or
bad, contrary to the hearer&apos;s opinion; make the hearer feel
inferior to, equal with, or dominant over the speaker; be
hasty, normal, or effusive. Hovy does not claim that these
goals are complete or adequate in a general way. They
rather appear to be necessary for the way PAULINE is
supposed to produce text. One might wonder how a system
would be able to choose rationally among these goals, and
this, of course, is beyond the scope of an implemented
system. The user has to determine the interpersonal goals
and the conversational setting by selecting a value for 23
relevant features, each of these being represented by three
possible values. For instance, the emotional state of the
hearer can be set to happy, angry, or calm; the conversa-
tional atmosphere with respect to time can be set to much
time, some time, or little time. Rhetorical goals relate the
pragmatic adjustments defined by the user, or rather the
experimenter, to decisions for the generator, e.g., word
choice, sentence inclusion. The chapter closes with a discus-
sion of rhetorical goals of opinion and of style. The latter
has seldom been investigated in computational linguistics
(exceptions are the EPISTLE project [Heidorn et al. 1982],
or the BOGUE system [Ryan 1989]), and it is certainly a
merit of Hovy&apos;s work to establish an initial set of 12 style
features (e.g., timidity, haste, respect).
In Chapter 3 (17 pp.), Hovy argues for an interpretation
component as part of a natural language generator. Inter-
pretation in this sense means that a set of inference rules
has to be present to &amp;quot;digest&amp;quot; the conceptual input represen-
tation and produce a structure that is suited for the task of
verbalization. Examples for such inference rules are the
determination of an appropriate level of detail, presenta-
tion in a confrontative or conciliatory manner, or the
inclusion of remindings. In PAULINE, the interpretation
mechanism is realized as patterns that change the original
network, when matched with some input configuration.
Chapter 4 (25 pp.) discusses the notion of affect in texts,
i.e., the opinion a speaker has about the content of his or her
utterances. The effect of such a biased generation strategy
is demonstrated with different issues of the fighting scene
between Jim and Mike, where the differences originate
from the agent the system sides with. Hovy develops an
&amp;quot;affect rule&amp;quot;: &amp;quot;In order to convince the hearer that some
topic is GOOD or BAD, combine it with other GOOD or
BAD topics using enhancers and mitigators&amp;quot; (p. 61). This
rule appeals to both aspects of the generation process.
Content is determined by the selection of topics and their
linearization. Three plans are provided that have a similar
function to the text schemata used in the TEXT system
(McKeown 1985). These plans are called DESCRIBE,
RELATE, and CONVINCE, the last one being presented
in some detail, as this is the one where affect sneaks in. The
production of form is influenced by affect-bearing phrases
(e.g. &amp;quot;not only X but Y&amp;quot;), by adverbs (e.g. &amp;quot;really,&amp;quot;
&amp;quot;only,&amp;quot; &amp;quot;slightly&amp;quot;), by clause order, and by word choice
(e.g. &amp;quot;wimpy&amp;quot; versus &amp;quot;small&amp;quot;). Chapter 4 closes with some
observations concerning partiality of the generated text.
The rhetorical goals &amp;quot;timidity&amp;quot; and &amp;quot;partiality,&amp;quot; or rather
their respective values, are calculated on the basis of other
features.
Chapter 5 (23 pp.) is about style. Hovy introduces three
style notions: formality, haste, and force. For each of these
rhetorical goals, some rules are given that allow the system
to derive values by combining already established rhetori-
cal goals. &amp;quot;RG:haste&amp;quot;, for example, &amp;quot;is set to &apos;pressured&apos; if
the time is marked &apos;little&apos;, the relative social status is
marked &apos;subordinate&apos;, and the depth of acquaintance is
marked &apos;acquaintances&apos; or &apos;strangers&apos; &amp;quot; (p. 93). Such an
&amp;quot;algorithmic approach to the creation of style&amp;quot; is what
leads to the diversity of texts generated by PAULINE and is
motivated by the assumption &amp;quot;that style is the result of
following a coherent policy&amp;quot; (p. 104).
In Chapter 6 (24 pp.), Hovy argues for the central role of
the lexicon as the one and only device for linguistic knowl-
edge within a generator. This, obviously, is quite contrary
to current grammatical theories, as for instance the vane-
Computational Linguistics Volume 16, Number 3, September 1990 187
Book Reviews Generating Natural Language under Pragmatic Constraints
ties of unification-based approaches. Hovy briefly sketches
issues in transformational, systemic, and functional gram-
mar, and stresses the need for a &amp;quot;phrasal lexicon&amp;quot; as
originally introduced by Becker (1975). He then introduces
the &amp;quot;syntax specialists,&amp;quot; which incorporate the three tasks
of inclusion of topics, ordering, and casting (selection of
syntactic classes) in a procedural manner. This approach
resembles the one earlier developed for analysis and known
as &amp;quot;word-expert parsing&amp;quot; (Small and Rieger 1982). This
analogy, however, is an inference of the reviewer and is not
discussed in the book. The last section of Chapter 6 illus-
trates the incremental generation of the sentence &amp;quot;The
small 23-year-old from New Haven, Sue, was told by Jim
that Janet died&amp;quot; as a realization of a conceptual depen-
dency representation of something like &amp;quot;Jim told Sue that
Janet died.&amp;quot; Appendix B contains the complete phrasal
grammar implemented for PAULINE.
Chapter 7 (17 pp.) once more turns to the planning
process incorporated in PAULINE. Hovy distinguishes be-
tween prescriptive planning and restrictive planning, the
first of these methods being the one used in generators up to
now. He argues for the incorporation of both techniques,
which is called &amp;quot;limited-commitment or interleaved
planning.&amp;quot; The essence of this method of planning is that
there is no fixed and predefined order between the planner
(expansion of states) and the realizer (testing whether a
planned action has contributed to the ultimate goal). Thus
results of the realizer should be able to improve the planner
at all times during the planning process. Hovy discusses
planning approaches in other generators, e.g., KAMP
(Appelt 1985), the system which is best known for its
extreme plan-based philosophy. The last section of this
chapter describes the interleaved planning employed by
PAULINE.
The book&apos;s last chapter contains a nine-page section,
&amp;quot;Review of Language Generation&amp;quot; a title that is not fully
justified as it is with McKeown and Swartout (1988) or
McDonald (1987). The main contribution of this chapter is
the addition of one fundamental question for generation to
the already existing two questions. Hovy adds to &apos;What
should I say?&apos; and &apos;How should I say it?&apos; the question &apos;Why
should I say it?&apos;, thus manifesting his pragmatic approach.
A question, but quite a different one, has always puzzled
me with American dissertations: why is the state of the art
always contained in the last chapter, unlike European
dissertations, where this is usually the starting point? For
my part, I have developed the habit of starting to read an
American dissertation at the last chapter and then continu-
ing at the beginning.
Appendix A contains a short annotated example, which
shows the generation of a shantytown text. Appendix B
gives a description of PAULINE&apos;S phrasal grammar. The
book is supplemented with an index of names and a topic
index.
A few years ago there was a discussion in Computational
Linguistics initiated by Ballard (1983) about standards for
reports on natural language systems. This discussion was
motivated by the observation that descriptions of NL sys-
tems very often lack the desirable details of implementa-
tion, examples, or theoretical underpinning, as well as
precision. Probably everybody who has been engaged in
teaching courses on NL processing has been frustrated in
trying to get a careful description of how some natural
language system really works. Now, let&apos;s look at Hovy&apos;s
book from this angle. Unlike some other published work
from the well-known Yale Department, Hovy is very mod-
est in his claims of generality or coverage. It is often
pointed out that the specific approach of PAULINE is in no
way a solution of all pragmatic issues in NL generation,
which, by the way, would be difficult to prove. So here we
have a pleasant side of the book.
There are, however, other properties that make the
reader feel a little uneasy. Hovy never states precisely what
the input representations of the texts look like. One can
easily infer that there is some version of conceptual depen-
dency (Schank 1975) representing the contents of the
stories. There is even a linearized ATRANS example for a
prototypical sentence, such as &amp;quot;John gave Mary the book
and she gave him the money&amp;quot; (p. 26). And at the end of the
book (p. 140), Hovy informs us that &amp;quot;PAULINE&apos;S input is
represented in a standard case-frame—type language based
on conceptual dependency. . . and is embedded in a prop-
erty inheritance network.&amp;quot; But still, one wonders how
complex events, as the shantytown example, might be
represented in conceptual dependency networks, which
types of primitive actions are employed, and how a basic
network is enhanced with pragmatic features.
A second shortcoming is the vagueness in how concep-
tual dependency constructs are verbalized. One might guess
that parts of older generators (e.g., Goldman 1975; Mee-
han 1976; or even Simmons and Slocum 1972) are em-
ployed for, e.g., the verbalization of primitive acts, but how
this is done in PAULINE is not stated explicitly.
T:ae interaction of the various pragmatic attributes and
their respective values is described in several different parts
of the book. The reader gets the impression that this is a
very complicated and tricky business, but it is very hard to
state in a somewhat complete way just how this is achieved.
The impact of the features is often demonstrated with
generated texts, and these examples are quite convincing.
However, they are not sufficient for invoking a detailed
picture of how things interact with each other. Apart from
this, there is only a marginal remark on the programming
language, which is T, and on the size of the program (over
12,000 lines, a rather dubious indication). Neither hard-
ware nor processing time is mentioned.
Hovy refers to most of the important work done in NL
generation so far (226 entries in the bibliography). He
could have been more detailed in using these entries. One
often sees a heap of references at the end of a statement,
especially in the theoretical sections. A more specific use of
other people&apos;s work would have been a benefit for the book.
</bodyText>
<page confidence="0.841578">
188 Computational Linguistics Volume 16, Number 3, September 1990
</page>
<note confidence="0.485591">
Book Reviews Briefly Noted
</note>
<bodyText confidence="0.999930846153846">
The final critical remark is related to the degree of
redundancy. There are many statements that occur time
and again throughout the book. The reader often wonders
why one is being told things one already knows. It is not as
bad as in another recent dissertation from Yale (Hammond
1988), which was recently reviewed in a German AI jour-
nal (Hertzberg and Horz 1990). A translated quotation
from this review: &amp;quot;If the reader really wants to read the
complete book, she is punished and lulled to sleep by the
recurrent repetitions. . . . The sentence &apos;Plans are indexed
by the goals they satisfy and by the problems they avoid&apos; is
learned by heart just by reading any two chapters. If
Hammond is an efficient LaTeX user, he has probably
written a macro; at least, it would have been worthwhile.&amp;quot;
Once more: it is not as bad with Hovy, but it is annoying
nevertheless.
My general opinion about the book is that it is necessary
reading for anybody working in NL generation. It is profit-
able reading for anybody engaged in natural language
processing. And it is worthwhile reading for linguists spe-
cializing in pragmatics, style, or language production. I
have stated above those properties that I missed or didn&apos;t
like. What I did like was Hovy&apos;s scientific rigor in advocat-
ing the pragmatic basis of text generation. This attitude is
significant for an AI-oriented approach to language process-
ing, but this does not imply that it is a scruffy one.
</bodyText>
<sectionHeader confidence="0.998097" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.976103254237288">
Appelt, D. E. 1985 Planning English Sentences. Cambridge University
Press, Cambridge, U.K.
Ballard, B. W. 1983 On the Need for Careful Description of NL Proto-
types. Computational Linguistics 9(1):23-24.
Becker, J. D. 1975 The Phrasal Lexicon. In: Schank, R. C. and Nash-
Webber, B. L., eds. Theoretical Issues in Natural Language Process-
ing. Cambridge, MA, 70-73.
Goldman, N. M. 1975 Conceptual Generation. In: Schank R. C. Concep-
tual Information Processing. North Holland, Amsterdam, The Nether-
lands, 289-371.
Hammond, K. 1988. Case-Based Planning: Viewing Planning as a Mem-
ory Task. Academic Press, San Diego, CA.
Heidorn, G. E., Jensen, K., Miller, L. A., Byrd, R. J., and Chodorow,
M. S. 1982 The EPISTLE Text-Critiquing System. IBM Systems
Journal 21(3):305-326.
Hertzberg, J., and Horz, A. 1990 Review of Hammond 1988. nnstliche
Intelligenz: Forschung und Entwicklung 1:61-62.
Hovy, E. H. 1987 Generating Natural Language under Pragmatic Con-
straints. Ph.D. thesis, research report 521, Department of Computer
Sciences, Yale University, New Haven, CT.
McDonald, D. D. 1987. Natural Language Generation. In: Shapiro, S.,
ed. Encyclopedia of Artificial Intelligence. John Wiley, New York,
NY, 642-655.
McKeown, K. R. 1985 Text Generation. Cambridge University Press,
Cambridge, U.K.
McKeown, K. R. and Swartout, W. R. 1988. Language Generation and
Explanation. In: Zock, M. and Sabah, G., eds. Advances in Natural
Language Generation: An Interdisciplinary Perspective. Pinter Publish-
ers, London, U.K., 1-51.
Meehan, J. 1976. The Metanovel: Writing Stories by Computer. Garland
Press, New York, NY.
Ryan, M. B. P. 1989 The Computational Codification of the Semantic
Aspects of Style. Technical report CSRI-231, Computer Systems
Research Institute, University of Toronto, Toronto, Canada.
Schank, R. C. 1975 Conceptual Information Processing. North-Holland,
Amsterdam, The Netherlands.
Simmons, R. F. and Slocum, J. 1972 Generating English Discourse from
Semantic Networks. Communications of the ACM 15(10):891-905.
Small, S. and Rieger, C. 1982 Parsing and Comprehending with Word
Experts (A Theory and its Realization). In: Lehnert, W. G. and Ringle,
M. H., eds. Strategies for Natural Language Processing. Lawrence
Erlbaum Associates, Hillsdale, N.J., 89-148.
Wolfgang Hoeppner is a professor of Computational Linguistics
and Artificial Intelligence at the University of Koblenz, West
Germany. He holds a doctorate from the University of Hamburg
in Computational Linguistics. Formerly engaged in various AI
projects (HAM-ANS, WISBER) at the University of Hamburg,
his recent research has been in knowledge representation, espe-
cially temporal and spatial reasoning, and in NL generation.
Hoeppner&apos;s address is: University of Koblenz, FB Informatik,
Computerlinguistik, Rheinau 3-4, D-5400 Koblenz, West Ger-
many. E-mail: hoeppner@infko.uucp
BRIEFLY NOTED
LEARNABILITY AND LINGUISTIC THEORY
Robert J. Matthews and William Demopoulos, eds.
(Rutgers University and University of Western Ontario)
Dordrecht: Kluwer, 1989, vii + 217 pp.
(Studies in Theoretical Psycholinguistics 9)
Hardbound, ISBN 0-7923-0247-8, Dfl 130.-, $64.00, £42.00
</reference>
<bodyText confidence="0.98568176">
The ninth volume of Kluwer&apos;s Studies in Theoretical Psycholin-
guistics is, like the previous ones, devoted to what any modern
linguistic theory has inevitably to face, namely the logical prob-
lem of language acquisition. As before, in most contributed papers
the problem is discussed within the parametrized Government-
Binding framework. This time, however, issues from formal learn-
ability theory serve as a starting point and as a basis for the
subsequent reformulation of the rationalist-empiricist debate on
language acquisition. Less general questions are also addressed;
as, for example, how children eventually succeed in avoiding
overgeneralizations—an intriguing puzzle, given the widely ac-
cepted view that not enough negative evidence is directly accessi-
ble in the course of first language learning.
No doubt the book may be of interest to cognitive scientists and
those computational linguists who deal with modeling natural
language acquisition. The idea of having a computer system that
gradually learns a language from examples, very much like people
do, is an exciting one, and it will surely receive much attention in
the foreseeable future. Though the book under consideration does
not provide the would-be designers of a computer learning system
with algorithms they might immediately employ, it presents a
wide selection of topics characteristic of the current literature on
modeling natural language acquisition and, consequently, can be
used by computational linguists as an important source of informa-
tion relevant to their research.—Mirostaw Bahko, Institute of
</bodyText>
<footnote confidence="0.3894415">
Polish Language, Warsaw University
Computational Linguistics Volume 16, Number 3, September 1990 189
</footnote>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.934657">Book Reviews Generating Natural Language under Pragmatic Constraints</title>
<abstract confidence="0.996464677419355">the book more of a recommended reading is the renewed empiricism in the field, largely promoted by the very practical need to scale up natural language systems, and largely due to the realization that linguistic information about words could be derived from massive on-line text resources. Whether these resources come in the shape of on-line dictionaries or text corpora is immaterial here. By reading Up, becomes acutely aware of the richness of lexical information available in (and distributed over) millions of words of text. One also understands that careful inspection of a dictionary entry (or a set of related entries) is likely to reveal considerably more in terms of lexical properties of the word (or class of words) than is apparently visible. The nature of lexical information in, and its extractability from, such text resources has been much discussed recently in the computational linguistics and computational lexicography literature; in particular, the statement that there is a wealth of implicit information available in on-line dictionaries and corpora has been made over and over again recently (see, for instance, Atkins et al. 1988; Boguraev and Briscoe 1989; Hindle 1989; Church and Hanks 1989). However, there is a world of difference between &amp;quot;retro-engineering,&amp;quot; by whatever means, methods and rules for inferring lexical information from dictionary entries, and being told in advance the kinds of lexical regularities, generalizations, and properties encoded in these entries. For that reason alone, and particularly given that the COBUILD dictionary is available in machine-readable Up a book that should not be ignored by researchers interested in computational lexicography, lexical semantics, or simply the nature of word meaning.</abstract>
<note confidence="0.931632411764706">REFERENCES B. J. and Levin, B. 1988 Anatomy of a Verb Entry. International Journal of Lexicographyl(2):84-126. B. K., and Briscoe, E. J. 1989 Lexicography for Language Processing. London and New York. Church, K. W. and Hanks, P. 1989 Word Association Norms, Mutual and Lexicography. of the 27th Annual Meetof the Associaton for Computational Linguistics, B.C. C. J. 1989 Two Dictionaries. Journal of Lexicog- D. 1989 Acquiring Disambiguation Rules from Text. of the 27th Annual Meeting of the Association for Computational B.C. S. I. 1989 The Art and Craft of Lexicography. Cambridge University Press, Cambridge, U.K. Boguraev is Research Staff Member at the IBM T.J. Watson Research Center, where he is also in charge of the Lexical Systems Project. He holds a Ph.D. degree from the University of</note>
<abstract confidence="0.938782">Cambridge, where he subsequently worked on a number of projects in natural language processing. His recent work has been in the areas of computational lexicography and lexicology. Boguraev&apos;s</abstract>
<address confidence="0.8011285">is: IBM Research, P.O. Box 704, Yorktown Heights, New York 10598. E-mail: bkb@ibm.com</address>
<title confidence="0.871459">GENERATING NATURAL LANGUAGE UNDER</title>
<author confidence="0.999731">Eduard H Hovy</author>
<affiliation confidence="0.7842495">(Information Sciences Institute, University of Southern California)</affiliation>
<address confidence="0.897156">Hillsdale, NJ: Lawrence Erlbaum Associates, 1988, xiii +</address>
<note confidence="0.8233765">214 pp Hardbound, ISBN 0-8058-0248-7, $29.95 Paperbound, ISBN 0-8058-0249-5, $19.95 Reviewed by</note>
<author confidence="0.997203">Wolfgang Hoeppner</author>
<affiliation confidence="0.986006">University of Koblenz</affiliation>
<abstract confidence="0.992004148148147">This book is a revised version of the author&apos;s dissertation, which was submitted to Yale University in February 1987 and published as a research report (Hovy 1987). One shouldn&apos;t be too surprised to learn from the preface that Roger C. Schank, Drew McDermott, and Bob Abelson were the honorable members of the thesis committee. Various well-known Al researchers—not exclusively from Yale--have given a hand while the thesis was on its way and are therefore mentioned in the acknowledgments. The acknowledgments, by the way, give a first example of how stylistic features affect the generation of text: Hovy switches between several styles (formality, verbosity, gratefulness, haste) while expressing his thanks to different classes of persons. The preface stresses that the book is not only useful to computational linguists, but also to theoretical linguists, especially those working in generation. Thus, the last section of every chapter deals with implementation and might be skipped by readers not interested in the computational issues. book describes the generation system was developed and implemented by the author. The system&apos;s name is an acronym: Planning And Uttering Language In Natural Environments. (It is also the name of Hov:y&apos;s sister). Chapter 1 (11 pp.) introduces the specific research area: how do pragmatic and stylistic issues influence the generation of natural language texts? Starting with real-world descriptions of an event at Yale, the destruction of a shantytown by university authorities, it is demonstrated how different viewpoints of the respective authors affect The same event is then described by different pragmatic adjustments. The event itself is represented in a network of about 120 elements (presumably some version of conceptual dependency), and it is claimed that the system produces over 100 different texts. The second example is a description of a fictitious primary election between Carter and Kennedy as Democratic presidential candidates. &amp;quot;Well, so Carter lost the primary to Kennedy by 1335 votes&amp;quot; is one example of a very condensed description. This terseness is beaten only by example numconsists of nothing but blanks: &amp;quot;The program didn&apos;t find any topics that it liked and the hearer also liked, 186 Computational Linguistics Volume 16, Number 3, September 1990 Book Reviews Generating Natural Language under Pragmatic Constraints and it didn&apos;t have time to search the story representation for other topics or to find ways of mitigating the unpleasant ones&amp;quot; (p. 8). The third event deals with a fight between Mike and Jim, resulting in the death of the latter. All three events are used throughout the book. I didn&apos;t count the different versions of the texts generated by the system and included as examples in the book, but one gets a little bored when reading, e.g., the nth version of the shantytown story. The programmatic essence of Hovy&apos;s approach can best be characterized by the following quotation: artificial work in natural language processing places far too little emphasis on the role of pragmatics. This is a mistake. . . . The time is ripe to start examining what kinds of goals are relevant, what strategies achieve them, and what planners and realizers must be like to operate under their control. We must face the fact that, in order to have real, flexible text, we simply cannot do without recourse to the airy world of pragmatics. We need to experiment! (p. 11) True enough! Chapter 2 (23 pp.) is briefly called &amp;quot;Pragmatics,&amp;quot; and the theoretical underpinnings of Starta short discussion on what pragmatics is all about and what relation to semantics might be established, Hovy identifies three basic categories that are relevant to his system (p. 17): • Interlocutor&apos;s personal characteristics: factual knowledge, opinions, emotional states, interpersonal relationship, etc. • The speaker&apos;s goals with respect to the hearer: effects on future behavior, opinions, relative status, etc. • The conversational atmosphere: tone, time, physical setting, etc. Each of these categories is illustrated with several examples in subsequent sections. This leads to a set of 13 goals, which are a first approximation to the underlying pragmatic features necessary for generation. Some examples of these goals are: increase knowledge; make the topic seem good or bad, contrary to the hearer&apos;s opinion; make the hearer feel inferior to, equal with, or dominant over the speaker; be hasty, normal, or effusive. Hovy does not claim that these goals are complete or adequate in a general way. They appear to be necessary for the way supposed to produce text. One might wonder how a system would be able to choose rationally among these goals, and this, of course, is beyond the scope of an implemented system. The user has to determine the interpersonal goals and the conversational setting by selecting a value for 23 relevant features, each of these being represented by three possible values. For instance, the emotional state of the hearer can be set to happy, angry, or calm; the conversational atmosphere with respect to time can be set to much time, some time, or little time. Rhetorical goals relate the pragmatic adjustments defined by the user, or rather the experimenter, to decisions for the generator, e.g., word sentence inclusion. The chapter closes with a discussion of rhetorical goals of opinion and of style. The latter has seldom been investigated in computational linguistics (exceptions are the EPISTLE project [Heidorn et al. 1982], or the BOGUE system [Ryan 1989]), and it is certainly a merit of Hovy&apos;s work to establish an initial set of 12 style features (e.g., timidity, haste, respect). In Chapter 3 (17 pp.), Hovy argues for an interpretation as part of a natural language generator. Interpretation in this sense means that a set of inference rules has to be present to &amp;quot;digest&amp;quot; the conceptual input representation and produce a structure that is suited for the task of verbalization. Examples for such inference rules are the determination of an appropriate level of detail, presentation in a confrontative or conciliatory manner, or the of remindings. In interpretation mechanism is realized as patterns that change the original network, when matched with some input configuration. Chapter 4 (25 pp.) discusses the notion of affect in texts, i.e., the opinion a speaker has about the content of his or her utterances. The effect of such a biased generation strategy is demonstrated with different issues of the fighting scene between Jim and Mike, where the differences originate from the agent the system sides with. Hovy develops an &amp;quot;affect rule&amp;quot;: &amp;quot;In order to convince the hearer that some topic is GOOD or BAD, combine it with other GOOD or BAD topics using enhancers and mitigators&amp;quot; (p. 61). This rule appeals to both aspects of the generation process. Content is determined by the selection of topics and their linearization. Three plans are provided that have a similar function to the text schemata used in the TEXT system (McKeown 1985). These plans are called DESCRIBE, RELATE, and CONVINCE, the last one being presented in some detail, as this is the one where affect sneaks in. The production of form is influenced by affect-bearing phrases (e.g. &amp;quot;not only X but Y&amp;quot;), by adverbs (e.g. &amp;quot;really,&amp;quot; &amp;quot;only,&amp;quot; &amp;quot;slightly&amp;quot;), by clause order, and by word choice (e.g. &amp;quot;wimpy&amp;quot; versus &amp;quot;small&amp;quot;). Chapter 4 closes with some observations concerning partiality of the generated text. The rhetorical goals &amp;quot;timidity&amp;quot; and &amp;quot;partiality,&amp;quot; or rather their respective values, are calculated on the basis of other features. Chapter 5 (23 pp.) is about style. Hovy introduces three style notions: formality, haste, and force. For each of these rhetorical goals, some rules are given that allow the system to derive values by combining already established rhetorical goals. &amp;quot;RG:haste&amp;quot;, for example, &amp;quot;is set to &apos;pressured&apos; if the time is marked &apos;little&apos;, the relative social status is marked &apos;subordinate&apos;, and the depth of acquaintance is marked &apos;acquaintances&apos; or &apos;strangers&apos; &amp;quot; (p. 93). Such an &amp;quot;algorithmic approach to the creation of style&amp;quot; is what to the diversity of texts generated by is motivated by the assumption &amp;quot;that style is the result of following a coherent policy&amp;quot; (p. 104). In Chapter 6 (24 pp.), Hovy argues for the central role of the lexicon as the one and only device for linguistic knowledge within a generator. This, obviously, is quite contrary current grammatical theories, as for instance the vane- Computational Linguistics Volume 16, Number 3, September 1990 187 Book Reviews Generating Natural Language under Pragmatic Constraints ties of unification-based approaches. Hovy briefly sketches issues in transformational, systemic, and functional grammar, and stresses the need for a &amp;quot;phrasal lexicon&amp;quot; as originally introduced by Becker (1975). He then introduces the &amp;quot;syntax specialists,&amp;quot; which incorporate the three tasks of inclusion of topics, ordering, and casting (selection of syntactic classes) in a procedural manner. This approach resembles the one earlier developed for analysis and known as &amp;quot;word-expert parsing&amp;quot; (Small and Rieger 1982). This analogy, however, is an inference of the reviewer and is not discussed in the book. The last section of Chapter 6 illustrates the incremental generation of the sentence &amp;quot;The small 23-year-old from New Haven, Sue, was told by Jim that Janet died&amp;quot; as a realization of a conceptual dependency representation of something like &amp;quot;Jim told Sue that Janet died.&amp;quot; Appendix B contains the complete phrasal implemented for Chapter 7 (17 pp.) once more turns to the planning incorporated in distinguishes between prescriptive planning and restrictive planning, the first of these methods being the one used in generators up to now. He argues for the incorporation of both techniques, which is called &amp;quot;limited-commitment or interleaved planning.&amp;quot; The essence of this method of planning is that there is no fixed and predefined order between the planner (expansion of states) and the realizer (testing whether a planned action has contributed to the ultimate goal). Thus results of the realizer should be able to improve the planner at all times during the planning process. Hovy discusses planning approaches in other generators, e.g., KAMP (Appelt 1985), the system which is best known for its extreme plan-based philosophy. The last section of this chapter describes the interleaved planning employed by PAULINE. The book&apos;s last chapter contains a nine-page section, &amp;quot;Review of Language Generation&amp;quot; a title that is not fully justified as it is with McKeown and Swartout (1988) or McDonald (1987). The main contribution of this chapter is the addition of one fundamental question for generation to the already existing two questions. Hovy adds to &apos;What should I say?&apos; and &apos;How should I say it?&apos; the question &apos;Why should I say it?&apos;, thus manifesting his pragmatic approach. A question, but quite a different one, has always puzzled me with American dissertations: why is the state of the art always contained in the last chapter, unlike European dissertations, where this is usually the starting point? For my part, I have developed the habit of starting to read an American dissertation at the last chapter and then continuing at the beginning. Appendix A contains a short annotated example, which shows the generation of a shantytown text. Appendix B a description of grammar. The book is supplemented with an index of names and a topic index. few years ago there was a discussion in by Ballard (1983) about standards for reports on natural language systems. This discussion was motivated by the observation that descriptions of NL systems very often lack the desirable details of implementation, examples, or theoretical underpinning, as well as precision. Probably everybody who has been engaged in teaching courses on NL processing has been frustrated in trying to get a careful description of how some natural system Now, let&apos;s look at Hovy&apos;s book from this angle. Unlike some other published work from the well-known Yale Department, Hovy is very modest in his claims of generality or coverage. It is often out that the specific approach of in no way a solution of all pragmatic issues in NL generation, which, by the way, would be difficult to prove. So here we have a pleasant side of the book. There are, however, other properties that make the reader feel a little uneasy. Hovy never states precisely what the input representations of the texts look like. One can easily infer that there is some version of conceptual dependency (Schank 1975) representing the contents of the stories. There is even a linearized ATRANS example for a prototypical sentence, such as &amp;quot;John gave Mary the book and she gave him the money&amp;quot; (p. 26). And at the end of the (p. 140), Hovy informs us that is represented in a standard case-frame—type language based conceptual dependency. . . and is embedded in a property inheritance network.&amp;quot; But still, one wonders how complex events, as the shantytown example, might be represented in conceptual dependency networks, which types of primitive actions are employed, and how a basic network is enhanced with pragmatic features. A second shortcoming is the vagueness in how conceptual dependency constructs are verbalized. One might guess that parts of older generators (e.g., Goldman 1975; Meehan 1976; or even Simmons and Slocum 1972) are employed for, e.g., the verbalization of primitive acts, but how is done in not stated explicitly. T:ae interaction of the various pragmatic attributes and their respective values is described in several different parts of the book. The reader gets the impression that this is a very complicated and tricky business, but it is very hard to state in a somewhat complete way just how this is achieved. The impact of the features is often demonstrated with generated texts, and these examples are quite convincing. However, they are not sufficient for invoking a detailed picture of how things interact with each other. Apart from this, there is only a marginal remark on the programming language, which is T, and on the size of the program (over 12,000 lines, a rather dubious indication). Neither hardware nor processing time is mentioned. Hovy refers to most of the important work done in NL generation so far (226 entries in the bibliography). He could have been more detailed in using these entries. One often sees a heap of references at the end of a statement, especially in the theoretical sections. A more specific use of other people&apos;s work would have been a benefit for the book.</abstract>
<note confidence="0.616073">188 Computational Linguistics Volume 16, Number 3, September 1990 Book Reviews Briefly Noted The final critical remark is related to the degree of</note>
<abstract confidence="0.99902224">redundancy. There are many statements that occur time and again throughout the book. The reader often wonders why one is being told things one already knows. It is not as bad as in another recent dissertation from Yale (Hammond which was recently reviewed in a German journal (Hertzberg and Horz 1990). A translated quotation from this review: &amp;quot;If the reader really wants to read the complete book, she is punished and lulled to sleep by the recurrent repetitions. . . . The sentence &apos;Plans are indexed by the goals they satisfy and by the problems they avoid&apos; is learned by heart just by reading any two chapters. If Hammond is an efficient LaTeX user, he has probably written a macro; at least, it would have been worthwhile.&amp;quot; Once more: it is not as bad with Hovy, but it is annoying nevertheless. My general opinion about the book is that it is necessary reading for anybody working in NL generation. It is profitable reading for anybody engaged in natural language processing. And it is worthwhile reading for linguists specializing in pragmatics, style, or language production. I have stated above those properties that I missed or didn&apos;t like. What I did like was Hovy&apos;s scientific rigor in advocating the pragmatic basis of text generation. This attitude is significant for an AI-oriented approach to language processing, but this does not imply that it is a scruffy one.</abstract>
<note confidence="0.84488">REFERENCES E. 1985 English Sentences. University Press, Cambridge, U.K. Ballard, B. W. 1983 On the Need for Careful Description of NL Proto- Linguistics Becker, J. D. 1975 The Phrasal Lexicon. In: Schank, R. C. and Nash- B. L., eds. Issues in Natural Language Process- MA, 70-73. N. M. 1975 Conceptual Generation. In: Schank R. C. Concep- Information Processing. Holland, Amsterdam, The Netherlands, 289-371. K. 1988. Planning: Viewing Planning as a Mem- Task. Press, San Diego, CA. Heidorn, G. E., Jensen, K., Miller, L. A., Byrd, R. J., and Chodorow, S. 1982 The EPISTLE Text-Critiquing System. J., and Horz, A. 1990 Review of Hammond 1988. Forschung und Entwicklung E. H. 1987 Natural Language under Pragmatic Conthesis, research report 521, Department of Computer Sciences, Yale University, New Haven, CT. D. Natural Language Generation. In: Shapiro, S., of Artificial Intelligence. Wiley, New York, NY, 642-655. K. R. 1985 Generation. University Press,</note>
<address confidence="0.7079685">Cambridge, U.K. McKeown, K. R. and Swartout, W. R. 1988. Language Generation and</address>
<note confidence="0.936904222222222">In: Zock, M. and Sabah, G., eds. in Natural Generation: An Interdisciplinary Perspective. Publishers, London, U.K., 1-51. J. 1976. Metanovel: Writing Stories by Computer. Press, New York, NY. M. B. P. 1989 Computational Codification of the Semantic of Style. report CSRI-231, Computer Systems Research Institute, University of Toronto, Toronto, Canada. R. C. 1975 Information Processing. Amsterdam, The Netherlands. Simmons, R. F. and Slocum, J. 1972 Generating English Discourse from Networks. of the ACM Small, S. and Rieger, C. 1982 Parsing and Comprehending with Word Experts (A Theory and its Realization). In: Lehnert, W. G. and Ringle, H., eds. for Natural Language Processing. Erlbaum Associates, Hillsdale, N.J., 89-148. Hoeppner is professor of Computational Linguistics and Artificial Intelligence at the University of Koblenz, West</note>
<abstract confidence="0.8940278">Germany. He holds a doctorate from the University of Hamburg in Computational Linguistics. Formerly engaged in various AI projects (HAM-ANS, WISBER) at the University of Hamburg, his recent research has been in knowledge representation, especially temporal and spatial reasoning, and in NL generation.</abstract>
<affiliation confidence="0.639611">Hoeppner&apos;s address is: University of Koblenz, FB Informatik,</affiliation>
<address confidence="0.823284">Computerlinguistik, Rheinau 3-4, D-5400 Koblenz, West Ger-</address>
<email confidence="0.810562">many.E-mail:hoeppner@infko.uucp</email>
<title confidence="0.841655">BRIEFLY NOTED LEARNABILITY AND LINGUISTIC THEORY</title>
<author confidence="0.993635">Robert J Matthews</author>
<author confidence="0.993635">William Demopoulos</author>
<author confidence="0.993635">eds</author>
<affiliation confidence="0.937941">(Rutgers University and University of Western Ontario)</affiliation>
<address confidence="0.447135">Dordrecht: Kluwer, 1989, vii + 217 pp.</address>
<note confidence="0.837523">(Studies in Theoretical Psycholinguistics 9) Hardbound, ISBN 0-7923-0247-8, Dfl 130.-, $64.00, £42.00 The ninth volume of Kluwer&apos;s Studies in Theoretical Psycholin-</note>
<abstract confidence="0.980171541666667">guistics is, like the previous ones, devoted to what any modern linguistic theory has inevitably to face, namely the logical problem of language acquisition. As before, in most contributed papers the problem is discussed within the parametrized Government- Binding framework. This time, however, issues from formal learnability theory serve as a starting point and as a basis for the subsequent reformulation of the rationalist-empiricist debate on language acquisition. Less general questions are also addressed; as, for example, how children eventually succeed in avoiding overgeneralizations—an intriguing puzzle, given the widely accepted view that not enough negative evidence is directly accessible in the course of first language learning. No doubt the book may be of interest to cognitive scientists and those computational linguists who deal with modeling natural language acquisition. The idea of having a computer system that gradually learns a language from examples, very much like people do, is an exciting one, and it will surely receive much attention in the foreseeable future. Though the book under consideration does not provide the would-be designers of a computer learning system with algorithms they might immediately employ, it presents a wide selection of topics characteristic of the current literature on modeling natural language acquisition and, consequently, can be used by computational linguists as an important source of informarelevant to their research.—Mirostaw Institute of</abstract>
<affiliation confidence="0.93144">Polish Language, Warsaw University</affiliation>
<address confidence="0.859089">Computational Linguistics Volume 16, Number 3, September 1990 189</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B T Atkins</author>
<author>J Kegl</author>
<author>B Levin</author>
</authors>
<title>Anatomy of a Verb Entry.</title>
<date>1988</date>
<journal>International Journal of Lexicographyl(2):84-126.</journal>
<contexts>
<context position="1237" citStr="Atkins et al. 1988" startWordPosition="191" endWordPosition="194">o understands that careful inspection of a dictionary entry (or a set of related entries) is likely to reveal considerably more in terms of lexical properties of the word (or class of words) than is apparently visible. The nature of lexical information in, and its extractability from, such text resources has been much discussed recently in the computational linguistics and computational lexicography literature; in particular, the statement that there is a wealth of implicit information available in on-line dictionaries and corpora has been made over and over again recently (see, for instance, Atkins et al. 1988; Boguraev and Briscoe 1989; Hindle 1989; Church and Hanks 1989). However, there is a world of difference between &amp;quot;retro-engineering,&amp;quot; by whatever means, methods and rules for inferring lexical information from dictionary entries, and being told in advance the kinds of lexical regularities, generalizations, and properties encoded in these entries. For that reason alone, and particularly given that the COBUILD dictionary is available in machine-readable form, Looking Up is a book that should not be ignored by researchers interested in computational lexicography, lexical semantics, or simply the</context>
</contexts>
<marker>Atkins, Kegl, Levin, 1988</marker>
<rawString>Atkins, B. T., Kegl, J. and Levin, B. 1988 Anatomy of a Verb Entry. International Journal of Lexicographyl(2):84-126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B K Boguraev</author>
<author>E J Briscoe</author>
</authors>
<date>1989</date>
<booktitle>Computational Lexicography for Natural Language Processing. Longman,</booktitle>
<location>London and New York.</location>
<contexts>
<context position="1264" citStr="Boguraev and Briscoe 1989" startWordPosition="195" endWordPosition="198">areful inspection of a dictionary entry (or a set of related entries) is likely to reveal considerably more in terms of lexical properties of the word (or class of words) than is apparently visible. The nature of lexical information in, and its extractability from, such text resources has been much discussed recently in the computational linguistics and computational lexicography literature; in particular, the statement that there is a wealth of implicit information available in on-line dictionaries and corpora has been made over and over again recently (see, for instance, Atkins et al. 1988; Boguraev and Briscoe 1989; Hindle 1989; Church and Hanks 1989). However, there is a world of difference between &amp;quot;retro-engineering,&amp;quot; by whatever means, methods and rules for inferring lexical information from dictionary entries, and being told in advance the kinds of lexical regularities, generalizations, and properties encoded in these entries. For that reason alone, and particularly given that the COBUILD dictionary is available in machine-readable form, Looking Up is a book that should not be ignored by researchers interested in computational lexicography, lexical semantics, or simply the nature of word meaning. RE</context>
</contexts>
<marker>Boguraev, Briscoe, 1989</marker>
<rawString>Boguraev, B. K., and Briscoe, E. J. 1989 Computational Lexicography for Natural Language Processing. Longman, London and New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>P Hanks</author>
</authors>
<title>Word Association Norms, Mutual Information, and Lexicography.</title>
<date>1989</date>
<booktitle>Proceedings of the 27th Annual Meeting of the Associaton for Computational Linguistics,</booktitle>
<location>Vancouver, B.C.</location>
<marker>Church, Hanks, 1989</marker>
<rawString>Church, K. W. and Hanks, P. 1989 Word Association Norms, Mutual Information, and Lexicography. Proceedings of the 27th Annual Meeting of the Associaton for Computational Linguistics, Vancouver, B.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Fillmore</author>
</authors>
<title>Two Dictionaries.</title>
<date>1989</date>
<journal>International Journal of Lexicography</journal>
<pages>2--1</pages>
<marker>Fillmore, 1989</marker>
<rawString>Fillmore, C. J. 1989 Two Dictionaries. International Journal of Lexicography 2(1):57-83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Acquiring Disambiguation Rules from Text.</title>
<date>1989</date>
<booktitle>Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Vancouver, B.C.</location>
<marker>Hindle, 1989</marker>
<rawString>Hindle, D. 1989 Acquiring Disambiguation Rules from Text. Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics, Vancouver, B.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S I Landau</author>
</authors>
<title>Dictionaries: The Art and Craft of Lexicography.</title>
<date>1989</date>
<marker>Landau, 1989</marker>
<rawString>Landau, S. I. 1989 Dictionaries: The Art and Craft of Lexicography.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Appelt</author>
</authors>
<title>Planning English Sentences.</title>
<date>1985</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, U.K.</location>
<marker>Appelt, 1985</marker>
<rawString>Appelt, D. E. 1985 Planning English Sentences. Cambridge University Press, Cambridge, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B W Ballard</author>
</authors>
<title>On the Need for Careful Description of NL Prototypes.</title>
<date>1983</date>
<journal>Computational Linguistics</journal>
<pages>9--1</pages>
<marker>Ballard, 1983</marker>
<rawString>Ballard, B. W. 1983 On the Need for Careful Description of NL Prototypes. Computational Linguistics 9(1):23-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Becker</author>
</authors>
<title>The Phrasal Lexicon.</title>
<date>1975</date>
<booktitle>Theoretical Issues in Natural Language Processing.</booktitle>
<pages>70--73</pages>
<editor>In: Schank, R. C. and NashWebber, B. L., eds.</editor>
<location>Cambridge, MA,</location>
<marker>Becker, 1975</marker>
<rawString>Becker, J. D. 1975 The Phrasal Lexicon. In: Schank, R. C. and NashWebber, B. L., eds. Theoretical Issues in Natural Language Processing. Cambridge, MA, 70-73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N M Goldman</author>
</authors>
<title>Conceptual Generation. In: Schank</title>
<date>1975</date>
<pages>289--371</pages>
<publisher>North</publisher>
<location>Holland, Amsterdam, The</location>
<marker>Goldman, 1975</marker>
<rawString>Goldman, N. M. 1975 Conceptual Generation. In: Schank R. C. Conceptual Information Processing. North Holland, Amsterdam, The Netherlands, 289-371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hammond</author>
</authors>
<title>Case-Based Planning: Viewing Planning as a Memory Task.</title>
<date>1988</date>
<publisher>Academic Press,</publisher>
<location>San Diego, CA.</location>
<marker>Hammond, 1988</marker>
<rawString>Hammond, K. 1988. Case-Based Planning: Viewing Planning as a Memory Task. Academic Press, San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Heidorn</author>
<author>K Jensen</author>
<author>L A Miller</author>
<author>R J Byrd</author>
<author>M S Chodorow</author>
</authors>
<title>The EPISTLE Text-Critiquing System.</title>
<date>1982</date>
<journal>IBM Systems Journal</journal>
<pages>21--3</pages>
<marker>Heidorn, Jensen, Miller, Byrd, Chodorow, 1982</marker>
<rawString>Heidorn, G. E., Jensen, K., Miller, L. A., Byrd, R. J., and Chodorow, M. S. 1982 The EPISTLE Text-Critiquing System. IBM Systems Journal 21(3):305-326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hertzberg</author>
<author>A Horz</author>
</authors>
<title>Review of Hammond</title>
<date>1990</date>
<pages>1--61</pages>
<marker>Hertzberg, Horz, 1990</marker>
<rawString>Hertzberg, J., and Horz, A. 1990 Review of Hammond 1988. nnstliche Intelligenz: Forschung und Entwicklung 1:61-62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E H Hovy</author>
</authors>
<title>Generating Natural Language under Pragmatic Constraints.</title>
<date>1987</date>
<tech>Ph.D. thesis, research report 521,</tech>
<institution>Department of Computer Sciences, Yale University,</institution>
<location>New Haven, CT.</location>
<marker>Hovy, 1987</marker>
<rawString>Hovy, E. H. 1987 Generating Natural Language under Pragmatic Constraints. Ph.D. thesis, research report 521, Department of Computer Sciences, Yale University, New Haven, CT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D McDonald</author>
</authors>
<title>Natural Language Generation.</title>
<date>1987</date>
<booktitle>Encyclopedia of Artificial Intelligence.</booktitle>
<pages>642--655</pages>
<editor>In: Shapiro, S., ed.</editor>
<publisher>John Wiley,</publisher>
<location>New York, NY,</location>
<marker>McDonald, 1987</marker>
<rawString>McDonald, D. D. 1987. Natural Language Generation. In: Shapiro, S., ed. Encyclopedia of Artificial Intelligence. John Wiley, New York, NY, 642-655.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K R McKeown</author>
</authors>
<title>Text Generation.</title>
<date>1985</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, U.K.</location>
<marker>McKeown, 1985</marker>
<rawString>McKeown, K. R. 1985 Text Generation. Cambridge University Press, Cambridge, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K R McKeown</author>
<author>W R Swartout</author>
</authors>
<title>Language Generation and Explanation.</title>
<date>1988</date>
<booktitle>Advances in Natural Language Generation: An Interdisciplinary Perspective.</booktitle>
<pages>1--51</pages>
<editor>In: Zock, M. and Sabah, G., eds.</editor>
<publisher>Pinter Publishers,</publisher>
<location>London, U.K.,</location>
<marker>McKeown, Swartout, 1988</marker>
<rawString>McKeown, K. R. and Swartout, W. R. 1988. Language Generation and Explanation. In: Zock, M. and Sabah, G., eds. Advances in Natural Language Generation: An Interdisciplinary Perspective. Pinter Publishers, London, U.K., 1-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Meehan</author>
</authors>
<title>The Metanovel: Writing Stories by Computer.</title>
<date>1976</date>
<publisher>Garland Press,</publisher>
<location>New York, NY.</location>
<marker>Meehan, 1976</marker>
<rawString>Meehan, J. 1976. The Metanovel: Writing Stories by Computer. Garland Press, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M B P Ryan</author>
</authors>
<title>The Computational Codification of the Semantic Aspects of Style.</title>
<date>1989</date>
<tech>Technical report CSRI-231,</tech>
<institution>Computer Systems Research Institute, University of Toronto,</institution>
<location>Toronto, Canada.</location>
<marker>Ryan, 1989</marker>
<rawString>Ryan, M. B. P. 1989 The Computational Codification of the Semantic Aspects of Style. Technical report CSRI-231, Computer Systems Research Institute, University of Toronto, Toronto, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Schank</author>
</authors>
<title>Conceptual Information Processing.</title>
<date>1975</date>
<publisher>North-Holland,</publisher>
<location>Amsterdam, The Netherlands.</location>
<marker>Schank, 1975</marker>
<rawString>Schank, R. C. 1975 Conceptual Information Processing. North-Holland, Amsterdam, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R F Simmons</author>
<author>J Slocum</author>
</authors>
<title>Generating English Discourse from Semantic Networks.</title>
<date>1972</date>
<journal>Communications of the ACM</journal>
<pages>15--10</pages>
<marker>Simmons, Slocum, 1972</marker>
<rawString>Simmons, R. F. and Slocum, J. 1972 Generating English Discourse from Semantic Networks. Communications of the ACM 15(10):891-905.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Small</author>
<author>C Rieger</author>
</authors>
<title>Parsing and Comprehending with Word Experts (A Theory and its Realization).</title>
<date>1982</date>
<booktitle>Strategies for Natural Language Processing. Lawrence Erlbaum Associates,</booktitle>
<pages>89--148</pages>
<editor>In: Lehnert, W. G. and Ringle, M. H., eds.</editor>
<location>Hillsdale, N.J.,</location>
<marker>Small, Rieger, 1982</marker>
<rawString>Small, S. and Rieger, C. 1982 Parsing and Comprehending with Word Experts (A Theory and its Realization). In: Lehnert, W. G. and Ringle, M. H., eds. Strategies for Natural Language Processing. Lawrence Erlbaum Associates, Hillsdale, N.J., 89-148.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Wolfgang</author>
</authors>
<title>Hoeppner is a professor of Computational Linguistics and Artificial Intelligence at the University of Koblenz, West Germany. He holds a doctorate from the University of Hamburg in Computational Linguistics. Formerly engaged in various AI projects (HAM-ANS, WISBER) at the University of Hamburg, his recent research has been in knowledge representation, especially temporal and spatial reasoning, and in NL generation. Hoeppner&apos;s address is:</title>
<date></date>
<institution>University of Koblenz, FB Informatik, Computerlinguistik,</institution>
<location>Rheinau</location>
<marker>Wolfgang, </marker>
<rawString>Wolfgang Hoeppner is a professor of Computational Linguistics and Artificial Intelligence at the University of Koblenz, West Germany. He holds a doctorate from the University of Hamburg in Computational Linguistics. Formerly engaged in various AI projects (HAM-ANS, WISBER) at the University of Hamburg, his recent research has been in knowledge representation, especially temporal and spatial reasoning, and in NL generation. Hoeppner&apos;s address is: University of Koblenz, FB Informatik, Computerlinguistik, Rheinau 3-4, D-5400 Koblenz, West Germany. E-mail: hoeppner@infko.uucp</rawString>
</citation>
<citation valid="true">
<date>1989</date>
<journal>vii +</journal>
<booktitle>(Studies in Theoretical Psycholinguistics 9)</booktitle>
<volume>217</volume>
<pages>pp.</pages>
<editor>BRIEFLY NOTED LEARNABILITY AND LINGUISTIC THEORY Robert J. Matthews and William Demopoulos, eds.</editor>
<publisher>Kluwer,</publisher>
<institution>(Rutgers University and University of Western Ontario)</institution>
<location>Dordrecht:</location>
<marker>1989</marker>
<rawString>BRIEFLY NOTED LEARNABILITY AND LINGUISTIC THEORY Robert J. Matthews and William Demopoulos, eds. (Rutgers University and University of Western Ontario) Dordrecht: Kluwer, 1989, vii + 217 pp. (Studies in Theoretical Psycholinguistics 9)</rawString>
</citation>
<citation valid="false">
<authors>
<author>ISBN Hardbound</author>
</authors>
<journal>Dfl</journal>
<volume>130</volume>
<pages>0--7923</pages>
<marker>Hardbound, </marker>
<rawString>Hardbound, ISBN 0-7923-0247-8, Dfl 130.-, $64.00, £42.00</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>