<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.920396">
Shared Components Topic Models
</title>
<author confidence="0.997056">
Matthew R. Gormley Mark Dredze Benjamin Van Durme Jason Eisner
</author>
<affiliation confidence="0.960285">
Center for Language and Speech Processing
Human Language Technology Center of Excellence
Department of Computer Science
Johns Hopkins University, Baltimore, MD
</affiliation>
<email confidence="0.999457">
{mrg,mdredze,vandurme,jason}@cs.jhu.edu
</email>
<sectionHeader confidence="0.997394" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.992367894736842">
With a few exceptions, extensions to latent
Dirichlet allocation (LDA) have focused on
the distribution over topics for each document.
Much less attention has been given to the un-
derlying structure of the topics themselves. As
a result, most topic models generate topics in-
dependently from a single underlying distri-
bution and require millions of parameters, in
the form of multinomial distributions over the
vocabulary. In this paper, we introduce the
Shared Components Topic Model (SCTM), in
which each topic is a normalized product of a
smaller number of underlying component dis-
tributions. Our model learns these component
distributions and the structure of how to com-
bine subsets of them into topics. The SCTM
can represent topics in a much more compact
representation than LDA and achieves better
perplexity with fewer parameters.
</bodyText>
<sectionHeader confidence="0.999506" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999829553191489">
Topic models are probabilistic graphical models
meant to capture the semantic associations underly-
ing corpora. Since the introduction of latent Dirich-
let allocation (LDA) (Blei et al., 2003), these mod-
els have been extended to account for more complex
distributions over topics, such as adding supervision
(Blei and McAuliffe, 2007), non-parametric priors
(Blei et al., 2004; Teh et al., 2006), topic correla-
tions (Li and McCallum, 2006; Mimno et al., 2007;
Blei and Lafferty, 2006) and sparsity (Williamson et
al., 2010; Eisenstein et al., 2011).
While much research has focused on modeling
distributions over topics, less focus has been given to
the makeup of the topics themselves. This emphasis
leads us to find two problems with LDA and its vari-
ants mentioned above: (1) independently generated
topics and (2) overparameterized models.
Independent Topics In the models above, the top-
ics are modeled as independent draws from a single
underlying distribution, typically a Dirichlet. This
violates the topic modeling community’s intuition
that these distributions over words are often related.
As an example, consider a corpus that supports two
related topics, baseball and hockey. These topics
likely overlap in their allocation of mass to high
probability words (e.g. team, season, game, play-
ers), even though the two topics are unlikely to ap-
pear in the same documents. When topics are gen-
erated independently, the model does not provide a
way to capture this sharing between related topics.
Many extensions to LDA have addressed a related
issue, LDA’s inability to model topic correlation,1
by changing the distributions over topics (Blei and
Lafferty, 2006; Li and McCallum, 2006; Mimno et
al., 2007; Paisley et al., 2011). Yet, none of these
change the underlying structure of the topic’s distri-
butions over words.
Overparameterization Topics are most often
parameterized as multinomial distributions over
words: increasing the topics means learning new
multinomials over large vocabularies, resulting in
models consisting of millions of parameters. This
issue was partially addressed in SAGE (Eisenstein
et al., 2011) by encouraging sparsity in the topics
which are parameterized by their difference in log-
frequencies from a fixed background distribution.
Yet the problem of overparameterization is also tied
</bodyText>
<footnote confidence="0.985235">
1Two correlated topics, e.g. nutrition and exercise, are likely
to co-occur, but their word distributions might not overlap.
</footnote>
<page confidence="0.647555333333333">
783
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 783–792,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.999849032258064">
to the number of topics, and though SAGE reduces
the number of non-zero parameters, it still requires
a vocabulary-sized parameter vector for each topic.
We present the Shared Components Topic Model
(SCTM), which addresses both of these issues by
generating each topic as a normalized product of a
smaller number of underlying components. Rather
than learning each new topic from scratch, we model
a set of underlying component distributions that
constrain topic formation. Each topic can then be
viewed as a combination of these underlying com-
ponents, where in a model such as LDA, we would
say that components and topics stand in a one to one
relationship. The key advantages of the SCTM are
that it can learn and share structure between overlap-
ping topics (e.g. baseball and hockey) and that it can
represent the same number of topics in a much more
compact representation, with far fewer parameters.
Because the topics are products of components,
we present a new training algorithm for the sig-
nificantly more complex product case which re-
lies on a Contrastive Divergence (CD) objective.
Since SCTM topics, which are products of distri-
butions, could be represented directly by distribu-
tions as in LDA, our goal is not necessarily to learn
better topics, but to learn models that are substan-
tially smaller in size and generalize better to unseen
data. Experiments on two corpora show that our
model uses fewer underlying multinomials and still
achieves lower perplexity than LDA, which suggests
that these constraints could lead to better topics.
</bodyText>
<sectionHeader confidence="0.992819" genericHeader="method">
2 Shared Components Topic Models
</sectionHeader>
<bodyText confidence="0.999752">
The Shared Components Topic Model (SCTM) fol-
lows previous topic models in inducing admixture
distributions of topics that are used to generate each
document. However, here each topic multinomial
distribution over words itself results from a normal-
ized product of shared components, each a multino-
mial over words. Each topic selects a subset of com-
ponents. We begin with a review and then introduce
the SCTM.
Latent Dirichlet allocation (LDA) (Blei et al.,
2003) is a probabilistic topic model which defines
a generative process whereby sets of observations
are generated from latent topic distributions. In the
SCTM, we use the same generative process of topic
assignments as LDA, but replace the K indepen-
dently generated topics (multinomials over words)
with products of C components.
</bodyText>
<subsectionHeader confidence="0.584507">
Latent Dirichlet allocation generative process
</subsectionHeader>
<bodyText confidence="0.339247">
For each topic k ∈ {1, ... , K}:
</bodyText>
<equation confidence="0.934714375">
Ok ∼ Dir(0) [draw distribution over words]
For each document m ∈ {1, ... , M}:
Om ∼ Dir(ca) [draw distribution over topics]
For each word n ∈ {1, ... , Nm}:
zmn ∼ Mult(1, Om) [draw topic]
xmn ∼ Ozm% [draw word]
EvE 7--i� I
=1 l cEC Ocv
</equation>
<bodyText confidence="0.986563810810811">
the summation in the denominator is over the vocab-
ulary. In a PoE, each component can overrule the
others by giving low probability to some word. A
PoE can be viewed as a soft intersection of its com-
ponents, whereas a mixture is a soft union.
The Beta-Bernoulli model (Griffiths and
Ghahramani, 2006) is a distribution over binary
matrices with a fixed number of rows and columns.
It is the finite counterpart to the Indian Buffet
Process. In this work, we use the Beta-Bernoulli as
our prior for an unobserved binary matrix B with C
columns and K rows. In the SCTM, each row bk of
the matrix, a binary feature vector, defines a topic
distribution. The binary vector acts as a selector
for the structure of the PoE for that topic. The row
determines which components to include in the
product by which entries bk, are “on” (equal to 1)
in that row. Under Beta-Bernoulli prior, for each
column, a coin with weight 7r, is chosen. For each
entry in the column, the coin is flipped to determine
if the entry is “on” or “off”. This corresponds to
LDA draws each topic 0k independently from a
Dirichlet. The model generates each document m
of length M, by first sampling a distribution over
topics em. Then, for each word n, a topic zmn is
chosen and a word type xmn is generated from that
topic’s distribution over words 0zm,.
A Product of Experts (PoE) model (Hinton,
1999) is the normalized product of the expert dis-
tributions. In the SCTM, each component (an ex-
pert) models an underlying multinomial word dis-
tribution. We let 0, be the parameters of the cth
component, where 0, is the probability of the cth
component generating word v. If the structure of a
PoE included only components c E C in the prod-
uct, it would have the form: p(x|01, ... , 0C) =
IIcEC 0c� where there are C components, and
</bodyText>
<page confidence="0.980645">
784
</page>
<bodyText confidence="0.901473666666667">
the notion that some components are a priori more
likely to be included in topics.
The Beta-Bernoulli model generative process
</bodyText>
<equation confidence="0.9395426">
For each component c E 11, ... , C}: [columns]
πc — Beta(γC , 1) [draw probability of component c]
For each topic k E 11, ... , K}: [rows]
bkc — Bernoulli(πc) [draw whether topic includes cth
component in its PoE]
</equation>
<subsectionHeader confidence="0.98404">
2.1 Shared Components Topic Models
</subsectionHeader>
<bodyText confidence="0.999904727272727">
The Shared Components Topic Model generates
each document just like LDA, the only difference
is the topics are not drawn independently from a
Dirichlet prior. Instead, topics are soft intersections
of underlying components, each of which is a multi-
nomial distribution over words. These components
are combined via a PoE model, and each topic is
constructed according to a length C binary vector
bk; where bkc = 1 includes and bkc = 0 excludes
component c. Stacking the K vectors forms a K xC
matrix; rows correspond to topics and columns to
components. Overlapping topics share components
in common.
Generative process SCTM’s generative process
generates topics and words, but must also generate
the binary matrix. For each of the C shared com-
ponents, we generate a distribution φc over the V
words from a Dirichlet parametrized by β. Next,
we generate a K x C binary matrix using the Beta-
Bernoulli prior. These components and the binary
matrix implicitly define the complete set of K topic
distributions, each of which is a PoE.
</bodyText>
<equation confidence="0.9982875">
p(x |bk, φ) = IICc=� Obcxcbkc (1)
�v=1 l 1c=1 �cv
</equation>
<bodyText confidence="0.9991505">
The distribution p(·|bk, φ) defines the kth topic.
Conditioned on these K topics, the remainder of the
generative process, which generates the documents,
is just like LDA.
</bodyText>
<subsectionHeader confidence="0.818985">
The Shared Components Topic Model generative process
</subsectionHeader>
<equation confidence="0.970489454545454">
For each component c E 11, ... , C}:
dic — Dir(β) [draw distribution over words]
πc — Beta(γC , 1) [draw probability of component c]
For each topic k E 11, ... , K}:
bkc — Bernoulli(πc) [draw whether topic includes cth
component in its PoE]
For each document m E 11, ... , M}
Om — Dir(α) [draw distribution over topics]
For each word n E 11, ... , Nm}
zmn — Mult(1, Om) [draw topic]
xmn — p(· |bzgnn, di) given by Eq. (1) [draw word]
</equation>
<bodyText confidence="0.999457592592593">
See Figure 1 for the graphical model.
Discussion An advantage of this formulation is the
ability to model many topics using few components.
While LDA must maintain V xK parameters for the
topic distributions, the SCTM maintains just V x C
parameters, plus an additional K x C binary matrix.
Since C &lt; K « V this results in many fewer pa-
rameters for the SCTM.2 Extending the number of
topics (rows) requires storing additional binary vec-
tors, a lightweight requirement. In theory, we could
enable all 2C possible component combinations, al-
though we expect to use far less. On the other hand,
constraining the SCTM’s topics by the components
gives less flexible topics as compared to LDA. How-
ever, we find empirically that a large number of top-
ics can be effectively modeled with a smaller num-
ber of components.
Observe that we can reparameterize the SCTM as
LDA by assuming an identity square matrix; each
component corresponds to a topic in LDA, making
LDA a special case of the SCTM with an identity
matrix IC. Intuitively, SCTM learning could pro-
duce an LDA model where appropriate. Finally, we
can also think of the SCTM as learning the struc-
ture of many PoE models. In applications where ex-
perts abstain, the SCTM could learn in which setting
(row) each expert casts a vote.
</bodyText>
<sectionHeader confidence="0.994324" genericHeader="method">
3 Parameter Estimation
</sectionHeader>
<bodyText confidence="0.99991125">
Parameter estimation infers values for model pa-
rameters φ, π, and θ from data using an unsuper-
vised training procedure. Because exact inference
is intractable in the SCTM, we turn to approximate
methods. As is common in these models, we will
integrate out π and θ, sample latent variables Z and
B, and optimize the components φ. Our algorithm
follows the outline of the Monte Carlo EM (MCEM)
algorithm (Wei and Tanner, 1990). In the Monte
Carlo E-step, we will re-sample the latent variables
Z and B based on current model parameters φ and
observed data X. In the M-step, we will find new
model parameters φ. Since these parameters corre-
spond to experts in the PoE, we rely on a contrastive
divergence (CD) objective (Hinton, 2002), popular
for PoE training, rather than maximizing the data
</bodyText>
<footnote confidence="0.993968">
2The vocabulary size V could be much larger if n-grams or
relational triples are used, as opposed to unigrams.
</footnote>
<page confidence="0.998179">
785
</page>
<bodyText confidence="0.998912625">
log-likelihood. Normally, CD only estimates the pa-
rameters of the expert distributions. However, in our
model, the structure of the PoEs themselves change
based on the E-step. Since we generate multiple
samples in the E-step, we modify the CD objective
to compute the gradient for each E-step sample and
take the average to approximate the expectation un-
der B and Z.3
</bodyText>
<subsectionHeader confidence="0.999664">
3.1 E-Step
</subsectionHeader>
<bodyText confidence="0.999991454545455">
The E-step approximates an expectation under
p(B, Z|X, 0, α, ry) for latent topic assignments Z
and matrix B using Gibbs sampling. The Gibbs
sampler uses the full conditionals for both zi (7) and
bkc (12), which we derive in Appendix A. Using this
sampler, we obtain J samples of Z and B by iterat-
ing through each value of zi and bkc J times (in our
experiments, we use J=1, which appears to work as
well on this task as multiple samples). These J sam-
ples are then used in the M-step as an approximation
of the expectation of the latent variables.
</bodyText>
<subsectionHeader confidence="0.999871">
3.2 M-Step
</subsectionHeader>
<bodyText confidence="0.997354675675676">
Given many samples of B and Z, the M-step opti-
mizes the component parameters 0 which cannot be
collapsed out. We utilize the standard PoE training
procedure for experts: contrastive divergence (CD).
We approximate the CD gradient as the difference of
the data distribution and the one-step reconstruction
of the data according to the current parameters. As
in Generalized EM (Dempster et al., 1977), a single
gradient step in the direction of the contrastive di-
vergence objective is sufficient for each M-step. A
key difference in our model is that we must incor-
porate the expectation of the PoE model structure,
which in our case is a random variable instead of a
fixed observed structure. We achieve this by simply
3CD training within MCEM is not the only possible ap-
proach. One alternative would be to compute the CD gradient
summing over all values of B and Z, effectively training the
entire model using CD. This approach prevents the normal CD
objective derivation from being simplified into a more tractable
form. Another approach would be a pure MCMC algorithm,
which sampled 0 directly. While using the natural parameters
allows the sampler to mix, it is too computationally intensive to
be practical. Finally, we could train with Generalized MCEM,
where the exact gradient of the log-likelihood (or log-posterior)
is used, but this easily gets stuck in local minima. After exper-
imenting with these and other options, we present our current
most effective estimation method.
computing the CD gradient for each PoE given each
of the J samples {Z, B}(P from the E-Step, then
average the result.
Another difficulty arises from computing the gra-
dient directly for the multinomial 0c due to the V −1
degrees of freedom imposed by sum-to-one con-
straints. Therefore, we switch to the natural pa-
rameters, which obviates the need for considering
the sum-to-one constraint in the optimization, by
defining 0c in terms of V real valued parameters
</bodyText>
<equation confidence="0.9305895">
{�c1,...,G-V }:
exp(�cv)
0cv = (2)
EV, exp(�cv)
</equation>
<bodyText confidence="0.997882692307692">
The V parameters �cv are then used to compute 0cv
for use in the E-step.
As explained above, the M-step does not maxi-
mize the data log-likelihood, but instead minimizes
contrastive divergence. Hinton (2002) explains that
maximizing data log-likelihood is equivalent to min-
imizing Q0||Q&apos; , the KL divergence between the
observed data distribution, Q0, and the model’s
equilibrium distribution, Q—.4 Minimizing Q0||Q-
would require the computation of an intractable ex-
pectation under the equilibrium distribution. We
avoid this by instead minimizing the contrastive di-
vergence objective,
</bodyText>
<equation confidence="0.641175">
CD(�|{Z,B}(�)) = Q0||Q- − Q1�||Q- , (3)
</equation>
<bodyText confidence="0.999979230769231">
where Q1� is the distribution over one-step recon-
structions of the data, X given Z, B, �, that are gen-
erated by a single step of Gibbs sampling.
Unlike standard applications of CD training, the
hidden variables (Z,B) are not contained within the
experts. Instead they define the structure of the PoE
model, where B indicates which experts to use in
each product (topic) and Z indicates which PoE gen-
erates each word. Unfortunately, CD training cannot
infer this structure since the CD derivation makes
use of a fixed structure in the one-step reconstruc-
tion. Therefore, we have taken a MCEM approach,
first sampling the PoE structure in the E-step, then
</bodyText>
<footnote confidence="0.8218814">
4Hinton (2002) used this notation because the data distribu-
tion, Q°, can be described as the state of a Markov chain at time
0 that was started at the data distribution. Similarly, the equilib-
rium distribution, Q£ could be obtained by running the same
Markov chain to time oo.
</footnote>
<page confidence="0.99482">
786
</page>
<figureCaption confidence="0.999887">
Figure 1: The graphical model for the SCTM.
</figureCaption>
<bodyText confidence="0.995998666666667">
fixing these samples for Z and B when computing
the one-step reconstruction of the data, X.
Contrastive Divergence Gradient We provide
the approximate derivative of the contrastive di-
vergence objective, where Z and B are treated as
fixed.5
</bodyText>
<equation confidence="0.97240025">
dCD(�|{Z,B}(j)) ≈ − /dlog f(x|bz,O))Q0 &lt; (\ &lt;
+ / d log f (x  |bz , O)
\ d )Q1
ξ
</equation>
<bodyText confidence="0.990622">
where f(x|bz, O) = HC c=1 Obzc
cx is the numerator of
p(x|bz,O) and the derivative of its log is efficient to
compute:
To approximate the expectation under Q1ξ, we hold
Z, B, � fixed and resample the data, X, using one
step of Gibbs sampling.
</bodyText>
<subsectionHeader confidence="0.996814">
3.3 Summary
</subsectionHeader>
<bodyText confidence="0.945723285714286">
Our learning algorithm can be viewed
in terms of a Q function: Q(�|�(t)) ≈
1 Ej=1 CD(�|{Z, B}(j))where we average over
J samples. The E-step computes Q(�|�(t)). The
M-step minimizes Q with respect to � to obtain the
updated �(t+1) by performing gradient descent on
the Q function as �(t+1)
</bodyText>
<equation confidence="0.836823">
cv =�(t)
cv − �7 · d Q(ξ|ξ(t)) dξcvfor
</equation>
<bodyText confidence="0.841843">
all values of c, v.
</bodyText>
<footnote confidence="0.5868205">
dd Q1 , which is ‘problematic to compute’ (Hinton,
2002). This is the standard use of CD.
</footnote>
<figure confidence="0.860099">
Algorithm 1 SCTM Training
Initialize parameters: ξc, bkc, zi.
while not converged do
{E-step:}
for j = 1 to J do
{Draw jth sample {Z, B}(j)}
for i = 1 to N do
Sample zi using Eq. (7)
fork = 1 to K do
forc= 1 to C do
Sample bkc using ratio in Eq. (12)
{M-step:}
for c = 1toCdo
for v = 1 to V do
Single gradient step over ξ
cv − η ·d Q(0|0(t))
ξ(t+1)
cv = ξ(t)
dξcv
</figure>
<sectionHeader confidence="0.998195" genericHeader="method">
4 Related Models
</sectionHeader>
<bodyText confidence="0.991052161290322">
The SCTM is closely related to the the Infinite
Overlapping Mixture Model (IOMM) (Heller and
Ghahramani, 2007), yet our model differs from and,
in some ways, extends theirs. The IOMM mod-
els the geometric overlap of Gaussian clusters us-
ing PoEs, and models the structure of the PoEs with
the rows of a binary matrix. The SCTM models a
finite number of columns, where the IOMM mod-
els an infinite number. The IOMM generates a row
for each data point, whereas the SCTM generates a
row for each topic. Thus, the SCTM goes beyond
the IOMM by allowing the rows to be shared among
documents and models document-specific mixtures
over the rows of the matrix.6
SAGE for topic modeling (Eisenstein et al., 2011)
can be viewed as a restricted form of the SCTM.
Consider an SCTM in which the binary matrix is re-
stricted such that the first column, b·,1, consists of
all ones and the remainder forms a diagonal matrix.
If we then set the first component, 01, to the cor-
pus background distribution, and add a Laplace prior
on the natural parameters, acv, we have the SAGE
model. Note that by removing the restriction that
the matrix contain a diagonal, we could allow mul-
tiple components to combine in the SCTM fashion,
while incorporating SAGE’s sparsity benefits.
6The IOMM uses Metropolis-Hastings (MH) to sample the
parameters of the experts. This approach is computationally
feasible because their experts are Gaussian, unlike the SCTM
in which the experts are multinomials and the MH step too ex-
pensive.
</bodyText>
<figure confidence="0.97628155">
xmn
zmn
9m
Ct
Nm
M
bkc
0c
7rc
�
K
C
N
d log f(x|bz, O)
d�cv
�
bzc(1 − Ocv) for x = v
−bzcOcv for x =6
=
v
</figure>
<footnote confidence="0.495853333333333">
5The derivative is approximate because we drop the term:
d Q1
dξ
</footnote>
<page confidence="0.977959">
787
</page>
<bodyText confidence="0.999920391304348">
The relation of TagLDA (Zhu et al., 2006) to
the SCTM is similar to that of SAGE and SCTM.
TagLDA has a PoE of exactly two experts: one ex-
pert for the topic, and one for the supervised word-
level tag. Examples of tags are abstract or body,
indicating which part of a research paper the word
appears in.
Unlike the SCTM and SAGE, most prior exten-
sions to LDA have enhanced the distribution over
topics for each document. One of the closest is hier-
archical LDA (hLDA) (Blei et al., 2004) and its ap-
plication to PAM (Mimno et al., 2007). Though top-
ics are still generated independently from a Dirich-
let prior, hLDA learns a tree structure underlying
the topics. Each document samples a single path
through the tree and samples words from topics
along that path. The SCTM models an orthogonal
issue to topic hierarchy: how the topics themselves
are represented as the intersection of components.
Finally, while prior work has primarily used mix-
tures for the sake of conjugacy, we take a fundamen-
tally different approach to modeling the structure by
using normalized product distributions.
</bodyText>
<sectionHeader confidence="0.999225" genericHeader="method">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.9999135">
We compare the SCTM with LDA in terms of over-
all model performance (held-out perplexity) as well
as parameter usage (varying numbers of components
and topics). We select LDA as our baseline since our
model differs only in how it forms topics, which fo-
cuses evaluation on the benefit of this model change.
We consider two popular data sets for compar-
ison: NIPS: A collection of 1,617 NIPS abstracts
from 1987 to 19997, with 77,952 tokens and 1,632
types. 20NEWS: 1,000 randomly selected articles
from the 20 Newsgroups dataset,8 with 70,011 to-
kens and 1,722 types. Both data sets excluded stop
words and words occurring in fewer than 10 docu-
ments. For 20NEWS, we used the standard by-date
train/test split. For NIPS, we randomly partitioned
the data by document into 75% train and 25% test.
We compare the SCTM to LDA by evaluating
the average perplexity-per-word of the held-out test
</bodyText>
<footnote confidence="0.999054">
7We follow prior work (Blei et al., 2004; Li and Mc-
Callum, 2006; Li et al., 2007) in using only the abstracts:
http://www.cs.nyu.edu/˜roweis/data.html
8Williamson et al. (2010) created a similar subset:
http://people.csail.mit.edu/jrennie/20Newsgroups/
</footnote>
<bodyText confidence="0.999862666666667">
data, perplexity = 2− lo92(data|model)/iv. Exact com-
putation is intractable, so we use the left-to-right al-
gorithm (Wallach et al., 2009) as an accurate alter-
native. With the topics fixed, the SCTM is equiva-
lent to LDA and requires no adaptation of the left-
to-right algorithm.
We used a collapsed Gibbs sampler for training
LDA and the algorithm described above for training
the SCTM. Both were trained for 4000 iterations,
sampling topics every 10 iterations after a burn-in of
3000. The hyperparameter α was optimized as an
asymmetric Dirichlet, ,3 as a symmetric Dirichlet,
and ry = 3.0 was fixed.9 Following the observation of
Hinton (2002) that CD training benefits from initial-
izing the experts to nearly uniform distributions, we
initialize the component distributions from a sym-
metric Dirichlet with parameter, = 1×106. We use
J = 1 samples per iteration and a decaying learning
rate centered at q = 100.10 We ranged LDA from 10
to 200 topics, and the SCTM from 10 to 100 com-
ponents (C). We then selected the number of SCTM
topics (K) as K E {C, 2C, 3C, 4C, 5C}. For each
model, we used five random restarts, selecting the
model with the highest training data likelihood.
</bodyText>
<sectionHeader confidence="0.73205" genericHeader="evaluation">
5.1 Results
</sectionHeader>
<bodyText confidence="0.998448684210526">
Our goal is to demonstrate that (1) modeling topics
as products of components is an expressive alterna-
tive to generating topics independently and (2) the
SCTM can both achieve lower perplexity than LDA
and use fewer model parameters in doing so.
Topics as Products of Components Figures 3b
and 3c show the perplexity for the held-out portions
of 20NEWS and NIPS for different numbers of com-
ponents C. The shaded region shows the full SCTM
perplexity range we observed for different K and
at each value of C, we label the number of topics
K (rows in the binary matrix). For each number of
components, LDA falls within the upper portion of
the shaded region. While for some (small) values of
K for the SCTM, LDA does better, the SCTM can
easily include more K (requiring few new param-
eters) to achieve better results. This supports our
hypothesis that topics can be comprised of the over-
lap between shared underlying components. More-
</bodyText>
<footnote confidence="0.998874">
9On development data the model was rather insensitive to -y.
10We experimented with larger J but it had no effect.
</footnote>
<page confidence="0.987375">
788
</page>
<figure confidence="0.999657">
(b)
(b) (c) (d)
(c)
(d)
</figure>
<figureCaption confidence="0.988509166666667">
Figure 3: Perplexity results on held-out data for 20NEWS (b) and NIPS (c) showing the results of LDA and the SCTM
for the same number of components and varying K (SCTM). For the same number of components (multinomials), the
SCTM achieves lower perplexity by combining them into more topics. Results for 20NEWS (a) and NIPS (d) showing
non-square SCTM achieves lower perplexity than LDA with a more compact model.
Figure 2: SCTM binary matrix and topics from 3599 training documents of 20NEWS for C = 10, K = 20. Blue
squares are “on” (equal to 1).
</figureCaption>
<figure confidence="0.99927745505618">
(a)
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
k αk Top words for topic Top words for topic after ablating component c=1
1 0.306 subject organization israel return define law org organization subject israel law peace define israeli
2 0.031 encryption chip clipper keys des escrow security law administration president year market money senior
3 0.025 turkish armenian armenians war turkey turks armenia years food center year air russian war army
4 0.102 drive card disk scsi hard controller mac drives opinions drive hard power support cost research price
5 0.071 image jpeg window display code gif color mit pitt file program year center programs image division
6 0.018 jews israeli jewish arab peace land war arabs
7 0.074 org money back question years thing things point
8 0.106 christian bible church question christ christians life
1400 10,4 11 0,30 ● LDA SCTM
●
0,20 10 ●
20,40 021 ●
9 0.011 administration president year market money senior
10 0.055 health medical center research information april
11 0.063 gun law state guns control bill rights states
10,50 40,80 40 ●
20,60 60●
20,80
20,100
12 0.160 world organization system israel state usa cwru reply
13 0.042 space nasa ov launch ower wire round air
P g P g
erplex. 40,12060, 120 18&amp;0,160 100 ● 120 ●
� 0 80●
o 0
14 0.038 1 n
space nasa gov launch power wire ground air
15 0.079 team game year play games season players hockey
16 0.158 car lines dod bike good uiuc sun cars
P1000 40,20 60.900�80 ,240 2$�•200 +40
606O 100,300
w 320
80,400100,i%,500
17 0.136 windows file government key jesus system program
18 0.122 article writes center page harvard virginia research
800 0 100 200 300 400 500
# of Model Paramet­ (thou ­nds)
600
19 0.017 max output access digex int entry col line
20 0.380 lines ole don university posting host nn time
people tYP g tP
1800
1600
Perplexity
1400
1200
1000
800
10
x
● LDA
SCTM
20 ●● 20
30
40 40● ● 40
50 60 60
80 80 ● 80
100 ●
120 100
120 ●
160 ●
200 160
180 200
240 240
300 300
320
4005400
10
10
11
0,20
●
700
● LDA
SCTM
●
● LDA
SCTM
600
0,30
0,40021
10,50
20,40
●
●
● ●
20
550
600
20
30
40
50
●●
Perplexity
Perplexity
20,60
500
40
40
●
40
20,8040,80
500
60
450
●
60
20,100
●
80
80
60
80
●
100
●
400
100
●
120
160
200
80
120
400
●
100
350
180
240
300
160
240
●
2300
400
500
300
320
400
300
40,12060,120
40,160
40,200
60,180
80,160
60,240 80
120 140 160
●
●
,240 ●
60,30 180 ●
100,200100,300100,400
80,320 200
80,400 100,500
</figure>
<bodyText confidence="0.993877333333333">
over, this suggests that our products (PoEs) provide
additional and complementary expressivity over just
mixtures of topics.
Model Compactness Including an additional
topic in the SCTM only adds C binary parameters,
for an extra row in the matrix. Whereas in LDA, an
additional topic requires V (the size of the vocab-
ulary) additional parameters to represent the multi-
nomial. In both cases, the number of document-
specific parameters must increase as well. Figures
3a and 3d present held-out perplexity vs. number
of model parameters on 20NEWS and NIPS, exclud-
ing the case of square (C = K) binary matrices for
the SCTM. The regions show a confidence inter-
val (p = 0.05) around the smoothed fit to the data,
</bodyText>
<page confidence="0.99151">
789
</page>
<bodyText confidence="0.999957285714286">
LDA labels show C, and SCTM labels show C, K.
The SCTM achieves lower perplexity with fewer
model parameters, even when the increase in non-
component parameters is taken into account. We ex-
pect that because of its smaller size the SCTM ex-
hibits lower sample complexity, allowing for better
generalization to unseen data.
</bodyText>
<subsectionHeader confidence="0.999338">
5.2 Analysis
</subsectionHeader>
<bodyText confidence="0.999803666666667">
Figure 2 gives the binary matrix and topics learned
on a larger section of 20NEWS training documents.
These topics evidence that the SCTM is able to
achieve a diversity of topics by combining various
subsets of components, and we expect that the low
perplexity achieved by the SCTM can be attributed
</bodyText>
<figure confidence="0.999313466321243">
c=9
visual image
images cells
cortex scene
support spatial
feature vision
cues stimulus
statistics
bayesian
results show
estimation
method based
parameters
likelihood
methods models
k=4 αk=0.12
algorithm
training error
function method
performance
input
classification
classifier
k=8 αk=0.23
paper units
output layer
networks
patterns unit
pattern set rule
network rules
weights training
c=4
information
analysis
component rules
signal
independent
representations
noise basis
k=18 αk=0.07
network
networks data
learning optimal
linear vector
independent
binary natural
algorithms pca
c=2
k=3 αk=0.06
object
recognition
system objects
information
visual matching
problem based
classification
models images
image problem
structure
analysis mixture
clustering
approach show
computational
k=14 αk=0.07
k=9 αk=0.02
vector feature
classification
support vectors
kernel
regression
weight inputs
dimensionality
c=1
model
information
parameters
kalman robust
matrices
likelihood
experimentally
k=5 αk=0.04
object
recognition
system objects
information
visual matching
problem based
classification
training units
paper hidden
number output
problem rule set
order unit show
present method
weights task
k=16 αk=0.11
problem state
control
reinforcement
problems models
time based
decision markov
systems function
k=12 αk=0.13
model learning
system
information
parameters
networks robust kalman
rules
estimation
k=1 αk=0.11
learning
networks system
recognition time
network
describes hand
context views
classification
k=11 αk=0.08
networks
network learning
str
diibuted
system weight
vectors property
binary point
optimal real
k=13 αk=0.05
neural neurons
analog synaptic
neuron networks
memory time
capacity model
associative
noise dynamics
k=10 αk=0.09
k=19 αk=0.03
system networks
set neurons
visual phase
feature
processing
features output
associative
data paper
networks network
output feature
features
patterns set
train introduced
unit functions
k=7 αk=0.08
number
functions
weights function
layer
generalization
error results
loss linear size
network input
information time
recurrent back
propagation
units
architecture
forward layer
k=17 αk=0.10
k=2 αk=0.13
k=20 αk=0.02
time network
weights
activation delay
current chaotic
connected
discrete
connections
cells neurons
visual cortex
motion response
processing
spatial cell
properties
patterns spike
k=6 αk=0.23
k=15 αk=0.12
neural network
paper
recognition
speech systems
based results
performance
artificial
</figure>
<figureCaption confidence="0.94842">
Figure 4: Hasse diagram on NIPS for C = 10, K = 20 showing the top words for topics and unrepresented com-
ponents (in shaded box). Notice that some topics only consist of a single component. The shaded box contains the
components that didn’t appear as a topic. For the sake of clarity, we only show arrows for the subsumption rela-
tionships between the topics, and we omit the implicit arrows between the components in the shaded box and the
topics.
</figureCaption>
<bodyText confidence="0.998497818181818">
to the high-level of component re-use across topics.
Topics are typically interpreted by looking at the
top-N words, whereas the top-N words of a compo-
nent often do not even appear in the topics to which
it contributes. Instead, we find that the components
contribution to a topic is typically through vetoing
words. For example, the top words of component
c=1, corresponding to the first column of the binary
matrix in figure 2, are [subject organization posting apple mit
screen write window video port], yet only a few of these ap-
pear in topics k=1,2,3,4,5, which use it.
On the right of figure 2, we show what the top-
ics become when we ablate component c=1 from
the matrix by setting the column to all zeros. Topic
k=2 changes from being about information security
to general politics and is identical to k=9. Topic k=3
changes from the Turkish-Armenian War to a more
general war topic. Topic k=4 changes to a less fo-
cused version of itself. In this way, we can gain fur-
ther insight into the contribution of this component,
and the way in which components tend to increase
the specificity of a topic to which they are added.
The SCTM learns each topic as a soft intersec-
tion of its components, as represented by the binary
matrix. We can describe the overlap between topics
based on the components that they have in common.
One topic subsumes another topic when the parent
consists of a subset of the child’s components. In
this way, the binary matrix defines a Hasse diagram,
a directed acyclic graph describing all the subsump-
tion relationships between topics. Figure 4 shows
such a Hasse diagram on the NIPS data. Several top-
ics consist of only a single component, such as k=12
on reinforcement learning and k=8 on optimization.
These two topics combine with the component c=1
so that their overlap forms the topic k=4 on Bayesian
methods. These subsumption relationships are dif-
ferent from and complementary to hLDA (see §4),
which models topic co-occurrence, not component
intersection. For example, topic k=10 on connec-
tionism and k=2 on neural networks intersect to
form k=20 which contains words that would only
appear in both of its subsuming topics, thereby ex-
plicitly modeling topic overlap.
</bodyText>
<page confidence="0.983072">
790
</page>
<bodyText confidence="0.9998668">
The SCTM sometimes learns identical topics (two
rows with the same binary entries “on”) such as
k=13 and k=14 in figure 2 and k=3 and k=5 in fig-
ure 4, which is likely due to the Gibbs sampler for
the binary matrix getting stuck in a local optimum.
</bodyText>
<sectionHeader confidence="0.999719" genericHeader="discussions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999984">
We have presented the Shared Components Topic
Model (SCTM), in which topics are products of
underlying component distributions. This model
change learns shared topic structures—as expressed
through components—as opposed to generating
each topic independently. Reducing the number of
components yields more compact models with lower
perplexity than LDA. The two main limitations of
the current SCTM are, when restricted to a square
binary matrix (C = K), the inference procedure is
unable to recover a model with perplexity as low as
a collapsed Gibbs sampler for LDA, and the compo-
nents are not consistently interpretable.
The use of components opens up interesting di-
rections of research. For example, task specific side
information can be expressed as priors or constraints
over the components, or by adding conditioning
variables tied to the components. Additionally, tasks
beyond document modeling may benefit from repre-
senting topics as products of distributions. For ex-
ample, in vision, where topics are classes of objects,
the components could be features of those objects.
For selectional preference, components could cor-
respond to semantic features that intersect to define
semantic classes (Gormley et al., 2011). We hope
new opportunities will arise as this work explores a
new research area for topic models.
</bodyText>
<sectionHeader confidence="0.978323" genericHeader="acknowledgments">
Appendix A: Derivation of Full Conditionals
</sectionHeader>
<bodyText confidence="0.976257">
The model’s complete data likelihood over all
variables—observed words X, latent topic assign-
ments Z, matrix B, and component/expert distribu-
tions φ:
</bodyText>
<equation confidence="0.6501085">
p(X, Z, B, φ|α,β, &apos;y) =
p(X|Z, B, φ)p(Z|α)p(B|&apos;y)p(φ|β) (4)
</equation>
<bodyText confidence="0.962361709677419">
This follows from the conditional independence as-
sumptions. It is tractable to integrate out all parame-
ters except Z, B, φ and hyperparameters α, β, &apos;y. 11
11For simplicity, we switch from indexing examples as x,,,,,,,
to xi. In this presentation, xi is the ith example in the corpus,
Full conditional of zi Recall that p(Z|α) is
the Dirichlet-Multinomial distribution over topic
assignments, where θ has been integrated out.
The form of this distribution is identical to the
corresponding distribution over topics in LDA.
The derivation of the full conditional of zi ∈
{1, ... , K}, follows from the factorization in Eq. 4:
p(zi|X, Z−(i),B,φ, α,β, &apos;y)
∝ p(X|Z, B, φ)p(Z|α)
∝ p(xi|bzi,φ)(�n−(i)
mzi + αzi)
Z−(i) is the set of all topic assignments except zi.
We use the independence of each document, recall-
ing that example i belongs to document m. In prac-
tice, we cache p(x|bz, φ) for all x, z (V ×K values)
and these are shared by all zi in a sampling iteration.
Above, just as in LDA, p(Z|α) is simplified by
proportionality to (�n−(i)
mzi + αzi), where �n−(i)
mk is the
count of examples for document m that are assigned
topic k excluding zi’s contribution (Heinrich, 2008).
Full conditional of bkc Recall that p(B|&apos;y) is the
prior for a Beta-Bernoulli matrix. The full condi-
tional distribution of a position in the binary vector
is (Griffiths and Ghahramani, 2006):
</bodyText>
<equation confidence="0.987233333333333">
(8)
K + γ
C
</equation>
<bodyText confidence="0.996686166666667">
where �n−(k)
c is the count of topics with component
c excluding topic k, and B−(kc) is the entire matrix
except for the entry bkc.
To find the full conditional for bkc ∈ {0, 1}, we
again start with the factorization from Eq. 4.
</bodyText>
<equation confidence="0.9922305">
p(bkc|X, Z, B−(kc), φ, α,β, &apos;y) (9)
∝ p(X|Z, B, φ)p(B|&apos;y) (10)
Y p(xi |bzi, φ)# p(bkc |B−(kc), &apos;y) (11)
Lzi=k
</equation>
<bodyText confidence="0.606695">
where p(bkc|B−(kc),&apos;y) is given by Eq. 8,
</bodyText>
<equation confidence="0.981326">
bk.
77�77V nkv
11v=1 �cv
V 7�7 c bki  ||�nk ||1
(Pv=1 llj=1 �jv )
(12)
</equation>
<bodyText confidence="0.96155575">
and where nkv is the count of words assigned topic
k that are type v, and ||nk||1 (the L1-norm of count
vector nk) is the count of all words with topic k.
which corresponds to some m, n pair.
</bodyText>
<equation confidence="0.8025354">
p(bkc = 1|B−(kc),&apos;y) =
+ γ
C
n −(k)
c
⎡
⎢ ⎣
=
⎤
⎦ ⎥p(bkc|B−(kc), &apos;y)
</equation>
<page confidence="0.994609">
791
</page>
<sectionHeader confidence="0.998188" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999797064102564">
David Blei and John Lafferty. 2006. Correlated topic
models. In Advances in Neural Information Process-
ing Systems (NIPS), volume 18.
David Blei and Jon McAuliffe. 2007. Supervised topic
models. In Advances in Neural Information Process-
ing Systems (NIPS).
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent dirichlet allocation. Journal of Machine Learning
Research, 3.
David Blei, Thomas Griffiths, Michael Jordan, and
Joshua Tenenbaum. 2004. Hierarchical topic models
and the nested chinese restaurant process. In Advances
in Neural Information Processing Systems (NIPS), vol-
ume 16.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological), 39(1):1–38.
Jacob Eisenstein, Amr Ahmed, and Eric P. Xing. 2011.
Sparse additive generative models of text. In Interna-
tional Conference on Machine Learning (ICML).
Matthew R. Gormley, Mark Dredze, Benjamin Van
Durme, and Jason Eisner. 2011. Shared components
topic models with application to selectional prefer-
ence. In Learning Semantics Workshop at NIPS 2011,
December.
Thomas Griffiths and Zoubin Ghahramani. 2006. Infinite
latent feature models and the indian buffet process. In
Advances in Neural Information Processing Systems
(NIPS), volume 18.
Gregor Heinrich. 2008. Parameter estimation for text
analysis. Technical report, Fraunhofer IGD.
Katherine A. Heller and Zoubin Ghahramani. 2007. A
nonparametric bayesian approach to modeling over-
lapping clusters. In Artificial Intelligence and Statis-
tics (AISTATS), pages 187–194.
Geoffrey Hinton. 1999. Products of experts. In In-
ternational Conference on Artificial Neural Networks
(ICANN).
Geoffrey Hinton. 2002. Training products of experts by
minimizing contrastive divergence. Neural Computa-
tion, 14(8):1771–1800.
Wei Li and Andrew McCallum. 2006. Pachinko alloca-
tion: DAG-structured mixture models of topic correla-
tions. In International Conference on Machine Learn-
ing (ICML), pages 577–584.
Wei Li, David Blei, and Andrew McCallum. 2007. Non-
parametric bayes pachinko allocation. In Uncertainty
in Artificial Intelligence (UAI).
David Mimno, Wei Li, and Andrew McCallum. 2007.
Mixtures of hierarchical topics with pachinko alloca-
tion. In International Conference on Machine Learn-
ing (ICML), pages 633–640.
John Paisley, Chong Wang, and David Blei. 2011. The
discrete infinite logistic normal distribution for Mixed-
Membership modeling. In International Conference
on Artificial Intelligence and Statistics (AISTATS).
Yee Whye Teh, Michael Jordan, Matthew Beal, and
David Blei. 2006. Hierarchical dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566–1581.
Hanna Wallach, Ian Murray, Ruslan Salakhutdinov, and
David Mimno. 2009. Evaluation methods for topic
models. In International Conference on Machine
Learning (ICML), pages 1105–1112.
Greg Wei and Martin Tanner. 1990. A monte carlo im-
plementation of the EM algorithm and the poor man’s
data augmentation algorithms. Journal of the Ameri-
can Statistical Association, 85(411):699–704.
Sinead Williamson, Chong Wang, Katherine Heller, and
David Blei. 2010. The IBP compound dirichlet
process and its application to focused topic model-
ing. In International Conference on Machine Learn-
ing (ICML).
Xiaojin Zhu, David Blei, and John Lafferty. 2006.
TagLDA: bringing document structure knowledge into
topic models. Technical Report TR-1553, University
of Wisconsin.
</reference>
<page confidence="0.997405">
792
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.880989">
<title confidence="0.999408">Shared Components Topic Models</title>
<author confidence="0.999991">Matthew R Gormley Mark Dredze Benjamin Van_Durme Jason Eisner</author>
<affiliation confidence="0.97261125">Center for Language and Speech Human Language Technology Center of Department of Computer Johns Hopkins University, Baltimore,</affiliation>
<abstract confidence="0.99898965">With a few exceptions, extensions to latent Dirichlet allocation (LDA) have focused on the distribution over topics for each document. Much less attention has been given to the underlying structure of the topics themselves. As a result, most topic models generate topics independently from a single underlying distribution and require millions of parameters, in the form of multinomial distributions over the vocabulary. In this paper, we introduce the Shared Components Topic Model (SCTM), in which each topic is a normalized product of a smaller number of underlying component distributions. Our model learns these component distributions and the structure of how to combine subsets of them into topics. The SCTM can represent topics in a much more compact representation than LDA and achieves better perplexity with fewer parameters.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>John Lafferty</author>
</authors>
<title>Correlated topic models.</title>
<date>2006</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS),</booktitle>
<volume>18</volume>
<contexts>
<context position="1637" citStr="Blei and Lafferty, 2006" startWordPosition="242" endWordPosition="245">SCTM can represent topics in a much more compact representation than LDA and achieves better perplexity with fewer parameters. 1 Introduction Topic models are probabilistic graphical models meant to capture the semantic associations underlying corpora. Since the introduction of latent Dirichlet allocation (LDA) (Blei et al., 2003), these models have been extended to account for more complex distributions over topics, such as adding supervision (Blei and McAuliffe, 2007), non-parametric priors (Blei et al., 2004; Teh et al., 2006), topic correlations (Li and McCallum, 2006; Mimno et al., 2007; Blei and Lafferty, 2006) and sparsity (Williamson et al., 2010; Eisenstein et al., 2011). While much research has focused on modeling distributions over topics, less focus has been given to the makeup of the topics themselves. This emphasis leads us to find two problems with LDA and its variants mentioned above: (1) independently generated topics and (2) overparameterized models. Independent Topics In the models above, the topics are modeled as independent draws from a single underlying distribution, typically a Dirichlet. This violates the topic modeling community’s intuition that these distributions over words are </context>
</contexts>
<marker>Blei, Lafferty, 2006</marker>
<rawString>David Blei and John Lafferty. 2006. Correlated topic models. In Advances in Neural Information Processing Systems (NIPS), volume 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Jon McAuliffe</author>
</authors>
<title>Supervised topic models.</title>
<date>2007</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="1487" citStr="Blei and McAuliffe, 2007" startWordPosition="217" endWordPosition="220">nderlying component distributions. Our model learns these component distributions and the structure of how to combine subsets of them into topics. The SCTM can represent topics in a much more compact representation than LDA and achieves better perplexity with fewer parameters. 1 Introduction Topic models are probabilistic graphical models meant to capture the semantic associations underlying corpora. Since the introduction of latent Dirichlet allocation (LDA) (Blei et al., 2003), these models have been extended to account for more complex distributions over topics, such as adding supervision (Blei and McAuliffe, 2007), non-parametric priors (Blei et al., 2004; Teh et al., 2006), topic correlations (Li and McCallum, 2006; Mimno et al., 2007; Blei and Lafferty, 2006) and sparsity (Williamson et al., 2010; Eisenstein et al., 2011). While much research has focused on modeling distributions over topics, less focus has been given to the makeup of the topics themselves. This emphasis leads us to find two problems with LDA and its variants mentioned above: (1) independently generated topics and (2) overparameterized models. Independent Topics In the models above, the topics are modeled as independent draws from a </context>
</contexts>
<marker>Blei, McAuliffe, 2007</marker>
<rawString>David Blei and Jon McAuliffe. 2007. Supervised topic models. In Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Andrew Ng</author>
<author>Michael Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<contexts>
<context position="1345" citStr="Blei et al., 2003" startWordPosition="195" endWordPosition="198">this paper, we introduce the Shared Components Topic Model (SCTM), in which each topic is a normalized product of a smaller number of underlying component distributions. Our model learns these component distributions and the structure of how to combine subsets of them into topics. The SCTM can represent topics in a much more compact representation than LDA and achieves better perplexity with fewer parameters. 1 Introduction Topic models are probabilistic graphical models meant to capture the semantic associations underlying corpora. Since the introduction of latent Dirichlet allocation (LDA) (Blei et al., 2003), these models have been extended to account for more complex distributions over topics, such as adding supervision (Blei and McAuliffe, 2007), non-parametric priors (Blei et al., 2004; Teh et al., 2006), topic correlations (Li and McCallum, 2006; Mimno et al., 2007; Blei and Lafferty, 2006) and sparsity (Williamson et al., 2010; Eisenstein et al., 2011). While much research has focused on modeling distributions over topics, less focus has been given to the makeup of the topics themselves. This emphasis leads us to find two problems with LDA and its variants mentioned above: (1) independently </context>
<context position="5871" citStr="Blei et al., 2003" startWordPosition="902" endWordPosition="905">ltinomials and still achieves lower perplexity than LDA, which suggests that these constraints could lead to better topics. 2 Shared Components Topic Models The Shared Components Topic Model (SCTM) follows previous topic models in inducing admixture distributions of topics that are used to generate each document. However, here each topic multinomial distribution over words itself results from a normalized product of shared components, each a multinomial over words. Each topic selects a subset of components. We begin with a review and then introduce the SCTM. Latent Dirichlet allocation (LDA) (Blei et al., 2003) is a probabilistic topic model which defines a generative process whereby sets of observations are generated from latent topic distributions. In the SCTM, we use the same generative process of topic assignments as LDA, but replace the K independently generated topics (multinomials over words) with products of C components. Latent Dirichlet allocation generative process For each topic k ∈ {1, ... , K}: Ok ∼ Dir(0) [draw distribution over words] For each document m ∈ {1, ... , M}: Om ∼ Dir(ca) [draw distribution over topics] For each word n ∈ {1, ... , Nm}: zmn ∼ Mult(1, Om) [draw topic] xmn ∼ </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David Blei, Andrew Ng, and Michael Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Thomas Griffiths</author>
<author>Michael Jordan</author>
<author>Joshua Tenenbaum</author>
</authors>
<title>Hierarchical topic models and the nested chinese restaurant process.</title>
<date>2004</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS),</booktitle>
<volume>16</volume>
<contexts>
<context position="1529" citStr="Blei et al., 2004" startWordPosition="223" endWordPosition="226">s these component distributions and the structure of how to combine subsets of them into topics. The SCTM can represent topics in a much more compact representation than LDA and achieves better perplexity with fewer parameters. 1 Introduction Topic models are probabilistic graphical models meant to capture the semantic associations underlying corpora. Since the introduction of latent Dirichlet allocation (LDA) (Blei et al., 2003), these models have been extended to account for more complex distributions over topics, such as adding supervision (Blei and McAuliffe, 2007), non-parametric priors (Blei et al., 2004; Teh et al., 2006), topic correlations (Li and McCallum, 2006; Mimno et al., 2007; Blei and Lafferty, 2006) and sparsity (Williamson et al., 2010; Eisenstein et al., 2011). While much research has focused on modeling distributions over topics, less focus has been given to the makeup of the topics themselves. This emphasis leads us to find two problems with LDA and its variants mentioned above: (1) independently generated topics and (2) overparameterized models. Independent Topics In the models above, the topics are modeled as independent draws from a single underlying distribution, typically </context>
<context position="20719" citStr="Blei et al., 2004" startWordPosition="3529" endWordPosition="3532">og f(x|bz, O) d�cv � bzc(1 − Ocv) for x = v −bzcOcv for x =6 = v 5The derivative is approximate because we drop the term: d Q1 dξ 787 The relation of TagLDA (Zhu et al., 2006) to the SCTM is similar to that of SAGE and SCTM. TagLDA has a PoE of exactly two experts: one expert for the topic, and one for the supervised wordlevel tag. Examples of tags are abstract or body, indicating which part of a research paper the word appears in. Unlike the SCTM and SAGE, most prior extensions to LDA have enhanced the distribution over topics for each document. One of the closest is hierarchical LDA (hLDA) (Blei et al., 2004) and its application to PAM (Mimno et al., 2007). Though topics are still generated independently from a Dirichlet prior, hLDA learns a tree structure underlying the topics. Each document samples a single path through the tree and samples words from topics along that path. The SCTM models an orthogonal issue to topic hierarchy: how the topics themselves are represented as the intersection of components. Finally, while prior work has primarily used mixtures for the sake of conjugacy, we take a fundamentally different approach to modeling the structure by using normalized product distributions. </context>
<context position="22256" citStr="Blei et al., 2004" startWordPosition="3786" endWordPosition="3789"> consider two popular data sets for comparison: NIPS: A collection of 1,617 NIPS abstracts from 1987 to 19997, with 77,952 tokens and 1,632 types. 20NEWS: 1,000 randomly selected articles from the 20 Newsgroups dataset,8 with 70,011 tokens and 1,722 types. Both data sets excluded stop words and words occurring in fewer than 10 documents. For 20NEWS, we used the standard by-date train/test split. For NIPS, we randomly partitioned the data by document into 75% train and 25% test. We compare the SCTM to LDA by evaluating the average perplexity-per-word of the held-out test 7We follow prior work (Blei et al., 2004; Li and McCallum, 2006; Li et al., 2007) in using only the abstracts: http://www.cs.nyu.edu/˜roweis/data.html 8Williamson et al. (2010) created a similar subset: http://people.csail.mit.edu/jrennie/20Newsgroups/ data, perplexity = 2− lo92(data|model)/iv. Exact computation is intractable, so we use the left-to-right algorithm (Wallach et al., 2009) as an accurate alternative. With the topics fixed, the SCTM is equivalent to LDA and requires no adaptation of the leftto-right algorithm. We used a collapsed Gibbs sampler for training LDA and the algorithm described above for training the SCTM. Bo</context>
</contexts>
<marker>Blei, Griffiths, Jordan, Tenenbaum, 2004</marker>
<rawString>David Blei, Thomas Griffiths, Michael Jordan, and Joshua Tenenbaum. 2004. Hierarchical topic models and the nested chinese restaurant process. In Advances in Neural Information Processing Systems (NIPS), volume 16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society. Series B (Methodological),</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="13972" citStr="Dempster et al., 1977" startWordPosition="2333" endWordPosition="2336"> times (in our experiments, we use J=1, which appears to work as well on this task as multiple samples). These J samples are then used in the M-step as an approximation of the expectation of the latent variables. 3.2 M-Step Given many samples of B and Z, the M-step optimizes the component parameters 0 which cannot be collapsed out. We utilize the standard PoE training procedure for experts: contrastive divergence (CD). We approximate the CD gradient as the difference of the data distribution and the one-step reconstruction of the data according to the current parameters. As in Generalized EM (Dempster et al., 1977), a single gradient step in the direction of the contrastive divergence objective is sufficient for each M-step. A key difference in our model is that we must incorporate the expectation of the PoE model structure, which in our case is a random variable instead of a fixed observed structure. We achieve this by simply 3CD training within MCEM is not the only possible approach. One alternative would be to compute the CD gradient summing over all values of B and Z, effectively training the entire model using CD. This approach prevents the normal CD objective derivation from being simplified into </context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Amr Ahmed</author>
<author>Eric P Xing</author>
</authors>
<title>Sparse additive generative models of text.</title>
<date>2011</date>
<booktitle>In International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="1701" citStr="Eisenstein et al., 2011" startWordPosition="252" endWordPosition="255">than LDA and achieves better perplexity with fewer parameters. 1 Introduction Topic models are probabilistic graphical models meant to capture the semantic associations underlying corpora. Since the introduction of latent Dirichlet allocation (LDA) (Blei et al., 2003), these models have been extended to account for more complex distributions over topics, such as adding supervision (Blei and McAuliffe, 2007), non-parametric priors (Blei et al., 2004; Teh et al., 2006), topic correlations (Li and McCallum, 2006; Mimno et al., 2007; Blei and Lafferty, 2006) and sparsity (Williamson et al., 2010; Eisenstein et al., 2011). While much research has focused on modeling distributions over topics, less focus has been given to the makeup of the topics themselves. This emphasis leads us to find two problems with LDA and its variants mentioned above: (1) independently generated topics and (2) overparameterized models. Independent Topics In the models above, the topics are modeled as independent draws from a single underlying distribution, typically a Dirichlet. This violates the topic modeling community’s intuition that these distributions over words are often related. As an example, consider a corpus that supports tw</context>
<context position="3282" citStr="Eisenstein et al., 2011" startWordPosition="494" endWordPosition="497">extensions to LDA have addressed a related issue, LDA’s inability to model topic correlation,1 by changing the distributions over topics (Blei and Lafferty, 2006; Li and McCallum, 2006; Mimno et al., 2007; Paisley et al., 2011). Yet, none of these change the underlying structure of the topic’s distributions over words. Overparameterization Topics are most often parameterized as multinomial distributions over words: increasing the topics means learning new multinomials over large vocabularies, resulting in models consisting of millions of parameters. This issue was partially addressed in SAGE (Eisenstein et al., 2011) by encouraging sparsity in the topics which are parameterized by their difference in logfrequencies from a fixed background distribution. Yet the problem of overparameterization is also tied 1Two correlated topics, e.g. nutrition and exercise, are likely to co-occur, but their word distributions might not overlap. 783 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 783–792, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics to the number of topics, and though SAGE reduces the nu</context>
<context position="19265" citStr="Eisenstein et al., 2011" startWordPosition="3258" endWordPosition="3261">OMM) (Heller and Ghahramani, 2007), yet our model differs from and, in some ways, extends theirs. The IOMM models the geometric overlap of Gaussian clusters using PoEs, and models the structure of the PoEs with the rows of a binary matrix. The SCTM models a finite number of columns, where the IOMM models an infinite number. The IOMM generates a row for each data point, whereas the SCTM generates a row for each topic. Thus, the SCTM goes beyond the IOMM by allowing the rows to be shared among documents and models document-specific mixtures over the rows of the matrix.6 SAGE for topic modeling (Eisenstein et al., 2011) can be viewed as a restricted form of the SCTM. Consider an SCTM in which the binary matrix is restricted such that the first column, b·,1, consists of all ones and the remainder forms a diagonal matrix. If we then set the first component, 01, to the corpus background distribution, and add a Laplace prior on the natural parameters, acv, we have the SAGE model. Note that by removing the restriction that the matrix contain a diagonal, we could allow multiple components to combine in the SCTM fashion, while incorporating SAGE’s sparsity benefits. 6The IOMM uses Metropolis-Hastings (MH) to sample</context>
</contexts>
<marker>Eisenstein, Ahmed, Xing, 2011</marker>
<rawString>Jacob Eisenstein, Amr Ahmed, and Eric P. Xing. 2011. Sparse additive generative models of text. In International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew R Gormley</author>
<author>Mark Dredze</author>
<author>Benjamin Van Durme</author>
<author>Jason Eisner</author>
</authors>
<title>Shared components topic models with application to selectional preference.</title>
<date>2011</date>
<booktitle>In Learning Semantics Workshop at NIPS</booktitle>
<marker>Gormley, Dredze, Van Durme, Eisner, 2011</marker>
<rawString>Matthew R. Gormley, Mark Dredze, Benjamin Van Durme, and Jason Eisner. 2011. Shared components topic models with application to selectional preference. In Learning Semantics Workshop at NIPS 2011, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Griffiths</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Infinite latent feature models and the indian buffet process.</title>
<date>2006</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS),</booktitle>
<volume>18</volume>
<contexts>
<context position="6813" citStr="Griffiths and Ghahramani, 2006" startWordPosition="1069" endWordPosition="1072"> C components. Latent Dirichlet allocation generative process For each topic k ∈ {1, ... , K}: Ok ∼ Dir(0) [draw distribution over words] For each document m ∈ {1, ... , M}: Om ∼ Dir(ca) [draw distribution over topics] For each word n ∈ {1, ... , Nm}: zmn ∼ Mult(1, Om) [draw topic] xmn ∼ Ozm% [draw word] EvE 7--i� I =1 l cEC Ocv the summation in the denominator is over the vocabulary. In a PoE, each component can overrule the others by giving low probability to some word. A PoE can be viewed as a soft intersection of its components, whereas a mixture is a soft union. The Beta-Bernoulli model (Griffiths and Ghahramani, 2006) is a distribution over binary matrices with a fixed number of rows and columns. It is the finite counterpart to the Indian Buffet Process. In this work, we use the Beta-Bernoulli as our prior for an unobserved binary matrix B with C columns and K rows. In the SCTM, each row bk of the matrix, a binary feature vector, defines a topic distribution. The binary vector acts as a selector for the structure of the PoE for that topic. The row determines which components to include in the product by which entries bk, are “on” (equal to 1) in that row. Under Beta-Bernoulli prior, for each column, a coin</context>
<context position="37364" citStr="Griffiths and Ghahramani, 2006" startWordPosition="6302" endWordPosition="6305">signments except zi. We use the independence of each document, recalling that example i belongs to document m. In practice, we cache p(x|bz, φ) for all x, z (V ×K values) and these are shared by all zi in a sampling iteration. Above, just as in LDA, p(Z|α) is simplified by proportionality to (�n−(i) mzi + αzi), where �n−(i) mk is the count of examples for document m that are assigned topic k excluding zi’s contribution (Heinrich, 2008). Full conditional of bkc Recall that p(B|&apos;y) is the prior for a Beta-Bernoulli matrix. The full conditional distribution of a position in the binary vector is (Griffiths and Ghahramani, 2006): (8) K + γ C where �n−(k) c is the count of topics with component c excluding topic k, and B−(kc) is the entire matrix except for the entry bkc. To find the full conditional for bkc ∈ {0, 1}, we again start with the factorization from Eq. 4. p(bkc|X, Z, B−(kc), φ, α,β, &apos;y) (9) ∝ p(X|Z, B, φ)p(B|&apos;y) (10) Y p(xi |bzi, φ)# p(bkc |B−(kc), &apos;y) (11) Lzi=k where p(bkc|B−(kc),&apos;y) is given by Eq. 8, bk. 77�77V nkv 11v=1 �cv V 7�7 c bki ||�nk ||1 (Pv=1 llj=1 �jv ) (12) and where nkv is the count of words assigned topic k that are type v, and ||nk||1 (the L1-norm of count vector nk) is the count of all </context>
</contexts>
<marker>Griffiths, Ghahramani, 2006</marker>
<rawString>Thomas Griffiths and Zoubin Ghahramani. 2006. Infinite latent feature models and the indian buffet process. In Advances in Neural Information Processing Systems (NIPS), volume 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor Heinrich</author>
</authors>
<title>Parameter estimation for text analysis.</title>
<date>2008</date>
<tech>Technical report, Fraunhofer IGD.</tech>
<contexts>
<context position="37172" citStr="Heinrich, 2008" startWordPosition="6273" endWordPosition="6274">of zi ∈ {1, ... , K}, follows from the factorization in Eq. 4: p(zi|X, Z−(i),B,φ, α,β, &apos;y) ∝ p(X|Z, B, φ)p(Z|α) ∝ p(xi|bzi,φ)(�n−(i) mzi + αzi) Z−(i) is the set of all topic assignments except zi. We use the independence of each document, recalling that example i belongs to document m. In practice, we cache p(x|bz, φ) for all x, z (V ×K values) and these are shared by all zi in a sampling iteration. Above, just as in LDA, p(Z|α) is simplified by proportionality to (�n−(i) mzi + αzi), where �n−(i) mk is the count of examples for document m that are assigned topic k excluding zi’s contribution (Heinrich, 2008). Full conditional of bkc Recall that p(B|&apos;y) is the prior for a Beta-Bernoulli matrix. The full conditional distribution of a position in the binary vector is (Griffiths and Ghahramani, 2006): (8) K + γ C where �n−(k) c is the count of topics with component c excluding topic k, and B−(kc) is the entire matrix except for the entry bkc. To find the full conditional for bkc ∈ {0, 1}, we again start with the factorization from Eq. 4. p(bkc|X, Z, B−(kc), φ, α,β, &apos;y) (9) ∝ p(X|Z, B, φ)p(B|&apos;y) (10) Y p(xi |bzi, φ)# p(bkc |B−(kc), &apos;y) (11) Lzi=k where p(bkc|B−(kc),&apos;y) is given by Eq. 8, bk. 77�77V nk</context>
</contexts>
<marker>Heinrich, 2008</marker>
<rawString>Gregor Heinrich. 2008. Parameter estimation for text analysis. Technical report, Fraunhofer IGD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katherine A Heller</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>A nonparametric bayesian approach to modeling overlapping clusters.</title>
<date>2007</date>
<booktitle>In Artificial Intelligence and Statistics (AISTATS),</booktitle>
<pages>187--194</pages>
<contexts>
<context position="18675" citStr="Heller and Ghahramani, 2007" startWordPosition="3152" endWordPosition="3155">− �7 · d Q(ξ|ξ(t)) dξcvfor all values of c, v. dd Q1 , which is ‘problematic to compute’ (Hinton, 2002). This is the standard use of CD. Algorithm 1 SCTM Training Initialize parameters: ξc, bkc, zi. while not converged do {E-step:} for j = 1 to J do {Draw jth sample {Z, B}(j)} for i = 1 to N do Sample zi using Eq. (7) fork = 1 to K do forc= 1 to C do Sample bkc using ratio in Eq. (12) {M-step:} for c = 1toCdo for v = 1 to V do Single gradient step over ξ cv − η ·d Q(0|0(t)) ξ(t+1) cv = ξ(t) dξcv 4 Related Models The SCTM is closely related to the the Infinite Overlapping Mixture Model (IOMM) (Heller and Ghahramani, 2007), yet our model differs from and, in some ways, extends theirs. The IOMM models the geometric overlap of Gaussian clusters using PoEs, and models the structure of the PoEs with the rows of a binary matrix. The SCTM models a finite number of columns, where the IOMM models an infinite number. The IOMM generates a row for each data point, whereas the SCTM generates a row for each topic. Thus, the SCTM goes beyond the IOMM by allowing the rows to be shared among documents and models document-specific mixtures over the rows of the matrix.6 SAGE for topic modeling (Eisenstein et al., 2011) can be vi</context>
</contexts>
<marker>Heller, Ghahramani, 2007</marker>
<rawString>Katherine A. Heller and Zoubin Ghahramani. 2007. A nonparametric bayesian approach to modeling overlapping clusters. In Artificial Intelligence and Statistics (AISTATS), pages 187–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Hinton</author>
</authors>
<title>Products of experts.</title>
<date>1999</date>
<booktitle>In International Conference on Artificial Neural Networks (ICANN).</booktitle>
<contexts>
<context position="7882" citStr="Hinton, 1999" startWordPosition="1265" endWordPosition="1266">components to include in the product by which entries bk, are “on” (equal to 1) in that row. Under Beta-Bernoulli prior, for each column, a coin with weight 7r, is chosen. For each entry in the column, the coin is flipped to determine if the entry is “on” or “off”. This corresponds to LDA draws each topic 0k independently from a Dirichlet. The model generates each document m of length M, by first sampling a distribution over topics em. Then, for each word n, a topic zmn is chosen and a word type xmn is generated from that topic’s distribution over words 0zm,. A Product of Experts (PoE) model (Hinton, 1999) is the normalized product of the expert distributions. In the SCTM, each component (an expert) models an underlying multinomial word distribution. We let 0, be the parameters of the cth component, where 0, is the probability of the cth component generating word v. If the structure of a PoE included only components c E C in the product, it would have the form: p(x|01, ... , 0C) = IIcEC 0c� where there are C components, and 784 the notion that some components are a priori more likely to be included in topics. The Beta-Bernoulli model generative process For each component c E 11, ... , C}: [colu</context>
</contexts>
<marker>Hinton, 1999</marker>
<rawString>Geoffrey Hinton. 1999. Products of experts. In International Conference on Artificial Neural Networks (ICANN).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Hinton</author>
</authors>
<title>Training products of experts by minimizing contrastive divergence.</title>
<date>2002</date>
<journal>Neural Computation,</journal>
<volume>14</volume>
<issue>8</issue>
<contexts>
<context position="12461" citStr="Hinton, 2002" startWordPosition="2074" endWordPosition="2075">. Because exact inference is intractable in the SCTM, we turn to approximate methods. As is common in these models, we will integrate out π and θ, sample latent variables Z and B, and optimize the components φ. Our algorithm follows the outline of the Monte Carlo EM (MCEM) algorithm (Wei and Tanner, 1990). In the Monte Carlo E-step, we will re-sample the latent variables Z and B based on current model parameters φ and observed data X. In the M-step, we will find new model parameters φ. Since these parameters correspond to experts in the PoE, we rely on a contrastive divergence (CD) objective (Hinton, 2002), popular for PoE training, rather than maximizing the data 2The vocabulary size V could be much larger if n-grams or relational triples are used, as opposed to unigrams. 785 log-likelihood. Normally, CD only estimates the parameters of the expert distributions. However, in our model, the structure of the PoEs themselves change based on the E-step. Since we generate multiple samples in the E-step, we modify the CD objective to compute the gradient for each E-step sample and take the average to approximate the expectation under B and Z.3 3.1 E-Step The E-step approximates an expectation under p</context>
<context position="15773" citStr="Hinton (2002)" startWordPosition="2634" endWordPosition="2635">age the result. Another difficulty arises from computing the gradient directly for the multinomial 0c due to the V −1 degrees of freedom imposed by sum-to-one constraints. Therefore, we switch to the natural parameters, which obviates the need for considering the sum-to-one constraint in the optimization, by defining 0c in terms of V real valued parameters {�c1,...,G-V }: exp(�cv) 0cv = (2) EV, exp(�cv) The V parameters �cv are then used to compute 0cv for use in the E-step. As explained above, the M-step does not maximize the data log-likelihood, but instead minimizes contrastive divergence. Hinton (2002) explains that maximizing data log-likelihood is equivalent to minimizing Q0||Q&apos; , the KL divergence between the observed data distribution, Q0, and the model’s equilibrium distribution, Q—.4 Minimizing Q0||Qwould require the computation of an intractable expectation under the equilibrium distribution. We avoid this by instead minimizing the contrastive divergence objective, CD(�|{Z,B}(�)) = Q0||Q- − Q1�||Q- , (3) where Q1� is the distribution over one-step reconstructions of the data, X given Z, B, �, that are generated by a single step of Gibbs sampling. Unlike standard applications of CD tr</context>
<context position="18150" citStr="Hinton, 2002" startWordPosition="3044" endWordPosition="3045">tor of p(x|bz,O) and the derivative of its log is efficient to compute: To approximate the expectation under Q1ξ, we hold Z, B, � fixed and resample the data, X, using one step of Gibbs sampling. 3.3 Summary Our learning algorithm can be viewed in terms of a Q function: Q(�|�(t)) ≈ 1 Ej=1 CD(�|{Z, B}(j))where we average over J samples. The E-step computes Q(�|�(t)). The M-step minimizes Q with respect to � to obtain the updated �(t+1) by performing gradient descent on the Q function as �(t+1) cv =�(t) cv − �7 · d Q(ξ|ξ(t)) dξcvfor all values of c, v. dd Q1 , which is ‘problematic to compute’ (Hinton, 2002). This is the standard use of CD. Algorithm 1 SCTM Training Initialize parameters: ξc, bkc, zi. while not converged do {E-step:} for j = 1 to J do {Draw jth sample {Z, B}(j)} for i = 1 to N do Sample zi using Eq. (7) fork = 1 to K do forc= 1 to C do Sample bkc using ratio in Eq. (12) {M-step:} for c = 1toCdo for v = 1 to V do Single gradient step over ξ cv − η ·d Q(0|0(t)) ξ(t+1) cv = ξ(t) dξcv 4 Related Models The SCTM is closely related to the the Infinite Overlapping Mixture Model (IOMM) (Heller and Ghahramani, 2007), yet our model differs from and, in some ways, extends theirs. The IOMM mo</context>
<context position="23113" citStr="Hinton (2002)" startWordPosition="3918" endWordPosition="3919">|model)/iv. Exact computation is intractable, so we use the left-to-right algorithm (Wallach et al., 2009) as an accurate alternative. With the topics fixed, the SCTM is equivalent to LDA and requires no adaptation of the leftto-right algorithm. We used a collapsed Gibbs sampler for training LDA and the algorithm described above for training the SCTM. Both were trained for 4000 iterations, sampling topics every 10 iterations after a burn-in of 3000. The hyperparameter α was optimized as an asymmetric Dirichlet, ,3 as a symmetric Dirichlet, and ry = 3.0 was fixed.9 Following the observation of Hinton (2002) that CD training benefits from initializing the experts to nearly uniform distributions, we initialize the component distributions from a symmetric Dirichlet with parameter, = 1×106. We use J = 1 samples per iteration and a decaying learning rate centered at q = 100.10 We ranged LDA from 10 to 200 topics, and the SCTM from 10 to 100 components (C). We then selected the number of SCTM topics (K) as K E {C, 2C, 3C, 4C, 5C}. For each model, we used five random restarts, selecting the model with the highest training data likelihood. 5.1 Results Our goal is to demonstrate that (1) modeling topics </context>
</contexts>
<marker>Hinton, 2002</marker>
<rawString>Geoffrey Hinton. 2002. Training products of experts by minimizing contrastive divergence. Neural Computation, 14(8):1771–1800.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Li</author>
<author>Andrew McCallum</author>
</authors>
<title>Pachinko allocation: DAG-structured mixture models of topic correlations.</title>
<date>2006</date>
<booktitle>In International Conference on Machine Learning (ICML),</booktitle>
<pages>577--584</pages>
<contexts>
<context position="1591" citStr="Li and McCallum, 2006" startWordPosition="234" endWordPosition="237">o combine subsets of them into topics. The SCTM can represent topics in a much more compact representation than LDA and achieves better perplexity with fewer parameters. 1 Introduction Topic models are probabilistic graphical models meant to capture the semantic associations underlying corpora. Since the introduction of latent Dirichlet allocation (LDA) (Blei et al., 2003), these models have been extended to account for more complex distributions over topics, such as adding supervision (Blei and McAuliffe, 2007), non-parametric priors (Blei et al., 2004; Teh et al., 2006), topic correlations (Li and McCallum, 2006; Mimno et al., 2007; Blei and Lafferty, 2006) and sparsity (Williamson et al., 2010; Eisenstein et al., 2011). While much research has focused on modeling distributions over topics, less focus has been given to the makeup of the topics themselves. This emphasis leads us to find two problems with LDA and its variants mentioned above: (1) independently generated topics and (2) overparameterized models. Independent Topics In the models above, the topics are modeled as independent draws from a single underlying distribution, typically a Dirichlet. This violates the topic modeling community’s intu</context>
<context position="2842" citStr="Li and McCallum, 2006" startWordPosition="431" endWordPosition="434"> over words are often related. As an example, consider a corpus that supports two related topics, baseball and hockey. These topics likely overlap in their allocation of mass to high probability words (e.g. team, season, game, players), even though the two topics are unlikely to appear in the same documents. When topics are generated independently, the model does not provide a way to capture this sharing between related topics. Many extensions to LDA have addressed a related issue, LDA’s inability to model topic correlation,1 by changing the distributions over topics (Blei and Lafferty, 2006; Li and McCallum, 2006; Mimno et al., 2007; Paisley et al., 2011). Yet, none of these change the underlying structure of the topic’s distributions over words. Overparameterization Topics are most often parameterized as multinomial distributions over words: increasing the topics means learning new multinomials over large vocabularies, resulting in models consisting of millions of parameters. This issue was partially addressed in SAGE (Eisenstein et al., 2011) by encouraging sparsity in the topics which are parameterized by their difference in logfrequencies from a fixed background distribution. Yet the problem of ov</context>
<context position="22279" citStr="Li and McCallum, 2006" startWordPosition="3790" endWordPosition="3794">ar data sets for comparison: NIPS: A collection of 1,617 NIPS abstracts from 1987 to 19997, with 77,952 tokens and 1,632 types. 20NEWS: 1,000 randomly selected articles from the 20 Newsgroups dataset,8 with 70,011 tokens and 1,722 types. Both data sets excluded stop words and words occurring in fewer than 10 documents. For 20NEWS, we used the standard by-date train/test split. For NIPS, we randomly partitioned the data by document into 75% train and 25% test. We compare the SCTM to LDA by evaluating the average perplexity-per-word of the held-out test 7We follow prior work (Blei et al., 2004; Li and McCallum, 2006; Li et al., 2007) in using only the abstracts: http://www.cs.nyu.edu/˜roweis/data.html 8Williamson et al. (2010) created a similar subset: http://people.csail.mit.edu/jrennie/20Newsgroups/ data, perplexity = 2− lo92(data|model)/iv. Exact computation is intractable, so we use the left-to-right algorithm (Wallach et al., 2009) as an accurate alternative. With the topics fixed, the SCTM is equivalent to LDA and requires no adaptation of the leftto-right algorithm. We used a collapsed Gibbs sampler for training LDA and the algorithm described above for training the SCTM. Both were trained for 400</context>
</contexts>
<marker>Li, McCallum, 2006</marker>
<rawString>Wei Li and Andrew McCallum. 2006. Pachinko allocation: DAG-structured mixture models of topic correlations. In International Conference on Machine Learning (ICML), pages 577–584.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Li</author>
<author>David Blei</author>
<author>Andrew McCallum</author>
</authors>
<title>Nonparametric bayes pachinko allocation.</title>
<date>2007</date>
<booktitle>In Uncertainty in Artificial Intelligence (UAI).</booktitle>
<contexts>
<context position="22297" citStr="Li et al., 2007" startWordPosition="3795" endWordPosition="3798">ison: NIPS: A collection of 1,617 NIPS abstracts from 1987 to 19997, with 77,952 tokens and 1,632 types. 20NEWS: 1,000 randomly selected articles from the 20 Newsgroups dataset,8 with 70,011 tokens and 1,722 types. Both data sets excluded stop words and words occurring in fewer than 10 documents. For 20NEWS, we used the standard by-date train/test split. For NIPS, we randomly partitioned the data by document into 75% train and 25% test. We compare the SCTM to LDA by evaluating the average perplexity-per-word of the held-out test 7We follow prior work (Blei et al., 2004; Li and McCallum, 2006; Li et al., 2007) in using only the abstracts: http://www.cs.nyu.edu/˜roweis/data.html 8Williamson et al. (2010) created a similar subset: http://people.csail.mit.edu/jrennie/20Newsgroups/ data, perplexity = 2− lo92(data|model)/iv. Exact computation is intractable, so we use the left-to-right algorithm (Wallach et al., 2009) as an accurate alternative. With the topics fixed, the SCTM is equivalent to LDA and requires no adaptation of the leftto-right algorithm. We used a collapsed Gibbs sampler for training LDA and the algorithm described above for training the SCTM. Both were trained for 4000 iterations, samp</context>
</contexts>
<marker>Li, Blei, McCallum, 2007</marker>
<rawString>Wei Li, David Blei, and Andrew McCallum. 2007. Nonparametric bayes pachinko allocation. In Uncertainty in Artificial Intelligence (UAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Wei Li</author>
<author>Andrew McCallum</author>
</authors>
<title>Mixtures of hierarchical topics with pachinko allocation.</title>
<date>2007</date>
<booktitle>In International Conference on Machine Learning (ICML),</booktitle>
<pages>633--640</pages>
<contexts>
<context position="1611" citStr="Mimno et al., 2007" startWordPosition="238" endWordPosition="241">em into topics. The SCTM can represent topics in a much more compact representation than LDA and achieves better perplexity with fewer parameters. 1 Introduction Topic models are probabilistic graphical models meant to capture the semantic associations underlying corpora. Since the introduction of latent Dirichlet allocation (LDA) (Blei et al., 2003), these models have been extended to account for more complex distributions over topics, such as adding supervision (Blei and McAuliffe, 2007), non-parametric priors (Blei et al., 2004; Teh et al., 2006), topic correlations (Li and McCallum, 2006; Mimno et al., 2007; Blei and Lafferty, 2006) and sparsity (Williamson et al., 2010; Eisenstein et al., 2011). While much research has focused on modeling distributions over topics, less focus has been given to the makeup of the topics themselves. This emphasis leads us to find two problems with LDA and its variants mentioned above: (1) independently generated topics and (2) overparameterized models. Independent Topics In the models above, the topics are modeled as independent draws from a single underlying distribution, typically a Dirichlet. This violates the topic modeling community’s intuition that these dis</context>
<context position="2862" citStr="Mimno et al., 2007" startWordPosition="435" endWordPosition="438">elated. As an example, consider a corpus that supports two related topics, baseball and hockey. These topics likely overlap in their allocation of mass to high probability words (e.g. team, season, game, players), even though the two topics are unlikely to appear in the same documents. When topics are generated independently, the model does not provide a way to capture this sharing between related topics. Many extensions to LDA have addressed a related issue, LDA’s inability to model topic correlation,1 by changing the distributions over topics (Blei and Lafferty, 2006; Li and McCallum, 2006; Mimno et al., 2007; Paisley et al., 2011). Yet, none of these change the underlying structure of the topic’s distributions over words. Overparameterization Topics are most often parameterized as multinomial distributions over words: increasing the topics means learning new multinomials over large vocabularies, resulting in models consisting of millions of parameters. This issue was partially addressed in SAGE (Eisenstein et al., 2011) by encouraging sparsity in the topics which are parameterized by their difference in logfrequencies from a fixed background distribution. Yet the problem of overparameterization i</context>
<context position="20767" citStr="Mimno et al., 2007" startWordPosition="3539" endWordPosition="3542">cOcv for x =6 = v 5The derivative is approximate because we drop the term: d Q1 dξ 787 The relation of TagLDA (Zhu et al., 2006) to the SCTM is similar to that of SAGE and SCTM. TagLDA has a PoE of exactly two experts: one expert for the topic, and one for the supervised wordlevel tag. Examples of tags are abstract or body, indicating which part of a research paper the word appears in. Unlike the SCTM and SAGE, most prior extensions to LDA have enhanced the distribution over topics for each document. One of the closest is hierarchical LDA (hLDA) (Blei et al., 2004) and its application to PAM (Mimno et al., 2007). Though topics are still generated independently from a Dirichlet prior, hLDA learns a tree structure underlying the topics. Each document samples a single path through the tree and samples words from topics along that path. The SCTM models an orthogonal issue to topic hierarchy: how the topics themselves are represented as the intersection of components. Finally, while prior work has primarily used mixtures for the sake of conjugacy, we take a fundamentally different approach to modeling the structure by using normalized product distributions. 5 Evaluation We compare the SCTM with LDA in ter</context>
</contexts>
<marker>Mimno, Li, McCallum, 2007</marker>
<rawString>David Mimno, Wei Li, and Andrew McCallum. 2007. Mixtures of hierarchical topics with pachinko allocation. In International Conference on Machine Learning (ICML), pages 633–640.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Paisley</author>
<author>Chong Wang</author>
<author>David Blei</author>
</authors>
<title>The discrete infinite logistic normal distribution for MixedMembership modeling.</title>
<date>2011</date>
<booktitle>In International Conference on Artificial Intelligence and Statistics (AISTATS).</booktitle>
<contexts>
<context position="2885" citStr="Paisley et al., 2011" startWordPosition="439" endWordPosition="442">e, consider a corpus that supports two related topics, baseball and hockey. These topics likely overlap in their allocation of mass to high probability words (e.g. team, season, game, players), even though the two topics are unlikely to appear in the same documents. When topics are generated independently, the model does not provide a way to capture this sharing between related topics. Many extensions to LDA have addressed a related issue, LDA’s inability to model topic correlation,1 by changing the distributions over topics (Blei and Lafferty, 2006; Li and McCallum, 2006; Mimno et al., 2007; Paisley et al., 2011). Yet, none of these change the underlying structure of the topic’s distributions over words. Overparameterization Topics are most often parameterized as multinomial distributions over words: increasing the topics means learning new multinomials over large vocabularies, resulting in models consisting of millions of parameters. This issue was partially addressed in SAGE (Eisenstein et al., 2011) by encouraging sparsity in the topics which are parameterized by their difference in logfrequencies from a fixed background distribution. Yet the problem of overparameterization is also tied 1Two correl</context>
</contexts>
<marker>Paisley, Wang, Blei, 2011</marker>
<rawString>John Paisley, Chong Wang, and David Blei. 2011. The discrete infinite logistic normal distribution for MixedMembership modeling. In International Conference on Artificial Intelligence and Statistics (AISTATS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>Michael Jordan</author>
<author>Matthew Beal</author>
<author>David Blei</author>
</authors>
<title>Hierarchical dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>101</volume>
<issue>476</issue>
<contexts>
<context position="1548" citStr="Teh et al., 2006" startWordPosition="227" endWordPosition="230">istributions and the structure of how to combine subsets of them into topics. The SCTM can represent topics in a much more compact representation than LDA and achieves better perplexity with fewer parameters. 1 Introduction Topic models are probabilistic graphical models meant to capture the semantic associations underlying corpora. Since the introduction of latent Dirichlet allocation (LDA) (Blei et al., 2003), these models have been extended to account for more complex distributions over topics, such as adding supervision (Blei and McAuliffe, 2007), non-parametric priors (Blei et al., 2004; Teh et al., 2006), topic correlations (Li and McCallum, 2006; Mimno et al., 2007; Blei and Lafferty, 2006) and sparsity (Williamson et al., 2010; Eisenstein et al., 2011). While much research has focused on modeling distributions over topics, less focus has been given to the makeup of the topics themselves. This emphasis leads us to find two problems with LDA and its variants mentioned above: (1) independently generated topics and (2) overparameterized models. Independent Topics In the models above, the topics are modeled as independent draws from a single underlying distribution, typically a Dirichlet. This v</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Yee Whye Teh, Michael Jordan, Matthew Beal, and David Blei. 2006. Hierarchical dirichlet processes. Journal of the American Statistical Association, 101(476):1566–1581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna Wallach</author>
<author>Ian Murray</author>
<author>Ruslan Salakhutdinov</author>
<author>David Mimno</author>
</authors>
<title>Evaluation methods for topic models.</title>
<date>2009</date>
<booktitle>In International Conference on Machine Learning (ICML),</booktitle>
<pages>1105--1112</pages>
<contexts>
<context position="22606" citStr="Wallach et al., 2009" startWordPosition="3831" endWordPosition="3834">NEWS, we used the standard by-date train/test split. For NIPS, we randomly partitioned the data by document into 75% train and 25% test. We compare the SCTM to LDA by evaluating the average perplexity-per-word of the held-out test 7We follow prior work (Blei et al., 2004; Li and McCallum, 2006; Li et al., 2007) in using only the abstracts: http://www.cs.nyu.edu/˜roweis/data.html 8Williamson et al. (2010) created a similar subset: http://people.csail.mit.edu/jrennie/20Newsgroups/ data, perplexity = 2− lo92(data|model)/iv. Exact computation is intractable, so we use the left-to-right algorithm (Wallach et al., 2009) as an accurate alternative. With the topics fixed, the SCTM is equivalent to LDA and requires no adaptation of the leftto-right algorithm. We used a collapsed Gibbs sampler for training LDA and the algorithm described above for training the SCTM. Both were trained for 4000 iterations, sampling topics every 10 iterations after a burn-in of 3000. The hyperparameter α was optimized as an asymmetric Dirichlet, ,3 as a symmetric Dirichlet, and ry = 3.0 was fixed.9 Following the observation of Hinton (2002) that CD training benefits from initializing the experts to nearly uniform distributions, we </context>
</contexts>
<marker>Wallach, Murray, Salakhutdinov, Mimno, 2009</marker>
<rawString>Hanna Wallach, Ian Murray, Ruslan Salakhutdinov, and David Mimno. 2009. Evaluation methods for topic models. In International Conference on Machine Learning (ICML), pages 1105–1112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Wei</author>
<author>Martin Tanner</author>
</authors>
<title>A monte carlo implementation of the EM algorithm and the poor man’s data augmentation algorithms.</title>
<date>1990</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>85</volume>
<issue>411</issue>
<contexts>
<context position="12154" citStr="Wei and Tanner, 1990" startWordPosition="2018" endWordPosition="2021"> think of the SCTM as learning the structure of many PoE models. In applications where experts abstain, the SCTM could learn in which setting (row) each expert casts a vote. 3 Parameter Estimation Parameter estimation infers values for model parameters φ, π, and θ from data using an unsupervised training procedure. Because exact inference is intractable in the SCTM, we turn to approximate methods. As is common in these models, we will integrate out π and θ, sample latent variables Z and B, and optimize the components φ. Our algorithm follows the outline of the Monte Carlo EM (MCEM) algorithm (Wei and Tanner, 1990). In the Monte Carlo E-step, we will re-sample the latent variables Z and B based on current model parameters φ and observed data X. In the M-step, we will find new model parameters φ. Since these parameters correspond to experts in the PoE, we rely on a contrastive divergence (CD) objective (Hinton, 2002), popular for PoE training, rather than maximizing the data 2The vocabulary size V could be much larger if n-grams or relational triples are used, as opposed to unigrams. 785 log-likelihood. Normally, CD only estimates the parameters of the expert distributions. However, in our model, the str</context>
</contexts>
<marker>Wei, Tanner, 1990</marker>
<rawString>Greg Wei and Martin Tanner. 1990. A monte carlo implementation of the EM algorithm and the poor man’s data augmentation algorithms. Journal of the American Statistical Association, 85(411):699–704.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sinead Williamson</author>
<author>Chong Wang</author>
<author>Katherine Heller</author>
<author>David Blei</author>
</authors>
<title>The IBP compound dirichlet process and its application to focused topic modeling.</title>
<date>2010</date>
<booktitle>In International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="1675" citStr="Williamson et al., 2010" startWordPosition="248" endWordPosition="251">e compact representation than LDA and achieves better perplexity with fewer parameters. 1 Introduction Topic models are probabilistic graphical models meant to capture the semantic associations underlying corpora. Since the introduction of latent Dirichlet allocation (LDA) (Blei et al., 2003), these models have been extended to account for more complex distributions over topics, such as adding supervision (Blei and McAuliffe, 2007), non-parametric priors (Blei et al., 2004; Teh et al., 2006), topic correlations (Li and McCallum, 2006; Mimno et al., 2007; Blei and Lafferty, 2006) and sparsity (Williamson et al., 2010; Eisenstein et al., 2011). While much research has focused on modeling distributions over topics, less focus has been given to the makeup of the topics themselves. This emphasis leads us to find two problems with LDA and its variants mentioned above: (1) independently generated topics and (2) overparameterized models. Independent Topics In the models above, the topics are modeled as independent draws from a single underlying distribution, typically a Dirichlet. This violates the topic modeling community’s intuition that these distributions over words are often related. As an example, consider</context>
<context position="22392" citStr="Williamson et al. (2010)" startWordPosition="3805" endWordPosition="3808"> and 1,632 types. 20NEWS: 1,000 randomly selected articles from the 20 Newsgroups dataset,8 with 70,011 tokens and 1,722 types. Both data sets excluded stop words and words occurring in fewer than 10 documents. For 20NEWS, we used the standard by-date train/test split. For NIPS, we randomly partitioned the data by document into 75% train and 25% test. We compare the SCTM to LDA by evaluating the average perplexity-per-word of the held-out test 7We follow prior work (Blei et al., 2004; Li and McCallum, 2006; Li et al., 2007) in using only the abstracts: http://www.cs.nyu.edu/˜roweis/data.html 8Williamson et al. (2010) created a similar subset: http://people.csail.mit.edu/jrennie/20Newsgroups/ data, perplexity = 2− lo92(data|model)/iv. Exact computation is intractable, so we use the left-to-right algorithm (Wallach et al., 2009) as an accurate alternative. With the topics fixed, the SCTM is equivalent to LDA and requires no adaptation of the leftto-right algorithm. We used a collapsed Gibbs sampler for training LDA and the algorithm described above for training the SCTM. Both were trained for 4000 iterations, sampling topics every 10 iterations after a burn-in of 3000. The hyperparameter α was optimized as </context>
</contexts>
<marker>Williamson, Wang, Heller, Blei, 2010</marker>
<rawString>Sinead Williamson, Chong Wang, Katherine Heller, and David Blei. 2010. The IBP compound dirichlet process and its application to focused topic modeling. In International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>David Blei</author>
<author>John Lafferty</author>
</authors>
<title>TagLDA: bringing document structure knowledge into topic models.</title>
<date>2006</date>
<tech>Technical Report TR-1553,</tech>
<institution>University of Wisconsin.</institution>
<contexts>
<context position="20276" citStr="Zhu et al., 2006" startWordPosition="3445" endWordPosition="3448">riction that the matrix contain a diagonal, we could allow multiple components to combine in the SCTM fashion, while incorporating SAGE’s sparsity benefits. 6The IOMM uses Metropolis-Hastings (MH) to sample the parameters of the experts. This approach is computationally feasible because their experts are Gaussian, unlike the SCTM in which the experts are multinomials and the MH step too expensive. xmn zmn 9m Ct Nm M bkc 0c 7rc � K C N d log f(x|bz, O) d�cv � bzc(1 − Ocv) for x = v −bzcOcv for x =6 = v 5The derivative is approximate because we drop the term: d Q1 dξ 787 The relation of TagLDA (Zhu et al., 2006) to the SCTM is similar to that of SAGE and SCTM. TagLDA has a PoE of exactly two experts: one expert for the topic, and one for the supervised wordlevel tag. Examples of tags are abstract or body, indicating which part of a research paper the word appears in. Unlike the SCTM and SAGE, most prior extensions to LDA have enhanced the distribution over topics for each document. One of the closest is hierarchical LDA (hLDA) (Blei et al., 2004) and its application to PAM (Mimno et al., 2007). Though topics are still generated independently from a Dirichlet prior, hLDA learns a tree structure underl</context>
</contexts>
<marker>Zhu, Blei, Lafferty, 2006</marker>
<rawString>Xiaojin Zhu, David Blei, and John Lafferty. 2006. TagLDA: bringing document structure knowledge into topic models. Technical Report TR-1553, University of Wisconsin.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>