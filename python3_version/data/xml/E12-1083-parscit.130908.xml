<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.961042">
Structural and Topical Dimensions in Multi-Task Patent Translation
</title>
<author confidence="0.990626">
Katharina W¨aschle and Stefan Riezler
</author>
<affiliation confidence="0.9002485">
Department of Computational Linguistics
Heidelberg University, Germany
</affiliation>
<email confidence="0.995278">
{waeschle,riezler}@cl.uni-heidelberg.de
</email>
<sectionHeader confidence="0.994682" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997655">
Patent translation is a complex problem due
to the highly specialized technical vocab-
ulary and the peculiar textual structure of
patent documents. In this paper we analyze
patents along the orthogonal dimensions of
topic and textual structure. We view differ-
ent patent classes and different patent text
sections such as title, abstract, and claims,
as separate translation tasks, and investi-
gate the influence of such tasks on machine
translation performance. We study multi-
task learning techniques that exploit com-
monalities between tasks by mixtures of
translation models or by multi-task meta-
parameter tuning. We find small but sig-
nificant gains over task-specific training
by techniques that model commonalities
through shared parameters. A by-product
of our work is a parallel patent corpus of 23
million German-English sentence pairs.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999953423076923">
Patents are an important tool for the protection
of intellectual property and also play a significant
role in business strategies in modern economies.
Patent translation is an enabling technique for
patent prior art search which aims to detect a
patent’s novelty and thus needs to be cross-lingual
for a multitude of languages. Patent translation is
complicated by a highly specialized vocabulary,
consisting of technical terms specific to the field
of invention the patent relates to. Patents are writ-
ten in a sophisticated legal jargon (“patentese”)
that is not found in everyday language and ex-
hibits a complex textual structure. Also, patents
are often intentionally ambiguous or vague in or-
der to maximize the coverage of the claims.
In this paper, we analyze patents with respect
to the orthogonal dimensions of topic – the tech-
nical field covered by the patent – and structure
– a patent’s text sections –, with respect to their
influence on machine translation performance.
The topical dimension of patents is charac-
terized by the International Patent Classification
(IPC)1 which categorizes patents hierarchically
into 8 sections, 120 classes, 600 subclasses, down
to 70,000 subgroups at the leaf level. Table 1
shows the 8 top level sections.
</bodyText>
<figure confidence="0.970004444444445">
A Human Necessities
B Performing Operations, Transporting
C Chemistry, Metallurgy
D Textiles, Paper
E Fixed Constructions
F Mechanical Engineering, Lighting,
Heating, Weapons
G Physics
H Electricity
</figure>
<tableCaption confidence="0.992893">
Table 1: IPC top level sections.
</tableCaption>
<bodyText confidence="0.860731666666667">
Orthogonal to the patent classification, patent
documents can be sub-categorized along the di-
mension of textual structure. Article 78.1 of the
European Patent Convention (EPC) lists all sec-
tions required in a patent document2:
”A European patent application shall
contain:
(a) a request for the grant of a Euro-
pean patent;
</bodyText>
<footnote confidence="0.995178333333333">
1http://www.wipo.int/classifications/
ipc/en/
2Highlights by the authors.
</footnote>
<page confidence="0.775323">
818
</page>
<note confidence="0.9823265">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 818–828,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<listItem confidence="0.983566">
(b) a description of the invention;
(c) one or more claims;
</listItem>
<bodyText confidence="0.9723383">
(d) any drawings referred to in the de-
scription or the claims;
(e) an abstract,
and satisfy the requirements laid down
in the Implementing Regulations.”
The request for grant contains the patent title; thus
a patent document comprises the textual elements
of title, description, claim, and abstract.
We investigate whether it is worthwhile to treat
different values along the structural and topical
dimensions as different tasks that are not com-
pletely independent of each other but share some
commonalities, yet differ enough to counter a
simple pooling of data. For example, we con-
sider different tasks such as patents from different
IPC classes, or along an orthogonal dimension,
patent documents of all IPC classes but consisting
only of titles or only of claims. We ask whether
such tasks should be addressed as separate trans-
lation tasks, or whether translation performance
can be improved by learning several tasks simul-
taneously through shared models that are more so-
phisticated than simple data pooling. Our goal is
to learn a patent translation system that performs
well across several different tasks, thus benefits
from shared information, but is yet able to address
the specifics of each task.
One contribution of this paper is a thorough
analysis of the differences and similarities of mul-
tilingual patent data along the dimensions of tex-
tual structure and topic. The second contribution
is the experimental investigation of the influence
of various such tasks on patent translation perfor-
mance. Starting from baseline models that are
trained on individual tasks or on data pooled from
all tasks, we apply mixtures of translation mod-
els and multi-task minimum error rate training to
multiple patent translation tasks. A by-product of
our research is a parallel patent corpus of over 23
million sentence pairs.
</bodyText>
<sectionHeader confidence="0.9998" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999974789473685">
Multi-task learning has mostly been discussed un-
der the name of multi-domain adaptation in the
area of statistical machine translation (SMT). If
we consider domains as tasks, domain adapta-
tion is a special two-task case of multi-task learn-
ing. Most previous work has concentrated on
adapting unsupervised generative modules such
as translation models or language models to new
tasks. For example, transductive approaches have
used automatic translations of monolingual cor-
pora for self-training modules of the generative
SMT pipeline (Ueffing et al., 2007; Schwenk,
2008; Bertoldi and Federico, 2009). Other ap-
proaches have extracted parallel data from similar
or comparable corpora (Zhao et al., 2004; Snover
et al., 2008). Several approaches have been pre-
sented that train separate translation and language
models on task-specific subsets of the data and
combine them in different mixture models (Fos-
ter and Kuhn, 2007; Koehn and Schroeder, 2007;
Foster et al., 2010). The latter kind of approach is
applied in our work to multiple patent tasks.
Multi-task learning efforts in patent transla-
tion have so far been restricted to experimental
combinations of translation and language mod-
els from different sets of IPC sections. For ex-
ample, Utiyama and Isahara (2007) and Tinsley
et al. (2010) investigate translation and language
models trained on different sets of patent sections,
with larger pools of parallel data improving re-
sults. Ceaus¸u et al. (2011) find that language mod-
els always and translation model mostly benefit
from larger pools of data from different sections.
Models trained on pooled patent data are used as
baselines in our approach.
The machine learning community has devel-
oped several different formalizations of the cen-
tral idea of trading off optimality of parameter
vectors for each task-specific model and close-
ness of these model parameters to the average pa-
rameter vector across models. For example, start-
ing from a separate SVM for each task, Evgeniou
and Pontil (2004) present a regularization method
that trades off optimization of the task-specific pa-
rameter vectors and the distance of each SVM to
the average SVM. Equivalent formalizations re-
place parameter regularization by Bayesian prior
distributions on the parameters (Finkel and Man-
ning, 2009) or by augmentation of the feature
space with domain independent features (Daum´e,
2007). Besides SVMs, several learning algo-
rithms have been extended to the multi-task sce-
nario in a parameter regularization setting, e.g.,
perceptron-type algorithms (Dredze et al., 2010)
or boosting (Chapelle et al., 2011). Further vari-
ants include different formalizations of norms for
parameter regularization, e.g., f1,2 regularization
</bodyText>
<page confidence="0.994415">
819
</page>
<bodyText confidence="0.965366">
(Obozinski et al., 2010) or Ei regularization
(Quattoni et al., 2009), where only the features
that are most important across all tasks are kept in
the model. In our experiments, we apply parame-
ter regularization for multi-task learning to mini-
mum error rate training for patent translation.
</bodyText>
<sectionHeader confidence="0.6503695" genericHeader="method">
3 Extraction of a parallel patent corpus
from comparable data
</sectionHeader>
<bodyText confidence="0.999231342857143">
Our work on patent translation is based on the
MAREC3 patent data corpus. MAREC con-
tains over 19 million patent applications and
granted patents in a standardized format from
four patent organizations (European Patent Of-
fice (EP), World Intellectual Property Organiza-
tion (WO), United States Patent and Trademark
Office (US), Japan Patent Office (JP)), from 1976
to 2008. The data for our experiments are ex-
tracted from the EP and WO collections which
contain patent documents that include translations
of some of the patent text. To extract such parallel
patent sections, we first determine the longest in-
stance, if different kinds4 exist for a patent. We
assume titles to be sentence-aligned by default,
and define sections with a token ratio larger than
0.7 as parallel. For the language pair German-
English we extracted a total of 2,101,107 parallel
titles, 291,716 parallel abstracts, and 735,667 par-
allel claims sections.
The lack of directly translated descriptions
poses a serious limitation for patent translation,
since this section constitutes the largest part of the
document. It is possible to obtain comparable de-
scriptions from related patents that have been filed
in different countries and are connected through
the patent family id. We extracted 172,472 patents
that were both filed with the USPTO and the EPO
and contain an English and a German description,
respectively.
For sentence alignment, we used the Gargan-
tua5 tool (Braune and Fraser, 2010) that fil-
ters a sentence-length based alignment with IBM
Model-1 lexical word translation probabilities, es-
timated on parallel data obtained from the first-
</bodyText>
<footnote confidence="0.996241">
3http://www.ir-facility.org/
prototypes/marec
4A patent kind code indicates the document stage in the
filing process, e.g., A for applications and B for granted
patents, with publication levels from 1-9. See http://
www.wipo.int/standards/en/part\_03.html.
5http://gargantua.sourceforge.net
</footnote>
<bodyText confidence="0.997638888888889">
pass alignment. This yields the parallel corpus
listed in table 2 with high input-output ratios for
claims, and much lower ratios for abstracts and
descriptions, showing that claims exhibit a nat-
ural parallelism due to their structure, while ab-
stracts and descriptions are considerably less par-
allel. Removing duplicates and adding parallel ti-
tles results in a corpus of over 23 million parallel
sentence pairs.
</bodyText>
<table confidence="0.964538">
output de ratio en ratio
abstract 720,571 92.36% 76.81%
claims 8,346,863 97.82% 96.17%
descr. 14,082,381 86.23% 82.67%
</table>
<tableCaption confidence="0.991182">
Table 2: Number of parallel sentences in output with
input/output ratio of sentence aligner.
</tableCaption>
<bodyText confidence="0.968808727272727">
Differences between the text sections become
visible in an analysis of token to type ratios. Ta-
ble 3 gives the average number of tokens com-
pared to the average type frequencies for a win-
dow of 100,000 tokens from every subsection. It
shows that titles contain considerably fewer to-
kens than other sections, however, the disadvan-
tage is partially made up by a relatively large
amount of types, indicated by a lower average
type frequency.
tokens types
</bodyText>
<table confidence="0.736249">
de en de en
title 6.5 8.0 2.9 4.8
abstract 37.4 43.2 4.3 9.0
claims 53.2 61.3 5.5 9.5
description 27.5 35.5 4.0 7.0
</table>
<tableCaption confidence="0.997397">
Table 3: Average number of tokens and average type
frequencies in text sections.
</tableCaption>
<bodyText confidence="0.999971333333333">
We reserved patent data published between
1979 and 2007 for training and documents pub-
lished in 2008 for tuning and testing in SMT.
For the dimension of text sections, we sampled
500,000 sentences – distributed across all IPC
sections – for training and 2,000 sentences for
each text section for development and testing. Be-
cause of a relatively high number of identical sen-
tences in test and training set for titles, we re-
moved the overlap for this section.
Table 4 shows the distribution of IPC sections
on claims, with the smallest class accounting for
</bodyText>
<page confidence="0.97173">
820
</page>
<bodyText confidence="0.999557666666667">
around 300,000 parallel sentences. In order to ob-
tain similar amounts of training data for each task
along the topical dimension, we sampled 300,000
sentences from each IPC class for training, and
2,000 sentences for each IPC class for develop-
ment and testing.
</bodyText>
<figure confidence="0.993219875">
A 1,947,542
B 2,522,995
C 2,263,375
D 299,742
E 353,910
F 1,012,808
G 2,066,132
H 1,754,573
</figure>
<tableCaption confidence="0.954248">
Table 4: Distribution of IPC sections on claims.
</tableCaption>
<sectionHeader confidence="0.972416" genericHeader="method">
4 Machine translation experiments
</sectionHeader>
<subsectionHeader confidence="0.981472">
4.1 Individual task baselines
</subsectionHeader>
<bodyText confidence="0.999811625">
For our experiments we used the phrase-based,
open-source SMT toolkit Moses6 (Koehn et al.,
2007). For language modeling, we computed
5-gram models using IRSTLM7 (Federico et
al., 2008) and queried the model with KenLM
(Heafield, 2011). BLEU (Papineni et al., 2001)
scores were computed up to 4-grams on lower-
cased data.
</bodyText>
<sectionHeader confidence="0.554635" genericHeader="method">
Europarl-v6 MAREC
</sectionHeader>
<table confidence="0.9881125">
BLEU OOV BLEU OOV
abstract 0.1726 14.40% 0.3721 3.00%
claim 0.2301 15.80% 0.4711 4.20%
title 0.0964 26.00% 0.3228 9.20%
</table>
<tableCaption confidence="0.9927825">
Table 5: BLEU scores and OOV rate for Europarl base-
line and MAREC model.
</tableCaption>
<bodyText confidence="0.99578825">
Table 5 shows a first comparison of results of
Moses models trained on 500,000 parallel sen-
tences from patent text sections balanced over IPC
classes, against Moses trained on 1.7 Million sen-
tences of parliament proceedings from Europarl8
(Koehn, 2005). The best result on each section is
indicated in bold face. The Europarl model per-
forms very poorly on all three sections in compar-
</bodyText>
<footnote confidence="0.9992535">
6http://statmt.org/moses/
7http://sourceforge.net/projects/
irstlm/
8http://www.statmt.org/europarl/
</footnote>
<bodyText confidence="0.838671642857143">
ison to the task-specific MAREC model, although
the former has been learned on more than three
times the amount of data. An analysis of the out-
put of both system shows that the Europarl model
suffers from two problems: Firstly, there is an ob-
vious out of vocabulary (OOV) problem of the
Europarl model compared to the MAREC model.
Secondly, the Europarl model suffers from incor-
rect word sense disambiguation, as illustrated by
the samples in table 6.
source steuerbar leitet
Europarl taxable is in charge of
MAREC controllable guiding
reference controllable guides
</bodyText>
<tableCaption confidence="0.986777">
Table 6: Output of Europarl model on MAREC data.
</tableCaption>
<bodyText confidence="0.984048384615384">
Table 7 shows the results of the evaluation
across text sections; we measured the perfor-
mance of separately trained and tuned individual
models on every section. The results allow some
conclusions about the textual characteristics of the
sections and indicate similarities. Naturally, ev-
ery task is best translated with a model trained
on the respective section, as the BLEU scores
on the diagonal are the highest in every column.
Accordingly, we are interested in the runner-up
on each section, which is indicated in bold font.
The results on abstracts suggest that this section
bears the strongest resemblance to claims, since
the model trained on claims achieves a respectable
score. The abstract model seems to be the most
robust and varied model, yielding the runner-up
score on all other sections. Claims are easiest to
translate, yielding the highest overall BLEU score
of 0.4879. In contrast to that, all models score
considerably lower on titles.
test
train abstract claim title desc.
abstract 0.3737 0.4076 0.2681 0.2812
claim 0.3416 0.4879 0.2420 0.2623
title 0.2839 0.3512 0.3196 0.1743
desc. 0.32189 0.403 0.2342 0.3347
</bodyText>
<tableCaption confidence="0.957974">
Table 7: BLEU scores for 500k individual text section
models.
</tableCaption>
<bodyText confidence="0.819825">
The cross-section evaluation on the IPC classes
(table 8) shows similar patterns. Each section
</bodyText>
<page confidence="0.989875">
821
</page>
<bodyText confidence="0.999907261904762">
is best translated with a model trained on data
from the same section. Note that best section
scores vary considerably, ranging from 0.5719 on
C to 0.4714 on H, indicating that higher-scoring
classes, such as C and A, are more homogeneous
and therefore easier to translate. C, the Chem-
istry section, presumably benefits from the fact
that the data contain chemical formulae, which
are language-independent and do not have to be
translated. Again, for determining the relation-
ship between the classes, we examine the best
runner-up on each section, considering the BLEU
score, although asymmetrical, as a kind of mea-
sure of similarity between classes. We can es-
tablish symmetric relationships between sections
A and C, B and F as well as G and H, which
means that the models are mutual runner-up on
the other’s test section.
The similarities of translation tasks estab-
lished in the previous section can be confirmed
by information-theoretic similarity measures that
perform a pairwise comparison of the vocabulary
probability distribution of each task-specific cor-
pus. This distribution is calculated on the basis of
the 500 most frequent words in the union of two
corpora, normalized by vocabulary size. As met-
ric we use the A-distance measure of Kifer et al.
(2004). If A is the set of events on which the word
distributions of two corpora are defined, then the
A-distance is the supremum of the difference of
probabilities assigned to the same event. Low dis-
tance means higher similarity.
Table 9 shows the A-distance of corpora spe-
cific to IPC classes. The most similar section or
sections – apart from the section itself on the di-
agonal – is indicated in bold face. The pairwise
similarity of A and C, B and F, G and H obtained
by BLEU score is confirmed. Furthermore, a close
similarity between E and F is indicated. G and
H (electricity and physics, respectively) are very
similar to each other but not close to any other
section apart from B.
</bodyText>
<subsectionHeader confidence="0.999521">
4.2 Task pooling and mixture
</subsectionHeader>
<bodyText confidence="0.99995316">
One straightforward technique to exploit com-
monalities between tasks is pooling data from
separate tasks into a single training set. Instead of
a trivial enlargement of training data by pooling,
we train the pooled models on the same amount
of sentences as the individual models. For in-
stance, the pooled model for the pairing of IPC
section B and C is trained on a data set composed
of 150,000 sentences from each IPC section. The
pooled model for pairing data from abstracts and
claims is trained on data composed of 250,000
sentences from each text section.
Another approach to exploit commonalities be-
tween tasks is to train separate language and trans-
lation models9 on the sentences from each task
and combine the models in the global log-linear
model of the SMT framework, following Fos-
ter and Kuhn (2007) and Koehn and Schroeder
(2007). Model combination is accomplished by
adding additional language model and translation
model features to the log-linear model and tuning
the additional meta-parameters by standard mini-
mum error rate training (Bertoldi et al., 2009).
We try out mixture and pooling for all pairwise
combinations of the three structural sections, for
which we have high-quality data, i.e. abstract,
claims and title. Due to the large number of pos-
sible combinations of IPC sections, we limit the
experiments to pairs of similar sections, based on
the A-distance measure.
Table 10 lists the results for two combinations
of data from different sections: a log-linear mix-
ture of separately trained models and simple pool-
ing, i.e. concatenation, of the training data. Over-
all, the mixture models perform slightly better
than the pooled models on the text sections, al-
though the difference is significant only in two
cases. This is indicated by highlighting best re-
sults in bold face (with more than one result high-
lighted if the difference is not significant).10
We investigate the same mixture and pooling
techniques on the IPC sections we considered
pairwise similar (see table 11). Somehow contra-
dicting the former results, the mixture models per-
form significantly worse than the pooled model on
three sections. This might be the result of inade-
quate tuning, since most of the time the MERT
algorithm did not converge after the maximum
number of iterations, due to the larger number of
features when using several models.
</bodyText>
<footnote confidence="0.986873666666667">
9Following Duh et al. (2010), we use the alignment
model trained on the pooled data set in the phrase extraction
phase of the separate models. Similarly, we use a globally
trained lexical reordering model.
10For assessing significance, we apply the approximate
randomization method described in Riezler and Maxwell
(2005). We consider pairwise differing results scoring a p-
value smaller than 0.05 as significant; the assessment is re-
peated three times and the average value is taken.
</footnote>
<page confidence="0.963659">
822
</page>
<table confidence="0.9997603">
test
train A B C D E F G H
A 0.5349 0.4475 0.5472 0.4746 0.4438 0.4523 0.4318 0.4109
B 0.4846 0.4736 0.5161 0.4847 0.4578 0.4734 0.4396 0.4248
C 0.5047 0.4257 0.5719 0.462 0.4134 0.4249 0.409 0.3845
D 0.47 0.4387 0.5106 0.5167 0.4344 0.4435 0.407 0.3917
E 0.4486 0.4458 0.4681 0.4531 0.4771 0.4591 0.4073 0.4028
F 0.4595 0.4588 0.4761 0.4655 0.4517 0.4909 0.422 0.4188
G 0.4935 0.4489 0.5239 0.4629 0.4414 0.4565 0.4748 0.4532
H 0.4628 0.4484 0.4914 0.4621 0.4421 0.4616 0.4588 0.4714
</table>
<tableCaption confidence="0.951496">
Table 8: BLEU scores for 300k individual IPC section models.
</tableCaption>
<table confidence="0.999835111111111">
A B C D E F G H
A 0 0.1303 0.1317 0.1311 0.188 0.186 0.164 0.1906
B 0.1302 0 0.2388 0.1242 0.0974 0.0875 0.1417 0.1514
C 0.1317 0.2388 0 0.1992 0.311 0.3068 0.2506 0.2825
D 0.1311 0.1242 0.1992 0 0.1811 0.1808 0.1876 0.201
E 0.188 0.0974 0.311 0.1811 0 0.0921 0.2058 0.2025
F 0.186 0.0875 0.3068 0.1808 0.0921 0 0.1824 0.1743
G 0.164 0.1417 0.2506 0.1876 0.2056 0.1824 0 0.064
H 0.1906 0.1514 0.2825 0.201 0.2025 0.1743 0.064 0
</table>
<tableCaption confidence="0.965425">
Table 9: Pairwise A-distance for 300k IPC training sets.
</tableCaption>
<table confidence="0.823036142857143">
train test pooling mixture train test pooling mixture
abstract-claim abstract 0.3703 0.3704 A-C A 0.5271 0.5274
claim 0.4809 0.4834 C 0.5664 0.5632
claim-title claim 0.4799 0.4789 B-F B 0.4696 0.4354
title 0.3269 0.328 F 0.4859 0.4769
title-abstract title 0.3311 0.3275 G-H G 0.4735 0.4754
abstract 0.3643 0.366 H 0.4634 0.467
</table>
<tableCaption confidence="0.851383">
Table 10: Mixture and pooling on text sections. Table 11: Mixture and pooling on IPC sections.
</tableCaption>
<bodyText confidence="0.99998525">
A comparison of the results for pooling and
mixture with the respective results for individual
models (tables 7 and 8) shows that replacing data
from the same task by data from related tasks
decreases translation performance in almost all
cases. The exception is the title model that bene-
fits from pooling and mixing with both abstracts
and claims due to their richer data structure.
</bodyText>
<subsectionHeader confidence="0.99912">
4.3 Multi-task minimum error rate training
</subsectionHeader>
<bodyText confidence="0.9999634375">
In contrast to task pooling and task mixtures, the
specific setting addressed by multi-task minimum
error rate training is one in which the generative
SMT pipeline is not adaptable. Such situations
arise if there are not enough data to train transla-
tion models or language models on the new tasks.
However, we assume that there are enough paral-
lel data available to perform meta-parameter tun-
ing by minimum error rate training (MERT) (Och,
2003; Bertoldi et al., 2009) for each task.
A generic algorithm for multi-task learning
can be motivated as follows: Multi-task learning
aims to take advantage of commonalities shared
among tasks by learning several independent but
related tasks together. Information is shared be-
tween tasks through a joint representation and in-
</bodyText>
<page confidence="0.997804">
823
</page>
<table confidence="0.9932302">
tuning
test individual pooled average MMERT MMERT-average
abstract 0.3721 0.362 0.3657*+ 0.3719+ 0.3685*+
claim 0.4711 0.4681 0.4749*+ 0.475*+ 0.4734*+
title 0.3228 0.3152 0.3326*+ 0.3268*+ 0.3325*+
</table>
<tableCaption confidence="0.995785">
Table 12: Multi-task tuning on text sections.
</tableCaption>
<table confidence="0.9995541">
tuning
test individual pooled average MMERT MMERT-average
A 0.5187 0.5199 0.5213*+ 0.5195 0.5196
B 0.4877 0.4885 0.4908*+ 0.4911*+ 0.4921*+
C 0.5214 0.5175 0.5199*+ 0.5218+ 0.5162*+
D 0.4724 0.4730 0.4733 0.4736 0.4734
E 0.4666 0.4661 0.4679*+ 0.4669+ 0.4685*+
F 0.4794 0.4801 0.4811* 0.4821*+ 0.4830*+
G 0.4596 0.4576 0.4607+ 0.4606+ 0.4610*+
H 0.4573 0.4560 0.4578 0.4581+ 0.4581+
</table>
<tableCaption confidence="0.999586">
Table 13: Multi-task tuning on IPC sections.
</tableCaption>
<bodyText confidence="0.999678818181818">
troduces an inductive bias. Evgeniou and Pon-
til (2004) propose a regularization method that
balances task-specific parameter vectors and their
distance to the average. The learning objective is
to minimize task-specific loss functions ld across
all tasks d with weight vectors wd, while keep-
ing each parameter vector close to the average
D �d 1 wd = wavg. This is enforced by min-
imizing the norm (here the `1-norm) of the dif-
ference of each task-specific weight vector to the
avarage weight vector.
</bodyText>
<equation confidence="0.515429">
||wd − wavg||1 (1)
</equation>
<bodyText confidence="0.999527523809524">
The MMERT algorithm is given in figure 1.
The algorithm starts with initial weights w(0). At
each iteration step, the average of the parame-
ter vectors from the previous iteration is com-
puted. For each task d E D, one iteration of stan-
dard MERT is called, continuing from weight vec-
tor w(t−1) dand minimizing translation loss func-
tion ld on the data from task d. The individu-
ally tuned weight vectors returned by MERT are
then moved towards the previously calculated av-
erage by adding or subtracting a penalty term A
for each weight component w(t)
d [k]. If a weight
moves beyond the average, it is clipped to the av-
erage value. The process is iterated until a stop-
ping criterion is met, e.g. a threshold on the max-
imum change in the average weight vector. The
parameter A controls the influence of the regular-
ization. A larger A pulls the weights closer to the
average, a smaller A leaves more freedom to the
individual tasks.
</bodyText>
<equation confidence="0.993797722222222">
MMERT(w(0), D, {ld}Dd=1):
for t = 1,...,T do
(t) 1 D (t−1)
wavg = D �d=1 wd
for d = 1, ... , D parallel do
wd = MERT(w(t−1)
(t) d , ld)
fork= 1,...,K do
if w[k](dt) − w(t) [k] &gt; 0 then
avg
wd [k] = max(w(t)
(t) avg[k], w(t)
d [k]−A)
else if w(dt) [k] − w(t)
avg[k] &lt; 0 then
w(t)
d [k] = min(w(t) avg[k], w(t)
d [k] + A)
</equation>
<listItem confidence="0.68864625">
end if
end for
end for
end for
</listItem>
<equation confidence="0.8409955">
(T) (T)(T)
return w1 , ... , wD , wavg
</equation>
<figureCaption confidence="0.943245">
Figure 1: Multi-task MERT.
</figureCaption>
<equation confidence="0.818120857142857">
D
d=1
ld(wd) + A
min
w1,...,wD
D
d=1
</equation>
<page confidence="0.992375">
824
</page>
<bodyText confidence="0.993973">
The weight updates and the clipping strategy
can be motivated in a framework of gradient de-
scent optimization under `1-regularization (Tsu-
ruoka et al., 2009). Assuming MERT as algorith-
mic minimizer11 of the loss function ld in equa-
tion 1, the weight update towards the average
follows from the subgradient of the i1 regular-
izer. Since w(t)
avg is taken as average over weights
wd from the step before, the term w(t)
</bodyText>
<equation confidence="0.895763857142857">
(t−1) is con-
avg
stant with respect to w(d t), leading to the follow-
ing subgradient (where sgn(x) = 1 if x &gt; 0,
sgn(x) = −1 if x &lt; 0, and sgn(x) = 0 if x = 0):
� ��D
� � wd −
(t) 1 D
s=1
D
= A sgn w(t) [k] − 1
1
D
s=1
</equation>
<bodyText confidence="0.98421830120482">
Gradient descent minimization tells us to move in
the opposite direction of the subgradient, thus mo-
tivating the addition or subtraction of the regular-
ization penalty. Clipping is motivated by the de-
sire to avoid oscillating parameter weights and in
order to to enforce parameter sharing.
Experimental results for multi-task MERT
(MMERT) are reported for both dimensions of
patent tasks. For the IPC sections we trained
a pooled model on 1,000,000 sentences sampled
from abstracts and claims from all sections. We
did not balance the sections but kept their orig-
inal distribution, reflecting a real-life task where
the distribution of sections is unknown. We then
extend this experiment to the structural dimen-
sion. Since we do not have an intuitive notion of a
natural distribution for the text sections, we train
a balanced pooled model on a corpus composed
of 170,000 sentences each from abstracts, claims
and titles, i.e. 510,000 sentences in total. For
both dimensions, for each task, we sampled 2,000
parallel sentences for development, development-
testing, and testing from patents that were pub-
lished in different years than the training data.
We compare the multi-task experiments with
two baselines. The first baseline is individual
task learning, corresponding to standard separate
MERT tuning on each section (individual). This
results in three separately learned weight vectors
11MERT as presented in Och (2003) is not a gradient-
based optimization techniquem, thus MMERT is strictly
speaking only “inspired” by gradient descent optimization.
for each task, where no information has been
shared between the tasks. The second baseline
simulates the setting where the sections are not
differentiated at all. We tune the model on a
pooled development set of 2,000 sentences that
combines the same amount of data from all sec-
tions (pooled). This yields a single joint weight
vector for all tasks optimized to perform well
across all sections. Furthermore, we compare
multi-task MERT tuning with two parameter av-
eraging methods. The first method computes the
arithmetic mean of the weight vectors returned by
the individual baseline for each weight compo-
nent, yielding a joint average vector for all tasks
(average). The second method takes the last av-
erage vector computed during multi-task MERT
tuning (MMERT-average).12
Tables 12 and 13 give the results for multi-task
learning on text and IPC sections. The latter re-
sults have been presented earlier in Simianer et al.
(2011). The former table extends the technique
of multi-task MERT to the structural dimension
of patent SMT tasks. In all experiments, the pa-
rameter A was adjusted to 0.001 after evaluating
different settings on a development set. The best
result on each section is indicated in bold face; *
indicates significance with respect to the individ-
ual baseline, + the same for the pooled baseline.
We observe statistically significant improvements
of 0.5 to 1% BLEU over the individual baseline for
claims and titles; for abstracts, the multi-task vari-
ant yields the same result as the baseline, while
the averaging methods perform worse. Multi-task
MERT yields the best result for claims; on titles,
the simple average and the last MMERT average
dominate. Pooled tuning always performs signifi-
cantly worse than any other method, confirming
that it is beneficial to differentiate between the
text section sections.
Similarly for IPC sections, small but statisti-
cally significant improvements over the individual
and pooled baselines are achieved by multi-task
tuning and averaging over IPC sections, except-
ing C and D. However, an advantage of multi-task
tuning over averaging is hard to establish.
Note that the averaging techniques implicitly
benefit from a larger tuning set. In order to ascer-
tain that the improvements by averaging are not
12The aspect of averaging found in all of our multi-task
learning techniques effectively controls for optimizer insta-
bility as mentioned in Clark et al. (2011).
</bodyText>
<equation confidence="0.773906333333333">
a A
aw(t)
r [k]
D
d=1
� II � � � �1
w(t−1)
s
�w(t−1) s[k] �
</equation>
<page confidence="0.996499">
825
</page>
<table confidence="0.99866825">
test pooled-6k significance
abstract 0.3628 &lt;
claim 0.4696 &lt;
title 0.3174 &lt;
</table>
<tableCaption confidence="0.873895">
Table 14: Multi-task tuning on 6,000 sentences pooled
from text sections. “&lt;” denotes a statistically signifi-
cant difference to the best result.
</tableCaption>
<bodyText confidence="0.9999491">
simply due to increasing the size of the tuning set,
we ran a control experiment where we tuned the
model on a pooled development set of 3 x 2, 000
sentences for text sections and on a development
set of 8 x 2, 000 sentences for IPC sections. The
results given in table 14 show that tuning on a
pooled set of 6,000 text sections yields only min-
imal differences to tuning on 2,000 sentence pairs
such that the BLEU scores for the new pooled
models are still significantly lower than the best
results in table 12 (indicated by “&lt;”). However,
increasing the tuning set to 16,000 sentence pairs
for IPC sections makes the pooled baseline per-
form as well as the best results in table 13, except
for two cases (indicated by “&lt;”) (see table 15).
This is due to the smaller differences between best
and worst results for tuning on IPC sections com-
pared to tuning on text sections, indicating that
IPC sections are less well suited for multi-task
tuning than the textual domains.
</bodyText>
<table confidence="0.997602777777778">
test pooled-16k significance
A 0.5177 &lt;
B 0.4920
C 0.5133 &lt;
D 0.4737
E 0.4685
F 0.4832
G 0.4608
H 0.4579
</table>
<tableCaption confidence="0.780694333333333">
Table 15: Multi-task tuning on 16,000 sentences
pooled from IPC sections. “&lt;” denotes a statistically
significant difference to the best result.
</tableCaption>
<bodyText confidence="0.999979093023256">
ley et al. (2010) and Utiyama and Isahara (2007).
A caveat in this situation is that data need to be
from the general patent domain, as shown by the
inferior performance of a large Europarl-trained
model compared to a small patent-trained model.
The goal of this paper is to analyze patent data
along the topical dimension of IPC classes and
along the structural dimension of textual sections.
Instead of trying to beat a pooling baseline that
simply increases the data size, our research goal
is to investigate whether different subtasks along
these dimensions share commonalities that can
fruitfully be exploited by multi-task learning in
machine translation. We thus aim to investigate
the benefits of multi-task learning in realistic sit-
uations where a simple enlargement of training
data is not possible.
Starting from baseline models that are trained
on individual tasks or on data pooled from all
tasks, we apply mixtures of translation models
and multi-task MERT tuning to multiple patent
translation tasks. We find small, but statistically
significant improvements for multi-task MERT
tuning and parameter averaging techniques. Im-
provements are more pronounced for multi-task
learning on textual domains than on IPC domains.
This might indicate that the IPC sections are less
well delimitated than the structural domains. Fur-
thermore, this is owing to the limited expressive-
ness of a standard linear model including 14-20
features in tuning. The available features are very
coarse and more likely to capture structural dif-
ferences, such as sentence length, than the lexi-
cal differences that differentiate the semantic do-
mains. We expect to see larger gains due to multi-
task learning for discriminatively trained SMT
models that involve very large numbers of fea-
tures, especially when multi-task learning is done
in a framework that combines parameter regular-
ization with feature selection (Obozinski et al.,
2010). In future work, we will explore a combina-
tion of large-scale discriminative training (Liang
et al., 2006) with multi-task learning for SMT.
</bodyText>
<sectionHeader confidence="0.999183" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999565">
The most straightforward approach to improve
machine translation performance on patents is to
enlarge the training set to include all available
data. This question has been investigated by Tins-
</bodyText>
<sectionHeader confidence="0.998023" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.977291666666667">
This work was supported in part by DFG grant
“Cross-language Learning-to-Rank for Patent Re-
trieval”.
</bodyText>
<page confidence="0.997724">
826
</page>
<sectionHeader confidence="0.982287" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993960469026549">
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation
with monolingual resources. In Proceedings of the
4th EACL Workshop on Statistical Machine Trans-
lation, Athens, Greece.
Nicola Bertoldi, Barry Haddow, and Jean-Baptiste
Fouet. 2009. Improved minimum error rate train-
ing in Moses. The Prague Bulletin of Mathematical
Linguistics, 91:7–16.
Fabienne Braune and Alexander Fraser. 2010. Im-
proved unsupervised sentence alignment for sym-
metrical and asymmetrical parallel corpora. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics (COLING’10), Beijing,
China.
Alexandru Ceaus¸u, John Tinsley, Jian Zhang, and
Andy Way. 2011. Experiments on domain adap-
tation for patent machine translation in the PLuTO
project. In Proceedings of the 15th Conference of
the European Assocation for Machine Translation
(EAMT 2011), Leuven, Belgium.
Olivier Chapelle, Pannagadatta Shivaswamy, Srinivas
Vadrevu, Kilian Weinberger, Ya Zhang, and Belle
Tseng. 2011. Boosted multi-task learning. Ma-
chine Learning.
Jonathan Clark, Chris Dyer, Alon Lavie, and Noah
Smith. 2011. Better hypothesis testing for statis-
tical machine translation: Controlling for optimizer
instability. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL’11), Portland, OR.
Hal Daum´e. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL’07), Prague, Czech Republic.
Mark Dredze, Alex Kulesza, and Koby Crammer.
2010. Multi-domain learning by confidence-
weighted parameter combination. Machine Learn-
ing, 79:123–149.
Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada.
2010. Analysis of translation model adaptation in
statistical machine translation. In Proceedings of
the International Workshop on Spoken Language
Translation (IWSLT’10), Paris, France.
Theodoros Evgeniou and Massimiliano Pontil. 2004.
Regularized multi-task learning. In Proceedings of
the 10th ACM SIGKDD conference on knowledge
discovery and data mining (KDD’04), Seattle, WA.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In Proceed-
ings of Interspeech, Brisbane, Australia.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Hierarchical bayesian domain adaptation. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics - Human Language Technologies (NAACL-
HLT’09), Boulder, CO.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for SMT. In Proceedings of the
Second Workshop on Statistical Machine Transla-
tion, Prague, Czech Republic.
George Foster, Pierre Isabelle, and Roland Kuhn.
2010. Translating structured documents. In Pro-
ceedings of the 9th Conference of the Association
for Machine Translation in the Americas (AMTA
2010), Denver, CO.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the
EMNLP 2011 Sixth Workshop on Statistical Ma-
chine Translation (WMT’11), Edinburgh, UK.
Daniel Kifer, Shain Ben-David, and Johannes Gehrke.
2004. Detecting change in data streams. In Pro-
ceedings of the 30th international conference on
Very large data bases, Toronta, Ontario, Canada.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, Prague, Czech
Republic.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Birch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the ACL 2007 Demo and Poster Ses-
sions, Prague, Czech Republic.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of
Machine Translation Summit X, Phuket, Thailand.
Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein,
and Ben Taskar. 2006. An end-to-end dis-
criminative approach to machine translation. In
Proceedings of the joint conference of the Inter-
national Committee on Computational Linguistics
and the Association for Computational Linguistics
(COLING-ACL’06), Sydney, Australia.
Guillaume Obozinski, Ben Taskar, and Michael I. Jor-
dan. 2010. Joint covariate selection and joint sub-
space selection for multiple classification problems.
Statistics and Computing, 20:231–252.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the Human Language Technology Confer-
ence and the 3rd Meeting of the North American
Chapter of the Association for Computational Lin-
guistics (HLT-NAACL’03), Edmonton, Cananda.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2001. Bleu: a method for auto-
matic evaluation of machine translation. Technical
Report IBM Research Division Technical Report,
RC22176 (W0190-022), Yorktown Heights, N.Y.
</reference>
<page confidence="0.979348">
827
</page>
<reference confidence="0.999777081632653">
Ariadna Quattoni, Xavier Carreras, Michael Collins,
and Trevor Darrell. 2009. An efficient projec-
tion for Bl regularization. In Proceedings of the
26th International Conference on Machine Learn-
ing (ICML’09), Montreal, Canada.
Stefan Riezler and John Maxwell. 2005. On some pit-
falls in automatic evaluation and significance testing
for MT. In Proceedings of the ACL-05 Workshop on
Intrinsic and Extrinsic Evaluation Measures for MT
and/or Summarization, Ann Arbor, MI.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical ma-
chine translation. In Proceedings of the Interna-
tional Workshop on Spoken Language Translation
(IWSLT’08), Hawaii.
Patrick Simianer, Katharina W¨aschle, and Stefan Rie-
zler. 2011. Multi-task minimum error rate train-
ing for SMT. The Prague Bulletin of Mathematical
Linguistics, 96:99–108.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and translation model adaptation
using comparable corpora. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP’08), Honolulu, Hawaii.
John Tinsley, Andy Way, and Paraic Sheridan. 2010.
PLuTO: MT for online patent translation. In Pro-
ceedings of the 9th Conference of the Association
for Machine Translation in the Americas (AMTA
2010), Denver, CO.
Yoshimasa Tsuruoka, Jun’ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent train-
ing for Bl-regularized log-linear models with cumu-
lative penalty. In Proceedings of the 47th Annual
Meeting of the Association for Computational Lin-
guistics (ACL-IJCNLP’09), Singapore.
Nicola Ueffing, Gholamreza Haffari, and Anoop
Sarkar. 2007. Transductive learning for statistical
machine translation. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics (ACL’07), Prague, Czech Republic.
Masao Utiyama and Hitoshi Isahara. 2007. A
Japanese-English patent parallel corpus. In Pro-
ceedings of MT Summit XI, Copenhagen, Denmark.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Pro-
ceedings of the 20th International Conference on
Computational Linguistics (COLING’04), Geneva,
Switzerland.
</reference>
<page confidence="0.997618">
828
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.845533">
<title confidence="0.999641">Structural and Topical Dimensions in Multi-Task Patent Translation</title>
<author confidence="0.87449">W¨aschle</author>
<affiliation confidence="0.9995965">Department of Computational Heidelberg University,</affiliation>
<abstract confidence="0.998298904761905">Patent translation is a complex problem due to the highly specialized technical vocabulary and the peculiar textual structure of patent documents. In this paper we analyze patents along the orthogonal dimensions of topic and textual structure. We view different patent classes and different patent text sections such as title, abstract, and claims, as separate translation tasks, and investigate the influence of such tasks on machine translation performance. We study multitask learning techniques that exploit commonalities between tasks by mixtures of translation models or by multi-task metaparameter tuning. We find small but significant gains over task-specific training by techniques that model commonalities through shared parameters. A by-product of our work is a parallel patent corpus of 23 million German-English sentence pairs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nicola Bertoldi</author>
<author>Marcello Federico</author>
</authors>
<title>Domain adaptation for statistical machine translation with monolingual resources.</title>
<date>2009</date>
<booktitle>In Proceedings of the 4th EACL Workshop on Statistical Machine Translation,</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="5676" citStr="Bertoldi and Federico, 2009" startWordPosition="859" endWordPosition="862">tence pairs. 2 Related work Multi-task learning has mostly been discussed under the name of multi-domain adaptation in the area of statistical machine translation (SMT). If we consider domains as tasks, domain adaptation is a special two-task case of multi-task learning. Most previous work has concentrated on adapting unsupervised generative modules such as translation models or language models to new tasks. For example, transductive approaches have used automatic translations of monolingual corpora for self-training modules of the generative SMT pipeline (Ueffing et al., 2007; Schwenk, 2008; Bertoldi and Federico, 2009). Other approaches have extracted parallel data from similar or comparable corpora (Zhao et al., 2004; Snover et al., 2008). Several approaches have been presented that train separate translation and language models on task-specific subsets of the data and combine them in different mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Foster et al., 2010). The latter kind of approach is applied in our work to multiple patent tasks. Multi-task learning efforts in patent translation have so far been restricted to experimental combinations of translation and language models from diffe</context>
</contexts>
<marker>Bertoldi, Federico, 2009</marker>
<rawString>Nicola Bertoldi and Marcello Federico. 2009. Domain adaptation for statistical machine translation with monolingual resources. In Proceedings of the 4th EACL Workshop on Statistical Machine Translation, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Bertoldi</author>
<author>Barry Haddow</author>
<author>Jean-Baptiste Fouet</author>
</authors>
<title>Improved minimum error rate training in Moses.</title>
<date>2009</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>91--7</pages>
<contexts>
<context position="18332" citStr="Bertoldi et al., 2009" startWordPosition="2885" endWordPosition="2888">ring data from abstracts and claims is trained on data composed of 250,000 sentences from each text section. Another approach to exploit commonalities between tasks is to train separate language and translation models9 on the sentences from each task and combine the models in the global log-linear model of the SMT framework, following Foster and Kuhn (2007) and Koehn and Schroeder (2007). Model combination is accomplished by adding additional language model and translation model features to the log-linear model and tuning the additional meta-parameters by standard minimum error rate training (Bertoldi et al., 2009). We try out mixture and pooling for all pairwise combinations of the three structural sections, for which we have high-quality data, i.e. abstract, claims and title. Due to the large number of possible combinations of IPC sections, we limit the experiments to pairs of similar sections, based on the A-distance measure. Table 10 lists the results for two combinations of data from different sections: a log-linear mixture of separately trained models and simple pooling, i.e. concatenation, of the training data. Overall, the mixture models perform slightly better than the pooled models on the text</context>
<context position="22439" citStr="Bertoldi et al., 2009" startWordPosition="3566" endWordPosition="3569">model that benefits from pooling and mixing with both abstracts and claims due to their richer data structure. 4.3 Multi-task minimum error rate training In contrast to task pooling and task mixtures, the specific setting addressed by multi-task minimum error rate training is one in which the generative SMT pipeline is not adaptable. Such situations arise if there are not enough data to train translation models or language models on the new tasks. However, we assume that there are enough parallel data available to perform meta-parameter tuning by minimum error rate training (MERT) (Och, 2003; Bertoldi et al., 2009) for each task. A generic algorithm for multi-task learning can be motivated as follows: Multi-task learning aims to take advantage of commonalities shared among tasks by learning several independent but related tasks together. Information is shared between tasks through a joint representation and in823 tuning test individual pooled average MMERT MMERT-average abstract 0.3721 0.362 0.3657*+ 0.3719+ 0.3685*+ claim 0.4711 0.4681 0.4749*+ 0.475*+ 0.4734*+ title 0.3228 0.3152 0.3326*+ 0.3268*+ 0.3325*+ Table 12: Multi-task tuning on text sections. tuning test individual pooled average MMERT MMERT-</context>
</contexts>
<marker>Bertoldi, Haddow, Fouet, 2009</marker>
<rawString>Nicola Bertoldi, Barry Haddow, and Jean-Baptiste Fouet. 2009. Improved minimum error rate training in Moses. The Prague Bulletin of Mathematical Linguistics, 91:7–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabienne Braune</author>
<author>Alexander Fraser</author>
</authors>
<title>Improved unsupervised sentence alignment for symmetrical and asymmetrical parallel corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING’10),</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="9610" citStr="Braune and Fraser, 2010" startWordPosition="1476" endWordPosition="1479"> parallel titles, 291,716 parallel abstracts, and 735,667 parallel claims sections. The lack of directly translated descriptions poses a serious limitation for patent translation, since this section constitutes the largest part of the document. It is possible to obtain comparable descriptions from related patents that have been filed in different countries and are connected through the patent family id. We extracted 172,472 patents that were both filed with the USPTO and the EPO and contain an English and a German description, respectively. For sentence alignment, we used the Gargantua5 tool (Braune and Fraser, 2010) that filters a sentence-length based alignment with IBM Model-1 lexical word translation probabilities, estimated on parallel data obtained from the first3http://www.ir-facility.org/ prototypes/marec 4A patent kind code indicates the document stage in the filing process, e.g., A for applications and B for granted patents, with publication levels from 1-9. See http:// www.wipo.int/standards/en/part\_03.html. 5http://gargantua.sourceforge.net pass alignment. This yields the parallel corpus listed in table 2 with high input-output ratios for claims, and much lower ratios for abstracts and descri</context>
</contexts>
<marker>Braune, Fraser, 2010</marker>
<rawString>Fabienne Braune and Alexander Fraser. 2010. Improved unsupervised sentence alignment for symmetrical and asymmetrical parallel corpora. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING’10), Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandru Ceaus¸u</author>
<author>John Tinsley</author>
<author>Jian Zhang</author>
<author>Andy Way</author>
</authors>
<title>Experiments on domain adaptation for patent machine translation in the PLuTO project.</title>
<date>2011</date>
<booktitle>In Proceedings of the 15th Conference of the European Assocation for Machine Translation (EAMT 2011),</booktitle>
<location>Leuven, Belgium.</location>
<marker>Ceaus¸u, Tinsley, Zhang, Way, 2011</marker>
<rawString>Alexandru Ceaus¸u, John Tinsley, Jian Zhang, and Andy Way. 2011. Experiments on domain adaptation for patent machine translation in the PLuTO project. In Proceedings of the 15th Conference of the European Assocation for Machine Translation (EAMT 2011), Leuven, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Chapelle</author>
<author>Pannagadatta Shivaswamy</author>
<author>Srinivas Vadrevu</author>
<author>Kilian Weinberger</author>
<author>Ya Zhang</author>
<author>Belle Tseng</author>
</authors>
<title>Boosted multi-task learning.</title>
<date>2011</date>
<journal>Machine Learning.</journal>
<contexts>
<context position="7668" citStr="Chapelle et al., 2011" startWordPosition="1171" endWordPosition="1174">sk, Evgeniou and Pontil (2004) present a regularization method that trades off optimization of the task-specific parameter vectors and the distance of each SVM to the average SVM. Equivalent formalizations replace parameter regularization by Bayesian prior distributions on the parameters (Finkel and Manning, 2009) or by augmentation of the feature space with domain independent features (Daum´e, 2007). Besides SVMs, several learning algorithms have been extended to the multi-task scenario in a parameter regularization setting, e.g., perceptron-type algorithms (Dredze et al., 2010) or boosting (Chapelle et al., 2011). Further variants include different formalizations of norms for parameter regularization, e.g., f1,2 regularization 819 (Obozinski et al., 2010) or Ei regularization (Quattoni et al., 2009), where only the features that are most important across all tasks are kept in the model. In our experiments, we apply parameter regularization for multi-task learning to minimum error rate training for patent translation. 3 Extraction of a parallel patent corpus from comparable data Our work on patent translation is based on the MAREC3 patent data corpus. MAREC contains over 19 million patent applications </context>
</contexts>
<marker>Chapelle, Shivaswamy, Vadrevu, Weinberger, Zhang, Tseng, 2011</marker>
<rawString>Olivier Chapelle, Pannagadatta Shivaswamy, Srinivas Vadrevu, Kilian Weinberger, Ya Zhang, and Belle Tseng. 2011. Boosted multi-task learning. Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL’11),</booktitle>
<location>Portland, OR.</location>
<contexts>
<context position="29929" citStr="Clark et al. (2011)" startWordPosition="4827" endWordPosition="4830">een the text section sections. Similarly for IPC sections, small but statistically significant improvements over the individual and pooled baselines are achieved by multi-task tuning and averaging over IPC sections, excepting C and D. However, an advantage of multi-task tuning over averaging is hard to establish. Note that the averaging techniques implicitly benefit from a larger tuning set. In order to ascertain that the improvements by averaging are not 12The aspect of averaging found in all of our multi-task learning techniques effectively controls for optimizer instability as mentioned in Clark et al. (2011). a A aw(t) r [k] D d=1 � II � � � �1 w(t−1) s �w(t−1) s[k] � 825 test pooled-6k significance abstract 0.3628 &lt; claim 0.4696 &lt; title 0.3174 &lt; Table 14: Multi-task tuning on 6,000 sentences pooled from text sections. “&lt;” denotes a statistically significant difference to the best result. simply due to increasing the size of the tuning set, we ran a control experiment where we tuned the model on a pooled development set of 3 x 2, 000 sentences for text sections and on a development set of 8 x 2, 000 sentences for IPC sections. The results given in table 14 show that tuning on a pooled set of 6,00</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan Clark, Chris Dyer, Alon Lavie, and Noah Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL’11), Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL’07),</booktitle>
<location>Prague, Czech Republic.</location>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e. 2007. Frustratingly easy domain adaptation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL’07), Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Alex Kulesza</author>
<author>Koby Crammer</author>
</authors>
<title>Multi-domain learning by confidenceweighted parameter combination.</title>
<date>2010</date>
<booktitle>Machine Learning,</booktitle>
<pages>79--123</pages>
<contexts>
<context position="7632" citStr="Dredze et al., 2010" startWordPosition="1165" endWordPosition="1168">ng from a separate SVM for each task, Evgeniou and Pontil (2004) present a regularization method that trades off optimization of the task-specific parameter vectors and the distance of each SVM to the average SVM. Equivalent formalizations replace parameter regularization by Bayesian prior distributions on the parameters (Finkel and Manning, 2009) or by augmentation of the feature space with domain independent features (Daum´e, 2007). Besides SVMs, several learning algorithms have been extended to the multi-task scenario in a parameter regularization setting, e.g., perceptron-type algorithms (Dredze et al., 2010) or boosting (Chapelle et al., 2011). Further variants include different formalizations of norms for parameter regularization, e.g., f1,2 regularization 819 (Obozinski et al., 2010) or Ei regularization (Quattoni et al., 2009), where only the features that are most important across all tasks are kept in the model. In our experiments, we apply parameter regularization for multi-task learning to minimum error rate training for patent translation. 3 Extraction of a parallel patent corpus from comparable data Our work on patent translation is based on the MAREC3 patent data corpus. MAREC contains </context>
</contexts>
<marker>Dredze, Kulesza, Crammer, 2010</marker>
<rawString>Mark Dredze, Alex Kulesza, and Koby Crammer. 2010. Multi-domain learning by confidenceweighted parameter combination. Machine Learning, 79:123–149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Duh</author>
<author>Katsuhito Sudoh</author>
<author>Hajime Tsukada</author>
</authors>
<title>Analysis of translation model adaptation in statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT’10),</booktitle>
<location>Paris, France.</location>
<contexts>
<context position="19631" citStr="Duh et al. (2010)" startWordPosition="3099" endWordPosition="3102">ated by highlighting best results in bold face (with more than one result highlighted if the difference is not significant).10 We investigate the same mixture and pooling techniques on the IPC sections we considered pairwise similar (see table 11). Somehow contradicting the former results, the mixture models perform significantly worse than the pooled model on three sections. This might be the result of inadequate tuning, since most of the time the MERT algorithm did not converge after the maximum number of iterations, due to the larger number of features when using several models. 9Following Duh et al. (2010), we use the alignment model trained on the pooled data set in the phrase extraction phase of the separate models. Similarly, we use a globally trained lexical reordering model. 10For assessing significance, we apply the approximate randomization method described in Riezler and Maxwell (2005). We consider pairwise differing results scoring a pvalue smaller than 0.05 as significant; the assessment is repeated three times and the average value is taken. 822 test train A B C D E F G H A 0.5349 0.4475 0.5472 0.4746 0.4438 0.4523 0.4318 0.4109 B 0.4846 0.4736 0.5161 0.4847 0.4578 0.4734 0.4396 0.42</context>
</contexts>
<marker>Duh, Sudoh, Tsukada, 2010</marker>
<rawString>Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada. 2010. Analysis of translation model adaptation in statistical machine translation. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT’10), Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theodoros Evgeniou</author>
<author>Massimiliano Pontil</author>
</authors>
<title>Regularized multi-task learning.</title>
<date>2004</date>
<booktitle>In Proceedings of the 10th ACM SIGKDD conference on knowledge discovery and data mining (KDD’04),</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="7076" citStr="Evgeniou and Pontil (2004)" startWordPosition="1084" endWordPosition="1087">ections, with larger pools of parallel data improving results. Ceaus¸u et al. (2011) find that language models always and translation model mostly benefit from larger pools of data from different sections. Models trained on pooled patent data are used as baselines in our approach. The machine learning community has developed several different formalizations of the central idea of trading off optimality of parameter vectors for each task-specific model and closeness of these model parameters to the average parameter vector across models. For example, starting from a separate SVM for each task, Evgeniou and Pontil (2004) present a regularization method that trades off optimization of the task-specific parameter vectors and the distance of each SVM to the average SVM. Equivalent formalizations replace parameter regularization by Bayesian prior distributions on the parameters (Finkel and Manning, 2009) or by augmentation of the feature space with domain independent features (Daum´e, 2007). Besides SVMs, several learning algorithms have been extended to the multi-task scenario in a parameter regularization setting, e.g., perceptron-type algorithms (Dredze et al., 2010) or boosting (Chapelle et al., 2011). Furthe</context>
<context position="23471" citStr="Evgeniou and Pontil (2004)" startWordPosition="3714" endWordPosition="3718">85*+ claim 0.4711 0.4681 0.4749*+ 0.475*+ 0.4734*+ title 0.3228 0.3152 0.3326*+ 0.3268*+ 0.3325*+ Table 12: Multi-task tuning on text sections. tuning test individual pooled average MMERT MMERT-average A 0.5187 0.5199 0.5213*+ 0.5195 0.5196 B 0.4877 0.4885 0.4908*+ 0.4911*+ 0.4921*+ C 0.5214 0.5175 0.5199*+ 0.5218+ 0.5162*+ D 0.4724 0.4730 0.4733 0.4736 0.4734 E 0.4666 0.4661 0.4679*+ 0.4669+ 0.4685*+ F 0.4794 0.4801 0.4811* 0.4821*+ 0.4830*+ G 0.4596 0.4576 0.4607+ 0.4606+ 0.4610*+ H 0.4573 0.4560 0.4578 0.4581+ 0.4581+ Table 13: Multi-task tuning on IPC sections. troduces an inductive bias. Evgeniou and Pontil (2004) propose a regularization method that balances task-specific parameter vectors and their distance to the average. The learning objective is to minimize task-specific loss functions ld across all tasks d with weight vectors wd, while keeping each parameter vector close to the average D �d 1 wd = wavg. This is enforced by minimizing the norm (here the `1-norm) of the difference of each task-specific weight vector to the avarage weight vector. ||wd − wavg||1 (1) The MMERT algorithm is given in figure 1. The algorithm starts with initial weights w(0). At each iteration step, the average of the par</context>
</contexts>
<marker>Evgeniou, Pontil, 2004</marker>
<rawString>Theodoros Evgeniou and Massimiliano Pontil. 2004. Regularized multi-task learning. In Proceedings of the 10th ACM SIGKDD conference on knowledge discovery and data mining (KDD’04), Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Mauro Cettolo</author>
</authors>
<title>IRSTLM: an open source toolkit for handling large scale language models.</title>
<date>2008</date>
<booktitle>In Proceedings of Interspeech,</booktitle>
<location>Brisbane, Australia.</location>
<contexts>
<context position="12538" citStr="Federico et al., 2008" startWordPosition="1940" endWordPosition="1943">nces. In order to obtain similar amounts of training data for each task along the topical dimension, we sampled 300,000 sentences from each IPC class for training, and 2,000 sentences for each IPC class for development and testing. A 1,947,542 B 2,522,995 C 2,263,375 D 299,742 E 353,910 F 1,012,808 G 2,066,132 H 1,754,573 Table 4: Distribution of IPC sections on claims. 4 Machine translation experiments 4.1 Individual task baselines For our experiments we used the phrase-based, open-source SMT toolkit Moses6 (Koehn et al., 2007). For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011). BLEU (Papineni et al., 2001) scores were computed up to 4-grams on lowercased data. Europarl-v6 MAREC BLEU OOV BLEU OOV abstract 0.1726 14.40% 0.3721 3.00% claim 0.2301 15.80% 0.4711 4.20% title 0.0964 26.00% 0.3228 9.20% Table 5: BLEU scores and OOV rate for Europarl baseline and MAREC model. Table 5 shows a first comparison of results of Moses models trained on 500,000 parallel sentences from patent text sections balanced over IPC classes, against Moses trained on 1.7 Million sentences of parliament proceedings from Europarl8 (Koehn, 2005).</context>
</contexts>
<marker>Federico, Bertoldi, Cettolo, 2008</marker>
<rawString>Marcello Federico, Nicola Bertoldi, and Mauro Cettolo. 2008. IRSTLM: an open source toolkit for handling large scale language models. In Proceedings of Interspeech, Brisbane, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Hierarchical bayesian domain adaptation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACLHLT’09),</booktitle>
<location>Boulder, CO.</location>
<contexts>
<context position="7361" citStr="Finkel and Manning, 2009" startWordPosition="1125" endWordPosition="1129">machine learning community has developed several different formalizations of the central idea of trading off optimality of parameter vectors for each task-specific model and closeness of these model parameters to the average parameter vector across models. For example, starting from a separate SVM for each task, Evgeniou and Pontil (2004) present a regularization method that trades off optimization of the task-specific parameter vectors and the distance of each SVM to the average SVM. Equivalent formalizations replace parameter regularization by Bayesian prior distributions on the parameters (Finkel and Manning, 2009) or by augmentation of the feature space with domain independent features (Daum´e, 2007). Besides SVMs, several learning algorithms have been extended to the multi-task scenario in a parameter regularization setting, e.g., perceptron-type algorithms (Dredze et al., 2010) or boosting (Chapelle et al., 2011). Further variants include different formalizations of norms for parameter regularization, e.g., f1,2 regularization 819 (Obozinski et al., 2010) or Ei regularization (Quattoni et al., 2009), where only the features that are most important across all tasks are kept in the model. In our experi</context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>Jenny Rose Finkel and Christopher D. Manning. 2009. Hierarchical bayesian domain adaptation. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACLHLT’09), Boulder, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Mixturemodel adaptation for SMT.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5996" citStr="Foster and Kuhn, 2007" startWordPosition="909" endWordPosition="913">vised generative modules such as translation models or language models to new tasks. For example, transductive approaches have used automatic translations of monolingual corpora for self-training modules of the generative SMT pipeline (Ueffing et al., 2007; Schwenk, 2008; Bertoldi and Federico, 2009). Other approaches have extracted parallel data from similar or comparable corpora (Zhao et al., 2004; Snover et al., 2008). Several approaches have been presented that train separate translation and language models on task-specific subsets of the data and combine them in different mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Foster et al., 2010). The latter kind of approach is applied in our work to multiple patent tasks. Multi-task learning efforts in patent translation have so far been restricted to experimental combinations of translation and language models from different sets of IPC sections. For example, Utiyama and Isahara (2007) and Tinsley et al. (2010) investigate translation and language models trained on different sets of patent sections, with larger pools of parallel data improving results. Ceaus¸u et al. (2011) find that language models always and translation model mostly</context>
<context position="18069" citStr="Foster and Kuhn (2007)" startWordPosition="2846" endWordPosition="2850">ling, we train the pooled models on the same amount of sentences as the individual models. For instance, the pooled model for the pairing of IPC section B and C is trained on a data set composed of 150,000 sentences from each IPC section. The pooled model for pairing data from abstracts and claims is trained on data composed of 250,000 sentences from each text section. Another approach to exploit commonalities between tasks is to train separate language and translation models9 on the sentences from each task and combine the models in the global log-linear model of the SMT framework, following Foster and Kuhn (2007) and Koehn and Schroeder (2007). Model combination is accomplished by adding additional language model and translation model features to the log-linear model and tuning the additional meta-parameters by standard minimum error rate training (Bertoldi et al., 2009). We try out mixture and pooling for all pairwise combinations of the three structural sections, for which we have high-quality data, i.e. abstract, claims and title. Due to the large number of possible combinations of IPC sections, we limit the experiments to pairs of similar sections, based on the A-distance measure. Table 10 lists t</context>
</contexts>
<marker>Foster, Kuhn, 2007</marker>
<rawString>George Foster and Roland Kuhn. 2007. Mixturemodel adaptation for SMT. In Proceedings of the Second Workshop on Statistical Machine Translation, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Pierre Isabelle</author>
<author>Roland Kuhn</author>
</authors>
<title>Translating structured documents.</title>
<date>2010</date>
<booktitle>In Proceedings of the 9th Conference of the Association for Machine Translation in the Americas (AMTA 2010),</booktitle>
<location>Denver, CO.</location>
<contexts>
<context position="6045" citStr="Foster et al., 2010" startWordPosition="918" endWordPosition="921">s or language models to new tasks. For example, transductive approaches have used automatic translations of monolingual corpora for self-training modules of the generative SMT pipeline (Ueffing et al., 2007; Schwenk, 2008; Bertoldi and Federico, 2009). Other approaches have extracted parallel data from similar or comparable corpora (Zhao et al., 2004; Snover et al., 2008). Several approaches have been presented that train separate translation and language models on task-specific subsets of the data and combine them in different mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Foster et al., 2010). The latter kind of approach is applied in our work to multiple patent tasks. Multi-task learning efforts in patent translation have so far been restricted to experimental combinations of translation and language models from different sets of IPC sections. For example, Utiyama and Isahara (2007) and Tinsley et al. (2010) investigate translation and language models trained on different sets of patent sections, with larger pools of parallel data improving results. Ceaus¸u et al. (2011) find that language models always and translation model mostly benefit from larger pools of data from different</context>
</contexts>
<marker>Foster, Isabelle, Kuhn, 2010</marker>
<rawString>George Foster, Pierre Isabelle, and Roland Kuhn. 2010. Translating structured documents. In Proceedings of the 9th Conference of the Association for Machine Translation in the Americas (AMTA 2010), Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: faster and smaller language model queries.</title>
<date>2011</date>
<booktitle>In Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation (WMT’11),</booktitle>
<location>Edinburgh, UK.</location>
<contexts>
<context position="12588" citStr="Heafield, 2011" startWordPosition="1950" endWordPosition="1951"> for each task along the topical dimension, we sampled 300,000 sentences from each IPC class for training, and 2,000 sentences for each IPC class for development and testing. A 1,947,542 B 2,522,995 C 2,263,375 D 299,742 E 353,910 F 1,012,808 G 2,066,132 H 1,754,573 Table 4: Distribution of IPC sections on claims. 4 Machine translation experiments 4.1 Individual task baselines For our experiments we used the phrase-based, open-source SMT toolkit Moses6 (Koehn et al., 2007). For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011). BLEU (Papineni et al., 2001) scores were computed up to 4-grams on lowercased data. Europarl-v6 MAREC BLEU OOV BLEU OOV abstract 0.1726 14.40% 0.3721 3.00% claim 0.2301 15.80% 0.4711 4.20% title 0.0964 26.00% 0.3228 9.20% Table 5: BLEU scores and OOV rate for Europarl baseline and MAREC model. Table 5 shows a first comparison of results of Moses models trained on 500,000 parallel sentences from patent text sections balanced over IPC classes, against Moses trained on 1.7 Million sentences of parliament proceedings from Europarl8 (Koehn, 2005). The best result on each section is indicated in b</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: faster and smaller language model queries. In Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation (WMT’11), Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Kifer</author>
<author>Shain Ben-David</author>
<author>Johannes Gehrke</author>
</authors>
<title>Detecting change in data streams.</title>
<date>2004</date>
<booktitle>In Proceedings of the 30th international conference on Very large data bases,</booktitle>
<location>Toronta, Ontario, Canada.</location>
<contexts>
<context position="16545" citStr="Kifer et al. (2004)" startWordPosition="2579" endWordPosition="2582">es. We can establish symmetric relationships between sections A and C, B and F as well as G and H, which means that the models are mutual runner-up on the other’s test section. The similarities of translation tasks established in the previous section can be confirmed by information-theoretic similarity measures that perform a pairwise comparison of the vocabulary probability distribution of each task-specific corpus. This distribution is calculated on the basis of the 500 most frequent words in the union of two corpora, normalized by vocabulary size. As metric we use the A-distance measure of Kifer et al. (2004). If A is the set of events on which the word distributions of two corpora are defined, then the A-distance is the supremum of the difference of probabilities assigned to the same event. Low distance means higher similarity. Table 9 shows the A-distance of corpora specific to IPC classes. The most similar section or sections – apart from the section itself on the diagonal – is indicated in bold face. The pairwise similarity of A and C, B and F, G and H obtained by BLEU score is confirmed. Furthermore, a close similarity between E and F is indicated. G and H (electricity and physics, respective</context>
</contexts>
<marker>Kifer, Ben-David, Gehrke, 2004</marker>
<rawString>Daniel Kifer, Shain Ben-David, and Johannes Gehrke. 2004. Detecting change in data streams. In Proceedings of the 30th international conference on Very large data bases, Toronta, Ontario, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Josh Schroeder</author>
</authors>
<title>Experiments in domain adaptation for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="6023" citStr="Koehn and Schroeder, 2007" startWordPosition="914" endWordPosition="917">s such as translation models or language models to new tasks. For example, transductive approaches have used automatic translations of monolingual corpora for self-training modules of the generative SMT pipeline (Ueffing et al., 2007; Schwenk, 2008; Bertoldi and Federico, 2009). Other approaches have extracted parallel data from similar or comparable corpora (Zhao et al., 2004; Snover et al., 2008). Several approaches have been presented that train separate translation and language models on task-specific subsets of the data and combine them in different mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Foster et al., 2010). The latter kind of approach is applied in our work to multiple patent tasks. Multi-task learning efforts in patent translation have so far been restricted to experimental combinations of translation and language models from different sets of IPC sections. For example, Utiyama and Isahara (2007) and Tinsley et al. (2010) investigate translation and language models trained on different sets of patent sections, with larger pools of parallel data improving results. Ceaus¸u et al. (2011) find that language models always and translation model mostly benefit from larger pools </context>
<context position="18100" citStr="Koehn and Schroeder (2007)" startWordPosition="2852" endWordPosition="2855">odels on the same amount of sentences as the individual models. For instance, the pooled model for the pairing of IPC section B and C is trained on a data set composed of 150,000 sentences from each IPC section. The pooled model for pairing data from abstracts and claims is trained on data composed of 250,000 sentences from each text section. Another approach to exploit commonalities between tasks is to train separate language and translation models9 on the sentences from each task and combine the models in the global log-linear model of the SMT framework, following Foster and Kuhn (2007) and Koehn and Schroeder (2007). Model combination is accomplished by adding additional language model and translation model features to the log-linear model and tuning the additional meta-parameters by standard minimum error rate training (Bertoldi et al., 2009). We try out mixture and pooling for all pairwise combinations of the three structural sections, for which we have high-quality data, i.e. abstract, claims and title. Due to the large number of possible combinations of IPC sections, we limit the experiments to pairs of similar sections, based on the A-distance measure. Table 10 lists the results for two combinations</context>
</contexts>
<marker>Koehn, Schroeder, 2007</marker>
<rawString>Philipp Koehn and Josh Schroeder. 2007. Experiments in domain adaptation for statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Birch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL 2007 Demo and Poster Sessions,</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="12450" citStr="Koehn et al., 2007" startWordPosition="1927" endWordPosition="1930">s on claims, with the smallest class accounting for 820 around 300,000 parallel sentences. In order to obtain similar amounts of training data for each task along the topical dimension, we sampled 300,000 sentences from each IPC class for training, and 2,000 sentences for each IPC class for development and testing. A 1,947,542 B 2,522,995 C 2,263,375 D 299,742 E 353,910 F 1,012,808 G 2,066,132 H 1,754,573 Table 4: Distribution of IPC sections on claims. 4 Machine translation experiments 4.1 Individual task baselines For our experiments we used the phrase-based, open-source SMT toolkit Moses6 (Koehn et al., 2007). For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011). BLEU (Papineni et al., 2001) scores were computed up to 4-grams on lowercased data. Europarl-v6 MAREC BLEU OOV BLEU OOV abstract 0.1726 14.40% 0.3721 3.00% claim 0.2301 15.80% 0.4711 4.20% title 0.0964 26.00% 0.3228 9.20% Table 5: BLEU scores and OOV rate for Europarl baseline and MAREC model. Table 5 shows a first comparison of results of Moses models trained on 500,000 parallel sentences from patent text sections balanced over IPC classes, against Moses </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Birch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Birch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the ACL 2007 Demo and Poster Sessions, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of Machine Translation Summit X,</booktitle>
<location>Phuket, Thailand.</location>
<contexts>
<context position="13137" citStr="Koehn, 2005" startWordPosition="2040" endWordPosition="2041">et al., 2008) and queried the model with KenLM (Heafield, 2011). BLEU (Papineni et al., 2001) scores were computed up to 4-grams on lowercased data. Europarl-v6 MAREC BLEU OOV BLEU OOV abstract 0.1726 14.40% 0.3721 3.00% claim 0.2301 15.80% 0.4711 4.20% title 0.0964 26.00% 0.3228 9.20% Table 5: BLEU scores and OOV rate for Europarl baseline and MAREC model. Table 5 shows a first comparison of results of Moses models trained on 500,000 parallel sentences from patent text sections balanced over IPC classes, against Moses trained on 1.7 Million sentences of parliament proceedings from Europarl8 (Koehn, 2005). The best result on each section is indicated in bold face. The Europarl model performs very poorly on all three sections in compar6http://statmt.org/moses/ 7http://sourceforge.net/projects/ irstlm/ 8http://www.statmt.org/europarl/ ison to the task-specific MAREC model, although the former has been learned on more than three times the amount of data. An analysis of the output of both system shows that the Europarl model suffers from two problems: Firstly, there is an obvious out of vocabulary (OOV) problem of the Europarl model compared to the MAREC model. Secondly, the Europarl model suffers</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of Machine Translation Summit X, Phuket, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
<author>Ben Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the joint conference of the International Committee on Computational Linguistics and the Association for Computational Linguistics (COLING-ACL’06),</booktitle>
<location>Sydney, Australia.</location>
<marker>Liang, Bouchard-Cˆot´e, Klein, Taskar, 2006</marker>
<rawString>Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and Ben Taskar. 2006. An end-to-end discriminative approach to machine translation. In Proceedings of the joint conference of the International Committee on Computational Linguistics and the Association for Computational Linguistics (COLING-ACL’06), Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guillaume Obozinski</author>
<author>Ben Taskar</author>
<author>Michael I Jordan</author>
</authors>
<title>Joint covariate selection and joint subspace selection for multiple classification problems.</title>
<date>2010</date>
<journal>Statistics and Computing,</journal>
<pages>20--231</pages>
<contexts>
<context position="7813" citStr="Obozinski et al., 2010" startWordPosition="1190" endWordPosition="1193">nce of each SVM to the average SVM. Equivalent formalizations replace parameter regularization by Bayesian prior distributions on the parameters (Finkel and Manning, 2009) or by augmentation of the feature space with domain independent features (Daum´e, 2007). Besides SVMs, several learning algorithms have been extended to the multi-task scenario in a parameter regularization setting, e.g., perceptron-type algorithms (Dredze et al., 2010) or boosting (Chapelle et al., 2011). Further variants include different formalizations of norms for parameter regularization, e.g., f1,2 regularization 819 (Obozinski et al., 2010) or Ei regularization (Quattoni et al., 2009), where only the features that are most important across all tasks are kept in the model. In our experiments, we apply parameter regularization for multi-task learning to minimum error rate training for patent translation. 3 Extraction of a parallel patent corpus from comparable data Our work on patent translation is based on the MAREC3 patent data corpus. MAREC contains over 19 million patent applications and granted patents in a standardized format from four patent organizations (European Patent Office (EP), World Intellectual Property Organizatio</context>
</contexts>
<marker>Obozinski, Taskar, Jordan, 2010</marker>
<rawString>Guillaume Obozinski, Ben Taskar, and Michael I. Jordan. 2010. Joint covariate selection and joint subspace selection for multiple classification problems. Statistics and Computing, 20:231–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology Conference and the 3rd Meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL’03),</booktitle>
<location>Edmonton, Cananda.</location>
<contexts>
<context position="22415" citStr="Och, 2003" startWordPosition="3564" endWordPosition="3565"> the title model that benefits from pooling and mixing with both abstracts and claims due to their richer data structure. 4.3 Multi-task minimum error rate training In contrast to task pooling and task mixtures, the specific setting addressed by multi-task minimum error rate training is one in which the generative SMT pipeline is not adaptable. Such situations arise if there are not enough data to train translation models or language models on the new tasks. However, we assume that there are enough parallel data available to perform meta-parameter tuning by minimum error rate training (MERT) (Och, 2003; Bertoldi et al., 2009) for each task. A generic algorithm for multi-task learning can be motivated as follows: Multi-task learning aims to take advantage of commonalities shared among tasks by learning several independent but related tasks together. Information is shared between tasks through a joint representation and in823 tuning test individual pooled average MMERT MMERT-average abstract 0.3721 0.362 0.3657*+ 0.3719+ 0.3685*+ claim 0.4711 0.4681 0.4749*+ 0.475*+ 0.4734*+ title 0.3228 0.3152 0.3326*+ 0.3268*+ 0.3325*+ Table 12: Multi-task tuning on text sections. tuning test individual poo</context>
<context position="27376" citStr="Och (2003)" startWordPosition="4424" endWordPosition="4425">n a balanced pooled model on a corpus composed of 170,000 sentences each from abstracts, claims and titles, i.e. 510,000 sentences in total. For both dimensions, for each task, we sampled 2,000 parallel sentences for development, developmenttesting, and testing from patents that were published in different years than the training data. We compare the multi-task experiments with two baselines. The first baseline is individual task learning, corresponding to standard separate MERT tuning on each section (individual). This results in three separately learned weight vectors 11MERT as presented in Och (2003) is not a gradientbased optimization techniquem, thus MMERT is strictly speaking only “inspired” by gradient descent optimization. for each task, where no information has been shared between the tasks. The second baseline simulates the setting where the sections are not differentiated at all. We tune the model on a pooled development set of 2,000 sentences that combines the same amount of data from all sections (pooled). This yields a single joint weight vector for all tasks optimized to perform well across all sections. Furthermore, we compare multi-task MERT tuning with two parameter averagi</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the Human Language Technology Conference and the 3rd Meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL’03), Edmonton, Cananda.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<tech>Technical Report IBM Research Division Technical Report, RC22176 (W0190-022),</tech>
<location>Yorktown Heights, N.Y.</location>
<contexts>
<context position="12618" citStr="Papineni et al., 2001" startWordPosition="1953" endWordPosition="1956">e topical dimension, we sampled 300,000 sentences from each IPC class for training, and 2,000 sentences for each IPC class for development and testing. A 1,947,542 B 2,522,995 C 2,263,375 D 299,742 E 353,910 F 1,012,808 G 2,066,132 H 1,754,573 Table 4: Distribution of IPC sections on claims. 4 Machine translation experiments 4.1 Individual task baselines For our experiments we used the phrase-based, open-source SMT toolkit Moses6 (Koehn et al., 2007). For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011). BLEU (Papineni et al., 2001) scores were computed up to 4-grams on lowercased data. Europarl-v6 MAREC BLEU OOV BLEU OOV abstract 0.1726 14.40% 0.3721 3.00% claim 0.2301 15.80% 0.4711 4.20% title 0.0964 26.00% 0.3228 9.20% Table 5: BLEU scores and OOV rate for Europarl baseline and MAREC model. Table 5 shows a first comparison of results of Moses models trained on 500,000 parallel sentences from patent text sections balanced over IPC classes, against Moses trained on 1.7 Million sentences of parliament proceedings from Europarl8 (Koehn, 2005). The best result on each section is indicated in bold face. The Europarl model p</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2001. Bleu: a method for automatic evaluation of machine translation. Technical Report IBM Research Division Technical Report, RC22176 (W0190-022), Yorktown Heights, N.Y.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ariadna Quattoni</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
<author>Trevor Darrell</author>
</authors>
<title>An efficient projection for Bl regularization.</title>
<date>2009</date>
<booktitle>In Proceedings of the 26th International Conference on Machine Learning (ICML’09),</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="7858" citStr="Quattoni et al., 2009" startWordPosition="1197" endWordPosition="1200"> formalizations replace parameter regularization by Bayesian prior distributions on the parameters (Finkel and Manning, 2009) or by augmentation of the feature space with domain independent features (Daum´e, 2007). Besides SVMs, several learning algorithms have been extended to the multi-task scenario in a parameter regularization setting, e.g., perceptron-type algorithms (Dredze et al., 2010) or boosting (Chapelle et al., 2011). Further variants include different formalizations of norms for parameter regularization, e.g., f1,2 regularization 819 (Obozinski et al., 2010) or Ei regularization (Quattoni et al., 2009), where only the features that are most important across all tasks are kept in the model. In our experiments, we apply parameter regularization for multi-task learning to minimum error rate training for patent translation. 3 Extraction of a parallel patent corpus from comparable data Our work on patent translation is based on the MAREC3 patent data corpus. MAREC contains over 19 million patent applications and granted patents in a standardized format from four patent organizations (European Patent Office (EP), World Intellectual Property Organization (WO), United States Patent and Trademark Of</context>
</contexts>
<marker>Quattoni, Carreras, Collins, Darrell, 2009</marker>
<rawString>Ariadna Quattoni, Xavier Carreras, Michael Collins, and Trevor Darrell. 2009. An efficient projection for Bl regularization. In Proceedings of the 26th International Conference on Machine Learning (ICML’09), Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>John Maxwell</author>
</authors>
<title>On some pitfalls in automatic evaluation and significance testing for MT.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL-05 Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization,</booktitle>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="19924" citStr="Riezler and Maxwell (2005)" startWordPosition="3143" endWordPosition="3146">sults, the mixture models perform significantly worse than the pooled model on three sections. This might be the result of inadequate tuning, since most of the time the MERT algorithm did not converge after the maximum number of iterations, due to the larger number of features when using several models. 9Following Duh et al. (2010), we use the alignment model trained on the pooled data set in the phrase extraction phase of the separate models. Similarly, we use a globally trained lexical reordering model. 10For assessing significance, we apply the approximate randomization method described in Riezler and Maxwell (2005). We consider pairwise differing results scoring a pvalue smaller than 0.05 as significant; the assessment is repeated three times and the average value is taken. 822 test train A B C D E F G H A 0.5349 0.4475 0.5472 0.4746 0.4438 0.4523 0.4318 0.4109 B 0.4846 0.4736 0.5161 0.4847 0.4578 0.4734 0.4396 0.4248 C 0.5047 0.4257 0.5719 0.462 0.4134 0.4249 0.409 0.3845 D 0.47 0.4387 0.5106 0.5167 0.4344 0.4435 0.407 0.3917 E 0.4486 0.4458 0.4681 0.4531 0.4771 0.4591 0.4073 0.4028 F 0.4595 0.4588 0.4761 0.4655 0.4517 0.4909 0.422 0.4188 G 0.4935 0.4489 0.5239 0.4629 0.4414 0.4565 0.4748 0.4532 H 0.46</context>
</contexts>
<marker>Riezler, Maxwell, 2005</marker>
<rawString>Stefan Riezler and John Maxwell. 2005. On some pitfalls in automatic evaluation and significance testing for MT. In Proceedings of the ACL-05 Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Investigations on largescale lightly-supervised training for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT’08),</booktitle>
<location>Hawaii.</location>
<contexts>
<context position="5646" citStr="Schwenk, 2008" startWordPosition="857" endWordPosition="858"> 23 million sentence pairs. 2 Related work Multi-task learning has mostly been discussed under the name of multi-domain adaptation in the area of statistical machine translation (SMT). If we consider domains as tasks, domain adaptation is a special two-task case of multi-task learning. Most previous work has concentrated on adapting unsupervised generative modules such as translation models or language models to new tasks. For example, transductive approaches have used automatic translations of monolingual corpora for self-training modules of the generative SMT pipeline (Ueffing et al., 2007; Schwenk, 2008; Bertoldi and Federico, 2009). Other approaches have extracted parallel data from similar or comparable corpora (Zhao et al., 2004; Snover et al., 2008). Several approaches have been presented that train separate translation and language models on task-specific subsets of the data and combine them in different mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Foster et al., 2010). The latter kind of approach is applied in our work to multiple patent tasks. Multi-task learning efforts in patent translation have so far been restricted to experimental combinations of translation </context>
</contexts>
<marker>Schwenk, 2008</marker>
<rawString>Holger Schwenk. 2008. Investigations on largescale lightly-supervised training for statistical machine translation. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT’08), Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Simianer</author>
<author>Katharina W¨aschle</author>
<author>Stefan Riezler</author>
</authors>
<title>Multi-task minimum error rate training for SMT. The Prague Bulletin of Mathematical Linguistics,</title>
<date>2011</date>
<pages>96--99</pages>
<marker>Simianer, W¨aschle, Riezler, 2011</marker>
<rawString>Patrick Simianer, Katharina W¨aschle, and Stefan Riezler. 2011. Multi-task minimum error rate training for SMT. The Prague Bulletin of Mathematical Linguistics, 96:99–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>Language and translation model adaptation using comparable corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’08),</booktitle>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="5799" citStr="Snover et al., 2008" startWordPosition="879" endWordPosition="882">statistical machine translation (SMT). If we consider domains as tasks, domain adaptation is a special two-task case of multi-task learning. Most previous work has concentrated on adapting unsupervised generative modules such as translation models or language models to new tasks. For example, transductive approaches have used automatic translations of monolingual corpora for self-training modules of the generative SMT pipeline (Ueffing et al., 2007; Schwenk, 2008; Bertoldi and Federico, 2009). Other approaches have extracted parallel data from similar or comparable corpora (Zhao et al., 2004; Snover et al., 2008). Several approaches have been presented that train separate translation and language models on task-specific subsets of the data and combine them in different mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Foster et al., 2010). The latter kind of approach is applied in our work to multiple patent tasks. Multi-task learning efforts in patent translation have so far been restricted to experimental combinations of translation and language models from different sets of IPC sections. For example, Utiyama and Isahara (2007) and Tinsley et al. (2010) investigate translation and la</context>
</contexts>
<marker>Snover, Dorr, Schwartz, 2008</marker>
<rawString>Matthew Snover, Bonnie Dorr, and Richard Schwartz. 2008. Language and translation model adaptation using comparable corpora. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’08), Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Tinsley</author>
<author>Andy Way</author>
<author>Paraic Sheridan</author>
</authors>
<title>PLuTO: MT for online patent translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 9th Conference of the Association for Machine Translation in the Americas (AMTA 2010),</booktitle>
<location>Denver, CO.</location>
<contexts>
<context position="6368" citStr="Tinsley et al. (2010)" startWordPosition="971" endWordPosition="974">ble corpora (Zhao et al., 2004; Snover et al., 2008). Several approaches have been presented that train separate translation and language models on task-specific subsets of the data and combine them in different mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Foster et al., 2010). The latter kind of approach is applied in our work to multiple patent tasks. Multi-task learning efforts in patent translation have so far been restricted to experimental combinations of translation and language models from different sets of IPC sections. For example, Utiyama and Isahara (2007) and Tinsley et al. (2010) investigate translation and language models trained on different sets of patent sections, with larger pools of parallel data improving results. Ceaus¸u et al. (2011) find that language models always and translation model mostly benefit from larger pools of data from different sections. Models trained on pooled patent data are used as baselines in our approach. The machine learning community has developed several different formalizations of the central idea of trading off optimality of parameter vectors for each task-specific model and closeness of these model parameters to the average paramet</context>
</contexts>
<marker>Tinsley, Way, Sheridan, 2010</marker>
<rawString>John Tinsley, Andy Way, and Paraic Sheridan. 2010. PLuTO: MT for online patent translation. In Proceedings of the 9th Conference of the Association for Machine Translation in the Americas (AMTA 2010), Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Jun’ichi Tsujii</author>
<author>Sophia Ananiadou</author>
</authors>
<title>Stochastic gradient descent training for Bl-regularized log-linear models with cumulative penalty.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL-IJCNLP’09),</booktitle>
<contexts>
<context position="25477" citStr="Tsuruoka et al., 2009" startWordPosition="4092" endWordPosition="4096">tasks. MMERT(w(0), D, {ld}Dd=1): for t = 1,...,T do (t) 1 D (t−1) wavg = D �d=1 wd for d = 1, ... , D parallel do wd = MERT(w(t−1) (t) d , ld) fork= 1,...,K do if w[k](dt) − w(t) [k] &gt; 0 then avg wd [k] = max(w(t) (t) avg[k], w(t) d [k]−A) else if w(dt) [k] − w(t) avg[k] &lt; 0 then w(t) d [k] = min(w(t) avg[k], w(t) d [k] + A) end if end for end for end for (T) (T)(T) return w1 , ... , wD , wavg Figure 1: Multi-task MERT. D d=1 ld(wd) + A min w1,...,wD D d=1 824 The weight updates and the clipping strategy can be motivated in a framework of gradient descent optimization under `1-regularization (Tsuruoka et al., 2009). Assuming MERT as algorithmic minimizer11 of the loss function ld in equation 1, the weight update towards the average follows from the subgradient of the i1 regularizer. Since w(t) avg is taken as average over weights wd from the step before, the term w(t) (t−1) is conavg stant with respect to w(d t), leading to the following subgradient (where sgn(x) = 1 if x &gt; 0, sgn(x) = −1 if x &lt; 0, and sgn(x) = 0 if x = 0): � ��D � � wd − (t) 1 D s=1 D = A sgn w(t) [k] − 1 1 D s=1 Gradient descent minimization tells us to move in the opposite direction of the subgradient, thus motivating the addition or</context>
</contexts>
<marker>Tsuruoka, Tsujii, Ananiadou, 2009</marker>
<rawString>Yoshimasa Tsuruoka, Jun’ichi Tsujii, and Sophia Ananiadou. 2009. Stochastic gradient descent training for Bl-regularized log-linear models with cumulative penalty. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL-IJCNLP’09), Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
<author>Gholamreza Haffari</author>
<author>Anoop Sarkar</author>
</authors>
<title>Transductive learning for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL’07),</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5631" citStr="Ueffing et al., 2007" startWordPosition="853" endWordPosition="856"> patent corpus of over 23 million sentence pairs. 2 Related work Multi-task learning has mostly been discussed under the name of multi-domain adaptation in the area of statistical machine translation (SMT). If we consider domains as tasks, domain adaptation is a special two-task case of multi-task learning. Most previous work has concentrated on adapting unsupervised generative modules such as translation models or language models to new tasks. For example, transductive approaches have used automatic translations of monolingual corpora for self-training modules of the generative SMT pipeline (Ueffing et al., 2007; Schwenk, 2008; Bertoldi and Federico, 2009). Other approaches have extracted parallel data from similar or comparable corpora (Zhao et al., 2004; Snover et al., 2008). Several approaches have been presented that train separate translation and language models on task-specific subsets of the data and combine them in different mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Foster et al., 2010). The latter kind of approach is applied in our work to multiple patent tasks. Multi-task learning efforts in patent translation have so far been restricted to experimental combinations </context>
</contexts>
<marker>Ueffing, Haffari, Sarkar, 2007</marker>
<rawString>Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar. 2007. Transductive learning for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL’07), Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masao Utiyama</author>
<author>Hitoshi Isahara</author>
</authors>
<title>A Japanese-English patent parallel corpus.</title>
<date>2007</date>
<booktitle>In Proceedings of MT</booktitle>
<location>Summit XI, Copenhagen, Denmark.</location>
<contexts>
<context position="6342" citStr="Utiyama and Isahara (2007)" startWordPosition="966" endWordPosition="969">el data from similar or comparable corpora (Zhao et al., 2004; Snover et al., 2008). Several approaches have been presented that train separate translation and language models on task-specific subsets of the data and combine them in different mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Foster et al., 2010). The latter kind of approach is applied in our work to multiple patent tasks. Multi-task learning efforts in patent translation have so far been restricted to experimental combinations of translation and language models from different sets of IPC sections. For example, Utiyama and Isahara (2007) and Tinsley et al. (2010) investigate translation and language models trained on different sets of patent sections, with larger pools of parallel data improving results. Ceaus¸u et al. (2011) find that language models always and translation model mostly benefit from larger pools of data from different sections. Models trained on pooled patent data are used as baselines in our approach. The machine learning community has developed several different formalizations of the central idea of trading off optimality of parameter vectors for each task-specific model and closeness of these model paramet</context>
<context position="31485" citStr="Utiyama and Isahara (2007)" startWordPosition="5107" endWordPosition="5110">s well as the best results in table 13, except for two cases (indicated by “&lt;”) (see table 15). This is due to the smaller differences between best and worst results for tuning on IPC sections compared to tuning on text sections, indicating that IPC sections are less well suited for multi-task tuning than the textual domains. test pooled-16k significance A 0.5177 &lt; B 0.4920 C 0.5133 &lt; D 0.4737 E 0.4685 F 0.4832 G 0.4608 H 0.4579 Table 15: Multi-task tuning on 16,000 sentences pooled from IPC sections. “&lt;” denotes a statistically significant difference to the best result. ley et al. (2010) and Utiyama and Isahara (2007). A caveat in this situation is that data need to be from the general patent domain, as shown by the inferior performance of a large Europarl-trained model compared to a small patent-trained model. The goal of this paper is to analyze patent data along the topical dimension of IPC classes and along the structural dimension of textual sections. Instead of trying to beat a pooling baseline that simply increases the data size, our research goal is to investigate whether different subtasks along these dimensions share commonalities that can fruitfully be exploited by multi-task learning in machine</context>
</contexts>
<marker>Utiyama, Isahara, 2007</marker>
<rawString>Masao Utiyama and Hitoshi Isahara. 2007. A Japanese-English patent parallel corpus. In Proceedings of MT Summit XI, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Matthias Eck</author>
<author>Stephan Vogel</author>
</authors>
<title>Language model adaptation for statistical machine translation with structured query models.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING’04),</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="5777" citStr="Zhao et al., 2004" startWordPosition="875" endWordPosition="878">ion in the area of statistical machine translation (SMT). If we consider domains as tasks, domain adaptation is a special two-task case of multi-task learning. Most previous work has concentrated on adapting unsupervised generative modules such as translation models or language models to new tasks. For example, transductive approaches have used automatic translations of monolingual corpora for self-training modules of the generative SMT pipeline (Ueffing et al., 2007; Schwenk, 2008; Bertoldi and Federico, 2009). Other approaches have extracted parallel data from similar or comparable corpora (Zhao et al., 2004; Snover et al., 2008). Several approaches have been presented that train separate translation and language models on task-specific subsets of the data and combine them in different mixture models (Foster and Kuhn, 2007; Koehn and Schroeder, 2007; Foster et al., 2010). The latter kind of approach is applied in our work to multiple patent tasks. Multi-task learning efforts in patent translation have so far been restricted to experimental combinations of translation and language models from different sets of IPC sections. For example, Utiyama and Isahara (2007) and Tinsley et al. (2010) investig</context>
</contexts>
<marker>Zhao, Eck, Vogel, 2004</marker>
<rawString>Bing Zhao, Matthias Eck, and Stephan Vogel. 2004. Language model adaptation for statistical machine translation with structured query models. In Proceedings of the 20th International Conference on Computational Linguistics (COLING’04), Geneva, Switzerland.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>