<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.019729">
<title confidence="0.997287">
Transition-based Dependency Parsing with Rich Non-local Features
</title>
<author confidence="0.998605">
Yue Zhang Joakim Nivre
</author>
<affiliation confidence="0.9964225">
University of Cambridge Uppsala University
Computer Laboratory Department of Linguistics and Philology
</affiliation>
<email confidence="0.9979">
yue.zhang@cl.cam.ac.uk joakim.nivre@lingfil.uu.se
</email>
<sectionHeader confidence="0.995626" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999225933333334">
Transition-based dependency parsers gener-
ally use heuristic decoding algorithms but can
accommodate arbitrarily rich feature represen-
tations. In this paper, we show that we can im-
prove the accuracy of such parsers by consid-
ering even richer feature sets than those em-
ployed in previous systems. In the standard
Penn Treebank setup, our novel features im-
prove attachment score form 91.4% to 92.9%,
giving the best results so far for transition-
based parsing and rivaling the best results
overall. For the Chinese Treebank, they give a
signficant improvement of the state of the art.
An open source release of our parser is freely
available.
</bodyText>
<sectionHeader confidence="0.998996" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99960388">
Transition-based dependency parsing (Yamada and
Matsumoto, 2003; Nivre et al., 2006b; Zhang and
Clark, 2008; Huang and Sagae, 2010) utilize a deter-
ministic shift-reduce process for making structural
predictions. Compared to graph-based dependency
parsing, it typically offers linear time complexity
and the comparative freedom to define non-local fea-
tures, as exemplified by the comparison between
MaltParser and MSTParser (Nivre et al., 2006b; Mc-
Donald et al., 2005; McDonald and Nivre, 2007).
Recent research has addressed two potential dis-
advantages of systems like MaltParser. In the
aspect of decoding, beam-search (Johansson and
Nugues, 2007; Zhang and Clark, 2008; Huang et
al., 2009) and partial dynamic-programming (Huang
and Sagae, 2010) have been applied to improve upon
greedy one-best search, and positive results were re-
ported. In the aspect of training, global structural
learning has been used to replace local learning on
each decision (Zhang and Clark, 2008; Huang et al.,
2009), although the effect of global learning has not
been separated out and studied alone.
In this short paper, we study a third aspect in a
statistical system: feature definition. Representing
the type of information a statistical system uses to
make predictions, feature templates can be one of
the most important factors determining parsing ac-
curacy. Various recent attempts have been made
to include non-local features into graph-based de-
pendency parsing (Smith and Eisner, 2008; Martins
et al., 2009; Koo and Collins, 2010). Transition-
based parsing, by contrast, can easily accommodate
arbitrarily complex representations involving non-
local features. Complex non-local features, such as
bracket matching and rhythmic patterns, are used
in transition-based constituency parsing (Zhang and
Clark, 2009; Wang et al., 2006), and most transition-
based dependency parsers incorporate some non-
local features, but current practice is nevertheless to
use a rather restricted set of features, as exemplified
by the default feature models in MaltParser (Nivre et
al., 2006a). We explore considerably richer feature
representations and show that they improve parsing
accuracy significantly.
In standard experiments using the Penn Treebank,
our parser gets an unlabeled attachment score of
92.9%, which is the best result achieved with a
transition-based parser and comparable to the state
of the art. For the Chinese Treebank, our parser gets
a score of 86.0%, the best reported result so far.
</bodyText>
<page confidence="0.967019">
188
</page>
<note confidence="0.7942855">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 188–193,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.89086" genericHeader="method">
2 The Transition-based Parsing Algorithm
</sectionHeader>
<bodyText confidence="0.988957545454546">
In a typical transition-based parsing process, the in-
put words are put into a queue and partially built
structures are organized by a stack. A set of shift-
reduce actions are defined, which consume words
from the queue and build the output parse. Recent
research have focused on action sets that build pro-
jective dependency trees in an arc-eager (Nivre et
al., 2006b; Zhang and Clark, 2008) or arc-standard
(Yamada and Matsumoto, 2003; Huang and Sagae,
2010) process. We adopt the arc-eager system1, for
which the actions are:
</bodyText>
<listItem confidence="0.973693777777778">
• Shift, which removes the front of the queue
and pushes it onto the top of the stack;
• Reduce, which pops the top item off the stack;
• LeftArc, which pops the top item off the
stack, and adds it as a modifier to the front of
the queue;
• RightArc, which removes the front of the
queue, pushes it onto the stack and adds it as
a modifier to the top of the stack.
</listItem>
<bodyText confidence="0.999913416666666">
Further, we follow Zhang and Clark (2008) and
Huang et al. (2009) and use the generalized percep-
tron (Collins, 2002) for global learning and beam-
search for decoding. Unlike both earlier global-
learning parsers, which only perform unlabeled
parsing, we perform labeled parsing by augmenting
the LeftArc and RightArc actions with the set
of dependency labels. Hence our work is in line with
Titov and Henderson (2007) in using labeled transi-
tions with global learning. Moreover, we will see
that label information can actually improve link ac-
curacy.
</bodyText>
<sectionHeader confidence="0.9896" genericHeader="method">
3 Feature Templates
</sectionHeader>
<bodyText confidence="0.9995862">
At each step during a parsing process, the
parser configuration can be represented by a tuple
(S, N, A), where S is the stack, N is the queue of
incoming words, and A is the set of dependency
arcs that have been built. Denoting the top of stack
</bodyText>
<footnote confidence="0.7952055">
1It is very likely that the type of features explored in this
paper would be beneficial also for the arc-standard system, al-
though the exact same feature templates would not be applicable
because of differences in the parsing order.
</footnote>
<table confidence="0.9993422">
from single words
S0wp; S0w; S0p; N0wp; N0w; N0p;
N1wp; N1w; N1p; N2wp; N2w; N2p;
from word pairs
S0wpN0wp; S0wpN0w; S0wN0wp; S0wpN0p;
S0pN0wp; S0wN0w; S0pN0p
N0pN1p
from three words
N0pN1pN2p; S0pN0pN1p; S0hpS0pN0p;
S0pS0lpN0p; S0pS0rpN0p; S0pN0pN0lp
</table>
<tableCaption confidence="0.987913">
Table 1: Baseline feature templates.
</tableCaption>
<table confidence="0.686069125">
w – word; p – POS-tag.
distance
S0wd; S0pd; N0wd; N0pd;
S0wN0wd; S0pN0pd;
valency
S0wvr; S0pvr; S0wvl; S0pvl; N0wvl; N0pvl;
unigrams
S0hw; S0hp; S0l; S0lw; S0lp; S0ll;
S0rw; S0rp; S0rl;N0lw; N0lp; N0ll;
third-order
S0h2w; S0h2p; S0hl; S0l2w; S0l2p; S0l2l;
S0r2w; S0r2p; S0r2l; N0l2w; N0l2p; N0l2l;
S0pS0lpS0l2p; S0pS0rpS0r2p;
S0pS0hpS0h2p; N0pN0lpN0l2p;
label set
S0wsr; S0psr; S0wsl; S0psl; N0wsl; N0psl;
</table>
<tableCaption confidence="0.986907">
Table 2: New feature templates.
</tableCaption>
<bodyText confidence="0.999035666666666">
w – word; p – POS-tag; vl, vr – valency; l –
dependency label, sl, sr – labelset.
with S0, the front items from the queue with N0,
N1, and N2, the head of S0 (if any) with S0h, the
leftmost and rightmost modifiers of S0 (if any) with
S0l and S0r, respectively, and the leftmost modifier
of N0 (if any) with N0l, the baseline features are
shown in Table 1. These features are mostly taken
from Zhang and Clark (2008) and Huang and Sagae
(2010), and our parser reproduces the same accura-
cies as reported by both papers. In this table, w and
p represents the word and POS-tag, respectively. For
example, S0pN0wp represents the feature template
that takes the word and POS-tag of N0, and com-
bines it with the word of S0.
</bodyText>
<page confidence="0.9949">
189
</page>
<bodyText confidence="0.995835069444444">
In this short paper, we extend the baseline feature
templates with the following:
Distance between S0 and N0
Direction and distance between a pair of head and
modifier have been used in the standard feature
templates for maximum spanning tree parsing (Mc-
Donald et al., 2005). Distance information has
also been used in the easy-first parser of (Goldberg
and Elhadad, 2010). For a transition-based parser,
direction information is indirectly included in the
LeftArc and RightArc actions. We add the dis-
tance between S0 and N0 to the feature set by com-
bining it with the word and POS-tag of S0 and N0,
as shown in Table 2.
It is worth noticing that the use of distance in-
formation in our transition-based model is different
from that in a typical graph-based parser such as
MSTParser. The distance between S0 and N0 will
correspond to the distance between a pair of head
and modifier when an LeftArc action is taken, for
example, but not when a Shift action is taken.
Valency of S0 and N0
The number of modifiers to a given head is used
by the graph-based submodel of Zhang and Clark
(2008) and the models of Martins et al. (2009) and
Sagae and Tsujii (2007). We include similar infor-
mation in our model. In particular, we calculate the
number of left and right modifiers separately, call-
ing them left valency and right valency, respectively.
Left and right valencies are represented by vl and vr
in Table 2, respectively. They are combined with the
word and POS-tag of S0 and N0 to form new feature
templates.
Again, the use of valency information in our
transition-based parser is different from the afore-
mentioned graph-based models. In our case,
valency information is put into the context of the
shift-reduce process, and used together with each
action to give a score to the local decision.
Unigram information for S0h, S0l, S0r and N0l
The head, left/rightmost modifiers of S0 and the
leftmost modifier of N0 have been used by most
arc-eager transition-based parsers we are aware of
through the combination of their POS-tag with infor-
mation from S0 and N0. Such use is exemplified by
the feature templates “from three words” in Table 1.
We further use their word and POS-tag information
as “unigram” features in Table 2. Moreover, we
include the dependency label information in the
unigram features, represented by l in the table. Uni-
gram label information has been used in MaltParser
(Nivre et al., 2006a; Nivre, 2006).
Third-order features of S0 and N0
Higher-order context features have been used by
graph-based dependency parsers to improve accura-
cies (Carreras, 2007; Koo and Collins, 2010). We
include information of third order dependency arcs
in our new feature templates, when available. In
Table 2, S0h2, S0l2, S0r2 and N0l2 refer to the head
of S0h, the second leftmost modifier and the second
rightmost modifier of S0, and the second leftmost
modifier of N0, respectively. The new templates
include unigram word, POS-tag and dependency
labels of S0h2, S0l2, S0r2 and N0l2, as well as
POS-tag combinations with S0 and N0.
Set of dependency labels with S0 and N0
As a more global feature, we include the set of
unique dependency labels from the modifiers of S0
and N0. This information is combined with the word
and POS-tag of S0 and N0 to make feature templates.
In Table 2, sl and sr stands for the set of labels on
the left and right of the head, respectively.
</bodyText>
<sectionHeader confidence="0.999651" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999713933333333">
Our experiments were performed using the Penn
Treebank (PTB) and Chinese Treebank (CTB) data.
We follow the standard approach to split PTB3, using
sections 2 – 21 for training, section 22 for develop-
ment and 23 for final testing. Bracketed sentences
from PTB were transformed into dependency for-
mats using the Penn2Malt tool.2 Following Huang
and Sagae (2010), we assign POS-tags to the training
data using ten-way jackknifing. We used our imple-
mentation of the Collins (2002) tagger (with 97.3%
accuracy on a standard Penn Treebank test) to per-
form POS-tagging. For all experiments, we set the
beam size to 64 for the parser, and report unlabeled
and labeled attachment scores (UAS, LAS) and un-
labeled exact match (UEM) for evaluation.
</bodyText>
<footnote confidence="0.992756">
2http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html
</footnote>
<page confidence="0.986062">
190
</page>
<table confidence="0.999602142857143">
feature UAS UEM
baseline 92.18% 45.76%
+distance 92.25% 46.24%
+valency 92.49% 47.65%
+unigrams 92.89% 48.47%
+third-order 93.07% 49.59%
+label set 93.14% 50.12%
</table>
<tableCaption confidence="0.995275333333333">
Table 3: The effect of new features on the development
set for English. UAS = unlabeled attachment score; UEM
= unlabeled exact match.
</tableCaption>
<table confidence="0.999879444444444">
UAS UEM LAS
Z&amp;C08 transition 91.4% 41.8% —
H&amp;S10 91.4% — —
this paper baseline 91.4% 42.5% 90.1%
this paper extended 92.9% 48.0% 91.8%
MSTParser 91.5% 42.5% —
K08 standard 92.0% — —
K&amp;C10 model 1 93.0% — —
K&amp;C10 model 2 92.9% — —
</table>
<tableCaption confidence="0.979341333333334">
Table 4: Final test accuracies for English. UAS = unla-
beled attachment score; UEM = unlabeled exact match;
LAS = labeled attachment score.
</tableCaption>
<subsectionHeader confidence="0.992803">
4.1 Development Experiments
</subsectionHeader>
<bodyText confidence="0.99999125">
Table 3 shows the effect of new features on the de-
velopment test data for English. We start with the
baseline features in Table 1, and incrementally add
the distance, valency, unigram, third-order and label
set feature templates in Table 2. Each group of new
feature templates improved the accuracies over the
previous system, and the final accuracy with all new
features was 93.14% in unlabeled attachment score.
</bodyText>
<subsectionHeader confidence="0.988741">
4.2 Final Test Results
</subsectionHeader>
<bodyText confidence="0.998894272727273">
Table 4 shows the final test results of our
parser for English. We include in the table
results from the pure transition-based parser of
Zhang and Clark (2008) (row ‘Z&amp;C08 transition’),
the dynamic-programming arc-standard parser of
Huang and Sagae (2010) (row ‘H&amp;S10’), and graph-
based models including MSTParser (McDonald and
Pereira, 2006), the baseline feature parser of Koo et
al. (2008) (row ‘K08 baeline’), and the two models
of Koo and Collins (2010). Our extended parser sig-
nificantly outperformed the baseline parser, achiev-
</bodyText>
<table confidence="0.996333">
UAS UEM LAS
Z&amp;C08 transition 84.3% 32.8% —
H&amp;S10 85.2% 33.7% —
this paper extended 86.0% 36.9% 84.4%
</table>
<tableCaption confidence="0.98509">
Table 5: Final test accuracies for Chinese. UAS = unla-
beled attachment score; UEM = unlabeled exact match;
LAS = labeled attachment score.
</tableCaption>
<bodyText confidence="0.998676947368421">
ing the highest attachment score reported for a
transition-based parser, comparable to those of the
best graph-based parsers.
Our experiments were performed on a Linux plat-
form with a 2GHz CPU. The speed of our baseline
parser was 50 sentences per second. With all new
features added, the speed dropped to 29 sentences
per second.
As an alternative to Penn2Malt, bracketed sen-
tences can also be transformed into Stanford depen-
dencies (De Marneffe et al., 2006). Our parser gave
93.5% UAS, 91.9% LAS and 52.1% UEM when
trained and evaluated on Stanford basic dependen-
cies, which are projective dependency trees. Cer et
al. (2010) report results on Stanford collapsed de-
pendencies, which allow a word to have multiple
heads and therefore cannot be produced by a reg-
ular dependency parser. Their results are relevant
although not directly comparable with ours.
</bodyText>
<subsectionHeader confidence="0.998823">
4.3 Chinese Test Results
</subsectionHeader>
<bodyText confidence="0.999963285714286">
Table 5 shows the results of our final parser, the pure
transition-based parser of Zhang and Clark (2008),
and the parser of Huang and Sagae (2010) on Chi-
nese. We take the standard split of CTB and use gold
segmentation and POS-tags for the input. Our scores
for this test set are the best reported so far and sig-
nificantly better than the previous systems.
</bodyText>
<sectionHeader confidence="0.993859" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999955333333333">
We have shown that enriching the feature repre-
sentation significantly improves the accuracy of our
transition-based dependency parser. The effect of
the new features appears to outweigh the effect of
combining transition-based and graph-based mod-
els, reported by Zhang and Clark (2008), as well
as the effect of using dynamic programming, as in-
Huang and Sagae (2010). This shows that feature
definition is a crucial aspect of transition-based pars-
</bodyText>
<page confidence="0.962576">
191
</page>
<bodyText confidence="0.992358260869565">
ing. In fact, some of the new feature templates in this Yoav Goldberg and Michael Elhadad. 2010. An effi-
paper, such as distance and valency, are among those cient algorithm for easy-first non-directional depen-
which are in the graph-based submodel of Zhang dency parsing. In Porceedings of HLT/NAACL, pages
and Clark (2008), but not the transition-based sub- 742–750, Los Angeles, California, June.
model. Therefore our new features to some extent Liang Huang and Kenji Sagae. 2010. Dynamic pro-
achieved the same effect as their model combina- gramming for linear-time incremental parsing. In Pro-
tion. The new features are also hard to use in dy- ceedings ofACL, pages 1077–1086, Uppsala, Sweden,
namic programming because they add considerable July.
complexity to the parse items. Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Enriched feature representations have been stud- Bilingually-constrained (monolingual) shift-reduce
ied as an important factor for improving the accu- parsing. In Proceedings ofEMNLP, pages 1222–1231,
racies of graph-based dependency parsing also. Re- Singapore.
cent research including the use of loopy belief net- Richard Johansson and Pierre Nugues. 2007. Incre-
work (Smith and Eisner, 2008), integer linear pro- mental dependency parsing using online learning. In
gramming (Martins et al., 2009) and an improved Proceedings of CoNLL/EMNLP, pages 1134–1138,
dynamic programming algorithm (Koo and Collins, Prague, Czech Republic.
2010) can be seen as methods to incorporate non- Terry Koo and Michael Collins. 2010. Efficient third-
local features into a graph-based model. order dependency parsers. In Proceedings of ACL,
An open source release of our parser, together pages 1–11, Uppsala, Sweden, July.
with trained models for English and Chinese, are Terry Koo, Xavier Carreras, and Michael Collins. 2008.
freely available.3 Simple semi-supervised dependency parsing. In Pro-
Acknowledgements ceedings of ACL/HLT, pages 595–603, Columbus,
We thank the anonymous reviewers for their useful Ohio, June.
</bodyText>
<figureCaption confidence="0.865279181818182">
comments. Yue Zhang is supported by the Euro- Andre Martins, Noah Smith, and Eric Xing. 2009. Con-
pean Union Seventh Framework Programme (FP7- cise integer linear programming formulations for de-
ICT-2009-4) under grant agreement no. 247762. pendency parsing. In Proceedings of ACL/IJCNLP,
pages 342–350, Suntec, Singapore, August.
Ryan McDonald and Joakim Nivre. 2007. Characteriz-
ing the errors of data-driven dependency parsing mod-
els. In Proceedings of EMNLP/CoNLL, pages 122–
131, Prague, Czech Republic.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing algo-
rithms. In Proceedings ofEACL, pages 81–88, Trento,
Italy, April.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL, pages 91–98, Ann
Arbor, Michigan, June.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006a.
Maltparser: A data-driven parser-generator for depen-
dency parsing. pages 2216–2219.
Joakim Nivre, Johan Hall, Jens Nilsson, G¨uls¸en Eryiˇgit,
and Svetoslav Marinov. 2006b. Labeled pseudo-
projective dependency parsing with support vector ma-
</figureCaption>
<reference confidence="0.972686925925926">
chines. In Proceedings of CoNLL, pages 221–225,
New York, USA.
Joakim Nivre. 2006. Inductive Dependency Parsing.
Springer.
Kenji Sagae and Jun’ichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with LR models and parser
ensembles. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 1044–1050,
References
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session ofEMNLP/CoNLL, pages
957–961, Prague, Czech Republic.
Daniel Cer, Marie-Catherine de Marneffe, Dan Juraf-
sky, and Chris Manning. 2010. Parsing to stan-
ford dependencies: Trade-offs between speed and ac-
curacy. In Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC’10).
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP, pages 1–8, Philadelphia, USA.
Marie-catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings ofLREC.
3http://www.sourceforge.net/projects/zpar. version 0.5.
192
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
David Smith and Jason Eisner. 2008. Dependency pars-
ing by belief propagation. In Proceedings of EMNLP,
pages 145–156, Honolulu, Hawaii, October.
Ivan Titov and James Henderson. 2007. A latent variable
model for generative dependency parsing. In Proceed-
ings of IWPT, pages 144–155, Prague, Czech Repub-
lic, June.
Xinhao Wang, Xiaojun Lin, Dianhai Yu, Hao Tian, and
Xihong Wu. 2006. Chinese word segmentation with
maximum entropy and n-gram language model. In
Proceedings of SIGHAN Workshop, pages 138–141,
Sydney, Australia, July.
H Yamada and Y Matsumoto. 2003. Statistical depen-
dency analysis using support vector machines. In Pro-
ceedings ofIWPT, Nancy, France.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-based
and transition-based dependency parsing using beam-
search. In Proceedings ofEMNLP, Hawaii, USA.
Yue Zhang and Stephen Clark. 2009. Transition-based
parsing of the Chinese Treebank using a global dis-
criminative model. In Proceedings of IWPT, Paris,
France, October.
</reference>
<page confidence="0.999249">
193
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.434055">
<title confidence="0.99946">Transition-based Dependency Parsing with Rich Non-local Features</title>
<author confidence="0.999575">Yue Zhang Joakim Nivre</author>
<affiliation confidence="0.884856">University of Cambridge Uppsala University Computer Laboratory Department of Linguistics and Philology</affiliation>
<email confidence="0.69225">yue.zhang@cl.cam.ac.ukjoakim.nivre@lingfil.uu.se</email>
<abstract confidence="0.980999">Transition-based dependency parsers generally use heuristic decoding algorithms but can accommodate arbitrarily rich feature representations. In this paper, we show that we can improve the accuracy of such parsers by considering even richer feature sets than those employed in previous systems. In the standard Penn Treebank setup, our novel features improve attachment score form 91.4% to 92.9%, giving the best results so far for transitionbased parsing and rivaling the best results overall. For the Chinese Treebank, they give a signficant improvement of the state of the art. An open source release of our parser is freely available.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>