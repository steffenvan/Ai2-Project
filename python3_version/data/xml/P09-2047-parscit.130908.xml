<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.019932">
<title confidence="0.914674">
Query Segmentation Based on Eigenspace Similarity
</title>
<author confidence="0.993438">
Chao Zhang † ‡ Nan Sun ‡ Xia Hu ‡ Tingzhu Huang † Tat-Seng Chua ‡
</author>
<affiliation confidence="0.964087">
†School of Applied Math ‡School of Computing
University of Electronic Science National University of Singapore,
and Technology of China,
</affiliation>
<address confidence="0.917159">
Chengdu, 610054, P.R. China Computing 1, Singapore 117590
</address>
<email confidence="0.9673645">
zhangcha@comp.nus.edu.sg {sunn,huxia,chuats}@comp.nus.edu.sg
tzhuang@uestc.edu.cn
</email>
<sectionHeader confidence="0.993665" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997598">
Query segmentation is essential to query
processing. It aims to tokenize query
words into several semantic segments and
help the search engine to improve the
precision of retrieval. In this paper, we
present a novel unsupervised learning ap-
proach to query segmentation based on
principal eigenspace similarity of query-
word-frequency matrix derived from web
statistics. Experimental results show that
our approach could achieve superior per-
formance of 35.8% and 17.7% in F-
measure over the two baselines respec-
tively, i.e. MI (Mutual Information) ap-
proach and EM optimization approach.
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999845297872341">
People submit concise word-sequences to search
engines in order to obtain satisfying feedback.
However, the word sequences are generally am-
biguous and often fail to convey the exact informa-
tion to search engine, thus severely, affecting the
performance of the system. For example, given
the query ”free software testing tools download”.
A simple bag-of-words query model cannot ana-
lyze ”software testing tools” accurately. Instead, it
returns ”free software” or ”free download” which
are high frequency web phrases. Therefore, how
to segment a query into meaningful semantic com-
ponents for implicit description of user’s intention
is an important issue both in natural language pro-
cessing and information retrieval fields.
There are few related studies on query segmen-
tation in spite of its importance and applicability
in many query analysis tasks such as query sug-
gestion, query substitution, etc. To our knowl-
edge, three approaches have been studied in pre-
vious works: MI (Mutual Information) approach
(Jones et al., 2006; Risvik et al., 2003), supervised
learning approach (Bergsma and Wang, 2007) and
EM optimization approach (Tan and Peng, 2008).
However, MI approach calculates MI value just
between two adjacent words that cannot handle
long entities. Supervised learning approach re-
quires a sufficiently large number of labeled train-
ing data, which is not conducive in real applica-
tions. EM algorithm often converges to a local
maximum that depends on the initial conditions.
There are also many relevant research on Chinese
word segmentation (Teahan et al., 2000; Peng and
Schuurmans, 2001; Xu et al., 2008). However,
they cannot be applied directly to query segmenta-
tion (Tan and Peng, 2008).
Under this scenario, we propose a novel unsu-
pervised approach for query segmentation. Dif-
fering from previous work, we first adopt the n-
gram model to estimate the query term’s frequency
matrix based on word occurrence statistics on the
web. We then devise a new strategy to select prin-
cipal eigenvectors of the matrix. Finally we cal-
culate the similarity of query words for segmen-
tation. Experimental results demonstrate the ef-
fectiveness of our approach as compared to two
baselines.
</bodyText>
<sectionHeader confidence="0.9937" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.9996475">
In this Section, we introduce our proposed query
segmentation approach, which is based on query
word frequency matrix principal eigenspace simi-
larity. To facilitate understanding, we first present
a general overview of our approach in Section 2.1
and then describe the details in Section 2.2-2.5.
</bodyText>
<subsectionHeader confidence="0.922641">
2.1 Overview
</subsectionHeader>
<bodyText confidence="0.786389833333333">
Figure 1 briefly shows the main procedure of
our proposed query segmentation approach. It
starts with a query which consists of a vector of
words{w1w2 · · · wn}. Our approach first build a
query-word frequency matrix M based on web
statistics to describe the relationship between any
</bodyText>
<page confidence="0.973844">
185
</page>
<note confidence="0.9264835">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 185–188,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.992564153846154">
two query words (Step 1). After decomposing M
(step 2), the parameter k which defines the num-
ber of segments in the query is estimate in Step 3.
Besides, a principal eigenspace of M is built and
the projection vectors({αi},i ∈ [1, n]) associated
with each query-word are obtained (Step 4). Simi-
larities between projection vectors are then calcu-
lated, which determine whether the corresponding
two words should be segmented together (Step5).
If the number of segmented components is not
equal to k, our approach modifies the threshold 6
and repeats steps 5 and 6 until the correct k num-
ber of segmentations are obtained(Step 7).
</bodyText>
<figure confidence="0.631547714285714">
Input: one n words query: w1w2 · · · wn;
Output: k segmented components of query;
Step 1: Build a frequency matrix M (Section
2.2);
Step 2: Decompose M into sorted eigenvalues
and eigenvectors;
Step 3: Estimate parameter k (Section 2.4);
</figure>
<figureCaption confidence="0.993793333333333">
Step 4: Build principal eigenspace with first
k eigenvectors and get the projection
({αi}) of M in principal eigenspace
(Section 2.3);
Step 5: Segment the query: if (αi·αT j )/(kαik·
kαjk) ≥ 6, segment wi and wj to-
gether (Section 2.5)
Step 6: If the number of segmented parts does
not equal to k, modify 6, go to step 5;
Step 7: output the right segmentations
Figure 1: Query Segmentation based on query-
word-frequency matrix eigenspace similarity
</figureCaption>
<subsectionHeader confidence="0.990947">
2.2 Frequency Matrix
</subsectionHeader>
<bodyText confidence="0.999939666666667">
Let W = w1, w2, · · · , wn be a query of n words.
We can build the relationships of any two words
using a symmetric matrix: M = {mi,j}nxn
</bodyText>
<equation confidence="0.999307">
mi,j = ⎧ F(wi) if i = j (1)
⎨⎪ F (wiwi+1 · · · wj) if i &lt; j
⎪⎩ mj,i if i &gt; j
F(wiwi+1 ··· wj) = count(wiwi+wi ·· wj) (2)
</equation>
<bodyText confidence="0.9996645">
Here mi,j denotes the correlation between
(wi · · · wj_1) and wj, where (wi · · · wj_1) means
a sequence and wj is a word. Considering the dif-
ference of each matrix element mi,j, we normalize
</bodyText>
<equation confidence="0.976509">
mi,j with:
mi,j = 2 · mi,j/(mi,i + mj,j) (3)
</equation>
<bodyText confidence="0.999122636363637">
F(·) is a function measuring the frequency of
query words or sequences. To improve the preci-
sion of measurement and reduce the computation
cost, we adopt the approach proposed by (Wang
et al., 2007) here. First, we extract the relevant
documents associated with the query via Google
Soap Search API. Second, we count the number
of all possible n-gram sequences which are high-
lighted in the titles and snippets of the returned
documents. Finally, we use Eqn.(2) to estimate
the value of mi,j.
</bodyText>
<subsectionHeader confidence="0.995624">
2.3 Principal Eigenspace
</subsectionHeader>
<bodyText confidence="0.999675857142857">
Although matrix M depicts the correlation of
query words, it is rough and noisy. Under
this scenario, we transform M into its princi-
pal eigenspace which is spanned by k largest
eigenvectors, and each query word is denoted
by the corresponding eigenvector in the principal
eigenspace.
Since M is a symmetric positive definite ma-
trix, its eigenvalues are real numbers and the
corresponding eigenvectors are non-zero and or-
thotropic to each other. Here, we denote the eigen-
values of M as : A(M) = {A1, A2, · · · , An}
and A1 ≥ A2 ≥ · · · ≥ An. All eigenvalues
of M have corresponding eigenvectors:V (M) =
{x1, x2, ··· , xn}.
Suppose that principal eigenspace M(M ∈
Rnxk) is spanned by the first k eigenvectors, i.e.
M = Span{x1, x2, · · · xk}, then row i of M can
be represented by vector αi which denotes the i-th
word for similarity calculation in Section 2.5, and
αi is derived from:
</bodyText>
<equation confidence="0.990309">
{T T T T
α1 ,α2 ,··· αn } = {x1, x2, ··· , xk} (4)
</equation>
<bodyText confidence="0.9892755">
Section 2.4 discusses the details of how to select
the parameter k.
</bodyText>
<subsectionHeader confidence="0.992974">
2.4 Parameter k Selection
</subsectionHeader>
<bodyText confidence="0.893035">
PCA (principal component analysis) (Jolliffe,
2002) often selects k principal components by the
following criterion:
k is the smallest integer which satisfies:
</bodyText>
<equation confidence="0.885330666666667">
Pik=1 Ai &gt; Threshold (5)
n —
Pi=1 Ai
</equation>
<page confidence="0.979089">
186
</page>
<bodyText confidence="0.999979333333333">
where n is the number of eigenvalues. When λk À
λk+1, Eqn.(5) is very effective. However, accord-
ing to the Gerschgorin circle theorem, the non-
diagonal values of M are so small that the eigen-
values cannot be distinguished easily. Under this
circumstance, a prefixed threshold is too restric-
tive to be applied in complex situations. Therefore
a function of n is introduced into the threshold as
follows:
</bodyText>
<equation confidence="0.998736333333333">
�ki=1 λi≥ (n − 1)2 (6)
n
�i=1 λi n
</equation>
<bodyText confidence="0.997846">
If k eigenvalues are qualified to be the princi-
pal components, then the threshold in Eqn.(5) can-
not be lower than 0.5, and need not be higher than
. If the length of the shortest query we seg-
mented is 4, we choose (n−1
n )2 because it will be
and larger than 0.5 with n no
smaller than 4.
The k eigenvectors will be used to segment the
query into k meaningful segments (Weiss, 1999;
Ng et al., 2001). In the k-dimensional principal
eigenspace, each dimension of the space describes
a semantic concept of the query. When one eigen-
value is bigger, the corresponding dimension con-
tains more query words.
</bodyText>
<subsectionHeader confidence="0.989916">
2.5 Similarity Computation
</subsectionHeader>
<bodyText confidence="0.99998925">
If the word i and word j are co-occurrence, αi
and αj are approximately parallel in the principal
eigenspace; otherwise, they are approximately or-
thogonal to each other. Hence, we measure the
similarity of αi and αj with inner-product to per-
form the segmentation (Weiss, 1999; Ng et al.,
2001). Selecting a proper threshold δ, we segment
the query using Eqn.(7):
</bodyText>
<equation confidence="0.902617">
S(wi,wj) = { 0, (αi · α�)/(kαik · kαjk) &lt; δ
(7)
</equation>
<bodyText confidence="0.984303461538461">
If S(wi, wj) = 1, wi and wj should be segmented
together, otherwise, wi and wj belong to different
semantic concepts respectively. Here, we denote
the total number of segments of the query as inte-
ger m.
As mentioned in Section 2.4, m should be equal
to k, therefore, the threshold δ is modified by k
and m. We set the initial value δ = 0.5 and modify
it with binary search method until m = k. If k is
larger than m, it means δ is too small to be a proper
threshold, i.e. some segments should be further
segmented. Otherwise, δ is too large that it should
be reduced.
</bodyText>
<sectionHeader confidence="0.998931" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998232">
3.1 Data set
</subsectionHeader>
<bodyText confidence="0.999819375">
We experiment on the data set published by
(Bergsma and Wang, 2007). This data set com-
prises 500 queries which were randomly taken
from the AOL search query database and each
query. These queries are all segmented manually
by three annotators (the results are referred as A,
B and C).
We evaluate our results on the five test data sets
(Tan and Peng, 2008), i.e. we use A, B, C, the
intersection of three annotator’s results (referred
to as D) and the conjunction of three annotator’s
results (referred to as E). Besides, three evaluation
metrics are used in our experiments (Tan and Peng,
2008; Peng and Schuurmans, 2001), i.e. Precision
(referred to as Prec), Recall and F-Measure (re-
ferred to as F-mea).
</bodyText>
<subsectionHeader confidence="0.998809">
3.2 Experimental results
</subsectionHeader>
<bodyText confidence="0.999829322580645">
Two baselines are used in our experiments: one is
MI based method (referred to as MI), and the other
is EM optimization (referred to as EM). Since the
EM proposed in (Tan and Peng, 2008) is imple-
mented with Yahoo! web corpus and only Google
Soap Search API is available in our study, we
adopt t-test to evaluate the performance of MI
with Google data (referred to as MI(G)) and Ya-
hoo! web corpus (referred to as MI(Y)). With the
values of MI(Y) and MI(G) in Table 1 we get the
p-value (p = 0.316 À 0.05), which indicates that
the performance of MI with different corpuses has
no significant difference. Therefore, we can de-
duce that, the two corpuses have little influence on
the performance of the approaches. Here, we de-
note our approach as ”ES”, i.e. Eigenspace Simi-
larity approach.
Table 1 presents the performance of the three
approaches, i.e. MI (MI(Y) and MI(G)), EM and
our proposed ES on the five test data sets using the
three mentioned metrics. From Table 1 we find
that ES achieves significant improvements as com-
pared to the other two methods in any metric and
data set we used.
For further analysis, we compute statistical per-
formance on mathematical expectation and stan-
dard deviation as shown in Figure 2. We observe
a consistent trend of the three metrics increasing
from left to right as shown in Figure 2, i.e. EM
performs better than MI and ES is the best among
the three approaches.
</bodyText>
<figure confidence="0.824136">
n−1
n
smaller than n−1
n
</figure>
<page confidence="0.96975">
187
</page>
<table confidence="0.99930475">
MI(Y) MI(G) EM ES
A Prec 0.469 0.548 0.562 0.652
Recall 0.534 0.489 0.555 0.699
F-mea 0.499 0.517 0.558 0.675
B Prec 0.408 0.449 0.568 0.632
Recall 0.472 0.391 0.578 0.659
F-mea 0.438 0.418 0.573 0.645
C Prec 0.451 0.503 0.558 0.614
Recall 0.519 0.440 0.561 0.649
F-mea 0.483 0.469 0.559 0.631
D Prec 0.510 0.574 0.640 0.772
Recall 0.550 0.510 0.650 0.826
F-mea 0.530 0.540 0.645 0.798
E Prec 0.582 0.672 0.715 0.834
Recall 0.654 0.734 0.721 0.852
F-mea 0.616 0.702 0.718 0.843
</table>
<tableCaption confidence="0.997467">
Table 1: Performance of different approaches.
</tableCaption>
<figureCaption confidence="0.989287">
Figure 2: Statistical performance of approaches
</figureCaption>
<bodyText confidence="0.999898444444445">
First, we observe that, EM (Prec: 0.609, Recall:
0.613, F-mea: 0.611) performs much better than
MI (Prec: 0.549, Recall: 0.513, F-mea: 0.529).
This is because EM optimizes the frequencies of
query words with EM algorithms. In addition, it
should be noted that, the recall of MI is especially
unsatisfactory, which is caused by its shortcoming
on handling long entities.
Second, when compared with EM, ES also has
more than 15% increase in the three reference met-
rics (15.1% on Prec, 20.2% on Recall and 17.7%
on F-mea). Here all increases are statistically sig-
nificant with p-value closed to 0. In depth anal-
ysis indicates that this is because ES makes good
use of the frequencies of query words in its princi-
pal eigenspace, while EM algorithm trains the ob-
served data (frequencies of query words) by sim-
ply maximizing them using maximum likelihood.
</bodyText>
<sectionHeader confidence="0.987635" genericHeader="conclusions">
4 Conclusion and Future work
</sectionHeader>
<bodyText confidence="0.999995133333333">
We proposed an unsupervised approach for query
segmentation. After using n-gram model to es-
timate term frequency matrix using term occur-
rence statistics from the web, we explored a new
method to select principal eigenvectors and calcu-
late the similarities of query words for segmenta-
tion. Experiments demonstrated the effectiveness
of our approach, with significant improvement in
segmentation accuracy as compared to the previ-
ous works.
Our approach will be capable of extracting se-
mantic concepts from queries. Besides, it can ex-
tended to Chinese word segmentation. In future,
we will further explore a new method of parame-
ter k selection to achieve higher performance.
</bodyText>
<sectionHeader confidence="0.999441" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998971875">
S. Bergsma and Q. I. Wang. 2007. Learning Noun
Phrase Query Segmentation. In Proc of EMNLP-
CoNLL
R. Jones, B. Rey, O. Madani, and W. Greiner. 2006.
Generating query substitutions. In Proc of WWW.
I.T. Jolliffe. 2002. Principal Component Analysis.
Springer, NY, USA.
Andrew Y. Ng, Michael I. Jordan, Yair Weiss. 2001.
On spectral clustering: Analysis and an algorithm
In Proc of NIPS.
F. Peng and D. Schuurmans. 2001. Self-Supervised
Chinese Word Segmentation. Proc of the 4th Int’l
Conf. on Advances in Intelligent Data Analysis.
K. M. Risvik, T. Mikolajewski, and P. Boros. 2003.
Query Segmentation for Web Search. In Proc of
WWW.
Bin Tan, Fuchun Peng. 2008. Unsupervised Query
Segmentation Using Generative Language Models
and Wikipedia. In Proc of WWW.
W. J. Teahan Rodger Mcnab Yingying Wen Ian H. Wit-
ten. 2000. A compression-based algorithm for Chi-
nese word segmentation Computational Linguistics.
Xin-Jing Wang, Wen Liu, Yong Qin. 2007. A Search-
based Chinese Word Segmentation Method. In Proc
of WWW.
Yair Weiss. 1999. Segmentation using eigenvectors: a
unifying view. Proc. IEEE Int’l Conf. Computer Vi-
sion, vol. 2, pp. 975-982.
Jia Xu, Jianfeng Gao, Kristina Toutanova, Hermann.
2008. Bayesian Semi-Supervised Chinese Word Seg-
mentation for Statistical Machine Translation. In
Proc of COLING.
</reference>
<page confidence="0.997529">
188
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.830492">
<title confidence="0.999175">Query Segmentation Based on Eigenspace Similarity</title>
<author confidence="0.996454">Sun Hu Huang Chua</author>
<affiliation confidence="0.993311333333333">of Applied Math of Computing University of Electronic Science National University of Singapore, and Technology of China,</affiliation>
<address confidence="0.99884">Chengdu, 610054, P.R. China Computing 1, Singapore 117590</address>
<email confidence="0.932373">tzhuang@uestc.edu.cn</email>
<abstract confidence="0.99407675">Query segmentation is essential to query processing. It aims to tokenize query words into several semantic segments and help the search engine to improve the precision of retrieval. In this paper, we present a novel unsupervised learning approach to query segmentation based on principal eigenspace similarity of queryword-frequency matrix derived from web statistics. Experimental results show that our approach could achieve superior perof Fmeasure over the two baselines respectively, i.e. MI (Mutual Information) approach and EM optimization approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bergsma</author>
<author>Q I Wang</author>
</authors>
<title>Learning Noun Phrase Query Segmentation.</title>
<date>2007</date>
<booktitle>In Proc of EMNLPCoNLL</booktitle>
<contexts>
<context position="2102" citStr="Bergsma and Wang, 2007" startWordPosition="309" endWordPosition="312">which are high frequency web phrases. Therefore, how to segment a query into meaningful semantic components for implicit description of user’s intention is an important issue both in natural language processing and information retrieval fields. There are few related studies on query segmentation in spite of its importance and applicability in many query analysis tasks such as query suggestion, query substitution, etc. To our knowledge, three approaches have been studied in previous works: MI (Mutual Information) approach (Jones et al., 2006; Risvik et al., 2003), supervised learning approach (Bergsma and Wang, 2007) and EM optimization approach (Tan and Peng, 2008). However, MI approach calculates MI value just between two adjacent words that cannot handle long entities. Supervised learning approach requires a sufficiently large number of labeled training data, which is not conducive in real applications. EM algorithm often converges to a local maximum that depends on the initial conditions. There are also many relevant research on Chinese word segmentation (Teahan et al., 2000; Peng and Schuurmans, 2001; Xu et al., 2008). However, they cannot be applied directly to query segmentation (Tan and Peng, 2008</context>
<context position="9646" citStr="Bergsma and Wang, 2007" startWordPosition="1651" endWordPosition="1654">segmented together, otherwise, wi and wj belong to different semantic concepts respectively. Here, we denote the total number of segments of the query as integer m. As mentioned in Section 2.4, m should be equal to k, therefore, the threshold δ is modified by k and m. We set the initial value δ = 0.5 and modify it with binary search method until m = k. If k is larger than m, it means δ is too small to be a proper threshold, i.e. some segments should be further segmented. Otherwise, δ is too large that it should be reduced. 3 Experiments 3.1 Data set We experiment on the data set published by (Bergsma and Wang, 2007). This data set comprises 500 queries which were randomly taken from the AOL search query database and each query. These queries are all segmented manually by three annotators (the results are referred as A, B and C). We evaluate our results on the five test data sets (Tan and Peng, 2008), i.e. we use A, B, C, the intersection of three annotator’s results (referred to as D) and the conjunction of three annotator’s results (referred to as E). Besides, three evaluation metrics are used in our experiments (Tan and Peng, 2008; Peng and Schuurmans, 2001), i.e. Precision (referred to as Prec), Recal</context>
</contexts>
<marker>Bergsma, Wang, 2007</marker>
<rawString>S. Bergsma and Q. I. Wang. 2007. Learning Noun Phrase Query Segmentation. In Proc of EMNLPCoNLL</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jones</author>
<author>B Rey</author>
<author>O Madani</author>
<author>W Greiner</author>
</authors>
<title>Generating query substitutions.</title>
<date>2006</date>
<booktitle>In Proc of WWW.</booktitle>
<contexts>
<context position="2025" citStr="Jones et al., 2006" startWordPosition="298" endWordPosition="301">ols” accurately. Instead, it returns ”free software” or ”free download” which are high frequency web phrases. Therefore, how to segment a query into meaningful semantic components for implicit description of user’s intention is an important issue both in natural language processing and information retrieval fields. There are few related studies on query segmentation in spite of its importance and applicability in many query analysis tasks such as query suggestion, query substitution, etc. To our knowledge, three approaches have been studied in previous works: MI (Mutual Information) approach (Jones et al., 2006; Risvik et al., 2003), supervised learning approach (Bergsma and Wang, 2007) and EM optimization approach (Tan and Peng, 2008). However, MI approach calculates MI value just between two adjacent words that cannot handle long entities. Supervised learning approach requires a sufficiently large number of labeled training data, which is not conducive in real applications. EM algorithm often converges to a local maximum that depends on the initial conditions. There are also many relevant research on Chinese word segmentation (Teahan et al., 2000; Peng and Schuurmans, 2001; Xu et al., 2008). Howev</context>
</contexts>
<marker>Jones, Rey, Madani, Greiner, 2006</marker>
<rawString>R. Jones, B. Rey, O. Madani, and W. Greiner. 2006. Generating query substitutions. In Proc of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I T Jolliffe</author>
</authors>
<title>Principal Component Analysis.</title>
<date>2002</date>
<publisher>Springer,</publisher>
<location>NY, USA.</location>
<contexts>
<context position="7359" citStr="Jolliffe, 2002" startWordPosition="1234" endWordPosition="1235">eigenvalues of M as : A(M) = {A1, A2, · · · , An} and A1 ≥ A2 ≥ · · · ≥ An. All eigenvalues of M have corresponding eigenvectors:V (M) = {x1, x2, ··· , xn}. Suppose that principal eigenspace M(M ∈ Rnxk) is spanned by the first k eigenvectors, i.e. M = Span{x1, x2, · · · xk}, then row i of M can be represented by vector αi which denotes the i-th word for similarity calculation in Section 2.5, and αi is derived from: {T T T T α1 ,α2 ,··· αn } = {x1, x2, ··· , xk} (4) Section 2.4 discusses the details of how to select the parameter k. 2.4 Parameter k Selection PCA (principal component analysis) (Jolliffe, 2002) often selects k principal components by the following criterion: k is the smallest integer which satisfies: Pik=1 Ai &gt; Threshold (5) n — Pi=1 Ai 186 where n is the number of eigenvalues. When λk À λk+1, Eqn.(5) is very effective. However, according to the Gerschgorin circle theorem, the nondiagonal values of M are so small that the eigenvalues cannot be distinguished easily. Under this circumstance, a prefixed threshold is too restrictive to be applied in complex situations. Therefore a function of n is introduced into the threshold as follows: �ki=1 λi≥ (n − 1)2 (6) n �i=1 λi n If k eigenval</context>
</contexts>
<marker>Jolliffe, 2002</marker>
<rawString>I.T. Jolliffe. 2002. Principal Component Analysis. Springer, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
<author>Yair Weiss</author>
</authors>
<title>On spectral clustering: Analysis and an algorithm In</title>
<date>2001</date>
<booktitle>Proc of NIPS.</booktitle>
<contexts>
<context position="8345" citStr="Ng et al., 2001" startWordPosition="1416" endWordPosition="1419"> easily. Under this circumstance, a prefixed threshold is too restrictive to be applied in complex situations. Therefore a function of n is introduced into the threshold as follows: �ki=1 λi≥ (n − 1)2 (6) n �i=1 λi n If k eigenvalues are qualified to be the principal components, then the threshold in Eqn.(5) cannot be lower than 0.5, and need not be higher than . If the length of the shortest query we segmented is 4, we choose (n−1 n )2 because it will be and larger than 0.5 with n no smaller than 4. The k eigenvectors will be used to segment the query into k meaningful segments (Weiss, 1999; Ng et al., 2001). In the k-dimensional principal eigenspace, each dimension of the space describes a semantic concept of the query. When one eigenvalue is bigger, the corresponding dimension contains more query words. 2.5 Similarity Computation If the word i and word j are co-occurrence, αi and αj are approximately parallel in the principal eigenspace; otherwise, they are approximately orthogonal to each other. Hence, we measure the similarity of αi and αj with inner-product to perform the segmentation (Weiss, 1999; Ng et al., 2001). Selecting a proper threshold δ, we segment the query using Eqn.(7): S(wi,wj)</context>
</contexts>
<marker>Ng, Jordan, Weiss, 2001</marker>
<rawString>Andrew Y. Ng, Michael I. Jordan, Yair Weiss. 2001. On spectral clustering: Analysis and an algorithm In Proc of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Peng</author>
<author>D Schuurmans</author>
</authors>
<title>Self-Supervised Chinese Word Segmentation.</title>
<date>2001</date>
<booktitle>Proc of the 4th Int’l Conf. on Advances in Intelligent Data Analysis.</booktitle>
<contexts>
<context position="2600" citStr="Peng and Schuurmans, 2001" startWordPosition="387" endWordPosition="390"> (Mutual Information) approach (Jones et al., 2006; Risvik et al., 2003), supervised learning approach (Bergsma and Wang, 2007) and EM optimization approach (Tan and Peng, 2008). However, MI approach calculates MI value just between two adjacent words that cannot handle long entities. Supervised learning approach requires a sufficiently large number of labeled training data, which is not conducive in real applications. EM algorithm often converges to a local maximum that depends on the initial conditions. There are also many relevant research on Chinese word segmentation (Teahan et al., 2000; Peng and Schuurmans, 2001; Xu et al., 2008). However, they cannot be applied directly to query segmentation (Tan and Peng, 2008). Under this scenario, we propose a novel unsupervised approach for query segmentation. Differing from previous work, we first adopt the ngram model to estimate the query term’s frequency matrix based on word occurrence statistics on the web. We then devise a new strategy to select principal eigenvectors of the matrix. Finally we calculate the similarity of query words for segmentation. Experimental results demonstrate the effectiveness of our approach as compared to two baselines. 2 Methodol</context>
<context position="10201" citStr="Peng and Schuurmans, 2001" startWordPosition="1747" endWordPosition="1750">set We experiment on the data set published by (Bergsma and Wang, 2007). This data set comprises 500 queries which were randomly taken from the AOL search query database and each query. These queries are all segmented manually by three annotators (the results are referred as A, B and C). We evaluate our results on the five test data sets (Tan and Peng, 2008), i.e. we use A, B, C, the intersection of three annotator’s results (referred to as D) and the conjunction of three annotator’s results (referred to as E). Besides, three evaluation metrics are used in our experiments (Tan and Peng, 2008; Peng and Schuurmans, 2001), i.e. Precision (referred to as Prec), Recall and F-Measure (referred to as F-mea). 3.2 Experimental results Two baselines are used in our experiments: one is MI based method (referred to as MI), and the other is EM optimization (referred to as EM). Since the EM proposed in (Tan and Peng, 2008) is implemented with Yahoo! web corpus and only Google Soap Search API is available in our study, we adopt t-test to evaluate the performance of MI with Google data (referred to as MI(G)) and Yahoo! web corpus (referred to as MI(Y)). With the values of MI(Y) and MI(G) in Table 1 we get the p-value (p = </context>
</contexts>
<marker>Peng, Schuurmans, 2001</marker>
<rawString>F. Peng and D. Schuurmans. 2001. Self-Supervised Chinese Word Segmentation. Proc of the 4th Int’l Conf. on Advances in Intelligent Data Analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K M Risvik</author>
<author>T Mikolajewski</author>
<author>P Boros</author>
</authors>
<title>Query Segmentation for Web Search. In</title>
<date>2003</date>
<booktitle>Proc of WWW.</booktitle>
<contexts>
<context position="2047" citStr="Risvik et al., 2003" startWordPosition="302" endWordPosition="305">tead, it returns ”free software” or ”free download” which are high frequency web phrases. Therefore, how to segment a query into meaningful semantic components for implicit description of user’s intention is an important issue both in natural language processing and information retrieval fields. There are few related studies on query segmentation in spite of its importance and applicability in many query analysis tasks such as query suggestion, query substitution, etc. To our knowledge, three approaches have been studied in previous works: MI (Mutual Information) approach (Jones et al., 2006; Risvik et al., 2003), supervised learning approach (Bergsma and Wang, 2007) and EM optimization approach (Tan and Peng, 2008). However, MI approach calculates MI value just between two adjacent words that cannot handle long entities. Supervised learning approach requires a sufficiently large number of labeled training data, which is not conducive in real applications. EM algorithm often converges to a local maximum that depends on the initial conditions. There are also many relevant research on Chinese word segmentation (Teahan et al., 2000; Peng and Schuurmans, 2001; Xu et al., 2008). However, they cannot be app</context>
</contexts>
<marker>Risvik, Mikolajewski, Boros, 2003</marker>
<rawString>K. M. Risvik, T. Mikolajewski, and P. Boros. 2003. Query Segmentation for Web Search. In Proc of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bin Tan</author>
<author>Fuchun Peng</author>
</authors>
<title>Unsupervised Query Segmentation Using Generative Language Models and Wikipedia.</title>
<date>2008</date>
<booktitle>In Proc of WWW.</booktitle>
<contexts>
<context position="2152" citStr="Tan and Peng, 2008" startWordPosition="317" endWordPosition="320">o segment a query into meaningful semantic components for implicit description of user’s intention is an important issue both in natural language processing and information retrieval fields. There are few related studies on query segmentation in spite of its importance and applicability in many query analysis tasks such as query suggestion, query substitution, etc. To our knowledge, three approaches have been studied in previous works: MI (Mutual Information) approach (Jones et al., 2006; Risvik et al., 2003), supervised learning approach (Bergsma and Wang, 2007) and EM optimization approach (Tan and Peng, 2008). However, MI approach calculates MI value just between two adjacent words that cannot handle long entities. Supervised learning approach requires a sufficiently large number of labeled training data, which is not conducive in real applications. EM algorithm often converges to a local maximum that depends on the initial conditions. There are also many relevant research on Chinese word segmentation (Teahan et al., 2000; Peng and Schuurmans, 2001; Xu et al., 2008). However, they cannot be applied directly to query segmentation (Tan and Peng, 2008). Under this scenario, we propose a novel unsuper</context>
<context position="9935" citStr="Tan and Peng, 2008" startWordPosition="1703" endWordPosition="1706"> δ = 0.5 and modify it with binary search method until m = k. If k is larger than m, it means δ is too small to be a proper threshold, i.e. some segments should be further segmented. Otherwise, δ is too large that it should be reduced. 3 Experiments 3.1 Data set We experiment on the data set published by (Bergsma and Wang, 2007). This data set comprises 500 queries which were randomly taken from the AOL search query database and each query. These queries are all segmented manually by three annotators (the results are referred as A, B and C). We evaluate our results on the five test data sets (Tan and Peng, 2008), i.e. we use A, B, C, the intersection of three annotator’s results (referred to as D) and the conjunction of three annotator’s results (referred to as E). Besides, three evaluation metrics are used in our experiments (Tan and Peng, 2008; Peng and Schuurmans, 2001), i.e. Precision (referred to as Prec), Recall and F-Measure (referred to as F-mea). 3.2 Experimental results Two baselines are used in our experiments: one is MI based method (referred to as MI), and the other is EM optimization (referred to as EM). Since the EM proposed in (Tan and Peng, 2008) is implemented with Yahoo! web corpus</context>
</contexts>
<marker>Tan, Peng, 2008</marker>
<rawString>Bin Tan, Fuchun Peng. 2008. Unsupervised Query Segmentation Using Generative Language Models and Wikipedia. In Proc of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J Teahan Rodger Mcnab Yingying Wen Ian H Witten</author>
</authors>
<title>A compression-based algorithm for Chinese word segmentation Computational Linguistics.</title>
<date>2000</date>
<marker>Witten, 2000</marker>
<rawString>W. J. Teahan Rodger Mcnab Yingying Wen Ian H. Witten. 2000. A compression-based algorithm for Chinese word segmentation Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin-Jing Wang</author>
<author>Wen Liu</author>
<author>Yong Qin</author>
</authors>
<title>A Searchbased Chinese Word Segmentation Method.</title>
<date>2007</date>
<booktitle>In Proc of WWW.</booktitle>
<contexts>
<context position="5960" citStr="Wang et al., 2007" startWordPosition="977" endWordPosition="980">hips of any two words using a symmetric matrix: M = {mi,j}nxn mi,j = ⎧ F(wi) if i = j (1) ⎨⎪ F (wiwi+1 · · · wj) if i &lt; j ⎪⎩ mj,i if i &gt; j F(wiwi+1 ··· wj) = count(wiwi+wi ·· wj) (2) Here mi,j denotes the correlation between (wi · · · wj_1) and wj, where (wi · · · wj_1) means a sequence and wj is a word. Considering the difference of each matrix element mi,j, we normalize mi,j with: mi,j = 2 · mi,j/(mi,i + mj,j) (3) F(·) is a function measuring the frequency of query words or sequences. To improve the precision of measurement and reduce the computation cost, we adopt the approach proposed by (Wang et al., 2007) here. First, we extract the relevant documents associated with the query via Google Soap Search API. Second, we count the number of all possible n-gram sequences which are highlighted in the titles and snippets of the returned documents. Finally, we use Eqn.(2) to estimate the value of mi,j. 2.3 Principal Eigenspace Although matrix M depicts the correlation of query words, it is rough and noisy. Under this scenario, we transform M into its principal eigenspace which is spanned by k largest eigenvectors, and each query word is denoted by the corresponding eigenvector in the principal eigenspac</context>
</contexts>
<marker>Wang, Liu, Qin, 2007</marker>
<rawString>Xin-Jing Wang, Wen Liu, Yong Qin. 2007. A Searchbased Chinese Word Segmentation Method. In Proc of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yair Weiss</author>
</authors>
<title>Segmentation using eigenvectors: a unifying view.</title>
<date>1999</date>
<booktitle>Proc. IEEE Int’l Conf. Computer Vision,</booktitle>
<volume>2</volume>
<pages>975--982</pages>
<contexts>
<context position="8327" citStr="Weiss, 1999" startWordPosition="1414" endWordPosition="1415">distinguished easily. Under this circumstance, a prefixed threshold is too restrictive to be applied in complex situations. Therefore a function of n is introduced into the threshold as follows: �ki=1 λi≥ (n − 1)2 (6) n �i=1 λi n If k eigenvalues are qualified to be the principal components, then the threshold in Eqn.(5) cannot be lower than 0.5, and need not be higher than . If the length of the shortest query we segmented is 4, we choose (n−1 n )2 because it will be and larger than 0.5 with n no smaller than 4. The k eigenvectors will be used to segment the query into k meaningful segments (Weiss, 1999; Ng et al., 2001). In the k-dimensional principal eigenspace, each dimension of the space describes a semantic concept of the query. When one eigenvalue is bigger, the corresponding dimension contains more query words. 2.5 Similarity Computation If the word i and word j are co-occurrence, αi and αj are approximately parallel in the principal eigenspace; otherwise, they are approximately orthogonal to each other. Hence, we measure the similarity of αi and αj with inner-product to perform the segmentation (Weiss, 1999; Ng et al., 2001). Selecting a proper threshold δ, we segment the query using</context>
</contexts>
<marker>Weiss, 1999</marker>
<rawString>Yair Weiss. 1999. Segmentation using eigenvectors: a unifying view. Proc. IEEE Int’l Conf. Computer Vision, vol. 2, pp. 975-982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Xu</author>
<author>Jianfeng Gao</author>
<author>Kristina Toutanova</author>
<author>Hermann</author>
</authors>
<title>Bayesian Semi-Supervised Chinese Word Segmentation for Statistical Machine Translation. In</title>
<date>2008</date>
<booktitle>Proc of COLING.</booktitle>
<contexts>
<context position="2618" citStr="Xu et al., 2008" startWordPosition="391" endWordPosition="394">ach (Jones et al., 2006; Risvik et al., 2003), supervised learning approach (Bergsma and Wang, 2007) and EM optimization approach (Tan and Peng, 2008). However, MI approach calculates MI value just between two adjacent words that cannot handle long entities. Supervised learning approach requires a sufficiently large number of labeled training data, which is not conducive in real applications. EM algorithm often converges to a local maximum that depends on the initial conditions. There are also many relevant research on Chinese word segmentation (Teahan et al., 2000; Peng and Schuurmans, 2001; Xu et al., 2008). However, they cannot be applied directly to query segmentation (Tan and Peng, 2008). Under this scenario, we propose a novel unsupervised approach for query segmentation. Differing from previous work, we first adopt the ngram model to estimate the query term’s frequency matrix based on word occurrence statistics on the web. We then devise a new strategy to select principal eigenvectors of the matrix. Finally we calculate the similarity of query words for segmentation. Experimental results demonstrate the effectiveness of our approach as compared to two baselines. 2 Methodology In this Sectio</context>
</contexts>
<marker>Xu, Gao, Toutanova, Hermann, 2008</marker>
<rawString>Jia Xu, Jianfeng Gao, Kristina Toutanova, Hermann. 2008. Bayesian Semi-Supervised Chinese Word Segmentation for Statistical Machine Translation. In Proc of COLING.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>