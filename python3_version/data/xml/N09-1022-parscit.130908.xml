<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000482">
<title confidence="0.986733">
Japanese Query Alteration Based on Semantic Similarity
</title>
<author confidence="0.993476">
Masato Hagiwara
</author>
<affiliation confidence="0.995494">
Nagoya University
</affiliation>
<address confidence="0.405197">
Furo-cho, Chikusa-ku
Nagoya 464-8603, Japan
</address>
<email confidence="0.977012">
hagiwara@kl.i.is.nagoya-u.ac.jp
</email>
<author confidence="0.95142">
Hisami Suzuki
</author>
<affiliation confidence="0.921444">
Microsoft Research
</affiliation>
<address confidence="0.912078">
One Microsoft Way
Redmond, WA 98052, USA
</address>
<email confidence="0.995073">
hisamis@microsoft.com
</email>
<sectionHeader confidence="0.993795" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999746842105263">
We propose a unified approach to web search
query alterations in Japanese that is not lim-
ited to particular character types or ortho-
graphic similarity between a query and its al-
teration candidate. Our model is based on pre-
vious work on English query correction, but
makes some crucial improvements: (1) we
augment the query-candidate list to include
orthographically dissimilar but semantically
similar pairs; and (2) we use kernel-based
lexical semantic similarity to avoid the prob-
lem of data sparseness in computing query-
candidate similarity. We also propose an ef-
ficient method for generating query-candidate
pairs for model training and testing. We show
that the proposed method achieves about 80%
accuracy on the query alteration task, improv-
ing over previously proposed methods that use
semantic similarity.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999836066666667">
Web search query correction is an important prob-
lem to solve for robust information retrieval given
how pervasive errors are in search queries: it is said
that more than 10% of web search queries contain
errors (Cucerzan and Brill, 2004). English query
correction has been an area of active research in re-
cent years, building on previous work on general-
purpose spelling correction. However, there has
been little investigation of query correction in lan-
guages other than English.
In this paper, we address the issue of query cor-
rection, and more generally, query alteration in
Japanese. Japanese poses particular challenges to
the query correction task due to its complex writ-
ing system, summarized in Fig. 11. There are four
</bodyText>
<footnote confidence="0.98528275">
&apos;The figure is somewhat over-simplified as it does not in-
clude any word consisting of multiple character types. It also
does not include examples of spelling mistakes and variants in
word segmentation.
</footnote>
<figureCaption confidence="0.999622">
Figure 1: Japanese character types and spelling variants
</figureCaption>
<bodyText confidence="0.999552">
main character types, including two types of kana
(phonetic alphabet - hiragana and katakana), kanji
(ideographic - characters represent meaning) and
Roman alphabet; a word can be legitimately spelled
in multiple ways, combining any of these character
sets. For example, the word for ‘protein’ can be
spelled as たんぱくしつ (all in hiragana), タンパク
質 (katakana+kanji), 蛋白質 (all in kanji) or たん白
質 (hiragana+kanji), all pronounced in the same way
(tanpakushitsu). Some examples of these spelling
variants are shown in Fig. 1 with the prefix Sp: as is
observed from the figure, spelling variation occurs
within and across different character types. Resolv-
ing these variants will be essential not only for in-
formation retrieval but practically for all NLP tasks.
A particularly prolific source of spelling varia-
tions in Japanese is katakana. Katakana charac-
ters are used to transliterate words from English and
other foreign languages, and as such, the variations
in the source language pronunciation as well as the
ambiguity in sound adaptation are reflected in the
katakana spelling. For example, Masuyama et al.
(2004) report that at least six distinct translitera-
tions of the word ‘spaghetti’ (スパゲッティ, スパゲ
ティー, etc.) are attested in the newspaper corpus
they studied. Normalizing katakana spelling varia-
tions has been the subject of research by itself (Ara-
maki et al., 2008; Masuyama et al., 2004). Similarly,
English-to-katakana transliteration (e.g., ‘fedex’ as
フェデックス fedekkusuinFig. 1) and katakana-to-
</bodyText>
<page confidence="0.980048">
191
</page>
<note confidence="0.890529">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 191–199,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999844607142857">
English back-transliteration (e.g., 7r7—!y&apos;,x back
into ‘fedex’) have also been studied extensively (Bi-
lac and Tanaka, 2004; Brill et al., 2001; Knight and
Graehl, 1998), as it is an essential component for
machine translation. To our knowledge, however,
there has been no work that addresses spelling vari-
ation in Japanese generally.
In this paper, we propose a general approach to
query correction/alteration in Japanese. Our goal is
to find precise re-write candidates for a query, be
it a correction of a spelling error, normalization of
a spelling variant, or finding a strict synonym in-
cluding abbreviations (e.g., MS -,,4&apos;,ul71
‘Microsoft’, prefixed by Abbr in Fig. 1) and true
synonyms (e.g., k—W (translation of ‘seat’) ✓--1
(transliteration of ‘seat’, indicated by Syn in Fig. 1)2.
Our method is based on previous work on English
query correction in that we use both spelling and se-
mantic similarity between a query and its alteration
candidate, but is more general in that we include al-
teration candidates that are not similar to the original
query in spelling. In computing semantic similar-
ity, we adopt a kernel-based method (Kandola et al.,
2002), which improves the accuracy of the query al-
teration results over previously proposed methods.
We also introduce a novel approach to creating a
dataset of query and alteration candidate pairs effi-
ciently and reliably from query session logs.
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999181071428571">
The key difference between traditional general-
purpose spelling correction and search query cor-
rection lies in the fact that the latter cannot rely on
a lexicon: web queries are replete with valid out-
of-dictionary words which are not mis-spellings of
in-vocabulary words. Cucerzan and Brill (2004) pi-
oneered the research of query spelling correction,
with an excellent description of how a traditional
dictionary-based speller had to be adapted to solve
the realistic query correction problem. The model
they proposed is a source-channel model, where the
source model is a word bigram model trained on
query logs, and the channel model is based on a
weighted Damerau-Levenshtein edit distance. Brill
</bodyText>
<footnote confidence="0.98970475">
2Our goal is to harvest alternation candidates; therefore, ex-
actly how they are used in the search task (whether it is used to
substitute the original query, to expand it, or simply to suggest
an alternative) is not a concern to us here.
</footnote>
<bodyText confidence="0.999958375">
and Moore (2000) proposed a general, improved
source model for general spelling correction, while
Ahmad and Kondrak (2005) learned a spelling error
model from search query logs using the Expectation
Maximization algorithm, without relying on a train-
ing set of misspelled words and their corrections.
Extending the work of Cucerzan and Brill (2004),
Li et al. (2006) proposed to include semantic sim-
ilarity between the query and its correction candi-
date. They point out that adventura is a common
misspelling of aventura, not adventure, and this can-
not be captured by a simple string edit distance, but
requires some knowledge of distributional similar-
ity. Distributional similarity is measured by the sim-
ilarity of the context shared by two terms, and has
been successfully applied to many natural language
processing tasks, including semantic knowledge ac-
quisition (Lin, 1998).
Though the use of distributional similarity im-
proved the query correction results in Li et al.’s
work, one problem is that it is sparse and is not avail-
able for many rarer query strings. Chen et al. (2007)
addressed this problem by using external informa-
tion (i.e., web search results); we take a different ap-
proach to solve the sparseness problem, namely by
using semantic kernels.
Jones et al. (2006a) generated Japanese query al-
teration pairs from by mining query logs and built a
regression model which predicts the quality of query
rewriting pairs. Their model includes a wide variety
of orthographical features, but not semantic similar-
ity features.
</bodyText>
<sectionHeader confidence="0.995179" genericHeader="method">
3 Query Alteration Model
</sectionHeader>
<subsectionHeader confidence="0.998636">
3.1 Problem Formulation
</subsectionHeader>
<bodyText confidence="0.999299333333333">
We employ a formulation of query alteration model
that is similar to conventional query correction mod-
els. Given a query string q as input, a query correc-
tion model finds a correct alteration c* within the
confusion set of q, so that it maximizes the posterior
probability:
</bodyText>
<equation confidence="0.985036">
c* = arg max
cECF(q)CC
</equation>
<bodyText confidence="0.728596">
where C is the set of all white-space separated words
and their bigrams in query logs in our case3, and
</bodyText>
<footnote confidence="0.9810855">
3In regular text, Japanese uses no white spaces to separate
words; however, white spaces are often (but not consistently)
</footnote>
<equation confidence="0.986478">
P(c|q) (1)
</equation>
<page confidence="0.950278">
192
</page>
<bodyText confidence="0.993873894736842">
CF(q) C C is the confusion set of q, consisting of
the candidates within a certain edit distance from q,
i.e., CF(q) = {c E C|ED(q, c) &lt; 0}. We set 0 =
24 using an unnormalized edit distance. The detail
of the edit distance ED(q, c) is described in Section
3.2. The query string q itself is contained in CF(q),
and if the model output is different from q, it means
the model suggests a query alteration. Formulated
in this way, both query error detection and alteration
are performed in a unified way.
After computing the posterior probability of each
candidate in CF(q) by the source channel model
(Section 3.2), an N-best list is obtained as the ini-
tial candidate set C0, which is then augmented by
the bootstrapping method Tchai (Section 3.4) to cre-
ate the final candidate list C(q). The candidates in
C(q) are re-ranked by a maximum entropy model
(Section 3.5) and the candidate with the highest pos-
terior probability is selected as the output.
</bodyText>
<subsectionHeader confidence="0.997815">
3.2 Source Channel Model
</subsectionHeader>
<bodyText confidence="0.9988634">
Source channel models are widely used for spelling
and query correction (Brill and Moore, 2000;
Cucerzan and Brill, 2004). Instead of directly com-
puting Eq. (1), we can decompose the posterior
probability using Bayes’ rule as:
</bodyText>
<equation confidence="0.953724">
P(c)P(q|c), (2)
</equation>
<bodyText confidence="0.999986769230769">
where the source model P(c) measures how proba-
ble the candidate c is, while the error model P(q|c)
measures how similar q and c are.
For the source model, an n-gram based statisti-
cal language model is the standard in previous work
(Ahmad and Kondrak, 2005; Li et al., 2006). Word
n-gram models are simple to create for English,
which is easy to tokenize and to obtain word-based
statistics, but this is not the case with Japanese.
Therefore, we simply considered the whole input
string as a candidate to be altered, and used the rel-
ative frequency of candidates in the query logs to
build the language model:
</bodyText>
<equation confidence="0.999872">
Freq(c) (3)
P(c) = E&amp;EC Freq(c�).
</equation>
<bodyText confidence="0.976240941176471">
For the error model, we used an improved chan-
nel model described in (Brill and Moore, 2000),
used to separate words in Japanese search queries, due to their
keyword-based nature.
which we call the alpha-beta model in this paper.
The model is a weighted extension of the normal
Damerau-Levenshtein edit distance which equally
penalizes single character insertion, substitution, or
deletion operations (Damerau, 1964; Levenshtein,
1966), and considers generic edit operations of the
form α —* Q, where α and 0 are any (possibly
null) strings. From misspelled/correct word pairs,
alpha-beta trains the probability P(α —* 0|PSN),
conditioned by the position PSN of α in a word,
where PSN E {start of word, middle of word, end of
word}. Under this model, the probability of rewrit-
ing a string w to a string s is calculated as:
</bodyText>
<equation confidence="0.99785">
P«0(8jw) = max
REPart(w),TEPart(s)
</equation>
<bodyText confidence="0.9993455">
which corresponds to finding best partitions R and T
in all possible partitions Part(w) and Part(s). Brill
and Moore (2000) reported that this model gave a
significant improvement over conventional edit dis-
tance methods.
Brill et al. (2001) applied this model for ex-
tracting katakana-English transliteration pairs from
query logs. They trained the edit distance between
character chunks of katakana and Roman alphabets,
after converting katakana strings to Roman script.
We also trained this model using 59,754 katakana-
English pairs extracted from aligned Japanese and
English Wikipedia article titles. In this paper we al-
lowed |α|, |0 |&lt; 3. The resulting edit distance is
obtained as the negative logarithm of the alpha-beta
probability, i.e., EDαO(q|c) = −log Pα,3(q|c).
Since the edit operations are directional and c and
q can be any string consisting of katakana and En-
glish, distance in both directions were considered.
We also included a modified edit distance EDhd for
simple kana-kana variations after converting them
into Roman script. The distance EDhd is essen-
tially the same as the normal Damerau-Levenshtein
edit distance, with the modification that it does not
penalize character halving (aa —* a) and doubling
(a —* aa), because a large part of katakana vari-
ants only differ in halving/doubling (e.g. スパゲティ
(supageti) vs スパゲティー (supagetii)4.
The final error probability is obtained from the
minimum of these three distances:
4However, character length can be distinctive in katakana,
as in ビル biru ‘building’ vs. ビール biiru ‘beer’.
</bodyText>
<equation confidence="0.952320142857143">
c* = arg max
cECF(q)CC
� |R |P(Ri ! TijPSN(Ri)),
i=1
193
ED(q, c) = min[ED«p(q|c), ED«p(c|q), EDhd(q, c)],(4)
P(q|c) = exp[−ED(q, c)] (5)
</equation>
<bodyText confidence="0.999981">
where every edit distance is normalized to [0, 1] by
multiplying by a factor of 2/(|q||c|) so that it does
not depend on the length of the input strings5.
</bodyText>
<subsectionHeader confidence="0.999171">
3.3 Kernel-based Lexical Semantic Similarity
3.3.1 Distributional Similarity
</subsectionHeader>
<bodyText confidence="0.999984333333333">
The source channel model described in Sec-
tion 3.2 only considers language and error models
and cannot capture semantic similarity between the
query and its correction candidate. To address this
issue, we use distributional similarity (Lin, 1998) es-
timated from query logs as additional evidence for
query alteration, following Li et al. (2006).
For English, it is relatively easy to define the con-
text of a word based on the bag-of-words model. As
this is not expected to work on Japanese, we de-
fine context as everything but the query string in a
query log, as Pas¸ca et al. (2006) and Komachi and
Suzuki (2008) did for their information extraction
tasks. This formulation does not involve any seg-
mentation or boundary detection, which makes this
method fast and robust. On the other hand, this may
cause additional sparseness in the vector representa-
tion; we address this issue in the next two sections.
Once the context of a candidate ci is de-
fined as the patterns that the candidate co-occurs
with, it can be represented as a vector ci =
[pmi(ci,p1), ... , pmi(ci, pM)]&apos;, where M denotes
the number of context patterns and x&apos; is the trans-
position of a vector (or possibly a matrix) x. The el-
ements of the vector are given by pointwise mutual
information between the candidate ci and the pattern
pj, computed as:
</bodyText>
<equation confidence="0.942639">
pmi(ci, pj) = log |ci, pj |(6)
|ci, ∗||∗, pj|,
</equation>
<bodyText confidence="0.996481388888889">
where |ci, pj |is the frequency of the pattern pj in-
stantiated with the candidate ci, and ‘*’ denotes a
5We did not include kanji variants here, because disam-
biguating kanji readings is a very difficult task, and the ma-
jority of the variations in queries are in katakana and Roman
alphabet. The framework proposed in this paper, however, can
incorporate kanji variants straightforwardly into ED(q, c) once
we have reasonable edit distance functions for kanji variations.
wildcard, i.e., |ci, ∗ |= Ep |ci, p |and |∗, pj |=
&amp; |c, pj|. With these defined, the distributional
similarity can be calculated as cosine similarity. Let
ci be the L2-normalized pattern vector of the candi-
date ci, and X = {ci} be the candidate-pattern co-
occurrence matrix. The candidate similarity matrix
K can then be obtained as K = X&apos;X. In the follow-
ing, the (i, j)-element of the matrix K is denoted as
Kij, which corresponds to the cosine similarity be-
tween candidates ci and cj.
</bodyText>
<subsectionHeader confidence="0.921804">
3.3.2 Semantic Kernels
</subsectionHeader>
<bodyText confidence="0.990038848484849">
Although distributional similarity serves as strong
evidence for semantically relevant candidates, di-
rectly applying the technique to query logs faces the
sparseness problem. Because context patterns are
drawn from query logs and can also contain spelling
errors, alterations, and word permutations as much
as queries do, context differs so greatly in represen-
tations that even related candidates might not have
sufficient contextual overlap between them. For
example, a candidate “YouTube” matched against
the patterns “YouTube+movie”, “movie+YouTube”
and “You-Tube+movii” (with a minor spelling er-
ror) will yield three distinct patterns “#+movie”,
“movie+#” and “#+movii”6, which will be treated as
three separate dimensions in the vector space model.
This sparseness problem can be partially ad-
dressed by considering the correlation between pat-
terns. Kandola et al. (2002) proposed new kernel-
based similarity methods which incorporate indirect
similarity between terms for a text retrieval task. Al-
though their kernels are built on a document-term
co-occurrence model, they can also be applied to our
candidate-pattern co-occurrence model. The pro-
posed kernel is recursively defined as:
K� = βX&apos; GX + K, G� = βX KX&apos; + G, (7)
where G = XX&apos; is the correlation matrix between
patterns and β is the factor to ensure that longer
range effects decay exponentially. This can be in-
terpreted as augmenting the similarity matrix K
through indirect similarities of patterns G and vice
versa. Semantically related pairs of patterns are ex-
pected to be given high correlation in the matrix G
and this will alleviate the sparseness problem. By
</bodyText>
<footnote confidence="0.6284475">
6‘+’ denotes a white space, and ‘#’ indicates where the word
of interest is found in a context pattern.
</footnote>
<page confidence="0.994399">
194
</page>
<figureCaption confidence="0.998995">
Figure 2: Orthographically Augmented Graph
</figureCaption>
<bodyText confidence="0.9754645">
solving the above recursive definition, one obtains
the von Neumann kernel:
</bodyText>
<equation confidence="0.997631">
�K(β) = K(I − βK)−1 = �∞ βt−1Kt. (8)
t=1
</equation>
<bodyText confidence="0.974517444444444">
This can also be interpreted in terms of a random
walk in a graph where the nodes correspond to all the
candidates and the weight of an edge (i, j) is given
by KZj. A simple calculation shows that KZj equals
the sum of the products of the edge weights over all
possible paths between the nodes corresponding cZ
and cj in the graph. Also, KtZj corresponds to the
probability that a random walk beginning at node cZ
ends up at node cj after t steps, assuming that the en-
tries are all positive and the sum of the connections
is 1 at each node. Following this notion, Kandola
et al. (2002) proposed another kernel called expo-
nential kernel, with alternative faster decay factors:
= K exp(βK). (9)
They showed that this alternative kernel achieved a
better performance for their text retrieval task. We
employed these two kernels to compute distribu-
tional similarity for our query correction task.
</bodyText>
<subsectionHeader confidence="0.889767">
3.3.3 Orthographically Augmented Kernels
</subsectionHeader>
<bodyText confidence="0.99830125">
Although semantic relatedness can be partially
captured by the semantic kernels introduced in the
previous section, they may still have difficulties
computing correlations between candidates and pat-
terns especially for only sparsely connected graphs.
Take the graph (a) in Fig. 2 for example, which is
a simplified yet representative graph topology for
candidate-pattern co-occurrence we often encounter.
In this case K = X0X equals I, meaning that the
connections between candidates and patterns are too
sparse to obtain sufficient correlation even when se-
mantic kernels are used.
</bodyText>
<figureCaption confidence="0.993455">
Figure 3: Bootstrapping Additional Candidates
</figureCaption>
<bodyText confidence="0.999795571428571">
In order to address this issue, we propose to aug-
ment the graph by weakly connecting the candidate
and pattern nodes as shown in the graph (b) of Fig. 2
based on prior knowledge of orthographic similarity
about candidates and patterns. This can be achieved
using the following candidate similarity matrix K+
instead of K:
</bodyText>
<equation confidence="0.999794">
K+ = γSC + (1 − γ)X0 [δSP + (1 − δ)I]X (10)
</equation>
<bodyText confidence="0.999993041666667">
where SC = {s,(i, j)} is the orthographical similar-
ity matrix of candidates in which the (i, j)-element
is given by the edit distance based similarity, i.e.,
s,(i, j) = exp [−ED(cZ, cj)]. The orthographical
similarity matrix of patterns SP = {sP(i, j)} is de-
fined similarly, i.e., sP(i, j) = exp[−ED(pZ, pj)].
Note that using this similarity matrix K+ can be
interpreted as a random walk process on a bipar-
tite graph as follows. Let C and P as the sets of
candidates and patterns. K+ corresponds to a sin-
gle walking step from C to C, by either remaining
within C with a probability of γ or moving to “the
other side” P of the graph with a probability of 1−γ.
When the walking remains in C, it is allowed to
move to another candidate node following the candi-
date orthographic similarity SC. Otherwise it moves
to P by the matrix X, chooses either to move within
P with a probability γSP or to stay with a probabil-
ity 1 − γ, and finally comes back to C by the matrix
X0. Multiplication (K+)t corresponds to repeating
this process t times. Using this similarity, we can de-
fine two orthographically augmented semantic ker-
nels which differ in the decaying factors, augmented
von Neumann kernel and exponential kernel:
</bodyText>
<equation confidence="0.9999055">
k+(β) = K+(I − βK+)−1 (11)
k+(β) = K+ exp(βK+). (12)
</equation>
<subsectionHeader confidence="0.993906">
3.4 Bootstrapping Additional Candidates
</subsectionHeader>
<bodyText confidence="0.999543">
Now that we have a semantic model, our query
correction model can cover query-candidate pairs
</bodyText>
<equation confidence="0.908812666666667">
K(β) = K
βtKt
t!
∞
E
t=1
</equation>
<page confidence="0.994045">
195
</page>
<bodyText confidence="0.99980236">
which are only semantically related. However, pre-
vious work on query correction all used a string dis-
tance function and a threshold to restrict the space of
potential candidates, allowing only the orthographi-
cally similar candidates.
To collect additional candidates, the use of
context-based semantic extraction methods would
be effective because semantically related candidates
are likely to share context with the initial query
q, or at least with the initial candidate set C0.
Here we used the Tchai algorithm (Komachi and
Suzuki, 2008), a modified version of Espresso (Pan-
tel and Pennacchiotti, 2006) to collect such candi-
dates. This algorithm starts with initial seed in-
stances, then induces reliable context patterns co-
occurring with the seeds, induces instances from
the patterns, and iterates this process to obtain cat-
egories of semantically related words. Using the
candidates in C0 as the seed instances, one boot-
strapping iteration of the Tchai algorithm is executed
to obtain the semantically related set of instances
C1. The seed instance reliabilities are given by the
source channel probabilities P(c)P(q|c). Finally we
take the union C0 U C1 to obtain the candidate set
C(q). This process is outlined in Fig. 3.
</bodyText>
<subsectionHeader confidence="0.835749">
3.5 Maximum Entropy Model
</subsectionHeader>
<bodyText confidence="0.999089142857143">
In order to build a unified probabilistic query al-
teration model, we used the maximum entropy ap-
proach of (Beger et al., 1996), which Li et al. (2006)
also employed for their query correction task and
showed its effectiveness. It defines a conditional
probabilistic distribution P(c|q) based on a set of
feature functions f1, ... , fK:
</bodyText>
<equation confidence="0.852439">
P( |) =
exp EKi= 1 λifi(c, q)
c q
</equation>
<bodyText confidence="0.9999875625">
where λ1, ... , λK are the feature weights. The op-
timal set of feature weights λ* can be computed by
maximizing the log-likelihood of the training set.
We used the Generalized Iterative Scaling (GIS)
algorithm (Darroch and Ratcliff, 1972) to optimize
the feature weights. GIS trains conditional proba-
bility in Eq. (13), which requires the normalization
over all possible candidates. However, the number
of all possible candidates C obtained from a query
log can be very large, so we only calculated the sum
over the candidates in C(q). This is the same ap-
proach that Och and Ney (2002) took for statistical
machine translation, and Li et al. (2006) for query
spelling correction.
We used the following four categories of func-
tions as the features:
</bodyText>
<listItem confidence="0.996392153846154">
1. Language model feature, given by the logarithm
of the source model probability: log P(c).
2. Error model features, which are composed of
three edit distance functions: −EDαβ(q|c),
−EDαβ(c|q), and −EDhd(q, c).
3. Similarity based feature, computed as the loga-
rithm of distributional similarity between q and c:
log sim(q, c), which is calcualted using one of the
following kernels (Section 3.3): K, K, K, K+,
and K+. The similarity values were normalized
to [0, 1] after adding a small discounting factor
ε = 1.0 X 10−5.
4. Similarity based correction candidate features,
</listItem>
<bodyText confidence="0.710002222222222">
which are binary features with a value of 1 if and
only if the frequency of c is higher than that of
q, and distributional similarity between them is
higher than a certain threshold. Li et al. (2006)
used this set of features, and suggested that these
features give the evidence that q may be a com-
mon misspelling of c. The thresholds on the nor-
malized distributional similarity are enumerated
from 0.5 to 0.9 with the interval 0.1.
</bodyText>
<sectionHeader confidence="0.999695" genericHeader="method">
4 Experiment
</sectionHeader>
<subsectionHeader confidence="0.998688">
4.1 Dataset Creation
</subsectionHeader>
<bodyText confidence="0.999637210526316">
For all the experiments conducted in this paper, we
used a subset of the Japanese search query logs sub-
mitted to Live Search (www.live.com) in November
and December of 2007. Queries submitted less than
eight times were deleted. The query log we used
contained 83,080,257 tokens and 1,038,499 unique
queries.
Models of query correction in previous work were
trained and evaluated using manually created query-
candidate pairs. That is, human annotators were
given a set of queries and were asked to provide a
correction for each query when it needed to be re-
written. As Cucerzan and Brill (2004) point out,
however, this method is seriously flawed in that the
intention of the original query is completely lost to
the annotator, without which the correction is often
impossible: it is not clear if gogle should be cor-
rected to google or goggle, or neither — gogle may
be a brand new product name. Cucerzan and Brill
</bodyText>
<equation confidence="0.990445333333333">
K
Ec exp Ei=1 λifi(c, q),
(13)
</equation>
<page confidence="0.995297">
196
</page>
<bodyText confidence="0.999967522727273">
therefore performed a second evaluation, where the
test data was drawn by sampling the query logs for
successive queries (q1, q2) by the same user where
the edit distance between q1 and q2 are within a cer-
tain threshold, which are then submitted to annota-
tors for generating the correction. While this method
makes the annotation more reliable by relying on
user (rather than annotator) reformulation, the task
is still overly difficult: going back to the example
in Section 1, it is unclear which spelling of ‘protein’
produces the best search results — it can only be em-
pirically determined. Their method also eliminates
all pairs of candidates that are not orthographically
similar. We have therefore improved their method
in the following manner, making the process more
automated and thus more reliable.
We first collected a subset of the query log that
contains only those pairs (q1, q2) that are issued suc-
cessively by the same user, q2 is issued within 3 min-
utes of q1, and q2 resulted in a click of the resulting
page while q1 did not. The last condition adds the
evidence that q2 was a better formulation than q1.
We then ranked the collected query pairs using log-
likelihood ratio (LLR) (Dunning, 1993), which mea-
sures the dependence between q1 and q2 within the
context of web queries (Jones et al., 2006b). We ran-
domly sampled 10,000 query pairs with LLR &gt; 200,
and submitted them to annotators, who only confirm
or reject a query pair as being synonymous. For ex-
ample, q1 = nikon and q2 = canon are related but
not synonymous, while we are reasonably sure q1 =
ipot and q2 = ipod are synonymous, given that this
pair has a high LLR value. This verification process
is extremely fast and consistent across annotators:
it takes less than 1 hour to go through 1,000 query
pairs, and the inter-annotator agreement rate of two
annotators on 2,000 query pairs was 95.7%. We
annotated 10,000 query pairs consisting of alpha-
numerical and kana characters in this manner. After
rejecting non-synonymous pairs and those which do
not co-occur with any context patterns, 6,489 pairs
remained, and we used 1,243 pairs for testing, 628
as a development set, and 4,618 for training the max-
imum entropy model.
</bodyText>
<subsectionHeader confidence="0.987717">
4.2 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.9986255">
The performance of query alteration was evaluated
based on the following measures (Li et al., 2006).
</bodyText>
<tableCaption confidence="0.995937">
Table 1: Performance results (%)
</tableCaption>
<table confidence="0.998959375">
Model Accuracy Recall Precision
SC 71.12 39.29 45.09
ME-NoSim 74.58 44.58 52.52
ME-Cos 74.18 45.84 50.70
ME-vN 74.34 45.59 52.16
ME-Exp 73.61 44.84 50.57
ME-vN+ 75.06 44.33 53.01
ME-Exp+ 75.14 44.08 53.52
</table>
<bodyText confidence="0.863986">
The input queries, correct suggestions, and outputs
were matched in a case-insensitive manner.
</bodyText>
<listItem confidence="0.952399555555556">
• Accuracy: The number of correct outputs gener-
ated by the system divided by the total number of
queries in the test set;
• Recall: The number of correct suggestions for al-
tered queries divided by the total number of al-
tered queries in the test set;
• Precision: The number of correct suggestions for
altered queries divided by the total number of al-
terations made by the system.
</listItem>
<bodyText confidence="0.999751631578947">
The parameters for the kernels, namely, Q, -y, and
6, are tuned using the development set. The finally
employed values are: Q = 0.3 for K, K, and K+,
Q = 0.2 for K+, -y = 0.2 and 6 = 0.4 for K+, and
-y = 0.35 and 6 = 0.7 for K+. In the source channel
model, we manually scaled the language probability
by a factor of 0.1 to alleviate the bias toward highly
frequent candidates.
As the initial candidate set C0, top-50 instances
were selected by the source channel model, and 100
patterns were extracted as P0 by the Tchai iteration
after removing generic patterns, which we detected
simply by rejecting those which induced more than
200 unique instances. Finally top-30 instances were
induced using P0 to create C1. Generic instances
were not removed in this process because they may
still be alterations of input query q. The maximum
size of P1 was set to 2,000, after removing unreliable
patterns with reliability smaller than 0.0001.
</bodyText>
<subsectionHeader confidence="0.694405">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999963285714286">
Table 1 shows the evaluation results. SC is the
source channel model, while the others are maxi-
mum entropy (ME) models with different features.
ME-NoSim uses the same features as SC, but con-
siderably outperforms SC in all three measures, con-
firming the superiority of the ME approach. Decom-
posing the three edit distance functions into three
</bodyText>
<page confidence="0.994058">
197
</page>
<bodyText confidence="0.999964375">
separate features in the ME model may also explain
the better result. All the ME approaches outper-
formed SC in accuracy with a statistically significant
difference (p &lt; 0.0001 on McNemar’s test).
The model with the cosine similarity (ME-Cos)
in addition to the basic set of features yielded higher
recall compared to ME-NoSim, but decreased accu-
racy and precision, which are more important than
recall for our purposes because a false alteration
does more damage than no alteration. This is also
the case when the kernel-based methods, ME-vN
(the von Neumann kernel) and ME-Exp (the expo-
nential kernel), are used in place of the cosine sim-
ilarity. This shows that using semantic similarity
does not always help, which we believe is due to
the sparseness of the contextual information used in
computing semantic similarity.
On the other hand, ME-vN+ (with augmented von
Neumann kernel) and ME-Exp+ (with augmented
exponential kernel) increased both accuracy and pre-
cision with a slight decrease of recall, compared to
the distributional similarity baseline and the non-
augmented kernel-based methods. ME-Exp+ was
significantly better than ME-Exp (p &lt; 0.01).
Note that the accuracy values appear lower than
some of the previous results on English (e.g., more
than 80% in (Li et al., 2006)), but this is because
the dataset creation method we employed tends to
over-represent the pairs that lead to alteration: the
simplest baseline (= always propose no alteration)
performs 67.3% accuracy on our data, in contrast to
83.4% on the data used in (Li et al., 2006).
Manually examining the suggestions made by the
system also confirms the effectiveness of our model.
For example, the similarity-based models altered the
query ipot to ipod, while the simple ME-NoSim
model failed, because it depends too much on the
edit distance-based features. We also observed that
many of the suggestions made by the system were
actually reasonable, even though they were differ-
ent from the annotated gold standard. For example,
ME-vN+ suggests a re-write of the query 2tyann as
2 &apos;6-�-A/to;5 (‘2-channel’), while the gold standard
was an abbreviated form 2 &apos;6-�-A/ (‘2-chan’).
To incorporate such possibly correct candidates
into account, we conducted a follow-up experiment
where we considered multiple reference alterations,
created automatically from our data set in the fol-
</bodyText>
<tableCaption confidence="0.998788">
Table 2: Performance with the multiple reference model
</tableCaption>
<table confidence="0.99854125">
Model Accuracy Recall Precision
SC 75.30 48.61 55.78
ME-NoSim 79.49 56.17 66.17
ME-Cos 79.32 58.19 64.35
ME-vN 79.24 57.18 65.42
ME-Exp 78.52 56.42 63.64
ME-vN+ 79.89 55.67 66.57
ME-Exp+ 79.81 54.91 66.67
</table>
<bodyText confidence="0.9970714">
lowing manner. Suppose that a query q1 is corrected
as q2, and q2 is corrected as q3 in our annotated data.
If this is the case, we considered q1 —* q3 as a valid
alteration as well. By applying this chaining oper-
ation up to 5 degrees of separation, we re-created a
set of valid alterations for each input query. Note
that directionality is important — in the above ex-
ample, q1 —* q3 is valid, while q3 —* q1 is not. Table
2 shows the results of evaluation with multiple refer-
ences. The numbers substantially improved over the
single reference cases, as expected, but did not af-
fect the relative performance of each model. Again,
the differences in accuracy between the SC and ME
models, and ME-Exp and ME-Exp+ were statisti-
cally significant (p &lt; 0.01).
</bodyText>
<sectionHeader confidence="0.964087" genericHeader="conclusions">
5 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999589785714286">
In this paper we have presented a unified approach
to Japanese query alteration. Our approach draws
on previous research in English spelling and query
correction, Japanese katakana variation and translit-
eration, and semantic similarity, and builds a model
that makes improvements over previously proposed
query correction methods. In particular, the use of
orthographically augmented semantic kernels pro-
posed in this paper is general and applicable to other
languages, including English, for query alteration,
especially when the data sparseness is an issue. In
the future, we also plan to investigate other meth-
ods, such as PLSI (Hofmann, 1999), to deal with
data sparseness in computing semantic similarity.
</bodyText>
<sectionHeader confidence="0.998302" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9832326">
This research was conducted during the first au-
thor’s internship at Micorosoft Research. We thank
the colleagues, especially Dmitriy Belenko, Chris
Brockett, Jianfeng Gao, Christian K¨onig, and Chris
Quirk for their help in conducting this research.
</bodyText>
<page confidence="0.998106">
198
</page>
<sectionHeader confidence="0.995826" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99986801010101">
Farooq Ahmad and Grzegorz Kondrak. 2005. Learning
a spelling error model from search query logs. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP-2005), pages
955–962.
Eiji Aramaki, Takeshi Imai, Kengo Miyo, and Kazuhiko
Ohe. 2008. Orthographic disambiguation incorporat-
ing transliterated probability. In Proceedings in the
third International Joint Conference on Natural Lan-
guage Processing (IJCNLP-2008), pages 48–55.
Adam L. Beger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39–72.
Slaven Bilac and Hozumi Tanaka. 2004. A hybrid back-
transliteration system for japanese. In Proceedings of
the 20th international conference on Computational
Linguistics (COLING-2004), pages 597–603.
Eric Brill and Robert C. Moore. 2000. An improved er-
ror model for noisy channel spelling. In Proceedings
of the 38th Annual Meeting on Association for Com-
putational Linguistics (ACL-2000), pages 286–293.
Eric Brill, Gary Kacmarcik, and Chris Brockett. 2001.
Automatically harvesting katakana-english term pairs
from search engine query logs. In Proceedings of the
Sixth Natural Language Processing Pacific Rim Sym-
posium (NLPRS-2001), pages 393–399.
Qing Chen, Mu Li, , and Ming Zhou. 2007. Improv-
ing query spelling correction using web search results.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 181–189.
Silviu Cucerzan and Eric Brill. 2004. Spelling correc-
tion as an iterative process that exploits the collective
knowledge of web users. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-2004), pages 293–300.
Fred Damerau. 1964. A technique for computer detec-
tion and correction of spelling errors. Communication
of the ACM, 7(3):659–664.
J.N. Darroch and D. Ratcliff. 1972. Generalized iterative
scaling for log-linear models. Annuals of Mathemati-
cal Statistics, 43:1470–1480.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61–74.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Research and Development in Informa-
tion Retrieval, pages 50–57.
Rosie Jones, Kevin Bartz, Pero Subasic, and Benjamin
Rey. 2006a. Automatically generating related queries
in japanese. Language Resources and Evaluation
(LRE), 40(3-4):219–232.
Rosie Jones, Benjamin Rey, Omid Madani, and Wiley
Greiner. 2006b. Generating query substitutions. In
Proceedings of the 15th international World Wide Web
conference (WWW-06), pages 387–396.
Jaz Kandola, John Shawe-Taylor, and Nello Cristianini.
2002. Learning semantic similarity. In Neural Infor-
mation Processing Systems (NIPS 15), pages 657–664.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4):599–
612.
Mamoru Komachi and Hisami Suzuki. 2008. Mini-
mally supervised learning of semantic knowledge from
query logs. In Proceedings of the 3rd International
Joint Conference on Natural Language Processing
(IJCNLP-2008), pages 358–365.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions and reversals. Soviet
Physice - Doklady, 10:707–710.
Mu Li, Muhua Zhu, Yang Zhang, and Ming Zhou.
2006. Exploring distributional similarity based mod-
els for query spelling correction. In Proceedings of
COLING/ACL-2006, pages 1025–1032.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING/ACL-1998,
pages 786–774.
Takeshi Masuyama, Satoshi Sekine, and Hiroshi Nak-
agawa. 2004. Automatic construction of japanese
katakana variant list from large corpus. In Proceed-
ings of Proceedings of the 20th international confer-
ence on Computational Linguistics (COLING-2004),
pages 1214–1219.
Franz Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proceedings of the 40th an-
nual meeting of ACL, pages 295–302.
Marius Pas¸ca, Dekang Lin, Jeffrey Bigham, Andrei Lif-
chits, and Alpa Jain. 2006. Organizing and searching
the world wide web of facts - step one: the one-million
fact extraction challenge. In Proceedings of the 21st
National Conference on Artificial Intelligence (AAAI-
06), pages 1400–1405.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically har-
vesting semantic relations. In Proceedings of
COLING/ACL-2006, pages 113–120.
</reference>
<page confidence="0.998913">
199
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.107246">
<title confidence="0.994955">Japanese Query Alteration Based on Semantic Similarity</title>
<author confidence="0.72497">Masato</author>
<affiliation confidence="0.5861045">Nagoya Furo-cho,</affiliation>
<address confidence="0.972332">Nagoya 464-8603,</address>
<email confidence="0.961061">hagiwara@kl.i.is.nagoya-u.ac.jp</email>
<author confidence="0.436422">Hisami</author>
<affiliation confidence="0.895708">Microsoft</affiliation>
<address confidence="0.8766755">One Microsoft Redmond, WA 98052,</address>
<email confidence="0.999343">hisamis@microsoft.com</email>
<abstract confidence="0.9992567">We propose a unified approach to web search query alterations in Japanese that is not limited to particular character types or orthographic similarity between a query and its alteration candidate. Our model is based on previous work on English query correction, but makes some crucial improvements: (1) we augment the query-candidate list to include orthographically dissimilar but semantically similar pairs; and (2) we use kernel-based lexical semantic similarity to avoid the problem of data sparseness in computing querycandidate similarity. We also propose an efficient method for generating query-candidate pairs for model training and testing. We show that the proposed method achieves about 80% accuracy on the query alteration task, improving over previously proposed methods that use semantic similarity.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Farooq Ahmad</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Learning a spelling error model from search query logs.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2005),</booktitle>
<pages>955--962</pages>
<contexts>
<context position="6258" citStr="Ahmad and Kondrak (2005)" startWordPosition="970" endWordPosition="973"> adapted to solve the realistic query correction problem. The model they proposed is a source-channel model, where the source model is a word bigram model trained on query logs, and the channel model is based on a weighted Damerau-Levenshtein edit distance. Brill 2Our goal is to harvest alternation candidates; therefore, exactly how they are used in the search task (whether it is used to substitute the original query, to expand it, or simply to suggest an alternative) is not a concern to us here. and Moore (2000) proposed a general, improved source model for general spelling correction, while Ahmad and Kondrak (2005) learned a spelling error model from search query logs using the Expectation Maximization algorithm, without relying on a training set of misspelled words and their corrections. Extending the work of Cucerzan and Brill (2004), Li et al. (2006) proposed to include semantic similarity between the query and its correction candidate. They point out that adventura is a common misspelling of aventura, not adventure, and this cannot be captured by a simple string edit distance, but requires some knowledge of distributional similarity. Distributional similarity is measured by the similarity of the con</context>
<context position="9730" citStr="Ahmad and Kondrak, 2005" startWordPosition="1556" endWordPosition="1559">l (Section 3.5) and the candidate with the highest posterior probability is selected as the output. 3.2 Source Channel Model Source channel models are widely used for spelling and query correction (Brill and Moore, 2000; Cucerzan and Brill, 2004). Instead of directly computing Eq. (1), we can decompose the posterior probability using Bayes’ rule as: P(c)P(q|c), (2) where the source model P(c) measures how probable the candidate c is, while the error model P(q|c) measures how similar q and c are. For the source model, an n-gram based statistical language model is the standard in previous work (Ahmad and Kondrak, 2005; Li et al., 2006). Word n-gram models are simple to create for English, which is easy to tokenize and to obtain word-based statistics, but this is not the case with Japanese. Therefore, we simply considered the whole input string as a candidate to be altered, and used the relative frequency of candidates in the query logs to build the language model: Freq(c) (3) P(c) = E&amp;EC Freq(c�). For the error model, we used an improved channel model described in (Brill and Moore, 2000), used to separate words in Japanese search queries, due to their keyword-based nature. which we call the alpha-beta mode</context>
</contexts>
<marker>Ahmad, Kondrak, 2005</marker>
<rawString>Farooq Ahmad and Grzegorz Kondrak. 2005. Learning a spelling error model from search query logs. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2005), pages 955–962.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eiji Aramaki</author>
<author>Takeshi Imai</author>
<author>Kengo Miyo</author>
<author>Kazuhiko Ohe</author>
</authors>
<title>Orthographic disambiguation incorporating transliterated probability.</title>
<date>2008</date>
<booktitle>In Proceedings in the third International Joint Conference on Natural Language Processing (IJCNLP-2008),</booktitle>
<pages>48--55</pages>
<contexts>
<context position="3453" citStr="Aramaki et al., 2008" startWordPosition="529" endWordPosition="533">asks. A particularly prolific source of spelling variations in Japanese is katakana. Katakana characters are used to transliterate words from English and other foreign languages, and as such, the variations in the source language pronunciation as well as the ambiguity in sound adaptation are reflected in the katakana spelling. For example, Masuyama et al. (2004) report that at least six distinct transliterations of the word ‘spaghetti’ (スパゲッティ, スパゲ ティー, etc.) are attested in the newspaper corpus they studied. Normalizing katakana spelling variations has been the subject of research by itself (Aramaki et al., 2008; Masuyama et al., 2004). Similarly, English-to-katakana transliteration (e.g., ‘fedex’ as フェデックス fedekkusuinFig. 1) and katakana-to191 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 191–199, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics English back-transliteration (e.g., 7r7—!y&apos;,x back into ‘fedex’) have also been studied extensively (Bilac and Tanaka, 2004; Brill et al., 2001; Knight and Graehl, 1998), as it is an essential component for machine translation. To our knowledge, however, there has been no</context>
</contexts>
<marker>Aramaki, Imai, Miyo, Ohe, 2008</marker>
<rawString>Eiji Aramaki, Takeshi Imai, Kengo Miyo, and Kazuhiko Ohe. 2008. Orthographic disambiguation incorporating transliterated probability. In Proceedings in the third International Joint Conference on Natural Language Processing (IJCNLP-2008), pages 48–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Beger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="21875" citStr="Beger et al., 1996" startWordPosition="3573" endWordPosition="3576">tances from the patterns, and iterates this process to obtain categories of semantically related words. Using the candidates in C0 as the seed instances, one bootstrapping iteration of the Tchai algorithm is executed to obtain the semantically related set of instances C1. The seed instance reliabilities are given by the source channel probabilities P(c)P(q|c). Finally we take the union C0 U C1 to obtain the candidate set C(q). This process is outlined in Fig. 3. 3.5 Maximum Entropy Model In order to build a unified probabilistic query alteration model, we used the maximum entropy approach of (Beger et al., 1996), which Li et al. (2006) also employed for their query correction task and showed its effectiveness. It defines a conditional probabilistic distribution P(c|q) based on a set of feature functions f1, ... , fK: P( |) = exp EKi= 1 λifi(c, q) c q where λ1, ... , λK are the feature weights. The optimal set of feature weights λ* can be computed by maximizing the log-likelihood of the training set. We used the Generalized Iterative Scaling (GIS) algorithm (Darroch and Ratcliff, 1972) to optimize the feature weights. GIS trains conditional probability in Eq. (13), which requires the normalization ove</context>
</contexts>
<marker>Beger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Beger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slaven Bilac</author>
<author>Hozumi Tanaka</author>
</authors>
<title>A hybrid backtransliteration system for japanese.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics (COLING-2004),</booktitle>
<pages>597--603</pages>
<contexts>
<context position="3904" citStr="Bilac and Tanaka, 2004" startWordPosition="588" endWordPosition="592">スパゲ ティー, etc.) are attested in the newspaper corpus they studied. Normalizing katakana spelling variations has been the subject of research by itself (Aramaki et al., 2008; Masuyama et al., 2004). Similarly, English-to-katakana transliteration (e.g., ‘fedex’ as フェデックス fedekkusuinFig. 1) and katakana-to191 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 191–199, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics English back-transliteration (e.g., 7r7—!y&apos;,x back into ‘fedex’) have also been studied extensively (Bilac and Tanaka, 2004; Brill et al., 2001; Knight and Graehl, 1998), as it is an essential component for machine translation. To our knowledge, however, there has been no work that addresses spelling variation in Japanese generally. In this paper, we propose a general approach to query correction/alteration in Japanese. Our goal is to find precise re-write candidates for a query, be it a correction of a spelling error, normalization of a spelling variant, or finding a strict synonym including abbreviations (e.g., MS -,,4&apos;,ul71 ‘Microsoft’, prefixed by Abbr in Fig. 1) and true synonyms (e.g., k—W (translation of ‘s</context>
</contexts>
<marker>Bilac, Tanaka, 2004</marker>
<rawString>Slaven Bilac and Hozumi Tanaka. 2004. A hybrid backtransliteration system for japanese. In Proceedings of the 20th international conference on Computational Linguistics (COLING-2004), pages 597–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Robert C Moore</author>
</authors>
<title>An improved error model for noisy channel spelling.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics (ACL-2000),</booktitle>
<pages>286--293</pages>
<contexts>
<context position="9326" citStr="Brill and Moore, 2000" startWordPosition="1487" endWordPosition="1490">tection and alteration are performed in a unified way. After computing the posterior probability of each candidate in CF(q) by the source channel model (Section 3.2), an N-best list is obtained as the initial candidate set C0, which is then augmented by the bootstrapping method Tchai (Section 3.4) to create the final candidate list C(q). The candidates in C(q) are re-ranked by a maximum entropy model (Section 3.5) and the candidate with the highest posterior probability is selected as the output. 3.2 Source Channel Model Source channel models are widely used for spelling and query correction (Brill and Moore, 2000; Cucerzan and Brill, 2004). Instead of directly computing Eq. (1), we can decompose the posterior probability using Bayes’ rule as: P(c)P(q|c), (2) where the source model P(c) measures how probable the candidate c is, while the error model P(q|c) measures how similar q and c are. For the source model, an n-gram based statistical language model is the standard in previous work (Ahmad and Kondrak, 2005; Li et al., 2006). Word n-gram models are simple to create for English, which is easy to tokenize and to obtain word-based statistics, but this is not the case with Japanese. Therefore, we simply</context>
<context position="11098" citStr="Brill and Moore (2000)" startWordPosition="1784" endWordPosition="1787">ion, substitution, or deletion operations (Damerau, 1964; Levenshtein, 1966), and considers generic edit operations of the form α —* Q, where α and 0 are any (possibly null) strings. From misspelled/correct word pairs, alpha-beta trains the probability P(α —* 0|PSN), conditioned by the position PSN of α in a word, where PSN E {start of word, middle of word, end of word}. Under this model, the probability of rewriting a string w to a string s is calculated as: P«0(8jw) = max REPart(w),TEPart(s) which corresponds to finding best partitions R and T in all possible partitions Part(w) and Part(s). Brill and Moore (2000) reported that this model gave a significant improvement over conventional edit distance methods. Brill et al. (2001) applied this model for extracting katakana-English transliteration pairs from query logs. They trained the edit distance between character chunks of katakana and Roman alphabets, after converting katakana strings to Roman script. We also trained this model using 59,754 katakanaEnglish pairs extracted from aligned Japanese and English Wikipedia article titles. In this paper we allowed |α|, |0 |&lt; 3. The resulting edit distance is obtained as the negative logarithm of the alpha-be</context>
</contexts>
<marker>Brill, Moore, 2000</marker>
<rawString>Eric Brill and Robert C. Moore. 2000. An improved error model for noisy channel spelling. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics (ACL-2000), pages 286–293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Gary Kacmarcik</author>
<author>Chris Brockett</author>
</authors>
<title>Automatically harvesting katakana-english term pairs from search engine query logs.</title>
<date>2001</date>
<booktitle>In Proceedings of the Sixth Natural Language Processing Pacific Rim Symposium (NLPRS-2001),</booktitle>
<pages>393--399</pages>
<contexts>
<context position="3924" citStr="Brill et al., 2001" startWordPosition="593" endWordPosition="596">ted in the newspaper corpus they studied. Normalizing katakana spelling variations has been the subject of research by itself (Aramaki et al., 2008; Masuyama et al., 2004). Similarly, English-to-katakana transliteration (e.g., ‘fedex’ as フェデックス fedekkusuinFig. 1) and katakana-to191 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 191–199, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics English back-transliteration (e.g., 7r7—!y&apos;,x back into ‘fedex’) have also been studied extensively (Bilac and Tanaka, 2004; Brill et al., 2001; Knight and Graehl, 1998), as it is an essential component for machine translation. To our knowledge, however, there has been no work that addresses spelling variation in Japanese generally. In this paper, we propose a general approach to query correction/alteration in Japanese. Our goal is to find precise re-write candidates for a query, be it a correction of a spelling error, normalization of a spelling variant, or finding a strict synonym including abbreviations (e.g., MS -,,4&apos;,ul71 ‘Microsoft’, prefixed by Abbr in Fig. 1) and true synonyms (e.g., k—W (translation of ‘seat’) ✓--1 (translit</context>
<context position="11215" citStr="Brill et al. (2001)" startWordPosition="1802" endWordPosition="1805">he form α —* Q, where α and 0 are any (possibly null) strings. From misspelled/correct word pairs, alpha-beta trains the probability P(α —* 0|PSN), conditioned by the position PSN of α in a word, where PSN E {start of word, middle of word, end of word}. Under this model, the probability of rewriting a string w to a string s is calculated as: P«0(8jw) = max REPart(w),TEPart(s) which corresponds to finding best partitions R and T in all possible partitions Part(w) and Part(s). Brill and Moore (2000) reported that this model gave a significant improvement over conventional edit distance methods. Brill et al. (2001) applied this model for extracting katakana-English transliteration pairs from query logs. They trained the edit distance between character chunks of katakana and Roman alphabets, after converting katakana strings to Roman script. We also trained this model using 59,754 katakanaEnglish pairs extracted from aligned Japanese and English Wikipedia article titles. In this paper we allowed |α|, |0 |&lt; 3. The resulting edit distance is obtained as the negative logarithm of the alpha-beta probability, i.e., EDαO(q|c) = −log Pα,3(q|c). Since the edit operations are directional and c and q can be any st</context>
</contexts>
<marker>Brill, Kacmarcik, Brockett, 2001</marker>
<rawString>Eric Brill, Gary Kacmarcik, and Chris Brockett. 2001. Automatically harvesting katakana-english term pairs from search engine query logs. In Proceedings of the Sixth Natural Language Processing Pacific Rim Symposium (NLPRS-2001), pages 393–399.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qing Chen</author>
<author>Mu Li</author>
</authors>
<title>Improving query spelling correction using web search results.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>181--189</pages>
<marker>Chen, Li, 2007</marker>
<rawString>Qing Chen, Mu Li, , and Ming Zhou. 2007. Improving query spelling correction using web search results. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 181–189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu Cucerzan</author>
<author>Eric Brill</author>
</authors>
<title>Spelling correction as an iterative process that exploits the collective knowledge of web users.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2004),</booktitle>
<pages>293--300</pages>
<contexts>
<context position="1337" citStr="Cucerzan and Brill, 2004" startWordPosition="194" endWordPosition="197">cal semantic similarity to avoid the problem of data sparseness in computing querycandidate similarity. We also propose an efficient method for generating query-candidate pairs for model training and testing. We show that the proposed method achieves about 80% accuracy on the query alteration task, improving over previously proposed methods that use semantic similarity. 1 Introduction Web search query correction is an important problem to solve for robust information retrieval given how pervasive errors are in search queries: it is said that more than 10% of web search queries contain errors (Cucerzan and Brill, 2004). English query correction has been an area of active research in recent years, building on previous work on generalpurpose spelling correction. However, there has been little investigation of query correction in languages other than English. In this paper, we address the issue of query correction, and more generally, query alteration in Japanese. Japanese poses particular challenges to the query correction task due to its complex writing system, summarized in Fig. 11. There are four &apos;The figure is somewhat over-simplified as it does not include any word consisting of multiple character types.</context>
<context position="5495" citStr="Cucerzan and Brill (2004)" startWordPosition="846" endWordPosition="849">antic similarity, we adopt a kernel-based method (Kandola et al., 2002), which improves the accuracy of the query alteration results over previously proposed methods. We also introduce a novel approach to creating a dataset of query and alteration candidate pairs efficiently and reliably from query session logs. 2 Related Work The key difference between traditional generalpurpose spelling correction and search query correction lies in the fact that the latter cannot rely on a lexicon: web queries are replete with valid outof-dictionary words which are not mis-spellings of in-vocabulary words. Cucerzan and Brill (2004) pioneered the research of query spelling correction, with an excellent description of how a traditional dictionary-based speller had to be adapted to solve the realistic query correction problem. The model they proposed is a source-channel model, where the source model is a word bigram model trained on query logs, and the channel model is based on a weighted Damerau-Levenshtein edit distance. Brill 2Our goal is to harvest alternation candidates; therefore, exactly how they are used in the search task (whether it is used to substitute the original query, to expand it, or simply to suggest an a</context>
<context position="9353" citStr="Cucerzan and Brill, 2004" startWordPosition="1491" endWordPosition="1494">are performed in a unified way. After computing the posterior probability of each candidate in CF(q) by the source channel model (Section 3.2), an N-best list is obtained as the initial candidate set C0, which is then augmented by the bootstrapping method Tchai (Section 3.4) to create the final candidate list C(q). The candidates in C(q) are re-ranked by a maximum entropy model (Section 3.5) and the candidate with the highest posterior probability is selected as the output. 3.2 Source Channel Model Source channel models are widely used for spelling and query correction (Brill and Moore, 2000; Cucerzan and Brill, 2004). Instead of directly computing Eq. (1), we can decompose the posterior probability using Bayes’ rule as: P(c)P(q|c), (2) where the source model P(c) measures how probable the candidate c is, while the error model P(q|c) measures how similar q and c are. For the source model, an n-gram based statistical language model is the standard in previous work (Ahmad and Kondrak, 2005; Li et al., 2006). Word n-gram models are simple to create for English, which is easy to tokenize and to obtain word-based statistics, but this is not the case with Japanese. Therefore, we simply considered the whole input</context>
<context position="24500" citStr="Cucerzan and Brill (2004)" startWordPosition="4019" endWordPosition="4022"> Experiment 4.1 Dataset Creation For all the experiments conducted in this paper, we used a subset of the Japanese search query logs submitted to Live Search (www.live.com) in November and December of 2007. Queries submitted less than eight times were deleted. The query log we used contained 83,080,257 tokens and 1,038,499 unique queries. Models of query correction in previous work were trained and evaluated using manually created querycandidate pairs. That is, human annotators were given a set of queries and were asked to provide a correction for each query when it needed to be rewritten. As Cucerzan and Brill (2004) point out, however, this method is seriously flawed in that the intention of the original query is completely lost to the annotator, without which the correction is often impossible: it is not clear if gogle should be corrected to google or goggle, or neither — gogle may be a brand new product name. Cucerzan and Brill K Ec exp Ei=1 λifi(c, q), (13) 196 therefore performed a second evaluation, where the test data was drawn by sampling the query logs for successive queries (q1, q2) by the same user where the edit distance between q1 and q2 are within a certain threshold, which are then submitte</context>
</contexts>
<marker>Cucerzan, Brill, 2004</marker>
<rawString>Silviu Cucerzan and Eric Brill. 2004. Spelling correction as an iterative process that exploits the collective knowledge of web users. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2004), pages 293–300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred Damerau</author>
</authors>
<title>A technique for computer detection and correction of spelling errors.</title>
<date>1964</date>
<journal>Communication of the ACM,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="10532" citStr="Damerau, 1964" startWordPosition="1689" endWordPosition="1690"> simply considered the whole input string as a candidate to be altered, and used the relative frequency of candidates in the query logs to build the language model: Freq(c) (3) P(c) = E&amp;EC Freq(c�). For the error model, we used an improved channel model described in (Brill and Moore, 2000), used to separate words in Japanese search queries, due to their keyword-based nature. which we call the alpha-beta model in this paper. The model is a weighted extension of the normal Damerau-Levenshtein edit distance which equally penalizes single character insertion, substitution, or deletion operations (Damerau, 1964; Levenshtein, 1966), and considers generic edit operations of the form α —* Q, where α and 0 are any (possibly null) strings. From misspelled/correct word pairs, alpha-beta trains the probability P(α —* 0|PSN), conditioned by the position PSN of α in a word, where PSN E {start of word, middle of word, end of word}. Under this model, the probability of rewriting a string w to a string s is calculated as: P«0(8jw) = max REPart(w),TEPart(s) which corresponds to finding best partitions R and T in all possible partitions Part(w) and Part(s). Brill and Moore (2000) reported that this model gave a s</context>
</contexts>
<marker>Damerau, 1964</marker>
<rawString>Fred Damerau. 1964. A technique for computer detection and correction of spelling errors. Communication of the ACM, 7(3):659–664.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J N Darroch</author>
<author>D Ratcliff</author>
</authors>
<title>Generalized iterative scaling for log-linear models.</title>
<date>1972</date>
<journal>Annuals of Mathematical Statistics,</journal>
<pages>43--1470</pages>
<contexts>
<context position="22357" citStr="Darroch and Ratcliff, 1972" startWordPosition="3657" endWordPosition="3660">imum Entropy Model In order to build a unified probabilistic query alteration model, we used the maximum entropy approach of (Beger et al., 1996), which Li et al. (2006) also employed for their query correction task and showed its effectiveness. It defines a conditional probabilistic distribution P(c|q) based on a set of feature functions f1, ... , fK: P( |) = exp EKi= 1 λifi(c, q) c q where λ1, ... , λK are the feature weights. The optimal set of feature weights λ* can be computed by maximizing the log-likelihood of the training set. We used the Generalized Iterative Scaling (GIS) algorithm (Darroch and Ratcliff, 1972) to optimize the feature weights. GIS trains conditional probability in Eq. (13), which requires the normalization over all possible candidates. However, the number of all possible candidates C obtained from a query log can be very large, so we only calculated the sum over the candidates in C(q). This is the same approach that Och and Ney (2002) took for statistical machine translation, and Li et al. (2006) for query spelling correction. We used the following four categories of functions as the features: 1. Language model feature, given by the logarithm of the source model probability: log P(c</context>
</contexts>
<marker>Darroch, Ratcliff, 1972</marker>
<rawString>J.N. Darroch and D. Ratcliff. 1972. Generalized iterative scaling for log-linear models. Annuals of Mathematical Statistics, 43:1470–1480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="26068" citStr="Dunning, 1993" startWordPosition="4294" endWordPosition="4295">r method also eliminates all pairs of candidates that are not orthographically similar. We have therefore improved their method in the following manner, making the process more automated and thus more reliable. We first collected a subset of the query log that contains only those pairs (q1, q2) that are issued successively by the same user, q2 is issued within 3 minutes of q1, and q2 resulted in a click of the resulting page while q1 did not. The last condition adds the evidence that q2 was a better formulation than q1. We then ranked the collected query pairs using loglikelihood ratio (LLR) (Dunning, 1993), which measures the dependence between q1 and q2 within the context of web queries (Jones et al., 2006b). We randomly sampled 10,000 query pairs with LLR &gt; 200, and submitted them to annotators, who only confirm or reject a query pair as being synonymous. For example, q1 = nikon and q2 = canon are related but not synonymous, while we are reasonably sure q1 = ipot and q2 = ipod are synonymous, given that this pair has a high LLR value. This verification process is extremely fast and consistent across annotators: it takes less than 1 hour to go through 1,000 query pairs, and the inter-annotator</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic latent semantic indexing.</title>
<date>1999</date>
<booktitle>In Research and Development in Information Retrieval,</booktitle>
<pages>50--57</pages>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probabilistic latent semantic indexing. In Research and Development in Information Retrieval, pages 50–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rosie Jones</author>
<author>Kevin Bartz</author>
<author>Pero Subasic</author>
<author>Benjamin Rey</author>
</authors>
<title>Automatically generating related queries in japanese.</title>
<date>2006</date>
<booktitle>Language Resources and Evaluation (LRE),</booktitle>
<pages>40--3</pages>
<contexts>
<context position="7416" citStr="Jones et al. (2006" startWordPosition="1159" endWordPosition="1162">butional similarity is measured by the similarity of the context shared by two terms, and has been successfully applied to many natural language processing tasks, including semantic knowledge acquisition (Lin, 1998). Though the use of distributional similarity improved the query correction results in Li et al.’s work, one problem is that it is sparse and is not available for many rarer query strings. Chen et al. (2007) addressed this problem by using external information (i.e., web search results); we take a different approach to solve the sparseness problem, namely by using semantic kernels. Jones et al. (2006a) generated Japanese query alteration pairs from by mining query logs and built a regression model which predicts the quality of query rewriting pairs. Their model includes a wide variety of orthographical features, but not semantic similarity features. 3 Query Alteration Model 3.1 Problem Formulation We employ a formulation of query alteration model that is similar to conventional query correction models. Given a query string q as input, a query correction model finds a correct alteration c* within the confusion set of q, so that it maximizes the posterior probability: c* = arg max cECF(q)CC</context>
<context position="26171" citStr="Jones et al., 2006" startWordPosition="4311" endWordPosition="4314">refore improved their method in the following manner, making the process more automated and thus more reliable. We first collected a subset of the query log that contains only those pairs (q1, q2) that are issued successively by the same user, q2 is issued within 3 minutes of q1, and q2 resulted in a click of the resulting page while q1 did not. The last condition adds the evidence that q2 was a better formulation than q1. We then ranked the collected query pairs using loglikelihood ratio (LLR) (Dunning, 1993), which measures the dependence between q1 and q2 within the context of web queries (Jones et al., 2006b). We randomly sampled 10,000 query pairs with LLR &gt; 200, and submitted them to annotators, who only confirm or reject a query pair as being synonymous. For example, q1 = nikon and q2 = canon are related but not synonymous, while we are reasonably sure q1 = ipot and q2 = ipod are synonymous, given that this pair has a high LLR value. This verification process is extremely fast and consistent across annotators: it takes less than 1 hour to go through 1,000 query pairs, and the inter-annotator agreement rate of two annotators on 2,000 query pairs was 95.7%. We annotated 10,000 query pairs consi</context>
</contexts>
<marker>Jones, Bartz, Subasic, Rey, 2006</marker>
<rawString>Rosie Jones, Kevin Bartz, Pero Subasic, and Benjamin Rey. 2006a. Automatically generating related queries in japanese. Language Resources and Evaluation (LRE), 40(3-4):219–232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rosie Jones</author>
<author>Benjamin Rey</author>
<author>Omid Madani</author>
<author>Wiley Greiner</author>
</authors>
<title>Generating query substitutions.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th international World Wide Web conference (WWW-06),</booktitle>
<pages>387--396</pages>
<contexts>
<context position="7416" citStr="Jones et al. (2006" startWordPosition="1159" endWordPosition="1162">butional similarity is measured by the similarity of the context shared by two terms, and has been successfully applied to many natural language processing tasks, including semantic knowledge acquisition (Lin, 1998). Though the use of distributional similarity improved the query correction results in Li et al.’s work, one problem is that it is sparse and is not available for many rarer query strings. Chen et al. (2007) addressed this problem by using external information (i.e., web search results); we take a different approach to solve the sparseness problem, namely by using semantic kernels. Jones et al. (2006a) generated Japanese query alteration pairs from by mining query logs and built a regression model which predicts the quality of query rewriting pairs. Their model includes a wide variety of orthographical features, but not semantic similarity features. 3 Query Alteration Model 3.1 Problem Formulation We employ a formulation of query alteration model that is similar to conventional query correction models. Given a query string q as input, a query correction model finds a correct alteration c* within the confusion set of q, so that it maximizes the posterior probability: c* = arg max cECF(q)CC</context>
<context position="26171" citStr="Jones et al., 2006" startWordPosition="4311" endWordPosition="4314">refore improved their method in the following manner, making the process more automated and thus more reliable. We first collected a subset of the query log that contains only those pairs (q1, q2) that are issued successively by the same user, q2 is issued within 3 minutes of q1, and q2 resulted in a click of the resulting page while q1 did not. The last condition adds the evidence that q2 was a better formulation than q1. We then ranked the collected query pairs using loglikelihood ratio (LLR) (Dunning, 1993), which measures the dependence between q1 and q2 within the context of web queries (Jones et al., 2006b). We randomly sampled 10,000 query pairs with LLR &gt; 200, and submitted them to annotators, who only confirm or reject a query pair as being synonymous. For example, q1 = nikon and q2 = canon are related but not synonymous, while we are reasonably sure q1 = ipot and q2 = ipod are synonymous, given that this pair has a high LLR value. This verification process is extremely fast and consistent across annotators: it takes less than 1 hour to go through 1,000 query pairs, and the inter-annotator agreement rate of two annotators on 2,000 query pairs was 95.7%. We annotated 10,000 query pairs consi</context>
</contexts>
<marker>Jones, Rey, Madani, Greiner, 2006</marker>
<rawString>Rosie Jones, Benjamin Rey, Omid Madani, and Wiley Greiner. 2006b. Generating query substitutions. In Proceedings of the 15th international World Wide Web conference (WWW-06), pages 387–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaz Kandola</author>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
</authors>
<title>Learning semantic similarity.</title>
<date>2002</date>
<booktitle>In Neural Information Processing Systems (NIPS 15),</booktitle>
<pages>657--664</pages>
<contexts>
<context position="4941" citStr="Kandola et al., 2002" startWordPosition="759" endWordPosition="762">of a spelling variant, or finding a strict synonym including abbreviations (e.g., MS -,,4&apos;,ul71 ‘Microsoft’, prefixed by Abbr in Fig. 1) and true synonyms (e.g., k—W (translation of ‘seat’) ✓--1 (transliteration of ‘seat’, indicated by Syn in Fig. 1)2. Our method is based on previous work on English query correction in that we use both spelling and semantic similarity between a query and its alteration candidate, but is more general in that we include alteration candidates that are not similar to the original query in spelling. In computing semantic similarity, we adopt a kernel-based method (Kandola et al., 2002), which improves the accuracy of the query alteration results over previously proposed methods. We also introduce a novel approach to creating a dataset of query and alteration candidate pairs efficiently and reliably from query session logs. 2 Related Work The key difference between traditional generalpurpose spelling correction and search query correction lies in the fact that the latter cannot rely on a lexicon: web queries are replete with valid outof-dictionary words which are not mis-spellings of in-vocabulary words. Cucerzan and Brill (2004) pioneered the research of query spelling corr</context>
<context position="16111" citStr="Kandola et al. (2002)" startWordPosition="2599" endWordPosition="2602">g errors, alterations, and word permutations as much as queries do, context differs so greatly in representations that even related candidates might not have sufficient contextual overlap between them. For example, a candidate “YouTube” matched against the patterns “YouTube+movie”, “movie+YouTube” and “You-Tube+movii” (with a minor spelling error) will yield three distinct patterns “#+movie”, “movie+#” and “#+movii”6, which will be treated as three separate dimensions in the vector space model. This sparseness problem can be partially addressed by considering the correlation between patterns. Kandola et al. (2002) proposed new kernelbased similarity methods which incorporate indirect similarity between terms for a text retrieval task. Although their kernels are built on a document-term co-occurrence model, they can also be applied to our candidate-pattern co-occurrence model. The proposed kernel is recursively defined as: K� = βX&apos; GX + K, G� = βX KX&apos; + G, (7) where G = XX&apos; is the correlation matrix between patterns and β is the factor to ensure that longer range effects decay exponentially. This can be interpreted as augmenting the similarity matrix K through indirect similarities of patterns G and vic</context>
<context position="17721" citStr="Kandola et al. (2002)" startWordPosition="2884" endWordPosition="2887"> = K(I − βK)−1 = �∞ βt−1Kt. (8) t=1 This can also be interpreted in terms of a random walk in a graph where the nodes correspond to all the candidates and the weight of an edge (i, j) is given by KZj. A simple calculation shows that KZj equals the sum of the products of the edge weights over all possible paths between the nodes corresponding cZ and cj in the graph. Also, KtZj corresponds to the probability that a random walk beginning at node cZ ends up at node cj after t steps, assuming that the entries are all positive and the sum of the connections is 1 at each node. Following this notion, Kandola et al. (2002) proposed another kernel called exponential kernel, with alternative faster decay factors: = K exp(βK). (9) They showed that this alternative kernel achieved a better performance for their text retrieval task. We employed these two kernels to compute distributional similarity for our query correction task. 3.3.3 Orthographically Augmented Kernels Although semantic relatedness can be partially captured by the semantic kernels introduced in the previous section, they may still have difficulties computing correlations between candidates and patterns especially for only sparsely connected graphs. </context>
</contexts>
<marker>Kandola, Shawe-Taylor, Cristianini, 2002</marker>
<rawString>Jaz Kandola, John Shawe-Taylor, and Nello Cristianini. 2002. Learning semantic similarity. In Neural Information Processing Systems (NIPS 15), pages 657–664.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<date>1998</date>
<booktitle>Machine transliteration. Computational Linguistics,</booktitle>
<volume>24</volume>
<issue>4</issue>
<pages>612</pages>
<contexts>
<context position="3950" citStr="Knight and Graehl, 1998" startWordPosition="597" endWordPosition="600"> corpus they studied. Normalizing katakana spelling variations has been the subject of research by itself (Aramaki et al., 2008; Masuyama et al., 2004). Similarly, English-to-katakana transliteration (e.g., ‘fedex’ as フェデックス fedekkusuinFig. 1) and katakana-to191 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 191–199, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics English back-transliteration (e.g., 7r7—!y&apos;,x back into ‘fedex’) have also been studied extensively (Bilac and Tanaka, 2004; Brill et al., 2001; Knight and Graehl, 1998), as it is an essential component for machine translation. To our knowledge, however, there has been no work that addresses spelling variation in Japanese generally. In this paper, we propose a general approach to query correction/alteration in Japanese. Our goal is to find precise re-write candidates for a query, be it a correction of a spelling error, normalization of a spelling variant, or finding a strict synonym including abbreviations (e.g., MS -,,4&apos;,ul71 ‘Microsoft’, prefixed by Abbr in Fig. 1) and true synonyms (e.g., k—W (translation of ‘seat’) ✓--1 (transliteration of ‘seat’, indicat</context>
</contexts>
<marker>Knight, Graehl, 1998</marker>
<rawString>Kevin Knight and Jonathan Graehl. 1998. Machine transliteration. Computational Linguistics, 24(4):599– 612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mamoru Komachi</author>
<author>Hisami Suzuki</author>
</authors>
<title>Minimally supervised learning of semantic knowledge from query logs.</title>
<date>2008</date>
<booktitle>In Proceedings of the 3rd International Joint Conference on Natural Language Processing (IJCNLP-2008),</booktitle>
<pages>358--365</pages>
<contexts>
<context position="13504" citStr="Komachi and Suzuki (2008)" startWordPosition="2172" endWordPosition="2175">l Similarity The source channel model described in Section 3.2 only considers language and error models and cannot capture semantic similarity between the query and its correction candidate. To address this issue, we use distributional similarity (Lin, 1998) estimated from query logs as additional evidence for query alteration, following Li et al. (2006). For English, it is relatively easy to define the context of a word based on the bag-of-words model. As this is not expected to work on Japanese, we define context as everything but the query string in a query log, as Pas¸ca et al. (2006) and Komachi and Suzuki (2008) did for their information extraction tasks. This formulation does not involve any segmentation or boundary detection, which makes this method fast and robust. On the other hand, this may cause additional sparseness in the vector representation; we address this issue in the next two sections. Once the context of a candidate ci is defined as the patterns that the candidate co-occurs with, it can be represented as a vector ci = [pmi(ci,p1), ... , pmi(ci, pM)]&apos;, where M denotes the number of context patterns and x&apos; is the transposition of a vector (or possibly a matrix) x. The elements of the vec</context>
<context position="21033" citStr="Komachi and Suzuki, 2008" startWordPosition="3435" endWordPosition="3438"> query correction model can cover query-candidate pairs K(β) = K βtKt t! ∞ E t=1 195 which are only semantically related. However, previous work on query correction all used a string distance function and a threshold to restrict the space of potential candidates, allowing only the orthographically similar candidates. To collect additional candidates, the use of context-based semantic extraction methods would be effective because semantically related candidates are likely to share context with the initial query q, or at least with the initial candidate set C0. Here we used the Tchai algorithm (Komachi and Suzuki, 2008), a modified version of Espresso (Pantel and Pennacchiotti, 2006) to collect such candidates. This algorithm starts with initial seed instances, then induces reliable context patterns cooccurring with the seeds, induces instances from the patterns, and iterates this process to obtain categories of semantically related words. Using the candidates in C0 as the seed instances, one bootstrapping iteration of the Tchai algorithm is executed to obtain the semantically related set of instances C1. The seed instance reliabilities are given by the source channel probabilities P(c)P(q|c). Finally we tak</context>
</contexts>
<marker>Komachi, Suzuki, 2008</marker>
<rawString>Mamoru Komachi and Hisami Suzuki. 2008. Minimally supervised learning of semantic knowledge from query logs. In Proceedings of the 3rd International Joint Conference on Natural Language Processing (IJCNLP-2008), pages 358–365.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions and reversals. Soviet Physice - Doklady,</title>
<date>1966</date>
<pages>10--707</pages>
<contexts>
<context position="10552" citStr="Levenshtein, 1966" startWordPosition="1691" endWordPosition="1692">red the whole input string as a candidate to be altered, and used the relative frequency of candidates in the query logs to build the language model: Freq(c) (3) P(c) = E&amp;EC Freq(c�). For the error model, we used an improved channel model described in (Brill and Moore, 2000), used to separate words in Japanese search queries, due to their keyword-based nature. which we call the alpha-beta model in this paper. The model is a weighted extension of the normal Damerau-Levenshtein edit distance which equally penalizes single character insertion, substitution, or deletion operations (Damerau, 1964; Levenshtein, 1966), and considers generic edit operations of the form α —* Q, where α and 0 are any (possibly null) strings. From misspelled/correct word pairs, alpha-beta trains the probability P(α —* 0|PSN), conditioned by the position PSN of α in a word, where PSN E {start of word, middle of word, end of word}. Under this model, the probability of rewriting a string w to a string s is calculated as: P«0(8jw) = max REPart(w),TEPart(s) which corresponds to finding best partitions R and T in all possible partitions Part(w) and Part(s). Brill and Moore (2000) reported that this model gave a significant improveme</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Vladimir I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions and reversals. Soviet Physice - Doklady, 10:707–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mu Li</author>
<author>Muhua Zhu</author>
<author>Yang Zhang</author>
<author>Ming Zhou</author>
</authors>
<title>Exploring distributional similarity based models for query spelling correction.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL-2006,</booktitle>
<pages>1025--1032</pages>
<contexts>
<context position="6501" citStr="Li et al. (2006)" startWordPosition="1009" endWordPosition="1012">tance. Brill 2Our goal is to harvest alternation candidates; therefore, exactly how they are used in the search task (whether it is used to substitute the original query, to expand it, or simply to suggest an alternative) is not a concern to us here. and Moore (2000) proposed a general, improved source model for general spelling correction, while Ahmad and Kondrak (2005) learned a spelling error model from search query logs using the Expectation Maximization algorithm, without relying on a training set of misspelled words and their corrections. Extending the work of Cucerzan and Brill (2004), Li et al. (2006) proposed to include semantic similarity between the query and its correction candidate. They point out that adventura is a common misspelling of aventura, not adventure, and this cannot be captured by a simple string edit distance, but requires some knowledge of distributional similarity. Distributional similarity is measured by the similarity of the context shared by two terms, and has been successfully applied to many natural language processing tasks, including semantic knowledge acquisition (Lin, 1998). Though the use of distributional similarity improved the query correction results in L</context>
<context position="9748" citStr="Li et al., 2006" startWordPosition="1560" endWordPosition="1563">andidate with the highest posterior probability is selected as the output. 3.2 Source Channel Model Source channel models are widely used for spelling and query correction (Brill and Moore, 2000; Cucerzan and Brill, 2004). Instead of directly computing Eq. (1), we can decompose the posterior probability using Bayes’ rule as: P(c)P(q|c), (2) where the source model P(c) measures how probable the candidate c is, while the error model P(q|c) measures how similar q and c are. For the source model, an n-gram based statistical language model is the standard in previous work (Ahmad and Kondrak, 2005; Li et al., 2006). Word n-gram models are simple to create for English, which is easy to tokenize and to obtain word-based statistics, but this is not the case with Japanese. Therefore, we simply considered the whole input string as a candidate to be altered, and used the relative frequency of candidates in the query logs to build the language model: Freq(c) (3) P(c) = E&amp;EC Freq(c�). For the error model, we used an improved channel model described in (Brill and Moore, 2000), used to separate words in Japanese search queries, due to their keyword-based nature. which we call the alpha-beta model in this paper. T</context>
<context position="13235" citStr="Li et al. (2006)" startWordPosition="2120" endWordPosition="2123">d(q, c)],(4) P(q|c) = exp[−ED(q, c)] (5) where every edit distance is normalized to [0, 1] by multiplying by a factor of 2/(|q||c|) so that it does not depend on the length of the input strings5. 3.3 Kernel-based Lexical Semantic Similarity 3.3.1 Distributional Similarity The source channel model described in Section 3.2 only considers language and error models and cannot capture semantic similarity between the query and its correction candidate. To address this issue, we use distributional similarity (Lin, 1998) estimated from query logs as additional evidence for query alteration, following Li et al. (2006). For English, it is relatively easy to define the context of a word based on the bag-of-words model. As this is not expected to work on Japanese, we define context as everything but the query string in a query log, as Pas¸ca et al. (2006) and Komachi and Suzuki (2008) did for their information extraction tasks. This formulation does not involve any segmentation or boundary detection, which makes this method fast and robust. On the other hand, this may cause additional sparseness in the vector representation; we address this issue in the next two sections. Once the context of a candidate ci is</context>
<context position="21899" citStr="Li et al. (2006)" startWordPosition="3578" endWordPosition="3581">nd iterates this process to obtain categories of semantically related words. Using the candidates in C0 as the seed instances, one bootstrapping iteration of the Tchai algorithm is executed to obtain the semantically related set of instances C1. The seed instance reliabilities are given by the source channel probabilities P(c)P(q|c). Finally we take the union C0 U C1 to obtain the candidate set C(q). This process is outlined in Fig. 3. 3.5 Maximum Entropy Model In order to build a unified probabilistic query alteration model, we used the maximum entropy approach of (Beger et al., 1996), which Li et al. (2006) also employed for their query correction task and showed its effectiveness. It defines a conditional probabilistic distribution P(c|q) based on a set of feature functions f1, ... , fK: P( |) = exp EKi= 1 λifi(c, q) c q where λ1, ... , λK are the feature weights. The optimal set of feature weights λ* can be computed by maximizing the log-likelihood of the training set. We used the Generalized Iterative Scaling (GIS) algorithm (Darroch and Ratcliff, 1972) to optimize the feature weights. GIS trains conditional probability in Eq. (13), which requires the normalization over all possible candidate</context>
<context position="23640" citStr="Li et al. (2006)" startWordPosition="3874" endWordPosition="3877">ce functions: −EDαβ(q|c), −EDαβ(c|q), and −EDhd(q, c). 3. Similarity based feature, computed as the logarithm of distributional similarity between q and c: log sim(q, c), which is calcualted using one of the following kernels (Section 3.3): K, K, K, K+, and K+. The similarity values were normalized to [0, 1] after adding a small discounting factor ε = 1.0 X 10−5. 4. Similarity based correction candidate features, which are binary features with a value of 1 if and only if the frequency of c is higher than that of q, and distributional similarity between them is higher than a certain threshold. Li et al. (2006) used this set of features, and suggested that these features give the evidence that q may be a common misspelling of c. The thresholds on the normalized distributional similarity are enumerated from 0.5 to 0.9 with the interval 0.1. 4 Experiment 4.1 Dataset Creation For all the experiments conducted in this paper, we used a subset of the Japanese search query logs submitted to Live Search (www.live.com) in November and December of 2007. Queries submitted less than eight times were deleted. The query log we used contained 83,080,257 tokens and 1,038,499 unique queries. Models of query correcti</context>
<context position="27187" citStr="Li et al., 2006" startWordPosition="4484" endWordPosition="4487">ss annotators: it takes less than 1 hour to go through 1,000 query pairs, and the inter-annotator agreement rate of two annotators on 2,000 query pairs was 95.7%. We annotated 10,000 query pairs consisting of alphanumerical and kana characters in this manner. After rejecting non-synonymous pairs and those which do not co-occur with any context patterns, 6,489 pairs remained, and we used 1,243 pairs for testing, 628 as a development set, and 4,618 for training the maximum entropy model. 4.2 Experimental Settings The performance of query alteration was evaluated based on the following measures (Li et al., 2006). Table 1: Performance results (%) Model Accuracy Recall Precision SC 71.12 39.29 45.09 ME-NoSim 74.58 44.58 52.52 ME-Cos 74.18 45.84 50.70 ME-vN 74.34 45.59 52.16 ME-Exp 73.61 44.84 50.57 ME-vN+ 75.06 44.33 53.01 ME-Exp+ 75.14 44.08 53.52 The input queries, correct suggestions, and outputs were matched in a case-insensitive manner. • Accuracy: The number of correct outputs generated by the system divided by the total number of queries in the test set; • Recall: The number of correct suggestions for altered queries divided by the total number of altered queries in the test set; • Precision: Th</context>
<context position="30482" citStr="Li et al., 2006" startWordPosition="5037" endWordPosition="5040"> semantic similarity does not always help, which we believe is due to the sparseness of the contextual information used in computing semantic similarity. On the other hand, ME-vN+ (with augmented von Neumann kernel) and ME-Exp+ (with augmented exponential kernel) increased both accuracy and precision with a slight decrease of recall, compared to the distributional similarity baseline and the nonaugmented kernel-based methods. ME-Exp+ was significantly better than ME-Exp (p &lt; 0.01). Note that the accuracy values appear lower than some of the previous results on English (e.g., more than 80% in (Li et al., 2006)), but this is because the dataset creation method we employed tends to over-represent the pairs that lead to alteration: the simplest baseline (= always propose no alteration) performs 67.3% accuracy on our data, in contrast to 83.4% on the data used in (Li et al., 2006). Manually examining the suggestions made by the system also confirms the effectiveness of our model. For example, the similarity-based models altered the query ipot to ipod, while the simple ME-NoSim model failed, because it depends too much on the edit distance-based features. We also observed that many of the suggestions ma</context>
</contexts>
<marker>Li, Zhu, Zhang, Zhou, 2006</marker>
<rawString>Mu Li, Muhua Zhu, Yang Zhang, and Ming Zhou. 2006. Exploring distributional similarity based models for query spelling correction. In Proceedings of COLING/ACL-2006, pages 1025–1032.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL-1998,</booktitle>
<pages>786--774</pages>
<contexts>
<context position="7013" citStr="Lin, 1998" startWordPosition="1091" endWordPosition="1092">led words and their corrections. Extending the work of Cucerzan and Brill (2004), Li et al. (2006) proposed to include semantic similarity between the query and its correction candidate. They point out that adventura is a common misspelling of aventura, not adventure, and this cannot be captured by a simple string edit distance, but requires some knowledge of distributional similarity. Distributional similarity is measured by the similarity of the context shared by two terms, and has been successfully applied to many natural language processing tasks, including semantic knowledge acquisition (Lin, 1998). Though the use of distributional similarity improved the query correction results in Li et al.’s work, one problem is that it is sparse and is not available for many rarer query strings. Chen et al. (2007) addressed this problem by using external information (i.e., web search results); we take a different approach to solve the sparseness problem, namely by using semantic kernels. Jones et al. (2006a) generated Japanese query alteration pairs from by mining query logs and built a regression model which predicts the quality of query rewriting pairs. Their model includes a wide variety of ortho</context>
<context position="13137" citStr="Lin, 1998" startWordPosition="2106" endWordPosition="2107">arg max cECF(q)CC � |R |P(Ri ! TijPSN(Ri)), i=1 193 ED(q, c) = min[ED«p(q|c), ED«p(c|q), EDhd(q, c)],(4) P(q|c) = exp[−ED(q, c)] (5) where every edit distance is normalized to [0, 1] by multiplying by a factor of 2/(|q||c|) so that it does not depend on the length of the input strings5. 3.3 Kernel-based Lexical Semantic Similarity 3.3.1 Distributional Similarity The source channel model described in Section 3.2 only considers language and error models and cannot capture semantic similarity between the query and its correction candidate. To address this issue, we use distributional similarity (Lin, 1998) estimated from query logs as additional evidence for query alteration, following Li et al. (2006). For English, it is relatively easy to define the context of a word based on the bag-of-words model. As this is not expected to work on Japanese, we define context as everything but the query string in a query log, as Pas¸ca et al. (2006) and Komachi and Suzuki (2008) did for their information extraction tasks. This formulation does not involve any segmentation or boundary detection, which makes this method fast and robust. On the other hand, this may cause additional sparseness in the vector rep</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of COLING/ACL-1998, pages 786–774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takeshi Masuyama</author>
<author>Satoshi Sekine</author>
<author>Hiroshi Nakagawa</author>
</authors>
<title>Automatic construction of japanese katakana variant list from large corpus.</title>
<date>2004</date>
<booktitle>In Proceedings of Proceedings of the 20th international conference on Computational Linguistics (COLING-2004),</booktitle>
<pages>1214--1219</pages>
<contexts>
<context position="3197" citStr="Masuyama et al. (2004)" startWordPosition="488" endWordPosition="491">ariants are shown in Fig. 1 with the prefix Sp: as is observed from the figure, spelling variation occurs within and across different character types. Resolving these variants will be essential not only for information retrieval but practically for all NLP tasks. A particularly prolific source of spelling variations in Japanese is katakana. Katakana characters are used to transliterate words from English and other foreign languages, and as such, the variations in the source language pronunciation as well as the ambiguity in sound adaptation are reflected in the katakana spelling. For example, Masuyama et al. (2004) report that at least six distinct transliterations of the word ‘spaghetti’ (スパゲッティ, スパゲ ティー, etc.) are attested in the newspaper corpus they studied. Normalizing katakana spelling variations has been the subject of research by itself (Aramaki et al., 2008; Masuyama et al., 2004). Similarly, English-to-katakana transliteration (e.g., ‘fedex’ as フェデックス fedekkusuinFig. 1) and katakana-to191 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 191–199, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics English back-tra</context>
</contexts>
<marker>Masuyama, Sekine, Nakagawa, 2004</marker>
<rawString>Takeshi Masuyama, Satoshi Sekine, and Hiroshi Nakagawa. 2004. Automatic construction of japanese katakana variant list from large corpus. In Proceedings of Proceedings of the 20th international conference on Computational Linguistics (COLING-2004), pages 1214–1219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th annual meeting of ACL,</booktitle>
<pages>295--302</pages>
<contexts>
<context position="22704" citStr="Och and Ney (2002)" startWordPosition="3717" endWordPosition="3720">( |) = exp EKi= 1 λifi(c, q) c q where λ1, ... , λK are the feature weights. The optimal set of feature weights λ* can be computed by maximizing the log-likelihood of the training set. We used the Generalized Iterative Scaling (GIS) algorithm (Darroch and Ratcliff, 1972) to optimize the feature weights. GIS trains conditional probability in Eq. (13), which requires the normalization over all possible candidates. However, the number of all possible candidates C obtained from a query log can be very large, so we only calculated the sum over the candidates in C(q). This is the same approach that Och and Ney (2002) took for statistical machine translation, and Li et al. (2006) for query spelling correction. We used the following four categories of functions as the features: 1. Language model feature, given by the logarithm of the source model probability: log P(c). 2. Error model features, which are composed of three edit distance functions: −EDαβ(q|c), −EDαβ(c|q), and −EDhd(q, c). 3. Similarity based feature, computed as the logarithm of distributional similarity between q and c: log sim(q, c), which is calcualted using one of the following kernels (Section 3.3): K, K, K, K+, and K+. The similarity val</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of the 40th annual meeting of ACL, pages 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pas¸ca</author>
<author>Dekang Lin</author>
<author>Jeffrey Bigham</author>
<author>Andrei Lifchits</author>
<author>Alpa Jain</author>
</authors>
<title>Organizing and searching the world wide web of facts - step one: the one-million fact extraction challenge.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st National Conference on Artificial Intelligence (AAAI06),</booktitle>
<pages>1400--1405</pages>
<marker>Pas¸ca, Lin, Bigham, Lifchits, Jain, 2006</marker>
<rawString>Marius Pas¸ca, Dekang Lin, Jeffrey Bigham, Andrei Lifchits, and Alpa Jain. 2006. Organizing and searching the world wide web of facts - step one: the one-million fact extraction challenge. In Proceedings of the 21st National Conference on Artificial Intelligence (AAAI06), pages 1400–1405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Espresso: Leveraging generic patterns for automatically harvesting semantic relations.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL-2006,</booktitle>
<pages>113--120</pages>
<contexts>
<context position="21098" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="3444" endWordPosition="3448">) = K βtKt t! ∞ E t=1 195 which are only semantically related. However, previous work on query correction all used a string distance function and a threshold to restrict the space of potential candidates, allowing only the orthographically similar candidates. To collect additional candidates, the use of context-based semantic extraction methods would be effective because semantically related candidates are likely to share context with the initial query q, or at least with the initial candidate set C0. Here we used the Tchai algorithm (Komachi and Suzuki, 2008), a modified version of Espresso (Pantel and Pennacchiotti, 2006) to collect such candidates. This algorithm starts with initial seed instances, then induces reliable context patterns cooccurring with the seeds, induces instances from the patterns, and iterates this process to obtain categories of semantically related words. Using the candidates in C0 as the seed instances, one bootstrapping iteration of the Tchai algorithm is executed to obtain the semantically related set of instances C1. The seed instance reliabilities are given by the source channel probabilities P(c)P(q|c). Finally we take the union C0 U C1 to obtain the candidate set C(q). This proces</context>
</contexts>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: Leveraging generic patterns for automatically harvesting semantic relations. In Proceedings of COLING/ACL-2006, pages 113–120.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>