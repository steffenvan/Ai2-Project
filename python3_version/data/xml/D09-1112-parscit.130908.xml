<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000354">
<title confidence="0.9958855">
Re-Ranking Models Based-on Small Training Data for Spoken Language
Understanding
</title>
<author confidence="0.996738">
Marco Dinarelli
</author>
<affiliation confidence="0.9224135">
University of Trento
Italy
</affiliation>
<email confidence="0.991308">
dinarelli@disi.unitn.it
</email>
<author confidence="0.992394">
Alessandro Moschitti
</author>
<affiliation confidence="0.922172">
University of Trento
Italy
</affiliation>
<email confidence="0.992976">
moschitti@disi.unitn.it
</email>
<author confidence="0.990616">
Giuseppe Riccardi
</author>
<affiliation confidence="0.92176">
University of Trento
Italy
</affiliation>
<email confidence="0.992636">
riccardi@disi.unitn.it
</email>
<sectionHeader confidence="0.997321" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999961086956522">
The design of practical language applica-
tions by means of statistical approaches
requires annotated data, which is one of
the most critical constraint. This is par-
ticularly true for Spoken Dialog Systems
since considerably domain-specific con-
ceptual annotation is needed to obtain ac-
curate Language Understanding models.
Since data annotation is usually costly,
methods to reduce the amount of data are
needed. In this paper, we show that bet-
ter feature representations serve the above
purpose and that structure kernels pro-
vide the needed improved representation.
Given the relatively high computational
cost of kernel methods, we apply them to
just re-rank the list of hypotheses provided
by a fast generative model. Experiments
with Support Vector Machines and differ-
ent kernels on two different dialog cor-
pora show that our re-ranking models can
achieve better results than state-of-the-art
approaches when small data is available.
</bodyText>
<sectionHeader confidence="0.999478" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999552472727273">
Spoken Dialog Systems carry out automatic
speech recognition and shallow natural language
understanding by heavily relying on statistical
models. These in turn need annotated data de-
scribing the application domain. Such annotation
is far the most expensive part of the system de-
sign. Therefore, methods reducing the amount of
labeled data can speed up and lower the overall
amount of work.
Among others, Spoken Language Understand-
ing (SLU) is an important component of the sys-
tems above, which requires training data to trans-
late a spoken sentence into its meaning repre-
sentation based on semantic constituents. These
are conceptual units instantiated by sequences of
words.
In the last decade two major approaches have
been proposed to automatically map words in con-
cepts: (i) generative models, whose parameters re-
fer to the joint probability of concepts and con-
stituents; and (ii) discriminative models, which
learn a classification function based on conditional
probabilities of concepts given words.
A simple but effective generative model is the
one based on Finite State Transducers. It performs
SLU as a translation process from words to con-
cepts using Finite State Transducers (FST). An ex-
ample of discriminative model used for SLU is the
one based on Support Vector Machines (SVMs)
(Vapnik, 1995), as shown in (Raymond and Ric-
cardi, 2007). In this approach, data is mapped into
a vector space and SLU is performed as a clas-
sification problem using Maximal Margin Clas-
sifiers (Vapnik, 1995). A relatively more recent
approach for SLU is based on Conditional Ran-
dom Fields (CRF) (Lafferty et al., 2001). CRFs
are undirected graphical and discriminative mod-
els. They use conditional probabilities to account
for many feature dependencies without the need of
explicitly representing such dependencies.
Generative models have the advantage to be
more robust to overfitting on training data, while
discriminative models are more robust to irrele-
vant features. Both approaches, used separately,
have shown good accuracy (Raymond and Ric-
cardi, 2007), but they have very different charac-
teristics and the way they encode prior knowledge
is very different, thus designing models that take
into account characteristics of both approaches are
particularly promising.
In this paper, we propose a method for SLU
based on generative and discriminative models:
the former uses FSTs to generate a list of SLU
hypotheses, which are re-ranked by SVMs. To
effectively design our re-ranker, we use all pos-
</bodyText>
<page confidence="0.963265">
1076
</page>
<note confidence="0.9965895">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1076–1085,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.998715666666667">
sible word/concept subsequences with gaps of the
spoken sentence as features (i.e. all possible n-
grams). Gaps allow for encoding long distance de-
pendencies between words in relatively small se-
quences. Since the space of such features is huge,
we adopted kernel methods, i.e. sequence kernels
(Shawe-Taylor and Cristianini, 2004) and tree ker-
nels (Collins and Duffy, 2002; Moschitti, 2006a)
to implicitly encode them along with other struc-
tural information in SVMs.
We experimented with different approaches for
training the discriminative models and two differ-
ent corpora: the french MEDIA corpus (Bonneau-
Maynard et al., 2005) and a corpus made available
by the European project LUNA1 (Dinarelli et al.,
2009b). In particular, the new contents with re-
spect to our previous work (Dinarelli et al., 2009a)
are:
</bodyText>
<listItem confidence="0.9644014">
• We designed a new sequential structure
(SK2) and two new hierarchical tree struc-
tures (MULTILEVEL and FEATURES) for
re-ranking models (see Section 4.2). The lat-
ter combined with two different tree kernels
originate four new different models.
• We experimented with automatic speech
transcriptions thus assessing the robustness to
noise of our models.
• We compare our models against Conditional
Random Field (CRF) approaches described
in (Hahn et al., 2008), which are the cur-
rent state-of-the-art in SLU. Learning curves
clearly show that our models improve CRF,
especially when small data sets are used.
</listItem>
<bodyText confidence="0.9997505">
The remainder of the paper is organized as fol-
lows: Section 2 introduces kernel methods for
structured data, Section 3 describes the generative
model producing the initial hypotheses whereas
Section 4 presents the discriminative models for
re-ranking them. The experiments and results
are reported in Section 5 and the conclusions are
drawn in Section 6.
</bodyText>
<sectionHeader confidence="0.986125" genericHeader="introduction">
2 Feature Engineering via Structure
Kernels
</sectionHeader>
<bodyText confidence="0.957144666666667">
Kernel methods are viable approaches to engi-
neer features for text processing, e.g. (Collins and
Duffy, 2002; Kudo and Matsumoto, 2003; Cumby
</bodyText>
<footnote confidence="0.820968">
1Contract n. 33549
</footnote>
<bodyText confidence="0.998945">
and Roth, 2003; Cancedda et al., 2003; Culotta
and Sorensen, 2004; Toutanova et al., 2004; Kudo
et al., 2005; Moschitti, 2006a; Moschitti et al.,
2007; Moschitti, 2008; Moschitti et al., 2008;
Moschitti and Quarteroni, 2008). In the follow-
ing, we describe structure kernels, which will be
used to engineer features for our discriminative re-
ranker.
</bodyText>
<subsectionHeader confidence="0.99897">
2.1 String Kernels
</subsectionHeader>
<bodyText confidence="0.973444846153846">
The String Kernels that we consider count the
number of substrings containing gaps shared by
two sequences, i.e. some of the symbols of the
original string are skipped. We adopted the ef-
ficient algorithm described in (Shawe-Taylor and
Cristianini, 2004; Lodhi et al., 2000). More
specifically, we used words and markers as sym-
bols in a style similar to (Cancedda et al., 2003;
Moschitti, 2008). For example, given the sen-
tence: How may I help you ? sample substrings,
extracted by the Sequence Kernel (SK), are: How
help you ?, How help ?, help you, may help you,
etc.
</bodyText>
<subsectionHeader confidence="0.998918">
2.2 Tree kernels
</subsectionHeader>
<bodyText confidence="0.999698875">
Tree kernels represent trees in terms of their sub-
structures (fragments). The kernel function detects
if a tree subpart (common to both trees) belongs to
the feature space that we intend to generate. For
such purpose, the desired fragments need to be de-
scribed. We consider two important characteriza-
tions: the syntactic tree (STF) and the partial tree
(PTF) fragments.
</bodyText>
<subsectionHeader confidence="0.793502">
2.2.1 Tree Fragment Types
</subsectionHeader>
<bodyText confidence="0.999826411764706">
An STF is a general subtree whose leaves can
be non-terminal symbols (also called SubSet Tree
(SST) in (Moschitti, 2006a)). For example, Fig-
ure 1(a) shows 10 STFs (out of 17) of the sub-
tree rooted in VP (of the left tree). The STFs sat-
isfy the constraint that grammatical rules cannot
be broken. For example, [VP [V NP]] is an
STF, which has two non-terminal symbols, V and
NP, as leaves whereas [VP [V]] is not an STF.
If we relax the constraint over the STFs, we ob-
tain more general substructures called partial trees
fragments (PTFs). These can be generated by the
application of partial production rules of the gram-
mar, consequently [VP [V]] and [VP [NP]]
are valid PTFs. Figure 1(b) shows that the num-
ber of PTFs derived from the same tree as before
is still higher (i.e. 30 PTs).
</bodyText>
<page confidence="0.993291">
1077
</page>
<figure confidence="0.994556">
(a) Syntactic Tree fragments (STF) (b) Partial Tree fragments (PTF)
</figure>
<figureCaption confidence="0.999981">
Figure 1: Examples of different classes of tree fragments.
</figureCaption>
<subsectionHeader confidence="0.999949">
2.3 Counting Shared Subtrees
</subsectionHeader>
<bodyText confidence="0.999800555555556">
The main idea of tree kernels is to compute the
number of common substructures between two
trees T1 and T2 without explicitly considering the
whole fragment space. To evaluate the above ker-
nels between two T1 and T2, we need to define a
set F = {f1, f2, ... , f|F|}, i.e. a tree fragment
space and an indicator function Ii(n), equal to 1
if the target fi is rooted at node n and equal to 0
otherwise. A tree-kernel function over T1 and T2
</bodyText>
<equation confidence="0.99317">
is TK(T1, T2) = � �n2∈NT2 A(n1, n2),
n1∈NT1
</equation>
<bodyText confidence="0.992209166666667">
where NT1 and NT2 are the sets of the T1’s
and T2’s nodes, respectively and A(n1, n2) =
�|F|
i=1Ii(n1)Ii(n2). The latter is equal to the num-
ber of common fragments rooted in the n1 and n2
nodes.
The algorithm for the efficient evaluation of A
for the syntactic tree kernel (STK) has been widely
discussed in (Collins and Duffy, 2002) whereas its
fast evaluation is proposed in (Moschitti, 2006b),
so we only describe the equations of the partial
tree kernel (PTK).
</bodyText>
<subsectionHeader confidence="0.994738">
2.4 The Partial Tree Kernel (PTK)
</subsectionHeader>
<bodyText confidence="0.997984666666667">
PTFs have been defined in (Moschitti, 2006a).
Their computation is carried out by the following
A function:
</bodyText>
<listItem confidence="0.971314666666667">
1. if the node labels of n1 and n2 are different
then A(n1, n2) = 0;
2. else A(n1, n2) =
</listItem>
<equation confidence="0.85539375">
1 + F_I1,~I2,l(~I1)=l(~I2) 11l(~I1)
j=1 A(cn1 (~I1j), cn2(~I2j))
where ~I1 = (h1, h2, h3, ..) and ~I2 =
(k1, k2, k3, ..) are index sequences associated with
</equation>
<bodyText confidence="0.9898668">
the ordered child sequences cn1 of n1 and cn2 of
~I2j point to the j-th child
n2, respectively, ~I1j an
in the corresponding sequence, and, again, l(·) re-
turns the sequence length, i.e. the number of chil-
dren.
Furthermore, we add two decay factors: µ for
the depth of the tree and λ for the length of the
child subsequences with respect to the original se-
quence, i.e. we account for gaps. It follows that
</bodyText>
<equation confidence="0.93134075">
A(n1, n2) =
( �
µ λ2+
~I1,~I2,l(~I1)=l(
</equation>
<bodyText confidence="0.989004142857143">
where d(~I1) = ~I1l(~I1) −
I21. This way, we penalize both larger trees and
child subsequences with gaps. Eq. 1 is more gen-
eral than the A equation for STK. Indeed, if we
only consider the contribution of the longest child
sequence from node pairs that have the same chil-
dren, we implement STK.
</bodyText>
<sectionHeader confidence="0.979904" genericHeader="method">
3 Generative Model: Stochastic
Conceptual Language Model (SCLM)
</sectionHeader>
<bodyText confidence="0.982459916666667">
The first step of our approach is to produce a list
of SLU hypotheses using a Stochastic Conceptual
Language Model. This is the same described in
(Raymond and Riccardi, 2007) with the only dif-
ference that we train the language model using the
SRILM toolkit (Stolcke, 2002) and we then con-
vert it into a Stochastic Finite State Transducer
(SFST). Such method allows us to use a wide
group of language models, backed-off or inter-
polated with many kind of smoothing techniques
(Chen and Goodman, 1998).
To exemplify our SCLM let us consider the
following input italian sentence taken from the
LUNA corpus along with its English translation:
Ho un problema col monitor.
(I have a problem with my screen).
A possible semantic annotation is:
null{ho} PROBLEM{un problema} HARD-
WARE{col monitor},
where PROBLEM and HARDWARE are two
domain concepts and null is the label used for
words not meaningful for the task. To associate
word sequences with concepts, we use begin
d
</bodyText>
<equation confidence="0.9274883125">
λd(~I1)+d( ~I2)
~I2)
A(cn1(
~I1j), cn2(
)~I2j)) ,
(1)
l( ~I1)
H
j=1
~I11 and d( ~I2) = ~I2l(~I2) −
1078
(B) and inside (I) markers after each word of a
sequence, e.g.:
null{ho} PROBLEM-B{un} PROBLEM-
I{problema} HARDWARE-B{col} HARD-
WARE-I{monitor}
</equation>
<bodyText confidence="0.959698">
This annotation is automatically performed
by a model based on a combination of three
transducers:
</bodyText>
<equation confidence="0.866698">
ASLU = AW o AW2C o ASLM,
</equation>
<bodyText confidence="0.999531857142857">
where AW is the transducer representation of the
input sentence, AW2C is the transducer mapping
words to concepts and ASLM is the Stochastic
Conceptual Language Model trained with SRILM
toolkit and converted in FST. The SCLM repre-
sents joint probability of word and concept se-
quences by using the joint probability:
</bodyText>
<equation confidence="0.997941666666667">
k
P(W, C) = P(wi, ci|hi),
i=1
</equation>
<bodyText confidence="0.959595">
where W = w1..wk, C = c1..ck and hi =
wi−1ci−1..w1c1.
</bodyText>
<sectionHeader confidence="0.998145" genericHeader="method">
4 Discriminative re-ranking
</sectionHeader>
<bodyText confidence="0.999971166666667">
Our discriminative re-ranking is based on SVMs
trained with pairs of conceptually annotated sen-
tences produced by the FST-based generative
model described in the previous section. An SVM
learn to classify which annotation has an error rate
lower than the others so that it can be used to sort
the m-best annotations based on their correctness.
While for SVMs details we remaind to the wide
literature available, for example (Vapnik, 1995) or
(Shawe-Taylor and Cristianini, 2004), in this sec-
tion we focus on hypotheses generation and on the
kernels used to implement our re-ranking model.
</bodyText>
<subsectionHeader confidence="0.999756">
4.1 Generation of m-best concept labeling
</subsectionHeader>
<bodyText confidence="0.999986882352941">
Using the FST-based model described in Section
3, we can generate the list of m best hypotheses
ranked by the joint probability of the Stochastic
Conceptual Language Model (SCLM). The Re-
ranking model proposed in this paper re-ranks
such list.
After an analysis of the m-best hypothesis list,
we noticed that many times the first hypothesis
ranked by SCLM is not the most accurate, i.e.
the error rate evaluated with its Levenshtein dis-
tance from the manual annotation is not the low-
est among the m hypotheses. This means that re-
ranking hypotheses could improve the SLU ac-
curacy. Intuitively, to achieve satisfactory re-
sults, different features from those used by SCLM
should be considered to exploit in a different way
the information encoded in the training data.
</bodyText>
<subsectionHeader confidence="0.997158">
4.2 Structural features for re-ranking
</subsectionHeader>
<bodyText confidence="0.99989304">
The kernels described in previous sections pro-
vide a powerful technology for exploiting features
of structured data. These kernels were originally
designed for data annotated with syntactic parse
trees. In Spoken Language Understanding the data
available are text sentences with their semantic
annotation based on basic semantic constituents.
This kind of data has a rather flat structure with
respect to syntactic parse trees. Thus, to exploit
the power of kernels, a careful design of the struc-
tures used to represent data must be carried out,
where the goal is to build tree-like annotation from
the semantic annotation. For this purpose, we
note that the latter is made upon sentence chunks,
which implicitly define syntactic structures as long
as the annotation is consistent in the corpus.
We took into account the characteristics of the
presented kernels and the structure of semantic an-
notated data. As a result we designed the tree
structures shown in figures 2(a), 2(b) and 3 for
STK and PTK and sequential structures for SK
defined in the following (where all the structures
refer to the same example presented in Section 3,
i.e. Ho un problema col monitor). The structures
used with SK are:
</bodyText>
<listItem confidence="0.9142866">
(SK1) NULL ho PROBLEM-B un
PROBLEM-I problema HARDWARE-B col
HARDWARE-I monitor
(SK2) NULL ho PROBLEM B un PROB-
LEM I problema HARDWARE B col HARD-
</listItem>
<sectionHeader confidence="0.767054" genericHeader="method">
WARE I monitor,
</sectionHeader>
<bodyText confidence="0.999612230769231">
For simplicity, from now on, the two structures
will be referred as SK1 and SK2 (String Kernel 1
and 2). They differer in the use of chunk mark-
ers B and I. In SK1, markers are part of the con-
cept, thus they increase the number of semantic
tags in the data whereas in SK2 markers are put
apart as separated words so that they can mark ef-
fectively the beginning and the end of a concept,
but for the same reason they can add noise in the
sentence. Notice that the order of words and con-
cepts is meaningful since each word is preceded
by its corresponding concepts.
The structures shown in Figure 2(a), 2(b) and 3
</bodyText>
<page confidence="0.990156">
1079
</page>
<bodyText confidence="0.999971617647059">
have been designed for STK and PTK. They pro-
vide trees with increasing structure complexity as
described in the following.
The first structure (FLAT) is a simple tree
providing direct dependency between words and
chunked concepts. From it, STK and PTK can ex-
tract relevant features (tree fragments).
The second structure (MULTILEVEL) has one
more level of nodes and yields the same separation
of concepts and markers shown in SK1. Notice
that the same separation can be carried out putting
the markers B and I as features at the same level of
the words. This would increase exponentially (in
the number of leaves) the number of subtrees taken
into account by the STK computation. Since STK
doesn’t separate children, as described in Section
2.3, the structure we chose is lighter but also more
rigid.
The third structure (FEATURES) is a more
complex structure. It allows to use a wide num-
ber of features (like Word categories, POS tags,
morpho-syntactic features), which are commonly
used in this kind of task. As described above, the
use of features exponentially increases the num-
ber of subtrees taken into account by kernel com-
putations but they also increase the robustness of
the model. In this work we only used Word Cate-
gories as features. They are domain independent,
e.g. ”Months”, ”Dates”, ”Number” etc. or POS
tags, which are useful to generalize target words.
Note also that the features in common between
two trees must appear in the same child-position,
hence we sort them based on their indices, e.g.’F0’
for words and ’F1’ for word categories.
</bodyText>
<subsectionHeader confidence="0.998992">
4.3 Re-ranking models using sequences
</subsectionHeader>
<bodyText confidence="0.989665766666667">
The FST generates the m most likely concept an-
notations. These are used to build annotation
pairs, �si,sj), which are positive instances if si
has a lower concept annotation error than sj, with
respect to the manual annotation. Thus, a trained
binary classifier can decide if si is more accurate
than sj. Each candidate annotation si is described
by a word sequence with its concept annotation.
Considering the example in the previous section, a
pair of annotations �si, sj) could be
si: NULL ho PROBLEM-B un PROBLEM-
I problema HARDWARE-B col HARDWARE-I
monitor
sj: NULL ho ACTION-B un ACTION-I prob-
lema HARDWARE-B col HARDWARE-B moni-
tor
where NULL, ACTION and HARDWARE are
the assigned concepts. The second annotation is
less accurate than the first since problema is erro-
neously annotated as ACTION and ”col monitor”
is split in two different concepts.
Given the above data, the sequence kernel
is used to evaluate the number of common n-
grams between si and sj. Since the string ker-
nel skips some elements of the target sequences,
the counted n-grams include: concept sequences,
word sequences and any subsequence of words
and concepts at any distance in the sentence.
Such counts are used in our re-ranking function
as follows: let ek be the pair N, s2 � we evaluate
</bodyText>
<equation confidence="0.982717">
k
the kernel:
KR(e1, e2) = SK(s11, s12) + SK(s21, s22) (2)
− SK(s11, s22) − SK(s21, s12)
</equation>
<bodyText confidence="0.999895260869565">
This schema, consisting in summing four different
kernels, has been already applied in (Collins and
Duffy, 2002; Shen et al., 2003) for syntactic pars-
ing re-ranking, where the basic kernel was a tree
kernel instead of SK. It was also used also in (Shen
et al., 2004) to re-rank different candidates of the
same hypothesis for machine translation. Notice
that our goal is different from the one tackled in
such paper and, in general, it is more difficult: we
try to learn which is the best annotation of a given
input sentence, while in (Shen et al., 2004), they
learn to distinguish between ”good” and ”bad”
translations of a sentence. Even if our goal is more
difficult, our approach is very effective, as shown
in (Dinarelli et al., 2009a). It is more appropriate
since in parse re-ranking there is only one best hy-
pothesis, while in machine translation a sentence
can have more than one correct translations.
Additionally, in (Moschitti et al., 2006; Mos-
chitti et al., 2008) a tree kernel was applied to se-
mantic trees similar to the one introduced in the
next section to re-rank Semantic Role Labeling an-
notations.
</bodyText>
<subsectionHeader confidence="0.996314">
4.4 Re-ranking models using trees
</subsectionHeader>
<bodyText confidence="0.996695428571429">
Since the aim of concept annotation re-ranking is
to exploit innovative and effective source of infor-
mation, we can use, in addition to sequence ker-
nels, the power of tree kernels to generate correla-
tion between concepts and word structures.
Figures 2(a), 2(b) and 3 describe the struc-
tural association between the concept and the word
</bodyText>
<page confidence="0.975089">
1080
</page>
<figure confidence="0.994134">
(a) FLAT Tree (b) MULTILEVEL Tree
</figure>
<figureCaption confidence="0.9999885">
Figure 2: Examples of structures used for STK and PTK
Figure 3: The FEATURES semantic tree used for STK or PTK
</figureCaption>
<table confidence="0.99643">
Corpus Train set Test set
LUNA
words concepts words concepts
Dialogs 183 67
Turns 1.019 373
Tokens 8.512 2.887 2.888 984
Vocab. 1.172 34 - -
OOV rate - - 3.2% 0.1%
</table>
<tableCaption confidence="0.99847">
Table 1: Statistics on the LUNA corpus
</tableCaption>
<table confidence="0.996083142857143">
Corpus Train set Test set
Media
words concepts words concepts
Turns 12,922 3,518
# of tokens 94,912 43,078 26,676 12,022
Vocabulary 5,307 80 - -
OOV rate - - 0.01% 0.0%
</table>
<tableCaption confidence="0.999728">
Table 2: Statistics on the MEDIA corpus
</tableCaption>
<bodyText confidence="0.99997625">
level. This kind of trees allows us to engineer new
kernels and consequently new features (Moschitti
et al., 2008), e.g. their subparts extracted by STK
or PTK, like the tree fragments in figures 1(a) and
1(b). These can be used in SVMs to learn the clas-
sification of words in concepts.
More specifically, in our approach, we use tree
fragments to establish the order of correctness
between two alternative annotations. Therefore,
given two trees associated with two annotations, a
re-ranker based on tree kernel can be built in the
same way of the sequence-based kernel by substi-
tuting SK in Eq. 2 with STK or PTK. The major
advantage of using trees is the hierarchical depen-
dencies between its nodes, allowing for the use of
richer n-grams with back-off models.
</bodyText>
<sectionHeader confidence="0.999793" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999768714285714">
In this section, we describe the corpora, parame-
ters, models and results of our experiments on re-
ranking for SLU. Our baseline is constituted by the
error rate of systems solely based on either FST
or SVMs. The re-ranking models are built on the
FST output, which in turn is applied to both man-
ual or automatic transcriptions.
</bodyText>
<subsectionHeader confidence="0.976129">
5.1 Corpora
</subsectionHeader>
<bodyText confidence="0.999970681818182">
We used two different speech corpora:
The LUNA corpus, produced in the homony-
mous European project, is the first Italian dataset
of spontaneous speech on spoken dialogs. It is
based on help-desk conversations in a domain
of software/hardware repairing (Dinarelli et al.,
2009b). The data is organized in transcriptions
and annotations of speech based on a new multi-
level protocol. Although data acquisition is still in
progress, 250 dialogs have been already acquired
with a WOZ approach and other 180 Human-
Human (HH) dialogs have been annotated. In this
work, we only use WOZ dialogs, whose statistics
are reported in Table 1.
The corpus MEDIA was collected within
the French project MEDIA-EVALDA (Bonneau-
Maynard et al., 2005) for development and evalu-
ation of spoken understanding models and linguis-
tic studies. The corpus is composed of 1257 di-
alogs (from 250 different speakers) acquired with
a Wizard of Oz (WOZ) approach in the context
of hotel room reservations and tourist information.
</bodyText>
<page confidence="0.990487">
1081
</page>
<bodyText confidence="0.990068">
Statistics on transcribed and conceptually anno-
tated data are reported in Table 2.
</bodyText>
<subsectionHeader confidence="0.997451">
5.2 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999978604651163">
Given the small size of LUNA corpus, we did not
carried out any parameterization thus we used de-
fault or a priori parameters. We experimented with
LUNA and three different re-rankers obtained with
the combination of SVMs with STK, PTK and SK,
described in Section 4. The initial annotation to be
re-ranked is the list of the ten best hypotheses out-
put by an FST model.
We point out that, on the large Media dataset the
processing time is considerably high2 so we could
not run all the models.
We trained all the SCLMs used in our experi-
ments with the SRILM toolkit (Stolcke, 2002) and
we used an interpolated model for probability es-
timation with the Kneser-Ney discount (Chen and
Goodman, 1998). We then converted the model in
an FST again with SRILM toolkit.
The model used to obtain the SVM baseline for
concept classification was trained using YamCHA
(Kudo and Matsumoto, 2001). As re-ranking
models based on structure kernels and SVMs,
we used the SVM-Light-TK toolkit (available at
disi.unitn.it/moschitti). For λ (see Section 3), cost-
factor and trade-off parameters, we used, 0.4, 1
and 1, respectively (i.e. the default parameters).
The number m of hypotheses was always set to 10.
The CRF model we compare with was
trained with the CRF++ tool, available at
http://crfpp.sourceforge.net/. The model is equiva-
lent to the one described in (Hahn et al., 2008). As
features, we used word and morpho-syntactic cat-
egories in a window of [-2, +2] with respect to the
current token, plus bigrams of concept tags (see
(Hahn et al., 2008) and the CRF++ web site for
more details).
Such model is very effective for SLU. In (Hahn
et al., 2008), it is compared with other four models
(Stochastic Finite State Transducers, Support Vec-
tor Machines, Machine Translation, Positional-
Based Log-linear model) and it is by far the best
on MEDIA. Additionally, in (Raymond and Ric-
cardi, 2007), a similar CRF model was compared
with FST and SVMs on ATIS and on a different
</bodyText>
<footnote confidence="0.8776785">
2The number of parameters of the models and the number
of training approaches make the exhaustive experimentation
very expensive in terms of processing time, which would be
roughly between 2 and 3 months of a typical workstation.
</footnote>
<table confidence="0.997665833333333">
Structure STK PTK SK
FLAT 18.5 19.3 -
MULTILEVEL 20.6 19.1 -
FEATURES 19.9 18.4 -
SK1 - - 16.2
SK2 - - 18.5
</table>
<tableCaption confidence="0.98701925">
Table 3: CER of SVMs using STK, PTK and SK
on LUNA (manual transcriptions). The Baselines,
FST and SVMs alone, show a CER of 23.2% and
26.3%, respectively.
</tableCaption>
<table confidence="0.998868">
Model MEDIA (CER) LUNA (CER)
FST 13.7% 23.2%
CRF 11.5% 20.4%
SVM-RR (PTK) 12.1% 18.4%
</table>
<tableCaption confidence="0.9570075">
Table 4: Results of SLU experiments on MEDIA
and LUNA test set (manual transcriptions).
</tableCaption>
<bodyText confidence="0.980188142857143">
version of MEDIA, showing again to be very ef-
fective.
We ran SLU experiments on manual and auto-
matic transcriptions. The latter are produced by
a speech recognizer with a WER of 41.0% and
31.4% on the LUNA and the MEDIA test sets, re-
spectively.
</bodyText>
<subsectionHeader confidence="0.999735">
5.3 Training approaches
</subsectionHeader>
<bodyText confidence="0.999815076923077">
The FST model generates the 10-best annotations,
i.e. the data used to train the re-ranker based on
SVMs. Different training approaches can be car-
ried out based on the use of the data. We divided
the training set in two parts. We train FSTs on
part 1 and generate the 10-best hypotheses using
part 2, thus providing the first chunk of re-ranking
data. Then, we re-apply these steps inverting part
1 with part 2 to provide the second data chunk.
Finally, we train the re-ranker on the merged data.
For classification, we generate the 10-best hy-
potheses of the whole test set using the FST
trained on all training data.
</bodyText>
<subsectionHeader confidence="0.998564">
5.4 Re-ranking results
</subsectionHeader>
<bodyText confidence="0.9997837">
In Tables 3, 4 and 5 and Figures 4(a) and 4(b) we
report the results of our experiments, expressed in
terms of concept error rate (CER). CER is a stan-
dard measure based on the Levensthein alignment
of sentences and it is computed as the ratio be-
tween inserted, deleted and confused concepts and
the number of concepts in the reference sentence.
Table 3 shows the results on the LUNA cor-
pus using the different training approaches, ker-
nels and structures described in this paper. The
</bodyText>
<page confidence="0.994873">
1082
</page>
<figure confidence="0.996402571428571">
CER
CER
100 500 1000 2000 3000 4000 5000 6000
Training Sentences
(a) Learning Curve on MEDIA corpus using the RR model
based on SVMs and STK
100 300 500 700 1000
Training Sentences
(b) Learning Curve on LUNA corpus using the RR model
based on SVMs and SK
60
55
50
45
40
35
30
25
20
15
FST
CRF
RR
55
50
45
40
35
30
25
20
15
FST
CRF
RR
</figure>
<figureCaption confidence="0.999735">
Figure 4: Learning curves on MEDIA and LUNA corpora using FST, CRF and RR on the FST hypotheses
</figureCaption>
<table confidence="0.99741225">
Model MEDIA (CER) LUNA (CER)
FST 28.6% 42.7%
CRF 24.0% 41.8%
SVM-RR (PTK) 25.0% 38.9%
</table>
<tableCaption confidence="0.790757333333333">
Table 5: Results of SLU experiments on MEDIA
and LUNA test set (automatic transcriptions with
a WER 31.4% on MEDIA and 41% on LUNA)
</tableCaption>
<bodyText confidence="0.999913943396227">
dash symbol means that the structure cannot be
applied to the corresponding kernel. We note that
our re-rankers significantly improve our baselines,
i.e. 23.2% CER for FST and 26.3% CER for SVM
concept classifiers. For example, SVM re-ranker
using SK, in the best case, improves FST concept
classifier of 23.2-16.2 = 7 points.
Note also that the structures designed for trees
yield quite different results depending on which
kernel is used. We can see in Table 3 that the
best result using STK is obtained with the simplest
structure (FLAT), while with PTK the best result
is achieved with the most complex structure (FEA-
TURES). This is due to the fact that STK does
not split the children of each node, as explained in
Section 2.2, and so structures like MULTILEVEL
and FEATURES are too rigid and prevent the STK
to be effective.
For lack of space we do not report all the results
using different kernels and structures on MEDIA,
but we underline that as MEDIA is a more com-
plex task (34 concepts in LUNA, 80 in MEDIA),
the more complex structures are more effective to
capture word-concept dependencies and the best
results were obtained using the FEATURES tree.
Table 4 shows the results of the SLU exper-
iments on the MEDIA and LUNA test sets us-
ing the manual transcriptions of spoken sentences
and a re-ranker based on PTK and the FEATURES
structure (already reported in the previous table).
We used PTK since it is enough efficient to carry
out the computation on the much larger Media cor-
pus although as previously shown it is less accu-
rate than SK.
We note that on a big corpus like MEDIA, the
baseline models (FST and CRF) can be accurately
learned thus less errors can be ”corrected”. As
a consequence, our re-ranking approach does not
improve CRF but it still improves the FSTs base-
line of 1.6% points (11.7% of relative improve-
ment).
The same behavior is reproduced for the SLU
experiments on automatic transcriptions, shown in
Table 5. We note that, on the LUNA corpus, CRFs
are more accurate than FSTs (0.9% points), but
they are significantly improved by the re-ranking
model (2.9% points), which also improves the
FSTs baseline by 3.8% points. On the MEDIA
corpus, the re-ranking model is again very accu-
rate improving the FSTs baseline of 3.6% points
(12.6% relative improvement) on attribute anno-
tation, but the most accurate model is again CRF
(1% points better than the re-ranking model).
</bodyText>
<subsectionHeader confidence="0.796169">
5.5 Discussion
</subsectionHeader>
<bodyText confidence="0.999962111111111">
The different behavior of the re-ranking model in
the LUNA and MEDIA corpora is due partially to
the task complexity, but it is mainly due to the fact
that CRFs have been deeply studied and experi-
mented (see (Hahn et al., 2008)) on MEDIA. Thus
CRF parameters and features have been largely
optimized. We believe that the re-ranking model
can be relevantly improved by carrying out param-
eter optimization and new structural feature de-
</bodyText>
<page confidence="0.976078">
1083
</page>
<bodyText confidence="0.940502685714286">
sign.
Moreover, our re-ranking models achieve the
highest accuracy for automatic concept annota-
tion when small data sets are available. To show
this, we report in Figure 4(a) and 4(b) the learning
curves according to an increasing number of train-
ing sentences on the MEDIA and LUNA corpora,
respectively. To draw the first plot, we used a re-
ranker based on STK (and the FLAT tree), which
is less accurate than the other kernels but also the
most efficient in terms of training time. In the sec-
ond plot, we report the re-ranker accuracy using
SK applied to SK1 structure.
In these figures, the FST baseline performance
is compared with our re-ranking (RR) and a Con-
ditional Random Field (CRF) model. The above
curves clearly shows that for small datasets our
RR model is better than CRF whereas when the
data increases, CRF accuracy approaches the one
of the RR.
Regarding the use of kernels two main findings
can be derived:
• Kernels producing a high number of features,
e.g. SK, produce accuracy higher than ker-
nels less rich in terms of features, i.e. STK. In
particular STK is improved by 18.5-16.2=2.3
points (Table 3). This is an interesting re-
sult since it shows that (a) a kernel producing
more features also produces better re-ranking
models and (b) kernel methods give a remark-
able help in feature design.
• Although the training data is small, the re-
rankers based on kernels appear to be very
effective. This may also alleviate the burden
of annotating large amount of data.
</bodyText>
<sectionHeader confidence="0.999491" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999917333333333">
In this paper, we propose discriminative re-
ranking of concept annotation to jointly exploit
generative and discriminative models. We im-
prove the FST-based generative approach, which
is a state-of-the-art model in LUNA, by 7 points,
where the more limited availability of annotated
data leaves a larger room for improvement. Our
re-ranking model also improves FST and CRF on
MEDIA when small data sets are used.
Kernel methods show that combinations of fea-
ture vectors, sequence kernels and other structural
kernels, e.g. on shallow or deep syntactic parse
trees, appear to be a promising future research
line3. Finally, the experimentation with automatic
speech transcriptions revealed that to test the ro-
bustness of our models to transcription errors.
In the future we would like to extend this re-
search by focusing on advanced shallow semantic
approaches such as predicate argument structures,
e.g. (Giuglea and Moschitti, 2004; Moschitti and
Cosmin, 2004; Moschitti et al., 2008). Addition-
ally, term similarity kernels, e.g. (Basili et al.,
2005; Bloehdorn et al., 2006), will be likely im-
prove our models, especially when combined syn-
tactic and semantic kernels are used, i.e. (Bloe-
hdorn and Moschitti, 2007a; Bloehdorn and Mos-
chitti, 2007b).
</bodyText>
<sectionHeader confidence="0.999403" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.948732694444444">
Roberto Basili, Alessandro Moschitti, and
Maria Teresa Pazienza. 1999. A text classifier
based on linguistic processing. In Proceedings
of IJCAI 99, Machine Learning for Information
Filtering.
Roberto Basili, Marco Cammisa, and Alessandro Mos-
chitti. 2005. Effective use of WordNet seman-
tics via kernel-based learning. In Proceedings of
CoNLL-2005, Ann Arbor, Michigan.
Stephan Bloehdorn and Alessandro Moschitti. 2007a.
Combined syntactic and semantic kernels for text
classification. In Proceedings of ECIR 2007, Rome,
Italy.
Stephan Bloehdorn and Alessandro Moschitti. 2007b.
Structure and semantics for expressive text kernels.
In In proceedings of CIKM ’07.
Stephan Bloehdorn, Roberto Basili, Marco Cammisa,
and Alessandro Moschitti. 2006. Semantic kernels
for text classification based on topological measures
of feature similarity. In Proceedings of ICDM 06,
Hong Kong, 2006.
H. Bonneau-Maynard, S. Rosset, C. Ayache, A. Kuhn,
and D. Mostefa. 2005. Semantic annotation of the
french media dialog corpus. In Proceedings of In-
terspeech2005, Lisbon, Portugal.
N. Cancedda, E. Gaussier, C. Goutte, and J. M. Ren-
ders. 2003. Word sequence kernels. J. Mach.
Learn. Res., 3.
S. F. Chen and J. Goodman. 1998. An empirical study
of smoothing techniques for language modeling. In
Technical Report of Computer Science Group, Har-
vard, USA.
3A basic approach is the use of part-of-speech tags like for
example in text categorization (Basili et al., 1999) but given
the high efficiency of modern syntactic parsers we can use the
complete parse tree.
</reference>
<page confidence="0.946925">
1084
</page>
<reference confidence="0.999947958333334">
M. Collins and N. Duffy. 2002. New Ranking Al-
gorithms for Parsing and Tagging: Kernels over
Discrete structures, and the voted perceptron. In
ACL02, pages 263–270.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
Tree Kernels for Relation Extraction. In Proceed-
ings of ACL’04.
Chad Cumby and Dan Roth. 2003. Kernel Methods for
Relational Learning. In Proceedings of ICML 2003.
Marco Dinarelli, Alessandro Moschitti, and Giuseppe
Riccardi. 2009a. Re-ranking models for spoken lan-
guage understanding. In Proceedings of EACL2009,
Athens, Greece.
Marco Dinarelli, Silvia Quarteroni, Sara Tonelli,
Alessandro Moschitti, and Giuseppe Riccardi.
2009b. Annotating spoken dialogs: from speech
segments to dialog acts and frame semantics. In
Proceedings of SRSL 2009 Workshop of EACL,
Athens, Greece.
Ana-Maria Giuglea and Alessandro Moschitti. 2004.
Knowledge Discovery using Framenet, Verbnet and
Propbank. In A. Meyers, editor, Workshop on On-
tology and Knowledge Discovering at ECML 2004,
Pisa, Italy.
Stefan Hahn, Patrick Lehnen, Christian Raymond, and
Hermann Ney. 2008. A comparison of various
methods for concept tagging for spoken language
understanding. In Proceedings of LREC, Mar-
rakech, Morocco.
T. Kudo and Y. Matsumoto. 2001. Chunking
with support vector machines. In Proceedings of
NAACL2001, Pittsburg, USA.
Taku Kudo and Yuji Matsumoto. 2003. Fast meth-
ods for kernel-based text analysis. In Proceedings
of ACL’03.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based parse reranking with subtree fea-
tures. In Proceedings of ACL’05.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML2001, US.
Huma Lodhi, John S. Taylor, Nello Cristianini, and
Christopher J. C. H. Watkins. 2000. Text classifi-
cation using string kernels. In NIPS.
Alessandro Moschitti and Adrian Bejan Cosmin. 2004.
A semantic kernel for predicate argument classifica-
tion. In CoNLL-2004, Boston, MA, USA.
Alessandro Moschitti and Silvia Quarteroni. 2008.
Kernels on linguistic structures for answer extrac-
tion. In Proceedings ofACL-08: HLT, Short Papers,
Columbus, Ohio.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2006. Semantic role labeling via tree kernel
joint inference. In Proceedings of CoNLL-X, New
York City.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploit-
ing syntactic and shallow semantic kernels for
question/answer classification. In Proceedings of
ACL’07, Prague, Czech Republic.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role label-
ing. Computational Linguistics, 34(2):193–224.
Alessandro Moschitti. 2006a. Efficient Convolution
Kernels for Dependency and Constituent Syntactic
Trees. In Proceedings of ECML 2006, pages 318–
329, Berlin, Germany.
Alessandro Moschitti. 2006b. Making Tree Kernels
Practical for Natural Language Learning. In Pro-
ceedings of EACL2006.
Alessandro Moschitti. 2008. Kernel methods, syntax
and semantics for relational text categorization. In
Proceeding of CIKM ’08, NY, USA.
C. Raymond and G. Riccardi. 2007. Generative and
discriminative algorithms for spoken language un-
derstanding. In Proceedings of Interspeech2007,
Antwerp,Belgium.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge Univer-
sity Press.
Libin Shen, Anoop Sarkar, and Aravind k. Joshi. 2003.
Using LTAG Based Features in Parse Reranking. In
Proceedings of EMNLP’06.
Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004.
Discriminative reranking for machine translation. In
HLT-NAACL, pages 177–184.
A. Stolcke. 2002. Srilm: an extensible language mod-
eling toolkit. In Proceedings of SLP2002, Denver,
USA.
Kristina Toutanova, Penka Markova, and Christopher
Manning. 2004. The Leaf Path Projection View
of Parse Trees: Exploring String Kernels for HPSG
Parse Selection. In Proceedings of EMNLP 2004.
V. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer.
</reference>
<page confidence="0.995232">
1085
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.506406">
<title confidence="0.998443">Re-Ranking Models Based-on Small Training Data for Spoken Language Understanding</title>
<author confidence="0.991229">Marco</author>
<affiliation confidence="0.999699">University of</affiliation>
<address confidence="0.968002">Italy</address>
<email confidence="0.997591">dinarelli@disi.unitn.it</email>
<author confidence="0.95361">Alessandro</author>
<affiliation confidence="0.999788">University of</affiliation>
<address confidence="0.761316">Italy</address>
<email confidence="0.998176">moschitti@disi.unitn.it</email>
<author confidence="0.791477">Giuseppe</author>
<affiliation confidence="0.99958">University of</affiliation>
<address confidence="0.942064">Italy</address>
<email confidence="0.998746">riccardi@disi.unitn.it</email>
<abstract confidence="0.999226708333333">The design of practical language applications by means of statistical approaches requires annotated data, which is one of the most critical constraint. This is particularly true for Spoken Dialog Systems since considerably domain-specific conceptual annotation is needed to obtain accurate Language Understanding models. Since data annotation is usually costly, methods to reduce the amount of data are needed. In this paper, we show that better feature representations serve the above purpose and that structure kernels provide the needed improved representation. Given the relatively high computational cost of kernel methods, we apply them to just re-rank the list of hypotheses provided by a fast generative model. Experiments with Support Vector Machines and different kernels on two different dialog corpora show that our re-ranking models can achieve better results than state-of-the-art approaches when small data is available.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Roberto Basili</author>
<author>Alessandro Moschitti</author>
<author>Maria Teresa Pazienza</author>
</authors>
<title>A text classifier based on linguistic processing.</title>
<date>1999</date>
<booktitle>In Proceedings of IJCAI 99, Machine Learning for Information Filtering.</booktitle>
<marker>Basili, Moschitti, Pazienza, 1999</marker>
<rawString>Roberto Basili, Alessandro Moschitti, and Maria Teresa Pazienza. 1999. A text classifier based on linguistic processing. In Proceedings of IJCAI 99, Machine Learning for Information Filtering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Basili</author>
<author>Marco Cammisa</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Effective use of WordNet semantics via kernel-based learning.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL-2005,</booktitle>
<location>Ann Arbor, Michigan.</location>
<marker>Basili, Cammisa, Moschitti, 2005</marker>
<rawString>Roberto Basili, Marco Cammisa, and Alessandro Moschitti. 2005. Effective use of WordNet semantics via kernel-based learning. In Proceedings of CoNLL-2005, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Bloehdorn</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Combined syntactic and semantic kernels for text classification.</title>
<date>2007</date>
<booktitle>In Proceedings of ECIR 2007,</booktitle>
<location>Rome, Italy.</location>
<marker>Bloehdorn, Moschitti, 2007</marker>
<rawString>Stephan Bloehdorn and Alessandro Moschitti. 2007a. Combined syntactic and semantic kernels for text classification. In Proceedings of ECIR 2007, Rome, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Bloehdorn</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Structure and semantics for expressive text kernels.</title>
<date>2007</date>
<booktitle>In In proceedings of CIKM ’07.</booktitle>
<marker>Bloehdorn, Moschitti, 2007</marker>
<rawString>Stephan Bloehdorn and Alessandro Moschitti. 2007b. Structure and semantics for expressive text kernels. In In proceedings of CIKM ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Bloehdorn</author>
<author>Roberto Basili</author>
<author>Marco Cammisa</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Semantic kernels for text classification based on topological measures of feature similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of ICDM 06,</booktitle>
<location>Hong Kong,</location>
<marker>Bloehdorn, Basili, Cammisa, Moschitti, 2006</marker>
<rawString>Stephan Bloehdorn, Roberto Basili, Marco Cammisa, and Alessandro Moschitti. 2006. Semantic kernels for text classification based on topological measures of feature similarity. In Proceedings of ICDM 06, Hong Kong, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Bonneau-Maynard</author>
<author>S Rosset</author>
<author>C Ayache</author>
<author>A Kuhn</author>
<author>D Mostefa</author>
</authors>
<title>Semantic annotation of the french media dialog corpus.</title>
<date>2005</date>
<booktitle>In Proceedings of Interspeech2005,</booktitle>
<location>Lisbon, Portugal.</location>
<marker>Bonneau-Maynard, Rosset, Ayache, Kuhn, Mostefa, 2005</marker>
<rawString>H. Bonneau-Maynard, S. Rosset, C. Ayache, A. Kuhn, and D. Mostefa. 2005. Semantic annotation of the french media dialog corpus. In Proceedings of Interspeech2005, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Cancedda</author>
<author>E Gaussier</author>
<author>C Goutte</author>
<author>J M Renders</author>
</authors>
<title>Word sequence kernels.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<volume>3</volume>
<contexts>
<context position="5915" citStr="Cancedda et al., 2003" startWordPosition="904" endWordPosition="907">all data sets are used. The remainder of the paper is organized as follows: Section 2 introduces kernel methods for structured data, Section 3 describes the generative model producing the initial hypotheses whereas Section 4 presents the discriminative models for re-ranking them. The experiments and results are reported in Section 5 and the conclusions are drawn in Section 6. 2 Feature Engineering via Structure Kernels Kernel methods are viable approaches to engineer features for text processing, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby 1Contract n. 33549 and Roth, 2003; Cancedda et al., 2003; Culotta and Sorensen, 2004; Toutanova et al., 2004; Kudo et al., 2005; Moschitti, 2006a; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008; Moschitti and Quarteroni, 2008). In the following, we describe structure kernels, which will be used to engineer features for our discriminative reranker. 2.1 String Kernels The String Kernels that we consider count the number of substrings containing gaps shared by two sequences, i.e. some of the symbols of the original string are skipped. We adopted the efficient algorithm described in (Shawe-Taylor and Cristianini, 2004; Lodhi et al., 20</context>
</contexts>
<marker>Cancedda, Gaussier, Goutte, Renders, 2003</marker>
<rawString>N. Cancedda, E. Gaussier, C. Goutte, and J. M. Renders. 2003. Word sequence kernels. J. Mach. Learn. Res., 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling. In</title>
<date>1998</date>
<tech>Technical Report of</tech>
<institution>Computer Science Group,</institution>
<location>Harvard, USA.</location>
<contexts>
<context position="10812" citStr="Chen and Goodman, 1998" startWordPosition="1767" endWordPosition="1770">om node pairs that have the same children, we implement STK. 3 Generative Model: Stochastic Conceptual Language Model (SCLM) The first step of our approach is to produce a list of SLU hypotheses using a Stochastic Conceptual Language Model. This is the same described in (Raymond and Riccardi, 2007) with the only difference that we train the language model using the SRILM toolkit (Stolcke, 2002) and we then convert it into a Stochastic Finite State Transducer (SFST). Such method allows us to use a wide group of language models, backed-off or interpolated with many kind of smoothing techniques (Chen and Goodman, 1998). To exemplify our SCLM let us consider the following input italian sentence taken from the LUNA corpus along with its English translation: Ho un problema col monitor. (I have a problem with my screen). A possible semantic annotation is: null{ho} PROBLEM{un problema} HARDWARE{col monitor}, where PROBLEM and HARDWARE are two domain concepts and null is the label used for words not meaningful for the task. To associate word sequences with concepts, we use begin d λd(~I1)+d( ~I2) ~I2) A(cn1( ~I1j), cn2( )~I2j)) , (1) l( ~I1) H j=1 ~I11 and d( ~I2) = ~I2l(~I2) − 1078 (B) and inside (I) markers aft</context>
<context position="23415" citStr="Chen and Goodman, 1998" startWordPosition="3904" endWordPosition="3907">rization thus we used default or a priori parameters. We experimented with LUNA and three different re-rankers obtained with the combination of SVMs with STK, PTK and SK, described in Section 4. The initial annotation to be re-ranked is the list of the ten best hypotheses output by an FST model. We point out that, on the large Media dataset the processing time is considerably high2 so we could not run all the models. We trained all the SCLMs used in our experiments with the SRILM toolkit (Stolcke, 2002) and we used an interpolated model for probability estimation with the Kneser-Ney discount (Chen and Goodman, 1998). We then converted the model in an FST again with SRILM toolkit. The model used to obtain the SVM baseline for concept classification was trained using YamCHA (Kudo and Matsumoto, 2001). As re-ranking models based on structure kernels and SVMs, we used the SVM-Light-TK toolkit (available at disi.unitn.it/moschitti). For λ (see Section 3), costfactor and trade-off parameters, we used, 0.4, 1 and 1, respectively (i.e. the default parameters). The number m of hypotheses was always set to 10. The CRF model we compare with was trained with the CRF++ tool, available at http://crfpp.sourceforge.net/</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>S. F. Chen and J. Goodman. 1998. An empirical study of smoothing techniques for language modeling. In Technical Report of Computer Science Group, Harvard, USA.</rawString>
</citation>
<citation valid="true">
<title>3A basic approach is the use of part-of-speech tags like for example in text categorization (Basili et al.,</title>
<date>1999</date>
<marker>1999</marker>
<rawString>3A basic approach is the use of part-of-speech tags like for example in text categorization (Basili et al., 1999) but given the high efficiency of modern syntactic parsers we can use the complete parse tree.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Duffy</author>
</authors>
<title>New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In ACL02,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="4271" citStr="Collins and Duffy, 2002" startWordPosition="647" endWordPosition="650">U hypotheses, which are re-ranked by SVMs. To effectively design our re-ranker, we use all pos1076 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1076–1085, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP sible word/concept subsequences with gaps of the spoken sentence as features (i.e. all possible ngrams). Gaps allow for encoding long distance dependencies between words in relatively small sequences. Since the space of such features is huge, we adopted kernel methods, i.e. sequence kernels (Shawe-Taylor and Cristianini, 2004) and tree kernels (Collins and Duffy, 2002; Moschitti, 2006a) to implicitly encode them along with other structural information in SVMs. We experimented with different approaches for training the discriminative models and two different corpora: the french MEDIA corpus (BonneauMaynard et al., 2005) and a corpus made available by the European project LUNA1 (Dinarelli et al., 2009b). In particular, the new contents with respect to our previous work (Dinarelli et al., 2009a) are: • We designed a new sequential structure (SK2) and two new hierarchical tree structures (MULTILEVEL and FEATURES) for re-ranking models (see Section 4.2). The la</context>
<context position="5825" citStr="Collins and Duffy, 2002" startWordPosition="889" endWordPosition="892">the-art in SLU. Learning curves clearly show that our models improve CRF, especially when small data sets are used. The remainder of the paper is organized as follows: Section 2 introduces kernel methods for structured data, Section 3 describes the generative model producing the initial hypotheses whereas Section 4 presents the discriminative models for re-ranking them. The experiments and results are reported in Section 5 and the conclusions are drawn in Section 6. 2 Feature Engineering via Structure Kernels Kernel methods are viable approaches to engineer features for text processing, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby 1Contract n. 33549 and Roth, 2003; Cancedda et al., 2003; Culotta and Sorensen, 2004; Toutanova et al., 2004; Kudo et al., 2005; Moschitti, 2006a; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008; Moschitti and Quarteroni, 2008). In the following, we describe structure kernels, which will be used to engineer features for our discriminative reranker. 2.1 String Kernels The String Kernels that we consider count the number of substrings containing gaps shared by two sequences, i.e. some of the symbols of the original string are skipped. We adopted </context>
<context position="8990" citStr="Collins and Duffy, 2002" startWordPosition="1444" endWordPosition="1447">s between two T1 and T2, we need to define a set F = {f1, f2, ... , f|F|}, i.e. a tree fragment space and an indicator function Ii(n), equal to 1 if the target fi is rooted at node n and equal to 0 otherwise. A tree-kernel function over T1 and T2 is TK(T1, T2) = � �n2∈NT2 A(n1, n2), n1∈NT1 where NT1 and NT2 are the sets of the T1’s and T2’s nodes, respectively and A(n1, n2) = �|F| i=1Ii(n1)Ii(n2). The latter is equal to the number of common fragments rooted in the n1 and n2 nodes. The algorithm for the efficient evaluation of A for the syntactic tree kernel (STK) has been widely discussed in (Collins and Duffy, 2002) whereas its fast evaluation is proposed in (Moschitti, 2006b), so we only describe the equations of the partial tree kernel (PTK). 2.4 The Partial Tree Kernel (PTK) PTFs have been defined in (Moschitti, 2006a). Their computation is carried out by the following A function: 1. if the node labels of n1 and n2 are different then A(n1, n2) = 0; 2. else A(n1, n2) = 1 + F_I1,~I2,l(~I1)=l(~I2) 11l(~I1) j=1 A(cn1 (~I1j), cn2(~I2j)) where ~I1 = (h1, h2, h3, ..) and ~I2 = (k1, k2, k3, ..) are index sequences associated with the ordered child sequences cn1 of n1 and cn2 of ~I2j point to the j-th child n2</context>
<context position="18558" citStr="Collins and Duffy, 2002" startWordPosition="3070" endWordPosition="3073">ncepts. Given the above data, the sequence kernel is used to evaluate the number of common ngrams between si and sj. Since the string kernel skips some elements of the target sequences, the counted n-grams include: concept sequences, word sequences and any subsequence of words and concepts at any distance in the sentence. Such counts are used in our re-ranking function as follows: let ek be the pair N, s2 � we evaluate k the kernel: KR(e1, e2) = SK(s11, s12) + SK(s21, s22) (2) − SK(s11, s22) − SK(s21, s12) This schema, consisting in summing four different kernels, has been already applied in (Collins and Duffy, 2002; Shen et al., 2003) for syntactic parsing re-ranking, where the basic kernel was a tree kernel instead of SK. It was also used also in (Shen et al., 2004) to re-rank different candidates of the same hypothesis for machine translation. Notice that our goal is different from the one tackled in such paper and, in general, it is more difficult: we try to learn which is the best annotation of a given input sentence, while in (Shen et al., 2004), they learn to distinguish between ”good” and ”bad” translations of a sentence. Even if our goal is more difficult, our approach is very effective, as show</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>M. Collins and N. Duffy. 2002. New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete structures, and the voted perceptron. In ACL02, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Dependency Tree Kernels for Relation Extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL’04.</booktitle>
<contexts>
<context position="5943" citStr="Culotta and Sorensen, 2004" startWordPosition="908" endWordPosition="911"> The remainder of the paper is organized as follows: Section 2 introduces kernel methods for structured data, Section 3 describes the generative model producing the initial hypotheses whereas Section 4 presents the discriminative models for re-ranking them. The experiments and results are reported in Section 5 and the conclusions are drawn in Section 6. 2 Feature Engineering via Structure Kernels Kernel methods are viable approaches to engineer features for text processing, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby 1Contract n. 33549 and Roth, 2003; Cancedda et al., 2003; Culotta and Sorensen, 2004; Toutanova et al., 2004; Kudo et al., 2005; Moschitti, 2006a; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008; Moschitti and Quarteroni, 2008). In the following, we describe structure kernels, which will be used to engineer features for our discriminative reranker. 2.1 String Kernels The String Kernels that we consider count the number of substrings containing gaps shared by two sequences, i.e. some of the symbols of the original string are skipped. We adopted the efficient algorithm described in (Shawe-Taylor and Cristianini, 2004; Lodhi et al., 2000). More specifically, we u</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffrey Sorensen. 2004. Dependency Tree Kernels for Relation Extraction. In Proceedings of ACL’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chad Cumby</author>
<author>Dan Roth</author>
</authors>
<title>Kernel Methods for Relational Learning.</title>
<date>2003</date>
<booktitle>In Proceedings of ICML</booktitle>
<marker>Cumby, Roth, 2003</marker>
<rawString>Chad Cumby and Dan Roth. 2003. Kernel Methods for Relational Learning. In Proceedings of ICML 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Dinarelli</author>
<author>Alessandro Moschitti</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Re-ranking models for spoken language understanding.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL2009,</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="4609" citStr="Dinarelli et al., 2009" startWordPosition="699" endWordPosition="702"> possible ngrams). Gaps allow for encoding long distance dependencies between words in relatively small sequences. Since the space of such features is huge, we adopted kernel methods, i.e. sequence kernels (Shawe-Taylor and Cristianini, 2004) and tree kernels (Collins and Duffy, 2002; Moschitti, 2006a) to implicitly encode them along with other structural information in SVMs. We experimented with different approaches for training the discriminative models and two different corpora: the french MEDIA corpus (BonneauMaynard et al., 2005) and a corpus made available by the European project LUNA1 (Dinarelli et al., 2009b). In particular, the new contents with respect to our previous work (Dinarelli et al., 2009a) are: • We designed a new sequential structure (SK2) and two new hierarchical tree structures (MULTILEVEL and FEATURES) for re-ranking models (see Section 4.2). The latter combined with two different tree kernels originate four new different models. • We experimented with automatic speech transcriptions thus assessing the robustness to noise of our models. • We compare our models against Conditional Random Field (CRF) approaches described in (Hahn et al., 2008), which are the current state-of-the-art</context>
<context position="19186" citStr="Dinarelli et al., 2009" startWordPosition="3182" endWordPosition="3185"> et al., 2003) for syntactic parsing re-ranking, where the basic kernel was a tree kernel instead of SK. It was also used also in (Shen et al., 2004) to re-rank different candidates of the same hypothesis for machine translation. Notice that our goal is different from the one tackled in such paper and, in general, it is more difficult: we try to learn which is the best annotation of a given input sentence, while in (Shen et al., 2004), they learn to distinguish between ”good” and ”bad” translations of a sentence. Even if our goal is more difficult, our approach is very effective, as shown in (Dinarelli et al., 2009a). It is more appropriate since in parse re-ranking there is only one best hypothesis, while in machine translation a sentence can have more than one correct translations. Additionally, in (Moschitti et al., 2006; Moschitti et al., 2008) a tree kernel was applied to semantic trees similar to the one introduced in the next section to re-rank Semantic Role Labeling annotations. 4.4 Re-ranking models using trees Since the aim of concept annotation re-ranking is to exploit innovative and effective source of information, we can use, in addition to sequence kernels, the power of tree kernels to gen</context>
<context position="21890" citStr="Dinarelli et al., 2009" startWordPosition="3647" endWordPosition="3650">riments In this section, we describe the corpora, parameters, models and results of our experiments on reranking for SLU. Our baseline is constituted by the error rate of systems solely based on either FST or SVMs. The re-ranking models are built on the FST output, which in turn is applied to both manual or automatic transcriptions. 5.1 Corpora We used two different speech corpora: The LUNA corpus, produced in the homonymous European project, is the first Italian dataset of spontaneous speech on spoken dialogs. It is based on help-desk conversations in a domain of software/hardware repairing (Dinarelli et al., 2009b). The data is organized in transcriptions and annotations of speech based on a new multilevel protocol. Although data acquisition is still in progress, 250 dialogs have been already acquired with a WOZ approach and other 180 HumanHuman (HH) dialogs have been annotated. In this work, we only use WOZ dialogs, whose statistics are reported in Table 1. The corpus MEDIA was collected within the French project MEDIA-EVALDA (BonneauMaynard et al., 2005) for development and evaluation of spoken understanding models and linguistic studies. The corpus is composed of 1257 dialogs (from 250 different sp</context>
</contexts>
<marker>Dinarelli, Moschitti, Riccardi, 2009</marker>
<rawString>Marco Dinarelli, Alessandro Moschitti, and Giuseppe Riccardi. 2009a. Re-ranking models for spoken language understanding. In Proceedings of EACL2009, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Dinarelli</author>
<author>Silvia Quarteroni</author>
<author>Sara Tonelli</author>
<author>Alessandro Moschitti</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Annotating spoken dialogs: from speech segments to dialog acts and frame semantics.</title>
<date>2009</date>
<booktitle>In Proceedings of SRSL 2009 Workshop of EACL,</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="4609" citStr="Dinarelli et al., 2009" startWordPosition="699" endWordPosition="702"> possible ngrams). Gaps allow for encoding long distance dependencies between words in relatively small sequences. Since the space of such features is huge, we adopted kernel methods, i.e. sequence kernels (Shawe-Taylor and Cristianini, 2004) and tree kernels (Collins and Duffy, 2002; Moschitti, 2006a) to implicitly encode them along with other structural information in SVMs. We experimented with different approaches for training the discriminative models and two different corpora: the french MEDIA corpus (BonneauMaynard et al., 2005) and a corpus made available by the European project LUNA1 (Dinarelli et al., 2009b). In particular, the new contents with respect to our previous work (Dinarelli et al., 2009a) are: • We designed a new sequential structure (SK2) and two new hierarchical tree structures (MULTILEVEL and FEATURES) for re-ranking models (see Section 4.2). The latter combined with two different tree kernels originate four new different models. • We experimented with automatic speech transcriptions thus assessing the robustness to noise of our models. • We compare our models against Conditional Random Field (CRF) approaches described in (Hahn et al., 2008), which are the current state-of-the-art</context>
<context position="19186" citStr="Dinarelli et al., 2009" startWordPosition="3182" endWordPosition="3185"> et al., 2003) for syntactic parsing re-ranking, where the basic kernel was a tree kernel instead of SK. It was also used also in (Shen et al., 2004) to re-rank different candidates of the same hypothesis for machine translation. Notice that our goal is different from the one tackled in such paper and, in general, it is more difficult: we try to learn which is the best annotation of a given input sentence, while in (Shen et al., 2004), they learn to distinguish between ”good” and ”bad” translations of a sentence. Even if our goal is more difficult, our approach is very effective, as shown in (Dinarelli et al., 2009a). It is more appropriate since in parse re-ranking there is only one best hypothesis, while in machine translation a sentence can have more than one correct translations. Additionally, in (Moschitti et al., 2006; Moschitti et al., 2008) a tree kernel was applied to semantic trees similar to the one introduced in the next section to re-rank Semantic Role Labeling annotations. 4.4 Re-ranking models using trees Since the aim of concept annotation re-ranking is to exploit innovative and effective source of information, we can use, in addition to sequence kernels, the power of tree kernels to gen</context>
<context position="21890" citStr="Dinarelli et al., 2009" startWordPosition="3647" endWordPosition="3650">riments In this section, we describe the corpora, parameters, models and results of our experiments on reranking for SLU. Our baseline is constituted by the error rate of systems solely based on either FST or SVMs. The re-ranking models are built on the FST output, which in turn is applied to both manual or automatic transcriptions. 5.1 Corpora We used two different speech corpora: The LUNA corpus, produced in the homonymous European project, is the first Italian dataset of spontaneous speech on spoken dialogs. It is based on help-desk conversations in a domain of software/hardware repairing (Dinarelli et al., 2009b). The data is organized in transcriptions and annotations of speech based on a new multilevel protocol. Although data acquisition is still in progress, 250 dialogs have been already acquired with a WOZ approach and other 180 HumanHuman (HH) dialogs have been annotated. In this work, we only use WOZ dialogs, whose statistics are reported in Table 1. The corpus MEDIA was collected within the French project MEDIA-EVALDA (BonneauMaynard et al., 2005) for development and evaluation of spoken understanding models and linguistic studies. The corpus is composed of 1257 dialogs (from 250 different sp</context>
</contexts>
<marker>Dinarelli, Quarteroni, Tonelli, Moschitti, Riccardi, 2009</marker>
<rawString>Marco Dinarelli, Silvia Quarteroni, Sara Tonelli, Alessandro Moschitti, and Giuseppe Riccardi. 2009b. Annotating spoken dialogs: from speech segments to dialog acts and frame semantics. In Proceedings of SRSL 2009 Workshop of EACL, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Giuglea</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Knowledge Discovery using Framenet, Verbnet and Propbank.</title>
<date>2004</date>
<booktitle>Workshop on Ontology and Knowledge Discovering at ECML 2004,</booktitle>
<editor>In A. Meyers, editor,</editor>
<location>Pisa, Italy.</location>
<marker>Giuglea, Moschitti, 2004</marker>
<rawString>Ana-Maria Giuglea and Alessandro Moschitti. 2004. Knowledge Discovery using Framenet, Verbnet and Propbank. In A. Meyers, editor, Workshop on Ontology and Knowledge Discovering at ECML 2004, Pisa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Hahn</author>
<author>Patrick Lehnen</author>
<author>Christian Raymond</author>
<author>Hermann Ney</author>
</authors>
<title>A comparison of various methods for concept tagging for spoken language understanding.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC,</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="5169" citStr="Hahn et al., 2008" startWordPosition="787" endWordPosition="790">able by the European project LUNA1 (Dinarelli et al., 2009b). In particular, the new contents with respect to our previous work (Dinarelli et al., 2009a) are: • We designed a new sequential structure (SK2) and two new hierarchical tree structures (MULTILEVEL and FEATURES) for re-ranking models (see Section 4.2). The latter combined with two different tree kernels originate four new different models. • We experimented with automatic speech transcriptions thus assessing the robustness to noise of our models. • We compare our models against Conditional Random Field (CRF) approaches described in (Hahn et al., 2008), which are the current state-of-the-art in SLU. Learning curves clearly show that our models improve CRF, especially when small data sets are used. The remainder of the paper is organized as follows: Section 2 introduces kernel methods for structured data, Section 3 describes the generative model producing the initial hypotheses whereas Section 4 presents the discriminative models for re-ranking them. The experiments and results are reported in Section 5 and the conclusions are drawn in Section 6. 2 Feature Engineering via Structure Kernels Kernel methods are viable approaches to engineer fea</context>
<context position="24084" citStr="Hahn et al., 2008" startWordPosition="4012" endWordPosition="4015">SRILM toolkit. The model used to obtain the SVM baseline for concept classification was trained using YamCHA (Kudo and Matsumoto, 2001). As re-ranking models based on structure kernels and SVMs, we used the SVM-Light-TK toolkit (available at disi.unitn.it/moschitti). For λ (see Section 3), costfactor and trade-off parameters, we used, 0.4, 1 and 1, respectively (i.e. the default parameters). The number m of hypotheses was always set to 10. The CRF model we compare with was trained with the CRF++ tool, available at http://crfpp.sourceforge.net/. The model is equivalent to the one described in (Hahn et al., 2008). As features, we used word and morpho-syntactic categories in a window of [-2, +2] with respect to the current token, plus bigrams of concept tags (see (Hahn et al., 2008) and the CRF++ web site for more details). Such model is very effective for SLU. In (Hahn et al., 2008), it is compared with other four models (Stochastic Finite State Transducers, Support Vector Machines, Machine Translation, PositionalBased Log-linear model) and it is by far the best on MEDIA. Additionally, in (Raymond and Riccardi, 2007), a similar CRF model was compared with FST and SVMs on ATIS and on a different 2The n</context>
<context position="30042" citStr="Hahn et al., 2008" startWordPosition="5073" endWordPosition="5076">t they are significantly improved by the re-ranking model (2.9% points), which also improves the FSTs baseline by 3.8% points. On the MEDIA corpus, the re-ranking model is again very accurate improving the FSTs baseline of 3.6% points (12.6% relative improvement) on attribute annotation, but the most accurate model is again CRF (1% points better than the re-ranking model). 5.5 Discussion The different behavior of the re-ranking model in the LUNA and MEDIA corpora is due partially to the task complexity, but it is mainly due to the fact that CRFs have been deeply studied and experimented (see (Hahn et al., 2008)) on MEDIA. Thus CRF parameters and features have been largely optimized. We believe that the re-ranking model can be relevantly improved by carrying out parameter optimization and new structural feature de1083 sign. Moreover, our re-ranking models achieve the highest accuracy for automatic concept annotation when small data sets are available. To show this, we report in Figure 4(a) and 4(b) the learning curves according to an increasing number of training sentences on the MEDIA and LUNA corpora, respectively. To draw the first plot, we used a reranker based on STK (and the FLAT tree), which i</context>
</contexts>
<marker>Hahn, Lehnen, Raymond, Ney, 2008</marker>
<rawString>Stefan Hahn, Patrick Lehnen, Christian Raymond, and Hermann Ney. 2008. A comparison of various methods for concept tagging for spoken language understanding. In Proceedings of LREC, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>Chunking with support vector machines.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL2001,</booktitle>
<location>Pittsburg, USA.</location>
<contexts>
<context position="23601" citStr="Kudo and Matsumoto, 2001" startWordPosition="3935" endWordPosition="3938">Section 4. The initial annotation to be re-ranked is the list of the ten best hypotheses output by an FST model. We point out that, on the large Media dataset the processing time is considerably high2 so we could not run all the models. We trained all the SCLMs used in our experiments with the SRILM toolkit (Stolcke, 2002) and we used an interpolated model for probability estimation with the Kneser-Ney discount (Chen and Goodman, 1998). We then converted the model in an FST again with SRILM toolkit. The model used to obtain the SVM baseline for concept classification was trained using YamCHA (Kudo and Matsumoto, 2001). As re-ranking models based on structure kernels and SVMs, we used the SVM-Light-TK toolkit (available at disi.unitn.it/moschitti). For λ (see Section 3), costfactor and trade-off parameters, we used, 0.4, 1 and 1, respectively (i.e. the default parameters). The number m of hypotheses was always set to 10. The CRF model we compare with was trained with the CRF++ tool, available at http://crfpp.sourceforge.net/. The model is equivalent to the one described in (Hahn et al., 2008). As features, we used word and morpho-syntactic categories in a window of [-2, +2] with respect to the current token</context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>T. Kudo and Y. Matsumoto. 2001. Chunking with support vector machines. In Proceedings of NAACL2001, Pittsburg, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Fast methods for kernel-based text analysis.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL’03.</booktitle>
<contexts>
<context position="5851" citStr="Kudo and Matsumoto, 2003" startWordPosition="893" endWordPosition="896">curves clearly show that our models improve CRF, especially when small data sets are used. The remainder of the paper is organized as follows: Section 2 introduces kernel methods for structured data, Section 3 describes the generative model producing the initial hypotheses whereas Section 4 presents the discriminative models for re-ranking them. The experiments and results are reported in Section 5 and the conclusions are drawn in Section 6. 2 Feature Engineering via Structure Kernels Kernel methods are viable approaches to engineer features for text processing, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby 1Contract n. 33549 and Roth, 2003; Cancedda et al., 2003; Culotta and Sorensen, 2004; Toutanova et al., 2004; Kudo et al., 2005; Moschitti, 2006a; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008; Moschitti and Quarteroni, 2008). In the following, we describe structure kernels, which will be used to engineer features for our discriminative reranker. 2.1 String Kernels The String Kernels that we consider count the number of substrings containing gaps shared by two sequences, i.e. some of the symbols of the original string are skipped. We adopted the efficient algorithm de</context>
</contexts>
<marker>Kudo, Matsumoto, 2003</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2003. Fast methods for kernel-based text analysis. In Proceedings of ACL’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
</authors>
<title>Boosting-based parse reranking with subtree features.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL’05.</booktitle>
<contexts>
<context position="5986" citStr="Kudo et al., 2005" startWordPosition="916" endWordPosition="919"> Section 2 introduces kernel methods for structured data, Section 3 describes the generative model producing the initial hypotheses whereas Section 4 presents the discriminative models for re-ranking them. The experiments and results are reported in Section 5 and the conclusions are drawn in Section 6. 2 Feature Engineering via Structure Kernels Kernel methods are viable approaches to engineer features for text processing, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby 1Contract n. 33549 and Roth, 2003; Cancedda et al., 2003; Culotta and Sorensen, 2004; Toutanova et al., 2004; Kudo et al., 2005; Moschitti, 2006a; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008; Moschitti and Quarteroni, 2008). In the following, we describe structure kernels, which will be used to engineer features for our discriminative reranker. 2.1 String Kernels The String Kernels that we consider count the number of substrings containing gaps shared by two sequences, i.e. some of the symbols of the original string are skipped. We adopted the efficient algorithm described in (Shawe-Taylor and Cristianini, 2004; Lodhi et al., 2000). More specifically, we used words and markers as symbols in a style</context>
</contexts>
<marker>Kudo, Suzuki, Isozaki, 2005</marker>
<rawString>Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005. Boosting-based parse reranking with subtree features. In Proceedings of ACL’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML2001,</booktitle>
<pages>US.</pages>
<contexts>
<context position="2858" citStr="Lafferty et al., 2001" startWordPosition="432" endWordPosition="435">pts given words. A simple but effective generative model is the one based on Finite State Transducers. It performs SLU as a translation process from words to concepts using Finite State Transducers (FST). An example of discriminative model used for SLU is the one based on Support Vector Machines (SVMs) (Vapnik, 1995), as shown in (Raymond and Riccardi, 2007). In this approach, data is mapped into a vector space and SLU is performed as a classification problem using Maximal Margin Classifiers (Vapnik, 1995). A relatively more recent approach for SLU is based on Conditional Random Fields (CRF) (Lafferty et al., 2001). CRFs are undirected graphical and discriminative models. They use conditional probabilities to account for many feature dependencies without the need of explicitly representing such dependencies. Generative models have the advantage to be more robust to overfitting on training data, while discriminative models are more robust to irrelevant features. Both approaches, used separately, have shown good accuracy (Raymond and Riccardi, 2007), but they have very different characteristics and the way they encode prior knowledge is very different, thus designing models that take into account characte</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML2001, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huma Lodhi</author>
<author>John S Taylor</author>
<author>Nello Cristianini</author>
<author>Christopher J C H Watkins</author>
</authors>
<title>Text classification using string kernels.</title>
<date>2000</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="6518" citStr="Lodhi et al., 2000" startWordPosition="999" endWordPosition="1002">dda et al., 2003; Culotta and Sorensen, 2004; Toutanova et al., 2004; Kudo et al., 2005; Moschitti, 2006a; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008; Moschitti and Quarteroni, 2008). In the following, we describe structure kernels, which will be used to engineer features for our discriminative reranker. 2.1 String Kernels The String Kernels that we consider count the number of substrings containing gaps shared by two sequences, i.e. some of the symbols of the original string are skipped. We adopted the efficient algorithm described in (Shawe-Taylor and Cristianini, 2004; Lodhi et al., 2000). More specifically, we used words and markers as symbols in a style similar to (Cancedda et al., 2003; Moschitti, 2008). For example, given the sentence: How may I help you ? sample substrings, extracted by the Sequence Kernel (SK), are: How help you ?, How help ?, help you, may help you, etc. 2.2 Tree kernels Tree kernels represent trees in terms of their substructures (fragments). The kernel function detects if a tree subpart (common to both trees) belongs to the feature space that we intend to generate. For such purpose, the desired fragments need to be described. We consider two important</context>
</contexts>
<marker>Lodhi, Taylor, Cristianini, Watkins, 2000</marker>
<rawString>Huma Lodhi, John S. Taylor, Nello Cristianini, and Christopher J. C. H. Watkins. 2000. Text classification using string kernels. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Adrian Bejan Cosmin</author>
</authors>
<title>A semantic kernel for predicate argument classification.</title>
<date>2004</date>
<booktitle>In CoNLL-2004,</booktitle>
<location>Boston, MA, USA.</location>
<marker>Moschitti, Cosmin, 2004</marker>
<rawString>Alessandro Moschitti and Adrian Bejan Cosmin. 2004. A semantic kernel for predicate argument classification. In CoNLL-2004, Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Silvia Quarteroni</author>
</authors>
<title>Kernels on linguistic structures for answer extraction.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT, Short Papers,</booktitle>
<location>Columbus, Ohio.</location>
<contexts>
<context position="6102" citStr="Moschitti and Quarteroni, 2008" startWordPosition="932" endWordPosition="935">ducing the initial hypotheses whereas Section 4 presents the discriminative models for re-ranking them. The experiments and results are reported in Section 5 and the conclusions are drawn in Section 6. 2 Feature Engineering via Structure Kernels Kernel methods are viable approaches to engineer features for text processing, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby 1Contract n. 33549 and Roth, 2003; Cancedda et al., 2003; Culotta and Sorensen, 2004; Toutanova et al., 2004; Kudo et al., 2005; Moschitti, 2006a; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008; Moschitti and Quarteroni, 2008). In the following, we describe structure kernels, which will be used to engineer features for our discriminative reranker. 2.1 String Kernels The String Kernels that we consider count the number of substrings containing gaps shared by two sequences, i.e. some of the symbols of the original string are skipped. We adopted the efficient algorithm described in (Shawe-Taylor and Cristianini, 2004; Lodhi et al., 2000). More specifically, we used words and markers as symbols in a style similar to (Cancedda et al., 2003; Moschitti, 2008). For example, given the sentence: How may I help you ? sample s</context>
</contexts>
<marker>Moschitti, Quarteroni, 2008</marker>
<rawString>Alessandro Moschitti and Silvia Quarteroni. 2008. Kernels on linguistic structures for answer extraction. In Proceedings ofACL-08: HLT, Short Papers, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Daniele Pighin</author>
<author>Roberto Basili</author>
</authors>
<title>Semantic role labeling via tree kernel joint inference.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL-X,</booktitle>
<location>New York City.</location>
<contexts>
<context position="19399" citStr="Moschitti et al., 2006" startWordPosition="3216" endWordPosition="3219">ine translation. Notice that our goal is different from the one tackled in such paper and, in general, it is more difficult: we try to learn which is the best annotation of a given input sentence, while in (Shen et al., 2004), they learn to distinguish between ”good” and ”bad” translations of a sentence. Even if our goal is more difficult, our approach is very effective, as shown in (Dinarelli et al., 2009a). It is more appropriate since in parse re-ranking there is only one best hypothesis, while in machine translation a sentence can have more than one correct translations. Additionally, in (Moschitti et al., 2006; Moschitti et al., 2008) a tree kernel was applied to semantic trees similar to the one introduced in the next section to re-rank Semantic Role Labeling annotations. 4.4 Re-ranking models using trees Since the aim of concept annotation re-ranking is to exploit innovative and effective source of information, we can use, in addition to sequence kernels, the power of tree kernels to generate correlation between concepts and word structures. Figures 2(a), 2(b) and 3 describe the structural association between the concept and the word 1080 (a) FLAT Tree (b) MULTILEVEL Tree Figure 2: Examples of st</context>
</contexts>
<marker>Moschitti, Pighin, Basili, 2006</marker>
<rawString>Alessandro Moschitti, Daniele Pighin, and Roberto Basili. 2006. Semantic role labeling via tree kernel joint inference. In Proceedings of CoNLL-X, New York City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Silvia Quarteroni</author>
<author>Roberto Basili</author>
<author>Suresh Manandhar</author>
</authors>
<title>Exploiting syntactic and shallow semantic kernels for question/answer classification.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL’07,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="6028" citStr="Moschitti et al., 2007" startWordPosition="922" endWordPosition="925">for structured data, Section 3 describes the generative model producing the initial hypotheses whereas Section 4 presents the discriminative models for re-ranking them. The experiments and results are reported in Section 5 and the conclusions are drawn in Section 6. 2 Feature Engineering via Structure Kernels Kernel methods are viable approaches to engineer features for text processing, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby 1Contract n. 33549 and Roth, 2003; Cancedda et al., 2003; Culotta and Sorensen, 2004; Toutanova et al., 2004; Kudo et al., 2005; Moschitti, 2006a; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008; Moschitti and Quarteroni, 2008). In the following, we describe structure kernels, which will be used to engineer features for our discriminative reranker. 2.1 String Kernels The String Kernels that we consider count the number of substrings containing gaps shared by two sequences, i.e. some of the symbols of the original string are skipped. We adopted the efficient algorithm described in (Shawe-Taylor and Cristianini, 2004; Lodhi et al., 2000). More specifically, we used words and markers as symbols in a style similar to (Cancedda et al., 2003; Moschi</context>
</contexts>
<marker>Moschitti, Quarteroni, Basili, Manandhar, 2007</marker>
<rawString>Alessandro Moschitti, Silvia Quarteroni, Roberto Basili, and Suresh Manandhar. 2007. Exploiting syntactic and shallow semantic kernels for question/answer classification. In Proceedings of ACL’07, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Daniele Pighin</author>
<author>Roberto Basili</author>
</authors>
<title>Tree kernels for semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="6069" citStr="Moschitti et al., 2008" startWordPosition="928" endWordPosition="931">the generative model producing the initial hypotheses whereas Section 4 presents the discriminative models for re-ranking them. The experiments and results are reported in Section 5 and the conclusions are drawn in Section 6. 2 Feature Engineering via Structure Kernels Kernel methods are viable approaches to engineer features for text processing, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby 1Contract n. 33549 and Roth, 2003; Cancedda et al., 2003; Culotta and Sorensen, 2004; Toutanova et al., 2004; Kudo et al., 2005; Moschitti, 2006a; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008; Moschitti and Quarteroni, 2008). In the following, we describe structure kernels, which will be used to engineer features for our discriminative reranker. 2.1 String Kernels The String Kernels that we consider count the number of substrings containing gaps shared by two sequences, i.e. some of the symbols of the original string are skipped. We adopted the efficient algorithm described in (Shawe-Taylor and Cristianini, 2004; Lodhi et al., 2000). More specifically, we used words and markers as symbols in a style similar to (Cancedda et al., 2003; Moschitti, 2008). For example, given the senten</context>
<context position="19424" citStr="Moschitti et al., 2008" startWordPosition="3220" endWordPosition="3224">that our goal is different from the one tackled in such paper and, in general, it is more difficult: we try to learn which is the best annotation of a given input sentence, while in (Shen et al., 2004), they learn to distinguish between ”good” and ”bad” translations of a sentence. Even if our goal is more difficult, our approach is very effective, as shown in (Dinarelli et al., 2009a). It is more appropriate since in parse re-ranking there is only one best hypothesis, while in machine translation a sentence can have more than one correct translations. Additionally, in (Moschitti et al., 2006; Moschitti et al., 2008) a tree kernel was applied to semantic trees similar to the one introduced in the next section to re-rank Semantic Role Labeling annotations. 4.4 Re-ranking models using trees Since the aim of concept annotation re-ranking is to exploit innovative and effective source of information, we can use, in addition to sequence kernels, the power of tree kernels to generate correlation between concepts and word structures. Figures 2(a), 2(b) and 3 describe the structural association between the concept and the word 1080 (a) FLAT Tree (b) MULTILEVEL Tree Figure 2: Examples of structures used for STK and</context>
</contexts>
<marker>Moschitti, Pighin, Basili, 2008</marker>
<rawString>Alessandro Moschitti, Daniele Pighin, and Roberto Basili. 2008. Tree kernels for semantic role labeling. Computational Linguistics, 34(2):193–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees.</title>
<date>2006</date>
<booktitle>In Proceedings of ECML 2006,</booktitle>
<pages>318--329</pages>
<location>Berlin, Germany.</location>
<contexts>
<context position="4288" citStr="Moschitti, 2006" startWordPosition="651" endWordPosition="652">e-ranked by SVMs. To effectively design our re-ranker, we use all pos1076 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1076–1085, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP sible word/concept subsequences with gaps of the spoken sentence as features (i.e. all possible ngrams). Gaps allow for encoding long distance dependencies between words in relatively small sequences. Since the space of such features is huge, we adopted kernel methods, i.e. sequence kernels (Shawe-Taylor and Cristianini, 2004) and tree kernels (Collins and Duffy, 2002; Moschitti, 2006a) to implicitly encode them along with other structural information in SVMs. We experimented with different approaches for training the discriminative models and two different corpora: the french MEDIA corpus (BonneauMaynard et al., 2005) and a corpus made available by the European project LUNA1 (Dinarelli et al., 2009b). In particular, the new contents with respect to our previous work (Dinarelli et al., 2009a) are: • We designed a new sequential structure (SK2) and two new hierarchical tree structures (MULTILEVEL and FEATURES) for re-ranking models (see Section 4.2). The latter combined wit</context>
<context position="6003" citStr="Moschitti, 2006" startWordPosition="920" endWordPosition="921">es kernel methods for structured data, Section 3 describes the generative model producing the initial hypotheses whereas Section 4 presents the discriminative models for re-ranking them. The experiments and results are reported in Section 5 and the conclusions are drawn in Section 6. 2 Feature Engineering via Structure Kernels Kernel methods are viable approaches to engineer features for text processing, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby 1Contract n. 33549 and Roth, 2003; Cancedda et al., 2003; Culotta and Sorensen, 2004; Toutanova et al., 2004; Kudo et al., 2005; Moschitti, 2006a; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008; Moschitti and Quarteroni, 2008). In the following, we describe structure kernels, which will be used to engineer features for our discriminative reranker. 2.1 String Kernels The String Kernels that we consider count the number of substrings containing gaps shared by two sequences, i.e. some of the symbols of the original string are skipped. We adopted the efficient algorithm described in (Shawe-Taylor and Cristianini, 2004; Lodhi et al., 2000). More specifically, we used words and markers as symbols in a style similar to (Canc</context>
<context position="7346" citStr="Moschitti, 2006" startWordPosition="1143" endWordPosition="1144">equence Kernel (SK), are: How help you ?, How help ?, help you, may help you, etc. 2.2 Tree kernels Tree kernels represent trees in terms of their substructures (fragments). The kernel function detects if a tree subpart (common to both trees) belongs to the feature space that we intend to generate. For such purpose, the desired fragments need to be described. We consider two important characterizations: the syntactic tree (STF) and the partial tree (PTF) fragments. 2.2.1 Tree Fragment Types An STF is a general subtree whose leaves can be non-terminal symbols (also called SubSet Tree (SST) in (Moschitti, 2006a)). For example, Figure 1(a) shows 10 STFs (out of 17) of the subtree rooted in VP (of the left tree). The STFs satisfy the constraint that grammatical rules cannot be broken. For example, [VP [V NP]] is an STF, which has two non-terminal symbols, V and NP, as leaves whereas [VP [V]] is not an STF. If we relax the constraint over the STFs, we obtain more general substructures called partial trees fragments (PTFs). These can be generated by the application of partial production rules of the grammar, consequently [VP [V]] and [VP [NP]] are valid PTFs. Figure 1(b) shows that the number of PTFs d</context>
<context position="9050" citStr="Moschitti, 2006" startWordPosition="1455" endWordPosition="1456">|F|}, i.e. a tree fragment space and an indicator function Ii(n), equal to 1 if the target fi is rooted at node n and equal to 0 otherwise. A tree-kernel function over T1 and T2 is TK(T1, T2) = � �n2∈NT2 A(n1, n2), n1∈NT1 where NT1 and NT2 are the sets of the T1’s and T2’s nodes, respectively and A(n1, n2) = �|F| i=1Ii(n1)Ii(n2). The latter is equal to the number of common fragments rooted in the n1 and n2 nodes. The algorithm for the efficient evaluation of A for the syntactic tree kernel (STK) has been widely discussed in (Collins and Duffy, 2002) whereas its fast evaluation is proposed in (Moschitti, 2006b), so we only describe the equations of the partial tree kernel (PTK). 2.4 The Partial Tree Kernel (PTK) PTFs have been defined in (Moschitti, 2006a). Their computation is carried out by the following A function: 1. if the node labels of n1 and n2 are different then A(n1, n2) = 0; 2. else A(n1, n2) = 1 + F_I1,~I2,l(~I1)=l(~I2) 11l(~I1) j=1 A(cn1 (~I1j), cn2(~I2j)) where ~I1 = (h1, h2, h3, ..) and ~I2 = (k1, k2, k3, ..) are index sequences associated with the ordered child sequences cn1 of n1 and cn2 of ~I2j point to the j-th child n2, respectively, ~I1j an in the corresponding sequence, and, </context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006a. Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees. In Proceedings of ECML 2006, pages 318– 329, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Making Tree Kernels Practical for Natural Language Learning.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL2006.</booktitle>
<contexts>
<context position="4288" citStr="Moschitti, 2006" startWordPosition="651" endWordPosition="652">e-ranked by SVMs. To effectively design our re-ranker, we use all pos1076 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1076–1085, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP sible word/concept subsequences with gaps of the spoken sentence as features (i.e. all possible ngrams). Gaps allow for encoding long distance dependencies between words in relatively small sequences. Since the space of such features is huge, we adopted kernel methods, i.e. sequence kernels (Shawe-Taylor and Cristianini, 2004) and tree kernels (Collins and Duffy, 2002; Moschitti, 2006a) to implicitly encode them along with other structural information in SVMs. We experimented with different approaches for training the discriminative models and two different corpora: the french MEDIA corpus (BonneauMaynard et al., 2005) and a corpus made available by the European project LUNA1 (Dinarelli et al., 2009b). In particular, the new contents with respect to our previous work (Dinarelli et al., 2009a) are: • We designed a new sequential structure (SK2) and two new hierarchical tree structures (MULTILEVEL and FEATURES) for re-ranking models (see Section 4.2). The latter combined wit</context>
<context position="6003" citStr="Moschitti, 2006" startWordPosition="920" endWordPosition="921">es kernel methods for structured data, Section 3 describes the generative model producing the initial hypotheses whereas Section 4 presents the discriminative models for re-ranking them. The experiments and results are reported in Section 5 and the conclusions are drawn in Section 6. 2 Feature Engineering via Structure Kernels Kernel methods are viable approaches to engineer features for text processing, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby 1Contract n. 33549 and Roth, 2003; Cancedda et al., 2003; Culotta and Sorensen, 2004; Toutanova et al., 2004; Kudo et al., 2005; Moschitti, 2006a; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008; Moschitti and Quarteroni, 2008). In the following, we describe structure kernels, which will be used to engineer features for our discriminative reranker. 2.1 String Kernels The String Kernels that we consider count the number of substrings containing gaps shared by two sequences, i.e. some of the symbols of the original string are skipped. We adopted the efficient algorithm described in (Shawe-Taylor and Cristianini, 2004; Lodhi et al., 2000). More specifically, we used words and markers as symbols in a style similar to (Canc</context>
<context position="7346" citStr="Moschitti, 2006" startWordPosition="1143" endWordPosition="1144">equence Kernel (SK), are: How help you ?, How help ?, help you, may help you, etc. 2.2 Tree kernels Tree kernels represent trees in terms of their substructures (fragments). The kernel function detects if a tree subpart (common to both trees) belongs to the feature space that we intend to generate. For such purpose, the desired fragments need to be described. We consider two important characterizations: the syntactic tree (STF) and the partial tree (PTF) fragments. 2.2.1 Tree Fragment Types An STF is a general subtree whose leaves can be non-terminal symbols (also called SubSet Tree (SST) in (Moschitti, 2006a)). For example, Figure 1(a) shows 10 STFs (out of 17) of the subtree rooted in VP (of the left tree). The STFs satisfy the constraint that grammatical rules cannot be broken. For example, [VP [V NP]] is an STF, which has two non-terminal symbols, V and NP, as leaves whereas [VP [V]] is not an STF. If we relax the constraint over the STFs, we obtain more general substructures called partial trees fragments (PTFs). These can be generated by the application of partial production rules of the grammar, consequently [VP [V]] and [VP [NP]] are valid PTFs. Figure 1(b) shows that the number of PTFs d</context>
<context position="9050" citStr="Moschitti, 2006" startWordPosition="1455" endWordPosition="1456">|F|}, i.e. a tree fragment space and an indicator function Ii(n), equal to 1 if the target fi is rooted at node n and equal to 0 otherwise. A tree-kernel function over T1 and T2 is TK(T1, T2) = � �n2∈NT2 A(n1, n2), n1∈NT1 where NT1 and NT2 are the sets of the T1’s and T2’s nodes, respectively and A(n1, n2) = �|F| i=1Ii(n1)Ii(n2). The latter is equal to the number of common fragments rooted in the n1 and n2 nodes. The algorithm for the efficient evaluation of A for the syntactic tree kernel (STK) has been widely discussed in (Collins and Duffy, 2002) whereas its fast evaluation is proposed in (Moschitti, 2006b), so we only describe the equations of the partial tree kernel (PTK). 2.4 The Partial Tree Kernel (PTK) PTFs have been defined in (Moschitti, 2006a). Their computation is carried out by the following A function: 1. if the node labels of n1 and n2 are different then A(n1, n2) = 0; 2. else A(n1, n2) = 1 + F_I1,~I2,l(~I1)=l(~I2) 11l(~I1) j=1 A(cn1 (~I1j), cn2(~I2j)) where ~I1 = (h1, h2, h3, ..) and ~I2 = (k1, k2, k3, ..) are index sequences associated with the ordered child sequences cn1 of n1 and cn2 of ~I2j point to the j-th child n2, respectively, ~I1j an in the corresponding sequence, and, </context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006b. Making Tree Kernels Practical for Natural Language Learning. In Proceedings of EACL2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Kernel methods, syntax and semantics for relational text categorization.</title>
<date>2008</date>
<booktitle>In Proceeding of CIKM ’08,</booktitle>
<location>NY, USA.</location>
<contexts>
<context position="6045" citStr="Moschitti, 2008" startWordPosition="926" endWordPosition="927">tion 3 describes the generative model producing the initial hypotheses whereas Section 4 presents the discriminative models for re-ranking them. The experiments and results are reported in Section 5 and the conclusions are drawn in Section 6. 2 Feature Engineering via Structure Kernels Kernel methods are viable approaches to engineer features for text processing, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby 1Contract n. 33549 and Roth, 2003; Cancedda et al., 2003; Culotta and Sorensen, 2004; Toutanova et al., 2004; Kudo et al., 2005; Moschitti, 2006a; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008; Moschitti and Quarteroni, 2008). In the following, we describe structure kernels, which will be used to engineer features for our discriminative reranker. 2.1 String Kernels The String Kernels that we consider count the number of substrings containing gaps shared by two sequences, i.e. some of the symbols of the original string are skipped. We adopted the efficient algorithm described in (Shawe-Taylor and Cristianini, 2004; Lodhi et al., 2000). More specifically, we used words and markers as symbols in a style similar to (Cancedda et al., 2003; Moschitti, 2008). For e</context>
</contexts>
<marker>Moschitti, 2008</marker>
<rawString>Alessandro Moschitti. 2008. Kernel methods, syntax and semantics for relational text categorization. In Proceeding of CIKM ’08, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Raymond</author>
<author>G Riccardi</author>
</authors>
<title>Generative and discriminative algorithms for spoken language understanding.</title>
<date>2007</date>
<booktitle>In Proceedings of Interspeech2007,</booktitle>
<location>Antwerp,Belgium.</location>
<contexts>
<context position="2596" citStr="Raymond and Riccardi, 2007" startWordPosition="386" endWordPosition="390">een proposed to automatically map words in concepts: (i) generative models, whose parameters refer to the joint probability of concepts and constituents; and (ii) discriminative models, which learn a classification function based on conditional probabilities of concepts given words. A simple but effective generative model is the one based on Finite State Transducers. It performs SLU as a translation process from words to concepts using Finite State Transducers (FST). An example of discriminative model used for SLU is the one based on Support Vector Machines (SVMs) (Vapnik, 1995), as shown in (Raymond and Riccardi, 2007). In this approach, data is mapped into a vector space and SLU is performed as a classification problem using Maximal Margin Classifiers (Vapnik, 1995). A relatively more recent approach for SLU is based on Conditional Random Fields (CRF) (Lafferty et al., 2001). CRFs are undirected graphical and discriminative models. They use conditional probabilities to account for many feature dependencies without the need of explicitly representing such dependencies. Generative models have the advantage to be more robust to overfitting on training data, while discriminative models are more robust to irrel</context>
<context position="10488" citStr="Raymond and Riccardi, 2007" startWordPosition="1711" endWordPosition="1714">l sequence, i.e. we account for gaps. It follows that A(n1, n2) = ( � µ λ2+ ~I1,~I2,l(~I1)=l( where d(~I1) = ~I1l(~I1) − I21. This way, we penalize both larger trees and child subsequences with gaps. Eq. 1 is more general than the A equation for STK. Indeed, if we only consider the contribution of the longest child sequence from node pairs that have the same children, we implement STK. 3 Generative Model: Stochastic Conceptual Language Model (SCLM) The first step of our approach is to produce a list of SLU hypotheses using a Stochastic Conceptual Language Model. This is the same described in (Raymond and Riccardi, 2007) with the only difference that we train the language model using the SRILM toolkit (Stolcke, 2002) and we then convert it into a Stochastic Finite State Transducer (SFST). Such method allows us to use a wide group of language models, backed-off or interpolated with many kind of smoothing techniques (Chen and Goodman, 1998). To exemplify our SCLM let us consider the following input italian sentence taken from the LUNA corpus along with its English translation: Ho un problema col monitor. (I have a problem with my screen). A possible semantic annotation is: null{ho} PROBLEM{un problema} HARDWARE</context>
<context position="24598" citStr="Raymond and Riccardi, 2007" startWordPosition="4099" endWordPosition="4103">, available at http://crfpp.sourceforge.net/. The model is equivalent to the one described in (Hahn et al., 2008). As features, we used word and morpho-syntactic categories in a window of [-2, +2] with respect to the current token, plus bigrams of concept tags (see (Hahn et al., 2008) and the CRF++ web site for more details). Such model is very effective for SLU. In (Hahn et al., 2008), it is compared with other four models (Stochastic Finite State Transducers, Support Vector Machines, Machine Translation, PositionalBased Log-linear model) and it is by far the best on MEDIA. Additionally, in (Raymond and Riccardi, 2007), a similar CRF model was compared with FST and SVMs on ATIS and on a different 2The number of parameters of the models and the number of training approaches make the exhaustive experimentation very expensive in terms of processing time, which would be roughly between 2 and 3 months of a typical workstation. Structure STK PTK SK FLAT 18.5 19.3 - MULTILEVEL 20.6 19.1 - FEATURES 19.9 18.4 - SK1 - - 16.2 SK2 - - 18.5 Table 3: CER of SVMs using STK, PTK and SK on LUNA (manual transcriptions). The Baselines, FST and SVMs alone, show a CER of 23.2% and 26.3%, respectively. Model MEDIA (CER) LUNA (CE</context>
</contexts>
<marker>Raymond, Riccardi, 2007</marker>
<rawString>C. Raymond and G. Riccardi. 2007. Generative and discriminative algorithms for spoken language understanding. In Proceedings of Interspeech2007, Antwerp,Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Shawe-Taylor</author>
<author>N Cristianini</author>
</authors>
<title>Kernel Methods for Pattern Analysis.</title>
<date>2004</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="4229" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="639" endWordPosition="642"> models: the former uses FSTs to generate a list of SLU hypotheses, which are re-ranked by SVMs. To effectively design our re-ranker, we use all pos1076 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1076–1085, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP sible word/concept subsequences with gaps of the spoken sentence as features (i.e. all possible ngrams). Gaps allow for encoding long distance dependencies between words in relatively small sequences. Since the space of such features is huge, we adopted kernel methods, i.e. sequence kernels (Shawe-Taylor and Cristianini, 2004) and tree kernels (Collins and Duffy, 2002; Moschitti, 2006a) to implicitly encode them along with other structural information in SVMs. We experimented with different approaches for training the discriminative models and two different corpora: the french MEDIA corpus (BonneauMaynard et al., 2005) and a corpus made available by the European project LUNA1 (Dinarelli et al., 2009b). In particular, the new contents with respect to our previous work (Dinarelli et al., 2009a) are: • We designed a new sequential structure (SK2) and two new hierarchical tree structures (MULTILEVEL and FEATURES) for r</context>
<context position="6497" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="995" endWordPosition="998">tract n. 33549 and Roth, 2003; Cancedda et al., 2003; Culotta and Sorensen, 2004; Toutanova et al., 2004; Kudo et al., 2005; Moschitti, 2006a; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008; Moschitti and Quarteroni, 2008). In the following, we describe structure kernels, which will be used to engineer features for our discriminative reranker. 2.1 String Kernels The String Kernels that we consider count the number of substrings containing gaps shared by two sequences, i.e. some of the symbols of the original string are skipped. We adopted the efficient algorithm described in (Shawe-Taylor and Cristianini, 2004; Lodhi et al., 2000). More specifically, we used words and markers as symbols in a style similar to (Cancedda et al., 2003; Moschitti, 2008). For example, given the sentence: How may I help you ? sample substrings, extracted by the Sequence Kernel (SK), are: How help you ?, How help ?, help you, may help you, etc. 2.2 Tree kernels Tree kernels represent trees in terms of their substructures (fragments). The kernel function detects if a tree subpart (common to both trees) belongs to the feature space that we intend to generate. For such purpose, the desired fragments need to be described. We c</context>
<context position="12554" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="2053" endWordPosition="2056">pt sequences by using the joint probability: k P(W, C) = P(wi, ci|hi), i=1 where W = w1..wk, C = c1..ck and hi = wi−1ci−1..w1c1. 4 Discriminative re-ranking Our discriminative re-ranking is based on SVMs trained with pairs of conceptually annotated sentences produced by the FST-based generative model described in the previous section. An SVM learn to classify which annotation has an error rate lower than the others so that it can be used to sort the m-best annotations based on their correctness. While for SVMs details we remaind to the wide literature available, for example (Vapnik, 1995) or (Shawe-Taylor and Cristianini, 2004), in this section we focus on hypotheses generation and on the kernels used to implement our re-ranking model. 4.1 Generation of m-best concept labeling Using the FST-based model described in Section 3, we can generate the list of m best hypotheses ranked by the joint probability of the Stochastic Conceptual Language Model (SCLM). The Reranking model proposed in this paper re-ranks such list. After an analysis of the m-best hypothesis list, we noticed that many times the first hypothesis ranked by SCLM is not the most accurate, i.e. the error rate evaluated with its Levenshtein distance from t</context>
</contexts>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>J. Shawe-Taylor and N. Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Anoop Sarkar</author>
<author>Aravind k Joshi</author>
</authors>
<title>Using LTAG Based Features in Parse Reranking.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP’06.</booktitle>
<contexts>
<context position="18578" citStr="Shen et al., 2003" startWordPosition="3074" endWordPosition="3077">ata, the sequence kernel is used to evaluate the number of common ngrams between si and sj. Since the string kernel skips some elements of the target sequences, the counted n-grams include: concept sequences, word sequences and any subsequence of words and concepts at any distance in the sentence. Such counts are used in our re-ranking function as follows: let ek be the pair N, s2 � we evaluate k the kernel: KR(e1, e2) = SK(s11, s12) + SK(s21, s22) (2) − SK(s11, s22) − SK(s21, s12) This schema, consisting in summing four different kernels, has been already applied in (Collins and Duffy, 2002; Shen et al., 2003) for syntactic parsing re-ranking, where the basic kernel was a tree kernel instead of SK. It was also used also in (Shen et al., 2004) to re-rank different candidates of the same hypothesis for machine translation. Notice that our goal is different from the one tackled in such paper and, in general, it is more difficult: we try to learn which is the best annotation of a given input sentence, while in (Shen et al., 2004), they learn to distinguish between ”good” and ”bad” translations of a sentence. Even if our goal is more difficult, our approach is very effective, as shown in (Dinarelli et a</context>
</contexts>
<marker>Shen, Sarkar, Joshi, 2003</marker>
<rawString>Libin Shen, Anoop Sarkar, and Aravind k. Joshi. 2003. Using LTAG Based Features in Parse Reranking. In Proceedings of EMNLP’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Anoop Sarkar</author>
<author>Franz Josef Och</author>
</authors>
<title>Discriminative reranking for machine translation. In</title>
<date>2004</date>
<booktitle>HLT-NAACL,</booktitle>
<pages>177--184</pages>
<contexts>
<context position="18713" citStr="Shen et al., 2004" startWordPosition="3100" endWordPosition="3103"> of the target sequences, the counted n-grams include: concept sequences, word sequences and any subsequence of words and concepts at any distance in the sentence. Such counts are used in our re-ranking function as follows: let ek be the pair N, s2 � we evaluate k the kernel: KR(e1, e2) = SK(s11, s12) + SK(s21, s22) (2) − SK(s11, s22) − SK(s21, s12) This schema, consisting in summing four different kernels, has been already applied in (Collins and Duffy, 2002; Shen et al., 2003) for syntactic parsing re-ranking, where the basic kernel was a tree kernel instead of SK. It was also used also in (Shen et al., 2004) to re-rank different candidates of the same hypothesis for machine translation. Notice that our goal is different from the one tackled in such paper and, in general, it is more difficult: we try to learn which is the best annotation of a given input sentence, while in (Shen et al., 2004), they learn to distinguish between ”good” and ”bad” translations of a sentence. Even if our goal is more difficult, our approach is very effective, as shown in (Dinarelli et al., 2009a). It is more appropriate since in parse re-ranking there is only one best hypothesis, while in machine translation a sentence</context>
</contexts>
<marker>Shen, Sarkar, Och, 2004</marker>
<rawString>Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004. Discriminative reranking for machine translation. In HLT-NAACL, pages 177–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>Srilm: an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of SLP2002,</booktitle>
<location>Denver, USA.</location>
<contexts>
<context position="10586" citStr="Stolcke, 2002" startWordPosition="1730" endWordPosition="1731">1l(~I1) − I21. This way, we penalize both larger trees and child subsequences with gaps. Eq. 1 is more general than the A equation for STK. Indeed, if we only consider the contribution of the longest child sequence from node pairs that have the same children, we implement STK. 3 Generative Model: Stochastic Conceptual Language Model (SCLM) The first step of our approach is to produce a list of SLU hypotheses using a Stochastic Conceptual Language Model. This is the same described in (Raymond and Riccardi, 2007) with the only difference that we train the language model using the SRILM toolkit (Stolcke, 2002) and we then convert it into a Stochastic Finite State Transducer (SFST). Such method allows us to use a wide group of language models, backed-off or interpolated with many kind of smoothing techniques (Chen and Goodman, 1998). To exemplify our SCLM let us consider the following input italian sentence taken from the LUNA corpus along with its English translation: Ho un problema col monitor. (I have a problem with my screen). A possible semantic annotation is: null{ho} PROBLEM{un problema} HARDWARE{col monitor}, where PROBLEM and HARDWARE are two domain concepts and null is the label used for w</context>
<context position="23300" citStr="Stolcke, 2002" startWordPosition="3888" endWordPosition="3889">n Table 2. 5.2 Experimental setup Given the small size of LUNA corpus, we did not carried out any parameterization thus we used default or a priori parameters. We experimented with LUNA and three different re-rankers obtained with the combination of SVMs with STK, PTK and SK, described in Section 4. The initial annotation to be re-ranked is the list of the ten best hypotheses output by an FST model. We point out that, on the large Media dataset the processing time is considerably high2 so we could not run all the models. We trained all the SCLMs used in our experiments with the SRILM toolkit (Stolcke, 2002) and we used an interpolated model for probability estimation with the Kneser-Ney discount (Chen and Goodman, 1998). We then converted the model in an FST again with SRILM toolkit. The model used to obtain the SVM baseline for concept classification was trained using YamCHA (Kudo and Matsumoto, 2001). As re-ranking models based on structure kernels and SVMs, we used the SVM-Light-TK toolkit (available at disi.unitn.it/moschitti). For λ (see Section 3), costfactor and trade-off parameters, we used, 0.4, 1 and 1, respectively (i.e. the default parameters). The number m of hypotheses was always s</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. Srilm: an extensible language modeling toolkit. In Proceedings of SLP2002, Denver, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Penka Markova</author>
<author>Christopher Manning</author>
</authors>
<title>The Leaf Path Projection View of Parse Trees: Exploring String Kernels for HPSG Parse Selection.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="5967" citStr="Toutanova et al., 2004" startWordPosition="912" endWordPosition="915">is organized as follows: Section 2 introduces kernel methods for structured data, Section 3 describes the generative model producing the initial hypotheses whereas Section 4 presents the discriminative models for re-ranking them. The experiments and results are reported in Section 5 and the conclusions are drawn in Section 6. 2 Feature Engineering via Structure Kernels Kernel methods are viable approaches to engineer features for text processing, e.g. (Collins and Duffy, 2002; Kudo and Matsumoto, 2003; Cumby 1Contract n. 33549 and Roth, 2003; Cancedda et al., 2003; Culotta and Sorensen, 2004; Toutanova et al., 2004; Kudo et al., 2005; Moschitti, 2006a; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008; Moschitti and Quarteroni, 2008). In the following, we describe structure kernels, which will be used to engineer features for our discriminative reranker. 2.1 String Kernels The String Kernels that we consider count the number of substrings containing gaps shared by two sequences, i.e. some of the symbols of the original string are skipped. We adopted the efficient algorithm described in (Shawe-Taylor and Cristianini, 2004; Lodhi et al., 2000). More specifically, we used words and markers as</context>
</contexts>
<marker>Toutanova, Markova, Manning, 2004</marker>
<rawString>Kristina Toutanova, Penka Markova, and Christopher Manning. 2004. The Leaf Path Projection View of Parse Trees: Exploring String Kernels for HPSG Parse Selection. In Proceedings of EMNLP 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer.</publisher>
<contexts>
<context position="2554" citStr="Vapnik, 1995" startWordPosition="381" endWordPosition="382"> two major approaches have been proposed to automatically map words in concepts: (i) generative models, whose parameters refer to the joint probability of concepts and constituents; and (ii) discriminative models, which learn a classification function based on conditional probabilities of concepts given words. A simple but effective generative model is the one based on Finite State Transducers. It performs SLU as a translation process from words to concepts using Finite State Transducers (FST). An example of discriminative model used for SLU is the one based on Support Vector Machines (SVMs) (Vapnik, 1995), as shown in (Raymond and Riccardi, 2007). In this approach, data is mapped into a vector space and SLU is performed as a classification problem using Maximal Margin Classifiers (Vapnik, 1995). A relatively more recent approach for SLU is based on Conditional Random Fields (CRF) (Lafferty et al., 2001). CRFs are undirected graphical and discriminative models. They use conditional probabilities to account for many feature dependencies without the need of explicitly representing such dependencies. Generative models have the advantage to be more robust to overfitting on training data, while disc</context>
<context position="12514" citStr="Vapnik, 1995" startWordPosition="2050" endWordPosition="2051"> of word and concept sequences by using the joint probability: k P(W, C) = P(wi, ci|hi), i=1 where W = w1..wk, C = c1..ck and hi = wi−1ci−1..w1c1. 4 Discriminative re-ranking Our discriminative re-ranking is based on SVMs trained with pairs of conceptually annotated sentences produced by the FST-based generative model described in the previous section. An SVM learn to classify which annotation has an error rate lower than the others so that it can be used to sort the m-best annotations based on their correctness. While for SVMs details we remaind to the wide literature available, for example (Vapnik, 1995) or (Shawe-Taylor and Cristianini, 2004), in this section we focus on hypotheses generation and on the kernels used to implement our re-ranking model. 4.1 Generation of m-best concept labeling Using the FST-based model described in Section 3, we can generate the list of m best hypotheses ranked by the joint probability of the Stochastic Conceptual Language Model (SCLM). The Reranking model proposed in this paper re-ranks such list. After an analysis of the m-best hypothesis list, we noticed that many times the first hypothesis ranked by SCLM is not the most accurate, i.e. the error rate evalua</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>V. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>