<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.942484">
Fast and Robust Joint Models for Biomedical Event Extraction
</title>
<author confidence="0.999069">
Sebastian Riedel Andrew McCallum
</author>
<affiliation confidence="0.9994465">
Department of Computer Science
University of Massachusetts, Amherst
</affiliation>
<email confidence="0.997633">
{riedel,mccallum}@cs.umass.edu
</email>
<sectionHeader confidence="0.996669" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995896304347826">
Extracting biomedical events from literature
has attracted much recent attention. The best-
performing systems so far have been pipelines
of simple subtask-specific local classifiers. A
natural drawback of such approaches are cas-
cading errors introduced in early stages of the
pipeline. We present three joint models of
increasing complexity designed to overcome
this problem. The first model performs joint
trigger and argument extraction, and lends it-
self to a simple, efficient and exact infer-
ence algorithm. The second model captures
correlations between events, while the third
model ensures consistency between arguments
of the same event. Inference in these models
is kept tractable through dual decomposition.
The first two models outperform the previous
best joint approaches and are very competi-
tive with respect to the current state-of-the-
art. The third model yields the best results re-
ported so far on the BioNLP 2009 shared task,
the BioNLP 2011 Genia task and the BioNLP
2011 Infectious Diseases task.
</bodyText>
<sectionHeader confidence="0.99883" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996579888888889">
Whenever we advance our scientific understanding
of the world, we seek to publish our findings. The
result is a vast and ever-expanding body of natural
language text that is becoming increasingly difficult
to leverage. This is particularly true in the context
of life sciences, where large quantities of biomedi-
cal articles are published on a daily basis. To sup-
port tasks such data mining, search and visualiza-
tion, there is a clear need for structured representa-
tions of the knowledge these articles convey. This is
1
indicated by a large number of public databases with
content ranging from simple protein-protein interac-
tions to complex pathways. To increase coverage of
such databases, and to keep up with the rate of pub-
lishing, we need to automatically extract structured
representations from biomedical text—a process of-
ten referred to as biomedical text mining.
One major focus of biomedical text mining has
been the extraction of named entities, such genes
or gene products, and of flat binary relations be-
tween such entities, such as protein-protein interac-
tions. However, in recent years there has also been
an increasing interest in the extraction of biomedi-
cal events and their causal relations. This gave rise
to the BioNLP 2009 and 2011 shared tasks which
challenged participants to gather such events from
biomedical text (Kim et al., 2009; Kim et al., 2011).
Notably, these events can be complex and recursive:
they may have several arguments, and some of the
arguments may be events themselves.
Current state-of-the-art event extractors fol-
low the same architectural blueprint and divide
the extraction process into a pipeline of three
stages (Björne et al., 2009; Miwa et al., 2010c). First
they predict a set of candidate event trigger words
(say, tokens 2, 5 and 6 in figure 1), then argument
mentions are attached to these triggers (say, token
4 for trigger 2). The final stage decides how ar-
guments are shared between events—compare how
one event subsumes all arguments of trigger 6 in fig-
ure 1, while two events share the three arguments
of trigger 4 in figure 2. This architecture is prone
to cascading errors: If we miss a trigger in the first
stage, we will never be able to extract the full event
</bodyText>
<note confidence="0.952832">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1–12,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<figure confidence="0.999792666666667">
(a)
E1:Phosphorylation Cause E2:Regulation Theme E3:Binding
Theme
Theme
Theme
1 2 3 4 5 6 7 8 9 10 11
</figure>
<figureCaption confidence="0.993266">
Figure 1: (a) sentence with target event structure to extract; (b) projection to a set of labelled graph over tokens.
</figureCaption>
<figure confidence="0.981718615384615">
Phosphorylation
e2,PhoB.
Regulation Binding
(b)
a6,9,Theme
Theme
Theme
b4,9
Theme
Theme
Same Binding
Cause
... the phosphorylation of TRAF2 inhibits binding to the CD40 cytoplasmic domain ...
</figure>
<bodyText confidence="0.998046797297298">
it concerns. Current systems attempt to tackle this
problem by passing several candidates to the next
stage. However, this tends to increase the false pos-
itive rate. In fact, Miwa et al. (2010c) observe that
30% of their errors stem from this type of ad-hoc
module communication.
Joint models have been proposed to overcome this
problem (Poon and Vanderwende, 2010; Riedel et
al., 2009). However, besides not being as accurate
as their pipelined competitors, mostly because they
do not yet exploit the rich set of features used by
Miwa et al. (2010b) and Björne et al. (2009), they
also suffer from the complexity of inference. For
example, to remain tractable, the best joint system
so far (Poon and Vanderwende, 2010) works with
a simplified representation of the problem in which
certain features are harder to capture, employs local
search without certificates of optimality, and further-
more requires a 32-core cluster for quick train-test
cycles. Existing joint models also rely on heuristics
when it comes to deciding which arguments share
the same event. Contrast this with the best current
pipeline (Miwa et al., 2010c; Miwa et al., 2010b)
which uses a classifier for this task.
We present a family of event extraction mod-
els that address the aforementioned problems. The
first model jointly predicts triggers and arguments.
Notably, the highest scoring event structure under
this model can be found efficiently in O (mn) time
where m is the number of trigger candidates, and
n the number of argument candidates. This is
only slightly slower than the O (m&apos;n) runtime of a
pipeline, where m&apos; is the number of trigger candi-
dates as filtered by the first stage. We achieve these
guarantees through a novel algorithm that jointly
picks best trigger label and arguments on a per-token
basis. Remarkably, it takes roughly as much time to
train this model on one core as the model of Poon
and Vanderwende (2010) on 32 cores, and leads to
better results.
The second model enforces additional constraints
that ensure consistency between events in hierarchi-
cal regulation structures. While inference in this
model is more complicated, we show how dual de-
composition (Komodakis et al., 2007; Rush et al.,
2010) can be used to efficiently find exact solutions
for a large fraction of problems.
Our third model includes the first two, and explic-
itly captures which arguments are part in the same
event—the third stage of existing pipelines. Due to
a complex coupling between this model and the first
two, inference here requires a projected version of
the sub-gradient technique demonstrated by Rush et
al. (2010).
When evaluated on the BioNLP 2009 shared task,
the first two models outperform the previous best
joint approaches and are competitive when com-
pared to current state-of-the-art. With 57.4 F1 on
the test set, the third model yields the best results
reported so far with a 1.1 F1 margin to the results
of Miwa et al. (2010b). For the BioNLP 2011 Ge-
nia task 1 and the BioNLP 2011 Infectious Diseases
task, Model 3 yields the second-best and best results
reported so far. The second-best results are achieved
with Model 3 as is (Riedel and McCallum, 2011),
the best results when using Stanford event predic-
tions as input features (Riedel et al., 2011). The
margins between Model 3 and the best runner-ups
range from 1.9 F1 to 2.8 F1.
In the following we will first introduce biomedical
event extraction and our notation. Then we go on to
present our models and their inference routines. We
present related work, show our empirical evaluation,
and conclude.
</bodyText>
<page confidence="0.988695">
2
</page>
<subsectionHeader confidence="0.54226">
Binding Binding
</subsectionHeader>
<bodyText confidence="0.425949">
Theme
Theme Theme Theme
</bodyText>
<equation confidence="0.6508795">
Grb2 can be coimmunoprecipitated with Sos1 and Sos2
1 2 3 4 5 6 7 8
</equation>
<bodyText confidence="0.866227">
quickly validate extracted events. For example, the
trigger for event E2 is “inhibit”, as indicated by a
dashed line.
</bodyText>
<figure confidence="0.991560666666667">
Theme
Theme
Theme
</figure>
<figureCaption confidence="0.988935666666667">
Figure 2: Two binding events with identical trigger. The
projection graph does not change even if both events are
merged.
</figureCaption>
<sectionHeader confidence="0.910168" genericHeader="introduction">
2 Biomedical Event Extraction
</sectionHeader>
<bodyText confidence="0.999811378378378">
By bio-molecular event we mean a change of state
of one or more bio-molecules. Our task is to extract
structured information about such events from nat-
ural language text. More concretely, let us consider
part (a) of figure 1. We see a snippet of text from a
biomedical abstract, and the three events that can be
extracted from it. We will use these to characterize
the types of events we ought to extract, as defined
by the 2009 BioNLP shared task. Note that for the
shared task, protein mentions are given by the task
organizers and hence do not need to be extracted.
The event E1 in the figure refers to a Phosphory-
lation of the TRAF2 protein. It is an instance of a
set of simple events that describe changes to a sin-
gle gene or gene product. Other members of this
set are: Expression, Transcription, Localization, and
Catabolism. Each of these events has to have exactly
one theme, the protein of which a state change is de-
scribed. A labelled edge in figure 1a) shows that
TRAF2 is the theme of E1.
Event E3 is a Binding of TRAF2 and CD40.
Binding events are particular in that they may have
more than one theme, as there can be several bio-
molecules associated in a binding structure. This is
in fact the case for E3.
In the top-center of figure 1a) we see the Regu-
lation event E2. Such events describe regulatory or
causal relations between events. Other instances of
this type of events are: Positive Regulation and Neg-
ative Regulation. Regulations have to have exactly
one theme; this theme can a be protein or, as in our
case, another event. Regulations may also have zero
or one cause arguments that denote events or pro-
teins which trigger the regulation.
In the BioNLP shared task, we are also asked to
find a trigger (or clue) token for each event. This
token grounds the event in text and allows users to
</bodyText>
<subsectionHeader confidence="0.990915">
2.1 Event Projection
</subsectionHeader>
<bodyText confidence="0.999598166666667">
To formulate the search for event structures of the
form shown in figure 1a) as an optimization prob-
lem, it will be convenient to represent them through
a set of binary variables. We introduce such a rep-
resentation, inspired by previous work (Riedel et al.,
2009; Björne et al., 2009) and based on a projection
of events to a graph structure over tokens, as seen
figure 1b).
Consider sentence x and a set of candidate trig-
ger tokens, denoted by Trig (x). We label each can-
didate i with the event type it is a trigger for, or
None if it is not a trigger. This decision is rep-
resented through a set of binary variables ei,t, one
for each possible event type t. In our example we
have e6,Binding = 1. The set of possible event types
will be denoted as T, the regulation event types as
TReg def = {PosReg, NegReg, Reg} and its complement
as T¬reg def = T \ TReg.
For each candidate trigger i we consider the argu-
ments of all events that have i as trigger. Each ar-
gument a will either be an event itself, or a protein.
For events we add a labelled edge between i and the
trigger j of a. For proteins we add an edge between
i and the syntactic head j of the protein mention. In
both cases we label the edge i → j with the role
of the argument a. The edge is represented through
a binary variable ai,j,r, where r E R is the argu-
ment role and R def= {Theme, Cause, None}. The
role None is active whenever no Theme or Cause
role is present. In our example we get, among oth-
ers, a2,4,Theme = 1.
So far our representation is equivalent to map-
pings in previous work (Riedel et al., 2009; Björne et
al., 2009) and hence shares their main shortcoming:
we cannot differentiate between two (or more) bind-
ing events with the same trigger but different argu-
ments, or one binding event with several arguments.
Consider, for example, the arguments of trigger 6 in
figure 1b) that are all subsumed in a single event. By
contrast, the arguments of trigger 4 shown in figure
2 are split between two events.
Previous work has resolved this ambiguity
</bodyText>
<page confidence="0.994989">
3
</page>
<bodyText confidence="0.996494216216216">
through ad-hoc rules (Björne et al., 2009) or with
a post-processing classifier (Miwa et al., 2010c).
We propose to augment the graph representation
through edges between pairs of proteins that are
themes in the same binding event. For two protein
tokens p and q we represent this edge through the
binary variable bp,q. Hence, in figure 1b) we have
b4,9 = 1, whereas for figure 2 we get b1,6 = b1,8 = 1
but b6,8 = 0. By explicitly modeling such “sib-
ling” edges we not only minimize the need for post-
processing. We can also improve attachment deci-
sions akin to second order models in dependency
parsing (McDonald and Pereira, 2006). Note that
while merely introducing such variables is easy, en-
forcing consistency between them and the ei,t and
ai,j,r variables is not. We address this in section
3.3.1.
Reconstruction of events from solutions (e, a, b)
can be done almost exactly as described by Björne
et al. (2009). However, while they group binding
arguments according to ad-hoc rules based on de-
pendency paths from trigger to argument, we simply
query the variables bp,q.
To simplify our exposition we introduce addi-
tional notation. We denote the set of protein head
tokens with Prot (x); the set of a possible targets
for outgoing edges from a trigger is Cand(x) def =
Trig (x) ∪ Prot (x). We will often omit the do-
mains of indices and instead assign them a fixed do-
main in advance: i, l ∈ Trig (x), j, k ∈ Cand (x),
p, q ∈ Prot (x), r ∈ R and t ∈ T . Bold face
letters are used to denote composite vectors e, a
and b of variables ei,t, ai,j,r and bp,q. The vector
y is the joint vector of e, a and b. The short-form
ei ← t will mean ∀t&apos; : ei,t, ← δt,t, where δt,t, is
the Kronecker Delta. Likewise, ai,j ← r means
∀r&apos; : ai,j,r, ← δr,r,.
</bodyText>
<sectionHeader confidence="0.988515" genericHeader="method">
3 Models
</sectionHeader>
<bodyText confidence="0.999739">
In this section we will present three structured pre-
diction models of increasing complexity and expres-
siveness, as well as their corresponding MAP infer-
ence algorithms. Each model m can be represented
by a mapping from sentence x to a set of legal struc-
tures Ym (x), and a linear scoring function
</bodyText>
<equation confidence="0.907675">
sm (y; x, w) = hw, f (y, x)i . (1)
</equation>
<bodyText confidence="0.9988102">
Here f is a feature function on structures y and input
x, and w is a weight vector for these features.
We can use the scoring function sm and the set of
legal structures Ym (x) to predict the event hm (x)
for a given sentence x according to
</bodyText>
<equation confidence="0.894458">
hm (x) def = arg max sm (y; x, w) . (2)
YEYm(X)
</equation>
<bodyText confidence="0.999261">
For brevity we will from now on omit observations x
and weights w when they are clear from the context.
</bodyText>
<subsectionHeader confidence="0.999572">
3.1 Model 1
</subsectionHeader>
<bodyText confidence="0.999886">
Model 1 performs a simple version of joint trigger
and argument extraction. It independently scores
trigger labels and argument roles:
</bodyText>
<equation confidence="0.9770055">
�
def
s1 (e, a) =
ei,t=1
</equation>
<bodyText confidence="0.985589772727273">
Here sT (i, t) = hwT, fT (i, t)i is a per-trigger scor-
ing function that measures how well the event la-
bel t fits to token i. Likewise, sR (i, j, r) =
hwR, fR (i, j, r)i measures the compatibility of role
r as label for the edge i → j.
The jointness of Model 1 stems from enforcing
consistency between the trigger label of i and its out-
going edges. By consistency we mean that: (a) there
is at least one Theme whenever there is an event at i;
(b) only regulation events are allowed to have Cause
arguments; (c) all arguments of a None trigger must
have the None role. We will denote the set assign-
ments that fulfill these constraints by O and hence
def
have Y1 = O.
Enforcing (e, a) ∈ O guarantees that we never
predict triggers i for which no sensible, high-
scoring, argument j can be found. It also ensures
that when we see an “obvious” argument edge i r→ j
with high score sR (i, j, r) there is pressure to extract
a trigger at i, even if the fact that i is a trigger may
not be as obvious.
</bodyText>
<sectionHeader confidence="0.656593" genericHeader="method">
3.1.1 Inference
</sectionHeader>
<bodyText confidence="0.999948428571429">
As it turns out, the maximizer of equation 2 can be
found very efficiently in O (mn) time where m =
|Trig (x) |and n = |Cand (x)|. The corresponding
procedure, bestOut(·), is shown in algorithm 1. It
takes as input a vector of trigger and edge penalties
c that are added to the local scores of the sT and
sR functions. For Model 2 and 3 we will use these
</bodyText>
<equation confidence="0.9907755">
sT (i, t) + � sR (i, j, r) . (3)
ai,j,r=1
</equation>
<page confidence="0.956872">
4
</page>
<bodyText confidence="0.999899083333333">
penalties to enforce agreement with predictions of
other inference subroutines. When using Model 1
by itself we set them to 0. We point out that the
scoring function s1 is multiplied with 21 throughout
the algorithm. For doing inference in Model 1 and
2 this has no effect, but when we use bestOut(·) for
Model 3 inference, it is required.
The bestOut (c) routine exploits the fact that the
constraints of Model 1 only act on the label for
trigger i and its outgoing edges. In particular, en-
forcing consistency between ei,t and outgoing edges
ai,j,r has no effect on consistency between el,t and
ai0,j0,r0 for any other trigger it =6 i. Moreover,
for a given trigger the constraints only differenti-
ate between three cases: (a) regulation event, (b)
non-regulation event and (c) no event. This means
that we can extract events on a per-trigger basis,
and find the best per-trigger structure by compar-
ing cases (a), (b) and (c). Note that bestOut (c)
uses the shorthand emptyOut (i) to denote the par-
tial assignment ei ← None and ∀j : ai,j ← None.
= The function sc1 (i, y)def Et ei,t (ci,t + 12 sT (i, t)) +
Ej,r ai j,r (ci j r + 1 2 sR (i, j, r)) is a per-trigger
frame score with penalties c.
</bodyText>
<subsectionHeader confidence="0.998718">
3.2 Model 2
</subsectionHeader>
<bodyText confidence="0.999988">
Model 1 may still predict structures that cannot be
mapped to events. For example, in figure 1b) we
may label token 5 as Regulation, add the edge
</bodyText>
<sectionHeader confidence="0.736931" genericHeader="method">
5 Cause
</sectionHeader>
<bodyText confidence="0.999678181818182">
→ 2 but fail to label token 2 as an event. While
consistent with (e, a) ∈ O, this violates the con-
straint that every active edge must either end at a
protein, or at an active event trigger. This is a re-
quirement on the label of a trigger and the assign-
ment of roles for its incoming edges.
Model 2 enforces the above constraint in addition
to (e, a) ∈ O, while inheriting the scoring function
from Model 1. Hence, using I to denote the set of as-
signments with consistent trigger labels and incom-
ing edges, we get Y2 def= Y1 ∩ I and s2 (y) def= s1 (y).
</bodyText>
<sectionHeader confidence="0.911667" genericHeader="method">
3.2.1 Inference
</sectionHeader>
<bodyText confidence="0.976780444444444">
Inference in Model 2 amounts to optimizing
s2 (e, a) over O ∩ I. This is more involved, as we
now have to ensure that when predicting an outgoing
edge from trigger i to trigger l there is a high-scoring
event at l. We follow Rush et al. (2010) and solve
this problem in the framework of dual decomposi-
Algorithm 1 Sub-procedures for inference in Model
1, 2 and 3.
best label and outgoing edges for all triggers under penalties c
</bodyText>
<equation confidence="0.983624925925926">
bestOut (c) :
∀i yc ← emptyOut (i)
y1 ← out (i, c, Treg, R)
y2 ← out (i, c, T�reg, R \ {Cause})
yi ← arg maxyE{y0,y1,y2} sc1(i, y)
return (yi)i
best label and incoming edges for all triggers under penalties c
bestIn (c) :
∀l yc ← emptyIn (l)
y1 ← in (l, c, T , R \ {None})
yl ← arg maxyE{y0,y1} sc2 (l, y)
return (yl)l
pick best binding pairs p, q and trigger i for each using penalties c
bestBind (c) :
∀p, q bp,q ← [sB (p, q) + maxi ci,p,q &gt; 0]
Ip,q ←{i|ci,p,q = maxi0 ci0,p,q l
if bp,q = 1 or maxi0 ci0,p,q &gt; 0
∀i : ti,p,q ←[i ∈ Ip,q] |Ip,q|−1
else
∀i : ti,p,q ← 0
return (b, t)
best label in T and outgoing edge roles in R for i, using penalties c
out (i, c, T, R) :
ei ← arg maxtET 12sT (i, t) + ci,t
ai,bestTheme(i,c) ← Theme
∀j ai,j ← arg maxrER 12sR (i, j, r) + ci,j,r
return (ei, ai)
</equation>
<bodyText confidence="0.6187495">
best label in T, incoming edge roles in R
and outgoing protein roles, using costs c
</bodyText>
<equation confidence="0.9003375">
in (l, c, T, R) :
el ← arg maxtET 2sT (l, t) + cl,t
1
∀i ai,l ← arg maxrER 121sR (i, l, r) + ci,l,r
∀p al,p ← arg maxrER 2sR (l, p, r) + cl,p,r
return (ei, ai)
best Theme argument for i
bestTheme (i, c) :
s (j) def = maxj,r 12sR (i, j, r) + ci,j,r
Δ (j)
</equation>
<bodyText confidence="0.766562">
= def 1 2sR (i, j, Theme) + ci,j,Theme − s (j)
return arg maxj Δ (j)
</bodyText>
<page confidence="0.952916">
5
</page>
<bodyText confidence="0.999695125">
tion. To this end we write our optimization problem
as
and note that this problem could be solved separately
for e, a and ¯e, a¯ if the coupling constraints e = e¯
and a = a¯ were removed.
M2 is an Integer Linear Program, as variables are
binary and both objective and constraints can be rep-
resented through linear constraints.1 Dual decompo-
sition solves a Linear Programming (LP) relaxation
of M2 (that allows fractional values for all binary
variables) through subgradient descent on a particu-
lar dual of M2. This dual can be derived by intro-
ducing Lagrange multipliers for the coupling con-
straints. Its attractiveness stems from the fact that
calculating the subgradient amounts to solving the
decoupled problems in isolation. If, by design, these
decoupled problems can be solved efficiently, we
can often quickly find the optimal solution to an LP
relaxation of our original problem.
Dual decomposition applied to Model 2 is shown
in algorithm 2. It maintains the dual variables λ
that will appear as local penalties in the subprob-
lems to be solved. The algorithm will try to tune
these variables such that at convergence the coupling
constraints will be fulfilled. This is done by first op-
timizing s2 (e, a) over O and s2 (¯e, ¯a) over I. Now,
whenever there is disagreement between two vari-
ables to be coupled, the corresponding dual param-
eter is shifted, increasing the chance that next time
both models will agree. For example, if in the first
iteration we predict e6,Bind = 1 but ¯e6,Bind = 0, we
set λ6,Bind = −α where α is some stepsize (chosen
according to Koo et al. (2010)). This will decrease
the coefficient for e6,Bind, and increase the coeffi-
cient for ¯e6,Bind. Hence, we have a higher chance of
agreement for this variable in the next iteration.
The algorithm repeats the process described
above until all variables agree, or some predefined
number R of iterations is reached. In the former case
we in fact have the exact solution to the original ILP.
</bodyText>
<footnote confidence="0.835726">
1The ILP representation could be taken from the MLNs of
Riedel et al. (2009) and the mapping to ILPs of Riedel (2008).
</footnote>
<bodyText confidence="0.2807545">
Algorithm 2 Subgradient descent for Model 2, and
projected subgradient descent for Model 3.
</bodyText>
<equation confidence="0.808649958333333">
require:
R: max. iteration, αt: stepsizes
t 0 [model 2,3] λ 0 [model 2,3] µ 0 [model 3]
repeat
model
2 (e, a) bestOut (λ)
2,3 (¯e, ¯a) bestIn (−λ)
3 (e, a) bestOut (cout (λ, µ))
3 (b, t) bestBind (cbind (µ))
2,3 λi,t λi,t − αt (ei,t − ¯ei,t) 2,3 λi,j,r λi,j,r − αt (ai,j,r − ¯ai,j,r)
h i
3 µi,p,q
trig µtrig
i,p,q − αt (ei,Bind − ti,p,q)
h + i
arg1 arg1 (
3 µi,j,k µi,p,q − αt lai,p,Theme − ti,p,q)
h i+
arg2 arg2
3 µi,p,q µi,p,q − αt (ai,q,Theme − ti,p,q)
+
2,3 t t + 1
until no λ, µ changed or t &gt; R
return (e, a)[model2] or (e, a, b) [model 3]
</equation>
<bodyText confidence="0.993641388888889">
In the later case we have no such guarantee, but find
that in practice the solutions are still of high qual-
ity. Notice that we could still assess the quality of
this approximation by measuring the duality gap be-
tween primal score and the final dual score.
Algorithm 2 for Model 2 requires us to opti-
mize s2 (e, a) over O and s2 (¯e, ¯a) over I. The
former, with added penalties, can be done with
bestOut(c). As the constraint set for I again
decomposes on a per-token basis, solving the
latter problem requires a very similar procedure,
and again O (mn) time. Algorithm 1 shows this
procedure under bestIn(c). It chooses, for each
trigger candidate, the best label and incoming
set of arguments together with the best outgoing
edges to proteins. Adding edges to proteins is
not strictly required, but simplifies our exposition.
Algorithm bestIn(c) requires a per-trigger incoming
</bodyText>
<equation confidence="0.519733">
score: sc2 (l, yl) a�f (Et el,t (cl,t + 2sT (l, t)) +
Pi,r ai,l,r (ci,l,r + 12 sR (2, l, r)) +
</equation>
<bodyText confidence="0.8654775">
Pp r al,p,r (cl,p,r + 2 sR (l, p, r)) . Finally, note 1
that emptyIn (i) not only assigns None as trigger la-
bel of i and to all incoming edges, but also greedily
picks outgoing protein edges (as done within in(·)).
</bodyText>
<equation confidence="0.78629675">
maximize 1 1
e,a,¯e,¯a 2s2 (e, a) + 2s2 (¯e, ¯a)
subject to (e, a) E O n (¯e, ¯a) E In (M2)
e = e¯n a = a¯
</equation>
<page confidence="0.985599">
6
</page>
<subsectionHeader confidence="0.993566">
3.3 Model 3
</subsectionHeader>
<bodyText confidence="0.999738333333333">
Model 2 does not predict the bp,q variables that rep-
resent protein pairs p, q in bindings. Model 3 fixes
this by (a) adding binding variables bp,q into the ob-
jective, and (b) enforcing that the binding assign-
ment b is consistent with the trigger and argument
assignments e and a. We will also enforce that the
same pair of entities p, q cannot be arguments in
more than one event together.
The scoring function for Model 3 is simply
</bodyText>
<equation confidence="0.9890585">
�s3 (e, a, b) def= s2 (e, a, b) + sB (p, q) . (4)
bp,q=1
</equation>
<bodyText confidence="0.99947412">
Here sB (p, q) = hwB, fB (p, q)i is a per-protein-pair
score based on a feature representation of the lexical
and syntactic relation between both protein heads.
Our strategy will be based on enforcing consis-
tency partly through linear constraints which we du-
alize, and partly within our search algorithm. To
this end we first introduce a set of auxiliary binary
variables ti,p,q . When a ti,p,q is active, we enforce
that there is a binding trigger at i with proteins p
and q as Theme arguments. A set of linear con-
straints can be used for this: ei,Bind − ti,p,q ≥ 0,
ai,p,Theme − ti,p,q ≥ 0 and ai,q,Theme − ti,p,q ≥ 0 for
all suitable i, p and q. We denote the set of assign-
ments (e, a, t) that fulfill these constraints by T.
Consistency between e, a and b can now be en-
forced by making sure that t is consistent with e and
a, and that b is consistent with this t. The latter
means that an active bp,q requires a trigger i to point
to p and q. Or in other words, ti,p,q = 1 for exactly
one trigger i.
With the set of consistent assignments (b, t) re-
ferred to as B, and a slight abuse of notation, this
gives us Y3 def = Y2∩T∩B. Note that it is (e, a, t) ∈ T
that will be enforced by dualizing constraints, and
(b, t) ∈ B that will be enforced within search.
</bodyText>
<sectionHeader confidence="0.780629" genericHeader="method">
3.3.1 Inference
</sectionHeader>
<bodyText confidence="0.893509666666667">
We note that inference in Model 3 can be per-
formed by solving the following problem:
maximize 1 s1 (e, a) + 1 s2 (¯e, ¯a) +
</bodyText>
<equation confidence="0.9849386">
e,a,¯e,¯a,b,t 2 2
bp,q=1
subject to (e, a) ∈ O ∧ (¯e, ¯a) ∈ I ∧ (b, t) ∈ B∧
e = e¯∧ a = a¯∧ (e,a,t) ∈ T.
(M3)
</equation>
<bodyText confidence="0.999411266666667">
Again, without the final row, M3 would be separa-
ble. We exploit this by performing dual decompo-
sition with a dual objective that has multipliers A
for the coupling constraints and multipliers µ for the
constraints which enforce (e, a, t) ∈ T. The result-
ing subgradient descent method is also shown in al-
gorithm 2. Notably, since the constraints for T are
inequalities, we require a projected version of the
descent algorithm which enforces µ ≥ 0. This man-
ifests itself when µ is updated using the [·]+ projec-
tion.
We have already described how to find the best
e, a and ¯e, a¯ assignments. What changes for Model
3 is the derivation of the penalties for e and a
that now come from both A and µ. We set
</bodyText>
<equation confidence="0.9816946">
( def tri
ci, tt A, µ) = Ai,t + δt,Bind Ep,q µi,P q. For j ∈/
Prot (x) we set cout i,j,r (A, µ) def= Ai,j,r; otherwise we
use cout
i,j,r (A, µ) def Ai,j,r + Ep 4 1 + Eq µi,q j.
</equation>
<bodyText confidence="0.999201294117647">
For finding a (b, t) ∈ B that maximizes
Ebp,q=1 sB (p, q) we use bestBind (c), as shown in
algorithm 1. It groups together two proteins p, q if
their score plus the penalty of the best possible trig-
ger i exceeds 0. In this case, or if there is at least one
trigger with positive penalty ci,p,q &gt; 0 , we activate
the set of triggers I (p, q) with maximal score.
Note that when several triggers i maximize the
score, we assign them all the same fractional value
|I (p, q)|−1. This enforces the constraint that at most
one binding event can point to both p and q and also
means that we are solving an LP relaxation. We
could enforce integer solutions and pick arbitrary
triggers at a tie, but this would lower the chances
of matching against predictions of other routines.
The penalties for bestBind (c) are derived from
the dual µ by setting cbind
</bodyText>
<equation confidence="0.9596576">
i,p,q (µ) = −µtrig
i,p,q − µarg1
i,p,q −
arg2
µi,,p,q.
</equation>
<subsectionHeader confidence="0.963523">
3.4 Training
</subsectionHeader>
<bodyText confidence="0.999897">
We choose prediction-based passive-aggressive (PA)
online learning (Crammer and Singer, 2003) with
averaging to estimate the weights w for each of our
models. PA is an error-driven learner that shifts
weights towards features of the gold solution, and
away from features of the current guess, whenever
the current model makes a mistake.
PA learning takes into account a user-defined
loss function for which we use a weighted sum
</bodyText>
<equation confidence="0.9077">
sB (p, q)
</equation>
<page confidence="0.987366">
7
</page>
<bodyText confidence="0.9977976">
of false positives and false negatives: l (y, y&apos;) def =
FP (y, y&apos;) + αFN (y, y&apos;). We set α = 3.8 by op-
timizing on the BioNLP 2009 development set.
guments; such compatibilities have also been shown
to be helpful in SRL (Toutanova et al., 2005).
</bodyText>
<sectionHeader confidence="0.99984" genericHeader="evaluation">
5 Experiments
4 Related Work
</sectionHeader>
<bodyText confidence="0.999992276595745">
Riedel et al. (2009) use Integer Linear Programming
and cutting planes (Riedel, 2008) for inference in
a model similar to Model 2. By using dual de-
composition instead, we can exploit tractable sub-
structure and achieve quadratic (Model 2) and cu-
bic (Model 3) runtime guarantees. An advantage of
ILP inference are guaranteed certificates of optimal-
ity. However, in practice we also gain certificates
of optimality for a large fraction of the instances
we process. Poon and Vanderwende (2010) use lo-
cal search and hence provide no such certificates.
Their problem formulation also makes n-gram de-
pendency path features harder to incorporate. Mc-
Closky et al. (2011b) cast event extraction as depen-
dency parsing task. Their model assumes that event
structures are trees, an assumption that is frequently
violated in practice. Finally, all previous joint ap-
proaches use heuristics to decide whether binding
arguments are part of the same event, while we cap-
ture these decisions in the joint model.
We follow a long line of research in NLP that ad-
dresses search problems using (Integer) Linear Pro-
grams (Germann et al., 2001; Roth and Yih, 2004;
Riedel and Clarke, 2006). However, instead of us-
ing off-the-shelf solvers, we work in the framework
of dual decomposition. Here we extend the approach
of Rush et al. (2010) in that in addition to equality
constraints we dualize more complex coupling con-
straints between models. This requires us to work
with a projected version of subgradient descent.
While tailored towards (biomedical) event extrac-
tion, we believe that our models can also be ef-
fective in a more general Semantic Role Label-
ing (SRL) context. Using variants of Model 1,
we can enforce many of the SRL constraints—such
as “unique agent” constraints (Punyakanok et al.,
2004)—without having to call out to ILP optimiz-
ers. Meza-Ruiz and Riedel (2009) showed that in-
ducing pressure on arguments to be attached to at
least one predicate is helpful; this is a soft incoming
edge constraint. Finally, Model 3 can be used to effi-
ciently capture compatibilities between semantic ar-
We evaluate our models on several tracks of the 2009
and 2011 BioNLP shared tasks, using the official
“Approximate Span Matching/Approximate Recur-
sive Matching” F1 metric for each. We also investi-
gate the runtime behavior of our algorithms.
</bodyText>
<subsectionHeader confidence="0.998707">
5.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999803666666667">
Each document is first processed by the Stanford
CoreNLP2 tokenizer and sentence splitter. Parse
trees come from the Charniak-Johnson parser (Char-
niak and Johnson, 2005) with a self-trained biomed-
ical parsing model (McClosky and Charniak, 2008),
and are converted to dependency structures again us-
ing Stanford CoreNLP. Based on trigger words col-
lected from the training set, a set of candidate trigger
tokens Trig (x) is generated for each sentence x.
</bodyText>
<subsectionHeader confidence="0.991656">
5.2 Features
</subsectionHeader>
<bodyText confidence="0.999960777777778">
The feature function fT (i, t) extracts a per-trigger
feature vector for trigger i and type t E T .
It creates one active feature for each element in
{t, t E TRegI x feats (i). Here feats (i) denotes a
collection of representations for the token i: word-
form, lemma, POS tag, syntactic heads, syntactic
children, and membership in two dictionaries taken
from Riedel et al. (2009).
For fR (i, j, r) we create active features for each
element of {r} x feats (i, j). Here feats (i, j) is
a collection of representations of the token pair
(i, j) taken from Miwa et al. (2010c) and contains:
labelled and unlabeled n-gram dependency paths;
edge and vertex walk features, argument and trigger
modifiers and heads, words in between.
For fB (p, q) we re-use the token pair representa-
tions from fR. In particular, we create one active
feature for each element in feats (p, q).
</bodyText>
<subsectionHeader confidence="0.983146">
5.3 Shared Task 2009
</subsectionHeader>
<bodyText confidence="0.5439915">
We first evaluate our models on the Bionlp 2009 task
1. The training, development and test sets for this
</bodyText>
<footnote confidence="0.9866855">
2http://nlp.stanford.edu/software/
corenlp.shtml
</footnote>
<page confidence="0.998603">
8
</page>
<tableCaption confidence="0.977698">
Table 1: F1 scores for the development set of Task 1 of
</tableCaption>
<table confidence="0.996161777777778">
the BioNLP 2009 shared task.
SVT BIND REG TOT
McClosky 75.4 48.4 40.4 53.5
Poon 77.5 47.9 44.1 55.5
Bjoerne 77.9 42.2 45.5 55.7
Miwa 78.6 46.9 47.7 57.8
M1 77.2 43.0 45.8 56.2
M2 77.9 42.4 47.6 57.2
M3 78.4 48.0 49.1 58.7
SVT BIND REG TOT
McClosky 68.3 46.9 33.3 48.6
Poon 69.5 42.5 37.5 50.0
Bjoerne 70.2 44.4 40.1 52.0
Miwa 72.1 50.6 45.3 56.3
M1 71.0 42.1 41.9 53.4
M2 70.5 41.3 43.6 53.7
M3 71.1 52.9 45.2 55.8
M3+enju 72.6 52.6 46.9 57.4
</table>
<tableCaption confidence="0.93999">
Table 2: F1 scores for the test set of Task 1 of the BioNLP
2009 shared task.
</tableCaption>
<bodyText confidence="0.999717920634921">
task consist of 797, 150 and 250 documents, respec-
tively.
Table 1 shows our results for the development set.
We compare our three models (M1, M2 and M3) and
previous state-of-the-art systems: McClosky (Mc-
Closky et al., 2011a), Poon (Poon and Vander-
wende, 2010), Bjoerne (Björne et al., 2009) and
Miwa (Miwa et al., 2010b; Miwa et al., 2010a). Pre-
sented is F1 score for all events (TOT), regulation
events (REG), binding events (BIND) and simple
events (SVT).
Model 1 is outperforming the previous best joint
models of Poon and Vanderwende (2010), as well as
the best entry of the 2009 task (Björne et al., 2009).
This is achieved without careful tuning of thresh-
olds that control flow of information between trigger
and argument extraction. Notably, training Model 1
takes approximately 20 minutes using a single core
implementation. Contrast this with 20 minutes on 32
cores reported by Poon and Vanderwende (2010).
Model 2 focuses on regulation structures and re-
sults demonstrate this: F1 for regulations goes up by
nearly 2 points. While the impact of joint modeling
relative to weaker local baselines has been shown
shown by Poon and Vanderwende (2010) and Riedel
et al. (2009), our findings here provide evidence that
it remains effective even when the baseline system
is very competitive.
With Model 3 our focus is extended to binding
events, improving F1 for such events by at least 5 F1.
This also has a positive effect on regulation events,
as regulations of binding events can now be more
accurately extracted. In total we see a 1.1 F1 in-
crease over the best results reported so far (Miwa et
al., 2010b). Crucially, this is achieved using only a
single parse tree per sentence, as opposed to three
used by Miwa et al. (2010a).
Table 2 shows results for the test set. Here with
Model 1 we again already outperform all but the re-
sults of Miwa et al. (2010a). Model 2 improves F1
for regulations, while Model 3 again increases F1
for both regulations and binding events. This yields
the best binding event results reported so far. No-
tably, not only are we able to resolve binding am-
biguity better. Binding attachments themselves also
improve, as we increase attachment F1 from 61.4 to
62.7 when going from Model 2 to Model 3.
Miwa et al. (2010b) use two parsers to generate
their input features. For fairer comparison we aug-
ment Model 3 with syntactic features based on the
enju parser (Miyao et al., 2009). With these features
(M3+enju) we achieve the best results on this dataset
reported so far, and outperform Miwa et al. (2010b)
by 1.1 F1 in total, 1.6 F1 on regulation events and
2.0 F1 on binding events.
We also apply Model 3, with slight modifications,
to the BioNLP 2009 task 2 which requires cellu-
lar locations to be extracted as well. With 53.0 F1
we fall 2 points short of the results of Miwa et al.
(2010b) but still substantially outperform any other
reported results on the dataset. More parse trees may
again substantially improve results, as well as task-
specific constraint and feature sets.
</bodyText>
<subsectionHeader confidence="0.98261">
5.4 Shared Task 2011
</subsectionHeader>
<bodyText confidence="0.9998885">
We entered the Shared Task 2011 with Model 3,
primarily focusing on Genia track (task 1), and the
Infectious Diseases track. The Genia track differs
from the 2009 task by including both abstracts and
full text articles. In total 908 training, 259 develop-
ment and 347 test documents are provided.
</bodyText>
<page confidence="0.988352">
9
</page>
<table confidence="0.999907285714286">
Genia Task 1 TOT Infectious Diseases TOT
System System
M3+Stanford 56.0 M3+Stanford 55.6
M3 55.2 M3 53.4
UTurku 53.3 Stanford 50.6
MSR-NLP 51.5 UTurku 44.2
ConcordU 50.3 PNNL 42.6
</table>
<tableCaption confidence="0.973525">
Table 3: F1 scores for the test sets of two tracks in the
BioNLP 2011 Shared Task.
</tableCaption>
<bodyText confidence="0.999525125">
The top five entries are shown in table 3. Model
3 is the best-performing system that does not use
model combination, only outperformed by a version
of Model 3 that includes Stanford predictions (Mc-
Closky et al., 2011b) as input features (Riedel et al.,
2011). Not shown in the table are results for full pa-
pers only. Here M3 ranks first with 53.1 F1, while
M3+Stanford comes in second with 52.7 F1.
The Infectious Diseases (ID) track of the 2011
task has 152 train, 46 development and 118 test
documents. Relative to Genia it provides less data
and introduces more types of entities as well as
the biological process event type. Incorporating
these changes into our models is straightforward,
and hence we omit details for brevity.
Table 3 shows the top five entries for the Infec-
tious Diseases track. Again Model 3 is the best-
performing system that does not use model combi-
nation, outperformed only by Model 3 with Stanford
predictions as features. We should point out that
the feature sets and learning parameters were kept
constant when moving from Genia to ID data. The
strong results we observe without any tuning to the
domain indicate the robustness of joint modeling.
</bodyText>
<subsectionHeader confidence="0.989061">
5.5 Runtime Behavior
</subsectionHeader>
<bodyText confidence="0.999874777777778">
Table 4 shows the asymptotic complexity of our
three models with respect to m = |Trig (x)|, n =
|Cand (x) |and p = |Prot (x)|. We also show the
number of iterations needed on average, the average
time in milliseconds per sentence,3 and the fraction
of sentences we get certificates of optimality for.
As expected, Model 1 is most efficient, both
asymptotically and on average. Given that its ac-
curacy is already good, it can serve as a basis for
</bodyText>
<footnote confidence="0.857802">
3Measured without preprocessing and feature extraction.
</footnote>
<table confidence="0.966708">
Complexity Iter. Time Exact
M1 O (nm) 1.0 60ms 100%
M2 O (Rnm) 10.4 183ms 96%
M3 O (Rnm + Rp2m) 11.7 297ms 94%
</table>
<tableCaption confidence="0.999571">
Table 4: Complexity and Runtime Behavior.
</tableCaption>
<bodyText confidence="0.999539571428572">
large-scale extraction tasks. Models 2 and 3 re-
quire several iterations and more time, while pro-
viding slightly less certificates. However, given the
improvement in F1 they deliver, and the fact prepro-
cessing steps such as parsing would still dominate
the average time, this seems like a reasonable price
to pay.
</bodyText>
<sectionHeader confidence="0.999439" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999994444444445">
We presented three joint models for biomedical
event extraction. Model 1 reaches near-state-of-the-
art results, outperforms all previous joint models
and has quadratic runtime guarantees. By explicitly
capturing regulation events (Model 2), and binding
events (Model 3) we achieve the best results reported
so far on several event extraction tasks. The runtime
penalty we pay is kept minimal by using dual de-
composition. We also show how dual decomposition
can be used for constraints that go beyond coupling
equalities.
We use joint models, a decomposition technique
and supervised online learning. This recipe can be
successful in many settings, but requires expensive
manual annotation. In the future we want to inte-
grate weak supervision techniques to train extractors
with existing biomedical databases, such as KEGG,
and only minimal amounts of annotated text.
</bodyText>
<sectionHeader confidence="0.994985" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999633545454545">
This work was supported in part by the Center
for Intelligent Information Retrieval. The Univer-
sity of Massachusetts gratefully acknowledges the
support of Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program under
Air Force Research Laboratory (AFRL) prime con-
tract no. FA8750-09-C-0181. Any opinions, find-
ings, and conclusion or recommendations expressed
in this material are those of the authors and do not
necessarily reflect the view of the DARPA, AFRL,
or the US government.
</bodyText>
<page confidence="0.997991">
10
</page>
<sectionHeader confidence="0.982626" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998727528301887">
Jari Björne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Extract-
ing complex biological events with rich graph-based
feature sets. In Proceedings of the Natural Language
Processing in Biomedicine NAACL 2009 Workshop
(BioNLP ’09), pages 10–18, Morristown, NJ, USA.
Association for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL ’05),
pages 173–180.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. Jour-
nal of Machine Learning Research, 3:951–991.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast decoding and
optimal decoding for machine translation. In Proceed-
ings of the 39th Annual Meeting of the Association for
Computational Linguistics (ACL ’01), pages 228–235.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun’ichi Tsujii. 2009. Overview
of bionlp’09 shared task on event extraction. In
Proceedings of the Natural Language Processing in
Biomedicine NAACL 2009 Workshop (BioNLP ’09).
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun’ichi Tsujii. 2011. Overview of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Nikos Komodakis, Nikos Paragios, and Georgios Tziri-
tas. 2007. Mrf optimization via dual decomposition:
Message-passing revisited. In Proceedings of the 11st
IEEE International Conference on Computer Vision
(ICCV ’07).
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decomposi-
tion for parsing with nonprojective head automata. In
Proceedings of the Conference on Empirical methods
in natural language processing (EMNLP ’10).
David McClosky and Eugene Charniak. 2008. Self-
training for biomedical parsing. In Proceedings of the
46th Annual Meeting of the Association for Computa-
tional Linguistics (ACL ’08).
David McClosky, Mihai Surdeanu, and Chris Manning.
2011a. Event extraction as dependency parsing. In
Proceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL ’11), Port-
land, Oregon, June.
David McClosky, Mihai Surdeanu, and Christopher D.
Manning. 2011b. Event extraction as dependency
parsing in bionlp 2011. In BioNLP 2011 Shared Task.
R. McDonald and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proceedings of the 11th Conference of the European
Chapter of the ACL (EACL ’06), pages 81–88.
Ivan Meza-Ruiz and Sebastian Riedel. 2009. Jointly
identifying predicates, arguments and senses using
markov logic. In Joint Human Language Technol-
ogy Conference/Annual Meeting of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics (HLT-NAACL ’09).
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun’ichi Tsujii. 2010a. A comparative study of syn-
tactic parsers for event extraction. In Proceedings of
the 2010 Workshop on Biomedical Natural Language
Processing, BioNLP ’10, pages 37–45, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun’ichi Tsujii. 2010b. Evaluating dependency rep-
resentation for event extraction. In Proceedings of the
23rd International Conference on Computational Lin-
guistics, COLING ’10, pages 779–787, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Makoto Miwa, Rune Saetre, Jin-Dong D. Kim, and
Jun’ichi Tsujii. 2010c. Event extraction with com-
plex event classification using rich features. Journal of
bioinformatics and computational biology, 8(1):131–
146, February.
Yusuke Miyao, Kenji Sagae, Rune Sætre, Takuya Mat-
suzaki, and Jun ichi Tsujii. 2009. Evaluating contribu-
tions of natural language parsers to protein-protein in-
teraction extraction. Bioinformatics/computer Appli-
cations in The Biosciences, 25:394–400.
Hoifung Poon and Lucy Vanderwende. 2010. Joint Infer-
ence for Knowledge Extraction from Biomedical Lit-
erature. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
813–821, Los Angeles, California, June. Association
for Computational Linguistics.
Vasin Punyakanok, Dan Roth, Wen tau Yih, and Dav Zi-
mak. 2004. Semantic role labeling via integer linear
programming inference. In Proceedings of the 20th in-
ternational conference on Computational Linguistics
(COLING ’04), pages 1346–1352, Morristown, NJ,
USA. Association for Computational Linguistics.
Sebastian Riedel and James Clarke. 2006. Incremen-
tal integer linear programming for non-projective de-
pendency parsing. In Proceedings of the Conference
on Empirical methods in natural language processing
(EMNLP ’06), pages 129–137.
Sebastian Riedel and Andrew McCallum. 2011. Robust
biomedical event extraction with dual decomposition
and minimal domain adaptation. In Proceedings of the
</reference>
<page confidence="0.990287">
11
</page>
<reference confidence="0.999391121212121">
Natural Language Processing in Biomedicine NAACL
2011 Workshop (BioNLP ’11), June.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun’ichi Tsujii. 2009. A markov logic approach to
bio-molecular event extraction. In Proceedings of the
Natural Language Processing in Biomedicine NAACL
2009 Workshop (BioNLP ’09), pages 41–49.
Sebastian Riedel, David McClosky, Mihai Surdeanu,
Christopher D. Manning, and Andrew McCallum.
2011. Model combination for event extraction in
BioNLP 2011. In Proceedings of the Natural Lan-
guage Processing in Biomedicine NAACL 2011 Work-
shop (BioNLP ’11), June.
Sebastian Riedel. 2008. Improving the accuracy and ef-
ficiency of MAP inference for markov logic. In Pro-
ceedings of the 24th Annual Conference on Uncer-
tainty in AI (UAI ’08), pages 468–475.
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Proceedings of the 8th Conference on Computational
Natural Language Learning (CoNLL’ 04), pages 1–8.
Alexander M. Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition
and linear programming relaxations for natural lan-
guage processing. In Proceedings of the Conference
on Empirical methods in natural language processing
(EMNLP ’10).
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic role
labeling. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics (ACL
’05), pages 589–596, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
</reference>
<page confidence="0.998461">
12
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.322534">
<title confidence="0.999997">Fast and Robust Joint Models for Biomedical Event Extraction</title>
<author confidence="0.999962">Sebastian Riedel Andrew McCallum</author>
<affiliation confidence="0.9974105">Department of Computer Science University of Massachusetts, Amherst</affiliation>
<email confidence="0.999691">riedel@cs.umass.edu</email>
<email confidence="0.999691">mccallum@cs.umass.edu</email>
<abstract confidence="0.977809173913043">Extracting biomedical events from literature has attracted much recent attention. The bestperforming systems so far have been pipelines of simple subtask-specific local classifiers. A natural drawback of such approaches are cascading errors introduced in early stages of the pipeline. We present three joint models of increasing complexity designed to overcome this problem. The first model performs joint trigger and argument extraction, and lends itself to a simple, efficient and exact inference algorithm. The second model captures correlations between events, while the third model ensures consistency between arguments of the same event. Inference in these models is kept tractable through dual decomposition. The first two models outperform the previous best joint approaches and are very competitive with respect to the current state-of-theart. The third model yields the best results reported so far on the BioNLP 2009 shared task, the BioNLP 2011 Genia task and the BioNLP</abstract>
<address confidence="0.404087">2011 Infectious Diseases task.</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jari Björne</author>
<author>Juho Heimonen</author>
<author>Filip Ginter</author>
<author>Antti Airola</author>
<author>Tapio Pahikkala</author>
<author>Tapio Salakoski</author>
</authors>
<title>Extracting complex biological events with rich graph-based feature sets.</title>
<date>2009</date>
<booktitle>In Proceedings of the Natural Language Processing in Biomedicine NAACL 2009 Workshop (BioNLP ’09),</booktitle>
<pages>10--18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2913" citStr="Björne et al., 2009" startWordPosition="448" endWordPosition="451">teractions. However, in recent years there has also been an increasing interest in the extraction of biomedical events and their causal relations. This gave rise to the BioNLP 2009 and 2011 shared tasks which challenged participants to gather such events from biomedical text (Kim et al., 2009; Kim et al., 2011). Notably, these events can be complex and recursive: they may have several arguments, and some of the arguments may be events themselves. Current state-of-the-art event extractors follow the same architectural blueprint and divide the extraction process into a pipeline of three stages (Björne et al., 2009; Miwa et al., 2010c). First they predict a set of candidate event trigger words (say, tokens 2, 5 and 6 in figure 1), then argument mentions are attached to these triggers (say, token 4 for trigger 2). The final stage decides how arguments are shared between events—compare how one event subsumes all arguments of trigger 6 in figure 1, while two events share the three arguments of trigger 4 in figure 2. This architecture is prone to cascading errors: If we miss a trigger in the first stage, we will never be able to extract the full event Proceedings of the 2011 Conference on Empirical Methods </context>
<context position="4636" citStr="Björne et al. (2009)" startWordPosition="737" endWordPosition="740">s binding to the CD40 cytoplasmic domain ... it concerns. Current systems attempt to tackle this problem by passing several candidates to the next stage. However, this tends to increase the false positive rate. In fact, Miwa et al. (2010c) observe that 30% of their errors stem from this type of ad-hoc module communication. Joint models have been proposed to overcome this problem (Poon and Vanderwende, 2010; Riedel et al., 2009). However, besides not being as accurate as their pipelined competitors, mostly because they do not yet exploit the rich set of features used by Miwa et al. (2010b) and Björne et al. (2009), they also suffer from the complexity of inference. For example, to remain tractable, the best joint system so far (Poon and Vanderwende, 2010) works with a simplified representation of the problem in which certain features are harder to capture, employs local search without certificates of optimality, and furthermore requires a 32-core cluster for quick train-test cycles. Existing joint models also rely on heuristics when it comes to deciding which arguments share the same event. Contrast this with the best current pipeline (Miwa et al., 2010c; Miwa et al., 2010b) which uses a classifier for</context>
<context position="10143" citStr="Björne et al., 2009" startWordPosition="1687" endWordPosition="1690">a be protein or, as in our case, another event. Regulations may also have zero or one cause arguments that denote events or proteins which trigger the regulation. In the BioNLP shared task, we are also asked to find a trigger (or clue) token for each event. This token grounds the event in text and allows users to 2.1 Event Projection To formulate the search for event structures of the form shown in figure 1a) as an optimization problem, it will be convenient to represent them through a set of binary variables. We introduce such a representation, inspired by previous work (Riedel et al., 2009; Björne et al., 2009) and based on a projection of events to a graph structure over tokens, as seen figure 1b). Consider sentence x and a set of candidate trigger tokens, denoted by Trig (x). We label each candidate i with the event type it is a trigger for, or None if it is not a trigger. This decision is represented through a set of binary variables ei,t, one for each possible event type t. In our example we have e6,Binding = 1. The set of possible event types will be denoted as T, the regulation event types as TReg def = {PosReg, NegReg, Reg} and its complement as T¬reg def = T \ TReg. For each candidate trigge</context>
<context position="11455" citStr="Björne et al., 2009" startWordPosition="1949" endWordPosition="1952">her be an event itself, or a protein. For events we add a labelled edge between i and the trigger j of a. For proteins we add an edge between i and the syntactic head j of the protein mention. In both cases we label the edge i → j with the role of the argument a. The edge is represented through a binary variable ai,j,r, where r E R is the argument role and R def= {Theme, Cause, None}. The role None is active whenever no Theme or Cause role is present. In our example we get, among others, a2,4,Theme = 1. So far our representation is equivalent to mappings in previous work (Riedel et al., 2009; Björne et al., 2009) and hence shares their main shortcoming: we cannot differentiate between two (or more) binding events with the same trigger but different arguments, or one binding event with several arguments. Consider, for example, the arguments of trigger 6 in figure 1b) that are all subsumed in a single event. By contrast, the arguments of trigger 4 shown in figure 2 are split between two events. Previous work has resolved this ambiguity 3 through ad-hoc rules (Björne et al., 2009) or with a post-processing classifier (Miwa et al., 2010c). We propose to augment the graph representation through edges betwe</context>
<context position="12802" citStr="Björne et al. (2009)" startWordPosition="2182" endWordPosition="2185">he binary variable bp,q. Hence, in figure 1b) we have b4,9 = 1, whereas for figure 2 we get b1,6 = b1,8 = 1 but b6,8 = 0. By explicitly modeling such “sibling” edges we not only minimize the need for postprocessing. We can also improve attachment decisions akin to second order models in dependency parsing (McDonald and Pereira, 2006). Note that while merely introducing such variables is easy, enforcing consistency between them and the ei,t and ai,j,r variables is not. We address this in section 3.3.1. Reconstruction of events from solutions (e, a, b) can be done almost exactly as described by Björne et al. (2009). However, while they group binding arguments according to ad-hoc rules based on dependency paths from trigger to argument, we simply query the variables bp,q. To simplify our exposition we introduce additional notation. We denote the set of protein head tokens with Prot (x); the set of a possible targets for outgoing edges from a trigger is Cand(x) def = Trig (x) ∪ Prot (x). We will often omit the domains of indices and instead assign them a fixed domain in advance: i, l ∈ Trig (x), j, k ∈ Cand (x), p, q ∈ Prot (x), r ∈ R and t ∈ T . Bold face letters are used to denote composite vectors e, a</context>
<context position="32765" citStr="Björne et al., 2009" startWordPosition="5962" endWordPosition="5965">6 57.2 M3 78.4 48.0 49.1 58.7 SVT BIND REG TOT McClosky 68.3 46.9 33.3 48.6 Poon 69.5 42.5 37.5 50.0 Bjoerne 70.2 44.4 40.1 52.0 Miwa 72.1 50.6 45.3 56.3 M1 71.0 42.1 41.9 53.4 M2 70.5 41.3 43.6 53.7 M3 71.1 52.9 45.2 55.8 M3+enju 72.6 52.6 46.9 57.4 Table 2: F1 scores for the test set of Task 1 of the BioNLP 2009 shared task. task consist of 797, 150 and 250 documents, respectively. Table 1 shows our results for the development set. We compare our three models (M1, M2 and M3) and previous state-of-the-art systems: McClosky (McClosky et al., 2011a), Poon (Poon and Vanderwende, 2010), Bjoerne (Björne et al., 2009) and Miwa (Miwa et al., 2010b; Miwa et al., 2010a). Presented is F1 score for all events (TOT), regulation events (REG), binding events (BIND) and simple events (SVT). Model 1 is outperforming the previous best joint models of Poon and Vanderwende (2010), as well as the best entry of the 2009 task (Björne et al., 2009). This is achieved without careful tuning of thresholds that control flow of information between trigger and argument extraction. Notably, training Model 1 takes approximately 20 minutes using a single core implementation. Contrast this with 20 minutes on 32 cores reported by Poo</context>
</contexts>
<marker>Björne, Heimonen, Ginter, Airola, Pahikkala, Salakoski, 2009</marker>
<rawString>Jari Björne, Juho Heimonen, Filip Ginter, Antti Airola, Tapio Pahikkala, and Tapio Salakoski. 2009. Extracting complex biological events with rich graph-based feature sets. In Proceedings of the Natural Language Processing in Biomedicine NAACL 2009 Workshop (BioNLP ’09), pages 10–18, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL ’05),</booktitle>
<pages>173--180</pages>
<contexts>
<context position="30557" citStr="Charniak and Johnson, 2005" startWordPosition="5565" endWordPosition="5569">sure on arguments to be attached to at least one predicate is helpful; this is a soft incoming edge constraint. Finally, Model 3 can be used to efficiently capture compatibilities between semantic arWe evaluate our models on several tracks of the 2009 and 2011 BioNLP shared tasks, using the official “Approximate Span Matching/Approximate Recursive Matching” F1 metric for each. We also investigate the runtime behavior of our algorithms. 5.1 Preprocessing Each document is first processed by the Stanford CoreNLP2 tokenizer and sentence splitter. Parse trees come from the Charniak-Johnson parser (Charniak and Johnson, 2005) with a self-trained biomedical parsing model (McClosky and Charniak, 2008), and are converted to dependency structures again using Stanford CoreNLP. Based on trigger words collected from the training set, a set of candidate trigger tokens Trig (x) is generated for each sentence x. 5.2 Features The feature function fT (i, t) extracts a per-trigger feature vector for trigger i and type t E T . It creates one active feature for each element in {t, t E TRegI x feats (i). Here feats (i) denotes a collection of representations for the token i: wordform, lemma, POS tag, syntactic heads, syntactic ch</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and maxent discriminative reranking. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL ’05), pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--951</pages>
<contexts>
<context position="27428" citStr="Crammer and Singer, 2003" startWordPosition="5045" endWordPosition="5048">l triggers i maximize the score, we assign them all the same fractional value |I (p, q)|−1. This enforces the constraint that at most one binding event can point to both p and q and also means that we are solving an LP relaxation. We could enforce integer solutions and pick arbitrary triggers at a tie, but this would lower the chances of matching against predictions of other routines. The penalties for bestBind (c) are derived from the dual µ by setting cbind i,p,q (µ) = −µtrig i,p,q − µarg1 i,p,q − arg2 µi,,p,q. 3.4 Training We choose prediction-based passive-aggressive (PA) online learning (Crammer and Singer, 2003) with averaging to estimate the weights w for each of our models. PA is an error-driven learner that shifts weights towards features of the gold solution, and away from features of the current guess, whenever the current model makes a mistake. PA learning takes into account a user-defined loss function for which we use a weighted sum sB (p, q) 7 of false positives and false negatives: l (y, y&apos;) def = FP (y, y&apos;) + αFN (y, y&apos;). We set α = 3.8 by optimizing on the BioNLP 2009 development set. guments; such compatibilities have also been shown to be helpful in SRL (Toutanova et al., 2005). 5 Exper</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>Koby Crammer and Yoram Singer. 2003. Ultraconservative online algorithms for multiclass problems. Journal of Machine Learning Research, 3:951–991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Kenji Yamada</author>
</authors>
<title>Fast decoding and optimal decoding for machine translation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL ’01),</booktitle>
<pages>228--235</pages>
<contexts>
<context position="29167" citStr="Germann et al., 2001" startWordPosition="5342" endWordPosition="5345">al search and hence provide no such certificates. Their problem formulation also makes n-gram dependency path features harder to incorporate. McClosky et al. (2011b) cast event extraction as dependency parsing task. Their model assumes that event structures are trees, an assumption that is frequently violated in practice. Finally, all previous joint approaches use heuristics to decide whether binding arguments are part of the same event, while we capture these decisions in the joint model. We follow a long line of research in NLP that addresses search problems using (Integer) Linear Programs (Germann et al., 2001; Roth and Yih, 2004; Riedel and Clarke, 2006). However, instead of using off-the-shelf solvers, we work in the framework of dual decomposition. Here we extend the approach of Rush et al. (2010) in that in addition to equality constraints we dualize more complex coupling constraints between models. This requires us to work with a projected version of subgradient descent. While tailored towards (biomedical) event extraction, we believe that our models can also be effective in a more general Semantic Role Labeling (SRL) context. Using variants of Model 1, we can enforce many of the SRL constrain</context>
</contexts>
<marker>Germann, Jahr, Knight, Marcu, Yamada, 2001</marker>
<rawString>Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu, and Kenji Yamada. 2001. Fast decoding and optimal decoding for machine translation. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL ’01), pages 228–235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Tomoko Ohta</author>
<author>Sampo Pyysalo</author>
<author>Yoshinobu Kano</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Overview of bionlp’09 shared task on event extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the Natural Language Processing in Biomedicine NAACL 2009 Workshop (BioNLP ’09).</booktitle>
<contexts>
<context position="2587" citStr="Kim et al., 2009" startWordPosition="397" endWordPosition="400">ed to automatically extract structured representations from biomedical text—a process often referred to as biomedical text mining. One major focus of biomedical text mining has been the extraction of named entities, such genes or gene products, and of flat binary relations between such entities, such as protein-protein interactions. However, in recent years there has also been an increasing interest in the extraction of biomedical events and their causal relations. This gave rise to the BioNLP 2009 and 2011 shared tasks which challenged participants to gather such events from biomedical text (Kim et al., 2009; Kim et al., 2011). Notably, these events can be complex and recursive: they may have several arguments, and some of the arguments may be events themselves. Current state-of-the-art event extractors follow the same architectural blueprint and divide the extraction process into a pipeline of three stages (Björne et al., 2009; Miwa et al., 2010c). First they predict a set of candidate event trigger words (say, tokens 2, 5 and 6 in figure 1), then argument mentions are attached to these triggers (say, token 4 for trigger 2). The final stage decides how arguments are shared between events—compare</context>
</contexts>
<marker>Kim, Ohta, Pyysalo, Kano, Tsujii, 2009</marker>
<rawString>Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshinobu Kano, and Jun’ichi Tsujii. 2009. Overview of bionlp’09 shared task on event extraction. In Proceedings of the Natural Language Processing in Biomedicine NAACL 2009 Workshop (BioNLP ’09).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Sampo Pyysalo</author>
<author>Tomoko Ohta</author>
<author>Robert Bossy</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Overview of BioNLP Shared Task</title>
<date>2011</date>
<booktitle>In Proceedings of the BioNLP 2011 Workshop Companion Volume for Shared Task,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon,</location>
<contexts>
<context position="2606" citStr="Kim et al., 2011" startWordPosition="401" endWordPosition="404">y extract structured representations from biomedical text—a process often referred to as biomedical text mining. One major focus of biomedical text mining has been the extraction of named entities, such genes or gene products, and of flat binary relations between such entities, such as protein-protein interactions. However, in recent years there has also been an increasing interest in the extraction of biomedical events and their causal relations. This gave rise to the BioNLP 2009 and 2011 shared tasks which challenged participants to gather such events from biomedical text (Kim et al., 2009; Kim et al., 2011). Notably, these events can be complex and recursive: they may have several arguments, and some of the arguments may be events themselves. Current state-of-the-art event extractors follow the same architectural blueprint and divide the extraction process into a pipeline of three stages (Björne et al., 2009; Miwa et al., 2010c). First they predict a set of candidate event trigger words (say, tokens 2, 5 and 6 in figure 1), then argument mentions are attached to these triggers (say, token 4 for trigger 2). The final stage decides how arguments are shared between events—compare how one event subs</context>
</contexts>
<marker>Kim, Pyysalo, Ohta, Bossy, Tsujii, 2011</marker>
<rawString>Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert Bossy, and Jun’ichi Tsujii. 2011. Overview of BioNLP Shared Task 2011. In Proceedings of the BioNLP 2011 Workshop Companion Volume for Shared Task, Portland, Oregon, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikos Komodakis</author>
<author>Nikos Paragios</author>
<author>Georgios Tziritas</author>
</authors>
<title>Mrf optimization via dual decomposition: Message-passing revisited.</title>
<date>2007</date>
<booktitle>In Proceedings of the 11st IEEE International Conference on Computer Vision (ICCV ’07).</booktitle>
<contexts>
<context position="6248" citStr="Komodakis et al., 2007" startWordPosition="1000" endWordPosition="1003">) runtime of a pipeline, where m&apos; is the number of trigger candidates as filtered by the first stage. We achieve these guarantees through a novel algorithm that jointly picks best trigger label and arguments on a per-token basis. Remarkably, it takes roughly as much time to train this model on one core as the model of Poon and Vanderwende (2010) on 32 cores, and leads to better results. The second model enforces additional constraints that ensure consistency between events in hierarchical regulation structures. While inference in this model is more complicated, we show how dual decomposition (Komodakis et al., 2007; Rush et al., 2010) can be used to efficiently find exact solutions for a large fraction of problems. Our third model includes the first two, and explicitly captures which arguments are part in the same event—the third stage of existing pipelines. Due to a complex coupling between this model and the first two, inference here requires a projected version of the sub-gradient technique demonstrated by Rush et al. (2010). When evaluated on the BioNLP 2009 shared task, the first two models outperform the previous best joint approaches and are competitive when compared to current state-of-the-art. </context>
</contexts>
<marker>Komodakis, Paragios, Tziritas, 2007</marker>
<rawString>Nikos Komodakis, Nikos Paragios, and Georgios Tziritas. 2007. Mrf optimization via dual decomposition: Message-passing revisited. In Proceedings of the 11st IEEE International Conference on Computer Vision (ICCV ’07).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
<author>David Sontag</author>
</authors>
<title>Dual decomposition for parsing with nonprojective head automata.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical methods in natural language processing (EMNLP ’10).</booktitle>
<contexts>
<context position="21087" citStr="Koo et al. (2010)" startWordPosition="3810" endWordPosition="3813">ual variables λ that will appear as local penalties in the subproblems to be solved. The algorithm will try to tune these variables such that at convergence the coupling constraints will be fulfilled. This is done by first optimizing s2 (e, a) over O and s2 (¯e, ¯a) over I. Now, whenever there is disagreement between two variables to be coupled, the corresponding dual parameter is shifted, increasing the chance that next time both models will agree. For example, if in the first iteration we predict e6,Bind = 1 but ¯e6,Bind = 0, we set λ6,Bind = −α where α is some stepsize (chosen according to Koo et al. (2010)). This will decrease the coefficient for e6,Bind, and increase the coefficient for ¯e6,Bind. Hence, we have a higher chance of agreement for this variable in the next iteration. The algorithm repeats the process described above until all variables agree, or some predefined number R of iterations is reached. In the former case we in fact have the exact solution to the original ILP. 1The ILP representation could be taken from the MLNs of Riedel et al. (2009) and the mapping to ILPs of Riedel (2008). Algorithm 2 Subgradient descent for Model 2, and projected subgradient descent for Model 3. requ</context>
</contexts>
<marker>Koo, Rush, Collins, Jaakkola, Sontag, 2010</marker>
<rawString>Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola, and David Sontag. 2010. Dual decomposition for parsing with nonprojective head automata. In Proceedings of the Conference on Empirical methods in natural language processing (EMNLP ’10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
</authors>
<title>Selftraining for biomedical parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL ’08).</booktitle>
<contexts>
<context position="30632" citStr="McClosky and Charniak, 2008" startWordPosition="5577" endWordPosition="5580">s is a soft incoming edge constraint. Finally, Model 3 can be used to efficiently capture compatibilities between semantic arWe evaluate our models on several tracks of the 2009 and 2011 BioNLP shared tasks, using the official “Approximate Span Matching/Approximate Recursive Matching” F1 metric for each. We also investigate the runtime behavior of our algorithms. 5.1 Preprocessing Each document is first processed by the Stanford CoreNLP2 tokenizer and sentence splitter. Parse trees come from the Charniak-Johnson parser (Charniak and Johnson, 2005) with a self-trained biomedical parsing model (McClosky and Charniak, 2008), and are converted to dependency structures again using Stanford CoreNLP. Based on trigger words collected from the training set, a set of candidate trigger tokens Trig (x) is generated for each sentence x. 5.2 Features The feature function fT (i, t) extracts a per-trigger feature vector for trigger i and type t E T . It creates one active feature for each element in {t, t E TRegI x feats (i). Here feats (i) denotes a collection of representations for the token i: wordform, lemma, POS tag, syntactic heads, syntactic children, and membership in two dictionaries taken from Riedel et al. (2009).</context>
</contexts>
<marker>McClosky, Charniak, 2008</marker>
<rawString>David McClosky and Eugene Charniak. 2008. Selftraining for biomedical parsing. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL ’08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Mihai Surdeanu</author>
<author>Chris Manning</author>
</authors>
<title>Event extraction as dependency parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL ’11),</booktitle>
<location>Portland, Oregon,</location>
<contexts>
<context position="28710" citStr="McClosky et al. (2011" startWordPosition="5265" endWordPosition="5269"> Programming and cutting planes (Riedel, 2008) for inference in a model similar to Model 2. By using dual decomposition instead, we can exploit tractable substructure and achieve quadratic (Model 2) and cubic (Model 3) runtime guarantees. An advantage of ILP inference are guaranteed certificates of optimality. However, in practice we also gain certificates of optimality for a large fraction of the instances we process. Poon and Vanderwende (2010) use local search and hence provide no such certificates. Their problem formulation also makes n-gram dependency path features harder to incorporate. McClosky et al. (2011b) cast event extraction as dependency parsing task. Their model assumes that event structures are trees, an assumption that is frequently violated in practice. Finally, all previous joint approaches use heuristics to decide whether binding arguments are part of the same event, while we capture these decisions in the joint model. We follow a long line of research in NLP that addresses search problems using (Integer) Linear Programs (Germann et al., 2001; Roth and Yih, 2004; Riedel and Clarke, 2006). However, instead of using off-the-shelf solvers, we work in the framework of dual decomposition</context>
<context position="32697" citStr="McClosky et al., 2011" startWordPosition="5950" endWordPosition="5954">55.7 Miwa 78.6 46.9 47.7 57.8 M1 77.2 43.0 45.8 56.2 M2 77.9 42.4 47.6 57.2 M3 78.4 48.0 49.1 58.7 SVT BIND REG TOT McClosky 68.3 46.9 33.3 48.6 Poon 69.5 42.5 37.5 50.0 Bjoerne 70.2 44.4 40.1 52.0 Miwa 72.1 50.6 45.3 56.3 M1 71.0 42.1 41.9 53.4 M2 70.5 41.3 43.6 53.7 M3 71.1 52.9 45.2 55.8 M3+enju 72.6 52.6 46.9 57.4 Table 2: F1 scores for the test set of Task 1 of the BioNLP 2009 shared task. task consist of 797, 150 and 250 documents, respectively. Table 1 shows our results for the development set. We compare our three models (M1, M2 and M3) and previous state-of-the-art systems: McClosky (McClosky et al., 2011a), Poon (Poon and Vanderwende, 2010), Bjoerne (Björne et al., 2009) and Miwa (Miwa et al., 2010b; Miwa et al., 2010a). Presented is F1 score for all events (TOT), regulation events (REG), binding events (BIND) and simple events (SVT). Model 1 is outperforming the previous best joint models of Poon and Vanderwende (2010), as well as the best entry of the 2009 task (Björne et al., 2009). This is achieved without careful tuning of thresholds that control flow of information between trigger and argument extraction. Notably, training Model 1 takes approximately 20 minutes using a single core imple</context>
<context position="36290" citStr="McClosky et al., 2011" startWordPosition="6573" endWordPosition="6577"> 2009 task by including both abstracts and full text articles. In total 908 training, 259 development and 347 test documents are provided. 9 Genia Task 1 TOT Infectious Diseases TOT System System M3+Stanford 56.0 M3+Stanford 55.6 M3 55.2 M3 53.4 UTurku 53.3 Stanford 50.6 MSR-NLP 51.5 UTurku 44.2 ConcordU 50.3 PNNL 42.6 Table 3: F1 scores for the test sets of two tracks in the BioNLP 2011 Shared Task. The top five entries are shown in table 3. Model 3 is the best-performing system that does not use model combination, only outperformed by a version of Model 3 that includes Stanford predictions (McClosky et al., 2011b) as input features (Riedel et al., 2011). Not shown in the table are results for full papers only. Here M3 ranks first with 53.1 F1, while M3+Stanford comes in second with 52.7 F1. The Infectious Diseases (ID) track of the 2011 task has 152 train, 46 development and 118 test documents. Relative to Genia it provides less data and introduces more types of entities as well as the biological process event type. Incorporating these changes into our models is straightforward, and hence we omit details for brevity. Table 3 shows the top five entries for the Infectious Diseases track. Again Model 3 </context>
</contexts>
<marker>McClosky, Surdeanu, Manning, 2011</marker>
<rawString>David McClosky, Mihai Surdeanu, and Chris Manning. 2011a. Event extraction as dependency parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL ’11), Portland, Oregon, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Mihai Surdeanu</author>
<author>Christopher D Manning</author>
</authors>
<title>Event extraction as dependency parsing in bionlp 2011. In BioNLP</title>
<date>2011</date>
<note>Shared Task.</note>
<contexts>
<context position="28710" citStr="McClosky et al. (2011" startWordPosition="5265" endWordPosition="5269"> Programming and cutting planes (Riedel, 2008) for inference in a model similar to Model 2. By using dual decomposition instead, we can exploit tractable substructure and achieve quadratic (Model 2) and cubic (Model 3) runtime guarantees. An advantage of ILP inference are guaranteed certificates of optimality. However, in practice we also gain certificates of optimality for a large fraction of the instances we process. Poon and Vanderwende (2010) use local search and hence provide no such certificates. Their problem formulation also makes n-gram dependency path features harder to incorporate. McClosky et al. (2011b) cast event extraction as dependency parsing task. Their model assumes that event structures are trees, an assumption that is frequently violated in practice. Finally, all previous joint approaches use heuristics to decide whether binding arguments are part of the same event, while we capture these decisions in the joint model. We follow a long line of research in NLP that addresses search problems using (Integer) Linear Programs (Germann et al., 2001; Roth and Yih, 2004; Riedel and Clarke, 2006). However, instead of using off-the-shelf solvers, we work in the framework of dual decomposition</context>
<context position="32697" citStr="McClosky et al., 2011" startWordPosition="5950" endWordPosition="5954">55.7 Miwa 78.6 46.9 47.7 57.8 M1 77.2 43.0 45.8 56.2 M2 77.9 42.4 47.6 57.2 M3 78.4 48.0 49.1 58.7 SVT BIND REG TOT McClosky 68.3 46.9 33.3 48.6 Poon 69.5 42.5 37.5 50.0 Bjoerne 70.2 44.4 40.1 52.0 Miwa 72.1 50.6 45.3 56.3 M1 71.0 42.1 41.9 53.4 M2 70.5 41.3 43.6 53.7 M3 71.1 52.9 45.2 55.8 M3+enju 72.6 52.6 46.9 57.4 Table 2: F1 scores for the test set of Task 1 of the BioNLP 2009 shared task. task consist of 797, 150 and 250 documents, respectively. Table 1 shows our results for the development set. We compare our three models (M1, M2 and M3) and previous state-of-the-art systems: McClosky (McClosky et al., 2011a), Poon (Poon and Vanderwende, 2010), Bjoerne (Björne et al., 2009) and Miwa (Miwa et al., 2010b; Miwa et al., 2010a). Presented is F1 score for all events (TOT), regulation events (REG), binding events (BIND) and simple events (SVT). Model 1 is outperforming the previous best joint models of Poon and Vanderwende (2010), as well as the best entry of the 2009 task (Björne et al., 2009). This is achieved without careful tuning of thresholds that control flow of information between trigger and argument extraction. Notably, training Model 1 takes approximately 20 minutes using a single core imple</context>
<context position="36290" citStr="McClosky et al., 2011" startWordPosition="6573" endWordPosition="6577"> 2009 task by including both abstracts and full text articles. In total 908 training, 259 development and 347 test documents are provided. 9 Genia Task 1 TOT Infectious Diseases TOT System System M3+Stanford 56.0 M3+Stanford 55.6 M3 55.2 M3 53.4 UTurku 53.3 Stanford 50.6 MSR-NLP 51.5 UTurku 44.2 ConcordU 50.3 PNNL 42.6 Table 3: F1 scores for the test sets of two tracks in the BioNLP 2011 Shared Task. The top five entries are shown in table 3. Model 3 is the best-performing system that does not use model combination, only outperformed by a version of Model 3 that includes Stanford predictions (McClosky et al., 2011b) as input features (Riedel et al., 2011). Not shown in the table are results for full papers only. Here M3 ranks first with 53.1 F1, while M3+Stanford comes in second with 52.7 F1. The Infectious Diseases (ID) track of the 2011 task has 152 train, 46 development and 118 test documents. Relative to Genia it provides less data and introduces more types of entities as well as the biological process event type. Incorporating these changes into our models is straightforward, and hence we omit details for brevity. Table 3 shows the top five entries for the Infectious Diseases track. Again Model 3 </context>
</contexts>
<marker>McClosky, Surdeanu, Manning, 2011</marker>
<rawString>David McClosky, Mihai Surdeanu, and Christopher D. Manning. 2011b. Event extraction as dependency parsing in bionlp 2011. In BioNLP 2011 Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the ACL (EACL ’06),</booktitle>
<pages>81--88</pages>
<contexts>
<context position="12517" citStr="McDonald and Pereira, 2006" startWordPosition="2134" endWordPosition="2137">ough ad-hoc rules (Björne et al., 2009) or with a post-processing classifier (Miwa et al., 2010c). We propose to augment the graph representation through edges between pairs of proteins that are themes in the same binding event. For two protein tokens p and q we represent this edge through the binary variable bp,q. Hence, in figure 1b) we have b4,9 = 1, whereas for figure 2 we get b1,6 = b1,8 = 1 but b6,8 = 0. By explicitly modeling such “sibling” edges we not only minimize the need for postprocessing. We can also improve attachment decisions akin to second order models in dependency parsing (McDonald and Pereira, 2006). Note that while merely introducing such variables is easy, enforcing consistency between them and the ei,t and ai,j,r variables is not. We address this in section 3.3.1. Reconstruction of events from solutions (e, a, b) can be done almost exactly as described by Björne et al. (2009). However, while they group binding arguments according to ad-hoc rules based on dependency paths from trigger to argument, we simply query the variables bp,q. To simplify our exposition we introduce additional notation. We denote the set of protein head tokens with Prot (x); the set of a possible targets for outg</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>R. McDonald and F. Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proceedings of the 11th Conference of the European Chapter of the ACL (EACL ’06), pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Meza-Ruiz</author>
<author>Sebastian Riedel</author>
</authors>
<title>Jointly identifying predicates, arguments and senses using markov logic.</title>
<date>2009</date>
<booktitle>In Joint Human Language Technology Conference/Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL ’09).</booktitle>
<contexts>
<context position="29904" citStr="Meza-Ruiz and Riedel (2009)" startWordPosition="5463" endWordPosition="5466">e framework of dual decomposition. Here we extend the approach of Rush et al. (2010) in that in addition to equality constraints we dualize more complex coupling constraints between models. This requires us to work with a projected version of subgradient descent. While tailored towards (biomedical) event extraction, we believe that our models can also be effective in a more general Semantic Role Labeling (SRL) context. Using variants of Model 1, we can enforce many of the SRL constraints—such as “unique agent” constraints (Punyakanok et al., 2004)—without having to call out to ILP optimizers. Meza-Ruiz and Riedel (2009) showed that inducing pressure on arguments to be attached to at least one predicate is helpful; this is a soft incoming edge constraint. Finally, Model 3 can be used to efficiently capture compatibilities between semantic arWe evaluate our models on several tracks of the 2009 and 2011 BioNLP shared tasks, using the official “Approximate Span Matching/Approximate Recursive Matching” F1 metric for each. We also investigate the runtime behavior of our algorithms. 5.1 Preprocessing Each document is first processed by the Stanford CoreNLP2 tokenizer and sentence splitter. Parse trees come from the</context>
</contexts>
<marker>Meza-Ruiz, Riedel, 2009</marker>
<rawString>Ivan Meza-Ruiz and Sebastian Riedel. 2009. Jointly identifying predicates, arguments and senses using markov logic. In Joint Human Language Technology Conference/Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL ’09).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Miwa</author>
<author>Sampo Pyysalo</author>
<author>Tadayoshi Hara</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>A comparative study of syntactic parsers for event extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, BioNLP ’10,</booktitle>
<pages>37--45</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2932" citStr="Miwa et al., 2010" startWordPosition="452" endWordPosition="455">in recent years there has also been an increasing interest in the extraction of biomedical events and their causal relations. This gave rise to the BioNLP 2009 and 2011 shared tasks which challenged participants to gather such events from biomedical text (Kim et al., 2009; Kim et al., 2011). Notably, these events can be complex and recursive: they may have several arguments, and some of the arguments may be events themselves. Current state-of-the-art event extractors follow the same architectural blueprint and divide the extraction process into a pipeline of three stages (Björne et al., 2009; Miwa et al., 2010c). First they predict a set of candidate event trigger words (say, tokens 2, 5 and 6 in figure 1), then argument mentions are attached to these triggers (say, token 4 for trigger 2). The final stage decides how arguments are shared between events—compare how one event subsumes all arguments of trigger 6 in figure 1, while two events share the three arguments of trigger 4 in figure 2. This architecture is prone to cascading errors: If we miss a trigger in the first stage, we will never be able to extract the full event Proceedings of the 2011 Conference on Empirical Methods in Natural Language</context>
<context position="4253" citStr="Miwa et al. (2010" startWordPosition="673" endWordPosition="676"> Linguistics (a) E1:Phosphorylation Cause E2:Regulation Theme E3:Binding Theme Theme Theme 1 2 3 4 5 6 7 8 9 10 11 Figure 1: (a) sentence with target event structure to extract; (b) projection to a set of labelled graph over tokens. Phosphorylation e2,PhoB. Regulation Binding (b) a6,9,Theme Theme Theme b4,9 Theme Theme Same Binding Cause ... the phosphorylation of TRAF2 inhibits binding to the CD40 cytoplasmic domain ... it concerns. Current systems attempt to tackle this problem by passing several candidates to the next stage. However, this tends to increase the false positive rate. In fact, Miwa et al. (2010c) observe that 30% of their errors stem from this type of ad-hoc module communication. Joint models have been proposed to overcome this problem (Poon and Vanderwende, 2010; Riedel et al., 2009). However, besides not being as accurate as their pipelined competitors, mostly because they do not yet exploit the rich set of features used by Miwa et al. (2010b) and Björne et al. (2009), they also suffer from the complexity of inference. For example, to remain tractable, the best joint system so far (Poon and Vanderwende, 2010) works with a simplified representation of the problem in which certain f</context>
<context position="6990" citStr="Miwa et al. (2010" startWordPosition="1127" endWordPosition="1130">es the first two, and explicitly captures which arguments are part in the same event—the third stage of existing pipelines. Due to a complex coupling between this model and the first two, inference here requires a projected version of the sub-gradient technique demonstrated by Rush et al. (2010). When evaluated on the BioNLP 2009 shared task, the first two models outperform the previous best joint approaches and are competitive when compared to current state-of-the-art. With 57.4 F1 on the test set, the third model yields the best results reported so far with a 1.1 F1 margin to the results of Miwa et al. (2010b). For the BioNLP 2011 Genia task 1 and the BioNLP 2011 Infectious Diseases task, Model 3 yields the second-best and best results reported so far. The second-best results are achieved with Model 3 as is (Riedel and McCallum, 2011), the best results when using Stanford event predictions as input features (Riedel et al., 2011). The margins between Model 3 and the best runner-ups range from 1.9 F1 to 2.8 F1. In the following we will first introduce biomedical event extraction and our notation. Then we go on to present our models and their inference routines. We present related work, show our emp</context>
<context position="11985" citStr="Miwa et al., 2010" startWordPosition="2038" endWordPosition="2041">n is equivalent to mappings in previous work (Riedel et al., 2009; Björne et al., 2009) and hence shares their main shortcoming: we cannot differentiate between two (or more) binding events with the same trigger but different arguments, or one binding event with several arguments. Consider, for example, the arguments of trigger 6 in figure 1b) that are all subsumed in a single event. By contrast, the arguments of trigger 4 shown in figure 2 are split between two events. Previous work has resolved this ambiguity 3 through ad-hoc rules (Björne et al., 2009) or with a post-processing classifier (Miwa et al., 2010c). We propose to augment the graph representation through edges between pairs of proteins that are themes in the same binding event. For two protein tokens p and q we represent this edge through the binary variable bp,q. Hence, in figure 1b) we have b4,9 = 1, whereas for figure 2 we get b1,6 = b1,8 = 1 but b6,8 = 0. By explicitly modeling such “sibling” edges we not only minimize the need for postprocessing. We can also improve attachment decisions akin to second order models in dependency parsing (McDonald and Pereira, 2006). Note that while merely introducing such variables is easy, enforci</context>
<context position="31422" citStr="Miwa et al. (2010" startWordPosition="5721" endWordPosition="5724"> is generated for each sentence x. 5.2 Features The feature function fT (i, t) extracts a per-trigger feature vector for trigger i and type t E T . It creates one active feature for each element in {t, t E TRegI x feats (i). Here feats (i) denotes a collection of representations for the token i: wordform, lemma, POS tag, syntactic heads, syntactic children, and membership in two dictionaries taken from Riedel et al. (2009). For fR (i, j, r) we create active features for each element of {r} x feats (i, j). Here feats (i, j) is a collection of representations of the token pair (i, j) taken from Miwa et al. (2010c) and contains: labelled and unlabeled n-gram dependency paths; edge and vertex walk features, argument and trigger modifiers and heads, words in between. For fB (p, q) we re-use the token pair representations from fR. In particular, we create one active feature for each element in feats (p, q). 5.3 Shared Task 2009 We first evaluate our models on the Bionlp 2009 task 1. The training, development and test sets for this 2http://nlp.stanford.edu/software/ corenlp.shtml 8 Table 1: F1 scores for the development set of Task 1 of the BioNLP 2009 shared task. SVT BIND REG TOT McClosky 75.4 48.4 40.4</context>
<context position="32793" citStr="Miwa et al., 2010" startWordPosition="5968" endWordPosition="5971">VT BIND REG TOT McClosky 68.3 46.9 33.3 48.6 Poon 69.5 42.5 37.5 50.0 Bjoerne 70.2 44.4 40.1 52.0 Miwa 72.1 50.6 45.3 56.3 M1 71.0 42.1 41.9 53.4 M2 70.5 41.3 43.6 53.7 M3 71.1 52.9 45.2 55.8 M3+enju 72.6 52.6 46.9 57.4 Table 2: F1 scores for the test set of Task 1 of the BioNLP 2009 shared task. task consist of 797, 150 and 250 documents, respectively. Table 1 shows our results for the development set. We compare our three models (M1, M2 and M3) and previous state-of-the-art systems: McClosky (McClosky et al., 2011a), Poon (Poon and Vanderwende, 2010), Bjoerne (Björne et al., 2009) and Miwa (Miwa et al., 2010b; Miwa et al., 2010a). Presented is F1 score for all events (TOT), regulation events (REG), binding events (BIND) and simple events (SVT). Model 1 is outperforming the previous best joint models of Poon and Vanderwende (2010), as well as the best entry of the 2009 task (Björne et al., 2009). This is achieved without careful tuning of thresholds that control flow of information between trigger and argument extraction. Notably, training Model 1 takes approximately 20 minutes using a single core implementation. Contrast this with 20 minutes on 32 cores reported by Poon and Vanderwende (2010). Mo</context>
<context position="34085" citStr="Miwa et al., 2010" startWordPosition="6187" endWordPosition="6190"> regulations goes up by nearly 2 points. While the impact of joint modeling relative to weaker local baselines has been shown shown by Poon and Vanderwende (2010) and Riedel et al. (2009), our findings here provide evidence that it remains effective even when the baseline system is very competitive. With Model 3 our focus is extended to binding events, improving F1 for such events by at least 5 F1. This also has a positive effect on regulation events, as regulations of binding events can now be more accurately extracted. In total we see a 1.1 F1 increase over the best results reported so far (Miwa et al., 2010b). Crucially, this is achieved using only a single parse tree per sentence, as opposed to three used by Miwa et al. (2010a). Table 2 shows results for the test set. Here with Model 1 we again already outperform all but the results of Miwa et al. (2010a). Model 2 improves F1 for regulations, while Model 3 again increases F1 for both regulations and binding events. This yields the best binding event results reported so far. Notably, not only are we able to resolve binding ambiguity better. Binding attachments themselves also improve, as we increase attachment F1 from 61.4 to 62.7 when going fro</context>
</contexts>
<marker>Miwa, Pyysalo, Hara, Tsujii, 2010</marker>
<rawString>Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and Jun’ichi Tsujii. 2010a. A comparative study of syntactic parsers for event extraction. In Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, BioNLP ’10, pages 37–45, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Miwa</author>
<author>Sampo Pyysalo</author>
<author>Tadayoshi Hara</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Evaluating dependency representation for event extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10,</booktitle>
<pages>779--787</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2932" citStr="Miwa et al., 2010" startWordPosition="452" endWordPosition="455">in recent years there has also been an increasing interest in the extraction of biomedical events and their causal relations. This gave rise to the BioNLP 2009 and 2011 shared tasks which challenged participants to gather such events from biomedical text (Kim et al., 2009; Kim et al., 2011). Notably, these events can be complex and recursive: they may have several arguments, and some of the arguments may be events themselves. Current state-of-the-art event extractors follow the same architectural blueprint and divide the extraction process into a pipeline of three stages (Björne et al., 2009; Miwa et al., 2010c). First they predict a set of candidate event trigger words (say, tokens 2, 5 and 6 in figure 1), then argument mentions are attached to these triggers (say, token 4 for trigger 2). The final stage decides how arguments are shared between events—compare how one event subsumes all arguments of trigger 6 in figure 1, while two events share the three arguments of trigger 4 in figure 2. This architecture is prone to cascading errors: If we miss a trigger in the first stage, we will never be able to extract the full event Proceedings of the 2011 Conference on Empirical Methods in Natural Language</context>
<context position="4253" citStr="Miwa et al. (2010" startWordPosition="673" endWordPosition="676"> Linguistics (a) E1:Phosphorylation Cause E2:Regulation Theme E3:Binding Theme Theme Theme 1 2 3 4 5 6 7 8 9 10 11 Figure 1: (a) sentence with target event structure to extract; (b) projection to a set of labelled graph over tokens. Phosphorylation e2,PhoB. Regulation Binding (b) a6,9,Theme Theme Theme b4,9 Theme Theme Same Binding Cause ... the phosphorylation of TRAF2 inhibits binding to the CD40 cytoplasmic domain ... it concerns. Current systems attempt to tackle this problem by passing several candidates to the next stage. However, this tends to increase the false positive rate. In fact, Miwa et al. (2010c) observe that 30% of their errors stem from this type of ad-hoc module communication. Joint models have been proposed to overcome this problem (Poon and Vanderwende, 2010; Riedel et al., 2009). However, besides not being as accurate as their pipelined competitors, mostly because they do not yet exploit the rich set of features used by Miwa et al. (2010b) and Björne et al. (2009), they also suffer from the complexity of inference. For example, to remain tractable, the best joint system so far (Poon and Vanderwende, 2010) works with a simplified representation of the problem in which certain f</context>
<context position="6990" citStr="Miwa et al. (2010" startWordPosition="1127" endWordPosition="1130">es the first two, and explicitly captures which arguments are part in the same event—the third stage of existing pipelines. Due to a complex coupling between this model and the first two, inference here requires a projected version of the sub-gradient technique demonstrated by Rush et al. (2010). When evaluated on the BioNLP 2009 shared task, the first two models outperform the previous best joint approaches and are competitive when compared to current state-of-the-art. With 57.4 F1 on the test set, the third model yields the best results reported so far with a 1.1 F1 margin to the results of Miwa et al. (2010b). For the BioNLP 2011 Genia task 1 and the BioNLP 2011 Infectious Diseases task, Model 3 yields the second-best and best results reported so far. The second-best results are achieved with Model 3 as is (Riedel and McCallum, 2011), the best results when using Stanford event predictions as input features (Riedel et al., 2011). The margins between Model 3 and the best runner-ups range from 1.9 F1 to 2.8 F1. In the following we will first introduce biomedical event extraction and our notation. Then we go on to present our models and their inference routines. We present related work, show our emp</context>
<context position="11985" citStr="Miwa et al., 2010" startWordPosition="2038" endWordPosition="2041">n is equivalent to mappings in previous work (Riedel et al., 2009; Björne et al., 2009) and hence shares their main shortcoming: we cannot differentiate between two (or more) binding events with the same trigger but different arguments, or one binding event with several arguments. Consider, for example, the arguments of trigger 6 in figure 1b) that are all subsumed in a single event. By contrast, the arguments of trigger 4 shown in figure 2 are split between two events. Previous work has resolved this ambiguity 3 through ad-hoc rules (Björne et al., 2009) or with a post-processing classifier (Miwa et al., 2010c). We propose to augment the graph representation through edges between pairs of proteins that are themes in the same binding event. For two protein tokens p and q we represent this edge through the binary variable bp,q. Hence, in figure 1b) we have b4,9 = 1, whereas for figure 2 we get b1,6 = b1,8 = 1 but b6,8 = 0. By explicitly modeling such “sibling” edges we not only minimize the need for postprocessing. We can also improve attachment decisions akin to second order models in dependency parsing (McDonald and Pereira, 2006). Note that while merely introducing such variables is easy, enforci</context>
<context position="31422" citStr="Miwa et al. (2010" startWordPosition="5721" endWordPosition="5724"> is generated for each sentence x. 5.2 Features The feature function fT (i, t) extracts a per-trigger feature vector for trigger i and type t E T . It creates one active feature for each element in {t, t E TRegI x feats (i). Here feats (i) denotes a collection of representations for the token i: wordform, lemma, POS tag, syntactic heads, syntactic children, and membership in two dictionaries taken from Riedel et al. (2009). For fR (i, j, r) we create active features for each element of {r} x feats (i, j). Here feats (i, j) is a collection of representations of the token pair (i, j) taken from Miwa et al. (2010c) and contains: labelled and unlabeled n-gram dependency paths; edge and vertex walk features, argument and trigger modifiers and heads, words in between. For fB (p, q) we re-use the token pair representations from fR. In particular, we create one active feature for each element in feats (p, q). 5.3 Shared Task 2009 We first evaluate our models on the Bionlp 2009 task 1. The training, development and test sets for this 2http://nlp.stanford.edu/software/ corenlp.shtml 8 Table 1: F1 scores for the development set of Task 1 of the BioNLP 2009 shared task. SVT BIND REG TOT McClosky 75.4 48.4 40.4</context>
<context position="32793" citStr="Miwa et al., 2010" startWordPosition="5968" endWordPosition="5971">VT BIND REG TOT McClosky 68.3 46.9 33.3 48.6 Poon 69.5 42.5 37.5 50.0 Bjoerne 70.2 44.4 40.1 52.0 Miwa 72.1 50.6 45.3 56.3 M1 71.0 42.1 41.9 53.4 M2 70.5 41.3 43.6 53.7 M3 71.1 52.9 45.2 55.8 M3+enju 72.6 52.6 46.9 57.4 Table 2: F1 scores for the test set of Task 1 of the BioNLP 2009 shared task. task consist of 797, 150 and 250 documents, respectively. Table 1 shows our results for the development set. We compare our three models (M1, M2 and M3) and previous state-of-the-art systems: McClosky (McClosky et al., 2011a), Poon (Poon and Vanderwende, 2010), Bjoerne (Björne et al., 2009) and Miwa (Miwa et al., 2010b; Miwa et al., 2010a). Presented is F1 score for all events (TOT), regulation events (REG), binding events (BIND) and simple events (SVT). Model 1 is outperforming the previous best joint models of Poon and Vanderwende (2010), as well as the best entry of the 2009 task (Björne et al., 2009). This is achieved without careful tuning of thresholds that control flow of information between trigger and argument extraction. Notably, training Model 1 takes approximately 20 minutes using a single core implementation. Contrast this with 20 minutes on 32 cores reported by Poon and Vanderwende (2010). Mo</context>
<context position="34085" citStr="Miwa et al., 2010" startWordPosition="6187" endWordPosition="6190"> regulations goes up by nearly 2 points. While the impact of joint modeling relative to weaker local baselines has been shown shown by Poon and Vanderwende (2010) and Riedel et al. (2009), our findings here provide evidence that it remains effective even when the baseline system is very competitive. With Model 3 our focus is extended to binding events, improving F1 for such events by at least 5 F1. This also has a positive effect on regulation events, as regulations of binding events can now be more accurately extracted. In total we see a 1.1 F1 increase over the best results reported so far (Miwa et al., 2010b). Crucially, this is achieved using only a single parse tree per sentence, as opposed to three used by Miwa et al. (2010a). Table 2 shows results for the test set. Here with Model 1 we again already outperform all but the results of Miwa et al. (2010a). Model 2 improves F1 for regulations, while Model 3 again increases F1 for both regulations and binding events. This yields the best binding event results reported so far. Notably, not only are we able to resolve binding ambiguity better. Binding attachments themselves also improve, as we increase attachment F1 from 61.4 to 62.7 when going fro</context>
</contexts>
<marker>Miwa, Pyysalo, Hara, Tsujii, 2010</marker>
<rawString>Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and Jun’ichi Tsujii. 2010b. Evaluating dependency representation for event extraction. In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10, pages 779–787, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Miwa</author>
<author>Rune Saetre</author>
<author>Jin-Dong D Kim</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Event extraction with complex event classification using rich features.</title>
<date>2010</date>
<journal>Journal of bioinformatics and computational biology,</journal>
<volume>8</volume>
<issue>1</issue>
<pages>146</pages>
<contexts>
<context position="2932" citStr="Miwa et al., 2010" startWordPosition="452" endWordPosition="455">in recent years there has also been an increasing interest in the extraction of biomedical events and their causal relations. This gave rise to the BioNLP 2009 and 2011 shared tasks which challenged participants to gather such events from biomedical text (Kim et al., 2009; Kim et al., 2011). Notably, these events can be complex and recursive: they may have several arguments, and some of the arguments may be events themselves. Current state-of-the-art event extractors follow the same architectural blueprint and divide the extraction process into a pipeline of three stages (Björne et al., 2009; Miwa et al., 2010c). First they predict a set of candidate event trigger words (say, tokens 2, 5 and 6 in figure 1), then argument mentions are attached to these triggers (say, token 4 for trigger 2). The final stage decides how arguments are shared between events—compare how one event subsumes all arguments of trigger 6 in figure 1, while two events share the three arguments of trigger 4 in figure 2. This architecture is prone to cascading errors: If we miss a trigger in the first stage, we will never be able to extract the full event Proceedings of the 2011 Conference on Empirical Methods in Natural Language</context>
<context position="4253" citStr="Miwa et al. (2010" startWordPosition="673" endWordPosition="676"> Linguistics (a) E1:Phosphorylation Cause E2:Regulation Theme E3:Binding Theme Theme Theme 1 2 3 4 5 6 7 8 9 10 11 Figure 1: (a) sentence with target event structure to extract; (b) projection to a set of labelled graph over tokens. Phosphorylation e2,PhoB. Regulation Binding (b) a6,9,Theme Theme Theme b4,9 Theme Theme Same Binding Cause ... the phosphorylation of TRAF2 inhibits binding to the CD40 cytoplasmic domain ... it concerns. Current systems attempt to tackle this problem by passing several candidates to the next stage. However, this tends to increase the false positive rate. In fact, Miwa et al. (2010c) observe that 30% of their errors stem from this type of ad-hoc module communication. Joint models have been proposed to overcome this problem (Poon and Vanderwende, 2010; Riedel et al., 2009). However, besides not being as accurate as their pipelined competitors, mostly because they do not yet exploit the rich set of features used by Miwa et al. (2010b) and Björne et al. (2009), they also suffer from the complexity of inference. For example, to remain tractable, the best joint system so far (Poon and Vanderwende, 2010) works with a simplified representation of the problem in which certain f</context>
<context position="6990" citStr="Miwa et al. (2010" startWordPosition="1127" endWordPosition="1130">es the first two, and explicitly captures which arguments are part in the same event—the third stage of existing pipelines. Due to a complex coupling between this model and the first two, inference here requires a projected version of the sub-gradient technique demonstrated by Rush et al. (2010). When evaluated on the BioNLP 2009 shared task, the first two models outperform the previous best joint approaches and are competitive when compared to current state-of-the-art. With 57.4 F1 on the test set, the third model yields the best results reported so far with a 1.1 F1 margin to the results of Miwa et al. (2010b). For the BioNLP 2011 Genia task 1 and the BioNLP 2011 Infectious Diseases task, Model 3 yields the second-best and best results reported so far. The second-best results are achieved with Model 3 as is (Riedel and McCallum, 2011), the best results when using Stanford event predictions as input features (Riedel et al., 2011). The margins between Model 3 and the best runner-ups range from 1.9 F1 to 2.8 F1. In the following we will first introduce biomedical event extraction and our notation. Then we go on to present our models and their inference routines. We present related work, show our emp</context>
<context position="11985" citStr="Miwa et al., 2010" startWordPosition="2038" endWordPosition="2041">n is equivalent to mappings in previous work (Riedel et al., 2009; Björne et al., 2009) and hence shares their main shortcoming: we cannot differentiate between two (or more) binding events with the same trigger but different arguments, or one binding event with several arguments. Consider, for example, the arguments of trigger 6 in figure 1b) that are all subsumed in a single event. By contrast, the arguments of trigger 4 shown in figure 2 are split between two events. Previous work has resolved this ambiguity 3 through ad-hoc rules (Björne et al., 2009) or with a post-processing classifier (Miwa et al., 2010c). We propose to augment the graph representation through edges between pairs of proteins that are themes in the same binding event. For two protein tokens p and q we represent this edge through the binary variable bp,q. Hence, in figure 1b) we have b4,9 = 1, whereas for figure 2 we get b1,6 = b1,8 = 1 but b6,8 = 0. By explicitly modeling such “sibling” edges we not only minimize the need for postprocessing. We can also improve attachment decisions akin to second order models in dependency parsing (McDonald and Pereira, 2006). Note that while merely introducing such variables is easy, enforci</context>
<context position="31422" citStr="Miwa et al. (2010" startWordPosition="5721" endWordPosition="5724"> is generated for each sentence x. 5.2 Features The feature function fT (i, t) extracts a per-trigger feature vector for trigger i and type t E T . It creates one active feature for each element in {t, t E TRegI x feats (i). Here feats (i) denotes a collection of representations for the token i: wordform, lemma, POS tag, syntactic heads, syntactic children, and membership in two dictionaries taken from Riedel et al. (2009). For fR (i, j, r) we create active features for each element of {r} x feats (i, j). Here feats (i, j) is a collection of representations of the token pair (i, j) taken from Miwa et al. (2010c) and contains: labelled and unlabeled n-gram dependency paths; edge and vertex walk features, argument and trigger modifiers and heads, words in between. For fB (p, q) we re-use the token pair representations from fR. In particular, we create one active feature for each element in feats (p, q). 5.3 Shared Task 2009 We first evaluate our models on the Bionlp 2009 task 1. The training, development and test sets for this 2http://nlp.stanford.edu/software/ corenlp.shtml 8 Table 1: F1 scores for the development set of Task 1 of the BioNLP 2009 shared task. SVT BIND REG TOT McClosky 75.4 48.4 40.4</context>
<context position="32793" citStr="Miwa et al., 2010" startWordPosition="5968" endWordPosition="5971">VT BIND REG TOT McClosky 68.3 46.9 33.3 48.6 Poon 69.5 42.5 37.5 50.0 Bjoerne 70.2 44.4 40.1 52.0 Miwa 72.1 50.6 45.3 56.3 M1 71.0 42.1 41.9 53.4 M2 70.5 41.3 43.6 53.7 M3 71.1 52.9 45.2 55.8 M3+enju 72.6 52.6 46.9 57.4 Table 2: F1 scores for the test set of Task 1 of the BioNLP 2009 shared task. task consist of 797, 150 and 250 documents, respectively. Table 1 shows our results for the development set. We compare our three models (M1, M2 and M3) and previous state-of-the-art systems: McClosky (McClosky et al., 2011a), Poon (Poon and Vanderwende, 2010), Bjoerne (Björne et al., 2009) and Miwa (Miwa et al., 2010b; Miwa et al., 2010a). Presented is F1 score for all events (TOT), regulation events (REG), binding events (BIND) and simple events (SVT). Model 1 is outperforming the previous best joint models of Poon and Vanderwende (2010), as well as the best entry of the 2009 task (Björne et al., 2009). This is achieved without careful tuning of thresholds that control flow of information between trigger and argument extraction. Notably, training Model 1 takes approximately 20 minutes using a single core implementation. Contrast this with 20 minutes on 32 cores reported by Poon and Vanderwende (2010). Mo</context>
<context position="34085" citStr="Miwa et al., 2010" startWordPosition="6187" endWordPosition="6190"> regulations goes up by nearly 2 points. While the impact of joint modeling relative to weaker local baselines has been shown shown by Poon and Vanderwende (2010) and Riedel et al. (2009), our findings here provide evidence that it remains effective even when the baseline system is very competitive. With Model 3 our focus is extended to binding events, improving F1 for such events by at least 5 F1. This also has a positive effect on regulation events, as regulations of binding events can now be more accurately extracted. In total we see a 1.1 F1 increase over the best results reported so far (Miwa et al., 2010b). Crucially, this is achieved using only a single parse tree per sentence, as opposed to three used by Miwa et al. (2010a). Table 2 shows results for the test set. Here with Model 1 we again already outperform all but the results of Miwa et al. (2010a). Model 2 improves F1 for regulations, while Model 3 again increases F1 for both regulations and binding events. This yields the best binding event results reported so far. Notably, not only are we able to resolve binding ambiguity better. Binding attachments themselves also improve, as we increase attachment F1 from 61.4 to 62.7 when going fro</context>
</contexts>
<marker>Miwa, Saetre, Kim, Tsujii, 2010</marker>
<rawString>Makoto Miwa, Rune Saetre, Jin-Dong D. Kim, and Jun’ichi Tsujii. 2010c. Event extraction with complex event classification using rich features. Journal of bioinformatics and computational biology, 8(1):131– 146, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
</authors>
<title>Kenji Sagae, Rune Sætre, Takuya Matsuzaki, and Jun ichi Tsujii.</title>
<date>2009</date>
<pages>25--394</pages>
<marker>Miyao, 2009</marker>
<rawString>Yusuke Miyao, Kenji Sagae, Rune Sætre, Takuya Matsuzaki, and Jun ichi Tsujii. 2009. Evaluating contributions of natural language parsers to protein-protein interaction extraction. Bioinformatics/computer Applications in The Biosciences, 25:394–400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Joint Inference for Knowledge Extraction from Biomedical Literature. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>813--821</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="4425" citStr="Poon and Vanderwende, 2010" startWordPosition="700" endWordPosition="703">re to extract; (b) projection to a set of labelled graph over tokens. Phosphorylation e2,PhoB. Regulation Binding (b) a6,9,Theme Theme Theme b4,9 Theme Theme Same Binding Cause ... the phosphorylation of TRAF2 inhibits binding to the CD40 cytoplasmic domain ... it concerns. Current systems attempt to tackle this problem by passing several candidates to the next stage. However, this tends to increase the false positive rate. In fact, Miwa et al. (2010c) observe that 30% of their errors stem from this type of ad-hoc module communication. Joint models have been proposed to overcome this problem (Poon and Vanderwende, 2010; Riedel et al., 2009). However, besides not being as accurate as their pipelined competitors, mostly because they do not yet exploit the rich set of features used by Miwa et al. (2010b) and Björne et al. (2009), they also suffer from the complexity of inference. For example, to remain tractable, the best joint system so far (Poon and Vanderwende, 2010) works with a simplified representation of the problem in which certain features are harder to capture, employs local search without certificates of optimality, and furthermore requires a 32-core cluster for quick train-test cycles. Existing joi</context>
<context position="5973" citStr="Poon and Vanderwende (2010)" startWordPosition="958" endWordPosition="961"> model jointly predicts triggers and arguments. Notably, the highest scoring event structure under this model can be found efficiently in O (mn) time where m is the number of trigger candidates, and n the number of argument candidates. This is only slightly slower than the O (m&apos;n) runtime of a pipeline, where m&apos; is the number of trigger candidates as filtered by the first stage. We achieve these guarantees through a novel algorithm that jointly picks best trigger label and arguments on a per-token basis. Remarkably, it takes roughly as much time to train this model on one core as the model of Poon and Vanderwende (2010) on 32 cores, and leads to better results. The second model enforces additional constraints that ensure consistency between events in hierarchical regulation structures. While inference in this model is more complicated, we show how dual decomposition (Komodakis et al., 2007; Rush et al., 2010) can be used to efficiently find exact solutions for a large fraction of problems. Our third model includes the first two, and explicitly captures which arguments are part in the same event—the third stage of existing pipelines. Due to a complex coupling between this model and the first two, inference he</context>
<context position="28539" citStr="Poon and Vanderwende (2010)" startWordPosition="5238" endWordPosition="5241">pment set. guments; such compatibilities have also been shown to be helpful in SRL (Toutanova et al., 2005). 5 Experiments 4 Related Work Riedel et al. (2009) use Integer Linear Programming and cutting planes (Riedel, 2008) for inference in a model similar to Model 2. By using dual decomposition instead, we can exploit tractable substructure and achieve quadratic (Model 2) and cubic (Model 3) runtime guarantees. An advantage of ILP inference are guaranteed certificates of optimality. However, in practice we also gain certificates of optimality for a large fraction of the instances we process. Poon and Vanderwende (2010) use local search and hence provide no such certificates. Their problem formulation also makes n-gram dependency path features harder to incorporate. McClosky et al. (2011b) cast event extraction as dependency parsing task. Their model assumes that event structures are trees, an assumption that is frequently violated in practice. Finally, all previous joint approaches use heuristics to decide whether binding arguments are part of the same event, while we capture these decisions in the joint model. We follow a long line of research in NLP that addresses search problems using (Integer) Linear Pr</context>
<context position="32734" citStr="Poon and Vanderwende, 2010" startWordPosition="5956" endWordPosition="5960">1 77.2 43.0 45.8 56.2 M2 77.9 42.4 47.6 57.2 M3 78.4 48.0 49.1 58.7 SVT BIND REG TOT McClosky 68.3 46.9 33.3 48.6 Poon 69.5 42.5 37.5 50.0 Bjoerne 70.2 44.4 40.1 52.0 Miwa 72.1 50.6 45.3 56.3 M1 71.0 42.1 41.9 53.4 M2 70.5 41.3 43.6 53.7 M3 71.1 52.9 45.2 55.8 M3+enju 72.6 52.6 46.9 57.4 Table 2: F1 scores for the test set of Task 1 of the BioNLP 2009 shared task. task consist of 797, 150 and 250 documents, respectively. Table 1 shows our results for the development set. We compare our three models (M1, M2 and M3) and previous state-of-the-art systems: McClosky (McClosky et al., 2011a), Poon (Poon and Vanderwende, 2010), Bjoerne (Björne et al., 2009) and Miwa (Miwa et al., 2010b; Miwa et al., 2010a). Presented is F1 score for all events (TOT), regulation events (REG), binding events (BIND) and simple events (SVT). Model 1 is outperforming the previous best joint models of Poon and Vanderwende (2010), as well as the best entry of the 2009 task (Björne et al., 2009). This is achieved without careful tuning of thresholds that control flow of information between trigger and argument extraction. Notably, training Model 1 takes approximately 20 minutes using a single core implementation. Contrast this with 20 minu</context>
</contexts>
<marker>Poon, Vanderwende, 2010</marker>
<rawString>Hoifung Poon and Lucy Vanderwende. 2010. Joint Inference for Knowledge Extraction from Biomedical Literature. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 813–821, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen tau Yih</author>
<author>Dav Zimak</author>
</authors>
<title>Semantic role labeling via integer linear programming inference.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics (COLING ’04),</booktitle>
<pages>1346--1352</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="29830" citStr="Punyakanok et al., 2004" startWordPosition="5451" endWordPosition="5454">, 2006). However, instead of using off-the-shelf solvers, we work in the framework of dual decomposition. Here we extend the approach of Rush et al. (2010) in that in addition to equality constraints we dualize more complex coupling constraints between models. This requires us to work with a projected version of subgradient descent. While tailored towards (biomedical) event extraction, we believe that our models can also be effective in a more general Semantic Role Labeling (SRL) context. Using variants of Model 1, we can enforce many of the SRL constraints—such as “unique agent” constraints (Punyakanok et al., 2004)—without having to call out to ILP optimizers. Meza-Ruiz and Riedel (2009) showed that inducing pressure on arguments to be attached to at least one predicate is helpful; this is a soft incoming edge constraint. Finally, Model 3 can be used to efficiently capture compatibilities between semantic arWe evaluate our models on several tracks of the 2009 and 2011 BioNLP shared tasks, using the official “Approximate Span Matching/Approximate Recursive Matching” F1 metric for each. We also investigate the runtime behavior of our algorithms. 5.1 Preprocessing Each document is first processed by the St</context>
</contexts>
<marker>Punyakanok, Roth, Yih, Zimak, 2004</marker>
<rawString>Vasin Punyakanok, Dan Roth, Wen tau Yih, and Dav Zimak. 2004. Semantic role labeling via integer linear programming inference. In Proceedings of the 20th international conference on Computational Linguistics (COLING ’04), pages 1346–1352, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>James Clarke</author>
</authors>
<title>Incremental integer linear programming for non-projective dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Empirical methods in natural language processing (EMNLP ’06),</booktitle>
<pages>129--137</pages>
<contexts>
<context position="29213" citStr="Riedel and Clarke, 2006" startWordPosition="5350" endWordPosition="5353">icates. Their problem formulation also makes n-gram dependency path features harder to incorporate. McClosky et al. (2011b) cast event extraction as dependency parsing task. Their model assumes that event structures are trees, an assumption that is frequently violated in practice. Finally, all previous joint approaches use heuristics to decide whether binding arguments are part of the same event, while we capture these decisions in the joint model. We follow a long line of research in NLP that addresses search problems using (Integer) Linear Programs (Germann et al., 2001; Roth and Yih, 2004; Riedel and Clarke, 2006). However, instead of using off-the-shelf solvers, we work in the framework of dual decomposition. Here we extend the approach of Rush et al. (2010) in that in addition to equality constraints we dualize more complex coupling constraints between models. This requires us to work with a projected version of subgradient descent. While tailored towards (biomedical) event extraction, we believe that our models can also be effective in a more general Semantic Role Labeling (SRL) context. Using variants of Model 1, we can enforce many of the SRL constraints—such as “unique agent” constraints (Punyaka</context>
</contexts>
<marker>Riedel, Clarke, 2006</marker>
<rawString>Sebastian Riedel and James Clarke. 2006. Incremental integer linear programming for non-projective dependency parsing. In Proceedings of the Conference on Empirical methods in natural language processing (EMNLP ’06), pages 129–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Robust biomedical event extraction with dual decomposition and minimal domain adaptation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Natural Language Processing in Biomedicine NAACL 2011 Workshop (BioNLP ’11),</booktitle>
<contexts>
<context position="7221" citStr="Riedel and McCallum, 2011" startWordPosition="1167" endWordPosition="1170">d version of the sub-gradient technique demonstrated by Rush et al. (2010). When evaluated on the BioNLP 2009 shared task, the first two models outperform the previous best joint approaches and are competitive when compared to current state-of-the-art. With 57.4 F1 on the test set, the third model yields the best results reported so far with a 1.1 F1 margin to the results of Miwa et al. (2010b). For the BioNLP 2011 Genia task 1 and the BioNLP 2011 Infectious Diseases task, Model 3 yields the second-best and best results reported so far. The second-best results are achieved with Model 3 as is (Riedel and McCallum, 2011), the best results when using Stanford event predictions as input features (Riedel et al., 2011). The margins between Model 3 and the best runner-ups range from 1.9 F1 to 2.8 F1. In the following we will first introduce biomedical event extraction and our notation. Then we go on to present our models and their inference routines. We present related work, show our empirical evaluation, and conclude. 2 Binding Binding Theme Theme Theme Theme Grb2 can be coimmunoprecipitated with Sos1 and Sos2 1 2 3 4 5 6 7 8 quickly validate extracted events. For example, the trigger for event E2 is “inhibit”, a</context>
</contexts>
<marker>Riedel, McCallum, 2011</marker>
<rawString>Sebastian Riedel and Andrew McCallum. 2011. Robust biomedical event extraction with dual decomposition and minimal domain adaptation. In Proceedings of the Natural Language Processing in Biomedicine NAACL 2011 Workshop (BioNLP ’11), June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Hong-Woo Chun</author>
<author>Toshihisa Takagi</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>A markov logic approach to bio-molecular event extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the Natural Language Processing in Biomedicine NAACL 2009 Workshop (BioNLP ’09),</booktitle>
<pages>41--49</pages>
<contexts>
<context position="4447" citStr="Riedel et al., 2009" startWordPosition="704" endWordPosition="707">n to a set of labelled graph over tokens. Phosphorylation e2,PhoB. Regulation Binding (b) a6,9,Theme Theme Theme b4,9 Theme Theme Same Binding Cause ... the phosphorylation of TRAF2 inhibits binding to the CD40 cytoplasmic domain ... it concerns. Current systems attempt to tackle this problem by passing several candidates to the next stage. However, this tends to increase the false positive rate. In fact, Miwa et al. (2010c) observe that 30% of their errors stem from this type of ad-hoc module communication. Joint models have been proposed to overcome this problem (Poon and Vanderwende, 2010; Riedel et al., 2009). However, besides not being as accurate as their pipelined competitors, mostly because they do not yet exploit the rich set of features used by Miwa et al. (2010b) and Björne et al. (2009), they also suffer from the complexity of inference. For example, to remain tractable, the best joint system so far (Poon and Vanderwende, 2010) works with a simplified representation of the problem in which certain features are harder to capture, employs local search without certificates of optimality, and furthermore requires a 32-core cluster for quick train-test cycles. Existing joint models also rely on</context>
<context position="10121" citStr="Riedel et al., 2009" startWordPosition="1683" endWordPosition="1686">heme; this theme can a be protein or, as in our case, another event. Regulations may also have zero or one cause arguments that denote events or proteins which trigger the regulation. In the BioNLP shared task, we are also asked to find a trigger (or clue) token for each event. This token grounds the event in text and allows users to 2.1 Event Projection To formulate the search for event structures of the form shown in figure 1a) as an optimization problem, it will be convenient to represent them through a set of binary variables. We introduce such a representation, inspired by previous work (Riedel et al., 2009; Björne et al., 2009) and based on a projection of events to a graph structure over tokens, as seen figure 1b). Consider sentence x and a set of candidate trigger tokens, denoted by Trig (x). We label each candidate i with the event type it is a trigger for, or None if it is not a trigger. This decision is represented through a set of binary variables ei,t, one for each possible event type t. In our example we have e6,Binding = 1. The set of possible event types will be denoted as T, the regulation event types as TReg def = {PosReg, NegReg, Reg} and its complement as T¬reg def = T \ TReg. For</context>
<context position="11433" citStr="Riedel et al., 2009" startWordPosition="1945" endWordPosition="1948">h argument a will either be an event itself, or a protein. For events we add a labelled edge between i and the trigger j of a. For proteins we add an edge between i and the syntactic head j of the protein mention. In both cases we label the edge i → j with the role of the argument a. The edge is represented through a binary variable ai,j,r, where r E R is the argument role and R def= {Theme, Cause, None}. The role None is active whenever no Theme or Cause role is present. In our example we get, among others, a2,4,Theme = 1. So far our representation is equivalent to mappings in previous work (Riedel et al., 2009; Björne et al., 2009) and hence shares their main shortcoming: we cannot differentiate between two (or more) binding events with the same trigger but different arguments, or one binding event with several arguments. Consider, for example, the arguments of trigger 6 in figure 1b) that are all subsumed in a single event. By contrast, the arguments of trigger 4 shown in figure 2 are split between two events. Previous work has resolved this ambiguity 3 through ad-hoc rules (Björne et al., 2009) or with a post-processing classifier (Miwa et al., 2010c). We propose to augment the graph representati</context>
<context position="21548" citStr="Riedel et al. (2009)" startWordPosition="3888" endWordPosition="3891">example, if in the first iteration we predict e6,Bind = 1 but ¯e6,Bind = 0, we set λ6,Bind = −α where α is some stepsize (chosen according to Koo et al. (2010)). This will decrease the coefficient for e6,Bind, and increase the coefficient for ¯e6,Bind. Hence, we have a higher chance of agreement for this variable in the next iteration. The algorithm repeats the process described above until all variables agree, or some predefined number R of iterations is reached. In the former case we in fact have the exact solution to the original ILP. 1The ILP representation could be taken from the MLNs of Riedel et al. (2009) and the mapping to ILPs of Riedel (2008). Algorithm 2 Subgradient descent for Model 2, and projected subgradient descent for Model 3. require: R: max. iteration, αt: stepsizes t 0 [model 2,3] λ 0 [model 2,3] µ 0 [model 3] repeat model 2 (e, a) bestOut (λ) 2,3 (¯e, ¯a) bestIn (−λ) 3 (e, a) bestOut (cout (λ, µ)) 3 (b, t) bestBind (cbind (µ)) 2,3 λi,t λi,t − αt (ei,t − ¯ei,t) 2,3 λi,j,r λi,j,r − αt (ai,j,r − ¯ai,j,r) h i 3 µi,p,q trig µtrig i,p,q − αt (ei,Bind − ti,p,q) h + i arg1 arg1 ( 3 µi,j,k µi,p,q − αt lai,p,Theme − ti,p,q) h i+ arg2 arg2 3 µi,p,q µi,p,q − αt (ai,q,Theme − ti,p,q) + 2,3 t </context>
<context position="28070" citStr="Riedel et al. (2009)" startWordPosition="5163" endWordPosition="5166">imate the weights w for each of our models. PA is an error-driven learner that shifts weights towards features of the gold solution, and away from features of the current guess, whenever the current model makes a mistake. PA learning takes into account a user-defined loss function for which we use a weighted sum sB (p, q) 7 of false positives and false negatives: l (y, y&apos;) def = FP (y, y&apos;) + αFN (y, y&apos;). We set α = 3.8 by optimizing on the BioNLP 2009 development set. guments; such compatibilities have also been shown to be helpful in SRL (Toutanova et al., 2005). 5 Experiments 4 Related Work Riedel et al. (2009) use Integer Linear Programming and cutting planes (Riedel, 2008) for inference in a model similar to Model 2. By using dual decomposition instead, we can exploit tractable substructure and achieve quadratic (Model 2) and cubic (Model 3) runtime guarantees. An advantage of ILP inference are guaranteed certificates of optimality. However, in practice we also gain certificates of optimality for a large fraction of the instances we process. Poon and Vanderwende (2010) use local search and hence provide no such certificates. Their problem formulation also makes n-gram dependency path features hard</context>
<context position="31231" citStr="Riedel et al. (2009)" startWordPosition="5682" endWordPosition="5685">y and Charniak, 2008), and are converted to dependency structures again using Stanford CoreNLP. Based on trigger words collected from the training set, a set of candidate trigger tokens Trig (x) is generated for each sentence x. 5.2 Features The feature function fT (i, t) extracts a per-trigger feature vector for trigger i and type t E T . It creates one active feature for each element in {t, t E TRegI x feats (i). Here feats (i) denotes a collection of representations for the token i: wordform, lemma, POS tag, syntactic heads, syntactic children, and membership in two dictionaries taken from Riedel et al. (2009). For fR (i, j, r) we create active features for each element of {r} x feats (i, j). Here feats (i, j) is a collection of representations of the token pair (i, j) taken from Miwa et al. (2010c) and contains: labelled and unlabeled n-gram dependency paths; edge and vertex walk features, argument and trigger modifiers and heads, words in between. For fB (p, q) we re-use the token pair representations from fR. In particular, we create one active feature for each element in feats (p, q). 5.3 Shared Task 2009 We first evaluate our models on the Bionlp 2009 task 1. The training, development and test</context>
<context position="33655" citStr="Riedel et al. (2009)" startWordPosition="6110" endWordPosition="6113"> best entry of the 2009 task (Björne et al., 2009). This is achieved without careful tuning of thresholds that control flow of information between trigger and argument extraction. Notably, training Model 1 takes approximately 20 minutes using a single core implementation. Contrast this with 20 minutes on 32 cores reported by Poon and Vanderwende (2010). Model 2 focuses on regulation structures and results demonstrate this: F1 for regulations goes up by nearly 2 points. While the impact of joint modeling relative to weaker local baselines has been shown shown by Poon and Vanderwende (2010) and Riedel et al. (2009), our findings here provide evidence that it remains effective even when the baseline system is very competitive. With Model 3 our focus is extended to binding events, improving F1 for such events by at least 5 F1. This also has a positive effect on regulation events, as regulations of binding events can now be more accurately extracted. In total we see a 1.1 F1 increase over the best results reported so far (Miwa et al., 2010b). Crucially, this is achieved using only a single parse tree per sentence, as opposed to three used by Miwa et al. (2010a). Table 2 shows results for the test set. Here</context>
</contexts>
<marker>Riedel, Chun, Takagi, Tsujii, 2009</marker>
<rawString>Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi, and Jun’ichi Tsujii. 2009. A markov logic approach to bio-molecular event extraction. In Proceedings of the Natural Language Processing in Biomedicine NAACL 2009 Workshop (BioNLP ’09), pages 41–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>David McClosky</author>
<author>Mihai Surdeanu</author>
<author>Christopher D Manning</author>
<author>Andrew McCallum</author>
</authors>
<title>Model combination for event extraction in BioNLP</title>
<date>2011</date>
<booktitle>In Proceedings of the Natural Language Processing in Biomedicine NAACL 2011 Workshop (BioNLP ’11),</booktitle>
<contexts>
<context position="7317" citStr="Riedel et al., 2011" startWordPosition="1183" endWordPosition="1186">LP 2009 shared task, the first two models outperform the previous best joint approaches and are competitive when compared to current state-of-the-art. With 57.4 F1 on the test set, the third model yields the best results reported so far with a 1.1 F1 margin to the results of Miwa et al. (2010b). For the BioNLP 2011 Genia task 1 and the BioNLP 2011 Infectious Diseases task, Model 3 yields the second-best and best results reported so far. The second-best results are achieved with Model 3 as is (Riedel and McCallum, 2011), the best results when using Stanford event predictions as input features (Riedel et al., 2011). The margins between Model 3 and the best runner-ups range from 1.9 F1 to 2.8 F1. In the following we will first introduce biomedical event extraction and our notation. Then we go on to present our models and their inference routines. We present related work, show our empirical evaluation, and conclude. 2 Binding Binding Theme Theme Theme Theme Grb2 can be coimmunoprecipitated with Sos1 and Sos2 1 2 3 4 5 6 7 8 quickly validate extracted events. For example, the trigger for event E2 is “inhibit”, as indicated by a dashed line. Theme Theme Theme Figure 2: Two binding events with identical trig</context>
<context position="36332" citStr="Riedel et al., 2011" startWordPosition="6581" endWordPosition="6584">full text articles. In total 908 training, 259 development and 347 test documents are provided. 9 Genia Task 1 TOT Infectious Diseases TOT System System M3+Stanford 56.0 M3+Stanford 55.6 M3 55.2 M3 53.4 UTurku 53.3 Stanford 50.6 MSR-NLP 51.5 UTurku 44.2 ConcordU 50.3 PNNL 42.6 Table 3: F1 scores for the test sets of two tracks in the BioNLP 2011 Shared Task. The top five entries are shown in table 3. Model 3 is the best-performing system that does not use model combination, only outperformed by a version of Model 3 that includes Stanford predictions (McClosky et al., 2011b) as input features (Riedel et al., 2011). Not shown in the table are results for full papers only. Here M3 ranks first with 53.1 F1, while M3+Stanford comes in second with 52.7 F1. The Infectious Diseases (ID) track of the 2011 task has 152 train, 46 development and 118 test documents. Relative to Genia it provides less data and introduces more types of entities as well as the biological process event type. Incorporating these changes into our models is straightforward, and hence we omit details for brevity. Table 3 shows the top five entries for the Infectious Diseases track. Again Model 3 is the bestperforming system that does not</context>
</contexts>
<marker>Riedel, McClosky, Surdeanu, Manning, McCallum, 2011</marker>
<rawString>Sebastian Riedel, David McClosky, Mihai Surdeanu, Christopher D. Manning, and Andrew McCallum. 2011. Model combination for event extraction in BioNLP 2011. In Proceedings of the Natural Language Processing in Biomedicine NAACL 2011 Workshop (BioNLP ’11), June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
</authors>
<title>Improving the accuracy and efficiency of MAP inference for markov logic.</title>
<date>2008</date>
<booktitle>In Proceedings of the 24th Annual Conference on Uncertainty in AI (UAI ’08),</booktitle>
<pages>468--475</pages>
<contexts>
<context position="21589" citStr="Riedel (2008)" startWordPosition="3898" endWordPosition="3899">,Bind = 1 but ¯e6,Bind = 0, we set λ6,Bind = −α where α is some stepsize (chosen according to Koo et al. (2010)). This will decrease the coefficient for e6,Bind, and increase the coefficient for ¯e6,Bind. Hence, we have a higher chance of agreement for this variable in the next iteration. The algorithm repeats the process described above until all variables agree, or some predefined number R of iterations is reached. In the former case we in fact have the exact solution to the original ILP. 1The ILP representation could be taken from the MLNs of Riedel et al. (2009) and the mapping to ILPs of Riedel (2008). Algorithm 2 Subgradient descent for Model 2, and projected subgradient descent for Model 3. require: R: max. iteration, αt: stepsizes t 0 [model 2,3] λ 0 [model 2,3] µ 0 [model 3] repeat model 2 (e, a) bestOut (λ) 2,3 (¯e, ¯a) bestIn (−λ) 3 (e, a) bestOut (cout (λ, µ)) 3 (b, t) bestBind (cbind (µ)) 2,3 λi,t λi,t − αt (ei,t − ¯ei,t) 2,3 λi,j,r λi,j,r − αt (ai,j,r − ¯ai,j,r) h i 3 µi,p,q trig µtrig i,p,q − αt (ei,Bind − ti,p,q) h + i arg1 arg1 ( 3 µi,j,k µi,p,q − αt lai,p,Theme − ti,p,q) h i+ arg2 arg2 3 µi,p,q µi,p,q − αt (ai,q,Theme − ti,p,q) + 2,3 t t + 1 until no λ, µ changed or t &gt; R retu</context>
<context position="28135" citStr="Riedel, 2008" startWordPosition="5174" endWordPosition="5175">r that shifts weights towards features of the gold solution, and away from features of the current guess, whenever the current model makes a mistake. PA learning takes into account a user-defined loss function for which we use a weighted sum sB (p, q) 7 of false positives and false negatives: l (y, y&apos;) def = FP (y, y&apos;) + αFN (y, y&apos;). We set α = 3.8 by optimizing on the BioNLP 2009 development set. guments; such compatibilities have also been shown to be helpful in SRL (Toutanova et al., 2005). 5 Experiments 4 Related Work Riedel et al. (2009) use Integer Linear Programming and cutting planes (Riedel, 2008) for inference in a model similar to Model 2. By using dual decomposition instead, we can exploit tractable substructure and achieve quadratic (Model 2) and cubic (Model 3) runtime guarantees. An advantage of ILP inference are guaranteed certificates of optimality. However, in practice we also gain certificates of optimality for a large fraction of the instances we process. Poon and Vanderwende (2010) use local search and hence provide no such certificates. Their problem formulation also makes n-gram dependency path features harder to incorporate. McClosky et al. (2011b) cast event extraction </context>
</contexts>
<marker>Riedel, 2008</marker>
<rawString>Sebastian Riedel. 2008. Improving the accuracy and efficiency of MAP inference for markov logic. In Proceedings of the 24th Annual Conference on Uncertainty in AI (UAI ’08), pages 468–475.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>A linear programming formulation for global inference in natural language tasks.</title>
<date>2004</date>
<booktitle>In Proceedings of the 8th Conference on Computational Natural Language Learning (CoNLL’ 04),</booktitle>
<pages>1--8</pages>
<contexts>
<context position="29187" citStr="Roth and Yih, 2004" startWordPosition="5346" endWordPosition="5349">ovide no such certificates. Their problem formulation also makes n-gram dependency path features harder to incorporate. McClosky et al. (2011b) cast event extraction as dependency parsing task. Their model assumes that event structures are trees, an assumption that is frequently violated in practice. Finally, all previous joint approaches use heuristics to decide whether binding arguments are part of the same event, while we capture these decisions in the joint model. We follow a long line of research in NLP that addresses search problems using (Integer) Linear Programs (Germann et al., 2001; Roth and Yih, 2004; Riedel and Clarke, 2006). However, instead of using off-the-shelf solvers, we work in the framework of dual decomposition. Here we extend the approach of Rush et al. (2010) in that in addition to equality constraints we dualize more complex coupling constraints between models. This requires us to work with a projected version of subgradient descent. While tailored towards (biomedical) event extraction, we believe that our models can also be effective in a more general Semantic Role Labeling (SRL) context. Using variants of Model 1, we can enforce many of the SRL constraints—such as “unique a</context>
</contexts>
<marker>Roth, Yih, 2004</marker>
<rawString>D. Roth and W. Yih. 2004. A linear programming formulation for global inference in natural language tasks. In Proceedings of the 8th Conference on Computational Natural Language Learning (CoNLL’ 04), pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>David Sontag</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
</authors>
<title>On dual decomposition and linear programming relaxations for natural language processing.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical methods in natural language processing (EMNLP ’10).</booktitle>
<contexts>
<context position="6268" citStr="Rush et al., 2010" startWordPosition="1004" endWordPosition="1007"> where m&apos; is the number of trigger candidates as filtered by the first stage. We achieve these guarantees through a novel algorithm that jointly picks best trigger label and arguments on a per-token basis. Remarkably, it takes roughly as much time to train this model on one core as the model of Poon and Vanderwende (2010) on 32 cores, and leads to better results. The second model enforces additional constraints that ensure consistency between events in hierarchical regulation structures. While inference in this model is more complicated, we show how dual decomposition (Komodakis et al., 2007; Rush et al., 2010) can be used to efficiently find exact solutions for a large fraction of problems. Our third model includes the first two, and explicitly captures which arguments are part in the same event—the third stage of existing pipelines. Due to a complex coupling between this model and the first two, inference here requires a projected version of the sub-gradient technique demonstrated by Rush et al. (2010). When evaluated on the BioNLP 2009 shared task, the first two models outperform the previous best joint approaches and are competitive when compared to current state-of-the-art. With 57.4 F1 on the </context>
<context position="18113" citStr="Rush et al. (2010)" startWordPosition="3228" endWordPosition="3231">s is a requirement on the label of a trigger and the assignment of roles for its incoming edges. Model 2 enforces the above constraint in addition to (e, a) ∈ O, while inheriting the scoring function from Model 1. Hence, using I to denote the set of assignments with consistent trigger labels and incoming edges, we get Y2 def= Y1 ∩ I and s2 (y) def= s1 (y). 3.2.1 Inference Inference in Model 2 amounts to optimizing s2 (e, a) over O ∩ I. This is more involved, as we now have to ensure that when predicting an outgoing edge from trigger i to trigger l there is a high-scoring event at l. We follow Rush et al. (2010) and solve this problem in the framework of dual decomposiAlgorithm 1 Sub-procedures for inference in Model 1, 2 and 3. best label and outgoing edges for all triggers under penalties c bestOut (c) : ∀i yc ← emptyOut (i) y1 ← out (i, c, Treg, R) y2 ← out (i, c, T�reg, R \ {Cause}) yi ← arg maxyE{y0,y1,y2} sc1(i, y) return (yi)i best label and incoming edges for all triggers under penalties c bestIn (c) : ∀l yc ← emptyIn (l) y1 ← in (l, c, T , R \ {None}) yl ← arg maxyE{y0,y1} sc2 (l, y) return (yl)l pick best binding pairs p, q and trigger i for each using penalties c bestBind (c) : ∀p, q bp,q </context>
<context position="29361" citStr="Rush et al. (2010)" startWordPosition="5375" endWordPosition="5378">dency parsing task. Their model assumes that event structures are trees, an assumption that is frequently violated in practice. Finally, all previous joint approaches use heuristics to decide whether binding arguments are part of the same event, while we capture these decisions in the joint model. We follow a long line of research in NLP that addresses search problems using (Integer) Linear Programs (Germann et al., 2001; Roth and Yih, 2004; Riedel and Clarke, 2006). However, instead of using off-the-shelf solvers, we work in the framework of dual decomposition. Here we extend the approach of Rush et al. (2010) in that in addition to equality constraints we dualize more complex coupling constraints between models. This requires us to work with a projected version of subgradient descent. While tailored towards (biomedical) event extraction, we believe that our models can also be effective in a more general Semantic Role Labeling (SRL) context. Using variants of Model 1, we can enforce many of the SRL constraints—such as “unique agent” constraints (Punyakanok et al., 2004)—without having to call out to ILP optimizers. Meza-Ruiz and Riedel (2009) showed that inducing pressure on arguments to be attache</context>
</contexts>
<marker>Rush, Sontag, Collins, Jaakkola, 2010</marker>
<rawString>Alexander M. Rush, David Sontag, Michael Collins, and Tommi Jaakkola. 2010. On dual decomposition and linear programming relaxations for natural language processing. In Proceedings of the Conference on Empirical methods in natural language processing (EMNLP ’10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Aria Haghighi</author>
<author>Christopher D Manning</author>
</authors>
<title>Joint learning improves semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL ’05),</booktitle>
<pages>589--596</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="28019" citStr="Toutanova et al., 2005" startWordPosition="5154" endWordPosition="5157">rning (Crammer and Singer, 2003) with averaging to estimate the weights w for each of our models. PA is an error-driven learner that shifts weights towards features of the gold solution, and away from features of the current guess, whenever the current model makes a mistake. PA learning takes into account a user-defined loss function for which we use a weighted sum sB (p, q) 7 of false positives and false negatives: l (y, y&apos;) def = FP (y, y&apos;) + αFN (y, y&apos;). We set α = 3.8 by optimizing on the BioNLP 2009 development set. guments; such compatibilities have also been shown to be helpful in SRL (Toutanova et al., 2005). 5 Experiments 4 Related Work Riedel et al. (2009) use Integer Linear Programming and cutting planes (Riedel, 2008) for inference in a model similar to Model 2. By using dual decomposition instead, we can exploit tractable substructure and achieve quadratic (Model 2) and cubic (Model 3) runtime guarantees. An advantage of ILP inference are guaranteed certificates of optimality. However, in practice we also gain certificates of optimality for a large fraction of the instances we process. Poon and Vanderwende (2010) use local search and hence provide no such certificates. Their problem formulat</context>
</contexts>
<marker>Toutanova, Haghighi, Manning, 2005</marker>
<rawString>Kristina Toutanova, Aria Haghighi, and Christopher D. Manning. 2005. Joint learning improves semantic role labeling. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL ’05), pages 589–596, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>