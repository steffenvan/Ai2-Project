<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000049">
<title confidence="0.394354">
Word Sense Disambiguation by Learning from Unlabeled Data
</title>
<author confidence="0.84042">
Seong-Bae Parkt, Byoung-Tak Zhangt and Yung Taek Kimt
</author>
<affiliation confidence="0.940919333333333">
Artificial Intelligence Lab (SCAI)
School of Computer Science and Engineering
Seoul National University
</affiliation>
<address confidence="0.929058">
Seoul 151-742, Korea
</address>
<email confidence="0.99865">
t{sbpark,btzhangl@scai.snu.ac.kr tytkim@cse.snu.ac.kr
</email>
<sectionHeader confidence="0.995628" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998303125">
Most corpus-based approaches to
natural language processing suffer
from lack of training data. This
is because acquiring a large num-
ber of labeled data is expensive.
This paper describes a learning
method that exploits unlabeled data
to tackle data sparseness problem.
The method uses committee learn-
ing to predict the labels of unla-
beled data that augment the exist-
ing training data. Our experiments
on word sense disambiguation show
that predictive accuracy is signifi-
cantly improved by using additional
unlabeled data.
</bodyText>
<sectionHeader confidence="0.998522" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999384111111111">
The objective of word sense disambiguation
(WSD) is to identify the correct sense of a
word in context. It is one of the most critical
tasks in most natural language applications,
including information retrieval, information
extraction, and machine translation. The
availability of large-scale corpus and various
machine learning algorithms enabled corpus-
based approach to WSD (Cho and Kim, 1995;
Hwee and Lee, 1996; Wilks and Stevenson,
1998),but a large scale sense-tagged corpus
or aligned bilingual corpus is needed for a
corpus-based approach.
However, most languages except English
do not have a large-scale sense-tagged cor-
pus. Therefore, any corpus-based approach
to WSD for such languages should consider
the following problems:
</bodyText>
<listItem confidence="0.970383166666667">
• There&apos;s no reliable and available sense-
tagged corpus.
• Most words are sense ambiguous.
• Annotating the large corpora requires
human experts, so that it is too expen-
sive.
</listItem>
<bodyText confidence="0.999667225806451">
Because it is expensive to construct sense-
tagged corpus or bilingual corpus, many re-
searchers tried to reduce the number of ex-
amples needed to learn WSD (Atsushi et al.,
1998; Pedersen and Bruce, 1997). Atsushi et
al. (Atsushi et al., 1998) adopted a selec-
tive sampling method to use small number of
examples in training. They defined a train-
ing utility function to select examples with
minimum certainty, and at each training it-
eration the examples with less certainty were
saved in the example database. However, at
each iteration of training the similarity among
word property vectors must be calculated due
to their k-NN like implementation of training
utility.
While labeled examples obtained from a
sense-tagged corpus is expensive and time-
consuming, it is significantly easier to ob-
tain the unlabeled examples. Yarowsky
(Yarowsky, 1995) presented, for the first time,
the possibility that unlabeled examples can
be used for WSD. He used a learning algo-
rithm based on the local context under the
assumption that all instances of a word have
the same intended meaning within any fixed
document and achieved good results with only
a few labeled examples and many unlabeled
ones. Nigam et al. (Nigam et al., 2000) also
showed the unlabeled examples can enhance
the accuracy of text categorization.
</bodyText>
<figure confidence="0.9494486">
Attribute
GFUNC
PARENT
SUBJECT
OBJECT
NMODWORD
ADNWORD
ADNSUBJ
ADNOBJ
Substance
</figure>
<bodyText confidence="0.97787375">
the grammatical function of w
the word of the node modified by w
whether or not PARENT of w has a subject
whether or not PARENT of w has an object
the word of the noun modifier of w
the head word of the adnominal phrase of w
whether or not the adnominal phrase of w has a subject
whether or not the adnominal phrase of w has an object
</bodyText>
<tableCaption confidence="0.994168">
Table 1: The properties used to distinguish the sense of an ambiguous Korean noun w.
</tableCaption>
<bodyText confidence="0.999822">
In this paper, we present a new approach
to word sense disambiguation that is based
on selective sampling algorithm with commit-
tees. In this approach, the number of train-
ing examples is reduced, by determining by
weighted majority voting of multiple classi-
fiers, whether a given training example should
be learned or not. The classifiers of the com-
mittee are first trained on a small set of la-
beled examples and the training set is aug-
mented by a large number of unlabeled exam-
ples. One might think that this has the pos-
sibility that the committee is misled by unla-
beled examples. But, the experimental results
confirm that the accuracy of WSD is increased
by using unlabeled examples when the mem-
bers of the committee are well trained with
labeled examples. We also theoretically show
that performance improvement is guaranteed
by a mild requirement, i.e., the base classi-
fiers need to guess better than random selec-
tion. This is because the possibility misled by
unlabeled examples is reduced by integrating
outputs of multiple classifiers. One advantage
of this method is that it effectively performs
WSD with only a small number of labeled ex-
amples and thus shows possibility of building
word sense disambiguators for the languages
which have no sense-tagged corpus.
The rest of this paper is organized as fol-
lows. Section 2 introduces the general proce-
dure for word sense disambiguation and the
necessity of unlabeled examples. Section 3 ex-
plains how the proposed method works using
both labeled and unlabeled examples. Section
4 presents the experimental results obtained
by using the KAIST raw corpus. Section 5
draws conclusions.
</bodyText>
<sectionHeader confidence="0.955467" genericHeader="method">
2 Word Sense Disambiguation
</sectionHeader>
<bodyText confidence="0.9986749">
Let S 2 fst, ... , skg be the set of possible
senses of a word to be disambiguated. To
determine the sense of the word, we need
to consider the contextual properties. Let
x =&lt; xt, ... , xn &gt; be the vector for rep-
resenting selected contextual features. If we
have a classifier f(x, 0) parameterized with 0,
then the sense of a word with property vec-
tor x can be determined by choosing the most
probable sense s*:
</bodyText>
<equation confidence="0.9294945">
s* = arg max f(x, 0).
SES
</equation>
<bodyText confidence="0.76919625">
The parameters 0 are determined by training
the classifier on a set of labeled examples, L
=
f(xt, st), ... , (xN, sN)g.
</bodyText>
<subsectionHeader confidence="0.994399">
2.1 Property Sets
</subsectionHeader>
<bodyText confidence="0.9626138">
In general, the first step of WSD is to extract
a set of contextual features. To select particu-
lar properties for Korean, the language of our
cencern, the following characteristics should
be considered:
</bodyText>
<listItem confidence="0.90026475">
• Korean is a partially free-order language.
The ordering information on the neigh-
bors of the ambiguous word, therefore,
does not give significantly meaningful in-
formation in Korean.
• In Korean, ellipses appear very often
with a nominative case or objective case.
Therefore, it is difficult to build a large
</listItem>
<bodyText confidence="0.9656052">
scale database of labeled examples with
case markers.
Considering both characteristics and re-
sults of previous work, we select eight prop-
erties for WSD of Korean nouns (Table 1).
Three of them (PARENT, NMODWORD,
ADNWORD) take morphological form as
their value, one (GFUNC) takes 11 values of
grammatical functions1, and others take only
true or false.
</bodyText>
<subsectionHeader confidence="0.998663">
2.2 Unlabeled Data for WSD
</subsectionHeader>
<bodyText confidence="0.999043326086956">
Many researchers tried to develop automated
methods to reduce training cost in language
learning and found out that the cost can be
reduced by active learning which has control
over the training examples (Dagan and Engel-
son, 1997; Liere and Tadepalli, 1997; Zhang,
1994). Though the number of labeled exam-
ples needed is reduced by active learning, the
label of the selected examples must be given
by the human experts. Thus, active learn-
ing is still expensive and a method for auto-
matic labeling unlabeled examples is needed
to have the learner automatically gather in-
formation (Blum and Mitchell, 1998; Peder-
sen and Bruce, 1997; Yarowsky, 1995).
As the unlabeled examples can be obtained
with ease without human experts it makes
WSD robust. Yarowsky (Yarowsky, 1995)
presented the possibility of automatic label-
ing of training examples in WSD and achieved
good results with only a few labeled exam-
ples and many unlabeled examples. On the
other hand, Blum and Mitchell tried to clas-
sify Web pages, in which the description of
each example can be partitioned into distinct
views such as the words occurring on that
page and the words occurring in hyperlinks
(Blum and Mitchell, 1998). By using both
views together, they augmented a small set
of labeled examples with a lot of unlabeled
examples.
The unlabeled examples in WSD can pro-
vide information about the joint probability
These 11 grammatical functions are from
the parser, KEMTS (Korean-to-English Machine
Translation System) developed in Seoul National Uni-
versity, Korea.
distribution over properties but they also can
mislead the learner. However, the possibility
of being misled by the unlabeled examples is
reduced by the committee of classifiers since
combining or integrating the outputs of sev-
eral classifiers in general leads to improved
performance. This is why we use active learn-
ing with committees to select informative un-
labeled examples and label them.
</bodyText>
<sectionHeader confidence="0.985783" genericHeader="method">
3 Active Learning with
Committees for WSD
</sectionHeader>
<subsectionHeader confidence="0.991753">
3.1 Active Learning Using Unlabeled
Examples
</subsectionHeader>
<bodyText confidence="0.999804333333333">
The algorithm for active learning using unla-
beled data is given in Figure 1. It takes two
sets of examples as inputs. A Set L is the one
with labeled examples and D = {x1, ... , xT I
is the one with unlabeled examples where xi
is a property vector. First of all, the training
</bodyText>
<equation confidence="0.9321495">
set L(1)
j (1 &lt; j &lt; M) of labeled examples is
</equation>
<bodyText confidence="0.9998475">
constructed for each base classifier Cj. This
is done by random resampling as in Bagging
(Breiman, 1996). Then, each base classifier
Cj is trained with the set of labeled examples
</bodyText>
<equation confidence="0.8527615">
L(1)
j .
</equation>
<bodyText confidence="0.999920888888889">
After the classifiers are trained on labeled
examples, the training set is augmented by
the unlabeled examples. For each unlabeled
example xt E D, each classifier computes the
sense yj E S which is the label associated with
it, where S is the set of possible sense of xt.
The distribution W over the base classi-
fiers represents the importance weights. As
the distribution can be changed each iter-
ation, the distribution in iteration t is de-
noted by Wt. The importance weight of clas-
sifier Cj under distribution Wt is denoted by
Wt(j). Initially, the base classifiers have equal
weights, so that Wt(j) = 11M.
The sense of the unlabeled example xt is de-
termined by majority voting among Cj&apos;s with
weight distribution W. Formally, the sense yt
of xt is predicted by
</bodyText>
<equation confidence="0.897594">
yt(xt) = arg max
yES
j:Cj(Xt)=y
</equation>
<bodyText confidence="0.7423995">
If most classifiers believe that yt is the correct
Wt(j).
</bodyText>
<figureCaption confidence="0.949333">
Figure 1: The active learning algorithm
</figureCaption>
<bodyText confidence="0.973973166666667">
with committees using unlabeled examples for
WSD.
sense of xt, they need not learn xt because
this example makes no contribution to reduce
the variance over the distribution of exam-
ples. In this case, instead of learning the ex-
ample, the weight of each classifier is updated
in such a way that the classifiers whose pre-
dictions were correct get a higher importance
weight and the classifiers whose predictions
were wrong get a lower importance weight
under the assumption that the correct sense
of xt is yt. This is done by multiplying the
weight of the classifier whose prediction is yt
by certainty at. To ensure the updated Wt+1
form a distribution, Wt+1 is normalized by
constant Zt. Formally, the importance weight
is updated as follows:
</bodyText>
<equation confidence="0.915344">
= Wt (j) at if yj = yt
Wt+1 Zt X 1 otherwise.
</equation>
<bodyText confidence="0.9989278">
The certainty at is computed from error et.
Because we trust that the correct sense of xt
is yt, the error et is the ratio of the number of
classifiers whose predictions are not yt. That
is, at is computed as
</bodyText>
<equation confidence="0.950011666666667">
1 — et
at =
et
</equation>
<bodyText confidence="0.971005">
where et is given as
et = No. of Cj&apos;s whose predictions are not yt
�
M
Note that the smaller et, the larger the value
of at. This implies that, if the sense of xt
is certainly yt and a classifier predicts it, a
higher weight is assigned to the classifier. We
assume that most classifiers believe that yt is
the sense of xt if the value of yt is larger than
a certainty threshold B which is set by trial-
and-error.
However, if the certainty is below the
threshold, the classifiers need to learn the ex-
ample xt yet with belief that the sense of it
is yt. Therefore, the set of training examples,
</bodyText>
<equation confidence="0.456092">
L(t)
</equation>
<bodyText confidence="0.891411">
j ,for the classifier Cj is expanded by
</bodyText>
<equation confidence="0.959885333333333">
L(t+1)
j = L(t)
j + {(xt, yt)}•
Then, each classifier Cj is restructured with
L(t+1)
j .
</equation>
<bodyText confidence="0.880904333333333">
Given an unlabeled example set D = {x1; ::: ;xT}
and a labeled example set L
and a word sense set S E {s1; ::: ; sk} for xi,
Initialize W1(j) = M 1 ,
where M is the number of classifiers in the
committee.
</bodyText>
<figure confidence="0.990850323529412">
Resample L(1) jfrom L for each classifier Cj,
where IL(1)
j I = ILI as done in Bagging.
Train base classifier Cj (1 &lt; j &lt; M) from L(1)
j .
For t = 1; ::: ; T:
1. Each Cj predicts the sense yj E S for xt E D.
Y =&lt; y1; ::: ; yM &gt;
2. Find the most likely sense yt from Y using
distribution W:
3. Set at = 1��t
~t , where
Et = No. of Cj&apos;s whose predictions are not yt
:
M
4. If at is larger than a certainty threshold B,
then update Wt:
_ Wt(j) x at if yj = yt
Wt+1 (j) — Zt 1 otherwise,
where Zt is a normalization constant.
5. Otherwise, every classifier Cj is restructured
from new training set L(t+1)
j :
L(t+1)
j = L(t)
j+ {(xt; yt)}:
Output the final classifier:
f(x) = arg max WT (j):
yES
j:Cj (x)=y
Wt(j):
yt = arg max
yES
j:Cj (xt)=y
</figure>
<bodyText confidence="0.9987895">
This process is repeated until the unlabeled
examples are exhausted. The sense of a new
example x is then determined by weighted
majority voting among the trained classifiers:
</bodyText>
<equation confidence="0.969831333333333">
f(x) = arg max E
yES
j:C�(�)=y
</equation>
<bodyText confidence="0.999548">
where WT (j) is the importance weight of clas-
sifier Cj after the learning process.
</bodyText>
<subsectionHeader confidence="0.999491">
3.2 Theoretical Analysis
</subsectionHeader>
<bodyText confidence="0.999881857142857">
Previous studies show that using multiple
classifiers rather than a single classifier leads
to improved generalization (Breiman, 1996;
Freund et al., 1992) and learning algorithms
which use weak classifiers can be boosted
into strong algorithms (Freund and Schapire,
1996). In addition, Littlestone and Warmuth
(Littlestone and Warmuth, 1994) showed that
the error of the weighted majority algorithm
is linearly bounded on that of the best mem-
ber when the weight of each classifier is de-
termined by held-out examples.
The performance of the proposed method
depends on that of initial base classifiers.
This is because it is highly possible for unla-
beled examples to mislead the learning algo-
rithm if they are poorly trained in their initial
state. However, if the accuracy of the initial
majority voting is larger than 12, the proposed
method performs well as the following theo-
rem shows.
</bodyText>
<construct confidence="0.9858655">
Theorem 1 Assume that every unlabeled
data xt is added to the set of training ex-
amples for all classifiers and the importance
weights are not updated. Suppose that p0 be
the probability that the initial classifiers do
not make errors and , t (0 &lt; , t &lt; 1) be the
probability by which the accuracy is increased
in adding one more correct example or de-
creased in adding one more incorrect example
at iteration t. If p0 &gt; 12, the accuracy does
not decrease as a new unlabeled data is added
to the training data set.
</construct>
<bodyText confidence="0.861928333333333">
Proof. The probability for the classifiers
to predict the correct sense at iteration t = 1,
p1, is
</bodyText>
<equation confidence="0.929767">
p1 = p0(p0 + , 0) + (1 — p0)(p0 — , 0)
= p0(2, 0 + 1) — , 0
because the accuracy can be increased or de-
creased by , 0 with the probability p0 and
1 — p0, respectively. Therefore, without loss
of generality, at iteration t = i + 1, we have
pi+1 = pi(2, i + 1) — , i.
</equation>
<bodyText confidence="0.9721785">
To ensure the accuracy does not decrease, the
condition pi+1 &gt; pi should be satisfied.
</bodyText>
<equation confidence="0.996852">
pi+1 — pi = pi(2, i + 1) — , i — pi
= pi(2, i) — , i &gt; 0
1
</equation>
<bodyText confidence="0.812304666666667">
.&apos;. pi &gt; 2
The theorem follows immediately from this
result. ■
</bodyText>
<subsectionHeader confidence="0.969025">
3.3 Decision Trees as Base Classifiers
</subsectionHeader>
<bodyText confidence="0.999816346153846">
Although any kind of learning algorithms
which meet the conditions for Theorem 1 can
be used as base classifiers, Quinlan&apos;s C4.5 re-
lease 8 (Quinlan, 1993) is used in this paper.
The main reason why decision trees are used
as base classifiers is that there is a fast restruc-
turing algorithm for decision trees. Adding an
unlabeled example with a predicted label to
the existing set of training examples makes
the classifiers restructured. Because the re-
structuring of classifiers is time-consuming,
the proposed method is of little practical use
without an efficient way to restructure. Ut-
goff et al. (Utgoff et al., 1997) presented two
kinds of efficient algorithms for restructuring
decision trees and showed experimentally that
their methods perform well with only small
restructuring cost.
We modified C4.5 so that word match-
ing is accomplished not by comparing mor-
phological forms but by calculating similar-
ity between words to tackle data-sparseness
problem. The similarity between two Ko-
rean words is measured by averaged distance
in WordNet of their English-translated words
(Kim and Kim, 1996).
</bodyText>
<table confidence="0.991655">
WT (j)�
Word No. of Senses No. of Examples Sense Percentage
pear 6.2%
bae 4 876 ship 55.2%
times 13.7%
stomach 24.9%
person 46.2%
bun 3 796 minute 50.8%
indignation 3.0%
the former 28.6%
jonja 2 350 electron 71.4%
bridge 30.9%
dari 2 498 leg 69.1%
</table>
<tableCaption confidence="0.9654145">
Table 2: Various senses of Korean nouns used for the experiments and their distributions in
the corpus.
</tableCaption>
<sectionHeader confidence="0.994009" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.994042">
4.1 Data Set
</subsectionHeader>
<bodyText confidence="0.999993166666667">
We used the KAIST Korean raw corpus for
the experiments. The entire corpus consists
of 10 million words but we used in this pa-
per the corpus containing one million words
excluding the duplicated news articles. Ta-
ble 2 shows various senses of ambiguous Ko-
rean nouns considered and their sense distri-
butions. The percentage column in the table
denotes the ratio that the word is used with
the sense in the corpus. Therefore, we can
regard the maximum percentage as a lower
bound on the correct sense for each word.
</bodyText>
<subsectionHeader confidence="0.99457">
4.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999917461538462">
For the experiments, 15 base classifiers are
used. If there is a tie in predicting senses,
the sense with the lowest order is chosen as
in (Breiman, 1996). For each noun, 90% of
the examples are used for training and the
remaining 10% are used for testing.
Table 3 shows the 10-fold cross validation
result of WSD experiments for nouns listed
in Table 2. The accuracy of the proposed
method shown in Table 3 is measured when
the accuracy is in its best for various ratios of
the number of labeled examples for base clas-
sifiers to total examples. The results show
</bodyText>
<footnote confidence="0.579370333333333">
This corpus is distributed by the Korea Termi-
nology Research Center for Language and Knowledge
Engineering.
</footnote>
<bodyText confidence="0.99974525">
that WSD by selective sampling with com-
mittees using both labeled and unlabeled ex-
amples is comparable to a single learner us-
ing all the labeled examples. In addition, the
method proposed in this paper achieves 26.3%
improvement over the lower bound for `bae&apos;,
41.5% for `bun&apos;, 22.1% for `jonja&apos;, and 4.2%
for `dari&apos;, which is 23.6% improvement on the
average. Especially, for `jonja&apos; the proposed
method shows higher accuracy than the single
C4.5 trained on the whole labeled examples.
Figure 2 shows the performance improved
by using unlabeled examples. This figure
demonstrates that the proposed method out-
performs the one without using unlabeled ex-
amples. The initial learning in the figure
means that the committee is trained on la-
beled examples, but is not augmented by un-
labeled examples. The difference between two
lines is the improved accuracy obtained by
using unlabeled examples. When the accu-
racy of the proposed method gets stabilized
for the first time, the improved accuracy by
using unlabeled examples is 20.2% for `bae&apos;,
9.9% for `bun, 13.5% `jonja&apos;, and 13.4% for
`dari&apos;. It should be mentioned that the results
also show that the accuracy of the proposed
method may be dropped when the classifiers
are trained on too small a set of labeled data,
as is the case in the early stages of Figure 2.
However, in typical situations where the clas-
sifiers are trained on minimum training set
</bodyText>
<table confidence="0.985409461538461">
Using Partially Using All
Labeled Data Labeled Data Lower Bound
81.5 ± 7.7% 82.3% ± 5.9% 55.2%
92.3 ± 7.7% 94.3% ± 5.7% 50.8%
93.5 ± 6.5% 90.6% ± 9.4% 71.4%
73.3 ± 14.2% 80.8 ± 10.9% 69.1%
85.2% 87.0% 61.6%
Word
bae
bun
jonja
dari
Average
</table>
<tableCaption confidence="0.999589">
Table 3: The accuracy of WSD for Korean nouns by the proposed method.
</tableCaption>
<bodyText confidence="0.9999813125">
size, this does not happen as the results of
other nouns show. In addition, we can find in
this particular experiment that the accuracy
is always improved by using unlabeled exam-
ples if only about 22% of training examples,
on the average, are labeled in advance.
In Figure 2(a), it is interesting to observe
jumps in the accuracy curve. The jump ap-
pears because the unlabeled examples mislead
the classifiers only when the classifiers are
poorly trained, but they play an important
role as information to select senses when the
classifiers are well trained on labeled exam-
ples. Other nouns show similar phenomena
though the percentage of labeled examples is
different when the accuracy gets flat.
</bodyText>
<sectionHeader confidence="0.999672" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999664375">
In this paper, we proposed a new method
for word sense disambiguation that is based
on unlabeled data. Using unlabeled data is
especially important in corpus-based natural
language processing because raw corpora are
ubiquitous while labeled data are expensive
to obtain. In a series of experiments on word
sense disambiguation of Korean nouns we ob-
served that the accuracy is improved up to
20.2% using only 32% of labeled data. This
implies, the learning model trained on a small
number of labeled data can be enhanced by
using additional unlabeled data. We also the-
oretically showed that the predictive accuracy
is always improved if the individual classifiers
do better than random selection after being
trained on labeled data.
As the labels of unlabeled data are es-
timated by committees of multiple decision
trees, the burden of manual labeling is min-
imized by using unlabeled data. Thus, the
proposed method seems especially effective
and useful for the languages for which a large-
scale sense-tagged corpus is not available yet.
Another advantage of the proposed method
is that it can be applied to other kinds of
language learning problems such as POS-
tagging, PP attachment, and text classifica-
tion. These problems are similar to word
sense disambiguation in the sense that unla-
beled raw data are abundant but labeled data
are limited and expensive to obtain.
</bodyText>
<sectionHeader confidence="0.996905" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9995196">
This research was supported in part by the
Korean Ministry of Education under the
BK21 Program and by the Korean Ministry
of Information and Communication through
IITA under grant 98-199.
</bodyText>
<sectionHeader confidence="0.998965" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.971632058823529">
F. Atsushi, I. Kentaro, T. Takenobu, and
T. Hozumi. 1998. Selective sampling of ef-
fective example sentence sets for word sense
disambiguation. Computational Linguistics,
24(4):573-597.
A. Blum and T. Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In
Proceedings of COLT-98, pages 92{100.
L. Breiman. 1996. Bagging predictors. Machine
Learning, 24:123{140.
J.-M. Cho and G.-C. Kim. 1995. Korean verb
sense disambiguation using distributional infor-
mation from corpora. In Proceedings of Natural
Language Processing Pacific Rim Symposium,
pages 691{696.
I. Dagan and S. Engelson. 1997. Committee-
based sampling for training probabilistic classi-
</reference>
<figure confidence="0.999246529411765">
Accuracy (%)
10 20 30 40 50 60 70 80 90 100
Ratio of The Number of Labeled Examples to The Number of Total Examples (%)
10 20 30 40 50 60 70 80 90 100
Ratio of The Number of Labeled Examples to The Number of Total Examples (%)
(c) jonja
10 20 30 40 50 60 70 80 90 100
Ratio of The Number of Labeled Examples to The Number of Total Examples (%)
10 20 30 40 50 60 70 80 90 100
Ratio of The Number of Labeled Examples to The Number of Total Examples (%)
(d) dari
Initial Learning
The Proposed Method
(a) bae
Accuracy (%)
90
80
70
60
50
40
30
20
Initial Learning
The Proposed Method
90
80
70
60
50
40
Initial Learning
The Proposed Method
(b) bun
Accuracy (%)
80
70
60
50
40
30
Initial Learning
The Proposed Method
Accuracy (%) 85
80
75
70
65
60
55
50
</figure>
<figureCaption confidence="0.996944">
Figure 2: Improvement in accuracy by using unlabeled examples.
</figureCaption>
<reference confidence="0.99984598">
fiers. In Proceedings of the Fourteenth Interna-
tional Conference on Machine Learning, pages
150-157.
Y. Freund and R. Schapire. 1996. Experiments
with a new boosting algorithm. In Proceedings
of the Thirteenth International Conference on
Machine Learning, pages 148-156.
Y. Freund, H. Seung, E. Shamir, and N. Tishby.
1992. Selective sampling with query by com-
mittee algorithm. In Proceedings of NIPS-92,
pages 483-490.
T. Hwee and H. Lee. 1996. Integrating multiple
knowledge sources to disambiguate word sense:
An exemplar-based approach. In Proceedings
of the 34th Annual Meeting of the ACL, pages
40-47.
Nari Kim and Y.-T. Kim. 1996. Ambiguity reso-
lution of korean sentence analysis and korean-
english transfer based on korean verb patterns.
Journal of KISS, 23(7):766-775. in Korean.
R. Liere and P. Tadepalli. 1997. Active learn-
ing with committees for text categorization. In
Proceedings of AAAI-97, pages 591-596.
N. Littlestone and M. Warmuth. 1994. The
weighted majority algorithm. Information and
Computation, 108(2):212-261.
K. Nigam, A. McCallum, S. Thrun, and
T. Mitchell. 2000. Learning to classify text
from labeled and unlabeled documents. Ma-
chine Learning, 39:1-32.
T. Pedersen and R. Bruce. 1997. Distinguishing
word senses in untagged text. In Proceedings of
the Second Conference on Empirical Methods in
Natural Language Processing, pages 399-401.
R. Quinlan. 1993. C4.5: Programs For Machine
Learning. Morgan Kaufmann Publishers.
P. Utgoff, N. Berkman, and J. Clouse. 1997. De-
cision tree induction based on efficient tree re-
structuring. Machine Learning, 29:5-44.
Y. Wilks and M. Stevenson. 1998. Word sense dis-
ambiguation using optimised combinations of
knowledge sources. In Proceedings of COLING-
ACL &apos;98, pages 1398-1402.
D. Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In
Proceedings of the 33rd Annual Meeting of the
ACL, pages 189-196.
B.-T. Zhang. 1994. Accerlated learning by ac-
tive example selection. International Journal
of Neural Systems, 5(1):67-75.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.890031">
<title confidence="0.999587">Word Sense Disambiguation by Learning from Unlabeled Data</title>
<author confidence="0.993012">Byoung-Tak Yung Taek</author>
<affiliation confidence="0.98128">Artificial Intelligence Lab (SCAI) School of Computer Science and Engineering Seoul National University</affiliation>
<address confidence="0.969375">Seoul 151-742, Korea</address>
<abstract confidence="0.998449470588235">Most corpus-based approaches to natural language processing suffer from lack of training data. This is because acquiring a large number of labeled data is expensive. This paper describes a learning method that exploits unlabeled data to tackle data sparseness problem. The method uses committee learning to predict the labels of unlabeled data that augment the existing training data. Our experiments on word sense disambiguation show that predictive accuracy is significantly improved by using additional unlabeled data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>F Atsushi</author>
<author>I Kentaro</author>
<author>T Takenobu</author>
<author>T Hozumi</author>
</authors>
<title>Selective sampling of effective example sentence sets for word sense disambiguation.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--4</pages>
<contexts>
<context position="1923" citStr="Atsushi et al., 1998" startWordPosition="284" endWordPosition="287">e-tagged corpus or aligned bilingual corpus is needed for a corpus-based approach. However, most languages except English do not have a large-scale sense-tagged corpus. Therefore, any corpus-based approach to WSD for such languages should consider the following problems: • There&apos;s no reliable and available sensetagged corpus. • Most words are sense ambiguous. • Annotating the large corpora requires human experts, so that it is too expensive. Because it is expensive to construct sensetagged corpus or bilingual corpus, many researchers tried to reduce the number of examples needed to learn WSD (Atsushi et al., 1998; Pedersen and Bruce, 1997). Atsushi et al. (Atsushi et al., 1998) adopted a selective sampling method to use small number of examples in training. They defined a training utility function to select examples with minimum certainty, and at each training iteration the examples with less certainty were saved in the example database. However, at each iteration of training the similarity among word property vectors must be calculated due to their k-NN like implementation of training utility. While labeled examples obtained from a sense-tagged corpus is expensive and timeconsuming, it is significant</context>
</contexts>
<marker>Atsushi, Kentaro, Takenobu, Hozumi, 1998</marker>
<rawString>F. Atsushi, I. Kentaro, T. Takenobu, and T. Hozumi. 1998. Selective sampling of effective example sentence sets for word sense disambiguation. Computational Linguistics, 24(4):573-597.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>T Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of COLT-98,</booktitle>
<pages>92--100</pages>
<contexts>
<context position="7286" citStr="Blum and Mitchell, 1998" startWordPosition="1194" endWordPosition="1197">labeled Data for WSD Many researchers tried to develop automated methods to reduce training cost in language learning and found out that the cost can be reduced by active learning which has control over the training examples (Dagan and Engelson, 1997; Liere and Tadepalli, 1997; Zhang, 1994). Though the number of labeled examples needed is reduced by active learning, the label of the selected examples must be given by the human experts. Thus, active learning is still expensive and a method for automatic labeling unlabeled examples is needed to have the learner automatically gather information (Blum and Mitchell, 1998; Pedersen and Bruce, 1997; Yarowsky, 1995). As the unlabeled examples can be obtained with ease without human experts it makes WSD robust. Yarowsky (Yarowsky, 1995) presented the possibility of automatic labeling of training examples in WSD and achieved good results with only a few labeled examples and many unlabeled examples. On the other hand, Blum and Mitchell tried to classify Web pages, in which the description of each example can be partitioned into distinct views such as the words occurring on that page and the words occurring in hyperlinks (Blum and Mitchell, 1998). By using both view</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>A. Blum and T. Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of COLT-98, pages 92{100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Breiman</author>
</authors>
<title>Bagging predictors.</title>
<date>1996</date>
<booktitle>Machine Learning,</booktitle>
<pages>24--123</pages>
<contexts>
<context position="9120" citStr="Breiman, 1996" startWordPosition="1505" endWordPosition="1506"> why we use active learning with committees to select informative unlabeled examples and label them. 3 Active Learning with Committees for WSD 3.1 Active Learning Using Unlabeled Examples The algorithm for active learning using unlabeled data is given in Figure 1. It takes two sets of examples as inputs. A Set L is the one with labeled examples and D = {x1, ... , xT I is the one with unlabeled examples where xi is a property vector. First of all, the training set L(1) j (1 &lt; j &lt; M) of labeled examples is constructed for each base classifier Cj. This is done by random resampling as in Bagging (Breiman, 1996). Then, each base classifier Cj is trained with the set of labeled examples L(1) j . After the classifiers are trained on labeled examples, the training set is augmented by the unlabeled examples. For each unlabeled example xt E D, each classifier computes the sense yj E S which is the label associated with it, where S is the set of possible sense of xt. The distribution W over the base classifiers represents the importance weights. As the distribution can be changed each iteration, the distribution in iteration t is denoted by Wt. The importance weight of classifier Cj under distribution Wt i</context>
<context position="13230" citStr="Breiman, 1996" startWordPosition="2287" endWordPosition="2288">om new training set L(t+1) j : L(t+1) j = L(t) j+ {(xt; yt)}: Output the final classifier: f(x) = arg max WT (j): yES j:Cj (x)=y Wt(j): yt = arg max yES j:Cj (xt)=y This process is repeated until the unlabeled examples are exhausted. The sense of a new example x is then determined by weighted majority voting among the trained classifiers: f(x) = arg max E yES j:C�(�)=y where WT (j) is the importance weight of classifier Cj after the learning process. 3.2 Theoretical Analysis Previous studies show that using multiple classifiers rather than a single classifier leads to improved generalization (Breiman, 1996; Freund et al., 1992) and learning algorithms which use weak classifiers can be boosted into strong algorithms (Freund and Schapire, 1996). In addition, Littlestone and Warmuth (Littlestone and Warmuth, 1994) showed that the error of the weighted majority algorithm is linearly bounded on that of the best member when the weight of each classifier is determined by held-out examples. The performance of the proposed method depends on that of initial base classifiers. This is because it is highly possible for unlabeled examples to mislead the learning algorithm if they are poorly trained in their </context>
<context position="17286" citStr="Breiman, 1996" startWordPosition="3012" endWordPosition="3013">sists of 10 million words but we used in this paper the corpus containing one million words excluding the duplicated news articles. Table 2 shows various senses of ambiguous Korean nouns considered and their sense distributions. The percentage column in the table denotes the ratio that the word is used with the sense in the corpus. Therefore, we can regard the maximum percentage as a lower bound on the correct sense for each word. 4.2 Experimental Results For the experiments, 15 base classifiers are used. If there is a tie in predicting senses, the sense with the lowest order is chosen as in (Breiman, 1996). For each noun, 90% of the examples are used for training and the remaining 10% are used for testing. Table 3 shows the 10-fold cross validation result of WSD experiments for nouns listed in Table 2. The accuracy of the proposed method shown in Table 3 is measured when the accuracy is in its best for various ratios of the number of labeled examples for base classifiers to total examples. The results show This corpus is distributed by the Korea Terminology Research Center for Language and Knowledge Engineering. that WSD by selective sampling with committees using both labeled and unlabeled exa</context>
</contexts>
<marker>Breiman, 1996</marker>
<rawString>L. Breiman. 1996. Bagging predictors. Machine Learning, 24:123{140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-M Cho</author>
<author>G-C Kim</author>
</authors>
<title>Korean verb sense disambiguation using distributional information from corpora.</title>
<date>1995</date>
<booktitle>In Proceedings of Natural Language Processing Pacific Rim Symposium,</booktitle>
<pages>691--696</pages>
<contexts>
<context position="1232" citStr="Cho and Kim, 1995" startWordPosition="173" endWordPosition="176">the labels of unlabeled data that augment the existing training data. Our experiments on word sense disambiguation show that predictive accuracy is significantly improved by using additional unlabeled data. 1 Introduction The objective of word sense disambiguation (WSD) is to identify the correct sense of a word in context. It is one of the most critical tasks in most natural language applications, including information retrieval, information extraction, and machine translation. The availability of large-scale corpus and various machine learning algorithms enabled corpusbased approach to WSD (Cho and Kim, 1995; Hwee and Lee, 1996; Wilks and Stevenson, 1998),but a large scale sense-tagged corpus or aligned bilingual corpus is needed for a corpus-based approach. However, most languages except English do not have a large-scale sense-tagged corpus. Therefore, any corpus-based approach to WSD for such languages should consider the following problems: • There&apos;s no reliable and available sensetagged corpus. • Most words are sense ambiguous. • Annotating the large corpora requires human experts, so that it is too expensive. Because it is expensive to construct sensetagged corpus or bilingual corpus, many r</context>
</contexts>
<marker>Cho, Kim, 1995</marker>
<rawString>J.-M. Cho and G.-C. Kim. 1995. Korean verb sense disambiguation using distributional information from corpora. In Proceedings of Natural Language Processing Pacific Rim Symposium, pages 691{696.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>S Engelson</author>
</authors>
<title>Committeebased sampling for training probabilistic classifiers.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth International Conference on Machine Learning,</booktitle>
<pages>150--157</pages>
<contexts>
<context position="6913" citStr="Dagan and Engelson, 1997" startWordPosition="1131" endWordPosition="1135">o build a large scale database of labeled examples with case markers. Considering both characteristics and results of previous work, we select eight properties for WSD of Korean nouns (Table 1). Three of them (PARENT, NMODWORD, ADNWORD) take morphological form as their value, one (GFUNC) takes 11 values of grammatical functions1, and others take only true or false. 2.2 Unlabeled Data for WSD Many researchers tried to develop automated methods to reduce training cost in language learning and found out that the cost can be reduced by active learning which has control over the training examples (Dagan and Engelson, 1997; Liere and Tadepalli, 1997; Zhang, 1994). Though the number of labeled examples needed is reduced by active learning, the label of the selected examples must be given by the human experts. Thus, active learning is still expensive and a method for automatic labeling unlabeled examples is needed to have the learner automatically gather information (Blum and Mitchell, 1998; Pedersen and Bruce, 1997; Yarowsky, 1995). As the unlabeled examples can be obtained with ease without human experts it makes WSD robust. Yarowsky (Yarowsky, 1995) presented the possibility of automatic labeling of training e</context>
</contexts>
<marker>Dagan, Engelson, 1997</marker>
<rawString>I. Dagan and S. Engelson. 1997. Committeebased sampling for training probabilistic classifiers. In Proceedings of the Fourteenth International Conference on Machine Learning, pages 150-157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R Schapire</author>
</authors>
<title>Experiments with a new boosting algorithm.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth International Conference on Machine Learning,</booktitle>
<pages>148--156</pages>
<contexts>
<context position="13369" citStr="Freund and Schapire, 1996" startWordPosition="2306" endWordPosition="2309">=y Wt(j): yt = arg max yES j:Cj (xt)=y This process is repeated until the unlabeled examples are exhausted. The sense of a new example x is then determined by weighted majority voting among the trained classifiers: f(x) = arg max E yES j:C�(�)=y where WT (j) is the importance weight of classifier Cj after the learning process. 3.2 Theoretical Analysis Previous studies show that using multiple classifiers rather than a single classifier leads to improved generalization (Breiman, 1996; Freund et al., 1992) and learning algorithms which use weak classifiers can be boosted into strong algorithms (Freund and Schapire, 1996). In addition, Littlestone and Warmuth (Littlestone and Warmuth, 1994) showed that the error of the weighted majority algorithm is linearly bounded on that of the best member when the weight of each classifier is determined by held-out examples. The performance of the proposed method depends on that of initial base classifiers. This is because it is highly possible for unlabeled examples to mislead the learning algorithm if they are poorly trained in their initial state. However, if the accuracy of the initial majority voting is larger than 12, the proposed method performs well as the followin</context>
</contexts>
<marker>Freund, Schapire, 1996</marker>
<rawString>Y. Freund and R. Schapire. 1996. Experiments with a new boosting algorithm. In Proceedings of the Thirteenth International Conference on Machine Learning, pages 148-156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>H Seung</author>
<author>E Shamir</author>
<author>N Tishby</author>
</authors>
<title>Selective sampling with query by committee algorithm.</title>
<date>1992</date>
<booktitle>In Proceedings of NIPS-92,</booktitle>
<pages>483--490</pages>
<contexts>
<context position="13252" citStr="Freund et al., 1992" startWordPosition="2289" endWordPosition="2292"> set L(t+1) j : L(t+1) j = L(t) j+ {(xt; yt)}: Output the final classifier: f(x) = arg max WT (j): yES j:Cj (x)=y Wt(j): yt = arg max yES j:Cj (xt)=y This process is repeated until the unlabeled examples are exhausted. The sense of a new example x is then determined by weighted majority voting among the trained classifiers: f(x) = arg max E yES j:C�(�)=y where WT (j) is the importance weight of classifier Cj after the learning process. 3.2 Theoretical Analysis Previous studies show that using multiple classifiers rather than a single classifier leads to improved generalization (Breiman, 1996; Freund et al., 1992) and learning algorithms which use weak classifiers can be boosted into strong algorithms (Freund and Schapire, 1996). In addition, Littlestone and Warmuth (Littlestone and Warmuth, 1994) showed that the error of the weighted majority algorithm is linearly bounded on that of the best member when the weight of each classifier is determined by held-out examples. The performance of the proposed method depends on that of initial base classifiers. This is because it is highly possible for unlabeled examples to mislead the learning algorithm if they are poorly trained in their initial state. However</context>
</contexts>
<marker>Freund, Seung, Shamir, Tishby, 1992</marker>
<rawString>Y. Freund, H. Seung, E. Shamir, and N. Tishby. 1992. Selective sampling with query by committee algorithm. In Proceedings of NIPS-92, pages 483-490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hwee</author>
<author>H Lee</author>
</authors>
<title>Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the ACL,</booktitle>
<pages>40--47</pages>
<contexts>
<context position="1252" citStr="Hwee and Lee, 1996" startWordPosition="177" endWordPosition="180">eled data that augment the existing training data. Our experiments on word sense disambiguation show that predictive accuracy is significantly improved by using additional unlabeled data. 1 Introduction The objective of word sense disambiguation (WSD) is to identify the correct sense of a word in context. It is one of the most critical tasks in most natural language applications, including information retrieval, information extraction, and machine translation. The availability of large-scale corpus and various machine learning algorithms enabled corpusbased approach to WSD (Cho and Kim, 1995; Hwee and Lee, 1996; Wilks and Stevenson, 1998),but a large scale sense-tagged corpus or aligned bilingual corpus is needed for a corpus-based approach. However, most languages except English do not have a large-scale sense-tagged corpus. Therefore, any corpus-based approach to WSD for such languages should consider the following problems: • There&apos;s no reliable and available sensetagged corpus. • Most words are sense ambiguous. • Annotating the large corpora requires human experts, so that it is too expensive. Because it is expensive to construct sensetagged corpus or bilingual corpus, many researchers tried to </context>
</contexts>
<marker>Hwee, Lee, 1996</marker>
<rawString>T. Hwee and H. Lee. 1996. Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach. In Proceedings of the 34th Annual Meeting of the ACL, pages 40-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nari Kim</author>
<author>Y-T Kim</author>
</authors>
<title>Ambiguity resolution of korean sentence analysis and koreanenglish transfer based on korean verb patterns.</title>
<date>1996</date>
<journal>Journal of KISS,</journal>
<pages>23--7</pages>
<note>in Korean.</note>
<contexts>
<context position="16213" citStr="Kim and Kim, 1996" startWordPosition="2820" endWordPosition="2823">ime-consuming, the proposed method is of little practical use without an efficient way to restructure. Utgoff et al. (Utgoff et al., 1997) presented two kinds of efficient algorithms for restructuring decision trees and showed experimentally that their methods perform well with only small restructuring cost. We modified C4.5 so that word matching is accomplished not by comparing morphological forms but by calculating similarity between words to tackle data-sparseness problem. The similarity between two Korean words is measured by averaged distance in WordNet of their English-translated words (Kim and Kim, 1996). WT (j)� Word No. of Senses No. of Examples Sense Percentage pear 6.2% bae 4 876 ship 55.2% times 13.7% stomach 24.9% person 46.2% bun 3 796 minute 50.8% indignation 3.0% the former 28.6% jonja 2 350 electron 71.4% bridge 30.9% dari 2 498 leg 69.1% Table 2: Various senses of Korean nouns used for the experiments and their distributions in the corpus. 4 Experiments 4.1 Data Set We used the KAIST Korean raw corpus for the experiments. The entire corpus consists of 10 million words but we used in this paper the corpus containing one million words excluding the duplicated news articles. Table 2 s</context>
</contexts>
<marker>Kim, Kim, 1996</marker>
<rawString>Nari Kim and Y.-T. Kim. 1996. Ambiguity resolution of korean sentence analysis and koreanenglish transfer based on korean verb patterns. Journal of KISS, 23(7):766-775. in Korean.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Liere</author>
<author>P Tadepalli</author>
</authors>
<title>Active learning with committees for text categorization.</title>
<date>1997</date>
<booktitle>In Proceedings of AAAI-97,</booktitle>
<pages>591--596</pages>
<contexts>
<context position="6940" citStr="Liere and Tadepalli, 1997" startWordPosition="1136" endWordPosition="1139">base of labeled examples with case markers. Considering both characteristics and results of previous work, we select eight properties for WSD of Korean nouns (Table 1). Three of them (PARENT, NMODWORD, ADNWORD) take morphological form as their value, one (GFUNC) takes 11 values of grammatical functions1, and others take only true or false. 2.2 Unlabeled Data for WSD Many researchers tried to develop automated methods to reduce training cost in language learning and found out that the cost can be reduced by active learning which has control over the training examples (Dagan and Engelson, 1997; Liere and Tadepalli, 1997; Zhang, 1994). Though the number of labeled examples needed is reduced by active learning, the label of the selected examples must be given by the human experts. Thus, active learning is still expensive and a method for automatic labeling unlabeled examples is needed to have the learner automatically gather information (Blum and Mitchell, 1998; Pedersen and Bruce, 1997; Yarowsky, 1995). As the unlabeled examples can be obtained with ease without human experts it makes WSD robust. Yarowsky (Yarowsky, 1995) presented the possibility of automatic labeling of training examples in WSD and achieved</context>
</contexts>
<marker>Liere, Tadepalli, 1997</marker>
<rawString>R. Liere and P. Tadepalli. 1997. Active learning with committees for text categorization. In Proceedings of AAAI-97, pages 591-596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Littlestone</author>
<author>M Warmuth</author>
</authors>
<title>The weighted majority algorithm.</title>
<date>1994</date>
<journal>Information and Computation,</journal>
<pages>108--2</pages>
<contexts>
<context position="13439" citStr="Littlestone and Warmuth, 1994" startWordPosition="2315" endWordPosition="2318">til the unlabeled examples are exhausted. The sense of a new example x is then determined by weighted majority voting among the trained classifiers: f(x) = arg max E yES j:C�(�)=y where WT (j) is the importance weight of classifier Cj after the learning process. 3.2 Theoretical Analysis Previous studies show that using multiple classifiers rather than a single classifier leads to improved generalization (Breiman, 1996; Freund et al., 1992) and learning algorithms which use weak classifiers can be boosted into strong algorithms (Freund and Schapire, 1996). In addition, Littlestone and Warmuth (Littlestone and Warmuth, 1994) showed that the error of the weighted majority algorithm is linearly bounded on that of the best member when the weight of each classifier is determined by held-out examples. The performance of the proposed method depends on that of initial base classifiers. This is because it is highly possible for unlabeled examples to mislead the learning algorithm if they are poorly trained in their initial state. However, if the accuracy of the initial majority voting is larger than 12, the proposed method performs well as the following theorem shows. Theorem 1 Assume that every unlabeled data xt is adde</context>
</contexts>
<marker>Littlestone, Warmuth, 1994</marker>
<rawString>N. Littlestone and M. Warmuth. 1994. The weighted majority algorithm. Information and Computation, 108(2):212-261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Nigam</author>
<author>A McCallum</author>
<author>S Thrun</author>
<author>T Mitchell</author>
</authors>
<title>Learning to classify text from labeled and unlabeled documents.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--1</pages>
<contexts>
<context position="2965" citStr="Nigam et al., 2000" startWordPosition="454" endWordPosition="457">alculated due to their k-NN like implementation of training utility. While labeled examples obtained from a sense-tagged corpus is expensive and timeconsuming, it is significantly easier to obtain the unlabeled examples. Yarowsky (Yarowsky, 1995) presented, for the first time, the possibility that unlabeled examples can be used for WSD. He used a learning algorithm based on the local context under the assumption that all instances of a word have the same intended meaning within any fixed document and achieved good results with only a few labeled examples and many unlabeled ones. Nigam et al. (Nigam et al., 2000) also showed the unlabeled examples can enhance the accuracy of text categorization. Attribute GFUNC PARENT SUBJECT OBJECT NMODWORD ADNWORD ADNSUBJ ADNOBJ Substance the grammatical function of w the word of the node modified by w whether or not PARENT of w has a subject whether or not PARENT of w has an object the word of the noun modifier of w the head word of the adnominal phrase of w whether or not the adnominal phrase of w has a subject whether or not the adnominal phrase of w has an object Table 1: The properties used to distinguish the sense of an ambiguous Korean noun w. In this paper, </context>
</contexts>
<marker>Nigam, McCallum, Thrun, Mitchell, 2000</marker>
<rawString>K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. 2000. Learning to classify text from labeled and unlabeled documents. Machine Learning, 39:1-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>R Bruce</author>
</authors>
<title>Distinguishing word senses in untagged text.</title>
<date>1997</date>
<booktitle>In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>399--401</pages>
<contexts>
<context position="1950" citStr="Pedersen and Bruce, 1997" startWordPosition="288" endWordPosition="291">gned bilingual corpus is needed for a corpus-based approach. However, most languages except English do not have a large-scale sense-tagged corpus. Therefore, any corpus-based approach to WSD for such languages should consider the following problems: • There&apos;s no reliable and available sensetagged corpus. • Most words are sense ambiguous. • Annotating the large corpora requires human experts, so that it is too expensive. Because it is expensive to construct sensetagged corpus or bilingual corpus, many researchers tried to reduce the number of examples needed to learn WSD (Atsushi et al., 1998; Pedersen and Bruce, 1997). Atsushi et al. (Atsushi et al., 1998) adopted a selective sampling method to use small number of examples in training. They defined a training utility function to select examples with minimum certainty, and at each training iteration the examples with less certainty were saved in the example database. However, at each iteration of training the similarity among word property vectors must be calculated due to their k-NN like implementation of training utility. While labeled examples obtained from a sense-tagged corpus is expensive and timeconsuming, it is significantly easier to obtain the unl</context>
<context position="7312" citStr="Pedersen and Bruce, 1997" startWordPosition="1198" endWordPosition="1202"> researchers tried to develop automated methods to reduce training cost in language learning and found out that the cost can be reduced by active learning which has control over the training examples (Dagan and Engelson, 1997; Liere and Tadepalli, 1997; Zhang, 1994). Though the number of labeled examples needed is reduced by active learning, the label of the selected examples must be given by the human experts. Thus, active learning is still expensive and a method for automatic labeling unlabeled examples is needed to have the learner automatically gather information (Blum and Mitchell, 1998; Pedersen and Bruce, 1997; Yarowsky, 1995). As the unlabeled examples can be obtained with ease without human experts it makes WSD robust. Yarowsky (Yarowsky, 1995) presented the possibility of automatic labeling of training examples in WSD and achieved good results with only a few labeled examples and many unlabeled examples. On the other hand, Blum and Mitchell tried to classify Web pages, in which the description of each example can be partitioned into distinct views such as the words occurring on that page and the words occurring in hyperlinks (Blum and Mitchell, 1998). By using both views together, they augmented</context>
</contexts>
<marker>Pedersen, Bruce, 1997</marker>
<rawString>T. Pedersen and R. Bruce. 1997. Distinguishing word senses in untagged text. In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, pages 399-401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Quinlan</author>
</authors>
<title>C4.5: Programs For Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann Publishers.</publisher>
<contexts>
<context position="15266" citStr="Quinlan, 1993" startWordPosition="2673" endWordPosition="2674">— , 0 because the accuracy can be increased or decreased by , 0 with the probability p0 and 1 — p0, respectively. Therefore, without loss of generality, at iteration t = i + 1, we have pi+1 = pi(2, i + 1) — , i. To ensure the accuracy does not decrease, the condition pi+1 &gt; pi should be satisfied. pi+1 — pi = pi(2, i + 1) — , i — pi = pi(2, i) — , i &gt; 0 1 .&apos;. pi &gt; 2 The theorem follows immediately from this result. ■ 3.3 Decision Trees as Base Classifiers Although any kind of learning algorithms which meet the conditions for Theorem 1 can be used as base classifiers, Quinlan&apos;s C4.5 release 8 (Quinlan, 1993) is used in this paper. The main reason why decision trees are used as base classifiers is that there is a fast restructuring algorithm for decision trees. Adding an unlabeled example with a predicted label to the existing set of training examples makes the classifiers restructured. Because the restructuring of classifiers is time-consuming, the proposed method is of little practical use without an efficient way to restructure. Utgoff et al. (Utgoff et al., 1997) presented two kinds of efficient algorithms for restructuring decision trees and showed experimentally that their methods perform we</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>R. Quinlan. 1993. C4.5: Programs For Machine Learning. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Utgoff</author>
<author>N Berkman</author>
<author>J Clouse</author>
</authors>
<title>Decision tree induction based on efficient tree restructuring.</title>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<pages>29--5</pages>
<contexts>
<context position="15733" citStr="Utgoff et al., 1997" startWordPosition="2748" endWordPosition="2751">lthough any kind of learning algorithms which meet the conditions for Theorem 1 can be used as base classifiers, Quinlan&apos;s C4.5 release 8 (Quinlan, 1993) is used in this paper. The main reason why decision trees are used as base classifiers is that there is a fast restructuring algorithm for decision trees. Adding an unlabeled example with a predicted label to the existing set of training examples makes the classifiers restructured. Because the restructuring of classifiers is time-consuming, the proposed method is of little practical use without an efficient way to restructure. Utgoff et al. (Utgoff et al., 1997) presented two kinds of efficient algorithms for restructuring decision trees and showed experimentally that their methods perform well with only small restructuring cost. We modified C4.5 so that word matching is accomplished not by comparing morphological forms but by calculating similarity between words to tackle data-sparseness problem. The similarity between two Korean words is measured by averaged distance in WordNet of their English-translated words (Kim and Kim, 1996). WT (j)� Word No. of Senses No. of Examples Sense Percentage pear 6.2% bae 4 876 ship 55.2% times 13.7% stomach 24.9% p</context>
</contexts>
<marker>Utgoff, Berkman, Clouse, 1997</marker>
<rawString>P. Utgoff, N. Berkman, and J. Clouse. 1997. Decision tree induction based on efficient tree restructuring. Machine Learning, 29:5-44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wilks</author>
<author>M Stevenson</author>
</authors>
<title>Word sense disambiguation using optimised combinations of knowledge sources.</title>
<date>1998</date>
<booktitle>In Proceedings of COLINGACL &apos;98,</booktitle>
<pages>1398--1402</pages>
<contexts>
<context position="1280" citStr="Wilks and Stevenson, 1998" startWordPosition="181" endWordPosition="184">nt the existing training data. Our experiments on word sense disambiguation show that predictive accuracy is significantly improved by using additional unlabeled data. 1 Introduction The objective of word sense disambiguation (WSD) is to identify the correct sense of a word in context. It is one of the most critical tasks in most natural language applications, including information retrieval, information extraction, and machine translation. The availability of large-scale corpus and various machine learning algorithms enabled corpusbased approach to WSD (Cho and Kim, 1995; Hwee and Lee, 1996; Wilks and Stevenson, 1998),but a large scale sense-tagged corpus or aligned bilingual corpus is needed for a corpus-based approach. However, most languages except English do not have a large-scale sense-tagged corpus. Therefore, any corpus-based approach to WSD for such languages should consider the following problems: • There&apos;s no reliable and available sensetagged corpus. • Most words are sense ambiguous. • Annotating the large corpora requires human experts, so that it is too expensive. Because it is expensive to construct sensetagged corpus or bilingual corpus, many researchers tried to reduce the number of example</context>
</contexts>
<marker>Wilks, Stevenson, 1998</marker>
<rawString>Y. Wilks and M. Stevenson. 1998. Word sense disambiguation using optimised combinations of knowledge sources. In Proceedings of COLINGACL &apos;98, pages 1398-1402.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the ACL,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="2592" citStr="Yarowsky, 1995" startWordPosition="391" endWordPosition="392">t al., 1998) adopted a selective sampling method to use small number of examples in training. They defined a training utility function to select examples with minimum certainty, and at each training iteration the examples with less certainty were saved in the example database. However, at each iteration of training the similarity among word property vectors must be calculated due to their k-NN like implementation of training utility. While labeled examples obtained from a sense-tagged corpus is expensive and timeconsuming, it is significantly easier to obtain the unlabeled examples. Yarowsky (Yarowsky, 1995) presented, for the first time, the possibility that unlabeled examples can be used for WSD. He used a learning algorithm based on the local context under the assumption that all instances of a word have the same intended meaning within any fixed document and achieved good results with only a few labeled examples and many unlabeled ones. Nigam et al. (Nigam et al., 2000) also showed the unlabeled examples can enhance the accuracy of text categorization. Attribute GFUNC PARENT SUBJECT OBJECT NMODWORD ADNWORD ADNSUBJ ADNOBJ Substance the grammatical function of w the word of the node modified by</context>
<context position="7329" citStr="Yarowsky, 1995" startWordPosition="1203" endWordPosition="1204">lop automated methods to reduce training cost in language learning and found out that the cost can be reduced by active learning which has control over the training examples (Dagan and Engelson, 1997; Liere and Tadepalli, 1997; Zhang, 1994). Though the number of labeled examples needed is reduced by active learning, the label of the selected examples must be given by the human experts. Thus, active learning is still expensive and a method for automatic labeling unlabeled examples is needed to have the learner automatically gather information (Blum and Mitchell, 1998; Pedersen and Bruce, 1997; Yarowsky, 1995). As the unlabeled examples can be obtained with ease without human experts it makes WSD robust. Yarowsky (Yarowsky, 1995) presented the possibility of automatic labeling of training examples in WSD and achieved good results with only a few labeled examples and many unlabeled examples. On the other hand, Blum and Mitchell tried to classify Web pages, in which the description of each example can be partitioned into distinct views such as the words occurring on that page and the words occurring in hyperlinks (Blum and Mitchell, 1998). By using both views together, they augmented a small set of l</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd Annual Meeting of the ACL, pages 189-196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B-T Zhang</author>
</authors>
<title>Accerlated learning by active example selection.</title>
<date>1994</date>
<journal>International Journal of Neural Systems,</journal>
<pages>5--1</pages>
<contexts>
<context position="6954" citStr="Zhang, 1994" startWordPosition="1140" endWordPosition="1141">th case markers. Considering both characteristics and results of previous work, we select eight properties for WSD of Korean nouns (Table 1). Three of them (PARENT, NMODWORD, ADNWORD) take morphological form as their value, one (GFUNC) takes 11 values of grammatical functions1, and others take only true or false. 2.2 Unlabeled Data for WSD Many researchers tried to develop automated methods to reduce training cost in language learning and found out that the cost can be reduced by active learning which has control over the training examples (Dagan and Engelson, 1997; Liere and Tadepalli, 1997; Zhang, 1994). Though the number of labeled examples needed is reduced by active learning, the label of the selected examples must be given by the human experts. Thus, active learning is still expensive and a method for automatic labeling unlabeled examples is needed to have the learner automatically gather information (Blum and Mitchell, 1998; Pedersen and Bruce, 1997; Yarowsky, 1995). As the unlabeled examples can be obtained with ease without human experts it makes WSD robust. Yarowsky (Yarowsky, 1995) presented the possibility of automatic labeling of training examples in WSD and achieved good results </context>
</contexts>
<marker>Zhang, 1994</marker>
<rawString>B.-T. Zhang. 1994. Accerlated learning by active example selection. International Journal of Neural Systems, 5(1):67-75.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>