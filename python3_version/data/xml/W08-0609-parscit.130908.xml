<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000891">
<title confidence="0.957365">
How to Make the Most of NE Dictionaries in Statistical NER
</title>
<author confidence="0.990715">
Yutaka Sasaki2 Yoshimasa Tsuruoka2 John McNaught1 2 Sophia Ananiadou1 2
</author>
<affiliation confidence="0.803174333333333">
1 National Centre for Text Mining
2 School of Computer Science, University of Manchester
MIB, 131 Princess Street, Manchester, M1 7DN, UK
</affiliation>
<sectionHeader confidence="0.998174" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999113">
The accumulation of online biomedical informa-
tion has been growing at a rapid pace, mainly at-
tributed to a rapid growth of a wide range of repos-
itories of biomedical data and literature. The auto-
matic construction and update of scientific knowl-
edge bases is a major research topic in Bioinformat-
ics. One way of populating these knowledge bases
is through named entity recognition (NER). Unfortu-
nately, biomedical NER faces many problems, e.g.,
protein names are extremely difficult to recognize
due to ambiguity, complexity and variability. A fur-
ther problem in protein name recognition arises at
the tokenization stage. Some protein names include
punctuation or special symbols, which may cause to-
kenization to lose some word concatenation infor-
mation in the original sentence. For example, IL-2
and IL - 2 fall into the same token sequence IL
- 2 as usually dash (or hyphen) is designated as a
token delimiter.
Research into NER is centred around three ap-
proaches: dictionary-based, rule-based and machine
learning-based approaches. To overcome the usual
NER pitfalls, we have opted for a hybrid approach
combining dictionary-based and machine learning
approaches, which we call dictionary-based statisti-
cal NER approach. After identifying protein names
in text, we link these to semantic identifiers, such as
UniProt accession numbers. In this paper, we focus
on the evaluation of our dictionary-based statistical
NER.
</bodyText>
<sectionHeader confidence="0.998035" genericHeader="keywords">
2 Methods
</sectionHeader>
<bodyText confidence="0.999462">
Our dictionary-based statistical approach consists of
two components: dictionary-based POS/PROTEIN
tagging and statistical sequential labelling. First,
</bodyText>
<sectionHeader confidence="0.665592" genericHeader="introduction">
Abstract
</sectionHeader>
<bodyText confidence="0.999028861111111">
When term ambiguity and variability are very
high, dictionary-based Named Entity Recogni-
tion (NER) is not an ideal solution even though
large-scale terminological resources are avail-
able. Many researches on statistical NER have
tried to cope with these problems. However,
it is not straightforward how to exploit exist-
ing and additional Named Entity (NE) dictio-
naries in statistical NER. Presumably, addi-
tion of NEs to an NE dictionary leads to bet-
ter performance. However, in reality, the re-
training of NER models is required to achieve
this. We have established a novel way to im-
prove the NER performance by addition of
NEs to an NE dictionary without retraining.
We chose protein name recognition as a case
study because it most suffers the problems re-
lated to heavy term variation and ambiguity.
In our approach, first, known NEs are identi-
fied in parallel with Part-of-Speech (POS) tag-
ging based on a general word dictionary and
an NE dictionary. Then, statistical NER is
trained on the tagger outputs with correct NE
labels attached. We evaluated performance of
our NER on the standard JNLPBA-2004 data
set. The F-score on the test set has been im-
proved from 73.14 to 73.78 after adding the
protein names appearing in the training data to
the POS tagger dictionary without any model
retraining. The performance further increased
to 78.72 after enriching the tagging dictionary
with test set protein names. Our approach
has demonstrated high performance in pro-
tein name recognition, which indicates how
to make the most of known NEs in statistical
NER.
</bodyText>
<page confidence="0.993604">
63
</page>
<note confidence="0.780534">
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 63–70,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.992539541666666">
dictionary-based POS/PROTEIN tagging finds can-
didates for protein names using a dictionary. The
dictionary maps strings to parts of speech (POS),
where the POS tagset is augmented with a tag
NN-PROTEIN. Then, sequential labelling applies
to reduce false positives and false negatives in the
POS/PROTEIN tagging results. Expandability is
supported through allowing a user of the NER tool to
improve NER coverage by adding entries to the dic-
tionary. In our approach, retraining is not required
after dictionary enrichment.
Recently, Conditional Random Fields (CRFs)
have been successfully applied to sequence labelling
problems, such as POS tagging and NER, and have
outperformed other machine learning techniques.
The main idea of CRFs is to estimate a conditional
probability distribution over label sequences, rather
than over local directed label sequences as with Hid-
den Markov Models (Baum and Petrie, 1966) and
Maximum Entropy Markov Models (McCallum et
al., 2000). Parameters of CRFs can be efficiently
estimated through the log-likelihood parameter esti-
mation using the forward-backward algorithm, a dy-
namic programming method.
</bodyText>
<subsectionHeader confidence="0.994811">
2.1 Training and test data
</subsectionHeader>
<bodyText confidence="0.997311722222222">
Experiments were conducted using the training and
test sets of the JNLPBA-2004 data set(Kim et al.,
2004).
Training data The training data set used in
JNLPBA-2004 is a set of tokenized sentences with
manually annotated term class labels. The sentences
are taken from the Genia corpus (version 3.02) (Kim
et al., 2003), in which 2,000 abstracts were manu-
ally annotated by a biologist, drawing on a set of
POS tags and 36 biomedical term classes. In the
JNLPBA-2004 shared task, performance in extract-
ing five term classes, i.e., protein, DNA, RNA, cell
line, and cell type classes, were evaluated.
Test Data The test data set used in JNLPBA-2004
is a set of tokenized sentences extracted from 404
separately collected MEDLINE abstracts, where the
term class labels were manually assigned, following
the annotation specification of the Genia corpus.
</bodyText>
<subsectionHeader confidence="0.959745">
2.2 Overview of dictionary-based statistical
NER
</subsectionHeader>
<bodyText confidence="0.895399333333333">
Figure 1 shows the block diagram of dictionary-
based statistical NER. Raw text is analyzed by
a POS/PROTEIN tagger based on a CRF tagging
</bodyText>
<figureCaption confidence="0.995162">
Figure 1: Block diagram of dictionary-based statistical
NER
Figure 2: Block diagram of training procedure
</figureCaption>
<bodyText confidence="0.99991124">
model and dictionary, and then converted into to-
ken sequences. Strings in the text that match with
protein names in the dictionary will be tagged as
NN-PROTEIN depending on the context around the
protein names. Since it is not realistic to enumer-
ate all protein names in the dictionary, due to their
high variability of form, instead previously unseen
forms are predicted to be protein names by statisti-
cal sequential labelling. Finally, protein names are
identified from the POS/PROTEIN tagged token se-
quences via a CRF labelling model.
Figure 2 shows the block diagram of the train-
ing procedure for both POS/PROTEIN tagging and
sequential labelling. The tagging model is created
using the Genia corpus (version 3.02) and a dic-
tionary. Using the tagging model, MEDLINE ab-
stracts used for the JNLPBA-2004 training data set
are then POS/PROTEIN-tagged. The output token
sequences over these abstracts are then integrated
with the correct protein labels of the JNLPBA-2004
training data. This process results in the preparation
of token sequences with features and correct protein
labels. A CRF labelling model is finally generated
by applying a CRF tool to these decorated token se-
quences.
</bodyText>
<page confidence="0.996353">
64
</page>
<figure confidence="0.651911">
IL-2-mediated activation ...
</figure>
<figureCaption confidence="0.995599">
Figure 3: Dictionary based approach
</figureCaption>
<subsubsectionHeader confidence="0.557217">
2.2.1 Dictionary-based POS/PROTEIN tagging
</subsubsectionHeader>
<bodyText confidence="0.999893870967742">
The dictionary-based approach is beneficial when
a sentence contains some protein names that con-
flict with general English words. Otherwise, if the
POS tags of sentences are decided without consider-
ing possible occurrences of protein names, POS se-
quences could be disrupted. For example, in “met
proto-oncogene precursor”, met might be falsely
recognized as a verb by a non dictionary-based tag-
ger.
Given a sentence, the dictionary-based approach
extracts protein names as follows. Find all word se-
quences that match the lexical entries, and create a
token graph (i.e., trellis) according to the word order.
Estimate the score of every path using the weights of
node and edges estimated by training using Condi-
tional Random Fields. Select the best path.
Figure 3 shows an example of our dictionary-
based approach. Suppose that the input is “IL-
2-mediated activation”. A trellis is created based
on the lexical entries in a dictionary. The se-
lection criteria for the best path are determined
by the CRF tagging model trained on the Genia
corpus. In this example, IL-2/NN-PROTEIN
-/- mediated/VVN activation/NN is se-
lected as the best path. Following Kudo et al. (Kudo
et al., 2004), we adapted the core engine of the
CRF-based morphological analyzer, MeCab1, to our
POS/PROTEIN tagging task. MeCab’s dictionary
databases employ double arrays (Aoe, 1989) which
enable efficient lexical look-ups.
The features used were:
</bodyText>
<listItem confidence="0.99994525">
• POS-PROTEIN
• bigram of adjacent POS
• bigram of adjacent PROTEIN
• bigram of adjacent POS-PROTEIN
</listItem>
<bodyText confidence="0.92775325">
During the construction of the trellis, white space
is considered as the delimiter unless otherwise stated
within dictionary entries. This means that unknown
tokens are character sequences without spaces.
</bodyText>
<subsubsectionHeader confidence="0.626068">
2.2.2 Dictionary construction
</subsubsectionHeader>
<bodyText confidence="0.995219666666667">
A dictionary-based approach requires the dictio-
nary to cover not only a wide variety of biomedical
terms but also entries with:
</bodyText>
<listItem confidence="0.99996">
• all possible capitalization
• all possible linguistic inflections
</listItem>
<bodyText confidence="0.999973125">
We constructed a freely available, wide-coverage
English word dictionary that satisfies these condi-
tions. We did consider the MedPost pos-tagger
package2 which contains a free dictionary that has
downcased English words; however, this dictionary
is not well curated as a dictionary and the number of
entries is limited to only 100,000, including inflec-
tions.
Therefore, we started by constructing an English
word dictionary. Eventually, we created a dictionary
with about 266,000 entries for English words (sys-
tematically covering inflections) and about 1.3 mil-
lion entries for protein names.
We created the general English part of the dictio-
nary from WordNet by semi-automatically adding
POS tags. The POS tag set is a minor modifica-
tion of the Penn Treebank POS tag set3, in that pro-
tein names are given a new POS tag, NN-PROTEIN.
Further details on construction of the dictionary now
follow.
Protein names were extracted from the BioThe-
saurus4. After selecting only those terms
clearly stated as protein names, 1,341,992 pro-
tein names in total were added to the dictionary.
</bodyText>
<figure confidence="0.98339635">
POS/PROTEIN tagging
IL/NNP
IL-2/NN-PROTEIN
-/-
2/CD
-/-
mediated/VVD
mediated/VVN
activation/NN
2/CD
-/-
IL-2/NN-PROTEIN
activation/NN
IL/NNP
Lexicon
mediate/VV
mediated/VVD
mediated/VVN
mediate/VVP
• POS 2ftp://ftp.ncbi.nlm.nih.gov/pub/lsmith/MedPost/
</figure>
<footnote confidence="0.948636">
3ftp://ftp.cis.upenn.edu/pub/treebank/
doc/tagguide.ps.gz
4http://pir.georgetown.edu/iprolink/
/biothesaurus/
• PROTEIN
1http://sourceforge.net/project/showfiles.php?group id=177856
</footnote>
<page confidence="0.997502">
65
</page>
<bodyText confidence="0.998412233333333">
Nouns were extracted from WordNet’s noun list.
Words starting with lower case and upper case
letters were determined as NN and NNP, re-
spectively. Nouns in NNS and NNPS cate-
gories were collected from the results of POS
tagging articles from Plos Biology Journal5
with TreeTagger6.
Verbs were extracted from WordNet’s verb list. We
manually curated VBD, VBN, VBG and VBZ
verbs with irregular inflections based on Word-
Net. Next, VBN, VBD, VBG and VBZ forms
of regular verbs were automatically generated
from the WordNet verb list.
Adjectives were extracted from WordNet’s adjec-
tive list. We manually curated JJ, JJR and JJS
of irregular inflections of adjectives based on
the WordNet irregular adjective list. Base form
(JJ) and regular inflections (JJR, JJS) of adjec-
tives were also created based on the list of ad-
jectives.
Adverbs were extracted from WordNet’s adverb
list. Both the original and capitalised forms
were added as RB.
Pronouns were manually curated. PRP and PRP$
words were added to the dictionary.
Wh-words were manually curated. As a result,
WDT, WP, WP$ and WRB words were added
to the dictionary.
Words for other parts of speech were manually
curated.
</bodyText>
<subsectionHeader confidence="0.618078">
2.2.3 Statistical prediction of protein names
</subsectionHeader>
<bodyText confidence="0.999935">
Statistical sequential labelling was employed to
improve the coverage of protein name recognition
and to remove false positives resulting from the pre-
vious stage (dictionary-based tagging).
We used the JNLPBA-2004 training data, which
is a set of tokenized word sequences with
IOB2(Tjong Kim Sang and Veenstra, 1999) protein
labels. As shown in Figure 2, POSs of tokens re-
sulting from tagging and tokens of the JNLPBA-
2004 data set are integrated to yield training data for
sequential labelling. During integration, when the
single token of a protein name found after tagging
</bodyText>
<footnote confidence="0.985262">
5http://biology.plosjournals.org/
6http://www.ims.uni-stuttgart.de/projekte/
corplex/TreeTagger/DecisionTreeTagger.html/
</footnote>
<bodyText confidence="0.998920416666667">
corresponds to a sequence of tokens from JNLPBA-
2004, its POS is given as NN-PROTEIN1, NN-
PROTEIN2,..., according to the corresponding token
order in the JNLPBA-2004 sequence.
Following the data format of the JNLPBA-2004
training set, our training and test data use the IOB2
labels, which are “B-protein” for the first token of
the target sequence, “I-protein” for each remaining
token in the target sequence, and “O” for other to-
kens. For example, “Activation of the IL 2 precursor
provides” is analyzed by the POS/PROTEIN tagger
as follows.
</bodyText>
<table confidence="0.991714230769231">
Activation NN
of IN
the DT
IL 2 precursor NN-PROTEIN
provides VVZ
The tagger output is given IOB2 labels as follows.
Activation NN O
of IN O
the DT O
IL NN-PROTEIN1 B-protein
2 NN-PROTEIN2 I-protein
precursor NN-PROTEINS I-protein
provides VVZ O
</table>
<bodyText confidence="0.995356">
We used CRF models to predict the IOB2 la-
bels. The following features were used in our ex-
periments.
</bodyText>
<listItem confidence="0.999764">
• word feature
• orthographic features
</listItem>
<bodyText confidence="0.9687015">
– the first letter and the last four letters of
the word form, in which capital letters in
a word are normalized to “A”, lower case
letters are normalized to “a”, and digits are
replaced by “0”, e.g., the word form of IL-
2 is AA-0.
</bodyText>
<listItem confidence="0.857862666666667">
– postfixes, the last two and four letters
• POS feature
• PROTEIN feature
</listItem>
<bodyText confidence="0.994177">
The window size was set to f2 of the current to-
ken.
</bodyText>
<sectionHeader confidence="0.99354" genericHeader="related work">
3 Results and discussion
</sectionHeader>
<page confidence="0.996882">
66
</page>
<tableCaption confidence="0.999536">
Table 1: Experimental Rusults
</tableCaption>
<table confidence="0.986579">
Tagging R P F
Full 52.91 43.85 47.96
(a) POS/PROTEIN tagging Left 61.48 50.95 55.72
Right 61.38 50.87 55.63
Sequential Labelling R P F
Full 63.23 70.39 66.62
(b) Word feature Left 68.15 75.86 71.80
Right 69.88 77.79 73.63
Full 77.17 67.52 72.02
(c) (b) + orthographic feature Left 82.51 72.20 77.01
Right 84.29 73.75 78.67
Full 76.46 68.41 72.21
(d) (c) + POS feature Left 81.94 73.32 77.39
Right 83.54 74.75 78.90
Full 77.58 69.18 73.14
(e) (d) + PROTEIN feature Left 82.69 73.74 77.96
Right 84.37 75.24 79.54
Full 79.85 68.58 73.78
(f) (e) + after adding protein names in the Left 84.82 72.85 78.38
training set to the dictionary Right 86.60 74.37 80.02
</table>
<subsectionHeader confidence="0.985786">
3.1 Protein name recognition performance
</subsectionHeader>
<bodyText confidence="0.999893259259259">
Table 1 shows our protein name recognition results,
showing the differential effect of various combina-
tions of strategies. Results are expressed accord-
ing to recall (R), precision (P), and F-measure (F),
which here measure how accurately our various ex-
periments determined the left boundary (Left), the
right boundary (Right), and both boundaries (Full)
of protein names. The baseline for tagging (row
(a)) shows the protein name detection performance
of our dictionary-based tagging using our large pro-
tein name dictionary, where no training for protein
name prediction was involved. The F-score of this
baseline tagging method was 47.96.
The baseline for sequential labelling (row (b))
shows the prediction performance when using only
word features where no orthographic and POS fea-
tures were used. The F-score of the baseline la-
belling method was 66.62. When orthographic fea-
ture was added (row (c)), the F-score increased by
5.40 to 72.02. When the POS feature was added
(row (d)), the F-score increased by 0.19 to 72.21.
Using all features (row (e)), the F-score reached
73.14. Surprisingly, adding protein names appear-
ing in the training data to the dictionary further im-
proved the F-score by 0.64 to 73.78, which is the
second best score for protein name recognition us-
ing the JNLPBA-2004 data set.
</bodyText>
<tableCaption confidence="0.977926">
Table 2: After Dictionary Enrichment
</tableCaption>
<table confidence="0.994930142857143">
Method R P F
Tagging Full 79.02 61.87 69.40
(+test set Left 82.28 64.42 72.26
protein names) Right 80.96 63.38 71.10
Labelling full 86.13 72.49 78.72
(+test set Left 89.58 75.40 81.88
protein names) Right 90.23 75.95 82.47
</table>
<bodyText confidence="0.9974089">
Tagging and labelling speeds were measured us-
ing an unloaded Linux server with quad 1.8 GHz
Opteron cores and 16GB memory. The dictionary-
based POS/PROTEIN tagger is very fast even
though the total size of the dictionary is more than
one million. The processing speed for tagging and
sequential labelling of the 4,259 sentences of the test
set data took 0.3 sec and 7.3 sec, respectively, which
means that in total it took 7.6 sec. for recognizing
protein names in the plain text of 4,259 sentences.
</bodyText>
<subsectionHeader confidence="0.997324">
3.2 Dictionary enrichment
</subsectionHeader>
<bodyText confidence="0.999730166666667">
The advantage of the dictionary-based statistical ap-
proach is that it is versatile, as the user can easily
improve its performance with no retraining. We as-
sume the following situation as the ideal case: sup-
pose that a user needs to analyze a large amount of
text with protein names. The user wants to know
</bodyText>
<page confidence="0.998775">
67
</page>
<bodyText confidence="0.999981882352941">
the maximum performance achievable for identify-
ing protein names with our dictionary-based statis-
tical recognizer which can be achieved by adding
more protein names to the current dictionary. Note
that protein names should be identified in context.
That is, recall of the NER results with the ideal dic-
tionary is not 100%. Some protein names in the ideal
dictionary are dropped during statistical tagging or
labelling.
Table 2 shows the scores after each step of dic-
tionary enrichment. The first block (Tagging) shows
the tagging performance after adding protein names
appearing in the test set to the dictionary. The sec-
ond block (Labelling) shows the performance of the
sequence labelling of the output of the first step.
Note that tagging and the sequence labelling mod-
els are not retrained using the test set.
</bodyText>
<subsectionHeader confidence="0.982962">
3.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999857321428572">
It is not possible in reality to train the recognizer
on target data, i.e., the test set, but it would be pos-
sible for users to add discovered protein names to
the dictionary so that they could improve the overall
performance of the recognizer without retraining.
Rule-based and procedural approaches are taken
in (Fukuda et al., 1998; Franzen et al., 2002). Ma-
chine learning-based approaches are taken in (Col-
lier et al., 2000; Lee et al., 2003; Kazama et al.,
2002; Tanabe and Wilbur, 2002; Yamamoto et al.,
2003; Tsuruoka, 2006; Okanohara et al., 2006).
Machine learning algorithms used in these studies
are Naive Bayes, C4.5, Maximum Entropy Models,
Support Vector Machines, and Conditional Random
Fields. Most of these studies applied machine learn-
ing techniques to tokenized sentences.
Table 3 shows the scores reported by other sys-
tems. Tsai et al. (Tsai et al., 2006) and Zhou and
Su (Zhou and Su, 2004) combined machine learning
techniques and hand-crafted rules. Tsai et al. (Tsai
et al., 2006) applied CRFs to the JNLPBA-2004
data. After applying pattern-based post-processing,
they achieved the best F-score (75.12) among those
reported so far. Kim and Yoon(Kim and Yoon, 2007)
also applied heuristic post-processing. Zhou and Su
(Zhou and Su, 2004) achieved an F-score of 73.77.
Purely machine learning-based approaches have
been investigated by several researchers. The
GENIA Tagger (Tsuruoka, 2006) is trained on
the JNLPBA-2004 Corpus. Okanohara et al.
(Okanohara et al., 2006) employed semi-Markov
CRFs whose performance was evaluated against the
JNLPBA-2004 data set. Yamamoto et al. (Ya-
mamoto et al., 2003) used SVMs for character-
based protein name recognition and sequential la-
belling. Their protein name extraction performance
was 69%. This paper extends the machine learning
approach with a curated dictionary and CRFs and
achieved high F-score 73.78, which is the top score
among the heuristics-free NER systems. Table 4
shows typical recognition errors found in the recog-
nition results that achieved F-score 73.78. In some
cases, protein name boundaries of the JNLPBA-
2004 data set are not consistent. It is also one of
the reasons for the recognition errors that the data
set contains general protein names, such as domain,
family, and binding site names as well as anaphoric
expressions, which are usually not covered by pro-
tein name repositories. Therefore, our impression on
the performance is that an F-score of 73.78 is suffi-
ciently high.
Furthermore, thanks to the dictionary-based ap-
proach, it has been shown that the upper bound per-
formance using ideal dictionary enrichment, with-
out any retraining of the models, has an F-score of
78.72.
</bodyText>
<sectionHeader confidence="0.997438" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.99992825">
This paper has demonstrated how to utilize known
named entities to achieve better performance in sta-
tistical named entity recognition. We took a two-
step approach where sentences are first tokenized
and tagged based on a biomedical dictionary that
consists of general English words and about 1.3 mil-
lion protein names. Then, a statistical sequence
labelling step predicted protein names that are not
listed in the dictionary and, at the same time, re-
duced false negatives in the POS/PROTEIN tagging
results. The significant benefit of this approach is
that a user, not a system developer, can easily en-
hance the performance by augmenting the dictio-
nary. This paper demonstrated that the state-of-
the-art F-score 73.78 on the standard JNLPBA-2004
data set was achieved by our approach. Further-
more, thanks to the dictionary-based NER approach,
the upper bound performance using ideal dictionary
enrichment, without any retraining of the models,
yielded F-score 78.72.
</bodyText>
<sectionHeader confidence="0.998872" genericHeader="acknowledgments">
5 Acknowledgments
</sectionHeader>
<bodyText confidence="0.857723">
This research is partly supported by EC IST project
FP6-028099 (BOOTStrep), whose Manchester team
is hosted by the JISC/BBSRC/EPSRC sponsored
National Centre for Text Mining.
</bodyText>
<page confidence="0.999356">
68
</page>
<tableCaption confidence="0.99804">
Table 3: Conventional results for protein name recognition
</tableCaption>
<table confidence="0.999406666666667">
Authors R P F
Tsai et al.(Tsai et al., 2006) 71.31 79.36 75.12
Our system 79.85 68.58 73.78
Zhou and Su(Zhou and Su, 2004) 69.01 79.24 73.77
Kim and Yoon(Kim and Yoon, 2007) 75.82 71.02 73.34
Okanohara et al.(Okanohara et al., 2006) 77.74 68.92 73.07
Tsuruoka(Tsuruoka, 2006) 81.41 65.82 72.79
Finkel et al.(Finkel et al., 2004) 77.40 68.48 72.67
Settles(Settles, 2004) 76.1 68.2 72.0
Song et al.(Song et al., 2004) 65.50 73.04 69.07
R¨ossler(R¨ossler, 2004) 72.9 62.0 67.0
Park et al.(Park et al., 2004) 69.71 59.37 64.12
</table>
<sectionHeader confidence="0.993462" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998372551282051">
J. Aoe, An Efficient Digital Search Algorithm by Using
a Double-Array Structure, IEEE Transactions on Soft-
ware Engineering, 15(9):1066–1077, 1989.
L.E. Baum and T. Petrie, Statistical inference for proba-
bilistic functions of finite state Markov chains, The An-
nals ofMathematical Statistics, 37:1554–1563, 1966.
J. Chang, H. Schutze, R. Altman, GAPSCORE: Finding
Gene and Protein names one Word at a Time, Bioin-
formatics, Vol. 20, pp. 216-225, 2004.
N. Collier, C. Nobata, J. Tsujii, Extracting the Names
of Genes and Gene Products with a Hidden Markov
Model, Proc. of the 18th International Conference
on Computational Linguistics (COLING’2000), Saar-
brucken, 2000.
Jenny Finkel, Shipra Dingare, Huy Nguyen, Malvina Nis-
sim, Gail Sinclair and Christopher Manning, Exploit-
ing Context for Biomedical Entity Recognition: From
Syntax to the Web, Proc. ofthe Joint Workshop on Nat-
ural Language Processing in Biomedicine and its Ap-
plications (JNLPBA-2004), pp. 88–91, 2004.
K. Franzen, G. Eriksson, F. Olsson, L. Asker, P. Liden,
and J. Koster, Protein Names and How to Find Them,
Int. J. Med. Inf., Vol. 67, pp. 49–61, 2002.
K. Fukuda, T. Tsunoda, A. Tamura, and T. Takagi,
Toward information extraction: identifying protein
names from biological papers, PSB, pp. 705-716,
1998.
J. Kazama, T. Makino, Y. Ohta, J. Tsujii, Tuning Support
Vector Machines for Biomedical Named Entity Recog-
nition, Proc. of ACL-2002 Workshop on Natural Lan-
guage Processing in the Biomedical Domain, pp. 1–8,
2002.
J.-D. Kim, T. Ohta, Y. Tateisi, J. Tsujii: GENIA corpus
- semantically annotated corpus for bio-textmining,
Bioinformatics 2003, 19:i180-i182.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,
Yuka Tateisi, Introduction to the Bio-Entity Recogni-
tion Task at JNLPBA, Proc. of the Joint Workshop on
Natural Language Processing in Biomedicine and its
Applications (JNLPBA-2004), pp. 70–75, 2004.
S. Kim, J. Yoon: Experimental Study on a Two Phase
Method for Biomedical Named Entity Recognition,
IEICE Transactions on Informaion and Systems 2007,
E90-D(7):1103–1120.
Taku Kudo and Kaoru Yamamoto and Yuuji Matsumoto,
Applying Conditional Random Fields to Japanese
Morphological Analysis, Proc. of Empirical Methods
in Natural Language Processing (EMNLP), pp. 230–
237, 2004.
J. Lafferty, A. McCallum, and F. Pereira, Conditional
Random Fields: Probabilistic Models for Segment-
ing and Labeling Sequence Data, Proc. ofICML-2001,
pp.282–289, 2001
K. J. Lee, Y. S. Hwang and H. C. Rim (2003), Two-Phase
Biomedical NE Recognition based on SVMs, Proc. of
ACL 2003 Workshop on Natural Language Processing
in Biomedicine, Sapporo, 2003.
McCallum A, Freitag D, Pereira F.: Maximum entropy
Markov models for information extraction and seg-
mentation, Proceedings of the Seventeenth Interna-
tional Conference on Machine Learning, 2000:591-
598.
Daisuke, Okanohara, Yusuke Miyao, Yoshimasa Tsu-
ruoka and Jun’ichi Tsujii, Improving the Scalability of
Semi-Markov Conditional Random Fields for Named
Entity Recognition, Proc. ofACL 2006, Sydney, 2006.
Kyung-Mi Park, Seon-Ho Kim, Do-Gil Lee and
Hae-Chang Rim. Boosting Lexical Knowledge for
Biomedical Named Entity Recognition, Proc. of the
Joint Workshop on Natural Language Processing in
Biomedicine and its Applications (JNLPBA-2004), pp.
76-79, 2004.
Marc R¨ossler, Adapting an NER-System for German to
the Biomedical Domain, Proc. of the Joint Workshop
on Natural Language Processing in Biomedicine and
its Applications (JNLPBA-2004), pp. 92–95, 2004.
Burr Settles, Biomedical Named Entity Recognition Us-
ing Conditional Random Fields and Novel Feature
</reference>
<page confidence="0.999771">
69
</page>
<tableCaption confidence="0.997165">
Table 4: Error Analysis
</tableCaption>
<figure confidence="0.96869045">
False positives
Cause Correct extraction Identified term
1 dictionary - protein, binding sites
2 prefix word trans-acting factor common trans-acting factor
3 unknown word - ATTTGCAT
4 sequential labelling error - additional proteins
5 test set error - Estradiol receptors
False negatives
Cause Correct extraction Identified term
1 anaphoric (the) receptor, (the) binding sites -
2 coordination (and, or) transcription factors NF-kappa B and AP-1 transcription factors NF-kappa B
3 prefix word activation protein-1 protein-1
catfish STAT STAT
4 postfix word nuclear factor kappa B complex nuclear factor kappa B
5 plural protein tyrosine kinase(s) protein tyrosine kinase
6 family name, biding site, T3 binding sites -
and domain residues 639-656 -
7 sequential labelling error PCNA -
Chloramphenicol acetyltransferase -
8 test set error superfamily member -
</figure>
<reference confidence="0.997156333333333">
Sets, Proc. of the Joint Workshop on Natural Lan-
guage Processing in Biomedicine and its Applications
(JNLPBA-2004), pp. 104–1007, 2004.
Yu Song, Eunju Kim, Gary Geunbae Lee and Byoung-
kee Yi, POSBIOTM-NER in the shared task of
BioNLP/NLPBA 2004, Proc. of the Joint Workshop on
Natural Language Processing in Biomedicine and its
Applications (JNLPBA-2004), pp. 100-103, 2004.
L. Tanabe and W. J. Wilbur, Tagging Gene and Protein
Names in Biomedical Text, Bioinformatics, 18(8), pp.
1124–1132, 2002.
E.F. Tjong Kim Sang and J. Veenstra, Representing Text
Chunks,EACL-99, pp. 173-179, 1999.
Richard Tzong-Han Tsai, W.-C. Chou, S.-H. Wu, T.-Y.
Sung, J. Hsiang, and W.-L. Hsu, Integrating Linguistic
Knowledge into a Conditional Random Field Frame-
work to Identify Biomedical Named Entities, Expert
Systems with Applications, 30 (1), 2006.
Yoshimasa Tsuruoka, GENIA Tagger 3.0,
http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA/tagger/,2006.
K. Yamamoto, T. Kudo, A. Konagaya and Y. Matsumoto,
Protein Name Tagging for Biomedical Annotation in
Text, in Proc. of ACL-2003 Workshop on Natural Lan-
guage Processing in Biomedicine, Sapporo, 2003.
Guofeng Zhou and Jian Su, Exploring Deep Knowledge
Resources in Biomedical Name Recognition, Proceed-
ings of the Joint Workshop on Natural Language Pro-
cessing ofBiomedicine and its Applications (JNLPBA-
2004), pp. 96-99, 2004.
</reference>
<page confidence="0.998479">
70
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.787531">
<title confidence="0.977475">How to Make the Most of NE Dictionaries in Statistical NER</title>
<author confidence="0.96014">Yoshimasa John Sophia</author>
<affiliation confidence="0.85969">1National Centre for Text 2School of Computer Science, University of</affiliation>
<address confidence="0.934726">MIB, 131 Princess Street, Manchester, M1 7DN, UK</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Aoe</author>
</authors>
<title>An Efficient Digital Search Algorithm by Using a Double-Array Structure,</title>
<date>1989</date>
<journal>IEEE Transactions on Software Engineering,</journal>
<volume>15</volume>
<issue>9</issue>
<contexts>
<context position="8555" citStr="Aoe, 1989" startWordPosition="1338" endWordPosition="1339">he best path. Figure 3 shows an example of our dictionarybased approach. Suppose that the input is “IL2-mediated activation”. A trellis is created based on the lexical entries in a dictionary. The selection criteria for the best path are determined by the CRF tagging model trained on the Genia corpus. In this example, IL-2/NN-PROTEIN -/- mediated/VVN activation/NN is selected as the best path. Following Kudo et al. (Kudo et al., 2004), we adapted the core engine of the CRF-based morphological analyzer, MeCab1, to our POS/PROTEIN tagging task. MeCab’s dictionary databases employ double arrays (Aoe, 1989) which enable efficient lexical look-ups. The features used were: • POS-PROTEIN • bigram of adjacent POS • bigram of adjacent PROTEIN • bigram of adjacent POS-PROTEIN During the construction of the trellis, white space is considered as the delimiter unless otherwise stated within dictionary entries. This means that unknown tokens are character sequences without spaces. 2.2.2 Dictionary construction A dictionary-based approach requires the dictionary to cover not only a wide variety of biomedical terms but also entries with: • all possible capitalization • all possible linguistic inflections We</context>
</contexts>
<marker>Aoe, 1989</marker>
<rawString>J. Aoe, An Efficient Digital Search Algorithm by Using a Double-Array Structure, IEEE Transactions on Software Engineering, 15(9):1066–1077, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L E Baum</author>
<author>T Petrie</author>
</authors>
<title>Statistical inference for probabilistic functions of finite state Markov chains, The Annals ofMathematical Statistics,</title>
<date>1966</date>
<pages>37--1554</pages>
<contexts>
<context position="4514" citStr="Baum and Petrie, 1966" startWordPosition="698" endWordPosition="701">e POS/PROTEIN tagging results. Expandability is supported through allowing a user of the NER tool to improve NER coverage by adding entries to the dictionary. In our approach, retraining is not required after dictionary enrichment. Recently, Conditional Random Fields (CRFs) have been successfully applied to sequence labelling problems, such as POS tagging and NER, and have outperformed other machine learning techniques. The main idea of CRFs is to estimate a conditional probability distribution over label sequences, rather than over local directed label sequences as with Hidden Markov Models (Baum and Petrie, 1966) and Maximum Entropy Markov Models (McCallum et al., 2000). Parameters of CRFs can be efficiently estimated through the log-likelihood parameter estimation using the forward-backward algorithm, a dynamic programming method. 2.1 Training and test data Experiments were conducted using the training and test sets of the JNLPBA-2004 data set(Kim et al., 2004). Training data The training data set used in JNLPBA-2004 is a set of tokenized sentences with manually annotated term class labels. The sentences are taken from the Genia corpus (version 3.02) (Kim et al., 2003), in which 2,000 abstracts were </context>
</contexts>
<marker>Baum, Petrie, 1966</marker>
<rawString>L.E. Baum and T. Petrie, Statistical inference for probabilistic functions of finite state Markov chains, The Annals ofMathematical Statistics, 37:1554–1563, 1966.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chang</author>
<author>H Schutze</author>
<author>R Altman</author>
</authors>
<title>GAPSCORE: Finding Gene and Protein names one Word at a Time,</title>
<date>2004</date>
<journal>Bioinformatics,</journal>
<volume>20</volume>
<pages>216--225</pages>
<marker>Chang, Schutze, Altman, 2004</marker>
<rawString>J. Chang, H. Schutze, R. Altman, GAPSCORE: Finding Gene and Protein names one Word at a Time, Bioinformatics, Vol. 20, pp. 216-225, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Collier</author>
<author>C Nobata</author>
<author>J Tsujii</author>
</authors>
<title>Extracting the Names of Genes and Gene Products with a Hidden Markov Model,</title>
<date>2000</date>
<booktitle>Proc. of the 18th International Conference on Computational Linguistics (COLING’2000), Saarbrucken,</booktitle>
<contexts>
<context position="18275" citStr="Collier et al., 2000" startWordPosition="2880" endWordPosition="2884">lling) shows the performance of the sequence labelling of the output of the first step. Note that tagging and the sequence labelling models are not retrained using the test set. 3.3 Discussion It is not possible in reality to train the recognizer on target data, i.e., the test set, but it would be possible for users to add discovered protein names to the dictionary so that they could improve the overall performance of the recognizer without retraining. Rule-based and procedural approaches are taken in (Fukuda et al., 1998; Franzen et al., 2002). Machine learning-based approaches are taken in (Collier et al., 2000; Lee et al., 2003; Kazama et al., 2002; Tanabe and Wilbur, 2002; Yamamoto et al., 2003; Tsuruoka, 2006; Okanohara et al., 2006). Machine learning algorithms used in these studies are Naive Bayes, C4.5, Maximum Entropy Models, Support Vector Machines, and Conditional Random Fields. Most of these studies applied machine learning techniques to tokenized sentences. Table 3 shows the scores reported by other systems. Tsai et al. (Tsai et al., 2006) and Zhou and Su (Zhou and Su, 2004) combined machine learning techniques and hand-crafted rules. Tsai et al. (Tsai et al., 2006) applied CRFs to the JN</context>
</contexts>
<marker>Collier, Nobata, Tsujii, 2000</marker>
<rawString>N. Collier, C. Nobata, J. Tsujii, Extracting the Names of Genes and Gene Products with a Hidden Markov Model, Proc. of the 18th International Conference on Computational Linguistics (COLING’2000), Saarbrucken, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Finkel</author>
<author>Shipra Dingare</author>
<author>Huy Nguyen</author>
<author>Malvina Nissim</author>
<author>Gail Sinclair</author>
<author>Christopher Manning</author>
</authors>
<title>Exploiting Context for Biomedical Entity Recognition: From Syntax to the Web,</title>
<date>2004</date>
<booktitle>Proc. ofthe Joint Workshop on Natural Language Processing in Biomedicine and its Applications (JNLPBA-2004),</booktitle>
<pages>88--91</pages>
<marker>Finkel, Dingare, Nguyen, Nissim, Sinclair, Manning, 2004</marker>
<rawString>Jenny Finkel, Shipra Dingare, Huy Nguyen, Malvina Nissim, Gail Sinclair and Christopher Manning, Exploiting Context for Biomedical Entity Recognition: From Syntax to the Web, Proc. ofthe Joint Workshop on Natural Language Processing in Biomedicine and its Applications (JNLPBA-2004), pp. 88–91, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Franzen</author>
<author>G Eriksson</author>
<author>F Olsson</author>
<author>L Asker</author>
<author>P Liden</author>
<author>J Koster</author>
</authors>
<title>Protein Names and How to Find Them,</title>
<date>2002</date>
<journal>Int. J. Med. Inf.,</journal>
<volume>67</volume>
<pages>49--61</pages>
<contexts>
<context position="18205" citStr="Franzen et al., 2002" startWordPosition="2869" endWordPosition="2872">mes appearing in the test set to the dictionary. The second block (Labelling) shows the performance of the sequence labelling of the output of the first step. Note that tagging and the sequence labelling models are not retrained using the test set. 3.3 Discussion It is not possible in reality to train the recognizer on target data, i.e., the test set, but it would be possible for users to add discovered protein names to the dictionary so that they could improve the overall performance of the recognizer without retraining. Rule-based and procedural approaches are taken in (Fukuda et al., 1998; Franzen et al., 2002). Machine learning-based approaches are taken in (Collier et al., 2000; Lee et al., 2003; Kazama et al., 2002; Tanabe and Wilbur, 2002; Yamamoto et al., 2003; Tsuruoka, 2006; Okanohara et al., 2006). Machine learning algorithms used in these studies are Naive Bayes, C4.5, Maximum Entropy Models, Support Vector Machines, and Conditional Random Fields. Most of these studies applied machine learning techniques to tokenized sentences. Table 3 shows the scores reported by other systems. Tsai et al. (Tsai et al., 2006) and Zhou and Su (Zhou and Su, 2004) combined machine learning techniques and hand</context>
</contexts>
<marker>Franzen, Eriksson, Olsson, Asker, Liden, Koster, 2002</marker>
<rawString>K. Franzen, G. Eriksson, F. Olsson, L. Asker, P. Liden, and J. Koster, Protein Names and How to Find Them, Int. J. Med. Inf., Vol. 67, pp. 49–61, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Fukuda</author>
<author>T Tsunoda</author>
<author>A Tamura</author>
<author>T Takagi</author>
</authors>
<title>Toward information extraction: identifying protein names from biological papers, PSB,</title>
<date>1998</date>
<pages>705--716</pages>
<contexts>
<context position="18182" citStr="Fukuda et al., 1998" startWordPosition="2865" endWordPosition="2868">ter adding protein names appearing in the test set to the dictionary. The second block (Labelling) shows the performance of the sequence labelling of the output of the first step. Note that tagging and the sequence labelling models are not retrained using the test set. 3.3 Discussion It is not possible in reality to train the recognizer on target data, i.e., the test set, but it would be possible for users to add discovered protein names to the dictionary so that they could improve the overall performance of the recognizer without retraining. Rule-based and procedural approaches are taken in (Fukuda et al., 1998; Franzen et al., 2002). Machine learning-based approaches are taken in (Collier et al., 2000; Lee et al., 2003; Kazama et al., 2002; Tanabe and Wilbur, 2002; Yamamoto et al., 2003; Tsuruoka, 2006; Okanohara et al., 2006). Machine learning algorithms used in these studies are Naive Bayes, C4.5, Maximum Entropy Models, Support Vector Machines, and Conditional Random Fields. Most of these studies applied machine learning techniques to tokenized sentences. Table 3 shows the scores reported by other systems. Tsai et al. (Tsai et al., 2006) and Zhou and Su (Zhou and Su, 2004) combined machine learn</context>
</contexts>
<marker>Fukuda, Tsunoda, Tamura, Takagi, 1998</marker>
<rawString>K. Fukuda, T. Tsunoda, A. Tamura, and T. Takagi, Toward information extraction: identifying protein names from biological papers, PSB, pp. 705-716, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kazama</author>
<author>T Makino</author>
<author>Y Ohta</author>
<author>J Tsujii</author>
</authors>
<title>Tuning Support Vector Machines for Biomedical Named Entity Recognition,</title>
<date>2002</date>
<booktitle>Proc. of ACL-2002 Workshop on Natural Language Processing in the Biomedical Domain,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="18314" citStr="Kazama et al., 2002" startWordPosition="2889" endWordPosition="2892">ence labelling of the output of the first step. Note that tagging and the sequence labelling models are not retrained using the test set. 3.3 Discussion It is not possible in reality to train the recognizer on target data, i.e., the test set, but it would be possible for users to add discovered protein names to the dictionary so that they could improve the overall performance of the recognizer without retraining. Rule-based and procedural approaches are taken in (Fukuda et al., 1998; Franzen et al., 2002). Machine learning-based approaches are taken in (Collier et al., 2000; Lee et al., 2003; Kazama et al., 2002; Tanabe and Wilbur, 2002; Yamamoto et al., 2003; Tsuruoka, 2006; Okanohara et al., 2006). Machine learning algorithms used in these studies are Naive Bayes, C4.5, Maximum Entropy Models, Support Vector Machines, and Conditional Random Fields. Most of these studies applied machine learning techniques to tokenized sentences. Table 3 shows the scores reported by other systems. Tsai et al. (Tsai et al., 2006) and Zhou and Su (Zhou and Su, 2004) combined machine learning techniques and hand-crafted rules. Tsai et al. (Tsai et al., 2006) applied CRFs to the JNLPBA-2004 data. After applying pattern-</context>
</contexts>
<marker>Kazama, Makino, Ohta, Tsujii, 2002</marker>
<rawString>J. Kazama, T. Makino, Y. Ohta, J. Tsujii, Tuning Support Vector Machines for Biomedical Named Entity Recognition, Proc. of ACL-2002 Workshop on Natural Language Processing in the Biomedical Domain, pp. 1–8, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-D Kim</author>
<author>T Ohta</author>
<author>Y Tateisi</author>
<author>J</author>
</authors>
<title>Tsujii: GENIA corpus - semantically annotated corpus for bio-textmining, Bioinformatics</title>
<date>2003</date>
<contexts>
<context position="5082" citStr="Kim et al., 2003" startWordPosition="786" endWordPosition="789">with Hidden Markov Models (Baum and Petrie, 1966) and Maximum Entropy Markov Models (McCallum et al., 2000). Parameters of CRFs can be efficiently estimated through the log-likelihood parameter estimation using the forward-backward algorithm, a dynamic programming method. 2.1 Training and test data Experiments were conducted using the training and test sets of the JNLPBA-2004 data set(Kim et al., 2004). Training data The training data set used in JNLPBA-2004 is a set of tokenized sentences with manually annotated term class labels. The sentences are taken from the Genia corpus (version 3.02) (Kim et al., 2003), in which 2,000 abstracts were manually annotated by a biologist, drawing on a set of POS tags and 36 biomedical term classes. In the JNLPBA-2004 shared task, performance in extracting five term classes, i.e., protein, DNA, RNA, cell line, and cell type classes, were evaluated. Test Data The test data set used in JNLPBA-2004 is a set of tokenized sentences extracted from 404 separately collected MEDLINE abstracts, where the term class labels were manually assigned, following the annotation specification of the Genia corpus. 2.2 Overview of dictionary-based statistical NER Figure 1 shows the b</context>
</contexts>
<marker>Kim, Ohta, Tateisi, J, 2003</marker>
<rawString>J.-D. Kim, T. Ohta, Y. Tateisi, J. Tsujii: GENIA corpus - semantically annotated corpus for bio-textmining, Bioinformatics 2003, 19:i180-i182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
</authors>
<title>Tomoko Ohta, Yoshimasa Tsuruoka, Yuka Tateisi, Introduction to the Bio-Entity Recognition Task at</title>
<date>2004</date>
<booktitle>JNLPBA, Proc. of the Joint Workshop on Natural Language Processing in Biomedicine and its Applications (JNLPBA-2004),</booktitle>
<pages>70--75</pages>
<marker>Kim, 2004</marker>
<rawString>Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka, Yuka Tateisi, Introduction to the Bio-Entity Recognition Task at JNLPBA, Proc. of the Joint Workshop on Natural Language Processing in Biomedicine and its Applications (JNLPBA-2004), pp. 70–75, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kim</author>
<author>J Yoon</author>
</authors>
<title>Experimental Study on a Two Phase Method for Biomedical Named Entity Recognition,</title>
<date>2007</date>
<booktitle>IEICE Transactions on Informaion and Systems</booktitle>
<pages>90--7</pages>
<contexts>
<context position="19037" citStr="Kim and Yoon, 2007" startWordPosition="3002" endWordPosition="3005">ing algorithms used in these studies are Naive Bayes, C4.5, Maximum Entropy Models, Support Vector Machines, and Conditional Random Fields. Most of these studies applied machine learning techniques to tokenized sentences. Table 3 shows the scores reported by other systems. Tsai et al. (Tsai et al., 2006) and Zhou and Su (Zhou and Su, 2004) combined machine learning techniques and hand-crafted rules. Tsai et al. (Tsai et al., 2006) applied CRFs to the JNLPBA-2004 data. After applying pattern-based post-processing, they achieved the best F-score (75.12) among those reported so far. Kim and Yoon(Kim and Yoon, 2007) also applied heuristic post-processing. Zhou and Su (Zhou and Su, 2004) achieved an F-score of 73.77. Purely machine learning-based approaches have been investigated by several researchers. The GENIA Tagger (Tsuruoka, 2006) is trained on the JNLPBA-2004 Corpus. Okanohara et al. (Okanohara et al., 2006) employed semi-Markov CRFs whose performance was evaluated against the JNLPBA-2004 data set. Yamamoto et al. (Yamamoto et al., 2003) used SVMs for characterbased protein name recognition and sequential labelling. Their protein name extraction performance was 69%. This paper extends the machine l</context>
</contexts>
<marker>Kim, Yoon, 2007</marker>
<rawString>S. Kim, J. Yoon: Experimental Study on a Two Phase Method for Biomedical Named Entity Recognition, IEICE Transactions on Informaion and Systems 2007, E90-D(7):1103–1120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Kaoru Yamamoto</author>
<author>Yuuji Matsumoto</author>
</authors>
<title>Applying Conditional Random Fields to Japanese Morphological Analysis,</title>
<date>2004</date>
<booktitle>Proc. of Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>230--237</pages>
<contexts>
<context position="8383" citStr="Kudo et al., 2004" startWordPosition="1312" endWordPosition="1315">ph (i.e., trellis) according to the word order. Estimate the score of every path using the weights of node and edges estimated by training using Conditional Random Fields. Select the best path. Figure 3 shows an example of our dictionarybased approach. Suppose that the input is “IL2-mediated activation”. A trellis is created based on the lexical entries in a dictionary. The selection criteria for the best path are determined by the CRF tagging model trained on the Genia corpus. In this example, IL-2/NN-PROTEIN -/- mediated/VVN activation/NN is selected as the best path. Following Kudo et al. (Kudo et al., 2004), we adapted the core engine of the CRF-based morphological analyzer, MeCab1, to our POS/PROTEIN tagging task. MeCab’s dictionary databases employ double arrays (Aoe, 1989) which enable efficient lexical look-ups. The features used were: • POS-PROTEIN • bigram of adjacent POS • bigram of adjacent PROTEIN • bigram of adjacent POS-PROTEIN During the construction of the trellis, white space is considered as the delimiter unless otherwise stated within dictionary entries. This means that unknown tokens are character sequences without spaces. 2.2.2 Dictionary construction A dictionary-based approac</context>
</contexts>
<marker>Kudo, Yamamoto, Matsumoto, 2004</marker>
<rawString>Taku Kudo and Kaoru Yamamoto and Yuuji Matsumoto, Applying Conditional Random Fields to Japanese Morphological Analysis, Proc. of Empirical Methods in Natural Language Processing (EMNLP), pp. 230– 237, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data,</title>
<date>2001</date>
<booktitle>Proc. ofICML-2001,</booktitle>
<pages>282--289</pages>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira, Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data, Proc. ofICML-2001, pp.282–289, 2001</rawString>
</citation>
<citation valid="true">
<authors>
<author>K J Lee</author>
<author>Y S Hwang</author>
<author>H C Rim</author>
</authors>
<title>Two-Phase Biomedical NE Recognition based on SVMs,</title>
<date>2003</date>
<booktitle>Proc. of ACL 2003 Workshop on Natural Language Processing in Biomedicine,</booktitle>
<location>Sapporo,</location>
<contexts>
<context position="18293" citStr="Lee et al., 2003" startWordPosition="2885" endWordPosition="2888">rmance of the sequence labelling of the output of the first step. Note that tagging and the sequence labelling models are not retrained using the test set. 3.3 Discussion It is not possible in reality to train the recognizer on target data, i.e., the test set, but it would be possible for users to add discovered protein names to the dictionary so that they could improve the overall performance of the recognizer without retraining. Rule-based and procedural approaches are taken in (Fukuda et al., 1998; Franzen et al., 2002). Machine learning-based approaches are taken in (Collier et al., 2000; Lee et al., 2003; Kazama et al., 2002; Tanabe and Wilbur, 2002; Yamamoto et al., 2003; Tsuruoka, 2006; Okanohara et al., 2006). Machine learning algorithms used in these studies are Naive Bayes, C4.5, Maximum Entropy Models, Support Vector Machines, and Conditional Random Fields. Most of these studies applied machine learning techniques to tokenized sentences. Table 3 shows the scores reported by other systems. Tsai et al. (Tsai et al., 2006) and Zhou and Su (Zhou and Su, 2004) combined machine learning techniques and hand-crafted rules. Tsai et al. (Tsai et al., 2006) applied CRFs to the JNLPBA-2004 data. Af</context>
</contexts>
<marker>Lee, Hwang, Rim, 2003</marker>
<rawString>K. J. Lee, Y. S. Hwang and H. C. Rim (2003), Two-Phase Biomedical NE Recognition based on SVMs, Proc. of ACL 2003 Workshop on Natural Language Processing in Biomedicine, Sapporo, 2003.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A McCallum</author>
<author>D Freitag</author>
<author>F Pereira</author>
</authors>
<title>Maximum entropy Markov models for information extraction and segmentation,</title>
<booktitle>Proceedings of the Seventeenth International Conference on Machine Learning,</booktitle>
<pages>2000--591</pages>
<marker>McCallum, Freitag, Pereira, </marker>
<rawString>McCallum A, Freitag D, Pereira F.: Maximum entropy Markov models for information extraction and segmentation, Proceedings of the Seventeenth International Conference on Machine Learning, 2000:591-598.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Okanohara Daisuke</author>
<author>Yusuke Miyao</author>
</authors>
<title>Yoshimasa Tsuruoka and Jun’ichi Tsujii, Improving the Scalability of Semi-Markov Conditional Random Fields for Named Entity Recognition,</title>
<date>2006</date>
<booktitle>Proc. ofACL 2006,</booktitle>
<location>Sydney,</location>
<marker>Daisuke, Miyao, 2006</marker>
<rawString>Daisuke, Okanohara, Yusuke Miyao, Yoshimasa Tsuruoka and Jun’ichi Tsujii, Improving the Scalability of Semi-Markov Conditional Random Fields for Named Entity Recognition, Proc. ofACL 2006, Sydney, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyung-Mi Park</author>
<author>Seon-Ho Kim</author>
<author>Do-Gil Lee</author>
<author>Hae-Chang Rim</author>
</authors>
<title>Boosting Lexical Knowledge for Biomedical Named Entity Recognition,</title>
<date>2004</date>
<booktitle>Proc. of the Joint Workshop on Natural Language Processing in Biomedicine and its Applications (JNLPBA-2004),</booktitle>
<pages>76--79</pages>
<marker>Park, Kim, Lee, Rim, 2004</marker>
<rawString>Kyung-Mi Park, Seon-Ho Kim, Do-Gil Lee and Hae-Chang Rim. Boosting Lexical Knowledge for Biomedical Named Entity Recognition, Proc. of the Joint Workshop on Natural Language Processing in Biomedicine and its Applications (JNLPBA-2004), pp. 76-79, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc R¨ossler</author>
</authors>
<title>Adapting an NER-System for German to the Biomedical Domain,</title>
<date>2004</date>
<booktitle>Proc. of the Joint Workshop on Natural Language Processing in Biomedicine and its Applications (JNLPBA-2004),</booktitle>
<pages>92--95</pages>
<marker>R¨ossler, 2004</marker>
<rawString>Marc R¨ossler, Adapting an NER-System for German to the Biomedical Domain, Proc. of the Joint Workshop on Natural Language Processing in Biomedicine and its Applications (JNLPBA-2004), pp. 92–95, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burr Settles</author>
</authors>
<title>Biomedical Named Entity Recognition Using Conditional Random Fields and Novel Feature Sets,</title>
<date>2004</date>
<booktitle>Proc. of the Joint Workshop on Natural Language Processing in Biomedicine and its Applications (JNLPBA-2004),</booktitle>
<pages>104--1007</pages>
<marker>Settles, 2004</marker>
<rawString>Burr Settles, Biomedical Named Entity Recognition Using Conditional Random Fields and Novel Feature Sets, Proc. of the Joint Workshop on Natural Language Processing in Biomedicine and its Applications (JNLPBA-2004), pp. 104–1007, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu Song</author>
<author>Eunju Kim</author>
<author>Gary Geunbae Lee</author>
<author>Byoungkee Yi</author>
</authors>
<title>POSBIOTM-NER in the shared task of BioNLP/NLPBA</title>
<date>2004</date>
<booktitle>Proc. of the Joint Workshop on Natural Language Processing in Biomedicine and its Applications (JNLPBA-2004),</booktitle>
<pages>100--103</pages>
<marker>Song, Kim, Lee, Yi, 2004</marker>
<rawString>Yu Song, Eunju Kim, Gary Geunbae Lee and Byoungkee Yi, POSBIOTM-NER in the shared task of BioNLP/NLPBA 2004, Proc. of the Joint Workshop on Natural Language Processing in Biomedicine and its Applications (JNLPBA-2004), pp. 100-103, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Tanabe</author>
<author>W J Wilbur</author>
</authors>
<title>Tagging Gene and Protein</title>
<date>2002</date>
<journal>Names in Biomedical Text, Bioinformatics,</journal>
<volume>18</volume>
<issue>8</issue>
<pages>1124--1132</pages>
<contexts>
<context position="18339" citStr="Tanabe and Wilbur, 2002" startWordPosition="2893" endWordPosition="2896"> output of the first step. Note that tagging and the sequence labelling models are not retrained using the test set. 3.3 Discussion It is not possible in reality to train the recognizer on target data, i.e., the test set, but it would be possible for users to add discovered protein names to the dictionary so that they could improve the overall performance of the recognizer without retraining. Rule-based and procedural approaches are taken in (Fukuda et al., 1998; Franzen et al., 2002). Machine learning-based approaches are taken in (Collier et al., 2000; Lee et al., 2003; Kazama et al., 2002; Tanabe and Wilbur, 2002; Yamamoto et al., 2003; Tsuruoka, 2006; Okanohara et al., 2006). Machine learning algorithms used in these studies are Naive Bayes, C4.5, Maximum Entropy Models, Support Vector Machines, and Conditional Random Fields. Most of these studies applied machine learning techniques to tokenized sentences. Table 3 shows the scores reported by other systems. Tsai et al. (Tsai et al., 2006) and Zhou and Su (Zhou and Su, 2004) combined machine learning techniques and hand-crafted rules. Tsai et al. (Tsai et al., 2006) applied CRFs to the JNLPBA-2004 data. After applying pattern-based post-processing, th</context>
</contexts>
<marker>Tanabe, Wilbur, 2002</marker>
<rawString>L. Tanabe and W. J. Wilbur, Tagging Gene and Protein Names in Biomedical Text, Bioinformatics, 18(8), pp. 1124–1132, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Tjong Kim Sang</author>
<author>J Veenstra</author>
</authors>
<date>1999</date>
<booktitle>Representing Text Chunks,EACL-99,</booktitle>
<pages>173--179</pages>
<contexts>
<context position="12199" citStr="Sang and Veenstra, 1999" startWordPosition="1872" endWordPosition="1875">forms were added as RB. Pronouns were manually curated. PRP and PRP$ words were added to the dictionary. Wh-words were manually curated. As a result, WDT, WP, WP$ and WRB words were added to the dictionary. Words for other parts of speech were manually curated. 2.2.3 Statistical prediction of protein names Statistical sequential labelling was employed to improve the coverage of protein name recognition and to remove false positives resulting from the previous stage (dictionary-based tagging). We used the JNLPBA-2004 training data, which is a set of tokenized word sequences with IOB2(Tjong Kim Sang and Veenstra, 1999) protein labels. As shown in Figure 2, POSs of tokens resulting from tagging and tokens of the JNLPBA2004 data set are integrated to yield training data for sequential labelling. During integration, when the single token of a protein name found after tagging 5http://biology.plosjournals.org/ 6http://www.ims.uni-stuttgart.de/projekte/ corplex/TreeTagger/DecisionTreeTagger.html/ corresponds to a sequence of tokens from JNLPBA2004, its POS is given as NN-PROTEIN1, NNPROTEIN2,..., according to the corresponding token order in the JNLPBA-2004 sequence. Following the data format of the JNLPBA-2004 t</context>
</contexts>
<marker>Sang, Veenstra, 1999</marker>
<rawString>E.F. Tjong Kim Sang and J. Veenstra, Representing Text Chunks,EACL-99, pp. 173-179, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Tzong-Han Tsai</author>
<author>W-C Chou</author>
<author>S-H Wu</author>
<author>T-Y Sung</author>
<author>J Hsiang</author>
<author>W-L Hsu</author>
</authors>
<title>Integrating Linguistic Knowledge into a Conditional Random Field Framework to Identify Biomedical Named Entities, Expert Systems with Applications,</title>
<date>2006</date>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="18723" citStr="Tsai et al., 2006" startWordPosition="2953" endWordPosition="2956">ining. Rule-based and procedural approaches are taken in (Fukuda et al., 1998; Franzen et al., 2002). Machine learning-based approaches are taken in (Collier et al., 2000; Lee et al., 2003; Kazama et al., 2002; Tanabe and Wilbur, 2002; Yamamoto et al., 2003; Tsuruoka, 2006; Okanohara et al., 2006). Machine learning algorithms used in these studies are Naive Bayes, C4.5, Maximum Entropy Models, Support Vector Machines, and Conditional Random Fields. Most of these studies applied machine learning techniques to tokenized sentences. Table 3 shows the scores reported by other systems. Tsai et al. (Tsai et al., 2006) and Zhou and Su (Zhou and Su, 2004) combined machine learning techniques and hand-crafted rules. Tsai et al. (Tsai et al., 2006) applied CRFs to the JNLPBA-2004 data. After applying pattern-based post-processing, they achieved the best F-score (75.12) among those reported so far. Kim and Yoon(Kim and Yoon, 2007) also applied heuristic post-processing. Zhou and Su (Zhou and Su, 2004) achieved an F-score of 73.77. Purely machine learning-based approaches have been investigated by several researchers. The GENIA Tagger (Tsuruoka, 2006) is trained on the JNLPBA-2004 Corpus. Okanohara et al. (Okano</context>
</contexts>
<marker>Tsai, Chou, Wu, Sung, Hsiang, Hsu, 2006</marker>
<rawString>Richard Tzong-Han Tsai, W.-C. Chou, S.-H. Wu, T.-Y. Sung, J. Hsiang, and W.-L. Hsu, Integrating Linguistic Knowledge into a Conditional Random Field Framework to Identify Biomedical Named Entities, Expert Systems with Applications, 30 (1), 2006.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Yoshimasa Tsuruoka</author>
</authors>
<journal>GENIA Tagger</journal>
<volume>3</volume>
<pages>2006</pages>
<marker>Tsuruoka, </marker>
<rawString>Yoshimasa Tsuruoka, GENIA Tagger 3.0, http://www-tsujii.is.s.u-tokyo.ac.jp/ GENIA/tagger/,2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yamamoto</author>
<author>T Kudo</author>
<author>A Konagaya</author>
<author>Y Matsumoto</author>
</authors>
<title>Protein Name Tagging for Biomedical Annotation in Text,</title>
<date>2003</date>
<booktitle>in Proc. of ACL-2003 Workshop on Natural Language Processing in Biomedicine,</booktitle>
<location>Sapporo,</location>
<contexts>
<context position="18362" citStr="Yamamoto et al., 2003" startWordPosition="2897" endWordPosition="2900">. Note that tagging and the sequence labelling models are not retrained using the test set. 3.3 Discussion It is not possible in reality to train the recognizer on target data, i.e., the test set, but it would be possible for users to add discovered protein names to the dictionary so that they could improve the overall performance of the recognizer without retraining. Rule-based and procedural approaches are taken in (Fukuda et al., 1998; Franzen et al., 2002). Machine learning-based approaches are taken in (Collier et al., 2000; Lee et al., 2003; Kazama et al., 2002; Tanabe and Wilbur, 2002; Yamamoto et al., 2003; Tsuruoka, 2006; Okanohara et al., 2006). Machine learning algorithms used in these studies are Naive Bayes, C4.5, Maximum Entropy Models, Support Vector Machines, and Conditional Random Fields. Most of these studies applied machine learning techniques to tokenized sentences. Table 3 shows the scores reported by other systems. Tsai et al. (Tsai et al., 2006) and Zhou and Su (Zhou and Su, 2004) combined machine learning techniques and hand-crafted rules. Tsai et al. (Tsai et al., 2006) applied CRFs to the JNLPBA-2004 data. After applying pattern-based post-processing, they achieved the best F-</context>
</contexts>
<marker>Yamamoto, Kudo, Konagaya, Matsumoto, 2003</marker>
<rawString>K. Yamamoto, T. Kudo, A. Konagaya and Y. Matsumoto, Protein Name Tagging for Biomedical Annotation in Text, in Proc. of ACL-2003 Workshop on Natural Language Processing in Biomedicine, Sapporo, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guofeng Zhou</author>
<author>Jian Su</author>
</authors>
<title>Exploring Deep Knowledge Resources in Biomedical Name Recognition,</title>
<date>2004</date>
<booktitle>Proceedings of the Joint Workshop on Natural Language Processing ofBiomedicine and its Applications (JNLPBA2004),</booktitle>
<pages>96--99</pages>
<contexts>
<context position="18759" citStr="Zhou and Su, 2004" startWordPosition="2961" endWordPosition="2964">roaches are taken in (Fukuda et al., 1998; Franzen et al., 2002). Machine learning-based approaches are taken in (Collier et al., 2000; Lee et al., 2003; Kazama et al., 2002; Tanabe and Wilbur, 2002; Yamamoto et al., 2003; Tsuruoka, 2006; Okanohara et al., 2006). Machine learning algorithms used in these studies are Naive Bayes, C4.5, Maximum Entropy Models, Support Vector Machines, and Conditional Random Fields. Most of these studies applied machine learning techniques to tokenized sentences. Table 3 shows the scores reported by other systems. Tsai et al. (Tsai et al., 2006) and Zhou and Su (Zhou and Su, 2004) combined machine learning techniques and hand-crafted rules. Tsai et al. (Tsai et al., 2006) applied CRFs to the JNLPBA-2004 data. After applying pattern-based post-processing, they achieved the best F-score (75.12) among those reported so far. Kim and Yoon(Kim and Yoon, 2007) also applied heuristic post-processing. Zhou and Su (Zhou and Su, 2004) achieved an F-score of 73.77. Purely machine learning-based approaches have been investigated by several researchers. The GENIA Tagger (Tsuruoka, 2006) is trained on the JNLPBA-2004 Corpus. Okanohara et al. (Okanohara et al., 2006) employed semi-Mar</context>
</contexts>
<marker>Zhou, Su, 2004</marker>
<rawString>Guofeng Zhou and Jian Su, Exploring Deep Knowledge Resources in Biomedical Name Recognition, Proceedings of the Joint Workshop on Natural Language Processing ofBiomedicine and its Applications (JNLPBA2004), pp. 96-99, 2004.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>