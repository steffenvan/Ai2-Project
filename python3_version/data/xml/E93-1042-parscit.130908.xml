<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.765276">
NEW FRONTIERS BEYOND CONTEXT-FREENESS:
DI-GRAMMARS AND DI-AUTOMATA.
</note>
<author confidence="0.566415">
Peter Staudacher
</author>
<affiliation confidence="0.390361">
Institut fur Allgemeine und Indogermanische
</affiliation>
<address confidence="0.6839174">
Sprachwissenschaft
Universitat Regensburg
Postfach 397
8400 Regensburg 1
Germany
</address>
<sectionHeader confidence="0.978299" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999924111111111">
A new class of formal languages will be defined
- the Distributed Index Languages (DI-lan-
guages). The grammar-formalism generating the
new class - the DI-grammars - cover unbound
dependencies in a rather natural way. The place
of DI-languages in the Chomsky-hierarchy will
be determined: Like Aho&apos;s indexed Languages,
DI-languages represent a proper subclass of
Type 1 (contextsensitive languages) and prop-
erly include Type 2 (context-free languages), but
the DI-class is neither a subclass nor a super-
class of Aho&apos;s indexed class. It will be shown
that, apart from DI-grammars, DI-languages can
equivalently be characterized by a special type of
automata - DI-automata. Finally, the time com-
plexity of the recognition-problem for an inter-
esting subclass of DI-Grammars will approxi-
mately be determined.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.984977555555555">
It is common practice to parse nested Wh-dependen-
cies, like the classical example of Rizzi (1982) in (1),
(1) Tuo fratello, [a cui] 1 mi domando [che storie]2
abbiano raccontato t2 t1, era molto preoccupato
(Your Brother, [to whom]1 I wonder [which sto-
riesb they told t2 t1 was very troubled)
using a stack mechanism. Under the binary branching
hypothesis the relevant structure of (1) augmented by
wh-stacks is as follows:
</bodyText>
<equation confidence="0.939846333333333">
[a cui]i mi domando
I-push --4t11—&gt; 1—push —,[t2,t11-1
[che storie]2abbiano V2[t2,t1]
/ \
V1 [t2] PP[ti]
/ \
[] NP [t2] pop
I P°P I
raccontato t2 t1
</equation>
<bodyText confidence="0.999102181818182">
Up to now it is unclear, how far beyond context-
freeness the generative power of a Type 2 grammar
formalism is being extended if such a stack mechanism
is grafted on it (assuming, of course, that an upper
bound for the size of the stack can not be motivated).
Fernando Pereira&apos;s concept of Extraposition Gram-
mar (XG), introduced in his influential paper (Pereira,
1981; 1983; cf. Stabler, 1987) in order to delimit the
new territory, can be shown to be inadequate for this
purpose, since it is provable that the class of languages
generable by XGs coincides with Type 0 (i.e. XGs have
the power of Turing machines), whereas the increase of
power by the stack mechanism is not even enough to
generate all Type 1 languages (see below).
In (2) an additional point is illustrated:
the stack R2,t1] belonging to V2 has to be divided into
the substacks It21 and [til, which are then inherited by
the daughters VI and PP. For the PP-index t is not dis-
charged from the top of the V2-stack [t2,t1]. Generaliz-
ing to stacks of unlimited size, the partition of a stack
among the inheriting subconstituents K1 and K2 of a
constituent Ko is as in (3)
</bodyText>
<note confidence="0.289697">
(3) KO [t
K1 R K2 [ti+1,...,tk]
</note>
<page confidence="0.997034">
358
</page>
<bodyText confidence="0.999834333333333">
If the generalization in (3) is tenable, the extension of
context-free grammars (Vijay-Shanker and Weir, 1991,
call the resulting formalism &amp;quot;linear indexed grammar&amp;quot;
(LIG)) discussed by Gazdar in (Gazdar, 1988), in
which stacks are exclusively passed over to a single
daughter (as in (3.1)), is too weak.
</bodyText>
<equation confidence="0.397604666666667">
(3.1) a)Keti„...,tk] b)
1
K2
</equation>
<bodyText confidence="0.965592">
Stack-transmission by distribution, however, as in (3)
suggests the definition of a new class of grammars
properly containing the context-free class.
</bodyText>
<sectionHeader confidence="0.928998" genericHeader="introduction">
2 DI-Grammars and DI-languages
</sectionHeader>
<bodyText confidence="0.999235666666667">
A DI-grammar is a 5-tupel G= (N,T,F,P,S), where
N,T,S are as usual, F is a alphabet of indices, P is a set
of rules of the following form
</bodyText>
<listItem confidence="0.9967695">
1) (a) A a (b) A aBfB (c) Af a ,
(A, BEN; a, BE(NuT)*;fEF)
</listItem>
<bodyText confidence="0.618142">
The relation &amp;quot;= &gt; &amp;quot; or &amp;quot;directly derives&amp;quot; is defined as
follows:
</bodyText>
<listItem confidence="0.953159666666667">
2) a =&gt;
if either i)
a = Mindex y, 8,y E (NeuT)*, indexEe , AEN,
</listItem>
<equation confidence="0.967458545454545">
A B1132...Bn is a rule of form 1)(a)
B = Olhinder1B2inder2...13ninderny
or ii)
a = Mindevc y, 5,y E (NeuT)*, index EF*, AEN,
A —&gt; B1..13kf...Bn is a rule of form 1)(b), fEF
B = OB1indec1...Bkfindexk...13ninderny
or iii)
a = Mfindex y ,6 ,y E (14F*LID*, index EF*, AEN,
Al —&gt; BiB2...Bn is a rule of form 1)(c), fEF
B = 5131index/B2inder2...Bnindexny
(*) and index = index iinder2...indexn,
</equation>
<bodyText confidence="0.997236444444444">
and for B e T: indexi = s (i.e. the empty word)
(05in)
The reflexive and transitive closure *=&gt; of =&gt; is de-
fined as usual.
Replacing (*) by&amp;quot;indexi= index for BiE N, indexi= s
for Bi E T&amp;quot;, changes the above definition into a defini-
tion of Aho&apos;s well known indexed grammars. How in-
dex-percolation differs in indexed and Di-grammars is
illustrated in (4).
</bodyText>
<equation confidence="0.586682">
(4) Index-Percolation
(i) in (ii) in
Aho &apos;s Indexed-Grammars DI-Grammars
mill2f3f4
/ \
Lf1l31314 Wil21314
</equation>
<bodyText confidence="0.5945174">
i.e:index multiplication vs. index distribution
The region in the Chomsky hierarchy occupied by the
class of DI-languages is indicated in (5)
(5)
where
</bodyText>
<equation confidence="0.9947326">
(5.1) L1 = {anbncn; n?_1}
(5.2) L2 = {ak; k = 2n, 0 5 n}
(5.3) L3 = { wiw2...wnziwn...znwizn+im(wn)m(wn..1)
...m(w2)m(w1); n.?_1 &amp; wiE{a,b}+ (1 i._411) &amp;
z1z2...znzn+1 E 131)
</equation>
<bodyText confidence="0.998959">
m(y) is the mirror image of y and DI is the
Dyck language generated by the following
CFG Gk (Di=L(Gk)), Gk = ({S},{[,]},Rk, S),
where Rk = {S [S], S —&gt; SS, S s}
</bodyText>
<equation confidence="0.661609">
(5.4) L4 = {ak; k = nn, (L4 is not an indexed
language, s. Takeshi Hayashi (1973)).
</equation>
<bodyText confidence="0.998822">
By definition (see above), the intersection of the class
of indexed languages and the class of DI-languages in-
cludes the context-free (cfr) languages. The inclusion is
proper, since the (non-cfr) language L1 is generated by
G1 = ({S,A,B}, {a,b,c), {f,g}, RI, S), where R1 = {S
—&gt; aAfc, A aAgc, A —&gt; B, Bg --&gt; bB, Bf b}, and
G1 obviously is both a DI-grammar and and an indexed
grammar.-
Like cfr. languages and unlike indexed languages,
DI-languages have the constant growth property (i.e.
for every DI-granunar G there exists a kEN, s.th. for
every wEL(G), s.th. Twl&gt;k, there exists a sequence w1
</bodyText>
<figure confidence="0.992143714285714">
Type-0
Type-I
.
14
.13
Aho&apos;s Indexed
Languages
.11
DI-Languages
Type-2
contextfsve
mfil2f3f4
/ \
Lf1f2 Rf314
</figure>
<page confidence="0.997064">
359
</page>
<bodyText confidence="0.978514375">
W2,1173,...(WiEL(G)), such that Iyvnl&lt; wn+11 &lt;
(n+1)xlwl for every member wn of the sequence). Hence
L2, and a fortiori L4, is not a DI-language. But L2 is an
indexed language, since it is generated by the indexed
grammar G2 =({S,A,D}, {a}, {f,g}, R2, S), where R2
= {S —&gt;Ag, A —&gt; Al, A --&gt; D, Df —&gt; DD, Dg —&gt; a).
L3 is a DI-language, since it is generated by the DI-
grammar G3 = (I S,M,Z),{ a,b,[,]),{f,g},R3,S) where
</bodyText>
<equation confidence="0.779096166666667">
R3 = {S aSfa, Zf Za, Zg Zb,
S —› bSgb, M —&gt; MM, Zf —&gt; a, Zg b
S—&gt;M, M—&gt;Z)
e.g. abb[b[ab]]bba (E L3) is derived as follows:
S aSfa abSgjba abbSggjhba abbMggfiba
abb[MggAbba abb[MgMgAbba (here the index &amp;quot;ggf&apos;
</equation>
<bodyText confidence="0.969277666666667">
has been distributed) abb[ZgMgAbba
abb[bMgAbba abb[b[Mabba abb[b[Zabba
abb[b[Zfb]]bba abb[b[abfibba.
</bodyText>
<subsectionHeader confidence="0.990198">
2.1 DI-Grammars and Indexed Grammars
</subsectionHeader>
<bodyText confidence="0.996131428571429">
Considering the well known generative strength of in-
dexed grammars, it is by no means obvious that L3 is
not an indexed language. In view of the complexity of
the proof that L3 is not indexed, only some important
points can be indicated - referring to the 3 main parts
of every word x E L3 by x1, [xm],xpas illustrated in the
example (6):
</bodyText>
<equation confidence="0.980952">
(6)
ab abb abbb abbbbffabbbb[[abbb]abbflab]bbbbabbbabbaba
Lw 1J Lw2J L1,3,1 Lw L-w4-.I Lw3_1 Lw2J LJ
xi. 1L--__[ xm
=X
</equation>
<bodyText confidence="0.97781325">
Assume that there is a indexed grammar Gi=
(N,T,F,P,S) such that L3=L(G1):
1. Since G1 can not be contextfree, it follows from the
intercalation (or &amp;quot;pumping&amp;quot;) lemma for indexed gram-
mars proved by Takeshi Hayashi in (Hayashi, 1973)
that there exists for G1 an integer k such that for any x
E L3 such that lxl&gt;k a derivation can be found with the
following properties:
S =*=&gt; zAfig&apos;=*=&gt; zsiAfpfzisi &apos;z&apos;
=*=&gt; zs r Af,u &apos;fig &apos;s &apos;z&apos;=*=&gt;zs r 113fau furl &apos; s &apos;z &apos;
=*=&gt; zs rtiBfriti &apos;r&apos;s &apos;z&apos; =*=&gt; x,
(zz&apos;, rirl&apos;E T*, E T±, f E F, E F*)
By intercalating subderivations which can effectively
be constructed this derivation can be extended as fol-
lows
S =*=&gt; zAfliz&apos;=*=&gt;zsiAf,ufrisi&apos;z&apos;
=*=&gt; zs 1... snA(fp s &apos;z&apos;
=*=&gt; zsi...snrnB(f,u s &apos;z
=*=&gt; zs 1... snrntn... t iBfrit &apos;...tn &apos;rn&apos;sn s &apos;z &apos;
--*=&gt; zs snrntn... tiwt &apos;...tn&apos;rn&apos;sn&apos;... 51Z
The interdependently extendible parts of x s1...sn,
tn...ti, ti&apos;...tn&apos;, rnrn&apos;, and sn&apos;... s1&apos;, can not all be sub-
words of the central component [xm] of x (or all be
subwords of the peripheral components xixr), else,
[xin] (or xixr) could be increased and decreased inde-
pendently of the peripheral components x1 and xr (or of
[xm], respectively) of x, contradicting the assumption
that x E L3. Rather, the structure of x necessitates that
.si...sn and sn&apos;...si&apos; be subwords of xixr and that the
&amp;quot;pumped&amp;quot; index (fp )fl be discharged deriving the cen-
tral component [xm]. Thus, we know that for every /&gt;0
there exists an index g E Ft a x E L3, and a subword
[xin&apos;l of the central part [xin] of x such that [xml&gt;/
and Mg=*=&gt;[xm] (M=B or the nonterminal of a de-
scendant of A(fiu )flA). To simplify our exposition we
write [xin&apos;l instead of [xtn] and have
</bodyText>
<listItem confidence="0.607394">
(7) Mg =*=&gt; [xm]
</listItem>
<bodyText confidence="0.8017815">
with the structure of x1 and xr being encoded and stored
in the index g.
</bodyText>
<listItem confidence="0.75384905">
2. The balanced parentheses of [xin] can not be en-
coded in the index g in (7) in such a manner that [xin]
is a homomorphic image of g. For the set I={g&apos;;
S=*=&gt;xiMg&apos;xr =*=&gt;xi[xmlxrEL3) of all indices which
satisfy (7) is regular (or of Type 3), but because of the
Dyck-structure of [xm], Livr--{[xin];xi[xm]xrEL3) is
not regular but essentially context-free or of Type 2.
3. In the derivation underlying (7) essential use of
branching rules of the type A—*B1B2...Bk (12) has to
be made in the sense that the effect of the rules can not
be simulated by linear rules. Else the central part [xin]
could only have linear and not trans-linear Dyck-struc-
ture. Without branching rules the required trans-linear
parenthetical structure could only be generated by the
use of additional index-introducing rules in (7), in or-
der to &amp;quot;store&amp;quot; and coordinate parentheses, which, how-
ever, would destroy the dependence of [xml from x1 and
xr.
4.For every L3 contains words w of the form
(8)
</listItem>
<equation confidence="0.790038">
Ell [wk1 iwk-i 1 1 liwk-211wk-3111-1111-111mOvO• •Itt(w1)
1-n+1-1
x r
</equation>
<page confidence="0.962471">
360
</page>
<bodyText confidence="0.99975075">
where k--211, WiE {a,b}± for lsi211., m(w) is the mirror
image of w1.
i.e the central part [xm] of such a word contains
211+1-1 pairs of parentheses, as shown in (9) for n=3:
</bodyText>
<equation confidence="0.552522">
(9) [Ww81 Ew711[[w6] [w5]]] [[[w4] [w3]1[[w2] [w i]]]]
</equation>
<bodyText confidence="0.987062151515152">
According to our assumption, GI generates all words
having the form (8). Referring to the derivation in (7),
consider a path from MIA to any of the parenthesized
parts wi of [xm] in (8). (Ignoring for expositional pur-
poses the possibility of &amp;quot;storing&amp;quot; (a constant amount of)
parentheses in nonterminal nodes,) because of 2. and 3.
an injective mapping can be defined from the set of
pairs of parentheses containing at least two other (and
because of the structure of (8) disjunct) pairs of paren-
theses into the set of branching nodes with (at least)
two nonterminal daughters. Call a node in the range of
the mapping a P-Node. Assuming without loss of gen-
erality that each node has at most two nonterminal
daughters, there are 2n-1 such P-nodes in the subtree
rooted in Mp and yielding the parenthesized part [xm]
of (8). Furthermore, every path from Mi to the root Wi
of the subtree yielding [wi] contains exactly n P-nodes (
where 211=k in (8)).
Call an index-symbol f inside the index-stack p. a wi-
index if f is discharged into a terminal constituting a
parenthesized wi in (8) (or equivalently, if f encodes a
symbol of the peripheral x1..xr).
Let ft be the first (or leftmost) wi-index from above in
the index-stack m, and let wt be the subword of [xm]
containing the terminal into which ft is discharged, i.e
all other wi-indices in are only accessible after ft has
been consumed. Thus, for 1.1.—Kfta we get from (7)
Mafto—+=&gt;uBt[rtftcriv—+=&gt;utWarftcrIvt and
wt[tftal&apos;+&apos;&gt;wt
The path Pt from Mp. to wt contains n B-nodes, for
k=2&amp;quot; in (8). For every B-node B (0..&lt;j&lt;n) of Pt we ob-
tain because of the index-multiplication effected by no-
terminal branching:
</bodyText>
<equation confidence="0.86182525">
Bi [Tiro] =&gt; Li [xi ftcr]Ri [tifta] and
1-j [94°]&apos; * = &gt; nj +113j +1N+lfealvj +1
(Bi,Bi +1,Li,Ri ,a EF*,ft EF,ui +1,vi + 1E {a,
b5[,] }*)
</equation>
<bodyText confidence="0.9690851875">
Every path Pi branching off from Pt at Bi [yes] leads
to a word wJ . derived exclusively by discharging wrin-
dices situated in p below (or on the right side of) ft.
Consequently, ft has to be deleted on every such path
P. before the appropriate indices become accessible,
J&apos;
i.e. we get for every j with 05 j&lt;n:
ft
BJ. ler] = &gt; uiRi[rifta]yi =*= &gt; yiCi[ftcr]zi,
(Bi,Ri,Ci EN,Ti,crEF*,ft EF)
Thus, for n&gt;INI in (8) (INI the cardinality of the non-
terminal alphabet N of Gj, ignoring, as before the con-
stant amount of parenthesis-storing in nonterminals)
because of 1{Ci;05j&lt;n}l=n the node-label Ci[fics] occurs
twice on two different paths branching off from Pt , i.e.
there exist p, q (Op&lt;q&lt;n) such that:
</bodyText>
<equation confidence="0.9846646">
Matta = + = &gt; upRp[tpfta]vepqftcr]y and
Rp[xpftcrl = *= &gt; ypC[fto]zp= + = &gt; ypzzp
RciNftcr] =*= &gt; yciC[ftcr]zq.= + = &gt; yezq,
(u=itftcs, ap,Tcro-EF ,ft:F; M,Rp,N,CEN;
up,vq,y,y(pyp,zp,z(pzET )
</equation>
<bodyText confidence="0.9351760625">
where ZE{Z1W1...ZrWrZr+1; WiE{a,b}+&amp; ED&apos;
(= the Dyck-language from (5.3)).
I.e. GI generates words w&amp;quot; =x1&amp;quot;[xmixr&amp;quot;, the central
part of which contain a duplication (of &amp;quot;z&amp;quot; in
[xml=yizy2zy3) without correspondence in x1&amp;quot; or xr&amp;quot;,
thus contradicting the general form of words of L3.
Hence L3 is not indexed.
2.2 DI-Grammars and Linear Indexed Grammars&apos;
As already mentioned above, Gazdar in (Gazdar, 1988)
introduced and discussed a grammar formalism, after-
wards (e.g. in (Weir and Joshi, 1988)) called linear in-
dexed grammars (LIG&apos;s), using index stacks in which
only one nonterminal on the right-hand-side of a rule
can inherit the stack from the left-hand-side, i.e. the
rules of a LIG G=(N,T, F, P, S) with N,F,T,S as above,
are of the Form
</bodyText>
<figure confidence="0.62889725">
i. A[..] -÷Ai[]...Ai[..].. An
A[..] Ai[ f..]...An
iii.A[f..]-&gt;A1[]..A[..l..A
iv. An —&gt;a
</figure>
<bodyText confidence="0.901726">
where AI,...,AnEN, fEF, and a ETV(6). The &amp;quot;derives&amp;quot;-
</bodyText>
<equation confidence="0.706549">
relation =&gt; is defined as follows
aA[fl..iniP=&gt;01,41[1- -A [11-inl-AnUR
</equation>
<bodyText confidence="0.691071333333333">
if A[..]
&apos;Thanks to the anonymous referees for suggestions for this
section and the next one.
</bodyText>
<page confidence="0.990269">
361
</page>
<construct confidence="0.687349166666667">
aAVI frJ13—&gt;all 1 [1 n
if A[..]--&gt;Ain..Ai[f..]..AneP
aAlffl...frOP=&gt;°&amp;quot; 1 [1- -A ill ifn] AR
if A[f..]
[]0=&gt;aaf3
if A[]
</construct>
<bodyText confidence="0.989199555555556">
=*=&gt; is the reflexive and transitive closure of =&gt;, and
L(G)={w; WET* &amp; S[]=*=&gt;w}.
Gazdar has shown that LIGs are a (proper) subclass of
indexed grammars. Joshi, Vijay-Shanker, and Weir
(Joshi, Vijay-Shanker, and Weir, 1989; Weir and Joshi,
1988) have shown that LIGs, Combinatory Categorial
Grammars (CCG), Tree Adjoinig Grammars (TAGs),
and Head Grammars (HGs) are weakly equivalent.
Thus, if an inclusion relation can be shown to hold be-
tween DI-languages (DEL) and LILs, it simultaneously
holds between the DEL-class and all members of the
family.
To simulate the restriction on stack transmission in a
LIG GHNI,T, F1, P1, S1) the following construction of
a DI-grammar Gd suggests itself:
Let Gd =(N, T, F, P, S) where N-{S}={X&apos;; XEN1},
F={f ; fEFI}u{#}, and P={S—&gt;S1&apos;#}
{ A&apos;f --&gt;A . A A nEP1)
u{A&apos;#—a; -±aEP1}
It follows by induction on the number of derivation
steps that for X&apos; EN, XENI, R&apos;EF*, LEFT*, and w ET*
(10) X&apos;1.1.&apos;i*G=&gt;w if and only if X[p.]=*Gi=&gt;w
where X&apos;=h(X) and g&apos;=h(g) (h is the homomorphism
from (N1k..)F0* into (NuF)* with h(Z)=Z). For the
nontrivial part of the induction, note that A&apos;filif can not
be terminated in G.
Together with S=&gt;S1&apos;1# (10) yields L(G1)=L(G).
The inclusion of the LIG-class in the DI-class is
proper, since L3 above is not a LIG-language, or to
give a more simple example:
Lw= o11a1nla2n1b1n2b2n2nnin= ni n2) is accord-
ing to (Vijay-Shanker, Weir and Joshi, 1987) not in
TAL, hence not in LIL. But (the indexed langauge)
is generated by the DI-Grammar
Gw=({ S,A,B },{a,b,a1 ,a2,b1,b2 }, {
a lAa2,Bf-÷b
</bodyText>
<subsectionHeader confidence="0.999834">
2.3 Generalized Composition and Combinatory
Categorial Grammars
</subsectionHeader>
<bodyText confidence="0.998777">
The relation of DI-grammars to Steedman&apos;s Combina-
tory Categorial Grammars with Generalized Composi-
tion (GC-CCG for short) in the sense of (Weir and
Joshi, 1988) is not so easy to determine. If for each
composition rules of the form
</bodyText>
<equation confidence="0.5761425">
(x/Y) (...(Ylizil)I2....Inzn)---&gt; and
(x\Y)-3
</equation>
<bodyText confidence="0.997589">
are permitted, the generative power of the resulting
grammars is known to be stronger than TAGs (Weir
and Joshi, 1988).
</bodyText>
<equation confidence="0.53118675">
Now, the GC-CCG given by
f(6)=0) f(ai)={S/X/#, S/XVi, NX/#,#/X\#}
f(a)={A,X\A} f(b1)={S/Y/#, S/Y\#, #1/Y/#,NY\#}
f(b)={B,Y\B} f([) ={K} fa)={#/#\K, #VAK.}
</equation>
<bodyText confidence="0.9604245">
generates a language Lc, which when intersected with
the regular set
{a,b}+{[,],ai,b0+{a,b}+
yields a language Lp which is for similar reasons as L3
not even an indexed language. But Lp does not seem to
be a DI-language either. Hence, since indexed lan-
guages and DI-languages are closed under intersection
with regular sets., Lc is neither an indexed nor (so it
appears) a DI-language.
The problem of a comparison of DI-grammars and
GC-CCGs is that, inspite of all appearances, the com-
bination of generalized forward and backward com-
position can not directly simulate nor be simulated by
index-distribution, at least so it seems.
</bodyText>
<sectionHeader confidence="0.981359" genericHeader="method">
3 DI-Automata
</sectionHeader>
<bodyText confidence="0.999304">
An alternative method of characterizing DI-languages
is by means of DI-automata defined below.
DI-automata (dia) have a remote resemblance to
Aho&apos;s nested stack automata (nsa). They can best be
viewed as push down automata (pda) with additional po-
wer: they can not only read and write on top of their
push down store, but also travel down the stack and
(recursively) create new embedded substacks (which can
be left only after deletion). dia &apos;s and nsa&apos;s differ in the
following respects:
</bodyText>
<listItem confidence="0.526075">
1. a dia is only allowed to begin to travel down the
</listItem>
<bodyText confidence="0.999938111111111">
stack or enter the stack reading mode, if a tape-symbol
A on top of the stack has been deleted and stored in a
special stack-reading-state qA, and the stack-reading
mode has to be terminated as soon as the first index-
symbol f from above is being scanned, in which case
the index-symbol concerned is deleted and an embed-
ded stack is created, provided the transition-function
gives permission. Thus, every occurrence of an index-
symbol on the stack can only be &amp;quot;consumed&amp;quot; once, and
</bodyText>
<page confidence="0.992278">
362
</page>
<bodyText confidence="0.897356529411765">
only in combination with a &amp;quot;matching&amp;quot; non-index-sym-
bol.
A nsa, on the other hand, embeds new stacks behind
tape symbols which are preserved and can, thus, be
used for further stack-embeddings. This provides for
part of the stack multiplication effect.
2. Moving through the stack in the stack reading mode,
a dia is not allowed to pass or skip an index symbol.
Moreover, no scanning of the input or change of state is
permitted in this mode.
A nsa, however, is allowed both to scan its input and
change its state in the stack reading mode, which, to-
gether with the license to pass tape symbols repeatedly,
provides for another part of the stack multiplication ef-
fect.
3. Unlike a nsa, a dia needs two tape alphabets, since
only &amp;quot;index symbols&amp;quot; can be replaced by new stacks,
moreover it requires two sets of states in order to di-
stinguish the pushdown mode from the stack reading
mode.
Formally, a di-automaton is a 10-tuple D =(q,QFT,F,
where q is the control state for the pushdown mode,
QJ-=(q; Aff) a finite set of stack reading states,
Ta finite set of input symbols,
Fa finite set of storage symbols,
I a finite set of index symbols where InF=.0,
ZoEFiS the initial storage symbol,
$ is the top-of-stack marker on the storage tape,
¢ is the bottom-of embedded stack marker on the
storage tape,
# marks the bottom of the storage tape,
where $,¢,#eruTul,
Dir = (-1,0,1) (for &amp;quot;1 step upwards&amp;quot;,&amp;quot;stay&amp;quot;,&amp;quot;1 step
downwards&amp;quot;, respectivly,
</bodyText>
<equation confidence="0.696803285714286">
E = (0,1) (&amp;quot;halt input tape&amp;quot;, &amp;quot;shift input tape&amp;quot;, respec-
tively),
T&apos;= T (#), r = FL) {¢),
is a mapping
1) in the push down mode:
from (q) x T&apos; x $Finto finite subsets of
(q) x D x $F((R1)*)
</equation>
<bodyText confidence="0.8444838">
2) in the stack reading mode: for every A el-
(a)from (qA) x T&apos; x r into subsets of (qA) x (0) x (1)
(for walking down the stack)
(b)from (q) xT&apos; x $(A) into subsets of (qA) x (0) x (1)
(for initiating the stack reading mode)
</bodyText>
<listItem confidence="0.875054444444444">
(c) from (q) x x (A) into subsets of (q) x (0) x (-1)
(for climbing up the stack)
3) in the stack creation mode:
from Qrx T&apos; xl into finite subsets of
(q) x (0) x $F((Ful)*)¢, and from Qrx T&apos; x $1 into
finite subsets of {q) x (0) x $$F((ruI)*)¢ (for re-
placing index symbols by new stacks, preserving the
top-of-stack marker $)
4) in the stack destruction mode:
</listItem>
<bodyText confidence="0.917715">
from (q) x T&apos; x (4) into subsets of (q) x (0).
As in the case of Aho&apos;s nested stack automaton a confi-
guration of a DI-automaton D is a quadruple
(p, a an#,i, X ... ...Xm),
where
</bodyText>
<listItem confidence="0.9978334">
1. p E {q}uQF is the current state of D;
2. a1.. .an is the input string, # the input endmarker;
3. i (1 _]1._n+1) the position of the symbol on the input
tape currently being scanned by the input head (=ai);
4. Xi...^Xj...Xm the content of the storage tape where
</listItem>
<bodyText confidence="0.9969264">
for m&gt;1 .X1=$A, X2...Xm_i E (Fl.)
KJ{$,0})*; Xi is the stack symbol currrently being read
by the storage tape head. If m=1, then Xm=$#.
As usual, a relation ID representing a move by the au-
tomaton is defined over the set of configurations:
</bodyText>
<equation confidence="0.588428583333333">
(i)(q, a I a$AAYO)
FD (q, a ... anti, i+d,a$AZ ZkY0 ),
if (q,d,$Z Zj,) es5(q,ai,$A).
(ii)(p,a an#, i,X ...Xm)
I-D(qA, a anit, i,X .. XiAXi+ Xm),
if, (qA, 0,1) E5(p, ai,Xj), where either Xj=$A and p=q, or
Xj•#$A (AEF) and p=qA;
(iii)(q, a ... snit, i,X1 ... ^Xi ...Xm) FD
if (q,0,$Ai...Ak0)E6(q,ai,Xj), where NEI and 0=s, or
•X $= F (FED and 0=$;
(q,a ...AN_ Xj+ ...Xm),
if (q,0)E6(q,ai,$^0).
</equation>
<bodyText confidence="0.999374">
h3* is the reflexive and transitive closure of fa. N(D)
or the language accepted by empty stack by D is defined
as follows
</bodyText>
<equation confidence="0.9959335">
N(D)={w; WET* &amp; (q,w#,L$AZO#)
hjo* (q,w#,1w1+1,$^#)
</equation>
<bodyText confidence="0.946492">
To illustrate, the DI-automaton DI3 accepting L3 by
empty stack is specified:
</bodyText>
<listItem confidence="0.7530642">
DI3 = (q (state for pda-mode), (Qr =) {q,9qm,qz,qs)
(states for stack reading mode), (/&apos;=) (a,b,[,]) (=input
alphabet), (G=)(S,M,Z,a,b, [,],) (=tape symbols for pda-
mode), (I=)(/g) (=tape symbols representing indices),
6, S, $, ¢,#)
</listItem>
<bodyText confidence="0.983718">
where for every XET:
</bodyText>
<equation confidence="0.85640936">
6(q,x,$S) = ((q,0,$aSfa),(q,0,$bSgb),(q,0,¢10,),
(for the G3-rules: S aSfa, S --&gt;bSgb, S —&gt; M)
5(q,x,SM) = ((q,0,$[M1),(q,0, SABI), (q,O,SZ),),
363
(for: M--&gt;[M], M--,.MM, M—&gt;Z)
8(q,x,$x) = ((q, 1,$))
(i.e.: if input symbol x = &amp;quot;predicted&amp;quot; terminal symbol
x, then shift input-tape one step (&amp;quot;P) and delete suc-
cessful prediction&amp;quot; (replace $x by $))
8(q,x,SZ) contains {(q z, 0,$)),
(i.e.: change into stack reading mode in order to find
indices belonging to the nontemllnal Z)
8(qz,x,SY) = 8(qz,x,19 contain ((qz, 0,1)) (for every x e
T, Y El)
(i.e.seek first index-symbol belonging to Z inside the
stack)
$fi = ((qz, 0,SSZa¢),(qz,0,SSa¢)},
8(qz,x,Sg) = {(qz,O,SSZb¢),(qz,0,$$b¢)},
8(qz,x,fi = {(q,x,SZa¢),(q,x,Sa¢)),
8(qz,x,g) =
(i.e. simulate the index-rules Zf--&gt;Za, Zf--&gt;a by
creation of embedded stacks)
=
(i.e. delete empty sub-stack)
8(q,x,Y) = {(q,0,-1)) (for x e T, Y e G-(f,g))
</equation>
<bodyText confidence="0.931045666666667">
(i.e. move to top of (sub-)stack).
The following theorem expresses the equivalence of DI-
grammars and DI-automata
</bodyText>
<listItem confidence="0.480265">
(11) DI-THEOREM: L is a DI-language (i.e. L
</listItem>
<bodyText confidence="0.927533095238095">
is generated by a DI-grammar) if and only if L
is accepted by a DI-automaton.
Proof sketch:
I. &amp;quot;only if&apos;:(to facilitate a comparison this part follows
closely Aho&apos;s corresponding proof for indexed gram-
mars and nsa&apos;s (theorem 5.1) in (Aho, 1969))
If L is a DI-language, then there exists a DI-grammar
G=(N,T,F,P,S) with L(G)=L. For every DI-grammar an
equivalent DI-grammar in a normal form can be con-
structed in which each rule has the form A—.BC, A--&gt;a,
A-4Bf or Af—&gt;B, with AEN; B,CE(N-{S}), aET, fEF;
and E E L(G), only if S--&gt;c is in P. (The proof is com-
pletely analogous to the corresponding one for indexed
grammars in (Aho, 1968) and is therefore omitted).
Thus, we can assume without loss of generality that G
is in normal form.
A DI-automaton D such that N(D)=L(G) is constructed
as follows:
Let D=(q,QFT,T,1,5,Z0,S,¢,#), with F=NyTki($, #),
Qr—{qA,Aer), I=F, Z0=S where g is constructed in
the following manner for all aET:
</bodyText>
<equation confidence="0.939743307692308">
1. (q,O,SBC)eg(q,a,SA), if A--&gt;BC E P,
(q,0,$b) n 15(q,a,SA), if A—&gt;b E P,
(q,O,SBfi E g(q,a,$,4), if A—*13f E P
(q,1,$) E g(q,a,$a)
(qA,0,$) E g(q,a,SA) for all A E r,
(qA,0,1) E 5(qA,a,B) for all A E r and all B Er,
i.(q, 0,SBO) E 8(&apos;qA,a,fi and
ii.(q,0,$$B¢) E g(&apos;qA,a,SP for all A a I- with
Af—&gt;B E P,
(q,0) a
(q,0,-1) E 8(q,a,B) for all B E r\J{Ø}
(q,0,$) a 8(q,#,SS) if and only if S—&gt; s is in P.
LEMMA 1.1
</equation>
<construct confidence="0.48406">
if
</construct>
<listItem confidence="0.677663">
(i) Afl•••fk =n=&gt; al...am
is a valid leftmost derivation in G with k_&gt;_0, and
</listItem>
<bodyText confidence="0.966698692307692">
AEN, then for Zp1...13kE(NL40})*,
(Nuf$,Op*,11E(NuFk-){0})*:
(ii) (q, a 1. ..am#,1,(XSAAZI31f1 ...13kfkg#)
hj,*(q, am#, m+1,(x$AZ131...13kA.
Proof by induction on n (i.e. the number of derivation
steps):
If n=1, then (i) is of the form A.--&gt;a where aET and
k=0, since only a rule of the form can be applied
because of the normal form of G and since in
DI-grammars (unlike in indexed grammars) unconsu-
med indices can not be swallowed up by terminals. Be-
cause of the construction of 8, (ii) is of the form
(q, a#, 1,a$AAZ13F NO) = (q,a#,1,a$AAZ11#)
</bodyText>
<equation confidence="0.891053">
I-D(q,a#,Loc$AaZtt#)
FD(q,a#,2,oc$AZu#)
=(q,a#,2,a$AZ3F..f3k0)
</equation>
<bodyText confidence="0.996175916666666">
Suppose Lemma 1.1 is true for all n&lt;ne with n&apos;&gt;1.
A leftmost derivation Afi...fk =&gt; a1.. .am can have
the following three forms according as A is expanded
in the first step:
=n1=&gt;ai... aiCfj÷1...fk
=n2=&gt;a ... aiai+F.. am
with ni&lt;n&apos; and n2&lt;n&apos;
2)Af ..fk--&gt;Bffi...fk=n1=-&gt;a I ...am with ni&lt;nl.
3)Af fk—&gt;13f2 fk=n1=&gt;ai... am
with ni&lt;n&apos; and (M1—&gt;B)EP.
From the inductive hypothesis and from 1.-8. above, it
follows
</bodyText>
<listItem confidence="0.7167806">
1&apos;)
(q,a1...am#,1,aVAZO1f1•••13jfjOj÷lfj+1•••13kfki14)
..am#,1,a,SABCZ13111 . • • 13j9j+lfj+1. • • Pkfklill)
.amii,i+1,a$ACZp 102. • . i3j Pj+lfj+1. • • PkfklA
1-D*(q,a1...amtm+1,04^Z01...0j0j+1••• PIO)
</listItem>
<bodyText confidence="0.499149">
an
</bodyText>
<page confidence="0.87676">
364
</page>
<equation confidence="0.7570775">
21)(q, am#, loot$AAZO1f1...13kfkillt)
(q,ai...am#,1,(x$ABfZ131f1...0kfkil#)
h-)* (q,a1...am#,m+1,a$AZI31...I300)
31)(q,a1...am#,1,a$AAZI31f1...flkfo#)
I-D
hp* obk,a1...am#,1,4ZI31^f1...13kfkg#)
(q,ai...am#,1,a$ZoisAB0132f2...okfko)
h3,* (q,ai...am#,m+1,a$Zpis^0132...oko)
fr (q,a1...am#,m+1,a$ZaAX132.../3kp.#)
(q,ai...am#,m+1,a$AZGX02..-Pk11#)
</equation>
<bodyText confidence="0.933972791666667">
where aX=1:11.
LEMMA 1.2
If for ZI31...13kE(Nu{0))*, aE(Nu{$,0})*, and
I.LE(NuFk..){0})*
Okfkji#)
ID* (q,a1...am#,m+1,cdAZI31...43kg#)
then for all
The proof (by induction on n) is similar to the proof of
Lemma 1.1 and is, therefore, omitted.
/L(w)
If L is accepted by a DI-automaton D=(q,QFT,T
5,1„Zo$,¢,#), then we can assume without loss of gen-
erality
a) that D writes at most two symbols on a stack in ei-
ther the push down mode or the stack creation mode (it
follows from the Di-automaton definition that the first
one of the two symbols cannot be a index symbol from
I),
b) that T and r are disjunct
A DI-grammar G with L(G)=N(D)=L can be con-
structed as follows:
Let G=(N,F,T,P,S) with N=T, F=I, S=Zo. P contains
for all aeT, A,B,CeN, and feF the productions
(da=a, if d=1, else des)
</bodyText>
<table confidence="0.99618175">
A---&gt;daBC, if (q,d,$BC)E
A—&gt;daBf, if (q,d,$B0
A—KlaB, if (q,d,$B)E .5(q,a,$A),
A—&gt;da, if (q,d,$)E
Af--)13C, if (q,0,$BC¢)e5(qA,a,P or
(q,O,S$BC¢) E 5(qA,a,$fi
Af—&gt;B, if (q,0,$B0)Eo(qA,a,fi or
(q,0,$$B¢) E 5(qA,a,$fi
</table>
<bodyText confidence="0.630609">
For all 01.-Pke(Nu{0}):, ocE(NQ4$,OD*4
AEN, I.LE(NuFk.){0}) , and ai...ameT
11.1 and 11.2 is true:
11.1: (qa1...am#,1,4^AP1f1... Pkfk11#)
1-pn (q,ai...am#,m+1,a$^131...130ift)
then in G the derivation is valid
Afl...fk=*=&gt;al —am.
11.2: If
is a leftmost derivation in G, then the following transi-
tion of D is valid
(q, al... am#,1, oc$AA(31f1 kfki.t#)
</bodyText>
<subsectionHeader confidence="0.391664">
J*
</subsectionHeader>
<bodyText confidence="0.999989">
The proofs by induction of 1.1 and 11.2 (unlike the
proofs of the corresponding lemmata for nsa&apos;s and in-
dexed grammars (s.Aho, (1969)) are as elementary as
the one given above for 1.1 and are omitted.
The DI-automaton concept can be used to show the
inclusion of the class of DI-languages in the class of
context-sensitive languages. The proof is strucurally
very similar to the one given by Aho (Aho, 1968) for
the inclusion of the indexed class in the context-sensi-
tive class: For every DI-automaton A, an equivalent DI-
automaton A&apos; can be constructed which accepts its in-
put w if and only if A accepts w and which in addition
uses a stack the length of which is bounded by a linear
function of the length of the input w. For A&apos; a linear
bounded automaton M (i.e the type of automaton char-
acteristic of the context-sensitive class) can be con-
structed which simulates A&apos;. For reasons of space the
extensive proof can not be given here.
</bodyText>
<sectionHeader confidence="0.9768965" genericHeader="method">
4 Some Remarks on the Complexity of
DI-Recognition
</sectionHeader>
<bodyText confidence="0.996373636363636">
The time complexity of the recognition problem for DI-
grammars will only be considered for a subclass of DI-
grammars. As the restriction on the form of the rules is
reminiscent of the Chomsky normal form for context-
free grammars (CFG), the grammars in the subclass
will be called DI-Chomsky normal form (DI-CNF)
grammars
A DI-grammar G=(N,T,F,P,S) is a DI-CNF grammar
if and only if each rule in P is of one of the following
forms where A,B,CEN-{S}, fEF, aET, S-÷E, if
L(G),
</bodyText>
<listItem confidence="0.5164745">
(a) A—&gt;BC, (b)A-BfC, (c) A—&gt;BCf,
(d) Af—.BC, (e)Af—&gt;a, (f) A—&gt;a
</listItem>
<bodyText confidence="0.99828">
The question whether the class of languages generated
by DI-CNF grammars is a proper or improper subclass
of the DI-languages will be left open.
In considering the recognition of DI-CNF grammars
an extension of the CKY algorithm for CFGs (Kasami,
1965; Younger, 1967) will be used which is essentially
</bodyText>
<page confidence="0.996843">
365
</page>
<bodyText confidence="0.999686625">
inspired by an idea of Vijay-Shanker and Weir in
(Vijay-Shanker and Weir, 1991).
Let the n(n+1)/2 cells of a CKY-table for an input
of length n be indexed by i and j (1i_&lt;j_n) in such a
manner that cell Zj j builds the top of a pyramid the ba-
se of which consists of the input
As in the case of CFGs a label E of a node of a deri-
vation tree (or a code of E) should be placed into cell
</bodyText>
<listItem confidence="0.7462">
•
</listItem>
<bodyText confidence="0.999047060606061">
Z1,3only if in G the derivation E=*=&gt;ai...ai is valid.
Since nonterminal nodes of DI-derivation trees are la-
beled by pairs (A,u) consisting of a nonterminal A and
an index stack It and since the number of such pairs
with (A,g) =*=&gt; w can grow exponentially with the
length of w, intractability can only be avoided if index
stacks can be encoded in such a way that substacks
shared by several nodes are represented only once.
Vijay-Shanker and Weir solved the problem for lin-
ear indexed grammars (LIGs) by storing for each node
K not its complete label Af1f2...1n, but the nonterminal
part A together with only the top fi of its index stack
and an indication of the cell where the label of a de-
scendant of K can be found with its top index f2 conti-
nuing the stack of its ancestor K. In the following this
idea will be adopted for DI-grammars, which, however,
require a supplementation.
Thus, if the cell Z11 of the CKY-table contains an
entry beginning with .&amp;quot;&lt;A,fi, (B,f2,q,p),..&gt;&amp;quot;, then we
know that
Ap=*=&gt;ai...ai with it=f1 EF*
is valid, and further that the top index symbol f2 on
gi(i.e. the continuation of f1) is in an entry of cell Zpq
beginning with the notenninal B. If, descending in
such a manner and guided by pointer quadruples like
an entry of the form &lt;C,fn,-,..&gt; is met,
then, in the case of a LIG-table, the bottom of stack
has been reached. So, entries of the form
are sufficient for LIGs.
But, of course, in the case of DI-derivations the bot-
tom of stack of a node, because of index distribution,
does not coincide with the bottom of stack of an arbitr-
ary index inheriting descendant, cf.
</bodyText>
<equation confidence="0.806678">
(13)
LIG-Percolation vs. DI-Percolation
Af1f2•••fn Af1f2. • •fjfj+1. • •fn
</equation>
<bodyText confidence="0.999939333333333">
Rather, the bottom of stack of a DI-node coincides with
the bottom of stack of its rightmost index inheriting de-
scendant. Therefore, the pointer mechanism for DI-en-
tries has to be more complicated. In particular, it must
be possible to add an &amp;quot;intersemital&amp;quot; pointer to a sister
path. However, since the continuation of the unary
stack (like of Cft in ( )) of a node without index inher-
iting descendants is necessarily unknown at the time its
entry is created in a bottom up manner, it must be pos-
sible to add an intersemital pointer to an entry later on.
That is why a DI-entry for a node K in a CKY-cell
requires an additional pointer to the entry for a descen-
dant C, which contains the end-of-stack symbol of K
and which eventually has to be supplemented by an in-
tersemital continuation pointer. E.g. the entry
</bodyText>
<equation confidence="0.891279">
(14) &lt;B1,f2,(D,f3,p,q),(C,f1,r,$)&gt; in Zi,j
</equation>
<bodyText confidence="0.95061468">
indicates that the next symbol f3 below f2 on the index
stack belonging to B1 can be found in cell Zwi in the
entry for the nonterminal D; the second quadruple
(C,ft,r,$) points to the descendant C of B &apos;carrying the
last index ft of Bland containing a place where a con-
tinuation pointer to a neighbouring path can be added
or has already been added.
To illustrate the extended CKY-algorithm, one of
the more complicated cases of specifying an entry for
the cell Zi,j is added below which dominates most of
the other cases in time complexity:
FOR i:=n TO 1 DO
FOR j:=i TO n DO
FOR k:=i TO j-1 DO
For each rule A-4 A1fA2:
if &lt;A1,f,(Bi,fi,p1,q1),(Ci,f3,si ti)&gt;EZi,k
for some Bi, C1EN,f1,f3EF?,
Pi , qi (i4115_qic), si,ti (ip15s15..t1Sk)
and &lt;A2,fc,-,-&gt; EZki_ii for some fcEF
then 1. if
&lt;B1,f1,,(B2,f2,p2,q2), X&gt; E Zp 1 Alfor
some B2,EN, f2EF, 1)2, q2
with i..q25q2k, and if q154)2, then
X=- , else X=(C,ft, u,v) for some
CEN, ftEF, u,v (piSuSvS qi)
</bodyText>
<figure confidence="0.818842692307692">
then
Bf2...f11 B2 Bf2f3...fi B2ft+1...fn Z1i:=Z1p{&lt;A,f1,(B2,f2,p2,q3),
Cfn (bottom of Df3...fi 012tic,k+ /JP}
I \ stack) /1 else
/ \ (stack continuation) if &lt;B1,f1,-,-&gt;
Zi J:=Zij1/4.){&lt;A,fi,(A2,fc,k+lj),
642kfc,k+1,0&gt;)
2.11
&lt;C1,f3,-,-&gt;eLs1,t1
366
then
Zsl,t1:-
Zsl,t1u{&lt;C1,f3,(A24,k+1:/),-&gt;}
</figure>
<bodyText confidence="0.999916473684211">
The pointer (A2,,fc,k+/,j) in the new entry of Z11 points
to the cell of the node where the end of stack of the
newly created node with noterminal A can be found.
The same pointer (A2,fc,,k+/,j) appears in cell Zsim
as &amp;quot;supplement&amp;quot; in order to indicate where the stack of
A is continued behind the end-of-stack of Al. Note that
supplemented quadruples of a cell Z1, are uniquely
identifiable by their form &lt;N,f1,(C,f2,r,$),-&gt;, i.e. the
empty fourth component, and by the relation
j5_rss. Supplemented quadruples cannot be used as en-
tries for daughters of &amp;quot;active&amp;quot; nodes, i.e. nodes the en-
tries of which are currently being constructed.
Let a1.. .a be the input. The number of entries of the
form &lt;B,f1,(D,f2,p,q),(C,f3,r,$)&gt; (f1,f2,f3eF, B,C, DE
N, li,p,q,r,s,$n) in each cell Zi,iwill then be bounded
by a polynomial of degree 4, i.e. 0(n4). For a fixed
value of ij,k, steps like the one above may require
0(n8) time (in some cases 0(112)). The three initial
loops increase the complexity by degree 3.
</bodyText>
<sectionHeader confidence="0.998747" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995695166666667">
[Aho, 1968] A. V. Aho. Indexed Grammars,
J.Ass.ComputMach. 15, 647-671, 1968.
[Aho, 1969] A. V. Aho. Nested Stack Automata,
JAss.Comp.Mach. 16, 383-, 1969.
[Gazdar, 19881 G. Gazdar. Applicability of Indexed
Grammars to Natural Languages,in: U.Reyle and
C.Rohrer (eds.)Natural Language Parsing and Lin-
guistic Theories, 69-94, 1988.
[Joshi, Vijay-Shanker, and Weir, 1989] A. K. Joshi, K.
Vijay-Shanker, and D. J. Weir. The convergence of
mildly context-sensitive grammar formalisms. In T.
Wasow and P. Sells (Eds.), The processing of lin-
guistic structure. MIT Press, 1989.
[Kasami, 1965] T. ICasami. An efficient recognition
and syntax algorithm for context-free lan-
guages.(Tech. Rep. No. AF-CRL-65-758). Bedford,
MA: Air Force Cambridge Research Laboratory,
1965.
[Pereira, 1981] F. Pereira. Extraposition Grammars, in:
American Journal of ComputationalLinguistics,7,
243-256, 1981.
[Pereira, 1983] F. Pereira. Logic for Natural Language
Analysis, SRI International, Technical Note 275,
1983.
[Rizzi, 19821 L. Rizzi. Issues in Italian Syntax,
Dordrecht, 1982.
[Stabler, 1987] E. P Stabler. Restricting Logic Gram-
mars with Government-Binding Theory, Computa-
tional Linguistics, 13, 1-10, 1987.
[Takeshi, 1973] Hayashi Takeshi. On Derivation Trees
of Indexed Grammars, PubLR1MS, Kyoto Univ., 9,
61-92, 1973.
[Vijay-Shanker, Weir, and Joshi, 1986] K. Vijay-
Shanker, D. J. Weir, and A. K. Joshi. Tree adjoining
and head wrapping. 1 1 th International Conference
on Comput. Ling. 1986.
[Vijay-Shanker, Weir, and Joshi, 1987] K. Vijay-
Shanker, D. J. Weir, A. K. Joshi. Characterizing
structural descriptions produced by various gram-
matical formalisms. 25th Meeting Assoc.Comput.
Ling., 104-111. 1987.
[Vijay-Shanker and Weir, 1991] K. Vijay-Shanker and
David J. Weir. Polynomial Parsing of Extensions of
Context-Free Grammars. In: Tomita, M.(ed.) Cur-
rent Issues in Parsing Technology, 191-206, London
1991.
[Weir and Joshi, 1988] David J. Weir and Aravind K.
Joshi. Combinatory Categorial Grammars: Genera-
tive power and relationship to linear context-free re-
writing systems. 26th Meeting Assoc.Comput. Ling.,
278-285, 1988.
[Younger, 1967] D. H. Younger. Recognition and
parsing context-free languages in time n3. In! Con-
trol, 10, 189-208.
</reference>
<page confidence="0.998333">
367
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.795565">
<title confidence="0.995245">NEW FRONTIERS BEYOND CONTEXT-FREENESS: DI-GRAMMARS AND DI-AUTOMATA.</title>
<author confidence="0.998388">Peter Staudacher</author>
<affiliation confidence="0.965821333333333">Institut fur Allgemeine und Indogermanische Sprachwissenschaft Universitat Regensburg</affiliation>
<address confidence="0.976751666666667">Postfach 397 8400 Regensburg 1 Germany</address>
<abstract confidence="0.99711352631579">A new class of formal languages will be defined the Distributed Index Languages (DI-languages). The grammar-formalism generating the new class the DI-grammars cover unbound dependencies in a rather natural way. The place of DI-languages in the Chomsky-hierarchy will be determined: Like Aho&apos;s indexed Languages, DI-languages represent a proper subclass of Type 1 (contextsensitive languages) and properly include Type 2 (context-free languages), but the DI-class is neither a subclass nor a superclass of Aho&apos;s indexed class. It will be shown that, apart from DI-grammars, DI-languages can equivalently be characterized by a special type of automata - DI-automata. Finally, the time complexity of the recognition-problem for an interesting subclass of DI-Grammars will approximately be determined.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A V Aho</author>
</authors>
<date>1968</date>
<journal>Indexed Grammars, J.Ass.ComputMach.</journal>
<volume>15</volume>
<pages>647--671</pages>
<marker>[Aho, 1968]</marker>
<rawString>A. V. Aho. Indexed Grammars, J.Ass.ComputMach. 15, 647-671, 1968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A V Aho</author>
</authors>
<title>Nested Stack Automata,</title>
<date>1969</date>
<journal>JAss.Comp.Mach.</journal>
<booktitle>Gazdar, 19881 G. Gazdar. Applicability of Indexed Grammars to Natural Languages,in: U.Reyle and C.Rohrer (eds.)Natural Language Parsing and Linguistic Theories,</booktitle>
<volume>16</volume>
<pages>69--94</pages>
<marker>[Aho, 1969]</marker>
<rawString>A. V. Aho. Nested Stack Automata, JAss.Comp.Mach. 16, 383-, 1969. [Gazdar, 19881 G. Gazdar. Applicability of Indexed Grammars to Natural Languages,in: U.Reyle and C.Rohrer (eds.)Natural Language Parsing and Linguistic Theories, 69-94, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>K Vijay-Shanker</author>
<author>D J Weir</author>
</authors>
<title>The convergence of mildly context-sensitive grammar formalisms. In</title>
<date>1989</date>
<publisher>MIT Press,</publisher>
<marker>[Joshi, Vijay-Shanker, and Weir, 1989]</marker>
<rawString>A. K. Joshi, K. Vijay-Shanker, and D. J. Weir. The convergence of mildly context-sensitive grammar formalisms. In T. Wasow and P. Sells (Eds.), The processing of linguistic structure. MIT Press, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T ICasami</author>
</authors>
<title>An efficient recognition and syntax algorithm for context-free languages.(Tech.</title>
<date>1965</date>
<tech>Rep. No. AF-CRL-65-758).</tech>
<institution>Air Force Cambridge Research Laboratory,</institution>
<location>Bedford, MA:</location>
<marker>[Kasami, 1965]</marker>
<rawString>T. ICasami. An efficient recognition and syntax algorithm for context-free languages.(Tech. Rep. No. AF-CRL-65-758). Bedford, MA: Air Force Cambridge Research Laboratory, 1965.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
</authors>
<title>Extraposition Grammars, in:</title>
<date>1981</date>
<journal>American Journal of ComputationalLinguistics,7,</journal>
<pages>243--256</pages>
<marker>[Pereira, 1981]</marker>
<rawString>F. Pereira. Extraposition Grammars, in: American Journal of ComputationalLinguistics,7, 243-256, 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
</authors>
<title>Logic for Natural Language Analysis,</title>
<date>1983</date>
<journal>SRI International, Technical Note</journal>
<volume>275</volume>
<location>Dordrecht,</location>
<marker>[Pereira, 1983]</marker>
<rawString>F. Pereira. Logic for Natural Language Analysis, SRI International, Technical Note 275, 1983. [Rizzi, 19821 L. Rizzi. Issues in Italian Syntax, Dordrecht, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E P Stabler</author>
</authors>
<title>Restricting Logic Grammars with Government-Binding Theory,</title>
<date>1987</date>
<journal>Computational Linguistics,</journal>
<volume>13</volume>
<pages>1--10</pages>
<marker>[Stabler, 1987]</marker>
<rawString>E. P Stabler. Restricting Logic Grammars with Government-Binding Theory, Computational Linguistics, 13, 1-10, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hayashi Takeshi</author>
</authors>
<title>On Derivation Trees of Indexed Grammars,</title>
<date>1973</date>
<journal>PubLR1MS, Kyoto Univ.,</journal>
<volume>9</volume>
<pages>61--92</pages>
<marker>[Takeshi, 1973]</marker>
<rawString>Hayashi Takeshi. On Derivation Trees of Indexed Grammars, PubLR1MS, Kyoto Univ., 9, 61-92, 1973.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K VijayShanker</author>
<author>D J Weir</author>
<author>A K Joshi</author>
</authors>
<title>Tree adjoining and head wrapping.</title>
<date>1986</date>
<booktitle>th International Conference on Comput. Ling.</booktitle>
<volume>1</volume>
<marker>[Vijay-Shanker, Weir, and Joshi, 1986]</marker>
<rawString>K. VijayShanker, D. J. Weir, and A. K. Joshi. Tree adjoining and head wrapping. 1 1 th International Conference on Comput. Ling. 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K VijayShanker</author>
<author>D J Weir</author>
<author>A K Joshi</author>
</authors>
<title>Characterizing structural descriptions produced by various grammatical formalisms. 25th Meeting Assoc.Comput.</title>
<date>1987</date>
<pages>104--111</pages>
<publisher>Ling.,</publisher>
<marker>[Vijay-Shanker, Weir, and Joshi, 1987]</marker>
<rawString>K. VijayShanker, D. J. Weir, A. K. Joshi. Characterizing structural descriptions produced by various grammatical formalisms. 25th Meeting Assoc.Comput. Ling., 104-111. 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>David J Weir</author>
</authors>
<title>Polynomial Parsing of Extensions of Context-Free Grammars.</title>
<date>1991</date>
<booktitle>M.(ed.) Current Issues in Parsing Technology,</booktitle>
<pages>191--206</pages>
<editor>In: Tomita,</editor>
<location>London</location>
<marker>[Vijay-Shanker and Weir, 1991]</marker>
<rawString>K. Vijay-Shanker and David J. Weir. Polynomial Parsing of Extensions of Context-Free Grammars. In: Tomita, M.(ed.) Current Issues in Parsing Technology, 191-206, London 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David J Weir</author>
<author>Aravind K Joshi</author>
</authors>
<title>Combinatory Categorial Grammars: Generative power and relationship to linear context-free rewriting systems. 26th Meeting Assoc.Comput.</title>
<date>1988</date>
<journal>Ling.,</journal>
<pages>278--285</pages>
<marker>[Weir and Joshi, 1988]</marker>
<rawString>David J. Weir and Aravind K. Joshi. Combinatory Categorial Grammars: Generative power and relationship to linear context-free rewriting systems. 26th Meeting Assoc.Comput. Ling., 278-285, 1988.</rawString>
</citation>
<citation valid="false">
<authors>
<author>D H Younger</author>
</authors>
<title>Recognition and parsing context-free languages in time n3.</title>
<journal>In! Control,</journal>
<volume>10</volume>
<pages>189--208</pages>
<marker>[Younger, 1967]</marker>
<rawString>D. H. Younger. Recognition and parsing context-free languages in time n3. In! Control, 10, 189-208.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>