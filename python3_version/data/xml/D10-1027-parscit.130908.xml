<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.995442">
Efficient Incremental Decoding for Tree-to-String Translation
</title>
<author confidence="0.997324">
Liang Huang 1
</author>
<affiliation confidence="0.7708485">
1Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292, USA
</affiliation>
<email confidence="0.998037">
{lhuang,haitaomi}@isi.edu
</email>
<author confidence="0.568741">
Haitao Mi 2,1
</author>
<affiliation confidence="0.815446">
2Key Lab. of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
</affiliation>
<address confidence="0.741384">
P.O. Box 2704, Beijing 100190, China
</address>
<email confidence="0.997329">
htmi@ict.ac.cn
</email>
<sectionHeader confidence="0.993869" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998692944444445">
Syntax-based translation models should in
principle be efficient with polynomially-sized
search space, but in practice they are often
embarassingly slow, partly due to the cost
of language model integration. In this paper
we borrow from phrase-based decoding the
idea to generate a translation incrementally
left-to-right, and show that for tree-to-string
models, with a clever encoding of deriva-
tion history, this method runs in average-
case polynomial-time in theory, and linear-
time with beam search in practice (whereas
phrase-based decoding is exponential-time in
theory and quadratic-time in practice). Exper-
iments show that, with comparable translation
quality, our tree-to-string system (in Python)
can run more than 30 times faster than the
phrase-based system Moses (in C++).
</bodyText>
<sectionHeader confidence="0.998975" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994745055555556">
Most efforts in statistical machine translation so far
are variants of either phrase-based or syntax-based
models. From a theoretical point of view, phrase-
based models are neither expressive nor efficient:
they typically allow arbitrary permutations and re-
sort to language models to decide the best order. In
theory, this process can be reduced to the Traveling
Salesman Problem and thus requires an exponential-
time algorithm (Knight, 1999). In practice, the de-
coder has to employ beam search to make it tractable
(Koehn, 2004). However, even beam search runs in
quadratic-time in general (see Sec. 2), unless a small
distortion limit (say, d=5) further restricts the possi-
ble set of reorderings to those local ones by ruling
out any long-distance reorderings that have a “jump”
in theory in practice
phrase-based
tree-to-string
</bodyText>
<tableCaption confidence="0.762272">
Table 1: [main result] Time complexity of our incremen-
tal tree-to-string decoding compared with phrase-based.
In practice means “approximate search with beams.”
</tableCaption>
<bodyText confidence="0.963963275862069">
longer than d. This has been the standard prac-
tice with phrase-based models (Koehn et al., 2007),
which fails to capture important long-distance re-
orderings like SVO-to-SOV.
Syntax-based models, on the other hand, use
syntactic information to restrict reorderings to
a computationally-tractable and linguistically-
motivated subset, for example those generated by
synchronous context-free grammars (Wu, 1997;
Chiang, 2007). In theory the advantage seems quite
obvious: we can now express global reorderings
(like SVO-to-VSO) in polynomial-time (as opposed
to exponential in phrase-based). But unfortunately,
this polynomial complexity is super-linear (being
generally cubic-time or worse), which is slow in
practice. Furthermore, language model integration
becomes more expensive here since the decoder now
has to maintain target-language boundary words at
both ends of a subtranslation (Huang and Chiang,
2007), whereas a phrase-based decoder only needs
to do this at one end since the translation is always
growing left-to-right. As a result, syntax-based
models are often embarassingly slower than their
phrase-based counterparts, preventing them from
becoming widely useful.
Can we combine the merits of both approaches?
While other authors have explored the possibilities
exponential quadratic
polynomial linear
</bodyText>
<page confidence="0.982504">
273
</page>
<note confidence="0.817563">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 273–283,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.910331925925926">
of enhancing phrase-based decoding with syntax-
aware reordering (Galley and Manning, 2008), we
are more interested in the other direction, i.e., can
syntax-based models learn from phrase-based de-
coding, so that they still model global reordering, but
in an efficient (preferably linear-time) fashion?
Watanabe et al. (2006) is an early attempt in
this direction: they design a phrase-based-style de-
coder for the hierarchical phrase-based model (Chi-
ang, 2007). However, this algorithm even with the
beam search still runs in quadratic-time in prac-
tice. Furthermore, their approach requires grammar
transformation that converts the original grammar
into an equivalent binary-branching Greibach Nor-
mal Form, which is not always feasible in practice.
We take a fresh look on this problem and turn our
focus to one particular syntax-based paradigm, tree-
to-string translation (Liu et al., 2006; Huang et al.,
2006), since this is the simplest and fastest among
syntax-based approaches. We develop an incremen-
tal dynamic programming algorithm and make the
following contributions:
• we show that, unlike previous work, our in-
cremental decoding algorithm runs in average-
case polynomial-time in theory for tree-to-
string models, and the beam search version runs
in linear-time in practice (see Table 1);
</bodyText>
<listItem confidence="0.989004333333333">
• large-scale experiments on a tree-to-string sys-
tem confirm that, with comparable translation
quality, our incremental decoder (in Python)
can run more than 30 times faster than the
phrase-based system Moses (in C++) (Koehn
et al., 2007);
• furthermore, on the same tree-to-string system,
incremental decoding is slightly faster than the
standard cube pruning method at the same level
of translation quality;
• this is also the first linear-time incremental de-
coder that performs global reordering.
</listItem>
<bodyText confidence="0.990186666666667">
We will first briefly review phrase-based decod-
ing in this section, which inspires our incremental
algorithm in the next section.
</bodyText>
<sectionHeader confidence="0.852807" genericHeader="method">
2 Background: Phrase-based Decoding
</sectionHeader>
<bodyText confidence="0.999679">
We will use the following running example from
Chinese to English to explain both phrase-based and
syntax-based decoding throughout this paper:
</bodyText>
<equation confidence="0.2612395">
0 B`ushi1 yˇu 2 Sh¯al´ong 3 jˇuxing 4 le 5 huit´an 6
Bush with Sharon hold -ed meeting
</equation>
<bodyText confidence="0.936204">
‘Bush held talks with Sharon’
</bodyText>
<subsectionHeader confidence="0.99686">
2.1 Basic Dynamic Programming Algorithm
</subsectionHeader>
<bodyText confidence="0.998785545454545">
Phrase-based decoders generate partial target-
language outputs in left-to-right order in the form
of hypotheses (Koehn, 2004). Each hypothesis has
a coverage vector capturing the source-language
words translated so far, and can be extended into a
longer hypothesis by a phrase-pair translating an un-
covered segment. This process can be formalized as
a deductive system. For example, the following de-
duction step grows a hypothesis by the phrase-pair
hyˇu Sh¯al´ong, with Sharoni covering Chinese span
[1-3]:
</bodyText>
<equation confidence="0.957326">
(• •••6) : (w, “Bush held talks”)
(•••3•••) : (w′, “Bush held talks with Sharon”) (1)
</equation>
<bodyText confidence="0.998745818181818">
where a • in the coverage vector indicates the source
word at this position is “covered” and where w and
w′ = w+c+d are the weights of the two hypotheses,
respectively, with c being the cost of the phrase-pair,
and d being the distortion cost. To compute d we
also need to maintain the ending position of the last
phrase (the 3 and 6 in the coverage vector).
To add a bigram model, we split each −LM item
above into a series of +LM items; each +LM item
has the form (v,a ) where a is the last word of the
hypothesis. Thus a +LM version of (1) might be:
</bodyText>
<equation confidence="0.75695">
(• •••6,talks ) : (w, “Bush held talks”)
</equation>
<bodyText confidence="0.90932">
(•••3•••,Sharon ) : (w′, “Bush held talks with Sharon”)
where the score of the resulting +LM item
</bodyText>
<equation confidence="0.974323">
w′ = w + c + d − log Plm(with  |talk)
</equation>
<bodyText confidence="0.996359166666667">
now includes a combination cost due to the bigrams
formed when applying the phrase-pair. The com-
plexity of this dynamic programming algorithm for
g-gram decoding is O(2nn2|V |g−1) where n is the
sentence length and |V  |is the English vocabulary
size (Huang and Chiang, 2007).
</bodyText>
<page confidence="0.993485">
274
</page>
<figure confidence="0.955536">
1 2 3 4 5
</figure>
<figureCaption confidence="0.993733">
Figure 1: Beam search in phrase-based decoding expands
the hypotheses in the current bin (#2) into longer ones.
</figureCaption>
<figure confidence="0.718831">
VP
AS
VV
le
jˇux´ıng
(a) B`ushi[yˇu Sh¯al´ong ]1 [jˇux´ıng le huit´an ]2
⇓ 1-best parser
(b) IP@E
NP@1
B`ushi
PP@2.1
VP@2.2
VP@2
r1 ⇓
P NP@2.1.2
yˇu Sh¯al´ong
NP@2.2.3
huit´an
PP VP
P x1:NP VV AS x2:NP
</figure>
<figureCaption confidence="0.992269">
Figure 2: Tree-to-string rule r3 for reordering.
</figureCaption>
<subsectionHeader confidence="0.995442">
2.2 Beam Search in Practice
</subsectionHeader>
<bodyText confidence="0.999929642857143">
To make the exponential algorithm practical, beam
search is the standard approximate search method
(Koehn, 2004). Here we group +LM items into n
bins, with each bin BZ hosting at most b items that
cover exactly i Chinese words (see Figure 1). The
complexity becomes O(n2b) because there are a to-
tal of O(nb) items in all bins, and to expand each
item we need to scan the whole coverage vector,
which costs O(n). This quadratic complexity is still
too slow in practice and we often set a small distor-
tion limit of dmax (say, 5) so that no jumps longer
than dmax are allowed. This method reduces the
complexity to O(nbdmax) but fails to capture long-
distance reorderings (Galley and Manning, 2008).
</bodyText>
<sectionHeader confidence="0.975937" genericHeader="method">
3 Incremental Decoding for Tree-to-String
Translation
</sectionHeader>
<bodyText confidence="0.999839333333333">
We will first briefly review tree-to-string translation
paradigm and then develop an incremental decoding
algorithm for it inspired by phrase-based decoding.
</bodyText>
<subsectionHeader confidence="0.998079">
3.1 Tree-to-string Translation
</subsectionHeader>
<bodyText confidence="0.999957">
A typical tree-to-string system (Liu et al., 2006;
Huang et al., 2006) performs translation in two
steps: parsing and decoding. A parser first parses the
source language input into a 1-best tree T, and the
decoder then searches for the best derivation (a se-
</bodyText>
<figure confidence="0.6850495">
r4 ⇓ r5 ⇓
(e) Bush [held talks]2 [with Sharon]1
</figure>
<figureCaption confidence="0.999052666666667">
Figure 3: An example derivation of tree-to-string trans-
lation (much simplified from Mi et al. (2008)). Shaded
regions denote parts of the tree that matches the rule.
</figureCaption>
<bodyText confidence="0.997889285714286">
quence of translation steps) d∗ that converts source
tree T into a target-language string.
Figure 3 shows how this process works. The Chi-
nese sentence (a) is first parsed into tree (b), which
will be converted into an English string in 5 steps.
First, at the root node, we apply rule r1 preserving
the top-level word-order
</bodyText>
<equation confidence="0.591453">
(r1) IP (x1:NP x2:VP) → x1 x2
</equation>
<bodyText confidence="0.7839134">
which results in two unfinished subtrees, NP@1 and
VP@2 in (c). Here X@η denotes a tree node of la-
bel X at tree address q (Shieber et al., 1995). (The
root node has address E, and the first child of node q
has address q.1, etc.) Then rule r2 grabs the B`ushisubtree and transliterate it into the English word
</bodyText>
<figure confidence="0.91419265625">
NP@1
B`ushi
with
NP@2.1.2
Sh¯al´ong
NP@2.2.3
huit´an
(d) Bush held
PP@2.1
VP@2.2
VP@2
r2 ⇓ r3 ⇓
VV
jˇux´ıng
NP@2.2.3
huit´an
AS
le
P
yˇu
NP@2.1.2
Sh¯al´ong
→ held x2 with x1
ˇ
le
yu
jˇuxing
275
in theory in practice
phrase* O(2nn2 · |V |9−1) O(n2b)
tree-to-str O(nc · |V |4(9−1)) O(ncb2)
this work* O(nk log2(_) · |V |9−1) O(ncb)
</figure>
<figureCaption confidence="0.8157666">
Table 2: Summary of time complexities of various algo-
rithms. b is the beam width, V is the English vocabulary,
and c is the number of translation rules per node. As a
special case, phrase-based decoding with distortion limit
dm,,. is O(nbdm,,.). *: incremental decoding algorithms.
</figureCaption>
<bodyText confidence="0.983780454545455">
“Bush”. Similarly, rule r3 shown in Figure 2 is ap-
plied to the VP subtree, which swaps the two NPs,
yielding the situation in (d). Finally two phrasal
rules r4 and r5 translate the two remaining NPs and
finish the translation.
In this framework, decoding without language
model (−LM decoding) is simply a linear-time
depth-first search with memoization (Huang et al.,
2006), since a tree of n words is also of size
O(n) and we visit every node only once. Adding
a language model, however, slows it down signifi-
cantly because we now have to keep track of target-
language boundary words, but unlike the phrase-
based case in Section 2, here we have to remember
both sides the leftmost and the rightmost boundary
words: each node is now split into +LM items like
(q a * b) where q is a tree node, and a and b are left
and right English boundary words. For example, a
bigram +LM item for node VP@2 might be
(VP@2 held * Sharon).
This is also the case with other syntax-based models
like Hiero or GHKM: language model integration
overhead is the most significant factor that causes
syntax-based decoding to be slow (Chiang, 2007). In
theory +LM decoding is O(nc|V |4(9−1)), where V
denotes English vocabulary (Huang, 2007). In prac-
tice we have to resort to beam search again: at each
node we would only allow top-b +LM items. With
beam search, tree-to-string decoding with an inte-
grated language model runs in time O(ncb2), where
b is the size of the beam at each node, and c is (max-
imum) number of translation rules matched at each
node (Huang, 2007). See Table 2 for a summary.
</bodyText>
<subsectionHeader confidence="0.998213">
3.2 Incremental Decoding
</subsectionHeader>
<bodyText confidence="0.999991333333333">
Can we borrow the idea of phrase-based decoding,
so that we also grow the hypothesis strictly left-
to-right, and only need to maintain the rightmost
boundary words?
The key intuition is to adapt the coverage-vector
idea from phrase-based decoding to tree-to-string
decoding. Basically, a coverage-vector keeps track
of which Chinese spans have already been translated
and which have not. Similarly, here we might need
a “tree coverage-vector” that indicates which sub-
trees have already been translated and which have
not. But unlike in phrase-based decoding, we can
not simply choose any arbitrary uncovered subtree
for the next step, since rules already dictate which
subtree to visit next. In other words what we need
here is not really a tree coverage vector, but more of
a derivation history.
We develop this intuition into an agenda repre-
sented as a stack. Since tree-to-string decoding is a
top-down depth-first search, we can simulate this re-
cursion with a stack of active rules, i.e., rules that are
not completed yet. For example we can simulate the
derivation in Figure 3 as follows. At the root node
IP@E, we choose rule r1, and push its English-side
to the stack, with variables replaced by matched tree
nodes, here x1 for NP@1 and x2 for VP@2. So we
have the following stack
</bodyText>
<equation confidence="0.909488">
s = [. NP@1 VP@2],
</equation>
<bodyText confidence="0.941897714285714">
where the dot. indicates the next symbol to process
in the English word-order. Since node NP@1 is the
first in the English word-order, we expand it first,
and push rule r2 rooted at NP to the stack:
[. NP@1 VP@2 ] [. Bush].
Since the symbol right after the dot in the top rule is
a word, we immediately grab it, and append it to the
current hypothesis, which results in the new stack
[. NP@1 VP@2 ] [Bush. ].
Now the top rule on the stack has finished (dot is at
the end), so we trigger a “pop” operation which pops
the top rule and advances the dot in the second-to-
top rule, denoting that NP@1 is now completed:
[NP@1 . VP@2].
</bodyText>
<page confidence="0.987581">
276
</page>
<table confidence="0.947159055555556">
stack hypothesis
[&lt;s&gt; . IP@ǫ &lt;/s&gt;] &lt;s&gt;
p [&lt;s&gt; . IP@ǫ &lt;/s&gt;] [. NP@1 VP@2] &lt;s&gt;
p [&lt;s&gt; . IP@ǫ &lt;/s&gt;] [. NP@1 VP@2] [. Bush] &lt;s&gt;
s [&lt;s&gt; . IP@ǫ &lt;/s&gt;] [. NP@1 VP@2] [Bush. ] &lt;s&gt; Bush
c [&lt;s&gt; . IP@ǫ &lt;/s&gt;] [NP@1 . VP@2] &lt;s&gt; Bush
p [&lt;s&gt; . IP@ǫ &lt;/s&gt;] L[&apos;N�P@1 . VP@2] [. held NP@2.2.3 with NP@2.1.2] &lt;s&gt; Bush
s [&lt;s&gt; . IP@ǫ &lt;/s&gt;] [NP@1 . VP@2] [held . NP@2.2.3 with NP@2.1.2] &lt;s&gt; Bush held
p [&lt;s&gt; . IP@ǫ &lt;/s&gt;] L[&apos;N�P@1 . VP@2] [held . NP@2.2.3 with NP@2.1.2] [. talks] &lt;s&gt; Bush held
s [&lt;s&gt; . IP@ǫ &lt;/s&gt;] [NP@1 . VP@2] [held . NP@2.2.3 with NP@2.1.2] [talks . ] &lt;s&gt; Bush held talks
c [&lt;s&gt; . IP@ǫ &lt;/s&gt;] L[&apos;N�P@1 . VP@2] [held NP@2.2.3 . with NP@2.1.2] &lt;s&gt; Bush held talks
s [&lt;s&gt; . IP@ǫ &lt;/s&gt;] [NP@1 . VP@2] [held NP@2.2.3 with . NP@2.1.2] &lt;s&gt; Bush held talks with
p [&lt;s&gt; . IP@ǫ &lt;/s&gt;] L[&apos;N�P@1 . VP@2] [held NP@2.2.3 with . NP@2.1.2] [. Sharon] &lt;s&gt; Bush held talks with
s [&lt;s&gt; . IP@ǫ &lt;/s&gt;] [NP@1 . VP@2] [held NP@2.2.3 with . NP@2.1.2] [Sharon. ] &lt;s&gt; Bush held talks with Sharon
c [&lt;s&gt; . IP@ǫ &lt;/s&gt;] L[&apos;N�P@1 . VP@2] [held NP@2.2.3 with NP@2.1.2. ] &lt;s&gt; Bush held talks with Sharon
c [&lt;s&gt; . IP@ǫ &lt;/s&gt;] [NP@1 VP@2. ] &lt;s&gt; Bush held talks with Sharon
c [&lt;s&gt; IP@ǫ . &lt;/s&gt;] &lt;s&gt; Bush held talks with Sharon
s [&lt;s&gt; IP@ǫ &lt;/s&gt;. ] &lt;s&gt; Bush held talks with Sharon &lt;/s&gt;
</table>
<figureCaption confidence="0.96508">
Figure 4: Simulation of tree-to-string derivation in Figure 3 in the incremental decoding algorithm. Actions: p, predict;
s, scan; c, complete (see Figure 5).
</figureCaption>
<figure confidence="0.537692">
Item ℓ : (s, ρ) : w; ℓ: step, s: stack, ρ: hypothesis, w: weight
Equivalence ℓ : (s, ρ) — ℓ : (s′, ρ′) iff. s = s′ and lastg−1(ρ) = lastg−1(ρ′)
Axiom 0 : ([&lt;s&gt;g−1 . ǫ &lt;/s&gt;], &lt;s&gt;g−1) : 0
</figure>
<table confidence="0.902687428571428">
Predict ℓ : (... [α . η β], ρ) : w match(η, C(r))
ℓ + |C(r)  |: (... [α . η β] [. f (η, E(r))] , ρ) : w + c(r)
Scan ℓ : (... [α . e β], ρ) : w
ℓ : (... [α e . β], ρe) : w — log Pr(e  |lastg−1(ρ))
Complete ℓ : (... [α . η β] [γ.], ρ) : w
ℓ : (... [α η . β], ρ) : w
Goal |T |: ([&lt;s&gt;g−1 ǫ &lt;/s&gt;.], ρ&lt;/s&gt;) : w
</table>
<figureCaption confidence="0.88276625">
Figure 5: Deductive system for the incremental tree-to-string decoding algorithm. Function lastg−1(·) returns the
rightmost g — 1 words (for g-gram LM), and match(η, C(r)) tests matching of rule r against the subtree rooted at
node η. C(r) and E(r) are the Chinese and English sides of rule r, and function f(η, E(r)) = [xi �--&gt; η.var(i)]E(r)
replaces each variable xi on the English side of the rule with the descendant node η.var(i) under η that matches xi.
</figureCaption>
<page confidence="0.977542">
277
</page>
<bodyText confidence="0.99846675">
The next step is to expand VP@2, and we use rule r3
and push its English-side “VP —* held x2 with x1”
onto the stack, again with variables replaced by
matched nodes:
</bodyText>
<equation confidence="0.690418">
[NP@1 . VP@2] [. held NP@2.2.3 with NP@2.1.2]
</equation>
<bodyText confidence="0.999940529411765">
Note that this is a reordering rule, and the stack al-
ways follows the English word order because we
generate hypothesis incrementally left-to-right. Fig-
ure 4 works out the full example.
We formalize this algorithm in Figure 5. Each
item (s, ρ) consists of a stack s and a hypothesis
ρ. Similar to phrase-based dynamic programming,
only the last g−1 words of ρ are part of the signature
for decoding with g-gram LM. Each stack is a list of
dotted rules, i.e., rules with dot positions indicting
progress, in the style of Earley (1970). We call the
last (rightmost) rule on the stack the top rule, which
is the rule being processed currently. The symbol af-
ter the dot in the top rule is called the next symbol,
since it is the symbol to expand or process next. De-
pending on the next symbol a, we can perform one
of the three actions:
</bodyText>
<listItem confidence="0.9882614">
• if a is a node η, we perform a Predict action
which expands η using a rule r that can pattern-
match the subtree rooted at η; we push r is to
the stack, with the dot at the beginning;
• if a is an English word, we perform a Scan ac-
tion which immediately adds it to the current
hypothesis, advancing the dot by one position;
• if the dot is at the end of the top rule, we
perform a Complete action which simply pops
stack and advance the dot in the new top rule.
</listItem>
<subsectionHeader confidence="0.988408">
3.3 Polynomial Time Complexity
</subsectionHeader>
<bodyText confidence="0.9289835">
Unlike phrase-based models, we show here
that incremental decoding runs in average-case
polynomial-time for tree-to-string systems.
Lemma 1. For an input sentence of n words and
its parse tree of depth d, the worst-case complex-
ity of our algorithm is f(n, d) = c(cr)d|V |g−1 =
O((cr)dng−1), assuming relevant English vocabu-
lary |V  |= O(n), and where constants c, r and g are
the maximum number of rules matching each tree
node, the maximum arity ofa rule, and the language-
model order, respectively.
Proof. The time complexity depends (in part) on the
number of all possible stacks for a tree of depth d. A
stack is a list of rules covering a path from the root
node to one of the leaf nodes in the following form:
where η1 = ǫ is the root node and ηs is a leaf node,
with stack depth s &lt; d. Each rule Ri(i &gt; 1) ex-
pands node ηi−1, and thus has c choices by the defi-
nition of grammar constant c. Furthermore, each rule
in the stack is actually a dotted-rule, i.e., it is associ-
ated with a dot position ranging from 0 to r, where r
is the arity of the rule (length of English side of the
rule). So the total number of stacks is O((cr)d).
Besides the stack, each state also maintains (g−1)
rightmost words of the hypothesis as the language
model signature, which amounts to O(|V |g−1). So
the total number of states is O((cr)d|V |g−1). Fol-
lowing previous work (Chiang, 2007), we assume
a constant number of English translations for each
foreign word in the input sentence, so |V  |= O(n).
And as mentioned above, for each state, there are c
possible expansions, so the overall time complexity
is f(n,d) = c(cr)d|V |g−1 = O((cr)dng−1).
We do average-case analysis below because the
tree depth (height) for a sentence of n words is a
random variable: in the worst-case it can be linear in
n (degenerated into a linear-chain), but we assume
this adversarial situation does not happen frequently,
and the average tree depth is O(log n).
Theorem 1. Assume for each n, the depth of a
parse tree of n words, notated dn, distributes nor-
mally with logarithmic mean and variance, i.e.,
dn — N(µn, σ2n), where µn = O(log n) and σ2n =
O(log n), then the average-case complexity of the
algorithm is h(n) = O(nk log2(cr)+g−1) for constant
k, thus polynomial in n.
Proof. From Lemma 1 and the definition of average-
case complexity, we have
</bodyText>
<equation confidence="0.990281">
h(n) = Edn—N(µn,σ2n)[f(n, dn)],
</equation>
<bodyText confidence="0.999389">
where Ex—D[] denotes the expectation with respect
</bodyText>
<figure confidence="0.993918111111111">
� R1
A. 1
[...
�R2
A. 1
[... .η2...] ...
� R3
A. 1
[...
</figure>
<page confidence="0.984135">
278
</page>
<bodyText confidence="0.917647">
to the random variable x in distribution D.
</bodyText>
<equation confidence="0.99737225">
h(n) = Edn∼N(µn,σ2n)[f(n, dn)]
= Edn∼N (µn,σ2 n)[O((cr)dnng−1)],
= O(ng−1Edn∼N (µn,σ2 n)[(cr)dn]),
= O(ng−1Edn∼N(µn,σ2n)[exp(dn log(cr))]) (2)
</equation>
<bodyText confidence="0.989529166666667">
Since dn ∼ N(µn, σ2 n) is a normal distribution,
dn log(cr) ∼ N(µ′, σ′2) is also a normal distribu-
tion, where µ′ = µn log(cr) and σ′ = σn log(cr).
Therefore exp(dn log(cr)) is a log-normal distribu-
tion, and by the property of log-normal distribution,
its expectation is exp (µ′ + σ′2/2). So we have
</bodyText>
<equation confidence="0.9556055">
Edn∼N (µn,σ2/2)[exp(dn log(cr))]
= exp (µ′ + σ′2/2)
= exp (µn log(cr) + σ2n log2(cr)/2)
= exp (O(log n) log(cr) + O(log n) log2(cr)/2)
= exp (O(log n) log2(cr))
≤ exp (k(log n) log2(cr)), for some constant k
= exp (log nk log2(cr))
= nk log2(cr). (3)
Plug it back to Equation (2), and we have the
average-case complexity
Edn[f(n,dn)] ≤ O(ng−1nk log2(cr))
= O(nk log2(cr)+g−1). (4)
</equation>
<bodyText confidence="0.9985695">
Since k, c, r and g are constants, the average-case
complexity is polynomial in sentence length n.
The assumption dn ∼ N(O(log n), O(log n))
will be empirically verified in Section 5.
</bodyText>
<subsectionHeader confidence="0.99767">
3.4 Linear-time Beam Search
</subsectionHeader>
<bodyText confidence="0.999972868421053">
Though polynomial complexity is a desirable prop-
erty in theory, the degree of the polynomial,
O(log cr) might still be too high in practice, depend-
ing on the translation grammar. To make it linear-
time, we apply the beam search idea from phrase-
based again. And once again, the only question to
decide is the choice of “binning”: how to assign each
item to a particular bin, depending on their progress?
While the number of Chinese words covered is a
natural progress indicator for phrase-based, it does
not work for tree-to-string because, among the three
actions, only scanning grows the hypothesis. The
prediction and completion actions do not make real
progress in terms of words, though they do make
progress on the tree. So we devise a novel progress
indicator natural for tree-to-string translation: the
number of tree nodes covered so far. Initially that
number is zero, and in a prediction step which ex-
pands node η using rule r, the number increments by
|C(r)|, the size of the Chinese-side treelet of r. For
example, a prediction step using rule r3 in Figure 2
to expand VP@2 will increase the tree-node count by
|C(r3) |= 6, since there are six tree nodes in that
rule (not counting leaf nodes or variables).
Scanning and completion do not make progress
in this definition since there is no new tree node
covered. In fact, since both of them are determin-
istic operations, they are treated as “closure” op-
erators in the real implementation, which means
that after a prediction, we always do as many scan-
ning/completion steps as possible until the symbol
after the dot is another node, where we have to wait
for the next prediction step.
This method has |T |= O(n) bins where |T |is
the size of the parse tree, and each bin holds b items.
Each item can expand to c new items, so the overall
complexity of this beam search is O(ncb), which is
linear in sentence length.
</bodyText>
<sectionHeader confidence="0.999948" genericHeader="method">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999962166666667">
The work of Watanabe et al. (2006) is closest in
spirit to ours: they also design an incremental decod-
ing algorithm, but for the hierarchical phrase-based
system (Chiang, 2007) instead. While we leave de-
tailed comparison and theoretical analysis to a future
work, here we point out some obvious differences:
</bodyText>
<listItem confidence="0.638475666666667">
1. due to the difference in the underlying trans-
lation models, their algorithm runs in O(n2b)
time with beam search in practice while ours
is linear. This is because each prediction step
now has O(n) choices, since they need to ex-
pand nodes like VP[1, 6] as:
</listItem>
<equation confidence="0.936996">
VP[1,6] → PP[1, i] VP[i, 6],
</equation>
<bodyText confidence="0.9033014">
where the midpoint i in general has O(n)
choices (just like in CKY). In other words, their
grammar constant c becomes O(n).
2. different binning criteria: we use the number of
tree nodes covered, while they stick to the orig-
</bodyText>
<page confidence="0.995726">
279
</page>
<bodyText confidence="0.999545243902439">
inal phrase-based idea of number of Chinese
words translated;
3. as a result, their framework requires gram-
mar transformation into the binary-branching
Greibach Normal Form (which is not always
possible) so that the resulting grammar always
contain at least one Chinese word in each rule
in order for a prediction step to always make
progress. Our framework, by contrast, works
with any grammar.
Besides, there are some other efforts less closely
related to ours. As mentioned in Section 1, while
we focus on enhancing syntax-based decoding with
phrase-based ideas, other authors have explored the
reverse, but also interesting, direction of enhancing
phrase-based decoding with syntax-aware reorder-
ing. For example Galley and Manning (2008) pro-
pose a shift-reduce style method to allow hiearar-
chical non-local reorderings in a phrase-based de-
coder. While this approach is certainly better than
pure phrase-based reordering, it remains quadratic
in run-time with beam search.
Within syntax-based paradigms, cube pruning
(Chiang, 2007; Huang and Chiang, 2007) has be-
come the standard method to speed up +LM de-
coding, which has been shown by many authors to
be highly effective; we will be comparing our incre-
mental decoder with a baseline decoder using cube
pruning in Section 5. It is also important to note
that cube pruning and incremental decoding are not
mutually exclusive, rather, they could potentially be
combined to further speed up decoding. We leave
this point to future work.
Multipass coarse-to-fine decoding is another pop-
ular idea (Venugopal et al., 2007; Zhang and Gildea,
2008; Dyer and Resnik, 2010). In particular, Dyer
and Resnik (2010) uses a two-pass approach, where
their first-pass, −LM decoding is also incremental
and polynomial-time (in the style of Earley (1970)
algorithm), but their second-pass, +LM decoding is
still bottom-up CKY with cube pruning.
</bodyText>
<sectionHeader confidence="0.999011" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999989">
To test the merits of our incremental decoder we
conduct large-scale experiments on a state-of-the-art
tree-to-string system, and compare it with the stan-
dard phrase-based system of Moses. Furturemore we
also compare our incremental decoder with the stan-
dard cube pruning approach on the same tree-to-
string decoder.
</bodyText>
<subsectionHeader confidence="0.998364">
5.1 Data and System Preparation
</subsectionHeader>
<bodyText confidence="0.999704909090909">
Our training corpus consists of 1.5M sentence pairs
with about 38M/32M words in Chinese/English, re-
spectively. We first word-align them by GIZA++ and
then parse the Chinese sentences using the Berke-
ley parser (Petrov and Klein, 2007), then apply
the GHKM algorithm (Galley et al., 2004) to ex-
tract tree-to-string translation rules. We use SRILM
Toolkit (Stolcke, 2002) to train a trigram language
model with modified Kneser-Ney smoothing on the
target side of training corpus. At decoding time,
we again parse the input sentences into trees, and
convert them into translation forest by rule pattern-
matching (Mi et al., 2008).
We use the newswire portion of 2006 NIST MT
Evaluation test set (616 sentences) as our develop-
ment set and the newswire portion of 2008 NIST
MT Evaluation test set (691 sentences) as our test
set. We evaluate the translation quality using the
BLEU-4 metric, which is calculated by the script
mteval-v13a.pl with its default setting which is case-
insensitive matching of n-grams. We use the stan-
dard minimum error-rate training (Och, 2003) to
tune the feature weights to maximize the system’s
BLEU score on development set.
We first verify the assumptions we made in Sec-
tion 3.3 in order to prove the theorem that tree depth
(as a random variable) is normally-distributed with
O(log n) mean and variance. Qualitatively, we veri-
fied that for most n, tree depth d(n) does look like a
normal distribution. Quantitatively, Figure 6 shows
that average tree height correlates extremely well
with 3.5 log n, while tree height variance is bounded
by 5.5 log n.
</bodyText>
<subsectionHeader confidence="0.999937">
5.2 Comparison with Cube pruning
</subsectionHeader>
<bodyText confidence="0.9998994">
We implemented our incremental decoding algo-
rithm in Python, and test its performance on the de-
velopment set. We first compare it with the stan-
dard cube pruning approach (also implemented in
Python) on the same tree-to-string system.1 Fig-
</bodyText>
<footnote confidence="0.890297">
1Our implementation of cube pruning follows (Chiang,
2007; Huang and Chiang, 2007) where besides a beam size b
of unique +LM items, there is also a hard limit (of 1000) on the
</footnote>
<page confidence="0.988391">
280
</page>
<figure confidence="0.998946">
5
4
3
2
1
0
Average Decoding Time (Secs)
BLEU Score
0 10 20 30 40 50 60 70
Sentence Length
0 0.2 0.4 0.6 0.8 1 1.2 1.4
Avg Decoding Time (secs per sentence)
incremental
cube pruning
30.1
29.9
29.8
29.7
29.6
29.5
30
incremental
cube pruning
(a) decoding time against sentence length (b) BLEU score against decoding time
</figure>
<figureCaption confidence="0.996214666666667">
Figure 7: Comparison with cube pruning. The scatter plot in (a) confirms that our incremental decoding scales linearly
with sentence length, while cube pruning super-linearly (b = 50 for both). The comparison in (b) shows that at the
same level of translation quality, incremental decoding is slightly faster than cube pruning, especially at smaller beams.
</figureCaption>
<figure confidence="0.985459">
0 10 20 30 40 50
Sentence Length (n)
</figure>
<figureCaption confidence="0.995039666666667">
Figure 6: Mean and variance of tree depth vs. sentence
length. The mean depth clearly scales with 3.5 log n, and
the variance is bounded by 5.5 log n.
</figureCaption>
<figure confidence="0.9905165">
0 10 20 30 40 50 60 70
Sentence Length
</figure>
<figureCaption confidence="0.997491">
Figure 8: Comparison of our incremental tree-to-string
decoder with Moses in terms of speed. Moses is shown
with various distortion limits (0, 6, 10, +∞; optimal: 10).
</figureCaption>
<figure confidence="0.99499068">
Tree Depth d(n)
25
20
15
10
5
0
Avg Depth
Variance
3.5 log n
Average Decoding Time (Secs)
40
35
30
25
20
15
10
5
0
M +∞
M 10
M 6
M 0
t2s
</figure>
<bodyText confidence="0.99940215">
ure 7(a) is a scatter plot of decoding times versus
sentence length (using beam b = 50 for both sys-
tems), where we confirm that our incremental de-
coder scales linearly, while cube pruning has a slight
tendency of superlinearity. Figure 7(b) is a side-by-
side comparison of decoding speed versus transla-
tion quality (in BLEU scores), using various beam
sizes for both systems (b=10–70 for cube pruning,
and b=10–110 for incremental). We can see that in-
cremental decoding is slightly faster than cube prun-
ing at the same levels of translation quality, and the
difference is more pronounced at smaller beams: for
number of (non-unique) pops from priority queues.
example, at the lowest levels of translation quality
(BLEU scores around 29.5), incremental decoding
takes only 0.12 seconds, which is about 4 times as
fast as cube pruning. We stress again that cube prun-
ing and incremental decoding are not mutually ex-
clusive, and rather they could potentially be com-
bined to further speed up decoding.
</bodyText>
<subsectionHeader confidence="0.999496">
5.3 Comparison with Moses
</subsectionHeader>
<bodyText confidence="0.99992675">
We also compare with the standard phrase-based
system of Moses (Koehn et al., 2007), with stan-
dard settings except for the ttable limit, which we set
to 100. Figure 8 compares our incremental decoder
</bodyText>
<page confidence="0.990814">
281
</page>
<table confidence="0.9994435">
system/decoder BLEU time
Moses (optimal dmax=10) 29.41 10.8
tree-to-str: cube pruning (b=10) 29.51 0.65
tree-to-str: cube pruning (b=20) 29.96 0.96
tree-to-str: incremental (b=10) 29.54 0.32
tree-to-str: incremental (b=50) 29.96 0.77
</table>
<tableCaption confidence="0.7966845">
Table 3: Final BLEU score and speed results on the test
data (691 sentences), compared with Moses and cube
pruning. Time is in seconds per sentence, including pars-
ing time (0.21s) for the two tree-to-string decoders.
</tableCaption>
<bodyText confidence="0.999568722222222">
with Moses at various distortion limits (dmax=0, 6,
10, and +∞). Consistent with the theoretical anal-
ysis in Section 2, Moses with no distortion limit
(dmax = +∞) scale quadratically, and monotone
decoding (dmax = 0) scale linearly. We use MERT
to tune the best weights for each distortion limit, and
dmax = 10 performs the best on our dev set.
Table 3 reports the final results in terms of BLEU
score and speed on the test set. Our linear-time
incremental decoder with the small beam of size
b = 10 achieves a BLEU score of 29.54, compara-
ble to Moses with the optimal distortion limit of 10
(BLEU score 29.41). But our decoding (including
source-language parsing) only takes 0.32 seconds a
sentences, which is more than 30 times faster than
Moses. With a larger beam of b = 50 our BLEU
score increases to 29.96, which is a half BLEU point
better than Moses, but still about 15 times faster.
</bodyText>
<sectionHeader confidence="0.999578" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999991777777778">
We have presented an incremental dynamic pro-
gramming algorithm for tree-to-string translation
which resembles phrase-based based decoding. This
algorithm is the first incremental algorithm that runs
in polynomial-time in theory, and linear-time in
practice with beam search. Large-scale experiments
on a state-of-the-art tree-to-string decoder confirmed
that, with a comparable (or better) translation qual-
ity, it can run more than 30 times faster than the
phrase-based system of Moses, even though ours is
in Python while Moses in C++. We also showed that
it is slightly faster (and scale better) than the popular
cube pruning technique. For future work we would
like to apply this algorithm to forest-based transla-
tion and hierarchical system by pruning the first-pass
−LM forest. We would also combine cube pruning
with our incremental algorithm, and study its perfor-
mance with higher-order language models.
</bodyText>
<sectionHeader confidence="0.992142" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999878545454545">
We would like to thank David Chiang, Kevin
Knight, and Jonanthan Graehl for discussions and
the anonymous reviewers for comments. In partic-
ular, we are indebted to the reviewer who pointed
out a crucial mistake in Theorem 1 and its proof
in the submission. This research was supported in
part by DARPA, under contract HR0011-06-C-0022
under subcontract to BBN Technologies, and under
DOI-NBC Grant N10AP20031, and in part by the
National Natural Science Foundation of China, Con-
tracts 60736014 and 90920004.
</bodyText>
<sectionHeader confidence="0.999415" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999902121212121">
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–208.
Chris Dyer and Philip Resnik. 2010. Context-free re-
ordering, finite-state translation. In Proceedings of
NAACL.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 13(2):94–102.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings ofEMNLP 2008.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In Pro-
ceedings ofHLT-NAACL, pages 273–280.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Fast decoding with integrated language models.
In Proceedings ofACL, Prague, Czech Rep., June.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings ofAMTA, Boston,
MA, August.
Liang Huang. 2007. Binarization, synchronous bina-
rization, and target-side binarization. In Proc. NAACL
Workshop on Syntax and Structure in Statistical Trans-
lation.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25(4):607–615.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings ofACL:
demonstration sesion.
</reference>
<page confidence="0.960616">
282
</page>
<reference confidence="0.999293861111111">
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings ofAMTA, pages 115–124.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of COLING-ACL, pages 609–
616.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL: HLT,
Columbus, OH.
Franz Joseph Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings ofACL,
pages 160–167.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL.
Stuart Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive
parsing. Journal ofLogic Programming, 24:3–36.
Andreas Stolcke. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings ofICSLP, vol-
ume 30, pages 901–904.
Ashish Venugopal, Andreas Zollmann, and Stephen Vo-
gel. 2007. An efficient two-pass approach to
synchronous-CFG driven statistical MT. In Proceed-
ings ofHLT-NAACL.
Taro Watanabe, Hajime Tsukuda, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In Proceedings of COLING-
ACL.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–404.
Hao Zhang and Daniel Gildea. 2008. Efficient multi-
pass decoding for synchronous context free grammars.
In Proceedings ofACL.
</reference>
<page confidence="0.998957">
283
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.189254">
<title confidence="0.999812">Efficient Incremental Decoding for Tree-to-String Translation</title>
<author confidence="0.714096">Huang</author>
<affiliation confidence="0.925828">Sciences University of Southern</affiliation>
<address confidence="0.982795">4676 Admiralty Way, Suite Marina del Rey, CA 90292,</address>
<author confidence="0.669398">Mi</author>
<affiliation confidence="0.998969">Lab. of Intelligent Information Institute of Computing</affiliation>
<address confidence="0.950654">Chinese Academy of P.O. Box 2704, Beijing 100190,</address>
<email confidence="0.486931">htmi@ict.ac.cn</email>
<abstract confidence="0.995269789473684">translation models should efficient with polynomially-sized search space, but in practice they are often embarassingly slow, partly due to the cost of language model integration. In this paper we borrow from phrase-based decoding the to generate a translation left-to-right, and show that for tree-to-string models, with a clever encoding of derivation history, this method runs in averagecase polynomial-time in theory, and lineartime with beam search in practice (whereas phrase-based decoding is exponential-time in theory and quadratic-time in practice). Experiments show that, with comparable translation quality, our tree-to-string system (in Python) can run more than 30 times faster than the phrase-based system Moses (in C++).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="2623" citStr="Chiang, 2007" startWordPosition="377" endWordPosition="378">ice phrase-based tree-to-string Table 1: [main result] Time complexity of our incremental tree-to-string decoding compared with phrase-based. In practice means “approximate search with beams.” longer than d. This has been the standard practice with phrase-based models (Koehn et al., 2007), which fails to capture important long-distance reorderings like SVO-to-SOV. Syntax-based models, on the other hand, use syntactic information to restrict reorderings to a computationally-tractable and linguisticallymotivated subset, for example those generated by synchronous context-free grammars (Wu, 1997; Chiang, 2007). In theory the advantage seems quite obvious: we can now express global reorderings (like SVO-to-VSO) in polynomial-time (as opposed to exponential in phrase-based). But unfortunately, this polynomial complexity is super-linear (being generally cubic-time or worse), which is slow in practice. Furthermore, language model integration becomes more expensive here since the decoder now has to maintain target-language boundary words at both ends of a subtranslation (Huang and Chiang, 2007), whereas a phrase-based decoder only needs to do this at one end since the translation is always growing left-</context>
<context position="4175" citStr="Chiang, 2007" startWordPosition="595" endWordPosition="597">pirical Methods in Natural Language Processing, pages 273–283, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics of enhancing phrase-based decoding with syntaxaware reordering (Galley and Manning, 2008), we are more interested in the other direction, i.e., can syntax-based models learn from phrase-based decoding, so that they still model global reordering, but in an efficient (preferably linear-time) fashion? Watanabe et al. (2006) is an early attempt in this direction: they design a phrase-based-style decoder for the hierarchical phrase-based model (Chiang, 2007). However, this algorithm even with the beam search still runs in quadratic-time in practice. Furthermore, their approach requires grammar transformation that converts the original grammar into an equivalent binary-branching Greibach Normal Form, which is not always feasible in practice. We take a fresh look on this problem and turn our focus to one particular syntax-based paradigm, treeto-string translation (Liu et al., 2006; Huang et al., 2006), since this is the simplest and fastest among syntax-based approaches. We develop an incremental dynamic programming algorithm and make the following</context>
<context position="7578" citStr="Chiang, 2007" startWordPosition="1156" endWordPosition="1157"> each −LM item above into a series of +LM items; each +LM item has the form (v,a ) where a is the last word of the hypothesis. Thus a +LM version of (1) might be: (• •••6,talks ) : (w, “Bush held talks”) (•••3•••,Sharon ) : (w′, “Bush held talks with Sharon”) where the score of the resulting +LM item w′ = w + c + d − log Plm(with |talk) now includes a combination cost due to the bigrams formed when applying the phrase-pair. The complexity of this dynamic programming algorithm for g-gram decoding is O(2nn2|V |g−1) where n is the sentence length and |V |is the English vocabulary size (Huang and Chiang, 2007). 274 1 2 3 4 5 Figure 1: Beam search in phrase-based decoding expands the hypotheses in the current bin (#2) into longer ones. VP AS VV le jˇux´ıng (a) B`ushi[yˇu Sh¯al´ong ]1 [jˇux´ıng le huit´an ]2 ⇓ 1-best parser (b) IP@E NP@1 B`ushi PP@2.1 VP@2.2 VP@2 r1 ⇓ P NP@2.1.2 yˇu Sh¯al´ong NP@2.2.3 huit´an PP VP P x1:NP VV AS x2:NP Figure 2: Tree-to-string rule r3 for reordering. 2.2 Beam Search in Practice To make the exponential algorithm practical, beam search is the standard approximate search method (Koehn, 2004). Here we group +LM items into n bins, with each bin BZ hosting at most b items t</context>
<context position="11790" citStr="Chiang, 2007" startWordPosition="1894" endWordPosition="1895">ficantly because we now have to keep track of targetlanguage boundary words, but unlike the phrasebased case in Section 2, here we have to remember both sides the leftmost and the rightmost boundary words: each node is now split into +LM items like (q a * b) where q is a tree node, and a and b are left and right English boundary words. For example, a bigram +LM item for node VP@2 might be (VP@2 held * Sharon). This is also the case with other syntax-based models like Hiero or GHKM: language model integration overhead is the most significant factor that causes syntax-based decoding to be slow (Chiang, 2007). In theory +LM decoding is O(nc|V |4(9−1)), where V denotes English vocabulary (Huang, 2007). In practice we have to resort to beam search again: at each node we would only allow top-b +LM items. With beam search, tree-to-string decoding with an integrated language model runs in time O(ncb2), where b is the size of the beam at each node, and c is (maximum) number of translation rules matched at each node (Huang, 2007). See Table 2 for a summary. 3.2 Incremental Decoding Can we borrow the idea of phrase-based decoding, so that we also grow the hypothesis strictly leftto-right, and only need to</context>
<context position="19460" citStr="Chiang, 2007" startWordPosition="3356" endWordPosition="3357">eaf node, with stack depth s &lt; d. Each rule Ri(i &gt; 1) expands node ηi−1, and thus has c choices by the definition of grammar constant c. Furthermore, each rule in the stack is actually a dotted-rule, i.e., it is associated with a dot position ranging from 0 to r, where r is the arity of the rule (length of English side of the rule). So the total number of stacks is O((cr)d). Besides the stack, each state also maintains (g−1) rightmost words of the hypothesis as the language model signature, which amounts to O(|V |g−1). So the total number of states is O((cr)d|V |g−1). Following previous work (Chiang, 2007), we assume a constant number of English translations for each foreign word in the input sentence, so |V |= O(n). And as mentioned above, for each state, there are c possible expansions, so the overall time complexity is f(n,d) = c(cr)d|V |g−1 = O((cr)dng−1). We do average-case analysis below because the tree depth (height) for a sentence of n words is a random variable: in the worst-case it can be linear in n (degenerated into a linear-chain), but we assume this adversarial situation does not happen frequently, and the average tree depth is O(log n). Theorem 1. Assume for each n, the depth of</context>
<context position="23698" citStr="Chiang, 2007" startWordPosition="4097" endWordPosition="4098"> means that after a prediction, we always do as many scanning/completion steps as possible until the symbol after the dot is another node, where we have to wait for the next prediction step. This method has |T |= O(n) bins where |T |is the size of the parse tree, and each bin holds b items. Each item can expand to c new items, so the overall complexity of this beam search is O(ncb), which is linear in sentence length. 4 Related Work The work of Watanabe et al. (2006) is closest in spirit to ours: they also design an incremental decoding algorithm, but for the hierarchical phrase-based system (Chiang, 2007) instead. While we leave detailed comparison and theoretical analysis to a future work, here we point out some obvious differences: 1. due to the difference in the underlying translation models, their algorithm runs in O(n2b) time with beam search in practice while ours is linear. This is because each prediction step now has O(n) choices, since they need to expand nodes like VP[1, 6] as: VP[1,6] → PP[1, i] VP[i, 6], where the midpoint i in general has O(n) choices (just like in CKY). In other words, their grammar constant c becomes O(n). 2. different binning criteria: we use the number of tree</context>
<context position="25379" citStr="Chiang, 2007" startWordPosition="4367" endWordPosition="4368">ther efforts less closely related to ours. As mentioned in Section 1, while we focus on enhancing syntax-based decoding with phrase-based ideas, other authors have explored the reverse, but also interesting, direction of enhancing phrase-based decoding with syntax-aware reordering. For example Galley and Manning (2008) propose a shift-reduce style method to allow hieararchical non-local reorderings in a phrase-based decoder. While this approach is certainly better than pure phrase-based reordering, it remains quadratic in run-time with beam search. Within syntax-based paradigms, cube pruning (Chiang, 2007; Huang and Chiang, 2007) has become the standard method to speed up +LM decoding, which has been shown by many authors to be highly effective; we will be comparing our incremental decoder with a baseline decoder using cube pruning in Section 5. It is also important to note that cube pruning and incremental decoding are not mutually exclusive, rather, they could potentially be combined to further speed up decoding. We leave this point to future work. Multipass coarse-to-fine decoding is another popular idea (Venugopal et al., 2007; Zhang and Gildea, 2008; Dyer and Resnik, 2010). In particular,</context>
<context position="28494" citStr="Chiang, 2007" startWordPosition="4873" endWordPosition="4874">ed with O(log n) mean and variance. Qualitatively, we verified that for most n, tree depth d(n) does look like a normal distribution. Quantitatively, Figure 6 shows that average tree height correlates extremely well with 3.5 log n, while tree height variance is bounded by 5.5 log n. 5.2 Comparison with Cube pruning We implemented our incremental decoding algorithm in Python, and test its performance on the development set. We first compare it with the standard cube pruning approach (also implemented in Python) on the same tree-to-string system.1 Fig1Our implementation of cube pruning follows (Chiang, 2007; Huang and Chiang, 2007) where besides a beam size b of unique +LM items, there is also a hard limit (of 1000) on the 280 5 4 3 2 1 0 Average Decoding Time (Secs) BLEU Score 0 10 20 30 40 50 60 70 Sentence Length 0 0.2 0.4 0.6 0.8 1 1.2 1.4 Avg Decoding Time (secs per sentence) incremental cube pruning 30.1 29.9 29.8 29.7 29.6 29.5 30 incremental cube pruning (a) decoding time against sentence length (b) BLEU score against decoding time Figure 7: Comparison with cube pruning. The scatter plot in (a) confirms that our incremental decoding scales linearly with sentence length, while cube prunin</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Philip Resnik</author>
</authors>
<title>Context-free reordering, finite-state translation.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="25963" citStr="Dyer and Resnik, 2010" startWordPosition="4464" endWordPosition="4467"> paradigms, cube pruning (Chiang, 2007; Huang and Chiang, 2007) has become the standard method to speed up +LM decoding, which has been shown by many authors to be highly effective; we will be comparing our incremental decoder with a baseline decoder using cube pruning in Section 5. It is also important to note that cube pruning and incremental decoding are not mutually exclusive, rather, they could potentially be combined to further speed up decoding. We leave this point to future work. Multipass coarse-to-fine decoding is another popular idea (Venugopal et al., 2007; Zhang and Gildea, 2008; Dyer and Resnik, 2010). In particular, Dyer and Resnik (2010) uses a two-pass approach, where their first-pass, −LM decoding is also incremental and polynomial-time (in the style of Earley (1970) algorithm), but their second-pass, +LM decoding is still bottom-up CKY with cube pruning. 5 Experiments To test the merits of our incremental decoder we conduct large-scale experiments on a state-of-the-art tree-to-string system, and compare it with the standard phrase-based system of Moses. Furturemore we also compare our incremental decoder with the standard cube pruning approach on the same tree-tostring decoder. 5.1 Da</context>
</contexts>
<marker>Dyer, Resnik, 2010</marker>
<rawString>Chris Dyer and Philip Resnik. 2010. Context-free reordering, finite-state translation. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="17298" citStr="Earley (1970)" startWordPosition="2941" endWordPosition="2942">iables replaced by matched nodes: [NP@1 . VP@2] [. held NP@2.2.3 with NP@2.1.2] Note that this is a reordering rule, and the stack always follows the English word order because we generate hypothesis incrementally left-to-right. Figure 4 works out the full example. We formalize this algorithm in Figure 5. Each item (s, ρ) consists of a stack s and a hypothesis ρ. Similar to phrase-based dynamic programming, only the last g−1 words of ρ are part of the signature for decoding with g-gram LM. Each stack is a list of dotted rules, i.e., rules with dot positions indicting progress, in the style of Earley (1970). We call the last (rightmost) rule on the stack the top rule, which is the rule being processed currently. The symbol after the dot in the top rule is called the next symbol, since it is the symbol to expand or process next. Depending on the next symbol a, we can perform one of the three actions: • if a is a node η, we perform a Predict action which expands η using a rule r that can patternmatch the subtree rooted at η; we push r is to the stack, with the dot at the beginning; • if a is an English word, we perform a Scan action which immediately adds it to the current hypothesis, advancing th</context>
<context position="26136" citStr="Earley (1970)" startWordPosition="4492" endWordPosition="4493">we will be comparing our incremental decoder with a baseline decoder using cube pruning in Section 5. It is also important to note that cube pruning and incremental decoding are not mutually exclusive, rather, they could potentially be combined to further speed up decoding. We leave this point to future work. Multipass coarse-to-fine decoding is another popular idea (Venugopal et al., 2007; Zhang and Gildea, 2008; Dyer and Resnik, 2010). In particular, Dyer and Resnik (2010) uses a two-pass approach, where their first-pass, −LM decoding is also incremental and polynomial-time (in the style of Earley (1970) algorithm), but their second-pass, +LM decoding is still bottom-up CKY with cube pruning. 5 Experiments To test the merits of our incremental decoder we conduct large-scale experiments on a state-of-the-art tree-to-string system, and compare it with the standard phrase-based system of Moses. Furturemore we also compare our incremental decoder with the standard cube pruning approach on the same tree-tostring decoder. 5.1 Data and System Preparation Our training corpus consists of 1.5M sentence pairs with about 38M/32M words in Chinese/English, respectively. We first word-align them by GIZA++ a</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Jay Earley. 1970. An efficient context-free parsing algorithm. Communications of the ACM, 13(2):94–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<booktitle>In Proceedings ofEMNLP</booktitle>
<contexts>
<context position="3807" citStr="Galley and Manning, 2008" startWordPosition="538" endWordPosition="541">e the translation is always growing left-to-right. As a result, syntax-based models are often embarassingly slower than their phrase-based counterparts, preventing them from becoming widely useful. Can we combine the merits of both approaches? While other authors have explored the possibilities exponential quadratic polynomial linear 273 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 273–283, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics of enhancing phrase-based decoding with syntaxaware reordering (Galley and Manning, 2008), we are more interested in the other direction, i.e., can syntax-based models learn from phrase-based decoding, so that they still model global reordering, but in an efficient (preferably linear-time) fashion? Watanabe et al. (2006) is an early attempt in this direction: they design a phrase-based-style decoder for the hierarchical phrase-based model (Chiang, 2007). However, this algorithm even with the beam search still runs in quadratic-time in practice. Furthermore, their approach requires grammar transformation that converts the original grammar into an equivalent binary-branching Greibac</context>
<context position="8679" citStr="Galley and Manning, 2008" startWordPosition="1351" endWordPosition="1354">ard approximate search method (Koehn, 2004). Here we group +LM items into n bins, with each bin BZ hosting at most b items that cover exactly i Chinese words (see Figure 1). The complexity becomes O(n2b) because there are a total of O(nb) items in all bins, and to expand each item we need to scan the whole coverage vector, which costs O(n). This quadratic complexity is still too slow in practice and we often set a small distortion limit of dmax (say, 5) so that no jumps longer than dmax are allowed. This method reduces the complexity to O(nbdmax) but fails to capture longdistance reorderings (Galley and Manning, 2008). 3 Incremental Decoding for Tree-to-String Translation We will first briefly review tree-to-string translation paradigm and then develop an incremental decoding algorithm for it inspired by phrase-based decoding. 3.1 Tree-to-string Translation A typical tree-to-string system (Liu et al., 2006; Huang et al., 2006) performs translation in two steps: parsing and decoding. A parser first parses the source language input into a 1-best tree T, and the decoder then searches for the best derivation (a ser4 ⇓ r5 ⇓ (e) Bush [held talks]2 [with Sharon]1 Figure 3: An example derivation of tree-to-string </context>
<context position="25087" citStr="Galley and Manning (2008)" startWordPosition="4323" endWordPosition="4326">formation into the binary-branching Greibach Normal Form (which is not always possible) so that the resulting grammar always contain at least one Chinese word in each rule in order for a prediction step to always make progress. Our framework, by contrast, works with any grammar. Besides, there are some other efforts less closely related to ours. As mentioned in Section 1, while we focus on enhancing syntax-based decoding with phrase-based ideas, other authors have explored the reverse, but also interesting, direction of enhancing phrase-based decoding with syntax-aware reordering. For example Galley and Manning (2008) propose a shift-reduce style method to allow hieararchical non-local reorderings in a phrase-based decoder. While this approach is certainly better than pure phrase-based reordering, it remains quadratic in run-time with beam search. Within syntax-based paradigms, cube pruning (Chiang, 2007; Huang and Chiang, 2007) has become the standard method to speed up +LM decoding, which has been shown by many authors to be highly effective; we will be comparing our incremental decoder with a baseline decoder using cube pruning in Section 5. It is also important to note that cube pruning and incremental</context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and Christopher D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In Proceedings ofEMNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings ofHLT-NAACL,</booktitle>
<pages>273--280</pages>
<contexts>
<context position="26875" citStr="Galley et al., 2004" startWordPosition="4604" endWordPosition="4607">rits of our incremental decoder we conduct large-scale experiments on a state-of-the-art tree-to-string system, and compare it with the standard phrase-based system of Moses. Furturemore we also compare our incremental decoder with the standard cube pruning approach on the same tree-tostring decoder. 5.1 Data and System Preparation Our training corpus consists of 1.5M sentence pairs with about 38M/32M words in Chinese/English, respectively. We first word-align them by GIZA++ and then parse the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007), then apply the GHKM algorithm (Galley et al., 2004) to extract tree-to-string translation rules. We use SRILM Toolkit (Stolcke, 2002) to train a trigram language model with modified Kneser-Ney smoothing on the target side of training corpus. At decoding time, we again parse the input sentences into trees, and convert them into translation forest by rule patternmatching (Mi et al., 2008). We use the newswire portion of 2006 NIST MT Evaluation test set (616 sentences) as our development set and the newswire portion of 2008 NIST MT Evaluation test set (691 sentences) as our test set. We evaluate the translation quality using the BLEU-4 metric, wh</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings ofHLT-NAACL, pages 273–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Fast decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL,</booktitle>
<location>Prague, Czech Rep.,</location>
<contexts>
<context position="3112" citStr="Huang and Chiang, 2007" startWordPosition="443" endWordPosition="446">ractable and linguisticallymotivated subset, for example those generated by synchronous context-free grammars (Wu, 1997; Chiang, 2007). In theory the advantage seems quite obvious: we can now express global reorderings (like SVO-to-VSO) in polynomial-time (as opposed to exponential in phrase-based). But unfortunately, this polynomial complexity is super-linear (being generally cubic-time or worse), which is slow in practice. Furthermore, language model integration becomes more expensive here since the decoder now has to maintain target-language boundary words at both ends of a subtranslation (Huang and Chiang, 2007), whereas a phrase-based decoder only needs to do this at one end since the translation is always growing left-to-right. As a result, syntax-based models are often embarassingly slower than their phrase-based counterparts, preventing them from becoming widely useful. Can we combine the merits of both approaches? While other authors have explored the possibilities exponential quadratic polynomial linear 273 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 273–283, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Lingui</context>
<context position="7578" citStr="Huang and Chiang, 2007" startWordPosition="1154" endWordPosition="1157">, we split each −LM item above into a series of +LM items; each +LM item has the form (v,a ) where a is the last word of the hypothesis. Thus a +LM version of (1) might be: (• •••6,talks ) : (w, “Bush held talks”) (•••3•••,Sharon ) : (w′, “Bush held talks with Sharon”) where the score of the resulting +LM item w′ = w + c + d − log Plm(with |talk) now includes a combination cost due to the bigrams formed when applying the phrase-pair. The complexity of this dynamic programming algorithm for g-gram decoding is O(2nn2|V |g−1) where n is the sentence length and |V |is the English vocabulary size (Huang and Chiang, 2007). 274 1 2 3 4 5 Figure 1: Beam search in phrase-based decoding expands the hypotheses in the current bin (#2) into longer ones. VP AS VV le jˇux´ıng (a) B`ushi[yˇu Sh¯al´ong ]1 [jˇux´ıng le huit´an ]2 ⇓ 1-best parser (b) IP@E NP@1 B`ushi PP@2.1 VP@2.2 VP@2 r1 ⇓ P NP@2.1.2 yˇu Sh¯al´ong NP@2.2.3 huit´an PP VP P x1:NP VV AS x2:NP Figure 2: Tree-to-string rule r3 for reordering. 2.2 Beam Search in Practice To make the exponential algorithm practical, beam search is the standard approximate search method (Koehn, 2004). Here we group +LM items into n bins, with each bin BZ hosting at most b items t</context>
<context position="25404" citStr="Huang and Chiang, 2007" startWordPosition="4369" endWordPosition="4372">ess closely related to ours. As mentioned in Section 1, while we focus on enhancing syntax-based decoding with phrase-based ideas, other authors have explored the reverse, but also interesting, direction of enhancing phrase-based decoding with syntax-aware reordering. For example Galley and Manning (2008) propose a shift-reduce style method to allow hieararchical non-local reorderings in a phrase-based decoder. While this approach is certainly better than pure phrase-based reordering, it remains quadratic in run-time with beam search. Within syntax-based paradigms, cube pruning (Chiang, 2007; Huang and Chiang, 2007) has become the standard method to speed up +LM decoding, which has been shown by many authors to be highly effective; we will be comparing our incremental decoder with a baseline decoder using cube pruning in Section 5. It is also important to note that cube pruning and incremental decoding are not mutually exclusive, rather, they could potentially be combined to further speed up decoding. We leave this point to future work. Multipass coarse-to-fine decoding is another popular idea (Venugopal et al., 2007; Zhang and Gildea, 2008; Dyer and Resnik, 2010). In particular, Dyer and Resnik (2010) u</context>
<context position="28519" citStr="Huang and Chiang, 2007" startWordPosition="4875" endWordPosition="4878">n) mean and variance. Qualitatively, we verified that for most n, tree depth d(n) does look like a normal distribution. Quantitatively, Figure 6 shows that average tree height correlates extremely well with 3.5 log n, while tree height variance is bounded by 5.5 log n. 5.2 Comparison with Cube pruning We implemented our incremental decoding algorithm in Python, and test its performance on the development set. We first compare it with the standard cube pruning approach (also implemented in Python) on the same tree-to-string system.1 Fig1Our implementation of cube pruning follows (Chiang, 2007; Huang and Chiang, 2007) where besides a beam size b of unique +LM items, there is also a hard limit (of 1000) on the 280 5 4 3 2 1 0 Average Decoding Time (Secs) BLEU Score 0 10 20 30 40 50 60 70 Sentence Length 0 0.2 0.4 0.6 0.8 1 1.2 1.4 Avg Decoding Time (secs per sentence) incremental cube pruning 30.1 29.9 29.8 29.7 29.6 29.5 30 incremental cube pruning (a) decoding time against sentence length (b) BLEU score against decoding time Figure 7: Comparison with cube pruning. The scatter plot in (a) confirms that our incremental decoding scales linearly with sentence length, while cube pruning super-linearly (b = 50 </context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Fast decoding with integrated language models. In Proceedings ofACL, Prague, Czech Rep., June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Statistical syntax-directed translation with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proceedings ofAMTA,</booktitle>
<location>Boston, MA,</location>
<contexts>
<context position="4625" citStr="Huang et al., 2006" startWordPosition="663" endWordPosition="666">e) fashion? Watanabe et al. (2006) is an early attempt in this direction: they design a phrase-based-style decoder for the hierarchical phrase-based model (Chiang, 2007). However, this algorithm even with the beam search still runs in quadratic-time in practice. Furthermore, their approach requires grammar transformation that converts the original grammar into an equivalent binary-branching Greibach Normal Form, which is not always feasible in practice. We take a fresh look on this problem and turn our focus to one particular syntax-based paradigm, treeto-string translation (Liu et al., 2006; Huang et al., 2006), since this is the simplest and fastest among syntax-based approaches. We develop an incremental dynamic programming algorithm and make the following contributions: • we show that, unlike previous work, our incremental decoding algorithm runs in averagecase polynomial-time in theory for tree-tostring models, and the beam search version runs in linear-time in practice (see Table 1); • large-scale experiments on a tree-to-string system confirm that, with comparable translation quality, our incremental decoder (in Python) can run more than 30 times faster than the phrase-based system Moses (in C</context>
<context position="8994" citStr="Huang et al., 2006" startWordPosition="1393" endWordPosition="1396">or, which costs O(n). This quadratic complexity is still too slow in practice and we often set a small distortion limit of dmax (say, 5) so that no jumps longer than dmax are allowed. This method reduces the complexity to O(nbdmax) but fails to capture longdistance reorderings (Galley and Manning, 2008). 3 Incremental Decoding for Tree-to-String Translation We will first briefly review tree-to-string translation paradigm and then develop an incremental decoding algorithm for it inspired by phrase-based decoding. 3.1 Tree-to-string Translation A typical tree-to-string system (Liu et al., 2006; Huang et al., 2006) performs translation in two steps: parsing and decoding. A parser first parses the source language input into a 1-best tree T, and the decoder then searches for the best derivation (a ser4 ⇓ r5 ⇓ (e) Bush [held talks]2 [with Sharon]1 Figure 3: An example derivation of tree-to-string translation (much simplified from Mi et al. (2008)). Shaded regions denote parts of the tree that matches the rule. quence of translation steps) d∗ that converts source tree T into a target-language string. Figure 3 shows how this process works. The Chinese sentence (a) is first parsed into tree (b), which will be</context>
<context position="11042" citStr="Huang et al., 2006" startWordPosition="1753" endWordPosition="1756">s of various algorithms. b is the beam width, V is the English vocabulary, and c is the number of translation rules per node. As a special case, phrase-based decoding with distortion limit dm,,. is O(nbdm,,.). *: incremental decoding algorithms. “Bush”. Similarly, rule r3 shown in Figure 2 is applied to the VP subtree, which swaps the two NPs, yielding the situation in (d). Finally two phrasal rules r4 and r5 translate the two remaining NPs and finish the translation. In this framework, decoding without language model (−LM decoding) is simply a linear-time depth-first search with memoization (Huang et al., 2006), since a tree of n words is also of size O(n) and we visit every node only once. Adding a language model, however, slows it down significantly because we now have to keep track of targetlanguage boundary words, but unlike the phrasebased case in Section 2, here we have to remember both sides the leftmost and the rightmost boundary words: each node is now split into +LM items like (q a * b) where q is a tree node, and a and b are left and right English boundary words. For example, a bigram +LM item for node VP@2 might be (VP@2 held * Sharon). This is also the case with other syntax-based model</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proceedings ofAMTA, Boston, MA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Binarization, synchronous binarization, and target-side binarization.</title>
<date>2007</date>
<booktitle>In Proc. NAACL Workshop on Syntax and Structure in Statistical Translation.</booktitle>
<contexts>
<context position="11883" citStr="Huang, 2007" startWordPosition="1908" endWordPosition="1909">rasebased case in Section 2, here we have to remember both sides the leftmost and the rightmost boundary words: each node is now split into +LM items like (q a * b) where q is a tree node, and a and b are left and right English boundary words. For example, a bigram +LM item for node VP@2 might be (VP@2 held * Sharon). This is also the case with other syntax-based models like Hiero or GHKM: language model integration overhead is the most significant factor that causes syntax-based decoding to be slow (Chiang, 2007). In theory +LM decoding is O(nc|V |4(9−1)), where V denotes English vocabulary (Huang, 2007). In practice we have to resort to beam search again: at each node we would only allow top-b +LM items. With beam search, tree-to-string decoding with an integrated language model runs in time O(ncb2), where b is the size of the beam at each node, and c is (maximum) number of translation rules matched at each node (Huang, 2007). See Table 2 for a summary. 3.2 Incremental Decoding Can we borrow the idea of phrase-based decoding, so that we also grow the hypothesis strictly leftto-right, and only need to maintain the rightmost boundary words? The key intuition is to adapt the coverage-vector ide</context>
</contexts>
<marker>Huang, 2007</marker>
<rawString>Liang Huang. 2007. Binarization, synchronous binarization, and target-side binarization. In Proc. NAACL Workshop on Syntax and Structure in Statistical Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
</authors>
<title>Decoding complexity in wordreplacement translation models.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="1653" citStr="Knight, 1999" startWordPosition="234" endWordPosition="235">at, with comparable translation quality, our tree-to-string system (in Python) can run more than 30 times faster than the phrase-based system Moses (in C++). 1 Introduction Most efforts in statistical machine translation so far are variants of either phrase-based or syntax-based models. From a theoretical point of view, phrasebased models are neither expressive nor efficient: they typically allow arbitrary permutations and resort to language models to decide the best order. In theory, this process can be reduced to the Traveling Salesman Problem and thus requires an exponentialtime algorithm (Knight, 1999). In practice, the decoder has to employ beam search to make it tractable (Koehn, 2004). However, even beam search runs in quadratic-time in general (see Sec. 2), unless a small distortion limit (say, d=5) further restricts the possible set of reorderings to those local ones by ruling out any long-distance reorderings that have a “jump” in theory in practice phrase-based tree-to-string Table 1: [main result] Time complexity of our incremental tree-to-string decoding compared with phrase-based. In practice means “approximate search with beams.” longer than d. This has been the standard practice</context>
</contexts>
<marker>Knight, 1999</marker>
<rawString>Kevin Knight. 1999. Decoding complexity in wordreplacement translation models. Computational Linguistics, 25(4):607–615.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL: demonstration sesion.</booktitle>
<contexts>
<context position="2299" citStr="Koehn et al., 2007" startWordPosition="334" endWordPosition="337">has to employ beam search to make it tractable (Koehn, 2004). However, even beam search runs in quadratic-time in general (see Sec. 2), unless a small distortion limit (say, d=5) further restricts the possible set of reorderings to those local ones by ruling out any long-distance reorderings that have a “jump” in theory in practice phrase-based tree-to-string Table 1: [main result] Time complexity of our incremental tree-to-string decoding compared with phrase-based. In practice means “approximate search with beams.” longer than d. This has been the standard practice with phrase-based models (Koehn et al., 2007), which fails to capture important long-distance reorderings like SVO-to-SOV. Syntax-based models, on the other hand, use syntactic information to restrict reorderings to a computationally-tractable and linguisticallymotivated subset, for example those generated by synchronous context-free grammars (Wu, 1997; Chiang, 2007). In theory the advantage seems quite obvious: we can now express global reorderings (like SVO-to-VSO) in polynomial-time (as opposed to exponential in phrase-based). But unfortunately, this polynomial complexity is super-linear (being generally cubic-time or worse), which is</context>
<context position="5249" citStr="Koehn et al., 2007" startWordPosition="759" endWordPosition="762">ce this is the simplest and fastest among syntax-based approaches. We develop an incremental dynamic programming algorithm and make the following contributions: • we show that, unlike previous work, our incremental decoding algorithm runs in averagecase polynomial-time in theory for tree-tostring models, and the beam search version runs in linear-time in practice (see Table 1); • large-scale experiments on a tree-to-string system confirm that, with comparable translation quality, our incremental decoder (in Python) can run more than 30 times faster than the phrase-based system Moses (in C++) (Koehn et al., 2007); • furthermore, on the same tree-to-string system, incremental decoding is slightly faster than the standard cube pruning method at the same level of translation quality; • this is also the first linear-time incremental decoder that performs global reordering. We will first briefly review phrase-based decoding in this section, which inspires our incremental algorithm in the next section. 2 Background: Phrase-based Decoding We will use the following running example from Chinese to English to explain both phrase-based and syntax-based decoding throughout this paper: 0 B`ushi1 yˇu 2 Sh¯al´ong 3 </context>
<context position="30930" citStr="Koehn et al., 2007" startWordPosition="5308" endWordPosition="5311">r than cube pruning at the same levels of translation quality, and the difference is more pronounced at smaller beams: for number of (non-unique) pops from priority queues. example, at the lowest levels of translation quality (BLEU scores around 29.5), incremental decoding takes only 0.12 seconds, which is about 4 times as fast as cube pruning. We stress again that cube pruning and incremental decoding are not mutually exclusive, and rather they could potentially be combined to further speed up decoding. 5.3 Comparison with Moses We also compare with the standard phrase-based system of Moses (Koehn et al., 2007), with standard settings except for the ttable limit, which we set to 100. Figure 8 compares our incremental decoder 281 system/decoder BLEU time Moses (optimal dmax=10) 29.41 10.8 tree-to-str: cube pruning (b=10) 29.51 0.65 tree-to-str: cube pruning (b=20) 29.96 0.96 tree-to-str: incremental (b=10) 29.54 0.32 tree-to-str: incremental (b=50) 29.96 0.77 Table 3: Final BLEU score and speed results on the test data (691 sentences), compared with Moses and cube pruning. Time is in seconds per sentence, including parsing time (0.21s) for the two tree-to-string decoders. with Moses at various distor</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings ofACL: demonstration sesion.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proceedings ofAMTA,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="1740" citStr="Koehn, 2004" startWordPosition="250" endWordPosition="251">ore than 30 times faster than the phrase-based system Moses (in C++). 1 Introduction Most efforts in statistical machine translation so far are variants of either phrase-based or syntax-based models. From a theoretical point of view, phrasebased models are neither expressive nor efficient: they typically allow arbitrary permutations and resort to language models to decide the best order. In theory, this process can be reduced to the Traveling Salesman Problem and thus requires an exponentialtime algorithm (Knight, 1999). In practice, the decoder has to employ beam search to make it tractable (Koehn, 2004). However, even beam search runs in quadratic-time in general (see Sec. 2), unless a small distortion limit (say, d=5) further restricts the possible set of reorderings to those local ones by ruling out any long-distance reorderings that have a “jump” in theory in practice phrase-based tree-to-string Table 1: [main result] Time complexity of our incremental tree-to-string decoding compared with phrase-based. In practice means “approximate search with beams.” longer than d. This has been the standard practice with phrase-based models (Koehn et al., 2007), which fails to capture important long-d</context>
<context position="6102" citStr="Koehn, 2004" startWordPosition="890" endWordPosition="891">s global reordering. We will first briefly review phrase-based decoding in this section, which inspires our incremental algorithm in the next section. 2 Background: Phrase-based Decoding We will use the following running example from Chinese to English to explain both phrase-based and syntax-based decoding throughout this paper: 0 B`ushi1 yˇu 2 Sh¯al´ong 3 jˇuxing 4 le 5 huit´an 6 Bush with Sharon hold -ed meeting ‘Bush held talks with Sharon’ 2.1 Basic Dynamic Programming Algorithm Phrase-based decoders generate partial targetlanguage outputs in left-to-right order in the form of hypotheses (Koehn, 2004). Each hypothesis has a coverage vector capturing the source-language words translated so far, and can be extended into a longer hypothesis by a phrase-pair translating an uncovered segment. This process can be formalized as a deductive system. For example, the following deduction step grows a hypothesis by the phrase-pair hyˇu Sh¯al´ong, with Sharoni covering Chinese span [1, 2, 3]: (• •••6) : (w, “Bush held talks”) (•••3•••) : (w′, “Bush held talks with Sharon”) (1) where a • in the coverage vector indicates the source word at this position is “covered” and where w and w′ = w+c+d are the wei</context>
<context position="8097" citStr="Koehn, 2004" startWordPosition="1246" endWordPosition="1247">here n is the sentence length and |V |is the English vocabulary size (Huang and Chiang, 2007). 274 1 2 3 4 5 Figure 1: Beam search in phrase-based decoding expands the hypotheses in the current bin (#2) into longer ones. VP AS VV le jˇux´ıng (a) B`ushi[yˇu Sh¯al´ong ]1 [jˇux´ıng le huit´an ]2 ⇓ 1-best parser (b) IP@E NP@1 B`ushi PP@2.1 VP@2.2 VP@2 r1 ⇓ P NP@2.1.2 yˇu Sh¯al´ong NP@2.2.3 huit´an PP VP P x1:NP VV AS x2:NP Figure 2: Tree-to-string rule r3 for reordering. 2.2 Beam Search in Practice To make the exponential algorithm practical, beam search is the standard approximate search method (Koehn, 2004). Here we group +LM items into n bins, with each bin BZ hosting at most b items that cover exactly i Chinese words (see Figure 1). The complexity becomes O(n2b) because there are a total of O(nb) items in all bins, and to expand each item we need to scan the whole coverage vector, which costs O(n). This quadratic complexity is still too slow in practice and we often set a small distortion limit of dmax (say, 5) so that no jumps longer than dmax are allowed. This method reduces the complexity to O(nbdmax) but fails to capture longdistance reorderings (Galley and Manning, 2008). 3 Incremental De</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. In Proceedings ofAMTA, pages 115–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-tostring alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>609--616</pages>
<contexts>
<context position="4604" citStr="Liu et al., 2006" startWordPosition="659" endWordPosition="662">ferably linear-time) fashion? Watanabe et al. (2006) is an early attempt in this direction: they design a phrase-based-style decoder for the hierarchical phrase-based model (Chiang, 2007). However, this algorithm even with the beam search still runs in quadratic-time in practice. Furthermore, their approach requires grammar transformation that converts the original grammar into an equivalent binary-branching Greibach Normal Form, which is not always feasible in practice. We take a fresh look on this problem and turn our focus to one particular syntax-based paradigm, treeto-string translation (Liu et al., 2006; Huang et al., 2006), since this is the simplest and fastest among syntax-based approaches. We develop an incremental dynamic programming algorithm and make the following contributions: • we show that, unlike previous work, our incremental decoding algorithm runs in averagecase polynomial-time in theory for tree-tostring models, and the beam search version runs in linear-time in practice (see Table 1); • large-scale experiments on a tree-to-string system confirm that, with comparable translation quality, our incremental decoder (in Python) can run more than 30 times faster than the phrase-bas</context>
<context position="8973" citStr="Liu et al., 2006" startWordPosition="1389" endWordPosition="1392">hole coverage vector, which costs O(n). This quadratic complexity is still too slow in practice and we often set a small distortion limit of dmax (say, 5) so that no jumps longer than dmax are allowed. This method reduces the complexity to O(nbdmax) but fails to capture longdistance reorderings (Galley and Manning, 2008). 3 Incremental Decoding for Tree-to-String Translation We will first briefly review tree-to-string translation paradigm and then develop an incremental decoding algorithm for it inspired by phrase-based decoding. 3.1 Tree-to-string Translation A typical tree-to-string system (Liu et al., 2006; Huang et al., 2006) performs translation in two steps: parsing and decoding. A parser first parses the source language input into a 1-best tree T, and the decoder then searches for the best derivation (a ser4 ⇓ r5 ⇓ (e) Bush [held talks]2 [with Sharon]1 Figure 3: An example derivation of tree-to-string translation (much simplified from Mi et al. (2008)). Shaded regions denote parts of the tree that matches the rule. quence of translation steps) d∗ that converts source tree T into a target-language string. Figure 3 shows how this process works. The Chinese sentence (a) is first parsed into tr</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-tostring alignment template for statistical machine translation. In Proceedings of COLING-ACL, pages 609– 616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Forestbased translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL: HLT,</booktitle>
<location>Columbus, OH.</location>
<contexts>
<context position="9329" citStr="Mi et al. (2008)" startWordPosition="1451" endWordPosition="1454">to-String Translation We will first briefly review tree-to-string translation paradigm and then develop an incremental decoding algorithm for it inspired by phrase-based decoding. 3.1 Tree-to-string Translation A typical tree-to-string system (Liu et al., 2006; Huang et al., 2006) performs translation in two steps: parsing and decoding. A parser first parses the source language input into a 1-best tree T, and the decoder then searches for the best derivation (a ser4 ⇓ r5 ⇓ (e) Bush [held talks]2 [with Sharon]1 Figure 3: An example derivation of tree-to-string translation (much simplified from Mi et al. (2008)). Shaded regions denote parts of the tree that matches the rule. quence of translation steps) d∗ that converts source tree T into a target-language string. Figure 3 shows how this process works. The Chinese sentence (a) is first parsed into tree (b), which will be converted into an English string in 5 steps. First, at the root node, we apply rule r1 preserving the top-level word-order (r1) IP (x1:NP x2:VP) → x1 x2 which results in two unfinished subtrees, NP@1 and VP@2 in (c). Here X@η denotes a tree node of label X at tree address q (Shieber et al., 1995). (The root node has address E, and t</context>
<context position="27213" citStr="Mi et al., 2008" startWordPosition="4658" endWordPosition="4661">ning corpus consists of 1.5M sentence pairs with about 38M/32M words in Chinese/English, respectively. We first word-align them by GIZA++ and then parse the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007), then apply the GHKM algorithm (Galley et al., 2004) to extract tree-to-string translation rules. We use SRILM Toolkit (Stolcke, 2002) to train a trigram language model with modified Kneser-Ney smoothing on the target side of training corpus. At decoding time, we again parse the input sentences into trees, and convert them into translation forest by rule patternmatching (Mi et al., 2008). We use the newswire portion of 2006 NIST MT Evaluation test set (616 sentences) as our development set and the newswire portion of 2008 NIST MT Evaluation test set (691 sentences) as our test set. We evaluate the translation quality using the BLEU-4 metric, which is calculated by the script mteval-v13a.pl with its default setting which is caseinsensitive matching of n-grams. We use the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the system’s BLEU score on development set. We first verify the assumptions we made in Section 3.3 in order to prove the</context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased translation. In Proceedings of ACL: HLT, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Joseph Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="27652" citStr="Och, 2003" startWordPosition="4733" endWordPosition="4734">ide of training corpus. At decoding time, we again parse the input sentences into trees, and convert them into translation forest by rule patternmatching (Mi et al., 2008). We use the newswire portion of 2006 NIST MT Evaluation test set (616 sentences) as our development set and the newswire portion of 2008 NIST MT Evaluation test set (691 sentences) as our test set. We evaluate the translation quality using the BLEU-4 metric, which is calculated by the script mteval-v13a.pl with its default setting which is caseinsensitive matching of n-grams. We use the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the system’s BLEU score on development set. We first verify the assumptions we made in Section 3.3 in order to prove the theorem that tree depth (as a random variable) is normally-distributed with O(log n) mean and variance. Qualitatively, we verified that for most n, tree depth d(n) does look like a normal distribution. Quantitatively, Figure 6 shows that average tree height correlates extremely well with 3.5 log n, while tree height variance is bounded by 5.5 log n. 5.2 Comparison with Cube pruning We implemented our incremental decoding algorithm in </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Joseph Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings ofACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of HLTNAACL.</booktitle>
<contexts>
<context position="26822" citStr="Petrov and Klein, 2007" startWordPosition="4595" endWordPosition="4598">m-up CKY with cube pruning. 5 Experiments To test the merits of our incremental decoder we conduct large-scale experiments on a state-of-the-art tree-to-string system, and compare it with the standard phrase-based system of Moses. Furturemore we also compare our incremental decoder with the standard cube pruning approach on the same tree-tostring decoder. 5.1 Data and System Preparation Our training corpus consists of 1.5M sentence pairs with about 38M/32M words in Chinese/English, respectively. We first word-align them by GIZA++ and then parse the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007), then apply the GHKM algorithm (Galley et al., 2004) to extract tree-to-string translation rules. We use SRILM Toolkit (Stolcke, 2002) to train a trigram language model with modified Kneser-Ney smoothing on the target side of training corpus. At decoding time, we again parse the input sentences into trees, and convert them into translation forest by rule patternmatching (Mi et al., 2008). We use the newswire portion of 2006 NIST MT Evaluation test set (616 sentences) as our development set and the newswire portion of 2008 NIST MT Evaluation test set (691 sentences) as our test set. We evaluat</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
<author>Yves Schabes</author>
<author>Fernando Pereira</author>
</authors>
<title>Principles and implementation of deductive parsing.</title>
<date>1995</date>
<booktitle>Journal ofLogic Programming,</booktitle>
<pages>24--3</pages>
<contexts>
<context position="9892" citStr="Shieber et al., 1995" startWordPosition="1554" endWordPosition="1557">-string translation (much simplified from Mi et al. (2008)). Shaded regions denote parts of the tree that matches the rule. quence of translation steps) d∗ that converts source tree T into a target-language string. Figure 3 shows how this process works. The Chinese sentence (a) is first parsed into tree (b), which will be converted into an English string in 5 steps. First, at the root node, we apply rule r1 preserving the top-level word-order (r1) IP (x1:NP x2:VP) → x1 x2 which results in two unfinished subtrees, NP@1 and VP@2 in (c). Here X@η denotes a tree node of label X at tree address q (Shieber et al., 1995). (The root node has address E, and the first child of node q has address q.1, etc.) Then rule r2 grabs the B`ushisubtree and transliterate it into the English word NP@1 B`ushi with NP@2.1.2 Sh¯al´ong NP@2.2.3 huit´an (d) Bush held PP@2.1 VP@2.2 VP@2 r2 ⇓ r3 ⇓ VV jˇux´ıng NP@2.2.3 huit´an AS le P yˇu NP@2.1.2 Sh¯al´ong → held x2 with x1 ˇ le yu jˇuxing 275 in theory in practice phrase* O(2nn2 · |V |9−1) O(n2b) tree-to-str O(nc · |V |4(9−1)) O(ncb2) this work* O(nk log2(_) · |V |9−1) O(ncb) Table 2: Summary of time complexities of various algorithms. b is the beam width, V is the English vocabu</context>
</contexts>
<marker>Shieber, Schabes, Pereira, 1995</marker>
<rawString>Stuart Shieber, Yves Schabes, and Fernando Pereira. 1995. Principles and implementation of deductive parsing. Journal ofLogic Programming, 24:3–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings ofICSLP,</booktitle>
<volume>30</volume>
<pages>901--904</pages>
<contexts>
<context position="26957" citStr="Stolcke, 2002" startWordPosition="4618" endWordPosition="4619"> tree-to-string system, and compare it with the standard phrase-based system of Moses. Furturemore we also compare our incremental decoder with the standard cube pruning approach on the same tree-tostring decoder. 5.1 Data and System Preparation Our training corpus consists of 1.5M sentence pairs with about 38M/32M words in Chinese/English, respectively. We first word-align them by GIZA++ and then parse the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007), then apply the GHKM algorithm (Galley et al., 2004) to extract tree-to-string translation rules. We use SRILM Toolkit (Stolcke, 2002) to train a trigram language model with modified Kneser-Ney smoothing on the target side of training corpus. At decoding time, we again parse the input sentences into trees, and convert them into translation forest by rule patternmatching (Mi et al., 2008). We use the newswire portion of 2006 NIST MT Evaluation test set (616 sentences) as our development set and the newswire portion of 2008 NIST MT Evaluation test set (691 sentences) as our test set. We evaluate the translation quality using the BLEU-4 metric, which is calculated by the script mteval-v13a.pl with its default setting which is c</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm - an extensible language modeling toolkit. In Proceedings ofICSLP, volume 30, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Andreas Zollmann</author>
<author>Stephen Vogel</author>
</authors>
<title>An efficient two-pass approach to synchronous-CFG driven statistical MT.</title>
<date>2007</date>
<booktitle>In Proceedings ofHLT-NAACL.</booktitle>
<contexts>
<context position="25915" citStr="Venugopal et al., 2007" startWordPosition="4456" endWordPosition="4459">n run-time with beam search. Within syntax-based paradigms, cube pruning (Chiang, 2007; Huang and Chiang, 2007) has become the standard method to speed up +LM decoding, which has been shown by many authors to be highly effective; we will be comparing our incremental decoder with a baseline decoder using cube pruning in Section 5. It is also important to note that cube pruning and incremental decoding are not mutually exclusive, rather, they could potentially be combined to further speed up decoding. We leave this point to future work. Multipass coarse-to-fine decoding is another popular idea (Venugopal et al., 2007; Zhang and Gildea, 2008; Dyer and Resnik, 2010). In particular, Dyer and Resnik (2010) uses a two-pass approach, where their first-pass, −LM decoding is also incremental and polynomial-time (in the style of Earley (1970) algorithm), but their second-pass, +LM decoding is still bottom-up CKY with cube pruning. 5 Experiments To test the merits of our incremental decoder we conduct large-scale experiments on a state-of-the-art tree-to-string system, and compare it with the standard phrase-based system of Moses. Furturemore we also compare our incremental decoder with the standard cube pruning ap</context>
</contexts>
<marker>Venugopal, Zollmann, Vogel, 2007</marker>
<rawString>Ashish Venugopal, Andreas Zollmann, and Stephen Vogel. 2007. An efficient two-pass approach to synchronous-CFG driven statistical MT. In Proceedings ofHLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Hajime Tsukuda</author>
<author>Hideki Isozaki</author>
</authors>
<title>Left-to-right target generation for hierarchical phrase-based translation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLINGACL.</booktitle>
<contexts>
<context position="4040" citStr="Watanabe et al. (2006)" startWordPosition="573" endWordPosition="576">oaches? While other authors have explored the possibilities exponential quadratic polynomial linear 273 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 273–283, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics of enhancing phrase-based decoding with syntaxaware reordering (Galley and Manning, 2008), we are more interested in the other direction, i.e., can syntax-based models learn from phrase-based decoding, so that they still model global reordering, but in an efficient (preferably linear-time) fashion? Watanabe et al. (2006) is an early attempt in this direction: they design a phrase-based-style decoder for the hierarchical phrase-based model (Chiang, 2007). However, this algorithm even with the beam search still runs in quadratic-time in practice. Furthermore, their approach requires grammar transformation that converts the original grammar into an equivalent binary-branching Greibach Normal Form, which is not always feasible in practice. We take a fresh look on this problem and turn our focus to one particular syntax-based paradigm, treeto-string translation (Liu et al., 2006; Huang et al., 2006), since this is</context>
<context position="23556" citStr="Watanabe et al. (2006)" startWordPosition="4073" endWordPosition="4076"> tree node covered. In fact, since both of them are deterministic operations, they are treated as “closure” operators in the real implementation, which means that after a prediction, we always do as many scanning/completion steps as possible until the symbol after the dot is another node, where we have to wait for the next prediction step. This method has |T |= O(n) bins where |T |is the size of the parse tree, and each bin holds b items. Each item can expand to c new items, so the overall complexity of this beam search is O(ncb), which is linear in sentence length. 4 Related Work The work of Watanabe et al. (2006) is closest in spirit to ours: they also design an incremental decoding algorithm, but for the hierarchical phrase-based system (Chiang, 2007) instead. While we leave detailed comparison and theoretical analysis to a future work, here we point out some obvious differences: 1. due to the difference in the underlying translation models, their algorithm runs in O(n2b) time with beam search in practice while ours is linear. This is because each prediction step now has O(n) choices, since they need to expand nodes like VP[1, 6] as: VP[1,6] → PP[1, i] VP[i, 6], where the midpoint i in general has O(</context>
</contexts>
<marker>Watanabe, Tsukuda, Isozaki, 2006</marker>
<rawString>Taro Watanabe, Hajime Tsukuda, and Hideki Isozaki. 2006. Left-to-right target generation for hierarchical phrase-based translation. In Proceedings of COLINGACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="2608" citStr="Wu, 1997" startWordPosition="375" endWordPosition="376">y in practice phrase-based tree-to-string Table 1: [main result] Time complexity of our incremental tree-to-string decoding compared with phrase-based. In practice means “approximate search with beams.” longer than d. This has been the standard practice with phrase-based models (Koehn et al., 2007), which fails to capture important long-distance reorderings like SVO-to-SOV. Syntax-based models, on the other hand, use syntactic information to restrict reorderings to a computationally-tractable and linguisticallymotivated subset, for example those generated by synchronous context-free grammars (Wu, 1997; Chiang, 2007). In theory the advantage seems quite obvious: we can now express global reorderings (like SVO-to-VSO) in polynomial-time (as opposed to exponential in phrase-based). But unfortunately, this polynomial complexity is super-linear (being generally cubic-time or worse), which is slow in practice. Furthermore, language model integration becomes more expensive here since the decoder now has to maintain target-language boundary words at both ends of a subtranslation (Huang and Chiang, 2007), whereas a phrase-based decoder only needs to do this at one end since the translation is alway</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
</authors>
<title>Efficient multipass decoding for synchronous context free grammars.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="25939" citStr="Zhang and Gildea, 2008" startWordPosition="4460" endWordPosition="4463">rch. Within syntax-based paradigms, cube pruning (Chiang, 2007; Huang and Chiang, 2007) has become the standard method to speed up +LM decoding, which has been shown by many authors to be highly effective; we will be comparing our incremental decoder with a baseline decoder using cube pruning in Section 5. It is also important to note that cube pruning and incremental decoding are not mutually exclusive, rather, they could potentially be combined to further speed up decoding. We leave this point to future work. Multipass coarse-to-fine decoding is another popular idea (Venugopal et al., 2007; Zhang and Gildea, 2008; Dyer and Resnik, 2010). In particular, Dyer and Resnik (2010) uses a two-pass approach, where their first-pass, −LM decoding is also incremental and polynomial-time (in the style of Earley (1970) algorithm), but their second-pass, +LM decoding is still bottom-up CKY with cube pruning. 5 Experiments To test the merits of our incremental decoder we conduct large-scale experiments on a state-of-the-art tree-to-string system, and compare it with the standard phrase-based system of Moses. Furturemore we also compare our incremental decoder with the standard cube pruning approach on the same tree-</context>
</contexts>
<marker>Zhang, Gildea, 2008</marker>
<rawString>Hao Zhang and Daniel Gildea. 2008. Efficient multipass decoding for synchronous context free grammars. In Proceedings ofACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>