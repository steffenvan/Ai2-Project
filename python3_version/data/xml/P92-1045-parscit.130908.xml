<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002258">
<title confidence="0.779112">
INFORMATION STATES AS FIRST CLASS CITIZENS
</title>
<author confidence="0.901887">
JOrgen Villadsen
</author>
<affiliation confidence="0.901354">
Centre for Language Technology, University of Copenhagen
</affiliation>
<address confidence="0.526864">
Njalsgade 80, DK-2300 Copenhagen S, Denmark
</address>
<email confidence="0.564239">
Internet: jvOcst.ku.dk
</email>
<sectionHeader confidence="0.955652" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999977448275862">
The information state of an agent is changed when
a text (in natural language) is processed. The
meaning of a text can be taken to be this informa-
tion state change potential. The inference of a con-
sequence make explicit something already implicit
in the premises — i.e. that no information state
change occurs if the (assumed) consequence text is
processed after the (given) premise texts have been
processed. Elementary logic (i.e. first-order logic)
can be used as a logical representation language
for texts, but the notion of a information state (a
set of possibilities — namely first-order models) is
not available from the object language (belongs to
the meta language). This means that texts with
other texts as parts (e.g. propositional attitudes
with embedded sentences) cannot be treated di-
rectly. Traditional intensional logics (i.e. modal
logic) allow (via modal operators) access to the
information states from the object language, but
the access is limited and interference with (exten-
sional) notions like (standard) identity, variables
etc. is introduced. This does not mean that the
ideas present in intensional logics will not work
(possibly improved by adding a notion of partial-
ity), but rather that often a formalisation in the
simple type theory (with sorts for entities and in-
dices making information states first class citizens
— like individuals) is more comprehensible, flexi-
ble and logically well-behaved.
</bodyText>
<sectionHeader confidence="0.998737" genericHeader="introduction">
INTRODUCTION
</sectionHeader>
<bodyText confidence="0.9461397">
Classical first-order logic (hereafter called elemen-
tary logic) is often used as logical representa-
tion language. For instance, elementary logic has
proven very useful when formalising mathemati-
cal structures like in axiomatic set theory, num-
ber theory etc. Also, in natural language process-
ing (NLP) systems, &amp;quot;toy&amp;quot; examples are easily for-
malised in elementary logic:
Every man lies. John is a man.
So, John lies.
</bodyText>
<equation confidence="0.984352">
Vz(man(x) lie(x)), man(John)
lie(John)
</equation>
<bodyText confidence="0.999448444444444">
The formalisation is judged adequate since the
model theory of elementary logic is in correspon-
dence with intuitions (when some logical maturity
is gained and some logical innocence is lost) —
moreover the proof theory gives a reasonable no-
tion of entailment for the &amp;quot;toy&amp;quot; examples.
Extending this success story to linguistically
more complicated cases is difficult. Two problem-
atic topics are:
</bodyText>
<subsectionHeader confidence="0.629491">
Anaphora
</subsectionHeader>
<bodyText confidence="0.999583333333333">
It must be explained how, in a text, a dependent
manages to pick up a referent that was introduced
by its antecedent.
</bodyText>
<subsectionHeader confidence="0.454936">
Every man lies. John is a man.
</subsectionHeader>
<bodyText confidence="0.939905">
So, he lies. (3)
</bodyText>
<subsectionHeader confidence="0.935544">
Attitude reports
</subsectionHeader>
<bodyText confidence="0.9533795">
Propositional attitudes involves reports about cog-
nition (belief/knowledge), perception etc.
Mary believes that every man lies.
John is a man.
So, Mary believes that John lies. (4)
It is a characteristic that if one starts with the
&amp;quot;toy&amp;quot; examples in elementary logic it is very dif-
ficult to make progress for the above-mentioned
problematic topics. Much of the work on the
first three topics comes from the last decade —
in case of the last topic pioneering work by Hin-
tikka, Kripke and Montague started in the sixties.
The aim of this paper is to show that by taking
an abstract notion of information states as start-
ing point the &amp;quot;toy&amp;quot; examples and the limitations
of elementary logic are better understood. We ar-
gue that information states are to be taken serious
in logic-based approaches to NLP. Furthermore,
we think that information states can be regarded
as sets of possibilities (structural aspects can be
added, but should not be taken as stand-alone).
Information states are at the meta-level only
(1) when elementary logic is used. Information states
are still mainly at the meta-level when intensional
logics (e.g. modal logic) are used, but some ma-
(2) nipulations are available at the object level.
</bodyText>
<page confidence="0.996694">
303
</page>
<bodyText confidence="0.999982714285714">
This limited access is problematic in connec-
tion with (extensional) notions like (standard)
identity, variables etc. Information states can be
put at object level by using a so-called simple type
theory (a classical higher-order logic based on the
simply typed A-calculus) — this gives a very ele-
gant framework for NLP applications.
The point is not that elementary or the vari-
ous intensional logics are wrong — on the contrary
they include many important ideas — but for the
purpose of understanding, integrating and imple-
menting a formalisation one is better off with a
simple type theory (stronger type theories are pos-
sible, of course).
</bodyText>
<sectionHeader confidence="0.998778" genericHeader="method">
AGENTS AND TEXTS
</sectionHeader>
<bodyText confidence="0.998944588235294">
Consider an agent processing the texts t1, . • • , t71.
By processing we mean that the agent ac-
cepts the information conveyed by the texts. The
texts are assumed to be declarative (purely infor-
mative) and unambiguous (uniquely informative).
The texts are processed one by one (dynamically)
— not considered as a whole (statically). The dy-
namic interpretation of texts seems more realistic
than the static interpretation.
By a text we consider (complete) discourses
— although as examples we use only single (com-
plete) sentences. We take the completeness to
mean that the order of the texts is irrelevant. In
general texts have expressions as parts whose or-
der is important — the completeness requirement
only means that the (top level) texts are complete
units.
</bodyText>
<sectionHeader confidence="0.9937" genericHeader="method">
INFORMATION STATES
</sectionHeader>
<bodyText confidence="0.999296666666667">
We first consider an abstract notion of an infor-
mation state (often called a knowledge state or a
belief state). The initial information state /0 is
assumed known (or assumed irrelevant). Changes
are of the information states of the agent as fol-
lows:
</bodyText>
<equation confidence="0.4434975">
10 -.11 - 11-- In
,`
</equation>
<bodyText confidence="0.991632581395349">
where ri is the change in the information state
when the text ti is processed.
An obvious approach is to identify information
states with the set of texts already processed —
hence nothing lost. Some improvements are pos-
sible (normalisation and the like). Since the texts
are concrete objects they are easy to treat compu-
tationally. We call this approach the syntactical
approach.
An orthogonal approach (the semantical ap-
proach) identifies information states with sets of
possibilities. This is the approach followed here.
Note that a possibility need not be a so-called
&amp;quot;possible world&amp;quot; — partiality and similar notions
can be introduced, see Muskens (1989).
A combination of the two approaches might
be the optimal solution. Many of these aspects
are discussed in Konolige (1986).
Observe that the universal and empty sets are
understood as opposites: the empty set of possi-
bility and the universal set of texts represent the
(absolute) inconsistent information state; and the
universal set of possibility and the empty set of
texts represent the (absolute) initial information
state. Other notions of consistency and initiality
can be defined.
A partial order on information states (&amp;quot;getting
better informed&amp;quot;) is easy obtained. For the syn-
tactical approach this is trivial — more texts make
one better informed. For the semantical approach
one could introduce previously eliminated possi-
bilities in the information state, but we assume
eliminative information state changes: r(l) C /
for all / (this does not necessarily hold for non-
monotonic logics / belief revision / anaphora(?)
— see Groenendijk and Stokhof (1991) for further
details).
Given the texts ti, , 4 the agent is asked
whether a text t can be inferred; i.e. whether pro-
cessing t after processing /1, ,i, would change
the information state or not:
/72
Here 7 is the identity function.
</bodyText>
<sectionHeader confidence="0.975995" genericHeader="method">
ELEMENTARY LOGIC
</sectionHeader>
<bodyText confidence="0.988618954545455">
When elementary logic is used as logical represen-
tation language for texts, information states are
identified with sets of models.
Let the formulas 01, , q, 0 be the transla-
tions of the texts Ii,...t„, t. The information
state when /1, ,4 has been processed is the
set of all models in which 01, , 0„ are all true.
,i entails t if the model set correspond-
ing to the processing of ti, tn does not change
when t is processed. I.e. alternatively, consider a
particular model M — if 01, , 0,, are all true in
M then 0 must be true in M as well (this is the
usual formulation of entailment).
Hence, although any proof theory for elemen-
tary logic matches the notion of entailment for
&amp;quot;toy&amp;quot; example texts, the notion of information
states is purely a notion of the model theory
(hence in the meta-language; not available from
the object language). This is problematic when
texts have other texts as parts, like the embedded
sentence in propositional attitudes, since a direct
formalisation in elementary logic is ruled out.
</bodyText>
<page confidence="0.99861">
304
</page>
<sectionHeader confidence="0.757872" genericHeader="method">
TRADITIONAL APPROACH
</sectionHeader>
<bodyText confidence="0.995767823529412">
When traditional intensional logics (e.g. modal
logics) are used as logical representation languages
for texts, information states are identified with
sets of possible worlds relative to a model M =
(W.. .), where W is the considered set of possible
worlds.
The information state when t1, • • • ,ti has
been processed is, relative to a model, the set of
possible worlds in which 01, , Ok are all true.
The truth definition for a formula cb allows for
modal operators, say C2, such that if rk is C2i,b then
rk is true in the possible worlds W4, C W if ti) is
true in the possible worlds W,k C W, where Wo =
fv(Wo.) for some function fv. : 2(W) P(W)
(hence M = (W, fv, ..)).
For the usual modal operator 0 the function
fp reduces to a relation Ro : W x W such that:
</bodyText>
<equation confidence="0.873174666666667">
=
Wtpfo(W,,) = U {w0 Ro(wO, wtk)}
woEW4,
</equation>
<bodyText confidence="0.99998515">
By introducing more modal operators the informa-
tion states can be manipulated further (a small set
of &amp;quot;permutational&amp;quot; and &amp;quot;quantificational&amp;quot; modal
operators would suffice — compare combinatory
logic and variable-free formulations of predicate
logic). However, the information states as well as
the possible worlds are never directly accessible
from the object language.
Another complication is that the iv function
cannot be specified in the object language directly
(although equivalent object language formulas can
often be found — cf. the correspondence theory for
modal logic).
Perhaps the most annoying complication is
the possible interference with (extensional) no-
tions like (standard) identity, where Leibniz&apos;s Law
fails (for non-modally closed formulas) — see
Muskens (1989) for examples. If variables are
present the inference rule of V-Introduction fails
in a similar way.
</bodyText>
<sectionHeader confidence="0.952572" genericHeader="method">
SIMPLE TYPE THEORY
</sectionHeader>
<bodyText confidence="0.99961946875">
The above-mentioned complications becomes even
more evident if elementary logic is replaced by a
simple type theory while keeping the modal oper-
ators (cf. Montague&apos;s Intensional Logic). The A-
calculus in the simple type theory allows for an el-
egant compositionality methodology (category to
type correspondence over the two algebras). Often
the higher-order logic (quantificational power) fa-
cilities of the simple type theory are not necessary
— or so-called general models are sufficient.
The complication regarding variables men-
tioned above manifests itself in the way that 3-
reduction does not hold for the A-calculus (again,
see Muskens (1989) and references herein). Even
more damaging: The (simply typed!) A-calculus is
not Church-Rosser (due to the limited a-renaming
capabilities of the modal operators).
What seems needed is a logical representation
language in which the information states are ex-
plicit manipulable, like the individuals in elemen-
tary logic. This point of view is forcefully defended
by Cresswell (1990), where the possibilities of the
information states are optimised using the well-
known technique of indexing. Hence we obtain an
ontology of entities and indices.
In recent papers we have presented and dis-
cussed a categorial grammar formalism capable
of (in a strict compositional way) parsing and
translating natural language texts, see Villadsen
(1991a,b,c). The resulting formulas are terms in a
many-sorted simple type theory. An example of a
translation (simplified):
</bodyText>
<subsectionHeader confidence="0.760631">
Mary believes that John lies. (5)
</subsectionHeader>
<bodyText confidence="0.901139666666667">
Ai .believe(i, Mary, (A j .lie(j , John))) (6)
Adding partiality along the lines in Muskens
(1989) is currently under investigation.
</bodyText>
<sectionHeader confidence="0.999009" genericHeader="acknowledgments">
ACKNOWLEDGMENTS
</sectionHeader>
<bodyText confidence="0.986748">
Reports work done while at Department of Com-
puter Science, Technical University of Denmark.
</bodyText>
<sectionHeader confidence="0.999057" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999730260869565">
M. J. Cresswell (1990). Entities and Indices.
Kluwer Academic Publishers.
J. Groenendijk and M. Stokhof (1991). Two Theo-
ries of Dynamic Semantics. In J. van Eijck, editor,
Logics in Al - 91, Amsterdam. Springer-Verlag
(Lecture Notes in Computer Science 478).
K. Konolige (1986) A Deduction Model of Belief.
Pitman.
R. Muskens (1989). Meaning and Partiality. PhD
thesis, University of Amsterdam.
J. Villadsen (1991a). Combinatory Categorial
Grammar for Intensional Fragment of Natural
Language. In B. Mayoh, editor, Scandinavian
Conference on Artificial Intelligence - 91, Roskilde.
IOS Press.
J. Villadsen (1991b). Categorial Grammar and In-
tensionality. In Annual Meeting of the Danish As-
sociation for Computational Linguistics - 91, Aal-
borg. Department of Computational Linguistics,
Arhus Business School.
J. Villadsen (1991c). Anaphora and Intensional-
ity in Classical Logic. In Nordic Computational
Linguistics Conference - 91, Bergen. To appear.
</reference>
<page confidence="0.999238">
305
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.591800">
<affiliation confidence="0.8519685">INFORMATION STATES AS FIRST CLASS CITIZENS Centre for Language Technology, University of Copenhagen</affiliation>
<address confidence="0.982365">Njalsgade 80, DK-2300 Copenhagen S, Denmark</address>
<email confidence="0.908766">Internet:jvOcst.ku.dk</email>
<abstract confidence="0.996970133333334">The information state of an agent is changed when a text (in natural language) is processed. The meaning of a text can be taken to be this information state change potential. The inference of a consequence make explicit something already implicit in the premises — i.e. that no information state change occurs if the (assumed) consequence text is processed after the (given) premise texts have been processed. Elementary logic (i.e. first-order logic) can be used as a logical representation language for texts, but the notion of a information state (a set of possibilities — namely first-order models) is not available from the object language (belongs to the meta language). This means that texts with other texts as parts (e.g. propositional attitudes with embedded sentences) cannot be treated directly. Traditional intensional logics (i.e. modal logic) allow (via modal operators) access to the information states from the object language, but the access is limited and interference with (extensional) notions like (standard) identity, variables etc. is introduced. This does not mean that the ideas present in intensional logics will not work (possibly improved by adding a notion of partiality), but rather that often a formalisation in the simple type theory (with sorts for entities and indices making information states first class citizens — like individuals) is more comprehensible, flexible and logically well-behaved.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M J Cresswell</author>
</authors>
<title>Entities and Indices.</title>
<date>1990</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="11254" citStr="Cresswell (1990)" startWordPosition="1834" endWordPosition="1835">type theory are not necessary — or so-called general models are sufficient. The complication regarding variables mentioned above manifests itself in the way that 3- reduction does not hold for the A-calculus (again, see Muskens (1989) and references herein). Even more damaging: The (simply typed!) A-calculus is not Church-Rosser (due to the limited a-renaming capabilities of the modal operators). What seems needed is a logical representation language in which the information states are explicit manipulable, like the individuals in elementary logic. This point of view is forcefully defended by Cresswell (1990), where the possibilities of the information states are optimised using the wellknown technique of indexing. Hence we obtain an ontology of entities and indices. In recent papers we have presented and discussed a categorial grammar formalism capable of (in a strict compositional way) parsing and translating natural language texts, see Villadsen (1991a,b,c). The resulting formulas are terms in a many-sorted simple type theory. An example of a translation (simplified): Mary believes that John lies. (5) Ai .believe(i, Mary, (A j .lie(j , John))) (6) Adding partiality along the lines in Muskens (1</context>
</contexts>
<marker>Cresswell, 1990</marker>
<rawString>M. J. Cresswell (1990). Entities and Indices. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Groenendijk</author>
<author>M Stokhof</author>
</authors>
<title>Two Theories of Dynamic Semantics.</title>
<date>1991</date>
<booktitle>Logics in Al - 91, Amsterdam. Springer-Verlag (Lecture Notes in Computer Science 478).</booktitle>
<editor>In J. van Eijck, editor,</editor>
<contexts>
<context position="7227" citStr="Groenendijk and Stokhof (1991)" startWordPosition="1159" endWordPosition="1162">t of possibility and the empty set of texts represent the (absolute) initial information state. Other notions of consistency and initiality can be defined. A partial order on information states (&amp;quot;getting better informed&amp;quot;) is easy obtained. For the syntactical approach this is trivial — more texts make one better informed. For the semantical approach one could introduce previously eliminated possibilities in the information state, but we assume eliminative information state changes: r(l) C / for all / (this does not necessarily hold for nonmonotonic logics / belief revision / anaphora(?) — see Groenendijk and Stokhof (1991) for further details). Given the texts ti, , 4 the agent is asked whether a text t can be inferred; i.e. whether processing t after processing /1, ,i, would change the information state or not: /72 Here 7 is the identity function. ELEMENTARY LOGIC When elementary logic is used as logical representation language for texts, information states are identified with sets of models. Let the formulas 01, , q, 0 be the translations of the texts Ii,...t„, t. The information state when /1, ,4 has been processed is the set of all models in which 01, , 0„ are all true. ,i entails t if the model set corresp</context>
</contexts>
<marker>Groenendijk, Stokhof, 1991</marker>
<rawString>J. Groenendijk and M. Stokhof (1991). Two Theories of Dynamic Semantics. In J. van Eijck, editor, Logics in Al - 91, Amsterdam. Springer-Verlag (Lecture Notes in Computer Science 478).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Konolige</author>
</authors>
<title>A Deduction Model of Belief.</title>
<date>1986</date>
<publisher>Pitman.</publisher>
<contexts>
<context position="6387" citStr="Konolige (1986)" startWordPosition="1030" endWordPosition="1031">essed — hence nothing lost. Some improvements are possible (normalisation and the like). Since the texts are concrete objects they are easy to treat computationally. We call this approach the syntactical approach. An orthogonal approach (the semantical approach) identifies information states with sets of possibilities. This is the approach followed here. Note that a possibility need not be a so-called &amp;quot;possible world&amp;quot; — partiality and similar notions can be introduced, see Muskens (1989). A combination of the two approaches might be the optimal solution. Many of these aspects are discussed in Konolige (1986). Observe that the universal and empty sets are understood as opposites: the empty set of possibility and the universal set of texts represent the (absolute) inconsistent information state; and the universal set of possibility and the empty set of texts represent the (absolute) initial information state. Other notions of consistency and initiality can be defined. A partial order on information states (&amp;quot;getting better informed&amp;quot;) is easy obtained. For the syntactical approach this is trivial — more texts make one better informed. For the semantical approach one could introduce previously elimina</context>
</contexts>
<marker>Konolige, 1986</marker>
<rawString>K. Konolige (1986) A Deduction Model of Belief. Pitman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Muskens</author>
</authors>
<title>Meaning and Partiality.</title>
<date>1989</date>
<tech>PhD thesis,</tech>
<institution>University of Amsterdam.</institution>
<contexts>
<context position="6264" citStr="Muskens (1989)" startWordPosition="1010" endWordPosition="1011">te when the text ti is processed. An obvious approach is to identify information states with the set of texts already processed — hence nothing lost. Some improvements are possible (normalisation and the like). Since the texts are concrete objects they are easy to treat computationally. We call this approach the syntactical approach. An orthogonal approach (the semantical approach) identifies information states with sets of possibilities. This is the approach followed here. Note that a possibility need not be a so-called &amp;quot;possible world&amp;quot; — partiality and similar notions can be introduced, see Muskens (1989). A combination of the two approaches might be the optimal solution. Many of these aspects are discussed in Konolige (1986). Observe that the universal and empty sets are understood as opposites: the empty set of possibility and the universal set of texts represent the (absolute) inconsistent information state; and the universal set of possibility and the empty set of texts represent the (absolute) initial information state. Other notions of consistency and initiality can be defined. A partial order on information states (&amp;quot;getting better informed&amp;quot;) is easy obtained. For the syntactical approac</context>
<context position="10105" citStr="Muskens (1989)" startWordPosition="1660" endWordPosition="1661">e — compare combinatory logic and variable-free formulations of predicate logic). However, the information states as well as the possible worlds are never directly accessible from the object language. Another complication is that the iv function cannot be specified in the object language directly (although equivalent object language formulas can often be found — cf. the correspondence theory for modal logic). Perhaps the most annoying complication is the possible interference with (extensional) notions like (standard) identity, where Leibniz&apos;s Law fails (for non-modally closed formulas) — see Muskens (1989) for examples. If variables are present the inference rule of V-Introduction fails in a similar way. SIMPLE TYPE THEORY The above-mentioned complications becomes even more evident if elementary logic is replaced by a simple type theory while keeping the modal operators (cf. Montague&apos;s Intensional Logic). The Acalculus in the simple type theory allows for an elegant compositionality methodology (category to type correspondence over the two algebras). Often the higher-order logic (quantificational power) facilities of the simple type theory are not necessary — or so-called general models are suf</context>
</contexts>
<marker>Muskens, 1989</marker>
<rawString>R. Muskens (1989). Meaning and Partiality. PhD thesis, University of Amsterdam.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Villadsen</author>
</authors>
<title>(1991a). Combinatory Categorial Grammar for Intensional Fragment of Natural Language.</title>
<booktitle>Scandinavian Conference on Artificial Intelligence - 91,</booktitle>
<editor>In B. Mayoh, editor,</editor>
<publisher>IOS Press.</publisher>
<location>Roskilde.</location>
<marker>Villadsen, </marker>
<rawString>J. Villadsen (1991a). Combinatory Categorial Grammar for Intensional Fragment of Natural Language. In B. Mayoh, editor, Scandinavian Conference on Artificial Intelligence - 91, Roskilde. IOS Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Villadsen</author>
</authors>
<title>Categorial Grammar and Intensionality.</title>
<date>1991</date>
<booktitle>In Annual Meeting of the Danish Association for Computational Linguistics - 91,</booktitle>
<institution>Department of Computational Linguistics, Arhus Business School.</institution>
<location>Aalborg.</location>
<marker>Villadsen, 1991</marker>
<rawString>J. Villadsen (1991b). Categorial Grammar and Intensionality. In Annual Meeting of the Danish Association for Computational Linguistics - 91, Aalborg. Department of Computational Linguistics, Arhus Business School.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Villadsen</author>
</authors>
<title>(1991c). Anaphora and Intensionality in Classical Logic.</title>
<booktitle>In Nordic Computational Linguistics Conference - 91,</booktitle>
<location>Bergen.</location>
<note>To appear.</note>
<marker>Villadsen, </marker>
<rawString>J. Villadsen (1991c). Anaphora and Intensionality in Classical Logic. In Nordic Computational Linguistics Conference - 91, Bergen. To appear.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>