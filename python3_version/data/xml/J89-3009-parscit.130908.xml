<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000048">
<bodyText confidence="0.9695845">
Book Reviews Medical Language Processing: Computer Management of Narrative Data
from many other writings on the subject, is its discus-
sion of the problem of evaluating MT systems. The
proposed methodology is decomposed into three dis-
tinct areas: (i) evaluation by the system&apos;s designer; (ii)
cost/benefit evaluation by the user; and (iii) linguistic
evaluation by the user. This delineation serves as the
framework for a more detailed and impressive, though
by no means final, study contained in Appendix A.
In the conclusion, Lehrberger and Bourbeau discuss
the feasibility of MT, its future prospects, and the
impact of evaluation methodology on those prospects.
To summarize: I thought that this book was very well
written and intended for the mature MT researcher. The
impact of the book would be even greater had it been
published earlier in the decade.
</bodyText>
<sectionHeader confidence="0.989429" genericHeader="references">
REFERENCES
</sectionHeader>
<bodyText confidence="0.660841230769231">
CMU-CBT 1989, KBMT-89, Technical Report, Carnegie Mellon
University, Center for Machine Translation.
Hutchins, W.J. 1986 Machine translation: Past, present, future.
Chichester, England: Ellis Horwood Limited.
Nirenburg, Sergei (ed.) 1987 Machine translation: Theoretical and
methodological issues. Cambridge, England: Cambridge Univer-
sity Press.
Rita McCarden is a Ph.D. candidate at the Computer Science
Department of the University of Maryland, Baltimore County.
Her interests include machine translation and natural language
generation. McCardell&apos;s address is: 111 Rodeo Circle, Balti-
more, MD 21220 E-mail: rita@nl.cs.cmu.edu or mccardel@
umbc3.umd.edu
</bodyText>
<sectionHeader confidence="0.9913085" genericHeader="method">
MEDICAL LANGUAGE PROCESSING: COMPUTER
MANAGEMENT OF NARRATIVE DATA
</sectionHeader>
<author confidence="0.43068">
Naomi Sager, Carol Friedman, and Margaret S.
</author>
<figure confidence="0.667955833333333">
Lyman
(New York University)
Reading, MA: Addison-Wesley, 1987, xiii + 348 pp.
ISBN 0-201-16810-3, $41.95 (hb)
Reviewed by
Nicoletta Calzolari
</figure>
<affiliation confidence="0.39908">
University of Pisa
</affiliation>
<bodyText confidence="0.997268526881721">
The book under review builds on and is an extension of
the New York University Linguistic String Project
system applied to medical language processing. The
system analyzes free text and converts the information
&apos;hidden&apos; in it, the syntactic and semantic regularities,
into an informationally equivalent structured form,
which is best suited for information retrieval and auto-
matic summarization. From the computational linguis-
tics point of view, the main interesting results consist on
the one hand of the demonstration that a real world text
processing application of linguistic analysis is possible
(i.e., the processing of real textual input), and on the
other hand in the fact that the methodology and the
techniques used here and described for medical Ian-
guage are by and large also applicable to other, com-
pletely different, environments. The work also has links
to knowledge representation, given that a method for
representing and processing semantic information is
provided, and the data supplied could be a testbed for
knowledge-based systems.
In Chapter 1 a general overview is given of the
problems, the methodology, and the theoretical support
involved in processing natural language and sublan-
guage in particular. It is by syntactic clues that a set of
semantic statement types are individuated, and there-
fore semantic results are achieved, but the main meth-
ods of analysis are dictionary look-up and pattern
matching.
Chapter 2 is of less relevance for linguistics; it is
mainly concerned with the medical aspects of the proj-
ect, and with its practical applications purely from the
physician&apos;s point of view.
Chapter 3 describes the types of information struc-
tures that are typical of the sublanguage of medical
narrative and the way in which they are mapped into
computer representations, i.e., rather simple informa-
tion formats. Grammatical paraphrase, deletion of re-
dundant words, and regularization procedures are some
of the main procedures used to obtain the information
formats from the surface grammatical structure. These
format structures, although resembling `classical&apos;
frames, are specifically designed to &amp;quot;reflect the linguis-
tic regularities observed in sublanguage texts, and
therefore differ from most uses of frames in artificial
intelligence applications.&amp;quot; Whether they really differ is
perhaps questionable: they both try to capture similar
types of regularities and formalize underlying grammat-
ical relations into predicational structures. In my opin-
ion, the real difference is in their suitability as to their
application to (and empirical derivation from) real texts.
It is interesting to learn that only six types of infor-
mation formats, plus seven types of modifier formats,
are sufficient for representing information in clinical
narrative texts. How many would be necessary if deal-
ing with other types of sublanguage texts? How many
for general language? An evaluation of them in other
fields and a comparison would be interesting.
Chapter 4 describes how the system uses lists of
sublanguage word subclasses, with constraints on the
syntactic relations occurring between them, in order to
accomplish some linguistic tasks, e.g., to rule out
inappropriate prepositional phrase attachments (a typi-
cal problem unsolved with pure syntactic analysis) and
to select the attachments permitted in the domain. The
same method, i.e., checking against a list of well-formed
word class patterns, is used for homograph disambigu-
ation. An essential tool is therefore the possibility of
classifying lexical entries into a well-defined set of
semantic word classes, for which it is possible to state a
number of syntactic and semantic properties in the
sublanguage being analyzed. These entries do all the
work. This approach, which gives good results, is of
Computational Linguistics, Volume 15, Number 3, September 1989 195
Book Reviews Medical Language Processing: Computer Management of Narrative Data
interest to me because it stresses the importance of the
lexicon in NLP systems and also encourages lexical
analysis in general language. Many parts and compo-
nents of the system are in fact essentially lexically
driven.
Another typical problem, the analysis of conjunc-
tions, is handled with far better results by means of
sublanguage selection: conjunction is restricted only to
members of the same or similar subclasses (i.e., belong-
ing to the same equivalence class). The list of word
classes is also used in computing the semantic subclass
of a phrase from the subclasses of the head noun and the
adjunct.
Procedures that use this classification in lists of
sublanguage word classes occurring in particular syn-
tactic relations (i.e., sublanguage co-occurrence pat-
terns) not only structurally improve the parse tree, but
also allow adding to the parse tree semantic information
that:
</bodyText>
<listItem confidence="0.992521428571428">
1. provides semantic characterization to the words
and phrases;
2. solves word sense and syntactic ambiguity (for
adjuncts);
3. identifies combinations of basic statement types;
4. determines the underlying structure of compound
noun phrases.
</listItem>
<bodyText confidence="0.998859384615384">
All these are advantages of dealing with a sublan-
guage. We must in fact say that these patterns are highly
dependent on the particular domain, so that the lists
would obviously have to be redefined for another sub-
language domain, but the same procedure can still be
used. All these pattern-matching procedures, and the
transformational machinery, make it possible to reduce
the various linguistic means of expressing the medical
information to standard forms.
A crucial step in ensuring the quality of the system to
be designed is obviously a preliminary linguistic analy-
sis of sample sublanguage documents. The main results
of this analysis must be to determine:
</bodyText>
<listItem confidence="0.982374333333333">
1. the special well-formed syntactic structures in the
domain;
2. the specialized word subclasses;
3. the basic statement types, based on recurring
syntactic patterns of sublanguage word class co-
occurrence.
</listItem>
<bodyText confidence="0.996018394366197">
All these must be stated in relation to general standard
English.
The overall system in fact is made up of components
(a processor and a grammar) for parsing and regulariz-
ing general English, which form the shell of the system,
and other modules that are sublanguage specific. How-
ever, the latter procedures are ultimately highly porta-
ble and applicable to other domains, since their con-
straints are mainly list-driven. This is a good feature of
the system, also due to its modularization.
In Chapter 5, various experiments using existing
database management systems are described, showing
both advantages and disadvantages. But too many de-
tails of the implementation—the actual organization,
storing, and content (in terms of types of data) of the
tables of the database and the query types and proce-
dures to implement retrievals—would be more interest-
ing for database people than for computational linguists,
and would seem more appropriate for a manual for the
system user than for a scientific book dealing with a
research program and results.
The final output form is a tabular parenthesized
structure which, &amp;quot;although structurally flat, represents
the complete hierarchical linguistic structure of the
original narrative,&amp;quot; and is suitable for successive que-
rying and retrieval in a database of relational type.
Chapter 6 describes the dictionary as a very impor-
tant component of the system. It is rather large com-
pared to the lexicons of usual CL systems: about 12,000
entries of common English words, subclassified for
approximately 150 syntactic properties. A subset, i.e.,
the specialized medical dictionary, contains additional
subcategorization, and is subdivided into 45 medical
sublanguage classes, which ultimately allow correct
syntactic analysis, determine the correct semantic state-
ment type, and map the words into the correct format
slots. Incorrect parses are ruled out after testing for
syntactic and semantic compatibility of co-occurring
text words. Very useful to the processing is the classi-
fication of verbs in terms of noun subclasses disallowed
as subjects and/or objects, and of permissible or admis-
sible object strings. This &amp;quot;makes it unnecessary for the
program to compute all the possible object options (66
in the English grammar) for every verb.&amp;quot;
&amp;quot;The medical subclasses are derived by examining
the cooccurrence patterns in medical documents,&amp;quot; a
method that has proved useful by analyzing a number of
sublanguages in order to extract word classes for struc-
turing the textual information. This same method of
analyzing distributional similarity of words in particular
syntactic relations has proved advantageous in a differ-
ent domain, i.e., in the analysis of standard dictionary
definitions, with the purpose of formalizing their seman-
tic informational content; the difference is that in these
research projects the analysis is not manual, but is done
mainly in a semi-automatic way. The usefulness and
cost-effectiveness of applying similar methodologies to
general English should be investigated and evaluated by
the analysis of a very large corpus of texts.
Chapter 7 deals with the creation of the dictionary. It
has been prepared manually because of the unsuitability
of the information contained in large dictionaries in
machine-readable form, judged not sufficiently detailed
for processing sublanguage documents. However, a
fully automatic dictionary coding procedure for lexical
entries has been designed and implemented, taking
advantage of the fact that in the medical vocabulary
there is a substantial number of words of Greek and
Latin extraction that exhibit a very high degree of
morphological regularity connected to a high degree of
semantic regularity, i.e., words that are morphologi-
</bodyText>
<page confidence="0.933194">
196 Computational Linguistics, Volume 15, Number 3, September 1989
</page>
<bodyText confidence="0.992410417721519">
Book Reviews Medical Language Processing: Computer Management of Narrative Data
cally and semantically transparent. This application of
morphosemantic analysis to the creation of fully speci-
fied entries is certainly peculiar and best suited to a
sublanguage, where complex words are often perfectly
compositional and therefore the category and subclass
can be inferred from the constituents; but it can find
application also in general English. The description of
the morphology program in all its steps is perhaps too
detailed and rather obvious, being a rather simple
scanning strategy looking for a match in both the suffix
and prefix dictionaries. And the results do not seem
extraordinary.
Chapter 8 deals with the parsing algorithm, top-
down, syntax-driven; the syntactic structures are spec-
ified in the grammar in BN formulas, divided into types
based on linguistic string analysis (Harris 1964). The
parser is thus enabled to &amp;quot;use a small set of linguisti-
cally motivated procedures.&amp;quot; The computer grammar of
English has been adapted to the clinical sublanguage by
means of the addition of new options and deletion of
rare or unlikely usages in medical narrative.
Chapter 9 deals with some interesting issues in proc-
essing temporal information, using linguistic clues as
adverbial expressions, verb tenses, coordinate and sub-
ordinate conjunctions, and &amp;quot;narrative time progres-
sion,&amp;quot; i.e., the implicit temporal ordering of events
suggested by their sequencing in the text. The result is
a directed acyclic graph that represents the partial order
of events that can be obtained from the text. The
transitive closure of this graph adds further information,
and more time relationships than those explicit in the
text can be inferred. &amp;quot;The time graph generated can be
used in conjunction with the database to answer time-
related queries.&amp;quot;
Chapter 10 has as its main topic the sublanguage
grammar, whose main characteristic is to capture the
typical co-occurrence restrictions. The patterns of word
co-occurrence provide a set of semantic structures for
representing subfield information. Everything in the
sublanguage-grammar hypothesis is based on the appli-
cation of the methods of descriptive linguistics to a
corpus of texts in the selected subfield of science, thus
establishing word classes on the basis of co-occurrence
similarity. The extracted word classes have been found
to correlate with recognizable semantic classes, and a
clustering program obtained the main semantic classes
only from the syntactically analyzed sentences. The
grammatical structure of the sublanguage can be pre-
sented as a prototype sentence form, and the hierarchi-
cal organization of a sentence can be displayed in
flattened form. &amp;quot;The overall structure of the format is
given by English grammar,&amp;quot; while particular sublan-
guage word classes in certain positions characterize
particular subtypes of sentences with a particular se-
mantic character, determining the sublanguage gram-
matical relations.
One of the characteristics of a study of this type is
that although each text in a single specialized discipline
&amp;quot;brings in some new features,&amp;quot; these texts are &amp;quot;suffi-
ciently similar so as to fit into an overall structural
characterization. The repeated structures, implemented
as information formats, are then a powerful tool for
organizing the information in subfield texts.&amp;quot;
There are in this section a number of repetitions and
also some too-obvious considerations.
Chapter 11 again describes in detail, in other types of
scientific texts, the technique of sublanguage analysis
(performed manually). Once again the important step is
the &amp;quot;establishment of equivalence classes of words
based on equivalence of word environments (Harris
1963, p. 8)&amp;quot;, after a partial normalization (by transfor-
mational decomposition) of the text, consisting of an
operator-argument analysis of the sentences (Harris
1982). This part is too repetitive; the same concept is
repeated again and again.
After storing the results in the database, queries can,
be mapped onto the set of sublanguage formats. The
database is searched for sentences:
</bodyText>
<listItem confidence="0.987431">
1. matching sentence types, or
2. matching members of the word categories.
</listItem>
<bodyText confidence="0.97854866101695">
In this way queries can be answered on a much more
detailed level than that afforded by existing information
retrieval systems (with simple keyword-identified mate-
rial), given that it allows asking for a specific piece of
information in the form of a relation between rather
complex facts.
Appendix A would really seem more suitable for a
system manual than for a book.
The book is addressed to the computational linguist.
The approach described successfully combines practice
with theory. Computational linguistics methods are not
so frequently applied with success to a real-world
problem that is not too narrowly restricted either in the
lexicon or in the grammar. That it is done here is very
positive. Even though the success is influenced by the
fact that it operates in a specific sublanguage, it deals
with the sublanguage in an extensive way without
imposing too-severe limitations: the vocabulary is quite
large, and the range of phenomena of language handled
is quite reasonable. Of less interest for the linguist are
the chapters or paragraphs where medical problems and
considerations are dealt with too extensively.
Another drawback is that, perhaps for reasons of
clarity, there is sometimes too much detail on too-
specific steps, or there are too many repetitions of
similar arguments in different chapters. This happens
also because the linguistic methodology applied is the
same to deal with different types of phenomena, e.g.,
for syntactic and semantic ones. The basic method is
pattern matching. For many of the pattern-matching
tasks, the program procedure is largely lists of patterns,
i.e., lexical items that fail to conform to rules. But the
overall value of the book outweighs the drawbacks of its
repetitiveness.
With reference to principles of system design, the
linguistic considerations of the problem are given prior-
Computational Linguistics, Volume 15, Number 3, September 1989 197
Book Reviews Information-based Syntax and Semantics. Vol 1: Fundamentals
ity over the technical or implementational point of view.
In this regard, it cannot be criticized as designed in a too
rigid way from the implementational viewpoint and not
adaptable to new situations and unforeseen phenomena.
The separation of data structures from the procedures
and the modularity of the system are features that are
essential to the extendability of the system to other
domains.
In general, the work is a good example of:
1. the necessity of creating extensive lexicons,
where &amp;quot;extensive&amp;quot; must be intended both in
breadth (i.e., in quantitative terms) and in depth
(i.e., from a qualitative viewpoint, as to the types
of information associated with the entries);
2. the necessity of working with large textual cor-
pora, both for obtaining linguistic data and for
testing systems.
This is encouraging for a trend that is in recent years
showing up, and having, for example, in Europe, great
success also in projects sponsored by national and
international organizations.
</bodyText>
<sectionHeader confidence="0.992549" genericHeader="method">
REFERENCES
</sectionHeader>
<bodyText confidence="0.95074075">
Harris, Z.S. 1963 Discourse analysis reprints. The Hague: Mouton.
Harris, Z.S. 1964 String analysis in sentence structure. The Hague:
Mouton.
Harris, Z.S. 1982 A grammar of English on mathematical principles.
</bodyText>
<subsectionHeader confidence="0.93948">
New York: Wiley-Interscience.
</subsectionHeader>
<bodyText confidence="0.924815142857143">
Nicoletta Calzolari is a researcher at the Department of
Linguistics of the University of Pisa and at the Institute of
Computational Linguistics of CNR, Pisa. Her main research
areas are in the field of computational lexicography and
lexicology. Calzolari&apos;s address is: Dipartimento di Linguis-
tica, Universita di Pisa, Via S. Maria 36, I 56100 Pisa, Italy.
E-mail: glottolo@icnucevm.bitnet
</bodyText>
<sectionHeader confidence="0.9984145" genericHeader="method">
INFORMATION-BASED SYNTAX AND SEMANTICS. VOL 1:
FUNDAMENTALS
</sectionHeader>
<subsectionHeader confidence="0.869242">
Carl Pollard and Ivan A. Sag
</subsectionHeader>
<bodyText confidence="0.973515">
(Carnegie Mellon University and Stanford University,
resp.)
Stanford: Center for the Study of Language and
Information, Stanford University, 1987, x + 227
pp.
(CSLI lecture notes no. 13)
Distributed by the University of Chicago Press
ISBN 0-937073-23-7, $39.95 (hb); ISBN 0-937073-24-5,
$17.95 (sb)
</bodyText>
<figure confidence="0.290082">
Reviewed by
Edward P. Stabler, Jr.
</figure>
<affiliation confidence="0.522017">
University of Western Ontario
</affiliation>
<bodyText confidence="0.999944532258065">
This book is an introductory text in linguistics, a very
pleasant and readable introduction to head-driven
phrase structure grammar (HPSG). HPSG will be of
particular interest to computational linguists who have
wanted to see situation semantics integrated with a
unification-based phrase structure syntax. However,
computational linguists should be warned, in the first
place, that the book is truly introductory, focusing on
the preliminaries to a sophisticated account of the
language. Many of the serious problems to be faced are
not discussed at all. In other places good, hard problems
are posed, only to reward the reader&apos;s anticipation of a
resolution with a promissory note about the forthcom-
ing Volume 2. There are so many promissory notes at
crucial places that it becomes clear that Volume 2 will
be the real test of the framework. The two volumes are
apparently organized not by topic, but by difficulty. All
the difficult material on a whole range of topics—
syntactic and semantic—is left for the second volume.
The second warning for readers of this journal is that
this book does not consider the computational proper-
ties of HPSG at all. No standard characterization of the
HPSG-definable languages, no algorithm for unification
or for parsing, and no complexity results are presented.
As one expects in all but the most superficial or artificial
approaches to human language, the grammar is incom-
plete in both its universal and its language-specific
components. The grammatical principles, rules, and
lexical entries are feature-based, where feature values
can be complex (i.e., lists or sets), and computationally
oriented readers might wonder how many features are
needed and whether the set of possible values of syn-
tactic features is finite, but we are not told.
Pollard and Sag describe HPSG as a &amp;quot;synthetic and
eclectic&amp;quot; theory that draws on the insights of GPSG,
LFG, GB, FUG, categorial grammar, situation seman-
tics, and other approaches to language, which makes
the title rather puzzling. How is HPSG &amp;quot;information-
based&amp;quot;? Even when acquainted with the contents of
this volume, I was still puzzled: I am not attuned to the
Californian sense of &amp;quot;information.&amp;quot; HPSG is &amp;quot;an infor-
mation-based (or unification-based) theory of lan-
guage,- and it fundamentally regards &amp;quot;the objects that
make up a human language as bearers of information
within the community of people who know how to use
them.- To call HPSG information-based for both rea-
sons, because it unifies the partial information struc-
tures called features and because utterances bear infor-
mation, strikes me as a pun. But if smoke meaning fire
is very much the same as fire meaning fire, then it is no
doubt natural to think that fire meaning fire is quite a lot
like a feature&apos;s being a partial specification, an element
of a meet semilattice under subsumption where unifica-
tion corresponds to the greatest lower bound.
HPSG includes a rather complex array of different
kinds of propositions. We are given, in the first place,
some basic facts about what types of linguistic objects
there are. For example, there are two mutually exclu-
sive types of signs, lexical and phrasal. We are told that
&amp;quot;in general, such facts about relationships among types
of linguistic objects are obvious, and we will not explic-
itly state them,&amp;quot; but then in later pages we are infor-
</bodyText>
<page confidence="0.95353">
198 Computational Linguistics, Volume 15, Number 3, September 1989
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000792">
<title confidence="0.987737">Reviews Language Processing: Computer Management of Narrative Data</title>
<abstract confidence="0.968579133333333">from many other writings on the subject, is its discussion of the problem of evaluating MT systems. The proposed methodology is decomposed into three distinct areas: (i) evaluation by the system&apos;s designer; (ii) cost/benefit evaluation by the user; and (iii) linguistic evaluation by the user. This delineation serves as the framework for a more detailed and impressive, though by no means final, study contained in Appendix A. In the conclusion, Lehrberger and Bourbeau discuss the feasibility of MT, its future prospects, and the impact of evaluation methodology on those prospects. To summarize: I thought that this book was very well written and intended for the mature MT researcher. The impact of the book would be even greater had it been published earlier in the decade.</abstract>
<note confidence="0.936020785714286">REFERENCES CMU-CBT 1989, KBMT-89, Technical Report, Carnegie Mellon University, Center for Machine Translation. W.J. 1986 translation: Past, present, future. Chichester, England: Ellis Horwood Limited. Sergei (ed.) 1987 translation: Theoretical and issues. England: Cambridge University Press. McCarden is Ph.D. candidate at the Computer Science Department of the University of Maryland, Baltimore County. Her interests include machine translation and natural language generation. McCardell&apos;s address is: 111 Rodeo Circle, Baltimore, MD 21220 E-mail: rita@nl.cs.cmu.edu or mccardel@ umbc3.umd.edu</note>
<title confidence="0.581894">MEDICAL LANGUAGE PROCESSING: COMPUTER MANAGEMENT OF NARRATIVE DATA</title>
<author confidence="0.635477">Naomi Sager</author>
<author confidence="0.635477">Carol Friedman</author>
<author confidence="0.635477">Margaret S Lyman</author>
<affiliation confidence="0.628028">(New York University)</affiliation>
<address confidence="0.416364">Reading, MA: Addison-Wesley, 1987, xiii + 348 pp.</address>
<note confidence="0.971801">ISBN 0-201-16810-3, $41.95 (hb) Reviewed by</note>
<author confidence="0.996335">Nicoletta Calzolari</author>
<affiliation confidence="0.995722">University of Pisa</affiliation>
<abstract confidence="0.997380039274925">The book under review builds on and is an extension of the New York University Linguistic String Project system applied to medical language processing. The system analyzes free text and converts the information &apos;hidden&apos; in it, the syntactic and semantic regularities, into an informationally equivalent structured form, which is best suited for information retrieval and automatic summarization. From the computational linguistics point of view, the main interesting results consist on the one hand of the demonstration that a real world text processing application of linguistic analysis is possible (i.e., the processing of real textual input), and on the other hand in the fact that the methodology and the used here and described for medical Ianguage are by and large also applicable to other, completely different, environments. The work also has links to knowledge representation, given that a method for representing and processing semantic information is provided, and the data supplied could be a testbed for knowledge-based systems. In Chapter 1 a general overview is given of the problems, the methodology, and the theoretical support involved in processing natural language and sublanguage in particular. It is by syntactic clues that a set of semantic statement types are individuated, and therefore semantic results are achieved, but the main methods of analysis are dictionary look-up and pattern matching. Chapter 2 is of less relevance for linguistics; it is mainly concerned with the medical aspects of the project, and with its practical applications purely from the physician&apos;s point of view. Chapter 3 describes the types of information structures that are typical of the sublanguage of medical narrative and the way in which they are mapped into computer representations, i.e., rather simple information formats. Grammatical paraphrase, deletion of redundant words, and regularization procedures are some of the main procedures used to obtain the information formats from the surface grammatical structure. These format structures, although resembling `classical&apos; frames, are specifically designed to &amp;quot;reflect the linguistic regularities observed in sublanguage texts, and therefore differ from most uses of frames in artificial intelligence applications.&amp;quot; Whether they really differ is perhaps questionable: they both try to capture similar types of regularities and formalize underlying grammatical relations into predicational structures. In my opinion, the real difference is in their suitability as to their application to (and empirical derivation from) real texts. It is interesting to learn that only six types of information formats, plus seven types of modifier formats, are sufficient for representing information in clinical narrative texts. How many would be necessary if dealing with other types of sublanguage texts? How many for general language? An evaluation of them in other fields and a comparison would be interesting. Chapter 4 describes how the system uses lists of sublanguage word subclasses, with constraints on the syntactic relations occurring between them, in order to accomplish some linguistic tasks, e.g., to rule out inappropriate prepositional phrase attachments (a typical problem unsolved with pure syntactic analysis) and to select the attachments permitted in the domain. The same method, i.e., checking against a list of well-formed word class patterns, is used for homograph disambiguation. An essential tool is therefore the possibility of classifying lexical entries into a well-defined set of semantic word classes, for which it is possible to state a number of syntactic and semantic properties in the sublanguage being analyzed. These entries do all the work. This approach, which gives good results, is of Computational Linguistics, Volume 15, Number 3, September 1989 195 Book Reviews Medical Language Processing: Computer Management of Narrative Data interest to me because it stresses the importance of the lexicon in NLP systems and also encourages lexical analysis in general language. Many parts and components of the system are in fact essentially lexically driven. Another typical problem, the analysis of conjunctions, is handled with far better results by means of sublanguage selection: conjunction is restricted only to members of the same or similar subclasses (i.e., belonging to the same equivalence class). The list of word classes is also used in computing the semantic subclass of a phrase from the subclasses of the head noun and the adjunct. Procedures that use this classification in lists of sublanguage word classes occurring in particular syntactic relations (i.e., sublanguage co-occurrence patterns) not only structurally improve the parse tree, but also allow adding to the parse tree semantic information that: 1. provides semantic characterization to the words and phrases; 2. solves word sense and syntactic ambiguity (for adjuncts); 3. identifies combinations of basic statement types; 4. determines the underlying structure of compound noun phrases. All these are advantages of dealing with a sublanguage. We must in fact say that these patterns are highly dependent on the particular domain, so that the lists would obviously have to be redefined for another sublanguage domain, but the same procedure can still be used. All these pattern-matching procedures, and the transformational machinery, make it possible to reduce the various linguistic means of expressing the medical information to standard forms. A crucial step in ensuring the quality of the system to be designed is obviously a preliminary linguistic analysis of sample sublanguage documents. The main results of this analysis must be to determine: 1. the special well-formed syntactic structures in the domain; 2. the specialized word subclasses; 3. the basic statement types, based on recurring syntactic patterns of sublanguage word class cooccurrence. All these must be stated in relation to general standard English. The overall system in fact is made up of components (a processor and a grammar) for parsing and regularizing general English, which form the shell of the system, and other modules that are sublanguage specific. However, the latter procedures are ultimately highly portable and applicable to other domains, since their constraints are mainly list-driven. This is a good feature of the system, also due to its modularization. In Chapter 5, various experiments using existing database management systems are described, showing both advantages and disadvantages. But too many details of the implementation—the actual organization, storing, and content (in terms of types of data) of the tables of the database and the query types and procedures to implement retrievals—would be more interesting for database people than for computational linguists, and would seem more appropriate for a manual for the system user than for a scientific book dealing with a research program and results. The final output form is a tabular parenthesized structure which, &amp;quot;although structurally flat, represents the complete hierarchical linguistic structure of the original narrative,&amp;quot; and is suitable for successive querying and retrieval in a database of relational type. Chapter 6 describes the dictionary as a very imporcomponent of the system. It is rather large compared to the lexicons of usual CL systems: about 12,000 entries of common English words, subclassified for approximately 150 syntactic properties. A subset, i.e., the specialized medical dictionary, contains additional subcategorization, and is subdivided into 45 medical sublanguage classes, which ultimately allow correct analysis, determine the correct semantic statement type, and map the words into the correct format slots. Incorrect parses are ruled out after testing for syntactic and semantic compatibility of co-occurring text words. Very useful to the processing is the classification of verbs in terms of noun subclasses disallowed as subjects and/or objects, and of permissible or admissible object strings. This &amp;quot;makes it unnecessary for the program to compute all the possible object options (66 in the English grammar) for every verb.&amp;quot; &amp;quot;The medical subclasses are derived by examining the cooccurrence patterns in medical documents,&amp;quot; a method that has proved useful by analyzing a number of sublanguages in order to extract word classes for structuring the textual information. This same method of analyzing distributional similarity of words in particular syntactic relations has proved advantageous in a different domain, i.e., in the analysis of standard dictionary with the purpose of formalizing their semantic informational content; the difference is that in these research projects the analysis is not manual, but is done mainly in a semi-automatic way. The usefulness and cost-effectiveness of applying similar methodologies to general English should be investigated and evaluated by the analysis of a very large corpus of texts. Chapter 7 deals with the creation of the dictionary. It has been prepared manually because of the unsuitability of the information contained in large dictionaries in machine-readable form, judged not sufficiently detailed for processing sublanguage documents. However, a fully automatic dictionary coding procedure for lexical entries has been designed and implemented, taking advantage of the fact that in the medical vocabulary there is a substantial number of words of Greek and Latin extraction that exhibit a very high degree of morphological regularity connected to a high degree of regularity, i.e., words that are morphologi- 196 Computational Linguistics, Volume 15, Number 3, September 1989 Book Reviews Medical Language Processing: Computer Management of Narrative Data cally and semantically transparent. This application of morphosemantic analysis to the creation of fully specified entries is certainly peculiar and best suited to a sublanguage, where complex words are often perfectly compositional and therefore the category and subclass can be inferred from the constituents; but it can find application also in general English. The description of the morphology program in all its steps is perhaps too detailed and rather obvious, being a rather simple scanning strategy looking for a match in both the suffix and prefix dictionaries. And the results do not seem extraordinary. Chapter 8 deals with the parsing algorithm, topsyntax-driven; the syntactic structures are specified in the grammar in BN formulas, divided into types based on linguistic string analysis (Harris 1964). The is thus enabled to &amp;quot;use a small set of linguistically motivated procedures.&amp;quot; The computer grammar of English has been adapted to the clinical sublanguage by means of the addition of new options and deletion of rare or unlikely usages in medical narrative. Chapter 9 deals with some interesting issues in processing temporal information, using linguistic clues as expressions, verb tenses, coordinate and subordinate conjunctions, and &amp;quot;narrative time progression,&amp;quot; i.e., the implicit temporal ordering of events suggested by their sequencing in the text. The result is a directed acyclic graph that represents the partial order of events that can be obtained from the text. The transitive closure of this graph adds further information, and more time relationships than those explicit in the text can be inferred. &amp;quot;The time graph generated can be used in conjunction with the database to answer timerelated queries.&amp;quot; Chapter 10 has as its main topic the sublanguage grammar, whose main characteristic is to capture the typical co-occurrence restrictions. The patterns of word co-occurrence provide a set of semantic structures for representing subfield information. Everything in the hypothesis is based on the application of the methods of descriptive linguistics to a corpus of texts in the selected subfield of science, thus establishing word classes on the basis of co-occurrence similarity. The extracted word classes have been found to correlate with recognizable semantic classes, and a clustering program obtained the main semantic classes only from the syntactically analyzed sentences. The structure of the sublanguage can be presented as a prototype sentence form, and the hierarchical organization of a sentence can be displayed in flattened form. &amp;quot;The overall structure of the format is given by English grammar,&amp;quot; while particular sublanguage word classes in certain positions characterize particular subtypes of sentences with a particular semantic character, determining the sublanguage grammatical relations. One of the characteristics of a study of this type is that although each text in a single specialized discipline &amp;quot;brings in some new features,&amp;quot; these texts are &amp;quot;sufficiently similar so as to fit into an overall structural characterization. The repeated structures, implemented as information formats, are then a powerful tool for organizing the information in subfield texts.&amp;quot; There are in this section a number of repetitions and also some too-obvious considerations. Chapter 11 again describes in detail, in other types of scientific texts, the technique of sublanguage analysis (performed manually). Once again the important step is the &amp;quot;establishment of equivalence classes of words based on equivalence of word environments (Harris 1963, p. 8)&amp;quot;, after a partial normalization (by transformational decomposition) of the text, consisting of an operator-argument analysis of the sentences (Harris 1982). This part is too repetitive; the same concept is repeated again and again. After storing the results in the database, queries can, be mapped onto the set of sublanguage formats. The database is searched for sentences: 1. matching sentence types, or 2. matching members of the word categories. In this way queries can be answered on a much more detailed level than that afforded by existing information retrieval systems (with simple keyword-identified material), given that it allows asking for a specific piece of information in the form of a relation between rather complex facts. Appendix A would really seem more suitable for a system manual than for a book. The book is addressed to the computational linguist. The approach described successfully combines practice with theory. Computational linguistics methods are not so frequently applied with success to a real-world problem that is not too narrowly restricted either in the lexicon or in the grammar. That it is done here is very positive. Even though the success is influenced by the fact that it operates in a specific sublanguage, it deals with the sublanguage in an extensive way without imposing too-severe limitations: the vocabulary is quite large, and the range of phenomena of language handled is quite reasonable. Of less interest for the linguist are the chapters or paragraphs where medical problems and considerations are dealt with too extensively. Another drawback is that, perhaps for reasons of clarity, there is sometimes too much detail on toospecific steps, or there are too many repetitions of similar arguments in different chapters. This happens also because the linguistic methodology applied is the same to deal with different types of phenomena, e.g., for syntactic and semantic ones. The basic method is pattern matching. For many of the pattern-matching tasks, the program procedure is largely lists of patterns, i.e., lexical items that fail to conform to rules. But the overall value of the book outweighs the drawbacks of its repetitiveness. With reference to principles of system design, the considerations of the problem are given prior- Computational Linguistics, Volume 15, Number 3, September 1989 197 Book Reviews Information-based Syntax and Semantics. Vol 1: Fundamentals ity over the technical or implementational point of view. regard, it cannot be criticized as designed in a too rigid way from the implementational viewpoint and not adaptable to new situations and unforeseen phenomena. The separation of data structures from the procedures and the modularity of the system are features that are essential to the extendability of the system to other domains. the work is a good example of: 1. the necessity of creating extensive lexicons, where &amp;quot;extensive&amp;quot; must be intended both in breadth (i.e., in quantitative terms) and in depth (i.e., from a qualitative viewpoint, as to the types of information associated with the entries); 2. the necessity of working with large textual corpora, both for obtaining linguistic data and for testing systems. This is encouraging for a trend that is in recent years showing up, and having, for example, in Europe, great success also in projects sponsored by national and international organizations.</abstract>
<note confidence="0.916601">REFERENCES Z.S. 1963 analysis reprints. Hague: Mouton. Z.S. 1964 analysis in sentence structure. Hague: Mouton. Z.S. 1982 A of English on mathematical principles. New York: Wiley-Interscience. Calzolari is researcher at the Department of</note>
<abstract confidence="0.838896">Linguistics of the University of Pisa and at the Institute of Computational Linguistics of CNR, Pisa. Her main research areas are in the field of computational lexicography and lexicology. Calzolari&apos;s address is: Dipartimento di Linguis-</abstract>
<address confidence="0.98242">tica, Universita di Pisa, Via S. Maria 36, I 56100 Pisa, Italy.</address>
<email confidence="0.996658">E-mail:glottolo@icnucevm.bitnet</email>
<note confidence="0.509414">INFORMATION-BASED SYNTAX AND SEMANTICS. VOL 1:</note>
<title confidence="0.975886">FUNDAMENTALS</title>
<author confidence="0.999952">Carl Pollard</author>
<author confidence="0.999952">Ivan A Sag</author>
<affiliation confidence="0.99965">Carnegie Mellon University and Stanford University,</affiliation>
<email confidence="0.751955">resp.)</email>
<address confidence="0.7057165">Stanford: Center for the Study of Language and Information, Stanford University, 1987, x + 227</address>
<email confidence="0.531203">pp.</email>
<note confidence="0.927252">(CSLI lecture notes no. 13) Distributed by the University of Chicago Press ISBN 0-937073-23-7, $39.95 (hb); ISBN 0-937073-24-5, $17.95 (sb) Reviewed by Jr. University of Western Ontario This book is an introductory text in linguistics, a very</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>