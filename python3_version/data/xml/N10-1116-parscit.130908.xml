<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000230">
<title confidence="0.993858">
From Baby Steps to Leapfrog: How “Less is More”
in Unsupervised Dependency Parsing*
</title>
<author confidence="0.988518">
Valentin I. Spitkovsky Hiyan Alshawi
</author>
<affiliation confidence="0.984472">
Stanford University and Google Inc. Google Inc., Mountain View, CA, 94043
</affiliation>
<email confidence="0.993853">
valentin@cs.stanford.edu hiyan@google.com
</email>
<author confidence="0.985948">
Daniel Jurafsky
</author>
<affiliation confidence="0.931509">
Stanford University, Stanford, CA, 94305
</affiliation>
<email confidence="0.998502">
jurafsky@stanford.edu
</email>
<sectionHeader confidence="0.993898" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999058">
We present three approaches for unsupervised
grammar induction that are sensitive to data
complexity and apply them to Klein and Man-
ning’s Dependency Model with Valence. The
first, Baby Steps, bootstraps itself via iterated
learning of increasingly longer sentences and
requires no initialization. This method sub-
stantially exceeds Klein and Manning’s pub-
lished scores and achieves 39.4% accuracy on
Section 23 (all sentences) of the Wall Street
Journal corpus. The second, Less is More,
uses a low-complexity subset of the avail-
able data: sentences up to length 15. Focus-
ing on fewer but simpler examples trades off
quantity against ambiguity; it attains 44.1%
accuracy, using the standard linguistically-
informed prior and batch training, beating
state-of-the-art. Leapfrog, our third heuristic,
combines Less is More with Baby Steps by
mixing their models of shorter sentences, then
rapidly ramping up exposure to the full train-
ing set, driving up accuracy to 45.0%. These
trends generalize to the Brown corpus; aware-
ness of data complexity may improve other
parsing models and unsupervised algorithms.
</bodyText>
<sectionHeader confidence="0.999334" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995128681818182">
Unsupervised learning of hierarchical syntactic
structure from free-form natural language text is a
hard problem whose eventual solution promises to
benefit applications ranging from question answer-
ing to speech recognition and machine translation.
A restricted version that targets dependencies and
*Partially funded by NSF award IIS-0811974; first author
supported by the Fannie &amp; John Hertz Foundation Fellowship.
assumes partial annotation, e.g., sentence bound-
aries, tokenization and typically even part-of-speech
(POS) tagging, has received much attention, elicit-
ing a diverse array of techniques (Smith and Eis-
ner, 2005; Seginer, 2007; Cohen et al., 2008). Klein
and Manning’s (2004) Dependency Model with Va-
lence (DMV) was the first to beat a simple parsing
heuristic — the right-branching baseline. Today’s
state-of-the-art systems (Headden et al., 2009; Co-
hen and Smith, 2009) are still rooted in the DMV.
Despite recent advances, unsupervised parsers lag
far behind their supervised counterparts. Although
large amounts of unlabeled data are known to im-
prove semi-supervised parsing (Suzuki et al., 2009),
the best unsupervised systems use less data than is
available for supervised training, relying on complex
models instead: Headden et al.’s (2009) Extended
Valence Grammar (EVG) combats data sparsity with
smoothing alone, training on the same small subset
of the tree-bank as the classic implementation of the
DMV; Cohen and Smith (2009) use more compli-
cated algorithms (variational EM and MBR decod-
ing) and stronger linguistic hints (tying related parts
of speech and syntactically similar bilingual data).
We explore what can be achieved through judi-
cious use of data and simple, scalable techniques.
Our first approach iterates over a series of training
sets that gradually increase in size and complex-
ity, forming an initialization-independent scaffold-
ing for learning a grammar. It works with Klein and
Manning’s simple model (the original DMV) and
training algorithm (classic EM) but eliminates their
crucial dependence on manually-tuned priors. The
second technique is consistent with the intuition that
learning is most successful within a band of the size-
complexity spectrum. Both could be applied to more
</bodyText>
<page confidence="0.973094">
751
</page>
<note confidence="0.752813">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 751–759,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.996414">
intricate models and advanced learning algorithms.
We combine them in a third, efficient hybrid method.
</bodyText>
<sectionHeader confidence="0.990782" genericHeader="introduction">
2 Intuition
</sectionHeader>
<bodyText confidence="0.999947146341463">
Focusing on simple examples helps guide unsuper-
vised learning,1 as blindly added confusing data can
easily mislead training. We suggest that unless it is
increased gradually, unbridled, complexity can over-
whelm a system. How to grade an example’s diffi-
culty? The cardinality of its solution space presents
a natural proxy. In the case of parsing, the num-
ber of possible syntactic trees grows exponentially
with sentence length. For longer sentences, the un-
supervised optimization problem becomes severely
under-constrained, whereas for shorter sentences,
learning is tightly reined in by data. In the extreme
case of a single-word sentence, there is no choice
but to parse it correctly. At two words, a raw 50%
chance of telling the head from its dependent is still
high, but as length increases, the accuracy of even
educated guessing rapidly plummets. In model re-
estimation, long sentences amplify ambiguity and
pollute fractional counts with noise. At times, batch
systems are better off using less data.
Baby Steps: Global non-convex optimization is
hard. We propose a meta-heuristic that takes the
guesswork out of initializing local search. Begin-
ning with an easy (convex) case, it slowly extends it
to the fully complex target task by taking tiny steps
in the problem space, trying not to stray far from
the relevant neighborhoods of the solution space. A
series of nested subsets of increasingly longer sen-
tences that culminates in the complete data set offers
a natural progression. Its base case — sentences of
length one — has a trivial solution that requires nei-
ther initialization nor search yet reveals something
of sentence heads. The next step — sentences of
length one and two — refines initial impressions
of heads, introduces dependents, and exposes their
identities and relative positions. Although not rep-
resentative of the full grammar, short sentences cap-
ture enough information to paint most of the picture
needed by slightly longer sentences. They set up an
easier, incremental subsequent learning task. Step
k + 1 augments training input to include lengths
</bodyText>
<footnote confidence="0.8592595">
1It mirrors the effect that boosting hard examples has for
supervised training (Freund and Schapire, 1997).
</footnote>
<bodyText confidence="0.99394975">
1, 2, ... , k, k + 1 of the full data set and executes
local search starting from the (smoothed) model es-
timated by step k. This truly is grammar induction.
Less is More: For standard batch training, just us-
ing simple, short sentences is not enough. They are
rare and do not reveal the full grammar. We find a
“sweet spot” — sentence lengths that are neither too
long (excluding the truly daunting examples) nor too
few (supplying enough accessible information), us-
ing Baby Steps’ learning curve as a guide. We train
where it flattens out, since remaining sentences con-
tribute little (incremental) educational value.2
Leapfrog: As an alternative to discarding data, a
better use of resources is to combine the results of
batch and iterative training up to the sweet spot data
gradation, then iterate with a large step size.
</bodyText>
<sectionHeader confidence="0.999965" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.978572193548387">
Two types of scaffolding for guiding language learn-
ing debuted in Elman’s (1993) experiments with
“starting small”: data complexity (restricting input)
and model complexity (restricting memory). In both
cases, gradually increasing complexity allowed ar-
tificial neural networks to master a pseudo-natural
grammar they otherwise failed to learn. Initially-
limited capacity resembled maturational changes in
working memory and attention span that occur over
time in children (Kail, 1984), in line with the “less
is more” proposal (Newport, 1988; 1990). Although
Rohde and Plaut (1999) failed to replicate this3 re-
sult with simple recurrent networks, other machine
learning techniques reliably benefit from scaffolded
model complexity on a variety of language tasks.
In word-alignment, Brown et al. (1993) used IBM
Models 1-4 as “stepping stones” to training Model 5.
Other prominent examples include “coarse-to-fine”
2This is akin to McClosky et al.’s (2006) “Goldilocks effect.”
3Worse, they found that limiting input hindered language
acquisition. And making the grammar more English-like (by
introducing and strengthening semantic constraints), increased
the already significant advantage for “starting large!” With it-
erative training invoking the optimizer multiple times, creating
extra opportunities to converge, Rohde and Plaut (1999) sus-
pected that Elman’s (1993) simulations simply did not allow
networks exposed exclusively to complex inputs sufficient train-
ing time. Our extremely generous, low termination threshold
for EM (see §5.1) addresses this concern. However, given the
DMV’s purely syntactic POS tag-based approach (see §5), it
would be prudent to re-test Baby Steps with a lexicalized model.
</bodyText>
<page confidence="0.996191">
752
</page>
<bodyText confidence="0.994107818181819">
approaches to parsing, translation and speech recog-
nition (Charniak and Johnson, 2005; Charniak et al.,
2006; Petrov et al., 2008; Petrov, 2009), and re-
cently unsupervised POS tagging (Ravi and Knight,
2009). Initial models tend to be particularly simple,4
and each refinement towards a full model introduces
only limited complexity, supporting incrementality.
Filtering complex data, the focus of our work,
is unconventional in natural language processing.
Such scaffolding qualifies as shaping — a method
of instruction (routinely exploited in animal train-
ing) in which the teacher decomposes a complete
task into sub-components, providing an easier path
to learning. When Skinner (1938) coined the term,
he described it as a “method of successive approx-
imations.” Ideas that gradually make a task more
difficult have been explored in robotics (typically,
for navigation), with reinforcement learning (Singh,
1992; Sanger, 1994; Saksida et al., 1997; Dorigo
and Colombetti, 1998; Savage, 1998; Savage, 2001).
Recently, Krueger and Dayan (2009) showed that
shaping speeds up language acquisition and leads
to better generalization in abstract neural networks.
Bengio et al. (2009) confirmed this for deep de-
terministic and stochastic networks, using simple
multi-stage curriculum strategies. They conjectured
that a well-chosen sequence of training criteria —
different sets of weights on the examples — could
act as a continuation method (Allgower and Georg,
1990), helping find better local optima for non-
convex objectives. Elman’s learners constrained the
peaky solution space by focusing on just the right
data (simple sentences that introduced basic repre-
sentational categories) at just the right time (early
on, when their plasticity was greatest). Self-shaping,
they simplified tasks through deliberate omission (or
misunderstanding). Analogously, Baby Steps in-
duces an early structural locality bias (Smith and
Eisner, 2006), then relaxes it, as if annealing (Smith
and Eisner, 2004). Its curriculum of binary weights
initially discards complex examples responsible for
“high-frequency noise,” with earlier, “smoothed”
objectives revealing more of the global picture.
There are important differences between our re-
sults and prior work. In contrast to Elman, we use a
4Brown et al.’s (1993) Model 1 (and, similarly, the first baby
step) has a global optimum that can be computed exactly, so that
no initial or subsequent parameters depend on initialization.
large data set (WSJ) of real English. Unlike Bengio
et al. and Krueger and Dayan, we shape a parser, not
a language model. Baby Steps is similar, in spirit, to
Smith and Eisner’s methods. Deterministic anneal-
ing (DA) shares nice properties with Baby Steps,
but performs worse than EM for (constituent) pars-
ing; Baby Steps handedly defeats standard training.
Structural annealing works well, but requires a hand-
tuned annealing schedule and direct manipulation of
the objective function; Baby Steps works “out of the
box,” its locality biases a natural consequence of a
complexity/data-guided tour of optimization prob-
lems. Skewed DA incorporates a good initializer
by interpolating between two probability distribu-
tions, whereas our hybrid, Leapfrog, admits multi-
ple initializers by mixing structures instead. “Less
is More” is novel and confirms the tacit consensus
implicit in training on small data sets (e.g., WSJ10).
</bodyText>
<sectionHeader confidence="0.86723" genericHeader="method">
4 Data Sets and Metrics
</sectionHeader>
<bodyText confidence="0.999976571428572">
Klein and Manning (2004) both trained and tested
the DMV on the same customized subset (WSJ10)
of Penn English Treebank’s Wall Street Journal por-
tion (Marcus et al., 1993). Its 49,208 annotated
parse trees were pruned5 down to 7,422 sentences
of at most 10 terminals, spanning 35 unique POS
tags. Following standard practice, automatic “head-
percolation” rules (Collins, 1999) were used to con-
vert the remaining trees into dependencies. Forced
to produce a single “best” parse, their algorithm
was judged on accuracy: its directed score was the
fraction of correct dependencies; a more flattering6
undirected score was also used. We employ the
same metrics, emphasizing directed scores, and gen-
eralize WSJk to be the subset of pre-processed sen-
tences with at most k terminals. Our experiments fo-
cus on k E 11, ... , 451, but we also test on WSJ100
and Section 23 of WSJ&apos; (the entire WSJ), as well as
the held-out Brown100 (similarly derived from the
Brown corpus (Francis and Kucera, 1979)). See Fig-
ure 1 for these corpora’s sentence and token counts.
</bodyText>
<footnote confidence="0.567829571428571">
5Stripped of all empty sub-trees, punctuation, and terminals
(tagged # and $) not pronounced where they appear, those sen-
tences still containing more than ten tokens were thrown out.
6Ignoring polarity of parent-child relations partially ob-
scured effects of alternate analyses (systematic choices between
modals and main verbs for heads of sentences, determiners for
noun phrases, etc.) and facilitated comparison with prior work.
</footnote>
<page confidence="0.987386">
753
</page>
<table confidence="0.943777076923077">
Corpus Sentences POS Tokens Corpus Sentences POS Tokens
WSJ1 159 159 WSJ13 12,270 110,760
WSJ2 499 839 WSJ14 14,095 136,310
WSJ3 876 1,970 WSJ15 15,922 163,715
WSJ4 1,394 4,042 WSJ20 25,523 336,555
WSJ5 2,008 7,112 WSJ25 34,431 540,895
WSJ6 2,745 11,534 WSJ30 41,227 730,099
WSJ7 3,623 17,680 WSJ35 45,191 860,053
WSJ8 4,730 26,536 WSJ40 47,385 942,801
WSJ9 5,938 37,408 WSJ45 48,418 986,830
WSJ10 7,422 52,248 WSJ100 49,206 1,028,054
WSJ11 8,856 68,022 Section 23 2,353 48,201
WSJ12 10,500 87,750 Brown100 24,208 391,796
</table>
<figure confidence="0.98391052173913">
WSJ� 5 10 15 20 25 30 35 40 45
45
40
35
30
25
20
15
10
5
Thousands
of Sentences
Thousands
of Tokens
900
800
600
500
400
300
200
700
100
</figure>
<figureCaption confidence="0.999988">
Figure 1: Sizes of WSJ11, ... , 45, 1001, Section 23 of WSJ&apos; and Brown100.
</figureCaption>
<equation confidence="0.9951642">
NNS VBD IN NN Q
Payrolls fell in September .
0
z } |{
P = (1 − PSTOP(0, L, T)) X PATTACH(0, L, VBD)
X (1 − PSTOP(VBD, L, T)) X PATTACH(VBD, L, NNS)
X (1 − PSTOP(VBD, R, T)) X PATTACH(VBD, R, IN)
X (1 − PSTOP(IN, R, T)) X PATTACH(IN, R, NN)
X PSTOP(VBD, L, F) X PSTOP(VBD, R, F)
X PSTOP(NNS, L, T) X PSTOP(NNS, R, T)
X PSTOP(IN, L, T) X PSTOP(IN, R, F)
X PSTOP(NN, L, T) X PSTOP(NN, R, T)
X PSTOP(0, L, F)
 |{z }
1 1
</equation>
<figureCaption confidence="0.9863985">
Figure 2: A simple dependency structure for a short sen-
tence and its probability, as factored by the DMV.
</figureCaption>
<sectionHeader confidence="0.895249" genericHeader="method">
5 New Algorithms for the Classic Model
</sectionHeader>
<bodyText confidence="0.999315333333333">
The DMV (Klein and Manning, 2004) is a single-
state head automata model (Alshawi, 1996) over lex-
ical word classes {cw} — POS tags. Its generative
story for a sub-tree rooted at a head (of class ch) rests
on three types of independent decisions: (i) initial
direction dir E {L, R} in which to attach children, via
probability PORDER(ch); (ii) whether to seal dir, stop-
ping with probability PSTOP(ch, dir, adj), conditioned
on adj E {T, F} (true iff considering dir’s first, i.e.,
adjacent, child); and (iii) attachments (of class ca),
according to PATTACH(ch, dir, ca). This produces only
projective trees.7 A root token Q generates the head
of a sentence as its left (and only) child. Figure 2
displays an example that ignores (sums out) PORDER.
The DMV lends itself to unsupervised learn-
</bodyText>
<footnote confidence="0.9146875">
7Unlike spanning tree algorithms (McDonald et al., 2005),
DMV’s chart-based method disallows crossing dependencies.
</footnote>
<bodyText confidence="0.998003111111111">
ing via inside-outside re-estimation (Baker, 1979).
Klein and Manning did not use smoothing and
started with an “ad-hoc harmonic” completion: aim-
ing for balanced trees, non-root heads attached de-
pendents in inverse proportion to (a constant plus)
their distance; Q generated heads uniformly at ran-
dom. This non-distributional heuristic created favor-
able initial conditions that nudged EM towards typi-
cal linguistic dependency structures.
</bodyText>
<subsectionHeader confidence="0.915361">
5.1 Algorithm #0: Ad-Hoc*
</subsectionHeader>
<bodyText confidence="0.986696">
— A Variation on Original Ad-Hoc Initialization
Since some of the important implementation details
are not available in the literature (Klein and Man-
ning, 2004; Klein, 2005), we had to improvise ini-
tialization and terminating conditions. We suspect
that our choices throughout this section do not match
Klein and Manning’s actual training of the DMV.
We use the following ad-hoc harmonic scores (for
all tokens other than Q): 1�ORDER = 1/2;
</bodyText>
<equation confidence="0.968583">
PSTOP = (ds + 6s)−1 = (ds + 3)−1, ds &gt; 0;
�PATTACH = (da + 6a)−1 = (da + 2)−1, da &gt; 1.
</equation>
<bodyText confidence="0.980683875">
Integers d1s,a} are distances from heads to stopping
boundaries and dependents.8 We initialize train-
ing by producing best-scoring parses of all input
sentences and converting them into proper proba-
bility distributions PSTOP and PATTACH via maximum-
likelihood estimation (a single step of Viterbi train-
ing (Brown et al., 1993)). Since left and right chil-
dren are independent, we drop PORDER altogether, mak-
</bodyText>
<footnote confidence="0.962171">
8Constants 61s,a} come from personal communication.
Note that 6s is one higher than is strictly necessary to avoid both
division by zero and determinism; 6a could have been safely ze-
roed out, since we never compute 1 − PATTACH (see Figure 2).
</footnote>
<equation confidence="0.746084">
X PSTOP(0, R, T)
 |{z }
�
</equation>
<page confidence="0.988742">
754
</page>
<bodyText confidence="0.9998145">
ing “headedness” deterministic. Our parser care-
fully randomizes tie-breaking, so that all parse trees
having the same score get an equal shot at being
selected (both during initialization and evaluation).
We terminate EM when a successive change in over-
all per-token cross-entropy drops below 2−20 bits.
</bodyText>
<subsectionHeader confidence="0.931561">
5.2 Algorithm #1: Baby Steps
</subsectionHeader>
<bodyText confidence="0.953788">
— An Initialization-Independent Scaffolding
We eliminate the need for initialization by first train-
ing on a trivial subset of the data — WSJ1; this
works, since there is only one (the correct) way to
parse a single-token sentence. We plug the resulting
model into training on WSJ2 (sentences of up to two
tokens), and so forth, building up to WSJ45.9 This
algorithm is otherwise identical to Ad-Hoc*, with
the exception that it re-estimates each model using
Laplace smoothing, so that earlier solutions could
be passed to next levels, which sometimes contain
previously unseen dependent and head POS tags.
</bodyText>
<subsectionHeader confidence="0.903265">
5.3 Algorithm #2: Less is More
</subsectionHeader>
<bodyText confidence="0.975936">
— Ad-Hoc* where Baby Steps Flatlines
We jettison long, complex sentences and deploy Ad-
Hoc*’s initializer and batch training at WSJk* — an
estimate of the sweet spot data gradation. To find
it, we track Baby Steps’ successive models’ cross-
entropies on the complete data set, WSJ45. An ini-
tial segment of rapid improvement is separated from
the final region of convergence by a knee — points
of maximum curvature (see Figure 3). We use an
improved10 L method (Salvador and Chan, 2004) to
automatically locate this area of diminishing returns.
Specifically, we determine its end-points [k0, k*] by
minimizing squared error, estimating �k0 = 7 and
k* = 15. Training at WSJ15 just misses the plateau.
</bodyText>
<subsectionHeader confidence="0.741508">
5.4 Algorithm #3: Leapfrog
— A Practical and Efficient Hybrid Mixture
</subsectionHeader>
<bodyText confidence="0.7987115">
Cherry-picking the best features of “Less is More”
and Baby Steps, we begin by combining their mod-
9Its 48,418 sentences (see Figure 1) cover 94.4% of all sen-
tences in WSJ; the longest of the missing 790 has length 171.
10Instead of iteratively fitting a two-segment form and adap-
tively discarding its tail, we use three line segments, applying
ordinary least squares to the first two, but requiring the third to
be horizontal and tangent to a minimum. The result is a batch
optimization routine that returns an interval for the knee, rather
than a point estimate (see Figure 3 for details).
</bodyText>
<figureCaption confidence="0.997795">
Figure 3: Cross-entropy on WSJ45 after each baby step, a
piece-wise linear fit, and an estimated region for the knee.
</figureCaption>
<bodyText confidence="0.99985175">
els at WSJk*. Using one best parse from each,
for every sentence in WSJk*, the base case re-
estimates a new model from a mixture of twice the
normal number of trees; inductive steps leap over k*
lengths, conveniently ending at WSJ45, and estimate
their initial models by applying a previous solution
to a new input set. Both follow up the single step of
Viterbi training with at most five iterations of EM.
Our hybrid makes use of two good (condition-
ally) independent initialization strategies and exe-
cutes many iterations of EM where that is cheap —
at shorter sentences (WSJ15 and below). It then in-
creases the step size, training just three more times
(at WSJ{15, 30, 451) and allowing only a few (more
expensive) iterations of EM. Early termination im-
proves efficiency and regularizes these final models.
</bodyText>
<subsectionHeader confidence="0.951941">
5.5 Reference Algorithms
</subsectionHeader>
<bodyText confidence="0.993201857142857">
— Baselines, a Skyline and Published Art
We carve out the problem space using two extreme
initialization strategies: (i) the uninformed uniform
prior, which serves as a fair “zero-knowledge” base-
line for comparing uninitialized models; and (ii) the
maximum-likelihood “oracle” prior, computed from
reference parses, which yields a skyline (a reverse
baseline) — how well any algorithm that stumbled
on the true solution would fare at EM’s convergence.
In addition to citing Klein and Manning’s (2004)
results, we compare our accuracies on Section 23
of WSJOO to two state-of-the-art systems and past
baselines (see Table 2). Headden et al.’s (2009)
lexicalized EVG is the best on short sentences, but
</bodyText>
<figure confidence="0.9876414">
bpt
Cross-entropy h (in bits per token) on WSJ45
(hk − b0 − m0k)2 +
(hk − b1 − m1k)2 +
k0−1
X
k=1
k*
X
k=k0
WSJk
5 10 15 20 25 30 35 40 45
45 45 2
„hk − min hj
k1 j=k*+1
=k*+
min
b0,
m0 ,b1 ,m1
2&lt;ko &lt;k* &lt;45
8
&lt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;:
3.0
Knee
[7,15] Tight, Flat, Asymptotic Bound
5.0
4.5
4.0
3.5
</figure>
<page confidence="0.879941">
755
</page>
<figureCaption confidence="0.9868984">
Figure 4: Directed and undirected accuracy scores attained by the DMV, when trained and tested on the same gradation
of WSJ, for several different initialization strategies. Green circles mark Klein and Manning’s (2004) published scores;
red, violet and blue curves represent the supervised (maximum-likelihood oracle) initialization, Baby Steps, and the
uninformed uniform prior. Dotted curves reflect starting performance, solid curves register performance at EM’s
convergence, and the arrows connecting them emphasize the impact of learning.
</figureCaption>
<figure confidence="0.999754413793103">
5 10 15 20 25 30 35 40
5 10 15 20 25 30 35 40 45
WSJk
90
80
70
60
(b) Undirected Accuracy (%) on WSJk
Oracle
Ad-Hoc
Ad-Hoc
Oracle
Baby Steps
Uninformed
Baby Steps
(a) Directed Accuracy (%) on WSJk
Uninformed
50
40
30
20
60
Oracle
Leapfrog
Baby Steps
Ad-Hoc*
Uninformed
WSJk
5 10 15 20 25 30 35 40 45
</figure>
<figureCaption confidence="0.9966795">
Figure 5: Directed accuracies for Ad-Hoc* (shown in
green) and Leapfrog (in gold); all else as in Figure 4(a).
</figureCaption>
<bodyText confidence="0.999965571428571">
its performance is unreported for longer sentences,
for which Cohen and Smith’s (2009) seem to be
the highest published scores; we include their in-
termediate results that preceded parameter-tying —
Bayesian models with Dirichlet and log-normal pri-
ors, coupled with both Viterbi and minimum Bayes-
risk (MBR) decoding (Cohen et al., 2008).
</bodyText>
<sectionHeader confidence="0.992707" genericHeader="evaluation">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.999945733333333">
We packed thousands of empirical outcomes into the
space of several graphs (Figures 4, 5 and 6). The col-
ors (also in Tables 1 and 2) correspond to different
initialization strategies — to a first approximation,
the learning algorithm was held constant (see §5).
Figures 4 and 5 tell one part of our story. As data
sets increase in size, training algorithms gain access
to more information; however, since in this unsu-
pervised setting training and test sets are the same,
additional longer sentences make for substantially
more challenging evaluation. To control for these
dynamics, we applied Laplace smoothing to all (oth-
erwise unsmoothed) models and re-plotted their per-
formance, holding several test sets fixed, in Figure 6.
We report undirected accuracies parenthetically.
</bodyText>
<subsectionHeader confidence="0.995501">
6.1 Result #1: Baby Steps
</subsectionHeader>
<bodyText confidence="0.999784692307692">
Figure 4 traces out performance on the training set.
Klein and Manning’s (2004) published scores ap-
pear as dots (Ad-Hoc) at WSJ10: 43.2% (63.7%).
Baby Steps achieves 53.0% (65.7%) by WSJ10;
trained and tested on WSJ45, it gets 39.7% (54.3%).
Uninformed, classic EM learns little about directed
dependencies: it improves only slightly, e.g., from
17.3% (34.2%) to 19.1% (46.5%) on WSJ45 (learn-
ing some of the structure, as evidenced by its undi-
rected scores), but degrades with shorter sentences,
where its initial guessing rate is high. In the case
of oracle training, we expected EM to walk away
from supervised solutions (Elworthy, 1994; Meri-
</bodyText>
<figure confidence="0.9770753125">
50
40
30
20
Ad-Hoc
Directed Accuracy (%) on WSJk
756
80
60
50
40
30
20
70
Ad-Hoc
Less is More
� v �
(a) Directed Accuracy (%) on WSJ10
Uninformed
Ad-Hoc*
Oracle
Leapfrog
Baby Steps
(b) Directed Accuracy (%) on WSJ40
Uninformed
Less is More
� v �
Oracle
Ad-Hoc*
Leapfrog
Baby Steps
5 10 15 20 25 30 35 40 WSJ� 5 10 15 20 25 30 35 40 45
</figure>
<figureCaption confidence="0.9986625">
Figure 6: Directed accuracies attained by the DMV, when trained at various gradations of WSJ, smoothed, then tested
against fixed evaluation sets — WSJ{10, 40}; graphs for WSJ{20, 30}, not shown, are qualitatively similar to WSJ40.
</figureCaption>
<bodyText confidence="0.996912571428571">
aldo, 1994; Liang and Klein, 2008), but the ex-
tent of its drops is alarming, e.g., from the super-
vised 69.8% (72.2%) to the skyline’s 50.6% (59.5%)
on WSJ45. In contrast, Baby Steps’ scores usu-
ally do not change much from one step to the
next, and where its impact of learning is big (at
WSJ{4, 5, 14}), it is invariably positive.
</bodyText>
<subsectionHeader confidence="0.998756">
6.2 Result #2: Less is More
</subsectionHeader>
<bodyText confidence="0.999831791666667">
Ad-Hoc*’s curve (see Figure 5) suggests how Klein
and Manning’s Ad-Hoc initializer may have scaled
with different gradations of WSJ. Strangely, our im-
plementation performs significantly above their re-
ported numbers at WSJ10: 54.5% (68.3%) is even
slightly higher than Baby Steps; nevertheless, given
enough data (from WSJ22 onwards), Baby Steps
overtakes Ad-Hoc*, whose ability to learn takes a se-
rious dive once the inputs become sufficiently com-
plex (at WSJ23), and never recovers. Note that Ad-
Hoc*’s biased prior peaks early (at WSJ6), eventu-
ally falls below the guessing rate (by WSJ24), yet
still remains well-positioned to climb, outperform-
ing uninformed learning.
Figure 6 shows that Baby Steps scales better with
more (complex) data — its curves do not trend
downwards. However, a good initializer induces a
sweet spot at WSJ15, where the DMV is learned
best using Ad-Hoc*. This mode is “Less is More,”
scoring 44.1% (58.9%) on WSJ45. Curiously, even
oracle training exhibits a bump at WSJ15: once sen-
tences get long enough (at WSJ36), its performance
degrades below that of oracle training with virtually
no supervision (at the hardly representative WSJ3).
</bodyText>
<subsectionHeader confidence="0.998701">
6.3 Result #3: Leapfrog
</subsectionHeader>
<bodyText confidence="0.999486142857143">
Mixing Ad-Hoc* with Baby Steps at WSJ15 yields
a model whose performance initially falls between
its two parents but surpasses both with a little train-
ing (see Figure 5). Leaping to WSJ45, via WSJ30,
results in our strongest model: its 45.0% (58.4%) ac-
curacy bridges half of the gap between Baby Steps
and the skyline, and at a tiny fraction of the cost.
</bodyText>
<subsectionHeader confidence="0.968286">
6.4 Result #4: Generalization
</subsectionHeader>
<bodyText confidence="0.9999455">
Our models carry over to the larger WSJ100, Section
23 of WSJ&apos;, and the independent Brown100 (see
Table 1). Baby Steps improves out of domain, con-
firming that shaping generalizes well (Krueger and
Dayan, 2009; Bengio et al., 2009). Leapfrog does
best across the board but dips on Brown100, despite
its safe-guards against over-fitting.
Section 23 (see Table 2) reveals, unexpectedly,
that Baby Steps would have been state-of-the-art in
2008, whereas “Less is More” outperforms all prior
work on longer sentences. Baby Steps is competi-
tive with log-normal families (Cohen et al., 2008),
scoring slightly better on longer sentences against
Viterbi decoding, though worse against MBR. “Less
is More” beats state-of-the-art on longer sentences
by close to 2%; Leapfrog gains another 1%.
</bodyText>
<page confidence="0.992157">
757
</page>
<table confidence="0.96458">
Ad-Hoc* Baby Steps Leapfrog Ad-Hoc* Baby Steps Leapfrog
Section 23 44.1 (58.8) 39.2 (53.8) 43.3 (55.7) 31.5 (51.6) 39.4 (54.0) 45.0 (58.4)
WSJ100 43.8 (58.6) 39.2 (53.8) 43.3 (55.6) @15 31.3 (51.5) 39.4 (54.1) 44.7 (58.1) @45
Brown100 43.3 (59.2) 42.3 (55.1) 42.8 (56.5) 32.0 (52.4) 42.5 (55.5) 43.6 (59.1)
</table>
<tableCaption confidence="0.9882275">
Table 1: Directed and undirected accuracies on Section 23 of WSJ&apos;, WSJ100 and Brown100 for Ad-Hoc*, Baby
Steps and Leapfrog, trained at WSJ15 and WSJ45.
</tableCaption>
<table confidence="0.999545117647059">
Attach-Right (Klein and Manning, 2004) Decoding WSJ10 WSJ20 WSJ°
— 38.4 33.4 31.7
DMV Ad-Hoc (Klein and Manning, 2004) Viterbi 45.8 39.1 34.2
Dirichlet (Cohen et al., 2008) Viterbi 45.9 39.4 34.9
Ad-Hoc (Cohen et al., 2008) MBR 46.1 39.9 35.9
Dirichlet (Cohen et al., 2008) MBR 46.1 40.6 36.9
Log-Normal Families (Cohen et al., 2008) Viterbi 59.3 45.1 39.0
Baby Steps (@15) Viterbi 55.5 44.3 39.2
Baby Steps (@45) Viterbi 55.1 44.4 39.4
Log-Normal Families (Cohen et al., 2008) MBR 59.4 45.9 40.5
Shared Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) MBR 61.3 47.4 41.4
Bilingual Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) MBR 62.0 48.0 42.2
Less is More (Ad-Hoc* @15) Viterbi 56.2 48.2 44.1
Leapfrog (Hybrid @45) Viterbi 57.1 48.7 45.0
EVG Smoothed (skip-val) (Headden et al., 2009) Viterbi 62.1
Smoothed (skip-head) (Headden et al., 2009) Viterbi 65.0
Smoothed (skip-head), Lexicalized (Headden et al., 2009) Viterbi 68.8
</table>
<tableCaption confidence="0.988101">
Table 2: Directed accuracies on Section 23 of WSJ{10, 20,&apos; } for several baselines and recent state-of-the-art systems.
</tableCaption>
<sectionHeader confidence="0.995577" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999988142857143">
We explored three simple ideas for unsupervised de-
pendency parsing. Pace Halevy et al. (2009), we
find “Less is More” — the paradoxical result that
better performance can be attained by training with
less data, even when removing samples from the true
(test) distribution. Our small tweaks to Klein and
Manning’s approach of 2004 break through the 2009
state-of-the-art on longer sentences, when trained at
WSJ15 (the auto-detected sweet spot gradation).
The second, Baby Steps, is an elegant meta-
heuristic for optimizing non-convex training crite-
ria. It eliminates the need for linguistically-biased
manually-tuned initializers, particularly if the loca-
tion of the sweet spot is not known. This tech-
nique scales gracefully with more (complex) data
and should easily carry over to more powerful pars-
ing models and learning algorithms.
Finally, Leapfrog forgoes the elegance and metic-
ulousness of Baby Steps in favor of pragmatism.
Employing both good initialization strategies at
its disposal, and spending CPU cycles wisely, it
achieves better performance than both “Less is
More” and Baby Steps.
Future work could explore unifying these tech-
niques with other state-of-the-art approaches. It may
be useful to scaffold on both data and model com-
plexity, e.g., by increasing head automata’s number
of states (Alshawi and Douglas, 2000). We see many
opportunities for improvement, considering the poor
performance of oracle training relative to the super-
vised state-of-the-art, and in turn the poor perfor-
mance of unsupervised state-of-the-art relative to the
oracle models.11 To this end, it would be instructive
to understand both the linguistic and statistical na-
ture of the sweet spot, and to test its universality.
</bodyText>
<sectionHeader confidence="0.997782" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.87085825">
We thank Angel X. Chang, Pi-Chuan Chang, David L.W. Hall,
Christopher D. Manning, David McClosky, Daniel Ramage and
the anonymous reviewers for many helpful comments on draft
versions of this paper.
</bodyText>
<sectionHeader confidence="0.995317" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.854026">
E. L. Allgower and K. Georg. 1990. Numerical Continuation
Methods: An Introduction. Springer-Verlag.
</reference>
<footnote confidence="0.951627">
11To facilitate future work, all of our models are publicly
available at http://cs.stanford.edu/∼valentin/.
</footnote>
<page confidence="0.988395">
758
</page>
<reference confidence="0.999059956140352">
H. Alshawi and S. Douglas. 2000. Learning dependency trans-
duction models from unannotated examples. In Royal Soci-
ety of London Philosophical Transactions Series A, volume
358.
H. Alshawi. 1996. Head automata for speech translation. In
Proc. ofICSLP.
J. K. Baker. 1979. Trainable grammars for speech recognition.
In Speech Communication Papers for the 97th Meeting of the
Acoustical Society ofAmerica.
Y. Bengio, J. Louradour, R. Collobert, and J. Weston. 2009.
Curriculum learning. In ICML.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L. Mer-
cer. 1993. The mathematics of statistical machine transla-
tion: Parameter estimation. Computational Linguistics, 19.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best pars-
ing and MaxEnt discriminative reranking. In Proc. ofACL.
E. Charniak, M. Johnson, M. Elsner, J. Austerweil, D. Ellis,
I. Haxton, C. Hill, R. Shrivaths, J. Moore, M. Pozar, and
T. Vu. 2006. Multilevel coarse-to-fine PCFG parsing. In
HLT-NAACL.
S. B. Cohen and N. A. Smith. 2009. Shared logistic normal dis-
tributions for soft parameter tying in unsupervised grammar
induction. In Proc. ofNAACL-HLT.
S. B. Cohen, K. Gimpel, and N. A. Smith. 2008. Logistic nor-
mal priors for unsupervised probabilistic grammar induction.
In NIPS.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
M. Dorigo and M. Colombetti. 1998. Robot Shaping: An
Experiment in Behavior Engineering. MIT Press/Bradford
Books.
J. L. Elman. 1993. Learning and development in neural net-
works: The importance of starting small. Cognition, 48.
D. Elworthy. 1994. Does Baum-Welch re-estimation help tag-
gers? In Proc. ofANLP.
W. N. Francis and H. Kucera, 1979. Manual ofInformation to
Accompany a Standard Corpus ofPresent-Day Edited Amer-
ican English, for use with Digital Computers. Department of
Linguistic, Brown University.
Y. Freund and R. E. Schapire. 1997. A decision-theoretic gen-
eralization of on-line learning and an application to boosting.
Journal of Computer and System Sciences, 55(1).
A. Halevy, P. Norvig, and F. Pereira. 2009. The unreasonable
effectiveness of data. IEEE Intelligent Systems, 24(2).
W. P. Headden, III, M. Johnson, and D. McClosky. 2009. Im-
proving unsupervised dependency parsing with richer con-
texts and smoothing. In Proc. ofNAACL-HLT.
R. Kail. 1984. The development of memory in children. W. H.
Freeman and Company, 2nd edition.
D. Klein and C. D. Manning. 2004. Corpus-based induction of
syntactic structure: Models of dependency and constituency.
In Proc. ofACL.
D. Klein. 2005. The Unsupervised Learning of Natural Lan-
guage Structure. Ph.D. thesis, Stanford University.
K. A. Krueger and P. Dayan. 2009. Flexible shaping: How
learning in small steps helps. Cognition, 110.
P. Liang and D. Klein. 2008. Analyzing the errors of unsuper-
vised learning. In Proc. of HLT-ACL.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19(2).
D. McClosky, E. Charniak, and M. Johnson. 2006. Effective
self-training for parsing. In Proc. ofNAACL-HLT.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005.
Non-projective dependency parsing using spanning tree al-
gorithms. In Proc. of HLT-EMNLP.
B. Merialdo. 1994. Tagging English text with a probabilistic
model. Computational Linguistics, 20(2):155–172.
E. L. Newport. 1988. Constraints on learning and their role in
language acquisition: Studies of the acquisition of American
Sign Language. Language Sciences, 10(1).
E. L. Newport. 1990. Maturational constraints on language
learning. Cognitive Science, 14(1).
S. Petrov, A. Haghighi, and D. Klein. 2008. Coarse-to-fine
syntactic machine translation using language projections. In
Proc. ofEMNLP.
S. O. Petrov. 2009. Coarse-to-Fine Natural Language Process-
ing. Ph.D. thesis, University of California, Berkeley.
S. Ravi and K. Knight. 2009. Minimized models for unsuper-
vised part-of-speech tagging. In Proc. ofACL-IJCNLP.
D. L. T. Rohde and D. C. Plaut. 1999. Language acquisition in
the absence of explicit negative evidence: How important is
starting small? Cognition, 72(1).
L. M. Saksida, S. M. Raymond, and D. S. Touretzky. 1997.
Shaping robot behavior using principles from instrumental
conditioning. Robotics and Autonomous Systems, 22(3).
S. Salvador and P. Chan. 2004. Determining the number of
clusters/segments in hierarchical clustering/segmentation al-
gorithms. In Proc. ofICTAI.
T. D. Sanger. 1994. Neural network learning control of
robot manipulators using gradually increasing task difficulty.
IEEE Trans. on Robotics and Automation, 10.
T. Savage. 1998. Shaping: The link between rats and robots.
Connection Science, 10(3).
T. Savage. 2001. Shaping: A multiple contingencies analysis
and its relevance to behaviour-based robotics. Connection
Science, 13(3).
Y. Seginer. 2007. Fast unsupervised incremental parsing. In
Proc. ofACL.
S. P. Singh. 1992. Transfer of learning by composing solutions
of elemental squential tasks. Machine Learning, 8.
B. F. Skinner. 1938. The behavior of organisms: An experi-
mental analysis. Appleton-Century-Crofts.
N. A. Smith and J. Eisner. 2004. Annealing techniques for
unsupervised statistical language learning. In Proc. ofACL.
N. A. Smith and J. Eisner. 2005. Guiding unsupervised gram-
mar induction using contrastive estimation. In Proc. of the
IJCAI Workshop on Grammatical Inference Applications.
N. A. Smith and J. Eisner. 2006. Annealing structural bias
in multilingual weighted grammar induction. In Proc. of
COLING-ACL.
J. Suzuki, H. Isozaki, X. Carreras, and M. Collins. 2009. An
empirical study of semi-supervised structured conditional
models for dependency parsing. In Proc. of EMNLP.
</reference>
<page confidence="0.998797">
759
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.472032">
<title confidence="0.9585405">From Baby Steps to Leapfrog: How “Less is More” Unsupervised Dependency</title>
<author confidence="0.975477">Valentin I Spitkovsky Hiyan Alshawi</author>
<affiliation confidence="0.560151">Stanford University and Google Inc. Google Inc., Mountain View, CA, 94043</affiliation>
<email confidence="0.989531">valentin@cs.stanford.eduhiyan@google.com</email>
<author confidence="0.996437">Daniel Jurafsky</author>
<affiliation confidence="0.924211">Stanford University, Stanford, CA,</affiliation>
<email confidence="0.999548">jurafsky@stanford.edu</email>
<abstract confidence="0.999494307692308">We present three approaches for unsupervised grammar induction that are sensitive to data complexity and apply them to Klein and Manning’s Dependency Model with Valence. The bootstraps itself via iterated learning of increasingly longer sentences and requires no initialization. This method substantially exceeds Klein and Manning’s published scores and achieves 39.4% accuracy on Section 23 (all sentences) of the Wall Street corpus. The second, is uses a low-complexity subset of the available data: sentences up to length 15. Focusing on fewer but simpler examples trades off quantity against ambiguity; it attains 44.1% accuracy, using the standard linguisticallyinformed prior and batch training, beating our third heuristic, is More Steps mixing their models of shorter sentences, then rapidly ramping up exposure to the full training set, driving up accuracy to 45.0%. These trends generalize to the Brown corpus; awareness of data complexity may improve other parsing models and unsupervised algorithms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E L Allgower</author>
<author>K Georg</author>
</authors>
<title>Numerical Continuation Methods: An Introduction.</title>
<date>1990</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="10203" citStr="Allgower and Georg, 1990" startWordPosition="1541" endWordPosition="1544">tics (typically, for navigation), with reinforcement learning (Singh, 1992; Sanger, 1994; Saksida et al., 1997; Dorigo and Colombetti, 1998; Savage, 1998; Savage, 2001). Recently, Krueger and Dayan (2009) showed that shaping speeds up language acquisition and leads to better generalization in abstract neural networks. Bengio et al. (2009) confirmed this for deep deterministic and stochastic networks, using simple multi-stage curriculum strategies. They conjectured that a well-chosen sequence of training criteria — different sets of weights on the examples — could act as a continuation method (Allgower and Georg, 1990), helping find better local optima for nonconvex objectives. Elman’s learners constrained the peaky solution space by focusing on just the right data (simple sentences that introduced basic representational categories) at just the right time (early on, when their plasticity was greatest). Self-shaping, they simplified tasks through deliberate omission (or misunderstanding). Analogously, Baby Steps induces an early structural locality bias (Smith and Eisner, 2006), then relaxes it, as if annealing (Smith and Eisner, 2004). Its curriculum of binary weights initially discards complex examples res</context>
</contexts>
<marker>Allgower, Georg, 1990</marker>
<rawString>E. L. Allgower and K. Georg. 1990. Numerical Continuation Methods: An Introduction. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Alshawi</author>
<author>S Douglas</author>
</authors>
<title>Learning dependency transduction models from unannotated examples.</title>
<date>2000</date>
<journal>In Royal Society of London Philosophical Transactions Series A,</journal>
<volume>358</volume>
<contexts>
<context position="30692" citStr="Alshawi and Douglas, 2000" startWordPosition="4926" endWordPosition="4929">ales gracefully with more (complex) data and should easily carry over to more powerful parsing models and learning algorithms. Finally, Leapfrog forgoes the elegance and meticulousness of Baby Steps in favor of pragmatism. Employing both good initialization strategies at its disposal, and spending CPU cycles wisely, it achieves better performance than both “Less is More” and Baby Steps. Future work could explore unifying these techniques with other state-of-the-art approaches. It may be useful to scaffold on both data and model complexity, e.g., by increasing head automata’s number of states (Alshawi and Douglas, 2000). We see many opportunities for improvement, considering the poor performance of oracle training relative to the supervised state-of-the-art, and in turn the poor performance of unsupervised state-of-the-art relative to the oracle models.11 To this end, it would be instructive to understand both the linguistic and statistical nature of the sweet spot, and to test its universality. Acknowledgments We thank Angel X. Chang, Pi-Chuan Chang, David L.W. Hall, Christopher D. Manning, David McClosky, Daniel Ramage and the anonymous reviewers for many helpful comments on draft versions of this paper. R</context>
</contexts>
<marker>Alshawi, Douglas, 2000</marker>
<rawString>H. Alshawi and S. Douglas. 2000. Learning dependency transduction models from unannotated examples. In Royal Society of London Philosophical Transactions Series A, volume 358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Alshawi</author>
</authors>
<title>Head automata for speech translation.</title>
<date>1996</date>
<booktitle>In Proc. ofICSLP.</booktitle>
<contexts>
<context position="15013" citStr="Alshawi, 1996" startWordPosition="2344" endWordPosition="2345"> September . 0 z } |{ P = (1 − PSTOP(0, L, T)) X PATTACH(0, L, VBD) X (1 − PSTOP(VBD, L, T)) X PATTACH(VBD, L, NNS) X (1 − PSTOP(VBD, R, T)) X PATTACH(VBD, R, IN) X (1 − PSTOP(IN, R, T)) X PATTACH(IN, R, NN) X PSTOP(VBD, L, F) X PSTOP(VBD, R, F) X PSTOP(NNS, L, T) X PSTOP(NNS, R, T) X PSTOP(IN, L, T) X PSTOP(IN, R, F) X PSTOP(NN, L, T) X PSTOP(NN, R, T) X PSTOP(0, L, F) |{z } 1 1 Figure 2: A simple dependency structure for a short sentence and its probability, as factored by the DMV. 5 New Algorithms for the Classic Model The DMV (Klein and Manning, 2004) is a singlestate head automata model (Alshawi, 1996) over lexical word classes {cw} — POS tags. Its generative story for a sub-tree rooted at a head (of class ch) rests on three types of independent decisions: (i) initial direction dir E {L, R} in which to attach children, via probability PORDER(ch); (ii) whether to seal dir, stopping with probability PSTOP(ch, dir, adj), conditioned on adj E {T, F} (true iff considering dir’s first, i.e., adjacent, child); and (iii) attachments (of class ca), according to PATTACH(ch, dir, ca). This produces only projective trees.7 A root token Q generates the head of a sentence as its left (and only) child. Fi</context>
</contexts>
<marker>Alshawi, 1996</marker>
<rawString>H. Alshawi. 1996. Head automata for speech translation. In Proc. ofICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J K Baker</author>
</authors>
<title>Trainable grammars for speech recognition.</title>
<date>1979</date>
<booktitle>In Speech Communication Papers for the 97th Meeting of the Acoustical Society ofAmerica.</booktitle>
<contexts>
<context position="15880" citStr="Baker, 1979" startWordPosition="2483" endWordPosition="2484">whether to seal dir, stopping with probability PSTOP(ch, dir, adj), conditioned on adj E {T, F} (true iff considering dir’s first, i.e., adjacent, child); and (iii) attachments (of class ca), according to PATTACH(ch, dir, ca). This produces only projective trees.7 A root token Q generates the head of a sentence as its left (and only) child. Figure 2 displays an example that ignores (sums out) PORDER. The DMV lends itself to unsupervised learn7Unlike spanning tree algorithms (McDonald et al., 2005), DMV’s chart-based method disallows crossing dependencies. ing via inside-outside re-estimation (Baker, 1979). Klein and Manning did not use smoothing and started with an “ad-hoc harmonic” completion: aiming for balanced trees, non-root heads attached dependents in inverse proportion to (a constant plus) their distance; Q generated heads uniformly at random. This non-distributional heuristic created favorable initial conditions that nudged EM towards typical linguistic dependency structures. 5.1 Algorithm #0: Ad-Hoc* — A Variation on Original Ad-Hoc Initialization Since some of the important implementation details are not available in the literature (Klein and Manning, 2004; Klein, 2005), we had to i</context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>J. K. Baker. 1979. Trainable grammars for speech recognition. In Speech Communication Papers for the 97th Meeting of the Acoustical Society ofAmerica.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>J Louradour</author>
<author>R Collobert</author>
<author>J Weston</author>
</authors>
<title>Curriculum learning. In</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<contexts>
<context position="9918" citStr="Bengio et al. (2009)" startWordPosition="1499" endWordPosition="1502">) in which the teacher decomposes a complete task into sub-components, providing an easier path to learning. When Skinner (1938) coined the term, he described it as a “method of successive approximations.” Ideas that gradually make a task more difficult have been explored in robotics (typically, for navigation), with reinforcement learning (Singh, 1992; Sanger, 1994; Saksida et al., 1997; Dorigo and Colombetti, 1998; Savage, 1998; Savage, 2001). Recently, Krueger and Dayan (2009) showed that shaping speeds up language acquisition and leads to better generalization in abstract neural networks. Bengio et al. (2009) confirmed this for deep deterministic and stochastic networks, using simple multi-stage curriculum strategies. They conjectured that a well-chosen sequence of training criteria — different sets of weights on the examples — could act as a continuation method (Allgower and Georg, 1990), helping find better local optima for nonconvex objectives. Elman’s learners constrained the peaky solution space by focusing on just the right data (simple sentences that introduced basic representational categories) at just the right time (early on, when their plasticity was greatest). Self-shaping, they simpli</context>
<context position="27270" citStr="Bengio et al., 2009" startWordPosition="4393" endWordPosition="4396">frog Mixing Ad-Hoc* with Baby Steps at WSJ15 yields a model whose performance initially falls between its two parents but surpasses both with a little training (see Figure 5). Leaping to WSJ45, via WSJ30, results in our strongest model: its 45.0% (58.4%) accuracy bridges half of the gap between Baby Steps and the skyline, and at a tiny fraction of the cost. 6.4 Result #4: Generalization Our models carry over to the larger WSJ100, Section 23 of WSJ&apos;, and the independent Brown100 (see Table 1). Baby Steps improves out of domain, confirming that shaping generalizes well (Krueger and Dayan, 2009; Bengio et al., 2009). Leapfrog does best across the board but dips on Brown100, despite its safe-guards against over-fitting. Section 23 (see Table 2) reveals, unexpectedly, that Baby Steps would have been state-of-the-art in 2008, whereas “Less is More” outperforms all prior work on longer sentences. Baby Steps is competitive with log-normal families (Cohen et al., 2008), scoring slightly better on longer sentences against Viterbi decoding, though worse against MBR. “Less is More” beats state-of-the-art on longer sentences by close to 2%; Leapfrog gains another 1%. 757 Ad-Hoc* Baby Steps Leapfrog Ad-Hoc* Baby St</context>
</contexts>
<marker>Bengio, Louradour, Collobert, Weston, 2009</marker>
<rawString>Y. Bengio, J. Louradour, R. Collobert, and J. Weston. 2009. Curriculum learning. In ICML. P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-fine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="8822" citStr="Charniak and Johnson, 2005" startWordPosition="1337" endWordPosition="1340">icant advantage for “starting large!” With iterative training invoking the optimizer multiple times, creating extra opportunities to converge, Rohde and Plaut (1999) suspected that Elman’s (1993) simulations simply did not allow networks exposed exclusively to complex inputs sufficient training time. Our extremely generous, low termination threshold for EM (see §5.1) addresses this concern. However, given the DMV’s purely syntactic POS tag-based approach (see §5), it would be prudent to re-test Baby Steps with a lexicalized model. 752 approaches to parsing, translation and speech recognition (Charniak and Johnson, 2005; Charniak et al., 2006; Petrov et al., 2008; Petrov, 2009), and recently unsupervised POS tagging (Ravi and Knight, 2009). Initial models tend to be particularly simple,4 and each refinement towards a full model introduces only limited complexity, supporting incrementality. Filtering complex data, the focus of our work, is unconventional in natural language processing. Such scaffolding qualifies as shaping — a method of instruction (routinely exploited in animal training) in which the teacher decomposes a complete task into sub-components, providing an easier path to learning. When Skinner (1</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best parsing and MaxEnt discriminative reranking. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
<author>M Elsner</author>
<author>J Austerweil</author>
<author>D Ellis</author>
<author>I Haxton</author>
<author>C Hill</author>
<author>R Shrivaths</author>
<author>J Moore</author>
<author>M Pozar</author>
<author>T Vu</author>
</authors>
<title>Multilevel coarse-to-fine PCFG parsing.</title>
<date>2006</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="8845" citStr="Charniak et al., 2006" startWordPosition="1341" endWordPosition="1344">g large!” With iterative training invoking the optimizer multiple times, creating extra opportunities to converge, Rohde and Plaut (1999) suspected that Elman’s (1993) simulations simply did not allow networks exposed exclusively to complex inputs sufficient training time. Our extremely generous, low termination threshold for EM (see §5.1) addresses this concern. However, given the DMV’s purely syntactic POS tag-based approach (see §5), it would be prudent to re-test Baby Steps with a lexicalized model. 752 approaches to parsing, translation and speech recognition (Charniak and Johnson, 2005; Charniak et al., 2006; Petrov et al., 2008; Petrov, 2009), and recently unsupervised POS tagging (Ravi and Knight, 2009). Initial models tend to be particularly simple,4 and each refinement towards a full model introduces only limited complexity, supporting incrementality. Filtering complex data, the focus of our work, is unconventional in natural language processing. Such scaffolding qualifies as shaping — a method of instruction (routinely exploited in animal training) in which the teacher decomposes a complete task into sub-components, providing an easier path to learning. When Skinner (1938) coined the term, h</context>
</contexts>
<marker>Charniak, Johnson, Elsner, Austerweil, Ellis, Haxton, Hill, Shrivaths, Moore, Pozar, Vu, 2006</marker>
<rawString>E. Charniak, M. Johnson, M. Elsner, J. Austerweil, D. Ellis, I. Haxton, C. Hill, R. Shrivaths, J. Moore, M. Pozar, and T. Vu. 2006. Multilevel coarse-to-fine PCFG parsing. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>N A Smith</author>
</authors>
<title>Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction.</title>
<date>2009</date>
<booktitle>In Proc. ofNAACL-HLT.</booktitle>
<contexts>
<context position="2331" citStr="Cohen and Smith, 2009" startWordPosition="334" endWordPosition="338">rsion that targets dependencies and *Partially funded by NSF award IIS-0811974; first author supported by the Fannie &amp; John Hertz Foundation Fellowship. assumes partial annotation, e.g., sentence boundaries, tokenization and typically even part-of-speech (POS) tagging, has received much attention, eliciting a diverse array of techniques (Smith and Eisner, 2005; Seginer, 2007; Cohen et al., 2008). Klein and Manning’s (2004) Dependency Model with Valence (DMV) was the first to beat a simple parsing heuristic — the right-branching baseline. Today’s state-of-the-art systems (Headden et al., 2009; Cohen and Smith, 2009) are still rooted in the DMV. Despite recent advances, unsupervised parsers lag far behind their supervised counterparts. Although large amounts of unlabeled data are known to improve semi-supervised parsing (Suzuki et al., 2009), the best unsupervised systems use less data than is available for supervised training, relying on complex models instead: Headden et al.’s (2009) Extended Valence Grammar (EVG) combats data sparsity with smoothing alone, training on the same small subset of the tree-bank as the classic implementation of the DMV; Cohen and Smith (2009) use more complicated algorithms </context>
<context position="28842" citStr="Cohen and Smith, 2009" startWordPosition="4643" endWordPosition="4646"> for Ad-Hoc*, Baby Steps and Leapfrog, trained at WSJ15 and WSJ45. Attach-Right (Klein and Manning, 2004) Decoding WSJ10 WSJ20 WSJ° — 38.4 33.4 31.7 DMV Ad-Hoc (Klein and Manning, 2004) Viterbi 45.8 39.1 34.2 Dirichlet (Cohen et al., 2008) Viterbi 45.9 39.4 34.9 Ad-Hoc (Cohen et al., 2008) MBR 46.1 39.9 35.9 Dirichlet (Cohen et al., 2008) MBR 46.1 40.6 36.9 Log-Normal Families (Cohen et al., 2008) Viterbi 59.3 45.1 39.0 Baby Steps (@15) Viterbi 55.5 44.3 39.2 Baby Steps (@45) Viterbi 55.1 44.4 39.4 Log-Normal Families (Cohen et al., 2008) MBR 59.4 45.9 40.5 Shared Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) MBR 61.3 47.4 41.4 Bilingual Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) MBR 62.0 48.0 42.2 Less is More (Ad-Hoc* @15) Viterbi 56.2 48.2 44.1 Leapfrog (Hybrid @45) Viterbi 57.1 48.7 45.0 EVG Smoothed (skip-val) (Headden et al., 2009) Viterbi 62.1 Smoothed (skip-head) (Headden et al., 2009) Viterbi 65.0 Smoothed (skip-head), Lexicalized (Headden et al., 2009) Viterbi 68.8 Table 2: Directed accuracies on Section 23 of WSJ{10, 20,&apos; } for several baselines and recent state-of-the-art systems. 7 Conclusion We explored three simple ideas for unsupervised dependency parsing. Pace Halevy et a</context>
</contexts>
<marker>Cohen, Smith, 2009</marker>
<rawString>S. B. Cohen and N. A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In Proc. ofNAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>K Gimpel</author>
<author>N A Smith</author>
</authors>
<title>Logistic normal priors for unsupervised probabilistic grammar induction.</title>
<date>2008</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="2107" citStr="Cohen et al., 2008" startWordPosition="300" endWordPosition="303">ctic structure from free-form natural language text is a hard problem whose eventual solution promises to benefit applications ranging from question answering to speech recognition and machine translation. A restricted version that targets dependencies and *Partially funded by NSF award IIS-0811974; first author supported by the Fannie &amp; John Hertz Foundation Fellowship. assumes partial annotation, e.g., sentence boundaries, tokenization and typically even part-of-speech (POS) tagging, has received much attention, eliciting a diverse array of techniques (Smith and Eisner, 2005; Seginer, 2007; Cohen et al., 2008). Klein and Manning’s (2004) Dependency Model with Valence (DMV) was the first to beat a simple parsing heuristic — the right-branching baseline. Today’s state-of-the-art systems (Headden et al., 2009; Cohen and Smith, 2009) are still rooted in the DMV. Despite recent advances, unsupervised parsers lag far behind their supervised counterparts. Although large amounts of unlabeled data are known to improve semi-supervised parsing (Suzuki et al., 2009), the best unsupervised systems use less data than is available for supervised training, relying on complex models instead: Headden et al.’s (2009)</context>
<context position="23067" citStr="Cohen et al., 2008" startWordPosition="3686" endWordPosition="3689">le Baby Steps Uninformed Baby Steps (a) Directed Accuracy (%) on WSJk Uninformed 50 40 30 20 60 Oracle Leapfrog Baby Steps Ad-Hoc* Uninformed WSJk 5 10 15 20 25 30 35 40 45 Figure 5: Directed accuracies for Ad-Hoc* (shown in green) and Leapfrog (in gold); all else as in Figure 4(a). its performance is unreported for longer sentences, for which Cohen and Smith’s (2009) seem to be the highest published scores; we include their intermediate results that preceded parameter-tying — Bayesian models with Dirichlet and log-normal priors, coupled with both Viterbi and minimum Bayesrisk (MBR) decoding (Cohen et al., 2008). 6 Experimental Results We packed thousands of empirical outcomes into the space of several graphs (Figures 4, 5 and 6). The colors (also in Tables 1 and 2) correspond to different initialization strategies — to a first approximation, the learning algorithm was held constant (see §5). Figures 4 and 5 tell one part of our story. As data sets increase in size, training algorithms gain access to more information; however, since in this unsupervised setting training and test sets are the same, additional longer sentences make for substantially more challenging evaluation. To control for these dyn</context>
<context position="27624" citStr="Cohen et al., 2008" startWordPosition="4447" endWordPosition="4450">ost. 6.4 Result #4: Generalization Our models carry over to the larger WSJ100, Section 23 of WSJ&apos;, and the independent Brown100 (see Table 1). Baby Steps improves out of domain, confirming that shaping generalizes well (Krueger and Dayan, 2009; Bengio et al., 2009). Leapfrog does best across the board but dips on Brown100, despite its safe-guards against over-fitting. Section 23 (see Table 2) reveals, unexpectedly, that Baby Steps would have been state-of-the-art in 2008, whereas “Less is More” outperforms all prior work on longer sentences. Baby Steps is competitive with log-normal families (Cohen et al., 2008), scoring slightly better on longer sentences against Viterbi decoding, though worse against MBR. “Less is More” beats state-of-the-art on longer sentences by close to 2%; Leapfrog gains another 1%. 757 Ad-Hoc* Baby Steps Leapfrog Ad-Hoc* Baby Steps Leapfrog Section 23 44.1 (58.8) 39.2 (53.8) 43.3 (55.7) 31.5 (51.6) 39.4 (54.0) 45.0 (58.4) WSJ100 43.8 (58.6) 39.2 (53.8) 43.3 (55.6) @15 31.3 (51.5) 39.4 (54.1) 44.7 (58.1) @45 Brown100 43.3 (59.2) 42.3 (55.1) 42.8 (56.5) 32.0 (52.4) 42.5 (55.5) 43.6 (59.1) Table 1: Directed and undirected accuracies on Section 23 of WSJ&apos;, WSJ100 and Brown100 for</context>
</contexts>
<marker>Cohen, Gimpel, Smith, 2008</marker>
<rawString>S. B. Cohen, K. Gimpel, and N. A. Smith. 2008. Logistic normal priors for unsupervised probabilistic grammar induction. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="12523" citStr="Collins, 1999" startWordPosition="1897" endWordPosition="1898">istributions, whereas our hybrid, Leapfrog, admits multiple initializers by mixing structures instead. “Less is More” is novel and confirms the tacit consensus implicit in training on small data sets (e.g., WSJ10). 4 Data Sets and Metrics Klein and Manning (2004) both trained and tested the DMV on the same customized subset (WSJ10) of Penn English Treebank’s Wall Street Journal portion (Marcus et al., 1993). Its 49,208 annotated parse trees were pruned5 down to 7,422 sentences of at most 10 terminals, spanning 35 unique POS tags. Following standard practice, automatic “headpercolation” rules (Collins, 1999) were used to convert the remaining trees into dependencies. Forced to produce a single “best” parse, their algorithm was judged on accuracy: its directed score was the fraction of correct dependencies; a more flattering6 undirected score was also used. We employ the same metrics, emphasizing directed scores, and generalize WSJk to be the subset of pre-processed sentences with at most k terminals. Our experiments focus on k E 11, ... , 451, but we also test on WSJ100 and Section 23 of WSJ&apos; (the entire WSJ), as well as the held-out Brown100 (similarly derived from the Brown corpus (Francis and </context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dorigo</author>
<author>M Colombetti</author>
</authors>
<title>Robot Shaping: An Experiment in Behavior Engineering.</title>
<date>1998</date>
<publisher>MIT Press/Bradford Books.</publisher>
<contexts>
<context position="9717" citStr="Dorigo and Colombetti, 1998" startWordPosition="1470" endWordPosition="1473">tality. Filtering complex data, the focus of our work, is unconventional in natural language processing. Such scaffolding qualifies as shaping — a method of instruction (routinely exploited in animal training) in which the teacher decomposes a complete task into sub-components, providing an easier path to learning. When Skinner (1938) coined the term, he described it as a “method of successive approximations.” Ideas that gradually make a task more difficult have been explored in robotics (typically, for navigation), with reinforcement learning (Singh, 1992; Sanger, 1994; Saksida et al., 1997; Dorigo and Colombetti, 1998; Savage, 1998; Savage, 2001). Recently, Krueger and Dayan (2009) showed that shaping speeds up language acquisition and leads to better generalization in abstract neural networks. Bengio et al. (2009) confirmed this for deep deterministic and stochastic networks, using simple multi-stage curriculum strategies. They conjectured that a well-chosen sequence of training criteria — different sets of weights on the examples — could act as a continuation method (Allgower and Georg, 1990), helping find better local optima for nonconvex objectives. Elman’s learners constrained the peaky solution space</context>
</contexts>
<marker>Dorigo, Colombetti, 1998</marker>
<rawString>M. Dorigo and M. Colombetti. 1998. Robot Shaping: An Experiment in Behavior Engineering. MIT Press/Bradford Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Elman</author>
</authors>
<title>Learning and development in neural networks: The importance of starting small.</title>
<date>1993</date>
<journal>Cognition,</journal>
<volume>48</volume>
<marker>Elman, 1993</marker>
<rawString>J. L. Elman. 1993. Learning and development in neural networks: The importance of starting small. Cognition, 48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Elworthy</author>
</authors>
<title>Does Baum-Welch re-estimation help taggers?</title>
<date>1994</date>
<booktitle>In Proc. ofANLP.</booktitle>
<contexts>
<context position="24533" citStr="Elworthy, 1994" startWordPosition="3922" endWordPosition="3923">ormance on the training set. Klein and Manning’s (2004) published scores appear as dots (Ad-Hoc) at WSJ10: 43.2% (63.7%). Baby Steps achieves 53.0% (65.7%) by WSJ10; trained and tested on WSJ45, it gets 39.7% (54.3%). Uninformed, classic EM learns little about directed dependencies: it improves only slightly, e.g., from 17.3% (34.2%) to 19.1% (46.5%) on WSJ45 (learning some of the structure, as evidenced by its undirected scores), but degrades with shorter sentences, where its initial guessing rate is high. In the case of oracle training, we expected EM to walk away from supervised solutions (Elworthy, 1994; Meri50 40 30 20 Ad-Hoc Directed Accuracy (%) on WSJk 756 80 60 50 40 30 20 70 Ad-Hoc Less is More � v � (a) Directed Accuracy (%) on WSJ10 Uninformed Ad-Hoc* Oracle Leapfrog Baby Steps (b) Directed Accuracy (%) on WSJ40 Uninformed Less is More � v � Oracle Ad-Hoc* Leapfrog Baby Steps 5 10 15 20 25 30 35 40 WSJ� 5 10 15 20 25 30 35 40 45 Figure 6: Directed accuracies attained by the DMV, when trained at various gradations of WSJ, smoothed, then tested against fixed evaluation sets — WSJ{10, 40}; graphs for WSJ{20, 30}, not shown, are qualitatively similar to WSJ40. aldo, 1994; Liang and Klein</context>
</contexts>
<marker>Elworthy, 1994</marker>
<rawString>D. Elworthy. 1994. Does Baum-Welch re-estimation help taggers? In Proc. ofANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W N Francis</author>
<author>H Kucera</author>
</authors>
<title>Manual ofInformation to Accompany a Standard Corpus ofPresent-Day Edited American English, for use with Digital Computers.</title>
<date>1979</date>
<institution>Department of Linguistic, Brown University.</institution>
<contexts>
<context position="13136" citStr="Francis and Kucera, 1979" startWordPosition="2001" endWordPosition="2004">llins, 1999) were used to convert the remaining trees into dependencies. Forced to produce a single “best” parse, their algorithm was judged on accuracy: its directed score was the fraction of correct dependencies; a more flattering6 undirected score was also used. We employ the same metrics, emphasizing directed scores, and generalize WSJk to be the subset of pre-processed sentences with at most k terminals. Our experiments focus on k E 11, ... , 451, but we also test on WSJ100 and Section 23 of WSJ&apos; (the entire WSJ), as well as the held-out Brown100 (similarly derived from the Brown corpus (Francis and Kucera, 1979)). See Figure 1 for these corpora’s sentence and token counts. 5Stripped of all empty sub-trees, punctuation, and terminals (tagged # and $) not pronounced where they appear, those sentences still containing more than ten tokens were thrown out. 6Ignoring polarity of parent-child relations partially obscured effects of alternate analyses (systematic choices between modals and main verbs for heads of sentences, determiners for noun phrases, etc.) and facilitated comparison with prior work. 753 Corpus Sentences POS Tokens Corpus Sentences POS Tokens WSJ1 159 159 WSJ13 12,270 110,760 WSJ2 499 839</context>
</contexts>
<marker>Francis, Kucera, 1979</marker>
<rawString>W. N. Francis and H. Kucera, 1979. Manual ofInformation to Accompany a Standard Corpus ofPresent-Day Edited American English, for use with Digital Computers. Department of Linguistic, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Schapire</author>
</authors>
<title>A decision-theoretic generalization of on-line learning and an application to boosting.</title>
<date>1997</date>
<journal>Journal of Computer and System Sciences,</journal>
<volume>55</volume>
<issue>1</issue>
<contexts>
<context position="6183" citStr="Freund and Schapire, 1997" startWordPosition="934" endWordPosition="937">equires neither initialization nor search yet reveals something of sentence heads. The next step — sentences of length one and two — refines initial impressions of heads, introduces dependents, and exposes their identities and relative positions. Although not representative of the full grammar, short sentences capture enough information to paint most of the picture needed by slightly longer sentences. They set up an easier, incremental subsequent learning task. Step k + 1 augments training input to include lengths 1It mirrors the effect that boosting hard examples has for supervised training (Freund and Schapire, 1997). 1, 2, ... , k, k + 1 of the full data set and executes local search starting from the (smoothed) model estimated by step k. This truly is grammar induction. Less is More: For standard batch training, just using simple, short sentences is not enough. They are rare and do not reveal the full grammar. We find a “sweet spot” — sentence lengths that are neither too long (excluding the truly daunting examples) nor too few (supplying enough accessible information), using Baby Steps’ learning curve as a guide. We train where it flattens out, since remaining sentences contribute little (incremental) </context>
</contexts>
<marker>Freund, Schapire, 1997</marker>
<rawString>Y. Freund and R. E. Schapire. 1997. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Halevy</author>
<author>P Norvig</author>
<author>F Pereira</author>
</authors>
<title>The unreasonable effectiveness of data.</title>
<date>2009</date>
<journal>IEEE Intelligent Systems,</journal>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="29451" citStr="Halevy et al. (2009)" startWordPosition="4735" endWordPosition="4738">mith, 2009) MBR 61.3 47.4 41.4 Bilingual Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) MBR 62.0 48.0 42.2 Less is More (Ad-Hoc* @15) Viterbi 56.2 48.2 44.1 Leapfrog (Hybrid @45) Viterbi 57.1 48.7 45.0 EVG Smoothed (skip-val) (Headden et al., 2009) Viterbi 62.1 Smoothed (skip-head) (Headden et al., 2009) Viterbi 65.0 Smoothed (skip-head), Lexicalized (Headden et al., 2009) Viterbi 68.8 Table 2: Directed accuracies on Section 23 of WSJ{10, 20,&apos; } for several baselines and recent state-of-the-art systems. 7 Conclusion We explored three simple ideas for unsupervised dependency parsing. Pace Halevy et al. (2009), we find “Less is More” — the paradoxical result that better performance can be attained by training with less data, even when removing samples from the true (test) distribution. Our small tweaks to Klein and Manning’s approach of 2004 break through the 2009 state-of-the-art on longer sentences, when trained at WSJ15 (the auto-detected sweet spot gradation). The second, Baby Steps, is an elegant metaheuristic for optimizing non-convex training criteria. It eliminates the need for linguistically-biased manually-tuned initializers, particularly if the location of the sweet spot is not known. Th</context>
</contexts>
<marker>Halevy, Norvig, Pereira, 2009</marker>
<rawString>A. Halevy, P. Norvig, and F. Pereira. 2009. The unreasonable effectiveness of data. IEEE Intelligent Systems, 24(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W P Headden</author>
<author>M Johnson</author>
<author>D McClosky</author>
</authors>
<title>Improving unsupervised dependency parsing with richer contexts and smoothing.</title>
<date>2009</date>
<booktitle>In Proc. ofNAACL-HLT.</booktitle>
<contexts>
<context position="2307" citStr="Headden et al., 2009" startWordPosition="330" endWordPosition="333">ation. A restricted version that targets dependencies and *Partially funded by NSF award IIS-0811974; first author supported by the Fannie &amp; John Hertz Foundation Fellowship. assumes partial annotation, e.g., sentence boundaries, tokenization and typically even part-of-speech (POS) tagging, has received much attention, eliciting a diverse array of techniques (Smith and Eisner, 2005; Seginer, 2007; Cohen et al., 2008). Klein and Manning’s (2004) Dependency Model with Valence (DMV) was the first to beat a simple parsing heuristic — the right-branching baseline. Today’s state-of-the-art systems (Headden et al., 2009; Cohen and Smith, 2009) are still rooted in the DMV. Despite recent advances, unsupervised parsers lag far behind their supervised counterparts. Although large amounts of unlabeled data are known to improve semi-supervised parsing (Suzuki et al., 2009), the best unsupervised systems use less data than is available for supervised training, relying on complex models instead: Headden et al.’s (2009) Extended Valence Grammar (EVG) combats data sparsity with smoothing alone, training on the same small subset of the tree-bank as the classic implementation of the DMV; Cohen and Smith (2009) use more</context>
<context position="29084" citStr="Headden et al., 2009" startWordPosition="4681" endWordPosition="4684">erbi 45.9 39.4 34.9 Ad-Hoc (Cohen et al., 2008) MBR 46.1 39.9 35.9 Dirichlet (Cohen et al., 2008) MBR 46.1 40.6 36.9 Log-Normal Families (Cohen et al., 2008) Viterbi 59.3 45.1 39.0 Baby Steps (@15) Viterbi 55.5 44.3 39.2 Baby Steps (@45) Viterbi 55.1 44.4 39.4 Log-Normal Families (Cohen et al., 2008) MBR 59.4 45.9 40.5 Shared Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) MBR 61.3 47.4 41.4 Bilingual Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) MBR 62.0 48.0 42.2 Less is More (Ad-Hoc* @15) Viterbi 56.2 48.2 44.1 Leapfrog (Hybrid @45) Viterbi 57.1 48.7 45.0 EVG Smoothed (skip-val) (Headden et al., 2009) Viterbi 62.1 Smoothed (skip-head) (Headden et al., 2009) Viterbi 65.0 Smoothed (skip-head), Lexicalized (Headden et al., 2009) Viterbi 68.8 Table 2: Directed accuracies on Section 23 of WSJ{10, 20,&apos; } for several baselines and recent state-of-the-art systems. 7 Conclusion We explored three simple ideas for unsupervised dependency parsing. Pace Halevy et al. (2009), we find “Less is More” — the paradoxical result that better performance can be attained by training with less data, even when removing samples from the true (test) distribution. Our small tweaks to Klein and Manning’s approach of 2</context>
</contexts>
<marker>Headden, Johnson, McClosky, 2009</marker>
<rawString>W. P. Headden, III, M. Johnson, and D. McClosky. 2009. Improving unsupervised dependency parsing with richer contexts and smoothing. In Proc. ofNAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kail</author>
</authors>
<title>The development of memory in</title>
<date>1984</date>
<contexts>
<context position="7507" citStr="Kail, 1984" startWordPosition="1149" endWordPosition="1150">esults of batch and iterative training up to the sweet spot data gradation, then iterate with a large step size. 3 Related Work Two types of scaffolding for guiding language learning debuted in Elman’s (1993) experiments with “starting small”: data complexity (restricting input) and model complexity (restricting memory). In both cases, gradually increasing complexity allowed artificial neural networks to master a pseudo-natural grammar they otherwise failed to learn. Initiallylimited capacity resembled maturational changes in working memory and attention span that occur over time in children (Kail, 1984), in line with the “less is more” proposal (Newport, 1988; 1990). Although Rohde and Plaut (1999) failed to replicate this3 result with simple recurrent networks, other machine learning techniques reliably benefit from scaffolded model complexity on a variety of language tasks. In word-alignment, Brown et al. (1993) used IBM Models 1-4 as “stepping stones” to training Model 5. Other prominent examples include “coarse-to-fine” 2This is akin to McClosky et al.’s (2006) “Goldilocks effect.” 3Worse, they found that limiting input hindered language acquisition. And making the grammar more English-l</context>
</contexts>
<marker>Kail, 1984</marker>
<rawString>R. Kail. 1984. The development of memory in children. W. H. Freeman and Company, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Corpus-based induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="12172" citStr="Klein and Manning (2004)" startWordPosition="1840" endWordPosition="1843">rd training. Structural annealing works well, but requires a handtuned annealing schedule and direct manipulation of the objective function; Baby Steps works “out of the box,” its locality biases a natural consequence of a complexity/data-guided tour of optimization problems. Skewed DA incorporates a good initializer by interpolating between two probability distributions, whereas our hybrid, Leapfrog, admits multiple initializers by mixing structures instead. “Less is More” is novel and confirms the tacit consensus implicit in training on small data sets (e.g., WSJ10). 4 Data Sets and Metrics Klein and Manning (2004) both trained and tested the DMV on the same customized subset (WSJ10) of Penn English Treebank’s Wall Street Journal portion (Marcus et al., 1993). Its 49,208 annotated parse trees were pruned5 down to 7,422 sentences of at most 10 terminals, spanning 35 unique POS tags. Following standard practice, automatic “headpercolation” rules (Collins, 1999) were used to convert the remaining trees into dependencies. Forced to produce a single “best” parse, their algorithm was judged on accuracy: its directed score was the fraction of correct dependencies; a more flattering6 undirected score was also u</context>
<context position="14960" citStr="Klein and Manning, 2004" startWordPosition="2333" endWordPosition="2336">ction 23 of WSJ&apos; and Brown100. NNS VBD IN NN Q Payrolls fell in September . 0 z } |{ P = (1 − PSTOP(0, L, T)) X PATTACH(0, L, VBD) X (1 − PSTOP(VBD, L, T)) X PATTACH(VBD, L, NNS) X (1 − PSTOP(VBD, R, T)) X PATTACH(VBD, R, IN) X (1 − PSTOP(IN, R, T)) X PATTACH(IN, R, NN) X PSTOP(VBD, L, F) X PSTOP(VBD, R, F) X PSTOP(NNS, L, T) X PSTOP(NNS, R, T) X PSTOP(IN, L, T) X PSTOP(IN, R, F) X PSTOP(NN, L, T) X PSTOP(NN, R, T) X PSTOP(0, L, F) |{z } 1 1 Figure 2: A simple dependency structure for a short sentence and its probability, as factored by the DMV. 5 New Algorithms for the Classic Model The DMV (Klein and Manning, 2004) is a singlestate head automata model (Alshawi, 1996) over lexical word classes {cw} — POS tags. Its generative story for a sub-tree rooted at a head (of class ch) rests on three types of independent decisions: (i) initial direction dir E {L, R} in which to attach children, via probability PORDER(ch); (ii) whether to seal dir, stopping with probability PSTOP(ch, dir, adj), conditioned on adj E {T, F} (true iff considering dir’s first, i.e., adjacent, child); and (iii) attachments (of class ca), according to PATTACH(ch, dir, ca). This produces only projective trees.7 A root token Q generates th</context>
<context position="16453" citStr="Klein and Manning, 2004" startWordPosition="2566" endWordPosition="2570">ing via inside-outside re-estimation (Baker, 1979). Klein and Manning did not use smoothing and started with an “ad-hoc harmonic” completion: aiming for balanced trees, non-root heads attached dependents in inverse proportion to (a constant plus) their distance; Q generated heads uniformly at random. This non-distributional heuristic created favorable initial conditions that nudged EM towards typical linguistic dependency structures. 5.1 Algorithm #0: Ad-Hoc* — A Variation on Original Ad-Hoc Initialization Since some of the important implementation details are not available in the literature (Klein and Manning, 2004; Klein, 2005), we had to improvise initialization and terminating conditions. We suspect that our choices throughout this section do not match Klein and Manning’s actual training of the DMV. We use the following ad-hoc harmonic scores (for all tokens other than Q): 1�ORDER = 1/2; PSTOP = (ds + 6s)−1 = (ds + 3)−1, ds &gt; 0; �PATTACH = (da + 6a)−1 = (da + 2)−1, da &gt; 1. Integers d1s,a} are distances from heads to stopping boundaries and dependents.8 We initialize training by producing best-scoring parses of all input sentences and converting them into proper probability distributions PSTOP and PAT</context>
<context position="28325" citStr="Klein and Manning, 2004" startWordPosition="4557" endWordPosition="4560">h worse against MBR. “Less is More” beats state-of-the-art on longer sentences by close to 2%; Leapfrog gains another 1%. 757 Ad-Hoc* Baby Steps Leapfrog Ad-Hoc* Baby Steps Leapfrog Section 23 44.1 (58.8) 39.2 (53.8) 43.3 (55.7) 31.5 (51.6) 39.4 (54.0) 45.0 (58.4) WSJ100 43.8 (58.6) 39.2 (53.8) 43.3 (55.6) @15 31.3 (51.5) 39.4 (54.1) 44.7 (58.1) @45 Brown100 43.3 (59.2) 42.3 (55.1) 42.8 (56.5) 32.0 (52.4) 42.5 (55.5) 43.6 (59.1) Table 1: Directed and undirected accuracies on Section 23 of WSJ&apos;, WSJ100 and Brown100 for Ad-Hoc*, Baby Steps and Leapfrog, trained at WSJ15 and WSJ45. Attach-Right (Klein and Manning, 2004) Decoding WSJ10 WSJ20 WSJ° — 38.4 33.4 31.7 DMV Ad-Hoc (Klein and Manning, 2004) Viterbi 45.8 39.1 34.2 Dirichlet (Cohen et al., 2008) Viterbi 45.9 39.4 34.9 Ad-Hoc (Cohen et al., 2008) MBR 46.1 39.9 35.9 Dirichlet (Cohen et al., 2008) MBR 46.1 40.6 36.9 Log-Normal Families (Cohen et al., 2008) Viterbi 59.3 45.1 39.0 Baby Steps (@15) Viterbi 55.5 44.3 39.2 Baby Steps (@45) Viterbi 55.1 44.4 39.4 Log-Normal Families (Cohen et al., 2008) MBR 59.4 45.9 40.5 Shared Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) MBR 61.3 47.4 41.4 Bilingual Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) M</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>D. Klein and C. D. Manning. 2004. Corpus-based induction of syntactic structure: Models of dependency and constituency. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
</authors>
<title>The Unsupervised Learning of Natural Language Structure.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="16467" citStr="Klein, 2005" startWordPosition="2571" endWordPosition="2572">-estimation (Baker, 1979). Klein and Manning did not use smoothing and started with an “ad-hoc harmonic” completion: aiming for balanced trees, non-root heads attached dependents in inverse proportion to (a constant plus) their distance; Q generated heads uniformly at random. This non-distributional heuristic created favorable initial conditions that nudged EM towards typical linguistic dependency structures. 5.1 Algorithm #0: Ad-Hoc* — A Variation on Original Ad-Hoc Initialization Since some of the important implementation details are not available in the literature (Klein and Manning, 2004; Klein, 2005), we had to improvise initialization and terminating conditions. We suspect that our choices throughout this section do not match Klein and Manning’s actual training of the DMV. We use the following ad-hoc harmonic scores (for all tokens other than Q): 1�ORDER = 1/2; PSTOP = (ds + 6s)−1 = (ds + 3)−1, ds &gt; 0; �PATTACH = (da + 6a)−1 = (da + 2)−1, da &gt; 1. Integers d1s,a} are distances from heads to stopping boundaries and dependents.8 We initialize training by producing best-scoring parses of all input sentences and converting them into proper probability distributions PSTOP and PATTACH via maxim</context>
</contexts>
<marker>Klein, 2005</marker>
<rawString>D. Klein. 2005. The Unsupervised Learning of Natural Language Structure. Ph.D. thesis, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K A Krueger</author>
<author>P Dayan</author>
</authors>
<title>Flexible shaping: How learning in small steps helps.</title>
<date>2009</date>
<journal>Cognition,</journal>
<volume>110</volume>
<contexts>
<context position="9782" citStr="Krueger and Dayan (2009)" startWordPosition="1479" endWordPosition="1482">onal in natural language processing. Such scaffolding qualifies as shaping — a method of instruction (routinely exploited in animal training) in which the teacher decomposes a complete task into sub-components, providing an easier path to learning. When Skinner (1938) coined the term, he described it as a “method of successive approximations.” Ideas that gradually make a task more difficult have been explored in robotics (typically, for navigation), with reinforcement learning (Singh, 1992; Sanger, 1994; Saksida et al., 1997; Dorigo and Colombetti, 1998; Savage, 1998; Savage, 2001). Recently, Krueger and Dayan (2009) showed that shaping speeds up language acquisition and leads to better generalization in abstract neural networks. Bengio et al. (2009) confirmed this for deep deterministic and stochastic networks, using simple multi-stage curriculum strategies. They conjectured that a well-chosen sequence of training criteria — different sets of weights on the examples — could act as a continuation method (Allgower and Georg, 1990), helping find better local optima for nonconvex objectives. Elman’s learners constrained the peaky solution space by focusing on just the right data (simple sentences that introd</context>
<context position="27248" citStr="Krueger and Dayan, 2009" startWordPosition="4389" endWordPosition="4392">SJ3). 6.3 Result #3: Leapfrog Mixing Ad-Hoc* with Baby Steps at WSJ15 yields a model whose performance initially falls between its two parents but surpasses both with a little training (see Figure 5). Leaping to WSJ45, via WSJ30, results in our strongest model: its 45.0% (58.4%) accuracy bridges half of the gap between Baby Steps and the skyline, and at a tiny fraction of the cost. 6.4 Result #4: Generalization Our models carry over to the larger WSJ100, Section 23 of WSJ&apos;, and the independent Brown100 (see Table 1). Baby Steps improves out of domain, confirming that shaping generalizes well (Krueger and Dayan, 2009; Bengio et al., 2009). Leapfrog does best across the board but dips on Brown100, despite its safe-guards against over-fitting. Section 23 (see Table 2) reveals, unexpectedly, that Baby Steps would have been state-of-the-art in 2008, whereas “Less is More” outperforms all prior work on longer sentences. Baby Steps is competitive with log-normal families (Cohen et al., 2008), scoring slightly better on longer sentences against Viterbi decoding, though worse against MBR. “Less is More” beats state-of-the-art on longer sentences by close to 2%; Leapfrog gains another 1%. 757 Ad-Hoc* Baby Steps Le</context>
</contexts>
<marker>Krueger, Dayan, 2009</marker>
<rawString>K. A. Krueger and P. Dayan. 2009. Flexible shaping: How learning in small steps helps. Cognition, 110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>D Klein</author>
</authors>
<title>Analyzing the errors of unsupervised learning.</title>
<date>2008</date>
<booktitle>In Proc. of HLT-ACL.</booktitle>
<contexts>
<context position="25140" citStr="Liang and Klein, 2008" startWordPosition="4036" endWordPosition="4039">(Elworthy, 1994; Meri50 40 30 20 Ad-Hoc Directed Accuracy (%) on WSJk 756 80 60 50 40 30 20 70 Ad-Hoc Less is More � v � (a) Directed Accuracy (%) on WSJ10 Uninformed Ad-Hoc* Oracle Leapfrog Baby Steps (b) Directed Accuracy (%) on WSJ40 Uninformed Less is More � v � Oracle Ad-Hoc* Leapfrog Baby Steps 5 10 15 20 25 30 35 40 WSJ� 5 10 15 20 25 30 35 40 45 Figure 6: Directed accuracies attained by the DMV, when trained at various gradations of WSJ, smoothed, then tested against fixed evaluation sets — WSJ{10, 40}; graphs for WSJ{20, 30}, not shown, are qualitatively similar to WSJ40. aldo, 1994; Liang and Klein, 2008), but the extent of its drops is alarming, e.g., from the supervised 69.8% (72.2%) to the skyline’s 50.6% (59.5%) on WSJ45. In contrast, Baby Steps’ scores usually do not change much from one step to the next, and where its impact of learning is big (at WSJ{4, 5, 14}), it is invariably positive. 6.2 Result #2: Less is More Ad-Hoc*’s curve (see Figure 5) suggests how Klein and Manning’s Ad-Hoc initializer may have scaled with different gradations of WSJ. Strangely, our implementation performs significantly above their reported numbers at WSJ10: 54.5% (68.3%) is even slightly higher than Baby St</context>
</contexts>
<marker>Liang, Klein, 2008</marker>
<rawString>P. Liang and D. Klein. 2008. Analyzing the errors of unsupervised learning. In Proc. of HLT-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="12319" citStr="Marcus et al., 1993" startWordPosition="1865" endWordPosition="1868"> works “out of the box,” its locality biases a natural consequence of a complexity/data-guided tour of optimization problems. Skewed DA incorporates a good initializer by interpolating between two probability distributions, whereas our hybrid, Leapfrog, admits multiple initializers by mixing structures instead. “Less is More” is novel and confirms the tacit consensus implicit in training on small data sets (e.g., WSJ10). 4 Data Sets and Metrics Klein and Manning (2004) both trained and tested the DMV on the same customized subset (WSJ10) of Penn English Treebank’s Wall Street Journal portion (Marcus et al., 1993). Its 49,208 annotated parse trees were pruned5 down to 7,422 sentences of at most 10 terminals, spanning 35 unique POS tags. Following standard practice, automatic “headpercolation” rules (Collins, 1999) were used to convert the remaining trees into dependencies. Forced to produce a single “best” parse, their algorithm was judged on accuracy: its directed score was the fraction of correct dependencies; a more flattering6 undirected score was also used. We employ the same metrics, emphasizing directed scores, and generalize WSJk to be the subset of pre-processed sentences with at most k termin</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McClosky</author>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proc. ofNAACL-HLT.</booktitle>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>D. McClosky, E. Charniak, and M. Johnson. 2006. Effective self-training for parsing. In Proc. ofNAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
<author>K Ribarov</author>
<author>J Hajic</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proc. of HLT-EMNLP.</booktitle>
<contexts>
<context position="15770" citStr="McDonald et al., 2005" startWordPosition="2469" endWordPosition="2472">independent decisions: (i) initial direction dir E {L, R} in which to attach children, via probability PORDER(ch); (ii) whether to seal dir, stopping with probability PSTOP(ch, dir, adj), conditioned on adj E {T, F} (true iff considering dir’s first, i.e., adjacent, child); and (iii) attachments (of class ca), according to PATTACH(ch, dir, ca). This produces only projective trees.7 A root token Q generates the head of a sentence as its left (and only) child. Figure 2 displays an example that ignores (sums out) PORDER. The DMV lends itself to unsupervised learn7Unlike spanning tree algorithms (McDonald et al., 2005), DMV’s chart-based method disallows crossing dependencies. ing via inside-outside re-estimation (Baker, 1979). Klein and Manning did not use smoothing and started with an “ad-hoc harmonic” completion: aiming for balanced trees, non-root heads attached dependents in inverse proportion to (a constant plus) their distance; Q generated heads uniformly at random. This non-distributional heuristic created favorable initial conditions that nudged EM towards typical linguistic dependency structures. 5.1 Algorithm #0: Ad-Hoc* — A Variation on Original Ad-Hoc Initialization Since some of the important </context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proc. of HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Merialdo</author>
</authors>
<title>Tagging English text with a probabilistic model.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<marker>Merialdo, 1994</marker>
<rawString>B. Merialdo. 1994. Tagging English text with a probabilistic model. Computational Linguistics, 20(2):155–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E L Newport</author>
</authors>
<title>Constraints on learning and their role in language acquisition:</title>
<date>1988</date>
<journal>Studies of the acquisition of American Sign Language. Language Sciences,</journal>
<volume>10</volume>
<issue>1</issue>
<contexts>
<context position="7564" citStr="Newport, 1988" startWordPosition="1159" endWordPosition="1160">spot data gradation, then iterate with a large step size. 3 Related Work Two types of scaffolding for guiding language learning debuted in Elman’s (1993) experiments with “starting small”: data complexity (restricting input) and model complexity (restricting memory). In both cases, gradually increasing complexity allowed artificial neural networks to master a pseudo-natural grammar they otherwise failed to learn. Initiallylimited capacity resembled maturational changes in working memory and attention span that occur over time in children (Kail, 1984), in line with the “less is more” proposal (Newport, 1988; 1990). Although Rohde and Plaut (1999) failed to replicate this3 result with simple recurrent networks, other machine learning techniques reliably benefit from scaffolded model complexity on a variety of language tasks. In word-alignment, Brown et al. (1993) used IBM Models 1-4 as “stepping stones” to training Model 5. Other prominent examples include “coarse-to-fine” 2This is akin to McClosky et al.’s (2006) “Goldilocks effect.” 3Worse, they found that limiting input hindered language acquisition. And making the grammar more English-like (by introducing and strengthening semantic constraint</context>
</contexts>
<marker>Newport, 1988</marker>
<rawString>E. L. Newport. 1988. Constraints on learning and their role in language acquisition: Studies of the acquisition of American Sign Language. Language Sciences, 10(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E L Newport</author>
</authors>
<title>Maturational constraints on language learning.</title>
<date>1990</date>
<journal>Cognitive Science,</journal>
<volume>14</volume>
<issue>1</issue>
<marker>Newport, 1990</marker>
<rawString>E. L. Newport. 1990. Maturational constraints on language learning. Cognitive Science, 14(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>A Haghighi</author>
<author>D Klein</author>
</authors>
<title>Coarse-to-fine syntactic machine translation using language projections.</title>
<date>2008</date>
<booktitle>In Proc. ofEMNLP.</booktitle>
<contexts>
<context position="8866" citStr="Petrov et al., 2008" startWordPosition="1345" endWordPosition="1348">e training invoking the optimizer multiple times, creating extra opportunities to converge, Rohde and Plaut (1999) suspected that Elman’s (1993) simulations simply did not allow networks exposed exclusively to complex inputs sufficient training time. Our extremely generous, low termination threshold for EM (see §5.1) addresses this concern. However, given the DMV’s purely syntactic POS tag-based approach (see §5), it would be prudent to re-test Baby Steps with a lexicalized model. 752 approaches to parsing, translation and speech recognition (Charniak and Johnson, 2005; Charniak et al., 2006; Petrov et al., 2008; Petrov, 2009), and recently unsupervised POS tagging (Ravi and Knight, 2009). Initial models tend to be particularly simple,4 and each refinement towards a full model introduces only limited complexity, supporting incrementality. Filtering complex data, the focus of our work, is unconventional in natural language processing. Such scaffolding qualifies as shaping — a method of instruction (routinely exploited in animal training) in which the teacher decomposes a complete task into sub-components, providing an easier path to learning. When Skinner (1938) coined the term, he described it as a “</context>
</contexts>
<marker>Petrov, Haghighi, Klein, 2008</marker>
<rawString>S. Petrov, A. Haghighi, and D. Klein. 2008. Coarse-to-fine syntactic machine translation using language projections. In Proc. ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S O Petrov</author>
</authors>
<title>Coarse-to-Fine Natural Language Processing.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California, Berkeley.</institution>
<contexts>
<context position="8881" citStr="Petrov, 2009" startWordPosition="1349" endWordPosition="1350">he optimizer multiple times, creating extra opportunities to converge, Rohde and Plaut (1999) suspected that Elman’s (1993) simulations simply did not allow networks exposed exclusively to complex inputs sufficient training time. Our extremely generous, low termination threshold for EM (see §5.1) addresses this concern. However, given the DMV’s purely syntactic POS tag-based approach (see §5), it would be prudent to re-test Baby Steps with a lexicalized model. 752 approaches to parsing, translation and speech recognition (Charniak and Johnson, 2005; Charniak et al., 2006; Petrov et al., 2008; Petrov, 2009), and recently unsupervised POS tagging (Ravi and Knight, 2009). Initial models tend to be particularly simple,4 and each refinement towards a full model introduces only limited complexity, supporting incrementality. Filtering complex data, the focus of our work, is unconventional in natural language processing. Such scaffolding qualifies as shaping — a method of instruction (routinely exploited in animal training) in which the teacher decomposes a complete task into sub-components, providing an easier path to learning. When Skinner (1938) coined the term, he described it as a “method of succe</context>
</contexts>
<marker>Petrov, 2009</marker>
<rawString>S. O. Petrov. 2009. Coarse-to-Fine Natural Language Processing. Ph.D. thesis, University of California, Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ravi</author>
<author>K Knight</author>
</authors>
<title>Minimized models for unsupervised part-of-speech tagging.</title>
<date>2009</date>
<booktitle>In Proc. ofACL-IJCNLP.</booktitle>
<contexts>
<context position="8944" citStr="Ravi and Knight, 2009" startWordPosition="1357" endWordPosition="1360">ies to converge, Rohde and Plaut (1999) suspected that Elman’s (1993) simulations simply did not allow networks exposed exclusively to complex inputs sufficient training time. Our extremely generous, low termination threshold for EM (see §5.1) addresses this concern. However, given the DMV’s purely syntactic POS tag-based approach (see §5), it would be prudent to re-test Baby Steps with a lexicalized model. 752 approaches to parsing, translation and speech recognition (Charniak and Johnson, 2005; Charniak et al., 2006; Petrov et al., 2008; Petrov, 2009), and recently unsupervised POS tagging (Ravi and Knight, 2009). Initial models tend to be particularly simple,4 and each refinement towards a full model introduces only limited complexity, supporting incrementality. Filtering complex data, the focus of our work, is unconventional in natural language processing. Such scaffolding qualifies as shaping — a method of instruction (routinely exploited in animal training) in which the teacher decomposes a complete task into sub-components, providing an easier path to learning. When Skinner (1938) coined the term, he described it as a “method of successive approximations.” Ideas that gradually make a task more di</context>
</contexts>
<marker>Ravi, Knight, 2009</marker>
<rawString>S. Ravi and K. Knight. 2009. Minimized models for unsupervised part-of-speech tagging. In Proc. ofACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D L T Rohde</author>
<author>D C Plaut</author>
</authors>
<title>Language acquisition in the absence of explicit negative evidence: How important is starting small?</title>
<date>1999</date>
<journal>Cognition,</journal>
<volume>72</volume>
<issue>1</issue>
<contexts>
<context position="7604" citStr="Rohde and Plaut (1999)" startWordPosition="1163" endWordPosition="1166">te with a large step size. 3 Related Work Two types of scaffolding for guiding language learning debuted in Elman’s (1993) experiments with “starting small”: data complexity (restricting input) and model complexity (restricting memory). In both cases, gradually increasing complexity allowed artificial neural networks to master a pseudo-natural grammar they otherwise failed to learn. Initiallylimited capacity resembled maturational changes in working memory and attention span that occur over time in children (Kail, 1984), in line with the “less is more” proposal (Newport, 1988; 1990). Although Rohde and Plaut (1999) failed to replicate this3 result with simple recurrent networks, other machine learning techniques reliably benefit from scaffolded model complexity on a variety of language tasks. In word-alignment, Brown et al. (1993) used IBM Models 1-4 as “stepping stones” to training Model 5. Other prominent examples include “coarse-to-fine” 2This is akin to McClosky et al.’s (2006) “Goldilocks effect.” 3Worse, they found that limiting input hindered language acquisition. And making the grammar more English-like (by introducing and strengthening semantic constraints), increased the already significant ad</context>
</contexts>
<marker>Rohde, Plaut, 1999</marker>
<rawString>D. L. T. Rohde and D. C. Plaut. 1999. Language acquisition in the absence of explicit negative evidence: How important is starting small? Cognition, 72(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L M Saksida</author>
<author>S M Raymond</author>
<author>D S Touretzky</author>
</authors>
<title>Shaping robot behavior using principles from instrumental conditioning. Robotics and Autonomous Systems,</title>
<date>1997</date>
<contexts>
<context position="9688" citStr="Saksida et al., 1997" startWordPosition="1466" endWordPosition="1469">y, supporting incrementality. Filtering complex data, the focus of our work, is unconventional in natural language processing. Such scaffolding qualifies as shaping — a method of instruction (routinely exploited in animal training) in which the teacher decomposes a complete task into sub-components, providing an easier path to learning. When Skinner (1938) coined the term, he described it as a “method of successive approximations.” Ideas that gradually make a task more difficult have been explored in robotics (typically, for navigation), with reinforcement learning (Singh, 1992; Sanger, 1994; Saksida et al., 1997; Dorigo and Colombetti, 1998; Savage, 1998; Savage, 2001). Recently, Krueger and Dayan (2009) showed that shaping speeds up language acquisition and leads to better generalization in abstract neural networks. Bengio et al. (2009) confirmed this for deep deterministic and stochastic networks, using simple multi-stage curriculum strategies. They conjectured that a well-chosen sequence of training criteria — different sets of weights on the examples — could act as a continuation method (Allgower and Georg, 1990), helping find better local optima for nonconvex objectives. Elman’s learners constra</context>
</contexts>
<marker>Saksida, Raymond, Touretzky, 1997</marker>
<rawString>L. M. Saksida, S. M. Raymond, and D. S. Touretzky. 1997. Shaping robot behavior using principles from instrumental conditioning. Robotics and Autonomous Systems, 22(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Salvador</author>
<author>P Chan</author>
</authors>
<title>Determining the number of clusters/segments in hierarchical clustering/segmentation algorithms.</title>
<date>2004</date>
<booktitle>In Proc. ofICTAI.</booktitle>
<contexts>
<context position="18950" citStr="Salvador and Chan, 2004" startWordPosition="2987" endWordPosition="2990">lutions could be passed to next levels, which sometimes contain previously unseen dependent and head POS tags. 5.3 Algorithm #2: Less is More — Ad-Hoc* where Baby Steps Flatlines We jettison long, complex sentences and deploy AdHoc*’s initializer and batch training at WSJk* — an estimate of the sweet spot data gradation. To find it, we track Baby Steps’ successive models’ crossentropies on the complete data set, WSJ45. An initial segment of rapid improvement is separated from the final region of convergence by a knee — points of maximum curvature (see Figure 3). We use an improved10 L method (Salvador and Chan, 2004) to automatically locate this area of diminishing returns. Specifically, we determine its end-points [k0, k*] by minimizing squared error, estimating �k0 = 7 and k* = 15. Training at WSJ15 just misses the plateau. 5.4 Algorithm #3: Leapfrog — A Practical and Efficient Hybrid Mixture Cherry-picking the best features of “Less is More” and Baby Steps, we begin by combining their mod9Its 48,418 sentences (see Figure 1) cover 94.4% of all sentences in WSJ; the longest of the missing 790 has length 171. 10Instead of iteratively fitting a two-segment form and adaptively discarding its tail, we use th</context>
</contexts>
<marker>Salvador, Chan, 2004</marker>
<rawString>S. Salvador and P. Chan. 2004. Determining the number of clusters/segments in hierarchical clustering/segmentation algorithms. In Proc. ofICTAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T D Sanger</author>
</authors>
<title>Neural network learning control of robot manipulators using gradually increasing task difficulty.</title>
<date>1994</date>
<journal>IEEE Trans. on Robotics and Automation,</journal>
<volume>10</volume>
<contexts>
<context position="9666" citStr="Sanger, 1994" startWordPosition="1464" endWordPosition="1465">ited complexity, supporting incrementality. Filtering complex data, the focus of our work, is unconventional in natural language processing. Such scaffolding qualifies as shaping — a method of instruction (routinely exploited in animal training) in which the teacher decomposes a complete task into sub-components, providing an easier path to learning. When Skinner (1938) coined the term, he described it as a “method of successive approximations.” Ideas that gradually make a task more difficult have been explored in robotics (typically, for navigation), with reinforcement learning (Singh, 1992; Sanger, 1994; Saksida et al., 1997; Dorigo and Colombetti, 1998; Savage, 1998; Savage, 2001). Recently, Krueger and Dayan (2009) showed that shaping speeds up language acquisition and leads to better generalization in abstract neural networks. Bengio et al. (2009) confirmed this for deep deterministic and stochastic networks, using simple multi-stage curriculum strategies. They conjectured that a well-chosen sequence of training criteria — different sets of weights on the examples — could act as a continuation method (Allgower and Georg, 1990), helping find better local optima for nonconvex objectives. El</context>
</contexts>
<marker>Sanger, 1994</marker>
<rawString>T. D. Sanger. 1994. Neural network learning control of robot manipulators using gradually increasing task difficulty. IEEE Trans. on Robotics and Automation, 10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Savage</author>
</authors>
<title>Shaping: The link between rats and robots.</title>
<date>1998</date>
<journal>Connection Science,</journal>
<volume>10</volume>
<issue>3</issue>
<contexts>
<context position="9731" citStr="Savage, 1998" startWordPosition="1474" endWordPosition="1475">a, the focus of our work, is unconventional in natural language processing. Such scaffolding qualifies as shaping — a method of instruction (routinely exploited in animal training) in which the teacher decomposes a complete task into sub-components, providing an easier path to learning. When Skinner (1938) coined the term, he described it as a “method of successive approximations.” Ideas that gradually make a task more difficult have been explored in robotics (typically, for navigation), with reinforcement learning (Singh, 1992; Sanger, 1994; Saksida et al., 1997; Dorigo and Colombetti, 1998; Savage, 1998; Savage, 2001). Recently, Krueger and Dayan (2009) showed that shaping speeds up language acquisition and leads to better generalization in abstract neural networks. Bengio et al. (2009) confirmed this for deep deterministic and stochastic networks, using simple multi-stage curriculum strategies. They conjectured that a well-chosen sequence of training criteria — different sets of weights on the examples — could act as a continuation method (Allgower and Georg, 1990), helping find better local optima for nonconvex objectives. Elman’s learners constrained the peaky solution space by focusing o</context>
</contexts>
<marker>Savage, 1998</marker>
<rawString>T. Savage. 1998. Shaping: The link between rats and robots. Connection Science, 10(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Savage</author>
</authors>
<title>Shaping: A multiple contingencies analysis and its relevance to behaviour-based robotics.</title>
<date>2001</date>
<journal>Connection Science,</journal>
<volume>13</volume>
<issue>3</issue>
<contexts>
<context position="9746" citStr="Savage, 2001" startWordPosition="1476" endWordPosition="1477">f our work, is unconventional in natural language processing. Such scaffolding qualifies as shaping — a method of instruction (routinely exploited in animal training) in which the teacher decomposes a complete task into sub-components, providing an easier path to learning. When Skinner (1938) coined the term, he described it as a “method of successive approximations.” Ideas that gradually make a task more difficult have been explored in robotics (typically, for navigation), with reinforcement learning (Singh, 1992; Sanger, 1994; Saksida et al., 1997; Dorigo and Colombetti, 1998; Savage, 1998; Savage, 2001). Recently, Krueger and Dayan (2009) showed that shaping speeds up language acquisition and leads to better generalization in abstract neural networks. Bengio et al. (2009) confirmed this for deep deterministic and stochastic networks, using simple multi-stage curriculum strategies. They conjectured that a well-chosen sequence of training criteria — different sets of weights on the examples — could act as a continuation method (Allgower and Georg, 1990), helping find better local optima for nonconvex objectives. Elman’s learners constrained the peaky solution space by focusing on just the righ</context>
</contexts>
<marker>Savage, 2001</marker>
<rawString>T. Savage. 2001. Shaping: A multiple contingencies analysis and its relevance to behaviour-based robotics. Connection Science, 13(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Seginer</author>
</authors>
<title>Fast unsupervised incremental parsing.</title>
<date>2007</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="2086" citStr="Seginer, 2007" startWordPosition="298" endWordPosition="299">rarchical syntactic structure from free-form natural language text is a hard problem whose eventual solution promises to benefit applications ranging from question answering to speech recognition and machine translation. A restricted version that targets dependencies and *Partially funded by NSF award IIS-0811974; first author supported by the Fannie &amp; John Hertz Foundation Fellowship. assumes partial annotation, e.g., sentence boundaries, tokenization and typically even part-of-speech (POS) tagging, has received much attention, eliciting a diverse array of techniques (Smith and Eisner, 2005; Seginer, 2007; Cohen et al., 2008). Klein and Manning’s (2004) Dependency Model with Valence (DMV) was the first to beat a simple parsing heuristic — the right-branching baseline. Today’s state-of-the-art systems (Headden et al., 2009; Cohen and Smith, 2009) are still rooted in the DMV. Despite recent advances, unsupervised parsers lag far behind their supervised counterparts. Although large amounts of unlabeled data are known to improve semi-supervised parsing (Suzuki et al., 2009), the best unsupervised systems use less data than is available for supervised training, relying on complex models instead: He</context>
</contexts>
<marker>Seginer, 2007</marker>
<rawString>Y. Seginer. 2007. Fast unsupervised incremental parsing. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S P Singh</author>
</authors>
<title>Transfer of learning by composing solutions of elemental squential tasks.</title>
<date>1992</date>
<booktitle>Machine Learning,</booktitle>
<volume>8</volume>
<contexts>
<context position="9652" citStr="Singh, 1992" startWordPosition="1462" endWordPosition="1463">uces only limited complexity, supporting incrementality. Filtering complex data, the focus of our work, is unconventional in natural language processing. Such scaffolding qualifies as shaping — a method of instruction (routinely exploited in animal training) in which the teacher decomposes a complete task into sub-components, providing an easier path to learning. When Skinner (1938) coined the term, he described it as a “method of successive approximations.” Ideas that gradually make a task more difficult have been explored in robotics (typically, for navigation), with reinforcement learning (Singh, 1992; Sanger, 1994; Saksida et al., 1997; Dorigo and Colombetti, 1998; Savage, 1998; Savage, 2001). Recently, Krueger and Dayan (2009) showed that shaping speeds up language acquisition and leads to better generalization in abstract neural networks. Bengio et al. (2009) confirmed this for deep deterministic and stochastic networks, using simple multi-stage curriculum strategies. They conjectured that a well-chosen sequence of training criteria — different sets of weights on the examples — could act as a continuation method (Allgower and Georg, 1990), helping find better local optima for nonconvex </context>
</contexts>
<marker>Singh, 1992</marker>
<rawString>S. P. Singh. 1992. Transfer of learning by composing solutions of elemental squential tasks. Machine Learning, 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B F Skinner</author>
</authors>
<title>The behavior of organisms: An experimental analysis. Appleton-Century-Crofts.</title>
<date>1938</date>
<contexts>
<context position="9426" citStr="Skinner (1938)" startWordPosition="1428" endWordPosition="1429">nson, 2005; Charniak et al., 2006; Petrov et al., 2008; Petrov, 2009), and recently unsupervised POS tagging (Ravi and Knight, 2009). Initial models tend to be particularly simple,4 and each refinement towards a full model introduces only limited complexity, supporting incrementality. Filtering complex data, the focus of our work, is unconventional in natural language processing. Such scaffolding qualifies as shaping — a method of instruction (routinely exploited in animal training) in which the teacher decomposes a complete task into sub-components, providing an easier path to learning. When Skinner (1938) coined the term, he described it as a “method of successive approximations.” Ideas that gradually make a task more difficult have been explored in robotics (typically, for navigation), with reinforcement learning (Singh, 1992; Sanger, 1994; Saksida et al., 1997; Dorigo and Colombetti, 1998; Savage, 1998; Savage, 2001). Recently, Krueger and Dayan (2009) showed that shaping speeds up language acquisition and leads to better generalization in abstract neural networks. Bengio et al. (2009) confirmed this for deep deterministic and stochastic networks, using simple multi-stage curriculum strategi</context>
</contexts>
<marker>Skinner, 1938</marker>
<rawString>B. F. Skinner. 1938. The behavior of organisms: An experimental analysis. Appleton-Century-Crofts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N A Smith</author>
<author>J Eisner</author>
</authors>
<title>Annealing techniques for unsupervised statistical language learning.</title>
<date>2004</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="10729" citStr="Smith and Eisner, 2004" startWordPosition="1617" endWordPosition="1620">t sets of weights on the examples — could act as a continuation method (Allgower and Georg, 1990), helping find better local optima for nonconvex objectives. Elman’s learners constrained the peaky solution space by focusing on just the right data (simple sentences that introduced basic representational categories) at just the right time (early on, when their plasticity was greatest). Self-shaping, they simplified tasks through deliberate omission (or misunderstanding). Analogously, Baby Steps induces an early structural locality bias (Smith and Eisner, 2006), then relaxes it, as if annealing (Smith and Eisner, 2004). Its curriculum of binary weights initially discards complex examples responsible for “high-frequency noise,” with earlier, “smoothed” objectives revealing more of the global picture. There are important differences between our results and prior work. In contrast to Elman, we use a 4Brown et al.’s (1993) Model 1 (and, similarly, the first baby step) has a global optimum that can be computed exactly, so that no initial or subsequent parameters depend on initialization. large data set (WSJ) of real English. Unlike Bengio et al. and Krueger and Dayan, we shape a parser, not a language model. Bab</context>
</contexts>
<marker>Smith, Eisner, 2004</marker>
<rawString>N. A. Smith and J. Eisner. 2004. Annealing techniques for unsupervised statistical language learning. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N A Smith</author>
<author>J Eisner</author>
</authors>
<title>Guiding unsupervised grammar induction using contrastive estimation.</title>
<date>2005</date>
<booktitle>In Proc. of the IJCAI Workshop on Grammatical Inference Applications.</booktitle>
<contexts>
<context position="2071" citStr="Smith and Eisner, 2005" startWordPosition="293" endWordPosition="297">pervised learning of hierarchical syntactic structure from free-form natural language text is a hard problem whose eventual solution promises to benefit applications ranging from question answering to speech recognition and machine translation. A restricted version that targets dependencies and *Partially funded by NSF award IIS-0811974; first author supported by the Fannie &amp; John Hertz Foundation Fellowship. assumes partial annotation, e.g., sentence boundaries, tokenization and typically even part-of-speech (POS) tagging, has received much attention, eliciting a diverse array of techniques (Smith and Eisner, 2005; Seginer, 2007; Cohen et al., 2008). Klein and Manning’s (2004) Dependency Model with Valence (DMV) was the first to beat a simple parsing heuristic — the right-branching baseline. Today’s state-of-the-art systems (Headden et al., 2009; Cohen and Smith, 2009) are still rooted in the DMV. Despite recent advances, unsupervised parsers lag far behind their supervised counterparts. Although large amounts of unlabeled data are known to improve semi-supervised parsing (Suzuki et al., 2009), the best unsupervised systems use less data than is available for supervised training, relying on complex mod</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>N. A. Smith and J. Eisner. 2005. Guiding unsupervised grammar induction using contrastive estimation. In Proc. of the IJCAI Workshop on Grammatical Inference Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N A Smith</author>
<author>J Eisner</author>
</authors>
<title>Annealing structural bias in multilingual weighted grammar induction.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL.</booktitle>
<contexts>
<context position="10670" citStr="Smith and Eisner, 2006" startWordPosition="1607" endWordPosition="1610">that a well-chosen sequence of training criteria — different sets of weights on the examples — could act as a continuation method (Allgower and Georg, 1990), helping find better local optima for nonconvex objectives. Elman’s learners constrained the peaky solution space by focusing on just the right data (simple sentences that introduced basic representational categories) at just the right time (early on, when their plasticity was greatest). Self-shaping, they simplified tasks through deliberate omission (or misunderstanding). Analogously, Baby Steps induces an early structural locality bias (Smith and Eisner, 2006), then relaxes it, as if annealing (Smith and Eisner, 2004). Its curriculum of binary weights initially discards complex examples responsible for “high-frequency noise,” with earlier, “smoothed” objectives revealing more of the global picture. There are important differences between our results and prior work. In contrast to Elman, we use a 4Brown et al.’s (1993) Model 1 (and, similarly, the first baby step) has a global optimum that can be computed exactly, so that no initial or subsequent parameters depend on initialization. large data set (WSJ) of real English. Unlike Bengio et al. and Krue</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>N. A. Smith and J. Eisner. 2006. Annealing structural bias in multilingual weighted grammar induction. In Proc. of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Suzuki</author>
<author>H Isozaki</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>An empirical study of semi-supervised structured conditional models for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="2560" citStr="Suzuki et al., 2009" startWordPosition="369" endWordPosition="372">lly even part-of-speech (POS) tagging, has received much attention, eliciting a diverse array of techniques (Smith and Eisner, 2005; Seginer, 2007; Cohen et al., 2008). Klein and Manning’s (2004) Dependency Model with Valence (DMV) was the first to beat a simple parsing heuristic — the right-branching baseline. Today’s state-of-the-art systems (Headden et al., 2009; Cohen and Smith, 2009) are still rooted in the DMV. Despite recent advances, unsupervised parsers lag far behind their supervised counterparts. Although large amounts of unlabeled data are known to improve semi-supervised parsing (Suzuki et al., 2009), the best unsupervised systems use less data than is available for supervised training, relying on complex models instead: Headden et al.’s (2009) Extended Valence Grammar (EVG) combats data sparsity with smoothing alone, training on the same small subset of the tree-bank as the classic implementation of the DMV; Cohen and Smith (2009) use more complicated algorithms (variational EM and MBR decoding) and stronger linguistic hints (tying related parts of speech and syntactically similar bilingual data). We explore what can be achieved through judicious use of data and simple, scalable techniqu</context>
</contexts>
<marker>Suzuki, Isozaki, Carreras, Collins, 2009</marker>
<rawString>J. Suzuki, H. Isozaki, X. Carreras, and M. Collins. 2009. An empirical study of semi-supervised structured conditional models for dependency parsing. In Proc. of EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>