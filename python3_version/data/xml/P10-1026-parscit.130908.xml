<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.985286">
A Bayesian Method for Robust Estimation of Distributional Similarities
</title>
<author confidence="0.874856">
Jun’ichi Kazama Stijn De Saeger Kow Kuroda
Masaki Murata† Kentaro Torisawa
</author>
<affiliation confidence="0.985237">
Language Infrastructure Group, MASTAR Project
National Institute of Information and Communications Technology (NICT)
</affiliation>
<address confidence="0.896699">
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0289 Japan
</address>
<email confidence="0.833209">
{kazama, stijn, kuroda, torisawa}@nict.go.jp
</email>
<affiliation confidence="0.855101">
†Department of Information and Knowledge Engineering
Faculty/Graduate School of Engineering, Tottori University
</affiliation>
<address confidence="0.874121">
4-101 Koyama-Minami, Tottori, 680-8550 Japan∗
</address>
<email confidence="0.99497">
murata@ike.tottori-u.ac.jp
</email>
<sectionHeader confidence="0.994962" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99997495">
Existing word similarity measures are not
robust to data sparseness since they rely
only on the point estimation of words’
context profiles obtained from a limited
amount of data. This paper proposes a
Bayesian method for robust distributional
word similarities. The method uses a dis-
tribution of context profiles obtained by
Bayesian estimation and takes the expec-
tation of a base similarity measure under
that distribution. When the context pro-
files are multinomial distributions, the pri-
ors are Dirichlet, and the base measure is
the Bhattacharyya coefficient, we can de-
rive an analytical form that allows efficient
calculation. For the task of word similar-
ity estimation using a large amount of Web
data in Japanese, we show that the pro-
posed measure gives better accuracies than
other well-known similarity measures.
</bodyText>
<sectionHeader confidence="0.99813" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995247533333333">
The semantic similarity of words is a long-
standing topic in computational linguistics be-
cause it is theoretically intriguing and has many
applications in the field. Many researchers have
conducted studies based on the distributional hy-
pothesis (Harris, 1954), which states that words
that occur in the same contexts tend to have similar
meanings. A number of semantic similarity mea-
sures have been proposed based on this hypothesis
(Hindle, 1990; Grefenstette, 1994; Dagan et al.,
1994; Dagan et al., 1995; Lin, 1998; Dagan et al.,
1999).
∗The work was done while the author was at NICT.
In general, most semantic similarity measures
have the following form:
</bodyText>
<equation confidence="0.899601">
sim(w1, w2) = g(v(w1), v(w2)), (1)
</equation>
<bodyText confidence="0.992019029411765">
where v(wi) is a vector that represents the con-
texts in which wi appears, which we call a context
profile of wi. The function g is a function on these
context profiles that is expected to produce good
similarities. Each dimension of the vector corre-
sponds to a context, fk, which is typically a neigh-
boring word or a word having dependency rela-
tions with wi in a corpus. Its value, vk(wi), is typ-
ically a co-occurrence frequency c(wi, fk), a con-
ditional probability p(fk1wi), or point-wise mu-
tual information (PMI) between wi and fk, which
are all calculated from a corpus. For g, various
works have used the cosine, the Jaccard coeffi-
cient, or the Jensen-Shannon divergence is uti-
lized, to name only a few measures.
Previous studies have focused on how to de-
vise good contexts and a good function g for se-
mantic similarities. On the other hand, our ap-
proach in this paper is to estimate context profiles
(v(wi)) robustly and thus to estimate the similarity
robustly. The problem here is that v(wi) is com-
puted from a corpus of limited size, and thus in-
evitably contains uncertainty and sparseness. The
guiding intuition behind our method is as follows.
All other things being equal, the similarity with
a more frequent word should be larger, since it
would be more reliable. For example, if p(fk|w1)
and p(fk|w2) for two given words w1 and w2 are
equal, but w1 is more frequent, we would expect
that sim(w0, w1) &gt; sim(w0, w2).
In the NLP field, data sparseness has been rec-
ognized as a serious problem and tackled in the
context of language modeling and supervised ma-
chine learning. However, to our knowledge, there
</bodyText>
<note confidence="0.809420333333333">
247
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 247–256,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999929722222222">
has been no study that seriously dealt with data
sparseness in the context of semantic similarity
calculation. The data sparseness problem is usu-
ally solved by smoothing, regularization, margin
maximization and so on (Chen and Goodman,
1998; Chen and Rosenfeld, 2000; Cortes and Vap-
nik, 1995). Recently, the Bayesian approach has
emerged and achieved promising results with a
clearer formulation (Teh, 2006; Mochihashi et al.,
2009).
In this paper, we apply the Bayesian framework
to the calculation of distributional similarity. The
method is straightforward: Instead of using the
point estimation of v(wi), we first estimate the
distribution of the context profile, p(v(wi)), by
Bayesian estimation and then take the expectation
of the original similarity under this distribution as
follows:
</bodyText>
<equation confidence="0.964117666666667">
simb(w1, w2) (2)
= E[sim(w1,w2)]{p(v(w1)),p(v(w2))}
= E[g(v(w1), v(w2))]{p(v(w1)),p(v(w2))}.
</equation>
<bodyText confidence="0.999983864864865">
The uncertainty due to data sparseness is repre-
sented by p(v(wi)), and taking the expectation en-
ables us to take this into account. The Bayesian
estimation usually gives diverging distributions for
infrequent observations and thus decreases the ex-
pectation value as expected.
The Bayesian estimation and the expectation
calculation in Eq. 2 are generally difficult and
usually require computationally expensive proce-
dures. Since our motivation for this research is to
calculate good semantic similarities for a large set
of words (e.g., one million nouns) and apply them
to a wide range of NLP tasks, such costs must be
minimized.
Our technical contribution in this paper is to
show that in the case where the context profiles are
multinomial distributions, the priors are Dirich-
let, and the base similarity measure is the Bhat-
tacharyya coefficient (Bhattacharyya, 1943), we
can derive an analytical form for Eq. 2, that en-
ables efficient calculation (with some implemen-
tation tricks).
In experiments, we estimate semantic similari-
ties using a large amount of Web data in Japanese
and show that the proposed measure gives bet-
ter word similarities than a non-Bayesian Bhat-
tacharyya coefficient or other well-known similar-
ity measures such as Jensen-Shannon divergence
and the cosine with PMI weights.
The rest of the paper is organized as follows. In
Section 2, we briefly introduce the Bayesian esti-
mation and the Bhattacharyya coefficient. Section
3 proposes our new Bayesian Bhattacharyya coef-
ficient for robust similarity calculation. Section 4
mentions some implementation issues and the so-
lutions. Then, Section 5 reports the experimental
results.
</bodyText>
<sectionHeader confidence="0.996193" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.984682">
2.1 Bayesian estimation with Dirichlet prior
</subsectionHeader>
<bodyText confidence="0.999826333333333">
Assume that we estimate a probabilistic model for
the observed data D, p(D|0), which is parame-
terized with parameters 0. In the maximum like-
lihood estimation (MLE), we find the point esti-
mation 0∗ = argmaxφp(D|0). For example, we
estimate p(fk|wi) as follows with MLE:
</bodyText>
<equation confidence="0.9985165">
p(fk|wi) = c(wi, fk)/ X c(wi, fk). (3)
k
</equation>
<bodyText confidence="0.9998863125">
On the other hand, the objective of the Bayesian
estimation is to find the distribution of 0 given
the observed data D, i.e., p(0|D), and use it in
later processes. Using Bayes’ rule, this can also
be viewed as:
pprior(0) is a prior distribution that represents the
plausibility of each 0 based on the prior knowl-
edge. In this paper, we consider the case where
0 is a multinomial distribution, i.e., ∑k 0k = 1,
that models the process of choosing one out of K
choices. Estimating a conditional probability dis-
tribution 0k = p(fk|wi) as a context profile for
each wi falls into this case. In this paper, we also
assume that the prior is the Dirichlet distribution,
Dir(α). The Dirichlet distribution is defined as
follows.
</bodyText>
<equation confidence="0.982103333333333">
a Γ(P 1 α
Dirk)
(φ |) = IIKk=1 Γ(αk)
</equation>
<bodyText confidence="0.9978535">
Γ(.) is the Gamma function. The Dirichlet distri-
bution is parametrized by hyperparameters αk(&gt;
0).
It is known that p(0|D) is also a Dirichlet dis-
tribution for this simplest case, and it can be ana-
lytically calculated as follows.
</bodyText>
<equation confidence="0.999846">
p(0|D) = Dir(0|lαk + c(k)}), (6)
</equation>
<bodyText confidence="0.99984825">
where c(k) is the frequency of choice k in data D.
For example, c(k) = c(wi, fk) in the estimation
of p(fk|wi). This is very simple: we just need to
add the observed counts to the hyperparameters.
</bodyText>
<equation confidence="0.998526909090909">
p(D|φ)pprior(φ)
.
(4)
p(D)
p(φ|D) =
φαk−1
k .
(5)
K
ri
k=1
</equation>
<page confidence="0.732915">
248
</page>
<subsectionHeader confidence="0.997854">
2.2 Bhattacharyya coefficient
</subsectionHeader>
<bodyText confidence="0.999802166666667">
When the context profiles are probability distribu-
tions, we usually utilize the measures on probabil-
ity distributions such as the Jensen-Shannon (JS)
divergence to calculate similarities (Dagan et al.,
1994; Dagan et al., 1997). The JS divergence is
defined as follows.
</bodyText>
<equation confidence="0.9983135">
1
JS(p1||p2) = 2(KL(p1||pavg) + KL(p2||pavg)),
</equation>
<bodyText confidence="0.9971164">
where pavg = p1�p2
2 is a point-wise average of p1
and p2 and KL(.) is the Kullback-Leibler diver-
gence. Although we found that the JS divergence
is a good measure, it is difficult to derive an ef-
ficient calculation of Eq. 2, even in the Dirichlet
prior case.l
In this study, we employ the Bhattacharyya co-
efficient (Bhattacharyya, 1943) (BC for short),
which is defined as follows:
</bodyText>
<equation confidence="0.945991">
BC(p1, p2) =
</equation>
<bodyText confidence="0.997511833333333">
The BC is also a similarity measure on probabil-
ity distributions and is suitable for our purposes as
we describe in the next section. Although BC has
not been explored well in the literature on distribu-
tional word similarities, it is also a good similarity
measure as the experiments show.
</bodyText>
<sectionHeader confidence="0.979486" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.999734">
In this section, we show that if our base similarity
measure is BC and the distributions under which
we take the expectation are Dirichlet distributions,
then Eq. 2 also has an analytical form, allowing
efficient calculation.
Here, we calculate the following value given
two Dirichlet distributions:
</bodyText>
<equation confidence="0.903967">
BCb(p1, p2) = E[BC(p1, p2)I{Dir(p1|α′),Dir(p2|β′)}
Dir(p1|α′)Dir(p2|β′)BC(p1, p2)dp1dp2.
</equation>
<bodyText confidence="0.9656705">
After several derivation steps (see Appendix A),
we obtain the following analytical solution for the
above:
&apos;A naive but general way might be to draw samples of
v(wi) from p(v(wi)) and approximate the expectation using
these samples. However, such a method will be slow.
</bodyText>
<equation confidence="0.9297915">
′
P(αk + 12)P(β′k + 12) (7)
P(α′k)P(β′k
′ 0 = E
where α0 = Ek α′ k and Q′ k Q′k. Note that
′
with the Dirichlet prior, αk = αk + c(w1, fk) and
Q′k = Qk + c(w2, fk), where αk and Qk are the
</equation>
<bodyText confidence="0.999228">
hyperparameters of the priors of w1 and w2, re-
spectively.
To put it all together, we can obtain a new
Bayesian similarity measure on words, which can
be calculated only from the hyperparameters for
the Dirichlet prior, α and Q, and the observed
counts c(wi, fk). It is written as follows.
</bodyText>
<equation confidence="0.999452166666667">
(8)
P(α0 + a0)P(β0 + b0)
)x
P(α0 + a0 + 12)P(β0 + b0 + 12
P(αk + c(w1, fk) + 12)P(βk + c(w2, fk) + 12 )
P(αk + c(w1, fk))P(βk + c(w2, fk)) ,
</equation>
<bodyText confidence="0.989897727272727">
where a0 = Ek c(w1, fk) and b0 =
Ek c(w2, fk). We call this new measure the
Bayesian Bhattacharyya coefficient (BCb for
short). For simplicity, we assume αk = Qk = α in
this paper.
We can see that BCb actually encodes our guid-
ing intuition. Consider four words, w0, w1, w2,
and w4, for which we have c(w0, f1) = 10,
c(w1, f1) = 2, c(w2, f1) = 10, and c(w3, f1) =
20. They have counts only for the first dimen-
sion, i.e., they have the same context profile:
</bodyText>
<equation confidence="0.9690625">
p(f1|wi) = 1.0, when we employ NILE. When
K = 10, 000 and αk = 1.0, the Bayesian similar-
ity between these words is calculated as
BCb(w0, w1) = 0.785368
BCb(w0, w2) = 0.785421
BCb(w0, w3) = 0.785463
</equation>
<bodyText confidence="0.9884414">
We can see that similarities are different ac-
cording to the number of observations, as ex-
pected. Note that the non-Bayesian BC will re-
turn the same value, 1.0, for all cases. Note
also that BCb(w0, w0) = 0.78542 if we use Eq.
8, meaning that the self-similarity might not be
the maximum. This is conceptually strange, al-
though not a serious problem since we hardly use
sim(wi, wi) in practice. If we want to fix this,
we can use the special definition: BCb(wi, wi) ≡
</bodyText>
<equation confidence="0.991498842105263">
1. This is equivalent to using simb(wi, wi) =
E[sim(wi, wi)]{p(v(wi))} = 1 only for this case.
K
E
k=1
✓p1k x p2k.
ZZ=
△×△
′
P(α0)P(β′ 0)
P(α′0 + 12 )P(β′0 + 12 )
=
K
E
k=1
BCb(w1, w2) =
K
E
k=1
</equation>
<page confidence="0.843696">
249
</page>
<sectionHeader confidence="0.994892" genericHeader="method">
4 Implementation Issues
</sectionHeader>
<bodyText confidence="0.999988633333333">
Although we have derived the analytical form
(Eq. 8), there are several problems in implement-
ing robust and efficient calculations.
First, the Gamma function in Eq. 8 overflows
when the argument is larger than 170. In such
cases, a commonly used way is to work in the log-
arithmic space. In this study, we utilize the “log
Gamma” function: lnF(x), which returns the log-
arithm of the Gamma function directly without the
overflow problem.2
Second, the calculation of the log Gamma func-
tion is heavier than operations such as simple mul-
tiplication, which is used in existing measures.
In fact, the log Gamma function is implemented
using an iterative algorithm such as the Lanczos
method. In addition, according to Eq. 8, it seems
that we have to sum up the values for all k, be-
cause even if c(wi, fk) is zero the value inside the
summation will not be zero. In the existing mea-
sures, it is often the case that we only need to sum
up for k where c(wi, fk) &gt; 0. Because c(wi, fk)
is usually sparse, that technique speeds up the cal-
culation of the existing measures drastically and
makes it practical.
In this study, the above problem is solved by
pre-computing the required log Gamma values, as-
suming that we calculate similarities for a large
set of words, and pre-computing default values for
cases where c(wi, fk) = 0. The following values
are pre-computed once at the start-up time.
</bodyText>
<equation confidence="0.9752738">
For each word:
(A) lnF(α0 + a0) − lnF(α0 + a0 + 12)
(B) lnF(αk+c(wi, fk))−lnF(αk+c(wi, fk)+12)
for all k where c(wi, fk) &gt; 0
(C) − exp(2(lnF(αk + 12) − lnF(αk)))) +
exp(lnF(αk + c(wi, fk)) − lnF(αk +
c(wi, fk) + 12) + lnF(αk + 12) − lnF(αk))
for all k where c(wi, fk) &gt; 0;
For each k:
(D): exp(2(lnF(αk + 12)).
</equation>
<bodyText confidence="0.994903142857143">
In the calculation of BCb(w1, w2), we first as-
sume that all c(wi, fk) = 0 and set the output
variable to the default value. Then, we iterate
over the sparse vectors c(w1, fk) and c(w2, fk). If
2We used the GNU Scientific Library (GSL)
(www.gnu.org/software/gsl/), which implements this
function.
c(w1, fk) &gt; 0 and c(w2, fk) = 0 (and vice versa),
we update the output variable just by adding (C).
If c(w1, fk) &gt; 0 and c(w2, fk) &gt; 0, we update
the output value using (B), (D) and one additional
exp(.) operation. With this implementation, we
can make the computation of BCb practically as
fast as using other measures.
</bodyText>
<sectionHeader confidence="0.999714" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999236">
5.1 Evaluation setting
</subsectionHeader>
<bodyText confidence="0.999527913043478">
We evaluated our method in the calculation of sim-
ilarities between nouns in Japanese.
Because human evaluation of word similari-
ties is very difficult and costly, we conducted au-
tomatic evaluation in the set expansion setting,
following previous studies such as Pantel et al.
(2009).
Given a word set, which is expected to con-
tain similar words, we assume that a good simi-
larity measure should output, for each word in the
set, the other words in the set as similar words.
For given word sets, we can construct input-and-
answers pairs, where the answers for each word
are the other words in the set the word appears in.
We output a ranked list of 500 similar words
for each word using a given similarity measure
and checked whether they are included in the an-
swers. This setting could be seen as document re-
trieval, and we can use an evaluation measure such
as the mean of the precision at top T (MP@T) or
the mean average precision (MAP). For each input
word, P@T (precision at top T) and AP (average
precision) are defined as follows.
</bodyText>
<equation confidence="0.996522">
1
P@T =
T
S(wi ∈ ans)P@i.
</equation>
<bodyText confidence="0.9869396">
S(wi E ans) returns 1 if the output word wi is
in the answers, and 0 otherwise. N is the number
of outputs and R is the number of the answers.
MP@T and MAP are the averages of these values
over all input words.
</bodyText>
<subsectionHeader confidence="0.999762">
5.2 Collecting context profiles
</subsectionHeader>
<bodyText confidence="0.99977275">
Dependency relations are used as context profiles
as in Kazama and Torisawa (2008) and Kazama et
al. (2009). From a large corpus of Japanese Web
documents (Shinzato et al., 2008) (100 million
</bodyText>
<equation confidence="0.966270142857143">
T
S(wi ∈ ans),
i=1
AP = 1 N
R
i=1
250
</equation>
<bodyText confidence="0.999942769230769">
documents), where each sentence has a depen-
dency parse, we extracted noun-verb and noun-
noun dependencies with relation types and then
calculated their frequencies in the corpus. If a
noun, n, depends on a word, w, with a relation,
r, we collect a dependency pair, (n, (w, r)). That
is, a context fk, is (w, r) here.
For noun-verb dependencies, postpositions
in Japanese represent relation types. For
example, we extract a dependency relation
(174,/, ( IN7, ;�- )) from the sentence below,
where a postposition “;�, (wo)” is used to mark
the verb object.
</bodyText>
<equation confidence="0.735895">
174,/ (wine) � (wo) IN7 (buy) (Pz� buy a wine)
</equation>
<bodyText confidence="0.999963777777778">
Note that we leave various auxiliary verb suf-
fixes, such as “*L6 (reru),” which is for passiviza-
tion, as a part of w, since these greatly change the
type of n in the dependent position.
As for noun-noun dependencies, we considered
expressions of type “n1 0) n2” (Pz� “n2 of n1”) as
dependencies (n1, (n2, 0) )).
We extracted about 470 million unique depen-
dencies from the corpus, containing 31 million
unique nouns (including compound nouns as de-
termined by our filters) and 22 million unique con-
texts, fk. We sorted the nouns according to the
number of unique co-occurring contexts and the
contexts according to the number of unique co-
occurring nouns, and then we selected the top one
million nouns and 100,000 contexts. We used only
260 million dependency pairs that contained both
the selected nouns and the selected contexts.
</bodyText>
<subsectionHeader confidence="0.999544">
5.3 Test sets
</subsectionHeader>
<bodyText confidence="0.98537515">
We prepared three test sets as follows.
Set “A” and “B”: Thesaurus siblings We
considered that words having a common
hypernym (i.e., siblings) in a manually
constructed thesaurus could constitute a
similar word set. We extracted such sets
from a Japanese dictionary, EDR (V3.0)
(CRL, 2002), which contains concept hier-
archies and the mapping between words and
concepts. The dictionary contains 304,884
nouns. In all, 6,703 noun sibling sets were
extracted with the average set size of 45.96.
We randomly chose 200 sets each for sets
“A” and “B.” Set “A” is a development set to
tune the value of the hyperparameters and
“B” is for the validation of the parameter
tuning.
Set “C”: Closed sets Murata et al. (2004) con-
structed a dataset that contains several closed
word sets such as the names of countries,
rivers, sumo wrestlers, etc. We used all of
the 45 sets that are marked as “complete” in
the data, containing 12,827 unique words in
total.
Note that we do not deal with ambiguities in the
construction of these sets as well as in the calcu-
lation of similarities. That is, a word can be con-
tained in several sets, and the answers for such a
word is the union of the words in the sets it belongs
to (excluding the word itself).
In addition, note that the words in these test sets
are different from those of our one-million-word
vocabulary. We filtered out the words that are not
included in our vocabulary and removed the sets
with size less than 2 after the filtering.
Set “A” contained 3,740 words that are actually
evaluated, with about 115 answers on average, and
“B” contained 3,657 words with about 65 answers
on average. Set “C” contained 8,853 words with
about 1,700 answers on average.
</bodyText>
<subsectionHeader confidence="0.989769">
5.4 Compared similarity measures
</subsectionHeader>
<bodyText confidence="0.9994246">
We compared our Bayesian Bhattacharyya simi-
larity measure, BCb, with the following similarity
measures.
JS Jensen-Shannon divergence between p(fk|w1)
and p(fk|w2) (Dagan et al., 1994; Dagan et
al., 1999).
PMI-cos The cosine of the context profile vec-
tors, where the k-th dimension is the point-
wise mutual information (PMI) between
wi and fk defined as: PMI(wi, fk) =
</bodyText>
<equation confidence="0.860735666666667">
log p(wi,fk)
p(wi)p(fk) (Pantel and Lin, 2002; Pantel
et al., 2009).3
</equation>
<bodyText confidence="0.848532666666667">
Cls-JS Kazama et al. (2009) proposed using
the Jensen-Shannon divergence between hid-
den class distributions, p(c|w1) and p(c|w2),
which are obtained by using an EM-based
clustering of dependency relations with a
model p(wi, fk) = ∑ c p(wi|c)p(fk|c)p(c)
(Kazama and Torisawa, 2008). In order to
3We did not use the discounting of the PMI values de-
scribed in Pantel and Lin (2002).
</bodyText>
<page confidence="0.677887">
251
</page>
<bodyText confidence="0.999921363636364">
alleviate the effect of local minima of the EM
clustering, they proposed averaging the simi-
larities by several different clustering results,
which can be obtained by using different ini-
tial parameters. In this study, we combined
two clustering results (denoted as “s1+s2” in
the results), each of which (“s1” and “s2”)
has 2,000 hidden classes.4 We included this
method since clustering can be regarded as
another way of treating data sparseness.
BC The Bhattacharyya coefficient (Bhat-
tacharyya, 1943) between p(fk|w1) and
p(fkIw2). This is the baseline for BCb.
BCa The Bhattacharyya coefficient with absolute
discounting. In calculating p(fk|wi), we sub-
tract the discounting value, α, from c(wi7 fk)
and equally distribute the residual probabil-
ity mass to the contexts whose frequency is
zero. This is included as an example of naive
smoothing methods.
Since it is very costly to calculate the sim-
ilarities with all of the other words (one mil-
lion in our case), we used the following approx-
imation method that exploits the sparseness of
c(wi7 fk). Similar methods were used in Pantel
and Lin (2002), Kazama et al. (2009), and Pan-
tel et al. (2009) as well. For a given word, wi,
we sort the contexts in descending order accord-
ing to c(wi7 fk) and retrieve the top-L contexts.5
For each selected context, we sort the words in de-
scending order according to c(wi7 fk) and retrieve
the top-M words (L = M = 1600).6 We merge
all of the words above as candidate words and cal-
culate the similarity only for the candidate words.
Finally, the top 500 similar words are output.
Note also that we used modified counts,
log(c(wi7 fk)) + 1, instead of raw counts,
c(wi7 fk), with the intention of alleviating the ef-
fect of strangely frequent dependencies, which can
be found in the Web data. In preliminary ex-
periments, we observed that this modification im-
proves the quality of the top 500 similar words as
reported in Terada et al. (2004) and Kazama et al.
(2009).
</bodyText>
<footnote confidence="0.879932">
4In the case of EM clustering, the number of unique con-
texts, fk, was also set to one million instead of 100,000, fol-
lowing Kazama et al. (2009).
5It is possible that the number of contexts with non-zero
counts is less than L. In that case, all of the contexts with
non-zero counts are used.
6Sorting is performed only once in the initialization step.
</footnote>
<tableCaption confidence="0.998434">
Table 1: Performance on siblings (Set A).
</tableCaption>
<table confidence="0.995895105263158">
MP
Measure MAP
@1 @5 @10 @20
JS 0.0299 0.197 0.122 0.0990 0.0792
PMI-cos 0.0332 0.195 0.124 0.0993 0.0798
Cls-JS (s1) 0.0319 0.195 0.122 0.0988 0.0796
Cls-JS (s2) 0.0295 0.198 0.122 0.0981 0.0786
Cls-JS (s1+s2) 0.0333 0.206 0.129 0.103 0.0841
BC 0.0334 0.211 0.131 0.106 0.0854
BCb (0.0002) 0.0345 0.223 0.138 0.109 0.0873
BCb (0.0016) 0.0356 0.242 0.148 0.119 0.0955
BCb (0.0032) 0.0325 0.223 0.137 0.111 0.0895
BCa (0.0016) 0.0337 0.212 0.133 0.107 0.0863
BCa (0.0362) 0.0345 0.221 0.136 0.110 0.0890
BCa (0.1) 0.0324 0.214 0.128 0.101 0.0825
without log(c(wi, fk)) + 1 modification
JS 0.0294 0.197 0.116 0.0912 0.0712
PMI-cos 0.0342 0.197 0.125 0.0987 0.0793
BC 0.0296 0.201 0.118 0.0915 0.0721
</table>
<bodyText confidence="0.998115833333333">
As for BCb, we assumed that all of the hyper-
parameters had the same value, i.e., αk = α. It
is apparent that an excessively large α is not ap-
propriate because it means ignoring observations.
Therefore, α must be tuned. The discounting value
of BCa is also tuned.
</bodyText>
<sectionHeader confidence="0.725357" genericHeader="method">
5.5 Results
</sectionHeader>
<bodyText confidence="0.999982037037037">
Table 1 shows the results for Set A. The MAP and
the MPs at the top 1, 5, 10, and 20 are shown for
each similarity measure. As for BCb and BCa, the
results for the tuned and several other values for α
are shown. Figure 1 shows the parameter tuning
for BCb with MAP as the y-axis (results for BCa
are shown as well). Figure 2 shows the same re-
sults with MPs as the y-axis. The MAP and MPs
showed a correlation here. From these results, we
can see that BCb surely improves upon BC, with
6.6% improvement in MAP and 14.7% improve-
ment in MP@1 when α = 0.0016. BCb achieved
the best performance among the compared mea-
sures with this setting. The absolute discounting,
BCa, improved upon BC as well, but the improve-
ment was smaller than with BCb. Table 1 also
shows the results for the case where we did not
use the log-modified counts. We can see that this
modification gives improvements (though slight or
unclear for PMI-cos).
Because tuning hyperparameters involves the
possibility of overfitting, its robustness should be
assessed. We checked whether the tuned α with
Set A works well for Set B. The results are shown
in Table 2. We can see that the best α (= 0.0016)
found for Set A works well for Set B as well. That
is, the tuning of α as above is not unrealistic in
</bodyText>
<figure confidence="0.848363">
252
1e-06 1e-05 0.0001 0.001 0.01 0.1 1
α (log-scale)
</figure>
<figureCaption confidence="0.999806666666667">
Figure 1: Tuning of α (MAP). The dashed hori-
zontal line indicates the score of BC.
Figure 2: Tuning of α (MP).
</figureCaption>
<bodyText confidence="0.996990758620689">
practice because it seems that we can tune it ro-
bustly using a small subset of the vocabulary as
shown by this experiment.
Next, we evaluated the measures on Set C, i.e.,
the closed set data. The results are shown in Ta-
ble 3. For this set, we observed a tendency that
is different from Sets A and B. Cls-JS showed a
particularly good performance. BCb surely im-
proves upon BC. For example, the improvement
was 7.5% for MP@1. However, the improvement
in MAP was slight, and MAP did not correlate
well with MPs, unlike in the case of Sets A and
B.
We thought one possible reason is that the num-
ber of outputs, 500, for each word was not large
enough to assess MAP values correctly because
the average number of answers is 1,700 for this
dataset. In fact, we could output more than 500
words if we ignored the cost of storage. Therefore,
we also included the results for the case where
L = M = 3600 and N = 2, 000. Even with
this setting, however, MAP did not correlate well
with MPs.
Although Cls-JS showed very good perfor-
mance for Set C, note that the EM clustering
is very time-consuming (Kazama and Torisawa,
2008), and it took about one week with 24 CPU
cores to get one clustering result in our computing
environment. On the other hand, the preparation
</bodyText>
<tableCaption confidence="0.99579">
Table 2: Performance on siblings (Set B).
</tableCaption>
<table confidence="0.9999475">
Measure MAP MP
@1 @5 @10 @20
JS 0.0265 0.208 0.116 0.0855 0.0627
PMI-cos 0.0283 0.203 0.116 0.0871 0.0660
Cls-JS (s1+s2) 0.0274 0.194 0.115 0.0859 0.0643
BC 0.0295 0.223 0.124 0.0922 0.0693
BCb (0.0002) 0.0301 0.225 0.128 0.0958 0.0718
BCb (0.0016) 0.0313 0.246 0.135 0.103 0.0758
BCb (0.0032) 0.0279 0.228 0.127 0.0938 0.0698
BCa (0.0016) 0.0297 0.223 0.125 0.0934 0.0700
BCa (0.0362) 0.0298 0.223 0.125 0.0934 0.0705
BCa (0.01) 0.0300 0.224 0.126 0.0949 0.0710
</table>
<tableCaption confidence="0.996694">
Table 3: Performance on closed-sets (Set C).
</tableCaption>
<table confidence="0.999445">
Measure MAP MP
@1 @5 @10 @20
JS 0.127 0.607 0.582 0.566 0.544
PMI-cos 0.124 0.531 0.519 0.508 0.493
Cls-JS (s1) 0.125 0.589 0.566 0.548 0.525
Cls-JS (s2) 0.137 0.608 0.592 0.576 0.554
Cls-JS (s1+s2) 0.152 0.638 0.617 0.603 0.583
BC 0.131 0.602 0.579 0.565 0.545
BCb (0.0004) 0.133 0.636 0.605 0.587 0.563
BCb (0.0008) 0.131 0.647 0.615 0.594 0.568
BCb (0.0016) 0.126 0.644 0.615 0.593 0.564
BCb (0.0032) 0.107 0.573 0.556 0.529 0.496
L = M = 3200 and N = 2000
JS 0.165 0.605 0.580 0.564 0.543
PMI-cos 0.165 0.530 0.517 0.507 0.492
Cls-JS (s1+s2) 0.209 0.639 0.618 0.603 0.584
BC 0.168 0.600 0.577 0.562 0.542
BCb (0.0004) 0.170 0.635 0.604 0.586 0.562
BCb (0.0008) 0.168 0.647 0.615 0.594 0.568
BCb (0.0016) 0.161 0.644 0.615 0.593 0.564
BCb (0.0032) 0.140 0.573 0.556 0.529 0.496
</table>
<bodyText confidence="0.8371415">
for our method requires just an hour with a single
core.
</bodyText>
<sectionHeader confidence="0.999184" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.961981058823529">
We should note that the improvement by using our
method is just “on average,” as in many other NLP
tasks, and observing clear qualitative change is rel-
atively difficult, for example, by just showing ex-
amples of similar word lists here. Comparing the
results of BCb and BC, Table 4 lists the numbers
of improved, unchanged, and degraded words in
terms of MP@20 for each evaluation set. As can
be seen, there are a number of degraded words, al-
though they are fewer than the improved words.
Next, Figure 3 shows the averaged differences of
MP@20 in each 40,000 word-ID range.7 We can
observe that the advantage of BCb is lessened es-
7Word IDs are assigned in ascending order when we chose
the top one million words as described in Section 5.2, and
they roughly correlate with frequencies. So, frequent words
tend to have low-IDs.
</bodyText>
<figure confidence="0.997726">
0.036
0.034
0.032
0.028
0.026
0.024
0.022
0.03
0.02
Bayes
Absolute Discounting
1e-06 1e-05 0.0001 0.001 0.01
α (log-scale)
MP
0.26
0.24
0.22
0.18
0.16
0.14
0.12
0.08
0.06
0.04
0.2
0.1
MP@1
MP@5
MP@10
MP@20
MP@30
MP@40
MAP
</figure>
<page confidence="0.717421">
253
</page>
<tableCaption confidence="0.833982333333333">
Table 5: Statistics on IDs. (A): Avg. ID of an-
swers. (B): Avg. ID of system outputs. (C): Avg.
ID of correct system outputs.
Table 4: The numbers of improved, unchanged,
and degraded words in terms of MP@20 for each
evaluation set.
</tableCaption>
<table confidence="0.978365">
# improved # unchanged # degraded
Set A 755 2,585 400
Set B 643 2,610 404
Set C 3,153 3,962 1,738
</table>
<figure confidence="0.996095571428572">
0 500000 1e+06
ID range
0.08
0.07
0.06
0.05
0.04
0.03
0.02
0.01
0
-0.01
0 500000 1e+06
ID range
</figure>
<figureCaption confidence="0.998622">
Figure 3: Averaged Differences of MP@20 be-
</figureCaption>
<bodyText confidence="0.987267392857143">
tween BCb (0.0016) and BC within each 40,000
ID range (Left: Set A. Right: Set B. Bottom: Set
C).
pecially for low-ID words (as expected) with on-
average degradation.8 The improvement is “on av-
erage” in this sense as well.
One might suspect that the answer words tended
to be low-ID words, and the proposed method is
simply biased towards low-ID words because of
its nature. Then, the observed improvement is a
trivial consequence. Table 5 lists some interest-
ing statistics about the IDs. We can see that BCb
surely outputs more low-ID words than BC, and
BC more than Cls-JS and JS.9 However, the av-
erage ID of the outputs of BC is already lower
than the average ID of the answer words. There-
fore, even if BCb preferred lower-ID words than
BC, it should not have the effect of improving
the accuracy. That is, the improvement by BCb
is not superficial. From BC/BCb, we can also see
that the IDs of the correct outputs did not become
smaller compared to the IDs of the system outputs.
Clearly, we need more analysis on what caused
the improvement by the proposed method and how
that affects the efficacy in real applications of sim-
ilarity measures.
The proposed Bayesian similarity measure out-
performed the baseline Bhattacharyya coefficient
</bodyText>
<footnote confidence="0.890637666666667">
8This suggests the use of different αs depending on ID
ranges (e.g., smaller α for low-ID words) in practice.
9The outputs of Cls-JS are well-balanced in the ID space.
</footnote>
<table confidence="0.96088975">
Set A Set C
(A) 238,483 255,248
(B) (C) (B) (C)
Cls-JS (s1+s2) 282,098 176,706 273,768 232,796
JS 183,054 11,3442 211,671 201,214
BC 162,758 98,433 193,508 189,345
BCb(0.0016) 55,915 54,786 90,472 127,877
BC/BCb 2.91 1.80 2.14 1.48
</table>
<bodyText confidence="0.99979258974359">
and other well-known similarity measures. As
a smoothing method, it also outperformed a
naive absolute discounting. Of course, we can-
not say that the proposed method is better than
any other sophisticated smoothing method at this
point. However, as noted above, there has
been no serious attempt to assess the effect of
smoothing in the context of word similarity cal-
culation. Recent studies have pointed out that
the Bayesian framework derives state-of-the-art
smoothing methods such as Kneser-Ney smooth-
ing as a special case (Teh, 2006; Mochihashi et
al., 2009). Consequently, it is reasonable to re-
sort to the Bayesian framework. Conceptually,
our method is equivalent to modifying p(1fk|wi)
as p(fk |wi) = { Γ(α0+a0+Γ21)Γ(αk+c(w k),fk)) J 2 and
taking the Bhattacharyya coefficient. However,
the implication of this form has not yet been in-
vestigated, and so we leave it for future research.
Our method is the simplest one as a Bayesian
method. We did not employ any numerical opti-
mization or sampling iterations, as in a more com-
plete use of the Bayesian framework (Teh, 2006;
Mochihashi et al., 2009). Instead, we used the ob-
tained analytical form directly with the assump-
tion that αk = α and α can be tuned directly by
using a simple grid search with a small subset of
the vocabulary as the development set. If substan-
tial additional costs are allowed, we can fine-tune
each αk using more complete Bayesian methods.
We also leave this for future research.
In terms of calculation procedure, BCb has the
same form as other similarity measures, which is
basically the same as the inner product of sparse
vectors. Thus, it can be as fast as other similar-
ity measures with some effort as we described in
Section 4 when our aim is to calculate similarities
between words in a fixed large vocabulary. For ex-
ample, BCb took about 100 hours to calculate the
</bodyText>
<figure confidence="0.991313095238095">
0 500000 1e+06
ID range
Avg. Diff. of MP@20
-0.01
0.06
0.05
0.04
0.03
0.02
0.01
0
Avg. Diff. of MP@20
-0.01
0.06
0.05
0.04
0.03
0.02
0.01
0
Avg. Diff. of MP@20
</figure>
<page confidence="0.559195">
254
</page>
<bodyText confidence="0.999827791666667">
top 500 similar nouns for all of the one million
nouns (using 16 CPU cores), while JS took about
57 hours. We think this is an acceptable additional
cost.
The limitation of our method is that it can-
not be used efficiently with similarity measures
other than the Bhattacharyya coefficient, although
that choice seems good as shown in the experi-
ments. For example, it seems difficult to use the
Jensen-Shannon divergence as the base similar-
ity because the analytical form cannot be derived.
One way we are considering to give more flexi-
bility to our method is to adjust αk depending on
external knowledge such as the importance of a
context (e.g., PMIs). In another direction, we will
be able to use a “weighted” Bhattacharyya coeffi-
cient: ∑k µ(w1, fk)µ(w2, fk)√p1k × p2k, where
the weights, µ(wi, fk), do not depend on pik, as
the base similarity measure. The analytical form
for it will be a weighted version of BCb.
BCb can also be generalized to the case where
the base similarity is BCd(p1, p2) = ∑Kk=1 pd1k ×
pd2k, where d &gt; 0. The Bayesian analytical form
becomes as follows.
</bodyText>
<equation confidence="0.9745808">
Γ(α0 + a0)Γ(β0 + b0)
BCd b (w1, w2) = x
Γ(α0 + a0 + d)Γ(β0 + b0 + d)
Γ(αk + c(w1, fk) + d)Γ(βk + c(w2, fk) + d)
Γ(αk + c(w1, fk))Γ(βk + c(w2, fk)) .
</equation>
<bodyText confidence="0.996646625">
See Appendix A for the derivation. However, we
restricted ourselves to the case of d = 21 in this
study.
Finally, note that our BCb is different from
the Bhattacharyya distance measure on Dirichlet
distributions of the following form described in
Rauber et al. (2008) in its motivation and analyti-
cal form:
</bodyText>
<equation confidence="0.872346333333333">
/r(ao)r(Qo) x FIk r((a&apos; +,Qk)/2)
1 (g)
�lik r(ak)�Q k Γ(βk) Γ(2 Pk (αk + β′k))
</equation>
<bodyText confidence="0.997527">
Empirical and theoretical comparisons with this
measure also form one of the future directions.10
</bodyText>
<sectionHeader confidence="0.997555" genericHeader="method">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9998384">
We proposed a Bayesian method for robust distri-
butional word similarities. Our method uses a dis-
tribution of context profiles obtained by Bayesian
10Our preliminary experiments show that calculating sim-
ilarity using Eq. 9 for the Dirichlet distributions obtained by
Eq. 6 does not produce meaningful similarity (i.e., the accu-
racy is very low).
estimation and takes the expectation of a base sim-
ilarity measure under that distribution. We showed
that, in the case where the context profiles are
multinomial distributions, the priors are Dirichlet,
and the base measure is the Bhattacharyya coeffi-
cient, we can derive an analytical form, permitting
efficient calculation. Experimental results show
that the proposed measure gives better word simi-
larities than a non-Bayesian Bhattacharyya coeffi-
cient, other well-known similarity measures such
as Jensen-Shannon divergence and the cosine with
PMI weights, and the Bhattacharyya coefficient
with absolute discounting.
</bodyText>
<sectionHeader confidence="0.95156" genericHeader="method">
Appendix A
</sectionHeader>
<bodyText confidence="0.99999425">
Here, we give the analytical form for the general-
ized case (BCdb ) in Section 6. Recall the following
relation, which is used to derive the normalization
factor of the Dirichlet distribution:
</bodyText>
<equation confidence="0.9599621">
ZQ ′
Y φα′ k−1 k Γ(αk)
k dφ = Γ(α′ 0) = Z(α′)−1. (10)
= Z(α′)Z(β′) x
′ Y
φαl−1
1l
m
 |{z }
A
</equation>
<bodyText confidence="0.992238">
Using Eq. 10, A in the above can be calculated as
follows:
</bodyText>
<page confidence="0.881177">
3
</page>
<equation confidence="0.996697761904762">
Z ′ Y ′
φd φαk+d−1 φαl−1 5 dφ2
2k 1l dφ1
1k
l̸=k
△
Z=
△
′ ′
Γ(αk + d) Q Z
l̸=k Γ(αl)
Γ(α′ 0 + d)
Γ(β′k + d) Qm̸=k Γ(β′m)
Γ(β′0 + d)
′ ′
Q Γ(αl) Q Γ(β′ m) X Γ(αk + d) Γ(β′ k + d)
=
Γ(α′0 + d)Γ(β′ k) .
0 + d) Γ(α′ Γ(β′
k)
k
</equation>
<bodyText confidence="0.430327">
This will give:
BCbd (w1, w2) =
</bodyText>
<figure confidence="0.936735210526316">
′
Γ(α0)Γ(β′ 0)
Γ(α′0 + d)Γ(β′0 + d)
K
X
k=1
△ k
Then, BCdb (w1, w2)
ZZ= X φd1kφd2k dφ1 dφ2
△×△ Dir(φ1|α′)Dir(φ2|β′)
k
Y
l
ZZ
△×△
′ φd1kφd2k dφ1 dφ2 .
−1
φ2mm,
k
−1
φβ′ m
2m
Y
m
2
4X
k
Z=
△
&amp;quot;X #
Γ(α′ k + d) Q l̸=k Γ(α′ l)
φd dφ2
2k Γ(α′ 0 + d)
k
−1
′
φβm
2m
Y
m
X=
k
X=
k
′ ′
Γ(αk + d) Ql̸=k Γ(αl)
Γ(α′0 + d)
φβ′ k+d−1 Y ′
2k m̸=k φβ2m dφ2
m−1
′
Γ(αk + d)Γ(β′k + d)
Γ(α′k)Γ(β′k) .
K
X
k=1
255
</figure>
<sectionHeader confidence="0.962719" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999740282352942">
A. Bhattacharyya. 1943. On a measure of divergence
between two statistical populations defined by their
probability distributions. Bull. Calcutta Math. Soc.,
49:214–224.
Stanley F. Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. TR-10-98, Computer Science Group,
Harvard University.
Stanley F. Chen and Ronald Rosenfeld. 2000. A
survey of smoothing techniques for ME models.
IEEE Transactions on Speech and Audio Process-
ing, 8(1):37–50.
Corinna Cortes and Vladimir Vapnik. 1995. Support
vector networks. Machine Learning, 20:273–297.
CRL. 2002. EDR electronic dictionary version 2.0
technical guide. Communications Research Labo-
ratory (CRL).
Ido Dagan, Fernando Pereira, and Lillian Lee. 1994.
Similarity-based estimation of word cooccurrence
probabilities. In Proceedings of ACL 94.
Ido Dagan, Shaul Marcus, and Shaul Markovitch.
1995. Contextual word similarity and estimation
from sparse data. Computer, Speech and Language,
9:123–152.
Ido Dagan, Lillian Lee, and Fernando Pereira. 1997.
Similarity-based methods for word sense disam-
biguation. In Proceedings of ACL 97.
Ido Dagan, Lillian Lee, and Fernando Pereira. 1999.
Similarity-based models of word cooccurrence
probabilities. Machine Learning, 34(1-3):43–69.
Gregory Grefenstette. 1994. Explorations In Auto-
matic Thesaurus Discovery. Kluwer Academic Pub-
lishers.
Zellig Harris. 1954. Distributional structure. Word,
pages 146–142.
Donald Hindle. 1990. Noun classification from
predicate-argument structures. In Proceedings of
ACL-90, pages 268–275.
Jun’ichi Kazama and Kentaro Torisawa. 2008. In-
ducing gazetteers for named entity recognition by
large-scale clustering of dependency relations. In
Proceedings of ACL-08: HLT.
Jun’ichi Kazama, Stijn De Saeger, Kentaro Torisawa,
and Masaki Murata. 2009. Generating a large-scale
analogy list using a probabilistic clustering based on
noun-verb dependency profiles. In Proceedings of
15th Annual Meeting of The Association for Natural
Language Processing (in Japanese).
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING/ACL-
98, pages 768–774.
Daichi Mochihashi, Takeshi Yamada, and Naonori
Ueda. 2009. Bayesian unsupervised word segmen-
tation with nested Pitman-Yor language modeling.
In Proceedings of ACL-IJCNLP 2009, pages 100–
108.
Masaki Murata, Qing Ma, Tamotsu Shirado, and Hi-
toshi Isahara. 2004. Database for evaluating ex-
tracted terms and tool for visualizing the terms. In
Proceedings of LREC 2004 Workshop: Computa-
tional and Computer-Assisted Terminology, pages
6–9.
Patrick Pantel and Dekang Lin. 2002. Discovering
word senses from text. In Proceedings of the eighth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 613–619.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of EMNLP 2009, pages 938–947.
T. W. Rauber, T. Braun, and K. Berns. 2008. Proba-
bilistic distance measures of the Dirichlet and Beta
distributions. Pattern Recognition, 41:637–645.
Keiji Shinzato, Tomohide Shibata, Daisuke Kawahara,
Chikara Hashimoto, and Sadao Kurohashi. 2008.
Tsubaki: An open search engine infrastructure for
developing new information access. In Proceedings
of IJCNLP 2008.
Yee Whye Teh. 2006. A hierarchical Bayesian lan-
guage model based on Pitman-Yor processes. In
Proceedings of COLING-ACL 2006, pages 985–992.
Akira Terada, Minoru Yoshida, and Hiroshi Nakagawa.
2004. A tool for constructing a synonym dictionary
using context information. In IPSJ SIG Technical
Report (in Japanese), pages 87–94.
</reference>
<page confidence="0.93041">
256
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.438484">
<title confidence="0.999712">A Bayesian Method for Robust Estimation of Distributional Similarities</title>
<author confidence="0.986007">Jun’ichi Kazama Stijn De_Saeger Kow Kuroda</author>
<affiliation confidence="0.902736">Torisawa Language Infrastructure Group, MASTAR Project National Institute of Information and Communications Technology (NICT)</affiliation>
<address confidence="0.96508">3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0289 Japan</address>
<email confidence="0.674718">stijn,kuroda,</email>
<affiliation confidence="0.975496">of Information and Knowledge Engineering Faculty/Graduate School of Engineering, Tottori University</affiliation>
<address confidence="0.998901">Koyama-Minami, Tottori, 680-8550</address>
<email confidence="0.972291">murata@ike.tottori-u.ac.jp</email>
<abstract confidence="0.999845619047619">Existing word similarity measures are not robust to data sparseness since they rely only on the point estimation of words’ context profiles obtained from a limited amount of data. This paper proposes a Bayesian method for robust distributional word similarities. The method uses a distribution of context profiles obtained by Bayesian estimation and takes the expectation of a base similarity measure under that distribution. When the context profiles are multinomial distributions, the priors are Dirichlet, and the base measure is the Bhattacharyya coefficient, we can derive an analytical form that allows efficient calculation. For the task of word similarity estimation using a large amount of Web data in Japanese, we show that the proposed measure gives better accuracies than other well-known similarity measures.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Bhattacharyya</author>
</authors>
<title>On a measure of divergence between two statistical populations defined by their probability distributions.</title>
<date>1943</date>
<journal>Bull. Calcutta Math. Soc.,</journal>
<pages>49--214</pages>
<contexts>
<context position="5658" citStr="Bhattacharyya, 1943" startWordPosition="878" endWordPosition="879">tation value as expected. The Bayesian estimation and the expectation calculation in Eq. 2 are generally difficult and usually require computationally expensive procedures. Since our motivation for this research is to calculate good semantic similarities for a large set of words (e.g., one million nouns) and apply them to a wide range of NLP tasks, such costs must be minimized. Our technical contribution in this paper is to show that in the case where the context profiles are multinomial distributions, the priors are Dirichlet, and the base similarity measure is the Bhattacharyya coefficient (Bhattacharyya, 1943), we can derive an analytical form for Eq. 2, that enables efficient calculation (with some implementation tricks). In experiments, we estimate semantic similarities using a large amount of Web data in Japanese and show that the proposed measure gives better word similarities than a non-Bayesian Bhattacharyya coefficient or other well-known similarity measures such as Jensen-Shannon divergence and the cosine with PMI weights. The rest of the paper is organized as follows. In Section 2, we briefly introduce the Bayesian estimation and the Bhattacharyya coefficient. Section 3 proposes our new Ba</context>
<context position="8774" citStr="Bhattacharyya, 1943" startWordPosition="1404" endWordPosition="1405">bability distributions, we usually utilize the measures on probability distributions such as the Jensen-Shannon (JS) divergence to calculate similarities (Dagan et al., 1994; Dagan et al., 1997). The JS divergence is defined as follows. 1 JS(p1||p2) = 2(KL(p1||pavg) + KL(p2||pavg)), where pavg = p1�p2 2 is a point-wise average of p1 and p2 and KL(.) is the Kullback-Leibler divergence. Although we found that the JS divergence is a good measure, it is difficult to derive an efficient calculation of Eq. 2, even in the Dirichlet prior case.l In this study, we employ the Bhattacharyya coefficient (Bhattacharyya, 1943) (BC for short), which is defined as follows: BC(p1, p2) = The BC is also a similarity measure on probability distributions and is suitable for our purposes as we describe in the next section. Although BC has not been explored well in the literature on distributional word similarities, it is also a good similarity measure as the experiments show. 3 Method In this section, we show that if our base similarity measure is BC and the distributions under which we take the expectation are Dirichlet distributions, then Eq. 2 also has an analytical form, allowing efficient calculation. Here, we calcula</context>
<context position="20136" citStr="Bhattacharyya, 1943" startWordPosition="3434" endWordPosition="3436">Torisawa, 2008). In order to 3We did not use the discounting of the PMI values described in Pantel and Lin (2002). 251 alleviate the effect of local minima of the EM clustering, they proposed averaging the similarities by several different clustering results, which can be obtained by using different initial parameters. In this study, we combined two clustering results (denoted as “s1+s2” in the results), each of which (“s1” and “s2”) has 2,000 hidden classes.4 We included this method since clustering can be regarded as another way of treating data sparseness. BC The Bhattacharyya coefficient (Bhattacharyya, 1943) between p(fk|w1) and p(fkIw2). This is the baseline for BCb. BCa The Bhattacharyya coefficient with absolute discounting. In calculating p(fk|wi), we subtract the discounting value, α, from c(wi7 fk) and equally distribute the residual probability mass to the contexts whose frequency is zero. This is included as an example of naive smoothing methods. Since it is very costly to calculate the similarities with all of the other words (one million in our case), we used the following approximation method that exploits the sparseness of c(wi7 fk). Similar methods were used in Pantel and Lin (2002),</context>
</contexts>
<marker>Bhattacharyya, 1943</marker>
<rawString>A. Bhattacharyya. 1943. On a measure of divergence between two statistical populations defined by their probability distributions. Bull. Calcutta Math. Soc., 49:214–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>TR-10-98,</tech>
<institution>Computer Science Group, Harvard University.</institution>
<contexts>
<context position="4141" citStr="Chen and Goodman, 1998" startWordPosition="650" endWordPosition="653">. In the NLP field, data sparseness has been recognized as a serious problem and tackled in the context of language modeling and supervised machine learning. However, to our knowledge, there 247 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 247–256, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics has been no study that seriously dealt with data sparseness in the context of semantic similarity calculation. The data sparseness problem is usually solved by smoothing, regularization, margin maximization and so on (Chen and Goodman, 1998; Chen and Rosenfeld, 2000; Cortes and Vapnik, 1995). Recently, the Bayesian approach has emerged and achieved promising results with a clearer formulation (Teh, 2006; Mochihashi et al., 2009). In this paper, we apply the Bayesian framework to the calculation of distributional similarity. The method is straightforward: Instead of using the point estimation of v(wi), we first estimate the distribution of the context profile, p(v(wi)), by Bayesian estimation and then take the expectation of the original similarity under this distribution as follows: simb(w1, w2) (2) = E[sim(w1,w2)]{p(v(w1)),p(v(</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. TR-10-98, Computer Science Group, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>A survey of smoothing techniques for ME models.</title>
<date>2000</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="4167" citStr="Chen and Rosenfeld, 2000" startWordPosition="654" endWordPosition="657"> sparseness has been recognized as a serious problem and tackled in the context of language modeling and supervised machine learning. However, to our knowledge, there 247 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 247–256, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics has been no study that seriously dealt with data sparseness in the context of semantic similarity calculation. The data sparseness problem is usually solved by smoothing, regularization, margin maximization and so on (Chen and Goodman, 1998; Chen and Rosenfeld, 2000; Cortes and Vapnik, 1995). Recently, the Bayesian approach has emerged and achieved promising results with a clearer formulation (Teh, 2006; Mochihashi et al., 2009). In this paper, we apply the Bayesian framework to the calculation of distributional similarity. The method is straightforward: Instead of using the point estimation of v(wi), we first estimate the distribution of the context profile, p(v(wi)), by Bayesian estimation and then take the expectation of the original similarity under this distribution as follows: simb(w1, w2) (2) = E[sim(w1,w2)]{p(v(w1)),p(v(w2))} = E[g(v(w1), v(w2))]</context>
</contexts>
<marker>Chen, Rosenfeld, 2000</marker>
<rawString>Stanley F. Chen and Ronald Rosenfeld. 2000. A survey of smoothing techniques for ME models. IEEE Transactions on Speech and Audio Processing, 8(1):37–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Support vector networks.</title>
<date>1995</date>
<booktitle>Machine Learning,</booktitle>
<pages>20--273</pages>
<contexts>
<context position="4193" citStr="Cortes and Vapnik, 1995" startWordPosition="658" endWordPosition="662">nized as a serious problem and tackled in the context of language modeling and supervised machine learning. However, to our knowledge, there 247 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 247–256, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics has been no study that seriously dealt with data sparseness in the context of semantic similarity calculation. The data sparseness problem is usually solved by smoothing, regularization, margin maximization and so on (Chen and Goodman, 1998; Chen and Rosenfeld, 2000; Cortes and Vapnik, 1995). Recently, the Bayesian approach has emerged and achieved promising results with a clearer formulation (Teh, 2006; Mochihashi et al., 2009). In this paper, we apply the Bayesian framework to the calculation of distributional similarity. The method is straightforward: Instead of using the point estimation of v(wi), we first estimate the distribution of the context profile, p(v(wi)), by Bayesian estimation and then take the expectation of the original similarity under this distribution as follows: simb(w1, w2) (2) = E[sim(w1,w2)]{p(v(w1)),p(v(w2))} = E[g(v(w1), v(w2))]{p(v(w1)),p(v(w2))}. The u</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>Corinna Cortes and Vladimir Vapnik. 1995. Support vector networks. Machine Learning, 20:273–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>CRL</author>
</authors>
<title>EDR electronic dictionary version 2.0 technical guide.</title>
<date>2002</date>
<journal>Communications Research Laboratory (CRL).</journal>
<contexts>
<context position="17370" citStr="CRL, 2002" startWordPosition="2970" endWordPosition="2971">uns according to the number of unique co-occurring contexts and the contexts according to the number of unique cooccurring nouns, and then we selected the top one million nouns and 100,000 contexts. We used only 260 million dependency pairs that contained both the selected nouns and the selected contexts. 5.3 Test sets We prepared three test sets as follows. Set “A” and “B”: Thesaurus siblings We considered that words having a common hypernym (i.e., siblings) in a manually constructed thesaurus could constitute a similar word set. We extracted such sets from a Japanese dictionary, EDR (V3.0) (CRL, 2002), which contains concept hierarchies and the mapping between words and concepts. The dictionary contains 304,884 nouns. In all, 6,703 noun sibling sets were extracted with the average set size of 45.96. We randomly chose 200 sets each for sets “A” and “B.” Set “A” is a development set to tune the value of the hyperparameters and “B” is for the validation of the parameter tuning. Set “C”: Closed sets Murata et al. (2004) constructed a dataset that contains several closed word sets such as the names of countries, rivers, sumo wrestlers, etc. We used all of the 45 sets that are marked as “complet</context>
</contexts>
<marker>CRL, 2002</marker>
<rawString>CRL. 2002. EDR electronic dictionary version 2.0 technical guide. Communications Research Laboratory (CRL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Fernando Pereira</author>
<author>Lillian Lee</author>
</authors>
<title>Similarity-based estimation of word cooccurrence probabilities.</title>
<date>1994</date>
<booktitle>In Proceedings of ACL 94.</booktitle>
<contexts>
<context position="1884" citStr="Dagan et al., 1994" startWordPosition="267" endWordPosition="270">ata in Japanese, we show that the proposed measure gives better accuracies than other well-known similarity measures. 1 Introduction The semantic similarity of words is a longstanding topic in computational linguistics because it is theoretically intriguing and has many applications in the field. Many researchers have conducted studies based on the distributional hypothesis (Harris, 1954), which states that words that occur in the same contexts tend to have similar meanings. A number of semantic similarity measures have been proposed based on this hypothesis (Hindle, 1990; Grefenstette, 1994; Dagan et al., 1994; Dagan et al., 1995; Lin, 1998; Dagan et al., 1999). ∗The work was done while the author was at NICT. In general, most semantic similarity measures have the following form: sim(w1, w2) = g(v(w1), v(w2)), (1) where v(wi) is a vector that represents the contexts in which wi appears, which we call a context profile of wi. The function g is a function on these context profiles that is expected to produce good similarities. Each dimension of the vector corresponds to a context, fk, which is typically a neighboring word or a word having dependency relations with wi in a corpus. Its value, vk(wi), i</context>
<context position="8327" citStr="Dagan et al., 1994" startWordPosition="1325" endWordPosition="1328">t distribution for this simplest case, and it can be analytically calculated as follows. p(0|D) = Dir(0|lαk + c(k)}), (6) where c(k) is the frequency of choice k in data D. For example, c(k) = c(wi, fk) in the estimation of p(fk|wi). This is very simple: we just need to add the observed counts to the hyperparameters. p(D|φ)pprior(φ) . (4) p(D) p(φ|D) = φαk−1 k . (5) K ri k=1 248 2.2 Bhattacharyya coefficient When the context profiles are probability distributions, we usually utilize the measures on probability distributions such as the Jensen-Shannon (JS) divergence to calculate similarities (Dagan et al., 1994; Dagan et al., 1997). The JS divergence is defined as follows. 1 JS(p1||p2) = 2(KL(p1||pavg) + KL(p2||pavg)), where pavg = p1�p2 2 is a point-wise average of p1 and p2 and KL(.) is the Kullback-Leibler divergence. Although we found that the JS divergence is a good measure, it is difficult to derive an efficient calculation of Eq. 2, even in the Dirichlet prior case.l In this study, we employ the Bhattacharyya coefficient (Bhattacharyya, 1943) (BC for short), which is defined as follows: BC(p1, p2) = The BC is also a similarity measure on probability distributions and is suitable for our purpo</context>
<context position="18996" citStr="Dagan et al., 1994" startWordPosition="3248" endWordPosition="3251"> from those of our one-million-word vocabulary. We filtered out the words that are not included in our vocabulary and removed the sets with size less than 2 after the filtering. Set “A” contained 3,740 words that are actually evaluated, with about 115 answers on average, and “B” contained 3,657 words with about 65 answers on average. Set “C” contained 8,853 words with about 1,700 answers on average. 5.4 Compared similarity measures We compared our Bayesian Bhattacharyya similarity measure, BCb, with the following similarity measures. JS Jensen-Shannon divergence between p(fk|w1) and p(fk|w2) (Dagan et al., 1994; Dagan et al., 1999). PMI-cos The cosine of the context profile vectors, where the k-th dimension is the pointwise mutual information (PMI) between wi and fk defined as: PMI(wi, fk) = log p(wi,fk) p(wi)p(fk) (Pantel and Lin, 2002; Pantel et al., 2009).3 Cls-JS Kazama et al. (2009) proposed using the Jensen-Shannon divergence between hidden class distributions, p(c|w1) and p(c|w2), which are obtained by using an EM-based clustering of dependency relations with a model p(wi, fk) = ∑ c p(wi|c)p(fk|c)p(c) (Kazama and Torisawa, 2008). In order to 3We did not use the discounting of the PMI values d</context>
</contexts>
<marker>Dagan, Pereira, Lee, 1994</marker>
<rawString>Ido Dagan, Fernando Pereira, and Lillian Lee. 1994. Similarity-based estimation of word cooccurrence probabilities. In Proceedings of ACL 94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Shaul Marcus</author>
<author>Shaul Markovitch</author>
</authors>
<title>Contextual word similarity and estimation from sparse data.</title>
<date>1995</date>
<journal>Computer, Speech and Language,</journal>
<pages>9--123</pages>
<contexts>
<context position="1904" citStr="Dagan et al., 1995" startWordPosition="271" endWordPosition="274">show that the proposed measure gives better accuracies than other well-known similarity measures. 1 Introduction The semantic similarity of words is a longstanding topic in computational linguistics because it is theoretically intriguing and has many applications in the field. Many researchers have conducted studies based on the distributional hypothesis (Harris, 1954), which states that words that occur in the same contexts tend to have similar meanings. A number of semantic similarity measures have been proposed based on this hypothesis (Hindle, 1990; Grefenstette, 1994; Dagan et al., 1994; Dagan et al., 1995; Lin, 1998; Dagan et al., 1999). ∗The work was done while the author was at NICT. In general, most semantic similarity measures have the following form: sim(w1, w2) = g(v(w1), v(w2)), (1) where v(wi) is a vector that represents the contexts in which wi appears, which we call a context profile of wi. The function g is a function on these context profiles that is expected to produce good similarities. Each dimension of the vector corresponds to a context, fk, which is typically a neighboring word or a word having dependency relations with wi in a corpus. Its value, vk(wi), is typically a co-occ</context>
</contexts>
<marker>Dagan, Marcus, Markovitch, 1995</marker>
<rawString>Ido Dagan, Shaul Marcus, and Shaul Markovitch. 1995. Contextual word similarity and estimation from sparse data. Computer, Speech and Language, 9:123–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Lillian Lee</author>
<author>Fernando Pereira</author>
</authors>
<title>Similarity-based methods for word sense disambiguation.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL 97.</booktitle>
<contexts>
<context position="8348" citStr="Dagan et al., 1997" startWordPosition="1329" endWordPosition="1332">his simplest case, and it can be analytically calculated as follows. p(0|D) = Dir(0|lαk + c(k)}), (6) where c(k) is the frequency of choice k in data D. For example, c(k) = c(wi, fk) in the estimation of p(fk|wi). This is very simple: we just need to add the observed counts to the hyperparameters. p(D|φ)pprior(φ) . (4) p(D) p(φ|D) = φαk−1 k . (5) K ri k=1 248 2.2 Bhattacharyya coefficient When the context profiles are probability distributions, we usually utilize the measures on probability distributions such as the Jensen-Shannon (JS) divergence to calculate similarities (Dagan et al., 1994; Dagan et al., 1997). The JS divergence is defined as follows. 1 JS(p1||p2) = 2(KL(p1||pavg) + KL(p2||pavg)), where pavg = p1�p2 2 is a point-wise average of p1 and p2 and KL(.) is the Kullback-Leibler divergence. Although we found that the JS divergence is a good measure, it is difficult to derive an efficient calculation of Eq. 2, even in the Dirichlet prior case.l In this study, we employ the Bhattacharyya coefficient (Bhattacharyya, 1943) (BC for short), which is defined as follows: BC(p1, p2) = The BC is also a similarity measure on probability distributions and is suitable for our purposes as we describe in</context>
</contexts>
<marker>Dagan, Lee, Pereira, 1997</marker>
<rawString>Ido Dagan, Lillian Lee, and Fernando Pereira. 1997. Similarity-based methods for word sense disambiguation. In Proceedings of ACL 97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Lillian Lee</author>
<author>Fernando Pereira</author>
</authors>
<title>Similarity-based models of word cooccurrence probabilities.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="1936" citStr="Dagan et al., 1999" startWordPosition="277" endWordPosition="280">gives better accuracies than other well-known similarity measures. 1 Introduction The semantic similarity of words is a longstanding topic in computational linguistics because it is theoretically intriguing and has many applications in the field. Many researchers have conducted studies based on the distributional hypothesis (Harris, 1954), which states that words that occur in the same contexts tend to have similar meanings. A number of semantic similarity measures have been proposed based on this hypothesis (Hindle, 1990; Grefenstette, 1994; Dagan et al., 1994; Dagan et al., 1995; Lin, 1998; Dagan et al., 1999). ∗The work was done while the author was at NICT. In general, most semantic similarity measures have the following form: sim(w1, w2) = g(v(w1), v(w2)), (1) where v(wi) is a vector that represents the contexts in which wi appears, which we call a context profile of wi. The function g is a function on these context profiles that is expected to produce good similarities. Each dimension of the vector corresponds to a context, fk, which is typically a neighboring word or a word having dependency relations with wi in a corpus. Its value, vk(wi), is typically a co-occurrence frequency c(wi, fk), a c</context>
<context position="19017" citStr="Dagan et al., 1999" startWordPosition="3252" endWordPosition="3255">ne-million-word vocabulary. We filtered out the words that are not included in our vocabulary and removed the sets with size less than 2 after the filtering. Set “A” contained 3,740 words that are actually evaluated, with about 115 answers on average, and “B” contained 3,657 words with about 65 answers on average. Set “C” contained 8,853 words with about 1,700 answers on average. 5.4 Compared similarity measures We compared our Bayesian Bhattacharyya similarity measure, BCb, with the following similarity measures. JS Jensen-Shannon divergence between p(fk|w1) and p(fk|w2) (Dagan et al., 1994; Dagan et al., 1999). PMI-cos The cosine of the context profile vectors, where the k-th dimension is the pointwise mutual information (PMI) between wi and fk defined as: PMI(wi, fk) = log p(wi,fk) p(wi)p(fk) (Pantel and Lin, 2002; Pantel et al., 2009).3 Cls-JS Kazama et al. (2009) proposed using the Jensen-Shannon divergence between hidden class distributions, p(c|w1) and p(c|w2), which are obtained by using an EM-based clustering of dependency relations with a model p(wi, fk) = ∑ c p(wi|c)p(fk|c)p(c) (Kazama and Torisawa, 2008). In order to 3We did not use the discounting of the PMI values described in Pantel an</context>
</contexts>
<marker>Dagan, Lee, Pereira, 1999</marker>
<rawString>Ido Dagan, Lillian Lee, and Fernando Pereira. 1999. Similarity-based models of word cooccurrence probabilities. Machine Learning, 34(1-3):43–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations In Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="1864" citStr="Grefenstette, 1994" startWordPosition="265" endWordPosition="266">arge amount of Web data in Japanese, we show that the proposed measure gives better accuracies than other well-known similarity measures. 1 Introduction The semantic similarity of words is a longstanding topic in computational linguistics because it is theoretically intriguing and has many applications in the field. Many researchers have conducted studies based on the distributional hypothesis (Harris, 1954), which states that words that occur in the same contexts tend to have similar meanings. A number of semantic similarity measures have been proposed based on this hypothesis (Hindle, 1990; Grefenstette, 1994; Dagan et al., 1994; Dagan et al., 1995; Lin, 1998; Dagan et al., 1999). ∗The work was done while the author was at NICT. In general, most semantic similarity measures have the following form: sim(w1, w2) = g(v(w1), v(w2)), (1) where v(wi) is a vector that represents the contexts in which wi appears, which we call a context profile of wi. The function g is a function on these context profiles that is expected to produce good similarities. Each dimension of the vector corresponds to a context, fk, which is typically a neighboring word or a word having dependency relations with wi in a corpus. </context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Gregory Grefenstette. 1994. Explorations In Automatic Thesaurus Discovery. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<title>Distributional structure.</title>
<date>1954</date>
<pages>146--142</pages>
<publisher>Word,</publisher>
<contexts>
<context position="1657" citStr="Harris, 1954" startWordPosition="232" endWordPosition="233"> the priors are Dirichlet, and the base measure is the Bhattacharyya coefficient, we can derive an analytical form that allows efficient calculation. For the task of word similarity estimation using a large amount of Web data in Japanese, we show that the proposed measure gives better accuracies than other well-known similarity measures. 1 Introduction The semantic similarity of words is a longstanding topic in computational linguistics because it is theoretically intriguing and has many applications in the field. Many researchers have conducted studies based on the distributional hypothesis (Harris, 1954), which states that words that occur in the same contexts tend to have similar meanings. A number of semantic similarity measures have been proposed based on this hypothesis (Hindle, 1990; Grefenstette, 1994; Dagan et al., 1994; Dagan et al., 1995; Lin, 1998; Dagan et al., 1999). ∗The work was done while the author was at NICT. In general, most semantic similarity measures have the following form: sim(w1, w2) = g(v(w1), v(w2)), (1) where v(wi) is a vector that represents the contexts in which wi appears, which we call a context profile of wi. The function g is a function on these context profi</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig Harris. 1954. Distributional structure. Word, pages 146–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
</authors>
<title>Noun classification from predicate-argument structures.</title>
<date>1990</date>
<booktitle>In Proceedings of ACL-90,</booktitle>
<pages>268--275</pages>
<contexts>
<context position="1844" citStr="Hindle, 1990" startWordPosition="263" endWordPosition="264">tion using a large amount of Web data in Japanese, we show that the proposed measure gives better accuracies than other well-known similarity measures. 1 Introduction The semantic similarity of words is a longstanding topic in computational linguistics because it is theoretically intriguing and has many applications in the field. Many researchers have conducted studies based on the distributional hypothesis (Harris, 1954), which states that words that occur in the same contexts tend to have similar meanings. A number of semantic similarity measures have been proposed based on this hypothesis (Hindle, 1990; Grefenstette, 1994; Dagan et al., 1994; Dagan et al., 1995; Lin, 1998; Dagan et al., 1999). ∗The work was done while the author was at NICT. In general, most semantic similarity measures have the following form: sim(w1, w2) = g(v(w1), v(w2)), (1) where v(wi) is a vector that represents the contexts in which wi appears, which we call a context profile of wi. The function g is a function on these context profiles that is expected to produce good similarities. Each dimension of the vector corresponds to a context, fk, which is typically a neighboring word or a word having dependency relations w</context>
</contexts>
<marker>Hindle, 1990</marker>
<rawString>Donald Hindle. 1990. Noun classification from predicate-argument structures. In Proceedings of ACL-90, pages 268–275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Inducing gazetteers for named entity recognition by large-scale clustering of dependency relations.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT.</booktitle>
<contexts>
<context position="15488" citStr="Kazama and Torisawa (2008)" startWordPosition="2642" endWordPosition="2645">rs. This setting could be seen as document retrieval, and we can use an evaluation measure such as the mean of the precision at top T (MP@T) or the mean average precision (MAP). For each input word, P@T (precision at top T) and AP (average precision) are defined as follows. 1 P@T = T S(wi ∈ ans)P@i. S(wi E ans) returns 1 if the output word wi is in the answers, and 0 otherwise. N is the number of outputs and R is the number of the answers. MP@T and MAP are the averages of these values over all input words. 5.2 Collecting context profiles Dependency relations are used as context profiles as in Kazama and Torisawa (2008) and Kazama et al. (2009). From a large corpus of Japanese Web documents (Shinzato et al., 2008) (100 million T S(wi ∈ ans), i=1 AP = 1 N R i=1 250 documents), where each sentence has a dependency parse, we extracted noun-verb and nounnoun dependencies with relation types and then calculated their frequencies in the corpus. If a noun, n, depends on a word, w, with a relation, r, we collect a dependency pair, (n, (w, r)). That is, a context fk, is (w, r) here. For noun-verb dependencies, postpositions in Japanese represent relation types. For example, we extract a dependency relation (174,/, ( </context>
<context position="19531" citStr="Kazama and Torisawa, 2008" startWordPosition="3334" endWordPosition="3337">ty measures. JS Jensen-Shannon divergence between p(fk|w1) and p(fk|w2) (Dagan et al., 1994; Dagan et al., 1999). PMI-cos The cosine of the context profile vectors, where the k-th dimension is the pointwise mutual information (PMI) between wi and fk defined as: PMI(wi, fk) = log p(wi,fk) p(wi)p(fk) (Pantel and Lin, 2002; Pantel et al., 2009).3 Cls-JS Kazama et al. (2009) proposed using the Jensen-Shannon divergence between hidden class distributions, p(c|w1) and p(c|w2), which are obtained by using an EM-based clustering of dependency relations with a model p(wi, fk) = ∑ c p(wi|c)p(fk|c)p(c) (Kazama and Torisawa, 2008). In order to 3We did not use the discounting of the PMI values described in Pantel and Lin (2002). 251 alleviate the effect of local minima of the EM clustering, they proposed averaging the similarities by several different clustering results, which can be obtained by using different initial parameters. In this study, we combined two clustering results (denoted as “s1+s2” in the results), each of which (“s1” and “s2”) has 2,000 hidden classes.4 We included this method since clustering can be regarded as another way of treating data sparseness. BC The Bhattacharyya coefficient (Bhattacharyya, </context>
<context position="25506" citStr="Kazama and Torisawa, 2008" startWordPosition="4404" endWordPosition="4407">late well with MPs, unlike in the case of Sets A and B. We thought one possible reason is that the number of outputs, 500, for each word was not large enough to assess MAP values correctly because the average number of answers is 1,700 for this dataset. In fact, we could output more than 500 words if we ignored the cost of storage. Therefore, we also included the results for the case where L = M = 3600 and N = 2, 000. Even with this setting, however, MAP did not correlate well with MPs. Although Cls-JS showed very good performance for Set C, note that the EM clustering is very time-consuming (Kazama and Torisawa, 2008), and it took about one week with 24 CPU cores to get one clustering result in our computing environment. On the other hand, the preparation Table 2: Performance on siblings (Set B). Measure MAP MP @1 @5 @10 @20 JS 0.0265 0.208 0.116 0.0855 0.0627 PMI-cos 0.0283 0.203 0.116 0.0871 0.0660 Cls-JS (s1+s2) 0.0274 0.194 0.115 0.0859 0.0643 BC 0.0295 0.223 0.124 0.0922 0.0693 BCb (0.0002) 0.0301 0.225 0.128 0.0958 0.0718 BCb (0.0016) 0.0313 0.246 0.135 0.103 0.0758 BCb (0.0032) 0.0279 0.228 0.127 0.0938 0.0698 BCa (0.0016) 0.0297 0.223 0.125 0.0934 0.0700 BCa (0.0362) 0.0298 0.223 0.125 0.0934 0.070</context>
</contexts>
<marker>Kazama, Torisawa, 2008</marker>
<rawString>Jun’ichi Kazama and Kentaro Torisawa. 2008. Inducing gazetteers for named entity recognition by large-scale clustering of dependency relations. In Proceedings of ACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Stijn De Saeger</author>
<author>Kentaro Torisawa</author>
<author>Masaki Murata</author>
</authors>
<title>Generating a large-scale analogy list using a probabilistic clustering based on noun-verb dependency profiles.</title>
<date>2009</date>
<booktitle>In Proceedings of 15th Annual Meeting of The Association for Natural Language Processing (in Japanese).</booktitle>
<marker>Kazama, De Saeger, Torisawa, Murata, 2009</marker>
<rawString>Jun’ichi Kazama, Stijn De Saeger, Kentaro Torisawa, and Masaki Murata. 2009. Generating a large-scale analogy list using a probabilistic clustering based on noun-verb dependency profiles. In Proceedings of 15th Annual Meeting of The Association for Natural Language Processing (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL98,</booktitle>
<pages>768--774</pages>
<contexts>
<context position="1915" citStr="Lin, 1998" startWordPosition="275" endWordPosition="276">ed measure gives better accuracies than other well-known similarity measures. 1 Introduction The semantic similarity of words is a longstanding topic in computational linguistics because it is theoretically intriguing and has many applications in the field. Many researchers have conducted studies based on the distributional hypothesis (Harris, 1954), which states that words that occur in the same contexts tend to have similar meanings. A number of semantic similarity measures have been proposed based on this hypothesis (Hindle, 1990; Grefenstette, 1994; Dagan et al., 1994; Dagan et al., 1995; Lin, 1998; Dagan et al., 1999). ∗The work was done while the author was at NICT. In general, most semantic similarity measures have the following form: sim(w1, w2) = g(v(w1), v(w2)), (1) where v(wi) is a vector that represents the contexts in which wi appears, which we call a context profile of wi. The function g is a function on these context profiles that is expected to produce good similarities. Each dimension of the vector corresponds to a context, fk, which is typically a neighboring word or a word having dependency relations with wi in a corpus. Its value, vk(wi), is typically a co-occurrence fre</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of COLING/ACL98, pages 768–774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daichi Mochihashi</author>
<author>Takeshi Yamada</author>
<author>Naonori Ueda</author>
</authors>
<title>Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP 2009,</booktitle>
<pages>100--108</pages>
<contexts>
<context position="4333" citStr="Mochihashi et al., 2009" startWordPosition="679" endWordPosition="682"> 247 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 247–256, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics has been no study that seriously dealt with data sparseness in the context of semantic similarity calculation. The data sparseness problem is usually solved by smoothing, regularization, margin maximization and so on (Chen and Goodman, 1998; Chen and Rosenfeld, 2000; Cortes and Vapnik, 1995). Recently, the Bayesian approach has emerged and achieved promising results with a clearer formulation (Teh, 2006; Mochihashi et al., 2009). In this paper, we apply the Bayesian framework to the calculation of distributional similarity. The method is straightforward: Instead of using the point estimation of v(wi), we first estimate the distribution of the context profile, p(v(wi)), by Bayesian estimation and then take the expectation of the original similarity under this distribution as follows: simb(w1, w2) (2) = E[sim(w1,w2)]{p(v(w1)),p(v(w2))} = E[g(v(w1), v(w2))]{p(v(w1)),p(v(w2))}. The uncertainty due to data sparseness is represented by p(v(wi)), and taking the expectation enables us to take this into account. The Bayesian </context>
<context position="30771" citStr="Mochihashi et al., 2009" startWordPosition="5312" endWordPosition="5315">016) 55,915 54,786 90,472 127,877 BC/BCb 2.91 1.80 2.14 1.48 and other well-known similarity measures. As a smoothing method, it also outperformed a naive absolute discounting. Of course, we cannot say that the proposed method is better than any other sophisticated smoothing method at this point. However, as noted above, there has been no serious attempt to assess the effect of smoothing in the context of word similarity calculation. Recent studies have pointed out that the Bayesian framework derives state-of-the-art smoothing methods such as Kneser-Ney smoothing as a special case (Teh, 2006; Mochihashi et al., 2009). Consequently, it is reasonable to resort to the Bayesian framework. Conceptually, our method is equivalent to modifying p(1fk|wi) as p(fk |wi) = { Γ(α0+a0+Γ21)Γ(αk+c(w k),fk)) J 2 and taking the Bhattacharyya coefficient. However, the implication of this form has not yet been investigated, and so we leave it for future research. Our method is the simplest one as a Bayesian method. We did not employ any numerical optimization or sampling iterations, as in a more complete use of the Bayesian framework (Teh, 2006; Mochihashi et al., 2009). Instead, we used the obtained analytical form directly </context>
</contexts>
<marker>Mochihashi, Yamada, Ueda, 2009</marker>
<rawString>Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda. 2009. Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling. In Proceedings of ACL-IJCNLP 2009, pages 100– 108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaki Murata</author>
<author>Qing Ma</author>
<author>Tamotsu Shirado</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Database for evaluating extracted terms and tool for visualizing the terms.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC 2004 Workshop: Computational and Computer-Assisted Terminology,</booktitle>
<pages>6--9</pages>
<contexts>
<context position="17793" citStr="Murata et al. (2004)" startWordPosition="3042" endWordPosition="3045">at words having a common hypernym (i.e., siblings) in a manually constructed thesaurus could constitute a similar word set. We extracted such sets from a Japanese dictionary, EDR (V3.0) (CRL, 2002), which contains concept hierarchies and the mapping between words and concepts. The dictionary contains 304,884 nouns. In all, 6,703 noun sibling sets were extracted with the average set size of 45.96. We randomly chose 200 sets each for sets “A” and “B.” Set “A” is a development set to tune the value of the hyperparameters and “B” is for the validation of the parameter tuning. Set “C”: Closed sets Murata et al. (2004) constructed a dataset that contains several closed word sets such as the names of countries, rivers, sumo wrestlers, etc. We used all of the 45 sets that are marked as “complete” in the data, containing 12,827 unique words in total. Note that we do not deal with ambiguities in the construction of these sets as well as in the calculation of similarities. That is, a word can be contained in several sets, and the answers for such a word is the union of the words in the sets it belongs to (excluding the word itself). In addition, note that the words in these test sets are different from those of </context>
</contexts>
<marker>Murata, Ma, Shirado, Isahara, 2004</marker>
<rawString>Masaki Murata, Qing Ma, Tamotsu Shirado, and Hitoshi Isahara. 2004. Database for evaluating extracted terms and tool for visualizing the terms. In Proceedings of LREC 2004 Workshop: Computational and Computer-Assisted Terminology, pages 6–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>Discovering word senses from text.</title>
<date>2002</date>
<booktitle>In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>613--619</pages>
<contexts>
<context position="19226" citStr="Pantel and Lin, 2002" startWordPosition="3288" endWordPosition="3291">aluated, with about 115 answers on average, and “B” contained 3,657 words with about 65 answers on average. Set “C” contained 8,853 words with about 1,700 answers on average. 5.4 Compared similarity measures We compared our Bayesian Bhattacharyya similarity measure, BCb, with the following similarity measures. JS Jensen-Shannon divergence between p(fk|w1) and p(fk|w2) (Dagan et al., 1994; Dagan et al., 1999). PMI-cos The cosine of the context profile vectors, where the k-th dimension is the pointwise mutual information (PMI) between wi and fk defined as: PMI(wi, fk) = log p(wi,fk) p(wi)p(fk) (Pantel and Lin, 2002; Pantel et al., 2009).3 Cls-JS Kazama et al. (2009) proposed using the Jensen-Shannon divergence between hidden class distributions, p(c|w1) and p(c|w2), which are obtained by using an EM-based clustering of dependency relations with a model p(wi, fk) = ∑ c p(wi|c)p(fk|c)p(c) (Kazama and Torisawa, 2008). In order to 3We did not use the discounting of the PMI values described in Pantel and Lin (2002). 251 alleviate the effect of local minima of the EM clustering, they proposed averaging the similarities by several different clustering results, which can be obtained by using different initial p</context>
<context position="20735" citStr="Pantel and Lin (2002)" startWordPosition="3533" endWordPosition="3536"> (Bhattacharyya, 1943) between p(fk|w1) and p(fkIw2). This is the baseline for BCb. BCa The Bhattacharyya coefficient with absolute discounting. In calculating p(fk|wi), we subtract the discounting value, α, from c(wi7 fk) and equally distribute the residual probability mass to the contexts whose frequency is zero. This is included as an example of naive smoothing methods. Since it is very costly to calculate the similarities with all of the other words (one million in our case), we used the following approximation method that exploits the sparseness of c(wi7 fk). Similar methods were used in Pantel and Lin (2002), Kazama et al. (2009), and Pantel et al. (2009) as well. For a given word, wi, we sort the contexts in descending order according to c(wi7 fk) and retrieve the top-L contexts.5 For each selected context, we sort the words in descending order according to c(wi7 fk) and retrieve the top-M words (L = M = 1600).6 We merge all of the words above as candidate words and calculate the similarity only for the candidate words. Finally, the top 500 similar words are output. Note also that we used modified counts, log(c(wi7 fk)) + 1, instead of raw counts, c(wi7 fk), with the intention of alleviating the</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Patrick Pantel and Dekang Lin. 2002. Discovering word senses from text. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 613–619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
</authors>
<title>Eric Crestan, Arkady Borkovsky, AnaMaria Popescu, and Vishnu Vyas.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>938--947</pages>
<marker>Pantel, 2009</marker>
<rawString>Patrick Pantel, Eric Crestan, Arkady Borkovsky, AnaMaria Popescu, and Vishnu Vyas. 2009. Web-scale distributional similarity and entity set expansion. In Proceedings of EMNLP 2009, pages 938–947.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T W Rauber</author>
<author>T Braun</author>
<author>K Berns</author>
</authors>
<date>2008</date>
<booktitle>Probabilistic distance measures of the Dirichlet and Beta distributions. Pattern Recognition,</booktitle>
<pages>41--637</pages>
<contexts>
<context position="33726" citStr="Rauber et al. (2008)" startWordPosition="5847" endWordPosition="5850">r it will be a weighted version of BCb. BCb can also be generalized to the case where the base similarity is BCd(p1, p2) = ∑Kk=1 pd1k × pd2k, where d &gt; 0. The Bayesian analytical form becomes as follows. Γ(α0 + a0)Γ(β0 + b0) BCd b (w1, w2) = x Γ(α0 + a0 + d)Γ(β0 + b0 + d) Γ(αk + c(w1, fk) + d)Γ(βk + c(w2, fk) + d) Γ(αk + c(w1, fk))Γ(βk + c(w2, fk)) . See Appendix A for the derivation. However, we restricted ourselves to the case of d = 21 in this study. Finally, note that our BCb is different from the Bhattacharyya distance measure on Dirichlet distributions of the following form described in Rauber et al. (2008) in its motivation and analytical form: /r(ao)r(Qo) x FIk r((a&apos; +,Qk)/2) 1 (g) �lik r(ak)�Q k Γ(βk) Γ(2 Pk (αk + β′k)) Empirical and theoretical comparisons with this measure also form one of the future directions.10 7 Conclusion We proposed a Bayesian method for robust distributional word similarities. Our method uses a distribution of context profiles obtained by Bayesian 10Our preliminary experiments show that calculating similarity using Eq. 9 for the Dirichlet distributions obtained by Eq. 6 does not produce meaningful similarity (i.e., the accuracy is very low). estimation and takes the </context>
</contexts>
<marker>Rauber, Braun, Berns, 2008</marker>
<rawString>T. W. Rauber, T. Braun, and K. Berns. 2008. Probabilistic distance measures of the Dirichlet and Beta distributions. Pattern Recognition, 41:637–645.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keiji Shinzato</author>
<author>Tomohide Shibata</author>
<author>Daisuke Kawahara</author>
<author>Chikara Hashimoto</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Tsubaki: An open search engine infrastructure for developing new information access.</title>
<date>2008</date>
<booktitle>In Proceedings of IJCNLP</booktitle>
<contexts>
<context position="15584" citStr="Shinzato et al., 2008" startWordPosition="2659" endWordPosition="2662">he mean of the precision at top T (MP@T) or the mean average precision (MAP). For each input word, P@T (precision at top T) and AP (average precision) are defined as follows. 1 P@T = T S(wi ∈ ans)P@i. S(wi E ans) returns 1 if the output word wi is in the answers, and 0 otherwise. N is the number of outputs and R is the number of the answers. MP@T and MAP are the averages of these values over all input words. 5.2 Collecting context profiles Dependency relations are used as context profiles as in Kazama and Torisawa (2008) and Kazama et al. (2009). From a large corpus of Japanese Web documents (Shinzato et al., 2008) (100 million T S(wi ∈ ans), i=1 AP = 1 N R i=1 250 documents), where each sentence has a dependency parse, we extracted noun-verb and nounnoun dependencies with relation types and then calculated their frequencies in the corpus. If a noun, n, depends on a word, w, with a relation, r, we collect a dependency pair, (n, (w, r)). That is, a context fk, is (w, r) here. For noun-verb dependencies, postpositions in Japanese represent relation types. For example, we extract a dependency relation (174,/, ( IN7, ;�- )) from the sentence below, where a postposition “;�, (wo)” is used to mark the verb ob</context>
</contexts>
<marker>Shinzato, Shibata, Kawahara, Hashimoto, Kurohashi, 2008</marker>
<rawString>Keiji Shinzato, Tomohide Shibata, Daisuke Kawahara, Chikara Hashimoto, and Sadao Kurohashi. 2008. Tsubaki: An open search engine infrastructure for developing new information access. In Proceedings of IJCNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
</authors>
<title>A hierarchical Bayesian language model based on Pitman-Yor processes.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL</booktitle>
<pages>985--992</pages>
<contexts>
<context position="4307" citStr="Teh, 2006" startWordPosition="677" endWordPosition="678">edge, there 247 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 247–256, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics has been no study that seriously dealt with data sparseness in the context of semantic similarity calculation. The data sparseness problem is usually solved by smoothing, regularization, margin maximization and so on (Chen and Goodman, 1998; Chen and Rosenfeld, 2000; Cortes and Vapnik, 1995). Recently, the Bayesian approach has emerged and achieved promising results with a clearer formulation (Teh, 2006; Mochihashi et al., 2009). In this paper, we apply the Bayesian framework to the calculation of distributional similarity. The method is straightforward: Instead of using the point estimation of v(wi), we first estimate the distribution of the context profile, p(v(wi)), by Bayesian estimation and then take the expectation of the original similarity under this distribution as follows: simb(w1, w2) (2) = E[sim(w1,w2)]{p(v(w1)),p(v(w2))} = E[g(v(w1), v(w2))]{p(v(w1)),p(v(w2))}. The uncertainty due to data sparseness is represented by p(v(wi)), and taking the expectation enables us to take this i</context>
<context position="30745" citStr="Teh, 2006" startWordPosition="5310" endWordPosition="5311">345 BCb(0.0016) 55,915 54,786 90,472 127,877 BC/BCb 2.91 1.80 2.14 1.48 and other well-known similarity measures. As a smoothing method, it also outperformed a naive absolute discounting. Of course, we cannot say that the proposed method is better than any other sophisticated smoothing method at this point. However, as noted above, there has been no serious attempt to assess the effect of smoothing in the context of word similarity calculation. Recent studies have pointed out that the Bayesian framework derives state-of-the-art smoothing methods such as Kneser-Ney smoothing as a special case (Teh, 2006; Mochihashi et al., 2009). Consequently, it is reasonable to resort to the Bayesian framework. Conceptually, our method is equivalent to modifying p(1fk|wi) as p(fk |wi) = { Γ(α0+a0+Γ21)Γ(αk+c(w k),fk)) J 2 and taking the Bhattacharyya coefficient. However, the implication of this form has not yet been investigated, and so we leave it for future research. Our method is the simplest one as a Bayesian method. We did not employ any numerical optimization or sampling iterations, as in a more complete use of the Bayesian framework (Teh, 2006; Mochihashi et al., 2009). Instead, we used the obtained</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Yee Whye Teh. 2006. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proceedings of COLING-ACL 2006, pages 985–992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akira Terada</author>
<author>Minoru Yoshida</author>
<author>Hiroshi Nakagawa</author>
</authors>
<title>A tool for constructing a synonym dictionary using context information.</title>
<date>2004</date>
<booktitle>In IPSJ SIG Technical Report (in Japanese),</booktitle>
<pages>87--94</pages>
<contexts>
<context position="21563" citStr="Terada et al. (2004)" startWordPosition="3684" endWordPosition="3687">sort the words in descending order according to c(wi7 fk) and retrieve the top-M words (L = M = 1600).6 We merge all of the words above as candidate words and calculate the similarity only for the candidate words. Finally, the top 500 similar words are output. Note also that we used modified counts, log(c(wi7 fk)) + 1, instead of raw counts, c(wi7 fk), with the intention of alleviating the effect of strangely frequent dependencies, which can be found in the Web data. In preliminary experiments, we observed that this modification improves the quality of the top 500 similar words as reported in Terada et al. (2004) and Kazama et al. (2009). 4In the case of EM clustering, the number of unique contexts, fk, was also set to one million instead of 100,000, following Kazama et al. (2009). 5It is possible that the number of contexts with non-zero counts is less than L. In that case, all of the contexts with non-zero counts are used. 6Sorting is performed only once in the initialization step. Table 1: Performance on siblings (Set A). MP Measure MAP @1 @5 @10 @20 JS 0.0299 0.197 0.122 0.0990 0.0792 PMI-cos 0.0332 0.195 0.124 0.0993 0.0798 Cls-JS (s1) 0.0319 0.195 0.122 0.0988 0.0796 Cls-JS (s2) 0.0295 0.198 0.1</context>
</contexts>
<marker>Terada, Yoshida, Nakagawa, 2004</marker>
<rawString>Akira Terada, Minoru Yoshida, and Hiroshi Nakagawa. 2004. A tool for constructing a synonym dictionary using context information. In IPSJ SIG Technical Report (in Japanese), pages 87–94.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>