<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000012">
<title confidence="0.947771">
Vine Pruning for Efficient Multi-Pass Dependency Parsing
</title>
<note confidence="0.581189333333333">
Alexander M. Rush* Slav Petrov
MIT CSAIL Google
Cambridge, MA 02139, USA New York, NY 10027, USA
</note>
<email confidence="0.997221">
srush@csail.mit.edu slav@google.com
</email>
<sectionHeader confidence="0.994643" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996742705882353">
Coarse-to-fine inference has been shown to be
a robust approximate method for improving
the efficiency of structured prediction models
while preserving their accuracy. We propose
a multi-pass coarse-to-fine architecture for de-
pendency parsing using linear-time vine prun-
ing and structured prediction cascades. Our
first-, second-, and third-order models achieve
accuracies comparable to those of their un-
pruned counterparts, while exploring only a
fraction of the search space. We observe
speed-ups of up to two orders of magnitude
compared to exhaustive search. Our pruned
third-order model is twice as fast as an un-
pruned first-order model and also compares
favorably to a state-of-the-art transition-based
parser for multiple languages.
</bodyText>
<sectionHeader confidence="0.998779" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999893">
Coarse-to-fine inference has been extensively used
to speed up structured prediction models. The gen-
eral idea is simple: use a coarse model where in-
ference is cheap to prune the search space for more
complex models. In this work, we present a multi-
pass coarse-to-fine architecture for graph-based de-
pendency parsing. We start with a linear-time vine
pruning pass and build up to higher-order models,
achieving speed-ups of two orders of magnitude
while maintaining state-of-the-art accuracies.
In constituency parsing, exhaustive inference for
all but the simplest grammars tends to be pro-
hibitively slow. Consequently, most high-accuracy
constituency parsers routinely employ a coarse
grammar to prune dynamic programming chart cells
</bodyText>
<note confidence="0.363971">
* Research conducted at Google.
</note>
<bodyText confidence="0.999967272727273">
of the final grammar of interest (Charniak et al.,
2006; Carreras et al., 2008; Petrov, 2009). While
there are no strong theoretical guarantees for these
approaches,1 in practice one can obtain significant
speed improvements with minimal loss in accuracy.
This benefit comes primarily from reducing the large
grammar constant |G |that can dominate the runtime
of the cubic-time CKY inference algorithm. De-
pendency parsers on the other hand do not have a
multiplicative grammar factor |G|, and until recently
were considered efficient enough for exhaustive in-
ference. However, the increased model complex-
ity of a third-order parser forced Koo and Collins
(2010) to prune with a first-order model in order to
make inference practical. While fairly effective, all
these approaches are limited by the fact that infer-
ence in the coarse model remains cubic in the sen-
tence length. The desire to parse vast amounts of
text necessitates more efficient dependency parsing
algorithms.
We thus propose a multi-pass coarse-to-fine ap-
proach where the initial pass is a linear-time sweep,
which tries to resolve local ambiguities, but leaves
arcs beyond a fixed length b unspecified (Section
3). The dynamic program is a form of vine parsing
(Eisner and Smith, 2005), which we use to compute
parse max-marginals, rather than for finding the 1-
best parse tree. To reduce pruning errors, the param-
eters of the vine parser (and all subsequent pruning
models) are trained using the structured prediction
cascades of Weiss and Taskar (2010) to optimize
for pruning efficiency, and not for 1-best prediction
(Section 4). Despite a limited scope of b = 3, the
</bodyText>
<footnote confidence="0.713614666666667">
1This is in contrast to optimality preserving methods such as
A* search, which typically do not provide sufficient speed-ups
(Pauls and Klein, 2009).
</footnote>
<page confidence="0.637600333333333">
498
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 498–507,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.9999615">
vine pruning pass is able to preserve &gt;98% of the
correct arcs, while ruling out —86% of all possible
arcs. Subsequent i-th order passes introduce larger
scope features, while further constraining the search
space. In Section 5 we present experiments in multi-
ple languages. Our coarse-to-fine first-, second-, and
third-order parsers preserve the accuracy of the un-
pruned models, but are faster by up to two orders of
magnitude. Our pruned third-order model is faster
than an unpruned first-order model, and compares
favorably in speed to the state-of-the-art transition-
based parser of Zhang and Nivre (2011).
It is worth noting the relationship to greedy
transition-based dependency parsers that are also
linear-time (Nivre et al., 2004) or quadratic-time
(Yamada and Matsumoto, 2003). It is their success
that motivates building explicitly trained, linear-time
pruning models. However, while a greedy solu-
tion for arc-standard transition-based parsers can be
computed in linear-time, Kuhlmann et al. (2011)
recently showed that computing exact solutions or
(max-)marginals has time complexity O(n4), mak-
ing these models inappropriate for coarse-to-fine
style pruning. As an alternative, Roark and Holling-
shead (2008) and Bergsma and Cherry (2010)
present approaches where individual classifiers are
used to prune chart cells. Such approaches have the
drawback that pruning decisions are made locally
and therefore can rule out all valid structures, despite
explicitly evaluating O(n2) chart cells. In contrast,
we make pruning decisions based on global parse
max-marginals using a vine pruning pass, which is
linear in the sentence length, but nonetheless guar-
antees to preserve a valid parse structure.
</bodyText>
<sectionHeader confidence="0.932883" genericHeader="method">
2 Motivation &amp; Overview
</sectionHeader>
<bodyText confidence="0.999712181818182">
The goal of this work is fast, high-order, graph-
based dependency parsing. Previous work on con-
stituency parsing demonstrates that performing sev-
eral passes with increasingly more complex mod-
els results in faster inference (Charniak et al., 2006;
Petrov and Klein, 2007). The same technique ap-
plies to dependency parsing with a cascade of mod-
els of increasing order; however, this strategy is
limited by the speed of the simplest model. The
algorithm for first-order dependency parsing (Eis-
ner, 2000) already requires O(n3) time, which Lee
</bodyText>
<figure confidence="0.988384166666667">
0
1
2
3
4
5
6
7
8
9
dependencylength
(b)
</figure>
<figureCaption confidence="0.9992188">
Figure 1: (a) Heat map indicating how likely a par-
ticular head position is for each modifier position.
Greener/darker is likelier. (b) Arc length frequency for
three common modifier tags. Both charts are computed
from all sentences in Section 22 of the PTB.
</figureCaption>
<bodyText confidence="0.999678466666667">
(2002) shows is a practical lower bound for parsing
of context-free grammars. This bound implies that
it is unlikely that there can be an exhaustive pars-
ing algorithm that is asymptotically faster than the
standard approach.
We thus need to leverage domain knowledge to
obtain faster parsing algorithms. It is well-known
that natural language is fairly linear, and most head-
modifier dependencies tend to be short. This prop-
erty is exploited by transition-based dependency
parsers (Yamada and Matsumoto, 2003; Nivre et
al., 2004) and empirically demonstrated in Figure 1.
The heat map on the left shows that most of the
probability mass of modifiers is concentrated among
nearby words, corresponding to a diagonal band in
the matrix representation. On the right we show the
frequency of arc lengths for different modifier part-
of-speech tags. As one can expect, almost all arcs
involving adjectives (ADJ) are very short (length 3
or less), but even arcs involving verbs and nouns are
often short. This structure suggests that it may be
possible to disambiguate most dependencies by con-
sidering only the “banded” portion of the sentence.
We exploit this linear structure by employing a
variant of vine parsing (Eisner and Smith, 2005).2
Vine parsing is a dependency parsing algorithm that
considers only close words as modifiers. Because of
this assumption it runs in linear time. Of course, any
parse tree with hard limits on dependency lengths
will contain major parse errors. We therefore use the
</bodyText>
<footnote confidence="0.8028">
2The term vine parsing is a slight misnomer, since the un-
derlying vine models are as expressive as finite-state automata.
However, this allows them to circumvent the cubic-time bound.
</footnote>
<figure confidence="0.996573266666667">
headindex
1 2 3 4 5 6 7 8 9
modifier index
(a)
1 2 3 4 5 6
frequency
0.5
0.4
0.3
0.2
0.1
0.0
ADJ
NOUN
VERB
</figure>
<page confidence="0.993615">
499
</page>
<table confidence="0.9336533">
* As McGwire neared , fans went wild * As McGwire neared , fans went wild * As McGwire neared , fans went wild
heads * heads * heads *
As As As
McGwire McGwire McGwire
neared neared neared
, , ,
fans fans fans
went went went
wild wild wild
modifiers modifiers modifiers
</table>
<figureCaption confidence="0.9856475">
Figure 2: Multi-pass pruning with a vine, first-order, and second-order model shown as dependencies and filtered
index sets after each pass. Darker cells have higher max-marginal values, while empty cells represent pruned arcs.
</figureCaption>
<bodyText confidence="0.999641928571429">
vine parser only for pruning and augment it to allow
arcs to remain unspecified (by including so called
outer arcs). The vine parser can thereby eliminate
a possibly quadratic number of arcs, while having
the flexibility to defer some decisions and preserve
ambiguity to be resolved by later passes. In Figure 2
for example, the vine pass correctly determined the
head-word of McGwire as neared, limited the head-
word candidates for fans to neared and went, and
decided that the head-word for went falls outside the
band by proposing an outer arc. A subsequent first-
order pass needs to score only a small fraction of all
possible arcs and can be used to further restrict the
search space for the following higher-order passes.
</bodyText>
<sectionHeader confidence="0.961223" genericHeader="method">
3 Graph-Based Dependency Parsing
</sectionHeader>
<bodyText confidence="0.9999224375">
Graph-based dependency parsing models factor all
valid parse trees for a given sentence into smaller
units, which can be scored independently. For in-
stance, in a first-order factorization, the units are just
dependency arcs. We represent these units by an in-
dex set I and use binary vectors Y C {0,1}|I |to
specify a parse tree y E Y such that y(i) = 1 iff the
index i exists in the tree. The index sets of higher-
order models can be constructed out of the index sets
of lower-order models, thus forming a hierarchy that
we will exploit in our coarse-to-fine cascade.
The inference problem is to find the 1-best parse
tree arg maxy∈Y y · w, where w E R|I |is a weight
vector that assigns a score to each index i (we dis-
cuss how w is learned in Section 4). A general-
ization of the 1-best inference problem is to find
the max-marginal score for each index i. Max-
marginals are given by the function M : I —* Y de-
fined as M(i; Y, w) = arg maxy∈Y:y(z)_1 y · w. For
first-order parsing, this corresponds to the best parse
utilizing a given dependency arc. Clearly there are
exponentially many possible parse tree structures,
but fortunately there exist well-known dynamic pro-
gramming algorithms for searching over all possible
structures. We review these below, starting with the
first-order factorization for ease of exposition.
Throughout the paper we make use of some ba-
sic mathematical notation. We write [c] for the enu-
meration {1, ... , c} and [c]a for {a, ... , c}. We use
1[c] for the indicator function, equal to 1 if con-
dition c is true and 0 otherwise. Finally we use
[c]+ = max{0, c} for the positive part of c.
</bodyText>
<subsectionHeader confidence="0.995528">
3.1 First-Order Parsing
</subsectionHeader>
<bodyText confidence="0.99985525">
The simplest way to index a dependency parse struc-
ture is by the individual arcs of the parse tree. This
model is known as first-order or arc-factored. For a
sentence of length n the index set is:
</bodyText>
<equation confidence="0.89004">
I1 = {(h, m) : h E [n]0, m E [n]}
</equation>
<bodyText confidence="0.9943846">
Each dependency tree has y(h, m) = 1 iff it includes
an arc from head h to modifier m. We follow com-
mon practice and use position 0 as the pseudo-root
(*) of the sentence. The full set I1 has cardinality
|I1 |= O(n2).
</bodyText>
<page confidence="0.988339">
500
</page>
<figureCaption confidence="0.9237055">
Figure 3: Parsing rules for first-order dependency pars-
ing. The complete items C are represented by triangles
and the incomplete items I are represented by trapezoids.
Symmetric left-facing versions are also included.
</figureCaption>
<bodyText confidence="0.999982153846154">
The first-order bilexical parsing algorithm of Eis-
ner (2000) can be used to find the best parse tree
and max-marginals. The algorithm defines a dy-
namic program over two types of items: incom-
plete items I(h, m) that denote the span between
a modifier m and its head h, and complete items
C(h, e) that contain a full subtree spanning from the
head h and to the word e on one side. The algo-
rithm builds larger items by applying the composi-
tion rules shown in Figure 3. Rule 3(a) builds an
incomplete item I(h, m) by attaching m as a modi-
fier to h. This rule has the effect that y(h, m) = 1 in
the final parse. Rule 3(b) completes item I(h, m) by
attaching item C(m, e). The existence of I(h, m)
implies that m modifies h, so this rule enforces that
the constituents of m are also constituents of h.
We can find the best derivation for each item
by adapting the standard CKY parsing algorithm
to these rules. Since both rule types contain three
variables that can range over the entire sentence
(h, m, e E [n]0), the bottom-up, inside dynamic pro-
gramming algorithm requires O(n3) time. Further-
more, we can find max-marginals with an additional
top-down outside pass also requiring cubic time. To
speed up search, we need to filter indices from I1
and reduce possible applications of Rule 3(a).
</bodyText>
<subsectionHeader confidence="0.994079">
3.2 Higher-Order Parsing
</subsectionHeader>
<bodyText confidence="0.999847666666667">
Higher-order models generalize the index set by us-
ing siblings s (modifiers that previously attached to
a head word) and grandparents g (head words above
the current head word). For compactness, we use g1
for the head word and sk+1 for the modifier and pa-
rameterize the index set to capture arbitrary higher-
</bodyText>
<figureCaption confidence="0.90588475">
Figure 4: Additional rules for vine parsing. Vine left
(V—) items are pictured as right-facing triangles and vine
right (V—) items are marked trapezoids. Each new item
is anchored at the root and grows to the right.
</figureCaption>
<bodyText confidence="0.716093">
order decisions in both directions:
</bodyText>
<equation confidence="0.765979">
Ik,l = {(g, s) : g E [n]l+1
</equation>
<bodyText confidence="0.99550005">
0 , s E [n]k+1}
where k + 1 is the sibling order, l + 1 is the par-
ent order, and k + l + 1 is the model order. The
canonical second-order model uses I1,0, which has
a cardinality of O(n3). Although there are several
possibilities for higher-order models, we use I1,1 as
our third-order model. Generally, the parsing index
set has cardinality |Ik,l |= O(n2+k+l). Inference
in higher-order models uses variants of the dynamic
program for first-order parsing, and we refer to pre-
vious work for the full set of rules. For second-order
models with index set I1,0, parsing can be done in
O(n3) time (McDonald and Pereira, 2006) and for
third-order models in O(n4) time (Koo and Collins,
2010). Even though second-order parsing has the
same asymptotic time complexity as first-order pars-
ing, inference is significantly slower due to the cost
of scoring the larger index set.
We aim to prune the index set, by mapping each
higher-order index down to a set of small set indices
</bodyText>
<figure confidence="0.969967244444444">
h e h m m e
(a) I �
h m
C
h r
+
r + 1
C
m
C
�
I
+
C
V_
m
I
m
←
V_
+
V_ ← V_
e
V_
I
m
←
V_
+
e m
e
← C
0 e − 1
(f) V_
e − 1
+
C
C
0 e
←
V_
+
e − 1
C
e − 1 e
</figure>
<page confidence="0.730351">
501
</page>
<table confidence="0.632269714285714">
that can be pruned using a coarse pruning model.
For example, to use a first-order model for pruning,
we would map the higher-order index to the individ-
ual indices for its arc, grandparents, and siblings:
* As McGwire neared , fans went wild
pk,l→1(g, s) = {(g1, sj) : j ∈ [k + 1]}
∪ {(gj+1,gj) : j ∈ [l]}
</table>
<bodyText confidence="0.9875076">
The first-order pruning model can then be used
to score these indices, and to produce a filtered in-
dex set F(I1) by removing low-scoring indices (see
Section 4). We retain only the higher-order indices
that are supported by the filtered index set:
</bodyText>
<equation confidence="0.610808">
{(g, s) ∈ Ik,l : pk,l→1(g, s) ⊂ F(I1)}
</equation>
<subsectionHeader confidence="0.977546">
3.3 Vine Parsing
</subsectionHeader>
<bodyText confidence="0.996324">
To further reduce the cost of parsing and produce
faster pruning models, we need a model with less
structure than the first-order model. A natural
choice, following Section 2, is to only consider
“short” arcs:
</bodyText>
<equation confidence="0.973321">
S = {(h,m) ∈ I1 : |h − m |≤ b}
</equation>
<bodyText confidence="0.999499333333333">
where b is a small constant. This constraint reduces
the size of the set to |S |= O(nb).
Clearly, this index set is severely limited; it is nec-
essary to have some long arcs for even short sen-
tences. We therefore augment the index set to in-
clude outer arcs:
</bodyText>
<equation confidence="0.8627495">
I0 = S ∪ {(d, m) : d ∈ {←, →}, m ∈ [n]}
∪ {(h, d) : h ∈ [n]0, d ∈ {←, →}}
</equation>
<bodyText confidence="0.999842714285714">
The first set lets modifiers choose an outer head-
word and the second set lets head words accept outer
modifiers, and both sets distinguish the direction of
the arc. Figure 5 shows a right outer arc. The size of
I0 is linear in the sentence length. To parse the in-
dex set I0, we can modify the parse rules in Figure 3
to enforce additional length constraints (|h − e |≤ b
for I(h, e) and |h − m |≤ b for C(h, m)). This way,
only indices in S are explored. Unfortunately, this is
not sufficient since the constraints also prevent the
algorithm from producing a full derivation, since no
item can expand beyond length b.
Eisner and Smith (2005) therefore introduce vine
parsing, which includes two new items, vine left,
</bodyText>
<figureCaption confidence="0.99535">
Figure 5: An outer arc (1, →) from the word “As” to pos-
sible right modifiers.
</figureCaption>
<bodyText confidence="0.982810342857143">
V←(e), and vine right, V→(e). Unlike the previous
items, these new items are left-anchored at the root
and grow only towards the right. The items V←(e)
and V→(e) encode the fact that a word e has not
taken a close (within b) head word to its left or right.
We incorporate these items by adding the five new
parsing rules shown in Figure 4.
The major addition is Rule 4(e) which converts a
vine left item V←(e) to a vine right item V→(e). This
implies that word e has no close head to either side,
and the parse has outer head arcs, y(←, e) = 1 or
y(→, e) = 1. The other rules are structural and dic-
tate creation and extension of vine items. Rules 4(c)
and 4(d) create vine left items from items that can-
not find a head word to their left. Rules 4(f) and
4(g) extend and finish vine right items. Rules 4(d)
and 4(f) each leave a head word incomplete, so they
may set y(e, ←) = 1 or y(m, →) = 1 respec-
tively. Note that for all the new parse rules, e ∈ [n]0
and m ∈ {e − b ... n}, so parsing time of this so
called vine parsing algorithm is linear in the sen-
tence length O(nb2).
Alone, vine parsing is a poor model of syntax - it
does not even score most dependency pairs. How-
ever, it can act as a pruning model for other parsers.
We prune a first-order model by mapping first-order
indices to indices in I0.
p1→0(h, m) = { {(→, m), (h, →)} if h &lt; m
{(h, m)} if |h − m |≤ b
{(←, m), (h, ←)} if h &gt; m
The remaining first-order indices are then given by:
{(h, m) ∈ I1 : p1→0(h, m) ⊂ F(I0)}
Figure 2 depicts a coarse-to-fine cascade, incor-
porating vine and first-order pruning passes and fin-
ishing with a higher-order parse model.
</bodyText>
<page confidence="0.994064">
502
</page>
<sectionHeader confidence="0.999836" genericHeader="method">
4 Training Methods
</sectionHeader>
<bodyText confidence="0.999947875">
Our coarse-to-fine parsing architecture consists of
multiple pruning passes followed by a final pass
of 1-best parsing. The training objective for the
pruning models comes from the prediction cascade
framework of Weiss and Taskar (2010), which ex-
plicitly trades off pruning efficiency versus accuracy.
The models used in the final pass on the other hand
are trained for 1-best prediction.
</bodyText>
<subsectionHeader confidence="0.984433">
4.1 Max-Marginal Filtering
</subsectionHeader>
<bodyText confidence="0.9954345">
At each pass of coarse-to-fine pruning, we apply an
index filter function F to trim the index set:
</bodyText>
<equation confidence="0.981059">
F(I) = {i ∈ I : f(i) = 1}
</equation>
<bodyText confidence="0.999670333333333">
Several types of filters have been proposed in the
literature, with most work in coarse-to-fine pars-
ing focusing on predicates that threshold the poste-
rior probabilities. In structured prediction cascades,
we use a non-probabilistic filter, based on the max-
marginal value of the index:
</bodyText>
<equation confidence="0.782215">
f(i; Y, w) = 1[ M(i; Y, w) · w &lt; tα(Y, w) ]
</equation>
<bodyText confidence="0.999950923076923">
where tα(Y, w) is a sentence-specific threshold
value. To counteract the fact that the max-marginals
are not normalized, the threshold tα(Y, w) is set as
a convex combination of the 1-best parse score and
the average max-marginal value:
where the model-specific parameter 0 ≤ α ≤ 1 is
the tradeoff between α = 1, pruning all indices i not
in the best parse, and α = 0, pruning all indices with
max-marginal value below the mean.
The threshold function has the important property
that for any parse y, if y · w ≥ tα(Y, w) then y(i) =
1 implies f(i) = 0, i.e. if the parse score is above
the threshold, then none of its indices will be pruned.
</bodyText>
<subsectionHeader confidence="0.91496">
4.2 Filter Loss Training
</subsectionHeader>
<bodyText confidence="0.999746">
The aim of our pruning models is to filter as many
indices as possible without losing the gold parse. In
structured prediction cascades, we incorporate this
pruning goal into our training objective.
Let y be the gold output for a sentence. We define
filter loss to be an indicator of whether any i with
</bodyText>
<equation confidence="0.843855">
y(i) = 1 is filtered:
Δ(y, Y, w) = 1[∃i ∈ y, M(i; Y, w)·w &lt; tα(Y, w)]
</equation>
<bodyText confidence="0.978431">
During training we minimize the expected filter loss
using a standard structured SVM setup (Tsochan-
taridis et al., 2006). First we form a convex, con-
tinuous upper-bound of our loss function:
</bodyText>
<equation confidence="0.9911235">
Δ(y, Y, w) ≤ 1[y · w &lt; tα(Y, w)]
≤ [1 − y · w + tα(Y,w)]+
</equation>
<bodyText confidence="0.999982444444444">
where the first inequality comes from the proper-
ties of max-marginals and the second is the standard
hinge-loss upper-bound on an indicator.
Now assume that we have a corpus of P train-
ing sentences. Let the sequence (y(1), ... , y(P)) be
the gold parses for each sentences and the sequence
(Y(1), ... , Y(P)) be the set of possible output struc-
tures. We can form the regularized risk minimiza-
tion for this upper bound of filter loss:
</bodyText>
<equation confidence="0.899289">
[1 − y(p) · w + tα(Y(p), w)]+
</equation>
<bodyText confidence="0.9539604">
This objective is convex and non-differentiable, due
to the max inside t. We optimize using stochastic
subgradient descent (Shalev-Shwartz et al., 2007).
The stochastic subgradient at example p, H(w, p) is
0 if y(p) − 1 ≥ tα(Y, w) otherwise,
</bodyText>
<equation confidence="0.992459142857143">
2Aw
H(w, p) = P − y(p) + α arg max) y · w
yEY
�
+ (1 − α) 1M(i; Y(p), w)
|I(p)|
iEZ(P)
</equation>
<bodyText confidence="0.956853888888889">
Each step of the algorithm has an update of the form:
wk = wk−1 − nkH(w,p)
where n is an appropriate update rate for subgradi-
ent convergence. If α = 1 the objective is identical
to structured SVM with 0/1 hinge loss. For other
values of α, the subgradient includes a term from
the features of all max-marginal structures at each
index. These feature counts can be computed using
dynamic programming.
</bodyText>
<equation confidence="0.9970682">
tα(Y, w) = α max (y · w)
yEY
+ (1 − α)  ||L M(i; Y, w) · w
iEZ
P
Akwk2 + 1
P
p--1
min
w
</equation>
<page confidence="0.997345">
503
</page>
<table confidence="0.99916325">
Setup Speed First-order Speed Second-order Speed Third-order
PE Oracle UAS PE Oracle UAS PE Oracle UAS
NOPRUNE 1.00 0.00 100 91.4 0.32 0.00 100 92.7 0.01 0.00 100 93.3
LENGTHDICTIONARY 1.94 43.9 99.9 91.5 0.76 43.9 99.9 92.8 0.05 43.9 99.9 93.3
LOCALSHORT 3.08 76.6 99.1 91.4 1.71 76.4 99.1 92.6 0.31 77.5 99.0 93.1
LOCAL 4.59 89.9 98.8 91.5 2.88 83.2 99.5 92.6 1.41 89.5 98.8 93.1
FIRSTONLY 3.10 95.5 95.9 91.5 2.83 92.5 98.4 92.6 1.61 92.2 98.5 93.1
FIRSTANDSECOND - - 1.80 97.6 97.7 93.1
VINEPOSTERIOR 3.92 94.6 96.5 91.5 3.66 93.2 97.7 92.6 1.67 96.5 97.9 93.1
VINECASCADE 5.24 95.0 95.7 91.5 3.99 91.8 98.7 92.6 2.22 97.8 97.4 93.1
k=8 k=16 k=64
ZHANGNIVRE 4.32 - - 92.4 2.39 - - 92.5 0.64 - - 92.7
</table>
<tableCaption confidence="0.990836">
Table 1: Results comparing pruning methods on PTB Section 22. Oracle is the max achievable UAS after pruning.
Pruning efficiency (PE) is the percentage of non-gold first-order dependency arcs pruned. Speed is parsing time relative
to the unpruned first-order model (around 2000 tokens/sec). UAS is the unlabeled attachment score of the final parses.
</tableCaption>
<subsectionHeader confidence="0.995637">
4.3 1-Best Training
</subsectionHeader>
<bodyText confidence="0.999983928571429">
For the final pass, we want to train the model for 1-
best output. Several different learning methods are
available for structured prediction models including
structured perceptron (Collins, 2002), max-margin
models (Taskar et al., 2003), and log-linear mod-
els (Lafferty et al., 2001). In this work, we use the
margin infused relaxed algorithm (MIRA) (Cram-
mer and Singer, 2003; Crammer et al., 2006) with
a hamming-loss margin. MIRA is an online algo-
rithm with similar benefits as structured perceptron
in terms of simplicity and fast training time. In prac-
tice, we found that MIRA with hamming-loss mar-
gin gives a performance improvement over struc-
tured perceptron and structured SVM.
</bodyText>
<sectionHeader confidence="0.966041" genericHeader="method">
5 Parsing Experiments
</sectionHeader>
<bodyText confidence="0.99998115625">
To empirically demonstrate the effectiveness of our
approach, we compare our vine pruning cascade
with a wide range of common pruning methods on
the Penn WSJ Treebank (PTB) (Marcus et al., 1993).
We then also show that vine pruning is effective
across a variety of different languages.
For English, we convert the PTB constituency
trees to dependencies using the Stanford dependency
framework (De Marneffe et al., 2006). We then
train on the standard PTB split with sections 2-21
as training, section 22 as validation, and section 23
as test. Results are similar using the Yamada and
Matsumoto (2003) conversion. We additionally se-
lected six languages from the CoNLL-X shared task
(Buchholz and Marsi, 2006) that cover a number
of different language families: Bulgarian, Chinese,
Japanese, German, Portuguese, and Swedish. We
use the standard CoNLL-X training/test split and
tune parameters with cross-validation.
All experiments use unlabeled dependencies for
training and test. Accuracy is reported as unlabeled
attachment score (UAS), the percentage of tokens
with the correct head word. For English, UAS ig-
nores punctuation tokens and the test set uses pre-
dicted POS tags. For the other languages we fol-
low the CoNLL-X setup and include punctuation in
UAS and use gold POS tags on the set set. Speed-
ups are given in terms of time relative to a highly
optimized C++ implementation. Our unpruned first-
order baseline can process roughly two thousand to-
kens a second and is comparable in speed to the
greedy shift-reduce parser of Nivre et al. (2004).
</bodyText>
<subsectionHeader confidence="0.99808">
5.1 Models
</subsectionHeader>
<bodyText confidence="0.999071416666667">
Our parsers perform multiple passes over each sen-
tence. In each pass we first construct a (pruned) hy-
pergraph (Klein and Manning, 2005) and then per-
form feature computation and inference. We choose
the highest α that produces a pruning error of no
more than 0.2 on the validation set (typically α �
0.6) to filter indices for subsequent rounds (similar
to Weiss and Taskar (2010)). We compare a variety
of pruning models:
LENGTHDICTIONARY a deterministic prun-
ing method that eliminates all arcs longer
than the maximum length observed for each
</bodyText>
<page confidence="0.994132">
504
</page>
<bodyText confidence="0.995662388888889">
head-modifier POS pair.
LOCAL an unstructured arc classifier that chooses
indices from 11 directly without enforcing
parse constraints. Similar to the quadratic-time
filter from Bergsma and Cherry (2010).
LOCALSHORT an unstructured arc classifier that
chooses indices from Z0 directly without en-
forcing parse constraints. Similar to the linear-
time filter from Bergsma and Cherry (2010).
FIRSTONLY a structured first-order model trained
with filter loss for pruning.
FIRSTANDSECOND a structured cascade with
first- and second-order pruning models.
VINECASCADE the full cascade with vine, first-
and second-order pruning models.
VINEPOSTERIOR the vine parsing cascade trained
as a CRF with L-BFGS (Nocedal and Wright,
1999) and using posterior probabilities for fil-
tering instead of max-marginals.
ZHANGNIVRE an unlabeled reimplementation of
the linear-time, k-best, transition-based parser
of Zhang and Nivre (2011). This parser uses
composite features up to third-order with a
greedy decoding algorithm. The reimplemen-
tation is about twice as fast as their reported
speed, but scores slightly lower.
We found LENGTHDICTIONARY pruning to give
significant speed-ups in all settings and therefore al-
ways use it as an initial pass. The maximum number
of passes in a cascade is five: dictionary, vine, first-,
and second-order pruning, and a final third-order 1-
best pass.3 We tune the pruning thresholds for each
round and each cascade separately. This is because
we might be willing to do a more aggressive vine
pruning pass if the final model is a first-order model,
since these two models tend to often agree.
</bodyText>
<subsectionHeader confidence="0.985603">
5.2 Features
</subsectionHeader>
<bodyText confidence="0.9931335">
For the non-pruning models, we use a standard set
of features proposed in the discriminative graph-
based dependency parsing literature (McDonald et
al., 2005; Carreras, 2007; Koo and Collins, 2010).
</bodyText>
<footnote confidence="0.95937575">
3For the first-order parser, we found it beneficial to employ a
reduced feature first-order pruner before the final model, i.e. the
cascade has four rounds: dictionary, vine, first-order pruning,
and first-order 1-best.
</footnote>
<figureCaption confidence="0.98692275">
Figure 6: Mean parsing speed by sentence length for
first-, second-, and third-order parsers as well as differ-
ent pruning methods for first-order parsing. [b] indicates
the empirical complexity obtained from fitting axb.
</figureCaption>
<bodyText confidence="0.999990611111111">
Included are lexical features, part-of-speech fea-
tures, features on in-between tokens, as well as fea-
ture conjunctions, surrounding part-of-speech tags,
and back-off features. In addition, we replicate each
part-of-speech (POS) feature with an additional fea-
ture using coarse POS representations (Petrov et al.,
2012). Our baseline parsing models replicate and,
for some experiments, surpass previous best results.
The first- and second-order pruning models have
the same structure, but for efficiency use only the
basic features from McDonald et al. (2005). As fea-
ture computation is quite costly, future work may
investigate whether this set can be reduced further.
VINEPRUNE and LOCALSHORT use the same fea-
ture sets for short arcs. Outer arcs have features of
the unary head or modifier token, as well as features
for the POS tag bordering the cutoff and the direc-
tion of the arc.
</bodyText>
<subsectionHeader confidence="0.867398">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.9993675">
A comparison between the pruning methods is
shown in Table 1. The table gives relative speed-
ups, compared to the unpruned first-order baseline,
as well as accuracy, pruning efficiency, and ora-
cle scores. Note particularly that the third-order
cascade is twice as fast as an unpruned first-order
model and &gt;200 times faster than the unpruned
third-order baseline. The comparison with poste-
</bodyText>
<figure confidence="0.998195">
first-order second-order
10 20 30 40 50 10 20 30 40 50
sentence length sentence length
third-order pruning methods
10 30 30 40 50 10 20 30 40 50
sentence length sentence length
mean time
No Prune [2.8]
Length [2.0]
Cascade [1.8]
No Prune [2.8]
Length [1.9]
Cascade [1.4]
mean time
mean time
No Prune [3.8]
Length [3.4]
Cascade [1.9]
mean time
Length [1.9]
Local [1.8]
Cascade [1.4]
</figure>
<page confidence="0.987282">
505
</page>
<table confidence="0.999542166666667">
Round First 1-Best Model
Second Third
Vine 37% 27% 16%
First 63% 30% 17%
Second - 43% 18%
Third - - 49%
</table>
<tableCaption confidence="0.982485333333333">
Table 2: Relative speed of pruning models in a multi-pass
cascade. Note that the 1-best models use richer features
than the corresponding pruning models.
</tableCaption>
<bodyText confidence="0.999943947368421">
rior pruning is less pronounced. Filter loss train-
ing is faster than VINEPOSTERIOR for first- and
third-order parsing, but the two models have similar
second-order speeds. It is also noteworthy that ora-
cle scores are consistently high even after multiple
pruning rounds: the oracle score of our third-order
model for example is 97.4%.
Vine pruning is particularly effective. The vine
pass is faster than both LOCAL and FIRSTONLY
and prunes more effectively than LOCALSHORT.
Vine pruning benefits from having a fast, linear-time
model, but still maintaining enough structure for
pruning. While our pruning approach does not pro-
vide any asymptotic guarantees, Figure 6 shows that
in practice our multi-pass parser scales well even
for long sentences: Our first-order cascade scales
almost linearly with the sentence length, while the
third-order cascade scales better than quadratic. Ta-
ble 2 shows that the final pass dominates the compu-
tational cost, while each of the pruning passes takes
up roughly the same amount of time.
Our second- and third-order cascades also signif-
icantly outperform ZHANGNIVRE. The transition-
based model with k = 8 is very efficient and effec-
tive, but increasing the k-best list size scales much
worse than employing multi-pass pruning. We also
note that while direct speed comparison are difficult,
our parser is significantly faster than the published
results for other high accuracy parsers, e.g. Huang
and Sagae (2010) and Koo et al. (2010).
Table 3 shows our results across a subset of the
CoNLL-X datasets, focusing on languages that dif-
fer greatly in structure. The unpruned models per-
form well across datasets, scoring comparably to the
top results from the CoNLL-X competition. We see
speed increases for our cascades with almost no loss
in accuracy across all languages, even for languages
with fairly free word order like German. This is
</bodyText>
<table confidence="0.9997284375">
Setup First-order Second-order Third-order
Speed UAS Speed UAS Speed UAS
B 1.90 90.7 0.67 92.0 0.05 92.1
BG V 6.17 90.5 5.30 91.6 1.99 91.9
DE B 1.40 89.2 0.48 90.3 0.02 90.8
V 4.72 89.0 3.54 90.1 1.44 90.8
JA B 1.77 92.0 0.58 92.1 0.04 92.4
V 8.14 91.7 8.64 92.0 4.30 92.3
PT B 0.89 90.1 0.28 91.2 0.01 91.7
V 3.98 90.0 3.45 90.9 1.45 91.5
SW B 1.37 88.5 0.45 89.7 0.01 90.4
V 6.35 88.3 6.25 89.4 2.66 90.1
ZH B 7.32 89.5 3.30 90.5 0.67 90.8
V 7.45 89.3 6.71 90.3 3.90 90.9
EN B 1.0 91.2 0.33 92.4 0.01 93.0
V 5.24 91.0 3.92 92.2 2.23 92.7
</table>
<tableCaption confidence="0.987152">
Table 3: Speed and accuracy results for the vine prun-
</tableCaption>
<figureCaption confidence="0.503142142857143">
ing cascade across various languages. B is the un-
pruned baseline model, and V is the vine pruning cas-
cade. The first section of the table gives results for
the CoNLL-X test datasets for Bulgarian (BG), German
(DE), Japanese (JA), Portuguese (PT), Swedish (SW),
and Chinese (ZH). The second section gives the result
for the English (EN) test set, PTB Section 23.
</figureCaption>
<bodyText confidence="0.998863">
encouraging and suggests that the outer arcs of the
vine-pruning model are able to cope with languages
that are not as linear as English.
</bodyText>
<sectionHeader confidence="0.999198" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999994111111111">
We presented a multi-pass architecture for depen-
dency parsing that leverages vine parsing and struc-
tured prediction cascades. The resulting 200-fold
speed-up leads to a third-order model that is twice
as fast as an unpruned first-order model for a vari-
ety of languages, and that also compares favorably
to a state-of-the-art transition-based parser. Possible
future work includes experiments using cascades to
explore much higher-order models.
</bodyText>
<sectionHeader confidence="0.99698" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99649625">
We would like to thank the members of the Google
NLP Parsing Team for comments, suggestions, bug-
fixes and help in general: Ryan McDonald, Hao
Zhang, Michael Ringgaard, Terry Koo, Keith Hall,
Kuzman Ganchev and Yoav Goldberg. We would
also like to thank Andre Martins for showing that
MIRA with hamming-loss margin performs better
than other 1-best training algorithms.
</bodyText>
<page confidence="0.997229">
506
</page>
<sectionHeader confidence="0.993882" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999947580952381">
S. Bergsma and C. Cherry. 2010. Fast and accurate arc
filtering for dependency parsing. In Proc. of COLING,
pages 53–61.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In CoNLL.
X. Carreras, M. Collins, and T. Koo. 2008. Tag, dynamic
programming, and the perceptron for efficient, feature-
rich parsing. In Proc. of CoNLL, pages 9–16.
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proc. of CoNLL
Shared Task Session of EMNLP-CoNLL, volume 7,
pages 957–961.
E. Charniak, M. Johnson, M. Elsner, J. Austerweil,
D. Ellis, I. Haxton, C. Hill, R. Shrivaths, J. Moore,
M. Pozar, et al. 2006. Multilevel coarse-to-fine PCFG
parsing. In Proc. of NAACL/HLT, pages 168–175.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proc. of EMNLP, pages 1–8.
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. The Journal
of Machine Learning Research, 3:951–991.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive algo-
rithms. The Journal of Machine Learning Research,
7:551–585.
M.C. De Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proc. of LREC, volume 6,
pages 449–454.
J. Eisner and N.A. Smith. 2005. Parsing with soft and
hard constraints on dependency length. In Proc. of
IWPT, pages 30–41.
J. Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. Advances in Probabilistic
and Other Parsing Technologies, pages 29–62.
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proc. of ACL,
pages 1077–1086.
D. Klein and C.D. Manning. 2005. Parsing and hy-
pergraphs. New developments in parsing technology,
pages 351–372.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proc. of ACL, pages 1–11.
T. Koo, A.M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In Proc. of EMNLP, pages
1288–1298.
M. Kuhlmann, C. G´omez-Rodr´ıguez, and G. Satta. 2011.
Dynamic programming algorithms for transition-
based dependency parsers. In Proc. of ACL/HLT,
pages 673–682.
J. Lafferty, A. McCallum, and F.C.N. Pereira. 2001.
Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Proc. of
ICML, pages 282–289.
L. Lee. 2002. Fast context-free grammar parsing re-
quires fast boolean matrix multiplication. Journal of
the ACM, 49(1):1–15.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational linguistics,
19(2):313–330.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Proc.
of EACL, volume 6, pages 81–88.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
of ACL, pages 91–98.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based
dependency parsing. In Proc. of CoNLL, pages 49–56.
J. Nocedal and S. J. Wright. 1999. Numerical Optimiza-
tion. Springer.
A. Pauls and D. Klein. 2009. Hierarchical search for
parsing. In Proc. of NAACL/HLT, pages 557–565.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In Proc. of NAACL/HLT, pages
404–411.
S. Petrov, D. Das, and R. McDonald. 2012. A universal
part-of-speech tagset. In LREC.
S. Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Bekeley, Berkeley, CA, USA.
B. Roark and K. Hollingshead. 2008. Classifying chart
cells for quadratic complexity context-free inference.
In Proc. of COLING, pages 745–751.
S. Shalev-Shwartz, Y. Singer, and N. Srebro. 2007. Pe-
gasos: Primal estimated sub-gradient solver for svm.
In Proc. of ICML, pages 807–814.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
markov networks. Advances in neural information
processing systems, 16:25–32.
I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Al-
tun. 2006. Large margin methods for structured and
interdependent output variables. Journal of Machine
Learning Research, 6(2):1453.
D. Weiss and B. Taskar. 2010. Structured prediction cas-
cades. In Proc. of AISTATS, volume 1284, pages 916–
923.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
of IWPT, volume 3, pages 195–206.
Y. Zhang and J. Nivre. 2011. Transition-based depen-
dency parsing with rich non-local features. In Proc. of
ACL, pages 188–193.
</reference>
<page confidence="0.99714">
507
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.988271">
<title confidence="0.99986">Vine Pruning for Efficient Multi-Pass Dependency Parsing</title>
<author confidence="0.999945">M Petrov</author>
<affiliation confidence="0.997903">MIT CSAIL Google</affiliation>
<address confidence="0.999926">Cambridge, MA 02139, USA New York, NY 10027, USA</address>
<email confidence="0.999134">srush@csail.mit.eduslav@google.com</email>
<abstract confidence="0.999518388888889">Coarse-to-fine inference has been shown to be a robust approximate method for improving the efficiency of structured prediction models while preserving their accuracy. We propose a multi-pass coarse-to-fine architecture for dependency parsing using linear-time vine pruning and structured prediction cascades. Our first-, second-, and third-order models achieve accuracies comparable to those of their unpruned counterparts, while exploring only a fraction of the search space. We observe speed-ups of up to two orders of magnitude compared to exhaustive search. Our pruned third-order model is twice as fast as an unpruned first-order model and also compares favorably to a state-of-the-art transition-based parser for multiple languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bergsma</author>
<author>C Cherry</author>
</authors>
<title>Fast and accurate arc filtering for dependency parsing.</title>
<date>2010</date>
<booktitle>In Proc. of COLING,</booktitle>
<pages>53--61</pages>
<contexts>
<context position="4987" citStr="Bergsma and Cherry (2010)" startWordPosition="748" endWordPosition="751">oting the relationship to greedy transition-based dependency parsers that are also linear-time (Nivre et al., 2004) or quadratic-time (Yamada and Matsumoto, 2003). It is their success that motivates building explicitly trained, linear-time pruning models. However, while a greedy solution for arc-standard transition-based parsers can be computed in linear-time, Kuhlmann et al. (2011) recently showed that computing exact solutions or (max-)marginals has time complexity O(n4), making these models inappropriate for coarse-to-fine style pruning. As an alternative, Roark and Hollingshead (2008) and Bergsma and Cherry (2010) present approaches where individual classifiers are used to prune chart cells. Such approaches have the drawback that pruning decisions are made locally and therefore can rule out all valid structures, despite explicitly evaluating O(n2) chart cells. In contrast, we make pruning decisions based on global parse max-marginals using a vine pruning pass, which is linear in the sentence length, but nonetheless guarantees to preserve a valid parse structure. 2 Motivation &amp; Overview The goal of this work is fast, high-order, graphbased dependency parsing. Previous work on constituency parsing demons</context>
<context position="25864" citStr="Bergsma and Cherry (2010)" startWordPosition="4517" endWordPosition="4520">ng, 2005) and then perform feature computation and inference. We choose the highest α that produces a pruning error of no more than 0.2 on the validation set (typically α � 0.6) to filter indices for subsequent rounds (similar to Weiss and Taskar (2010)). We compare a variety of pruning models: LENGTHDICTIONARY a deterministic pruning method that eliminates all arcs longer than the maximum length observed for each 504 head-modifier POS pair. LOCAL an unstructured arc classifier that chooses indices from 11 directly without enforcing parse constraints. Similar to the quadratic-time filter from Bergsma and Cherry (2010). LOCALSHORT an unstructured arc classifier that chooses indices from Z0 directly without enforcing parse constraints. Similar to the lineartime filter from Bergsma and Cherry (2010). FIRSTONLY a structured first-order model trained with filter loss for pruning. FIRSTANDSECOND a structured cascade with first- and second-order pruning models. VINECASCADE the full cascade with vine, firstand second-order pruning models. VINEPOSTERIOR the vine parsing cascade trained as a CRF with L-BFGS (Nocedal and Wright, 1999) and using posterior probabilities for filtering instead of max-marginals. ZHANGNIVR</context>
</contexts>
<marker>Bergsma, Cherry, 2010</marker>
<rawString>S. Bergsma and C. Cherry. 2010. Fast and accurate arc filtering for dependency parsing. In Proc. of COLING, pages 53–61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>E Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="24257" citStr="Buchholz and Marsi, 2006" startWordPosition="4258" endWordPosition="4261">pruning cascade with a wide range of common pruning methods on the Penn WSJ Treebank (PTB) (Marcus et al., 1993). We then also show that vine pruning is effective across a variety of different languages. For English, we convert the PTB constituency trees to dependencies using the Stanford dependency framework (De Marneffe et al., 2006). We then train on the standard PTB split with sections 2-21 as training, section 22 as validation, and section 23 as test. Results are similar using the Yamada and Matsumoto (2003) conversion. We additionally selected six languages from the CoNLL-X shared task (Buchholz and Marsi, 2006) that cover a number of different language families: Bulgarian, Chinese, Japanese, German, Portuguese, and Swedish. We use the standard CoNLL-X training/test split and tune parameters with cross-validation. All experiments use unlabeled dependencies for training and test. Accuracy is reported as unlabeled attachment score (UAS), the percentage of tokens with the correct head word. For English, UAS ignores punctuation tokens and the test set uses predicted POS tags. For the other languages we follow the CoNLL-X setup and include punctuation in UAS and use gold POS tags on the set set. Speedups </context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>M Collins</author>
<author>T Koo</author>
</authors>
<title>Tag, dynamic programming, and the perceptron for efficient, featurerich parsing.</title>
<date>2008</date>
<booktitle>In Proc. of CoNLL,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="1799" citStr="Carreras et al., 2008" startWordPosition="260" endWordPosition="263">work, we present a multipass coarse-to-fine architecture for graph-based dependency parsing. We start with a linear-time vine pruning pass and build up to higher-order models, achieving speed-ups of two orders of magnitude while maintaining state-of-the-art accuracies. In constituency parsing, exhaustive inference for all but the simplest grammars tends to be prohibitively slow. Consequently, most high-accuracy constituency parsers routinely employ a coarse grammar to prune dynamic programming chart cells * Research conducted at Google. of the final grammar of interest (Charniak et al., 2006; Carreras et al., 2008; Petrov, 2009). While there are no strong theoretical guarantees for these approaches,1 in practice one can obtain significant speed improvements with minimal loss in accuracy. This benefit comes primarily from reducing the large grammar constant |G |that can dominate the runtime of the cubic-time CKY inference algorithm. Dependency parsers on the other hand do not have a multiplicative grammar factor |G|, and until recently were considered efficient enough for exhaustive inference. However, the increased model complexity of a third-order parser forced Koo and Collins (2010) to prune with a f</context>
</contexts>
<marker>Carreras, Collins, Koo, 2008</marker>
<rawString>X. Carreras, M. Collins, and T. Koo. 2008. Tag, dynamic programming, and the perceptron for efficient, featurerich parsing. In Proc. of CoNLL, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
</authors>
<title>Experiments with a higher-order projective dependency parser.</title>
<date>2007</date>
<booktitle>In Proc. of CoNLL Shared Task Session of EMNLP-CoNLL,</booktitle>
<volume>7</volume>
<pages>957--961</pages>
<contexts>
<context position="27454" citStr="Carreras, 2007" startWordPosition="4762" endWordPosition="4763">nd therefore always use it as an initial pass. The maximum number of passes in a cascade is five: dictionary, vine, first-, and second-order pruning, and a final third-order 1- best pass.3 We tune the pruning thresholds for each round and each cascade separately. This is because we might be willing to do a more aggressive vine pruning pass if the final model is a first-order model, since these two models tend to often agree. 5.2 Features For the non-pruning models, we use a standard set of features proposed in the discriminative graphbased dependency parsing literature (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). 3For the first-order parser, we found it beneficial to employ a reduced feature first-order pruner before the final model, i.e. the cascade has four rounds: dictionary, vine, first-order pruning, and first-order 1-best. Figure 6: Mean parsing speed by sentence length for first-, second-, and third-order parsers as well as different pruning methods for first-order parsing. [b] indicates the empirical complexity obtained from fitting axb. Included are lexical features, part-of-speech features, features on in-between tokens, as well as feature conjunctions, surrounding p</context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>X. Carreras. 2007. Experiments with a higher-order projective dependency parser. In Proc. of CoNLL Shared Task Session of EMNLP-CoNLL, volume 7, pages 957–961.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
<author>M Elsner</author>
<author>J Austerweil</author>
<author>D Ellis</author>
<author>I Haxton</author>
<author>C Hill</author>
<author>R Shrivaths</author>
<author>J Moore</author>
<author>M Pozar</author>
</authors>
<title>Multilevel coarse-to-fine PCFG parsing.</title>
<date>2006</date>
<booktitle>In Proc. of NAACL/HLT,</booktitle>
<pages>168--175</pages>
<contexts>
<context position="1776" citStr="Charniak et al., 2006" startWordPosition="256" endWordPosition="259">omplex models. In this work, we present a multipass coarse-to-fine architecture for graph-based dependency parsing. We start with a linear-time vine pruning pass and build up to higher-order models, achieving speed-ups of two orders of magnitude while maintaining state-of-the-art accuracies. In constituency parsing, exhaustive inference for all but the simplest grammars tends to be prohibitively slow. Consequently, most high-accuracy constituency parsers routinely employ a coarse grammar to prune dynamic programming chart cells * Research conducted at Google. of the final grammar of interest (Charniak et al., 2006; Carreras et al., 2008; Petrov, 2009). While there are no strong theoretical guarantees for these approaches,1 in practice one can obtain significant speed improvements with minimal loss in accuracy. This benefit comes primarily from reducing the large grammar constant |G |that can dominate the runtime of the cubic-time CKY inference algorithm. Dependency parsers on the other hand do not have a multiplicative grammar factor |G|, and until recently were considered efficient enough for exhaustive inference. However, the increased model complexity of a third-order parser forced Koo and Collins (</context>
<context position="5713" citStr="Charniak et al., 2006" startWordPosition="859" endWordPosition="862">drawback that pruning decisions are made locally and therefore can rule out all valid structures, despite explicitly evaluating O(n2) chart cells. In contrast, we make pruning decisions based on global parse max-marginals using a vine pruning pass, which is linear in the sentence length, but nonetheless guarantees to preserve a valid parse structure. 2 Motivation &amp; Overview The goal of this work is fast, high-order, graphbased dependency parsing. Previous work on constituency parsing demonstrates that performing several passes with increasingly more complex models results in faster inference (Charniak et al., 2006; Petrov and Klein, 2007). The same technique applies to dependency parsing with a cascade of models of increasing order; however, this strategy is limited by the speed of the simplest model. The algorithm for first-order dependency parsing (Eisner, 2000) already requires O(n3) time, which Lee 0 1 2 3 4 5 6 7 8 9 dependencylength (b) Figure 1: (a) Heat map indicating how likely a particular head position is for each modifier position. Greener/darker is likelier. (b) Arc length frequency for three common modifier tags. Both charts are computed from all sentences in Section 22 of the PTB. (2002)</context>
</contexts>
<marker>Charniak, Johnson, Elsner, Austerweil, Ellis, Haxton, Hill, Shrivaths, Moore, Pozar, 2006</marker>
<rawString>E. Charniak, M. Johnson, M. Elsner, J. Austerweil, D. Ellis, I. Haxton, C. Hill, R. Shrivaths, J. Moore, M. Pozar, et al. 2006. Multilevel coarse-to-fine PCFG parsing. In Proc. of NAACL/HLT, pages 168–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="23038" citStr="Collins, 2002" startWordPosition="4063" endWordPosition="4064">NIVRE 4.32 - - 92.4 2.39 - - 92.5 0.64 - - 92.7 Table 1: Results comparing pruning methods on PTB Section 22. Oracle is the max achievable UAS after pruning. Pruning efficiency (PE) is the percentage of non-gold first-order dependency arcs pruned. Speed is parsing time relative to the unpruned first-order model (around 2000 tokens/sec). UAS is the unlabeled attachment score of the final parses. 4.3 1-Best Training For the final pass, we want to train the model for 1- best output. Several different learning methods are available for structured prediction models including structured perceptron (Collins, 2002), max-margin models (Taskar et al., 2003), and log-linear models (Lafferty et al., 2001). In this work, we use the margin infused relaxed algorithm (MIRA) (Crammer and Singer, 2003; Crammer et al., 2006) with a hamming-loss margin. MIRA is an online algorithm with similar benefits as structured perceptron in terms of simplicity and fast training time. In practice, we found that MIRA with hamming-loss margin gives a performance improvement over structured perceptron and structured SVM. 5 Parsing Experiments To empirically demonstrate the effectiveness of our approach, we compare our vine prunin</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proc. of EMNLP, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>Y Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--951</pages>
<contexts>
<context position="23218" citStr="Crammer and Singer, 2003" startWordPosition="4090" endWordPosition="4094">ciency (PE) is the percentage of non-gold first-order dependency arcs pruned. Speed is parsing time relative to the unpruned first-order model (around 2000 tokens/sec). UAS is the unlabeled attachment score of the final parses. 4.3 1-Best Training For the final pass, we want to train the model for 1- best output. Several different learning methods are available for structured prediction models including structured perceptron (Collins, 2002), max-margin models (Taskar et al., 2003), and log-linear models (Lafferty et al., 2001). In this work, we use the margin infused relaxed algorithm (MIRA) (Crammer and Singer, 2003; Crammer et al., 2006) with a hamming-loss margin. MIRA is an online algorithm with similar benefits as structured perceptron in terms of simplicity and fast training time. In practice, we found that MIRA with hamming-loss margin gives a performance improvement over structured perceptron and structured SVM. 5 Parsing Experiments To empirically demonstrate the effectiveness of our approach, we compare our vine pruning cascade with a wide range of common pruning methods on the Penn WSJ Treebank (PTB) (Marcus et al., 1993). We then also show that vine pruning is effective across a variety of dif</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>K. Crammer and Y. Singer. 2003. Ultraconservative online algorithms for multiclass problems. The Journal of Machine Learning Research, 3:951–991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>O Dekel</author>
<author>J Keshet</author>
<author>S Shalev-Shwartz</author>
<author>Y Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="23241" citStr="Crammer et al., 2006" startWordPosition="4095" endWordPosition="4098">age of non-gold first-order dependency arcs pruned. Speed is parsing time relative to the unpruned first-order model (around 2000 tokens/sec). UAS is the unlabeled attachment score of the final parses. 4.3 1-Best Training For the final pass, we want to train the model for 1- best output. Several different learning methods are available for structured prediction models including structured perceptron (Collins, 2002), max-margin models (Taskar et al., 2003), and log-linear models (Lafferty et al., 2001). In this work, we use the margin infused relaxed algorithm (MIRA) (Crammer and Singer, 2003; Crammer et al., 2006) with a hamming-loss margin. MIRA is an online algorithm with similar benefits as structured perceptron in terms of simplicity and fast training time. In practice, we found that MIRA with hamming-loss margin gives a performance improvement over structured perceptron and structured SVM. 5 Parsing Experiments To empirically demonstrate the effectiveness of our approach, we compare our vine pruning cascade with a wide range of common pruning methods on the Penn WSJ Treebank (PTB) (Marcus et al., 1993). We then also show that vine pruning is effective across a variety of different languages. For E</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. 2006. Online passive-aggressive algorithms. The Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M C De Marneffe</author>
<author>B MacCartney</author>
<author>C D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proc. of LREC,</booktitle>
<volume>6</volume>
<pages>449--454</pages>
<marker>De Marneffe, MacCartney, Manning, 2006</marker>
<rawString>M.C. De Marneffe, B. MacCartney, and C.D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proc. of LREC, volume 6, pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
<author>N A Smith</author>
</authors>
<title>Parsing with soft and hard constraints on dependency length.</title>
<date>2005</date>
<booktitle>In Proc. of IWPT,</booktitle>
<pages>30--41</pages>
<contexts>
<context position="2973" citStr="Eisner and Smith, 2005" startWordPosition="445" endWordPosition="448">r forced Koo and Collins (2010) to prune with a first-order model in order to make inference practical. While fairly effective, all these approaches are limited by the fact that inference in the coarse model remains cubic in the sentence length. The desire to parse vast amounts of text necessitates more efficient dependency parsing algorithms. We thus propose a multi-pass coarse-to-fine approach where the initial pass is a linear-time sweep, which tries to resolve local ambiguities, but leaves arcs beyond a fixed length b unspecified (Section 3). The dynamic program is a form of vine parsing (Eisner and Smith, 2005), which we use to compute parse max-marginals, rather than for finding the 1- best parse tree. To reduce pruning errors, the parameters of the vine parser (and all subsequent pruning models) are trained using the structured prediction cascades of Weiss and Taskar (2010) to optimize for pruning efficiency, and not for 1-best prediction (Section 4). Despite a limited scope of b = 3, the 1This is in contrast to optimality preserving methods such as A* search, which typically do not provide sufficient speed-ups (Pauls and Klein, 2009). 498 2012 Conference of the North American Chapter of the Assoc</context>
<context position="7538" citStr="Eisner and Smith, 2005" startWordPosition="1161" endWordPosition="1164">ost of the probability mass of modifiers is concentrated among nearby words, corresponding to a diagonal band in the matrix representation. On the right we show the frequency of arc lengths for different modifier partof-speech tags. As one can expect, almost all arcs involving adjectives (ADJ) are very short (length 3 or less), but even arcs involving verbs and nouns are often short. This structure suggests that it may be possible to disambiguate most dependencies by considering only the “banded” portion of the sentence. We exploit this linear structure by employing a variant of vine parsing (Eisner and Smith, 2005).2 Vine parsing is a dependency parsing algorithm that considers only close words as modifiers. Because of this assumption it runs in linear time. Of course, any parse tree with hard limits on dependency lengths will contain major parse errors. We therefore use the 2The term vine parsing is a slight misnomer, since the underlying vine models are as expressive as finite-state automata. However, this allows them to circumvent the cubic-time bound. headindex 1 2 3 4 5 6 7 8 9 modifier index (a) 1 2 3 4 5 6 frequency 0.5 0.4 0.3 0.2 0.1 0.0 ADJ NOUN VERB 499 * As McGwire neared , fans went wild * </context>
<context position="16522" citStr="Eisner and Smith (2005)" startWordPosition="2845" endWordPosition="2848"> modifiers choose an outer headword and the second set lets head words accept outer modifiers, and both sets distinguish the direction of the arc. Figure 5 shows a right outer arc. The size of I0 is linear in the sentence length. To parse the index set I0, we can modify the parse rules in Figure 3 to enforce additional length constraints (|h − e |≤ b for I(h, e) and |h − m |≤ b for C(h, m)). This way, only indices in S are explored. Unfortunately, this is not sufficient since the constraints also prevent the algorithm from producing a full derivation, since no item can expand beyond length b. Eisner and Smith (2005) therefore introduce vine parsing, which includes two new items, vine left, Figure 5: An outer arc (1, →) from the word “As” to possible right modifiers. V←(e), and vine right, V→(e). Unlike the previous items, these new items are left-anchored at the root and grow only towards the right. The items V←(e) and V→(e) encode the fact that a word e has not taken a close (within b) head word to its left or right. We incorporate these items by adding the five new parsing rules shown in Figure 4. The major addition is Rule 4(e) which converts a vine left item V←(e) to a vine right item V→(e). This imp</context>
</contexts>
<marker>Eisner, Smith, 2005</marker>
<rawString>J. Eisner and N.A. Smith. 2005. Parsing with soft and hard constraints on dependency length. In Proc. of IWPT, pages 30–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Bilexical grammars and their cubictime parsing algorithms.</title>
<date>2000</date>
<booktitle>Advances in Probabilistic and Other Parsing Technologies,</booktitle>
<pages>29--62</pages>
<contexts>
<context position="5968" citStr="Eisner, 2000" startWordPosition="902" endWordPosition="904">r in the sentence length, but nonetheless guarantees to preserve a valid parse structure. 2 Motivation &amp; Overview The goal of this work is fast, high-order, graphbased dependency parsing. Previous work on constituency parsing demonstrates that performing several passes with increasingly more complex models results in faster inference (Charniak et al., 2006; Petrov and Klein, 2007). The same technique applies to dependency parsing with a cascade of models of increasing order; however, this strategy is limited by the speed of the simplest model. The algorithm for first-order dependency parsing (Eisner, 2000) already requires O(n3) time, which Lee 0 1 2 3 4 5 6 7 8 9 dependencylength (b) Figure 1: (a) Heat map indicating how likely a particular head position is for each modifier position. Greener/darker is likelier. (b) Arc length frequency for three common modifier tags. Both charts are computed from all sentences in Section 22 of the PTB. (2002) shows is a practical lower bound for parsing of context-free grammars. This bound implies that it is unlikely that there can be an exhaustive parsing algorithm that is asymptotically faster than the standard approach. We thus need to leverage domain know</context>
<context position="11728" citStr="Eisner (2000)" startWordPosition="1913" endWordPosition="1915">n as first-order or arc-factored. For a sentence of length n the index set is: I1 = {(h, m) : h E [n]0, m E [n]} Each dependency tree has y(h, m) = 1 iff it includes an arc from head h to modifier m. We follow common practice and use position 0 as the pseudo-root (*) of the sentence. The full set I1 has cardinality |I1 |= O(n2). 500 Figure 3: Parsing rules for first-order dependency parsing. The complete items C are represented by triangles and the incomplete items I are represented by trapezoids. Symmetric left-facing versions are also included. The first-order bilexical parsing algorithm of Eisner (2000) can be used to find the best parse tree and max-marginals. The algorithm defines a dynamic program over two types of items: incomplete items I(h, m) that denote the span between a modifier m and its head h, and complete items C(h, e) that contain a full subtree spanning from the head h and to the word e on one side. The algorithm builds larger items by applying the composition rules shown in Figure 3. Rule 3(a) builds an incomplete item I(h, m) by attaching m as a modifier to h. This rule has the effect that y(h, m) = 1 in the final parse. Rule 3(b) completes item I(h, m) by attaching item C(</context>
</contexts>
<marker>Eisner, 2000</marker>
<rawString>J. Eisner. 2000. Bilexical grammars and their cubictime parsing algorithms. Advances in Probabilistic and Other Parsing Technologies, pages 29–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>K Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>1077--1086</pages>
<contexts>
<context position="31297" citStr="Huang and Sagae (2010)" startWordPosition="5373" endWordPosition="5376">e the third-order cascade scales better than quadratic. Table 2 shows that the final pass dominates the computational cost, while each of the pruning passes takes up roughly the same amount of time. Our second- and third-order cascades also significantly outperform ZHANGNIVRE. The transitionbased model with k = 8 is very efficient and effective, but increasing the k-best list size scales much worse than employing multi-pass pruning. We also note that while direct speed comparison are difficult, our parser is significantly faster than the published results for other high accuracy parsers, e.g. Huang and Sagae (2010) and Koo et al. (2010). Table 3 shows our results across a subset of the CoNLL-X datasets, focusing on languages that differ greatly in structure. The unpruned models perform well across datasets, scoring comparably to the top results from the CoNLL-X competition. We see speed increases for our cascades with almost no loss in accuracy across all languages, even for languages with fairly free word order like German. This is Setup First-order Second-order Third-order Speed UAS Speed UAS Speed UAS B 1.90 90.7 0.67 92.0 0.05 92.1 BG V 6.17 90.5 5.30 91.6 1.99 91.9 DE B 1.40 89.2 0.48 90.3 0.02 90.</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>L. Huang and K. Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proc. of ACL, pages 1077–1086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Parsing and hypergraphs. New developments in parsing technology,</title>
<date>2005</date>
<pages>351--372</pages>
<contexts>
<context position="25248" citStr="Klein and Manning, 2005" startWordPosition="4420" endWordPosition="4423">rrect head word. For English, UAS ignores punctuation tokens and the test set uses predicted POS tags. For the other languages we follow the CoNLL-X setup and include punctuation in UAS and use gold POS tags on the set set. Speedups are given in terms of time relative to a highly optimized C++ implementation. Our unpruned firstorder baseline can process roughly two thousand tokens a second and is comparable in speed to the greedy shift-reduce parser of Nivre et al. (2004). 5.1 Models Our parsers perform multiple passes over each sentence. In each pass we first construct a (pruned) hypergraph (Klein and Manning, 2005) and then perform feature computation and inference. We choose the highest α that produces a pruning error of no more than 0.2 on the validation set (typically α � 0.6) to filter indices for subsequent rounds (similar to Weiss and Taskar (2010)). We compare a variety of pruning models: LENGTHDICTIONARY a deterministic pruning method that eliminates all arcs longer than the maximum length observed for each 504 head-modifier POS pair. LOCAL an unstructured arc classifier that chooses indices from 11 directly without enforcing parse constraints. Similar to the quadratic-time filter from Bergsma a</context>
</contexts>
<marker>Klein, Manning, 2005</marker>
<rawString>D. Klein and C.D. Manning. 2005. Parsing and hypergraphs. New developments in parsing technology, pages 351–372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>M Collins</author>
</authors>
<title>Efficient third-order dependency parsers.</title>
<date>2010</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>1--11</pages>
<contexts>
<context position="2381" citStr="Koo and Collins (2010)" startWordPosition="348" endWordPosition="351">niak et al., 2006; Carreras et al., 2008; Petrov, 2009). While there are no strong theoretical guarantees for these approaches,1 in practice one can obtain significant speed improvements with minimal loss in accuracy. This benefit comes primarily from reducing the large grammar constant |G |that can dominate the runtime of the cubic-time CKY inference algorithm. Dependency parsers on the other hand do not have a multiplicative grammar factor |G|, and until recently were considered efficient enough for exhaustive inference. However, the increased model complexity of a third-order parser forced Koo and Collins (2010) to prune with a first-order model in order to make inference practical. While fairly effective, all these approaches are limited by the fact that inference in the coarse model remains cubic in the sentence length. The desire to parse vast amounts of text necessitates more efficient dependency parsing algorithms. We thus propose a multi-pass coarse-to-fine approach where the initial pass is a linear-time sweep, which tries to resolve local ambiguities, but leaves arcs beyond a fixed length b unspecified (Section 3). The dynamic program is a form of vine parsing (Eisner and Smith, 2005), which </context>
<context position="14256" citStr="Koo and Collins, 2010" startWordPosition="2368" endWordPosition="2371">arent order, and k + l + 1 is the model order. The canonical second-order model uses I1,0, which has a cardinality of O(n3). Although there are several possibilities for higher-order models, we use I1,1 as our third-order model. Generally, the parsing index set has cardinality |Ik,l |= O(n2+k+l). Inference in higher-order models uses variants of the dynamic program for first-order parsing, and we refer to previous work for the full set of rules. For second-order models with index set I1,0, parsing can be done in O(n3) time (McDonald and Pereira, 2006) and for third-order models in O(n4) time (Koo and Collins, 2010). Even though second-order parsing has the same asymptotic time complexity as first-order parsing, inference is significantly slower due to the cost of scoring the larger index set. We aim to prune the index set, by mapping each higher-order index down to a set of small set indices h e h m m e (a) I � h m C h r + r + 1 C m C � I + C V_ m I m ← V_ + V_ ← V_ e V_ I m ← V_ + e m e ← C 0 e − 1 (f) V_ e − 1 + C C 0 e ← V_ + e − 1 C e − 1 e 501 that can be pruned using a coarse pruning model. For example, to use a first-order model for pruning, we would map the higher-order index to the individual i</context>
<context position="27478" citStr="Koo and Collins, 2010" startWordPosition="4764" endWordPosition="4767">ays use it as an initial pass. The maximum number of passes in a cascade is five: dictionary, vine, first-, and second-order pruning, and a final third-order 1- best pass.3 We tune the pruning thresholds for each round and each cascade separately. This is because we might be willing to do a more aggressive vine pruning pass if the final model is a first-order model, since these two models tend to often agree. 5.2 Features For the non-pruning models, we use a standard set of features proposed in the discriminative graphbased dependency parsing literature (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). 3For the first-order parser, we found it beneficial to employ a reduced feature first-order pruner before the final model, i.e. the cascade has four rounds: dictionary, vine, first-order pruning, and first-order 1-best. Figure 6: Mean parsing speed by sentence length for first-, second-, and third-order parsers as well as different pruning methods for first-order parsing. [b] indicates the empirical complexity obtained from fitting axb. Included are lexical features, part-of-speech features, features on in-between tokens, as well as feature conjunctions, surrounding part-of-speech tags, and </context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>T. Koo and M. Collins. 2010. Efficient third-order dependency parsers. In Proc. of ACL, pages 1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>A M Rush</author>
<author>M Collins</author>
<author>T Jaakkola</author>
<author>D Sontag</author>
</authors>
<title>Dual decomposition for parsing with nonprojective head automata.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>1288--1298</pages>
<contexts>
<context position="31319" citStr="Koo et al. (2010)" startWordPosition="5378" endWordPosition="5381">cales better than quadratic. Table 2 shows that the final pass dominates the computational cost, while each of the pruning passes takes up roughly the same amount of time. Our second- and third-order cascades also significantly outperform ZHANGNIVRE. The transitionbased model with k = 8 is very efficient and effective, but increasing the k-best list size scales much worse than employing multi-pass pruning. We also note that while direct speed comparison are difficult, our parser is significantly faster than the published results for other high accuracy parsers, e.g. Huang and Sagae (2010) and Koo et al. (2010). Table 3 shows our results across a subset of the CoNLL-X datasets, focusing on languages that differ greatly in structure. The unpruned models perform well across datasets, scoring comparably to the top results from the CoNLL-X competition. We see speed increases for our cascades with almost no loss in accuracy across all languages, even for languages with fairly free word order like German. This is Setup First-order Second-order Third-order Speed UAS Speed UAS Speed UAS B 1.90 90.7 0.67 92.0 0.05 92.1 BG V 6.17 90.5 5.30 91.6 1.99 91.9 DE B 1.40 89.2 0.48 90.3 0.02 90.8 V 4.72 89.0 3.54 90.</context>
</contexts>
<marker>Koo, Rush, Collins, Jaakkola, Sontag, 2010</marker>
<rawString>T. Koo, A.M. Rush, M. Collins, T. Jaakkola, and D. Sontag. 2010. Dual decomposition for parsing with nonprojective head automata. In Proc. of EMNLP, pages 1288–1298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kuhlmann</author>
<author>C G´omez-Rodr´ıguez</author>
<author>G Satta</author>
</authors>
<title>Dynamic programming algorithms for transitionbased dependency parsers.</title>
<date>2011</date>
<booktitle>In Proc. of ACL/HLT,</booktitle>
<pages>673--682</pages>
<marker>Kuhlmann, G´omez-Rodr´ıguez, Satta, 2011</marker>
<rawString>M. Kuhlmann, C. G´omez-Rodr´ıguez, and G. Satta. 2011. Dynamic programming algorithms for transitionbased dependency parsers. In Proc. of ACL/HLT, pages 673–682.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proc. of ICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="23126" citStr="Lafferty et al., 2001" startWordPosition="4075" endWordPosition="4078">ng methods on PTB Section 22. Oracle is the max achievable UAS after pruning. Pruning efficiency (PE) is the percentage of non-gold first-order dependency arcs pruned. Speed is parsing time relative to the unpruned first-order model (around 2000 tokens/sec). UAS is the unlabeled attachment score of the final parses. 4.3 1-Best Training For the final pass, we want to train the model for 1- best output. Several different learning methods are available for structured prediction models including structured perceptron (Collins, 2002), max-margin models (Taskar et al., 2003), and log-linear models (Lafferty et al., 2001). In this work, we use the margin infused relaxed algorithm (MIRA) (Crammer and Singer, 2003; Crammer et al., 2006) with a hamming-loss margin. MIRA is an online algorithm with similar benefits as structured perceptron in terms of simplicity and fast training time. In practice, we found that MIRA with hamming-loss margin gives a performance improvement over structured perceptron and structured SVM. 5 Parsing Experiments To empirically demonstrate the effectiveness of our approach, we compare our vine pruning cascade with a wide range of common pruning methods on the Penn WSJ Treebank (PTB) (Ma</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F.C.N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. of ICML, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lee</author>
</authors>
<title>Fast context-free grammar parsing requires fast boolean matrix multiplication.</title>
<date>2002</date>
<journal>Journal of the ACM,</journal>
<volume>49</volume>
<issue>1</issue>
<marker>Lee, 2002</marker>
<rawString>L. Lee. 2002. Fast context-free grammar parsing requires fast boolean matrix multiplication. Journal of the ACM, 49(1):1–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>M A Marcinkiewicz</author>
<author>B Santorini</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<booktitle>Computational linguistics,</booktitle>
<pages>19--2</pages>
<contexts>
<context position="23744" citStr="Marcus et al., 1993" startWordPosition="4176" endWordPosition="4179">1). In this work, we use the margin infused relaxed algorithm (MIRA) (Crammer and Singer, 2003; Crammer et al., 2006) with a hamming-loss margin. MIRA is an online algorithm with similar benefits as structured perceptron in terms of simplicity and fast training time. In practice, we found that MIRA with hamming-loss margin gives a performance improvement over structured perceptron and structured SVM. 5 Parsing Experiments To empirically demonstrate the effectiveness of our approach, we compare our vine pruning cascade with a wide range of common pruning methods on the Penn WSJ Treebank (PTB) (Marcus et al., 1993). We then also show that vine pruning is effective across a variety of different languages. For English, we convert the PTB constituency trees to dependencies using the Stanford dependency framework (De Marneffe et al., 2006). We then train on the standard PTB split with sections 2-21 as training, section 22 as validation, and section 23 as test. Results are similar using the Yamada and Matsumoto (2003) conversion. We additionally selected six languages from the CoNLL-X shared task (Buchholz and Marsi, 2006) that cover a number of different language families: Bulgarian, Chinese, Japanese, Germ</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proc. of EACL,</booktitle>
<volume>6</volume>
<pages>81--88</pages>
<contexts>
<context position="14191" citStr="McDonald and Pereira, 2006" startWordPosition="2357" endWordPosition="2360">n]l+1 0 , s E [n]k+1} where k + 1 is the sibling order, l + 1 is the parent order, and k + l + 1 is the model order. The canonical second-order model uses I1,0, which has a cardinality of O(n3). Although there are several possibilities for higher-order models, we use I1,1 as our third-order model. Generally, the parsing index set has cardinality |Ik,l |= O(n2+k+l). Inference in higher-order models uses variants of the dynamic program for first-order parsing, and we refer to previous work for the full set of rules. For second-order models with index set I1,0, parsing can be done in O(n3) time (McDonald and Pereira, 2006) and for third-order models in O(n4) time (Koo and Collins, 2010). Even though second-order parsing has the same asymptotic time complexity as first-order parsing, inference is significantly slower due to the cost of scoring the larger index set. We aim to prune the index set, by mapping each higher-order index down to a set of small set indices h e h m m e (a) I � h m C h r + r + 1 C m C � I + C V_ m I m ← V_ + V_ ← V_ e V_ I m ← V_ + e m e ← C 0 e − 1 (f) V_ e − 1 + C C 0 e ← V_ + e − 1 C e − 1 e 501 that can be pruned using a coarse pruning model. For example, to use a first-order model for</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>R. McDonald and F. Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proc. of EACL, volume 6, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>91--98</pages>
<contexts>
<context position="27438" citStr="McDonald et al., 2005" startWordPosition="4758" endWordPosition="4761">d-ups in all settings and therefore always use it as an initial pass. The maximum number of passes in a cascade is five: dictionary, vine, first-, and second-order pruning, and a final third-order 1- best pass.3 We tune the pruning thresholds for each round and each cascade separately. This is because we might be willing to do a more aggressive vine pruning pass if the final model is a first-order model, since these two models tend to often agree. 5.2 Features For the non-pruning models, we use a standard set of features proposed in the discriminative graphbased dependency parsing literature (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). 3For the first-order parser, we found it beneficial to employ a reduced feature first-order pruner before the final model, i.e. the cascade has four rounds: dictionary, vine, first-order pruning, and first-order 1-best. Figure 6: Mean parsing speed by sentence length for first-, second-, and third-order parsers as well as different pruning methods for first-order parsing. [b] indicates the empirical complexity obtained from fitting axb. Included are lexical features, part-of-speech features, features on in-between tokens, as well as feature conjunction</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005. Online large-margin training of dependency parsers. In Proc. of ACL, pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
</authors>
<title>Memory-based dependency parsing.</title>
<date>2004</date>
<journal>J. Nocedal</journal>
<booktitle>In Proc. of CoNLL,</booktitle>
<pages>49--56</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="4477" citStr="Nivre et al., 2004" startWordPosition="677" endWordPosition="680">i-th order passes introduce larger scope features, while further constraining the search space. In Section 5 we present experiments in multiple languages. Our coarse-to-fine first-, second-, and third-order parsers preserve the accuracy of the unpruned models, but are faster by up to two orders of magnitude. Our pruned third-order model is faster than an unpruned first-order model, and compares favorably in speed to the state-of-the-art transitionbased parser of Zhang and Nivre (2011). It is worth noting the relationship to greedy transition-based dependency parsers that are also linear-time (Nivre et al., 2004) or quadratic-time (Yamada and Matsumoto, 2003). It is their success that motivates building explicitly trained, linear-time pruning models. However, while a greedy solution for arc-standard transition-based parsers can be computed in linear-time, Kuhlmann et al. (2011) recently showed that computing exact solutions or (max-)marginals has time complexity O(n4), making these models inappropriate for coarse-to-fine style pruning. As an alternative, Roark and Hollingshead (2008) and Bergsma and Cherry (2010) present approaches where individual classifiers are used to prune chart cells. Such appro</context>
<context position="6835" citStr="Nivre et al., 2004" startWordPosition="1045" endWordPosition="1048">ommon modifier tags. Both charts are computed from all sentences in Section 22 of the PTB. (2002) shows is a practical lower bound for parsing of context-free grammars. This bound implies that it is unlikely that there can be an exhaustive parsing algorithm that is asymptotically faster than the standard approach. We thus need to leverage domain knowledge to obtain faster parsing algorithms. It is well-known that natural language is fairly linear, and most headmodifier dependencies tend to be short. This property is exploited by transition-based dependency parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004) and empirically demonstrated in Figure 1. The heat map on the left shows that most of the probability mass of modifiers is concentrated among nearby words, corresponding to a diagonal band in the matrix representation. On the right we show the frequency of arc lengths for different modifier partof-speech tags. As one can expect, almost all arcs involving adjectives (ADJ) are very short (length 3 or less), but even arcs involving verbs and nouns are often short. This structure suggests that it may be possible to disambiguate most dependencies by considering only the “banded” portion of the sen</context>
<context position="25100" citStr="Nivre et al. (2004)" startWordPosition="4395" endWordPosition="4398">se unlabeled dependencies for training and test. Accuracy is reported as unlabeled attachment score (UAS), the percentage of tokens with the correct head word. For English, UAS ignores punctuation tokens and the test set uses predicted POS tags. For the other languages we follow the CoNLL-X setup and include punctuation in UAS and use gold POS tags on the set set. Speedups are given in terms of time relative to a highly optimized C++ implementation. Our unpruned firstorder baseline can process roughly two thousand tokens a second and is comparable in speed to the greedy shift-reduce parser of Nivre et al. (2004). 5.1 Models Our parsers perform multiple passes over each sentence. In each pass we first construct a (pruned) hypergraph (Klein and Manning, 2005) and then perform feature computation and inference. We choose the highest α that produces a pruning error of no more than 0.2 on the validation set (typically α � 0.6) to filter indices for subsequent rounds (similar to Weiss and Taskar (2010)). We compare a variety of pruning models: LENGTHDICTIONARY a deterministic pruning method that eliminates all arcs longer than the maximum length observed for each 504 head-modifier POS pair. LOCAL an unstru</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2004</marker>
<rawString>J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based dependency parsing. In Proc. of CoNLL, pages 49–56. J. Nocedal and S. J. Wright. 1999. Numerical Optimization. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Pauls</author>
<author>D Klein</author>
</authors>
<title>Hierarchical search for parsing.</title>
<date>2009</date>
<booktitle>In Proc. of NAACL/HLT,</booktitle>
<pages>557--565</pages>
<contexts>
<context position="3509" citStr="Pauls and Klein, 2009" startWordPosition="533" endWordPosition="536">ed (Section 3). The dynamic program is a form of vine parsing (Eisner and Smith, 2005), which we use to compute parse max-marginals, rather than for finding the 1- best parse tree. To reduce pruning errors, the parameters of the vine parser (and all subsequent pruning models) are trained using the structured prediction cascades of Weiss and Taskar (2010) to optimize for pruning efficiency, and not for 1-best prediction (Section 4). Despite a limited scope of b = 3, the 1This is in contrast to optimality preserving methods such as A* search, which typically do not provide sufficient speed-ups (Pauls and Klein, 2009). 498 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 498–507, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics vine pruning pass is able to preserve &gt;98% of the correct arcs, while ruling out —86% of all possible arcs. Subsequent i-th order passes introduce larger scope features, while further constraining the search space. In Section 5 we present experiments in multiple languages. Our coarse-to-fine first-, second-, and third-order parsers preserve the accuracy of the unprune</context>
</contexts>
<marker>Pauls, Klein, 2009</marker>
<rawString>A. Pauls and D. Klein. 2009. Hierarchical search for parsing. In Proc. of NAACL/HLT, pages 557–565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>D Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proc. of NAACL/HLT,</booktitle>
<pages>404--411</pages>
<contexts>
<context position="5738" citStr="Petrov and Klein, 2007" startWordPosition="863" endWordPosition="866">ecisions are made locally and therefore can rule out all valid structures, despite explicitly evaluating O(n2) chart cells. In contrast, we make pruning decisions based on global parse max-marginals using a vine pruning pass, which is linear in the sentence length, but nonetheless guarantees to preserve a valid parse structure. 2 Motivation &amp; Overview The goal of this work is fast, high-order, graphbased dependency parsing. Previous work on constituency parsing demonstrates that performing several passes with increasingly more complex models results in faster inference (Charniak et al., 2006; Petrov and Klein, 2007). The same technique applies to dependency parsing with a cascade of models of increasing order; however, this strategy is limited by the speed of the simplest model. The algorithm for first-order dependency parsing (Eisner, 2000) already requires O(n3) time, which Lee 0 1 2 3 4 5 6 7 8 9 dependencylength (b) Figure 1: (a) Heat map indicating how likely a particular head position is for each modifier position. Greener/darker is likelier. (b) Arc length frequency for three common modifier tags. Both charts are computed from all sentences in Section 22 of the PTB. (2002) shows is a practical low</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>S. Petrov and D. Klein. 2007. Improved inference for unlexicalized parsing. In Proc. of NAACL/HLT, pages 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>D Das</author>
<author>R McDonald</author>
</authors>
<title>A universal part-of-speech tagset.</title>
<date>2012</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="28238" citStr="Petrov et al., 2012" startWordPosition="4873" endWordPosition="4876">e has four rounds: dictionary, vine, first-order pruning, and first-order 1-best. Figure 6: Mean parsing speed by sentence length for first-, second-, and third-order parsers as well as different pruning methods for first-order parsing. [b] indicates the empirical complexity obtained from fitting axb. Included are lexical features, part-of-speech features, features on in-between tokens, as well as feature conjunctions, surrounding part-of-speech tags, and back-off features. In addition, we replicate each part-of-speech (POS) feature with an additional feature using coarse POS representations (Petrov et al., 2012). Our baseline parsing models replicate and, for some experiments, surpass previous best results. The first- and second-order pruning models have the same structure, but for efficiency use only the basic features from McDonald et al. (2005). As feature computation is quite costly, future work may investigate whether this set can be reduced further. VINEPRUNE and LOCALSHORT use the same feature sets for short arcs. Outer arcs have features of the unary head or modifier token, as well as features for the POS tag bordering the cutoff and the direction of the arc. 5.3 Results A comparison between </context>
</contexts>
<marker>Petrov, Das, McDonald, 2012</marker>
<rawString>S. Petrov, D. Das, and R. McDonald. 2012. A universal part-of-speech tagset. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
</authors>
<title>Coarse-to-Fine Natural Language Processing.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California at Bekeley,</institution>
<location>Berkeley, CA, USA.</location>
<contexts>
<context position="1814" citStr="Petrov, 2009" startWordPosition="264" endWordPosition="265">ipass coarse-to-fine architecture for graph-based dependency parsing. We start with a linear-time vine pruning pass and build up to higher-order models, achieving speed-ups of two orders of magnitude while maintaining state-of-the-art accuracies. In constituency parsing, exhaustive inference for all but the simplest grammars tends to be prohibitively slow. Consequently, most high-accuracy constituency parsers routinely employ a coarse grammar to prune dynamic programming chart cells * Research conducted at Google. of the final grammar of interest (Charniak et al., 2006; Carreras et al., 2008; Petrov, 2009). While there are no strong theoretical guarantees for these approaches,1 in practice one can obtain significant speed improvements with minimal loss in accuracy. This benefit comes primarily from reducing the large grammar constant |G |that can dominate the runtime of the cubic-time CKY inference algorithm. Dependency parsers on the other hand do not have a multiplicative grammar factor |G|, and until recently were considered efficient enough for exhaustive inference. However, the increased model complexity of a third-order parser forced Koo and Collins (2010) to prune with a first-order mode</context>
</contexts>
<marker>Petrov, 2009</marker>
<rawString>S. Petrov. 2009. Coarse-to-Fine Natural Language Processing. Ph.D. thesis, University of California at Bekeley, Berkeley, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
<author>K Hollingshead</author>
</authors>
<title>Classifying chart cells for quadratic complexity context-free inference.</title>
<date>2008</date>
<booktitle>In Proc. of COLING,</booktitle>
<pages>745--751</pages>
<contexts>
<context position="4957" citStr="Roark and Hollingshead (2008)" startWordPosition="742" endWordPosition="746">ng and Nivre (2011). It is worth noting the relationship to greedy transition-based dependency parsers that are also linear-time (Nivre et al., 2004) or quadratic-time (Yamada and Matsumoto, 2003). It is their success that motivates building explicitly trained, linear-time pruning models. However, while a greedy solution for arc-standard transition-based parsers can be computed in linear-time, Kuhlmann et al. (2011) recently showed that computing exact solutions or (max-)marginals has time complexity O(n4), making these models inappropriate for coarse-to-fine style pruning. As an alternative, Roark and Hollingshead (2008) and Bergsma and Cherry (2010) present approaches where individual classifiers are used to prune chart cells. Such approaches have the drawback that pruning decisions are made locally and therefore can rule out all valid structures, despite explicitly evaluating O(n2) chart cells. In contrast, we make pruning decisions based on global parse max-marginals using a vine pruning pass, which is linear in the sentence length, but nonetheless guarantees to preserve a valid parse structure. 2 Motivation &amp; Overview The goal of this work is fast, high-order, graphbased dependency parsing. Previous work </context>
</contexts>
<marker>Roark, Hollingshead, 2008</marker>
<rawString>B. Roark and K. Hollingshead. 2008. Classifying chart cells for quadratic complexity context-free inference. In Proc. of COLING, pages 745–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shalev-Shwartz</author>
<author>Y Singer</author>
<author>N Srebro</author>
</authors>
<title>Pegasos: Primal estimated sub-gradient solver for svm.</title>
<date>2007</date>
<booktitle>In Proc. of ICML,</booktitle>
<pages>807--814</pages>
<contexts>
<context position="21100" citStr="Shalev-Shwartz et al., 2007" startWordPosition="3703" endWordPosition="3706">+ tα(Y,w)]+ where the first inequality comes from the properties of max-marginals and the second is the standard hinge-loss upper-bound on an indicator. Now assume that we have a corpus of P training sentences. Let the sequence (y(1), ... , y(P)) be the gold parses for each sentences and the sequence (Y(1), ... , Y(P)) be the set of possible output structures. We can form the regularized risk minimization for this upper bound of filter loss: [1 − y(p) · w + tα(Y(p), w)]+ This objective is convex and non-differentiable, due to the max inside t. We optimize using stochastic subgradient descent (Shalev-Shwartz et al., 2007). The stochastic subgradient at example p, H(w, p) is 0 if y(p) − 1 ≥ tα(Y, w) otherwise, 2Aw H(w, p) = P − y(p) + α arg max) y · w yEY � + (1 − α) 1M(i; Y(p), w) |I(p)| iEZ(P) Each step of the algorithm has an update of the form: wk = wk−1 − nkH(w,p) where n is an appropriate update rate for subgradient convergence. If α = 1 the objective is identical to structured SVM with 0/1 hinge loss. For other values of α, the subgradient includes a term from the features of all max-marginal structures at each index. These feature counts can be computed using dynamic programming. tα(Y, w) = α max (y · w</context>
</contexts>
<marker>Shalev-Shwartz, Singer, Srebro, 2007</marker>
<rawString>S. Shalev-Shwartz, Y. Singer, and N. Srebro. 2007. Pegasos: Primal estimated sub-gradient solver for svm. In Proc. of ICML, pages 807–814.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>C Guestrin</author>
<author>D Koller</author>
</authors>
<date>2003</date>
<booktitle>Max-margin markov networks. Advances in neural information processing systems,</booktitle>
<pages>16--25</pages>
<contexts>
<context position="23079" citStr="Taskar et al., 2003" startWordPosition="4067" endWordPosition="4070">.64 - - 92.7 Table 1: Results comparing pruning methods on PTB Section 22. Oracle is the max achievable UAS after pruning. Pruning efficiency (PE) is the percentage of non-gold first-order dependency arcs pruned. Speed is parsing time relative to the unpruned first-order model (around 2000 tokens/sec). UAS is the unlabeled attachment score of the final parses. 4.3 1-Best Training For the final pass, we want to train the model for 1- best output. Several different learning methods are available for structured prediction models including structured perceptron (Collins, 2002), max-margin models (Taskar et al., 2003), and log-linear models (Lafferty et al., 2001). In this work, we use the margin infused relaxed algorithm (MIRA) (Crammer and Singer, 2003; Crammer et al., 2006) with a hamming-loss margin. MIRA is an online algorithm with similar benefits as structured perceptron in terms of simplicity and fast training time. In practice, we found that MIRA with hamming-loss margin gives a performance improvement over structured perceptron and structured SVM. 5 Parsing Experiments To empirically demonstrate the effectiveness of our approach, we compare our vine pruning cascade with a wide range of common pru</context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2003</marker>
<rawString>B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin markov networks. Advances in neural information processing systems, 16:25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Tsochantaridis</author>
<author>T Joachims</author>
<author>T Hofmann</author>
<author>Y Altun</author>
</authors>
<title>Large margin methods for structured and interdependent output variables.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>6</volume>
<issue>2</issue>
<contexts>
<context position="20355" citStr="Tsochantaridis et al., 2006" startWordPosition="3564" endWordPosition="3568"> 1 implies f(i) = 0, i.e. if the parse score is above the threshold, then none of its indices will be pruned. 4.2 Filter Loss Training The aim of our pruning models is to filter as many indices as possible without losing the gold parse. In structured prediction cascades, we incorporate this pruning goal into our training objective. Let y be the gold output for a sentence. We define filter loss to be an indicator of whether any i with y(i) = 1 is filtered: Δ(y, Y, w) = 1[∃i ∈ y, M(i; Y, w)·w &lt; tα(Y, w)] During training we minimize the expected filter loss using a standard structured SVM setup (Tsochantaridis et al., 2006). First we form a convex, continuous upper-bound of our loss function: Δ(y, Y, w) ≤ 1[y · w &lt; tα(Y, w)] ≤ [1 − y · w + tα(Y,w)]+ where the first inequality comes from the properties of max-marginals and the second is the standard hinge-loss upper-bound on an indicator. Now assume that we have a corpus of P training sentences. Let the sequence (y(1), ... , y(P)) be the gold parses for each sentences and the sequence (Y(1), ... , Y(P)) be the set of possible output structures. We can form the regularized risk minimization for this upper bound of filter loss: [1 − y(p) · w + tα(Y(p), w)]+ This ob</context>
</contexts>
<marker>Tsochantaridis, Joachims, Hofmann, Altun, 2006</marker>
<rawString>I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. 2006. Large margin methods for structured and interdependent output variables. Journal of Machine Learning Research, 6(2):1453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Weiss</author>
<author>B Taskar</author>
</authors>
<title>Structured prediction cascades.</title>
<date>2010</date>
<booktitle>In Proc. of AISTATS,</booktitle>
<volume>1284</volume>
<pages>916--923</pages>
<contexts>
<context position="3243" citStr="Weiss and Taskar (2010)" startWordPosition="489" endWordPosition="492"> amounts of text necessitates more efficient dependency parsing algorithms. We thus propose a multi-pass coarse-to-fine approach where the initial pass is a linear-time sweep, which tries to resolve local ambiguities, but leaves arcs beyond a fixed length b unspecified (Section 3). The dynamic program is a form of vine parsing (Eisner and Smith, 2005), which we use to compute parse max-marginals, rather than for finding the 1- best parse tree. To reduce pruning errors, the parameters of the vine parser (and all subsequent pruning models) are trained using the structured prediction cascades of Weiss and Taskar (2010) to optimize for pruning efficiency, and not for 1-best prediction (Section 4). Despite a limited scope of b = 3, the 1This is in contrast to optimality preserving methods such as A* search, which typically do not provide sufficient speed-ups (Pauls and Klein, 2009). 498 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 498–507, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics vine pruning pass is able to preserve &gt;98% of the correct arcs, while ruling out —86% of all possible ar</context>
<context position="18560" citStr="Weiss and Taskar (2010)" startWordPosition="3237" endWordPosition="3240">apping first-order indices to indices in I0. p1→0(h, m) = { {(→, m), (h, →)} if h &lt; m {(h, m)} if |h − m |≤ b {(←, m), (h, ←)} if h &gt; m The remaining first-order indices are then given by: {(h, m) ∈ I1 : p1→0(h, m) ⊂ F(I0)} Figure 2 depicts a coarse-to-fine cascade, incorporating vine and first-order pruning passes and finishing with a higher-order parse model. 502 4 Training Methods Our coarse-to-fine parsing architecture consists of multiple pruning passes followed by a final pass of 1-best parsing. The training objective for the pruning models comes from the prediction cascade framework of Weiss and Taskar (2010), which explicitly trades off pruning efficiency versus accuracy. The models used in the final pass on the other hand are trained for 1-best prediction. 4.1 Max-Marginal Filtering At each pass of coarse-to-fine pruning, we apply an index filter function F to trim the index set: F(I) = {i ∈ I : f(i) = 1} Several types of filters have been proposed in the literature, with most work in coarse-to-fine parsing focusing on predicates that threshold the posterior probabilities. In structured prediction cascades, we use a non-probabilistic filter, based on the maxmarginal value of the index: f(i; Y, w</context>
<context position="25492" citStr="Weiss and Taskar (2010)" startWordPosition="4463" endWordPosition="4466"> terms of time relative to a highly optimized C++ implementation. Our unpruned firstorder baseline can process roughly two thousand tokens a second and is comparable in speed to the greedy shift-reduce parser of Nivre et al. (2004). 5.1 Models Our parsers perform multiple passes over each sentence. In each pass we first construct a (pruned) hypergraph (Klein and Manning, 2005) and then perform feature computation and inference. We choose the highest α that produces a pruning error of no more than 0.2 on the validation set (typically α � 0.6) to filter indices for subsequent rounds (similar to Weiss and Taskar (2010)). We compare a variety of pruning models: LENGTHDICTIONARY a deterministic pruning method that eliminates all arcs longer than the maximum length observed for each 504 head-modifier POS pair. LOCAL an unstructured arc classifier that chooses indices from 11 directly without enforcing parse constraints. Similar to the quadratic-time filter from Bergsma and Cherry (2010). LOCALSHORT an unstructured arc classifier that chooses indices from Z0 directly without enforcing parse constraints. Similar to the lineartime filter from Bergsma and Cherry (2010). FIRSTONLY a structured first-order model tra</context>
</contexts>
<marker>Weiss, Taskar, 2010</marker>
<rawString>D. Weiss and B. Taskar. 2010. Structured prediction cascades. In Proc. of AISTATS, volume 1284, pages 916– 923.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proc. of IWPT,</booktitle>
<volume>3</volume>
<pages>195--206</pages>
<contexts>
<context position="4524" citStr="Yamada and Matsumoto, 2003" startWordPosition="683" endWordPosition="686">e features, while further constraining the search space. In Section 5 we present experiments in multiple languages. Our coarse-to-fine first-, second-, and third-order parsers preserve the accuracy of the unpruned models, but are faster by up to two orders of magnitude. Our pruned third-order model is faster than an unpruned first-order model, and compares favorably in speed to the state-of-the-art transitionbased parser of Zhang and Nivre (2011). It is worth noting the relationship to greedy transition-based dependency parsers that are also linear-time (Nivre et al., 2004) or quadratic-time (Yamada and Matsumoto, 2003). It is their success that motivates building explicitly trained, linear-time pruning models. However, while a greedy solution for arc-standard transition-based parsers can be computed in linear-time, Kuhlmann et al. (2011) recently showed that computing exact solutions or (max-)marginals has time complexity O(n4), making these models inappropriate for coarse-to-fine style pruning. As an alternative, Roark and Hollingshead (2008) and Bergsma and Cherry (2010) present approaches where individual classifiers are used to prune chart cells. Such approaches have the drawback that pruning decisions </context>
<context position="6814" citStr="Yamada and Matsumoto, 2003" startWordPosition="1041" endWordPosition="1044">length frequency for three common modifier tags. Both charts are computed from all sentences in Section 22 of the PTB. (2002) shows is a practical lower bound for parsing of context-free grammars. This bound implies that it is unlikely that there can be an exhaustive parsing algorithm that is asymptotically faster than the standard approach. We thus need to leverage domain knowledge to obtain faster parsing algorithms. It is well-known that natural language is fairly linear, and most headmodifier dependencies tend to be short. This property is exploited by transition-based dependency parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004) and empirically demonstrated in Figure 1. The heat map on the left shows that most of the probability mass of modifiers is concentrated among nearby words, corresponding to a diagonal band in the matrix representation. On the right we show the frequency of arc lengths for different modifier partof-speech tags. As one can expect, almost all arcs involving adjectives (ADJ) are very short (length 3 or less), but even arcs involving verbs and nouns are often short. This structure suggests that it may be possible to disambiguate most dependencies by considering only the “bande</context>
<context position="24150" citStr="Yamada and Matsumoto (2003)" startWordPosition="4242" endWordPosition="4245">SVM. 5 Parsing Experiments To empirically demonstrate the effectiveness of our approach, we compare our vine pruning cascade with a wide range of common pruning methods on the Penn WSJ Treebank (PTB) (Marcus et al., 1993). We then also show that vine pruning is effective across a variety of different languages. For English, we convert the PTB constituency trees to dependencies using the Stanford dependency framework (De Marneffe et al., 2006). We then train on the standard PTB split with sections 2-21 as training, section 22 as validation, and section 23 as test. Results are similar using the Yamada and Matsumoto (2003) conversion. We additionally selected six languages from the CoNLL-X shared task (Buchholz and Marsi, 2006) that cover a number of different language families: Bulgarian, Chinese, Japanese, German, Portuguese, and Swedish. We use the standard CoNLL-X training/test split and tune parameters with cross-validation. All experiments use unlabeled dependencies for training and test. Accuracy is reported as unlabeled attachment score (UAS), the percentage of tokens with the correct head word. For English, UAS ignores punctuation tokens and the test set uses predicted POS tags. For the other languages</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>H. Yamada and Y. Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proc. of IWPT, volume 3, pages 195–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>J Nivre</author>
</authors>
<title>Transition-based dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>188--193</pages>
<contexts>
<context position="4347" citStr="Zhang and Nivre (2011)" startWordPosition="658" endWordPosition="661">l Linguistics vine pruning pass is able to preserve &gt;98% of the correct arcs, while ruling out —86% of all possible arcs. Subsequent i-th order passes introduce larger scope features, while further constraining the search space. In Section 5 we present experiments in multiple languages. Our coarse-to-fine first-, second-, and third-order parsers preserve the accuracy of the unpruned models, but are faster by up to two orders of magnitude. Our pruned third-order model is faster than an unpruned first-order model, and compares favorably in speed to the state-of-the-art transitionbased parser of Zhang and Nivre (2011). It is worth noting the relationship to greedy transition-based dependency parsers that are also linear-time (Nivre et al., 2004) or quadratic-time (Yamada and Matsumoto, 2003). It is their success that motivates building explicitly trained, linear-time pruning models. However, while a greedy solution for arc-standard transition-based parsers can be computed in linear-time, Kuhlmann et al. (2011) recently showed that computing exact solutions or (max-)marginals has time complexity O(n4), making these models inappropriate for coarse-to-fine style pruning. As an alternative, Roark and Hollingsh</context>
<context position="26573" citStr="Zhang and Nivre (2011)" startWordPosition="4616" endWordPosition="4619">ut enforcing parse constraints. Similar to the lineartime filter from Bergsma and Cherry (2010). FIRSTONLY a structured first-order model trained with filter loss for pruning. FIRSTANDSECOND a structured cascade with first- and second-order pruning models. VINECASCADE the full cascade with vine, firstand second-order pruning models. VINEPOSTERIOR the vine parsing cascade trained as a CRF with L-BFGS (Nocedal and Wright, 1999) and using posterior probabilities for filtering instead of max-marginals. ZHANGNIVRE an unlabeled reimplementation of the linear-time, k-best, transition-based parser of Zhang and Nivre (2011). This parser uses composite features up to third-order with a greedy decoding algorithm. The reimplementation is about twice as fast as their reported speed, but scores slightly lower. We found LENGTHDICTIONARY pruning to give significant speed-ups in all settings and therefore always use it as an initial pass. The maximum number of passes in a cascade is five: dictionary, vine, first-, and second-order pruning, and a final third-order 1- best pass.3 We tune the pruning thresholds for each round and each cascade separately. This is because we might be willing to do a more aggressive vine prun</context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Y. Zhang and J. Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proc. of ACL, pages 188–193.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>