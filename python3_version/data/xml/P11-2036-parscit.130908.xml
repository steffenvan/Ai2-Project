<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000096">
<title confidence="0.991152">
Insertion Operator for Bayesian Tree Substitution Grammars
</title>
<author confidence="0.854227">
Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata
</author>
<affiliation confidence="0.544937">
NTT Communication Science Laboratories, NTT Corp.
</affiliation>
<address confidence="0.564035">
2-4 Hikaridai Seika-cho Soraku-gun Kyoto 619-0237 Japan
</address>
<email confidence="0.998982">
{shindo.hiroyuki,fujino.akinori,nagata.masaaki}@lab.ntt.co.jp
</email>
<sectionHeader confidence="0.993901" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999364727272727">
We propose a model that incorporates an in-
sertion operator in Bayesian tree substitution
grammars (BTSG). Tree insertion is helpful
for modeling syntax patterns accurately with
fewer grammar rules than BTSG. The exper-
imental parsing results show that our model
outperforms a standard PCFG and BTSG for
a small dataset. For a large dataset, our model
obtains comparable results to BTSG, making
the number of grammar rules much smaller
than with BTSG.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99990946875">
Tree substitution grammar (TSG) is a promising for-
malism for modeling language data. TSG general-
izes context free grammars (CFG) by allowing non-
terminal nodes to be replaced with subtrees of arbi-
trary size.
A natural extension of TSG involves adding an
insertion operator for combining subtrees as in
tree adjoining grammars (TAG) (Joshi, 1985) or
tree insertion grammars (TIG) (Schabes and Wa-
ters, 1995). An insertion operator is helpful for ex-
pressing various syntax patterns with fewer gram-
mar rules, thus we expect that adding an insertion
operator will improve parsing accuracy and realize a
compact grammar size.
One of the challenges of adding an insertion op-
erator is that the computational cost of grammar in-
duction is high since tree insertion significantly in-
creases the number of possible subtrees. Previous
work on TAG and TIG induction (Xia, 1999; Chi-
ang, 2003; Chen et al., 2006) has addressed the prob-
lem using language-specific heuristics and a maxi-
mum likelihood estimator, which leads to overfitting
the training data (Post and Gildea, 2009).
Instead, we incorporate an insertion operator in a
Bayesian TSG (BTSG) model (Cohn et al., 2011)
that learns grammar rules automatically without
heuristics. Our model uses a restricted variant of
subtrees for insertion to model the probability dis-
tribution simply and train the model efficiently. We
also present an inference technique for handling a
tree insertion that makes use of dynamic program-
ming.
</bodyText>
<sectionHeader confidence="0.905461" genericHeader="method">
2 Overview of BTSG Model
</sectionHeader>
<bodyText confidence="0.9999025">
We briefly review the BTSG model described in
(Cohn et al., 2011). TSG uses a substitution operator
(shown in Fig. 1a) to combine subtrees. Subtrees for
substitution are referred to as initial trees, and leaf
nonterminals in initial trees are referred to as fron-
tier nodes. Their task is the unsupervised induction
of TSG derivations from parse trees. A derivation
is information about how subtrees are combined to
form parse trees.
The probability distribution over initial trees is de-
fined by using a Pitman-Yor process prior (Pitman
and Yor, 1997), that is,
</bodyText>
<equation confidence="0.928097">
e JX — GX
GX JdX, BX — PYP (dX, BX, Po (· JX )) ,
</equation>
<bodyText confidence="0.995839">
where X is a nonterminal symbol, e is an initial tree
rooted with X, and Po (· JX ) is a base distribution
over the infinite space of initial trees rooted with X.
dX and BX are hyperparameters that are used to con-
trol the model’s behavior. Integrating out all possi-
ble values of GX, the resulting distribution is
</bodyText>
<page confidence="0.983666">
206
</page>
<note confidence="0.637531">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 206–211,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<equation confidence="0.976025">
(a)
p (ei le−i, X, dX, OX ) = αe;,X + NXP0 (ei, JX), (1)
−i
nei X−dX·tei,X
θX+dX·t·,X . e−i = e1, ... , ei−1 are previously gen-
θX+n−i
·,X
</equation>
<bodyText confidence="0.9701755">
erated initial trees, and n−i
ei,X is the number of times
ei has been used in e−i. tei,X is the number of ta-
bles labeled with ei. n−i
</bodyText>
<equation confidence="0.915634">
·,X = Ee n−i
</equation>
<bodyText confidence="0.955027545454546">
e,X and t·,X =
Ee te,X are the total counts of initial trees and ta-
bles, respectively. The PYP prior produces “rich get
richer” statistics: a few initial trees are often used
for derivation while many are rarely used, and this is
shown empirically to be well-suited for natural lan-
guage (Teh, 2006b; Johnson and Goldwater, 2009).
The base probability of an initial tree, P0 (e JX),
is given as follows.
where αei,X =
and Q
</bodyText>
<equation confidence="0.9814772">
θX+n −i X =
·,X
(b)
11
P0 (e �X ) =
r∈CFG(e)
11 x
11 PMLE (r) x sA
A∈LEAF(e)
(1 − sB) , (2)
</equation>
<figureCaption confidence="0.9726135">
Figure 1: Example of (a) substitution and (b) inser-
tion (dotted line).
</figureCaption>
<equation confidence="0.487036">
B∈INTER(e)
</equation>
<bodyText confidence="0.9999504">
where CFG (e) is a set of decomposed CFG produc-
tions of e, PMLE (r) is a maximum likelihood esti-
mate (MLE) of r. LEAF (e) and INTER (e) are sets
of leaf and internal symbols of e, respectively. sX is
a stopping probability defined for each X.
</bodyText>
<sectionHeader confidence="0.979584" genericHeader="method">
3 Insertion Operator for BTSG
</sectionHeader>
<subsectionHeader confidence="0.965675">
3.1 Tree Insertion Model
</subsectionHeader>
<bodyText confidence="0.999949097560975">
We propose a model that incorporates an insertion
operator in BTSG. Figure 1b shows an example of
an insertion operator. To distinguish them from ini-
tial trees, subtrees for insertion are referred to as
auxiliary trees. An auxiliary tree includes a special
nonterminal leaf node labeled with the same sym-
bol as the root node. This leaf node is referred to
as afoot node (marked with the subscript “*”). The
definitions of substitution and insertion operators are
identical with those of TIG and TAG.
Since it is computationally expensive to allow any
auxiliary trees, we tackle the problem by introduc-
ing simple auxiliary trees, i.e., auxiliary trees whose
root node must generate a foot node as an immediate
child. For example, “(N (JJ pretty) N*)” is a simple
auxiliary tree, but “(S (NP ) (VP (V think) S*))” is
not. Note that we place no restriction on the initial
trees.
Our restricted formalism is a strict subset of TIG.
We briefly refer to some differences between TAG,
TIG and our insertion model. TAG generates tree
adjoining languages, a strict superset of context-
free languages, and the computational complexity
of parsing is O (n6). TIG is a similar formalism
to TAG, but it does not allow wrapping adjunction
in TAG. Therefore, TIG generates context-free lan-
guages and the parsing complexity is O (n3), which
is a strict subset of TAG. On the other hand, our
model prohibits neither wrapping adjunction in TAG
nor simultaneous adjunction in TIG, and allows only
simple auxiliary trees. The expressive power and
computational complexity of our formalism is iden-
tical to TIG, however, our model allows us to de-
fine the probability distribution over auxiliary trees
as having the same form as BTSG model. This en-
sures that we can make use of a dynamic program-
ming technique for training our model, which we de-
scribe the detail in the next subsection.
We define a probability distribution over simple
auxiliary trees as having the same form as eq. 1, that
is,
</bodyText>
<page confidence="0.964919">
207
</page>
<bodyText confidence="0.921087125">
p (ei |e−i, X, d0X, θ0X) = α0ei,X + β0XP0 0 (ei, |X) , (3)
where d0X and θ0 X are hyperparameters of the in-
sertion model, and the definition of (α0ei X, β0X) is
the same as that of (αei,X, βX) in eq. 1.
However, we need modify the base distribution
over simple auxiliary trees, P00 (e |X), as follows,
so that all probabilities of the simple auxiliary trees
sum to one.
</bodyText>
<equation confidence="0.981388666666667">
P0 0 (e |X) = P0 MLE (TOP (e)) X 11 PMLE (r)
rEINTER_CFG(e)
(1 − sB) , (4)
</equation>
<bodyText confidence="0.999805166666667">
where TOP (e) is the CFG production that
starts with the root node of e. For example,
TOP (N (JJ pretty) (N*)) returns “N → JJ N*”.
INTER_CFG (e) is a set of CFG productions of e
excluding TOP (e). P0MLE (r0) is a modified MLE
for simple auxiliary trees, which is given by
</bodyText>
<equation confidence="0.739187">
C(X→X* Y )+C(X→Y X*) r0includes a foot node
</equation>
<bodyText confidence="0.9816085625">
{ 0 else
where C (r0) is the frequency of r0 in parse trees.
It is ensured that P00 (e |X ) generates a foot node as
an immediate child.
We define the probability distribution over both
initial trees and simple auxiliary trees with a PYP
prior. The base distribution over initial trees is de-
fined as P0 (e |X ), and the base distribution over
simple auxiliary trees is defined as P00 (e |X). An
initial tree ei replaces a frontier node with prob-
ability p (ei |e−i, X, dX, θX ). On the other hand,
a simple auxiliary tree e0i inserts an internal node
with probability aX ×p0 (ez I e&apos; i, X, d0X, θ0 ), where X
aX is an insertion probability defined for each X.
The stopping probabilities are common to both ini-
tial and auxiliary trees.
</bodyText>
<subsectionHeader confidence="0.992269">
3.2 Grammar Decomposition
</subsectionHeader>
<bodyText confidence="0.9909128">
We develop a grammar decomposition technique,
which is an extension of work (Cohn and Blunsom,
2010) on BTSG model, to deal with an insertion
operator. The motivation behind grammar decom-
position is that it is hard to consider all possible
</bodyText>
<figureCaption confidence="0.99055">
Figure 2: Derivation of Fig. 1b transformed by
grammar decomposition.
</figureCaption>
<table confidence="0.712405222222222">
CFG rule probability
NP(NP (DT the) (N girl)) →DT(DT the)Nins (N girl) (1 − aDT) × aN
DT(DT the) →the 1
Nins (N girl) →Nins (N girl) 0
(N (JJ pretty) N*) α(N (JJ pretty) N*),N
Nins (N girl)
(N (JJ pretty) N*) →JJ(JJ pretty)N(N girl) (1 − aJJ) × 1
JJ(JJ pretty) →pretty 1
N(N girl) →girl 1
</table>
<tableCaption confidence="0.962857">
Table 1: The rules and probabilities of grammar de-
composition for Fig. 2.
</tableCaption>
<bodyText confidence="0.999578315789474">
derivations explicitly since the base distribution as-
signs non-zero probability to an infinite number of
initial and auxiliary trees. Alternatively, we trans-
form a derivation into CFG productions and assign
the probability for each CFG production so that its
assignment is consistent with the probability distri-
butions. We can efficiently calculate an inside prob-
ability (described in the next subsection) by employ-
ing grammar decomposition.
Here we provide an example of the derivation
shown in Fig. 1b. First, we can transform the deriva-
tion in Fig. 1b to another form as shown in Fig. 2.
In Fig. 2, all the derivation information is embed-
ded in each symbol. That is, NP(NP (DT the) (N girl)) is
a root symbol of the initial tree “(NP (DT the) (N
girl))”, which generates two child nodes: DT(DT the)
and N(N girl). DT(DT the) generates the terminal node
“the”. On the other hand, Nins (N girl) denotes that
N(N girl) is inserted by some auxiliary tree, and
</bodyText>
<subsectionHeader confidence="0.603374">
Nins (N girl)
</subsectionHeader>
<bodyText confidence="0.94123025">
(N(JJ pretty) N*) denotes that the inserted simple aux-
iliary tree is “(N (JJ pretty) (N*))”. The inserted
auxiliary tree, “(N (JJ pretty) (N*))”, must generate
a foot node: “(N girl)” as an immediate child.
</bodyText>
<equation confidence="0.9946285">
sA X 1l 1
BEINTER (e)
11 X
AELEAF(e)
</equation>
<page confidence="0.988928">
208
</page>
<bodyText confidence="0.971708642857143">
Second, we decompose the transformed tree into
CFG productions and then assign the probability for
each CFG production as shown in Table 1, where
aDT, aN and aJJ are insertion probabilities for non-
terminal DT, N and JJ, respectively. Note that the
probability of a derivation according to Table 1 is
the same as the probability of a derivation obtained
from the distribution over the initial and auxiliary
trees (i.e. eq. 1 and eq. 3).
In Table 1, we assume that the auxiliary tree
“(N (JJ pretty) (N*))” is sampled from the first
term of eq. 3. When it is sampled from the sec-
ond term, we alternatively assign the probability
O(N (JJ pretty) N*), N.
</bodyText>
<subsectionHeader confidence="0.994404">
3.3 Training
</subsectionHeader>
<bodyText confidence="0.9999545">
We use a blocked Metropolis-Hastings (MH) algo-
rithm (Cohn and Blunsom, 2010) to train our model.
The MH algorithm learns BTSG model parameters
efficiently, and it can be applied to our insertion
model. The MH algorithm consists of the following
three steps. For each sentence,
</bodyText>
<listItem confidence="0.9969605">
1. Calculate the inside probability (Lari and
Young, 1991) in a bottom-up manner using the
grammar decomposition.
2. Sample a derivation tree in a top-down manner.
3. Accept or reject the derivation sample by using
the MH test.
</listItem>
<bodyText confidence="0.99892275">
The MH algorithm is described in detail in (Cohn
and Blunsom, 2010). The hyperparameters of our
model are updated with the auxiliary variable tech-
nique (Teh, 2006a).
</bodyText>
<sectionHeader confidence="0.999643" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999906125">
We ran experiments on the British National Cor-
pus (BNC) Treebank 3 and the WSJ English Penn
Treebank. We did not use a development set since
our model automatically updates the hyperparame-
ters for every iteration. The treebank data was bina-
rized using the CENTER-HEAD method (Matsuzaki
et al., 2005). We replaced lexical words with counts
&lt; 1 in the training set with one of three unknown
</bodyText>
<footnote confidence="0.997765666666667">
1Results from (Cohn and Blunsom, 2010).
2Results for length &lt; 40.
3http://nclt.computing.dcu.ie/~jfoster/resources/
</footnote>
<table confidence="0.990692888888889">
corpus method F1
CFG 54.08
BNC BTSG 67.73
BTSG + insertion 69.06
CFG 64.99
BTSG 77.19
WSJ BTSG + insertion 78.54
(Petrov et al., 2006) 77.931
(Cohn and Blunsom, 2010) 78.40
</table>
<tableCaption confidence="0.855672">
Table 2: Small dataset experiments
</tableCaption>
<table confidence="0.999655666666666">
# rules (# aux. trees) F1
CFG 35374 (-) 71.0
BTSG 80026 (0) 85.0
BTSG + insertion 65099 (25) 85.3
(Post and Gildea, 2009) - 82.62
(Cohn and Blunsom, 2010) - 85.3
</table>
<tableCaption confidence="0.999728">
Table 3: Full Penn Treebank dataset experiments
</tableCaption>
<bodyText confidence="0.999888461538462">
words using lexical features. We trained our model
using a training set, and then sampled 10k deriva-
tions for each sentence in a test set. Parsing results
were obtained with the MER algorithm (Cohn et al.,
2011) using the 10k derivation samples. We show
the bracketing F1 score of predicted parse trees eval-
uated by EVALB4, averaged over three independent
runs.
In small dataset experiments, we used BNC (1k
sentences, 90% for training and 10% for testing) and
WSJ (section 2 for training and section 22 for test-
ing). This was a small-scale experiment, but large
enough to be relevant for low-resource languages.
We trained the model with an MH sampler for 1k
iterations. Table 2 shows the parsing results for
the test set. We compared our model with standard
PCFG and BTSG models implemented by us.
Our insertion model successfully outperformed
CFG and BTSG. This suggests that adding an inser-
tion operator is helpful for modeling syntax trees ac-
curately. The BTSG model described in (Cohn and
Blunsom, 2010) is similar to ours. They reported
an F1 score of 78.40 (the score of our BTSG model
was 77.19). We speculate that the performance gap
is due to data preprocessing such as the treatment of
rare words.
</bodyText>
<footnote confidence="0.980028">
4http://nlp.cs.nyu.edu/evalb/
</footnote>
<page confidence="0.992208">
209
</page>
<table confidence="0.61827098">
(NP (NP ) (: –)) number of grammars much smaller than the BTSG
(NP (NP ) (ADVP (RB respectively))) model. We will extend our model to original TAG
(PP (PP) (, ,)) and evaluate its impact on statistical parsing perfor-
(VP (VP ) (RB then)) mance.
( QP (QP ) (IN of)) References
(�SBAR (SBAR) (RB not)) J. Chen, S. Bangalore, and K. Vijay-Shanker. 2006.
( S ( Automated extraction of Tree-Adjoining Grammars
S ) (: ;)) from treebanks. Natural Language Engineering,
12(03):251–299.
D. Chiang, 2003. Statistical Parsing with an Automati-
cally Extracted Tree Adjoining Grammar, chapter 16,
pages 299–316. CSLI Publications.
T. Cohn and P. Blunsom. 2010. Blocked inference in
Bayesian tree substitution grammars. In Proceedings
of the ACL 2010 Conference Short Papers, pages 225–
230, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
T. Cohn, P. Blunsom, and S. Goldwater. 2011. Induc-
ing tree-substitution grammars. Journal of Machine
Learning Research. To Appear.
M. Johnson and S. Goldwater. 2009. Improving non-
parameteric Bayesian inference: experiments on unsu-
pervised word segmentation with adaptor grammars.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics (HLT-NAACL), pages 317–325, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
A.K. Joshi. 1985. Tree adjoining grammars: How much
context-sensitivity is required to provide reasonable
structural descriptions? Natural Language Parsing:
Psychological, Computational, and Theoretical Per-
spectives, pages 206–250.
K. Lari and S.J. Young. 1991. Applications of stochastic
context-free grammars using the inside-outside algo-
rithm. Computer Speech &amp; Language, 5(3):237–257.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In Proceedings of
the 43rd Annual Meeting on Association for Compu-
tational Linguistics (ACL), pages 75–82. Association
for Computational Linguistics.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th Annual Meeting of the Association for Computa-
tional Linguistics (ICCL-ACL), pages 433–440, Syd-
ney, Australia, July. Association for Computational
Linguistics.
</table>
<tableCaption confidence="0.789766">
Table 4: Examples of lexicalized auxiliary trees ob-
</tableCaption>
<bodyText confidence="0.981537916666667">
tained from our model in the full treebank dataset.
Nonterminal symbols created by binarization are
shown with an over-bar.
We also applied our model to the full WSJ Penn
Treebank setting (section 2-21 for training and sec-
tion 23 for testing). The parsing results are shown in
Table 3. We trained the model with an MH sampler
for 3.5k iterations.
For the full treebank dataset, our model obtained
nearly identical results to those obtained with BTSG
model, making the grammar size approximately
19% smaller than that of BTSG. We can see that only
a small number of auxiliary trees have a great impact
on reducing the grammar size. Surprisingly, there
are many fewer auxiliary trees than initial trees. We
believe this to be due to the tree binarization and our
restricted assumption of simple auxiliary trees.
Table 4 shows examples of lexicalized auxiliary
trees obtained with our model for the full treebank
data. We can see that punctuation (“–”, “,”, and “;”)
and adverb (RB) tend to be inserted in other trees.
Punctuation and adverb appear in various positions
in English sentences. Our results suggest that rather
than treat those words as substitutions, it is more rea-
sonable to consider them to be “insertions”, which is
intuitively understandable.
5 Summary
We proposed a model that incorporates an inser-
tion operator in BTSG and developed an efficient
inference technique. Since it is computationally ex-
pensive to allow any auxiliary trees, we tackled the
problem by introducing a restricted variant of aux-
iliary trees. Our model outperformed the BTSG
model for a small dataset, and achieved compara-
ble parsing results for a large dataset, making the
210
</bodyText>
<reference confidence="0.99771">
J. Pitman and M. Yor. 1997. The two-parameter Poisson-
Dirichlet distribution derived from a stable subordina-
tor. The Annals of Probability, 25(2):855–900.
M. Post and D. Gildea. 2009. Bayesian learning of a
tree substitution grammar. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pages 45–48,
Suntec, Singapore, August. Association for Computa-
tional Linguistics.
Y. Schabes and R.C. Waters. 1995. Tree insertion gram-
mar: a cubic-time, parsable formalism that lexicalizes
context-free grammar without changing the trees pro-
duced. Fuzzy Sets and Systems, 76(3):309–317.
Y. W. Teh. 2006a. A Bayesian interpretation of interpo-
lated Kneser-Ney. Technical Report TRA2/06, School
of Computing, National University of Singapore.
Y. W. Teh. 2006b. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th Annual Meeting of
the Association for Computational Linguistics (ICCL-
ACL), pages 985–992.
F. Xia. 1999. Extracting tree adjoining grammars from
bracketed corpora. In Proceedings of the 5th Natu-
ral Language Processing Pacific Rim Symposium (NL-
PRS), pages 398–403.
</reference>
<page confidence="0.998788">
211
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.784163">
<title confidence="0.998946">Insertion Operator for Bayesian Tree Substitution Grammars</title>
<author confidence="0.994605">Akinori Fujino Shindo</author>
<affiliation confidence="0.926765">NTT Communication Science Laboratories, NTT</affiliation>
<address confidence="0.875474">2-4 Hikaridai Seika-cho Soraku-gun Kyoto 619-0237</address>
<email confidence="0.976334">shindo.hiroyuki@lab.ntt.co.jp</email>
<email confidence="0.976334">fujino.akinori@lab.ntt.co.jp</email>
<email confidence="0.976334">nagata.masaaki@lab.ntt.co.jp</email>
<abstract confidence="0.99689725">We propose a model that incorporates an insertion operator in Bayesian tree substitution grammars (BTSG). Tree insertion is helpful for modeling syntax patterns accurately with fewer grammar rules than BTSG. The experimental parsing results show that our model outperforms a standard PCFG and BTSG for a small dataset. For a large dataset, our model obtains comparable results to BTSG, making the number of grammar rules much smaller than with BTSG.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Pitman</author>
<author>M Yor</author>
</authors>
<title>The two-parameter PoissonDirichlet distribution derived from a stable subordinator. The Annals of Probability,</title>
<date>1997</date>
<pages>25--2</pages>
<contexts>
<context position="2795" citStr="Pitman and Yor, 1997" startWordPosition="429" endWordPosition="432">ertion that makes use of dynamic programming. 2 Overview of BTSG Model We briefly review the BTSG model described in (Cohn et al., 2011). TSG uses a substitution operator (shown in Fig. 1a) to combine subtrees. Subtrees for substitution are referred to as initial trees, and leaf nonterminals in initial trees are referred to as frontier nodes. Their task is the unsupervised induction of TSG derivations from parse trees. A derivation is information about how subtrees are combined to form parse trees. The probability distribution over initial trees is defined by using a Pitman-Yor process prior (Pitman and Yor, 1997), that is, e JX — GX GX JdX, BX — PYP (dX, BX, Po (· JX )) , where X is a nonterminal symbol, e is an initial tree rooted with X, and Po (· JX ) is a base distribution over the infinite space of initial trees rooted with X. dX and BX are hyperparameters that are used to control the model’s behavior. Integrating out all possible values of GX, the resulting distribution is 206 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 206–211, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics (a) p (ei le−i, X, dX</context>
</contexts>
<marker>Pitman, Yor, 1997</marker>
<rawString>J. Pitman and M. Yor. 1997. The two-parameter PoissonDirichlet distribution derived from a stable subordinator. The Annals of Probability, 25(2):855–900.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Post</author>
<author>D Gildea</author>
</authors>
<title>Bayesian learning of a tree substitution grammar.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACLIJCNLP 2009 Conference Short Papers,</booktitle>
<pages>45--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="1813" citStr="Post and Gildea, 2009" startWordPosition="271" endWordPosition="274"> for expressing various syntax patterns with fewer grammar rules, thus we expect that adding an insertion operator will improve parsing accuracy and realize a compact grammar size. One of the challenges of adding an insertion operator is that the computational cost of grammar induction is high since tree insertion significantly increases the number of possible subtrees. Previous work on TAG and TIG induction (Xia, 1999; Chiang, 2003; Chen et al., 2006) has addressed the problem using language-specific heuristics and a maximum likelihood estimator, which leads to overfitting the training data (Post and Gildea, 2009). Instead, we incorporate an insertion operator in a Bayesian TSG (BTSG) model (Cohn et al., 2011) that learns grammar rules automatically without heuristics. Our model uses a restricted variant of subtrees for insertion to model the probability distribution simply and train the model efficiently. We also present an inference technique for handling a tree insertion that makes use of dynamic programming. 2 Overview of BTSG Model We briefly review the BTSG model described in (Cohn et al., 2011). TSG uses a substitution operator (shown in Fig. 1a) to combine subtrees. Subtrees for substitution ar</context>
<context position="12127" citStr="Post and Gildea, 2009" startWordPosition="2102" endWordPosition="2105">. The treebank data was binarized using the CENTER-HEAD method (Matsuzaki et al., 2005). We replaced lexical words with counts &lt; 1 in the training set with one of three unknown 1Results from (Cohn and Blunsom, 2010). 2Results for length &lt; 40. 3http://nclt.computing.dcu.ie/~jfoster/resources/ corpus method F1 CFG 54.08 BNC BTSG 67.73 BTSG + insertion 69.06 CFG 64.99 BTSG 77.19 WSJ BTSG + insertion 78.54 (Petrov et al., 2006) 77.931 (Cohn and Blunsom, 2010) 78.40 Table 2: Small dataset experiments # rules (# aux. trees) F1 CFG 35374 (-) 71.0 BTSG 80026 (0) 85.0 BTSG + insertion 65099 (25) 85.3 (Post and Gildea, 2009) - 82.62 (Cohn and Blunsom, 2010) - 85.3 Table 3: Full Penn Treebank dataset experiments words using lexical features. We trained our model using a training set, and then sampled 10k derivations for each sentence in a test set. Parsing results were obtained with the MER algorithm (Cohn et al., 2011) using the 10k derivation samples. We show the bracketing F1 score of predicted parse trees evaluated by EVALB4, averaged over three independent runs. In small dataset experiments, we used BNC (1k sentences, 90% for training and 10% for testing) and WSJ (section 2 for training and section 22 for tes</context>
</contexts>
<marker>Post, Gildea, 2009</marker>
<rawString>M. Post and D. Gildea. 2009. Bayesian learning of a tree substitution grammar. In Proceedings of the ACLIJCNLP 2009 Conference Short Papers, pages 45–48, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Schabes</author>
<author>R C Waters</author>
</authors>
<title>Tree insertion grammar: a cubic-time, parsable formalism that lexicalizes context-free grammar without changing the trees produced. Fuzzy Sets and Systems,</title>
<date>1995</date>
<pages>76--3</pages>
<contexts>
<context position="1157" citStr="Schabes and Waters, 1995" startWordPosition="163" endWordPosition="167">w that our model outperforms a standard PCFG and BTSG for a small dataset. For a large dataset, our model obtains comparable results to BTSG, making the number of grammar rules much smaller than with BTSG. 1 Introduction Tree substitution grammar (TSG) is a promising formalism for modeling language data. TSG generalizes context free grammars (CFG) by allowing nonterminal nodes to be replaced with subtrees of arbitrary size. A natural extension of TSG involves adding an insertion operator for combining subtrees as in tree adjoining grammars (TAG) (Joshi, 1985) or tree insertion grammars (TIG) (Schabes and Waters, 1995). An insertion operator is helpful for expressing various syntax patterns with fewer grammar rules, thus we expect that adding an insertion operator will improve parsing accuracy and realize a compact grammar size. One of the challenges of adding an insertion operator is that the computational cost of grammar induction is high since tree insertion significantly increases the number of possible subtrees. Previous work on TAG and TIG induction (Xia, 1999; Chiang, 2003; Chen et al., 2006) has addressed the problem using language-specific heuristics and a maximum likelihood estimator, which leads </context>
</contexts>
<marker>Schabes, Waters, 1995</marker>
<rawString>Y. Schabes and R.C. Waters. 1995. Tree insertion grammar: a cubic-time, parsable formalism that lexicalizes context-free grammar without changing the trees produced. Fuzzy Sets and Systems, 76(3):309–317.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Teh</author>
</authors>
<title>A Bayesian interpretation of interpolated Kneser-Ney.</title>
<date>2006</date>
<tech>Technical Report TRA2/06,</tech>
<institution>School of Computing, National University of Singapore.</institution>
<contexts>
<context position="3956" citStr="Teh, 2006" startWordPosition="649" endWordPosition="650"> Computational Linguistics (a) p (ei le−i, X, dX, OX ) = αe;,X + NXP0 (ei, JX), (1) −i nei X−dX·tei,X θX+dX·t·,X . e−i = e1, ... , ei−1 are previously genθX+n−i ·,X erated initial trees, and n−i ei,X is the number of times ei has been used in e−i. tei,X is the number of tables labeled with ei. n−i ·,X = Ee n−i e,X and t·,X = Ee te,X are the total counts of initial trees and tables, respectively. The PYP prior produces “rich get richer” statistics: a few initial trees are often used for derivation while many are rarely used, and this is shown empirically to be well-suited for natural language (Teh, 2006b; Johnson and Goldwater, 2009). The base probability of an initial tree, P0 (e JX), is given as follows. where αei,X = and Q θX+n −i X = ·,X (b) 11 P0 (e �X ) = r∈CFG(e) 11 x 11 PMLE (r) x sA A∈LEAF(e) (1 − sB) , (2) Figure 1: Example of (a) substitution and (b) insertion (dotted line). B∈INTER(e) where CFG (e) is a set of decomposed CFG productions of e, PMLE (r) is a maximum likelihood estimate (MLE) of r. LEAF (e) and INTER (e) are sets of leaf and internal symbols of e, respectively. sX is a stopping probability defined for each X. 3 Insertion Operator for BTSG 3.1 Tree Insertion Model We</context>
<context position="11275" citStr="Teh, 2006" startWordPosition="1963" endWordPosition="1964">and Blunsom, 2010) to train our model. The MH algorithm learns BTSG model parameters efficiently, and it can be applied to our insertion model. The MH algorithm consists of the following three steps. For each sentence, 1. Calculate the inside probability (Lari and Young, 1991) in a bottom-up manner using the grammar decomposition. 2. Sample a derivation tree in a top-down manner. 3. Accept or reject the derivation sample by using the MH test. The MH algorithm is described in detail in (Cohn and Blunsom, 2010). The hyperparameters of our model are updated with the auxiliary variable technique (Teh, 2006a). 4 Experiments We ran experiments on the British National Corpus (BNC) Treebank 3 and the WSJ English Penn Treebank. We did not use a development set since our model automatically updates the hyperparameters for every iteration. The treebank data was binarized using the CENTER-HEAD method (Matsuzaki et al., 2005). We replaced lexical words with counts &lt; 1 in the training set with one of three unknown 1Results from (Cohn and Blunsom, 2010). 2Results for length &lt; 40. 3http://nclt.computing.dcu.ie/~jfoster/resources/ corpus method F1 CFG 54.08 BNC BTSG 67.73 BTSG + insertion 69.06 CFG 64.99 BT</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Y. W. Teh. 2006a. A Bayesian interpretation of interpolated Kneser-Ney. Technical Report TRA2/06, School of Computing, National University of Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Teh</author>
</authors>
<title>A hierarchical Bayesian language model based on Pitman-Yor processes.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics (ICCLACL),</booktitle>
<pages>985--992</pages>
<contexts>
<context position="3956" citStr="Teh, 2006" startWordPosition="649" endWordPosition="650"> Computational Linguistics (a) p (ei le−i, X, dX, OX ) = αe;,X + NXP0 (ei, JX), (1) −i nei X−dX·tei,X θX+dX·t·,X . e−i = e1, ... , ei−1 are previously genθX+n−i ·,X erated initial trees, and n−i ei,X is the number of times ei has been used in e−i. tei,X is the number of tables labeled with ei. n−i ·,X = Ee n−i e,X and t·,X = Ee te,X are the total counts of initial trees and tables, respectively. The PYP prior produces “rich get richer” statistics: a few initial trees are often used for derivation while many are rarely used, and this is shown empirically to be well-suited for natural language (Teh, 2006b; Johnson and Goldwater, 2009). The base probability of an initial tree, P0 (e JX), is given as follows. where αei,X = and Q θX+n −i X = ·,X (b) 11 P0 (e �X ) = r∈CFG(e) 11 x 11 PMLE (r) x sA A∈LEAF(e) (1 − sB) , (2) Figure 1: Example of (a) substitution and (b) insertion (dotted line). B∈INTER(e) where CFG (e) is a set of decomposed CFG productions of e, PMLE (r) is a maximum likelihood estimate (MLE) of r. LEAF (e) and INTER (e) are sets of leaf and internal symbols of e, respectively. sX is a stopping probability defined for each X. 3 Insertion Operator for BTSG 3.1 Tree Insertion Model We</context>
<context position="11275" citStr="Teh, 2006" startWordPosition="1963" endWordPosition="1964">and Blunsom, 2010) to train our model. The MH algorithm learns BTSG model parameters efficiently, and it can be applied to our insertion model. The MH algorithm consists of the following three steps. For each sentence, 1. Calculate the inside probability (Lari and Young, 1991) in a bottom-up manner using the grammar decomposition. 2. Sample a derivation tree in a top-down manner. 3. Accept or reject the derivation sample by using the MH test. The MH algorithm is described in detail in (Cohn and Blunsom, 2010). The hyperparameters of our model are updated with the auxiliary variable technique (Teh, 2006a). 4 Experiments We ran experiments on the British National Corpus (BNC) Treebank 3 and the WSJ English Penn Treebank. We did not use a development set since our model automatically updates the hyperparameters for every iteration. The treebank data was binarized using the CENTER-HEAD method (Matsuzaki et al., 2005). We replaced lexical words with counts &lt; 1 in the training set with one of three unknown 1Results from (Cohn and Blunsom, 2010). 2Results for length &lt; 40. 3http://nclt.computing.dcu.ie/~jfoster/resources/ corpus method F1 CFG 54.08 BNC BTSG 67.73 BTSG + insertion 69.06 CFG 64.99 BT</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Y. W. Teh. 2006b. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics (ICCLACL), pages 985–992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Xia</author>
</authors>
<title>Extracting tree adjoining grammars from bracketed corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of the 5th Natural Language Processing Pacific Rim Symposium (NLPRS),</booktitle>
<pages>398--403</pages>
<contexts>
<context position="1613" citStr="Xia, 1999" startWordPosition="241" endWordPosition="242">n insertion operator for combining subtrees as in tree adjoining grammars (TAG) (Joshi, 1985) or tree insertion grammars (TIG) (Schabes and Waters, 1995). An insertion operator is helpful for expressing various syntax patterns with fewer grammar rules, thus we expect that adding an insertion operator will improve parsing accuracy and realize a compact grammar size. One of the challenges of adding an insertion operator is that the computational cost of grammar induction is high since tree insertion significantly increases the number of possible subtrees. Previous work on TAG and TIG induction (Xia, 1999; Chiang, 2003; Chen et al., 2006) has addressed the problem using language-specific heuristics and a maximum likelihood estimator, which leads to overfitting the training data (Post and Gildea, 2009). Instead, we incorporate an insertion operator in a Bayesian TSG (BTSG) model (Cohn et al., 2011) that learns grammar rules automatically without heuristics. Our model uses a restricted variant of subtrees for insertion to model the probability distribution simply and train the model efficiently. We also present an inference technique for handling a tree insertion that makes use of dynamic progra</context>
</contexts>
<marker>Xia, 1999</marker>
<rawString>F. Xia. 1999. Extracting tree adjoining grammars from bracketed corpora. In Proceedings of the 5th Natural Language Processing Pacific Rim Symposium (NLPRS), pages 398–403.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>