<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.999491">
A non-contiguous Tree Sequence Alignment-based Model for
Statistical Machine Translation
</title>
<author confidence="0.970834">
Jun Sun1,2 Min Zhang1 Chew Lim Tan2
</author>
<affiliation confidence="0.781675">
1 Institute for Infocomm Research 2School of Computing, National University of Singapore
</affiliation>
<email confidence="0.984188">
sunjun@comp.nus.edu.sg mzhang@i2r.a-star.edu.sg tancl@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.992684" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998613444444445">
The tree sequence based translation model al-
lows the violation of syntactic boundaries in a
rule to capture non-syntactic phrases, where a
tree sequence is a contiguous sequence of sub-
trees. This paper goes further to present a trans-
lation model based on non-contiguous tree se-
quence alignment, where a non-contiguous tree
sequence is a sequence of sub-trees and gaps.
Compared with the contiguous tree sequence-
based model, the proposed model can well han-
dle non-contiguous phrases with any large gaps
by means of non-contiguous tree sequence
alignment. An algorithm targeting the non-
contiguous constituent decoding is also proposed.
Experimental results on the NIST MT-05 Chi-
nese-English translation task show that the pro-
posed model statistically significantly outper-
forms the baseline systems.
</bodyText>
<sectionHeader confidence="0.998967" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999464862068966">
Current research in statistical machine translation
(SMT) mostly settles itself in the domain of either
phrase-based or syntax-based. Between them, the
phrase-based approach (Marcu and Wong, 2002;
Koehn et al, 2003; Och and Ney, 2004) allows lo-
cal reordering and contiguous phrase translation.
However, it is hard for phrase-based models to
learn global reorderings and to deal with non-
contiguous phrases. To address this issue, many
syntax-based approaches (Yamada and Knight,
2001; Eisner, 2003; Gildea, 2003; Ding and Palmer,
2005; Quirk et al, 2005; Zhang et al, 2007, 2008a;
Bod, 2007; Liu et al, 2006, 2007; Hearne and Way,
2003) tend to integrate more syntactic information
to enhance the non-contiguous phrase modeling. In
general, most of them achieve this goal by intro-
ducing syntactic non-terminals as translational
equivalent placeholders in both source and target
sides. Nevertheless, the generated rules are strictly
required to be derived from the contiguous transla-
tional equivalences (Galley et al, 2006; Marcu et al,
2006; Zhang et al, 2007, 2008a, 2008b; Liu et al,
2006, 2007). Among them, Zhang et al. (2008a)
acquire the non-contiguous phrasal rules from the
contiguous tree sequence pairs1, and find them use-
less via real syntax-based translation systems.
However, Wellington et al. (2006) statistically re-
port that discontinuities are very useful for transla-
tional equivalence analysis using binary branching
structures under word alignment and parse tree
constraints. Bod (2007) also finds that discontinues
phrasal rules make significant improvement in lin-
guistically motivated STSG-based translation
model. The above observations are conflicting to
each other. In our opinion, the non-contiguous
phrasal rules themselves may not play a trivial role,
as reported in Zhang et al. (2008a). We believe that
the effectiveness of non-contiguous phrasal rules
highly depends on how to extract and utilize them.
To verify the above assumption, suppose there is
only one tree pair in the training data with its
alignment information illustrated as Fig. 1(a) 2. A
test sentence is given in Fig. 1(b): the source sen-
tence with its syntactic tree structure as the upper
tree and the expected target output with its syntac-
tic structure as the lower tree. In the tree sequence
alignment based model, in addition to the entire
tree pair, it is capable to acquire the contiguous
tree sequence pairs: TSP (1-4) 3 in Fig. 1. By
means of the rules derived from these contiguous
tree sequence pairs, it is easy to translate the conti-
guous phrase &amp;quot; /he /show up /&apos;s&amp;quot;. As for the
non-contiguous phrase &amp;quot; /at, ***, /time&amp;quot;, the
only related rule is r, derived from TSP4 and the
entire tree pair. However, the source side of r, does
not match the source tree structure of the test sen-
tence. Therefore, we can only partially translate the
illustrated test sentence with this training sample.
</bodyText>
<footnote confidence="0.624329375">
1 A tree sequence pair in this context is a kind of translational
equivalence comprised of a pair of tree sequences.
2 We illustrate the rule extraction with an example from the
tree-to-tree translation model based on tree sequence align-
ment (Zhang et al, 2008a) without losing of generality to most
syntactic tree based models.
3 We only list the contiguous tree sequence pairs with one
single sub-tree in both sides without losing of generality.
</footnote>
<page confidence="0.918871">
914
</page>
<note confidence="0.999642">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 914–922,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<figure confidence="0.995766630434783">
VP
VP
NP
NP
CP
IP
IP
CP
VV AS PN
NN
VV
DEC
NN
VV DEC
VV PN
(at) (NULL) (he) (show up) (`s) (time)
up
when
he shows
WRB
RP
PRP VBZ
VP
S
SBAR
(at) (he) (show up) (&apos;s) (time)
when
he shows
up
WRB
PRP VBZ
RP
VP
S
SBAR
TSP1: PN( ) PRP(he)
TSP2: VV( ) VP(VBZ(shows),RP(up))
TSP3: IP(PN( ),VV( ))
S((PRP(he), VP(VBZ(shows), RP(up))))
TSP4: CP(IP(PN( ),VV( )),DEC( ))
S((PRP(he), VP(VBZ(shows), RP(up))))
TSP5: VV( ), &amp;quot;&amp;quot;&amp;quot; ,NN( ) WRB(when)
r2: VV( ), &amp;quot;&amp;quot;&amp;quot; ,NN( ) WRB(when)
r1: VP(VV( ),AS( ),NP(CP[0],NN( )))
SBAR(WRB(when),S[0])
(a) (b)
</figure>
<figureCaption confidence="0.999921">
Figure 1: Rule extraction of tree-to-tree model based on tree sequence pairs
</figureCaption>
<bodyText confidence="0.999979666666667">
As discussed above, the problem lies in that the
non-contiguous phrases derived from the conti-
guous tree sequence pairs demand greater reliance
on the context. Consequently, when applying those
rules to unseen data, it may suffer from the data
sparseness problem. The expressiveness of the
model also slacks due to their weak ability of gene-
ralization.
To address this issue, we propose a syntactic
translation model based on non-contiguous tree
sequence alignment. This model extracts the
translation rules not only from the contiguous tree
sequence pairs but also from the non-contiguous
tree sequence pairs where a non-contiguous tree
sequence is a sequence of sub-trees and gaps. With
the help of the non-contiguous tree sequence, the
proposed model can well capture the non-
contiguous phrases in avoidance of the constraints
of large applicability of context and enhance the
non-contiguous constituent modeling. As for the
above example, the proposed model enables the
non-contiguous tree sequence pair indexed as
TSP5 in Fig. 1 and is allowed to further derive r2
from TSP5. By means of r2 and the same
processing to the contiguous phrase &amp;quot; /he
/show up /&apos;s&amp;quot; as the contiguous tree sequence
based model, we can successfully translate the en-
tire source sentence in Fig. 1(b).
We define a synchronous grammar, named Syn-
chronous non-contiguous Tree Sequence Substitu-
tion Grammar (SncTSSG), extended from syn-
chronous tree substitution grammar (STSG:
Chiang, 2006) to illustrate our model. The pro-
posed synchronous grammar is able to cover the
previous proposed grammar based on tree (STSG,
Eisner, 2003; Zhang et al, 2007) and tree sequence
(STSSG, Zhang et al, 2008a) alignment. Besides,
we modify the traditional parsing based decoding
algorithm for syntax-based SMT to facilitate the
non-contiguous constituent decoding for our model.
To the best of our knowledge, this is the first
attempt to acquire the translation rules with rich
syntactic structures from the non-contiguous
Translational Equivalences (non-contiguous tree
sequence pairs in this context).
The rest of this paper is organized as follows:
Section 2 presents a formal definition of our model
with detailed parameterization. Sections 3 and 4
elaborate the extraction of the non-contiguous tree
sequence pairs and the decoding algorithm respec-
tively. The experiments we conduct to assess the
effectiveness of the proposed method are reported
in Section 5. We finally conclude this work in Sec-
tion 6.
</bodyText>
<sectionHeader confidence="0.9431095" genericHeader="method">
2 Non-Contiguous Tree sequence Align-
ment-based Model
</sectionHeader>
<bodyText confidence="0.9982004">
In this section, we give a formal definition of
SncTSSG and accordingly we propose the align-
ment based translation model. The details of prob-
abilistic parameterization are elaborated based on
the log-linear framework.
</bodyText>
<listItem confidence="0.8427623">
2.1 Synchronous non-contiguous TSSG
(SncTSSG)
Extended from STSG (Shiever, 2004), SncTSSG
can be formalized as a quintuple G = &lt; , , ,
, R&gt;, where:
• and are source and target terminal
alphabets (words) respectively, and
• and are source and target non-
terminal alphabets (linguistically syntactic
tags, i.e. NP, VP) respectively; as well as the
</listItem>
<bodyText confidence="0.816628">
non-terminal to denote a gap,
</bodyText>
<page confidence="0.997175">
915
</page>
<figureCaption confidence="0.997316">
Figure 2: A word-aligned parse tree pair
Figure 3: A non-contiguous tree sequence pair
Figure 4: Two examples of non-contiguous
tree sequence translation rules
</figureCaption>
<bodyText confidence="0.897798818181818">
can represent any syntactic or non-
syntactic tree sequences, and
• R is a production rule set consisting of rules
derived from corresponding contiguous or
non-contiguous tree sequence pairs, where a
rule is a pair of contiguous or non-
contiguous tree sequence with alignment re-
lation between leaf nodes across the tree se-
quence pair.
A non-contiguous tree sequence translation rule
r R can be further defined as a triple
</bodyText>
<listItem confidence="0.9734315">
, where:
• is a non-contiguous source tree
</listItem>
<bodyText confidence="0.849942714285714">
sequence, covering the span set
in , where
which means each subspan has non-
zero width and which means there
is a non-zero gap between each pair of
consecutive intervals. A gap of interval
[ ] is denoted as , and
</bodyText>
<listItem confidence="0.881575">
• is a non-contiguous target tree
</listItem>
<bodyText confidence="0.972116285714286">
sequence, covering the span set
in , where
which means each subspan has non-zero
width and which means there is a
non-zero gap between each pair of
consecutive intervals. A gap of interval
[ ] is denoted as , and
</bodyText>
<listItem confidence="0.912075">
• are the alignments between leaf nodes of
the source and target non-contiguous tree
sequences, satisfying the following
conditions :
</listItem>
<bodyText confidence="0.996699739130435">
In SncTSSG, the leaf nodes in a non-contiguous
tree sequence rule can be either non-terminal
symbols (grammar tags) or terminal symbols
(lexical words) and the non-terminal symbols with
the same index which are subsumed
simultaneously are not required to be contiguous.
Fig. 4 shows two examples of non-contiguous tree
sequence rules (&amp;quot;non-contiguous rule&amp;quot; for short in
the following context) derived from the non-
contiguous tree sequence pair (in Fig. 3) which is
extracted from the bilingual tree pair in Fig. 2.
Between them, ncTSr1 is a tree rule with internal
nodes non-contiguously subsumed from a
contiguous tree sequence pair (dashed in Fig. 2)
while ncTSr2 is a non-contiguous rule with a
contiguous source side and a non-contiguous target
side. Obviously, the non-contiguous tree sequence
rule ncTSr2 is more flexible by neglecting the
context among the gaps of the tree sequence pair
while capturing all aligned counterparts with the
corresponding syntactic structure information. We
where and
,
</bodyText>
<page confidence="0.945507">
916
</page>
<bodyText confidence="0.999456">
expect these properties can well address the issues
of non-contiguous phrase modeling.
</bodyText>
<subsectionHeader confidence="0.994423">
2.2 SncTSSG based Translation Model
</subsectionHeader>
<bodyText confidence="0.998033777777778">
Given the source and target sentence and , as
well as the corresponding parse trees
and , our approach directly approximates the
posterior probability based on
the log-linear framework:
In this model, the feature function hm is log-
linearly combined by the corresponding parameter
(Och and Ney, 2002). The following features
are utilized in our model:
</bodyText>
<listItem confidence="0.9461647">
1) The bi-phrasal translation probabilities
2) The bi-lexical translation probabilities
3) The target language model
4) The # of words in the target sentence
5) The # of rules utilized
6) The average tree depth in the source side
of the rules adopted
7) The # of non-contiguous rules utilized
8) The # of reordering times caused by the
utilization of the non-contiguous rules
</listItem>
<bodyText confidence="0.999078333333333">
Feature 1~6 can be applied to either STSSG or
SncTSSG based models, while the last two targets
SncTSSG only.
</bodyText>
<sectionHeader confidence="0.960824" genericHeader="method">
3 Tree Sequence Pair Extraction
</sectionHeader>
<bodyText confidence="0.999946705882353">
In training, other than the contiguous tree sequence
pairs, we extract the non-contiguous ones as well.
Nevertheless, compared with the contiguous tree
sequence pairs, the non-contiguous ones suffer
more from the tree sequence pair redundancy
problem that one non-contiguous tree sequence
pair can be comprised of two or more unrelated
and nonadjacent contiguous ones. To model the
contiguous phrases, this problem is actually trivial,
since the contiguous phrases stay adjacently and
share the related syntactic constraints; however, as
for non-contiguous phrase modeling, the cohesion
of syntactically and semantically unrelated tree
sequence pairs is more likely to generate noisy
rules which do not benefit at all. In order to minim-
ize the number of redundant tree sequence pairs,
we limit the # of gaps of non-contiguous tree se-
</bodyText>
<table confidence="0.861778625">
exp
Algorithm 1: Tree Sequence Pair Extraction
Input: source tree and target tree
Output: the set of tree sequence pairs
Data structure:
p[j1, j2] to store tree sequence pairs covering source
span[j1, j2]
1: foreach source span [j1, j2], do
2: find a target span [i1,i2] with minimal length cov-
ering all the target words aligned to [j1, j2]
3: if all the target words in [i1,i2] are aligned with
source words only in [j1, j2], then
4: Pair each source tree sequence covering [j1, j2]
with those in target covering [i1,i2] as a conti-
guous tree sequence pair
5: Insert them into p[j1, j2]
</table>
<figure confidence="0.802268090909091">
6: else
7: create sub-span set s([i1,i2]) to cover all the tar-
get words aligned to [j1, j2]
8: Pair each source tree sequence covering [j1, j2]
with each target tree sequence covering
s([i1,i2]) as a non-contiguous tree sequence pair
9: Insert them into p[j1, j2]
10: end if
11:end do
12: foreach target span [i1,i2], do
13: find a source span [j1, j2] with minimal length
covering all the source words aligned to [i1,i2]
14: if any source word in [j1, j2] is aligned with tar-
get words outside [i1,i2], then
15: create sub-span set s([j1, j2]) to cover all the
source words aligned to [i1,i2]
16: Pair each source tree sequence covering s([j1,
j2]) with each target tree sequence covering
[i1,i2] as a non-contiguous tree sequence pair
17: Insert them into p[j1, j2]
18: end if
19: end do
</figure>
<bodyText confidence="0.99066835">
quence pairs to be 0 in either source or target side.
In other words, we only allow one side to be non-
contiguous (either source or target side) to partially
reserve its syntactic and semantic cohesion4. We
further design a two-phase algorithm to extract the
tree sequence pairs as described in Algorithm 1.
For the first phase (line 1-11), we extract the
contiguous tree sequence pairs (line 3-5) and the
non-contiguous ones with contiguous tree se-
quence in the source side (line 6-9). In the second
phase (line 12-19), the ones with contiguous tree
sequence in the target side and non-contiguous tree
sequence on the source side are extracted.
4 Wellington et al. (2006) also reports that allowing gaps in
one side only is enough to eliminate the hierarchical alignment
failure with word alignment and one side parse tree constraints.
This is a particular case of our definition of non-contiguous
tree sequence pair since a non-contiguous tree sequence can be
considered to overcome the structural constraint by neglecting
the structural information in the gaps.
</bodyText>
<page confidence="0.987857">
917
</page>
<figureCaption confidence="0.999834">
Figure 5: Illustration of &amp;quot;Source gap insertion&amp;quot;
</figureCaption>
<bodyText confidence="0.999980066666667">
The extracted tree sequence pairs are then uti-
lized to derive the translation rules. In fact, both
the contiguous and non-contiguous tree sequence
pairs themselves are applicable translation rules;
we denote these rules as Initial rules. By means of
the Initial rules, we derive the Abstract rules simi-
larly as in Zhang et al. (2008a).
Additionally, we develop a few constraints to
limit the number of Abstract rules. The depth of a
tree in a rule is no greater than h. The number of
non-terminals as leaf nodes is no greater than c.
The tree number is no greater than d. Besides, the
number of lexical words at leaf nodes in an Initial
rule is no greater than l. The maximal number of
gaps for a non-contiguous rule is no greater than .
</bodyText>
<sectionHeader confidence="0.996606" genericHeader="method">
4 The Pisces decoder
</sectionHeader>
<bodyText confidence="0.953667695652174">
We implement our decoder Pisces by simulating
the span based CYK parser constrained by the
rules of SncTSSG. The decoder translates each
span iteratively in a bottom up manner which guar-
antees that when translating a source span, any of
its sub-spans is already translated.
For each source span [j1, j2], we perform a three-
phase decoding process. In the first phase, the
source side contiguous translation rules are utilized
as described in Algorithm 2. When translating us-
ing a source side contiguous rule, the target tree
sequence of the rule whether contiguous or non-
contiguous is directly considered as a candidate
translation for this span (line 3), if the rule is an
Initial rule; otherwise, the non-terminal leaf nodes
are replaced with the corresponding sub-spans&apos;
translations (line 5).
In the second phase, the source side non-
contiguous rules5 for [j1, j2] are processed. As for
5 A source side non-contiguous translation rules which cover a
list of n non-contiguous spans s([ , ], i=1,...,n) is consi-
dered to cover the source span [j1, j2] if and only if = j1 and
= j2.
</bodyText>
<table confidence="0.636514">
Algorithm 2: Contiguous rule processing
Data structure:
h[j1, j2]to store translations covering source span[j1, j2]
</table>
<listItem confidence="0.9343412">
1: foreach rule r contiguous in source span [j1, j2], do
2: if r is an Initial rule, then
3: insert r into h[j1, j2]
4: else //Abstract rule
5: generate translations by replacing the non-
terminal leaf nodes of r with their correspond-
ing spans&apos; translation
6: insert the new translation into h[j1, j2]
7: end if
8: end do
</listItem>
<bodyText confidence="0.999953962962963">
the ones with non-terminal leaf nodes, the re-
placement with corresponding spans&apos; translations
is initially performed in the same way as with the
contiguous rules in the first phase. After that, an
operation specified for the source side non-
contiguous rules named &amp;quot;Source gap insertion&amp;quot; is
performed. As illustrated in Fig. 5, to use the non-
contiguous rule r1, which covers the source span
set ([0,0], [4,4]), the target portion &amp;quot;IN(in)&amp;quot; is first
attained, then the translations to the gap span [1,3]
is acquired from the previous steps and is inserted
either to the right or to the left of &amp;quot;IN(in)&amp;quot;. The
insertion is rather cohesion based but leaves a gap
&lt;***&gt; for further &amp;quot;Target tree sequence reordering&amp;quot;
in the next phase if necessary.
In the third phase, we carry out the other non-
contiguous rule specific operation named &amp;quot;Target
tree sequence reordering&amp;quot;. Algorithm 3 gives an
overview of this operation. For each source span,
we first binarize the span into the left one and the
right one. The translation hypothesis for this span
is generated by firstly inserting the candidate trans-
lations of the right span to each gap in the ones of
the left span respectively (line 2-9) and then re-
peating in the alternative direction (line10-17). The
gaps for the insertion of the tree sequences in the
target side are generated from either the inherit-
</bodyText>
<page confidence="0.991353">
918
</page>
<figure confidence="0.297711590909091">
Algorithm 3: Target tree sequence reordering
Data structure:
h[j1, j2]to store translations covering source span[j1,
j2]
1: foreach k [j1, j2), do
2: foreach translation h[j1, k], do
3: foreach gap in , do
4: foreach translation h[k+1, j2], do
5: insert into the position of
6: insert the new translation into h[j1, j2]
7: end do
8: end do
9: end do
10: foreach translation h[k+1, j2], do
11: foreach gap in , do
12: foreach translation h[j1, k], do
13: insert into the position of
14: insert the new translation into h[j1, j2]
15: end do
16: end do
17: end do
18:end do
</figure>
<bodyText confidence="0.99240676">
ance of the target side non-contiguous tree se-
quence pairs or the production of the previous op-
erations of &amp;quot;Source gap insertion&amp;quot;. Therefore, the
insertion for target gaps helps search for a better
order of the non-contiguous constituents in the tar-
get side. On the other hand, the non-contiguous
tree sequences with rich syntactic information are
reordered, nevertheless, without much considera-
tion of the constraints of the syntactic structure.
Consequently, this distortional operation, like
phrase-based models, is much more flexible in the
order of the target constituents than the traditional
syntax-based models which are limited by the syn-
tactic structure. As a result, &amp;quot;Target tree sequence
reordering&amp;quot; enhances the reordering ability of the
model.
To speed up the decoder, we use several thre-
sholds to limit the searching space for each span.
The maximal number of the rules in a source span
is no greater than . The maximal number of trans-
lation candidates for a source span is no greater
than . On the other hand, to simplify the compu-
tation of language model, we only compute for
source side contiguous translational hypothesis,
while neglecting gaps in the target side if any.
</bodyText>
<sectionHeader confidence="0.999899" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.995606">
5.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.992917821428572">
In the experiments, we train the translation model
on FBIS corpus (7.2M (Chinese) + 9.2M (English)
words) and train a 4-gram language model on the
Xinhua portion of the English Gigaword corpus
(181M words) using the SRILM Toolkits (Stolcke,
2002). We use these sentences with less than 50
characters from the NIST MT-2002 test set as the
development set and the NIST MT-2005 test set as
our test set. We use the Stanford parser (Klein and
Manning, 2003) to parse bilingual sentences on the
training set and Chinese sentences on the devel-
opment and test set. The evaluation metric is case-
sensitive BLEU-4 (Papineni et al., 2002). We base
on the m-to-n word alignments dumped by GI-
ZA++ to extract the tree sequence pairs. For the
MER training, we modify Koehn&apos;s version (Koehn,
2004). We use Zhang et al&apos;s implementation
(Zhang et al, 2004) for 95% confidence intervals
significant test.
We compare the SncTSSG based model against
two baseline models: the phrase-based and the
STSSG-based models. For the phrase-based model,
we use Moses (Koehn et al, 2007) with its default
settings; for the STSSG and SncTSSG based mod-
els we use our decoder Pisces by setting the fol-
lowing parameters: , , , ,
, . Additionally, for STSSG we set
, and for SncTSSG, we set .
</bodyText>
<subsectionHeader confidence="0.998702">
5.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999609777777778">
Table 1 compares the performance of different
models across the two systems. The proposed
SncTSSG based model significantly outperforms
(p &lt; 0.05) the two baseline models. Since the
SncTSSG based model covers the STSSG based
model in its modeling ability and obtains a superset
in rules, the improvement empirically verifies the
effectiveness of the additional non-contiguous
rules.
</bodyText>
<table confidence="0.988807">
Model BLEU
cBP 23.86
STSSG 25.92
SncTSSG 26.53
</table>
<tableCaption confidence="0.990594">
Table 1: Translation results of different models (cBP
refers to contiguous bilingual phrases without syntactic
structural information, as used in Moses)
</tableCaption>
<bodyText confidence="0.969646076923077">
Table 2 measures the contribution of different
combination of rules. cR refers to the rules derived
from contiguous tree sequence pairs (i.e., all
STSSG rules); ncPR refers to non-contiguous
phrasal rules derived from contiguous tree se-
quence pairs with at least one non-terminal leaf
node between two lexicalized leaf nodes (i.e., all
non-contiguous rules in STSSG defined as in
Zhang et al. (2008a)); srcncR refers to source side
non-contiguous rules (SncTSSG rules only, not
STSSG rules); tgtncR refers to target side non-
contiguous rules (SncTSSG rules only, not STSSG
rules) and src&amp;tgtncR refers non-contiguous rules
</bodyText>
<figure confidence="0.946392615384615">
System
Moses
Pisces
919
ID Rule Set BLEU
1 cR (STSSG) 25.92
2 cR w/o ncPR 25.87
3 cR w/o ncPR + tgtncR 26.14
4 cR w/o ncPR + srcncR 26.50
5 cR w/o ncPR + src&amp;tgtncR 26.51
6 cR + tgtncR 26.11
7 cR + srcncR 26.56
8 cR+src&amp;tgtncR(SncTSSG) 26.53
</figure>
<tableCaption confidence="0.992849">
Table 2: Performance of different rule combination
</tableCaption>
<bodyText confidence="0.973287023255814">
with gaps in either side (srcncR+ tgtncR). The last
three kinds of rules are all derived from non-
contiguous tree sequence pairs.
1) From Exp 1 and 2 in Table 2, we find that
non-contiguous phrasal rules (ncPR) derived from
contiguous tree sequence pairs make little impact
on the translation performance which is consistent
with the discovery of Zhang et al. (2008a). How-
ever, if we append the non-contiguous phrasal
rules derived from non-contiguous tree sequence
pairs, no matter whether non-contiguous in source
or in target, the performance statistically signifi-
cantly (p &lt; 0.05) improves (as presented in Exp
2-5), which validates our prediction that the non-
contiguous rules derived from non-contiguous tree
sequence pairs contribute more to the performance
than those acquired from contiguous tree sequence
pairs.
2) Not only that, after comparing Exp 6,7,8
against Exp 3,4,5 respectively, we find that the
ability of rules derived from non-contiguous tree
sequence pairs generally covers that of the rules
derived from the contiguous tree sequence pairs,
due to the slight change in BLEU score.
3) The further comparison of the non-
contiguous rules from non-contiguous spans in Exp.
6&amp;7 as well as Exp 3&amp;4, shows that non-
contiguity in the target side in Chinese-English
translation task is not so useful as that in the source
side when constructing the non-contiguous phrasal
rules. This also validates the findings in Welling-
ton et al. (2006) that varying the gaps on the Eng-
lish side (the target side in this context) seldom
reduce the hierarchical alignment failures.
Table 3 explores the contribution of the non-
contiguous translational equivalence to phrase-
based models (all the rules in Table 3 has no
grammar tags, but a gap &lt;***&gt; is allowed in the
last three rows). tgtncBP refers to the bilingual
phrases with gaps in the target side; srcncBP refers
to the bilingual phrases with gaps in the source
side; src&amp;tgtncBP refers to the bilingual phrases
with gaps in either side.
</bodyText>
<table confidence="0.9215055">
System Rule Set BLEU
Moses cBP 23.86
cBP 22.63
Pisces cBP + tgtncBP 23.74
cBP + srcncBP 23.93
cBP + src&amp;tgtncBP 24.24
</table>
<tableCaption confidence="0.999825">
Table 3: Performance of bilingual phrasal rules
</tableCaption>
<bodyText confidence="0.987305285714286">
1) As presented in Table 3, the effectiveness
of the bilingual phrases derived from non-
contiguous tree sequence pairs is clearly indicated.
Models adopting both tgtncBP and srcncBP sig-
nificantly (p &lt; 0.05) outperform the model adopt-
ing cBP only.
2) Pisces underperforms Moses when utiliz-
ing cBPs only, since Pisces can only perform mo-
notonic search with cBPs.
3) The bilingual phrase model with both
tgtncBP and srcncBP even outperforms Moses.
Compared with Moses, we only utilize plain fea-
tures in Pisces for the bilingual phrase model (Fea-
ture 1~5 for all phrases and additional 7, 8 only for
non-contiguous bilingual phrases as stated in Sec-
tion 2.2; None of the complex reordering features
or distortion features are employed by Pisces while
Moses uses them), which suggests the effective-
ness of the non-contiguous rules and the advantag-
es of the proposed decoding algorithm.
Table 4 studies the impact on performance when
setting different maximal gaps allowed for either
side in a tree sequence pair (parameter ) and the
relation with the quantity of rule set.
Significant improvement is achieved when al-
lowing at least one gap on either side compared
with when only allowing contiguous tree sequence
pairs. However, the further increment of gaps does
not benefit much. The result exhibits the accor-
dance with the growing amplitude of the rule set
filtered for the test set, in which the rule size in-
creases more slowly as the maximal number of
gaps increments. As a result, this slow increase
against the increment of gaps can be probably at-
tributed to the small augmentation of the effective
</bodyText>
<table confidence="0.996904166666667">
Rule # BLEU
1,661,045 25.92
+841,263 26.53
+447,161 26.55
+17,782 26.56
+8,223 26.57
</table>
<tableCaption confidence="0.9846335">
Table 4: Performance and rule size changing with
different maximal number of gaps
</tableCaption>
<figure confidence="0.993796714285714">
Max gaps allowed
source target
0 0
1 1
2 2
3 3
00
</figure>
<page confidence="0.951917">
920
</page>
<table confidence="0.9982946875">
Output &amp; References
Source /only /pass /null /five years /two people /null /confront at court
Reference after only five years the two confronted each other at court
STSSG only in the five years , the two candidates would
SncTSSG the two people can confront other countries at court leisurely manner only in the five years
key rules
VV( ) VB(confront)NP(JJ(other),NNS(countries))IN(at) NN(court) *** JJ(leisurely)NN(manner)
Source /Euro /&apos;s /substantial /appreciation /will /in /recent /&apos;s /survey /middle /continue
/for /economy /confidence /produce /impact
Reference substantial appreciation of the euro will continue to impact the economic confidence in the recent surveys
STSSG substantial appreciation of the euro has continued to have an impact on confidence in the economy , in the re-
cent surveys will
SncTSSG substantial appreciation of the euro will continue in the recent surveys have an impact on economic confidence
key rules
AD( ) *** VV( ) VP(1M(will),VB(continue))
P( ) *** LC( ) IN(in)
</table>
<tableCaption confidence="0.999775">
Table 5: Sample translations (tokens in italic match the reference provided)
</tableCaption>
<bodyText confidence="0.997909">
non-contiguous rules.
In order to facilitate a better intuition to the abil-
ity of the SncTSSG based model against the
STSSG based model, we present in Table 5, two
translation outputs produced by both models.
In the first example, GIZA++ wrongly aligns the
idiom word &amp;quot; /confront at court&amp;quot; to a non-
contiguous phrase &amp;quot;confront other countries at
court,***, leisurely manner&amp;quot; in training, in which
only the first constituent &amp;quot;confront other countries
at court&amp;quot; is reasonable, indicated from the key
rules of SncTSSG leant from the training set. The
STSSG or any contiguous translational equiva-
lence based model is unable to attain the corres-
ponding target output for this idiom word via the
non-contiguous word alignment and consider it as
an out-of-vocabulary (OOV). On the contrary, the
SncTSSG based model can capture the non-
contiguous tree sequence pair consistent with the
word alignment and further provide a reasonable
target translation. It suggests that SncTSSG can
easily capture the non-contiguous translational
candidates while STSSG cannot. Besides,
SncTSSG is less sensitive to the error of word
alignment when extracting the translation candi-
dates than the contiguous translational equivalence
based models.
In the second example, &amp;quot; /in /recent /&apos;s
/survey /middle&amp;quot; is correctly translated into &amp;quot;in
the recent surveys&amp;quot; by both the STSSG and
SncTSSG based models. This suggests that the
short non-contiguous phrase &amp;quot; /in *** /middle&amp;quot;
is well handled by both models. Nevertheless, as
for the one with a larger gap, &amp;quot; /will ***
/continue&amp;quot; is correctly translated and well reorder-
ing into &amp;quot;will continue&amp;quot; by SncTSSG but failed by
STSSG. Although the STSSG is theoretically able
to capture this phrase from the contiguous tree se-
quence pair, the richer context in the gap as in this
example, the more difficult STSSG can correctly
translate the non-contiguous phrases. This exhibits
the flexibility of SncTSSG to the rich context
among the non-contiguous constituents.
</bodyText>
<sectionHeader confidence="0.998496" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99991928">
In this paper, we present a non-contiguous tree se-
quence alignment model based on SncTSSG to
enhance the ability of non-contiguous phrase mod-
eling and the reordering caused by non-contiguous
constituents with large gaps. A three-phase decod-
ing algorithm is developed to facilitate the usage of
non-contiguous translational equivalences (tree
sequence pairs in this work) which provides much
flexibility for the reordering of the non-contiguous
constituents with rich syntactic structural informa-
tion. The experimental results show that our model
outperforms the baseline models and verify the
effectiveness of non-contiguous translational equi-
valences to non-contiguous phrase modeling in
both syntax-based and phrase-based systems. We
also find that in Chinese-English translation task,
gaps are more effective in Chinese side than in the
English side.
Although the characteristic of more sensitive-
ness to word alignment error enables SncTSSG to
capture the additional non-contiguous language
phenomenon, it also induces many redundant non-
contiguous rules. Therefore, further work of our
studies includes the optimization of the large rule
set of the SncTSSG based model.
</bodyText>
<page confidence="0.991411">
921
</page>
<bodyText confidence="0.743656">
S. Shieber. 2004. Synchronous grammars as tree trans-
ducers. In Proceedings of the Seventh International
Workshop on Tree Adjoining Grammar and Related
Formalisms
</bodyText>
<sectionHeader confidence="0.66393" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.978054118421053">
Rens Bod. 2007. Unsupervised Syntax-Based Machine
Translation: The Contribution of Discontinuous
Phrases. MT-Summmit-07. 51-56.
David Chiang. 2006. An Introduction to Synchronous
Grammars. Tutorial on ACL-06
Yuan Ding and Martha Palmer. 2005. Machine transla-
tion using probabilistic synchronous dependency in-
sert grammars. ACL-05. 541-548
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. ACL-03.
Michel Galley, J. Graehl, K. Knight, D. Marcu, S. De-
Neefe, W. Wang and I. Thayer. 2006. Scalable Infe-
rence and training of context-rich syntactic transla-
tion models. COLING-ACL-06. 961-968
Daniel Gildea. 2003. Loosely Tree-Based Alignment for
Machine Translation. ACL-03. 80-87.
Mary Hearne and Andy Way. 2003. Seeing the wood
for the trees: data-oriented translation. MT Summit
IX, 165-172.
Dan Klein and Christopher D. Manning. 2003. Accurate
Unlexicalized Parsing. ACL-03. 423-430.
Philipp Koehn, Franz J. Och and Daniel Marcu. 2003.
Statistical phrase-based translation. HLT-NAACL-
03. 127-133
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Ri-
chard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
ACL-07. 77-180.
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine
Translation. ACL-06, 609-616
Yang Liu, Yun Huang, Qun Liu and Shouxun Lin.
2007. Forest-to-String Statistical Translation Rules.
ACL-07. 704-711.
Daniel Marcu and William Wong. 2002. A phrase-
based, joint probability model for statistical machine
translation. EMNLP-02, 133-139
Daniel Marcu, W. Wang, A. Echihabi and K. Knight.
2006. SPMT: statistical machine translation with syn-
tactified target language phrases. EMNLP-06. 44-52.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417-449
Kishore Papineni, Salim Roukos, ToddWard and WeiJ-
ing Zhu. 2002. BLEU: a method for automatic evalu-
ation of machine translation. ACL-02. 311-318.
Chris Quirk, Arul Menezes and Colin Cherry. 2005.
Dependency treelet translation: syntactically in-
formed phrasal SMT. ACL-05. 271-279.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. ICSLP-02. 901-904.
Benjamin Wellington, Sonjia Waxmonsky and I. Dan
Melamed. 2006. Empirical Lower Bounds on the
Complexity of Translational Equivalence. ACL-06.
977-984
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. ACL-01. 523-530
Min Zhang, Hongfei Jiang, AiTi Aw, Jun Sun, Sheng Li
and Chew Lim Tan. 2007. A tree-to-tree alignment-
based model for statistical machine translation. MT-
Summit-07. 535-542.
Min Zhang, Hongfei Jiang, AiTi Aw, Haizhou Li, Chew
Lim Tan and Sheng Li. 2008a. A tree sequence
alignment-based tree-to-tree translation model. ACL-
08. 559-567.
Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw, Sheng
Li. 2008b. Grammar Comparison Study for Transla-
tional Equivalence Modeling and Statistical Machine
Translation. COLING-08. 1097-1104.
Ying Zhang. Stephan Vogel. Alex Waibel. 2004. Inter-
preting BLEU/NIST scores: How much improvement
do we need to have a better system? LREC-04. 2051-
2054.
</reference>
<page confidence="0.996915">
922
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.615348">
<title confidence="0.999516">A non-contiguous Tree Sequence Alignment-based Model for Statistical Machine Translation</title>
<author confidence="0.968875">Min Chew Lim</author>
<affiliation confidence="0.728043">for Infocomm Research of Computing, National University of Singapore</affiliation>
<email confidence="0.727448">sunjun@comp.nus.edu.sgmzhang@i2r.a-star.edu.sgtancl@comp.nus.edu.sg</email>
<abstract confidence="0.999615">The tree sequence based translation model allows the violation of syntactic boundaries in a rule to capture non-syntactic phrases, where a tree sequence is a contiguous sequence of subtrees. This paper goes further to present a translation model based on non-contiguous tree sequence alignment, where a non-contiguous tree sequence is a sequence of sub-trees and gaps. Compared with the contiguous tree sequencebased model, the proposed model can well handle non-contiguous phrases with any large gaps by means of non-contiguous tree sequence alignment. An algorithm targeting the noncontiguous constituent decoding is also proposed. Experimental results on the NIST MT-05 Chinese-English translation task show that the proposed model statistically significantly outperforms the baseline systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>Unsupervised Syntax-Based Machine Translation: The Contribution of Discontinuous Phrases.</title>
<date>2007</date>
<volume>07</volume>
<pages>51--56</pages>
<contexts>
<context position="1694" citStr="Bod, 2007" startWordPosition="246" endWordPosition="247">ems. 1 Introduction Current research in statistical machine translation (SMT) mostly settles itself in the domain of either phrase-based or syntax-based. Between them, the phrase-based approach (Marcu and Wong, 2002; Koehn et al, 2003; Och and Ney, 2004) allows local reordering and contiguous phrase translation. However, it is hard for phrase-based models to learn global reorderings and to deal with noncontiguous phrases. To address this issue, many syntax-based approaches (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Zhang et al, 2007, 2008a; Bod, 2007; Liu et al, 2006, 2007; Hearne and Way, 2003) tend to integrate more syntactic information to enhance the non-contiguous phrase modeling. In general, most of them achieve this goal by introducing syntactic non-terminals as translational equivalent placeholders in both source and target sides. Nevertheless, the generated rules are strictly required to be derived from the contiguous translational equivalences (Galley et al, 2006; Marcu et al, 2006; Zhang et al, 2007, 2008a, 2008b; Liu et al, 2006, 2007). Among them, Zhang et al. (2008a) acquire the non-contiguous phrasal rules from the contiguo</context>
</contexts>
<marker>Bod, 2007</marker>
<rawString>Rens Bod. 2007. Unsupervised Syntax-Based Machine Translation: The Contribution of Discontinuous Phrases. MT-Summmit-07. 51-56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>An Introduction to Synchronous Grammars.</title>
<date>2006</date>
<note>Tutorial on ACL-06</note>
<contexts>
<context position="6674" citStr="Chiang, 2006" startWordPosition="1061" endWordPosition="1062"> of context and enhance the non-contiguous constituent modeling. As for the above example, the proposed model enables the non-contiguous tree sequence pair indexed as TSP5 in Fig. 1 and is allowed to further derive r2 from TSP5. By means of r2 and the same processing to the contiguous phrase &amp;quot; /he /show up /&apos;s&amp;quot; as the contiguous tree sequence based model, we can successfully translate the entire source sentence in Fig. 1(b). We define a synchronous grammar, named Synchronous non-contiguous Tree Sequence Substitution Grammar (SncTSSG), extended from synchronous tree substitution grammar (STSG: Chiang, 2006) to illustrate our model. The proposed synchronous grammar is able to cover the previous proposed grammar based on tree (STSG, Eisner, 2003; Zhang et al, 2007) and tree sequence (STSSG, Zhang et al, 2008a) alignment. Besides, we modify the traditional parsing based decoding algorithm for syntax-based SMT to facilitate the non-contiguous constituent decoding for our model. To the best of our knowledge, this is the first attempt to acquire the translation rules with rich syntactic structures from the non-contiguous Translational Equivalences (non-contiguous tree sequence pairs in this context). </context>
</contexts>
<marker>Chiang, 2006</marker>
<rawString>David Chiang. 2006. An Introduction to Synchronous Grammars. Tutorial on ACL-06</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Ding</author>
<author>Martha Palmer</author>
</authors>
<title>Machine translation using probabilistic synchronous dependency insert grammars.</title>
<date>2005</date>
<pages>05--541</pages>
<contexts>
<context position="1638" citStr="Ding and Palmer, 2005" startWordPosition="233" endWordPosition="236">osed model statistically significantly outperforms the baseline systems. 1 Introduction Current research in statistical machine translation (SMT) mostly settles itself in the domain of either phrase-based or syntax-based. Between them, the phrase-based approach (Marcu and Wong, 2002; Koehn et al, 2003; Och and Ney, 2004) allows local reordering and contiguous phrase translation. However, it is hard for phrase-based models to learn global reorderings and to deal with noncontiguous phrases. To address this issue, many syntax-based approaches (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Zhang et al, 2007, 2008a; Bod, 2007; Liu et al, 2006, 2007; Hearne and Way, 2003) tend to integrate more syntactic information to enhance the non-contiguous phrase modeling. In general, most of them achieve this goal by introducing syntactic non-terminals as translational equivalent placeholders in both source and target sides. Nevertheless, the generated rules are strictly required to be derived from the contiguous translational equivalences (Galley et al, 2006; Marcu et al, 2006; Zhang et al, 2007, 2008a, 2008b; Liu et al, 2006, 2007). Among them, Zhang et al. (2008a) ac</context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Yuan Ding and Martha Palmer. 2005. Machine translation using probabilistic synchronous dependency insert grammars. ACL-05. 541-548</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Learning non-isomorphic tree mappings for machine translation.</title>
<date>2003</date>
<pages>03</pages>
<contexts>
<context position="1601" citStr="Eisner, 2003" startWordPosition="229" endWordPosition="230">tion task show that the proposed model statistically significantly outperforms the baseline systems. 1 Introduction Current research in statistical machine translation (SMT) mostly settles itself in the domain of either phrase-based or syntax-based. Between them, the phrase-based approach (Marcu and Wong, 2002; Koehn et al, 2003; Och and Ney, 2004) allows local reordering and contiguous phrase translation. However, it is hard for phrase-based models to learn global reorderings and to deal with noncontiguous phrases. To address this issue, many syntax-based approaches (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Zhang et al, 2007, 2008a; Bod, 2007; Liu et al, 2006, 2007; Hearne and Way, 2003) tend to integrate more syntactic information to enhance the non-contiguous phrase modeling. In general, most of them achieve this goal by introducing syntactic non-terminals as translational equivalent placeholders in both source and target sides. Nevertheless, the generated rules are strictly required to be derived from the contiguous translational equivalences (Galley et al, 2006; Marcu et al, 2006; Zhang et al, 2007, 2008a, 2008b; Liu et al, 2006, 2007)</context>
<context position="6813" citStr="Eisner, 2003" startWordPosition="1084" endWordPosition="1085">e sequence pair indexed as TSP5 in Fig. 1 and is allowed to further derive r2 from TSP5. By means of r2 and the same processing to the contiguous phrase &amp;quot; /he /show up /&apos;s&amp;quot; as the contiguous tree sequence based model, we can successfully translate the entire source sentence in Fig. 1(b). We define a synchronous grammar, named Synchronous non-contiguous Tree Sequence Substitution Grammar (SncTSSG), extended from synchronous tree substitution grammar (STSG: Chiang, 2006) to illustrate our model. The proposed synchronous grammar is able to cover the previous proposed grammar based on tree (STSG, Eisner, 2003; Zhang et al, 2007) and tree sequence (STSSG, Zhang et al, 2008a) alignment. Besides, we modify the traditional parsing based decoding algorithm for syntax-based SMT to facilitate the non-contiguous constituent decoding for our model. To the best of our knowledge, this is the first attempt to acquire the translation rules with rich syntactic structures from the non-contiguous Translational Equivalences (non-contiguous tree sequence pairs in this context). The rest of this paper is organized as follows: Section 2 presents a formal definition of our model with detailed parameterization. Section</context>
</contexts>
<marker>Eisner, 2003</marker>
<rawString>Jason Eisner. 2003. Learning non-isomorphic tree mappings for machine translation. ACL-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>J Graehl</author>
<author>K Knight</author>
<author>D Marcu</author>
<author>S DeNeefe</author>
<author>W Wang</author>
<author>I Thayer</author>
</authors>
<title>Scalable Inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<pages>06--961</pages>
<contexts>
<context position="2125" citStr="Galley et al, 2006" startWordPosition="308" endWordPosition="311">es. To address this issue, many syntax-based approaches (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Zhang et al, 2007, 2008a; Bod, 2007; Liu et al, 2006, 2007; Hearne and Way, 2003) tend to integrate more syntactic information to enhance the non-contiguous phrase modeling. In general, most of them achieve this goal by introducing syntactic non-terminals as translational equivalent placeholders in both source and target sides. Nevertheless, the generated rules are strictly required to be derived from the contiguous translational equivalences (Galley et al, 2006; Marcu et al, 2006; Zhang et al, 2007, 2008a, 2008b; Liu et al, 2006, 2007). Among them, Zhang et al. (2008a) acquire the non-contiguous phrasal rules from the contiguous tree sequence pairs1, and find them useless via real syntax-based translation systems. However, Wellington et al. (2006) statistically report that discontinuities are very useful for translational equivalence analysis using binary branching structures under word alignment and parse tree constraints. Bod (2007) also finds that discontinues phrasal rules make significant improvement in linguistically motivated STSG-based trans</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe, W. Wang and I. Thayer. 2006. Scalable Inference and training of context-rich syntactic translation models. COLING-ACL-06. 961-968</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Loosely Tree-Based Alignment for Machine Translation.</title>
<date>2003</date>
<volume>03</volume>
<pages>80--87</pages>
<contexts>
<context position="1615" citStr="Gildea, 2003" startWordPosition="231" endWordPosition="232"> that the proposed model statistically significantly outperforms the baseline systems. 1 Introduction Current research in statistical machine translation (SMT) mostly settles itself in the domain of either phrase-based or syntax-based. Between them, the phrase-based approach (Marcu and Wong, 2002; Koehn et al, 2003; Och and Ney, 2004) allows local reordering and contiguous phrase translation. However, it is hard for phrase-based models to learn global reorderings and to deal with noncontiguous phrases. To address this issue, many syntax-based approaches (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Zhang et al, 2007, 2008a; Bod, 2007; Liu et al, 2006, 2007; Hearne and Way, 2003) tend to integrate more syntactic information to enhance the non-contiguous phrase modeling. In general, most of them achieve this goal by introducing syntactic non-terminals as translational equivalent placeholders in both source and target sides. Nevertheless, the generated rules are strictly required to be derived from the contiguous translational equivalences (Galley et al, 2006; Marcu et al, 2006; Zhang et al, 2007, 2008a, 2008b; Liu et al, 2006, 2007). Among them, </context>
</contexts>
<marker>Gildea, 2003</marker>
<rawString>Daniel Gildea. 2003. Loosely Tree-Based Alignment for Machine Translation. ACL-03. 80-87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Hearne</author>
<author>Andy Way</author>
</authors>
<title>Seeing the wood for the trees: data-oriented translation.</title>
<date>2003</date>
<booktitle>MT Summit IX,</booktitle>
<pages>165--172</pages>
<contexts>
<context position="1740" citStr="Hearne and Way, 2003" startWordPosition="253" endWordPosition="256">ch in statistical machine translation (SMT) mostly settles itself in the domain of either phrase-based or syntax-based. Between them, the phrase-based approach (Marcu and Wong, 2002; Koehn et al, 2003; Och and Ney, 2004) allows local reordering and contiguous phrase translation. However, it is hard for phrase-based models to learn global reorderings and to deal with noncontiguous phrases. To address this issue, many syntax-based approaches (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Zhang et al, 2007, 2008a; Bod, 2007; Liu et al, 2006, 2007; Hearne and Way, 2003) tend to integrate more syntactic information to enhance the non-contiguous phrase modeling. In general, most of them achieve this goal by introducing syntactic non-terminals as translational equivalent placeholders in both source and target sides. Nevertheless, the generated rules are strictly required to be derived from the contiguous translational equivalences (Galley et al, 2006; Marcu et al, 2006; Zhang et al, 2007, 2008a, 2008b; Liu et al, 2006, 2007). Among them, Zhang et al. (2008a) acquire the non-contiguous phrasal rules from the contiguous tree sequence pairs1, and find them useless</context>
</contexts>
<marker>Hearne, Way, 2003</marker>
<rawString>Mary Hearne and Andy Way. 2003. Seeing the wood for the trees: data-oriented translation. MT Summit IX, 165-172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<date>2003</date>
<booktitle>Accurate Unlexicalized Parsing. ACL-03.</booktitle>
<pages>423--430</pages>
<contexts>
<context position="20755" citStr="Klein and Manning, 2003" startWordPosition="3407" endWordPosition="3410">guage model, we only compute for source side contiguous translational hypothesis, while neglecting gaps in the target side if any. 5 Experiments 5.1 Experimental Settings In the experiments, we train the translation model on FBIS corpus (7.2M (Chinese) + 9.2M (English) words) and train a 4-gram language model on the Xinhua portion of the English Gigaword corpus (181M words) using the SRILM Toolkits (Stolcke, 2002). We use these sentences with less than 50 characters from the NIST MT-2002 test set as the development set and the NIST MT-2005 test set as our test set. We use the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test set. The evaluation metric is casesensitive BLEU-4 (Papineni et al., 2002). We base on the m-to-n word alignments dumped by GIZA++ to extract the tree sequence pairs. For the MER training, we modify Koehn&apos;s version (Koehn, 2004). We use Zhang et al&apos;s implementation (Zhang et al, 2004) for 95% confidence intervals significant test. We compare the SncTSSG based model against two baseline models: the phrase-based and the STSSG-based models. For the phrase-based model, we use Moses (Koehn et al, 200</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. ACL-03. 423-430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<pages>03--127</pages>
<contexts>
<context position="1319" citStr="Koehn et al, 2003" startWordPosition="184" endWordPosition="187">cebased model, the proposed model can well handle non-contiguous phrases with any large gaps by means of non-contiguous tree sequence alignment. An algorithm targeting the noncontiguous constituent decoding is also proposed. Experimental results on the NIST MT-05 Chinese-English translation task show that the proposed model statistically significantly outperforms the baseline systems. 1 Introduction Current research in statistical machine translation (SMT) mostly settles itself in the domain of either phrase-based or syntax-based. Between them, the phrase-based approach (Marcu and Wong, 2002; Koehn et al, 2003; Och and Ney, 2004) allows local reordering and contiguous phrase translation. However, it is hard for phrase-based models to learn global reorderings and to deal with noncontiguous phrases. To address this issue, many syntax-based approaches (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Zhang et al, 2007, 2008a; Bod, 2007; Liu et al, 2006, 2007; Hearne and Way, 2003) tend to integrate more syntactic information to enhance the non-contiguous phrase modeling. In general, most of them achieve this goal by introducing syntactic non-terminals as t</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och and Daniel Marcu. 2003. Statistical phrase-based translation. HLT-NAACL03. 127-133</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<volume>07</volume>
<pages>77--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="21357" citStr="Koehn et al, 2007" startWordPosition="3506" endWordPosition="3509">d Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test set. The evaluation metric is casesensitive BLEU-4 (Papineni et al., 2002). We base on the m-to-n word alignments dumped by GIZA++ to extract the tree sequence pairs. For the MER training, we modify Koehn&apos;s version (Koehn, 2004). We use Zhang et al&apos;s implementation (Zhang et al, 2004) for 95% confidence intervals significant test. We compare the SncTSSG based model against two baseline models: the phrase-based and the STSSG-based models. For the phrase-based model, we use Moses (Koehn et al, 2007) with its default settings; for the STSSG and SncTSSG based models we use our decoder Pisces by setting the following parameters: , , , , , . Additionally, for STSSG we set , and for SncTSSG, we set . 5.2 Experimental Results Table 1 compares the performance of different models across the two systems. The proposed SncTSSG based model significantly outperforms (p &lt; 0.05) the two baseline models. Since the SncTSSG based model covers the STSSG based model in its modeling ability and obtains a superset in rules, the improvement empirically verifies the effectiveness of the additional non-contiguou</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. ACL-07. 77-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-toString Alignment Template for Statistical Machine Translation.</title>
<date>2006</date>
<volume>06</volume>
<pages>609--616</pages>
<contexts>
<context position="1711" citStr="Liu et al, 2006" startWordPosition="248" endWordPosition="251">oduction Current research in statistical machine translation (SMT) mostly settles itself in the domain of either phrase-based or syntax-based. Between them, the phrase-based approach (Marcu and Wong, 2002; Koehn et al, 2003; Och and Ney, 2004) allows local reordering and contiguous phrase translation. However, it is hard for phrase-based models to learn global reorderings and to deal with noncontiguous phrases. To address this issue, many syntax-based approaches (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Zhang et al, 2007, 2008a; Bod, 2007; Liu et al, 2006, 2007; Hearne and Way, 2003) tend to integrate more syntactic information to enhance the non-contiguous phrase modeling. In general, most of them achieve this goal by introducing syntactic non-terminals as translational equivalent placeholders in both source and target sides. Nevertheless, the generated rules are strictly required to be derived from the contiguous translational equivalences (Galley et al, 2006; Marcu et al, 2006; Zhang et al, 2007, 2008a, 2008b; Liu et al, 2006, 2007). Among them, Zhang et al. (2008a) acquire the non-contiguous phrasal rules from the contiguous tree sequence </context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-toString Alignment Template for Statistical Machine Translation. ACL-06, 609-616</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yun Huang</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Forest-to-String Statistical Translation Rules.</title>
<date>2007</date>
<volume>07</volume>
<pages>704--711</pages>
<marker>Liu, Huang, Liu, Lin, 2007</marker>
<rawString>Yang Liu, Yun Huang, Qun Liu and Shouxun Lin. 2007. Forest-to-String Statistical Translation Rules. ACL-07. 704-711.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>William Wong</author>
</authors>
<title>A phrasebased, joint probability model for statistical machine translation.</title>
<date>2002</date>
<volume>02</volume>
<pages>133--139</pages>
<contexts>
<context position="1300" citStr="Marcu and Wong, 2002" startWordPosition="180" endWordPosition="183">contiguous tree sequencebased model, the proposed model can well handle non-contiguous phrases with any large gaps by means of non-contiguous tree sequence alignment. An algorithm targeting the noncontiguous constituent decoding is also proposed. Experimental results on the NIST MT-05 Chinese-English translation task show that the proposed model statistically significantly outperforms the baseline systems. 1 Introduction Current research in statistical machine translation (SMT) mostly settles itself in the domain of either phrase-based or syntax-based. Between them, the phrase-based approach (Marcu and Wong, 2002; Koehn et al, 2003; Och and Ney, 2004) allows local reordering and contiguous phrase translation. However, it is hard for phrase-based models to learn global reorderings and to deal with noncontiguous phrases. To address this issue, many syntax-based approaches (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Zhang et al, 2007, 2008a; Bod, 2007; Liu et al, 2006, 2007; Hearne and Way, 2003) tend to integrate more syntactic information to enhance the non-contiguous phrase modeling. In general, most of them achieve this goal by introducing syntactic</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Daniel Marcu and William Wong. 2002. A phrasebased, joint probability model for statistical machine translation. EMNLP-02, 133-139</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>W Wang</author>
<author>A Echihabi</author>
<author>K Knight</author>
</authors>
<title>SPMT: statistical machine translation with syntactified target language phrases.</title>
<date>2006</date>
<pages>06--44</pages>
<contexts>
<context position="2144" citStr="Marcu et al, 2006" startWordPosition="312" endWordPosition="315">issue, many syntax-based approaches (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Zhang et al, 2007, 2008a; Bod, 2007; Liu et al, 2006, 2007; Hearne and Way, 2003) tend to integrate more syntactic information to enhance the non-contiguous phrase modeling. In general, most of them achieve this goal by introducing syntactic non-terminals as translational equivalent placeholders in both source and target sides. Nevertheless, the generated rules are strictly required to be derived from the contiguous translational equivalences (Galley et al, 2006; Marcu et al, 2006; Zhang et al, 2007, 2008a, 2008b; Liu et al, 2006, 2007). Among them, Zhang et al. (2008a) acquire the non-contiguous phrasal rules from the contiguous tree sequence pairs1, and find them useless via real syntax-based translation systems. However, Wellington et al. (2006) statistically report that discontinuities are very useful for translational equivalence analysis using binary branching structures under word alignment and parse tree constraints. Bod (2007) also finds that discontinues phrasal rules make significant improvement in linguistically motivated STSG-based translation model. The a</context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>Daniel Marcu, W. Wang, A. Echihabi and K. Knight. 2006. SPMT: statistical machine translation with syntactified target language phrases. EMNLP-06. 44-52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<pages>30--4</pages>
<contexts>
<context position="1339" citStr="Och and Ney, 2004" startWordPosition="188" endWordPosition="191">proposed model can well handle non-contiguous phrases with any large gaps by means of non-contiguous tree sequence alignment. An algorithm targeting the noncontiguous constituent decoding is also proposed. Experimental results on the NIST MT-05 Chinese-English translation task show that the proposed model statistically significantly outperforms the baseline systems. 1 Introduction Current research in statistical machine translation (SMT) mostly settles itself in the domain of either phrase-based or syntax-based. Between them, the phrase-based approach (Marcu and Wong, 2002; Koehn et al, 2003; Och and Ney, 2004) allows local reordering and contiguous phrase translation. However, it is hard for phrase-based models to learn global reorderings and to deal with noncontiguous phrases. To address this issue, many syntax-based approaches (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Zhang et al, 2007, 2008a; Bod, 2007; Liu et al, 2006, 2007; Hearne and Way, 2003) tend to integrate more syntactic information to enhance the non-contiguous phrase modeling. In general, most of them achieve this goal by introducing syntactic non-terminals as translational equival</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz J. Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417-449</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>ToddWard</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<pages>02--311</pages>
<contexts>
<context position="20929" citStr="Papineni et al., 2002" startWordPosition="3436" endWordPosition="3439">e experiments, we train the translation model on FBIS corpus (7.2M (Chinese) + 9.2M (English) words) and train a 4-gram language model on the Xinhua portion of the English Gigaword corpus (181M words) using the SRILM Toolkits (Stolcke, 2002). We use these sentences with less than 50 characters from the NIST MT-2002 test set as the development set and the NIST MT-2005 test set as our test set. We use the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test set. The evaluation metric is casesensitive BLEU-4 (Papineni et al., 2002). We base on the m-to-n word alignments dumped by GIZA++ to extract the tree sequence pairs. For the MER training, we modify Koehn&apos;s version (Koehn, 2004). We use Zhang et al&apos;s implementation (Zhang et al, 2004) for 95% confidence intervals significant test. We compare the SncTSSG based model against two baseline models: the phrase-based and the STSSG-based models. For the phrase-based model, we use Moses (Koehn et al, 2007) with its default settings; for the STSSG and SncTSSG based models we use our decoder Pisces by setting the following parameters: , , , , , . Additionally, for STSSG we set</context>
</contexts>
<marker>Papineni, Roukos, ToddWard, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, ToddWard and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. ACL-02. 311-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: syntactically informed phrasal SMT.</title>
<date>2005</date>
<pages>05--271</pages>
<contexts>
<context position="1657" citStr="Quirk et al, 2005" startWordPosition="237" endWordPosition="240">y significantly outperforms the baseline systems. 1 Introduction Current research in statistical machine translation (SMT) mostly settles itself in the domain of either phrase-based or syntax-based. Between them, the phrase-based approach (Marcu and Wong, 2002; Koehn et al, 2003; Och and Ney, 2004) allows local reordering and contiguous phrase translation. However, it is hard for phrase-based models to learn global reorderings and to deal with noncontiguous phrases. To address this issue, many syntax-based approaches (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Zhang et al, 2007, 2008a; Bod, 2007; Liu et al, 2006, 2007; Hearne and Way, 2003) tend to integrate more syntactic information to enhance the non-contiguous phrase modeling. In general, most of them achieve this goal by introducing syntactic non-terminals as translational equivalent placeholders in both source and target sides. Nevertheless, the generated rules are strictly required to be derived from the contiguous translational equivalences (Galley et al, 2006; Marcu et al, 2006; Zhang et al, 2007, 2008a, 2008b; Liu et al, 2006, 2007). Among them, Zhang et al. (2008a) acquire the non-conti</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes and Colin Cherry. 2005. Dependency treelet translation: syntactically informed phrasal SMT. ACL-05. 271-279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<pages>02--901</pages>
<contexts>
<context position="20548" citStr="Stolcke, 2002" startWordPosition="3371" endWordPosition="3372">l number of the rules in a source span is no greater than . The maximal number of translation candidates for a source span is no greater than . On the other hand, to simplify the computation of language model, we only compute for source side contiguous translational hypothesis, while neglecting gaps in the target side if any. 5 Experiments 5.1 Experimental Settings In the experiments, we train the translation model on FBIS corpus (7.2M (Chinese) + 9.2M (English) words) and train a 4-gram language model on the Xinhua portion of the English Gigaword corpus (181M words) using the SRILM Toolkits (Stolcke, 2002). We use these sentences with less than 50 characters from the NIST MT-2002 test set as the development set and the NIST MT-2005 test set as our test set. We use the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test set. The evaluation metric is casesensitive BLEU-4 (Papineni et al., 2002). We base on the m-to-n word alignments dumped by GIZA++ to extract the tree sequence pairs. For the MER training, we modify Koehn&apos;s version (Koehn, 2004). We use Zhang et al&apos;s implementation (Zhang et al, 2004) for 95%</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. ICSLP-02. 901-904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Wellington</author>
<author>Sonjia Waxmonsky</author>
<author>I Dan Melamed</author>
</authors>
<date>2006</date>
<booktitle>Empirical Lower Bounds on the Complexity of Translational Equivalence. ACL-06.</booktitle>
<pages>977--984</pages>
<contexts>
<context position="2417" citStr="Wellington et al. (2006)" startWordPosition="355" endWordPosition="358">nce the non-contiguous phrase modeling. In general, most of them achieve this goal by introducing syntactic non-terminals as translational equivalent placeholders in both source and target sides. Nevertheless, the generated rules are strictly required to be derived from the contiguous translational equivalences (Galley et al, 2006; Marcu et al, 2006; Zhang et al, 2007, 2008a, 2008b; Liu et al, 2006, 2007). Among them, Zhang et al. (2008a) acquire the non-contiguous phrasal rules from the contiguous tree sequence pairs1, and find them useless via real syntax-based translation systems. However, Wellington et al. (2006) statistically report that discontinuities are very useful for translational equivalence analysis using binary branching structures under word alignment and parse tree constraints. Bod (2007) also finds that discontinues phrasal rules make significant improvement in linguistically motivated STSG-based translation model. The above observations are conflicting to each other. In our opinion, the non-contiguous phrasal rules themselves may not play a trivial role, as reported in Zhang et al. (2008a). We believe that the effectiveness of non-contiguous phrasal rules highly depends on how to extract</context>
<context position="14426" citStr="Wellington et al. (2006)" startWordPosition="2331" endWordPosition="2334"> side. In other words, we only allow one side to be noncontiguous (either source or target side) to partially reserve its syntactic and semantic cohesion4. We further design a two-phase algorithm to extract the tree sequence pairs as described in Algorithm 1. For the first phase (line 1-11), we extract the contiguous tree sequence pairs (line 3-5) and the non-contiguous ones with contiguous tree sequence in the source side (line 6-9). In the second phase (line 12-19), the ones with contiguous tree sequence in the target side and non-contiguous tree sequence on the source side are extracted. 4 Wellington et al. (2006) also reports that allowing gaps in one side only is enough to eliminate the hierarchical alignment failure with word alignment and one side parse tree constraints. This is a particular case of our definition of non-contiguous tree sequence pair since a non-contiguous tree sequence can be considered to overcome the structural constraint by neglecting the structural information in the gaps. 917 Figure 5: Illustration of &amp;quot;Source gap insertion&amp;quot; The extracted tree sequence pairs are then utilized to derive the translation rules. In fact, both the contiguous and non-contiguous tree sequence pairs t</context>
<context position="24530" citStr="Wellington et al. (2006)" startWordPosition="4021" endWordPosition="4025">t only that, after comparing Exp 6,7,8 against Exp 3,4,5 respectively, we find that the ability of rules derived from non-contiguous tree sequence pairs generally covers that of the rules derived from the contiguous tree sequence pairs, due to the slight change in BLEU score. 3) The further comparison of the noncontiguous rules from non-contiguous spans in Exp. 6&amp;7 as well as Exp 3&amp;4, shows that noncontiguity in the target side in Chinese-English translation task is not so useful as that in the source side when constructing the non-contiguous phrasal rules. This also validates the findings in Wellington et al. (2006) that varying the gaps on the English side (the target side in this context) seldom reduce the hierarchical alignment failures. Table 3 explores the contribution of the noncontiguous translational equivalence to phrasebased models (all the rules in Table 3 has no grammar tags, but a gap &lt;***&gt; is allowed in the last three rows). tgtncBP refers to the bilingual phrases with gaps in the target side; srcncBP refers to the bilingual phrases with gaps in the source side; src&amp;tgtncBP refers to the bilingual phrases with gaps in either side. System Rule Set BLEU Moses cBP 23.86 cBP 22.63 Pisces cBP + </context>
</contexts>
<marker>Wellington, Waxmonsky, Melamed, 2006</marker>
<rawString>Benjamin Wellington, Sonjia Waxmonsky and I. Dan Melamed. 2006. Empirical Lower Bounds on the Complexity of Translational Equivalence. ACL-06. 977-984</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<pages>01--523</pages>
<contexts>
<context position="1587" citStr="Yamada and Knight, 2001" startWordPosition="225" endWordPosition="228">5 Chinese-English translation task show that the proposed model statistically significantly outperforms the baseline systems. 1 Introduction Current research in statistical machine translation (SMT) mostly settles itself in the domain of either phrase-based or syntax-based. Between them, the phrase-based approach (Marcu and Wong, 2002; Koehn et al, 2003; Och and Ney, 2004) allows local reordering and contiguous phrase translation. However, it is hard for phrase-based models to learn global reorderings and to deal with noncontiguous phrases. To address this issue, many syntax-based approaches (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Zhang et al, 2007, 2008a; Bod, 2007; Liu et al, 2006, 2007; Hearne and Way, 2003) tend to integrate more syntactic information to enhance the non-contiguous phrase modeling. In general, most of them achieve this goal by introducing syntactic non-terminals as translational equivalent placeholders in both source and target sides. Nevertheless, the generated rules are strictly required to be derived from the contiguous translational equivalences (Galley et al, 2006; Marcu et al, 2006; Zhang et al, 2007, 2008a, 2008b; Liu et a</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. ACL-01. 523-530</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
<author>AiTi Aw</author>
<author>Jun Sun</author>
<author>Sheng Li</author>
<author>Chew Lim Tan</author>
</authors>
<title>A tree-to-tree alignmentbased model for statistical machine translation.</title>
<date>2007</date>
<pages>07--535</pages>
<contexts>
<context position="1676" citStr="Zhang et al, 2007" startWordPosition="241" endWordPosition="244">performs the baseline systems. 1 Introduction Current research in statistical machine translation (SMT) mostly settles itself in the domain of either phrase-based or syntax-based. Between them, the phrase-based approach (Marcu and Wong, 2002; Koehn et al, 2003; Och and Ney, 2004) allows local reordering and contiguous phrase translation. However, it is hard for phrase-based models to learn global reorderings and to deal with noncontiguous phrases. To address this issue, many syntax-based approaches (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Ding and Palmer, 2005; Quirk et al, 2005; Zhang et al, 2007, 2008a; Bod, 2007; Liu et al, 2006, 2007; Hearne and Way, 2003) tend to integrate more syntactic information to enhance the non-contiguous phrase modeling. In general, most of them achieve this goal by introducing syntactic non-terminals as translational equivalent placeholders in both source and target sides. Nevertheless, the generated rules are strictly required to be derived from the contiguous translational equivalences (Galley et al, 2006; Marcu et al, 2006; Zhang et al, 2007, 2008a, 2008b; Liu et al, 2006, 2007). Among them, Zhang et al. (2008a) acquire the non-contiguous phrasal rules</context>
<context position="6833" citStr="Zhang et al, 2007" startWordPosition="1086" endWordPosition="1089">r indexed as TSP5 in Fig. 1 and is allowed to further derive r2 from TSP5. By means of r2 and the same processing to the contiguous phrase &amp;quot; /he /show up /&apos;s&amp;quot; as the contiguous tree sequence based model, we can successfully translate the entire source sentence in Fig. 1(b). We define a synchronous grammar, named Synchronous non-contiguous Tree Sequence Substitution Grammar (SncTSSG), extended from synchronous tree substitution grammar (STSG: Chiang, 2006) to illustrate our model. The proposed synchronous grammar is able to cover the previous proposed grammar based on tree (STSG, Eisner, 2003; Zhang et al, 2007) and tree sequence (STSSG, Zhang et al, 2008a) alignment. Besides, we modify the traditional parsing based decoding algorithm for syntax-based SMT to facilitate the non-contiguous constituent decoding for our model. To the best of our knowledge, this is the first attempt to acquire the translation rules with rich syntactic structures from the non-contiguous Translational Equivalences (non-contiguous tree sequence pairs in this context). The rest of this paper is organized as follows: Section 2 presents a formal definition of our model with detailed parameterization. Sections 3 and 4 elaborate </context>
</contexts>
<marker>Zhang, Jiang, Aw, Sun, Li, Tan, 2007</marker>
<rawString>Min Zhang, Hongfei Jiang, AiTi Aw, Jun Sun, Sheng Li and Chew Lim Tan. 2007. A tree-to-tree alignmentbased model for statistical machine translation. MTSummit-07. 535-542.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
<author>AiTi Aw</author>
<author>Haizhou Li</author>
<author>Chew Lim Tan</author>
<author>Sheng Li</author>
</authors>
<title>A tree sequence alignment-based tree-to-tree translation model.</title>
<date>2008</date>
<pages>08--559</pages>
<contexts>
<context position="2233" citStr="Zhang et al. (2008" startWordPosition="329" endWordPosition="332">; Ding and Palmer, 2005; Quirk et al, 2005; Zhang et al, 2007, 2008a; Bod, 2007; Liu et al, 2006, 2007; Hearne and Way, 2003) tend to integrate more syntactic information to enhance the non-contiguous phrase modeling. In general, most of them achieve this goal by introducing syntactic non-terminals as translational equivalent placeholders in both source and target sides. Nevertheless, the generated rules are strictly required to be derived from the contiguous translational equivalences (Galley et al, 2006; Marcu et al, 2006; Zhang et al, 2007, 2008a, 2008b; Liu et al, 2006, 2007). Among them, Zhang et al. (2008a) acquire the non-contiguous phrasal rules from the contiguous tree sequence pairs1, and find them useless via real syntax-based translation systems. However, Wellington et al. (2006) statistically report that discontinuities are very useful for translational equivalence analysis using binary branching structures under word alignment and parse tree constraints. Bod (2007) also finds that discontinues phrasal rules make significant improvement in linguistically motivated STSG-based translation model. The above observations are conflicting to each other. In our opinion, the non-contiguous phras</context>
<context position="4258" citStr="Zhang et al, 2008" startWordPosition="660" endWordPosition="663">ontiguous phrase &amp;quot; /he /show up /&apos;s&amp;quot;. As for the non-contiguous phrase &amp;quot; /at, ***, /time&amp;quot;, the only related rule is r, derived from TSP4 and the entire tree pair. However, the source side of r, does not match the source tree structure of the test sentence. Therefore, we can only partially translate the illustrated test sentence with this training sample. 1 A tree sequence pair in this context is a kind of translational equivalence comprised of a pair of tree sequences. 2 We illustrate the rule extraction with an example from the tree-to-tree translation model based on tree sequence alignment (Zhang et al, 2008a) without losing of generality to most syntactic tree based models. 3 We only list the contiguous tree sequence pairs with one single sub-tree in both sides without losing of generality. 914 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 914–922, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP VP VP NP NP CP IP IP CP VV AS PN NN VV DEC NN VV DEC VV PN (at) (NULL) (he) (show up) (`s) (time) up when he shows WRB RP PRP VBZ VP S SBAR (at) (he) (show up) (&apos;s) (time) when he shows up WRB PRP VBZ RP VP S SBAR TSP1: PN( ) PRP(he) TSP2: VV( ) VP(VBZ(</context>
<context position="6877" citStr="Zhang et al, 2008" startWordPosition="1094" endWordPosition="1097"> further derive r2 from TSP5. By means of r2 and the same processing to the contiguous phrase &amp;quot; /he /show up /&apos;s&amp;quot; as the contiguous tree sequence based model, we can successfully translate the entire source sentence in Fig. 1(b). We define a synchronous grammar, named Synchronous non-contiguous Tree Sequence Substitution Grammar (SncTSSG), extended from synchronous tree substitution grammar (STSG: Chiang, 2006) to illustrate our model. The proposed synchronous grammar is able to cover the previous proposed grammar based on tree (STSG, Eisner, 2003; Zhang et al, 2007) and tree sequence (STSSG, Zhang et al, 2008a) alignment. Besides, we modify the traditional parsing based decoding algorithm for syntax-based SMT to facilitate the non-contiguous constituent decoding for our model. To the best of our knowledge, this is the first attempt to acquire the translation rules with rich syntactic structures from the non-contiguous Translational Equivalences (non-contiguous tree sequence pairs in this context). The rest of this paper is organized as follows: Section 2 presents a formal definition of our model with detailed parameterization. Sections 3 and 4 elaborate the extraction of the non-contiguous tree se</context>
<context position="15204" citStr="Zhang et al. (2008" startWordPosition="2454" endWordPosition="2457">nts. This is a particular case of our definition of non-contiguous tree sequence pair since a non-contiguous tree sequence can be considered to overcome the structural constraint by neglecting the structural information in the gaps. 917 Figure 5: Illustration of &amp;quot;Source gap insertion&amp;quot; The extracted tree sequence pairs are then utilized to derive the translation rules. In fact, both the contiguous and non-contiguous tree sequence pairs themselves are applicable translation rules; we denote these rules as Initial rules. By means of the Initial rules, we derive the Abstract rules similarly as in Zhang et al. (2008a). Additionally, we develop a few constraints to limit the number of Abstract rules. The depth of a tree in a rule is no greater than h. The number of non-terminals as leaf nodes is no greater than c. The tree number is no greater than d. Besides, the number of lexical words at leaf nodes in an Initial rule is no greater than l. The maximal number of gaps for a non-contiguous rule is no greater than . 4 The Pisces decoder We implement our decoder Pisces by simulating the span based CYK parser constrained by the rules of SncTSSG. The decoder translates each span iteratively in a bottom up mann</context>
<context position="22564" citStr="Zhang et al. (2008" startWordPosition="3699" endWordPosition="3702">n-contiguous rules. Model BLEU cBP 23.86 STSSG 25.92 SncTSSG 26.53 Table 1: Translation results of different models (cBP refers to contiguous bilingual phrases without syntactic structural information, as used in Moses) Table 2 measures the contribution of different combination of rules. cR refers to the rules derived from contiguous tree sequence pairs (i.e., all STSSG rules); ncPR refers to non-contiguous phrasal rules derived from contiguous tree sequence pairs with at least one non-terminal leaf node between two lexicalized leaf nodes (i.e., all non-contiguous rules in STSSG defined as in Zhang et al. (2008a)); srcncR refers to source side non-contiguous rules (SncTSSG rules only, not STSSG rules); tgtncR refers to target side noncontiguous rules (SncTSSG rules only, not STSSG rules) and src&amp;tgtncR refers non-contiguous rules System Moses Pisces 919 ID Rule Set BLEU 1 cR (STSSG) 25.92 2 cR w/o ncPR 25.87 3 cR w/o ncPR + tgtncR 26.14 4 cR w/o ncPR + srcncR 26.50 5 cR w/o ncPR + src&amp;tgtncR 26.51 6 cR + tgtncR 26.11 7 cR + srcncR 26.56 8 cR+src&amp;tgtncR(SncTSSG) 26.53 Table 2: Performance of different rule combination with gaps in either side (srcncR+ tgtncR). The last three kinds of rules are all de</context>
</contexts>
<marker>Zhang, Jiang, Aw, Li, Tan, Li, 2008</marker>
<rawString>Min Zhang, Hongfei Jiang, AiTi Aw, Haizhou Li, Chew Lim Tan and Sheng Li. 2008a. A tree sequence alignment-based tree-to-tree translation model. ACL08. 559-567.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
<author>Haizhou Li</author>
</authors>
<title>Aiti Aw, Sheng Li.</title>
<booktitle>2008b. Grammar Comparison Study for Translational Equivalence Modeling and Statistical Machine Translation. COLING-08.</booktitle>
<pages>1097--1104</pages>
<marker>Zhang, Jiang, Li, </marker>
<rawString>Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw, Sheng Li. 2008b. Grammar Comparison Study for Translational Equivalence Modeling and Statistical Machine Translation. COLING-08. 1097-1104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel Alex Waibel</author>
</authors>
<title>Interpreting BLEU/NIST scores: How much improvement do we need to have a better system?</title>
<date>2004</date>
<pages>04--2051</pages>
<marker>Waibel, 2004</marker>
<rawString>Ying Zhang. Stephan Vogel. Alex Waibel. 2004. Interpreting BLEU/NIST scores: How much improvement do we need to have a better system? LREC-04. 2051-2054.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>