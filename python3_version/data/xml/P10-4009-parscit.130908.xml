<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.025431">
<title confidence="0.966991">
Personalising speech-to-speech translation in the EMIME project
</title>
<author confidence="0.9029935">
Mikko Kurimo1†, William Byrne6, John Dines3, Philip N. Garner3, Matthew Gibson6,
Yong Guan5, Teemu Hirsim¨aki1, Reima Karhila1, Simon King2, Hui Liang3, Keiichiro
Oura4, Lakshmi Saheer3, Matt Shannon6, Sayaka Shiota4, Jilei Tian5, Keiichi Tokuda4,
Mirjam Wester2, Yi-Jian Wu4, Junichi Yamagishi2
</author>
<affiliation confidence="0.978919666666667">
1 Aalto University, Finland, 2 University of Edinburgh, UK, 3 Idiap Research Institute,
Switzerland, 4 Nagoya Institute of Technology, Japan, 5 Nokia Research Center Beijing, China,
6 University of Cambridge, UK
</affiliation>
<email confidence="0.966419">
†Corresponding author: Mikko.Kurimo@tkk.fi
</email>
<sectionHeader confidence="0.993062" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999947631578947">
In the EMIME project we have studied un-
supervised cross-lingual speaker adapta-
tion. We have employed an HMM statisti-
cal framework for both speech recognition
and synthesis which provides transfor-
mation mechanisms to adapt the synthe-
sized voice in TTS (text-to-speech) using
the recognized voice in ASR (automatic
speech recognition). An important ap-
plication for this research is personalised
speech-to-speech translation that will use
the voice of the speaker in the input lan-
guage to utter the translated sentences in
the output language. In mobile environ-
ments this enhances the users’ interaction
across language barriers by making the
output speech sound more like the origi-
nal speaker’s way of speaking, even if she
or he could not speak the output language.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9995484">
A mobile real-time speech-to-speech translation
(S2ST) device is one of the grand challenges in
natural language processing (NLP). It involves
several important NLP research areas: auto-
matic speech recognition (ASR), statistical ma-
chine translation (SMT) and speech synthesis, also
known as text-to-speech (TTS). In recent years
significant advance have also been made in rele-
vant technological devices: the size of powerful
computers has decreased to fit in a mobile phone
and fast WiFi and 3G networks have spread widely
to connect them to even more powerful computa-
tion servers. Several hand-held S2ST applications
and devices have already become available, for ex-
ample by IBM, Google or Jibbigo1, but there are
still serious limitations in vocabulary and language
selection and performance.
When an S2ST device is used in practical hu-
man interaction across a language barrier, one fea-
ture that is often missed is the personalization of
the output voice. Whoever speaks to the device in
what ever manner, the output voice always sounds
the same. Producing high-quality synthesis voices
is expensive and even if the system had many out-
put voices, it is hard to select one that would sound
like the input voice. There are many features in the
output voice that could raise the interaction expe-
rience to a much more natural level, for example,
emotions, speaking rate, loudness and the speaker
identity.
After the recent development in hidden Markov
model (HMM) based TTS, it has become possi-
ble to adapt the output voice using model trans-
formations that can be estimated from a small
number of speech samples. These techniques, for
instance the maximum likelihood linear regres-
sion (MLLR), are adopted from HMM-based ASR
where they are very powerful in fast adaptation of
speaker and recording environment characteristics
(Gales, 1998). Using hierarchical regression trees,
the TTS and ASR models can further be coupled
in a way that enables unsupervised TTS adaptation
(King et al., 2008). In unsupervised adaptation
samples are annotated by applying ASR. By elimi-
nating the need for human intervention it becomes
possible to perform voice adaptation for TTS in
almost real-time.
The target in the EMIME project2 is to study
unsupervised cross-lingual speaker adaptation for
S2ST systems. The first results of the project have
</bodyText>
<footnote confidence="0.9999945">
1http://www.jibbigo.com
2http://emime.org
</footnote>
<page confidence="0.986872">
48
</page>
<note confidence="0.606143">
Proceedings of the ACL 2010 System Demonstrations, pages 48–53,
Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999922">
been, for example, to bridge the gap between the
ASR and TTS (Dines et al., 2009), to improve
the baseline ASR (Hirsim¨aki et al., 2009) and
SMT (de Gispert et al., 2009) systems for mor-
phologically rich languages, and to develop robust
TTS (Yamagishi et al., 2010). The next step has
been preliminary experiments in intra-lingual and
cross-lingual speaker adaptation (Wu et al., 2008).
For cross-lingual adaptation several new methods
have been proposed for mapping the HMM states,
adaptation data and model transformations (Wu et
al., 2009).
In this presentation we can demonstrate the var-
ious new results in ASR, SMT and TTS. Even
though the project is still ongoing, we have an
initial version of mobile S2ST system and cross-
lingual speaker adaptation to show.
</bodyText>
<sectionHeader confidence="0.794377" genericHeader="method">
2 Baseline ASR, TTS and SMT systems
</sectionHeader>
<bodyText confidence="0.999965102564102">
The baseline ASR systems in the project are devel-
oped using the HTK toolkit (Young et al., 2001)
for Finnish, English, Mandarin and Japanese. The
systems can also utilize various real-time decoders
such as Julius (Kawahara et al., 2000), Juicer at
IDIAP and the TKK decoder (Hirsim¨aki et al.,
2006). The main structure of the baseline sys-
tems for each of the four languages is similar and
fairly standard and in line with most other state-of-
the-art large vocabulary ASR systems. Some spe-
cial flavors for have been added, such as the mor-
phological analysis for Finnish (Hirsim¨aki et al.,
2009). For speaker adaptation, the MLLR trans-
formation based on hierarchical regression classes
is included for all languages.
The baseline TTS systems in the project utilize
the HTS toolkit (Yamagishi et al., 2009) which
is built on top of the HTK framework. The
HMM-based TTS systems have been developed
for Finnish, English, Mandarin and Japanese. The
systems include an average voice model for each
language trained over hundreds of speakers taken
from standard ASR corpora, such as Speecon
(Iskra et al., 2002). Using speaker adaptation
transforms, thousands of new voices have been
created (Yamagishi et al., 2010) and new voices
can be added using a small number of either su-
pervised or unsupervised speech samples. Cross-
lingual adaptation is possible by creating a map-
ping between the HMM states in the input and the
output language (Wu et al., 2009).
Because the resources of the EMIME project
have been focused on ASR, TTS and speaker
adaptation, we aim at relying on existing solu-
tions for SMT as far as possible. New methods
have been studied concerning the morphologically
rich languages (de Gispert et al., 2009), but for the
S2ST system we are currently using Google trans-
late3.
</bodyText>
<sectionHeader confidence="0.943562" genericHeader="method">
3 Demonstrations to show
</sectionHeader>
<subsectionHeader confidence="0.999272">
3.1 Monolingual systems
</subsectionHeader>
<bodyText confidence="0.999900833333333">
In robust speech synthesis, a computer can learn
to speak in the desired way after processing only a
relatively small amount of training speech. The
training speech can even be a normal quality
recording outside the studio environment, where
the target speaker is speaking to a standard micro-
phone and the speech is not annotated. This differs
dramatically from conventional TTS, where build-
ing a new voice requires an hour or more careful
repetition of specially selected prompts recorded
in an anechoic chamber with high quality equip-
ment.
Robust TTS has recently become possible us-
ing the statistical HMM framework for both ASR
and TTS. This framework enables the use of ef-
ficient speaker adaptation transformations devel-
oped for ASR to be used also for the TTS mod-
els. Using large corpora collected for ASR, we can
train average voice models for both ASR and TTS.
The training data may include a small amount of
speech with poor coverage of phonetic contexts
from each single speaker, but by summing the ma-
terial over hundreds of speakers, we can obtain
sufficient models for an average speaker. Only a
small amount of adaptation data is then required to
create transformations for tuning the average voice
closer to the target voice.
In addition to the supervised adaptation us-
ing annotated speech, it is also possible to em-
ploy ASR to create annotations. This unsu-
pervised adaptation enables the system to use a
much broader selection of sources, for example,
recorded samples from the internet, to learn a new
voice.
The following systems will demonstrate the re-
sults of monolingual adaptation:
</bodyText>
<listItem confidence="0.792506">
1. In EMIME Voice cloning in Finnish and En-
glish the goal is that the users can clone their
own voice. The user will dictate for about
</listItem>
<footnote confidence="0.988332">
3http://translate.google.com
</footnote>
<page confidence="0.998454">
49
</page>
<figureCaption confidence="0.96599">
Figure 1: Geographical representation of HTS voices trained on ASR corpora for EMIME projects.
</figureCaption>
<bodyText confidence="0.993256655172414">
Blue markers show male speakers and red markers show female speakers. Available online via
http://www.emime.org/learn/speech-synthesis/listen/Examples-for-D2.1
10 minutes and then after half an hour of
processing time, the TTS system has trans-
formed the average model towards the user’s
voice and can speak with this voice. The
cloned voices may become especially valu-
able, for example, if a person’s voice is later
damaged in an accident or by a disease.
2. In EMIME Thousand voices map the goal is
to browse the world’s largest collection of
synthetic voices by using a world map in-
terface (Yamagishi et al., 2010). The user
can zoom in the world map and select any
voice, which are organized according to the
place of living of the adapted speaker, to ut-
ter the given sentence. This interactive ge-
ographical representation is shown in Figure
1. Each marker corresponds to an individual
speaker. Blue markers show male speakers
and red markers show female speakers. Some
markers are in arbitrary locations (in the cor-
rect country) because precise location infor-
mation is not available for all speakers. This
geographical representation, which includes
an interactive TTS demonstration of many of
the voices, is available from the URL pro-
vided. Clicking on a marker will play syn-
thetic speech from that speaker4. As well as
</bodyText>
<footnote confidence="0.812791">
4Currently the interactive mode supports English and
Spanish only. For other languages this only provides pre-
</footnote>
<bodyText confidence="0.999835705882353">
being a convenient interface to compare the
many voices, the interactive map is an attrac-
tive and easy-to-understand demonstration of
the technology being developed in EMIME.
3. The models developed in the HMM frame-
work can be demonstrated also in adapta-
tion of an ASR system for large-vocabulary
continuous speech recognition. By utilizing
morpheme-based language models instead of
word-based models the Finnish ASR system
is able to cover practically an unlimited vo-
cabulary (Hirsim¨aki et al., 2006). This is
necessary for morphologically rich languages
where, due to inflection, derivation and com-
position, there exists so many different word
forms that word based language modeling be-
comes impractical.
</bodyText>
<subsectionHeader confidence="0.998646">
3.2 Cross-lingual systems
</subsectionHeader>
<bodyText confidence="0.999944333333333">
In the EMIME project the goal is to learn cross-
lingual speaker adaptation. Here the output lan-
guage ASR or TTS system is adapted from speech
samples in the input language. The results so far
are encouraging, especially for TTS: Even though
the cross-lingual adaptation may somewhat de-
grade the synthesis quality, the adapted speech
now sounds more like the target speaker. Sev-
eral recent evaluations of the cross-lingual speaker
</bodyText>
<footnote confidence="0.84293">
synthesised examples, but we plan to add an interactive type-
in text-to-speech feature in the near future.
</footnote>
<page confidence="0.996563">
50
</page>
<figureCaption confidence="0.999224">
Figure 2: All English HTS voices can be used as online TTS on the geographical map.
</figureCaption>
<bodyText confidence="0.997377">
adaptation methods can be found in (Gibson et al.,
2010; Oura et al., 2010; Liang et al., 2010; Oura
et al., 2009).
The following systems have been created to
demonstrate cross-lingual adaptation:
</bodyText>
<listItem confidence="0.918905703703704">
1. In EMIME Cross-lingual Finnish/English
and Mandarin/English TTS adaptation the
input language sentences dictated by the user
will be used to learn the characteristics of her
or his voice. The adapted cross-lingual model
will be used to speak output language (En-
glish) sentences in the user’s voice. The user
does not need to be bilingual and only reads
sentences in their native language.
2. In EMIME Real-time speech-to-speech mo-
bile translation demo two users will interact
using a pair of mobile N97 devices (see Fig-
ure 3). The system will recognize the phrase
the other user is speaking in his native lan-
guage and translate and speak it in the native
language of the other user. After a few sen-
tences the system will have the speaker adap-
tation transformations ready and can apply
them in the synthesized voices to make them
sound more like the original speaker instead
of a standard voice. The first real-time demo
version is available for the Mandarin/English
language pair.
3. The morpheme-based translation system for
Finnish/English and English/Finnish can be
compared to a word based translation for
arbitrary sentences. The morpheme-based
</listItem>
<bodyText confidence="0.9636500625">
approach is particularly useful for language
pairs where one or both languages are mor-
phologically rich ones where the amount and
complexity of different word forms severely
limits the performance for word-based trans-
lation. The morpheme-based systems can
learn translation models for phrases where
morphemes are used instead of words (de
Gispert et al., 2009). Recent evaluations (Ku-
rimo et al., 2009) have shown that the perfor-
mance of the unsupervised data-driven mor-
pheme segmentation can rival the conven-
tional rule-based ones. This is very useful if
hand-crafted morphological analyzers are not
available or their coverage is not sufficient for
all languages.
</bodyText>
<sectionHeader confidence="0.989762" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.995416">
The research leading to these results was partly
funded from the European Communitys Seventh
</bodyText>
<page confidence="0.99786">
51
</page>
<figureCaption confidence="0.9728875">
Figure 3: EMIME Real-time speech-to-speech
mobile translation demo
</figureCaption>
<bodyText confidence="0.9787565">
Framework Programme (FP7/2007-2013) under
grant agreement 213845 (the EMIME project).
</bodyText>
<sectionHeader confidence="0.99839" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99929336">
A. de Gispert, S. Virpioja, M. Kurimo, and W. Byrne.
2009. Minimum Bayes risk combination of transla-
tion hypotheses from alternative morphological de-
compositions. In Proc. NAACL-HLT.
J. Dines, J. Yamagishi, and S. King. 2009. Measur-
ing the gap between HMM-based ASR and TTS. In
Proc. Interspeech ’09, Brighton, UK.
M. Gales. 1998. Maximum likelihood linear transfor-
mations for HMM-based speech recognition. Com-
puter Speech and Language, 12(2):75–98.
M. Gibson, T. Hirsim¨aki, R. Karhila, M. Kurimo,
and W. Byrne. 2010. Unsupervised cross-lingual
speaker adaptation for HMM-based speech synthe-
sis using two-pass decision tree construction. In
Proc. of ICASSP, page to appear, March.
T. Hirsim¨aki, M. Creutz, V. Siivola, M. Kurimo, S.
Virpioja, and J. Pylkk¨onen. 2006. Unlimited vo-
cabulary speech recognition with morph language
models applied to finnish. Computer Speech &amp; Lan-
guage, 20(4):515–541, October.
T. Hirsim¨aki, J. Pylkk¨onen, and M Kurimo. 2009.
Importance of high-order n-gram models in morph-
based speech recognition. IEEE Trans. Audio,
Speech, and Language Process., 17:724–732.
D. Iskra, B. Grosskopf, K. Marasek, H. van den
Heuvel, F. Diehl, and A. Kiessling. 2002.
SPEECON speech databases for consumer devices:
Database specification and validation. In Proc.
LREC, pages 329–333.
T. Kawahara, A. Lee, T. Kobayashi, K. Takeda,
N. Minematsu, S. Sagayama, K. Itou, A. Ito, M. Ya-
mamoto, A. Yamada, T. Utsuro, and K. Shikano.
2000. Free software toolkit for japanese large vo-
cabulary continuous speech recognition. In Proc.
ICSLP-2000, volume 4, pages 476–479.
S. King, K. Tokuda, H. Zen, and J. Yamagishi. 2008.
Unsupervised adaptation for HMM-based speech
synthesis. In Proc. Interspeech 2008, pages 1869–
1872, September.
Mikko Kurimo, Sami Virpioja, Ville T. Turunen,
Graeme W. Blackwood, and William Byrne. 2009.
Overview and results of Morpho Challenge 2009. In
Working Notes for the CLEF 2009 Workshop, Corfu,
Greece, September.
H. Liang, J. Dines, and L. Saheer. 2010. A
comparison of supervised and unsupervised cross-
lingual speaker adaptation approaches for HMM-
based speech synthesis. In Proc. of ICASSP, page
to appear, March.
Keiichiro Oura, Junichi Yamagishi, Simon King, Mir-
jam Wester, and Keiichi Tokuda. 2009. Unsuper-
vised speaker adaptation for speech-to-speech trans-
lation system. In Proc. SLP (Spoken Language Pro-
cessing), number 356 in 109, pages 13–18.
K. Oura, K. Tokuda, J. Yamagishi, S. King, and
M. Wester. 2010. Unsupervised cross-lingual
speaker adaptation for HMM-based speech synthe-
sis. In Proc. of ICASSP, page to appear, March.
Y.-J. Wu, S. King, and K. Tokuda. 2008. Cross-lingual
speaker adaptation for HMM-based speech synthe-
sis. In Proc. of ISCSLP, pages 1–4, December.
Y.-J. Wu, Y. Nankaku, and K. Tokuda. 2009. State
mapping based method for cross-lingual speaker
adaptation in HMM-based speech synthesis. In
Proc. of Interspeech, pages 528–531, September.
J. Yamagishi, T. Nose, H. Zen, Z.-H. Ling, T. Toda,
K. Tokuda, S. King, and S. Renals. 2009. Robust
speaker-adaptive HMM-based text-to-speech syn-
thesis. IEEE Trans. Audio, Speech and Language
Process., 17(6):1208–1230. (in press).
J. Yamagishi, B. Usabaev, S. King, O. Watts, J. Dines,
J. Tian, R. Hu, K. Oura, K. Tokuda, R. Karhila, and
M. Kurimo. 2010. Thousands of voices for hmm-
based speech synthesis. IEEE Trans. Speech, Audio
&amp; Language Process. (in press).
</reference>
<figure confidence="0.9981892">
input
ASR
Speaker
adaptation
Cross-lingual
Speaker adaptation
SMT
TTS
output
speech
</figure>
<page confidence="0.975997">
52
</page>
<reference confidence="0.998486333333333">
S. Young, G. Everman, D. Kershaw, G. Moore, J.
Odell, D. Ollason, V. Valtchev, and P. Woodland,
2001. The HTK Book Version 3.1, December.
</reference>
<page confidence="0.99935">
53
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.298788">
<title confidence="0.994012">Personalising speech-to-speech translation in the EMIME project</title>
<author confidence="0.945819">William John Philip N Matthew Teemu Reima Simon Hui Keiichiro Lakshmi Matt Sayaka Jilei Keiichi Yi-Jian Junichi</author>
<affiliation confidence="0.794201">1Aalto University, Finland, 2University of Edinburgh, UK, 3Idiap Research Institute, 4Nagoya Institute of Technology, Japan, 5Nokia Research Center Beijing, China,</affiliation>
<address confidence="0.930421">6University of Cambridge, UK</address>
<email confidence="0.63884">author:</email>
<abstract confidence="0.9996306">In the EMIME project we have studied unsupervised cross-lingual speaker adaptation. We have employed an HMM statistical framework for both speech recognition and synthesis which provides transformation mechanisms to adapt the synthesized voice in TTS (text-to-speech) using the recognized voice in ASR (automatic speech recognition). An important application for this research is personalised speech-to-speech translation that will use the voice of the speaker in the input language to utter the translated sentences in the output language. In mobile environments this enhances the users’ interaction across language barriers by making the output speech sound more like the original speaker’s way of speaking, even if she or he could not speak the output language.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A de Gispert</author>
<author>S Virpioja</author>
<author>M Kurimo</author>
<author>W Byrne</author>
</authors>
<title>Minimum Bayes risk combination of translation hypotheses from alternative morphological decompositions.</title>
<date>2009</date>
<booktitle>In Proc. NAACL-HLT.</booktitle>
<marker>de Gispert, Virpioja, Kurimo, Byrne, 2009</marker>
<rawString>A. de Gispert, S. Virpioja, M. Kurimo, and W. Byrne. 2009. Minimum Bayes risk combination of translation hypotheses from alternative morphological decompositions. In Proc. NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Dines</author>
<author>J Yamagishi</author>
<author>S King</author>
</authors>
<title>Measuring the gap between HMM-based ASR and TTS.</title>
<date>2009</date>
<booktitle>In Proc. Interspeech ’09,</booktitle>
<location>Brighton, UK.</location>
<contexts>
<context position="4003" citStr="Dines et al., 2009" startWordPosition="608" endWordPosition="611">al., 2008). In unsupervised adaptation samples are annotated by applying ASR. By eliminating the need for human intervention it becomes possible to perform voice adaptation for TTS in almost real-time. The target in the EMIME project2 is to study unsupervised cross-lingual speaker adaptation for S2ST systems. The first results of the project have 1http://www.jibbigo.com 2http://emime.org 48 Proceedings of the ACL 2010 System Demonstrations, pages 48–53, Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics been, for example, to bridge the gap between the ASR and TTS (Dines et al., 2009), to improve the baseline ASR (Hirsim¨aki et al., 2009) and SMT (de Gispert et al., 2009) systems for morphologically rich languages, and to develop robust TTS (Yamagishi et al., 2010). The next step has been preliminary experiments in intra-lingual and cross-lingual speaker adaptation (Wu et al., 2008). For cross-lingual adaptation several new methods have been proposed for mapping the HMM states, adaptation data and model transformations (Wu et al., 2009). In this presentation we can demonstrate the various new results in ASR, SMT and TTS. Even though the project is still ongoing, we have an</context>
</contexts>
<marker>Dines, Yamagishi, King, 2009</marker>
<rawString>J. Dines, J. Yamagishi, and S. King. 2009. Measuring the gap between HMM-based ASR and TTS. In Proc. Interspeech ’09, Brighton, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gales</author>
</authors>
<title>Maximum likelihood linear transformations for HMM-based speech recognition.</title>
<date>1998</date>
<journal>Computer Speech and Language,</journal>
<volume>12</volume>
<issue>2</issue>
<contexts>
<context position="3240" citStr="Gales, 1998" startWordPosition="495" endWordPosition="496">any features in the output voice that could raise the interaction experience to a much more natural level, for example, emotions, speaking rate, loudness and the speaker identity. After the recent development in hidden Markov model (HMM) based TTS, it has become possible to adapt the output voice using model transformations that can be estimated from a small number of speech samples. These techniques, for instance the maximum likelihood linear regression (MLLR), are adopted from HMM-based ASR where they are very powerful in fast adaptation of speaker and recording environment characteristics (Gales, 1998). Using hierarchical regression trees, the TTS and ASR models can further be coupled in a way that enables unsupervised TTS adaptation (King et al., 2008). In unsupervised adaptation samples are annotated by applying ASR. By eliminating the need for human intervention it becomes possible to perform voice adaptation for TTS in almost real-time. The target in the EMIME project2 is to study unsupervised cross-lingual speaker adaptation for S2ST systems. The first results of the project have 1http://www.jibbigo.com 2http://emime.org 48 Proceedings of the ACL 2010 System Demonstrations, pages 48–53</context>
</contexts>
<marker>Gales, 1998</marker>
<rawString>M. Gales. 1998. Maximum likelihood linear transformations for HMM-based speech recognition. Computer Speech and Language, 12(2):75–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gibson</author>
<author>T Hirsim¨aki</author>
<author>R Karhila</author>
<author>M Kurimo</author>
<author>W Byrne</author>
</authors>
<title>Unsupervised cross-lingual speaker adaptation for HMM-based speech synthesis using two-pass decision tree construction.</title>
<date>2010</date>
<booktitle>In Proc. of ICASSP,</booktitle>
<pages>page</pages>
<note>to appear,</note>
<marker>Gibson, Hirsim¨aki, Karhila, Kurimo, Byrne, 2010</marker>
<rawString>M. Gibson, T. Hirsim¨aki, R. Karhila, M. Kurimo, and W. Byrne. 2010. Unsupervised cross-lingual speaker adaptation for HMM-based speech synthesis using two-pass decision tree construction. In Proc. of ICASSP, page to appear, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hirsim¨aki</author>
<author>M Creutz</author>
<author>V Siivola</author>
<author>M Kurimo</author>
<author>S Virpioja</author>
<author>J Pylkk¨onen</author>
</authors>
<title>Unlimited vocabulary speech recognition with morph language models applied to finnish.</title>
<date>2006</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>20</volume>
<issue>4</issue>
<marker>Hirsim¨aki, Creutz, Siivola, Kurimo, Virpioja, Pylkk¨onen, 2006</marker>
<rawString>T. Hirsim¨aki, M. Creutz, V. Siivola, M. Kurimo, S. Virpioja, and J. Pylkk¨onen. 2006. Unlimited vocabulary speech recognition with morph language models applied to finnish. Computer Speech &amp; Language, 20(4):515–541, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hirsim¨aki</author>
<author>J Pylkk¨onen</author>
<author>M Kurimo</author>
</authors>
<title>Importance of high-order n-gram models in morphbased speech recognition.</title>
<date>2009</date>
<journal>IEEE Trans. Audio, Speech, and Language Process.,</journal>
<pages>17--724</pages>
<marker>Hirsim¨aki, Pylkk¨onen, Kurimo, 2009</marker>
<rawString>T. Hirsim¨aki, J. Pylkk¨onen, and M Kurimo. 2009. Importance of high-order n-gram models in morphbased speech recognition. IEEE Trans. Audio, Speech, and Language Process., 17:724–732.</rawString>
</citation>
<citation valid="false">
<authors>
<author>D Iskra</author>
<author>B Grosskopf</author>
<author>K Marasek</author>
<author>H</author>
</authors>
<note>van den</note>
<marker>Iskra, Grosskopf, Marasek, H, </marker>
<rawString>D. Iskra, B. Grosskopf, K. Marasek, H. van den</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Diehl Heuvel</author>
<author>A Kiessling</author>
</authors>
<title>SPEECON speech databases for consumer devices: Database specification and validation.</title>
<date>2002</date>
<booktitle>In Proc. LREC,</booktitle>
<pages>329--333</pages>
<marker>Heuvel, Kiessling, 2002</marker>
<rawString>Heuvel, F. Diehl, and A. Kiessling. 2002. SPEECON speech databases for consumer devices: Database specification and validation. In Proc. LREC, pages 329–333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kawahara</author>
<author>A Lee</author>
<author>T Kobayashi</author>
<author>K Takeda</author>
<author>N Minematsu</author>
<author>S Sagayama</author>
<author>K Itou</author>
<author>A Ito</author>
<author>M Yamamoto</author>
<author>A Yamada</author>
<author>T Utsuro</author>
<author>K Shikano</author>
</authors>
<title>Free software toolkit for japanese large vocabulary continuous speech recognition.</title>
<date>2000</date>
<booktitle>In Proc. ICSLP-2000,</booktitle>
<volume>4</volume>
<pages>476--479</pages>
<contexts>
<context position="4959" citStr="Kawahara et al., 2000" startWordPosition="764" endWordPosition="767"> several new methods have been proposed for mapping the HMM states, adaptation data and model transformations (Wu et al., 2009). In this presentation we can demonstrate the various new results in ASR, SMT and TTS. Even though the project is still ongoing, we have an initial version of mobile S2ST system and crosslingual speaker adaptation to show. 2 Baseline ASR, TTS and SMT systems The baseline ASR systems in the project are developed using the HTK toolkit (Young et al., 2001) for Finnish, English, Mandarin and Japanese. The systems can also utilize various real-time decoders such as Julius (Kawahara et al., 2000), Juicer at IDIAP and the TKK decoder (Hirsim¨aki et al., 2006). The main structure of the baseline systems for each of the four languages is similar and fairly standard and in line with most other state-ofthe-art large vocabulary ASR systems. Some special flavors for have been added, such as the morphological analysis for Finnish (Hirsim¨aki et al., 2009). For speaker adaptation, the MLLR transformation based on hierarchical regression classes is included for all languages. The baseline TTS systems in the project utilize the HTS toolkit (Yamagishi et al., 2009) which is built on top of the HT</context>
</contexts>
<marker>Kawahara, Lee, Kobayashi, Takeda, Minematsu, Sagayama, Itou, Ito, Yamamoto, Yamada, Utsuro, Shikano, 2000</marker>
<rawString>T. Kawahara, A. Lee, T. Kobayashi, K. Takeda, N. Minematsu, S. Sagayama, K. Itou, A. Ito, M. Yamamoto, A. Yamada, T. Utsuro, and K. Shikano. 2000. Free software toolkit for japanese large vocabulary continuous speech recognition. In Proc. ICSLP-2000, volume 4, pages 476–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S King</author>
<author>K Tokuda</author>
<author>H Zen</author>
<author>J Yamagishi</author>
</authors>
<title>Unsupervised adaptation for HMM-based speech synthesis.</title>
<date>2008</date>
<booktitle>In Proc. Interspeech</booktitle>
<pages>1869--1872</pages>
<contexts>
<context position="3394" citStr="King et al., 2008" startWordPosition="518" endWordPosition="521">dness and the speaker identity. After the recent development in hidden Markov model (HMM) based TTS, it has become possible to adapt the output voice using model transformations that can be estimated from a small number of speech samples. These techniques, for instance the maximum likelihood linear regression (MLLR), are adopted from HMM-based ASR where they are very powerful in fast adaptation of speaker and recording environment characteristics (Gales, 1998). Using hierarchical regression trees, the TTS and ASR models can further be coupled in a way that enables unsupervised TTS adaptation (King et al., 2008). In unsupervised adaptation samples are annotated by applying ASR. By eliminating the need for human intervention it becomes possible to perform voice adaptation for TTS in almost real-time. The target in the EMIME project2 is to study unsupervised cross-lingual speaker adaptation for S2ST systems. The first results of the project have 1http://www.jibbigo.com 2http://emime.org 48 Proceedings of the ACL 2010 System Demonstrations, pages 48–53, Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics been, for example, to bridge the gap between the ASR and TTS (Dines et a</context>
</contexts>
<marker>King, Tokuda, Zen, Yamagishi, 2008</marker>
<rawString>S. King, K. Tokuda, H. Zen, and J. Yamagishi. 2008. Unsupervised adaptation for HMM-based speech synthesis. In Proc. Interspeech 2008, pages 1869– 1872, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikko Kurimo</author>
<author>Sami Virpioja</author>
<author>Ville T Turunen</author>
<author>Graeme W Blackwood</author>
<author>William Byrne</author>
</authors>
<title>Overview and results of Morpho Challenge</title>
<date>2009</date>
<booktitle>In Working Notes for the CLEF 2009 Workshop,</booktitle>
<location>Corfu, Greece,</location>
<marker>Kurimo, Virpioja, Turunen, Blackwood, Byrne, 2009</marker>
<rawString>Mikko Kurimo, Sami Virpioja, Ville T. Turunen, Graeme W. Blackwood, and William Byrne. 2009. Overview and results of Morpho Challenge 2009. In Working Notes for the CLEF 2009 Workshop, Corfu, Greece, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Liang</author>
<author>J Dines</author>
<author>L Saheer</author>
</authors>
<title>A comparison of supervised and unsupervised crosslingual speaker adaptation approaches for HMMbased speech synthesis.</title>
<date>2010</date>
<booktitle>In Proc. of ICASSP,</booktitle>
<pages>page</pages>
<note>to appear,</note>
<contexts>
<context position="11303" citStr="Liang et al., 2010" startWordPosition="1802" endWordPosition="1805">e ASR or TTS system is adapted from speech samples in the input language. The results so far are encouraging, especially for TTS: Even though the cross-lingual adaptation may somewhat degrade the synthesis quality, the adapted speech now sounds more like the target speaker. Several recent evaluations of the cross-lingual speaker synthesised examples, but we plan to add an interactive typein text-to-speech feature in the near future. 50 Figure 2: All English HTS voices can be used as online TTS on the geographical map. adaptation methods can be found in (Gibson et al., 2010; Oura et al., 2010; Liang et al., 2010; Oura et al., 2009). The following systems have been created to demonstrate cross-lingual adaptation: 1. In EMIME Cross-lingual Finnish/English and Mandarin/English TTS adaptation the input language sentences dictated by the user will be used to learn the characteristics of her or his voice. The adapted cross-lingual model will be used to speak output language (English) sentences in the user’s voice. The user does not need to be bilingual and only reads sentences in their native language. 2. In EMIME Real-time speech-to-speech mobile translation demo two users will interact using a pair of mo</context>
</contexts>
<marker>Liang, Dines, Saheer, 2010</marker>
<rawString>H. Liang, J. Dines, and L. Saheer. 2010. A comparison of supervised and unsupervised crosslingual speaker adaptation approaches for HMMbased speech synthesis. In Proc. of ICASSP, page to appear, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keiichiro Oura</author>
<author>Junichi Yamagishi</author>
<author>Simon King</author>
<author>Mirjam Wester</author>
<author>Keiichi Tokuda</author>
</authors>
<title>Unsupervised speaker adaptation for speech-to-speech translation system.</title>
<date>2009</date>
<booktitle>In Proc. SLP (Spoken Language Processing), number 356 in 109,</booktitle>
<pages>13--18</pages>
<contexts>
<context position="11323" citStr="Oura et al., 2009" startWordPosition="1806" endWordPosition="1809">is adapted from speech samples in the input language. The results so far are encouraging, especially for TTS: Even though the cross-lingual adaptation may somewhat degrade the synthesis quality, the adapted speech now sounds more like the target speaker. Several recent evaluations of the cross-lingual speaker synthesised examples, but we plan to add an interactive typein text-to-speech feature in the near future. 50 Figure 2: All English HTS voices can be used as online TTS on the geographical map. adaptation methods can be found in (Gibson et al., 2010; Oura et al., 2010; Liang et al., 2010; Oura et al., 2009). The following systems have been created to demonstrate cross-lingual adaptation: 1. In EMIME Cross-lingual Finnish/English and Mandarin/English TTS adaptation the input language sentences dictated by the user will be used to learn the characteristics of her or his voice. The adapted cross-lingual model will be used to speak output language (English) sentences in the user’s voice. The user does not need to be bilingual and only reads sentences in their native language. 2. In EMIME Real-time speech-to-speech mobile translation demo two users will interact using a pair of mobile N97 devices (se</context>
</contexts>
<marker>Oura, Yamagishi, King, Wester, Tokuda, 2009</marker>
<rawString>Keiichiro Oura, Junichi Yamagishi, Simon King, Mirjam Wester, and Keiichi Tokuda. 2009. Unsupervised speaker adaptation for speech-to-speech translation system. In Proc. SLP (Spoken Language Processing), number 356 in 109, pages 13–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Oura</author>
<author>K Tokuda</author>
<author>J Yamagishi</author>
<author>S King</author>
<author>M Wester</author>
</authors>
<title>Unsupervised cross-lingual speaker adaptation for HMM-based speech synthesis.</title>
<date>2010</date>
<booktitle>In Proc. of ICASSP,</booktitle>
<pages>page</pages>
<note>to appear,</note>
<contexts>
<context position="11283" citStr="Oura et al., 2010" startWordPosition="1798" endWordPosition="1801"> the output language ASR or TTS system is adapted from speech samples in the input language. The results so far are encouraging, especially for TTS: Even though the cross-lingual adaptation may somewhat degrade the synthesis quality, the adapted speech now sounds more like the target speaker. Several recent evaluations of the cross-lingual speaker synthesised examples, but we plan to add an interactive typein text-to-speech feature in the near future. 50 Figure 2: All English HTS voices can be used as online TTS on the geographical map. adaptation methods can be found in (Gibson et al., 2010; Oura et al., 2010; Liang et al., 2010; Oura et al., 2009). The following systems have been created to demonstrate cross-lingual adaptation: 1. In EMIME Cross-lingual Finnish/English and Mandarin/English TTS adaptation the input language sentences dictated by the user will be used to learn the characteristics of her or his voice. The adapted cross-lingual model will be used to speak output language (English) sentences in the user’s voice. The user does not need to be bilingual and only reads sentences in their native language. 2. In EMIME Real-time speech-to-speech mobile translation demo two users will interac</context>
</contexts>
<marker>Oura, Tokuda, Yamagishi, King, Wester, 2010</marker>
<rawString>K. Oura, K. Tokuda, J. Yamagishi, S. King, and M. Wester. 2010. Unsupervised cross-lingual speaker adaptation for HMM-based speech synthesis. In Proc. of ICASSP, page to appear, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y-J Wu</author>
<author>S King</author>
<author>K Tokuda</author>
</authors>
<title>Cross-lingual speaker adaptation for HMM-based speech synthesis.</title>
<date>2008</date>
<booktitle>In Proc. of ISCSLP,</booktitle>
<pages>1--4</pages>
<contexts>
<context position="4307" citStr="Wu et al., 2008" startWordPosition="656" endWordPosition="659">ms. The first results of the project have 1http://www.jibbigo.com 2http://emime.org 48 Proceedings of the ACL 2010 System Demonstrations, pages 48–53, Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics been, for example, to bridge the gap between the ASR and TTS (Dines et al., 2009), to improve the baseline ASR (Hirsim¨aki et al., 2009) and SMT (de Gispert et al., 2009) systems for morphologically rich languages, and to develop robust TTS (Yamagishi et al., 2010). The next step has been preliminary experiments in intra-lingual and cross-lingual speaker adaptation (Wu et al., 2008). For cross-lingual adaptation several new methods have been proposed for mapping the HMM states, adaptation data and model transformations (Wu et al., 2009). In this presentation we can demonstrate the various new results in ASR, SMT and TTS. Even though the project is still ongoing, we have an initial version of mobile S2ST system and crosslingual speaker adaptation to show. 2 Baseline ASR, TTS and SMT systems The baseline ASR systems in the project are developed using the HTK toolkit (Young et al., 2001) for Finnish, English, Mandarin and Japanese. The systems can also utilize various real-</context>
</contexts>
<marker>Wu, King, Tokuda, 2008</marker>
<rawString>Y.-J. Wu, S. King, and K. Tokuda. 2008. Cross-lingual speaker adaptation for HMM-based speech synthesis. In Proc. of ISCSLP, pages 1–4, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y-J Wu</author>
<author>Y Nankaku</author>
<author>K Tokuda</author>
</authors>
<title>State mapping based method for cross-lingual speaker adaptation in HMM-based speech synthesis.</title>
<date>2009</date>
<booktitle>In Proc. of Interspeech,</booktitle>
<pages>528--531</pages>
<contexts>
<context position="4464" citStr="Wu et al., 2009" startWordPosition="679" endWordPosition="682">a, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics been, for example, to bridge the gap between the ASR and TTS (Dines et al., 2009), to improve the baseline ASR (Hirsim¨aki et al., 2009) and SMT (de Gispert et al., 2009) systems for morphologically rich languages, and to develop robust TTS (Yamagishi et al., 2010). The next step has been preliminary experiments in intra-lingual and cross-lingual speaker adaptation (Wu et al., 2008). For cross-lingual adaptation several new methods have been proposed for mapping the HMM states, adaptation data and model transformations (Wu et al., 2009). In this presentation we can demonstrate the various new results in ASR, SMT and TTS. Even though the project is still ongoing, we have an initial version of mobile S2ST system and crosslingual speaker adaptation to show. 2 Baseline ASR, TTS and SMT systems The baseline ASR systems in the project are developed using the HTK toolkit (Young et al., 2001) for Finnish, English, Mandarin and Japanese. The systems can also utilize various real-time decoders such as Julius (Kawahara et al., 2000), Juicer at IDIAP and the TKK decoder (Hirsim¨aki et al., 2006). The main structure of the baseline syste</context>
<context position="6170" citStr="Wu et al., 2009" startWordPosition="965" endWordPosition="968">he HTK framework. The HMM-based TTS systems have been developed for Finnish, English, Mandarin and Japanese. The systems include an average voice model for each language trained over hundreds of speakers taken from standard ASR corpora, such as Speecon (Iskra et al., 2002). Using speaker adaptation transforms, thousands of new voices have been created (Yamagishi et al., 2010) and new voices can be added using a small number of either supervised or unsupervised speech samples. Crosslingual adaptation is possible by creating a mapping between the HMM states in the input and the output language (Wu et al., 2009). Because the resources of the EMIME project have been focused on ASR, TTS and speaker adaptation, we aim at relying on existing solutions for SMT as far as possible. New methods have been studied concerning the morphologically rich languages (de Gispert et al., 2009), but for the S2ST system we are currently using Google translate3. 3 Demonstrations to show 3.1 Monolingual systems In robust speech synthesis, a computer can learn to speak in the desired way after processing only a relatively small amount of training speech. The training speech can even be a normal quality recording outside the</context>
</contexts>
<marker>Wu, Nankaku, Tokuda, 2009</marker>
<rawString>Y.-J. Wu, Y. Nankaku, and K. Tokuda. 2009. State mapping based method for cross-lingual speaker adaptation in HMM-based speech synthesis. In Proc. of Interspeech, pages 528–531, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Yamagishi</author>
<author>T Nose</author>
<author>H Zen</author>
<author>Z-H Ling</author>
<author>T Toda</author>
<author>K Tokuda</author>
<author>S King</author>
<author>S Renals</author>
</authors>
<title>Robust speaker-adaptive HMM-based text-to-speech synthesis.</title>
<date>2009</date>
<journal>IEEE Trans. Audio, Speech and Language Process.,</journal>
<volume>17</volume>
<issue>6</issue>
<note>(in press).</note>
<contexts>
<context position="5527" citStr="Yamagishi et al., 2009" startWordPosition="858" endWordPosition="861">al-time decoders such as Julius (Kawahara et al., 2000), Juicer at IDIAP and the TKK decoder (Hirsim¨aki et al., 2006). The main structure of the baseline systems for each of the four languages is similar and fairly standard and in line with most other state-ofthe-art large vocabulary ASR systems. Some special flavors for have been added, such as the morphological analysis for Finnish (Hirsim¨aki et al., 2009). For speaker adaptation, the MLLR transformation based on hierarchical regression classes is included for all languages. The baseline TTS systems in the project utilize the HTS toolkit (Yamagishi et al., 2009) which is built on top of the HTK framework. The HMM-based TTS systems have been developed for Finnish, English, Mandarin and Japanese. The systems include an average voice model for each language trained over hundreds of speakers taken from standard ASR corpora, such as Speecon (Iskra et al., 2002). Using speaker adaptation transforms, thousands of new voices have been created (Yamagishi et al., 2010) and new voices can be added using a small number of either supervised or unsupervised speech samples. Crosslingual adaptation is possible by creating a mapping between the HMM states in the inpu</context>
</contexts>
<marker>Yamagishi, Nose, Zen, Ling, Toda, Tokuda, King, Renals, 2009</marker>
<rawString>J. Yamagishi, T. Nose, H. Zen, Z.-H. Ling, T. Toda, K. Tokuda, S. King, and S. Renals. 2009. Robust speaker-adaptive HMM-based text-to-speech synthesis. IEEE Trans. Audio, Speech and Language Process., 17(6):1208–1230. (in press).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Yamagishi</author>
<author>B Usabaev</author>
<author>S King</author>
<author>O Watts</author>
<author>J Dines</author>
<author>J Tian</author>
<author>R Hu</author>
<author>K Oura</author>
<author>K Tokuda</author>
<author>R Karhila</author>
<author>M Kurimo</author>
</authors>
<title>Thousands of voices for hmmbased speech synthesis.</title>
<date>2010</date>
<journal>IEEE Trans. Speech, Audio &amp; Language</journal>
<contexts>
<context position="4187" citStr="Yamagishi et al., 2010" startWordPosition="639" endWordPosition="642"> in almost real-time. The target in the EMIME project2 is to study unsupervised cross-lingual speaker adaptation for S2ST systems. The first results of the project have 1http://www.jibbigo.com 2http://emime.org 48 Proceedings of the ACL 2010 System Demonstrations, pages 48–53, Uppsala, Sweden, 13 July 2010. c�2010 Association for Computational Linguistics been, for example, to bridge the gap between the ASR and TTS (Dines et al., 2009), to improve the baseline ASR (Hirsim¨aki et al., 2009) and SMT (de Gispert et al., 2009) systems for morphologically rich languages, and to develop robust TTS (Yamagishi et al., 2010). The next step has been preliminary experiments in intra-lingual and cross-lingual speaker adaptation (Wu et al., 2008). For cross-lingual adaptation several new methods have been proposed for mapping the HMM states, adaptation data and model transformations (Wu et al., 2009). In this presentation we can demonstrate the various new results in ASR, SMT and TTS. Even though the project is still ongoing, we have an initial version of mobile S2ST system and crosslingual speaker adaptation to show. 2 Baseline ASR, TTS and SMT systems The baseline ASR systems in the project are developed using the </context>
<context position="5932" citStr="Yamagishi et al., 2010" startWordPosition="922" endWordPosition="925">., 2009). For speaker adaptation, the MLLR transformation based on hierarchical regression classes is included for all languages. The baseline TTS systems in the project utilize the HTS toolkit (Yamagishi et al., 2009) which is built on top of the HTK framework. The HMM-based TTS systems have been developed for Finnish, English, Mandarin and Japanese. The systems include an average voice model for each language trained over hundreds of speakers taken from standard ASR corpora, such as Speecon (Iskra et al., 2002). Using speaker adaptation transforms, thousands of new voices have been created (Yamagishi et al., 2010) and new voices can be added using a small number of either supervised or unsupervised speech samples. Crosslingual adaptation is possible by creating a mapping between the HMM states in the input and the output language (Wu et al., 2009). Because the resources of the EMIME project have been focused on ASR, TTS and speaker adaptation, we aim at relying on existing solutions for SMT as far as possible. New methods have been studied concerning the morphologically rich languages (de Gispert et al., 2009), but for the S2ST system we are currently using Google translate3. 3 Demonstrations to show 3</context>
<context position="9035" citStr="Yamagishi et al., 2010" startWordPosition="1437" endWordPosition="1440">jects. Blue markers show male speakers and red markers show female speakers. Available online via http://www.emime.org/learn/speech-synthesis/listen/Examples-for-D2.1 10 minutes and then after half an hour of processing time, the TTS system has transformed the average model towards the user’s voice and can speak with this voice. The cloned voices may become especially valuable, for example, if a person’s voice is later damaged in an accident or by a disease. 2. In EMIME Thousand voices map the goal is to browse the world’s largest collection of synthetic voices by using a world map interface (Yamagishi et al., 2010). The user can zoom in the world map and select any voice, which are organized according to the place of living of the adapted speaker, to utter the given sentence. This interactive geographical representation is shown in Figure 1. Each marker corresponds to an individual speaker. Blue markers show male speakers and red markers show female speakers. Some markers are in arbitrary locations (in the correct country) because precise location information is not available for all speakers. This geographical representation, which includes an interactive TTS demonstration of many of the voices, is ava</context>
</contexts>
<marker>Yamagishi, Usabaev, King, Watts, Dines, Tian, Hu, Oura, Tokuda, Karhila, Kurimo, 2010</marker>
<rawString>J. Yamagishi, B. Usabaev, S. King, O. Watts, J. Dines, J. Tian, R. Hu, K. Oura, K. Tokuda, R. Karhila, and M. Kurimo. 2010. Thousands of voices for hmmbased speech synthesis. IEEE Trans. Speech, Audio &amp; Language Process. (in press).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Young</author>
<author>G Everman</author>
<author>D Kershaw</author>
<author>G Moore</author>
<author>J Odell</author>
<author>D Ollason</author>
<author>V Valtchev</author>
<author>P Woodland</author>
</authors>
<date>2001</date>
<booktitle>The HTK Book Version 3.1,</booktitle>
<contexts>
<context position="4819" citStr="Young et al., 2001" startWordPosition="743" endWordPosition="746">ep has been preliminary experiments in intra-lingual and cross-lingual speaker adaptation (Wu et al., 2008). For cross-lingual adaptation several new methods have been proposed for mapping the HMM states, adaptation data and model transformations (Wu et al., 2009). In this presentation we can demonstrate the various new results in ASR, SMT and TTS. Even though the project is still ongoing, we have an initial version of mobile S2ST system and crosslingual speaker adaptation to show. 2 Baseline ASR, TTS and SMT systems The baseline ASR systems in the project are developed using the HTK toolkit (Young et al., 2001) for Finnish, English, Mandarin and Japanese. The systems can also utilize various real-time decoders such as Julius (Kawahara et al., 2000), Juicer at IDIAP and the TKK decoder (Hirsim¨aki et al., 2006). The main structure of the baseline systems for each of the four languages is similar and fairly standard and in line with most other state-ofthe-art large vocabulary ASR systems. Some special flavors for have been added, such as the morphological analysis for Finnish (Hirsim¨aki et al., 2009). For speaker adaptation, the MLLR transformation based on hierarchical regression classes is included</context>
</contexts>
<marker>Young, Everman, Kershaw, Moore, Odell, Ollason, Valtchev, Woodland, 2001</marker>
<rawString>S. Young, G. Everman, D. Kershaw, G. Moore, J. Odell, D. Ollason, V. Valtchev, and P. Woodland, 2001. The HTK Book Version 3.1, December.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>