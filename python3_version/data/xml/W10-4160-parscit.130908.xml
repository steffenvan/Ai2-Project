<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000075">
<title confidence="0.8784035">
DLUT: Chinese Personal Name Disambiguation with Rich
Features
</title>
<author confidence="0.998127">
Dongliang Wang
</author>
<affiliation confidence="0.821828333333333">
Department of Computer Science
and Engineering, Dalian University
of Technology
</affiliation>
<email confidence="0.990717">
wdl129@163.com
</email>
<author confidence="0.993179">
Degen Huang
</author>
<affiliation confidence="0.820697333333333">
Department of Computer Science
and Engineering, Dalian University
of Technology
</affiliation>
<email confidence="0.996312">
huangdg@dlut.edu.cn
</email>
<sectionHeader confidence="0.994945" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9966175625">
In this paper we describe a person clus-
tering system for a given document set
and report the results we have obtained
on the test set of Chinese personal name
(CPN) disambiguation task of CIPS-
SIGHAN 2010. This task consists of
clustering a set of Xinhua news docu-
ments that mention an ambiguous CPN
according to named entity in reality.
Several features including named entities
(NE) and common nouns generated from
the documents and a variety of rules are
employed in our system. This system
achieves F = 86.36% with B_Cubed
scoring metrics and F = 90.78% with pu-
rity_based metrics.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999912277777778">
As the amount of web information expands at an
ever more rapid pace, extraction of information
for specific named entity is more and more im-
portant. Usually there are named-entity ambigu-
ity in web data, for example more than one per-
son use a same name, therefore it is difficult to
decide which document refers to a specific
named entity.
The goal of CPN disambiguation is to cluster-
ing input Xinhua news corpus by the entity each
document refers to. The new documents which
span a time of fourteen years are extracted on
web.
As description of CPN disambiguation task of
CIPS-SIGHAN 2010, Chinese personal name
disambiguation is potentially more challenging
due to the need for word segmentation, which
could introduce errors that can in large part be
avoided in the English task.
In this paper we employ a CPN disambigua-
tion system that extracts NE and common nouns
from the input corpus as features, and then com-
putes the similarity of each two documents in the
corpus based on feature vector. Hierarchical Ag-
glomerative Clustering (HAC) algorithm (AK
Jain et al., 1999) is used to implement clustering.
After a great deal of analysis of news corpus,
we constitute several rules, the experiments
show that these rules can improve the result of
this task.
The remainder of this paper is organized as
follows. Section 2 introduces the preprocessing
of test corpus, and in section 3 we present the
methodology of our system. In section 4 we pre-
sent the experimental results and give a conclu-
sion in section 5.
</bodyText>
<sectionHeader confidence="0.966915" genericHeader="introduction">
2 Preprocessing
</sectionHeader>
<bodyText confidence="0.999729736842105">
In this step, we mainly complete the works as
follows.
Firstly, corpuses including a given name
string are in different files, one document one
file. In order to convenient for processing, we
combine these documents into one file, distin-
guish them with document id.
Secondly, some news corpuses have several
subtitles but usually only part of them including
focused name string, the others are noise of dis-
ambiguate of focused named entity, for example
a news about sports may contain several subti-
tles about basketball, swimming, race and so on.
These noises are removed from the corpus by us.
Lastly, there is a lack of date-line in a few
documents; in general, these data-lines are rec-
ognized as part of text, they can be recognized
through simple matching method. Because data-
lines have consistent format as “���**�*H
</bodyText>
<equation confidence="0.368703">
V”
.
</equation>
<sectionHeader confidence="0.995" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.9998286875">
The system follows a procedure include: word
segmentation, the detection of ambiguous ob-
jects, feature extractions, computation of docu-
ment similarity and clustering.
First, the text is segmented by a word segmen-
tation system explored by Luo and Huang
(2009). The second step is extract all features
from segmented text, all features are put into
two feature vectors: NE vector and common
noun vector. Then we will compute the distance
between corresponding vectors of each two
documents, the standard SoftTFIDF (Chen and
Martin, 2007) are employed to compute the dis-
tance between two feature vectors. Lastly, we
use the HAC algorithm for clustering of docu-
ments.
</bodyText>
<subsectionHeader confidence="0.999417">
3.1 Word Segmentation
</subsectionHeader>
<bodyText confidence="0.999911636363636">
Word segmentation is a base and difficult work
of natural language processing (NLP) and a
precondition of feature extraction. In this paper,
the word segmentation system explored by Luo
and Huang (2009) are employed to do this work.
This system training on the corpus of 2000’s
“People’s Daily”. In addition, this system can
recognize named entities including personal
name, location name and organization name. We
can extract these NEs by part-of-speech (POS)
directly.
</bodyText>
<subsectionHeader confidence="0.999625">
3.2 The Detection of Ambiguous Entities
</subsectionHeader>
<bodyText confidence="0.9997535">
Given a name string, the documents can be di-
vided into three groups:
</bodyText>
<listItem confidence="0.983033833333333">
(1) Documents which contain names that are
exactly match the query name string.
(2) Documents which contain names that have
a substring exactly match the query name string.
(3) Documents which contain the query name
string that is not personal name.
</listItem>
<bodyText confidence="0.9999566">
After word segmentation, all personal names
are labeled by system, when we find one per-
sonal name or its substring match the query
name string; we will cluster this document ac-
cording to the name. If we failure all over the
document, it’s considered that this document
belong to category (3), it will be discarded.
The ambiguous personal name in a document
may refer to multiple entities, for example a
news about party of namesakes, but this is a very
small probability event, so we assume that all
mentions in one document refers to the same
entity, viz. “one person one document”.
Although we assume that “one person one
document”, the same personal name may occur
more than once. Some times the word segmenta-
tion system will give the same personal name
different labels in one document, for example a
personal name “*7-1C§” may be recognized as
“*7-1C” and “*7-1C§” in different sentence in
one document. Suppose that P1, P2, ... , Pn are
recognized names that match the query name
string, T1, T2, ... , Tn are the corresponding oc-
cur times. We use the following method to en-
sure the final needed personal name:
</bodyText>
<listItem confidence="0.9813995">
(1) If Ti &gt; Tj for j = 1, 2, ..., i-1, i+1, ... , n,
Pi is selected as the final needed personal name,
else go to step (2).
(2) Define S = { T1, T2, ... , Tn }, E1 = {T11,
</listItem>
<bodyText confidence="0.762791625">
T12, ..., T1m}, E2 = S – E1 satisfying T11 = T12
= ...= T1m, E1 C S and Ti &gt; Tj (Ti E E1, Tj E E2).
Fi shows the word before Pi and Bi after Pi. For
each Ti E E1, connect Fi, Ti and Bi into a new
string named Ri, we can get R = {R11, R12, ...,
R1m} corresponding to E1, the longest common
substring of R are considered the final needed
personal name.
</bodyText>
<subsectionHeader confidence="0.954008">
3.3 Features
</subsectionHeader>
<bodyText confidence="0.999989948275862">
We define local sentence as sentences which
contain the query name string, the features ex-
tracted from local sentences named local fea-
tures. Otherwise, all sentences except local sen-
tences in a document are named global sentences;
the features extracted from global sentences are
global features. The reason to distinguish them is
because they have different contribution to simi-
larity computation. Local features are generally
considered more important than global features,
therefore a high weight should be given to local
features.
Named entities are important information
about focused name. In this paper, NEs include
personal names, location names and organization
names. Location name and organization name
usually indicate the region and department of
focused name, and personal names usually have
high co-occurrence rate, for example “AAN”
and “MVV” are two names of table tennis players,
so they always appear in a same news document
about table tennis. The NE features which have
been tagged by segmentation system can be ex-
tracted from the document directly.
We also consider the features of common
nouns. Semantically independent common nouns
such as person’s job and person’s hobby etc usu-
ally include some useful information about the
ambiguous object. We attempt to capture these
noun features and use them as elements in fea-
ture vector.
Location names in data-line. The location
name in the data-line indicates the place the
news had occurred, if two documents have the
same date-line location name, and then there is a
good chance that these two documents refer the
same person.
Appellation of query name. Appellation usu-
ally demonstrate a person’s identity, for example,
if the appellation of the query name is “记者”, it
shows that he or she is a journalist. As location
names in data-line, if two query names have the
same appellation, the possibility of them refer to
the same person increased. The word segmenta-
tion system doesn’t clearly marked out appella-
tion but marked as common noun. In generally,
appellations appear neighbor in front of name, so
we collect the common nouns neighbor front of
query names as their appellations.
So far, we have developed four feature vec-
tors: local NE vector, local common noun vector,
global NE vector and global common noun vec-
tor. Given feature vectors, we need to find a way
to learn the similarity matrix. In this paper, we
choose the standard TF-IDF method to calculate
the similarity matrix. Location name in date-line
and appellation of query name will be used in
rule method without similarity calculation.
</bodyText>
<subsectionHeader confidence="0.98849">
3.4 Similarity Matrix
</subsectionHeader>
<bodyText confidence="0.999669">
Given a pair of feature vectors consisting of NEs
or common nouns, we need to choose a similar-
ity scheme to calculate the similarity matrix. The
standard TF-IDF method is introduced here, then
a little change for Chinese string.
Standard TF-IDF: Given a pair of vector S
and T, S = (s1, s2, ..., sn), T = (t1, t2, ..., tm).
Here, si (i = 1, ..., n) and tj (j = 1, ..., m) are NE
or common noun. We define:
</bodyText>
<sectionHeader confidence="0.59507" genericHeader="method">
CLOSE(B; S;T )
</sectionHeader>
<subsectionHeader confidence="0.455553">
we S,∃veT,dist
</subsectionHeader>
<bodyText confidence="0.903429857142857">
(w,v)&gt;
Where dist(w;v) is the Jaro-Winkler dis-
tance function (Winkler, 1999), which will
be introduced later.
D(w;T)=maxveT dist(w;v) (2)
Then the standard TF-IDF SoftTFIDF is com-
puted as:
</bodyText>
<equation confidence="0.855422428571429">
SoftTFIDF(S,T
V w S V w T D w T
( , ) * ( , ) * ( , ) (3)
V(w,S)= V (w, S)
(4)
S
V (w,S)=log(TFwS+1) *log(IDFw) (5)
</equation>
<bodyText confidence="0.9276015">
Where TFw ,S is the frequency of substring
win S, and IDFw is the inverse of the fraction
of documents in the corpus that contain w . Sup-
pose Nt is total number of documents, Nw is total
number of documents which contain w . Then
IDFw computed as:
</bodyText>
<equation confidence="0.81073">
IDFω = N, (6) N
w
</equation>
<bodyText confidence="0.999701857142857">
The Jaro-Winkler distance Jw of two given
strings s1 and s2 as shown in formula (7), l is
the length of common prefix at the start of the
string up to a maximum of 4 characters, p is a
constant scaling factor for how much the score is
adjusted upwards for having common prefixes,
the value for p is 0.1.
</bodyText>
<equation confidence="0.971713333333333">
dw =dj +lp(1−dj) (7)
= ( m + m +m−t
 |1   ||2 |
</equation>
<subsectionHeader confidence="0.649176">
s s
</subsectionHeader>
<bodyText confidence="0.9656422">
In formula (8) m is the number of matching
characters, t is the number of transpositions. In
order to be consistent with the English strings, a
Chinese character is seen as two English charac-
ters.
Corresponding to four feature vectors, we can
calculate the four similarities: S(gNE), S(gCN),
S(lNE), S(lCN). The similarity between two
documents (DS) is computed as:
DS=
</bodyText>
<equation confidence="0.9820328">
* S(lNI)+S(gNl3 +(1−A) * S(lCN)+S(gCN
(9)
;
w
{
B} (1)
)
=
�w e CLOSE B S T
( ; ;
)
&apos;
Lwe
V &apos;2
(w, S)
&apos;
d j
m
) (8)
= A
</equation>
<page confidence="0.993428">
2
2
</page>
<bodyText confidence="0.999917666666667">
As time is tight, we just give λ a value of 0.8
with out experiment because we consider NEs
have stronger instructions.
</bodyText>
<subsectionHeader confidence="0.993398">
3.5 Clustering
</subsectionHeader>
<bodyText confidence="0.9999492">
Clustering is a key work of this task, it is very
important to choose a clustering algorithm. Here
we use HAC algorithm to do clustering. HAC
algorithm is an unsupervised clustering algo-
rithm, which can be described as follows:
</bodyText>
<listItem confidence="0.953627125">
(1) Initialization. Every document is re-
garded as a separate class.
(2) Repetition. Computing the similarity of
each of the two classes, merge the two classes
whose similarity are the highest and higher than
the threshold value of δ into a new class.
(3) Termination. Repeat step (2) until all
classes don’t satisfy the clustering condition.
</listItem>
<bodyText confidence="0.999861928571429">
Suppose document class F = {f1, f2, ..., fn}
and K = {k1, k2, ..., km}, fi and kj are documents
in class F and class K, then the similarity be-
tween F and K is:
the train set of Chinese personal name disam-
biguation task of CIPS-SIGHAN 2010. The re-
sult is shown in Table 1. R1 is the result without
rules, and R2 shows the accuracy after adding
the rules.
The system performance on the test set of
CPN disambiguation task of CIPS-SIGHAN
2010 is F = 90.78% evaluated with P_IP evalua-
tion, and F = 86.36% with B_Cubed evaluation.
The accuracy is shown in Table 2.
</bodyText>
<table confidence="0.945956571428572">
B_Cubed Precision Recall F
R1 70.56 86.77 74.74
R2 78.05 84.99 79.60
P_IP Purity Inverse F
Purity
R1 77.22 90.48 81.20
R2 82.92 88.30 84.29
</table>
<tableCaption confidence="0.983659">
Table 1. Experimental results for system with
rules and without rules on training set
</tableCaption>
<table confidence="0.577473333333333">
S( J,K) = EjS(fi,kj) (9) B_Cubed Precision Recall F
m*n
82.96 91.33 86.36
</table>
<bodyText confidence="0.998243681818182">
If two documents have different query name,
obviously they refer to different person, only
documents which have same query name will be
clustered. Before clustering, several rules are
afforded to improve the clustering condition.
These rules are generally applicable to news
corpus.
(1) If two documents have the same query
name and both of them are reporter, and both
date-lines have the same location name, then
combine the two documents into one class.
(2) If two documents have the same query
name and another same personal name, then
combine the two documents into one class.
(3) If two documents have the same query
name and both date-lines have the same location
name, then double the similarity, else halve the
similarity.
(4) If two documents have the same query
name and both personal names have the same
appellation, then double the similarity, else halve
the similarity.
</bodyText>
<sectionHeader confidence="0.999654" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9391305">
In order to prove the validity of the rule ap-
proach, a group of experiments are performed on
</bodyText>
<sectionHeader confidence="0.415821" genericHeader="method">
P_IP Purity Inverse F
</sectionHeader>
<subsectionHeader confidence="0.260634">
Purity
</subsectionHeader>
<bodyText confidence="0.261378">
87.94 94.21 90.78
</bodyText>
<tableCaption confidence="0.940721">
Table 2. The results on test set
</tableCaption>
<sectionHeader confidence="0.995783" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999979833333333">
We described our system that disambiguates
Chinese personal names in Xinhua corpus. We
mainly focus on extracting rich features from
documents and computing the similarity of each
two documents. Several rules are introduced to
improve the accuracy and have proved effective.
</bodyText>
<sectionHeader confidence="0.999276" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999638888888889">
Anil K. Jain, M. Narasimha Murty, and Patrick J.
Flynn. 1999. Data clustering: A review. ACM
Computing Surveys, 31(3): 264-323.
Bradley Malin. 2005. Unsupervised Name Disam-
biguation via Network Similarity. In proceedings
SIAM Conference on Data Mining, 2005.
Chen Ying, James Martin. 2007. CU-COMSEM: Ex-
ploring Rich Features for Unsupervised Web Per-
sonal Name Disambiguation. In proceedings of
Semeval 2007, Association for Computational
Linguistics, 2007.
Chen Ying, Sophia Y. M. Lee and Churen Huang.
2009. PolyUHK:A Robust Information Extraction
System for Web Personal Names. In proceedings
of Semeval 2009, Association for Computational
Linguistics, 2009.
Gusfield, Dan. 1997. Algorithms on Strings, Trees
and Sequences. Cambridge University Press,
Cambridge, UK
Javier Artiles, J. Gonzalo and S. Sekine. WePS2
Evaluation Campaign: Overview of the Web Peo-
ple Search Clustering Task. In proceedings of Se-
meval 2009, Association for Computational Lin-
guistics, 2009.
Luo Yanyan, Degen Huang. 2009. Chinese word seg-
mentation based on the marginal probabilities
Generated by CRFs. Journal of Chinese Informa-
tion Processing, 23(5): 3-8.
Octavian Popescu, B. Magnini. 2007. IRST-BP: Web
People Search Using Name Entities. In proceed-
ings of Semeval 2007, Association for Computa-
tional Linguistics, 2007.
William E. Winkler. 1999. The state of record linkage
and current research problems. Statistics of In-
come Division, Internal Revenue Service Publica-
tion R99/04.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.572489">
<title confidence="0.993505">DLUT: Chinese Personal Name Disambiguation with Rich Features</title>
<author confidence="0.896505">Dongliang</author>
<affiliation confidence="0.974676666666667">Department of Computer and Engineering, Dalian of Technology</affiliation>
<email confidence="0.992965">wdl129@163.com</email>
<author confidence="0.962622">Degen</author>
<affiliation confidence="0.972661666666667">Department of Computer and Engineering, Dalian of Technology</affiliation>
<email confidence="0.826736">huangdg@dlut.edu.cn</email>
<abstract confidence="0.996822941176471">In this paper we describe a person clustering system for a given document set and report the results we have obtained on the test set of Chinese personal name (CPN) disambiguation task of CIPS- SIGHAN 2010. This task consists of clustering a set of Xinhua news documents that mention an ambiguous CPN according to named entity in reality. Several features including named entities (NE) and common nouns generated from the documents and a variety of rules are employed in our system. This system achieves F = 86.36% with B_Cubed scoring metrics and F = 90.78% with purity_based metrics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anil K Jain</author>
<author>M Narasimha Murty</author>
<author>Patrick J Flynn</author>
</authors>
<title>Data clustering: A review.</title>
<date>1999</date>
<journal>ACM Computing Surveys,</journal>
<volume>31</volume>
<issue>3</issue>
<pages>264--323</pages>
<contexts>
<context position="1965" citStr="Jain et al., 1999" startWordPosition="317" endWordPosition="320">new documents which span a time of fourteen years are extracted on web. As description of CPN disambiguation task of CIPS-SIGHAN 2010, Chinese personal name disambiguation is potentially more challenging due to the need for word segmentation, which could introduce errors that can in large part be avoided in the English task. In this paper we employ a CPN disambiguation system that extracts NE and common nouns from the input corpus as features, and then computes the similarity of each two documents in the corpus based on feature vector. Hierarchical Agglomerative Clustering (HAC) algorithm (AK Jain et al., 1999) is used to implement clustering. After a great deal of analysis of news corpus, we constitute several rules, the experiments show that these rules can improve the result of this task. The remainder of this paper is organized as follows. Section 2 introduces the preprocessing of test corpus, and in section 3 we present the methodology of our system. In section 4 we present the experimental results and give a conclusion in section 5. 2 Preprocessing In this step, we mainly complete the works as follows. Firstly, corpuses including a given name string are in different files, one document one fil</context>
</contexts>
<marker>Jain, Murty, Flynn, 1999</marker>
<rawString>Anil K. Jain, M. Narasimha Murty, and Patrick J. Flynn. 1999. Data clustering: A review. ACM Computing Surveys, 31(3): 264-323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley Malin</author>
</authors>
<title>Unsupervised Name Disambiguation via Network Similarity.</title>
<date>2005</date>
<booktitle>In proceedings SIAM Conference on Data Mining,</booktitle>
<marker>Malin, 2005</marker>
<rawString>Bradley Malin. 2005. Unsupervised Name Disambiguation via Network Similarity. In proceedings SIAM Conference on Data Mining, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Ying</author>
<author>James Martin</author>
</authors>
<title>CU-COMSEM: Exploring Rich Features for Unsupervised Web Personal Name Disambiguation.</title>
<date>2007</date>
<booktitle>In proceedings of Semeval 2007, Association for Computational Linguistics,</booktitle>
<marker>Ying, Martin, 2007</marker>
<rawString>Chen Ying, James Martin. 2007. CU-COMSEM: Exploring Rich Features for Unsupervised Web Personal Name Disambiguation. In proceedings of Semeval 2007, Association for Computational Linguistics, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Ying</author>
<author>Sophia Y M Lee</author>
<author>Churen Huang</author>
</authors>
<title>PolyUHK:A Robust Information Extraction System for Web Personal Names.</title>
<date>2009</date>
<booktitle>In proceedings of Semeval 2009, Association for Computational Linguistics,</booktitle>
<marker>Ying, Lee, Huang, 2009</marker>
<rawString>Chen Ying, Sophia Y. M. Lee and Churen Huang. 2009. PolyUHK:A Robust Information Extraction System for Web Personal Names. In proceedings of Semeval 2009, Association for Computational Linguistics, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gusfield</author>
</authors>
<title>Algorithms on Strings, Trees and Sequences.</title>
<date>1997</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK</location>
<marker>Gusfield, 1997</marker>
<rawString>Gusfield, Dan. 1997. Algorithms on Strings, Trees and Sequences. Cambridge University Press, Cambridge, UK</rawString>
</citation>
<citation valid="true">
<authors>
<author>Javier Artiles</author>
<author>J Gonzalo</author>
<author>S Sekine</author>
</authors>
<title>WePS2 Evaluation Campaign: Overview of the Web People Search Clustering Task.</title>
<date>2009</date>
<booktitle>In proceedings of Semeval 2009, Association for Computational Linguistics,</booktitle>
<marker>Artiles, Gonzalo, Sekine, 2009</marker>
<rawString>Javier Artiles, J. Gonzalo and S. Sekine. WePS2 Evaluation Campaign: Overview of the Web People Search Clustering Task. In proceedings of Semeval 2009, Association for Computational Linguistics, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luo Yanyan</author>
<author>Degen Huang</author>
</authors>
<title>Chinese word segmentation based on the marginal probabilities Generated by CRFs.</title>
<date>2009</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>23</volume>
<issue>5</issue>
<pages>3--8</pages>
<marker>Yanyan, Huang, 2009</marker>
<rawString>Luo Yanyan, Degen Huang. 2009. Chinese word segmentation based on the marginal probabilities Generated by CRFs. Journal of Chinese Information Processing, 23(5): 3-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Octavian Popescu</author>
<author>B Magnini</author>
</authors>
<title>IRST-BP: Web People Search Using Name Entities.</title>
<date>2007</date>
<booktitle>In proceedings of Semeval 2007, Association for Computational Linguistics,</booktitle>
<marker>Popescu, Magnini, 2007</marker>
<rawString>Octavian Popescu, B. Magnini. 2007. IRST-BP: Web People Search Using Name Entities. In proceedings of Semeval 2007, Association for Computational Linguistics, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William E Winkler</author>
</authors>
<title>The state of record linkage and current research problems. Statistics of Income Division, Internal Revenue Service Publication R99/04.</title>
<date>1999</date>
<contexts>
<context position="9537" citStr="Winkler, 1999" startWordPosition="1618" endWordPosition="1619">ation of query name will be used in rule method without similarity calculation. 3.4 Similarity Matrix Given a pair of feature vectors consisting of NEs or common nouns, we need to choose a similarity scheme to calculate the similarity matrix. The standard TF-IDF method is introduced here, then a little change for Chinese string. Standard TF-IDF: Given a pair of vector S and T, S = (s1, s2, ..., sn), T = (t1, t2, ..., tm). Here, si (i = 1, ..., n) and tj (j = 1, ..., m) are NE or common noun. We define: CLOSE(B; S;T ) we S,∃veT,dist (w,v)&gt; Where dist(w;v) is the Jaro-Winkler distance function (Winkler, 1999), which will be introduced later. D(w;T)=maxveT dist(w;v) (2) Then the standard TF-IDF SoftTFIDF is computed as: SoftTFIDF(S,T V w S V w T D w T ( , ) * ( , ) * ( , ) (3) V(w,S)= V (w, S) (4) S V (w,S)=log(TFwS+1) *log(IDFw) (5) Where TFw ,S is the frequency of substring win S, and IDFw is the inverse of the fraction of documents in the corpus that contain w . Suppose Nt is total number of documents, Nw is total number of documents which contain w . Then IDFw computed as: IDFω = N, (6) N w The Jaro-Winkler distance Jw of two given strings s1 and s2 as shown in formula (7), l is the length of c</context>
</contexts>
<marker>Winkler, 1999</marker>
<rawString>William E. Winkler. 1999. The state of record linkage and current research problems. Statistics of Income Division, Internal Revenue Service Publication R99/04.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>