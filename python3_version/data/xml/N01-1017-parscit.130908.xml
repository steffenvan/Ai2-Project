<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000165">
<title confidence="0.98252">
Generating Training Data for Medical Dictations
</title>
<author confidence="0.941646">
Sergey Pakhomov
</author>
<affiliation confidence="0.990677">
University of Minnesota, MN
</affiliation>
<email confidence="0.971578">
pakhomov.sergey@mayo.edu
</email>
<bodyText confidence="0.275280166666667">
Michael Schonwetter
Linguistech Consortium, NJ
MSchonwetter@qwest.net
Joan Bachenko
Linguistech Consortium,NJ
bachenko@mnic.net
</bodyText>
<sectionHeader confidence="0.979766" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999850954545455">
In automatic speech recognition (ASR) enabled
applications for medical dictations, corpora of
literal transcriptions of speech are critical for
training both speaker independent and speaker
adapted acoustic models. Obtaining these
transcriptions is both costly and time consuming.
Non-literal transcriptions, on the other hand, are
easy to obtain because they are generated in the
normal course of a medical transcription operation.
This paper presents a method of automatically
generating texts that can take the place of literal
transcriptions for training acoustic and language
models. ATRS1 is an automatic transcription
reconstruction system that can produce near-literal
transcriptions with almost no human labor. We will
show that (i) adapted acoustic models trained on
ATRS data perform as well as or better than
adapted acoustic models trained on literal
transcriptions (as measured by recognition
accuracy) and (ii) language models trained on
ATRS data have lower perplexity than language
models trained on non-literal data.
</bodyText>
<sectionHeader confidence="0.960992" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.999751">
Dictation applications of automatic speech
recognition (ASR) require literal transcriptions of
speech in order to train both speaker independent
and speaker adapted acoustic models. Literal
transcriptions may also be used to train stochastic
language models that need to perform well on
spontaneous or disfluent speech. With the
exception of personal desktop systems, however,
obtaining these transcriptions is costly and time
consuming since they must be produced manually
</bodyText>
<sectionHeader confidence="0.483441" genericHeader="method">
1 patent pending (Serial No.: 09/487398)
</sectionHeader>
<bodyText confidence="0.999908916666666">
by humans educated for the task. The high cost
makes literal transcription unworkable for ASR
applications that require adapted acoustic models
for thousands of talkers as well as accurate
language models for idiosyncratic natural speech.
Non-literal transcriptions, on the other hand, are
easy to obtain because they are generated in the
normal course of a medical transcription operation.
It has been previously shown by Wightman and
Harder (1999) that the non-literal transcriptions can
be successfully used in acoustic adaptation.
However, non-literal transcriptions are incomplete.
They exclude many utterances that commonly
occur in medical dictation—filled pauses,
repetitions, repairs, ungrammatical phrases,
pleasantries, asides to the transcriptionist, etc.
Depending on the talker, such material may
constitute a significant portion of the dictation.
We present a method of automatically generating
texts that can take the place of literal transcriptions
for training acoustic and language models. ATRS
is an automatic transcription reconstruction system
that can produce near-literal transcriptions with
almost no human labor.
The following sections will describe ATRS and
present experimental results from language and
acoustic modeling. We will show that (i) adapted
acoustic models trained on ATRS data perform as
well as or better than adapted acoustic models
trained on literal transcriptions (as measured by
recognition accuracy) and (ii) language models
trained on ATRS data have lower perplexity than
language models trained on non-literal data. Data
used in the experiments comes from medical
dictations. All of the dictations are telephone
speech.
</bodyText>
<sectionHeader confidence="0.935136" genericHeader="method">
1 Dictation Applications of ASR
</sectionHeader>
<bodyText confidence="0.999959456140351">
The application for our work is medical dictation
over the telephone. Medical dictation differs from
other telephony based ASR applications, e.g. airline
reservation systems, because the talkers are repeat
users and utterances are long. Dictations usually
consist of 1-30 minutes of speech. The talkers call
in 3-5 days per week and produce between 1 and 12
dictations each day they call. Hence a medical
dictation operation has access to hours of speech
for each talker.
Spontaneous telephone speech presents additional
challenges that are caused partly by a poor acoustic
signal and partly by the disfluent nature of
spontaneous speech. A number of researchers have
noted the effects of disfluencies on speech
recognition and have suggested various approaches
to dealing with them at language modeling and
post-processing stages. (Shriberg 1994, Shriberg
1996, Stolcke and Shriberg 1996, Stolcke et al.
1998, Shriberg and Stolcke 1996, Siu and
Ostendorf 1996, Heeman et al. 1996) Medical over-
the-telephone dictations can be classified as
spontaneous or quasi-spontaneous discourse
(Pakhomov 1999, Pakhomov and Savova 1999).
Most physicians do not read a script prepared in
advance, instead, they engage in spontaneous
monologues that display the full spectrum of
disfluencies found in conversational dialogs in
addition to other &amp;quot;disfluencies&amp;quot; characteristic of
dictated speech. An example of the latter is when a
physician gives instructions to the transcriptionist
to modify something in the preceding discourse,
sometimes as far as several paragraphs back.
Most ASR dictation applications focus on desktop
users; for example, Dragon, IBM, Philips and
Lernout &amp; Hauspie all sell desktop dictation
recognizers that work on high quality microphone
speech. Typically, the desktop system builds an
adapted acoustic model if the talker &amp;quot;enrolls&amp;quot;, i.e.
reads a prepared script that serves as a literal
transcription. Forced alignment of the script and
the speech provides the input to acoustic model
adaptation.
Enrollment makes it relatively easy to obtain literal
transcriptions for adaptation. However, enrollment
is not feasible for dictation over the telephone
primarily because most physicians will refuse to
take the time to enroll. The alternative is to hire
humans who will type literal transcriptions of
dictation until enough have been accumulated to
build an adapted model, an impractical solution for
a large scale operation that processes speech from
thousands of talkers. ATRS is appealing because it
can generate an approximation of literal
transcription that can replace enrollment scripts and
the need for manually generated literal
transcriptions.
</bodyText>
<sectionHeader confidence="0.895362" genericHeader="method">
2 Three Classes of Training Data
</sectionHeader>
<bodyText confidence="0.999829680851064">
In this paper, training texts for language and
acoustic models fall into three categories:
Non-Literal: Non-literal transcripts present the
meaning of what was spoken in a written form
appropriate for the domain. In a commercial
medical transcription operation, the non-literal
transcript will present the dictation in a format
appropriate for a medical record. This typically
involves (i.) ignoring filled pauses, pleasantries,
and repeats; (ii.) acting on directions for repairs
(&amp;quot;delete the second paragraph and put this in
instead...&amp;quot;); (iii.) adding non-dictated punctuation;
(iv.) correcting grammatical errors; and (v.) re-
formatting certain phrases such as &amp;quot;Lung are
Clear&amp;quot;, to a standard form such as &amp;quot;Lungs - Clear&amp;quot;.
Literal: Literal transcriptions are exact
transcriptions of what was spoken. This includes
any elements not found in the non-literal transcript,
such as filled pauses (um&apos;s and ah&apos;s), pleasantries
and body noises (&amp;quot;thank you very much, just a
moment, cough&amp;quot;), repeats, fragments, repairs and
directions for repairs, and asides (&amp;quot;make that
bold&amp;quot;). Literal transcriptions require significant
human effort, and therefore are expensive to
produce. Even though they are carefully prepared,
some errors will be present in the result.
In their study of how humans deal with transcribing
spoken discourse, Lindsay and O&apos;Connell (1995)
have found that literal transcripts were &amp;quot;far from
verbatim.&amp;quot; (p.111) They find that the transcribers in
their study tended to have the most difficulty
transcribing hesitation phenomena, followed by
sentence fragments, adverbs and conjunctions and,
finally, nouns, verbs, adjectives and prepositions.
Our informal observations made from the
transcripts produced by highly trained medical
transcriptionists suggest approximately 5% error
margin and a gradation of errors similar to
the one found by Lindsay and O&apos;Connell.
Semi-Literal: Semi-literal transcripts are derived
using non-literal transcripts, the recognizer output,
a set of grammars, a dictionary, and an interpreter
to integrate the recognized material into the non-
literal transcription. Semi-literal transcripts will
more closely resemble the literal transcripts, as
many of the elements missing from the non-literal
transcripts will be restored.
</bodyText>
<sectionHeader confidence="0.981267" genericHeader="method">
3 Model Adaptation
</sectionHeader>
<bodyText confidence="0.999987394736843">
It is well known that ASR systems perform best
when acoustic models are adapted to a particular
talker’s speech. This is why commercial desktop
systems use enrollment. Although less widely
applied, language model adaptation based on linear
interpolation is an effective technique for tailoring
stochastic grammars to particular domains of
discourse and to particular speakers (Savova et al.
(2000), Weng et al. (1997)).
The training texts used in acoustic modeling come
from recognizer-generated texts, literal
transcriptions or non-literal transcriptions. Within
the family of transformation and combined
approaches to acoustic modeling (Digalakis and
Neumeyer (1996), Strom (1996), Wightman and
Harder (1999), Hazen and Glass (1997)) three basic
adaptation methods can be identified: unsupervised,
supervised, or semi-supervised. Each adaptation
method depends on a different type of training text.
What follows will briefly introduce the three
methods.
Unsupervised adaptation relies on the
recognizer’s output as the text guiding the
adaptation. Efficacy of unsupervised adaptation
fully depends on the recognition accuracy. As
Wightman and Harder (1999) pointed out,
unsupervised adaptation works well in laboratory
conditions when the speech signal has large
bandwidth and is relatively “clean” of background
noise, throat clearings, and other disturbances. In
laboratory conditions, the errors introduced by
unsupervised adaptation can be averaged out by
using more data (Zavaliagkos and Colthurst, 1997);
however, in a telephony operation with degraded
input that is not feasible.
Supervised adaptation is dependent on literal
transcription availability and is widely used in
enrollment in most desktop ASR systems. A
speaker’s speech sample is transcribed verbatim
and then the speech signal is aligned with
pronunciations frame by frame for each individual
word. A speaker independent model is augmented
to include the observations resulting from the
alignment.
Semi-supervised adaptation rests on the idea that
the speech signal can be partially aligned by using
of the recognition output and the non-literal
transcription. A significant problem with semi-
supervised adaptation is that only the speech that
the recognizer already recognizes successfully ends
up being used for adaptation. This reinforces what
is already well represented in the model.
Wightman and Harder (1999) report that semi-
supervised adaptation has a positive side effect of
excluding those segments of speech that were mis-
recognized for reasons other than a poor acoustic
model. They note that background noise and
speech disfluency are detrimental to the
unsupervised adaptation.
In addition to the two problems with semi-
supervised adaptation pointed out by Wightman
and Harder, we find one more potential problem.
As a result of matching the word labels produced
by the recognizer and the non-literal transcription,
some words may be skipped which may introduce
unnatural phone transitions at word boundaries.
Language model adaptation is not an appropriate
domain for acoustic adaptation methods. However,
adapted language models can be loosely described
as supervised or unsupervised, based on the types
of training texts—literal or non-literal—that were
used in building the model.
In the following sections we will describe the
system of generating data that is well suited for
acoustic and language adaptation and present
results of experimental evaluation of this system.
</bodyText>
<subsectionHeader confidence="0.999942">
3.2 Generating semi-literal data
</subsectionHeader>
<bodyText confidence="0.990227755319149">
ATRS is based on reconstruction of non-literal
transcriptions to train utterance specific language
models. First, a non-literal transcription is used to
train an augmented probabilistic finite state model
(APFSM) which is, in turn, used by the recognizer
to re-recognize the exact same utterance that the
non-literal transcription was generated from. The
APFSM is constructed by linear interpolation of a
finite state model where all transitional
probabilities are equal to 1 with two other
stochastic models.
One of the two models is a background model that
accounts for expressions such as greetings,
thanking, false starts and repairs. A list of these
out-of-transcription expressions is derived by
comparing already existing literal transcriptions
with their non-literal transcription counterparts.
The other model represents the same non-literal
transcription populated with filled pauses (FP)
(“um’s and ah’s”) using a stochastic FP model
derived from a relatively large corpus of literal
transcriptions (Pakhomov, 1999, Pakhomov and
Savova, 1999).
pronunciations based on the existing dictionary
spelling-pronunciation alignments. The result of
interpolating these two background models is that
some of the transitional probabilities found in the
finite state model are no longer 1.
The language model so derived can now be used to
produce a transcription that is likely to be more true
to what has actually been said than the non-literal
transcription that we started to work with.
Further refinement of the new semi-literal
transcription is carried out by using dynamic
programming alignment on the recognizer’s
hypothesis (HYP) and the non-literal transcription
that is used as reference (REF). The alignment
results in each HYP label being designated as a
MATCH, a DELETION, a SUBSTITUTION or an
INSERTION. Those labels present in the HYP
stream that do not align with anything in the REF
stream are designated as insertions and are assumed
to represent the out-of-transcription elements of the
dictation. Those labels that do align but do not
match are designated as substitutions. Finally, the
labels found in the REF stream that do not align
with anything in the HYP stream are designated as
deletions.
Figure { SEQ Figure \* ARABIC } Percent improvement in true data representation
of ATRS reconstruction vs. Non-Literal data
Interpolation weights are established empirically by The final semi-literal transcription is constructed
calculating the resulting model’s perplexity against differently depending on the intended purpose of
held out data. Out-of-vocabulary (OOV) items are
handled provisionally by generating on-the-fly
the transcription. If the transcription will be used
for acoustic modeling, then the MATCHES, the
REF portion of SUBSTITUTIONS and the HYP
portion of only those INSERTIONS that represent
punctuation and filled pauses make it into the final
semi-literal transcription. It is important to filter
out everything else because acoustic modeling is
very sensitive to misalignment errors. Language
modeling, on the other hand, is less sensitive to
alignment errors; therefore, INSERTIONS and
DELETIONS can be introduced into the semi-
literal transcription.
One method of ascertaining the quality of semi-
literal reconstruction is to measure its alignment
errors against literal data using a dynamic
programming application. By measuring the
correctness spread between ATRS and literal data,
as well as the correctness spread between non-
literal and literal data, the ATRS alignment
correctness rate was observed to be 4.4% higher
absolute over 774 dictation files tested. Chart 1
summarizes the results. The X axis represents the
number of dictations in each bin displayed along
the Y axis representing the % improvement over
the non-literal counterparts. The results showed
nearly all ATRS files had better alignment
correctness than their non-literal counterparts. The
majority of the reconstructed dictations resemble
literal transcriptions between 1% and 8% better
than their non-literal counterparts. These results
are statistically significant as evidenced by a t-test
at 0.05 confidence level. Much of the increase in
alignment can be attributed to the introduction of
filled pauses by ATRS. However, ignoring filled
pauses, we have observed informally that the
correctness still improves in ATRS files versus
non-literal.
In the following sections we will address acoustic
and language modeling and show that semi-literal
training data is a good substitute for literal data.
</bodyText>
<sectionHeader confidence="0.998477" genericHeader="method">
4 Experimental results
</sectionHeader>
<bodyText confidence="0.998483">
The usefulness of semi-literal transcriptions was
evaluated in two ways: acoustic adaptation and
language modeling.
</bodyText>
<subsectionHeader confidence="0.943253">
4.1 Adapted acoustic model evaluation
</subsectionHeader>
<bodyText confidence="0.999941">
Three speaker adapted acoustic models were
trained for each of the 5 talkers in this study using
the three types of label files and evaluated on the
talker’s testing data.
</bodyText>
<subsectionHeader confidence="0.701964">
4.1.1 Setup
</subsectionHeader>
<bodyText confidence="0.996378">
The data collected for each talker were split into
testing and training.
</bodyText>
<subsectionHeader confidence="0.805294">
Training Data
</subsectionHeader>
<bodyText confidence="0.864531">
45-55 minutes of audio data was collected for each
of the six talkers in this experiment:
</bodyText>
<figure confidence="0.9608214">
A female
B female
C male
D male
F female
</figure>
<bodyText confidence="0.991803153846154">
All talkers are native speakers of English, two
males and three females.
Non-literal transcriptions of this data were
obtained in the course of normal transcription
operation where trained medical transcriptionists
record the dictations while filtering out disfluency,
asides and ungrammatical utterances.
Literal transcriptions were obtained by having 5
medical transcriptionists specially trained not to
filter out disfluency and asides transcribe all the
dictations used in this study.
Semi-literal transcriptions were obtained with the
system described in section 5 of this paper.
</bodyText>
<subsectionHeader confidence="0.910689">
Testing Data
</subsectionHeader>
<bodyText confidence="0.946010285714286">
Three dictations (0.5 – 2 min) each were pulled out
of the Literal transcriptions training set and set
aside for each talker for testing.
Recognition and evaluation software and
formalism
Software licensed from Entropic Laboratory was
used for performing recognition, evaluating
accuracy and acoustic adaptation. (Valtchev, et al.
(1998)). Adapted models were trained using MLLR
technique (Legetter and Woodland, (1996)) speakers in this study were not used in training the
available as part of the Entropic package. SI model.
Recognition accuracy and correctness reported in
this study were calculated according to the
following formulas:
</bodyText>
<listItem confidence="0.997658">
(1) Acc = hits – insertions / total words
(2) Correctness = hits / total words
</listItem>
<subsectionHeader confidence="0.856732">
4.1.2 Experiment
</subsectionHeader>
<bodyText confidence="0.94044603125">
The following Acoustic Models were trained via
adaptation with a general SI model for each talker
using all available data (except for the testing data).
Each model’s name reflects the kind of label data
that was used for training.
LITERAL
Each audio file was aligned with the corresponding
literal transcription.
NON-LITERAL
Each audio file was recognized using SI acoustic
and language models. The recognition output was
aligned with the non-literal transcription using
dynamic programming. Only those portions of
audio that corresponded to direct matches in the
alignment were used to produce alignments for
acoustic modeling. This method was originally used
for medical dictations by Wightman and Harder
(1999).
SEMI-LITERAL
Each audio file has been processed to produce a
semi-literal transcription that was then aligned with
recognition output generated in the process of
creating semi-literal transcriptions. The portions of
the audio corresponding to matching segments were
used for acoustic adaptation training.
The SI model had been trained on all available at
the time (12 hours)2 similar medical dictations to
the ones used in this study. The data for the
2 Although 50-100 hours of data for SI modeling is the
industry standard, the population we are dealing with is
highly homogeneous and reasonable results can be
obtained with lesser amount of data.
</bodyText>
<sectionHeader confidence="0.616959" genericHeader="evaluation">
4.1.3 Results
</sectionHeader>
<bodyText confidence="0.999905769230769">
Table 1 shows the test results. As expected, both
recognition accuracy and correctness increase with
any of the three kinds of adaptation. Adaptation
using Literal transcriptions yields an overall
10.84% absolute gain in correctness and 11.49% in
accuracy over the baseline.
Adaptation using Non-literal transcriptions yields
an overall 6.36 % absolute gain in correctness and
5.23 % in accuracy over the baseline. Adaptation
with Semi-literal transcriptions yields an overall
11.39 % absolute gain in correctness and 11.05 %
in accuracy over the baseline. No statistical
significance tests were performed on this data.
</bodyText>
<table confidence="0.999895777777778">
Baseline (SI) Literal Semi-literal Non-literal
iia iia iia iia
Talker Cor Acc Cor Acc Cor Acc Cor Acc
A 58.76 48.47 66.57 58.09 68 58.28 64.76 51.8
B 41.28 32.2 58.36 49.46 64.59 56.22 55.87 44.66
C 57.22 54.99 64.38 61.54 61.25 59.31 60.65 58.71
D 56.86 51.47 68.69 63.3 65.91 59.13 64.69 58.26
F 54.83 43.69 61.97 53.57 64.7 54.41 61.13 48.73
AVG 52.49 44.81 63.33 56.3 63.81 55.86 58.85 50.04
</table>
<tableCaption confidence="0.8611595">
Table 1. Recognition results for three adaptation
methods
</tableCaption>
<subsectionHeader confidence="0.714707">
4.1.4 Discussion
</subsectionHeader>
<bodyText confidence="0.994924882352941">
The results of this experiment provide additional
support for using automatically generated semi-
literal transcriptions as a viable (and possibly
superior) substitute for literal data. The fact that
three SEMI-LITERAL adapted AM’s out of 5
performed better than their LITERAL counterparts
seems to indicate that there may be undesirable
noise either in the literal transcriptions or in the
corresponding audio. It may also be due to the
relatively small amount of training data used for SI
modeling thus providing a baseline that can be
improved with little effort. However, the results
still indicate that generating semi-literal
transcriptions may help eliminate the undesirable
noise and, at the same time, get the benefits of
broader coverage that semi-literal transcripts can
afford over NON-LITERAL transcriptions.
</bodyText>
<subsectionHeader confidence="0.97517">
4.2 Language Model Evaluation
</subsectionHeader>
<bodyText confidence="0.988107162790697">
For ASR applications where there are significant
discrepancies between an utterance and its formal
transcription, the inclusion of literal data in the
language model can reduce language model
perplexity and improve recognition accuracy. In
medical transcription, the non-literal texts typically
depart from what has actually been said. Hence if
the talker says &amp;quot;lungs are clear&amp;quot; or &amp;quot;lungs sound
pretty clear&amp;quot;, the typed transcription is likely to
have &amp;quot;Lungs - clear&amp;quot;. In addition, as we noted
earlier, the non-literal transcription will omit
disfluencies and asides and will correct
grammatical errors.
Literal and semi-literal texts can be added onto
language model training data or interpolated into
an existing language model. Below we will present
results of a language modeling experiment that
compares language models built from literal, semi-
literal and non-literal versions of the same training
set. The results substantiate our claim that
automatically generated semi-literal transcription
can lead to a significant improvement in language
model quality.
In order to test the proposed method’s suitability
for language modeling, we constructed three
trigram language models and used perplexity as the
measure of the models’ goodness.
Setup
The following models were trained on three
versions of a 270,000-word corpus. The size of the
training corpus is dictated by availability of literal
transcriptions. The vocabulary was derived from a
combination of all three corpora to keep the OOV
rate constant.
LLM – language model built from a corpus of
literal transcriptions
NLM – language model built from non-literal
transcriptions
SLM – language model built from semi-literal
transcriptions
Approximately 5,000-word literal transcriptions
corpus consisting of 24 dictations was set aside for
testing
</bodyText>
<sectionHeader confidence="0.923849" genericHeader="evaluation">
Results
</sectionHeader>
<bodyText confidence="0.994294">
The results of perplexity tests of the three models
on the held-out data at 3-gram level are
summarized in Table 2. The tests were carried out
using the Entropic Transcriber Toolkit
It is apparent that SLM yields considerably better
perplexity than NLM, which indicates that although
semi-literal transcriptions are not as good as actual
literal transcriptions, they are more suitable for
</bodyText>
<table confidence="0.99947">
Perplexity OOV rate (%)
LLM 185 2.61
NLM 613 2.61
SLM 313 2.61
</table>
<tableCaption confidence="0.999912">
Table 2. Perplexity tests on LLM, NLM, SLM
</tableCaption>
<bodyText confidence="0.999813666666667">
language modeling than non-literal transcriptions.
These results are obtained with 270,000 words of
training data; however, the typical amount is
dozens of million. We would expect the differences
in perplexity to become smaller with larger
amounts of training data.
</bodyText>
<subsectionHeader confidence="0.433017">
Conclusions and future work
</subsectionHeader>
<bodyText confidence="0.99996012">
We have described ATRS, a system for
reconstructing semi-literal transcriptions
automatically. ATRS texts can be used as a
substitute for literal transcriptions when the cost
and time required for generating literal
transcriptions are infeasible, e.g. in a telephony
based transcription operation that processes
thousands of acoustic and language models. Texts
produced with ATRS were used in training speaker
adapted acoustic models, speaker independent
acoustic models and language models.
Experimental results show that models built from
ATRS training data yield performance results that
are equivalent to those obtained with models
trained on literal transcriptions. In the future, we
will address the issue of the amount of training data
for the SI model. Also, current ATRS system does
not take advantage of various confidence scores
available in leading recognition engines. We
believe that using such confidence measures can
improve the generation of semi-literal transcriptions
considerably. We would also like to investigate the
point at which the size of the various kinds of data
used for adaptation stops making improvements in
recognition accuracy.
</bodyText>
<sectionHeader confidence="0.992142" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998948">
We would like to thank the anonymous reviewers
of this paper for very helpful feedback. We thank
Guergana Savova for excellent suggestions and
enthusiastic support. We would also like to thank
Jim Wu for valuable input.
</bodyText>
<sectionHeader confidence="0.999183" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999943784615385">
Digalakis, V and Neumyer, L. (1996). Speaker
Adaptation Using Combined Transformation
and Baysean Mehtods. IEEE Trans. Speech and
Audio Processing.
Hazen, T and Glass, J (1997). A Comparison of Novel
Techniques for Instantaneous Speaker
Adaptation. In Proc. Eurospeech ’97.
Heeman, P., Loken-Kim, K and Allen J. (1996).
Combining the Detection and Correction of
Speech Repairs. In Proc. ICSLP ‘96.
Huang, X. and Lee, K (1993). On Speaker –
Independent, Speaker-Dependent, and Speaker-
Adaptive Speech Recognition. In IEEE
Transactions on Speech and Audio processing,
Vol. 1, No. 2, pp. 150 – 157.
Legetter, C. and Woodland, P. (1996). Maximum
Likelihood Linear Regression for Speaker
Adaptation of Continuous Density HMM’s. In
Computer Speech and Language , 9, (171-186).
Pakhomov, S. (1999). Modeling Filled Pauses in
Medical Transcriptions. In Student Section of
Proc. ACL’99.
Pakhomov, S and Savova, G. (1999). Filled Pause
Modeling in Quasi-Spontaneous Speech. In
Proc. Disfluency in Spontaneous Speech
Workshop at ICPHIS ’99.
Savova, G, Schonwetter, M. and Pakhomov, S. (2000).
Improving language model perplexity and
recognition accuracy for medical dictations via
within-domaininterpolation with literal and
semi-literal corpora &amp;quot; In Proc. ICSLP ‘00.
Shriberg, E. 1994 Preliminaries to a Theory of Speech
Disfluencies. Ph. D. thesis, University of
California at Berkely.
Shriberg, E. and Stolcke, A. (1996). Word Predictability
after Hesitations: A Corpus-based Study. In
Proc. ICSLP ‘96.
Siu, M and Ostendorf, M. (1996). Modeling Disfluencies
in Conversational Speech. In Proc. ICSLP ‘96.
Stolcke, A. and Shriberg, E. (1996). Statistical Language
Modeling for Speech Disfluencies. In proc.
ICASSP ‘96.
Stolcke A., Shriberg E., Bates R., Ostendorf M., Hakkani
D., Plauche M., Tur G., and Lu Y. (1998).
Automatic Detection of Sentence Boundaries
and Disfluencies based on Recognized Words.
Proc. Intl. Conf. on Spoken Language
Processing.
Ström, N (1996): &amp;quot;Speaker Adaptation by Modeling the
Speaker Variation in a Continuous Speech
Recognition System,&amp;quot; In Proc. ICSLP &apos;96,
Philadelphia, pp. 989-992.
Valtchev, V. Kershaw, D. and Odell, J. (1998). The
Truetalk Transcriber Book. Entropic
Cambridge Research Laboratory, Cambridge,
England.
Wightman, C. W. and Harder T. A. (1999). Semi-
Supervised Adaptation of Acoustic Models for
Large-Volume Dictation” In Proc. Eurospeech
’98. pp 1371-1374.
Weng, F., Stolcke, A., Sankar, A. (1997). Hub4
Language Modeling Using Domain
Interpolation and Data Clustering. Proc.
DARPA Speech Recognition Workshop, pp.
147-151, Chantilly, VA.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.112279">
<title confidence="0.999786">Generating Training Data for Medical Dictations</title>
<author confidence="0.943245">Sergey</author>
<affiliation confidence="0.999949">University of Minnesota,</affiliation>
<email confidence="0.999782">pakhomov.sergey@mayo.edu</email>
<author confidence="0.986398">Michael Schonwetter</author>
<address confidence="0.43487">Linguistech Consortium, NJ</address>
<email confidence="0.8695">MSchonwetter@qwest.net</email>
<author confidence="0.7509375">Joan Bachenko Linguistech Consortium</author>
<author confidence="0.7509375">NJ</author>
<email confidence="0.976452">bachenko@mnic.net</email>
<abstract confidence="0.981722565217391">In automatic speech recognition (ASR) enabled applications for medical dictations, corpora of literal transcriptions of speech are critical for training both speaker independent and speaker adapted acoustic models. Obtaining these transcriptions is both costly and time consuming. Non-literal transcriptions, on the other hand, are easy to obtain because they are generated in the normal course of a medical transcription operation. This paper presents a method of automatically generating texts that can take the place of literal transcriptions for training acoustic and language is an automatic transcription reconstruction system that can produce near-literal transcriptions with almost no human labor. We will show that (i) adapted acoustic models trained on ATRS data perform as well as or better than adapted acoustic models trained on literal transcriptions (as measured by recognition accuracy) and (ii) language models trained on ATRS data have lower perplexity than language trained on non-literal</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>V Digalakis</author>
<author>L Neumyer</author>
</authors>
<title>Speaker Adaptation Using Combined Transformation and Baysean Mehtods.</title>
<date>1996</date>
<journal>IEEE Trans. Speech and Audio Processing.</journal>
<marker>Digalakis, Neumyer, 1996</marker>
<rawString>Digalakis, V and Neumyer, L. (1996). Speaker Adaptation Using Combined Transformation and Baysean Mehtods. IEEE Trans. Speech and Audio Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hazen</author>
<author>J Glass</author>
</authors>
<title>A Comparison of Novel Techniques for Instantaneous Speaker Adaptation.</title>
<date>1997</date>
<booktitle>In Proc. Eurospeech ’97.</booktitle>
<contexts>
<context position="9233" citStr="Hazen and Glass (1997)" startWordPosition="1320" endWordPosition="1323"> This is why commercial desktop systems use enrollment. Although less widely applied, language model adaptation based on linear interpolation is an effective technique for tailoring stochastic grammars to particular domains of discourse and to particular speakers (Savova et al. (2000), Weng et al. (1997)). The training texts used in acoustic modeling come from recognizer-generated texts, literal transcriptions or non-literal transcriptions. Within the family of transformation and combined approaches to acoustic modeling (Digalakis and Neumeyer (1996), Strom (1996), Wightman and Harder (1999), Hazen and Glass (1997)) three basic adaptation methods can be identified: unsupervised, supervised, or semi-supervised. Each adaptation method depends on a different type of training text. What follows will briefly introduce the three methods. Unsupervised adaptation relies on the recognizer’s output as the text guiding the adaptation. Efficacy of unsupervised adaptation fully depends on the recognition accuracy. As Wightman and Harder (1999) pointed out, unsupervised adaptation works well in laboratory conditions when the speech signal has large bandwidth and is relatively “clean” of background noise, throat clear</context>
</contexts>
<marker>Hazen, Glass, 1997</marker>
<rawString>Hazen, T and Glass, J (1997). A Comparison of Novel Techniques for Instantaneous Speaker Adaptation. In Proc. Eurospeech ’97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Heeman</author>
<author>K Loken-Kim</author>
<author>J Allen</author>
</authors>
<title>Combining the Detection and Correction of Speech Repairs.</title>
<date>1996</date>
<booktitle>In Proc. ICSLP ‘96.</booktitle>
<contexts>
<context position="4500" citStr="Heeman et al. 1996" startWordPosition="638" endWordPosition="641">ations each day they call. Hence a medical dictation operation has access to hours of speech for each talker. Spontaneous telephone speech presents additional challenges that are caused partly by a poor acoustic signal and partly by the disfluent nature of spontaneous speech. A number of researchers have noted the effects of disfluencies on speech recognition and have suggested various approaches to dealing with them at language modeling and post-processing stages. (Shriberg 1994, Shriberg 1996, Stolcke and Shriberg 1996, Stolcke et al. 1998, Shriberg and Stolcke 1996, Siu and Ostendorf 1996, Heeman et al. 1996) Medical overthe-telephone dictations can be classified as spontaneous or quasi-spontaneous discourse (Pakhomov 1999, Pakhomov and Savova 1999). Most physicians do not read a script prepared in advance, instead, they engage in spontaneous monologues that display the full spectrum of disfluencies found in conversational dialogs in addition to other &amp;quot;disfluencies&amp;quot; characteristic of dictated speech. An example of the latter is when a physician gives instructions to the transcriptionist to modify something in the preceding discourse, sometimes as far as several paragraphs back. Most ASR dictation </context>
</contexts>
<marker>Heeman, Loken-Kim, Allen, 1996</marker>
<rawString>Heeman, P., Loken-Kim, K and Allen J. (1996). Combining the Detection and Correction of Speech Repairs. In Proc. ICSLP ‘96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Huang</author>
<author>K Lee</author>
</authors>
<title>On Speaker – Independent, Speaker-Dependent, and SpeakerAdaptive Speech Recognition.</title>
<date>1993</date>
<booktitle>In IEEE Transactions on Speech and Audio processing,</booktitle>
<volume>1</volume>
<pages>150--157</pages>
<marker>Huang, Lee, 1993</marker>
<rawString>Huang, X. and Lee, K (1993). On Speaker – Independent, Speaker-Dependent, and SpeakerAdaptive Speech Recognition. In IEEE Transactions on Speech and Audio processing, Vol. 1, No. 2, pp. 150 – 157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Legetter</author>
<author>P Woodland</author>
</authors>
<title>Maximum Likelihood Linear Regression for Speaker Adaptation of Continuous Density HMM’s.</title>
<date>1996</date>
<journal>In Computer Speech and Language ,</journal>
<volume>9</volume>
<pages>171--186</pages>
<contexts>
<context position="18108" citStr="Legetter and Woodland, (1996)" startWordPosition="2633" endWordPosition="2636">ly trained not to filter out disfluency and asides transcribe all the dictations used in this study. Semi-literal transcriptions were obtained with the system described in section 5 of this paper. Testing Data Three dictations (0.5 – 2 min) each were pulled out of the Literal transcriptions training set and set aside for each talker for testing. Recognition and evaluation software and formalism Software licensed from Entropic Laboratory was used for performing recognition, evaluating accuracy and acoustic adaptation. (Valtchev, et al. (1998)). Adapted models were trained using MLLR technique (Legetter and Woodland, (1996)) speakers in this study were not used in training the available as part of the Entropic package. SI model. Recognition accuracy and correctness reported in this study were calculated according to the following formulas: (1) Acc = hits – insertions / total words (2) Correctness = hits / total words 4.1.2 Experiment The following Acoustic Models were trained via adaptation with a general SI model for each talker using all available data (except for the testing data). Each model’s name reflects the kind of label data that was used for training. LITERAL Each audio file was aligned with the corres</context>
</contexts>
<marker>Legetter, Woodland, 1996</marker>
<rawString>Legetter, C. and Woodland, P. (1996). Maximum Likelihood Linear Regression for Speaker Adaptation of Continuous Density HMM’s. In Computer Speech and Language , 9, (171-186).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pakhomov</author>
</authors>
<title>Modeling Filled Pauses in Medical Transcriptions.</title>
<date>1999</date>
<booktitle>In Student Section of Proc. ACL’99.</booktitle>
<contexts>
<context position="4616" citStr="Pakhomov 1999" startWordPosition="654" endWordPosition="655">telephone speech presents additional challenges that are caused partly by a poor acoustic signal and partly by the disfluent nature of spontaneous speech. A number of researchers have noted the effects of disfluencies on speech recognition and have suggested various approaches to dealing with them at language modeling and post-processing stages. (Shriberg 1994, Shriberg 1996, Stolcke and Shriberg 1996, Stolcke et al. 1998, Shriberg and Stolcke 1996, Siu and Ostendorf 1996, Heeman et al. 1996) Medical overthe-telephone dictations can be classified as spontaneous or quasi-spontaneous discourse (Pakhomov 1999, Pakhomov and Savova 1999). Most physicians do not read a script prepared in advance, instead, they engage in spontaneous monologues that display the full spectrum of disfluencies found in conversational dialogs in addition to other &amp;quot;disfluencies&amp;quot; characteristic of dictated speech. An example of the latter is when a physician gives instructions to the transcriptionist to modify something in the preceding discourse, sometimes as far as several paragraphs back. Most ASR dictation applications focus on desktop users; for example, Dragon, IBM, Philips and Lernout &amp; Hauspie all sell desktop dictat</context>
<context position="13020" citStr="Pakhomov, 1999" startWordPosition="1873" endWordPosition="1874"> state model where all transitional probabilities are equal to 1 with two other stochastic models. One of the two models is a background model that accounts for expressions such as greetings, thanking, false starts and repairs. A list of these out-of-transcription expressions is derived by comparing already existing literal transcriptions with their non-literal transcription counterparts. The other model represents the same non-literal transcription populated with filled pauses (FP) (“um’s and ah’s”) using a stochastic FP model derived from a relatively large corpus of literal transcriptions (Pakhomov, 1999, Pakhomov and Savova, 1999). pronunciations based on the existing dictionary spelling-pronunciation alignments. The result of interpolating these two background models is that some of the transitional probabilities found in the finite state model are no longer 1. The language model so derived can now be used to produce a transcription that is likely to be more true to what has actually been said than the non-literal transcription that we started to work with. Further refinement of the new semi-literal transcription is carried out by using dynamic programming alignment on the recognizer’s hypo</context>
</contexts>
<marker>Pakhomov, 1999</marker>
<rawString>Pakhomov, S. (1999). Modeling Filled Pauses in Medical Transcriptions. In Student Section of Proc. ACL’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pakhomov</author>
<author>G Savova</author>
</authors>
<title>Filled Pause Modeling in Quasi-Spontaneous Speech.</title>
<date>1999</date>
<booktitle>In Proc. Disfluency in Spontaneous Speech Workshop at ICPHIS ’99.</booktitle>
<contexts>
<context position="4643" citStr="Pakhomov and Savova 1999" startWordPosition="656" endWordPosition="659">h presents additional challenges that are caused partly by a poor acoustic signal and partly by the disfluent nature of spontaneous speech. A number of researchers have noted the effects of disfluencies on speech recognition and have suggested various approaches to dealing with them at language modeling and post-processing stages. (Shriberg 1994, Shriberg 1996, Stolcke and Shriberg 1996, Stolcke et al. 1998, Shriberg and Stolcke 1996, Siu and Ostendorf 1996, Heeman et al. 1996) Medical overthe-telephone dictations can be classified as spontaneous or quasi-spontaneous discourse (Pakhomov 1999, Pakhomov and Savova 1999). Most physicians do not read a script prepared in advance, instead, they engage in spontaneous monologues that display the full spectrum of disfluencies found in conversational dialogs in addition to other &amp;quot;disfluencies&amp;quot; characteristic of dictated speech. An example of the latter is when a physician gives instructions to the transcriptionist to modify something in the preceding discourse, sometimes as far as several paragraphs back. Most ASR dictation applications focus on desktop users; for example, Dragon, IBM, Philips and Lernout &amp; Hauspie all sell desktop dictation recognizers that work o</context>
<context position="13048" citStr="Pakhomov and Savova, 1999" startWordPosition="1875" endWordPosition="1878">re all transitional probabilities are equal to 1 with two other stochastic models. One of the two models is a background model that accounts for expressions such as greetings, thanking, false starts and repairs. A list of these out-of-transcription expressions is derived by comparing already existing literal transcriptions with their non-literal transcription counterparts. The other model represents the same non-literal transcription populated with filled pauses (FP) (“um’s and ah’s”) using a stochastic FP model derived from a relatively large corpus of literal transcriptions (Pakhomov, 1999, Pakhomov and Savova, 1999). pronunciations based on the existing dictionary spelling-pronunciation alignments. The result of interpolating these two background models is that some of the transitional probabilities found in the finite state model are no longer 1. The language model so derived can now be used to produce a transcription that is likely to be more true to what has actually been said than the non-literal transcription that we started to work with. Further refinement of the new semi-literal transcription is carried out by using dynamic programming alignment on the recognizer’s hypothesis (HYP) and the non-lit</context>
</contexts>
<marker>Pakhomov, Savova, 1999</marker>
<rawString>Pakhomov, S and Savova, G. (1999). Filled Pause Modeling in Quasi-Spontaneous Speech. In Proc. Disfluency in Spontaneous Speech Workshop at ICPHIS ’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Savova</author>
<author>M Schonwetter</author>
<author>S Pakhomov</author>
</authors>
<title>Improving language model perplexity and recognition accuracy for medical dictations via within-domaininterpolation with literal and semi-literal corpora &amp;quot; In</title>
<date>2000</date>
<booktitle>Proc. ICSLP ‘00.</booktitle>
<contexts>
<context position="8896" citStr="Savova et al. (2000)" startWordPosition="1275" endWordPosition="1278">material into the nonliteral transcription. Semi-literal transcripts will more closely resemble the literal transcripts, as many of the elements missing from the non-literal transcripts will be restored. 3 Model Adaptation It is well known that ASR systems perform best when acoustic models are adapted to a particular talker’s speech. This is why commercial desktop systems use enrollment. Although less widely applied, language model adaptation based on linear interpolation is an effective technique for tailoring stochastic grammars to particular domains of discourse and to particular speakers (Savova et al. (2000), Weng et al. (1997)). The training texts used in acoustic modeling come from recognizer-generated texts, literal transcriptions or non-literal transcriptions. Within the family of transformation and combined approaches to acoustic modeling (Digalakis and Neumeyer (1996), Strom (1996), Wightman and Harder (1999), Hazen and Glass (1997)) three basic adaptation methods can be identified: unsupervised, supervised, or semi-supervised. Each adaptation method depends on a different type of training text. What follows will briefly introduce the three methods. Unsupervised adaptation relies on the rec</context>
</contexts>
<marker>Savova, Schonwetter, Pakhomov, 2000</marker>
<rawString>Savova, G, Schonwetter, M. and Pakhomov, S. (2000). Improving language model perplexity and recognition accuracy for medical dictations via within-domaininterpolation with literal and semi-literal corpora &amp;quot; In Proc. ICSLP ‘00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
</authors>
<title>Preliminaries to a Theory of Speech Disfluencies.</title>
<date>1994</date>
<tech>Ph. D. thesis,</tech>
<institution>University of California at Berkely.</institution>
<contexts>
<context position="4365" citStr="Shriberg 1994" startWordPosition="618" endWordPosition="619">ng. Dictations usually consist of 1-30 minutes of speech. The talkers call in 3-5 days per week and produce between 1 and 12 dictations each day they call. Hence a medical dictation operation has access to hours of speech for each talker. Spontaneous telephone speech presents additional challenges that are caused partly by a poor acoustic signal and partly by the disfluent nature of spontaneous speech. A number of researchers have noted the effects of disfluencies on speech recognition and have suggested various approaches to dealing with them at language modeling and post-processing stages. (Shriberg 1994, Shriberg 1996, Stolcke and Shriberg 1996, Stolcke et al. 1998, Shriberg and Stolcke 1996, Siu and Ostendorf 1996, Heeman et al. 1996) Medical overthe-telephone dictations can be classified as spontaneous or quasi-spontaneous discourse (Pakhomov 1999, Pakhomov and Savova 1999). Most physicians do not read a script prepared in advance, instead, they engage in spontaneous monologues that display the full spectrum of disfluencies found in conversational dialogs in addition to other &amp;quot;disfluencies&amp;quot; characteristic of dictated speech. An example of the latter is when a physician gives instructions t</context>
</contexts>
<marker>Shriberg, 1994</marker>
<rawString>Shriberg, E. 1994 Preliminaries to a Theory of Speech Disfluencies. Ph. D. thesis, University of California at Berkely.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
<author>A Stolcke</author>
</authors>
<title>Word Predictability after Hesitations: A Corpus-based Study.</title>
<date>1996</date>
<booktitle>In Proc. ICSLP ‘96.</booktitle>
<contexts>
<context position="4455" citStr="Shriberg and Stolcke 1996" startWordPosition="630" endWordPosition="633">3-5 days per week and produce between 1 and 12 dictations each day they call. Hence a medical dictation operation has access to hours of speech for each talker. Spontaneous telephone speech presents additional challenges that are caused partly by a poor acoustic signal and partly by the disfluent nature of spontaneous speech. A number of researchers have noted the effects of disfluencies on speech recognition and have suggested various approaches to dealing with them at language modeling and post-processing stages. (Shriberg 1994, Shriberg 1996, Stolcke and Shriberg 1996, Stolcke et al. 1998, Shriberg and Stolcke 1996, Siu and Ostendorf 1996, Heeman et al. 1996) Medical overthe-telephone dictations can be classified as spontaneous or quasi-spontaneous discourse (Pakhomov 1999, Pakhomov and Savova 1999). Most physicians do not read a script prepared in advance, instead, they engage in spontaneous monologues that display the full spectrum of disfluencies found in conversational dialogs in addition to other &amp;quot;disfluencies&amp;quot; characteristic of dictated speech. An example of the latter is when a physician gives instructions to the transcriptionist to modify something in the preceding discourse, sometimes as far as</context>
</contexts>
<marker>Shriberg, Stolcke, 1996</marker>
<rawString>Shriberg, E. and Stolcke, A. (1996). Word Predictability after Hesitations: A Corpus-based Study. In Proc. ICSLP ‘96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Siu</author>
<author>M Ostendorf</author>
</authors>
<title>Modeling Disfluencies in Conversational Speech.</title>
<date>1996</date>
<booktitle>In Proc. ICSLP ‘96.</booktitle>
<contexts>
<context position="4479" citStr="Siu and Ostendorf 1996" startWordPosition="634" endWordPosition="637">ce between 1 and 12 dictations each day they call. Hence a medical dictation operation has access to hours of speech for each talker. Spontaneous telephone speech presents additional challenges that are caused partly by a poor acoustic signal and partly by the disfluent nature of spontaneous speech. A number of researchers have noted the effects of disfluencies on speech recognition and have suggested various approaches to dealing with them at language modeling and post-processing stages. (Shriberg 1994, Shriberg 1996, Stolcke and Shriberg 1996, Stolcke et al. 1998, Shriberg and Stolcke 1996, Siu and Ostendorf 1996, Heeman et al. 1996) Medical overthe-telephone dictations can be classified as spontaneous or quasi-spontaneous discourse (Pakhomov 1999, Pakhomov and Savova 1999). Most physicians do not read a script prepared in advance, instead, they engage in spontaneous monologues that display the full spectrum of disfluencies found in conversational dialogs in addition to other &amp;quot;disfluencies&amp;quot; characteristic of dictated speech. An example of the latter is when a physician gives instructions to the transcriptionist to modify something in the preceding discourse, sometimes as far as several paragraphs back</context>
</contexts>
<marker>Siu, Ostendorf, 1996</marker>
<rawString>Siu, M and Ostendorf, M. (1996). Modeling Disfluencies in Conversational Speech. In Proc. ICSLP ‘96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>E Shriberg</author>
</authors>
<title>Statistical Language Modeling for Speech Disfluencies. In proc.</title>
<date>1996</date>
<journal>ICASSP</journal>
<volume>96</volume>
<contexts>
<context position="4407" citStr="Stolcke and Shriberg 1996" startWordPosition="622" endWordPosition="625"> of 1-30 minutes of speech. The talkers call in 3-5 days per week and produce between 1 and 12 dictations each day they call. Hence a medical dictation operation has access to hours of speech for each talker. Spontaneous telephone speech presents additional challenges that are caused partly by a poor acoustic signal and partly by the disfluent nature of spontaneous speech. A number of researchers have noted the effects of disfluencies on speech recognition and have suggested various approaches to dealing with them at language modeling and post-processing stages. (Shriberg 1994, Shriberg 1996, Stolcke and Shriberg 1996, Stolcke et al. 1998, Shriberg and Stolcke 1996, Siu and Ostendorf 1996, Heeman et al. 1996) Medical overthe-telephone dictations can be classified as spontaneous or quasi-spontaneous discourse (Pakhomov 1999, Pakhomov and Savova 1999). Most physicians do not read a script prepared in advance, instead, they engage in spontaneous monologues that display the full spectrum of disfluencies found in conversational dialogs in addition to other &amp;quot;disfluencies&amp;quot; characteristic of dictated speech. An example of the latter is when a physician gives instructions to the transcriptionist to modify something</context>
</contexts>
<marker>Stolcke, Shriberg, 1996</marker>
<rawString>Stolcke, A. and Shriberg, E. (1996). Statistical Language Modeling for Speech Disfluencies. In proc. ICASSP ‘96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>E Shriberg</author>
<author>R Bates</author>
<author>M Ostendorf</author>
<author>D Hakkani</author>
<author>M Plauche</author>
<author>G Tur</author>
<author>Y Lu</author>
</authors>
<date>1998</date>
<booktitle>Automatic Detection of Sentence Boundaries and Disfluencies based on Recognized Words. Proc. Intl. Conf. on Spoken Language Processing.</booktitle>
<contexts>
<context position="4428" citStr="Stolcke et al. 1998" startWordPosition="626" endWordPosition="629"> The talkers call in 3-5 days per week and produce between 1 and 12 dictations each day they call. Hence a medical dictation operation has access to hours of speech for each talker. Spontaneous telephone speech presents additional challenges that are caused partly by a poor acoustic signal and partly by the disfluent nature of spontaneous speech. A number of researchers have noted the effects of disfluencies on speech recognition and have suggested various approaches to dealing with them at language modeling and post-processing stages. (Shriberg 1994, Shriberg 1996, Stolcke and Shriberg 1996, Stolcke et al. 1998, Shriberg and Stolcke 1996, Siu and Ostendorf 1996, Heeman et al. 1996) Medical overthe-telephone dictations can be classified as spontaneous or quasi-spontaneous discourse (Pakhomov 1999, Pakhomov and Savova 1999). Most physicians do not read a script prepared in advance, instead, they engage in spontaneous monologues that display the full spectrum of disfluencies found in conversational dialogs in addition to other &amp;quot;disfluencies&amp;quot; characteristic of dictated speech. An example of the latter is when a physician gives instructions to the transcriptionist to modify something in the preceding dis</context>
</contexts>
<marker>Stolcke, Shriberg, Bates, Ostendorf, Hakkani, Plauche, Tur, Lu, 1998</marker>
<rawString>Stolcke A., Shriberg E., Bates R., Ostendorf M., Hakkani D., Plauche M., Tur G., and Lu Y. (1998). Automatic Detection of Sentence Boundaries and Disfluencies based on Recognized Words. Proc. Intl. Conf. on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ström</author>
</authors>
<title>Speaker Adaptation by Modeling the Speaker Variation in a Continuous Speech Recognition System,&amp;quot;</title>
<date>1996</date>
<booktitle>In Proc. ICSLP &apos;96,</booktitle>
<pages>989--992</pages>
<location>Philadelphia,</location>
<marker>Ström, 1996</marker>
<rawString>Ström, N (1996): &amp;quot;Speaker Adaptation by Modeling the Speaker Variation in a Continuous Speech Recognition System,&amp;quot; In Proc. ICSLP &apos;96, Philadelphia, pp. 989-992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Kershaw Valtchev</author>
<author>D</author>
<author>J Odell</author>
</authors>
<title>The Truetalk Transcriber Book. Entropic Cambridge Research Laboratory,</title>
<date>1998</date>
<location>Cambridge, England.</location>
<contexts>
<context position="18026" citStr="Valtchev, et al. (1998)" startWordPosition="2622" endWordPosition="2625">l transcriptions were obtained by having 5 medical transcriptionists specially trained not to filter out disfluency and asides transcribe all the dictations used in this study. Semi-literal transcriptions were obtained with the system described in section 5 of this paper. Testing Data Three dictations (0.5 – 2 min) each were pulled out of the Literal transcriptions training set and set aside for each talker for testing. Recognition and evaluation software and formalism Software licensed from Entropic Laboratory was used for performing recognition, evaluating accuracy and acoustic adaptation. (Valtchev, et al. (1998)). Adapted models were trained using MLLR technique (Legetter and Woodland, (1996)) speakers in this study were not used in training the available as part of the Entropic package. SI model. Recognition accuracy and correctness reported in this study were calculated according to the following formulas: (1) Acc = hits – insertions / total words (2) Correctness = hits / total words 4.1.2 Experiment The following Acoustic Models were trained via adaptation with a general SI model for each talker using all available data (except for the testing data). Each model’s name reflects the kind of label da</context>
</contexts>
<marker>Valtchev, D, Odell, 1998</marker>
<rawString>Valtchev, V. Kershaw, D. and Odell, J. (1998). The Truetalk Transcriber Book. Entropic Cambridge Research Laboratory, Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C W Wightman</author>
<author>T A Harder</author>
</authors>
<title>SemiSupervised Adaptation of Acoustic Models for Large-Volume Dictation” In</title>
<date>1999</date>
<booktitle>Proc. Eurospeech ’98.</booktitle>
<pages>1371--1374</pages>
<contexts>
<context position="2266" citStr="Wightman and Harder (1999)" startWordPosition="312" endWordPosition="315"> of personal desktop systems, however, obtaining these transcriptions is costly and time consuming since they must be produced manually 1 patent pending (Serial No.: 09/487398) by humans educated for the task. The high cost makes literal transcription unworkable for ASR applications that require adapted acoustic models for thousands of talkers as well as accurate language models for idiosyncratic natural speech. Non-literal transcriptions, on the other hand, are easy to obtain because they are generated in the normal course of a medical transcription operation. It has been previously shown by Wightman and Harder (1999) that the non-literal transcriptions can be successfully used in acoustic adaptation. However, non-literal transcriptions are incomplete. They exclude many utterances that commonly occur in medical dictation—filled pauses, repetitions, repairs, ungrammatical phrases, pleasantries, asides to the transcriptionist, etc. Depending on the talker, such material may constitute a significant portion of the dictation. We present a method of automatically generating texts that can take the place of literal transcriptions for training acoustic and language models. ATRS is an automatic transcription recon</context>
<context position="9209" citStr="Wightman and Harder (1999)" startWordPosition="1316" endWordPosition="1319"> particular talker’s speech. This is why commercial desktop systems use enrollment. Although less widely applied, language model adaptation based on linear interpolation is an effective technique for tailoring stochastic grammars to particular domains of discourse and to particular speakers (Savova et al. (2000), Weng et al. (1997)). The training texts used in acoustic modeling come from recognizer-generated texts, literal transcriptions or non-literal transcriptions. Within the family of transformation and combined approaches to acoustic modeling (Digalakis and Neumeyer (1996), Strom (1996), Wightman and Harder (1999), Hazen and Glass (1997)) three basic adaptation methods can be identified: unsupervised, supervised, or semi-supervised. Each adaptation method depends on a different type of training text. What follows will briefly introduce the three methods. Unsupervised adaptation relies on the recognizer’s output as the text guiding the adaptation. Efficacy of unsupervised adaptation fully depends on the recognition accuracy. As Wightman and Harder (1999) pointed out, unsupervised adaptation works well in laboratory conditions when the speech signal has large bandwidth and is relatively “clean” of backgr</context>
<context position="10887" citStr="Wightman and Harder (1999)" startWordPosition="1557" endWordPosition="1560">ed verbatim and then the speech signal is aligned with pronunciations frame by frame for each individual word. A speaker independent model is augmented to include the observations resulting from the alignment. Semi-supervised adaptation rests on the idea that the speech signal can be partially aligned by using of the recognition output and the non-literal transcription. A significant problem with semisupervised adaptation is that only the speech that the recognizer already recognizes successfully ends up being used for adaptation. This reinforces what is already well represented in the model. Wightman and Harder (1999) report that semisupervised adaptation has a positive side effect of excluding those segments of speech that were misrecognized for reasons other than a poor acoustic model. They note that background noise and speech disfluency are detrimental to the unsupervised adaptation. In addition to the two problems with semisupervised adaptation pointed out by Wightman and Harder, we find one more potential problem. As a result of matching the word labels produced by the recognizer and the non-literal transcription, some words may be skipped which may introduce unnatural phone transitions at word bound</context>
<context position="19139" citStr="Wightman and Harder (1999)" startWordPosition="2793" endWordPosition="2796">ch talker using all available data (except for the testing data). Each model’s name reflects the kind of label data that was used for training. LITERAL Each audio file was aligned with the corresponding literal transcription. NON-LITERAL Each audio file was recognized using SI acoustic and language models. The recognition output was aligned with the non-literal transcription using dynamic programming. Only those portions of audio that corresponded to direct matches in the alignment were used to produce alignments for acoustic modeling. This method was originally used for medical dictations by Wightman and Harder (1999). SEMI-LITERAL Each audio file has been processed to produce a semi-literal transcription that was then aligned with recognition output generated in the process of creating semi-literal transcriptions. The portions of the audio corresponding to matching segments were used for acoustic adaptation training. The SI model had been trained on all available at the time (12 hours)2 similar medical dictations to the ones used in this study. The data for the 2 Although 50-100 hours of data for SI modeling is the industry standard, the population we are dealing with is highly homogeneous and reasonable </context>
</contexts>
<marker>Wightman, Harder, 1999</marker>
<rawString>Wightman, C. W. and Harder T. A. (1999). SemiSupervised Adaptation of Acoustic Models for Large-Volume Dictation” In Proc. Eurospeech ’98. pp 1371-1374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Weng</author>
<author>A Stolcke</author>
<author>A Sankar</author>
</authors>
<title>Hub4 Language Modeling Using Domain Interpolation and Data Clustering.</title>
<date>1997</date>
<booktitle>Proc. DARPA Speech Recognition Workshop,</booktitle>
<pages>147--151</pages>
<location>Chantilly, VA.</location>
<contexts>
<context position="8916" citStr="Weng et al. (1997)" startWordPosition="1279" endWordPosition="1282">iteral transcription. Semi-literal transcripts will more closely resemble the literal transcripts, as many of the elements missing from the non-literal transcripts will be restored. 3 Model Adaptation It is well known that ASR systems perform best when acoustic models are adapted to a particular talker’s speech. This is why commercial desktop systems use enrollment. Although less widely applied, language model adaptation based on linear interpolation is an effective technique for tailoring stochastic grammars to particular domains of discourse and to particular speakers (Savova et al. (2000), Weng et al. (1997)). The training texts used in acoustic modeling come from recognizer-generated texts, literal transcriptions or non-literal transcriptions. Within the family of transformation and combined approaches to acoustic modeling (Digalakis and Neumeyer (1996), Strom (1996), Wightman and Harder (1999), Hazen and Glass (1997)) three basic adaptation methods can be identified: unsupervised, supervised, or semi-supervised. Each adaptation method depends on a different type of training text. What follows will briefly introduce the three methods. Unsupervised adaptation relies on the recognizer’s output as </context>
</contexts>
<marker>Weng, Stolcke, Sankar, 1997</marker>
<rawString>Weng, F., Stolcke, A., Sankar, A. (1997). Hub4 Language Modeling Using Domain Interpolation and Data Clustering. Proc. DARPA Speech Recognition Workshop, pp. 147-151, Chantilly, VA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>