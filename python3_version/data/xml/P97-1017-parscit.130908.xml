<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.959821">
Machine Transliteration
</title>
<author confidence="0.999302">
Kevin Knight and Jonathan Graehl
</author>
<affiliation confidence="0.998203">
Information Sciences Institute
University of Southern California
</affiliation>
<address confidence="0.545488">
Marina del Rey, CA 90292
</address>
<email confidence="0.629612">
knight@isi .edu, graehAisi . edu
</email>
<sectionHeader confidence="0.981131" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995598125">
It is challenging to translate names and
technical terms across languages with differ-
ent alphabets and sound inventories. These
items are commonly transliterated, i.e., re-
placed with approximate phonetic equivalents.
For example, computer in English comes out
as :/ e..— — (konpyuutaa) in Japanese.
Translating such items from Japanese back to
English is even more challenging, and of prac-
tical interest, as transliterated items make up
the bulk of text phrases not found in bilin-
gual dictionaries. We describe and evaluate a
method for performing backwards translitera-
tions by machine. This method uses a gen-
erative model, incorporating several distinct
stages in the transliteration process.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998520193548387">
Translators must deal with many problems, and
one of the most frequent is translating proper
names and technical terms. For language pairs
like Spanish/English, this presents no great chal-
lenge: a phrase like Antonio Gil usually gets trans-
lated as Antonio Gil. However, the situation is
more complicated for language pairs that employ
very different alphabets and sound systems, such
as Japanese/English and Arabic/English. Phonetic
translation across these pairs is called translitera-
tion. We will look at Japanese/English translitera-
tion in this paper.
Japanese frequently imports vocabulary from
other languages, primarily (but not exclusively)
from English. It has a special phonetic alphabet
called katakana, which is used primarily (but not
exclusively) to write down foreign names and loan-
words. To write a word like golfbag in katakana,
some compromises must be made. For example,
Japanese has no distinct L and Ft sounds: the two En-
glish sounds collapse onto the same Japanese sound.
A similar compromise must be struck for English
II and F. Also, Japanese generally uses an alter-
nating consonant-vowel structure, making it impos-
sible to pronounce LFB without intervening vow-
els. Katakana writing is a syllabary rather than an
alphabet—there is one symbol for ga (f), another
for gi (*), another for gu (Y), etc. So the way
to write golfbag in katakana is =*)1., 7 7 roughly
pronounced goruhubaggu. Here are a few more ex-
amples:
</bodyText>
<figure confidence="0.980375">
Angela Johnson
7vii,•7avsly
(anjira jyonson)
New York Times
(nyuuyooku taimuzu)
ice cream
74 9 —4
(aisukuriimu)
</figure>
<bodyText confidence="0.992398791666667">
Notice how the transliteration is more phonetic than
orthographic; the letter h in Johnson does not pro-
duce any katakana. Also, a dot-separator ( • ) is
used to separate words, but not consistently. And
transliteration is clearly an information-losing oper-
ation: aisukuri imu loses the distinction between ice
cream and I scream.
Transliteration is not trivial to automate, but
we will be concerned with an even more challeng-
ing problem—going from katakana back to En-
glish, i.e., back-transliteration. Automating back-
transliteration has great practical importance in
Japanese/English machine translation. Katakana
phrases are the largest source of text phrases that
do not appear in bilingual dictionaries or training
corpora (a.k.a. &amp;quot;not-found words&amp;quot;). However, very
little computational work has been done in this area;
(Yamron et al., 1994) briefly mentions a pattern-
matching approach, while (Arbabi et al., 1994) dis-
cuss a hybrid neural-net/expert-system approach to
(forward) transliteration.
The information-losing aspect of transliteration
makes it hard to invert. Here are some problem in-
stances, taken from actual newspaper articles:1
</bodyText>
<note confidence="0.6428895">
&apos;Texts used in ARPA Machine Translation evalua-
tions, November 1994.
</note>
<page confidence="0.98269">
128
</page>
<equation confidence="0.6656725">
(aasudee)
1% • a — • *
(robaato shyoon renaado)
(masutaazutoonaraento)
</equation>
<bodyText confidence="0.994503333333333">
English translations appear later in this paper.
Here are a few observations about back-
transliteration:
</bodyText>
<listItem confidence="0.885202461538462">
• Back-transliteration is less forgiving than
transliteration. There are many ways to write
an English word like switch in katakana, all
equally valid, but we do not have this flexibility
in the reverse direction. For example, we can-
not drop the tin switch, nor can we write ariure
when we mean archer.
• Back-transliteration is harder than romaniza-
tion, which is a (frequently invertible) trans-
formation of a non-roman alphabet into ro-
man letters. There are several romanization
schemes for katakana writing—we have already
been using one in our examples. Katakana
writing follows Japanese sound patterns closely,
so katakana often doubles as a Japanese pro-
nunciation guide. However, as we shall see,
there are many spelling variations that compli-
cate the mapping between Japanese sounds and
katakana writing.
• Finally, not all katakana phrases can be
&amp;quot;sounded out&amp;quot; by back-transliteration. Some
phrases are shorthand, e.g., 7-113 (vaapuro)
should be translated as word processing. Oth-
ers are onomatopoetic and difficult to translate.
These cases must be solved by techniques other
than those described here.
The most desirable feature of an automatic back-
transliterator is accuracy. If possible, our techniques
should also be:
• portable to new language pairs like Ara-
bic/English with minimal effort, possibly
reusing resources.
• robust against errors introduced by optical
character recognition.
• relevant to speech recognition situations in
which the speaker has a heavy foreign accent.
• able to take textual (topical/syntactic) context
into account, or at least be able to return a
ranked list of possible English translations.
</listItem>
<bodyText confidence="0.998461375">
Like most problems in computational linguistics,
this one requires full world knowledge for a 100%
solution. Choosing between Katarina and Catalina
(both good guesses for 53! *) might even require
detailed knowledge of geography and figure skating.
At that level, human translators find the problem
quite difficult as well. so we only aim to match or
possibly exceed their performance.
</bodyText>
<sectionHeader confidence="0.903156" genericHeader="method">
2 A Modular Learning Approach
</sectionHeader>
<bodyText confidence="0.999870578947369">
Bilingual glossaries contain many entries mapping
katakana phrases onto English phrases, e.g.: ( air-
craft carrier 7 -t• 7 ). It is possible
to automatically analyze such pairs to gain enough
knowledge to accurately map new katakana phrases
that come along, and learning approach travels well
to other languages pairs. However, a naive approach
to finding direct correspondences between English
letters and katakana symbols suffers from a number
of problems. One can easily wind up with a sys-
tem that proposes iskrym as a back-transliteration of
aisukuriimu. Taking letter frequencies into account
improves this to a more plausible-looking isclim.
Moving to real words may give is crime: the i cor-
responds to ai, the $ corresponds to su, etc. Unfor-
tunately, the correct answer here is ice cream. Af-
ter initial experiments along these lines, we decided
to step back and build a generative model of the
transliteration process, which goes like this:
</bodyText>
<listItem confidence="0.999287">
1. An English phrase is written.
2. A translator pronounces it in English.
3. The pronunciation is modified to fit the
Japanese sound inventory.
4. The sounds are converted into katakana.
5. Katakana is written.
</listItem>
<bodyText confidence="0.998467">
This divides our problem into five sub-problems.
Fortunately, there are techniques for coordinating
solutions to such sub-problems, and for using gen-
erative models in the reverse direction. These tech-
niques rely on probabilities and Bayes&apos; Rule. Sup-
pose we build an English phrase generator that pro-
duces word sequences according to some probability
distribution P(w). And suppose we build an English
pronouncer that takes a word sequence and assigns
it a set of pronunciations, again probabilistically, ac-
cording to some P(plw). Given a pronunciation p,
we may want to search for the word sequence w that
maximizes P(wfp). Bayes. Rule lets us equivalently
maximize P(w) • P(p(w). exactly the two distribu-
tions we have modeled.
Extending this notion, we settled down to build
five probability distributions:
</bodyText>
<listItem confidence="0.9982662">
1. P(w) — generates written English word se-
quences.
2. P(ejw) — pronounces English word sequences.
3. P(jle) — converts English sounds into Japanese
sounds.
</listItem>
<page confidence="0.940227">
129
</page>
<listItem confidence="0.99821925">
4. P(k1j) — converts Japanese sounds to katakana
writing.
5. P(olk) — introduces misspellings caused by op-
tical character recognition (OCR).
</listItem>
<bodyText confidence="0.988656666666667">
Given a katakana string o observed by OCR, we
want to find the English word sequence w that max-
imizes the sum, over all e, j, and k, of
</bodyText>
<equation confidence="0.995448">
P(w) • P(elw) • P(jle) • P(k1j) • P(oik)
</equation>
<bodyText confidence="0.999937620689655">
Following (Pereira et al.. 1994; Pereira and Riley,
1996), we implement P(w) in a weighted finite-state
acceptor (WFSA) and we implement the other dis-
tributions in weighted finite-state transducers (WF-
STs). A WFSA is an state/transition diagram with
weights and symbols on the transitions, making
some output sequences more likely than others. A
WFST is a WFSA with a pair of symbols on each
transition, one input and one output. Inputs and
outputs may include the empty symbol c. Also fol-
lowing (Pereira and Riley, 1996), we have imple-
mented a general composition algorithm for con-
structing an integrated model P(xlz) from models
P(rly) and P(yjz), treating WFSAs as WFSTs with
identical inputs and outputs. We use this to combine
an observed katakana string with each of the mod-
els in turn. The result is a large WFSA containing
all possible English translations. We use Dijkstra&apos;s
shortest-path algorithm (Dijkstra, 1959) to extract
the most probable one.
The approach is modular. We can test each en-
gine independently and be confident that their re-
sults are combined correctly. We do no pruning,
so the final WFSA contains every solution, however
unlikely. The only approximation is the Viterbi one,
which searches for the best path through a WFSA
instead of the best sequence (i.e., the same sequence
does not receive bonus points for appearing more
than once).
</bodyText>
<sectionHeader confidence="0.989893" genericHeader="method">
3 Probabilistic Models
</sectionHeader>
<bodyText confidence="0.999869857142857">
This section describes how we designed and built
each of our five models. For consistency, we continue
to print written English word sequences in italics
(golf ball), English sound sequences in all capitals
(G AA I. F B AO 1..). Japanese sound sequences in
lower case (g or uhubo or u) and katakana
sequences naturally (=* A- 7 7ti — )
</bodyText>
<subsectionHeader confidence="0.998071">
3.1 Word Sequences
</subsectionHeader>
<bodyText confidence="0.999977826086957">
The first model generates scored word sequences,
the idea being that ice cream should score higher
than Ice creme, which should score higher than
aice kreem. We adopted a simple unigram scor-
ing method that multiplies the scores of the known
words and phrases in a sequence. Our 262,000-entry
frequency list draws its words and phrases from the
Wall Street Journal corpus. an online English name
list, and an online gazeteer of place names.2 A por-
tion of the WFSA looks like this:
An ideal word sequence model would look a bit
different. It would prefer exactly those strings
which are actually grist for Japanese translitera-
tors. For example, people rarely transliterate aux-
iliary verbs, but surnames are often transliterated.
We have approximated such a model by removing
high-frequency words like has, an, are, am, were,
them, and does, plus unlikely words corresponding
to Japanese sound bites, like coup and oh.
We also built a separate word sequence model con-
taining only English first and last names. If we know
(from context) that the transliterated phrase is a
personal name, this model is more precise.
</bodyText>
<subsectionHeader confidence="0.997178">
3.2 Words to English Sounds
</subsectionHeader>
<bodyText confidence="0.961310904761905">
The next WFST converts English word sequences
into English sound sequences. We use the English
phoneme inventory from the online CMU Pronuncia-
tion Dictionary,3 minus the stress marks. This gives
a total of 40 sounds, including 14 vowel sounds (e.g.,
AA, AE, UW), 25 consonant sounds (e.g., K, MB, P.), plus
our special symbol (PAUSE). The dictionary has pro-
nunciations for 110,000 words, and we organized a
phoneme-tree based WFST from it:
E:E
Note that we insert an optional PAUSE between word
pronunciations. Due to memory limitations, we only
used the 50,000 most frequent words.
We originally thought to build a general letter-
to-sound WFST, on the theory that while wrong
(overgeneralized) pronunciations might occasionally
be generated, Japanese transliterators also mispro-
nounce words. However, our letter-to-sound WFST
did not match the performance of Japanese translit-
2 Availa.ble from the ACL Data Collection Initiative.
http://www.speech.cs.cmu.edu/cgi-bin/cmudict.
</bodyText>
<equation confidence="0.5011925">
federal! 0.0013
los / 0.000087
angeles
month 0.000992
</equation>
<page confidence="0.950018">
130
</page>
<bodyText confidence="0.977363">
erators, and it turns out that mispronunciations are L OW L OW
modeled adequately in the next stage of the cascade. I A /\
r 0 0 r o o
</bodyText>
<subsectionHeader confidence="0.983051">
3.3 English Sounds to Japanese Sounds
</subsectionHeader>
<bodyText confidence="0.999893333333333">
Next, we map English sound sequences onto
Japanese sound sequences. This is an inherently
information-losing process, as English R and L
sounds collapse onto Japanese r, the 14 English
vowel sounds collapse onto the 5 Japanese vowel
sounds, etc. We face two immediate problems:
</bodyText>
<listItem confidence="0.995981333333333">
1. What is the target Japanese sound inventory?
2. How can we build a WFST to perform the se-
quence mapping?
</listItem>
<bodyText confidence="0.98195675">
An obvious target inventory is the Japanese syl-
labary itself, written down in katakana (e.g., =-) or
a roman equivalent (e.g., ni). With this approach,
the English sound K corresponds to one of t (ka),
(ki), (ku), r (ke), or (ko), depending on
its context. Unfortunately, because katakana is a
syllabary, we would be unable to express an obvi-
ous and useful generalization, namely that English
K usually corresponds to Japanese k, independent of
context. Moreover, the correspondence of Japanese
katakana writing to Japanese sound sequences is not
perfectly one-to-one (see next section), so an inde-
pendent sound inventory is well-motivated in any
case. Our Japanese sound inventory includes 39
symbols: 5 vowel sounds, 33 consonant sounds (in-
cluding doubled consonants like kk), and one spe-
cial symbol (pause). An English sound sequence
like (P R OW PAUSE S AA K ER) might map onto a
Japanese sound sequence like (p u r o pause s a
kk a a). Note that long Japanese vowel sounds are
written with two symbols (a a) instead of just one
(aa). This scheme is attractive because Japanese
sequences are almost always longer than English se-
quences.
Our WFST is learned automatically from 8,000
pairs of English/Japanese sound sequences, e.g., ( (S
AA K ER) — (s a kk a a)). We were able to pro-
duce&apos; these pairs by manipulating a small English-
katakana glossary. For each glossary entry, we
converted English words into English sounds us-
ing the previous section&apos;s model, and we converted
katakana words into Japanese sounds using the next
section&apos;s model. We then applied the estimation-
maximization (EM) algorithm (Baum, 1972) to gen-
erate symbol-mapping probabilities, shown in Fig-
ure 1. Our EM training goes like this:
1. For each English/Japanese sequence pair, com-
pute all possible alignments between their ele-
ments. In our case, an alignment is a drawing
that connects each English sound with one or
more Japanese sounds, such that all Japanese
sounds are covered and no lines cross. For ex-
ample, there are two ways to align the pair ( (L
OW) &lt;-&gt; (r o o)):
</bodyText>
<listItem confidence="0.997469157894737">
2. For each pair, assign an equal weight to each
of its alignments, such that those weights sum
to 1. In the case above, each alignment gets a
weight of 0.5.
3. For each of the 40 English sounds, count up in-
stances of its different mappings, as observed in
all alignments of all pairs. Each alignment con-
tributes counts in proportion to its own weight.
4. For each of the 40 English sounds, normalize the
scores of the Japanese sequences it maps to, so
that the scores sum to 1. These are the symbol-
mapping probabilities shown in Figure 1.
5. Recompute the alignment scores. Each align-
ment is scored with the product of the scores of
the symbol mappings it contains.
6. Normalize the alignment scores. Scores for each
pair&apos;s alignments should sum to 1.
7. Repeat 3-6 until the symbol-mapping probabil-
ities converge.
</listItem>
<bodyText confidence="0.91511">
We then build a WFST directly from the symbol-
mapping probabilities:
</bodyText>
<figure confidence="0.941201857142857">
PAUSE:pause
AA:o / 0.018
•
AA:a / 0.024
E :a
AA:a / 0.382
E:o
</figure>
<bodyText confidence="0.994711652173913">
Our WFST has 99 states and 283 arcs.
We have also built models that allow individual
English sounds to be &amp;quot;swallowed&amp;quot; (i.e., produce zero
Japanese sounds). However, these models are ex-
pensive to compute (many more alignments) and
lead to a vast number of hypotheses during WFST
composition. Furthermore, in disallowing &amp;quot;swallow-
ing,&amp;quot; we were able to automatically remove hun-
dreds of potentially harmful pairs from our train-
ing set, e.g., ((B AA R B ER SH AA P) (b a a
b a a)). Because no alignments are possible, such
pairs are skipped by the learning algorithm; cases
like these must be solved by dictionary lookup any-
way. Only two pairs failed to align when we wished
they had—both involved turning English Y UW into
Japanese u, as in ((Y UW K AH L EY L IY) Cu
kurere)).
Note also that our model translates each English
sound without regard to context. We have built also
context-based models, using decision trees recoded
as WFSTs. For example. at the end of a word, En-
glish T is likely to come out as (t o) rather than (t).
However, context-based models proved unnecessary
</bodyText>
<page confidence="0.964414">
131
</page>
<figure confidence="0.975072419354839">
e j PO I e)
AA o 0.566
a 0.382
a a 0.024
o o 0.018
AE a 0.942
y a 0.046
AN a 0.486
o 0.169
e 0.134
i 0.111
u 0.076
AO o 0.671
o o 0.257
a 0.047
AW a u 0.830
a v 0.095
o o 0.027
a o 0.020
a 0.014
AT a i 0.864
i 0.073
a 0.018
a i y 0.018
B b 0.802
b u 0.185
CH ch y 0.277
ch 0.240
tch i 0.199
ch i 0.159
tch 0.038
ch y u 0.021
tch y 0.020
D d 0.535
d o 0.329
dd o 0.053
j 0.032
DH z 0.670
z u 0.125
j 0.125
a z 0.080
EH e 0.901
a 0.069
ER a a 0.719
a 0.081
a r 0.063
e r 0.042
o r 0.029
e 3 P(i 1 e)
EY e e 0.641
a 0.122
e 0.114
e i 0.080
a i 0.014
F h 0.623
h u 0.331
hh 0.019
a h u 0.010
G g 0.598
g u 0.304
gg u 0.059
gg 0.010
</figure>
<table confidence="0.946913662650602">
HH h 0.959
w 0.014
IN i 0.908
e 0.071
IT i i 0.573
i 0.317
e 0.074
e e 0.016
JH j 0.329
j y 0.328
j i 0.129
ii i 0.066
e j i 0.057
z 0.032
N 0.018
jj 0.012
e 0.012
K k 0.528
k u 0.238
kk u 0.150
kk 0.043
k i 0.015
k y 0.012
L r 0.621
r u 0.362
M m 0.653
m u 0.207
n 0.123
n in 0.011
N n 0.978
NO n g u 0.743
n 0.220
n g 0.023
e j F(i I e)
OW o 0.516
o o 0.456
o u 0.011
OY o i 0.828
o o i 0.057
i 0.029
o i y 0.029
o 0.027
o o y 0.014
o o 0.014
P P 0.649
P u 0.218
pp u 0.085
pp 0.045
PAUSE pause 1.000
11 r 0.661
a 0.170
o 0.076
r u 0.042
u r 0.016
a r 0.012
S s u 0.539
s 0.269
sh 0.109
u 0.028
as 0.014
SH sh y 0.475
sh 0.175
ssh y u 0.166
ash y 0.088
sh i 0.029
ash 0.027
sh y u 0.015
T t 0.463
t o 0.305
tt o 0.103
ch 0.043
tt 0.021
ts 0.020
ts u 0.011
TH s u 0.418
s 0.303
sh 0.130
ch 0.038
t 0.029
e j P(j 1 e)
UH u 0.794
u u 0.098
dd 0.034
</table>
<figure confidence="0.934588743589743">
a 0.030
o 0.026
UW u u 0.550
u 0.302
y u u 0.109
y u 0.021
V b 0.810
b u 0.150
V 0.015
W w 0.693
u 0.194
o 0.039
i 0.027
a 0.015
e 0.012
Y y 0.652
i 0.220
y u 0.050
u 0.048
b 0.016
Z z 0.296
z u 0.283
j 0.107
S u 0.103
u 0.073
a 0.036
o 0.018
S 0.015
n 0.013
i 0.011
sh 0.011
ZH j y 0.324
sh i 0.270
j i 0.173
j 0.135
aj yu 0.027
sh y 0.027
s 0.027
a j i 0.016
</figure>
<figureCaption confidence="0.990995">
Figure 1: English sounds (in capitals) with probabilistic mappings to Japanese sound sequences (in lower
</figureCaption>
<bodyText confidence="0.498379">
case), as learned by estimation-maximization. Only mappings with conditional probabilities greater than
1% are shown, so the figures may not sum to 1.
</bodyText>
<page confidence="0.994609">
132
</page>
<bodyText confidence="0.9652725">
for back-transliteration.&apos; They are more useful for
English-to-Japanese forward transliteration.
</bodyText>
<subsectionHeader confidence="0.989727">
3.4 Japanese sounds to Katakana
</subsectionHeader>
<bodyText confidence="0.993956368421053">
To map Japanese sound sequences like (in o o t
a a) onto katakana sequences like —), we
manually constructed two WFSTs. Composed to-
gether, they yield an integrated WFST with 53
states and 303 arcs. The first WFST simply merges
long Japanese vowel sounds into new symbols aa,
uu, ee, and oo. The second WFST maps Japanese
sounds onto katakana symbols. The basic idea is
to consume a whole syllable worth of sounds before
producing any katakana, e.g.:
o: 3
This fragment shows one kind of spelling varia-
tion in Japanese: long vowel sounds (oo) are usu-
ally written with a long vowel mark (21--) but are
sometimes written with repeated katakana (71-7I&amp;quot;).
We combined corpus analysis with guidelines from
a Japanese textbook (Jorden and Chaplin, 1976)
to turn up many spelling variations and unusual
katakana symbols:
</bodyText>
<listItem confidence="0.997502272727273">
• the sound sequence (j i) is usually written
but occasionally I&apos;.
• (g u a) is usually 9&apos;7, but occasionally i.
• (w o o) is variously ,7 —, or with a
special, old-style katakana for wo.
• (y e) may be .T., .Lx., or -f s.
• (w i) is either 17 or 1, 4.
• (n y e) is a rare sound sequence, but is written
when it occurs.
• (t y u) is rarer than (ch y ), but is written
t&amp;quot;.. when it occurs.
</listItem>
<bodyText confidence="0.9731876875">
and so on.
Spelling variation is clearest in cases where an En-
glish word like switch shows up transliterated vari-
ously (7 4 7 &amp;quot;I&amp;quot; ;4 7 &amp;quot;1&amp;quot; 4 7 1- ) in different
dictionaries. Treating these variations as an equiv-
alence class enables us to learn general sound map-
pings even if our bilingual glossary adheres to a sin-
gle narrow spelling convention. We do not, however,
&apos;And harmfully restrictive in their unsinoothed
incarnations.
generate all katakana sequences with this model;
for example, we do not output strings that begin
with a subscripted vowel katakana. So this model
also serves to filter out some ill-formed katakana
sequences, possibly proposed by optical character
recognition.
</bodyText>
<subsectionHeader confidence="0.934813">
3.5 Katalcana to OCR
</subsectionHeader>
<bodyText confidence="0.999904363636363">
Perhaps uncharitably, we can view optical character
recognition (OCR) as a device that garbles perfectly
good katakana sequences. Typical confusions made
by our commercial OCR system include for
1- for 1&amp;quot;, 7 for 7 , and 7 for 1. To generate pre-
OCR text, we collected 19,500 characters worth of
katakana words, stored them in a file, and printed
them out. To generate post-OCR text, we OCR&apos;d
the printouts. We then ran the EM algorithm to de-
termine symbol-mapping (&amp;quot;garbling&amp;quot;) probabilities.
Here is part of that table:
</bodyText>
<figure confidence="0.667931625">
k o P (o I k)
&amp; 0.492
e 0.434
e 0.042
7 0.011
e&amp;quot; e 1.000
iN ,s 0.964
1. 0.036
</figure>
<bodyText confidence="0.999218">
This model outputs a superset of the 81 katakana
symbols, including spurious quote marks, alphabetic
symbols, and the numeral 7.
</bodyText>
<sectionHeader confidence="0.999659" genericHeader="method">
4 Example
</sectionHeader>
<bodyText confidence="0.975091272727273">
We can now use the models to do a sample back-
transliteration. We start with a katakana phrase
as observed by OCR. We then serially compose it
with the models, in reverse order. Each intermedi-
ate stage is a WFSA that encodes many possibilities.
The final stage contains all back-transliterations sug-
gested by the models, and we finally extract the best
one.
We start with the masutaazutoonamento problem
from Section 1. Our OCR observes:
-q :/ j.
This string has two recognition errors: (ku)
for (ta), and .1- (chi) for t (na). We turn the
string into a chained 12-state/11-arc WFSA and
compose it with the P(klo) model. This yields a fat-
ter 12-state/15-arc WFSA, which accepts the cor-
rect spelling at a lower probability. Next comes
the P(jlk) model, which produces a 28-state/31-arc
WFSA whose highest-scoring sequence is:
masutaazutoochiment o
Next comes P(e1j), yielding a 62-state/241-arc
WFSA whose best sequence is:
</bodyText>
<note confidence="0.472662">
M AE S T AE AE DH UH T AO AO CH TH M EH N T AO
</note>
<page confidence="0.997852">
133
</page>
<bodyText confidence="0.968129444444444">
Next to last comes P(wle), which results in a 2982-
state/4601-arc WFSA whose best sequence (out of
myriads) is:
masters tone am ent awe
This English string is closest phonetically to the
Japanese, but we are willing to trade phonetic prox-
imity for more sensical English; we rescore this
WFSA by composing it with P(w) and extract the
best translation:
</bodyText>
<subsubsectionHeader confidence="0.384687">
masters tournament
</subsubsectionHeader>
<bodyText confidence="0.9963735">
(Other Section 1 examples are translated correctly
as earth day and robert sean leonard.)
</bodyText>
<sectionHeader confidence="0.999679" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.976450578947369">
We have performed two large-scale experiments, one
using a full-language P(w) model, and one using a
personal name language model.
In the first experiment, we extracted 1449 unique
katakana phrases from a corpus of 100 short news
articles. Of these, 222 were missing from an on-
line 100,000-entry bilingual dictionary. We back-
transliterated these 222 phrases. Many of the trans-
lations are perfect: technical program, sex scandal,
omaha beach, new york times, ramon diaz. Oth-
ers are close: tanya harding, nickel simpson, danger
washington, world cap. Some miss the mark: nancy
care again, plus occur, patriot miss real. While it
is difficult to judge overall accuracy—some of the
phases are onomatopoetic, and others are simply too
hard even for good human translators—it is easier
to identify system weaknesses, and most of these lie
in the P(w) model. For example, nancy kerrigan
should be preferred over nancy care again.
In a second experiment, we took katakana
versions of the names of 100 U.S. politicians,
e.g.: :/ • 7°12— (jyon.buroo), • y
-q 7l (arhonsu.damatto), and -q • Y. 7
(maiku.dewain). We back-transliterated these by
machine and asked four human subjects to do the
same. These subjects were native English speakers
and news-aware: we gave them brief instructions, ex-
amples, and hints. The results were as follows:
human machine
27% 64%
7% 12%
66% 24%
There is room for improvement on both sides. Be-
ing English speakers, the human subjects were good
at English name spelling and U.S. politics, but not
at Japanese phonetics. A native Japanese speaker
might be expert at the latter but not the former.
People who are expert in all of these areas, however,
are rare.
On the automatic side, many errors can be cor-
rected. A first-name/last-name model would rank
richard bryan more highly than richard brian. A bi-
gram model would prefer orren hatch over olin hatch.
Other errors are due to unigram training problems,
or more rarely, incorrect or brittle phonetic models.
For example, &amp;quot;Long&amp;quot; occurs much more often than
&amp;quot;Ron&amp;quot; in newspaper text, and our word selection
does not exclude phrases like &amp;quot;Long Island.&amp;quot; So we
get long wyden instead of ron wyden. Rare errors
are due to incorrect or brittle phonetic models.
Still the machine&apos;s performance is impressive.
When word separators ( • ) are removed from the
katakana phrases, rendering the task exceedingly dif-
ficult for people, the machine&apos;s performance is un-
changed. When we use OCR. 7% of katakana tokens
are mis-recognized, affecting 50% of test strings, but
accuracy only drops from 64% to 52%.
</bodyText>
<sectionHeader confidence="0.999709" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.99999604">
We have presented a method for automatic back-
transliteration which, while far from perfect, is
highly competitive. It also achieves the objectives
outlined in Section 1. It ports easily to new lan-
guage pairs; the P(w) and P(eitv) models are entirely
reusable, while other models are learned automati-
cally. It is robust against OCR noise, in a rare ex-
ample of high-level language processing being useful
(necessary, even) in improving low-level OCR.
We plan to replace our shortest-path extraction
algorithm with one of the recently developed k-
shortest path algorithms (Eppstein, 1994). We will
then return a ranked list of the k best translations
for subsequent contextual disambiguation, either by
machine or as part of an interactive man-machine
system. We also plan to explore probabilistic models
for Arabic/English transliteration. Simply identify-
ing which Arabic words to transliterate is a difficult
task in itself; and while Japanese tends to insert ex-
tra vowel sounds, Arabic is usually written without
any (short) vowels. Finally, it should also be pos-
sible to embed our phonetic shift model P( jle) in-
side a speech recognizer, to help adjust for a heavy
Japanese accent, although we have not experimented
in this area.
</bodyText>
<sectionHeader confidence="0.999361" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.976397538461539">
We would like to thank Alton Earl Ingram, Yolanda
Gil, Bonnie Glover-Stalls, Richard &apos;Whitney, and
Kenji Yamada for their helpful comments. We would
correct
(e.g., spencer abraham I
spencer abraham)
phonetically equivalent,
but misspelled
(e.g., richard brian /
richard bryan)
incorrect
(e.g., olin hatch I
orren hatch)
</bodyText>
<page confidence="0.959785">
134
</page>
<bodyText confidence="0.991967">
also like to thank our sponsors at the Department of
Defense.
</bodyText>
<sectionHeader confidence="0.99554" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999832481481482">
M. Arbabi, S. M. Fischthal, and V. C. Cheng andd
E. Bart. 1994. Algorithms for Arabic name
transliteration. IBM J. Res. Develop., 38(2).
L. E. Baum. 1972. An inequality and associated
maximization technique in statistical estimation
of probabilistic functions of a Markov process. In-
equalities, 3.
E. W. Dijkstra. 1959. A note on two problems in
connexion with graphs. Numerische Mathematik,
1.
David Eppstein. 1994. Finding the k shortest paths.
In Proc. 35th Symp. Foundations of Computer
Science. IEEE.
E. H. Jorden and H. I. Chaplin. 1976. Reading
Japanese. Yale University Press, New Haven.
F. Pereira and M. Riley. 1996. Speech recognition
by composition of weighted finite automata. In
preprint, cmp-lg/9603001.
F. Pereira, M. Riley, and R. Sproat. 1994. Weighted
rational transductions and their application to hu-
man language processing. In Proc. ARPA Human
Language Technology Workshop.
J. Yamron, J. Cant, A. Demedts, T. Dietzel, and
Y. Ito. 1994. The automatic component of
the LINGSTAT machine-aided translation sys-
tem. In Proc. ARPA Workshop on Human Lan-
guage Technology.
</reference>
<page confidence="0.998788">
135
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.875489">
<title confidence="0.999989">Machine Transliteration</title>
<author confidence="0.999949">Kevin Knight</author>
<author confidence="0.999949">Jonathan Graehl</author>
<affiliation confidence="0.999594">Information Sciences Institute University of Southern California</affiliation>
<address confidence="0.999772">Marina del Rey, CA 90292</address>
<email confidence="0.887197">knight@isi.edu,graehAisi.edu</email>
<abstract confidence="0.999254705882353">It is challenging to translate names and technical terms across languages with different alphabets and sound inventories. These items are commonly transliterated, i.e., replaced with approximate phonetic equivalents. example, English comes out :/ — in Japanese. Translating such items from Japanese back to English is even more challenging, and of practical interest, as transliterated items make up the bulk of text phrases not found in bilingual dictionaries. We describe and evaluate a method for performing backwards transliterations by machine. This method uses a generative model, incorporating several distinct stages in the transliteration process.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Arbabi</author>
<author>S M Fischthal</author>
<author>V C Cheng andd E Bart</author>
</authors>
<title>Algorithms for Arabic name transliteration.</title>
<date>1994</date>
<journal>IBM J. Res. Develop.,</journal>
<volume>38</volume>
<issue>2</issue>
<contexts>
<context position="3364" citStr="Arbabi et al., 1994" startWordPosition="512" endWordPosition="515">n between ice cream and I scream. Transliteration is not trivial to automate, but we will be concerned with an even more challenging problem—going from katakana back to English, i.e., back-transliteration. Automating backtransliteration has great practical importance in Japanese/English machine translation. Katakana phrases are the largest source of text phrases that do not appear in bilingual dictionaries or training corpora (a.k.a. &amp;quot;not-found words&amp;quot;). However, very little computational work has been done in this area; (Yamron et al., 1994) briefly mentions a patternmatching approach, while (Arbabi et al., 1994) discuss a hybrid neural-net/expert-system approach to (forward) transliteration. The information-losing aspect of transliteration makes it hard to invert. Here are some problem instances, taken from actual newspaper articles:1 &apos;Texts used in ARPA Machine Translation evaluations, November 1994. 128 (aasudee) 1% • a — • * (robaato shyoon renaado) (masutaazutoonaraento) English translations appear later in this paper. Here are a few observations about backtransliteration: • Back-transliteration is less forgiving than transliteration. There are many ways to write an English word like switch in ka</context>
</contexts>
<marker>Arbabi, Fischthal, Bart, 1994</marker>
<rawString>M. Arbabi, S. M. Fischthal, and V. C. Cheng andd E. Bart. 1994. Algorithms for Arabic name transliteration. IBM J. Res. Develop., 38(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L E Baum</author>
</authors>
<title>An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process.</title>
<date>1972</date>
<journal>Inequalities,</journal>
<volume>3</volume>
<contexts>
<context position="14432" citStr="Baum, 1972" startWordPosition="2306" endWordPosition="2307">s (a a) instead of just one (aa). This scheme is attractive because Japanese sequences are almost always longer than English sequences. Our WFST is learned automatically from 8,000 pairs of English/Japanese sound sequences, e.g., ( (S AA K ER) — (s a kk a a)). We were able to produce&apos; these pairs by manipulating a small Englishkatakana glossary. For each glossary entry, we converted English words into English sounds using the previous section&apos;s model, and we converted katakana words into Japanese sounds using the next section&apos;s model. We then applied the estimationmaximization (EM) algorithm (Baum, 1972) to generate symbol-mapping probabilities, shown in Figure 1. Our EM training goes like this: 1. For each English/Japanese sequence pair, compute all possible alignments between their elements. In our case, an alignment is a drawing that connects each English sound with one or more Japanese sounds, such that all Japanese sounds are covered and no lines cross. For example, there are two ways to align the pair ( (L OW) &lt;-&gt; (r o o)): 2. For each pair, assign an equal weight to each of its alignments, such that those weights sum to 1. In the case above, each alignment gets a weight of 0.5. 3. For </context>
</contexts>
<marker>Baum, 1972</marker>
<rawString>L. E. Baum. 1972. An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process. Inequalities, 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E W Dijkstra</author>
</authors>
<title>A note on two problems in connexion with graphs.</title>
<date>1959</date>
<journal>Numerische Mathematik,</journal>
<volume>1</volume>
<contexts>
<context position="9262" citStr="Dijkstra, 1959" startWordPosition="1448" endWordPosition="1449">sequences more likely than others. A WFST is a WFSA with a pair of symbols on each transition, one input and one output. Inputs and outputs may include the empty symbol c. Also following (Pereira and Riley, 1996), we have implemented a general composition algorithm for constructing an integrated model P(xlz) from models P(rly) and P(yjz), treating WFSAs as WFSTs with identical inputs and outputs. We use this to combine an observed katakana string with each of the models in turn. The result is a large WFSA containing all possible English translations. We use Dijkstra&apos;s shortest-path algorithm (Dijkstra, 1959) to extract the most probable one. The approach is modular. We can test each engine independently and be confident that their results are combined correctly. We do no pruning, so the final WFSA contains every solution, however unlikely. The only approximation is the Viterbi one, which searches for the best path through a WFSA instead of the best sequence (i.e., the same sequence does not receive bonus points for appearing more than once). 3 Probabilistic Models This section describes how we designed and built each of our five models. For consistency, we continue to print written English word s</context>
</contexts>
<marker>Dijkstra, 1959</marker>
<rawString>E. W. Dijkstra. 1959. A note on two problems in connexion with graphs. Numerische Mathematik, 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Eppstein</author>
</authors>
<title>Finding the k shortest paths.</title>
<date>1994</date>
<booktitle>In Proc. 35th Symp. Foundations of Computer Science.</booktitle>
<publisher>IEEE.</publisher>
<contexts>
<context position="26377" citStr="Eppstein, 1994" startWordPosition="4560" endWordPosition="4561"> from 64% to 52%. 6 Discussion We have presented a method for automatic backtransliteration which, while far from perfect, is highly competitive. It also achieves the objectives outlined in Section 1. It ports easily to new language pairs; the P(w) and P(eitv) models are entirely reusable, while other models are learned automatically. It is robust against OCR noise, in a rare example of high-level language processing being useful (necessary, even) in improving low-level OCR. We plan to replace our shortest-path extraction algorithm with one of the recently developed kshortest path algorithms (Eppstein, 1994). We will then return a ranked list of the k best translations for subsequent contextual disambiguation, either by machine or as part of an interactive man-machine system. We also plan to explore probabilistic models for Arabic/English transliteration. Simply identifying which Arabic words to transliterate is a difficult task in itself; and while Japanese tends to insert extra vowel sounds, Arabic is usually written without any (short) vowels. Finally, it should also be possible to embed our phonetic shift model P( jle) inside a speech recognizer, to help adjust for a heavy Japanese accent, al</context>
</contexts>
<marker>Eppstein, 1994</marker>
<rawString>David Eppstein. 1994. Finding the k shortest paths. In Proc. 35th Symp. Foundations of Computer Science. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E H Jorden</author>
<author>H I Chaplin</author>
</authors>
<title>Reading Japanese.</title>
<date>1976</date>
<publisher>Yale University Press,</publisher>
<location>New Haven.</location>
<contexts>
<context position="19865" citStr="Jorden and Chaplin, 1976" startWordPosition="3443" endWordPosition="3446">omposed together, they yield an integrated WFST with 53 states and 303 arcs. The first WFST simply merges long Japanese vowel sounds into new symbols aa, uu, ee, and oo. The second WFST maps Japanese sounds onto katakana symbols. The basic idea is to consume a whole syllable worth of sounds before producing any katakana, e.g.: o: 3 This fragment shows one kind of spelling variation in Japanese: long vowel sounds (oo) are usually written with a long vowel mark (21--) but are sometimes written with repeated katakana (71-7I&amp;quot;). We combined corpus analysis with guidelines from a Japanese textbook (Jorden and Chaplin, 1976) to turn up many spelling variations and unusual katakana symbols: • the sound sequence (j i) is usually written but occasionally I&apos;. • (g u a) is usually 9&apos;7, but occasionally i. • (w o o) is variously ,7 —, or with a special, old-style katakana for wo. • (y e) may be .T., .Lx., or -f s. • (w i) is either 17 or 1, 4. • (n y e) is a rare sound sequence, but is written when it occurs. • (t y u) is rarer than (ch y ), but is written t&amp;quot;.. when it occurs. and so on. Spelling variation is clearest in cases where an English word like switch shows up transliterated variously (7 4 7 &amp;quot;I&amp;quot; ;4 7 &amp;quot;1&amp;quot; 4 7 1</context>
</contexts>
<marker>Jorden, Chaplin, 1976</marker>
<rawString>E. H. Jorden and H. I. Chaplin. 1976. Reading Japanese. Yale University Press, New Haven.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>M Riley</author>
</authors>
<title>Speech recognition by composition of weighted finite automata. In</title>
<date>1996</date>
<tech>preprint, cmp-lg/9603001.</tech>
<contexts>
<context position="8395" citStr="Pereira and Riley, 1996" startWordPosition="1305" endWordPosition="1308">odeled. Extending this notion, we settled down to build five probability distributions: 1. P(w) — generates written English word sequences. 2. P(ejw) — pronounces English word sequences. 3. P(jle) — converts English sounds into Japanese sounds. 129 4. P(k1j) — converts Japanese sounds to katakana writing. 5. P(olk) — introduces misspellings caused by optical character recognition (OCR). Given a katakana string o observed by OCR, we want to find the English word sequence w that maximizes the sum, over all e, j, and k, of P(w) • P(elw) • P(jle) • P(k1j) • P(oik) Following (Pereira et al.. 1994; Pereira and Riley, 1996), we implement P(w) in a weighted finite-state acceptor (WFSA) and we implement the other distributions in weighted finite-state transducers (WFSTs). A WFSA is an state/transition diagram with weights and symbols on the transitions, making some output sequences more likely than others. A WFST is a WFSA with a pair of symbols on each transition, one input and one output. Inputs and outputs may include the empty symbol c. Also following (Pereira and Riley, 1996), we have implemented a general composition algorithm for constructing an integrated model P(xlz) from models P(rly) and P(yjz), treatin</context>
</contexts>
<marker>Pereira, Riley, 1996</marker>
<rawString>F. Pereira and M. Riley. 1996. Speech recognition by composition of weighted finite automata. In preprint, cmp-lg/9603001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>M Riley</author>
<author>R Sproat</author>
</authors>
<title>Weighted rational transductions and their application to human language processing.</title>
<date>1994</date>
<booktitle>In Proc. ARPA Human Language Technology Workshop.</booktitle>
<marker>Pereira, Riley, Sproat, 1994</marker>
<rawString>F. Pereira, M. Riley, and R. Sproat. 1994. Weighted rational transductions and their application to human language processing. In Proc. ARPA Human Language Technology Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Yamron</author>
<author>J Cant</author>
<author>A Demedts</author>
<author>T Dietzel</author>
<author>Y Ito</author>
</authors>
<title>The automatic component of the LINGSTAT machine-aided translation system.</title>
<date>1994</date>
<booktitle>In Proc. ARPA Workshop on Human Language Technology.</booktitle>
<contexts>
<context position="3291" citStr="Yamron et al., 1994" startWordPosition="501" endWordPosition="504">learly an information-losing operation: aisukuri imu loses the distinction between ice cream and I scream. Transliteration is not trivial to automate, but we will be concerned with an even more challenging problem—going from katakana back to English, i.e., back-transliteration. Automating backtransliteration has great practical importance in Japanese/English machine translation. Katakana phrases are the largest source of text phrases that do not appear in bilingual dictionaries or training corpora (a.k.a. &amp;quot;not-found words&amp;quot;). However, very little computational work has been done in this area; (Yamron et al., 1994) briefly mentions a patternmatching approach, while (Arbabi et al., 1994) discuss a hybrid neural-net/expert-system approach to (forward) transliteration. The information-losing aspect of transliteration makes it hard to invert. Here are some problem instances, taken from actual newspaper articles:1 &apos;Texts used in ARPA Machine Translation evaluations, November 1994. 128 (aasudee) 1% • a — • * (robaato shyoon renaado) (masutaazutoonaraento) English translations appear later in this paper. Here are a few observations about backtransliteration: • Back-transliteration is less forgiving than transl</context>
</contexts>
<marker>Yamron, Cant, Demedts, Dietzel, Ito, 1994</marker>
<rawString>J. Yamron, J. Cant, A. Demedts, T. Dietzel, and Y. Ito. 1994. The automatic component of the LINGSTAT machine-aided translation system. In Proc. ARPA Workshop on Human Language Technology.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>