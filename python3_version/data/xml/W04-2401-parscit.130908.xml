<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005229">
<title confidence="0.9989935">
A Linear Programming Formulation for Global Inference in Natural
Language Tasks
</title>
<author confidence="0.997808">
Dan Roth Wen-tau Yih
</author>
<affiliation confidence="0.9993125">
Department of Computer Science
University of Illinois at Urbana-Champaign
</affiliation>
<email confidence="0.99878">
{danr, yih}@uiuc.edu
</email>
<sectionHeader confidence="0.995641" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999957882352941">
Given a collection of discrete random variables
representing outcomes of learned local predic-
tors in natural language, e.g., named entities
and relations, we seek an optimal global as-
signment to the variables in the presence of
general (non-sequential) constraints. Examples
of these constraints include the type of argu-
ments a relation can take, and the mutual activ-
ity of different relations, etc. We develop a lin-
ear programming formulation for this problem
and evaluate it in the context of simultaneously
learning named entities and relations. Our ap-
proach allows us to efficiently incorporate do-
main and task specific constraints at decision
time, resulting in significant improvements in
the accuracy and the “human-like” quality of
the inferences.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999781358490566">
Natural language decisions often depend on the out-
comes of several different but mutually dependent predic-
tions. These predictions must respect some constraints
that could arise from the nature of the data or from do-
main or task specific conditions. For example, in part-of-
speech tagging, a sentence must have at least one verb,
and cannot have three consecutive verbs. These facts can
be used as constraints. In named entity recognition, “no
entities can overlap” is a common constraint used in var-
ious works (Tjong Kim Sang and De Meulder, 2003).
Efficient solutions to problems of these sort have been
given when the constraints on the predictors are sequen-
tial (Dietterich, 2002). These solutions can be cate-
gorized into the following two frameworks. Learning
global models trains a probabilistic model under the con-
straints imposed by the domain. Examples include varia-
tions of HMMs, conditional models and sequential varia-
tions of Markov random fields (Lafferty et al., 2001). The
other framework, inference with classifiers (Roth, 2002),
views maintaining constraints and learning classifiers as
separate processes. Various local classifiers are trained
without the knowledge of constraints. The predictions
are taken as input on the inference procedure which then
finds the best global prediction. In addition to the concep-
tual simplicity of this approach, it also seems to perform
better experimentally (Tjong Kim Sang and De Meulder,
2003).
Typically, efficient inference procedures in both frame-
works rely on dynamic programming (e.g., Viterbi),
which works well in sequential data. However, in many
important problems, the structure is more general, result-
ing in computationally intractable inference. Problems of
these sorts have been studied in computer vision, where
inference is generally performed over low level measure-
ments rather than over higher level predictors (Levin et
al., 2002; Boykov et al., 2001).
This work develops a novel inference with classifiers
approach. Rather than being restricted on sequential data,
we study a fairly general setting. The problem is defined
in terms of a collection of discrete random variables rep-
resenting binary relations and their arguments; we seek
an optimal assignment to the variables in the presence of
the constraints on the binary relations between variables
and the relation types.
The key insight to this solution comes from re-
cent techniques developed for approximation algo-
rithms (Chekuri et al., 2001). Following this work, we
model inference as an optimization problem, and show
how to cast it as a linear program. Using existing numer-
ical packages, which are able to solve very large linear
programming problems in a very short time1, inference
can be done very quickly.
Our approach could be contrasted with other ap-
</bodyText>
<footnote confidence="0.8296325">
1For example, (CPLEX, 2003) is able to solve a linear pro-
gramming problem of 13 million variables within 5 minutes.
</footnote>
<bodyText confidence="0.998901875">
proaches to sequential inference or to general Markov
random field approaches (Lafferty et al., 2001; Taskar et
al., 2002). The key difference is that in these approaches,
the model is learned globally, under the constraints im-
posed by the domain. In our approach, predictors do not
need to be learned in the context of the decision tasks,
but rather can be learned in other contexts, or incorpo-
rated as background knowledge. This way, our approach
allows the incorporation of constraints into decisions in a
dynamic fashion and can therefore support task specific
inferences. The significance of this is clearly shown in
our experimental results.
We develop our models in the context of natural lan-
guage inferences and evaluate it here on the problem of
simultaneously recognizing named entities and relations
between them.
</bodyText>
<subsectionHeader confidence="0.994271">
1.1 Entity and Relation Recognition
</subsectionHeader>
<bodyText confidence="0.999915926829268">
This is the problem of recognizing the kill (KFJ, Os-
wald) relation in the sentence “J. V. Oswald was
murdered at JFK after his assassin,
R. U. KFJ...” This task requires making several
local decisions, such as identifying named entities in the
sentence, in order to support the relation identification.
For example, it may be useful to identify that Oswald
and KFJ are people, and JFK is a location. This, in turn,
may help to identify that the kill action is described in the
sentence. At the same time, the relation kill constrains its
arguments to be people (or at least, not to be locations)
and helps to enforce that Oswald and KFJ are likely to
be people, while JFK is not.
In our model, we first learn a collection of “local” pre-
dictors, e.g., entity and relation identifiers. At decision
time, given a sentence, we produce a global decision that
optimizes over the suggestions of the classifiers that are
active in the sentence, known constraints among them
and, potentially, domain or tasks specific constraints rel-
evant to the current decision.
Although a brute-force algorithm may seem feasible
for short sentences, as the number of entity variable
grows, the computation becomes intractable very quickly.
Given n entities in a sentence, there are O(n2) possible
relations between them. Assume that each variable (en-
tity or relation) can take l labels (“none” is one of these
labels). Thus, there are ln2 possible assignments, which
is too large even for a small n.
When evaluated on simultaneous learning of named
entities and relations, our approach not only provides
a significant improvement in the predictors’ accuracy;
more importantly, it provides coherent solutions. While
many statistical methods make “stupid” mistakes (i.e.,
inconsistency among predictions), that no human ever
makes, as we show, our approach improves also the qual-
ity of the inference significantly.
The rest of the paper is organized as follows. Section 2
formally defines our problem and section 3 describes the
computational approach we propose. Experimental re-
sults are given in section 4, followed by some discussion
and conclusion in section 5.
</bodyText>
<sectionHeader confidence="0.847311" genericHeader="method">
2 The Relational Inference Problem
</sectionHeader>
<bodyText confidence="0.998992773195877">
We consider the relational inference problem within the
reasoning with classifiers paradigm, and study a spe-
cific but fairly general instantiation of this problem, moti-
vated by the problem of recognizing named entities (e.g.,
persons, locations, organization names) and relations be-
tween them (e.g. work for, located in, live in). We con-
sider a set V which consists of two types of variables V =
E U R. The first set of variables E = {E1, E2, · · · , En}
ranges G£. The value (called “label”) assigned to Ei ∈ E
is denoted fE, ∈ G£. The second set of variables
R = {Rij}{1≤i,j≤n;i0j} is viewed as binary relations
over E. Specifically, for each pair of entities Ei and Ej,
i =� j, we use Rij and Rji to denote the (binary) relations
(Ei, Ej) and (Ej, Ei) respectively. The set of labels of
relations is GR and the label assigned to relation Rij ∈ R
is fR„ ∈ GR.
Apparently, there exists some constraints on the labels
of corresponding relation and entity variables. For in-
stance, if the relation is live in, then the first entity should
be a person, and the second entity should be a location.
The correspondence between the relation and entity vari-
ables can be represented by a bipartite graph. Each rela-
tion variable Rij is connected to its first entity Ei , and
second entity Ej. We use N1 and N2 to denote the entity
variables of a relation Rij. Specifically, Ei = N1(Rij)
and Ej =N2(Rij).
In addition, we define a set of constraints on the out-
comes of the variables in V. C1 : G£ x GR → {0, 1}
constraint values of the first argument of a relation. C2
is defined similarly and constrains the second argument
a relation can take. For example, (born in, person) is
in C1 but not in C2 because the first entity of relation
born in has to be a person and the second entity can only
be a location instead of a person. Note that while we
define the constraints here as Boolean, our formalisms
in fact allows for stochastic constraints. Also note that
we can define a large number of constraints, such as
CR : GR x GR → {0, 1} which constrain types of re-
lations, etc. In fact, as will be clear in Sec. 3 the language
for defining constraints is very rich – linear (in)equalities
over V.
We exemplify the framework using the problem of si-
multaneous recognition of named entities and relations in
sentences. Briefly speaking, we assume a learning mech-
anism that can recognize entity phrases in sentences,
based on local contextual features. Similarly, we assume
a learning mechanism that can recognize the semantic re-
lation between two given phrases in a sentence.
We seek an inference algorithm that can produce a co-
herent labeling of entities and relations in a given sen-
tence. Furthermore, it follows, as best as possible the
recommendation of the entity and relation classifiers, but
also satisfies natural constraints that exist on whether spe-
cific entities can be the argument of specific relations,
whether two relations can occur together at the same
time, or any other information that might be available at
the inference time (e.g., suppose it is known that enti-
ties A and B represent the same location; one may like to
incorporate an additional constraint that prevents an in-
ference of the type: “C lives in A; C does not live in B”).
We note that a large number of problems can be mod-
eled this way. Examples include problems such as chunk-
ing sentences (Punyakanok and Roth, 2001), coreference
resolution and sequencing problems in computational bi-
ology. In fact, each of the components of our problem
here, the separate task of recognizing named entities in
sentences and the task of recognizing semantic relations
between phrases, can be modeled this way. However,
our goal is specifically to consider interacting problems
at different levels, resulting in more complex constraints
among them, and exhibit the power of our method.
The most direct way to formalize our inference prob-
lem is via the formalism of Markov Random Field (MRF)
theory (Li, 2001). Rather than doing that, for compu-
tational reasons, we first use a fairly standard transfor-
mation of MRF to a discrete optimization problem (see
(Kleinberg and Tardos, 1999) for details). Specifically,
under weak assumptions we can view the inference prob-
lem as the following optimization problem, which aims
to minimize the objective function that is the sum of the
following two cost functions.
Assignment cost: the cost of deviating from the assign-
ment of the variables V given by the classifiers. The spe-
cific cost function we use is defined as follows: Let l be
the label assigned to variable u ∈ V. If the marginal prob-
ability estimation is p = P(fu = l), then the assignment
cost cu(l) is − log p.
Constraint cost: the cost imposed by breaking con-
straints between neighboring nodes. The specific cost
function we use is defined as follows: Consider two en-
tity nodes Ei, Ej and its corresponding relation node Rij;
that is, Ei = N 1(Rij) and Ej = N2(Rij). The con-
straint cost indicates whether the labels are consistent
with the constraints. In particular, we use: d1(fEi, fRij )
is 0 if (fRij, fEi) ∈ C1; otherwise, d1(fEi, fRij) is ∞ 2.
Similarly, we use d2 to force the consistency of the sec-
ond argument of a relation.
</bodyText>
<footnote confidence="0.805624">
2In practice, we use a very large number (e.g., 915).
</footnote>
<bodyText confidence="0.9970655">
Since we are seeking the most probable global assign-
ment that satisfies the constraints, therefore, the overall
cost function we optimize, for a global labeling f of all
variables is:
</bodyText>
<equation confidence="0.960301">
C(f) = E cu(fu)
uEV
+ E [d1(fRij, fEi) + d2(fRij, fEj)J (1)
Rij ER
</equation>
<sectionHeader confidence="0.9888785" genericHeader="method">
3 A Computational Approach to
Relational Inference
</sectionHeader>
<bodyText confidence="0.999710097560976">
Unfortunately, it is not hard to see that the combinatorial
problem (Eq. 1) is computationally intractable even when
placing assumptions on the cost function (Kleinberg and
Tardos, 1999). The computational approach we adopt is
to develop a linear programming (LP) formulation of the
problem, and then solve the corresponding integer lin-
ear programming (ILP) problem. Our LP formulation is
based on the method proposed by (Chekuri et al., 2001).
Since the objective function (Eq. 1) is not a linear func-
tion in terms of the labels, we introduce new binary vari-
ables to represent different possible assignments to each
original variable; we then represent the objective function
as a linear function of these binary variables.
Let x{u,i1 be a {0,1}-variable, defined to be 1 if and
only if variable u is labeled i, where u ∈ E, i ∈ GE or
u ∈ R, i ∈ GR. For example, x{E1,21 = 1 when the
label of entity E1 is 2; x{R23,31 = 0 when the label of re-
lation R23 is not 3. Let x{Rij,r,Ei,e11 be a {0,1}-variable
indicating whether relation Rij is assigned label r and
its first argument, Ei, is assigned label e1. For instance,
x{R12,1,E1,21 = 1 means the label of relation R12 is 1
and the label of its first argument, E1, is 2. Similarly,
x{Rij,r,Ej,e21 = 1 indicates that Rij is assigned label r
and its second argument, Ej, is assigned label e2. With
these definitions, the optimization problem can be repre-
sented as the following ILP problem (Figure 1).
Equations (2) and (3) require that each entity or rela-
tion variable can only be assigned one label. Equations
(4) and (5) assure that the assignment to each entity or
relation variable is consistent with the assignment to its
neighboring variables. (6), (7), and (8) are the integral
constraints on these binary variables.
There are several advantages of representing the prob-
lem in an LP formulation. First of all, linear (in)equalities
are fairly general and are able to represent many types
of constraints (e.g., the decision time constraint in the
experiment in Sec. 4). More importantly, an ILP prob-
lem at this scale can be solved very quickly using current
commercial LP/ILP packages, like (Xpress-MP, 2003) or
(CPLEX, 2003). We introduce the general strategies of
solving an ILP problem here.
</bodyText>
<equation confidence="0.763829857142857">
Xmin X XcE(e) · x{E,e} + X cR(r) · x{R,r}
E∈E e∈LE R∈R r∈LR
X X X X
d1(r, e1) · x{Rij,r,Ei,eJ} + d2(r, e2) · x{Rij,r,Ej,e2}
&amp;quot;r∈Le
subject to:
X
+
Ei,Ej∈E
Ei6=Ej
X
e∈LE
X
r∈LR
Xx{E,e} =
r∈LR
Xx{R,r} = x{R,r,E,e}
e∈LE
x{E,e} ∈ {0, 1}
x{R,r} ∈ {0, 1}
x{R,r,E,e} ∈ {0, 1}
rEG e EG
R 2 E
∀E ∈ E (2)
∀R ∈ R (3)
∀E ∈ E and ∀R ∈ {R : E = N1(R) or R : E = N2(R)} (4)
∀R ∈ R and ∀E = N1(R) or E = N2(R) (5)
∀E ∈ E, e ∈ LE (6)
∀R ∈ R, r ∈ LR (7)
∀R ∈ R, r ∈ LR, E ∈ E, e ∈ LE (8)
EG
R 1 E
x{E,e} = 1
x{R,r} = 1
x{R,r,E,e}
</equation>
<figureCaption confidence="0.996592">
Figure 1: Integer Linear Programming Formulation
</figureCaption>
<subsectionHeader confidence="0.994838">
3.1 Linear Programming Relaxation (LPR)
</subsectionHeader>
<bodyText confidence="0.998726">
To solve an ILP problem, a natural idea is to relax the
integral constraints. That is, replacing (6), (7), and (8)
with:
</bodyText>
<equation confidence="0.99870825">
x{E,e} ≥ 0 ∀E ∈ E, e ∈ LE (9)
x{R,r} ≥ 0 ∀R ∈ R, r ∈ LR (10)
x{R,r,E,e} ≥ 0 ∀R ∈ R, r ∈ LR,
E ∈ E,e ∈ LE (11)
</equation>
<bodyText confidence="0.999873333333333">
If LPR returns an integer solution, then it is also the
optimal solution to the ILP problem. If the solution is
non integer, then at least it gives a lower bound to the
value of the cost function, which can be used in modi-
fying the problem and getting closer to deriving an op-
timal integer solution. A direct way to handle the non
integer solution is called rounding, which finds an inte-
ger point that is close to the non integer solution. Un-
der some conditions of cost functions, which do not hold
here, a well designed rounding algorithm can be shown
that the rounded solution is a good approximation to the
optimal solution (Kleinberg and Tardos, 1999; Chekuri et
al., 2001). Nevertheless, in general, the outcomes of the
rounding procedure may not even be a legal solution to
the problem.
</bodyText>
<subsectionHeader confidence="0.999478">
3.2 Branch &amp; Bound and Cutting Plane
</subsectionHeader>
<bodyText confidence="0.99976946875">
Branch and bound is the method that divides an ILP prob-
lem into several LP subproblems. It uses LPR as a sub-
routine to generate dual (upper and lower) bounds to re-
duce the search space, and finds the optimal solution as
well. When LPR finds a non integer solution, it splits the
problem on the non integer variable. For example, sup-
pose variable xi is fractional in an non integer solution to
the ILP problem min{cx : x ∈ S, x ∈ {0,1}n}, where S
is the linear constraints. The ILP problem can be split into
two sub LPR problems, min{cx : x ∈ S∩{xi = 0}} and
min{cx : x ∈ S∩{xi = 1}}. Since any feasible solution
provides an upper bound and any LPR solution generates
a lower bound, the search tree can be effectively cut.
Another strategy of dealing with non integer points,
which is often combined with branch &amp; bound, is called
cutting plane. When a non integer solution is given by
LPR, it adds a new linear constraint that makes the non in-
teger point infeasible, while still keeps the optimal integer
solution in the feasible region. As a result, the feasible
region is closer to the ideal polyhedron, which is the con-
vex hull of feasible integer solutions. The most famous
cutting plane algorithm is Gomory’s fractional cutting
plane method (Wolsey, 1998), which can be shown that
only finite number of additional constraints are needed.
Moreover, researchers develop different cutting plane al-
gorithms for different types of ILP problems. One exam-
ple is (Wang and Regan, 2000), which only focuses on
binary ILP problems.
Although in theory, a search based strategy may need
several steps to find the optimal solution, LPR always
generates integer solutions in our experiments. This phe-
nomenon may link to the theory of unimodularity.
</bodyText>
<subsectionHeader confidence="0.995865">
3.3 Unimodularity
</subsectionHeader>
<bodyText confidence="0.9999762">
When the coefficient matrix of a given linear program
in its standard form is unimodular, it can be shown that
the optimal solution to the linear program is in fact inte-
gral (Schrijver, 1986). In other words, LPR is guaranteed
to produce an integer solution.
</bodyText>
<construct confidence="0.622884857142857">
Definition 3.1 A matrix A of rank m is called unimodu-
lar ifall the entries of A are integers, and the determinant
of every square submatrix of A of order m is in 0,+1,-1.
Theorem 3.1 (Veinott &amp; Dantzig) Let A be an (m, n)-
integral matrix with full row rank m. Then the polyhe-
dron {x|x ≥ 0; Ax = b} is integral for each integral
vector b, if and only if A is unimodular.
</construct>
<bodyText confidence="0.989191">
Theorem 3.1 indicates that if a linear programming
problem is in its standard form, then regardless of the
cost function and the integral vector b, the optimal so-
lution is an integer if and only if the coefficient matrix A
is unimodular.
Although the coefficient matrix in our problem is not
unimodular, LPR still produces integer solutions for all
the (thousands of cases) we have experimented with. This
may be due to the fact that the coefficient matrix shares
many properties of a unimodular matrix. As a result, most
of the vertices of the polyhedron are integer points. An-
other possible reason is that given the cost function we
have, the optimal solution is always integer. Because of
the availability of very efficient LP/ILP packages, we de-
fer the exploration of this direction for now.
</bodyText>
<sectionHeader confidence="0.999644" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.988639391304348">
We describe below two experiments on the problem of
simultaneously recognizing entities and relations. In the
first, we view the task as a knowledge acquisition task
– we let the system read sentences and identify entities
and relations among them. Given that this is a difficult
task which may require quite often information beyond
the sentence, we consider also a “forced decision” task,
in which we simulate a question answering situation –
we ask the system, say, “who killed whom” and evaluate
it on identifying correctly the relation and its arguments,
given that it is known that somewhere in this sentence
this relation is active. In addition, this evaluation exhibits
the ability of our approach to incorporate task specific
constraints at decision time.
Our experiments are based on the TREC data set
(which consists of articles from WSJ, AP, etc.) that we
annotated for named entities and relations. In order to
effectively observe the interaction between relations and
entities, we picked 1437 sentences that have at least one
active relation. Among those sentences, there are 5336
entities, and 19048 pairs of entities (binary relations). En-
tity labels include 1685 persons, 1968 locations, 978 or-
ganizations and 705 others. Relation labels include 406
located in, 394 work for, 451 orgBased in, 521 live in,
268 kill, and 17007 none. Note that most pairs of entities
have no active relations at all. Therefore, relation none
significantly outnumbers others. Examples of each rela-
tion label and the constraints between a relation variable
and its two entity arguments are shown as follows.
Relation Entity1 Entity2 Example
located in loc loc (New York, US)
work for per org (Bill Gates, Microsoft)
orgBased in org loc (HP, Palo Alto)
live in per loc (Bush, US)
kill per per (Oswald, JFK)
In order to focus on the evaluation of our inference
procedure, we assume the problem of segmentation (or
phrase detection) (Abney, 1991; Punyakanok and Roth,
2001) is solved, and the entity boundaries are given to us
as input; thus we only concentrate on their classifications.
We evaluate our LP based global inference procedure
against two simpler approaches and a third that is given
more information at learning time. Basic, only tests our
entity and relation classifiers, which are trained indepen-
dently using only local features. In particular, the relation
classifier does not know the labels of its entity arguments,
and the entity classifier does not know the labels of rela-
tions in the sentence either. Since basic classifiers are
used in all approaches, we describe how they are trained
here.
For the entity classifier, one set of features are ex-
tracted from words within a size 4 window around the
target phrase. They are: (1) words, part-of-speech tags,
and conjunctions of them; (2) bigrams and trigrams of
the mixture of words and tags. In addition, some other
features are extracted from the target phrase, including:
symbol explanation
icap the first character of a word is capitalized
acap all characters of a word are capitalized
incap some characters of a word are capitalized
suffix the suffix of a word is “ing”, “ment”, etc.
bigram bigram of words in the target phrase
len number of words in the target phrase
place3 the phrase is/has a known place’s name
prof3 the phrase is/has a professional title (e.g. Lt.)
name3 the phrase is/has a known person’s name
For the relation classifier, there are three sets of fea-
tures: (1) features similar to those used in the entity clas-
sification are extracted from the two argument entities of
</bodyText>
<footnote confidence="0.959675">
3We collect names of famous places, people and popular ti-
tles from other data sources in advance.
</footnote>
<note confidence="0.920638142857143">
Pattern Example
arg1 , arg2 San Jose, CA
arg1 , · · · a · · · arg2 prof John Smith, a Starbucks manager · · ·
in/at arg1 in/at/, arg2 Officials in Perugia in Umbria province said · · ·
arg2 prof arg1 CNN reporter David McKinley · · ·
arg1 · · · native of ··· arg2 Elizabeth Dole is a native of Salisbury, N.C.
arg1 · · · based in/at arg2 Leslie Kota, a spokeswoman for K mart based in Troy, Mich. said · · ·
</note>
<tableCaption confidence="0.999561">
Table 1: Some patterns used in relation classification
</tableCaption>
<bodyText confidence="0.999346">
the relation; (2) conjunctions of the features from the two
arguments; (3) some patterns extracted from the sentence
or between the two arguments. Some features in category
(3) are “the number of words between arg1 and arg2 ”,
“whether arg1 and arg2 are the same word”, or “arg1 is
the beginning of the sentence and has words that consist
of all capitalized characters”, where ar$1 and ar$2 rep-
resent the first and second argument entities respectively.
In addition, Table 1 presents some patterns we use.
The learning algorithm used is a variation of the Win-
now update rule incorporated in SNoW (Roth, 1998;
Roth and Yih, 2002), a multi-class classifier that is specif-
ically tailored for large scale learning tasks. SNoW learns
a sparse network of linear functions, in which the targets
(entity classes or relation classes, in this case) are repre-
sented as linear functions over a common feature space.
While SNoW can be used as a classifier and predicts us-
ing a winner-take-all mechanism over the activation value
of the target classes, we can also rely directly on the raw
activation value it outputs, which is the weighted linear
sum of the active features, to estimate the posteriors. It
can be verified that the resulting values are monotonic
with the confidence in the prediction, therefore provide a
good source of probability estimation. We use softmax
(Bishop, 1995) over the raw activation values as condi-
tional probabilities. Specifically, suppose the number of
classes is n, and the raw activation values of class i is
acti. The posterior estimation for class i is derived by the
following equation.
</bodyText>
<equation confidence="0.79072">
eacti
� 1≤j≤neactj
</equation>
<bodyText confidence="0.999988179487179">
Pipeline, mimics the typical strategy in solving com-
plex natural language problems – separating a task into
several stages and solving them sequentially. For exam-
ple, a named entity recognizer may be trained using a dif-
ferent corpus in advance, and given to a relation classifier
as a tool to extract features. This approach first trains an
entity classifier as described in the basic approach, and
then uses the prediction of entities in addition to other
local features to learn the relation identifier. Note that
although the true labels of entities are known here when
training the relation identifier, this may not be the case
in general NLP problems. Since only the predicted en-
tity labels are available in testing, learning on the predic-
tions of the entity classifier presumably makes the rela-
tion classifier more tolerant to the mistakes of the entity
classifier. In fact, we also observe this phenomenon em-
pirically. When the relation classifier is trained using the
true entity labels, the performance is much worse than
using the predicted entity labels.
LP, is our global inference procedure. It takes as in-
put the constraints between a relation and its entity argu-
ments, and the output (the estimated probability distribu-
tion of labels) of the basic classifiers. Note that LP may
change the predictions for either entity labels or relation
labels, while pipeline fully trusts the labels of entity clas-
sifier, and only the relation predictions may be different
from the basic relation classifier. In other words, LP is
able to enhance the performance of entity classification,
which is impossible for pipeline.
The final approach, Omniscience, tests the conceptual
upper bound of this entity/relation classification problem.
It also trains the two classifiers separately as the basic
approach. However, it assumes that the entity classifier
knows the correct relation labels, and similarly the rela-
tion classifier knows the right entity labels as well. This
additional information is then used as features in training
and testing. Note that this assumption is totally unrealis-
tic. Nevertheless, it may give us a hint that how much a
global inference can achieve.
</bodyText>
<subsectionHeader confidence="0.785429">
4.1 Results
</subsectionHeader>
<bodyText confidence="0.952371642857143">
Tables 2 &amp; 3 show the performance of each approach in
Fβ=1 using 5-fold cross-validation. The results show that
LP performs consistently better than basic and pipeline,
both in entities and relations. Note that LP does not apply
learning at all, but still outperforms pipeline, which uses
entity predictions as new features in learning. The results
of the omniscient classifiers reveal that there is still room
for improvement. One option is to apply learning to tune
a better cost function in the LP approach.
One of the more significant results in our experiments,
we believe, is the improvement in the quality of the deci-
sions. As mentioned in Sec. 1, incorporating constraints
helps to avoid inconsistency in classification. It is in-
pi =
</bodyText>
<table confidence="0.998878833333333">
Approach Rec. person F1 organization location F1
Prec. Rec. Prec. F1 Rec. Prec.
Basic 89.4 89.2 89.3 86.9 91.4 89.1 68.2 90.9 77.9
Pipeline 89.4 89.2 89.3 86.9 91.4 89.1 68.2 90.9 77.9
LP 90.4 90.0 90.2 88.5 91.7 90.1 71.5 91.0 80.1
Omniscient 94.9 93.5 94.2 92.3 96.5 94.4 88.3 93.4 90.8
</table>
<tableCaption confidence="0.988313">
Table 2: Results of Entity Classification
</tableCaption>
<table confidence="0.999917583333333">
Approach located in work for orgBased in
Rec. Prec. F1 Rec. Prec. F1 Rec. Prec. F1
Basic 54.7 43.0 48.2 42.1 51.6 46.4 36.1 84.9 50.6
Pipeline 51.2 51.6 51.4 41.4 55.6 47.5 36.9 76.6 49.9
LP 53.2 59.5 56.2 40.4 72.9 52.0 36.3 90.1 51.7
Omniscient 64.0 54.5 58.9 50.5 69.1 58.4 50.2 76.7 60.7
Approach Rec. live in F1 Rec. kill F1
Prec. Prec.
Basic 39.7 61.6 48.3 82.1 73.6 77.6
Pipeline 42.6 62.2 50.6 83.2 76.4 79.6
LP 41.5 68.1 51.6 81.3 82.2 81.7
Omniscient 57.0 60.7 58.8 82.1 74.6 78.2
</table>
<tableCaption confidence="0.999902">
Table 3: Results of Relation Classification
</tableCaption>
<bodyText confidence="0.999956081081081">
teresting to investigate how often such mistakes happen
without global inference, and see how effectively the
global inference enhances this.
For this purpose, we define the quality of the decision
as follows. For an active relation of which the label is
classified correctly, if both its argument entities are also
predicted correctly, we count it as a coherent prediction.
Quality is then the number of coherent predictions di-
vided by the sum of coherent and incoherent predictions.
Since the basic and pipeline approaches do not have a
global view of the labels of entities and relations, 5%
to 25% of the predictions are incoherent. Therefore, the
quality is not always good. On the other hand, our global
inference procedure, LP, takes the natural constraints into
account, so it never generates incoherent predictions. If
the relation classifier has the correct entity labels as fea-
tures, a good learner should learn the constraints as well.
As a result, the quality of omniscient is almost as good as
LP.
Another experiment we did is the forced decision test,
which boosts the F1 of “kill” relation to 86.2%. Here
we consider only sentences in which the “kill” relation
is active. We force the system to determine which of the
possible relations in a sentence (i.e., which pair of en-
tities) has this relation by adding a new linear equality.
This is a realistic situation (e.g., in the context of ques-
tion answering) in that it adds an external constraint, not
present at the time of learning the classifiers and it eval-
uates the ability of our inference algorithm to cope with
it. The results exhibit that our expectations are correct.
In fact, we believe that in natural situations the number
of constraints that can apply is even larger. Observing
the algorithm performs on other, specific, forced deci-
sion tasks verifies that LP is reliable in these situations.
As shown in the experiment, it even performs better than
omniscience, which is given more information at learning
time, but cannot adapt to the situation at decision time.
</bodyText>
<sectionHeader confidence="0.999675" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999961035714286">
We presented an linear programming based approach
for global inference where decisions depend on the out-
comes of several different but mutually dependent classi-
fiers. Even in the presence of a fairly general constraint
structure, deviating from the sequential nature typically
studied, this approach can find the optimal solution effi-
ciently.
Contrary to general search schemes (e.g., beam
search), which do not guarantee optimality, the linear pro-
gramming approach provides an efficient way to finding
the optimal solution. The key advantage of the linear
programming formulation is its generality and flexibility;
in particular, it supports the ability to incorporate classi-
fiers learned in other contexts, “hints” supplied and de-
cision time constraints, and reason with all these for the
best global prediction. In sharp contrast with the typi-
cally used pipeline framework, our formulation does not
blindly trust the results of some classifiers, and therefore
is able to overcome mistakes made by classifiers with the
help of constraints.
Our experiments have demonstrated these advantages
by considering the interaction between entity and rela-
tion classifiers. In fact, more classifiers can be added and
used within the same framework. For example, if coref-
erence resolution is available, it is possible to incorporate
it in the form of constraints that force the labels of the co-
referred entities to be the same (but, of course, allowing
the global solution to reject the suggestion of these clas-
sifiers). Consequently, this may enhance the performance
of entity/relation recognition and, at the same time, cor-
rect possible coreference resolution errors. Another ex-
ample is to use chunking information for better relation
identification; suppose, for example, that we have avail-
able chunking information that identifies Subj+Verb and
Verb+Object phrases. Given a sentence that has the verb
“murder”, we may conclude that the subject and object of
this verb are in a “kill” relation. Since the chunking in-
formation is used in the global inference procedure, this
information will contribute to enhancing its performance
and robustness, relying on having more constraints and
overcoming possible mistakes by some of the classifiers.
Moreover, in an interactive environment where a user can
supply new constraints (e.g., a question answering situa-
tion) this framework is able to make use of the new in-
formation and enhance the performance at decision time,
without retraining the classifiers.
As we show, our formulation supports not only im-
proved accuracy, but also improves the ‘human-like”
quality of the decisions. We believe that it has the poten-
tial to be a powerful way for supporting natural language
inferences.
Acknowledgements This research has been supported
by NFS grants CAREER IIS-9984168, ITR IIS-0085836,
EIA-0224453, an ONR MURI Award, and an equipment
donation from AMD. We also thank the anonymous ref-
erees for their useful comments.
</bodyText>
<sectionHeader confidence="0.999279" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999919705882353">
S. Abney. 1991. Parsing by chunks. In S. Abney
R. Berwick and C. Tenny, editors, Principle-based
parsing: Computation and Psycholinguistics, pages
257–278. Kluwer, Dordrecht.
C. Bishop, 1995. Neural Networks for Pattern Recogni-
tion, chapter 6.4: Modelling conditional distributions,
page 215. Oxford University Press.
Y. Boykov, O. Veksler, and R. Zabih. 2001. Fast ap-
proximate energy minimization via graph cuts. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 23(11):1222–1239, November.
C. Chekuri, S. Khanna, J. Naor, and L. Zosin. 2001. Ap-
proximation algorithms for the metric labeling prob-
lem via a new linear programming formulation. In
Symposium on Discrete Algorithms, pages 109–118.
CPLEX. 2003. ILOG, Inc. CPLEX.
http://www.ilog.com/products/cplex/.
T. Dietterich. 2002. Machine learning for sequential
data: A review. In Structural, Syntactic, and Statistical
Pattern Recognition, pages 15–30. Springer-Verlag.
J. Kleinberg and E. Tardos. 1999. Approximation algo-
rithms for classification problems with pairwise rela-
tionships: Metric labeling and markov random fields.
In IEEE Symposium on Foundations of Computer Sci-
ence, pages 14–23.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. 18th
International Conf. on Machine Learning, pages 282–
289. Morgan Kaufmann, San Francisco, CA.
A. Levin, A. Zomet, and Yair Weiss. 2002. Learning
to perceive transparency from the statistics of natu-
ral scenes. In NIPS-15; The 2002 Conference on Ad-
vances in Neural Information Processing Systems.
S. Li. 2001. Markov Random Field Modeling in Image
Analisys. Springer-Verlag.
V. Punyakanok and D. Roth. 2001. The use of classifiers
in sequential inference. In NIPS-13; The 2000 Confer-
ence on Advances in Neural Information Processing
Systems, pages 995–1001. MIT Press.
D. Roth and W. Yih. 2002. Probabilistic reasoning for
entity &amp; relation recognition. In COLING 2002, The
19th International Conference on Computational Lin-
guistics, pages 835–841.
D. Roth. 1998. Learning to resolve natural language am-
biguities: A unified approach. In Proc. ofAAAI, pages
806–813.
D. Roth. 2002. Reasoning with classifiers. In Proc. of
the European Conference on Machine Learning, pages
506–510.
A. Schrijver. 1986. Theory of Linear and Integer Pro-
gramming. Wiley Interscience series in discrete math-
matics. John Wiley &amp; Sons, December.
B. Taskar, A. Pieter, and D. Koller. 2002. Discrimina-
tive probabilistic models for relational data. In Proc. of
Uncertainty in Artificial Intelligence, pages 485–492.
E. Tjong Kim Sang and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Proc. of
CoNLL-2003, pages 142–147. Edmonton, Canada.
X. Wang and A. Regan. 2000. A cutting plane method
for integer programming problems with binary vari-
ables. Technical Report UCI-ITS-WP-00-12, Univer-
sity of California, Irvine.
L. Wolsey. 1998. Integer Programming. John Wiley &amp;
Sons, Inc.
Xpress-MP. 2003. Dash Optimization. Xpress-MP.
http://www.dashoptimization.com/products.html.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.958554">
<title confidence="0.9979345">A Linear Programming Formulation for Global Inference in Language Tasks</title>
<author confidence="0.999679">Dan Roth Wen-tau Yih</author>
<affiliation confidence="0.986143">Department of Computer University of Illinois at</affiliation>
<abstract confidence="0.999366055555555">Given a collection of discrete random variables representing outcomes of learned local predictors in natural language, e.g., named entities and relations, we seek an optimal global assignment to the variables in the presence of general (non-sequential) constraints. Examples of these constraints include the type of arguments a relation can take, and the mutual activity of different relations, etc. We develop a linear programming formulation for this problem and evaluate it in the context of simultaneously learning named entities and relations. Our approach allows us to efficiently incorporate domain and task specific constraints at decision time, resulting in significant improvements in the accuracy and the “human-like” quality of the inferences.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Parsing by chunks.</title>
<date>1991</date>
<booktitle>Principle-based parsing: Computation and Psycholinguistics,</booktitle>
<pages>257--278</pages>
<editor>In S. Abney R. Berwick and C. Tenny, editors,</editor>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="21516" citStr="Abney, 1991" startWordPosition="3692" endWordPosition="3693">kill, and 17007 none. Note that most pairs of entities have no active relations at all. Therefore, relation none significantly outnumbers others. Examples of each relation label and the constraints between a relation variable and its two entity arguments are shown as follows. Relation Entity1 Entity2 Example located in loc loc (New York, US) work for per org (Bill Gates, Microsoft) orgBased in org loc (HP, Palo Alto) live in per loc (Bush, US) kill per per (Oswald, JFK) In order to focus on the evaluation of our inference procedure, we assume the problem of segmentation (or phrase detection) (Abney, 1991; Punyakanok and Roth, 2001) is solved, and the entity boundaries are given to us as input; thus we only concentrate on their classifications. We evaluate our LP based global inference procedure against two simpler approaches and a third that is given more information at learning time. Basic, only tests our entity and relation classifiers, which are trained independently using only local features. In particular, the relation classifier does not know the labels of its entity arguments, and the entity classifier does not know the labels of relations in the sentence either. Since basic classifier</context>
</contexts>
<marker>Abney, 1991</marker>
<rawString>S. Abney. 1991. Parsing by chunks. In S. Abney R. Berwick and C. Tenny, editors, Principle-based parsing: Computation and Psycholinguistics, pages 257–278. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Bishop</author>
</authors>
<title>Neural Networks for Pattern Recognition, chapter 6.4: Modelling conditional distributions,</title>
<date>1995</date>
<pages>215</pages>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="25074" citStr="Bishop, 1995" startWordPosition="4306" endWordPosition="4307">ns, in which the targets (entity classes or relation classes, in this case) are represented as linear functions over a common feature space. While SNoW can be used as a classifier and predicts using a winner-take-all mechanism over the activation value of the target classes, we can also rely directly on the raw activation value it outputs, which is the weighted linear sum of the active features, to estimate the posteriors. It can be verified that the resulting values are monotonic with the confidence in the prediction, therefore provide a good source of probability estimation. We use softmax (Bishop, 1995) over the raw activation values as conditional probabilities. Specifically, suppose the number of classes is n, and the raw activation values of class i is acti. The posterior estimation for class i is derived by the following equation. eacti � 1≤j≤neactj Pipeline, mimics the typical strategy in solving complex natural language problems – separating a task into several stages and solving them sequentially. For example, a named entity recognizer may be trained using a different corpus in advance, and given to a relation classifier as a tool to extract features. This approach first trains an ent</context>
</contexts>
<marker>Bishop, 1995</marker>
<rawString>C. Bishop, 1995. Neural Networks for Pattern Recognition, chapter 6.4: Modelling conditional distributions, page 215. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Boykov</author>
<author>O Veksler</author>
<author>R Zabih</author>
</authors>
<title>Fast approximate energy minimization via graph cuts.</title>
<date>2001</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>23</volume>
<issue>11</issue>
<contexts>
<context position="2901" citStr="Boykov et al., 2001" startWordPosition="437" endWordPosition="440"> In addition to the conceptual simplicity of this approach, it also seems to perform better experimentally (Tjong Kim Sang and De Meulder, 2003). Typically, efficient inference procedures in both frameworks rely on dynamic programming (e.g., Viterbi), which works well in sequential data. However, in many important problems, the structure is more general, resulting in computationally intractable inference. Problems of these sorts have been studied in computer vision, where inference is generally performed over low level measurements rather than over higher level predictors (Levin et al., 2002; Boykov et al., 2001). This work develops a novel inference with classifiers approach. Rather than being restricted on sequential data, we study a fairly general setting. The problem is defined in terms of a collection of discrete random variables representing binary relations and their arguments; we seek an optimal assignment to the variables in the presence of the constraints on the binary relations between variables and the relation types. The key insight to this solution comes from recent techniques developed for approximation algorithms (Chekuri et al., 2001). Following this work, we model inference as an opt</context>
</contexts>
<marker>Boykov, Veksler, Zabih, 2001</marker>
<rawString>Y. Boykov, O. Veksler, and R. Zabih. 2001. Fast approximate energy minimization via graph cuts. IEEE Transactions on Pattern Analysis and Machine Intelligence, 23(11):1222–1239, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chekuri</author>
<author>S Khanna</author>
<author>J Naor</author>
<author>L Zosin</author>
</authors>
<title>Approximation algorithms for the metric labeling problem via a new linear programming formulation. In</title>
<date>2001</date>
<booktitle>Symposium on Discrete Algorithms,</booktitle>
<pages>109--118</pages>
<contexts>
<context position="3450" citStr="Chekuri et al., 2001" startWordPosition="523" endWordPosition="526">n over higher level predictors (Levin et al., 2002; Boykov et al., 2001). This work develops a novel inference with classifiers approach. Rather than being restricted on sequential data, we study a fairly general setting. The problem is defined in terms of a collection of discrete random variables representing binary relations and their arguments; we seek an optimal assignment to the variables in the presence of the constraints on the binary relations between variables and the relation types. The key insight to this solution comes from recent techniques developed for approximation algorithms (Chekuri et al., 2001). Following this work, we model inference as an optimization problem, and show how to cast it as a linear program. Using existing numerical packages, which are able to solve very large linear programming problems in a very short time1, inference can be done very quickly. Our approach could be contrasted with other ap1For example, (CPLEX, 2003) is able to solve a linear programming problem of 13 million variables within 5 minutes. proaches to sequential inference or to general Markov random field approaches (Lafferty et al., 2001; Taskar et al., 2002). The key difference is that in these approa</context>
<context position="12912" citStr="Chekuri et al., 2001" startWordPosition="2137" endWordPosition="2140">n we optimize, for a global labeling f of all variables is: C(f) = E cu(fu) uEV + E [d1(fRij, fEi) + d2(fRij, fEj)J (1) Rij ER 3 A Computational Approach to Relational Inference Unfortunately, it is not hard to see that the combinatorial problem (Eq. 1) is computationally intractable even when placing assumptions on the cost function (Kleinberg and Tardos, 1999). The computational approach we adopt is to develop a linear programming (LP) formulation of the problem, and then solve the corresponding integer linear programming (ILP) problem. Our LP formulation is based on the method proposed by (Chekuri et al., 2001). Since the objective function (Eq. 1) is not a linear function in terms of the labels, we introduce new binary variables to represent different possible assignments to each original variable; we then represent the objective function as a linear function of these binary variables. Let x{u,i1 be a {0,1}-variable, defined to be 1 if and only if variable u is labeled i, where u ∈ E, i ∈ GE or u ∈ R, i ∈ GR. For example, x{E1,21 = 1 when the label of entity E1 is 2; x{R23,31 = 0 when the label of relation R23 is not 3. Let x{Rij,r,Ei,e11 be a {0,1}-variable indicating whether relation Rij is assig</context>
<context position="16227" citStr="Chekuri et al., 2001" startWordPosition="2783" endWordPosition="2786">optimal solution to the ILP problem. If the solution is non integer, then at least it gives a lower bound to the value of the cost function, which can be used in modifying the problem and getting closer to deriving an optimal integer solution. A direct way to handle the non integer solution is called rounding, which finds an integer point that is close to the non integer solution. Under some conditions of cost functions, which do not hold here, a well designed rounding algorithm can be shown that the rounded solution is a good approximation to the optimal solution (Kleinberg and Tardos, 1999; Chekuri et al., 2001). Nevertheless, in general, the outcomes of the rounding procedure may not even be a legal solution to the problem. 3.2 Branch &amp; Bound and Cutting Plane Branch and bound is the method that divides an ILP problem into several LP subproblems. It uses LPR as a subroutine to generate dual (upper and lower) bounds to reduce the search space, and finds the optimal solution as well. When LPR finds a non integer solution, it splits the problem on the non integer variable. For example, suppose variable xi is fractional in an non integer solution to the ILP problem min{cx : x ∈ S, x ∈ {0,1}n}, where S i</context>
</contexts>
<marker>Chekuri, Khanna, Naor, Zosin, 2001</marker>
<rawString>C. Chekuri, S. Khanna, J. Naor, and L. Zosin. 2001. Approximation algorithms for the metric labeling problem via a new linear programming formulation. In Symposium on Discrete Algorithms, pages 109–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>CPLEX</author>
</authors>
<date>2003</date>
<publisher>ILOG, Inc. CPLEX. http://www.ilog.com/products/cplex/.</publisher>
<contexts>
<context position="3795" citStr="CPLEX, 2003" startWordPosition="583" endWordPosition="584">eek an optimal assignment to the variables in the presence of the constraints on the binary relations between variables and the relation types. The key insight to this solution comes from recent techniques developed for approximation algorithms (Chekuri et al., 2001). Following this work, we model inference as an optimization problem, and show how to cast it as a linear program. Using existing numerical packages, which are able to solve very large linear programming problems in a very short time1, inference can be done very quickly. Our approach could be contrasted with other ap1For example, (CPLEX, 2003) is able to solve a linear programming problem of 13 million variables within 5 minutes. proaches to sequential inference or to general Markov random field approaches (Lafferty et al., 2001; Taskar et al., 2002). The key difference is that in these approaches, the model is learned globally, under the constraints imposed by the domain. In our approach, predictors do not need to be learned in the context of the decision tasks, but rather can be learned in other contexts, or incorporated as background knowledge. This way, our approach allows the incorporation of constraints into decisions in a dy</context>
<context position="14649" citStr="CPLEX, 2003" startWordPosition="2444" endWordPosition="2445">(5) assure that the assignment to each entity or relation variable is consistent with the assignment to its neighboring variables. (6), (7), and (8) are the integral constraints on these binary variables. There are several advantages of representing the problem in an LP formulation. First of all, linear (in)equalities are fairly general and are able to represent many types of constraints (e.g., the decision time constraint in the experiment in Sec. 4). More importantly, an ILP problem at this scale can be solved very quickly using current commercial LP/ILP packages, like (Xpress-MP, 2003) or (CPLEX, 2003). We introduce the general strategies of solving an ILP problem here. Xmin X XcE(e) · x{E,e} + X cR(r) · x{R,r} E∈E e∈LE R∈R r∈LR X X X X d1(r, e1) · x{Rij,r,Ei,eJ} + d2(r, e2) · x{Rij,r,Ej,e2} &amp;quot;r∈Le subject to: X + Ei,Ej∈E Ei6=Ej X e∈LE X r∈LR Xx{E,e} = r∈LR Xx{R,r} = x{R,r,E,e} e∈LE x{E,e} ∈ {0, 1} x{R,r} ∈ {0, 1} x{R,r,E,e} ∈ {0, 1} rEG e EG R 2 E ∀E ∈ E (2) ∀R ∈ R (3) ∀E ∈ E and ∀R ∈ {R : E = N1(R) or R : E = N2(R)} (4) ∀R ∈ R and ∀E = N1(R) or E = N2(R) (5) ∀E ∈ E, e ∈ LE (6) ∀R ∈ R, r ∈ LR (7) ∀R ∈ R, r ∈ LR, E ∈ E, e ∈ LE (8) EG R 1 E x{E,e} = 1 x{R,r} = 1 x{R,r,E,e} Figure 1: Integer L</context>
</contexts>
<marker>CPLEX, 2003</marker>
<rawString>CPLEX. 2003. ILOG, Inc. CPLEX. http://www.ilog.com/products/cplex/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dietterich</author>
</authors>
<title>Machine learning for sequential data: A review.</title>
<date>2002</date>
<booktitle>In Structural, Syntactic, and Statistical Pattern Recognition,</booktitle>
<pages>15--30</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="1659" citStr="Dietterich, 2002" startWordPosition="255" endWordPosition="256"> different but mutually dependent predictions. These predictions must respect some constraints that could arise from the nature of the data or from domain or task specific conditions. For example, in part-ofspeech tagging, a sentence must have at least one verb, and cannot have three consecutive verbs. These facts can be used as constraints. In named entity recognition, “no entities can overlap” is a common constraint used in various works (Tjong Kim Sang and De Meulder, 2003). Efficient solutions to problems of these sort have been given when the constraints on the predictors are sequential (Dietterich, 2002). These solutions can be categorized into the following two frameworks. Learning global models trains a probabilistic model under the constraints imposed by the domain. Examples include variations of HMMs, conditional models and sequential variations of Markov random fields (Lafferty et al., 2001). The other framework, inference with classifiers (Roth, 2002), views maintaining constraints and learning classifiers as separate processes. Various local classifiers are trained without the knowledge of constraints. The predictions are taken as input on the inference procedure which then finds the b</context>
</contexts>
<marker>Dietterich, 2002</marker>
<rawString>T. Dietterich. 2002. Machine learning for sequential data: A review. In Structural, Syntactic, and Statistical Pattern Recognition, pages 15–30. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kleinberg</author>
<author>E Tardos</author>
</authors>
<title>Approximation algorithms for classification problems with pairwise relationships: Metric labeling and markov random fields.</title>
<date>1999</date>
<booktitle>In IEEE Symposium on Foundations of Computer Science,</booktitle>
<pages>14--23</pages>
<contexts>
<context position="11057" citStr="Kleinberg and Tardos, 1999" startWordPosition="1816" endWordPosition="1819">here, the separate task of recognizing named entities in sentences and the task of recognizing semantic relations between phrases, can be modeled this way. However, our goal is specifically to consider interacting problems at different levels, resulting in more complex constraints among them, and exhibit the power of our method. The most direct way to formalize our inference problem is via the formalism of Markov Random Field (MRF) theory (Li, 2001). Rather than doing that, for computational reasons, we first use a fairly standard transformation of MRF to a discrete optimization problem (see (Kleinberg and Tardos, 1999) for details). Specifically, under weak assumptions we can view the inference problem as the following optimization problem, which aims to minimize the objective function that is the sum of the following two cost functions. Assignment cost: the cost of deviating from the assignment of the variables V given by the classifiers. The specific cost function we use is defined as follows: Let l be the label assigned to variable u ∈ V. If the marginal probability estimation is p = P(fu = l), then the assignment cost cu(l) is − log p. Constraint cost: the cost imposed by breaking constraints between ne</context>
<context position="12655" citStr="Kleinberg and Tardos, 1999" startWordPosition="2096" endWordPosition="2099"> Similarly, we use d2 to force the consistency of the second argument of a relation. 2In practice, we use a very large number (e.g., 915). Since we are seeking the most probable global assignment that satisfies the constraints, therefore, the overall cost function we optimize, for a global labeling f of all variables is: C(f) = E cu(fu) uEV + E [d1(fRij, fEi) + d2(fRij, fEj)J (1) Rij ER 3 A Computational Approach to Relational Inference Unfortunately, it is not hard to see that the combinatorial problem (Eq. 1) is computationally intractable even when placing assumptions on the cost function (Kleinberg and Tardos, 1999). The computational approach we adopt is to develop a linear programming (LP) formulation of the problem, and then solve the corresponding integer linear programming (ILP) problem. Our LP formulation is based on the method proposed by (Chekuri et al., 2001). Since the objective function (Eq. 1) is not a linear function in terms of the labels, we introduce new binary variables to represent different possible assignments to each original variable; we then represent the objective function as a linear function of these binary variables. Let x{u,i1 be a {0,1}-variable, defined to be 1 if and only i</context>
<context position="16204" citStr="Kleinberg and Tardos, 1999" startWordPosition="2779" endWordPosition="2782">lution, then it is also the optimal solution to the ILP problem. If the solution is non integer, then at least it gives a lower bound to the value of the cost function, which can be used in modifying the problem and getting closer to deriving an optimal integer solution. A direct way to handle the non integer solution is called rounding, which finds an integer point that is close to the non integer solution. Under some conditions of cost functions, which do not hold here, a well designed rounding algorithm can be shown that the rounded solution is a good approximation to the optimal solution (Kleinberg and Tardos, 1999; Chekuri et al., 2001). Nevertheless, in general, the outcomes of the rounding procedure may not even be a legal solution to the problem. 3.2 Branch &amp; Bound and Cutting Plane Branch and bound is the method that divides an ILP problem into several LP subproblems. It uses LPR as a subroutine to generate dual (upper and lower) bounds to reduce the search space, and finds the optimal solution as well. When LPR finds a non integer solution, it splits the problem on the non integer variable. For example, suppose variable xi is fractional in an non integer solution to the ILP problem min{cx : x ∈ S,</context>
</contexts>
<marker>Kleinberg, Tardos, 1999</marker>
<rawString>J. Kleinberg and E. Tardos. 1999. Approximation algorithms for classification problems with pairwise relationships: Metric labeling and markov random fields. In IEEE Symposium on Foundations of Computer Science, pages 14–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. 18th International Conf. on Machine Learning,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="1957" citStr="Lafferty et al., 2001" startWordPosition="299" endWordPosition="302">ive verbs. These facts can be used as constraints. In named entity recognition, “no entities can overlap” is a common constraint used in various works (Tjong Kim Sang and De Meulder, 2003). Efficient solutions to problems of these sort have been given when the constraints on the predictors are sequential (Dietterich, 2002). These solutions can be categorized into the following two frameworks. Learning global models trains a probabilistic model under the constraints imposed by the domain. Examples include variations of HMMs, conditional models and sequential variations of Markov random fields (Lafferty et al., 2001). The other framework, inference with classifiers (Roth, 2002), views maintaining constraints and learning classifiers as separate processes. Various local classifiers are trained without the knowledge of constraints. The predictions are taken as input on the inference procedure which then finds the best global prediction. In addition to the conceptual simplicity of this approach, it also seems to perform better experimentally (Tjong Kim Sang and De Meulder, 2003). Typically, efficient inference procedures in both frameworks rely on dynamic programming (e.g., Viterbi), which works well in sequ</context>
<context position="3984" citStr="Lafferty et al., 2001" startWordPosition="612" endWordPosition="615">es from recent techniques developed for approximation algorithms (Chekuri et al., 2001). Following this work, we model inference as an optimization problem, and show how to cast it as a linear program. Using existing numerical packages, which are able to solve very large linear programming problems in a very short time1, inference can be done very quickly. Our approach could be contrasted with other ap1For example, (CPLEX, 2003) is able to solve a linear programming problem of 13 million variables within 5 minutes. proaches to sequential inference or to general Markov random field approaches (Lafferty et al., 2001; Taskar et al., 2002). The key difference is that in these approaches, the model is learned globally, under the constraints imposed by the domain. In our approach, predictors do not need to be learned in the context of the decision tasks, but rather can be learned in other contexts, or incorporated as background knowledge. This way, our approach allows the incorporation of constraints into decisions in a dynamic fashion and can therefore support task specific inferences. The significance of this is clearly shown in our experimental results. We develop our models in the context of natural lang</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. 18th International Conf. on Machine Learning, pages 282– 289. Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Levin</author>
<author>A Zomet</author>
<author>Yair Weiss</author>
</authors>
<title>Learning to perceive transparency from the statistics of natural scenes.</title>
<date>2002</date>
<booktitle>In NIPS-15; The 2002 Conference on Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="2879" citStr="Levin et al., 2002" startWordPosition="433" endWordPosition="436">t global prediction. In addition to the conceptual simplicity of this approach, it also seems to perform better experimentally (Tjong Kim Sang and De Meulder, 2003). Typically, efficient inference procedures in both frameworks rely on dynamic programming (e.g., Viterbi), which works well in sequential data. However, in many important problems, the structure is more general, resulting in computationally intractable inference. Problems of these sorts have been studied in computer vision, where inference is generally performed over low level measurements rather than over higher level predictors (Levin et al., 2002; Boykov et al., 2001). This work develops a novel inference with classifiers approach. Rather than being restricted on sequential data, we study a fairly general setting. The problem is defined in terms of a collection of discrete random variables representing binary relations and their arguments; we seek an optimal assignment to the variables in the presence of the constraints on the binary relations between variables and the relation types. The key insight to this solution comes from recent techniques developed for approximation algorithms (Chekuri et al., 2001). Following this work, we mod</context>
</contexts>
<marker>Levin, Zomet, Weiss, 2002</marker>
<rawString>A. Levin, A. Zomet, and Yair Weiss. 2002. Learning to perceive transparency from the statistics of natural scenes. In NIPS-15; The 2002 Conference on Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Li</author>
</authors>
<title>Markov Random Field Modeling in Image Analisys.</title>
<date>2001</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="10883" citStr="Li, 2001" startWordPosition="1790" endWordPosition="1791">tences (Punyakanok and Roth, 2001), coreference resolution and sequencing problems in computational biology. In fact, each of the components of our problem here, the separate task of recognizing named entities in sentences and the task of recognizing semantic relations between phrases, can be modeled this way. However, our goal is specifically to consider interacting problems at different levels, resulting in more complex constraints among them, and exhibit the power of our method. The most direct way to formalize our inference problem is via the formalism of Markov Random Field (MRF) theory (Li, 2001). Rather than doing that, for computational reasons, we first use a fairly standard transformation of MRF to a discrete optimization problem (see (Kleinberg and Tardos, 1999) for details). Specifically, under weak assumptions we can view the inference problem as the following optimization problem, which aims to minimize the objective function that is the sum of the following two cost functions. Assignment cost: the cost of deviating from the assignment of the variables V given by the classifiers. The specific cost function we use is defined as follows: Let l be the label assigned to variable u</context>
</contexts>
<marker>Li, 2001</marker>
<rawString>S. Li. 2001. Markov Random Field Modeling in Image Analisys. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
</authors>
<title>The use of classifiers in sequential inference.</title>
<date>2001</date>
<booktitle>In NIPS-13; The 2000 Conference on Advances in Neural Information Processing Systems,</booktitle>
<pages>995--1001</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="10308" citStr="Punyakanok and Roth, 2001" startWordPosition="1698" endWordPosition="1701">ation classifiers, but also satisfies natural constraints that exist on whether specific entities can be the argument of specific relations, whether two relations can occur together at the same time, or any other information that might be available at the inference time (e.g., suppose it is known that entities A and B represent the same location; one may like to incorporate an additional constraint that prevents an inference of the type: “C lives in A; C does not live in B”). We note that a large number of problems can be modeled this way. Examples include problems such as chunking sentences (Punyakanok and Roth, 2001), coreference resolution and sequencing problems in computational biology. In fact, each of the components of our problem here, the separate task of recognizing named entities in sentences and the task of recognizing semantic relations between phrases, can be modeled this way. However, our goal is specifically to consider interacting problems at different levels, resulting in more complex constraints among them, and exhibit the power of our method. The most direct way to formalize our inference problem is via the formalism of Markov Random Field (MRF) theory (Li, 2001). Rather than doing that,</context>
<context position="21544" citStr="Punyakanok and Roth, 2001" startWordPosition="3694" endWordPosition="3697">07 none. Note that most pairs of entities have no active relations at all. Therefore, relation none significantly outnumbers others. Examples of each relation label and the constraints between a relation variable and its two entity arguments are shown as follows. Relation Entity1 Entity2 Example located in loc loc (New York, US) work for per org (Bill Gates, Microsoft) orgBased in org loc (HP, Palo Alto) live in per loc (Bush, US) kill per per (Oswald, JFK) In order to focus on the evaluation of our inference procedure, we assume the problem of segmentation (or phrase detection) (Abney, 1991; Punyakanok and Roth, 2001) is solved, and the entity boundaries are given to us as input; thus we only concentrate on their classifications. We evaluate our LP based global inference procedure against two simpler approaches and a third that is given more information at learning time. Basic, only tests our entity and relation classifiers, which are trained independently using only local features. In particular, the relation classifier does not know the labels of its entity arguments, and the entity classifier does not know the labels of relations in the sentence either. Since basic classifiers are used in all approaches</context>
</contexts>
<marker>Punyakanok, Roth, 2001</marker>
<rawString>V. Punyakanok and D. Roth. 2001. The use of classifiers in sequential inference. In NIPS-13; The 2000 Conference on Advances in Neural Information Processing Systems, pages 995–1001. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>Probabilistic reasoning for entity &amp; relation recognition.</title>
<date>2002</date>
<booktitle>In COLING 2002, The 19th International Conference on Computational Linguistics,</booktitle>
<pages>835--841</pages>
<contexts>
<context position="24326" citStr="Roth and Yih, 2002" startWordPosition="4182" endWordPosition="4185">ion; (2) conjunctions of the features from the two arguments; (3) some patterns extracted from the sentence or between the two arguments. Some features in category (3) are “the number of words between arg1 and arg2 ”, “whether arg1 and arg2 are the same word”, or “arg1 is the beginning of the sentence and has words that consist of all capitalized characters”, where ar$1 and ar$2 represent the first and second argument entities respectively. In addition, Table 1 presents some patterns we use. The learning algorithm used is a variation of the Winnow update rule incorporated in SNoW (Roth, 1998; Roth and Yih, 2002), a multi-class classifier that is specifically tailored for large scale learning tasks. SNoW learns a sparse network of linear functions, in which the targets (entity classes or relation classes, in this case) are represented as linear functions over a common feature space. While SNoW can be used as a classifier and predicts using a winner-take-all mechanism over the activation value of the target classes, we can also rely directly on the raw activation value it outputs, which is the weighted linear sum of the active features, to estimate the posteriors. It can be verified that the resulting </context>
</contexts>
<marker>Roth, Yih, 2002</marker>
<rawString>D. Roth and W. Yih. 2002. Probabilistic reasoning for entity &amp; relation recognition. In COLING 2002, The 19th International Conference on Computational Linguistics, pages 835–841.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
</authors>
<title>Learning to resolve natural language ambiguities: A unified approach.</title>
<date>1998</date>
<booktitle>In Proc. ofAAAI,</booktitle>
<pages>806--813</pages>
<contexts>
<context position="24305" citStr="Roth, 1998" startWordPosition="4180" endWordPosition="4181">on the relation; (2) conjunctions of the features from the two arguments; (3) some patterns extracted from the sentence or between the two arguments. Some features in category (3) are “the number of words between arg1 and arg2 ”, “whether arg1 and arg2 are the same word”, or “arg1 is the beginning of the sentence and has words that consist of all capitalized characters”, where ar$1 and ar$2 represent the first and second argument entities respectively. In addition, Table 1 presents some patterns we use. The learning algorithm used is a variation of the Winnow update rule incorporated in SNoW (Roth, 1998; Roth and Yih, 2002), a multi-class classifier that is specifically tailored for large scale learning tasks. SNoW learns a sparse network of linear functions, in which the targets (entity classes or relation classes, in this case) are represented as linear functions over a common feature space. While SNoW can be used as a classifier and predicts using a winner-take-all mechanism over the activation value of the target classes, we can also rely directly on the raw activation value it outputs, which is the weighted linear sum of the active features, to estimate the posteriors. It can be verifie</context>
</contexts>
<marker>Roth, 1998</marker>
<rawString>D. Roth. 1998. Learning to resolve natural language ambiguities: A unified approach. In Proc. ofAAAI, pages 806–813.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
</authors>
<title>Reasoning with classifiers.</title>
<date>2002</date>
<booktitle>In Proc. of the European Conference on Machine Learning,</booktitle>
<pages>506--510</pages>
<contexts>
<context position="2019" citStr="Roth, 2002" startWordPosition="309" endWordPosition="310">ition, “no entities can overlap” is a common constraint used in various works (Tjong Kim Sang and De Meulder, 2003). Efficient solutions to problems of these sort have been given when the constraints on the predictors are sequential (Dietterich, 2002). These solutions can be categorized into the following two frameworks. Learning global models trains a probabilistic model under the constraints imposed by the domain. Examples include variations of HMMs, conditional models and sequential variations of Markov random fields (Lafferty et al., 2001). The other framework, inference with classifiers (Roth, 2002), views maintaining constraints and learning classifiers as separate processes. Various local classifiers are trained without the knowledge of constraints. The predictions are taken as input on the inference procedure which then finds the best global prediction. In addition to the conceptual simplicity of this approach, it also seems to perform better experimentally (Tjong Kim Sang and De Meulder, 2003). Typically, efficient inference procedures in both frameworks rely on dynamic programming (e.g., Viterbi), which works well in sequential data. However, in many important problems, the structur</context>
</contexts>
<marker>Roth, 2002</marker>
<rawString>D. Roth. 2002. Reasoning with classifiers. In Proc. of the European Conference on Machine Learning, pages 506–510.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Schrijver</author>
</authors>
<title>Theory of Linear and Integer Programming. Wiley Interscience series in discrete mathmatics.</title>
<date>1986</date>
<publisher>John Wiley &amp; Sons,</publisher>
<contexts>
<context position="18329" citStr="Schrijver, 1986" startWordPosition="3152" endWordPosition="3153">e needed. Moreover, researchers develop different cutting plane algorithms for different types of ILP problems. One example is (Wang and Regan, 2000), which only focuses on binary ILP problems. Although in theory, a search based strategy may need several steps to find the optimal solution, LPR always generates integer solutions in our experiments. This phenomenon may link to the theory of unimodularity. 3.3 Unimodularity When the coefficient matrix of a given linear program in its standard form is unimodular, it can be shown that the optimal solution to the linear program is in fact integral (Schrijver, 1986). In other words, LPR is guaranteed to produce an integer solution. Definition 3.1 A matrix A of rank m is called unimodular ifall the entries of A are integers, and the determinant of every square submatrix of A of order m is in 0,+1,-1. Theorem 3.1 (Veinott &amp; Dantzig) Let A be an (m, n)- integral matrix with full row rank m. Then the polyhedron {x|x ≥ 0; Ax = b} is integral for each integral vector b, if and only if A is unimodular. Theorem 3.1 indicates that if a linear programming problem is in its standard form, then regardless of the cost function and the integral vector b, the optimal s</context>
</contexts>
<marker>Schrijver, 1986</marker>
<rawString>A. Schrijver. 1986. Theory of Linear and Integer Programming. Wiley Interscience series in discrete mathmatics. John Wiley &amp; Sons, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>A Pieter</author>
<author>D Koller</author>
</authors>
<title>Discriminative probabilistic models for relational data. In</title>
<date>2002</date>
<booktitle>Proc. of Uncertainty in Artificial Intelligence,</booktitle>
<pages>485--492</pages>
<contexts>
<context position="4006" citStr="Taskar et al., 2002" startWordPosition="616" endWordPosition="619">es developed for approximation algorithms (Chekuri et al., 2001). Following this work, we model inference as an optimization problem, and show how to cast it as a linear program. Using existing numerical packages, which are able to solve very large linear programming problems in a very short time1, inference can be done very quickly. Our approach could be contrasted with other ap1For example, (CPLEX, 2003) is able to solve a linear programming problem of 13 million variables within 5 minutes. proaches to sequential inference or to general Markov random field approaches (Lafferty et al., 2001; Taskar et al., 2002). The key difference is that in these approaches, the model is learned globally, under the constraints imposed by the domain. In our approach, predictors do not need to be learned in the context of the decision tasks, but rather can be learned in other contexts, or incorporated as background knowledge. This way, our approach allows the incorporation of constraints into decisions in a dynamic fashion and can therefore support task specific inferences. The significance of this is clearly shown in our experimental results. We develop our models in the context of natural language inferences and ev</context>
</contexts>
<marker>Taskar, Pieter, Koller, 2002</marker>
<rawString>B. Taskar, A. Pieter, and D. Koller. 2002. Discriminative probabilistic models for relational data. In Proc. of Uncertainty in Artificial Intelligence, pages 485–492.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Tjong Kim Sang</author>
<author>F De Meulder</author>
</authors>
<title>Introduction to the CoNLL-2003 shared task: Languageindependent named entity recognition.</title>
<date>2003</date>
<booktitle>In Proc. of CoNLL-2003,</booktitle>
<pages>142--147</pages>
<location>Edmonton, Canada.</location>
<marker>Sang, De Meulder, 2003</marker>
<rawString>E. Tjong Kim Sang and F. De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Languageindependent named entity recognition. In Proc. of CoNLL-2003, pages 142–147. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wang</author>
<author>A Regan</author>
</authors>
<title>A cutting plane method for integer programming problems with binary variables.</title>
<date>2000</date>
<tech>Technical Report UCI-ITS-WP-00-12,</tech>
<institution>University of California, Irvine.</institution>
<contexts>
<context position="17862" citStr="Wang and Regan, 2000" startWordPosition="3073" endWordPosition="3076">er solution is given by LPR, it adds a new linear constraint that makes the non integer point infeasible, while still keeps the optimal integer solution in the feasible region. As a result, the feasible region is closer to the ideal polyhedron, which is the convex hull of feasible integer solutions. The most famous cutting plane algorithm is Gomory’s fractional cutting plane method (Wolsey, 1998), which can be shown that only finite number of additional constraints are needed. Moreover, researchers develop different cutting plane algorithms for different types of ILP problems. One example is (Wang and Regan, 2000), which only focuses on binary ILP problems. Although in theory, a search based strategy may need several steps to find the optimal solution, LPR always generates integer solutions in our experiments. This phenomenon may link to the theory of unimodularity. 3.3 Unimodularity When the coefficient matrix of a given linear program in its standard form is unimodular, it can be shown that the optimal solution to the linear program is in fact integral (Schrijver, 1986). In other words, LPR is guaranteed to produce an integer solution. Definition 3.1 A matrix A of rank m is called unimodular ifall th</context>
</contexts>
<marker>Wang, Regan, 2000</marker>
<rawString>X. Wang and A. Regan. 2000. A cutting plane method for integer programming problems with binary variables. Technical Report UCI-ITS-WP-00-12, University of California, Irvine.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Wolsey</author>
</authors>
<title>Integer Programming.</title>
<date>1998</date>
<publisher>John Wiley &amp; Sons, Inc.</publisher>
<contexts>
<context position="17640" citStr="Wolsey, 1998" startWordPosition="3040" endWordPosition="3041"> solution generates a lower bound, the search tree can be effectively cut. Another strategy of dealing with non integer points, which is often combined with branch &amp; bound, is called cutting plane. When a non integer solution is given by LPR, it adds a new linear constraint that makes the non integer point infeasible, while still keeps the optimal integer solution in the feasible region. As a result, the feasible region is closer to the ideal polyhedron, which is the convex hull of feasible integer solutions. The most famous cutting plane algorithm is Gomory’s fractional cutting plane method (Wolsey, 1998), which can be shown that only finite number of additional constraints are needed. Moreover, researchers develop different cutting plane algorithms for different types of ILP problems. One example is (Wang and Regan, 2000), which only focuses on binary ILP problems. Although in theory, a search based strategy may need several steps to find the optimal solution, LPR always generates integer solutions in our experiments. This phenomenon may link to the theory of unimodularity. 3.3 Unimodularity When the coefficient matrix of a given linear program in its standard form is unimodular, it can be sh</context>
</contexts>
<marker>Wolsey, 1998</marker>
<rawString>L. Wolsey. 1998. Integer Programming. John Wiley &amp; Sons, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xpress-MP</author>
</authors>
<title>Dash Optimization.</title>
<date>2003</date>
<note>Xpress-MP. http://www.dashoptimization.com/products.html.</note>
<contexts>
<context position="14632" citStr="Xpress-MP, 2003" startWordPosition="2441" endWordPosition="2442">l. Equations (4) and (5) assure that the assignment to each entity or relation variable is consistent with the assignment to its neighboring variables. (6), (7), and (8) are the integral constraints on these binary variables. There are several advantages of representing the problem in an LP formulation. First of all, linear (in)equalities are fairly general and are able to represent many types of constraints (e.g., the decision time constraint in the experiment in Sec. 4). More importantly, an ILP problem at this scale can be solved very quickly using current commercial LP/ILP packages, like (Xpress-MP, 2003) or (CPLEX, 2003). We introduce the general strategies of solving an ILP problem here. Xmin X XcE(e) · x{E,e} + X cR(r) · x{R,r} E∈E e∈LE R∈R r∈LR X X X X d1(r, e1) · x{Rij,r,Ei,eJ} + d2(r, e2) · x{Rij,r,Ej,e2} &amp;quot;r∈Le subject to: X + Ei,Ej∈E Ei6=Ej X e∈LE X r∈LR Xx{E,e} = r∈LR Xx{R,r} = x{R,r,E,e} e∈LE x{E,e} ∈ {0, 1} x{R,r} ∈ {0, 1} x{R,r,E,e} ∈ {0, 1} rEG e EG R 2 E ∀E ∈ E (2) ∀R ∈ R (3) ∀E ∈ E and ∀R ∈ {R : E = N1(R) or R : E = N2(R)} (4) ∀R ∈ R and ∀E = N1(R) or E = N2(R) (5) ∀E ∈ E, e ∈ LE (6) ∀R ∈ R, r ∈ LR (7) ∀R ∈ R, r ∈ LR, E ∈ E, e ∈ LE (8) EG R 1 E x{E,e} = 1 x{R,r} = 1 x{R,r,E,e} Fi</context>
</contexts>
<marker>Xpress-MP, 2003</marker>
<rawString>Xpress-MP. 2003. Dash Optimization. Xpress-MP. http://www.dashoptimization.com/products.html.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>