<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.945166">
Safe In-vehicle Dialogue Using Learned Predictions of User Utterances
</title>
<author confidence="0.731405">
Staffan Larsson
</author>
<affiliation confidence="0.536186">
Talkamatic AB
</affiliation>
<address confidence="0.660859">
F¨orsta L˚anggatan 18
413 28 G¨oteborg
Sweden
</address>
<email confidence="0.980545">
staffan@talkamatic.se
</email>
<author confidence="0.593456">
Fredrik Kronlid
</author>
<affiliation confidence="0.438923">
Talkamatic AB
</affiliation>
<address confidence="0.619414">
F¨orsta L˚anggatan 18
413 28 G¨oteborg
Sweden
</address>
<email confidence="0.985832">
fredrik@talkamatic.se
</email>
<author confidence="0.892637">
Pontus W¨arnest˚al
</author>
<affiliation confidence="0.930394">
Halmstad University
</affiliation>
<address confidence="0.731497333333333">
Box 823
301 18 Halmstad
Sweden
</address>
<email confidence="0.989339">
pontus.warnestal@hh.se
</email>
<sectionHeader confidence="0.997284" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998374">
We present a multimodal in-vehicle dia-
logue system which uses learned predic-
tions of user answers to enable shorter,
more efficient, and thus safer natural lan-
guage dialogues.
</bodyText>
<sectionHeader confidence="0.998926" genericHeader="keywords">
1 Background
</sectionHeader>
<subsectionHeader confidence="0.996594">
1.1 Driver Distraction
</subsectionHeader>
<bodyText confidence="0.999865888888889">
Driver distraction is a common cause of accidents,
and is often caused by the driver interacting with
technologies such as mobile phones, media play-
ers or navigation systems. A study, commonly
referred to as the “100 car study” (Neale et al.,
2005) revealed that secondary task distraction is
the largest cause of driver inattention, and that the
handling of wireless devices is the most common
secondary task.
As interaction complexity in the car increases
due to more advanced infotainment systems and
smartphones, drivers are often executing several
tasks in parallel to the primary task of driving.
The increased functionality of these systems has
resulted in large hierarchical information architec-
tures that prolong interaction time, thereby nega-
tively affecting safety as well as user experience
(Kern and Schmidt, 2009).
</bodyText>
<subsectionHeader confidence="0.993802">
1.2 Relation to state of the art
</subsectionHeader>
<bodyText confidence="0.999991346153846">
State-of-the-art infotainment systems typically do
not include user models at all. Siri, available on
the Apple iPhone 4S and later models, has a static
user model containing personal information ex-
plicitly provided by the user (home address, etc.).
This information is used in voice interactions; for
example, given that the user has entered their fam-
ily relations, phrases like “Call my wife” can be
used. A different approach is taken in Google
Now, which dynamically learns user patterns from
observations and presents unrequested informa-
tion as “cards” on the screen. However, Google
Now does not attempt to integrate predictions into
dialogue interaction.
The work reported here explores the use of
adaptive user modeling in multimodal dialogue
systems. User preferences and behaviour patterns
are learnt from observations of user interactions
with the infotainment system and the context in
which these interactions take place, and are used
proactively to predict user answers and thereby en-
able shorter and more efficient interaction. The
underlying motivating assumption is that using
apps and services in an in-vehicle context inher-
ently leads to distraction, and that reducing inter-
action time will reduce driver distraction.
</bodyText>
<subsectionHeader confidence="0.971828">
1.3 TDM
</subsectionHeader>
<bodyText confidence="0.999883538461538">
Based on Larsson (2002) and later work, Talka-
matic AB has developed the Talkamatic Dialogue
Manager (TDM).
TDM provides a general interaction model
based on interaction which are basic to human-
human linguistic interaction, resulting in a high
degree of naturalness and flexibility which in-
creases usability. The model is domain-
independent which means that dialogue behaviour
can be altered without touching application prop-
erties and vice versa. TDM also offers integrated
multi-modality which allows user to freely switch
between modalities (Larsson et al., 2011).
</bodyText>
<subsectionHeader confidence="0.998329">
1.4 Grounding in TDM
</subsectionHeader>
<bodyText confidence="0.9986078">
Grounding (Clark and Brennan, 1990) is, roughly,
the process of making sure that dialogue partici-
pants agree on what has been said so far and what
it meant. TDM has an extensive model of ground-
ing (Larsson, 2002). It operates on different levels:
</bodyText>
<listItem confidence="0.999769">
• Perception
• Semantic Understanding
</listItem>
<page confidence="0.823232">
37
</page>
<note confidence="0.500825">
Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 37–40,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<listItem confidence="0.9996685">
• Pragmatic Understanding
• Acceptance
</listItem>
<bodyText confidence="0.503929666666667">
System feedback (positive, negative and in
some cases interrogative) can be generated on each
level:
</bodyText>
<listItem confidence="0.993215">
• Examples: “I didn’t hear” – negative percep-
tion
• “To work, is that right?” – interrogative se-
mantic understanding
• “OK” – positive acceptance.
2 Learning and Classification
</listItem>
<bodyText confidence="0.999684555555556">
Many dialogue applications require the user to an-
swer a number of questions. To make dialogue
shorter, we have extended TDM so that it tries to
predict user answers on the basis of a user model
learned from observations of user behaviour. As
an illustration, we use a road information appli-
cation which tries to predict the user’s destina-
tion and thereby eliminate the need to ask the user
about this.
</bodyText>
<subsectionHeader confidence="0.997527">
2.1 Learning Method
</subsectionHeader>
<bodyText confidence="0.999990818181818">
Initially, a range of learning methods requir-
ing (N-gram, MDP, POMDP) were explored and
evaluated, but the KNN (K-Nearest Neighbours)
(Mitchell, 1997) was considered the best method.
An important advantage is that KNN can learn
from a relatively small set of observations. This
is in contrast to the MDP and POMDP (and to
a lesser extent, N-gram) methods, which require
large amounts of data to generate useful behaviour.
A potential drawback of KNN is that this model
cannot model sequences of user behaviours.
</bodyText>
<subsectionHeader confidence="0.995709">
2.2 Parameter Selection
</subsectionHeader>
<bodyText confidence="0.999973714285714">
On the basis of user studies provided from the
user partner of the project, it was decided that the
most important user model parameters was posi-
tion, day of the week and hour of the day. The
training data were simulated and correspond to the
behaviour of an archetypal persona provided by
the user partner in the project.
</bodyText>
<subsectionHeader confidence="0.998989">
2.3 Learning and Classification
</subsectionHeader>
<bodyText confidence="0.9999918125">
The learning part of the system listens for a num-
ber of events, such as “start-car”, “stop-car” etc..
From these events and information about cur-
rent position, the time of the day and the day of
the week, the system creates new data instances.
The system thus learns how the user’s destination
varies depending on these parameters. A sample
dataset is shown in Figure 1, where data points
show destinations of trips initiated at various times
of the week.
When the dialogue manager requests a predic-
tion of the destination, the KNN algorithm tries to
find the K data points closest to the present data
point, and the top alternatives are returned to the
dialogue manager together with confidence scores
indicating the reliability of the predictions.
</bodyText>
<sectionHeader confidence="0.969948" genericHeader="introduction">
3 Integration of Classifications into TDM
</sectionHeader>
<subsectionHeader confidence="0.997488">
3.1 Grounding uncertain information
</subsectionHeader>
<bodyText confidence="0.9998946">
We treat the information emanating from the user
model as uncertain information about a (predicted)
user utterance. Hence, the same mechanisms used
for grounding utterances have been adapted for in-
tegrating user model data.
</bodyText>
<subsectionHeader confidence="0.998448">
3.2 Integrating Classifier Output
</subsectionHeader>
<bodyText confidence="0.9999568">
TDM is based on the Information State Update
(ISU) approach to dialogue management. The in-
formation state in TDM is based on that of the
system described in Larsson (2002) and includes
Questions Under Discussion, a dialogue plan, and
shared commitments.
The rule for integrating the user model data is
a standard ISU rule, consisting of preconditions
and effects on the information state. We describe
these informally below:
</bodyText>
<listItem confidence="0.915982285714286">
PRECONDITIONS
• If there is a propositional answer from the
user model resolving a question in the current
plan...
• and if the confidence score reported from the
user model is sufficient, then...
EFFECTS
• accept the propositional answer (include it
into the shared commitments), and...
• give appropriate feedback to the user depend-
ing on the confidence score:
• High confidence ⇒ embedded feedback
– “Which route do you want to take to
work?”.
</listItem>
<page confidence="0.998797">
38
</page>
<figureCaption confidence="0.969370666666667">
Figure 1: A sample dataset. The horizontal axis shows days of the week (0=Monday, ..., 6=Sunday)
and the vertical axis shows hour of the day. Data points show destinations of trips initiated at the time
indicated by their position. (“Now” is the current time, in this case Thursday at lunchtime.)
</figureCaption>
<bodyText confidence="0.781259">
– The user can always reject the prediction
by requesting another destination.
</bodyText>
<listItem confidence="0.975536545454545">
• Medium confidence ⇒ positive feedback
– “I assume you’re going to work”.
– If the user says “no”, the answer is re-
jected
– Silence is interpreted as acceptance.
• Low confidence ⇒ interrogative feedback
– “To work, is that correct?”
– In this case, the user needs to explicitly
accept the proposed answer.
– Otherwise, the user is prompted for an
answer.
</listItem>
<subsectionHeader confidence="0.999065">
3.3 GUI output
</subsectionHeader>
<bodyText confidence="0.999924428571428">
If the ISU rule above does not apply because of
too low confidence scores, user model informa-
tion is still used in the GUI. When a Wh-question
is raised by the system, the GUI always presents a
list of possible alternatives. High-confidence alter-
natives are highlighted and sorted before the other
alternatives in the list.
</bodyText>
<sectionHeader confidence="0.978702" genericHeader="method">
4 Resulting behaviour
</sectionHeader>
<bodyText confidence="0.8567611">
The demonstrator enables interaction with a learn-
ing dialogue system which uses predictions to sim-
plify interactions. Here is an sample interaction:
User: Traffic information
Car: Ok. What road?
User: E6.
Car: Showing traffic on the E6
If this is repeated on a number of occasions,
eventually the system will use a prediction:
User: Traffic information
Car: Showing traffic on the E6
The system thus reduces the need for repetitive
and information-scarce utterances from the user.
As soon as the system has started identifying a pat-
tern, it will start to suggest the most probable al-
ternatives. Initially, the most probable answers are
presented to the user as the top items in a list. The
alternatives are also marked in a different color to
make them more visible to the user (not shown
here).
</bodyText>
<reference confidence="0.6189442">
User: Traffic information
Car: Ok. What road?
Car GUI: [E6] [E45] [E20] [155]
User: E6.
Car: Showing traffic on the E6
</reference>
<page confidence="0.999545">
39
</page>
<bodyText confidence="0.992876">
After some further use, the system has identi-
fied a pattern which is prominent enough for the
system to make a suggestion:
</bodyText>
<table confidence="0.2431485">
User: Traffic information
Car: E6, is that right?
User: Yes.
Car: Showing traffic on the E6
</table>
<bodyText confidence="0.9932076">
After getting further support for its hypothesis,
the system will merely inform the user that an as-
sumption has been made. If the user is satisfied
with the assumption, she does not need to do any-
thing, but can correct or confirm it if desired.
</bodyText>
<reference confidence="0.944799375">
User: Traffic information
Car: I assume E6.
User: [silence]
Car: Showing traffic on the E6
User: Traffic information
Car: I assume E6.
User: No, E45.
Car: Showing traffic on the E45
</reference>
<bodyText confidence="0.8677256875">
If the user rejects the system suggestion with-
out giving another answer, the system will show
a menu where the most probable choices are the
topmost ones, and marked in a distinct colour (not
shown here).
User: Traffic information
Car: I assume E6.
User: No.
Car: What road?
Car GUI: [E6] [E45] [E20] [155]
When the system is certain about its hypothe-
sis, the system will simply provide the user with
the desired information without asking the user for
parameters.
User: Traffic information
Car: Showing traffic on the E6
</bodyText>
<sectionHeader confidence="0.996334" genericHeader="conclusions">
5 Conclusions and further work
</sectionHeader>
<bodyText confidence="0.999971130434783">
We have designed and implemented a mechanism
which learns user patterns and uses them proac-
tively to simplify and shorten dialogue interac-
tions. The idea of learning user patterns from ob-
servations is similar to Google Now. However,
while Google Now uses “cards” to provide un-
requested information to the user, we show how
predictions can be integrated into spoken or multi-
modal dialogue.
It remains for future work to evaluate the sys-
tem to establish that this actually reduces the dis-
traction rate of drivers. We also want to test the
performance of the learning mechanism by train-
ing it on real observations of user behaviours (as
opposed to simulated data).
The current mechanism only predicts answers
to individual system questions, which may result
in suboptimal behaviour in cases where there are
dependencies between the questions pertaining to
some task. An interesting area for future work is
to instead predict sequences of answers; however,
this would require a more powerful learning and
classification mechanisms.
</bodyText>
<sectionHeader confidence="0.999024" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99943875">
This work was carried out within the FFI project
“Safe Speech by Knowledge” (2012-00941),
funded by VINNOVA, Volvo Car Corporation and
Talkamatic.
</bodyText>
<sectionHeader confidence="0.99942" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999616958333333">
H. H. Clark and S. E. Brennan. 1990. Grounding
in communication. In L. B. Resnick, J. Levine,
and S. D. Behrend, editors, Perspectives on Socially
Shared Cognition, pages 127 – 149. APA.
Dagmar Kern and Albrecht Schmidt. 2009. Design
space for driver-based automotive user interfaces. In
Proceedings of the 1st International Conference on
Automotive User Interfaces and Interactive Vehic-
ular Applications, AutomotiveUI ’09, pages 3–10,
New York, NY, USA. ACM.
Staffan Larsson, Alexander Berman, and Jessica
Villing. 2011. Adding a speech cursor to a mul-
timodal dialogue system. In INTERSPEECH 2011,
12th Annual Conference of the International Speech
Communication Association, Florence, Italy, 2011,
pages 3319–3320.
Staffan Larsson. 2002. Issue-based Dialogue Manage-
ment. Ph.D. thesis, G¨oteborg University.
Tom M. Mitchell. 1997. Machine Learning. McGraw-
Hill, New York.
Vicki L. Neale, Thomas A. Dingus, Sheila G. Klauer,
Jeremy Sudweeks, and Michael Goodman. 2005.
An overview of the 100-car naturalistic study and
findings. Technical report.
</reference>
<page confidence="0.998639">
40
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.970059333333333">Safe In-vehicle Dialogue Using Learned Predictions of User Utterances Staffan Talkamatic</title>
<author confidence="0.899576">F¨orsta L˚anggatan</author>
<phone confidence="0.721276">413 28</phone>
<title confidence="0.52244325">Sweden staffan@talkamatic.se Fredrik Talkamatic</title>
<author confidence="0.87654">F¨orsta L˚anggatan</author>
<phone confidence="0.688807">413 28</phone>
<author confidence="0.324779">Sweden</author>
<email confidence="0.958723">fredrik@talkamatic.se</email>
<affiliation confidence="0.765988">Halmstad</affiliation>
<address confidence="0.514421666666667">Box 301 18 Sweden</address>
<email confidence="0.827233">pontus.warnestal@hh.se</email>
<abstract confidence="0.944983668246446">We present a multimodal in-vehicle dialogue system which uses learned predictions of user answers to enable shorter, more efficient, and thus safer natural language dialogues. 1 Background 1.1 Driver Distraction Driver distraction is a common cause of accidents, and is often caused by the driver interacting with technologies such as mobile phones, media players or navigation systems. A study, commonly referred to as the “100 car study” (Neale et al., 2005) revealed that secondary task distraction is the largest cause of driver inattention, and that the handling of wireless devices is the most common secondary task. As interaction complexity in the car increases due to more advanced infotainment systems and smartphones, drivers are often executing several tasks in parallel to the primary task of driving. The increased functionality of these systems has resulted in large hierarchical information architectures that prolong interaction time, thereby negatively affecting safety as well as user experience (Kern and Schmidt, 2009). 1.2 Relation to state of the art State-of-the-art infotainment systems typically do not include user models at all. Siri, available on the Apple iPhone 4S and later models, has a static user model containing personal information explicitly provided by the user (home address, etc.). This information is used in voice interactions; for example, given that the user has entered their family relations, phrases like “Call my wife” can be used. A different approach is taken in Google Now, which dynamically learns user patterns from observations and presents unrequested information as “cards” on the screen. However, Google Now does not attempt to integrate predictions into dialogue interaction. The work reported here explores the use of adaptive user modeling in multimodal dialogue systems. User preferences and behaviour patterns are learnt from observations of user interactions with the infotainment system and the context in which these interactions take place, and are used proactively to predict user answers and thereby enable shorter and more efficient interaction. The underlying motivating assumption is that using apps and services in an in-vehicle context inherently leads to distraction, and that reducing interaction time will reduce driver distraction. 1.3 TDM Based on Larsson (2002) and later work, Talkamatic AB has developed the Talkamatic Dialogue Manager (TDM). TDM provides a general interaction model based on interaction which are basic to humanhuman linguistic interaction, resulting in a high of naturalness and flexibility which inusability. The model is independent which means that dialogue behaviour can be altered without touching application properties and vice versa. TDM also offers integrated multi-modality which allows user to freely switch between modalities (Larsson et al., 2011). 1.4 Grounding in TDM Grounding (Clark and Brennan, 1990) is, roughly, the process of making sure that dialogue participants agree on what has been said so far and what it meant. TDM has an extensive model of grounding (Larsson, 2002). It operates on different levels: • Perception • Semantic Understanding 37 of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational pages 37–40, Sweden, April 26-30 2014. Association for Computational Linguistics • Pragmatic Understanding • Acceptance System feedback (positive, negative and in some cases interrogative) can be generated on each level: • Examples: “I didn’t hear” – negative perception • “To work, is that right?” – interrogative semantic understanding • “OK” – positive acceptance. 2 Learning and Classification Many dialogue applications require the user to answer a number of questions. To make dialogue shorter, we have extended TDM so that it tries to predict user answers on the basis of a user model learned from observations of user behaviour. As an illustration, we use a road information application which tries to predict the user’s destination and thereby eliminate the need to ask the user about this. 2.1 Learning Method Initially, a range of learning methods requiring (N-gram, MDP, POMDP) were explored and evaluated, but the KNN (K-Nearest Neighbours) (Mitchell, 1997) was considered the best method. An important advantage is that KNN can learn from a relatively small set of observations. This is in contrast to the MDP and POMDP (and to a lesser extent, N-gram) methods, which require large amounts of data to generate useful behaviour. A potential drawback of KNN is that this model cannot model sequences of user behaviours. 2.2 Parameter Selection On the basis of user studies provided from the user partner of the project, it was decided that the most important user model parameters was position, day of the week and hour of the day. The training data were simulated and correspond to the behaviour of an archetypal persona provided by the user partner in the project. 2.3 Learning and Classification The learning part of the system listens for a number of events, such as “start-car”, “stop-car” etc.. From these events and information about current position, the time of the day and the day of the week, the system creates new data instances. The system thus learns how the user’s destination varies depending on these parameters. A sample dataset is shown in Figure 1, where data points show destinations of trips initiated at various times of the week. When the dialogue manager requests a prediction of the destination, the KNN algorithm tries to find the K data points closest to the present data point, and the top alternatives are returned to the dialogue manager together with confidence scores indicating the reliability of the predictions. 3 Integration of Classifications into TDM 3.1 Grounding uncertain information We treat the information emanating from the user model as uncertain information about a (predicted) user utterance. Hence, the same mechanisms used for grounding utterances have been adapted for integrating user model data. 3.2 Integrating Classifier Output TDM is based on the Information State Update (ISU) approach to dialogue management. The information state in TDM is based on that of the system described in Larsson (2002) and includes Questions Under Discussion, a dialogue plan, and shared commitments. The rule for integrating the user model data is a standard ISU rule, consisting of preconditions and effects on the information state. We describe these informally below: • If there is a propositional answer from the user model resolving a question in the current plan... • and if the confidence score reported from the user model is sufficient, then... • accept the propositional answer (include it into the shared commitments), and... • give appropriate feedback to the user depending on the confidence score: High confidence feedback route do you want to take to work?”. 38 Figure 1: A sample dataset. The horizontal axis shows days of the week (0=Monday, ..., 6=Sunday) and the vertical axis shows hour of the day. Data points show destinations of trips initiated at the time indicated by their position. (“Now” is the current time, in this case Thursday at lunchtime.) user can always reject the prediction by requesting another destination. Medium confidence feedback assume you’re going to work”. the user says “no”, the answer is rejected is interpreted as acceptance. Low confidence feedback work, is that correct?” this case, the user needs to explicitly accept the proposed answer. the user is prompted for an answer. 3.3 GUI output If the ISU rule above does not apply because of too low confidence scores, user model information is still used in the GUI. When a Wh-question is raised by the system, the GUI always presents a list of possible alternatives. High-confidence alternatives are highlighted and sorted before the other alternatives in the list. 4 Resulting behaviour The demonstrator enables interaction with a learndialogue system which uses predictions to simplify interactions. Here is an sample interaction: User: Traffic information Car: Ok. What road? User: E6. Car: Showing traffic on the E6 If this is repeated on a number of occasions, eventually the system will use a prediction: User: Traffic information Car: Showing traffic on the E6 The system thus reduces the need for repetitive and information-scarce utterances from the user. As soon as the system has started identifying a pattern, it will start to suggest the most probable alternatives. Initially, the most probable answers are presented to the user as the top items in a list. The alternatives are also marked in a different color to make them more visible to the user (not shown here).</abstract>
<title confidence="0.480113">User: Traffic information</title>
<author confidence="0.400912">What road</author>
<note confidence="0.785812">Car GUI: [E6] [E45] [E20] [155] User: E6. Car: Showing traffic on the E6 39</note>
<abstract confidence="0.745675416666667">After some further use, the system has identified a pattern which is prominent enough for the system to make a suggestion: User: Traffic information Car: E6, is that right? User: Yes. Car: Showing traffic on the E6 After getting further support for its hypothesis, the system will merely inform the user that an assumption has been made. If the user is satisfied with the assumption, she does not need to do anything, but can correct or confirm it if desired.</abstract>
<keyword confidence="0.756701">User: Traffic information</keyword>
<note confidence="0.870791857142857">Car: I assume E6. User: [silence] Car: Showing traffic on the E6 User: Traffic information Car: I assume E6. User: No, E45. Car: Showing traffic on the E45</note>
<abstract confidence="0.9893074">If the user rejects the system suggestion without giving another answer, the system will show a menu where the most probable choices are the topmost ones, and marked in a distinct colour (not shown here).</abstract>
<keyword confidence="0.5183555">User: Traffic information Car: I assume E6. User: No. Car: What road?</keyword>
<abstract confidence="0.975213741935484">Car GUI: [E6] [E45] [E20] [155] When the system is certain about its hypothesis, the system will simply provide the user with the desired information without asking the user for parameters. User: Traffic information Car: Showing traffic on the E6 5 Conclusions and further work We have designed and implemented a mechanism which learns user patterns and uses them proactively to simplify and shorten dialogue interactions. The idea of learning user patterns from observations is similar to Google Now. However, while Google Now uses “cards” to provide unrequested information to the user, we show how predictions can be integrated into spoken or multimodal dialogue. It remains for future work to evaluate the system to establish that this actually reduces the distraction rate of drivers. We also want to test the performance of the learning mechanism by training it on real observations of user behaviours (as opposed to simulated data). The current mechanism only predicts answers to individual system questions, which may result in suboptimal behaviour in cases where there are dependencies between the questions pertaining to some task. An interesting area for future work is to instead predict sequences of answers; however, this would require a more powerful learning and classification mechanisms.</abstract>
<note confidence="0.9039134">Acknowledgements This work was carried out within the FFI project “Safe Speech by Knowledge” (2012-00941), funded by VINNOVA, Volvo Car Corporation and Talkamatic.</note>
<title confidence="0.752141">References</title>
<author confidence="0.950832">Grounding in communication In L B Resnick</author>
<author confidence="0.950832">J Levine</author>
<author confidence="0.950832">S D Behrend</author>
<author confidence="0.950832">on Socially editors</author>
<note confidence="0.850751363636364">pages 127 – 149. APA. Dagmar Kern and Albrecht Schmidt. 2009. Design space for driver-based automotive user interfaces. In Proceedings of the 1st International Conference on Automotive User Interfaces and Interactive Vehic- AutomotiveUI ’09, pages 3–10, New York, NY, USA. ACM. Staffan Larsson, Alexander Berman, and Jessica Villing. 2011. Adding a speech cursor to a muldialogue system. In 2011, 12th Annual Conference of the International Speech Association, Florence, Italy, pages 3319–3320. Larsson. 2002. Dialogue Manage- Ph.D. thesis, G¨oteborg University. M. Mitchell. 1997. McGraw- Hill, New York. Vicki L. Neale, Thomas A. Dingus, Sheila G. Klauer, Jeremy Sudweeks, and Michael Goodman. 2005. An overview of the 100-car naturalistic study and findings. Technical report. 40</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>User: Traffic information Car: Ok. What road? Car GUI: [E6] [E45] [E20] [155] User: E6. Car: Showing traffic on the E6 User: Traffic information Car: I assume</title>
<booktitle>E6. User: [silence] Car: Showing traffic on the E6 User: Traffic information Car: I assume E6. User: No, E45. Car: Showing traffic on the E45</booktitle>
<marker></marker>
<rawString>User: Traffic information Car: Ok. What road? Car GUI: [E6] [E45] [E20] [155] User: E6. Car: Showing traffic on the E6 User: Traffic information Car: I assume E6. User: [silence] Car: Showing traffic on the E6 User: Traffic information Car: I assume E6. User: No, E45. Car: Showing traffic on the E45</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H Clark</author>
<author>S E Brennan</author>
</authors>
<title>Grounding in communication.</title>
<date>1990</date>
<booktitle>Perspectives on Socially Shared Cognition,</booktitle>
<pages>127--149</pages>
<editor>In L. B. Resnick, J. Levine, and S. D. Behrend, editors,</editor>
<publisher>APA.</publisher>
<contexts>
<context position="3309" citStr="Clark and Brennan, 1990" startWordPosition="493" endWordPosition="496">M Based on Larsson (2002) and later work, Talkamatic AB has developed the Talkamatic Dialogue Manager (TDM). TDM provides a general interaction model based on interaction which are basic to humanhuman linguistic interaction, resulting in a high degree of naturalness and flexibility which increases usability. The model is domainindependent which means that dialogue behaviour can be altered without touching application properties and vice versa. TDM also offers integrated multi-modality which allows user to freely switch between modalities (Larsson et al., 2011). 1.4 Grounding in TDM Grounding (Clark and Brennan, 1990) is, roughly, the process of making sure that dialogue participants agree on what has been said so far and what it meant. TDM has an extensive model of grounding (Larsson, 2002). It operates on different levels: • Perception • Semantic Understanding 37 Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 37–40, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics • Pragmatic Understanding • Acceptance System feedback (positive, negative and in some cases interrogative) can be g</context>
</contexts>
<marker>Clark, Brennan, 1990</marker>
<rawString>H. H. Clark and S. E. Brennan. 1990. Grounding in communication. In L. B. Resnick, J. Levine, and S. D. Behrend, editors, Perspectives on Socially Shared Cognition, pages 127 – 149. APA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dagmar Kern</author>
<author>Albrecht Schmidt</author>
</authors>
<title>Design space for driver-based automotive user interfaces.</title>
<date>2009</date>
<booktitle>In Proceedings of the 1st International Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI ’09,</booktitle>
<pages>3--10</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1407" citStr="Kern and Schmidt, 2009" startWordPosition="202" endWordPosition="205"> as the “100 car study” (Neale et al., 2005) revealed that secondary task distraction is the largest cause of driver inattention, and that the handling of wireless devices is the most common secondary task. As interaction complexity in the car increases due to more advanced infotainment systems and smartphones, drivers are often executing several tasks in parallel to the primary task of driving. The increased functionality of these systems has resulted in large hierarchical information architectures that prolong interaction time, thereby negatively affecting safety as well as user experience (Kern and Schmidt, 2009). 1.2 Relation to state of the art State-of-the-art infotainment systems typically do not include user models at all. Siri, available on the Apple iPhone 4S and later models, has a static user model containing personal information explicitly provided by the user (home address, etc.). This information is used in voice interactions; for example, given that the user has entered their family relations, phrases like “Call my wife” can be used. A different approach is taken in Google Now, which dynamically learns user patterns from observations and presents unrequested information as “cards” on the </context>
</contexts>
<marker>Kern, Schmidt, 2009</marker>
<rawString>Dagmar Kern and Albrecht Schmidt. 2009. Design space for driver-based automotive user interfaces. In Proceedings of the 1st International Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI ’09, pages 3–10, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Staffan Larsson</author>
<author>Alexander Berman</author>
<author>Jessica Villing</author>
</authors>
<title>Adding a speech cursor to a multimodal dialogue system.</title>
<date>2011</date>
<booktitle>In INTERSPEECH 2011, 12th Annual Conference of the International Speech Communication Association,</booktitle>
<pages>3319--3320</pages>
<location>Florence, Italy,</location>
<contexts>
<context position="3251" citStr="Larsson et al., 2011" startWordPosition="484" endWordPosition="487">interaction time will reduce driver distraction. 1.3 TDM Based on Larsson (2002) and later work, Talkamatic AB has developed the Talkamatic Dialogue Manager (TDM). TDM provides a general interaction model based on interaction which are basic to humanhuman linguistic interaction, resulting in a high degree of naturalness and flexibility which increases usability. The model is domainindependent which means that dialogue behaviour can be altered without touching application properties and vice versa. TDM also offers integrated multi-modality which allows user to freely switch between modalities (Larsson et al., 2011). 1.4 Grounding in TDM Grounding (Clark and Brennan, 1990) is, roughly, the process of making sure that dialogue participants agree on what has been said so far and what it meant. TDM has an extensive model of grounding (Larsson, 2002). It operates on different levels: • Perception • Semantic Understanding 37 Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 37–40, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics • Pragmatic Understanding • Acceptance System feedback (po</context>
</contexts>
<marker>Larsson, Berman, Villing, 2011</marker>
<rawString>Staffan Larsson, Alexander Berman, and Jessica Villing. 2011. Adding a speech cursor to a multimodal dialogue system. In INTERSPEECH 2011, 12th Annual Conference of the International Speech Communication Association, Florence, Italy, 2011, pages 3319–3320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Staffan Larsson</author>
</authors>
<title>Issue-based Dialogue Management.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>G¨oteborg University.</institution>
<contexts>
<context position="2710" citStr="Larsson (2002)" startWordPosition="405" endWordPosition="406">action. The work reported here explores the use of adaptive user modeling in multimodal dialogue systems. User preferences and behaviour patterns are learnt from observations of user interactions with the infotainment system and the context in which these interactions take place, and are used proactively to predict user answers and thereby enable shorter and more efficient interaction. The underlying motivating assumption is that using apps and services in an in-vehicle context inherently leads to distraction, and that reducing interaction time will reduce driver distraction. 1.3 TDM Based on Larsson (2002) and later work, Talkamatic AB has developed the Talkamatic Dialogue Manager (TDM). TDM provides a general interaction model based on interaction which are basic to humanhuman linguistic interaction, resulting in a high degree of naturalness and flexibility which increases usability. The model is domainindependent which means that dialogue behaviour can be altered without touching application properties and vice versa. TDM also offers integrated multi-modality which allows user to freely switch between modalities (Larsson et al., 2011). 1.4 Grounding in TDM Grounding (Clark and Brennan, 1990) </context>
<context position="6680" citStr="Larsson (2002)" startWordPosition="1044" endWordPosition="1045">ned to the dialogue manager together with confidence scores indicating the reliability of the predictions. 3 Integration of Classifications into TDM 3.1 Grounding uncertain information We treat the information emanating from the user model as uncertain information about a (predicted) user utterance. Hence, the same mechanisms used for grounding utterances have been adapted for integrating user model data. 3.2 Integrating Classifier Output TDM is based on the Information State Update (ISU) approach to dialogue management. The information state in TDM is based on that of the system described in Larsson (2002) and includes Questions Under Discussion, a dialogue plan, and shared commitments. The rule for integrating the user model data is a standard ISU rule, consisting of preconditions and effects on the information state. We describe these informally below: PRECONDITIONS • If there is a propositional answer from the user model resolving a question in the current plan... • and if the confidence score reported from the user model is sufficient, then... EFFECTS • accept the propositional answer (include it into the shared commitments), and... • give appropriate feedback to the user depending on the c</context>
</contexts>
<marker>Larsson, 2002</marker>
<rawString>Staffan Larsson. 2002. Issue-based Dialogue Management. Ph.D. thesis, G¨oteborg University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom M Mitchell</author>
</authors>
<date>1997</date>
<booktitle>Machine Learning.</booktitle>
<publisher>McGrawHill,</publisher>
<location>New York.</location>
<contexts>
<context position="4682" citStr="Mitchell, 1997" startWordPosition="714" endWordPosition="715"> acceptance. 2 Learning and Classification Many dialogue applications require the user to answer a number of questions. To make dialogue shorter, we have extended TDM so that it tries to predict user answers on the basis of a user model learned from observations of user behaviour. As an illustration, we use a road information application which tries to predict the user’s destination and thereby eliminate the need to ask the user about this. 2.1 Learning Method Initially, a range of learning methods requiring (N-gram, MDP, POMDP) were explored and evaluated, but the KNN (K-Nearest Neighbours) (Mitchell, 1997) was considered the best method. An important advantage is that KNN can learn from a relatively small set of observations. This is in contrast to the MDP and POMDP (and to a lesser extent, N-gram) methods, which require large amounts of data to generate useful behaviour. A potential drawback of KNN is that this model cannot model sequences of user behaviours. 2.2 Parameter Selection On the basis of user studies provided from the user partner of the project, it was decided that the most important user model parameters was position, day of the week and hour of the day. The training data were sim</context>
</contexts>
<marker>Mitchell, 1997</marker>
<rawString>Tom M. Mitchell. 1997. Machine Learning. McGrawHill, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vicki L Neale</author>
<author>Thomas A Dingus</author>
<author>Sheila G Klauer</author>
<author>Jeremy Sudweeks</author>
<author>Michael Goodman</author>
</authors>
<title>An overview of the 100-car naturalistic study and findings.</title>
<date>2005</date>
<tech>Technical report.</tech>
<contexts>
<context position="828" citStr="Neale et al., 2005" startWordPosition="116" endWordPosition="119">nggatan 18 413 28 G¨oteborg Sweden fredrik@talkamatic.se Pontus W¨arnest˚al Halmstad University Box 823 301 18 Halmstad Sweden pontus.warnestal@hh.se Abstract We present a multimodal in-vehicle dialogue system which uses learned predictions of user answers to enable shorter, more efficient, and thus safer natural language dialogues. 1 Background 1.1 Driver Distraction Driver distraction is a common cause of accidents, and is often caused by the driver interacting with technologies such as mobile phones, media players or navigation systems. A study, commonly referred to as the “100 car study” (Neale et al., 2005) revealed that secondary task distraction is the largest cause of driver inattention, and that the handling of wireless devices is the most common secondary task. As interaction complexity in the car increases due to more advanced infotainment systems and smartphones, drivers are often executing several tasks in parallel to the primary task of driving. The increased functionality of these systems has resulted in large hierarchical information architectures that prolong interaction time, thereby negatively affecting safety as well as user experience (Kern and Schmidt, 2009). 1.2 Relation to sta</context>
</contexts>
<marker>Neale, Dingus, Klauer, Sudweeks, Goodman, 2005</marker>
<rawString>Vicki L. Neale, Thomas A. Dingus, Sheila G. Klauer, Jeremy Sudweeks, and Michael Goodman. 2005. An overview of the 100-car naturalistic study and findings. Technical report.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>