<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000269">
<title confidence="0.977707">
Knowledge Base Population: Successful Approaches and Challenges
</title>
<author confidence="0.999307">
Heng Ji Ralph Grishman
</author>
<affiliation confidence="0.999118333333333">
Computer Science Department Computer Science Department
Queens College and Graduate Center New York University
City University of New York
</affiliation>
<address confidence="0.986045">
New York, NY 11367, USA New York, NY 10003, USA
</address>
<email confidence="0.999608">
hengji@cs.qc.cuny.edu grishman@cs.nyu.edu
</email>
<sectionHeader confidence="0.995888" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995688761904762">
In this paper we give an overview of the
Knowledge Base Population (KBP) track at
the 2010 Text Analysis Conference. The main
goal of KBP is to promote research in discov-
ering facts about entities and augmenting a
knowledge base (KB) with these facts. This is
done through two tasks, Entity Linking – link-
ing names in context to entities in the KB –
and Slot Filling – adding information about an
entity to the KB. A large source collection of
newswire and web documents is provided
from which systems are to discover informa-
tion. Attributes (“slots”) derived from
Wikipedia infoboxes are used to create the
reference KB. In this paper we provide an
overview of the techniques which can serve as
a basis for a good KBP system, lay out the
remaining challenges by comparison with tra-
ditional Information Extraction (IE) and Ques-
tion Answering (QA) tasks, and provide some
suggestions to address these challenges.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995070212766">
Traditional information extraction (IE) evaluations,
such as the Message Understanding Conferences
(MUC) and Automatic Content Extraction (ACE),
assess the ability to extract information from indi-
vidual documents in isolation. In practice, how-
ever, we may need to gather information about a
person or organization that is scattered among the
documents of a large collection. This requires the
ability to identify the relevant documents and to
integrate facts, possibly redundant, possibly com-
plementary, possibly in conflict, coming from
these documents. Furthermore, we may want to use
the extracted information to augment an existing
data base. This requires the ability to link indi-
viduals mentioned in a document, and information
about these individuals, to entries in the data base.
On the other hand, traditional Question Answering
(QA) evaluations made limited efforts at disam-
biguating entities in queries (e.g. Pizzato et al.,
2006), and limited use of relation/event extraction
in answer search (e.g. McNamee et al., 2008).
The Knowledge Base Population (KBP) shared
task, conducted as part of the NIST Text Analysis
Conference, aims to address and evaluate these
capabilities, and bridge the IE and QA communi-
ties to promote research in discovering facts about
entities and expanding a knowledge base with
these facts. KBP is done through two separate sub-
tasks, Entity Linking and Slot Filling; in 2010, 23
teams submitted results for one or both sub-tasks.
A variety of approaches have been proposed to
address both tasks with considerable success; nev-
ertheless, there are many aspects of the task that
remain unclear. What are the fundamental tech-
niques used to achieve reasonable performance?
What is the impact of each novel method? What
types of problems are represented in the current
KBP paradigm compared to traditional IE and QA?
In which way have the current testbeds and evalua-
tion methodology affected our perception of the
task difficulty? Have we reached a performance
ceiling with current state of the art techniques?
What are the remaining challenges and what are
the possible ways to address these challenges? In
this paper we aim to answer some of these ques-
tions based on our detailed analysis of evaluation
results.
</bodyText>
<page confidence="0.941985">
1148
</page>
<note confidence="0.813722333333333">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1148–1158,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
2 Task Definition and Evaluation Metrics
</note>
<bodyText confidence="0.999636853333334">
This section will summarize the tasks conducted at
KBP 2010. The overall goal of KBP is to auto-
matically identify salient and novel entities, link
them to corresponding Knowledge Base (KB) en-
tries (if the linkage exists), then discover attributes
about the entities, and finally expand the KB with
any new attributes.
In the Entity Linking task, given a person (PER),
organization (ORG) or geo-political entity (GPE, a
location with a government) query that consists of
a name string and a background document contain-
ing that name string, the system is required to pro-
vide the ID of the KB entry to which the name
refers; or NIL if there is no such KB entry. The
background document, drawn from the KBP cor-
pus, serves to disambiguate ambiguous name
strings.
In selecting among the KB entries, a system
could make use of the Wikipedia text associated
with each entry as well as the structured fields of
each entry. In addition, there was an optional task
where the system could only make use of the struc-
tured fields; this was intended to be representative
of applications where no backing text was avail-
able. Each site could submit up to three runs with
different parameters.
The goal of Slot Filling is to collect from the cor-
pus information regarding certain attributes of an
entity, which may be a person or some type of or-
ganization. Each query in the Slot Filling task con-
sists of the name of the entity, its type (person or
organization), a background document containing
the name (again, to disambiguate the query in case
there are multiple entities with the same name), its
node ID (if the entity appears in the knowledge
base), and the attributes which need not be filled.
Attributes are excluded if they are already filled in
the reference data base and can only take on a sin-
gle value. Along with each slot fill, the system
must provide the ID of a document which supports
the correctness of this fill. If the corpus does not
provide any information for a given attribute, the
system should generate a NIL response (and no
document ID). KBP2010 defined 26 types of at-
tributes for persons (such as the age, birthplace,
spouse, children, job title, and employing organiza-
tion) and 16 types of attributes for organizations
(such as the top employees, the founder, the year
founded, the headquarters location, and subsidiar-
ies). Some of these attributes are specified as only
taking a single value (e.g., birthplace), while some
can take multiple values (e.g., top employees).
The reference KB includes hundreds of thousands
of entities based on articles from an October 2008
dump of English Wikipedia which includes
818,741 nodes. The source collection includes
1,286,609 newswire documents, 490,596 web
documents and hundreds of transcribed spoken
documents.
To score Entity Linking, we take each query and
check whether the KB node ID (or NIL) returned
by a system is correct or not. Then we compute
the Micro-averaged Accuracy, computed across all
queries.
To score Slot Filling, we first pool all the system
responses (as is done for information retrieval
evaluations) together with a set of manually-
prepared slot fills. These responses are then as-
sessed by hand. Equivalent answers (such as “Bill
Clinton” and “William Jefferson Clinton”) are
grouped into equivalence classes. Each system
response is rated as correct, wrong, or redundant (a
response which is equivalent to another response
for the same slot or an entry already in the knowl-
edge base). Given these judgments, we count
</bodyText>
<construct confidence="0.74556">
Correct = total number of non-NIL system output
slots judged correct
System = total number of non-NIL system output
slots
Reference = number of single-valued slots with a
correct non-NIL response +
number of equivalence classes for all list-
valued slots
</construct>
<equation confidence="0.9953585">
Recall = Correct / Reference
Precision = Correct / System
F-Measure = (2 × Recall × Precision) / (Recall +
Precision)
</equation>
<sectionHeader confidence="0.991879" genericHeader="method">
3 Entity Linking: What Works
</sectionHeader>
<bodyText confidence="0.999529142857143">
In Entity Linking, we saw a general improvement
in performance over last year’s results – the top
system achieved 85.78% micro-averaged accuracy.
When measured against a benchmark based on in-
ter-annotator agreement, two systems’ perform-
ance approached and one system exceeded the
benchmark on person entities.
</bodyText>
<subsectionHeader confidence="0.998744">
3.1 A General Architecture
</subsectionHeader>
<bodyText confidence="0.983495">
A typical entity linking system architecture is de-
picted in Figure 1.
</bodyText>
<page confidence="0.998251">
1149
</page>
<figureCaption confidence="0.8447385">
Figure 1. General Entity Linking
System Architecture
</figureCaption>
<bodyText confidence="0.999979363636364">
It includes three steps: (1) query expansion – ex-
pand the query into a richer set of forms using
Wikipedia structure mining or coreference resolu-
tion in the background document. (2) candidate
generation – finding all possible KB entries that a
query might link to; (3) candidate ranking – rank
the probabilities of all candidates and NIL answer.
Table 1 summarizes the systems which ex-
ploited different approaches at each step. In the
following subsections we will highlight the new
and effective techniques used in entity linking.
</bodyText>
<subsectionHeader confidence="0.999889">
3.2 Wilipedia Structure Mining
</subsectionHeader>
<bodyText confidence="0.999309571428571">
Wikipedia articles are peppered with structured
information and hyperlinks to other (on average
25) articles (Medelyan et al., 2009). Such informa-
tion provides additional sources for entity linking:
(1). Query Expansion: For example, WebTLab
(Fernandez et al., 2010) used Wikipedia link struc-
ture (source, anchors, redirects and disambigua-
tion) to extend the KB and compute entity co-
occurrence estimates. Many other teams including
CUNY and Siel used redirect pages and disam-
biguation pages for query expansion. The Siel team
also exploited bold texts from first paragraphs be-
cause they often contain nicknames, alias names
and full names.
</bodyText>
<figure confidence="0.987120125">
KB Node Candidate Generation
unsupervised
similarity
computation
Document semantic analysis
Wiki
hyperlink
mining
Query Expansion
KB Node Candidate Ranking
Query
IR
Source doc
Coreference
Resolution
Answer
supervised
classifica-
tion
Graph
-based
Wiki KB
+Texts
IR
</figure>
<table confidence="0.993329952380952">
Methods System Examples System
Ranking
Range
Query Wikipedia Hyperlink Mining CUNY (Chen et al., 2010), NUSchime (Zhang et al., [2, 15]
Expansion 2010), Siel (Bysani et al., 2010), SMU-SIS (Gottipati et
al., 2010), USFD (Yu et al., 2010), WebTLab team (Fer-
nandez et al., 2010)
Source document coreference CUNY (Chen et al., 2010) 9
resolution
Candidate Document semantic analysis ARPANI (Thomas et al., 2010), CUNY (Chen et al., [1,14]
Generation and context modeling 2010), LCC (Lehmann et al., 2010)
IR CUNY (Chen et al., 2010), Budapestacad (Nemeskey et [9, 16]
al., 2010), USFD (Yu et al., 2010)
Candidate Unsupervised Similarity CUNY (Chen et al., 2010), SMU-SIS (Gottipati et al., [9, 14]
Ranking Computation (e.g. VSM) 2010), USFD (Yu et al., 2010)
Supervised LCC (Lehmann et al., 2010), NUSchime (Zhang et al., [1, 10]
Classification 2010), Stanford-UBC (Chang et al., 2010), HLTCOE
(McNamee, 2010), UC3M (Pablo-Sanchez et al., 2010)
Rule-based LCC (Lehmann et al., 2010), BuptPris (Gao et al., 2010) [1, 8]
Global Graph-based Ranking CMCRC (Radford et al., 2010) 3
IR Budapestacad (Nemeskey et al., 2010) 16
</table>
<tableCaption confidence="0.999758">
Table 1. Entity Linking Method Comparison
</tableCaption>
<page confidence="0.9838">
1150
</page>
<bodyText confidence="0.998271923076923">
(2). Candidate Ranking: Stanford-UBC used
Wikipedia hyperlinks (clarification, disambigua-
tion, title) for query re-mapping, and encoded lexi-
cal and part-of-speech features from Wikipedia
articles containing hyperlinks to the queries to train
a supervised classifier; they reported a significant
improvement on micro-averaged accuracy, from
74.85% to 82.15%. In fact, when the mined attrib-
utes become rich enough, they can be used as an
expanded query and sent into an information re-
trieval engine in order to obtain the relevant source
documents. Budapestacad team (Nemeskey et al.,
2010) adopted this strategy.
</bodyText>
<subsectionHeader confidence="0.999761">
3.3 Ranking Approach Comparison
</subsectionHeader>
<bodyText confidence="0.999738333333333">
The ranking approaches exploited in the KBP2010
entity linking systems can be generally categorized
into four types:
</bodyText>
<listItem confidence="0.743783076923077">
(1). Unsupervised or weakly-supervised learning,
in which annotated data is minimally used to tune
thresholds and parameters. The similarity measure
is largely based on the unlabeled contexts.
(2). Supervised learning, in which a pair of entity
and KB node is modeled as an instance for classi-
fication. Such a classifier can be learned from the
annotated training data based on many different
features.
(3). Graph-based ranking, in which context entities
are taken into account in order to reach a global
optimized solution together with the query entity.
(4). IR (Information Retrieval) approach, in which
</listItem>
<bodyText confidence="0.95691745">
the entire background source document is consid-
ered as a single query to retrieve the most relevant
Wikipedia article.
The first question we will investigate is how
much higher performance can be achieved by us-
ing supervised learning? Among the 16 entity link-
ing systems which participated in the regular
evaluation, LCC (Lehmann et al., 2010), HLTCOE
(McNamee, 2010), Stanford-UBC (Chang et al.,
2010), NUSchime (Zhang et al., 2010) and UC3M
(Pablo-Sanchez et al., 2010) have explicitly used
supervised classification based on many lexical
and name tagging features, and most of them are
ranked in top 6 in the evaluation. Therefore we can
conclude that supervised learning normally leads to
a reasonably good performance. However, a high-
performing entity linking system can also be im-
plemented in an unsupervised fashion by exploit-
ing effective characteristics and algorithms, as we
will discuss in the next sections.
</bodyText>
<subsectionHeader confidence="0.99184">
3.4 Semantic Relation Features
</subsectionHeader>
<bodyText confidence="0.999149733333333">
Almost all entity linking systems have used seman-
tic relations as features (e.g. BuptPris (Gao et al.,
2010), CUNY (Chen et al., 2010) and HLTCOE).
The semantic features used in the BuptPris system
include name tagging, infoboxes, synonyms, vari-
ants and abbreviations. In the CUNY system, the
semantic features are automatically extracted from
their slot filling system. The results are summa-
rized in Table 2, showing the gains over a baseline
system (using only Wikipedia title features in the
case of BuptPris, using tf-idf weighted word fea-
tures for CUNY). As we can see, except for person
entities in the BuptPris system, all types of entities
have obtained significant improvement by using
semantic features in entity linking.
</bodyText>
<table confidence="0.992145">
System Using Se- PER ORG GPE Overall
mantic
Features
BuptPris No 83.89 59.47 33.38 58.93
Yes 79.09 74.13 66.62 73.29
CUNY No 84.55 63.07 57.54 59.91
Yes 92.81 65.73 84.10 69.29
</table>
<tableCaption confidence="0.9598665">
Table 2. Impact of Semantic Features on Entity
Linking (Micro-Averaged Accuracy %)
</tableCaption>
<subsectionHeader confidence="0.965358">
3.5 Context Inference
</subsectionHeader>
<bodyText confidence="0.999694142857143">
In the current setting of KBP, a set of target enti-
ties is provided to each system in order to simplify
the task and its evaluation, because it’s not feasible
to require a system to generate answers for all pos-
sible entities in the entire source collection. How-
ever, ideally a fully-automatic KBP system should
be able to automatically discover novel entities
(“queries”) which have no KB entry or few slot
fills in the KB, extract their attributes, and conduct
global reasoning over these attributes in order to
generate the final output. At the very least, due to
the semantic coherence principle (McNamara,
2001), the information of an entity depends on the
information of other entities. For example, the
WebTLab team and the CMCRC team extracted all
entities in the context of a given query, and disam-
biguated all entities at the same time using a Pag-
eRank-like algorithm (Page et al., 1998) or a
Graph-based Re-ranking algorithm. The SMU-SIS
team (Gottipati and Jiang, 2010) re-formulated
queries using contexts. The LCC team modeled
</bodyText>
<page confidence="0.966461">
1151
</page>
<bodyText confidence="0.9999385">
contexts using Wikipedia page concepts, and com-
puted linkability scores iteratively. Consistent im-
provements were reported by the WebTLab system
(from 63.64% to 66.58%).
</bodyText>
<sectionHeader confidence="0.997558" genericHeader="method">
4 Entity Linking: Remaining Challenges
</sectionHeader>
<subsectionHeader confidence="0.9985835">
4.1 Comparison with Traditional Cross-
document Coreference Resolution
</subsectionHeader>
<bodyText confidence="0.999984285714286">
Part of the entity linking task can be modeled as a
cross-document entity resolution problem which
includes two principal challenges: the same entity
can be referred to by more than one name string
and the same name string can refer to more than
one entity. The research on cross-document entity
coreference resolution can be traced back to the
Web People Search task (Artiles et al., 2007) and
ACE2008 (e.g. Baron and Freedman, 2008).
Compared to WePS and ACE, KBP requires link-
ing an entity mention in a source document to a
knowledge base with or without Wikipedia arti-
cles. Therefore sometimes the linking decisions
heavily rely on entity profile comparison with
Wikipedia infoboxes. In addition, KBP introduced
GPE entity disambiguation. In source documents,
especially in web data, usually few explicit attrib-
utes about GPE entities are provided, so an entity
linking system also needs to conduct external
knowledge discovery from background related
documents or hyperlink mining.
</bodyText>
<subsectionHeader confidence="0.999293">
4.2 Analysis of Difficult Queries
</subsectionHeader>
<bodyText confidence="0.999747266666667">
There are 2250 queries in the Entity Linking
evaluation; for 58 of them at most 5 (out of the 46)
system runs produced correct answers. Most of
these queries have corresponding KB entries. For
19 queries all 46 systems produced different results
from the answer key. Interestingly, the systems
which perform well on the difficult queries are not
necessarily those achieved top overall performance
– they were ranked 13rd, 6th, 5th, 12nd, 10th, and 16th
respectively for overall queries. 11 queries are
highly ambiguous city names which can exist in
many states or countries (e.g. “Chester”), or refer
to person or organization entities. From these most
difficult queries we observed the following chal-
lenges and possible solutions.
</bodyText>
<listItem confidence="0.968342">
• Require deep understanding of context enti-
ties for GPE queries
</listItem>
<bodyText confidence="0.984985423076923">
In a document where the query entity is not a cen-
tral topic, the author often assumes that the readers
have enough background knowledge (‘anchor’ lo-
cation from the news release information, world
knowledge or related documents) about these enti-
ties. For 6 queries, a system would need to inter-
pret or extract attributes for their context entities.
For example, in the following passage:
...There are also photos of Jake on IHJ in
Brentwood, still looking somber...
in order to identify that the query “Brentwood” is
located in California, a system will need to under-
stand that “IHJ” is “I heart Jake community” and
that the “Jake” referred to lives in Los Angeles, of
which Brentwood is a part.
In the following example, a system is required to
capture the knowledge that “Chinese Christian
man” normally appears in “China” or there is a
“Mission School” in “Canton, China” in order to
link the query “Canton” to the correct KB entry.
This is a very difficult query also because the more
common way of spelling “Canton” in China is
“Guangdong”.
...and was from a Mission School in Canton, ...
but for the energetic efforts of this Chinese Chris-
tian man and the Refuge Matron...
</bodyText>
<listItem confidence="0.936941">
• Require external hyperlink analysis
</listItem>
<bodyText confidence="0.99869">
Some queries require a system to conduct detailed
analysis on the hyperlinks in the source document
or the Wikipedia document. For example, in the
source document “...Filed under: Falcons
&lt;http://sports.aol.com/fanhouse/category/atlanta-
falcons/&gt;”, a system will need to analyze the
document which this hyperlink refers to. Such
cases might require new query reformulation and
cross-document aggregation techniques, which are
both beyond traditional entity disambiguation
paradigms.
</bodyText>
<page confidence="0.972146">
1152
</page>
<listItem confidence="0.856083">
• Require Entity Salience Ranking
</listItem>
<bodyText confidence="0.9994016875">
Some of these queries represent salient entities and
so using web popularity rank (e.g. ranking/hit
counts of Wikipedia pages from search engine) can
yield correct answers in most cases (Bysani et al.,
2010; Dredze et al., 2010). In fact we found that a
naïve candidate ranking approach based on web
popularity alone can achieve 71% micro-averaged
accuracy, which is better than 24 system runs in
KBP2010.
Since the web information is used as a black box
(including query expansion and query log analysis)
which changes over time, it’s more difficult to du-
plicate research results. However, gazetteers with
entities ranked by salience or major entities
marked are worth encoding as additional features.
For example, in the following passages:
</bodyText>
<construct confidence="0.854175333333333">
... Tritschler brothers competed in gymnastics at the
1904 Games in St Louis 104 years ago” and “A char-
tered airliner carrying Democratic White House hope-
ful Barack Obama was forced to make an unscheduled
landing on Monday in St. Louis after its flight crew
detected mechanical problems...
</construct>
<bodyText confidence="0.999642625">
although there is little background information to
decide where the query “St Louis” is located, a sys-
tem can rely on such a major city list to generate
the correct linking. Similarly, if a system knows
that “Georgia Institute of Technology” has higher
salience than “Georgian Technical University”, it
can correctly link a query “Georgia Tech” in most
cases.
</bodyText>
<sectionHeader confidence="0.982433" genericHeader="method">
5 Slot Filling: What Works
</sectionHeader>
<subsectionHeader confidence="0.981134">
5.1 A General Architecture
</subsectionHeader>
<bodyText confidence="0.9999635">
The slot-filling task is a hybrid of traditional IE (a
fixed set of relations) and QA (responding to a
query, generating a unified response from a large
collection). Most participants met this challenge
through a hybrid system which combined aspects
of QA (passage retrieval) and IE (answer extrac-
tion). A few used off-the-shelf QA, either bypass-
ing question analysis or (if QA was used as a
“black box”) creating a set of questions corre-
sponding to each slot.
The basic system structure (Figure 2) involved
three phases: document/passage retrieval (retriev-
ing passages involving the queried entity), answer
extraction (getting specific answers from the re-
trieved passages), and answer combination (merg-
ing and selecting among the answers extracted).
The solutions adopted for answer extraction re-
flected the range of current IE methods as well as
QA answer extraction techniques (see Table 3).
Most systems used one main pipeline, while
CUNY and BuptPris adopted a hybrid approach of
combining multiple approaches.
One particular challenge for KBP, in compari-
son with earlier IE tasks, was the paucity of train-
ing data. The official training data, linked to
specific text from specific documents, consisted of
responses to 100 queries; the participants jointly
prepared responses to another 50. So traditional
supervised learning, based directly on the training
data, would provide limited coverage. Coverage
could be improved by using the training data as
seeds for a bootstrapping procedure.
</bodyText>
<figureCaption confidence="0.998528">
Figure 2. General Slot Filling System Architecture
</figureCaption>
<figure confidence="0.999555965517241">
Query
Source
Collection
Training
Data/
External
KB
IR, QA
IR
Sentence/Passage
Level
Document
Level
IE
(Distant Learning/
Bootstrapping)
QA
Answer
Level
Pattern
Classifier
Rules
Knowledge
Base
Redundancy
Removal
Answers
Query
Expansion
</figure>
<page confidence="0.945301">
1153
</page>
<table confidence="0.841518">
Methods System Examples
Trained Pattern Distant Learning (large CUNY (Chen et al., 2010)
IE Learning seed, one iteration)
Bootstrapping (small NYU (Grishman and Min, 2010)
seed, multiple iterations)
Supervised Distant Supervision Budapestacad (Nemeskey et al., 2010), lsv (Chrupala et al.,
Classifier 2010), Stanford (Surdeanu et al., 2010), UBC (Intxaurrondo
et al., 2010)
Trained from KBP train- BuptPris (Gao et al., 2010), CUNY (Chen et al., 2010), IBM
ing data and other re- (Castelli et al., 2010), ICL (Song et al., 2010), LCC
lated tasks (Lehmann et al., 2010), lsv (Chrupala et al., 2010), Siel
(Bysani et al., 2010)
QA CUNY (Chen et al., 2010), iirg (Byrne and Dunnion, 2010)
Hand-coded Heuristic Rules BuptPris (Gao et al., 2010), USFD (Yu et al., 2010)
</table>
<tableCaption confidence="0.997311">
Table 3. Slot Filling Answer Extraction Method Comparison
</tableCaption>
<bodyText confidence="0.999943041666667">
On the other hand, there were a lot of &apos;facts&apos; avail-
able – pairs of entities bearing a relationship corre-
sponding closely to the KBP relations – in the form
of filled Wikipedia infoboxes. These could be
used for various forms of indirect or distant learn-
ing, where instances in a large corpus of such pairs
are taken as (positive) training instances. How-
ever, such instances are noisy – if a pair of entities
participates in more than one relation, the found
instance may not be an example of the intended
relation – and so some filtering of the instances or
resulting patterns may be needed. Several sites
used such distant supervision to acquire patterns or
train classifiers, in some cases combined with di-
rect supervision using the training data (Chrupala
et al., 2010).
Several groups used and extended existing rela-
tion extraction systems, and then mapped the re-
sults into KBP slots. Mapping the ACE relations
and events by themselves provided limited cover-
age (34% of slot fills in the training data), but was
helpful when combined with other sources (e.g.
CUNY). Groups with more extensive existing ex-
traction systems could primarily build on these
(e.g. LCC, IBM).
For example, IBM (Castelli et al., 2010) ex-
tended their mention detection component to cover
36 entity types which include many non-ACE
types; and added new relation types between enti-
ties and event anchors. LCC and CUNY applied
active learning techniques to cover non-ACE types
of entities, such as “origin”, “religion”, “title”,
“charge”, “web-site” and “cause-of-death”, and
effectively develop lexicons to filter spurious an-
swers.
Top systems also benefited from customizing and
tightly integrating their recently enhanced extrac-
tion techniques into KBP. For example, IBM,
NYU (Grishman and Min, 2010) and CUNY ex-
ploited entity coreference in pattern learning and
reasoning. It is also notable that traditional extrac-
tion components trained from newswire data suffer
from noise in web data. In order to address this
problem, IBM applied their new robust mention
detection techniques for noisy inputs (Florian et al.,
2010); CUNY developed a component to recover
structured forms such as tables in web data auto-
matically and filter spurious answers.
</bodyText>
<subsectionHeader confidence="0.999517">
5.2 Use of External Knowledge Base
</subsectionHeader>
<bodyText confidence="0.9999766">
Many instance-centered knowledge bases that have
harvested Wikipedia are proliferating on the se-
mantic web. The most well known are probably
the Wikipedia derived resources, including DBpe-
dia (Auer 2007), Freebase (Bollacker 2008) and
YAGO (Suchanek et al., 2007) and Linked Open
Data (http://data.nytimes.com/). The main motiva-
tion of the KBP program is to automatically distill
information from news and web unstructured data
instead of manually constructed knowledge bases,
but these existing knowledge bases can provide a
large number of seed tuples to bootstrap slot filling
or guide distant learning.
Such resources can also be used in a more direct
way. For example, CUNY exploited Freebase and
LCC exploited DBpedia as fact validation in slot
filling. However, most of these resources are
manually created from single data modalities and
only cover well-known entities. For example,
while Freebase contains 116 million instances of
</bodyText>
<page confidence="0.989923">
1154
</page>
<bodyText confidence="0.999764428571429">
7,300 relations for 9 million entities, it only covers
48% of the slot types and 5% of the slot answers in
KBP2010 evaluation data. Therefore, both CUNY
and LCC observed limited gains from the answer
validation approach from Freebase. Both systems
gained about 1% improvement in recall with a
slight loss in precision.
</bodyText>
<subsectionHeader confidence="0.998664">
5.3 Cross-Slot and Cross-Query Reasoning
</subsectionHeader>
<bodyText confidence="0.999986705882353">
Slot Filling can also benefit from extracting re-
vertible queries from the context of any target
query, and conducting global ranking or reasoning
to refine the results. CUNY and IBM developed
recursive reasoning components to refine extrac-
tion results. For a given query, if there are no other
related answer candidates available, they built &amp;quot;re-
vertible” queries in the contexts, similar to (Prager
et al., 2006), to enrich the inference process itera-
tively. For example, if a is extracted as the answer
for org:subsidiaries of the query q, we can con-
sider a as a new revertible query and verify that a
org:parents answer of a is q. Both systems signifi-
cantly benefited from recursive reasoning (CUNY
F-measure on training data was enhanced from
33.57% to 35.29% and IBM F-measure was en-
hanced from 26% to 34.83%).
</bodyText>
<sectionHeader confidence="0.971524" genericHeader="method">
6 Slot Filling: Remaining Challenges
</sectionHeader>
<bodyText confidence="0.981404392857143">
Slot filling remains a very challenging task; only
one system exceeded 30% F-measure on the 2010
evaluation. During the 2010 evaluation data anno-
tation/adjudication process, an initial answer key
annotation was created by a manual search of the
corpus (resulting in 797 instances), and then an
independent adjudication pass was applied to as-
sess these annotations together with pooled system
responses. The Precision, Recall and F-measure for
the initial human annotation are only about 70%,
54% and 61% respectively. While we believe the
annotation consistency can be improved, in part by
refinement of the annotation guidelines, this does
place a limit on system performance.
Most of the shortfall in system performance re-
flects inadequacies in the answer extraction stage,
reflecting limitations in the current state-of-the-art
in information extraction. An analysis of the 2010
training data shows that cross-sentence coreference
and some types of inference are critical to slot fill-
ing. In only 60.4% of the cases do the entity name
and slot fill appear together in the same sentence,
so a system which processes sentences in isolation
is severely limited in its performance. 22.8% of
the cases require cross-sentence (identity) corefer-
ence; 15% require some cross-sentence inference
and 1.8% require cross-slot inference. The infer-
ences include:
</bodyText>
<listItem confidence="0.872787352941176">
• Non-identity coreference: in the following pas-
sage: “Lahoud is married to an Armenian and the
couple have three children. Eldest son Emile Emile
Lahoud was a member of parliament between 2000
and 2005.” the semantic relation between “chil-
dren” and “son” needs to be exploited in order
to generate “Emile Emile Lahoud” as the
per:children of the query entity “Lahoud”;
• Cross-slot inference based on revertible que-
ries, propagation links or even world knowl-
edge to capture some of the most challenging
cases. In the KBP slot filling task, slots are of-
ten dependent on each other, so we can im-
prove the results by improving the “coherence”
of the story (i.e. consistency among all gener-
ated answers (query profiles)). In the following
example:
</listItem>
<construct confidence="0.639215142857143">
“People Magazine has confirmed that actress Julia
Roberts has given birth to her third child a boy
named Henry Daniel Moder. Henry was born
Monday in Los Angeles and weighed 8? lbs. Rob-
erts, 39, and husband Danny Moder, 38, are al-
ready parents to twins Hazel and Phinnaeus who
were born in November 2006.”
</construct>
<bodyText confidence="0.989767333333333">
the following reasoning rules are needed to
generate the answer “Henry Daniel Moder” as
per:children of “Danny Moder”:
</bodyText>
<note confidence="0.57687925">
ChildOf (“Henry Daniel Moder”, “Julia Roberts”)
∧ Coreferential (“Julia Roberts”, “Roberts”)
∧ SpouseOf (“Roberts”, “Danny Moder”) →
ChildOf (“Henry Daniel Moder”, “Danny Moder”)
</note>
<bodyText confidence="0.998542545454545">
KBP Slot Filling is similar to ACE Relation Ex-
traction, which has been extensively studied for the
past 7 years. However, the amount of training data
is much smaller, forcing sites to adjust their train-
ing strategies. Also, some of the constraints of
ACE relation mention extraction – notably, that
both arguments are present in the same sentence –
are not present, making the role of coreference and
cross-sentence inference more critical.
The role of coreference and inference as limiting
factors, while generally recognized, is emphasized
</bodyText>
<page confidence="0.966655">
1155
</page>
<bodyText confidence="0.9992255">
by examining the 163 slot values that the human
annotators filled but that none of the systems were
able to get correct. Many of these difficult cases
involve a combination of problems, but we esti-
mate that at least 25% of the examples involve
coreference which is beyond current system capa-
bilities, such as nominal anaphors:
“Alexandra Burke is out with the video for her second
single ... taken from the British artist’s debut album”
“a woman charged with running a prostitution ring ...
her business, Pamela Martin and Associates”
(underlined phrases are coreferential).
While the types of inferences which may be re-
quired is open-ended, certain types come up re-
peatedly, reflecting the types of slots to be filled:
systems would benefit from specialists which are
able to reason about times, locations, family rela-
tionships, and employment relationships.
</bodyText>
<sectionHeader confidence="0.990884" genericHeader="method">
7 Toward System Combination
</sectionHeader>
<bodyText confidence="0.999956222222222">
The increasing number of diverse approaches
based on different resources provide new opportu-
nities for both entity linking and slot filling tasks to
benefit from system combination.
The NUSchime entity linking system trained a
SVM based re-scoring model to combine two indi-
vidual pipelines. Only one feature based on confi-
dence values from the pipelines was used for re-
scoring. The micro-averaged accuracy was en-
hanced from 79.29%/79.07% to 79.38% after
combination. We also applied a voting approach on
the top 9 entity linking systems and found that all
combination orders achieved significant gains,
with the highest absolute improvement of 4.7% in
micro-averaged accuracy over the top entity link-
ing system.
The CUNY slot filling system trained a maxi-
mum-entropy-based re-ranking model to combine
three individual pipelines, based on various global
features including voting and dependency rela-
tions. Significant gain in F-measure was achieved:
from 17.9%, 27.7% and 21.0% (on training data) to
34.3% after combination. When we applied the
same re-ranking approach to the slot filling sys-
tems which were ranked from the 2nd to 14th, we
achieved 4.3% higher F-score than the best of
these systems.
</bodyText>
<sectionHeader confidence="0.994266" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999828736842105">
Compared to traditional IE and QA tasks, KBP has
raised some interesting and important research is-
sues: It places more emphasis on cross-document
entity resolution which received limited effort in
ACE; it forces systems to deal with redundant and
conflicting answers across large corpora; it links
the facts in text to a knowledge base so that NLP
and data mining/database communities have a bet-
ter chance to collaborate; it provides opportunities
to develop novel training methods such as distant
(and noisy) supervision through Infoboxes (Sur-
deanu et al., 2010; Chen et al., 2010).
In this paper, we provided detailed analysis of the
reasons which have made KBP a more challenging
task, shared our observations and lessons learned
from the evaluation, and suggested some possible
research directions to address these challenges
which may be helpful for current and new partici-
pants, or IE and QA researchers in general.
</bodyText>
<sectionHeader confidence="0.994727" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99791">
The first author was supported by the U.S. Army Re-
search Laboratory under Cooperative Agreement Num-
ber W911NF-09-2-0053, the U.S. NSF CAREER
Award under Grant IIS-0953149 and PSC-CUNY Re-
search Program. The views and conclusions contained
in this document are those of the authors and should not
be interpreted as representing the official policies, either
expressed or implied, of the Army Research Laboratory
or the U.S. Government. The U.S. Government is au-
thorized to reproduce and distribute reprints for Gov-
ernment purposes notwithstanding any copyright
notation hereon.
</bodyText>
<sectionHeader confidence="0.998383" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.986615333333334">
Javier Artiles, Julio Gonzalo and Satoshi Sekine. 2007.
The SemEval-2007 WePS Evaluation: Establishing a
benchmark for the Web People Search Task. Proc.
the 4th International Workshop on Semantic Evalua-
tions (Semeval-2007).
S. Auer, C. Bizer, G. Kobilarov, J. Lehmann and Z. Ives.
2007. DBpedia: A nucleus for a web of open data.
Proc. 6th International Semantic Web Conference.
K. Balog, L. Azzopardi, M. de Rijke. 2008. Personal
Name Resolution of Web People Search. Proc.
WWW2008 Workshop: NLP Challenges in the Infor-
mation Explosion Era (NLPIX 2008).
</reference>
<page confidence="0.944279">
1156
</page>
<reference confidence="0.999464887755102">
Alex Baron and Marjorie Freedman. 2008. Who is Who
and What is What: Experiments in Cross-Document
Co-Reference. Proc. EMNLP 2008.
K. Bollacker, R. Cook, and P. Tufts. 2007. Freebase: A
Shared Database of Structured General Human
Knowledge. Proc. National Conference on Artificial
Intelligence (Volume 2).
Lorna Byrne and John Dunnion. 2010. UCD IIRG at
TAC 2010. Proc. TAC 2010 Workshop.
Praveen Bysani, Kranthi Reddy, Vijay Bharath Reddy,
Sudheer Kovelamudi, Prasad Pingali and Vasudeva
Varma. 2010. IIIT Hyderabad in Guided Summariza-
tion and Knowledge Base Population. Proc. TAC
2010 Workshop.
Vittorio Castelli, Radu Florian and Ding-jung Han.
2010. Slot Filling through Statistical Processing and
Inference Rules. Proc. TAC 2010 Workshop.
Angel X. Chang, Valentin I. Spitkovsky, Eric Yeh,
Eneko Agirre and Christopher D. Manning. 2010.
Stanford-UBC Entity Linking at TAC-KBP. Proc.
TAC 2010 Workshop.
Zheng Chen, Suzanne Tamang, Adam Lee, Xiang Li,
Wen-Pin Lin, Matthew Snover, Javier Artiles,
Marissa Passantino and Heng Ji. 2010. CUNY-
BLENDER TAC-KBP2010 Entity Linking and Slot
Filling System Description. Proc. TAC 2010 Work-
shop.
Grzegorz Chrupala, Saeedeh Momtazi, Michael Wie-
gand, Stefan Kazalski, Fang Xu, Benjamin Roth, Al-
exandra Balahur, Dietrick Klakow. Saarland
University Spoken Language Systems at the Slot Fill-
ing Task of TAC KBP 2010. Proc. TAC 2010 Work-
shop.
Mark Dredze, Paul McNamee, Delip Rao, Adam Gerber
and Tim Finin. 2010. Entity Disambiguation for
Knowledge Base Population. Proc. COLING 2010.
Norberto Fernandez, Jesus A. Fisteus, Luis Sanchez and
Eduardo Martin. 2010. WebTLab: A Cooccurence-
based Approach to KBP 2010 Entity-Linking Task.
Proc. TAC 2010 Workshop.
Radu Florian, John F. Pitrelli, Salim Roukos and Imed
Zitouni. 2010. Improving Mention Detection Robust-
ness to Noisy Input. Proc. EMNLP2010.
Sanyuan Gao, Yichao Cai, Si Li, Zongyu Zhang, Jingyi
Guan, Yan Li, Hao Zhang, Weiran Xu and Jun Guo.
2010. PRIS at TAC2010 KBP Track. Proc. TAC
2010 Workshop.
Swapna Gottipati and Jing Jiang. 2010. SMU-SIS at
TAC 2010 – KBP Track Entity Linking. Proc. TAC
2010 Workshop.
Ralph Grishman and Bonan Min. 2010. New York Uni-
versity KBP 2010 Slot-Filling System. Proc. TAC
2010 Workshop.
Ander Intxaurrondo, Oier Lopez de Lacalle and Eneko
Agirre. 2010. UBC at Slot Filling TAC-KBP2010.
Proc. TAC 2010 Workshop.
John Lehmann, Sean Monahan, Luke Nezda, Arnold
Jung and Ying Shi. 2010. LCC Approaches to
Knowledge Base Population at TAC 2010. Proc.
TAC 2010 Workshop.
Paul McNamee and Hoa Dang. 2009. Overview of the
TAC 2009 Knowledge Base Population Track. Proc.
TAC 2009 Workshop.
Paul McNamee, Hoa Trang Dang, Heather Simpson,
Patrick Schone and Stephanie M. Strassel. 2010. An
Evaluation of Technologies for Knowledge Base
Population. Proc. LREC2010.
Paul McNamee, Rion Snow, Patrick Schone and James
Mayfield. 2008. Learning Named Entity Hyponyms
for Question Answering. Proc. IJCNLP2008.
Paul McNamee. 2010. HLTCOE Efforts in Entity Link-
ing at TAC KBP 2010. Proc. TAC 2010 Workshop.
Danielle S McNamara. 2001. Reading both High-
coherence and Low-coherence Texts: Effects of Text
Sequence and Prior Knowledge. Canadian Journal of
Experimental Psychology.
Olena Medelyan, Catherine Legg, David Milne and Ian
H. Witten. 2009. Mining Meaning from Wikipedia.
International Journal of Human-Computer Studies
archive. Volume 67 , Issue 9.
David Nemeskey, Gabor Recski, Attila Zseder and An-
dras Kornai. 2010. BUDAPESTACAD at TAC 2010.
Proc. TAC 2010 Workshop.
Cesar de Pablo-Sanchez, Juan Perea and Paloma Marti-
nez. 2010. Combining Similarities with Regression
based Classifiers for Entity Linking at TAC 2010.
Proc. TAC 2010 Workshop.
Lawrence Page, Sergey Brin, Rajeev Motwani and
Terry Winograd. 1998. The PageRank Citation Rank-
ing: Bringing Order to the Web. Proc. the 7th Interna-
tional World Wide Web Conference.
Luiz Augusto Pizzato, Diego Molla and Cecile Paris.
2006. Pseudo Relevance Feedback Using Named En-
tities for Question Answering. Proc. the Australasian
Language Technology Workshop 2006.
J. Prager, P. Duboue, and J. Chu-Carroll. 2006. Improv-
ing QA Accuracy by Question Inversion. Proc. ACL-
COLING 2006.
</reference>
<page confidence="0.875229">
1157
</page>
<reference confidence="0.999709076923077">
Will Radford, Ben Hachey, Joel Nothman, Matthew
Honnibal and James R. Curran. 2010. CMCRC at
TAC10: Document-level Entity Linking with Graph-
based Re-ranking. Proc. TAC 2010 Workshop.
Yang Song, Zhengyan He and Houfeng Wang. 2010.
ICL_KBP Approaches to Knowledge Base Popula-
tion at TAC2010. Proc. TAC 2010 Workshop.
F. M. Suchanek, G. Kasneci, and G. Weikum. 2007.
Yago: A Core of Semantic Knowledge. Proc. 16th
International World Wide Web Conference.
Mihai Surdeanu, David McClosky, Julie Tibshirani,
John Bauer, Angel X. Chang, Valentin I. Spitkovsky,
Christopher D. Manning. 2010. A Simple Distant
Supervision Approach for the TAC-KBP Slot Filling
Task. Proc. TAC 2010 Workshop.
Ani Thomas, Arpana Rawai, M K Kowar, Sanjay
Sharma, Sarang Pitale and Neeraj Kharya. 2010.
Bhilai Institute of Technology Durg at TAC 2010:
Knowledge Base Population Task Challenge. Proc.
TAC 2010 Workshop.
Jingtao Yu, Omkar Mujgond and Rob Gaizauskas.
2010. The University of Sheffield System at TAC
KBP 2010. Proc. TAC 2010 Workshop.
Wei Zhang, Yan Chuan Sim, Jian Su and Chew Lim
Tan. 2010. NUS-I2R: Learning a Combined System
for Entity Linking. Proc. TAC 2010 Workshop.
</reference>
<page confidence="0.995281">
1158
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.713016">
<title confidence="0.999699">Knowledge Base Population: Successful Approaches and Challenges</title>
<author confidence="0.999879">Heng Ji Ralph Grishman</author>
<affiliation confidence="0.929061">Computer Science Department Computer Science Department Queens College and Graduate Center New York</affiliation>
<address confidence="0.928887">City University of New York New York, NY 11367, USA New York, NY 10003, USA</address>
<email confidence="0.99977">hengji@cs.qc.cuny.edugrishman@cs.nyu.edu</email>
<abstract confidence="0.999532772727273">In this paper we give an overview of the Knowledge Base Population (KBP) track at the 2010 Text Analysis Conference. The main goal of KBP is to promote research in discovering facts about entities and augmenting a knowledge base (KB) with these facts. This is through two tasks, Linking linking names in context to entities in the KB – Filling adding information about an entity to the KB. A large source collection of newswire and web documents is provided from which systems are to discover information. Attributes (“slots”) derived from Wikipedia infoboxes are used to create the reference KB. In this paper we provide an overview of the techniques which can serve as a basis for a good KBP system, lay out the remaining challenges by comparison with traditional Information Extraction (IE) and Question Answering (QA) tasks, and provide some suggestions to address these challenges.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Javier Artiles</author>
<author>Julio Gonzalo</author>
<author>Satoshi Sekine</author>
</authors>
<title>The SemEval-2007 WePS Evaluation: Establishing a benchmark for the Web People Search Task.</title>
<date>2007</date>
<booktitle>Proc. the 4th International Workshop on Semantic Evaluations (Semeval-2007).</booktitle>
<contexts>
<context position="15775" citStr="Artiles et al., 2007" startWordPosition="2506" endWordPosition="2509">puted linkability scores iteratively. Consistent improvements were reported by the WebTLab system (from 63.64% to 66.58%). 4 Entity Linking: Remaining Challenges 4.1 Comparison with Traditional Crossdocument Coreference Resolution Part of the entity linking task can be modeled as a cross-document entity resolution problem which includes two principal challenges: the same entity can be referred to by more than one name string and the same name string can refer to more than one entity. The research on cross-document entity coreference resolution can be traced back to the Web People Search task (Artiles et al., 2007) and ACE2008 (e.g. Baron and Freedman, 2008). Compared to WePS and ACE, KBP requires linking an entity mention in a source document to a knowledge base with or without Wikipedia articles. Therefore sometimes the linking decisions heavily rely on entity profile comparison with Wikipedia infoboxes. In addition, KBP introduced GPE entity disambiguation. In source documents, especially in web data, usually few explicit attributes about GPE entities are provided, so an entity linking system also needs to conduct external knowledge discovery from background related documents or hyperlink mining. 4.2</context>
</contexts>
<marker>Artiles, Gonzalo, Sekine, 2007</marker>
<rawString>Javier Artiles, Julio Gonzalo and Satoshi Sekine. 2007. The SemEval-2007 WePS Evaluation: Establishing a benchmark for the Web People Search Task. Proc. the 4th International Workshop on Semantic Evaluations (Semeval-2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Auer</author>
<author>C Bizer</author>
<author>G Kobilarov</author>
<author>J Lehmann</author>
<author>Z Ives</author>
</authors>
<title>DBpedia: A nucleus for a web of open data.</title>
<date>2007</date>
<booktitle>Proc. 6th International Semantic Web Conference.</booktitle>
<marker>Auer, Bizer, Kobilarov, Lehmann, Ives, 2007</marker>
<rawString>S. Auer, C. Bizer, G. Kobilarov, J. Lehmann and Z. Ives. 2007. DBpedia: A nucleus for a web of open data. Proc. 6th International Semantic Web Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Balog</author>
<author>L Azzopardi</author>
<author>M de Rijke</author>
</authors>
<title>Personal Name Resolution of Web People Search.</title>
<date>2008</date>
<booktitle>Proc. WWW2008 Workshop: NLP Challenges in the Information Explosion Era (NLPIX</booktitle>
<marker>Balog, Azzopardi, de Rijke, 2008</marker>
<rawString>K. Balog, L. Azzopardi, M. de Rijke. 2008. Personal Name Resolution of Web People Search. Proc. WWW2008 Workshop: NLP Challenges in the Information Explosion Era (NLPIX 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Baron</author>
<author>Marjorie Freedman</author>
</authors>
<title>Who is Who and What is What: Experiments in Cross-Document Co-Reference.</title>
<date>2008</date>
<booktitle>Proc. EMNLP</booktitle>
<contexts>
<context position="15819" citStr="Baron and Freedman, 2008" startWordPosition="2513" endWordPosition="2516">nsistent improvements were reported by the WebTLab system (from 63.64% to 66.58%). 4 Entity Linking: Remaining Challenges 4.1 Comparison with Traditional Crossdocument Coreference Resolution Part of the entity linking task can be modeled as a cross-document entity resolution problem which includes two principal challenges: the same entity can be referred to by more than one name string and the same name string can refer to more than one entity. The research on cross-document entity coreference resolution can be traced back to the Web People Search task (Artiles et al., 2007) and ACE2008 (e.g. Baron and Freedman, 2008). Compared to WePS and ACE, KBP requires linking an entity mention in a source document to a knowledge base with or without Wikipedia articles. Therefore sometimes the linking decisions heavily rely on entity profile comparison with Wikipedia infoboxes. In addition, KBP introduced GPE entity disambiguation. In source documents, especially in web data, usually few explicit attributes about GPE entities are provided, so an entity linking system also needs to conduct external knowledge discovery from background related documents or hyperlink mining. 4.2 Analysis of Difficult Queries There are 225</context>
</contexts>
<marker>Baron, Freedman, 2008</marker>
<rawString>Alex Baron and Marjorie Freedman. 2008. Who is Who and What is What: Experiments in Cross-Document Co-Reference. Proc. EMNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Bollacker</author>
<author>R Cook</author>
<author>P Tufts</author>
</authors>
<title>Freebase: A Shared Database of Structured General Human Knowledge.</title>
<date>2007</date>
<booktitle>Proc. National Conference on Artificial Intelligence</booktitle>
<volume>2</volume>
<marker>Bollacker, Cook, Tufts, 2007</marker>
<rawString>K. Bollacker, R. Cook, and P. Tufts. 2007. Freebase: A Shared Database of Structured General Human Knowledge. Proc. National Conference on Artificial Intelligence (Volume 2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lorna Byrne</author>
<author>John Dunnion</author>
</authors>
<date>2010</date>
<booktitle>UCD IIRG at TAC 2010. Proc. TAC 2010 Workshop.</booktitle>
<contexts>
<context position="22858" citStr="Byrne and Dunnion, 2010" startWordPosition="3620" endWordPosition="3623">ge CUNY (Chen et al., 2010) IE Learning seed, one iteration) Bootstrapping (small NYU (Grishman and Min, 2010) seed, multiple iterations) Supervised Distant Supervision Budapestacad (Nemeskey et al., 2010), lsv (Chrupala et al., Classifier 2010), Stanford (Surdeanu et al., 2010), UBC (Intxaurrondo et al., 2010) Trained from KBP train- BuptPris (Gao et al., 2010), CUNY (Chen et al., 2010), IBM ing data and other re- (Castelli et al., 2010), ICL (Song et al., 2010), LCC lated tasks (Lehmann et al., 2010), lsv (Chrupala et al., 2010), Siel (Bysani et al., 2010) QA CUNY (Chen et al., 2010), iirg (Byrne and Dunnion, 2010) Hand-coded Heuristic Rules BuptPris (Gao et al., 2010), USFD (Yu et al., 2010) Table 3. Slot Filling Answer Extraction Method Comparison On the other hand, there were a lot of &apos;facts&apos; available – pairs of entities bearing a relationship corresponding closely to the KBP relations – in the form of filled Wikipedia infoboxes. These could be used for various forms of indirect or distant learning, where instances in a large corpus of such pairs are taken as (positive) training instances. However, such instances are noisy – if a pair of entities participates in more than one relation, the found ins</context>
</contexts>
<marker>Byrne, Dunnion, 2010</marker>
<rawString>Lorna Byrne and John Dunnion. 2010. UCD IIRG at TAC 2010. Proc. TAC 2010 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Praveen Bysani</author>
</authors>
<title>Kranthi Reddy, Vijay Bharath Reddy, Sudheer Kovelamudi, Prasad Pingali and Vasudeva Varma.</title>
<date>2010</date>
<booktitle>IIIT Hyderabad in Guided Summarization and Knowledge Base Population. Proc. TAC 2010 Workshop.</booktitle>
<marker>Bysani, 2010</marker>
<rawString>Praveen Bysani, Kranthi Reddy, Vijay Bharath Reddy, Sudheer Kovelamudi, Prasad Pingali and Vasudeva Varma. 2010. IIIT Hyderabad in Guided Summarization and Knowledge Base Population. Proc. TAC 2010 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vittorio Castelli</author>
<author>Radu Florian</author>
<author>Ding-jung Han</author>
</authors>
<title>Slot Filling through Statistical Processing and Inference Rules.</title>
<date>2010</date>
<booktitle>Proc. TAC 2010 Workshop.</booktitle>
<contexts>
<context position="22676" citStr="Castelli et al., 2010" startWordPosition="3587" endWordPosition="3590">Bootstrapping) QA Answer Level Pattern Classifier Rules Knowledge Base Redundancy Removal Answers Query Expansion 1153 Methods System Examples Trained Pattern Distant Learning (large CUNY (Chen et al., 2010) IE Learning seed, one iteration) Bootstrapping (small NYU (Grishman and Min, 2010) seed, multiple iterations) Supervised Distant Supervision Budapestacad (Nemeskey et al., 2010), lsv (Chrupala et al., Classifier 2010), Stanford (Surdeanu et al., 2010), UBC (Intxaurrondo et al., 2010) Trained from KBP train- BuptPris (Gao et al., 2010), CUNY (Chen et al., 2010), IBM ing data and other re- (Castelli et al., 2010), ICL (Song et al., 2010), LCC lated tasks (Lehmann et al., 2010), lsv (Chrupala et al., 2010), Siel (Bysani et al., 2010) QA CUNY (Chen et al., 2010), iirg (Byrne and Dunnion, 2010) Hand-coded Heuristic Rules BuptPris (Gao et al., 2010), USFD (Yu et al., 2010) Table 3. Slot Filling Answer Extraction Method Comparison On the other hand, there were a lot of &apos;facts&apos; available – pairs of entities bearing a relationship corresponding closely to the KBP relations – in the form of filled Wikipedia infoboxes. These could be used for various forms of indirect or distant learning, where instances in a </context>
<context position="24210" citStr="Castelli et al., 2010" startWordPosition="3847" endWordPosition="3850"> Several sites used such distant supervision to acquire patterns or train classifiers, in some cases combined with direct supervision using the training data (Chrupala et al., 2010). Several groups used and extended existing relation extraction systems, and then mapped the results into KBP slots. Mapping the ACE relations and events by themselves provided limited coverage (34% of slot fills in the training data), but was helpful when combined with other sources (e.g. CUNY). Groups with more extensive existing extraction systems could primarily build on these (e.g. LCC, IBM). For example, IBM (Castelli et al., 2010) extended their mention detection component to cover 36 entity types which include many non-ACE types; and added new relation types between entities and event anchors. LCC and CUNY applied active learning techniques to cover non-ACE types of entities, such as “origin”, “religion”, “title”, “charge”, “web-site” and “cause-of-death”, and effectively develop lexicons to filter spurious answers. Top systems also benefited from customizing and tightly integrating their recently enhanced extraction techniques into KBP. For example, IBM, NYU (Grishman and Min, 2010) and CUNY exploited entity corefere</context>
</contexts>
<marker>Castelli, Florian, Han, 2010</marker>
<rawString>Vittorio Castelli, Radu Florian and Ding-jung Han. 2010. Slot Filling through Statistical Processing and Inference Rules. Proc. TAC 2010 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angel X Chang</author>
<author>Valentin I Spitkovsky</author>
</authors>
<title>Eric Yeh, Eneko Agirre</title>
<date>2010</date>
<booktitle>Proc. TAC 2010 Workshop.</booktitle>
<marker>Chang, Spitkovsky, 2010</marker>
<rawString>Angel X. Chang, Valentin I. Spitkovsky, Eric Yeh, Eneko Agirre and Christopher D. Manning. 2010. Stanford-UBC Entity Linking at TAC-KBP. Proc. TAC 2010 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng Chen</author>
<author>Suzanne Tamang</author>
<author>Adam Lee</author>
<author>Xiang Li</author>
<author>Wen-Pin Lin</author>
<author>Matthew Snover</author>
<author>Javier Artiles</author>
<author>Marissa Passantino</author>
<author>Heng Ji</author>
</authors>
<date>2010</date>
<booktitle>CUNYBLENDER TAC-KBP2010 Entity Linking and Slot Filling System Description. Proc. TAC 2010 Workshop.</booktitle>
<contexts>
<context position="9667" citStr="Chen et al., 2010" startWordPosition="1544" endWordPosition="1547">rrence estimates. Many other teams including CUNY and Siel used redirect pages and disambiguation pages for query expansion. The Siel team also exploited bold texts from first paragraphs because they often contain nicknames, alias names and full names. KB Node Candidate Generation unsupervised similarity computation Document semantic analysis Wiki hyperlink mining Query Expansion KB Node Candidate Ranking Query IR Source doc Coreference Resolution Answer supervised classification Graph -based Wiki KB +Texts IR Methods System Examples System Ranking Range Query Wikipedia Hyperlink Mining CUNY (Chen et al., 2010), NUSchime (Zhang et al., [2, 15] Expansion 2010), Siel (Bysani et al., 2010), SMU-SIS (Gottipati et al., 2010), USFD (Yu et al., 2010), WebTLab team (Fernandez et al., 2010) Source document coreference CUNY (Chen et al., 2010) 9 resolution Candidate Document semantic analysis ARPANI (Thomas et al., 2010), CUNY (Chen et al., [1,14] Generation and context modeling 2010), LCC (Lehmann et al., 2010) IR CUNY (Chen et al., 2010), Budapestacad (Nemeskey et [9, 16] al., 2010), USFD (Yu et al., 2010) Candidate Unsupervised Similarity CUNY (Chen et al., 2010), SMU-SIS (Gottipati et al., [9, 14] Ranking</context>
<context position="13182" citStr="Chen et al., 2010" startWordPosition="2089" endWordPosition="2092">010) have explicitly used supervised classification based on many lexical and name tagging features, and most of them are ranked in top 6 in the evaluation. Therefore we can conclude that supervised learning normally leads to a reasonably good performance. However, a highperforming entity linking system can also be implemented in an unsupervised fashion by exploiting effective characteristics and algorithms, as we will discuss in the next sections. 3.4 Semantic Relation Features Almost all entity linking systems have used semantic relations as features (e.g. BuptPris (Gao et al., 2010), CUNY (Chen et al., 2010) and HLTCOE). The semantic features used in the BuptPris system include name tagging, infoboxes, synonyms, variants and abbreviations. In the CUNY system, the semantic features are automatically extracted from their slot filling system. The results are summarized in Table 2, showing the gains over a baseline system (using only Wikipedia title features in the case of BuptPris, using tf-idf weighted word features for CUNY). As we can see, except for person entities in the BuptPris system, all types of entities have obtained significant improvement by using semantic features in entity linking. Sy</context>
<context position="22261" citStr="Chen et al., 2010" startWordPosition="3524" endWordPosition="3527">intly prepared responses to another 50. So traditional supervised learning, based directly on the training data, would provide limited coverage. Coverage could be improved by using the training data as seeds for a bootstrapping procedure. Figure 2. General Slot Filling System Architecture Query Source Collection Training Data/ External KB IR, QA IR Sentence/Passage Level Document Level IE (Distant Learning/ Bootstrapping) QA Answer Level Pattern Classifier Rules Knowledge Base Redundancy Removal Answers Query Expansion 1153 Methods System Examples Trained Pattern Distant Learning (large CUNY (Chen et al., 2010) IE Learning seed, one iteration) Bootstrapping (small NYU (Grishman and Min, 2010) seed, multiple iterations) Supervised Distant Supervision Budapestacad (Nemeskey et al., 2010), lsv (Chrupala et al., Classifier 2010), Stanford (Surdeanu et al., 2010), UBC (Intxaurrondo et al., 2010) Trained from KBP train- BuptPris (Gao et al., 2010), CUNY (Chen et al., 2010), IBM ing data and other re- (Castelli et al., 2010), ICL (Song et al., 2010), LCC lated tasks (Lehmann et al., 2010), lsv (Chrupala et al., 2010), Siel (Bysani et al., 2010) QA CUNY (Chen et al., 2010), iirg (Byrne and Dunnion, 2010) Ha</context>
<context position="33347" citStr="Chen et al., 2010" startWordPosition="5290" endWordPosition="5293">best of these systems. 8 Conclusion Compared to traditional IE and QA tasks, KBP has raised some interesting and important research issues: It places more emphasis on cross-document entity resolution which received limited effort in ACE; it forces systems to deal with redundant and conflicting answers across large corpora; it links the facts in text to a knowledge base so that NLP and data mining/database communities have a better chance to collaborate; it provides opportunities to develop novel training methods such as distant (and noisy) supervision through Infoboxes (Surdeanu et al., 2010; Chen et al., 2010). In this paper, we provided detailed analysis of the reasons which have made KBP a more challenging task, shared our observations and lessons learned from the evaluation, and suggested some possible research directions to address these challenges which may be helpful for current and new participants, or IE and QA researchers in general. Acknowledgements The first author was supported by the U.S. Army Research Laboratory under Cooperative Agreement Number W911NF-09-2-0053, the U.S. NSF CAREER Award under Grant IIS-0953149 and PSC-CUNY Research Program. The views and conclusions contained in th</context>
</contexts>
<marker>Chen, Tamang, Lee, Li, Lin, Snover, Artiles, Passantino, Ji, 2010</marker>
<rawString>Zheng Chen, Suzanne Tamang, Adam Lee, Xiang Li, Wen-Pin Lin, Matthew Snover, Javier Artiles, Marissa Passantino and Heng Ji. 2010. CUNYBLENDER TAC-KBP2010 Entity Linking and Slot Filling System Description. Proc. TAC 2010 Workshop.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Grzegorz Chrupala</author>
<author>Saeedeh Momtazi</author>
<author>Michael Wiegand</author>
<author>Stefan Kazalski</author>
<author>Fang Xu</author>
<author>Benjamin Roth</author>
</authors>
<booktitle>Language Systems at the Slot Filling Task of TAC KBP 2010. Proc. TAC 2010 Workshop.</booktitle>
<institution>Alexandra Balahur, Dietrick Klakow. Saarland University Spoken</institution>
<marker>Chrupala, Momtazi, Wiegand, Kazalski, Xu, Roth, </marker>
<rawString>Grzegorz Chrupala, Saeedeh Momtazi, Michael Wiegand, Stefan Kazalski, Fang Xu, Benjamin Roth, Alexandra Balahur, Dietrick Klakow. Saarland University Spoken Language Systems at the Slot Filling Task of TAC KBP 2010. Proc. TAC 2010 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Paul McNamee</author>
<author>Delip Rao</author>
<author>Adam Gerber</author>
<author>Tim Finin</author>
</authors>
<title>Entity Disambiguation for Knowledge Base Population.</title>
<date>2010</date>
<booktitle>Proc. COLING</booktitle>
<contexts>
<context position="19168" citStr="Dredze et al., 2010" startWordPosition="3043" endWordPosition="3046">r example, in the source document “...Filed under: Falcons &lt;http://sports.aol.com/fanhouse/category/atlantafalcons/&gt;”, a system will need to analyze the document which this hyperlink refers to. Such cases might require new query reformulation and cross-document aggregation techniques, which are both beyond traditional entity disambiguation paradigms. 1152 • Require Entity Salience Ranking Some of these queries represent salient entities and so using web popularity rank (e.g. ranking/hit counts of Wikipedia pages from search engine) can yield correct answers in most cases (Bysani et al., 2010; Dredze et al., 2010). In fact we found that a naïve candidate ranking approach based on web popularity alone can achieve 71% micro-averaged accuracy, which is better than 24 system runs in KBP2010. Since the web information is used as a black box (including query expansion and query log analysis) which changes over time, it’s more difficult to duplicate research results. However, gazetteers with entities ranked by salience or major entities marked are worth encoding as additional features. For example, in the following passages: ... Tritschler brothers competed in gymnastics at the 1904 Games in St Louis 104 year</context>
</contexts>
<marker>Dredze, McNamee, Rao, Gerber, Finin, 2010</marker>
<rawString>Mark Dredze, Paul McNamee, Delip Rao, Adam Gerber and Tim Finin. 2010. Entity Disambiguation for Knowledge Base Population. Proc. COLING 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norberto Fernandez</author>
<author>Jesus A Fisteus</author>
<author>Luis Sanchez</author>
<author>Eduardo Martin</author>
</authors>
<title>WebTLab: A Cooccurencebased Approach to KBP</title>
<date>2010</date>
<booktitle>Proc. TAC 2010 Workshop.</booktitle>
<contexts>
<context position="8928" citStr="Fernandez et al., 2010" startWordPosition="1435" endWordPosition="1438">inding all possible KB entries that a query might link to; (3) candidate ranking – rank the probabilities of all candidates and NIL answer. Table 1 summarizes the systems which exploited different approaches at each step. In the following subsections we will highlight the new and effective techniques used in entity linking. 3.2 Wilipedia Structure Mining Wikipedia articles are peppered with structured information and hyperlinks to other (on average 25) articles (Medelyan et al., 2009). Such information provides additional sources for entity linking: (1). Query Expansion: For example, WebTLab (Fernandez et al., 2010) used Wikipedia link structure (source, anchors, redirects and disambiguation) to extend the KB and compute entity cooccurrence estimates. Many other teams including CUNY and Siel used redirect pages and disambiguation pages for query expansion. The Siel team also exploited bold texts from first paragraphs because they often contain nicknames, alias names and full names. KB Node Candidate Generation unsupervised similarity computation Document semantic analysis Wiki hyperlink mining Query Expansion KB Node Candidate Ranking Query IR Source doc Coreference Resolution Answer supervised classific</context>
</contexts>
<marker>Fernandez, Fisteus, Sanchez, Martin, 2010</marker>
<rawString>Norberto Fernandez, Jesus A. Fisteus, Luis Sanchez and Eduardo Martin. 2010. WebTLab: A Cooccurencebased Approach to KBP 2010 Entity-Linking Task. Proc. TAC 2010 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Florian</author>
<author>John F Pitrelli</author>
</authors>
<title>Salim Roukos and Imed Zitouni.</title>
<date>2010</date>
<booktitle>Proc. EMNLP2010.</booktitle>
<marker>Florian, Pitrelli, 2010</marker>
<rawString>Radu Florian, John F. Pitrelli, Salim Roukos and Imed Zitouni. 2010. Improving Mention Detection Robustness to Noisy Input. Proc. EMNLP2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanyuan Gao</author>
<author>Yichao Cai</author>
<author>Si Li</author>
<author>Zongyu Zhang</author>
<author>Jingyi Guan</author>
</authors>
<date>2010</date>
<booktitle>PRIS at TAC2010 KBP Track. Proc. TAC 2010 Workshop.</booktitle>
<location>Yan Li, Hao Zhang, Weiran Xu</location>
<contexts>
<context position="10573" citStr="Gao et al., 2010" startWordPosition="1688" endWordPosition="1691">mas et al., 2010), CUNY (Chen et al., [1,14] Generation and context modeling 2010), LCC (Lehmann et al., 2010) IR CUNY (Chen et al., 2010), Budapestacad (Nemeskey et [9, 16] al., 2010), USFD (Yu et al., 2010) Candidate Unsupervised Similarity CUNY (Chen et al., 2010), SMU-SIS (Gottipati et al., [9, 14] Ranking Computation (e.g. VSM) 2010), USFD (Yu et al., 2010) Supervised LCC (Lehmann et al., 2010), NUSchime (Zhang et al., [1, 10] Classification 2010), Stanford-UBC (Chang et al., 2010), HLTCOE (McNamee, 2010), UC3M (Pablo-Sanchez et al., 2010) Rule-based LCC (Lehmann et al., 2010), BuptPris (Gao et al., 2010) [1, 8] Global Graph-based Ranking CMCRC (Radford et al., 2010) 3 IR Budapestacad (Nemeskey et al., 2010) 16 Table 1. Entity Linking Method Comparison 1150 (2). Candidate Ranking: Stanford-UBC used Wikipedia hyperlinks (clarification, disambiguation, title) for query re-mapping, and encoded lexical and part-of-speech features from Wikipedia articles containing hyperlinks to the queries to train a supervised classifier; they reported a significant improvement on micro-averaged accuracy, from 74.85% to 82.15%. In fact, when the mined attributes become rich enough, they can be used as an expanded</context>
<context position="13156" citStr="Gao et al., 2010" startWordPosition="2084" endWordPosition="2087"> (Pablo-Sanchez et al., 2010) have explicitly used supervised classification based on many lexical and name tagging features, and most of them are ranked in top 6 in the evaluation. Therefore we can conclude that supervised learning normally leads to a reasonably good performance. However, a highperforming entity linking system can also be implemented in an unsupervised fashion by exploiting effective characteristics and algorithms, as we will discuss in the next sections. 3.4 Semantic Relation Features Almost all entity linking systems have used semantic relations as features (e.g. BuptPris (Gao et al., 2010), CUNY (Chen et al., 2010) and HLTCOE). The semantic features used in the BuptPris system include name tagging, infoboxes, synonyms, variants and abbreviations. In the CUNY system, the semantic features are automatically extracted from their slot filling system. The results are summarized in Table 2, showing the gains over a baseline system (using only Wikipedia title features in the case of BuptPris, using tf-idf weighted word features for CUNY). As we can see, except for person entities in the BuptPris system, all types of entities have obtained significant improvement by using semantic feat</context>
<context position="22598" citStr="Gao et al., 2010" startWordPosition="3572" endWordPosition="3575">KB IR, QA IR Sentence/Passage Level Document Level IE (Distant Learning/ Bootstrapping) QA Answer Level Pattern Classifier Rules Knowledge Base Redundancy Removal Answers Query Expansion 1153 Methods System Examples Trained Pattern Distant Learning (large CUNY (Chen et al., 2010) IE Learning seed, one iteration) Bootstrapping (small NYU (Grishman and Min, 2010) seed, multiple iterations) Supervised Distant Supervision Budapestacad (Nemeskey et al., 2010), lsv (Chrupala et al., Classifier 2010), Stanford (Surdeanu et al., 2010), UBC (Intxaurrondo et al., 2010) Trained from KBP train- BuptPris (Gao et al., 2010), CUNY (Chen et al., 2010), IBM ing data and other re- (Castelli et al., 2010), ICL (Song et al., 2010), LCC lated tasks (Lehmann et al., 2010), lsv (Chrupala et al., 2010), Siel (Bysani et al., 2010) QA CUNY (Chen et al., 2010), iirg (Byrne and Dunnion, 2010) Hand-coded Heuristic Rules BuptPris (Gao et al., 2010), USFD (Yu et al., 2010) Table 3. Slot Filling Answer Extraction Method Comparison On the other hand, there were a lot of &apos;facts&apos; available – pairs of entities bearing a relationship corresponding closely to the KBP relations – in the form of filled Wikipedia infoboxes. These could be</context>
</contexts>
<marker>Gao, Cai, Li, Zhang, Guan, 2010</marker>
<rawString>Sanyuan Gao, Yichao Cai, Si Li, Zongyu Zhang, Jingyi Guan, Yan Li, Hao Zhang, Weiran Xu and Jun Guo. 2010. PRIS at TAC2010 KBP Track. Proc. TAC 2010 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Gottipati</author>
<author>Jing Jiang</author>
</authors>
<date>2010</date>
<booktitle>SMU-SIS at TAC 2010 – KBP Track Entity Linking. Proc. TAC 2010 Workshop.</booktitle>
<contexts>
<context position="15042" citStr="Gottipati and Jiang, 2010" startWordPosition="2394" endWordPosition="2397">ities (“queries”) which have no KB entry or few slot fills in the KB, extract their attributes, and conduct global reasoning over these attributes in order to generate the final output. At the very least, due to the semantic coherence principle (McNamara, 2001), the information of an entity depends on the information of other entities. For example, the WebTLab team and the CMCRC team extracted all entities in the context of a given query, and disambiguated all entities at the same time using a PageRank-like algorithm (Page et al., 1998) or a Graph-based Re-ranking algorithm. The SMU-SIS team (Gottipati and Jiang, 2010) re-formulated queries using contexts. The LCC team modeled 1151 contexts using Wikipedia page concepts, and computed linkability scores iteratively. Consistent improvements were reported by the WebTLab system (from 63.64% to 66.58%). 4 Entity Linking: Remaining Challenges 4.1 Comparison with Traditional Crossdocument Coreference Resolution Part of the entity linking task can be modeled as a cross-document entity resolution problem which includes two principal challenges: the same entity can be referred to by more than one name string and the same name string can refer to more than one entity.</context>
</contexts>
<marker>Gottipati, Jiang, 2010</marker>
<rawString>Swapna Gottipati and Jing Jiang. 2010. SMU-SIS at TAC 2010 – KBP Track Entity Linking. Proc. TAC 2010 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>Bonan Min</author>
</authors>
<title>Slot-Filling System.</title>
<date>2010</date>
<booktitle>Proc. TAC 2010 Workshop.</booktitle>
<location>New York University KBP</location>
<contexts>
<context position="22344" citStr="Grishman and Min, 2010" startWordPosition="3536" endWordPosition="3539">sed directly on the training data, would provide limited coverage. Coverage could be improved by using the training data as seeds for a bootstrapping procedure. Figure 2. General Slot Filling System Architecture Query Source Collection Training Data/ External KB IR, QA IR Sentence/Passage Level Document Level IE (Distant Learning/ Bootstrapping) QA Answer Level Pattern Classifier Rules Knowledge Base Redundancy Removal Answers Query Expansion 1153 Methods System Examples Trained Pattern Distant Learning (large CUNY (Chen et al., 2010) IE Learning seed, one iteration) Bootstrapping (small NYU (Grishman and Min, 2010) seed, multiple iterations) Supervised Distant Supervision Budapestacad (Nemeskey et al., 2010), lsv (Chrupala et al., Classifier 2010), Stanford (Surdeanu et al., 2010), UBC (Intxaurrondo et al., 2010) Trained from KBP train- BuptPris (Gao et al., 2010), CUNY (Chen et al., 2010), IBM ing data and other re- (Castelli et al., 2010), ICL (Song et al., 2010), LCC lated tasks (Lehmann et al., 2010), lsv (Chrupala et al., 2010), Siel (Bysani et al., 2010) QA CUNY (Chen et al., 2010), iirg (Byrne and Dunnion, 2010) Hand-coded Heuristic Rules BuptPris (Gao et al., 2010), USFD (Yu et al., 2010) Table </context>
<context position="24775" citStr="Grishman and Min, 2010" startWordPosition="3930" endWordPosition="3933"> (e.g. LCC, IBM). For example, IBM (Castelli et al., 2010) extended their mention detection component to cover 36 entity types which include many non-ACE types; and added new relation types between entities and event anchors. LCC and CUNY applied active learning techniques to cover non-ACE types of entities, such as “origin”, “religion”, “title”, “charge”, “web-site” and “cause-of-death”, and effectively develop lexicons to filter spurious answers. Top systems also benefited from customizing and tightly integrating their recently enhanced extraction techniques into KBP. For example, IBM, NYU (Grishman and Min, 2010) and CUNY exploited entity coreference in pattern learning and reasoning. It is also notable that traditional extraction components trained from newswire data suffer from noise in web data. In order to address this problem, IBM applied their new robust mention detection techniques for noisy inputs (Florian et al., 2010); CUNY developed a component to recover structured forms such as tables in web data automatically and filter spurious answers. 5.2 Use of External Knowledge Base Many instance-centered knowledge bases that have harvested Wikipedia are proliferating on the semantic web. The most </context>
</contexts>
<marker>Grishman, Min, 2010</marker>
<rawString>Ralph Grishman and Bonan Min. 2010. New York University KBP 2010 Slot-Filling System. Proc. TAC 2010 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ander Intxaurrondo</author>
</authors>
<title>Oier Lopez de Lacalle and Eneko Agirre.</title>
<date>2010</date>
<booktitle>UBC at Slot Filling TAC-KBP2010. Proc. TAC 2010 Workshop.</booktitle>
<marker>Intxaurrondo, 2010</marker>
<rawString>Ander Intxaurrondo, Oier Lopez de Lacalle and Eneko Agirre. 2010. UBC at Slot Filling TAC-KBP2010. Proc. TAC 2010 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lehmann</author>
<author>Sean Monahan</author>
<author>Luke Nezda</author>
<author>Arnold Jung</author>
<author>Ying Shi</author>
</authors>
<date>2010</date>
<booktitle>LCC Approaches to Knowledge Base Population at TAC 2010. Proc. TAC 2010 Workshop.</booktitle>
<contexts>
<context position="10066" citStr="Lehmann et al., 2010" startWordPosition="1608" endWordPosition="1611">date Ranking Query IR Source doc Coreference Resolution Answer supervised classification Graph -based Wiki KB +Texts IR Methods System Examples System Ranking Range Query Wikipedia Hyperlink Mining CUNY (Chen et al., 2010), NUSchime (Zhang et al., [2, 15] Expansion 2010), Siel (Bysani et al., 2010), SMU-SIS (Gottipati et al., 2010), USFD (Yu et al., 2010), WebTLab team (Fernandez et al., 2010) Source document coreference CUNY (Chen et al., 2010) 9 resolution Candidate Document semantic analysis ARPANI (Thomas et al., 2010), CUNY (Chen et al., [1,14] Generation and context modeling 2010), LCC (Lehmann et al., 2010) IR CUNY (Chen et al., 2010), Budapestacad (Nemeskey et [9, 16] al., 2010), USFD (Yu et al., 2010) Candidate Unsupervised Similarity CUNY (Chen et al., 2010), SMU-SIS (Gottipati et al., [9, 14] Ranking Computation (e.g. VSM) 2010), USFD (Yu et al., 2010) Supervised LCC (Lehmann et al., 2010), NUSchime (Zhang et al., [1, 10] Classification 2010), Stanford-UBC (Chang et al., 2010), HLTCOE (McNamee, 2010), UC3M (Pablo-Sanchez et al., 2010) Rule-based LCC (Lehmann et al., 2010), BuptPris (Gao et al., 2010) [1, 8] Global Graph-based Ranking CMCRC (Radford et al., 2010) 3 IR Budapestacad (Nemeskey e</context>
<context position="12440" citStr="Lehmann et al., 2010" startWordPosition="1973" endWordPosition="1976">earned from the annotated training data based on many different features. (3). Graph-based ranking, in which context entities are taken into account in order to reach a global optimized solution together with the query entity. (4). IR (Information Retrieval) approach, in which the entire background source document is considered as a single query to retrieve the most relevant Wikipedia article. The first question we will investigate is how much higher performance can be achieved by using supervised learning? Among the 16 entity linking systems which participated in the regular evaluation, LCC (Lehmann et al., 2010), HLTCOE (McNamee, 2010), Stanford-UBC (Chang et al., 2010), NUSchime (Zhang et al., 2010) and UC3M (Pablo-Sanchez et al., 2010) have explicitly used supervised classification based on many lexical and name tagging features, and most of them are ranked in top 6 in the evaluation. Therefore we can conclude that supervised learning normally leads to a reasonably good performance. However, a highperforming entity linking system can also be implemented in an unsupervised fashion by exploiting effective characteristics and algorithms, as we will discuss in the next sections. 3.4 Semantic Relation F</context>
<context position="22741" citStr="Lehmann et al., 2010" startWordPosition="3599" endWordPosition="3602">Base Redundancy Removal Answers Query Expansion 1153 Methods System Examples Trained Pattern Distant Learning (large CUNY (Chen et al., 2010) IE Learning seed, one iteration) Bootstrapping (small NYU (Grishman and Min, 2010) seed, multiple iterations) Supervised Distant Supervision Budapestacad (Nemeskey et al., 2010), lsv (Chrupala et al., Classifier 2010), Stanford (Surdeanu et al., 2010), UBC (Intxaurrondo et al., 2010) Trained from KBP train- BuptPris (Gao et al., 2010), CUNY (Chen et al., 2010), IBM ing data and other re- (Castelli et al., 2010), ICL (Song et al., 2010), LCC lated tasks (Lehmann et al., 2010), lsv (Chrupala et al., 2010), Siel (Bysani et al., 2010) QA CUNY (Chen et al., 2010), iirg (Byrne and Dunnion, 2010) Hand-coded Heuristic Rules BuptPris (Gao et al., 2010), USFD (Yu et al., 2010) Table 3. Slot Filling Answer Extraction Method Comparison On the other hand, there were a lot of &apos;facts&apos; available – pairs of entities bearing a relationship corresponding closely to the KBP relations – in the form of filled Wikipedia infoboxes. These could be used for various forms of indirect or distant learning, where instances in a large corpus of such pairs are taken as (positive) training insta</context>
</contexts>
<marker>Lehmann, Monahan, Nezda, Jung, Shi, 2010</marker>
<rawString>John Lehmann, Sean Monahan, Luke Nezda, Arnold Jung and Ying Shi. 2010. LCC Approaches to Knowledge Base Population at TAC 2010. Proc. TAC 2010 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul McNamee</author>
<author>Hoa Dang</author>
</authors>
<title>Knowledge Base Population Track.</title>
<date>2009</date>
<journal>Overview of the TAC</journal>
<booktitle>Proc. TAC</booktitle>
<note>Workshop.</note>
<marker>McNamee, Dang, 2009</marker>
<rawString>Paul McNamee and Hoa Dang. 2009. Overview of the TAC 2009 Knowledge Base Population Track. Proc. TAC 2009 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul McNamee</author>
<author>Hoa Trang Dang</author>
<author>Heather Simpson</author>
<author>Patrick Schone</author>
<author>Stephanie M Strassel</author>
</authors>
<title>An Evaluation of Technologies for Knowledge Base Population.</title>
<date>2010</date>
<booktitle>Proc. LREC2010.</booktitle>
<marker>McNamee, Dang, Simpson, Schone, Strassel, 2010</marker>
<rawString>Paul McNamee, Hoa Trang Dang, Heather Simpson, Patrick Schone and Stephanie M. Strassel. 2010. An Evaluation of Technologies for Knowledge Base Population. Proc. LREC2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul McNamee</author>
<author>Rion Snow</author>
<author>Patrick Schone</author>
<author>James Mayfield</author>
</authors>
<title>Learning Named Entity Hyponyms for Question Answering.</title>
<date>2008</date>
<booktitle>Proc. IJCNLP2008.</booktitle>
<contexts>
<context position="2284" citStr="McNamee et al., 2008" startWordPosition="351" endWordPosition="354">y the relevant documents and to integrate facts, possibly redundant, possibly complementary, possibly in conflict, coming from these documents. Furthermore, we may want to use the extracted information to augment an existing data base. This requires the ability to link individuals mentioned in a document, and information about these individuals, to entries in the data base. On the other hand, traditional Question Answering (QA) evaluations made limited efforts at disambiguating entities in queries (e.g. Pizzato et al., 2006), and limited use of relation/event extraction in answer search (e.g. McNamee et al., 2008). The Knowledge Base Population (KBP) shared task, conducted as part of the NIST Text Analysis Conference, aims to address and evaluate these capabilities, and bridge the IE and QA communities to promote research in discovering facts about entities and expanding a knowledge base with these facts. KBP is done through two separate subtasks, Entity Linking and Slot Filling; in 2010, 23 teams submitted results for one or both sub-tasks. A variety of approaches have been proposed to address both tasks with considerable success; nevertheless, there are many aspects of the task that remain unclear. W</context>
</contexts>
<marker>McNamee, Snow, Schone, Mayfield, 2008</marker>
<rawString>Paul McNamee, Rion Snow, Patrick Schone and James Mayfield. 2008. Learning Named Entity Hyponyms for Question Answering. Proc. IJCNLP2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul McNamee</author>
</authors>
<date>2010</date>
<booktitle>HLTCOE Efforts in Entity Linking at TAC KBP 2010. Proc. TAC 2010 Workshop.</booktitle>
<contexts>
<context position="10471" citStr="McNamee, 2010" startWordPosition="1674" endWordPosition="1675"> coreference CUNY (Chen et al., 2010) 9 resolution Candidate Document semantic analysis ARPANI (Thomas et al., 2010), CUNY (Chen et al., [1,14] Generation and context modeling 2010), LCC (Lehmann et al., 2010) IR CUNY (Chen et al., 2010), Budapestacad (Nemeskey et [9, 16] al., 2010), USFD (Yu et al., 2010) Candidate Unsupervised Similarity CUNY (Chen et al., 2010), SMU-SIS (Gottipati et al., [9, 14] Ranking Computation (e.g. VSM) 2010), USFD (Yu et al., 2010) Supervised LCC (Lehmann et al., 2010), NUSchime (Zhang et al., [1, 10] Classification 2010), Stanford-UBC (Chang et al., 2010), HLTCOE (McNamee, 2010), UC3M (Pablo-Sanchez et al., 2010) Rule-based LCC (Lehmann et al., 2010), BuptPris (Gao et al., 2010) [1, 8] Global Graph-based Ranking CMCRC (Radford et al., 2010) 3 IR Budapestacad (Nemeskey et al., 2010) 16 Table 1. Entity Linking Method Comparison 1150 (2). Candidate Ranking: Stanford-UBC used Wikipedia hyperlinks (clarification, disambiguation, title) for query re-mapping, and encoded lexical and part-of-speech features from Wikipedia articles containing hyperlinks to the queries to train a supervised classifier; they reported a significant improvement on micro-averaged accuracy, from 74</context>
<context position="12464" citStr="McNamee, 2010" startWordPosition="1978" endWordPosition="1979">ing data based on many different features. (3). Graph-based ranking, in which context entities are taken into account in order to reach a global optimized solution together with the query entity. (4). IR (Information Retrieval) approach, in which the entire background source document is considered as a single query to retrieve the most relevant Wikipedia article. The first question we will investigate is how much higher performance can be achieved by using supervised learning? Among the 16 entity linking systems which participated in the regular evaluation, LCC (Lehmann et al., 2010), HLTCOE (McNamee, 2010), Stanford-UBC (Chang et al., 2010), NUSchime (Zhang et al., 2010) and UC3M (Pablo-Sanchez et al., 2010) have explicitly used supervised classification based on many lexical and name tagging features, and most of them are ranked in top 6 in the evaluation. Therefore we can conclude that supervised learning normally leads to a reasonably good performance. However, a highperforming entity linking system can also be implemented in an unsupervised fashion by exploiting effective characteristics and algorithms, as we will discuss in the next sections. 3.4 Semantic Relation Features Almost all entit</context>
</contexts>
<marker>McNamee, 2010</marker>
<rawString>Paul McNamee. 2010. HLTCOE Efforts in Entity Linking at TAC KBP 2010. Proc. TAC 2010 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danielle S McNamara</author>
</authors>
<title>Reading both Highcoherence and Low-coherence Texts: Effects of Text Sequence and Prior Knowledge.</title>
<date>2001</date>
<journal>Canadian Journal of Experimental Psychology.</journal>
<contexts>
<context position="14677" citStr="McNamara, 2001" startWordPosition="2335" endWordPosition="2336">In the current setting of KBP, a set of target entities is provided to each system in order to simplify the task and its evaluation, because it’s not feasible to require a system to generate answers for all possible entities in the entire source collection. However, ideally a fully-automatic KBP system should be able to automatically discover novel entities (“queries”) which have no KB entry or few slot fills in the KB, extract their attributes, and conduct global reasoning over these attributes in order to generate the final output. At the very least, due to the semantic coherence principle (McNamara, 2001), the information of an entity depends on the information of other entities. For example, the WebTLab team and the CMCRC team extracted all entities in the context of a given query, and disambiguated all entities at the same time using a PageRank-like algorithm (Page et al., 1998) or a Graph-based Re-ranking algorithm. The SMU-SIS team (Gottipati and Jiang, 2010) re-formulated queries using contexts. The LCC team modeled 1151 contexts using Wikipedia page concepts, and computed linkability scores iteratively. Consistent improvements were reported by the WebTLab system (from 63.64% to 66.58%). </context>
</contexts>
<marker>McNamara, 2001</marker>
<rawString>Danielle S McNamara. 2001. Reading both Highcoherence and Low-coherence Texts: Effects of Text Sequence and Prior Knowledge. Canadian Journal of Experimental Psychology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olena Medelyan</author>
<author>Catherine Legg</author>
<author>David Milne</author>
<author>Ian H Witten</author>
</authors>
<title>Mining Meaning from Wikipedia.</title>
<date>2009</date>
<journal>International Journal of Human-Computer Studies archive. Volume 67 , Issue</journal>
<volume>9</volume>
<contexts>
<context position="8794" citStr="Medelyan et al., 2009" startWordPosition="1416" endWordPosition="1419">cher set of forms using Wikipedia structure mining or coreference resolution in the background document. (2) candidate generation – finding all possible KB entries that a query might link to; (3) candidate ranking – rank the probabilities of all candidates and NIL answer. Table 1 summarizes the systems which exploited different approaches at each step. In the following subsections we will highlight the new and effective techniques used in entity linking. 3.2 Wilipedia Structure Mining Wikipedia articles are peppered with structured information and hyperlinks to other (on average 25) articles (Medelyan et al., 2009). Such information provides additional sources for entity linking: (1). Query Expansion: For example, WebTLab (Fernandez et al., 2010) used Wikipedia link structure (source, anchors, redirects and disambiguation) to extend the KB and compute entity cooccurrence estimates. Many other teams including CUNY and Siel used redirect pages and disambiguation pages for query expansion. The Siel team also exploited bold texts from first paragraphs because they often contain nicknames, alias names and full names. KB Node Candidate Generation unsupervised similarity computation Document semantic analysis </context>
</contexts>
<marker>Medelyan, Legg, Milne, Witten, 2009</marker>
<rawString>Olena Medelyan, Catherine Legg, David Milne and Ian H. Witten. 2009. Mining Meaning from Wikipedia. International Journal of Human-Computer Studies archive. Volume 67 , Issue 9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Nemeskey</author>
</authors>
<title>Gabor Recski, Attila Zseder and Andras Kornai.</title>
<date>2010</date>
<booktitle>BUDAPESTACAD at TAC 2010. Proc. TAC 2010 Workshop.</booktitle>
<marker>Nemeskey, 2010</marker>
<rawString>David Nemeskey, Gabor Recski, Attila Zseder and Andras Kornai. 2010. BUDAPESTACAD at TAC 2010. Proc. TAC 2010 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cesar de Pablo-Sanchez</author>
<author>Juan Perea</author>
<author>Paloma Martinez</author>
</authors>
<title>Combining Similarities with Regression based Classifiers for Entity Linking at TAC</title>
<date>2010</date>
<booktitle>Proc. TAC 2010 Workshop.</booktitle>
<marker>de Pablo-Sanchez, Perea, Martinez, 2010</marker>
<rawString>Cesar de Pablo-Sanchez, Juan Perea and Paloma Martinez. 2010. Combining Similarities with Regression based Classifiers for Entity Linking at TAC 2010. Proc. TAC 2010 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Page</author>
<author>Sergey Brin</author>
<author>Rajeev Motwani</author>
<author>Terry Winograd</author>
</authors>
<title>The PageRank Citation Ranking: Bringing Order to the Web.</title>
<date>1998</date>
<booktitle>Proc. the 7th International World Wide Web Conference.</booktitle>
<contexts>
<context position="14958" citStr="Page et al., 1998" startWordPosition="2382" endWordPosition="2385">ully-automatic KBP system should be able to automatically discover novel entities (“queries”) which have no KB entry or few slot fills in the KB, extract their attributes, and conduct global reasoning over these attributes in order to generate the final output. At the very least, due to the semantic coherence principle (McNamara, 2001), the information of an entity depends on the information of other entities. For example, the WebTLab team and the CMCRC team extracted all entities in the context of a given query, and disambiguated all entities at the same time using a PageRank-like algorithm (Page et al., 1998) or a Graph-based Re-ranking algorithm. The SMU-SIS team (Gottipati and Jiang, 2010) re-formulated queries using contexts. The LCC team modeled 1151 contexts using Wikipedia page concepts, and computed linkability scores iteratively. Consistent improvements were reported by the WebTLab system (from 63.64% to 66.58%). 4 Entity Linking: Remaining Challenges 4.1 Comparison with Traditional Crossdocument Coreference Resolution Part of the entity linking task can be modeled as a cross-document entity resolution problem which includes two principal challenges: the same entity can be referred to by m</context>
</contexts>
<marker>Page, Brin, Motwani, Winograd, 1998</marker>
<rawString>Lawrence Page, Sergey Brin, Rajeev Motwani and Terry Winograd. 1998. The PageRank Citation Ranking: Bringing Order to the Web. Proc. the 7th International World Wide Web Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luiz Augusto Pizzato</author>
<author>Diego Molla</author>
<author>Cecile Paris</author>
</authors>
<title>Pseudo Relevance Feedback Using Named Entities for Question Answering.</title>
<date>2006</date>
<booktitle>Proc. the Australasian Language Technology Workshop</booktitle>
<contexts>
<context position="2193" citStr="Pizzato et al., 2006" startWordPosition="337" endWordPosition="340">s scattered among the documents of a large collection. This requires the ability to identify the relevant documents and to integrate facts, possibly redundant, possibly complementary, possibly in conflict, coming from these documents. Furthermore, we may want to use the extracted information to augment an existing data base. This requires the ability to link individuals mentioned in a document, and information about these individuals, to entries in the data base. On the other hand, traditional Question Answering (QA) evaluations made limited efforts at disambiguating entities in queries (e.g. Pizzato et al., 2006), and limited use of relation/event extraction in answer search (e.g. McNamee et al., 2008). The Knowledge Base Population (KBP) shared task, conducted as part of the NIST Text Analysis Conference, aims to address and evaluate these capabilities, and bridge the IE and QA communities to promote research in discovering facts about entities and expanding a knowledge base with these facts. KBP is done through two separate subtasks, Entity Linking and Slot Filling; in 2010, 23 teams submitted results for one or both sub-tasks. A variety of approaches have been proposed to address both tasks with co</context>
</contexts>
<marker>Pizzato, Molla, Paris, 2006</marker>
<rawString>Luiz Augusto Pizzato, Diego Molla and Cecile Paris. 2006. Pseudo Relevance Feedback Using Named Entities for Question Answering. Proc. the Australasian Language Technology Workshop 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Prager</author>
<author>P Duboue</author>
<author>J Chu-Carroll</author>
</authors>
<title>Improving QA Accuracy by Question Inversion.</title>
<date>2006</date>
<booktitle>Proc. ACLCOLING</booktitle>
<contexts>
<context position="26975" citStr="Prager et al., 2006" startWordPosition="4274" endWordPosition="4277"> and LCC observed limited gains from the answer validation approach from Freebase. Both systems gained about 1% improvement in recall with a slight loss in precision. 5.3 Cross-Slot and Cross-Query Reasoning Slot Filling can also benefit from extracting revertible queries from the context of any target query, and conducting global ranking or reasoning to refine the results. CUNY and IBM developed recursive reasoning components to refine extraction results. For a given query, if there are no other related answer candidates available, they built &amp;quot;revertible” queries in the contexts, similar to (Prager et al., 2006), to enrich the inference process iteratively. For example, if a is extracted as the answer for org:subsidiaries of the query q, we can consider a as a new revertible query and verify that a org:parents answer of a is q. Both systems significantly benefited from recursive reasoning (CUNY F-measure on training data was enhanced from 33.57% to 35.29% and IBM F-measure was enhanced from 26% to 34.83%). 6 Slot Filling: Remaining Challenges Slot filling remains a very challenging task; only one system exceeded 30% F-measure on the 2010 evaluation. During the 2010 evaluation data annotation/adjudica</context>
</contexts>
<marker>Prager, Duboue, Chu-Carroll, 2006</marker>
<rawString>J. Prager, P. Duboue, and J. Chu-Carroll. 2006. Improving QA Accuracy by Question Inversion. Proc. ACLCOLING 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Will Radford</author>
<author>Ben Hachey</author>
<author>Joel Nothman</author>
<author>Matthew Honnibal</author>
<author>James R Curran</author>
</authors>
<title>CMCRC at TAC10: Document-level Entity Linking with Graphbased Re-ranking.</title>
<date>2010</date>
<booktitle>Proc. TAC 2010 Workshop.</booktitle>
<contexts>
<context position="10636" citStr="Radford et al., 2010" startWordPosition="1698" endWordPosition="1701"> context modeling 2010), LCC (Lehmann et al., 2010) IR CUNY (Chen et al., 2010), Budapestacad (Nemeskey et [9, 16] al., 2010), USFD (Yu et al., 2010) Candidate Unsupervised Similarity CUNY (Chen et al., 2010), SMU-SIS (Gottipati et al., [9, 14] Ranking Computation (e.g. VSM) 2010), USFD (Yu et al., 2010) Supervised LCC (Lehmann et al., 2010), NUSchime (Zhang et al., [1, 10] Classification 2010), Stanford-UBC (Chang et al., 2010), HLTCOE (McNamee, 2010), UC3M (Pablo-Sanchez et al., 2010) Rule-based LCC (Lehmann et al., 2010), BuptPris (Gao et al., 2010) [1, 8] Global Graph-based Ranking CMCRC (Radford et al., 2010) 3 IR Budapestacad (Nemeskey et al., 2010) 16 Table 1. Entity Linking Method Comparison 1150 (2). Candidate Ranking: Stanford-UBC used Wikipedia hyperlinks (clarification, disambiguation, title) for query re-mapping, and encoded lexical and part-of-speech features from Wikipedia articles containing hyperlinks to the queries to train a supervised classifier; they reported a significant improvement on micro-averaged accuracy, from 74.85% to 82.15%. In fact, when the mined attributes become rich enough, they can be used as an expanded query and sent into an information retrieval engine in order t</context>
</contexts>
<marker>Radford, Hachey, Nothman, Honnibal, Curran, 2010</marker>
<rawString>Will Radford, Ben Hachey, Joel Nothman, Matthew Honnibal and James R. Curran. 2010. CMCRC at TAC10: Document-level Entity Linking with Graphbased Re-ranking. Proc. TAC 2010 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Song</author>
</authors>
<title>Zhengyan He and Houfeng Wang.</title>
<date>2010</date>
<booktitle>Proc. TAC 2010 Workshop.</booktitle>
<marker>Song, 2010</marker>
<rawString>Yang Song, Zhengyan He and Houfeng Wang. 2010. ICL_KBP Approaches to Knowledge Base Population at TAC2010. Proc. TAC 2010 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F M Suchanek</author>
<author>G Kasneci</author>
<author>G Weikum</author>
</authors>
<title>Yago: A Core of Semantic Knowledge.</title>
<date>2007</date>
<booktitle>Proc. 16th International World Wide Web Conference.</booktitle>
<contexts>
<context position="25521" citStr="Suchanek et al., 2007" startWordPosition="4046" endWordPosition="4049">ponents trained from newswire data suffer from noise in web data. In order to address this problem, IBM applied their new robust mention detection techniques for noisy inputs (Florian et al., 2010); CUNY developed a component to recover structured forms such as tables in web data automatically and filter spurious answers. 5.2 Use of External Knowledge Base Many instance-centered knowledge bases that have harvested Wikipedia are proliferating on the semantic web. The most well known are probably the Wikipedia derived resources, including DBpedia (Auer 2007), Freebase (Bollacker 2008) and YAGO (Suchanek et al., 2007) and Linked Open Data (http://data.nytimes.com/). The main motivation of the KBP program is to automatically distill information from news and web unstructured data instead of manually constructed knowledge bases, but these existing knowledge bases can provide a large number of seed tuples to bootstrap slot filling or guide distant learning. Such resources can also be used in a more direct way. For example, CUNY exploited Freebase and LCC exploited DBpedia as fact validation in slot filling. However, most of these resources are manually created from single data modalities and only cover well-k</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>F. M. Suchanek, G. Kasneci, and G. Weikum. 2007. Yago: A Core of Semantic Knowledge. Proc. 16th International World Wide Web Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>David McClosky</author>
<author>Julie Tibshirani</author>
<author>John Bauer</author>
<author>Angel X Chang</author>
<author>Valentin I Spitkovsky</author>
<author>Christopher D Manning</author>
</authors>
<title>A Simple Distant Supervision Approach for the TAC-KBP Slot Filling Task.</title>
<date>2010</date>
<booktitle>Proc. TAC 2010 Workshop.</booktitle>
<contexts>
<context position="22513" citStr="Surdeanu et al., 2010" startWordPosition="3558" endWordPosition="3561"> General Slot Filling System Architecture Query Source Collection Training Data/ External KB IR, QA IR Sentence/Passage Level Document Level IE (Distant Learning/ Bootstrapping) QA Answer Level Pattern Classifier Rules Knowledge Base Redundancy Removal Answers Query Expansion 1153 Methods System Examples Trained Pattern Distant Learning (large CUNY (Chen et al., 2010) IE Learning seed, one iteration) Bootstrapping (small NYU (Grishman and Min, 2010) seed, multiple iterations) Supervised Distant Supervision Budapestacad (Nemeskey et al., 2010), lsv (Chrupala et al., Classifier 2010), Stanford (Surdeanu et al., 2010), UBC (Intxaurrondo et al., 2010) Trained from KBP train- BuptPris (Gao et al., 2010), CUNY (Chen et al., 2010), IBM ing data and other re- (Castelli et al., 2010), ICL (Song et al., 2010), LCC lated tasks (Lehmann et al., 2010), lsv (Chrupala et al., 2010), Siel (Bysani et al., 2010) QA CUNY (Chen et al., 2010), iirg (Byrne and Dunnion, 2010) Hand-coded Heuristic Rules BuptPris (Gao et al., 2010), USFD (Yu et al., 2010) Table 3. Slot Filling Answer Extraction Method Comparison On the other hand, there were a lot of &apos;facts&apos; available – pairs of entities bearing a relationship corresponding clo</context>
<context position="33327" citStr="Surdeanu et al., 2010" startWordPosition="5285" endWordPosition="5289">igher F-score than the best of these systems. 8 Conclusion Compared to traditional IE and QA tasks, KBP has raised some interesting and important research issues: It places more emphasis on cross-document entity resolution which received limited effort in ACE; it forces systems to deal with redundant and conflicting answers across large corpora; it links the facts in text to a knowledge base so that NLP and data mining/database communities have a better chance to collaborate; it provides opportunities to develop novel training methods such as distant (and noisy) supervision through Infoboxes (Surdeanu et al., 2010; Chen et al., 2010). In this paper, we provided detailed analysis of the reasons which have made KBP a more challenging task, shared our observations and lessons learned from the evaluation, and suggested some possible research directions to address these challenges which may be helpful for current and new participants, or IE and QA researchers in general. Acknowledgements The first author was supported by the U.S. Army Research Laboratory under Cooperative Agreement Number W911NF-09-2-0053, the U.S. NSF CAREER Award under Grant IIS-0953149 and PSC-CUNY Research Program. The views and conclus</context>
</contexts>
<marker>Surdeanu, McClosky, Tibshirani, Bauer, Chang, Spitkovsky, Manning, 2010</marker>
<rawString>Mihai Surdeanu, David McClosky, Julie Tibshirani, John Bauer, Angel X. Chang, Valentin I. Spitkovsky, Christopher D. Manning. 2010. A Simple Distant Supervision Approach for the TAC-KBP Slot Filling Task. Proc. TAC 2010 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Thomas</author>
<author>Arpana Rawai</author>
<author>M K Kowar</author>
<author>Sanjay Sharma</author>
</authors>
<title>Sarang Pitale and Neeraj Kharya.</title>
<date>2010</date>
<booktitle>Bhilai Institute of Technology Durg at TAC 2010: Knowledge Base Population Task Challenge. Proc. TAC 2010 Workshop.</booktitle>
<contexts>
<context position="9973" citStr="Thomas et al., 2010" startWordPosition="1593" endWordPosition="1596">y computation Document semantic analysis Wiki hyperlink mining Query Expansion KB Node Candidate Ranking Query IR Source doc Coreference Resolution Answer supervised classification Graph -based Wiki KB +Texts IR Methods System Examples System Ranking Range Query Wikipedia Hyperlink Mining CUNY (Chen et al., 2010), NUSchime (Zhang et al., [2, 15] Expansion 2010), Siel (Bysani et al., 2010), SMU-SIS (Gottipati et al., 2010), USFD (Yu et al., 2010), WebTLab team (Fernandez et al., 2010) Source document coreference CUNY (Chen et al., 2010) 9 resolution Candidate Document semantic analysis ARPANI (Thomas et al., 2010), CUNY (Chen et al., [1,14] Generation and context modeling 2010), LCC (Lehmann et al., 2010) IR CUNY (Chen et al., 2010), Budapestacad (Nemeskey et [9, 16] al., 2010), USFD (Yu et al., 2010) Candidate Unsupervised Similarity CUNY (Chen et al., 2010), SMU-SIS (Gottipati et al., [9, 14] Ranking Computation (e.g. VSM) 2010), USFD (Yu et al., 2010) Supervised LCC (Lehmann et al., 2010), NUSchime (Zhang et al., [1, 10] Classification 2010), Stanford-UBC (Chang et al., 2010), HLTCOE (McNamee, 2010), UC3M (Pablo-Sanchez et al., 2010) Rule-based LCC (Lehmann et al., 2010), BuptPris (Gao et al., 2010)</context>
</contexts>
<marker>Thomas, Rawai, Kowar, Sharma, 2010</marker>
<rawString>Ani Thomas, Arpana Rawai, M K Kowar, Sanjay Sharma, Sarang Pitale and Neeraj Kharya. 2010. Bhilai Institute of Technology Durg at TAC 2010: Knowledge Base Population Task Challenge. Proc. TAC 2010 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingtao Yu</author>
</authors>
<title>Omkar Mujgond and Rob Gaizauskas.</title>
<date>2010</date>
<booktitle>The University of Sheffield System at TAC KBP 2010. Proc. TAC 2010 Workshop.</booktitle>
<marker>Yu, 2010</marker>
<rawString>Jingtao Yu, Omkar Mujgond and Rob Gaizauskas. 2010. The University of Sheffield System at TAC KBP 2010. Proc. TAC 2010 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Zhang</author>
<author>Yan Chuan Sim</author>
<author>Jian Su</author>
<author>Chew Lim Tan</author>
</authors>
<title>NUS-I2R: Learning a Combined System for Entity Linking.</title>
<date>2010</date>
<booktitle>Proc. TAC 2010 Workshop.</booktitle>
<contexts>
<context position="12530" citStr="Zhang et al., 2010" startWordPosition="1986" endWordPosition="1989">anking, in which context entities are taken into account in order to reach a global optimized solution together with the query entity. (4). IR (Information Retrieval) approach, in which the entire background source document is considered as a single query to retrieve the most relevant Wikipedia article. The first question we will investigate is how much higher performance can be achieved by using supervised learning? Among the 16 entity linking systems which participated in the regular evaluation, LCC (Lehmann et al., 2010), HLTCOE (McNamee, 2010), Stanford-UBC (Chang et al., 2010), NUSchime (Zhang et al., 2010) and UC3M (Pablo-Sanchez et al., 2010) have explicitly used supervised classification based on many lexical and name tagging features, and most of them are ranked in top 6 in the evaluation. Therefore we can conclude that supervised learning normally leads to a reasonably good performance. However, a highperforming entity linking system can also be implemented in an unsupervised fashion by exploiting effective characteristics and algorithms, as we will discuss in the next sections. 3.4 Semantic Relation Features Almost all entity linking systems have used semantic relations as features (e.g. B</context>
</contexts>
<marker>Zhang, Sim, Su, Tan, 2010</marker>
<rawString>Wei Zhang, Yan Chuan Sim, Jian Su and Chew Lim Tan. 2010. NUS-I2R: Learning a Combined System for Entity Linking. Proc. TAC 2010 Workshop.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>