<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001780">
<title confidence="0.997077">
Unsupervised Parse Selection for HPSG
</title>
<author confidence="0.996568">
Rebecca Dridan and Timothy Baldwin
</author>
<affiliation confidence="0.9995585">
Dept. of Computer Science and Software Engineering
University of Melbourne, Australia
</affiliation>
<email confidence="0.997414">
rdridan@csse.unimelb.edu.au, tb@ldwin.net
</email>
<sectionHeader confidence="0.995644" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9993131">
Parser disambiguation with precision gram-
mars generally takes place via statistical rank-
ing of the parse yield of the grammar using
a supervised parse selection model. In the
standard process, the parse selection model is
trained over a hand-disambiguated treebank,
meaning that without a significant investment
of effort to produce the treebank, parse selec-
tion is not possible. Furthermore, as treebank-
ing is generally streamlined with parse selec-
tion models, creating the initial treebank with-
out a model requires more resources than sub-
sequent treebanks. In this work, we show that,
by taking advantage of the constrained nature
of these BPSG grammars, we can learn a dis-
criminative parse selection model from raw
text in a purely unsupervised fashion. This al-
lows us to bootstrap the treebanking process
and provide better parsers faster, and with less
resources.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999940456521739">
Parsing with precision grammars is generally a two-
stage process: (1) the full parse yield of the preci-
sion grammar is calculated for a given item, often
in the form of a packed forest for efficiency (Oepen
and Carroll, 2000; Zhang et al., 2007); and (2) the
individual analyses in the parse forest are ranked us-
ing a statistical model (“parse selection”). In the do-
main of treebank parsing, the Charniak and Johnson
(2005) reranking parser adopts an analogous strat-
egy, except that ranking and pruning are incorpo-
rated into the first stage, and the second stage is
based on only the top-ranked parses from the first
stage. For both styles of parsing, however, parse se-
lection is based on a statistical model learned from a
pre-existing treebank associated with the grammar.
Our interest in this paper is in completely remov-
ing this requirement of parse selection on explicitly
treebanked data, ie the development of fully unsu-
pervised parse selection models.
The particular style of precision grammar we ex-
periment with in this paper is HPSG (Pollard and
Sag, 1994), in the form of the DELPH-IN suite
of grammars (http://www.delph-in.net/).
One of the main focuses of the DELPH-IN collab-
oration effort is multilinguality. To this end, the
Grammar Matrix project (Bender et al., 2002) has
been developed which, through a set of question-
naires, allows grammar engineers to quickly pro-
duce a core grammar for a language of their choice.
Bender (2008) showed that by using and expanding
on this core grammar, she was able to produce a
broad-coverage precision grammar of Wambaya in
a very short amount of time. However, the Gram-
mar Matrix can only help with the first stage of pars-
ing. The statistical model used in the second stage
of parsing (ie parse selection) requires a treebank to
learn the features, but as we explain in Section 2, the
treebanks are created by parsing, preferably with a
statistical model. In this work, we look at methods
for bootstrapping the production of these statistical
models without having an annotated treebank. Since
many of the languages that people are building new
grammars for are under-resourced, we can’t depend
on having any external information or NLP tools,
and so the methods we examine are purely unsuper-
vised, using nothing more than the grammars them-
</bodyText>
<page confidence="0.979841">
694
</page>
<note confidence="0.8179435">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 694–704,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.998990428571429">
selves and raw text. We find that, not only can we
produce models that are suitable for kick-starting the
treebanking process, but the accuracy of these mod-
els is comparable to parsers trained on gold standard
data (Clark and Curran, 2007b; Miyao and Tsujii,
2008), which have been successfully used in appli-
cations (Miyao et al., 2008).
</bodyText>
<sectionHeader confidence="0.943692" genericHeader="method">
2 The problem
</sectionHeader>
<bodyText confidence="0.999682952380953">
The current method of training a parse selection
model uses the [incr tsdb()] treebanking mechanism
(Oepen, 2001) and works well for updating models
for mature grammars, although even for these gram-
mars, building a new model for a different domain
requires a time-consuming initial treebanking effort.
The treebanks used with DELPH-IN grammars are
dynamic treebanks (Oepen et al., 2004) created by
parsing text and having an annotator select the cor-
rect analysis (or discard all of them). The annotation
process involves making binary decisions based on
so-called parse discriminants (Carter, 1997). When-
ever the grammar is changed, the treebank can be
quickly updated by re-parsing and re-applying the
old annotation decisions. This treebanking process
not only produces gold standard trees, but also a set
of non-gold trees which provides the negative train-
ing data necessary for a discriminative maximum en-
tropy model.
The standard process for creating a parse selection
model is:
</bodyText>
<listItem confidence="0.985127">
1. parse the training set, recording up to 500
highest-ranking parses for each sentence;
2. treebank the training set;
3. extract features from the gold and non-gold
parses;
4. learn feature weights using the TADM toolkit.l
(Malouf, 2002)
</listItem>
<bodyText confidence="0.9812605">
The useful training data from this process is the
parses from those sentences for which: more than
one parse was found; and at least one parse has been
annotated as correct. That is, there needs to be both
gold and non-gold trees for any sentence to be used
in training the discriminative model.
lhttp://tadm.sourceforge.net/
There are two issues with this process for new
grammars. Firstly, treebanking takes many person-
hours, and is hence both time-consuming and ex-
pensive. Complicating that is the second issue: N-
best parsing requires a statistical model. While it is
possible to parse exhaustively with no model, pars-
ing is much slower, since the unpacking of results
is time-consuming. Selective unpacking (Zhang et
al., 2007) speeds this up a great deal, but requires
a parse selection model. Treebanking is also much
slower when the parser must be run exhaustively,
since there are usually many more analyses to man-
ually discard.
This work hopes to alleviate both problems. By
producing a statistical model without requiring hu-
man treebanking, we can have a working and effi-
cient parser with less human effort. Even if the top-
1 parses this parser produces are not as accurate as
those trained on gold standard data, this model can
be used to produce the N-best analyses for the tree-
banker. Since our models are much better than ran-
dom selection, we can afford to reduce N and still
have a reasonable likelihood that the correct parse
is in that top N, making the job of the treebanker
much faster, and potentially leading to even better
parse selection accuracy based on semi-supervised
or fully-supervised parse selection.
</bodyText>
<sectionHeader confidence="0.970069" genericHeader="method">
3 Data and evaluation
</sectionHeader>
<bodyText confidence="0.995882166666667">
Our ultimate goal is to use these methods for under-
resourced languages but, since there are no pre-
existing treebanks for these languages, we have no
means to measure which method produces the best
results. Hence, in this work, we experiment with
languages and grammars where we have gold stan-
dard data, in order to be able to evaluate the qual-
ity of the parse selection models. Since we have
gold-standard trained models to compare with, this
enables us to fully explore how these unsupervised
methods work, and show which methods are worth
trying in the more time-consuming and resource-
intensive future experiments on other languages. It
is worth reinforcing that the gold-standard data is
used for evaluation only, except in calculating the
supervised parse selection accuracy as an upper-
bound.
The English Resource Grammar (ERG:
</bodyText>
<page confidence="0.996925">
695
</page>
<figure confidence="0.5926388">
Average Average
Language Sentences
words parses
Japanese 6769 10.5 49.6
English 4855 9.0 59.5
</figure>
<tableCaption confidence="0.8647725">
Table 1: Initial model training data, showing the average
word length per sentence, and also the ambiguity mea-
</tableCaption>
<bodyText confidence="0.981030222222222">
sured as the average number of parses found per sentence.
Flickinger (2002)) is an HPSG-based grammar
of English that has been under development for
many person years. In order to examine the
cross-lingual applicability of our methods, we also
use Jacy, an HPSG-based grammar of Japanese
(Siegel and Bender, 2002). In both cases, we use
grammar versions from the “Barcelona” release,
from mid-2009.
</bodyText>
<subsectionHeader confidence="0.999621">
3.1 Training Data
</subsectionHeader>
<bodyText confidence="0.999904181818182">
Both of our grammars come with statistical models,
and the parsed data and gold standard annotations
used to create these models are freely available. As
we are trying to simulate a fully unsupervised setup,
we didn’t want any influence from these earlier mod-
els. Hence, in our experiments we used the parsed
data from those sentences that received less than 500
parses and ignored any ranking, thus annulling the
effects of the statistical model. This led to a re-
duced data set, both in the number of sentences, and
in the fact that the more ambiguous sentences were
discarded, but it allows clean comparison between
different methods, without incorporating external in-
formation. The details of our training sets are shown
in Table 1,2 indicating that the sentence lengths are
relatively short, and hence the ambiguity (measured
as average parses per sentence) is low for both our
grammars. The ambiguity figures also suggest that
the Japanese grammar is more constrained (less am-
biguous) than the English grammar, since there are,
on average, more parses per sentence for English,
even with a lower average sentence length.
</bodyText>
<subsectionHeader confidence="0.99998">
3.2 Test Data
</subsectionHeader>
<bodyText confidence="0.9995495">
The test data sets used throughout our experiments
are described in Table 2. The tc-006 data set is from
</bodyText>
<footnote confidence="0.99032">
2Any sentences that do not have both gold and non-gold
analyses (ie, had no correct parse, only one parse, or none) are
not included in these figures.
</footnote>
<table confidence="0.824502333333333">
Average Average
Test Set Language Sentences
words parses
tc-006 Japanese 904 10.7 383.9
jhpstgt English 748 12.8 4115.1
catb English 534 17.6 9427.3
</table>
<tableCaption confidence="0.974420166666667">
Table 2: Test data, showing the average word length per
sentence, and also the ambiguity measured as the average
number of parses found per sentence. Note that the ambi-
guity figures for the English test sets are under-estimates,
since some of the longer sentences timed out before giv-
ing an analysis count.
</tableCaption>
<bodyText confidence="0.99990916">
the same Tanaka Corpus (Tanaka, 2001) which was
used for the Japanese training data. There is a wider
variety of treebanked data available for the English
grammar than for the Japanese. We use the jhp-
stgt data set, which consists of text from Norwegian
tourism brochures, from the same LOGON corpus
as the English training data (Oepen et al., 2004). In
order to have some idea of domain effects, we also
use the catb data set, the text of an essay on open-
source development.3 We see here that the sentences
are longer, particularly for the English data. Also,
since we are not artificially limiting the parse am-
biguity by ignoring those with 500 or more parses,
the ambiguity is much higher. This ambiguity figure
gives some indication of the difficulty of the parse
selection task. Again we see that the English sen-
tences are more ambiguous, much more in this case,
making the parse selection task difficult. In fact,
the English ambiguity figures are an under-estimate,
since some of the longer sentences timed out before
producing a parse count. This ambiguity can be a
function of the sentence length or the language it-
self, but also of the grammar. A more detailed and
informative grammar makes more distinctions, not
all of which are relevant for every analysis.
</bodyText>
<subsectionHeader confidence="0.996846">
3.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.9993855">
The exact match metric is the most common accu-
racy metric used in work with the DELPH-IN tool
set, and refers to the percentage of sentences for
which the top parse matched the gold parse in every
way. This is akin to the sentence accuracy that is oc-
casionally reported in the parsing literature, except
</bodyText>
<footnote confidence="0.995923">
3The Cathedral and the Bazaar, by Eric Raymond.
Available from: http://catb.org/esr/writings/
cathedral-bazaar/
</footnote>
<page confidence="0.997853">
696
</page>
<bodyText confidence="0.999722619047619">
that it also includes fine-grained syntactico-semantic
features that are not often present in other parsing
frameworks. Exact match is a useful metric for parse
selection evaluation, but it is very blunt-edged, and
gives no way of evaluating how close the top parse
was to the gold standard. Since these are very de-
tailed analyses, it is possible to get one detail wrong
and still have a useful analysis. Hence, in addition
to exact match, we also use the EDMNA evalua-
tion defined by Dridan (2009). This is a predicate–
argument style evaluation, based on the semantic
output of the parser (MRS: Minimal Recursion Se-
mantics (Copestake et al., 2005)). This metric is
broadly comparable to the predicate–argument de-
pendencies of CCGBank (Hockenmaier and Steed-
man, 2007) or of the ENJU grammar (Miyao and
Tsujii, 2008), and also somewhat similar to the
grammatical relations (GR) of the Briscoe and Car-
roll (2006) version of DepBank. The EDMNA met-
ric matches triples consisting of predicate names and
the argument type that connects them.
</bodyText>
<sectionHeader confidence="0.998049" genericHeader="method">
4 Initial Experiments
</sectionHeader>
<bodyText confidence="0.999996615384615">
All of our experiments are based on the same basic
process: (1) for each sentence in the training data
described in Section 3.1, label a subset of analyses
as correct and the remainder as incorrect; (2) train
a model using the same features and learner as in
the standard process of Section 2; (3) parse the test
data using that model; and (4) evaluate the accuracy
of the top analyses. The differences lay in how the
‘correct’ analyses are selected each time. Each of
the following sections detail different methods for
nominating which of the (up to 500) analyses from
the training data should be considered pseudo-gold
for training the parse selection model.
</bodyText>
<subsectionHeader confidence="0.997246">
4.1 Upperbound and baseline models
</subsectionHeader>
<bodyText confidence="0.999878857142857">
As a first step we evaluated each data set using an
upperbound and a baseline model. The upperbound
model in this case is the model trained with gold
standard annotations. These accuracy figures are
slightly lower than others found in the literature for
this data, since, to allow for comparison, we lim-
ited the training data to the sets described in Table 1.
</bodyText>
<footnote confidence="0.983300333333333">
4The full EDM metric also includes features such as tense
and aspect, but this is less comparable to the other metrics men-
tioned.
</footnote>
<table confidence="0.9644252">
Test Set Exact EDM
Match Precision Recall F-score
tc-006 72.90 0.961 0.957 0.959
jhpstgt 48.07 0.912 0.908 0.910
catb 22.29 0.838 0.839 0.839
</table>
<tableCaption confidence="0.9852255">
Table 3: Accuracy of the gold standard-based parse se-
lection model.
</tableCaption>
<table confidence="0.9717108">
Test Set Exact EDM
Match Precision Recall F-score
tc-006 17.26 0.779 0.839 0.807
jhpstgt 12.48 0.720 0.748 0.734
catb 8.30 0.646 0.698 0.671
</table>
<tableCaption confidence="0.9963375">
Table 4: Accuracy of the baseline model, trained on ran-
domly selected pseudo-gold analyses.
</tableCaption>
<bodyText confidence="0.9999905">
By throwing out those sentences with more than 500
parses, we exclude much of the data that is used in
the standard model and so our exact match figures
are slightly lower than might be expected.
For the baseline model, we used random selection
to select our gold analyses. For this experiment, we
randomly assigned one parse from each sentence in
the training data to be correct (and the remainder of
analyses as incorrect), and then used that ‘gold stan-
dard’ to train the model. Results for the upperbound
and baseline models are shown in Tables 3 and 4.
As expected, the results for Japanese are much
higher, since the lower ambiguity makes this an eas-
ier task. The catb test set results suffer, not only
from being longer, more ambiguous sentences, but
also because it is completely out of the domain of
the training data.
The exact match results from the random baseline
are approximately what one might expect, given the
respective ambiguity levels in Table 2. The EDM
figures are perhaps higher than might be expected
given random selection from the entire parse forest.
This results from using a precision grammar, with
an inbuilt notion of grammaticality, hence constrain-
ing the parser to only produce somewhat reasonable
parses, and creating a reasonably high baseline for
our parse selection experiments.
We also tried a separate baseline, eliminating the
parse selection model altogether, and using random
selection directly to select the top analysis. The ex-
act match and EDM precision results were slightly
lower than using random selection to train a model,
</bodyText>
<page confidence="0.99137">
697
</page>
<bodyText confidence="0.99972">
which may be due to the learner giving weight to
features that are common across the training data,
but the differences weren’t significant. Recall was
significantly lower when using random selection di-
rectly, due to the time outs caused by running with-
out a model. For this reason, we use the random
selection-based model results as our baseline for the
other unsupervised parse selection models, noting
that correctly identifying approximately three quar-
ters of the dependencies in the jhpstgt set, and over
80% when using the Japanese grammar, is a fairly
high baseline.
</bodyText>
<subsectionHeader confidence="0.99567">
4.2 First attempts
</subsectionHeader>
<bodyText confidence="0.999898382352941">
As a first approach to unsupervised parse selection,
we looked at two heuristics to designate some num-
ber of the analyses as ‘gold’ for training. Both of
these heuristics looked independently at the parses
of each sentence, rather than calculating any num-
bers across the whole training set.
The first method builds on the observation from
the random selection-based model baseline exper-
iment that just giving weight to common features
could improve parser accuracy. In this case, we
looked at the edges of the parsing chart. For each
sentence, we counted the number of times an edge
was present in an analysis, and used that number
(normalised by the total number of times any edge
was used) as the edge weight. We then calculated
an analysis score by summing the edge weights of
all the edges in that analysis, and dividing by the
number of edges, to give an average edge weight for
an analysis. All analyses that had the best analysis
score for a sentence were designated ‘gold’. Since it
was possible for multiple analyses to have the same
score, there could be multiple gold analyses for any
one sentence. If all the analyses had the same score,
this sentence could not be used as part of the train-
ing data. This method has the effect of selecting the
parse(s) most like all the others, by some definitions
the centroid of the parse forest. This has some rela-
tionship to the partial training method described by
Clark and Curran (2006), where the most frequent
dependencies where used to train a model for the
C&amp;C CCG parser. In that case, however, the de-
pendencies were extracted only from analyses that
matched the gold standard supertag sequence, rather
than the whole parse forest.
</bodyText>
<table confidence="0.9233234">
Test Set Exact Match F-score
Edges Branching Edges Branching
tc-006 17.48 21.35 0.815 0.822
jhpstgt 15.27 17.53 0.766 0.780
catb 9.36 10.86 0.713 0.712
</table>
<tableCaption confidence="0.980989333333333">
Table 5: Accuracy for each test set, measured both as per-
centage of sentences that exactly matched the gold stan-
dard, and f-score over elementary dependencies.
</tableCaption>
<bodyText confidence="0.999466238095238">
The second heuristic we tried is one often used as
a baseline method: degree of right (or left) branch-
ing. In this instance, we calculated the degree of
branching as the number of right branches in a parse
divided by the number of left branches (and vice
versa for Japanese, a predominantly left-branching
language). In the same way as above, we designated
all parses with the best branching score as ‘gold’.
Again, this is not fully discriminatory, and it was
common to get multiple gold trees for a given sen-
tence.
Table 5 shows the results for these two methods.
All the results show an improvement over the base-
line, with all but the F-score for the Edges method
of tc-006 being at a level of statistical significance.5
The only statistically significant difference between
the Edges and Branching methods is over the jhp-
stgt data set. While improvement over random is
encouraging, the results were still uninspiring and
so we moved on to slightly more complex methods,
described in the next section.
</bodyText>
<sectionHeader confidence="0.982365" genericHeader="method">
5 Supertagging Experiments
</sectionHeader>
<bodyText confidence="0.99968675">
The term supertags was first used by Bangalore and
Joshi (1999) to describe fine-grained part of speech
tags which include some structural or dependency
information. In that original work, the supertags
were LTAG (Schabes and Joshi, 1991) elementary
trees, and they were used for the purpose of speed-
ing up parsing by restricting the allowable leaf types.
Subsequent work involving supertags has mostly fo-
cussed on this efficiency goal, but they can also be
used to inform parse selection. Dalrymple (2006)
and Blunsom (2007) both look at how discrimina-
tory a tag sequence is in filtering a parse forest. This
</bodyText>
<footnote confidence="0.929912333333333">
5All statistical significance tests in these experiments use the
computationally-intensive randomisation test described in Yeh
(2000), with P &lt; 0.05.
</footnote>
<page confidence="0.992709">
698
</page>
<bodyText confidence="0.999932210526316">
work has shown that tag sequences can be success-
fully used to restrict the set of parses produced, but
generally are not discriminatory enough to distin-
guish a single best parse. Toutanova et al. (2002)
present a similar exploration but also go on to in-
clude probabilities from a HMM model into the
parse selection model as features. There has also
been some work on using lexical probabilities for
domain adaptation of a model (Hara et al., 2007;
Rimell and Clark, 2008). In Dridan (2009), tag se-
quences from a supertagger are used together with
other factors to re-rank the top 500 parses from the
same parser and English grammar we use in this re-
search, and achieve some improvement in the rank-
ing where tagger accuracy is sufficiently high. We
use a similar method, one level removed, in that we
use the tag sequences to select the ‘gold’ parse(s)
that are then used to train a model, as in the previous
sections.
</bodyText>
<subsectionHeader confidence="0.982735">
5.1 Gold Supertags
</subsectionHeader>
<bodyText confidence="0.99984837037037">
In order to test the viability of this method, we first
experimented using gold standard tags, extracted
from the gold standard parses. Supertags come in
many forms, depending on both the grammar for-
malism and the implementation. For this work, we
use HPSG lexical types (lextypes), the native word
classes in the grammars. These lextypes encode part
of speech and subcategorisation information, as well
as some more idiosyncratic features of words, such
as restrictions on preposition forms, mass/count dis-
tinctions and comparative versus superlative forms
of adjectives. As a few examples from the En-
glish grammar, v np le represents a basic transi-
tive verb, while n pp c-of le represents a count
noun that optionally takes a prepositional phrase
complement headed by of. The full definition of a
lextype consists of a many-featured AVM (attribute
value matrix), but the type names have been de-
liberately chosen to represent the main features of
each type. In the Dridan (2009) work, parse ranking
showed some improvement when morphological in-
formation was added to the tags. Hence, we also
look at more fine-grained tags constructed by con-
catenating appropriate morphological rules onto the
lextypes, as in v np le:past verb orule (ie a
simple transitive verb with past tense).
We used these tags by extracting the tag sequence
</bodyText>
<table confidence="0.9552722">
Test Set Exact Match F-score
lextype +morph lextype +morph
tc-006 40.49 41.37 0.903 0.903
jhpstgt 32.93 32.93 0.862 0.858
catb 20.41 19.85 0.798 0.794
</table>
<tableCaption confidence="0.978651">
Table 6: Accuracy using gold tag sequence compatibility
to select the ‘gold’ parse(s).
</tableCaption>
<bodyText confidence="0.999984052631579">
from the leaf types of all the parses in the forest,
marking as ‘gold’ any parse that had the same se-
quence as the gold standard parse and then training
the models as before. Table 6 shows the results from
parsing with models based on both the basic lextype
and the lextype with morphology. The results are
promising. They still fall well below training purely
on gold standard data (at least for the in-domain
sets), since the tag sequences are not fully discrimi-
natory and hence noise can creep in, but accuracy is
significantly better than the heuristic methods tried
earlier. This suggested that, at least with a reason-
ably accurate tagger, this was a viable strategy for
training a model. With no significant difference be-
tween the basic and +morph versions of the tag set,
we decided to use the basic lextypes as tags, since
a smaller tag set should be easier to tag with. How-
ever, we first had to train a tagger, without using any
gold standard data.
</bodyText>
<subsectionHeader confidence="0.998013">
5.2 Unsupervised Supertagging
</subsectionHeader>
<bodyText confidence="0.999774588235294">
Research into unsupervised part-of-speech tagging
with a tag dictionary (sometimes called weakly su-
pervised POS tagging) has been going on for many
years (cf Merialdo (1994), Brill (1995)), but gener-
ally using a fairly small tag set. The only work we
know of on unsupervised tagging for the more com-
plex supertags is from Baldridge (2008), and more
recently, Ravi et al. (2010a). In this work, the con-
straining nature of the (CCG) grammar is used to
mitigate the problem of having a much more am-
biguous tag set. Our method has a similar under-
lying idea, but the implementation differs both in
the way we extract the word-to-tag mappings, and
also how we extract and use the information from
the grammar to initialise the tagger model.
We chose to use a simple first-order Hidden
Markov Model (HMM) tagger, using the implemen-
</bodyText>
<page confidence="0.998274">
699
</page>
<bodyText confidence="0.999926051282051">
tation of Dekang Lin,6 which re-estimates probabil-
ities, given an initial model, using the Baum-Welch
variant of the Expectation-Maximisation (EM) algo-
rithm. One possibility for an initial model was to ex-
tract the word-to-lextype mappings from the gram-
mar lexicon as Baldridge does, and make all starting
probabilities uniform. However, our lexicon maps
between lextypes and lemmas, rather than inflected
word forms, which is what we’d be tagging. That
is to say, from the lexicon we could learn that the
lemma walk can be tagged as v pp* dir le, but
we could not directly extract the fact that therefore
walked should also receive that tag.7 For this rea-
son, we decided it would be simplest to initialise
our probability estimates using the output of the
parser, feeding in only those tag sequences which
are compatible with analyses in the parse forest for
that item. This method takes advantage of the fact
that, because the grammars are heavily constrained,
the parse forest only contains viable tag sequences.
Since parsing without a model is slow, we restricted
the training set to those sentences shorter than a
specific word length (12 for English and 15 for
Japanese, since that was the less ambiguous gram-
mar and hence faster).
Table 7 shows how much parsed data this gave us.
From this parsed data we extracted tag-to-word and
tag-to-tag frequency counts from all parses for all
sentences, and used these frequencies to produce the
emission and transition probabilities, respectively.
The emission probabilities were taken directly from
the normalised frequency counts, but for the tran-
sition probabilities we allow for all possible transi-
tions, and add one to all counts before normalising.
This model we call our initial counts model. The
EM trained model is then produced by starting with
this initial model and running the Baum-Welch al-
gorithm using raw text sentences from the training
corpus.
</bodyText>
<subsectionHeader confidence="0.999169">
5.3 Supertagging-based parse selection models
</subsectionHeader>
<bodyText confidence="0.999959666666667">
We use both the initial counts and EM trained
models to tag the training data from Table 1 and
then compared this with the extracted tag sequences
</bodyText>
<footnote confidence="0.97120725">
6Available from http://webdocs.cs.ualberta.
ca/˜lindek/hmm.htm
7Morphological processing occurs before lexicon lookup in
the PET parser.
</footnote>
<table confidence="0.9995845">
Japanese English
Parsed Sentences 9760 3770
Average Length 9.63 6.36
Average Parses 80.77 96.29
Raw Sentences 13500 9410
Raw Total Words 146053 151906
</table>
<tableCaption confidence="0.95562825">
Table 7: Training data for the HMM tagger (both the
parsed data from which the initial probabilities were de-
rived, and the raw data which was used to estimated the
EM trained models).
</tableCaption>
<table confidence="0.972897333333333">
Exact Match F-score
Test Set Initial EM Initial EM
counts trained counts trained
tc-006 32.85 40.38 0.888 0.898
jhpstgt 26.29 24.04 0.831 0.827
catb 14.61 14.42 0.782 0.783
</table>
<tableCaption confidence="0.999307">
Table 8: Accuracy using tag sequences from a HMM tag-
</tableCaption>
<bodyText confidence="0.9790864">
ger to select the ‘correct’ parse(s). The initial counts
model was based on using counts from a parse forest
to approximate the emission and transition probabilities.
The EM trained model used the Baum Welch algorithm to
estimate the probabilities, starting from the initial counts
state.
used in the gold tag experiment. Since we could
no longer assume that our tag sequence would be
present within the extracted tag sequences, we used
the percentage of tokens from a parse whose lextype
matched our tagged sequence as the parse score.
Again, we marked as ‘gold’ any parse that had the
best parse score for each sentence, and trained a new
parse selection model.
Table 8 shows the results of parsing with these
models. The results are impressive, significantly
higher than all our previous unsupervised methods.
Interestingly, we note that there is no significant
difference between the initial count and EM trained
models for the English data. To explore why this
might be so, we looked at the tagger accuracy for
both models over the respective training data sets,
shown in Table 9. The results are not conclusive. For
both languages, the EM trained model is less accu-
rate, though not significantly so for Japanese. How-
ever, this insignificant tagger accuracy decrease for
Japanese produced a significant increase in parser
accuracy, while a more pronounced tagger accuracy
decrease had no significant effect on parser accuracy
in English.
</bodyText>
<page confidence="0.987939">
700
</page>
<table confidence="0.765384">
Language Initial counts EM trained
Japanese 84.4 83.3
English 71.7 64.6
</table>
<tableCaption confidence="0.9785675">
Table 9: Tagger accuracy over the training data, using
both the initial counts and the EM trained models.
</tableCaption>
<bodyText confidence="0.999943923076923">
There is much potential for further work in this
direction, experimenting with more training data or
more estimation iterations, or even looking at dif-
ferent estimators as suggested in Johnson (2007)
and Ravi et al. (2010b). There is also the issue of
whether tag accuracy is the best measure for indicat-
ing potential parse accuracy. The Japanese parsing
results are already equivalent to those achieved us-
ing gold standard tags. It is possible that parsing ac-
curacy is reasonably insensitive to tagger accuracy,
but it is also possible that there is a better metric to
look at, such as tag accuracy over frequently con-
fused tags.
</bodyText>
<sectionHeader confidence="0.998527" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999867615384615">
The results of Table 8 show that, using no human
annotated data, we can get exact match results that
are almost half way between our random baseline
and our gold-standard-trained upperbound. EDM F-
scores of 90% and 83% over in-domain data com-
pare well with dependency-based scores from other
parsers, although a direct comparison is very diffi-
cult to do (Clark and Curran, 2007a; Miyao et al.,
2007). It still remains to see whether this level of ac-
curacy is good enough to be useful. The main aim of
this work is to bootstrap the treebanking process for
new grammars, but to conclusively show the efficacy
of our methods in that situation requires a long-term
experiment that we are now starting, based on the
results we have here. Another possible use for these
methods was alluded to in Section 2: producing a
new model for a new domain.
Results at every stage have been much worse for
the catb data set, compared to the other jhpstgt En-
glish data set. While sentence length plays some
part, the major reason for this discrepancy was do-
main mismatch between the training and test data.
One method that has been successfully used for do-
main adaption in parsing is self-training (McClosky
et al., 2006). In this process, data from the new do-
main is parsed with the parser trained on the old do-
</bodyText>
<table confidence="0.9977352">
Source of ‘Gold’ Data Exact Match F-score
Random Selection 8.30 0.671
Supertags (initial counts) 14.61 0.782
Gold Standard 22.29 0.839
Self-training 15.92 0.791
</table>
<tableCaption confidence="0.98580375">
Table 10: Accuracy results over the out-of-domain cath
data set, using the initial counts unsupervised model to
produce in-domain training data in a self-training set up.
The previous results are shown for easy comparison.
</tableCaption>
<bodyText confidence="0.999984068965517">
main, and then the top analyses of the parsed new
domain data are added to the training data, and the
parser is re-trained. This is generally considered a
semi-supervised method, since the original parser
is trained on gold standard data. In our case, we
wanted to test whether parsing data from the new do-
main using our unsupervised parse selection model
was accurate enough to still get an improvement us-
ing self-training for domain adaptation.
It is not immediately clear what one might con-
sider to be the ‘domain’ of the catb test set, since do-
main is generally very vaguely defined. In this case,
there was a limited amount of text available from
other essays by the same author.$ While the topics
of these essays vary, they all relate to the social side
of technical communities, and so we used this to rep-
resent in-domain data for the catb test set. It is, how-
ever, a fairly small amount of data for self-training,
being only around 1000 sentences. We added the re-
sults of parsing this data to the training set we used
to create the initial counts model and again retrained
and parsed. Table 10 shows the results. Previous re-
sults for the catb data set are given for comparison.
The results show that the completely unsuper-
vised parse selection method produces a top parse
that is at least accurate enough to be used in self-
training, providing a cheap means of domain adapta-
tion. In future work, we hope to explore this avenue
of research further.
</bodyText>
<sectionHeader confidence="0.995386" genericHeader="conclusions">
7 Conclusions and Further Work
</sectionHeader>
<bodyText confidence="0.998146">
Comparing Tables 8 and 4, we can see that for both
English and Japanese, we are able to achieve parse
selection accuracy well above our baseline of a ran-
</bodyText>
<footnote confidence="0.9950035">
8http://www.catb.org/esr/writings/
homesteading/
</footnote>
<page confidence="0.995852">
701
</page>
<bodyText confidence="0.999993076923077">
dom selection-based model using only the informa-
tion available in the grammar and raw text. This
was in part because it is possible to extract a rea-
sonable tagging model from uncorrected parse data,
due to the constrained nature of these grammars.
These models will hopefully allow grammar engi-
neers to more easily build statistical models for new
languages, using nothing more than their new gram-
mar and raw text.
Since fully evaluating the potential for building
models for new languages is a long-term ongoing
experiment, we looked at a more short-term eval-
uation of our unsupervised parse selection meth-
ods: building models for new domains. A pre-
liminary self-training experiment, using our initial
counts tagger trained model as the starting point,
showed promising results for domain adaptation.
There are plenty of directions for further work
arising from these results. The issues surrounding
what makes a good tagger for this purpose, and how
can we best learn one without gold training data,
would be one possibly fruitful avenue for further
exploration. Another interesting slant would be to
investigate domain effects of the tagger. Previous
work has already found that training just a lexical
model on a new domain can improve parsing results.
Since the optimal tagger ‘training’ we saw here (for
English) was merely to read off frequency counts for
parsed data, it would be easy to retrain the tagger on
different domains. Alternatively, it would be inter-
esting so see how much difference it makes to train
the tagger on one set of data, and use that to tag a
model training set from a different domain. Other
methods of incorporating the tagger output could
also be investigated. Finally, a user study involv-
ing a grammar engineer working on a new language
would be useful to validate the results we found here
and confirm whether they are indeed helpful in boot-
strapping a new grammar.
</bodyText>
<sectionHeader confidence="0.996334" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.989932">
This research was supported by Australian Research
Council grant no. DP0988242 and Microsoft Re-
search Asia.
</bodyText>
<sectionHeader confidence="0.990108" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997071653846154">
Jason Baldridge. 2008. Weakly supervised supertagging
with grammar-informed initialization. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics (Coling 2008), pages 57–64, Manch-
ester, UK.
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: an approach to almost parsing. Compu-
tational Linguistics, 25(2):237–265.
Emily M. Bender, Dan Flickinger, and Stephan Oepen.
2002. The grammar matrix: An open-source starter-
kit for the rapid development of cross-linguistically
consistent broad-coverage precision grammars. In
Proceedings of the Workshop on Grammar Engineer-
ing and Evaluation at the 19th International Con-
ference on Computational Linguistics, pages 8–14,
Taipei, Taiwan.
Emily M. Bender. 2008. Evaluating a crosslinguistic
grammar resource: A case study of Wambaya. In Pro-
ceedings of the 46th Annual Meeting of the ACL, pages
977–985, Columbus, USA.
Philip Blunsom. 2007. Structured Classification for
Multilingual Natural Language Processing. Ph.D.
thesis, Department of Computer Science and Software
Engineering, the University of Melbourne.
Eric Brill. 1995. Unsupervised learning of disambigua-
tion rules for part of speech tagging. In Proceedings
of the Third Workshop on Very Large Corpora, pages
1–13, Cambridge, USA.
Ted Briscoe and John Carroll. 2006. Evaluating the
accuracy of an unlexicalised statistical parser on the
PARC DepBank. In Proceedings of the 44th Annual
Meeting of the ACL and the 21st International Confer-
ence on Computational Linguistics, pages 41–48, Syd-
ney, Australia.
David Carter. 1997. The treebanker: a tool for super-
vised training of parsed corpora. In Proceedings of a
Workshop on Computational Environments for Gram-
mar Development and Linguistic Engineering, pages
9–15, Madrid, Spain.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
ACL, pages 173–180, Ann Arbor, USA.
Stephen Clark and James R. Curran. 2006. Partial train-
ing for a lexicalized-grammar parser. In Proceedings
of the Human Language Technology Conference of the
North American Chapter of the ACL (NAACL), pages
144–151, New York City, USA.
Stephen Clark and James R. Curran. 2007a. Formalism-
independent parser evaluation with CCG and Dep-
Bank. In Proceedings of the 45th Annual Meeting of
the ACL, pages 248–255, Prague, Czech Republic.
</reference>
<page confidence="0.988964">
702
</page>
<reference confidence="0.998550009345794">
Stephen Clark and James R. Curran. 2007b. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics,
33(4):493–552.
Ann Copestake, Dan Flickinger, Ivan A. Sag, and Carl
Pollard. 2005. Minimal recursion semantics: An in-
troduction. Research on Language and Computation,
vol 3(no 4):pp 281–332.
Mary Dalrymple. 2006. How much can part-of-speech
tagging help parsing? Natural Language Engineering,
12(4):373–389.
Rebecca Dridan. 2009. Using lexical statistics to im-
prove HPSG parsing. Ph.D. thesis, Saarland Univer-
sity.
Dan Flickinger. 2002. On building a more efficient
grammar by exploiting types. In Stephan Oepen, Dan
Flickinger, Jun’ichi Tsujii, and Hans Uszkoreit, edi-
tors, Collaborative Language Engineering, pages 1–
17. Stanford: CSLI Publications.
Tadayoshi Hara, Yusuke Miyao, and Jun’ichi Tsujii.
2007. Evaluating impact of re-training a lexical dis-
ambiguation model on domain adaptation of an HPSG
parser. In Proceedings of the 10th International Con-
ference on Parsing Technology (IWPT 2007), pages
11–22, Prague, Czech Republic.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Compu-
tational Linguistics, 33(3):355–396, September.
Mark Johnson. 2007. Why doesnt EM find good HMM
POS-taggers? In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 296–305,
Prague, Czech Republic.
Robert Malouf. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In Pro-
ceedings of the 6th Conference on Natural Language
Learning, Taipei, Taiwan.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the Human Language Technology Conference
of the NAACL, pages 152–159, New York City, USA.
Bernard Merialdo. 1994. Tagging english text with
a probabilistic model. Computational Linguistics,
20(2):155–171.
Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Computa-
tional Linguistics, 34(1):35–80.
Yusuke Miyao, Kenji Sagae, and Jun’ichi Tsujii. 2007.
Towards framework-independent evaluation of deep
linguistic parsers. In Proceedings of the GEAF 2007
Workshop, Palo Alto, California.
Yusuke Miyao, Rune Sætre, Kenji Sagae, Takuya Mat-
suzaki, and Jun’ichi Tsujii. 2008. Task-oriented eval-
uation of syntactic parsers and their representations. In
Proceedings of the 46th Annual Meeting of the ACL,
pages 46–54, Columbus, USA.
Stephan Oepen and John Carroll. 2000. Ambiguity pack-
ing in constraint-based parsing - practical results. In
Proceedings of the 1st Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 162–169, Seattle, USA.
Stephan Oepen, Dan Flickinger, Kristina Toutanova, and
Christopher D. Manning. 2004. LinGO redwoods. a
rich and dynamic treebank for HPSG. Journal of Re-
search in Language and Computation, 2(4):575–596.
Stephan Oepen. 2001. [incr tsdb()] – competence and
performance laboratory. User manual, Computational
Linguistics, Saarland University, Saarbr¨ucken, Ger-
many.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press,
Chicago, USA.
Sujith Ravi, Jason Baldridge, and Kevin Knight. 2010a.
Minimized models and grammar-informed initializa-
tion for supertagging with highly ambiguous lexicons.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 495–503,
Uppsala, Sweden.
Sujith Ravi, Ashish Vaswani, Kevin Knight, and David
Chiang. 2010b. Fast, greedy model minimization for
unsupervised tagging. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(Coling 2010), pages 940–948, Beijing, China.
Laura Rimell and Stephen Clark. 2008. Adapting
a lexicalized-grammar parser to contrasting domains.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2008), pages 475–484, Honolulu, USA.
Yves Schabes and Aravind K. Joshi. 1991. Parsing with
lexicalized tree adjoining grammar. In Masaru Tomita,
editor, Current Issues in Parsing Technology, chap-
ter 3, pages 25–48. Kluwer.
Melanie Siegel and Emily M. Bender. 2002. Efficient
deep processing of japanese. In Proceedings of the 3rd
Workshop on Asian Language Resources and Interna-
tional Standardization. Coling 2002 Post-Conference
Workshop., Taipei, Taiwan.
Yasuhito Tanaka. 2001. Compilation of a multilingual
parallel corpus. In Proceedings of PACLING 2001,
pages 265–268, Kitakyushu, Japan.
Kristina Toutanova, Chistopher D. Manning, Stuart M.
Shieber, Dan Flickinger, and Stephan Oepen. 2002.
Parse disambiguation for a rich HPSG grammar. In
First Workshop on Treebanks and Linguistic Theories
(TLT2002), pages 253–263.
</reference>
<page confidence="0.986177">
703
</page>
<reference confidence="0.995961">
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proceed-
ings of the 18th International Conference on Compu-
tational Linguistics (COLING 2000), pages 947–953,
Saarbrcken, Germany.
Yi Zhang, Stephan Oepen, and John Carroll. 2007. Ef-
ficiency in unification-based n-best parsing. In Pro-
ceedings of the 10th international conference on pars-
ing technologies (IWPT 2007), pages 48–59, Prague,
Czech Republic.
</reference>
<page confidence="0.998521">
704
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.875640">
<title confidence="0.998568">Unsupervised Parse Selection for HPSG</title>
<author confidence="0.90789">Dridan</author>
<affiliation confidence="0.999656">Dept. of Computer Science and Software University of Melbourne,</affiliation>
<abstract confidence="0.998318190476191">Parser disambiguation with precision grammars generally takes place via statistical ranking of the parse yield of the grammar using a supervised parse selection model. In the standard process, the parse selection model is trained over a hand-disambiguated treebank, meaning that without a significant investment of effort to produce the treebank, parse selection is not possible. Furthermore, as treebanking is generally streamlined with parse selection models, creating the initial treebank without a model requires more resources than subsequent treebanks. In this work, we show that, by taking advantage of the constrained nature of these BPSG grammars, we can learn a discriminative parse selection model from raw text in a purely unsupervised fashion. This allows us to bootstrap the treebanking process and provide better parsers faster, and with less resources.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
</authors>
<title>Weakly supervised supertagging with grammar-informed initialization.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>57--64</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="24318" citStr="Baldridge (2008)" startWordPosition="4025" endWordPosition="4026">ant difference between the basic and +morph versions of the tag set, we decided to use the basic lextypes as tags, since a smaller tag set should be easier to tag with. However, we first had to train a tagger, without using any gold standard data. 5.2 Unsupervised Supertagging Research into unsupervised part-of-speech tagging with a tag dictionary (sometimes called weakly supervised POS tagging) has been going on for many years (cf Merialdo (1994), Brill (1995)), but generally using a fairly small tag set. The only work we know of on unsupervised tagging for the more complex supertags is from Baldridge (2008), and more recently, Ravi et al. (2010a). In this work, the constraining nature of the (CCG) grammar is used to mitigate the problem of having a much more ambiguous tag set. Our method has a similar underlying idea, but the implementation differs both in the way we extract the word-to-tag mappings, and also how we extract and use the information from the grammar to initialise the tagger model. We chose to use a simple first-order Hidden Markov Model (HMM) tagger, using the implemen699 tation of Dekang Lin,6 which re-estimates probabilities, given an initial model, using the Baum-Welch variant </context>
</contexts>
<marker>Baldridge, 2008</marker>
<rawString>Jason Baldridge. 2008. Weakly supervised supertagging with grammar-informed initialization. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 57–64, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind K Joshi</author>
</authors>
<title>Supertagging: an approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="19785" citStr="Bangalore and Joshi (1999)" startWordPosition="3264" endWordPosition="3267">o get multiple gold trees for a given sentence. Table 5 shows the results for these two methods. All the results show an improvement over the baseline, with all but the F-score for the Edges method of tc-006 being at a level of statistical significance.5 The only statistically significant difference between the Edges and Branching methods is over the jhpstgt data set. While improvement over random is encouraging, the results were still uninspiring and so we moved on to slightly more complex methods, described in the next section. 5 Supertagging Experiments The term supertags was first used by Bangalore and Joshi (1999) to describe fine-grained part of speech tags which include some structural or dependency information. In that original work, the supertags were LTAG (Schabes and Joshi, 1991) elementary trees, and they were used for the purpose of speeding up parsing by restricting the allowable leaf types. Subsequent work involving supertags has mostly focussed on this efficiency goal, but they can also be used to inform parse selection. Dalrymple (2006) and Blunsom (2007) both look at how discriminatory a tag sequence is in filtering a parse forest. This 5All statistical significance tests in these experime</context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Srinivas Bangalore and Aravind K. Joshi. 1999. Supertagging: an approach to almost parsing. Computational Linguistics, 25(2):237–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily M Bender</author>
<author>Dan Flickinger</author>
<author>Stephan Oepen</author>
</authors>
<title>The grammar matrix: An open-source starterkit for the rapid development of cross-linguistically consistent broad-coverage precision grammars.</title>
<date>2002</date>
<booktitle>In Proceedings of the Workshop on Grammar Engineering and Evaluation at the 19th International Conference on Computational Linguistics,</booktitle>
<pages>8--14</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="2377" citStr="Bender et al., 2002" startWordPosition="373" endWordPosition="376">se selection is based on a statistical model learned from a pre-existing treebank associated with the grammar. Our interest in this paper is in completely removing this requirement of parse selection on explicitly treebanked data, ie the development of fully unsupervised parse selection models. The particular style of precision grammar we experiment with in this paper is HPSG (Pollard and Sag, 1994), in the form of the DELPH-IN suite of grammars (http://www.delph-in.net/). One of the main focuses of the DELPH-IN collaboration effort is multilinguality. To this end, the Grammar Matrix project (Bender et al., 2002) has been developed which, through a set of questionnaires, allows grammar engineers to quickly produce a core grammar for a language of their choice. Bender (2008) showed that by using and expanding on this core grammar, she was able to produce a broad-coverage precision grammar of Wambaya in a very short amount of time. However, the Grammar Matrix can only help with the first stage of parsing. The statistical model used in the second stage of parsing (ie parse selection) requires a treebank to learn the features, but as we explain in Section 2, the treebanks are created by parsing, preferabl</context>
</contexts>
<marker>Bender, Flickinger, Oepen, 2002</marker>
<rawString>Emily M. Bender, Dan Flickinger, and Stephan Oepen. 2002. The grammar matrix: An open-source starterkit for the rapid development of cross-linguistically consistent broad-coverage precision grammars. In Proceedings of the Workshop on Grammar Engineering and Evaluation at the 19th International Conference on Computational Linguistics, pages 8–14, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily M Bender</author>
</authors>
<title>Evaluating a crosslinguistic grammar resource: A case study of Wambaya.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the ACL,</booktitle>
<pages>977--985</pages>
<location>Columbus, USA.</location>
<contexts>
<context position="2541" citStr="Bender (2008)" startWordPosition="403" endWordPosition="404">quirement of parse selection on explicitly treebanked data, ie the development of fully unsupervised parse selection models. The particular style of precision grammar we experiment with in this paper is HPSG (Pollard and Sag, 1994), in the form of the DELPH-IN suite of grammars (http://www.delph-in.net/). One of the main focuses of the DELPH-IN collaboration effort is multilinguality. To this end, the Grammar Matrix project (Bender et al., 2002) has been developed which, through a set of questionnaires, allows grammar engineers to quickly produce a core grammar for a language of their choice. Bender (2008) showed that by using and expanding on this core grammar, she was able to produce a broad-coverage precision grammar of Wambaya in a very short amount of time. However, the Grammar Matrix can only help with the first stage of parsing. The statistical model used in the second stage of parsing (ie parse selection) requires a treebank to learn the features, but as we explain in Section 2, the treebanks are created by parsing, preferably with a statistical model. In this work, we look at methods for bootstrapping the production of these statistical models without having an annotated treebank. Sinc</context>
</contexts>
<marker>Bender, 2008</marker>
<rawString>Emily M. Bender. 2008. Evaluating a crosslinguistic grammar resource: A case study of Wambaya. In Proceedings of the 46th Annual Meeting of the ACL, pages 977–985, Columbus, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Blunsom</author>
</authors>
<title>Structured Classification for Multilingual Natural Language Processing.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science and Software Engineering, the University of Melbourne.</institution>
<contexts>
<context position="20247" citStr="Blunsom (2007)" startWordPosition="3339" endWordPosition="3340">slightly more complex methods, described in the next section. 5 Supertagging Experiments The term supertags was first used by Bangalore and Joshi (1999) to describe fine-grained part of speech tags which include some structural or dependency information. In that original work, the supertags were LTAG (Schabes and Joshi, 1991) elementary trees, and they were used for the purpose of speeding up parsing by restricting the allowable leaf types. Subsequent work involving supertags has mostly focussed on this efficiency goal, but they can also be used to inform parse selection. Dalrymple (2006) and Blunsom (2007) both look at how discriminatory a tag sequence is in filtering a parse forest. This 5All statistical significance tests in these experiments use the computationally-intensive randomisation test described in Yeh (2000), with P &lt; 0.05. 698 work has shown that tag sequences can be successfully used to restrict the set of parses produced, but generally are not discriminatory enough to distinguish a single best parse. Toutanova et al. (2002) present a similar exploration but also go on to include probabilities from a HMM model into the parse selection model as features. There has also been some wo</context>
</contexts>
<marker>Blunsom, 2007</marker>
<rawString>Philip Blunsom. 2007. Structured Classification for Multilingual Natural Language Processing. Ph.D. thesis, Department of Computer Science and Software Engineering, the University of Melbourne.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Unsupervised learning of disambiguation rules for part of speech tagging.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third Workshop on Very Large Corpora,</booktitle>
<pages>1--13</pages>
<location>Cambridge, USA.</location>
<contexts>
<context position="24167" citStr="Brill (1995)" startWordPosition="3997" endWordPosition="3998">s tried earlier. This suggested that, at least with a reasonably accurate tagger, this was a viable strategy for training a model. With no significant difference between the basic and +morph versions of the tag set, we decided to use the basic lextypes as tags, since a smaller tag set should be easier to tag with. However, we first had to train a tagger, without using any gold standard data. 5.2 Unsupervised Supertagging Research into unsupervised part-of-speech tagging with a tag dictionary (sometimes called weakly supervised POS tagging) has been going on for many years (cf Merialdo (1994), Brill (1995)), but generally using a fairly small tag set. The only work we know of on unsupervised tagging for the more complex supertags is from Baldridge (2008), and more recently, Ravi et al. (2010a). In this work, the constraining nature of the (CCG) grammar is used to mitigate the problem of having a much more ambiguous tag set. Our method has a similar underlying idea, but the implementation differs both in the way we extract the word-to-tag mappings, and also how we extract and use the information from the grammar to initialise the tagger model. We chose to use a simple first-order Hidden Markov M</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Unsupervised learning of disambiguation rules for part of speech tagging. In Proceedings of the Third Workshop on Very Large Corpora, pages 1–13, Cambridge, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Evaluating the accuracy of an unlexicalised statistical parser on the PARC DepBank.</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the ACL and the 21st International Conference on Computational Linguistics,</booktitle>
<pages>41--48</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="12740" citStr="Briscoe and Carroll (2006)" startWordPosition="2078" endWordPosition="2082">andard. Since these are very detailed analyses, it is possible to get one detail wrong and still have a useful analysis. Hence, in addition to exact match, we also use the EDMNA evaluation defined by Dridan (2009). This is a predicate– argument style evaluation, based on the semantic output of the parser (MRS: Minimal Recursion Semantics (Copestake et al., 2005)). This metric is broadly comparable to the predicate–argument dependencies of CCGBank (Hockenmaier and Steedman, 2007) or of the ENJU grammar (Miyao and Tsujii, 2008), and also somewhat similar to the grammatical relations (GR) of the Briscoe and Carroll (2006) version of DepBank. The EDMNA metric matches triples consisting of predicate names and the argument type that connects them. 4 Initial Experiments All of our experiments are based on the same basic process: (1) for each sentence in the training data described in Section 3.1, label a subset of analyses as correct and the remainder as incorrect; (2) train a model using the same features and learner as in the standard process of Section 2; (3) parse the test data using that model; and (4) evaluate the accuracy of the top analyses. The differences lay in how the ‘correct’ analyses are selected ea</context>
</contexts>
<marker>Briscoe, Carroll, 2006</marker>
<rawString>Ted Briscoe and John Carroll. 2006. Evaluating the accuracy of an unlexicalised statistical parser on the PARC DepBank. In Proceedings of the 44th Annual Meeting of the ACL and the 21st International Conference on Computational Linguistics, pages 41–48, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Carter</author>
</authors>
<title>The treebanker: a tool for supervised training of parsed corpora.</title>
<date>1997</date>
<booktitle>In Proceedings of a Workshop on Computational Environments for Grammar Development and Linguistic Engineering,</booktitle>
<pages>9--15</pages>
<location>Madrid,</location>
<contexts>
<context position="4539" citStr="Carter, 1997" startWordPosition="723" endWordPosition="724"> 2 The problem The current method of training a parse selection model uses the [incr tsdb()] treebanking mechanism (Oepen, 2001) and works well for updating models for mature grammars, although even for these grammars, building a new model for a different domain requires a time-consuming initial treebanking effort. The treebanks used with DELPH-IN grammars are dynamic treebanks (Oepen et al., 2004) created by parsing text and having an annotator select the correct analysis (or discard all of them). The annotation process involves making binary decisions based on so-called parse discriminants (Carter, 1997). Whenever the grammar is changed, the treebank can be quickly updated by re-parsing and re-applying the old annotation decisions. This treebanking process not only produces gold standard trees, but also a set of non-gold trees which provides the negative training data necessary for a discriminative maximum entropy model. The standard process for creating a parse selection model is: 1. parse the training set, recording up to 500 highest-ranking parses for each sentence; 2. treebank the training set; 3. extract features from the gold and non-gold parses; 4. learn feature weights using the TADM </context>
</contexts>
<marker>Carter, 1997</marker>
<rawString>David Carter. 1997. The treebanker: a tool for supervised training of parsed corpora. In Proceedings of a Workshop on Computational Environments for Grammar Development and Linguistic Engineering, pages 9–15, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the ACL,</booktitle>
<pages>173--180</pages>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="1516" citStr="Charniak and Johnson (2005)" startWordPosition="233" endWordPosition="236">inative parse selection model from raw text in a purely unsupervised fashion. This allows us to bootstrap the treebanking process and provide better parsers faster, and with less resources. 1 Introduction Parsing with precision grammars is generally a twostage process: (1) the full parse yield of the precision grammar is calculated for a given item, often in the form of a packed forest for efficiency (Oepen and Carroll, 2000; Zhang et al., 2007); and (2) the individual analyses in the parse forest are ranked using a statistical model (“parse selection”). In the domain of treebank parsing, the Charniak and Johnson (2005) reranking parser adopts an analogous strategy, except that ranking and pruning are incorporated into the first stage, and the second stage is based on only the top-ranked parses from the first stage. For both styles of parsing, however, parse selection is based on a statistical model learned from a pre-existing treebank associated with the grammar. Our interest in this paper is in completely removing this requirement of parse selection on explicitly treebanked data, ie the development of fully unsupervised parse selection models. The particular style of precision grammar we experiment with in</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and maxent discriminative reranking. In Proceedings of the 43rd Annual Meeting of the ACL, pages 173–180, Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Partial training for a lexicalized-grammar parser.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL (NAACL),</booktitle>
<pages>144--151</pages>
<location>New York City, USA.</location>
<contexts>
<context position="18127" citStr="Clark and Curran (2006)" startWordPosition="2987" endWordPosition="2990">dividing by the number of edges, to give an average edge weight for an analysis. All analyses that had the best analysis score for a sentence were designated ‘gold’. Since it was possible for multiple analyses to have the same score, there could be multiple gold analyses for any one sentence. If all the analyses had the same score, this sentence could not be used as part of the training data. This method has the effect of selecting the parse(s) most like all the others, by some definitions the centroid of the parse forest. This has some relationship to the partial training method described by Clark and Curran (2006), where the most frequent dependencies where used to train a model for the C&amp;C CCG parser. In that case, however, the dependencies were extracted only from analyses that matched the gold standard supertag sequence, rather than the whole parse forest. Test Set Exact Match F-score Edges Branching Edges Branching tc-006 17.48 21.35 0.815 0.822 jhpstgt 15.27 17.53 0.766 0.780 catb 9.36 10.86 0.713 0.712 Table 5: Accuracy for each test set, measured both as percentage of sentences that exactly matched the gold standard, and f-score over elementary dependencies. The second heuristic we tried is one </context>
</contexts>
<marker>Clark, Curran, 2006</marker>
<rawString>Stephen Clark and James R. Curran. 2006. Partial training for a lexicalized-grammar parser. In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL (NAACL), pages 144–151, New York City, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Formalismindependent parser evaluation with CCG and DepBank.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL,</booktitle>
<pages>248--255</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3827" citStr="Clark and Curran, 2007" startWordPosition="611" endWordPosition="614">r are under-resourced, we can’t depend on having any external information or NLP tools, and so the methods we examine are purely unsupervised, using nothing more than the grammars them694 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 694–704, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics selves and raw text. We find that, not only can we produce models that are suitable for kick-starting the treebanking process, but the accuracy of these models is comparable to parsers trained on gold standard data (Clark and Curran, 2007b; Miyao and Tsujii, 2008), which have been successfully used in applications (Miyao et al., 2008). 2 The problem The current method of training a parse selection model uses the [incr tsdb()] treebanking mechanism (Oepen, 2001) and works well for updating models for mature grammars, although even for these grammars, building a new model for a different domain requires a time-consuming initial treebanking effort. The treebanks used with DELPH-IN grammars are dynamic treebanks (Oepen et al., 2004) created by parsing text and having an annotator select the correct analysis (or discard all of them</context>
<context position="30253" citStr="Clark and Curran, 2007" startWordPosition="4998" endWordPosition="5001">ieved using gold standard tags. It is possible that parsing accuracy is reasonably insensitive to tagger accuracy, but it is also possible that there is a better metric to look at, such as tag accuracy over frequently confused tags. 6 Discussion The results of Table 8 show that, using no human annotated data, we can get exact match results that are almost half way between our random baseline and our gold-standard-trained upperbound. EDM Fscores of 90% and 83% over in-domain data compare well with dependency-based scores from other parsers, although a direct comparison is very difficult to do (Clark and Curran, 2007a; Miyao et al., 2007). It still remains to see whether this level of accuracy is good enough to be useful. The main aim of this work is to bootstrap the treebanking process for new grammars, but to conclusively show the efficacy of our methods in that situation requires a long-term experiment that we are now starting, based on the results we have here. Another possible use for these methods was alluded to in Section 2: producing a new model for a new domain. Results at every stage have been much worse for the catb data set, compared to the other jhpstgt English data set. While sentence length</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007a. Formalismindependent parser evaluation with CCG and DepBank. In Proceedings of the 45th Annual Meeting of the ACL, pages 248–255, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Widecoverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="3827" citStr="Clark and Curran, 2007" startWordPosition="611" endWordPosition="614">r are under-resourced, we can’t depend on having any external information or NLP tools, and so the methods we examine are purely unsupervised, using nothing more than the grammars them694 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 694–704, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics selves and raw text. We find that, not only can we produce models that are suitable for kick-starting the treebanking process, but the accuracy of these models is comparable to parsers trained on gold standard data (Clark and Curran, 2007b; Miyao and Tsujii, 2008), which have been successfully used in applications (Miyao et al., 2008). 2 The problem The current method of training a parse selection model uses the [incr tsdb()] treebanking mechanism (Oepen, 2001) and works well for updating models for mature grammars, although even for these grammars, building a new model for a different domain requires a time-consuming initial treebanking effort. The treebanks used with DELPH-IN grammars are dynamic treebanks (Oepen et al., 2004) created by parsing text and having an annotator select the correct analysis (or discard all of them</context>
<context position="30253" citStr="Clark and Curran, 2007" startWordPosition="4998" endWordPosition="5001">ieved using gold standard tags. It is possible that parsing accuracy is reasonably insensitive to tagger accuracy, but it is also possible that there is a better metric to look at, such as tag accuracy over frequently confused tags. 6 Discussion The results of Table 8 show that, using no human annotated data, we can get exact match results that are almost half way between our random baseline and our gold-standard-trained upperbound. EDM Fscores of 90% and 83% over in-domain data compare well with dependency-based scores from other parsers, although a direct comparison is very difficult to do (Clark and Curran, 2007a; Miyao et al., 2007). It still remains to see whether this level of accuracy is good enough to be useful. The main aim of this work is to bootstrap the treebanking process for new grammars, but to conclusively show the efficacy of our methods in that situation requires a long-term experiment that we are now starting, based on the results we have here. Another possible use for these methods was alluded to in Section 2: producing a new model for a new domain. Results at every stage have been much worse for the catb data set, compared to the other jhpstgt English data set. While sentence length</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007b. Widecoverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
<author>Ivan A Sag</author>
<author>Carl Pollard</author>
</authors>
<title>Minimal recursion semantics: An introduction.</title>
<date>2005</date>
<journal>Research on Language and Computation,</journal>
<volume>3</volume>
<pages>281--332</pages>
<contexts>
<context position="12478" citStr="Copestake et al., 2005" startWordPosition="2037" endWordPosition="2040">e-grained syntactico-semantic features that are not often present in other parsing frameworks. Exact match is a useful metric for parse selection evaluation, but it is very blunt-edged, and gives no way of evaluating how close the top parse was to the gold standard. Since these are very detailed analyses, it is possible to get one detail wrong and still have a useful analysis. Hence, in addition to exact match, we also use the EDMNA evaluation defined by Dridan (2009). This is a predicate– argument style evaluation, based on the semantic output of the parser (MRS: Minimal Recursion Semantics (Copestake et al., 2005)). This metric is broadly comparable to the predicate–argument dependencies of CCGBank (Hockenmaier and Steedman, 2007) or of the ENJU grammar (Miyao and Tsujii, 2008), and also somewhat similar to the grammatical relations (GR) of the Briscoe and Carroll (2006) version of DepBank. The EDMNA metric matches triples consisting of predicate names and the argument type that connects them. 4 Initial Experiments All of our experiments are based on the same basic process: (1) for each sentence in the training data described in Section 3.1, label a subset of analyses as correct and the remainder as in</context>
</contexts>
<marker>Copestake, Flickinger, Sag, Pollard, 2005</marker>
<rawString>Ann Copestake, Dan Flickinger, Ivan A. Sag, and Carl Pollard. 2005. Minimal recursion semantics: An introduction. Research on Language and Computation, vol 3(no 4):pp 281–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Dalrymple</author>
</authors>
<title>How much can part-of-speech tagging help parsing?</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>12</volume>
<issue>4</issue>
<contexts>
<context position="20228" citStr="Dalrymple (2006)" startWordPosition="3336" endWordPosition="3337">nd so we moved on to slightly more complex methods, described in the next section. 5 Supertagging Experiments The term supertags was first used by Bangalore and Joshi (1999) to describe fine-grained part of speech tags which include some structural or dependency information. In that original work, the supertags were LTAG (Schabes and Joshi, 1991) elementary trees, and they were used for the purpose of speeding up parsing by restricting the allowable leaf types. Subsequent work involving supertags has mostly focussed on this efficiency goal, but they can also be used to inform parse selection. Dalrymple (2006) and Blunsom (2007) both look at how discriminatory a tag sequence is in filtering a parse forest. This 5All statistical significance tests in these experiments use the computationally-intensive randomisation test described in Yeh (2000), with P &lt; 0.05. 698 work has shown that tag sequences can be successfully used to restrict the set of parses produced, but generally are not discriminatory enough to distinguish a single best parse. Toutanova et al. (2002) present a similar exploration but also go on to include probabilities from a HMM model into the parse selection model as features. There ha</context>
</contexts>
<marker>Dalrymple, 2006</marker>
<rawString>Mary Dalrymple. 2006. How much can part-of-speech tagging help parsing? Natural Language Engineering, 12(4):373–389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Dridan</author>
</authors>
<title>Using lexical statistics to improve HPSG parsing.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>Saarland University.</institution>
<contexts>
<context position="12327" citStr="Dridan (2009)" startWordPosition="2015" endWordPosition="2016">3The Cathedral and the Bazaar, by Eric Raymond. Available from: http://catb.org/esr/writings/ cathedral-bazaar/ 696 that it also includes fine-grained syntactico-semantic features that are not often present in other parsing frameworks. Exact match is a useful metric for parse selection evaluation, but it is very blunt-edged, and gives no way of evaluating how close the top parse was to the gold standard. Since these are very detailed analyses, it is possible to get one detail wrong and still have a useful analysis. Hence, in addition to exact match, we also use the EDMNA evaluation defined by Dridan (2009). This is a predicate– argument style evaluation, based on the semantic output of the parser (MRS: Minimal Recursion Semantics (Copestake et al., 2005)). This metric is broadly comparable to the predicate–argument dependencies of CCGBank (Hockenmaier and Steedman, 2007) or of the ENJU grammar (Miyao and Tsujii, 2008), and also somewhat similar to the grammatical relations (GR) of the Briscoe and Carroll (2006) version of DepBank. The EDMNA metric matches triples consisting of predicate names and the argument type that connects them. 4 Initial Experiments All of our experiments are based on the</context>
<context position="20975" citStr="Dridan (2009)" startWordPosition="3461" endWordPosition="3462">tests in these experiments use the computationally-intensive randomisation test described in Yeh (2000), with P &lt; 0.05. 698 work has shown that tag sequences can be successfully used to restrict the set of parses produced, but generally are not discriminatory enough to distinguish a single best parse. Toutanova et al. (2002) present a similar exploration but also go on to include probabilities from a HMM model into the parse selection model as features. There has also been some work on using lexical probabilities for domain adaptation of a model (Hara et al., 2007; Rimell and Clark, 2008). In Dridan (2009), tag sequences from a supertagger are used together with other factors to re-rank the top 500 parses from the same parser and English grammar we use in this research, and achieve some improvement in the ranking where tagger accuracy is sufficiently high. We use a similar method, one level removed, in that we use the tag sequences to select the ‘gold’ parse(s) that are then used to train a model, as in the previous sections. 5.1 Gold Supertags In order to test the viability of this method, we first experimented using gold standard tags, extracted from the gold standard parses. Supertags come i</context>
<context position="22402" citStr="Dridan (2009)" startWordPosition="3701" endWordPosition="3702">ubcategorisation information, as well as some more idiosyncratic features of words, such as restrictions on preposition forms, mass/count distinctions and comparative versus superlative forms of adjectives. As a few examples from the English grammar, v np le represents a basic transitive verb, while n pp c-of le represents a count noun that optionally takes a prepositional phrase complement headed by of. The full definition of a lextype consists of a many-featured AVM (attribute value matrix), but the type names have been deliberately chosen to represent the main features of each type. In the Dridan (2009) work, parse ranking showed some improvement when morphological information was added to the tags. Hence, we also look at more fine-grained tags constructed by concatenating appropriate morphological rules onto the lextypes, as in v np le:past verb orule (ie a simple transitive verb with past tense). We used these tags by extracting the tag sequence Test Set Exact Match F-score lextype +morph lextype +morph tc-006 40.49 41.37 0.903 0.903 jhpstgt 32.93 32.93 0.862 0.858 catb 20.41 19.85 0.798 0.794 Table 6: Accuracy using gold tag sequence compatibility to select the ‘gold’ parse(s). from the l</context>
</contexts>
<marker>Dridan, 2009</marker>
<rawString>Rebecca Dridan. 2009. Using lexical statistics to improve HPSG parsing. Ph.D. thesis, Saarland University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Flickinger</author>
</authors>
<title>On building a more efficient grammar by exploiting types.</title>
<date>2002</date>
<booktitle>Collaborative Language Engineering,</booktitle>
<pages>1--17</pages>
<editor>In Stephan Oepen, Dan Flickinger, Jun’ichi Tsujii, and Hans Uszkoreit, editors,</editor>
<publisher>CSLI Publications.</publisher>
<location>Stanford:</location>
<contexts>
<context position="7932" citStr="Flickinger (2002)" startWordPosition="1280" endWordPosition="1281">ods work, and show which methods are worth trying in the more time-consuming and resourceintensive future experiments on other languages. It is worth reinforcing that the gold-standard data is used for evaluation only, except in calculating the supervised parse selection accuracy as an upperbound. The English Resource Grammar (ERG: 695 Average Average Language Sentences words parses Japanese 6769 10.5 49.6 English 4855 9.0 59.5 Table 1: Initial model training data, showing the average word length per sentence, and also the ambiguity measured as the average number of parses found per sentence. Flickinger (2002)) is an HPSG-based grammar of English that has been under development for many person years. In order to examine the cross-lingual applicability of our methods, we also use Jacy, an HPSG-based grammar of Japanese (Siegel and Bender, 2002). In both cases, we use grammar versions from the “Barcelona” release, from mid-2009. 3.1 Training Data Both of our grammars come with statistical models, and the parsed data and gold standard annotations used to create these models are freely available. As we are trying to simulate a fully unsupervised setup, we didn’t want any influence from these earlier mo</context>
</contexts>
<marker>Flickinger, 2002</marker>
<rawString>Dan Flickinger. 2002. On building a more efficient grammar by exploiting types. In Stephan Oepen, Dan Flickinger, Jun’ichi Tsujii, and Hans Uszkoreit, editors, Collaborative Language Engineering, pages 1– 17. Stanford: CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tadayoshi Hara</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Evaluating impact of re-training a lexical disambiguation model on domain adaptation of an HPSG parser.</title>
<date>2007</date>
<booktitle>In Proceedings of the 10th International Conference on Parsing Technology (IWPT</booktitle>
<pages>11--22</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="20932" citStr="Hara et al., 2007" startWordPosition="3452" endWordPosition="3455">rse forest. This 5All statistical significance tests in these experiments use the computationally-intensive randomisation test described in Yeh (2000), with P &lt; 0.05. 698 work has shown that tag sequences can be successfully used to restrict the set of parses produced, but generally are not discriminatory enough to distinguish a single best parse. Toutanova et al. (2002) present a similar exploration but also go on to include probabilities from a HMM model into the parse selection model as features. There has also been some work on using lexical probabilities for domain adaptation of a model (Hara et al., 2007; Rimell and Clark, 2008). In Dridan (2009), tag sequences from a supertagger are used together with other factors to re-rank the top 500 parses from the same parser and English grammar we use in this research, and achieve some improvement in the ranking where tagger accuracy is sufficiently high. We use a similar method, one level removed, in that we use the tag sequences to select the ‘gold’ parse(s) that are then used to train a model, as in the previous sections. 5.1 Gold Supertags In order to test the viability of this method, we first experimented using gold standard tags, extracted from</context>
</contexts>
<marker>Hara, Miyao, Tsujii, 2007</marker>
<rawString>Tadayoshi Hara, Yusuke Miyao, and Jun’ichi Tsujii. 2007. Evaluating impact of re-training a lexical disambiguation model on domain adaptation of an HPSG parser. In Proceedings of the 10th International Conference on Parsing Technology (IWPT 2007), pages 11–22, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: A corpus of CCG derivations and dependency structures extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="12597" citStr="Hockenmaier and Steedman, 2007" startWordPosition="2053" endWordPosition="2057"> useful metric for parse selection evaluation, but it is very blunt-edged, and gives no way of evaluating how close the top parse was to the gold standard. Since these are very detailed analyses, it is possible to get one detail wrong and still have a useful analysis. Hence, in addition to exact match, we also use the EDMNA evaluation defined by Dridan (2009). This is a predicate– argument style evaluation, based on the semantic output of the parser (MRS: Minimal Recursion Semantics (Copestake et al., 2005)). This metric is broadly comparable to the predicate–argument dependencies of CCGBank (Hockenmaier and Steedman, 2007) or of the ENJU grammar (Miyao and Tsujii, 2008), and also somewhat similar to the grammatical relations (GR) of the Briscoe and Carroll (2006) version of DepBank. The EDMNA metric matches triples consisting of predicate names and the argument type that connects them. 4 Initial Experiments All of our experiments are based on the same basic process: (1) for each sentence in the training data described in Section 3.1, label a subset of analyses as correct and the remainder as incorrect; (2) train a model using the same features and learner as in the standard process of Section 2; (3) parse the t</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A corpus of CCG derivations and dependency structures extracted from the Penn Treebank. Computational Linguistics, 33(3):355–396, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Why doesnt EM find good HMM POS-taggers?</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>296--305</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="29432" citStr="Johnson (2007)" startWordPosition="4859" endWordPosition="4860">apanese. However, this insignificant tagger accuracy decrease for Japanese produced a significant increase in parser accuracy, while a more pronounced tagger accuracy decrease had no significant effect on parser accuracy in English. 700 Language Initial counts EM trained Japanese 84.4 83.3 English 71.7 64.6 Table 9: Tagger accuracy over the training data, using both the initial counts and the EM trained models. There is much potential for further work in this direction, experimenting with more training data or more estimation iterations, or even looking at different estimators as suggested in Johnson (2007) and Ravi et al. (2010b). There is also the issue of whether tag accuracy is the best measure for indicating potential parse accuracy. The Japanese parsing results are already equivalent to those achieved using gold standard tags. It is possible that parsing accuracy is reasonably insensitive to tagger accuracy, but it is also possible that there is a better metric to look at, such as tag accuracy over frequently confused tags. 6 Discussion The results of Table 8 show that, using no human annotated data, we can get exact match results that are almost half way between our random baseline and ou</context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>Mark Johnson. 2007. Why doesnt EM find good HMM POS-taggers? In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 296–305, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
</authors>
<title>A comparison of algorithms for maximum entropy parameter estimation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 6th Conference on Natural Language Learning,</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="5163" citStr="Malouf, 2002" startWordPosition="822" endWordPosition="823">the grammar is changed, the treebank can be quickly updated by re-parsing and re-applying the old annotation decisions. This treebanking process not only produces gold standard trees, but also a set of non-gold trees which provides the negative training data necessary for a discriminative maximum entropy model. The standard process for creating a parse selection model is: 1. parse the training set, recording up to 500 highest-ranking parses for each sentence; 2. treebank the training set; 3. extract features from the gold and non-gold parses; 4. learn feature weights using the TADM toolkit.l (Malouf, 2002) The useful training data from this process is the parses from those sentences for which: more than one parse was found; and at least one parse has been annotated as correct. That is, there needs to be both gold and non-gold trees for any sentence to be used in training the discriminative model. lhttp://tadm.sourceforge.net/ There are two issues with this process for new grammars. Firstly, treebanking takes many personhours, and is hence both time-consuming and expensive. Complicating that is the second issue: Nbest parsing requires a statistical model. While it is possible to parse exhaustive</context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>Robert Malouf. 2002. A comparison of algorithms for maximum entropy parameter estimation. In Proceedings of the 6th Conference on Natural Language Learning, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL,</booktitle>
<pages>152--159</pages>
<location>New York City, USA.</location>
<contexts>
<context position="31079" citStr="McClosky et al., 2006" startWordPosition="5145" endWordPosition="5148">usively show the efficacy of our methods in that situation requires a long-term experiment that we are now starting, based on the results we have here. Another possible use for these methods was alluded to in Section 2: producing a new model for a new domain. Results at every stage have been much worse for the catb data set, compared to the other jhpstgt English data set. While sentence length plays some part, the major reason for this discrepancy was domain mismatch between the training and test data. One method that has been successfully used for domain adaption in parsing is self-training (McClosky et al., 2006). In this process, data from the new domain is parsed with the parser trained on the old doSource of ‘Gold’ Data Exact Match F-score Random Selection 8.30 0.671 Supertags (initial counts) 14.61 0.782 Gold Standard 22.29 0.839 Self-training 15.92 0.791 Table 10: Accuracy results over the out-of-domain cath data set, using the initial counts unsupervised model to produce in-domain training data in a self-training set up. The previous results are shown for easy comparison. main, and then the top analyses of the parsed new domain data are added to the training data, and the parser is re-trained. T</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, pages 152–159, New York City, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Merialdo</author>
</authors>
<title>Tagging english text with a probabilistic model.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="24153" citStr="Merialdo (1994)" startWordPosition="3995" endWordPosition="3996"> heuristic methods tried earlier. This suggested that, at least with a reasonably accurate tagger, this was a viable strategy for training a model. With no significant difference between the basic and +morph versions of the tag set, we decided to use the basic lextypes as tags, since a smaller tag set should be easier to tag with. However, we first had to train a tagger, without using any gold standard data. 5.2 Unsupervised Supertagging Research into unsupervised part-of-speech tagging with a tag dictionary (sometimes called weakly supervised POS tagging) has been going on for many years (cf Merialdo (1994), Brill (1995)), but generally using a fairly small tag set. The only work we know of on unsupervised tagging for the more complex supertags is from Baldridge (2008), and more recently, Ravi et al. (2010a). In this work, the constraining nature of the (CCG) grammar is used to mitigate the problem of having a much more ambiguous tag set. Our method has a similar underlying idea, but the implementation differs both in the way we extract the word-to-tag mappings, and also how we extract and use the information from the grammar to initialise the tagger model. We chose to use a simple first-order H</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>Bernard Merialdo. 1994. Tagging english text with a probabilistic model. Computational Linguistics, 20(2):155–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Feature forest models for probabilistic HPSG parsing.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="3853" citStr="Miyao and Tsujii, 2008" startWordPosition="615" endWordPosition="618"> can’t depend on having any external information or NLP tools, and so the methods we examine are purely unsupervised, using nothing more than the grammars them694 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 694–704, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics selves and raw text. We find that, not only can we produce models that are suitable for kick-starting the treebanking process, but the accuracy of these models is comparable to parsers trained on gold standard data (Clark and Curran, 2007b; Miyao and Tsujii, 2008), which have been successfully used in applications (Miyao et al., 2008). 2 The problem The current method of training a parse selection model uses the [incr tsdb()] treebanking mechanism (Oepen, 2001) and works well for updating models for mature grammars, although even for these grammars, building a new model for a different domain requires a time-consuming initial treebanking effort. The treebanks used with DELPH-IN grammars are dynamic treebanks (Oepen et al., 2004) created by parsing text and having an annotator select the correct analysis (or discard all of them). The annotation process </context>
<context position="12645" citStr="Miyao and Tsujii, 2008" startWordPosition="2063" endWordPosition="2066"> very blunt-edged, and gives no way of evaluating how close the top parse was to the gold standard. Since these are very detailed analyses, it is possible to get one detail wrong and still have a useful analysis. Hence, in addition to exact match, we also use the EDMNA evaluation defined by Dridan (2009). This is a predicate– argument style evaluation, based on the semantic output of the parser (MRS: Minimal Recursion Semantics (Copestake et al., 2005)). This metric is broadly comparable to the predicate–argument dependencies of CCGBank (Hockenmaier and Steedman, 2007) or of the ENJU grammar (Miyao and Tsujii, 2008), and also somewhat similar to the grammatical relations (GR) of the Briscoe and Carroll (2006) version of DepBank. The EDMNA metric matches triples consisting of predicate names and the argument type that connects them. 4 Initial Experiments All of our experiments are based on the same basic process: (1) for each sentence in the training data described in Section 3.1, label a subset of analyses as correct and the remainder as incorrect; (2) train a model using the same features and learner as in the standard process of Section 2; (3) parse the test data using that model; and (4) evaluate the </context>
</contexts>
<marker>Miyao, Tsujii, 2008</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature forest models for probabilistic HPSG parsing. Computational Linguistics, 34(1):35–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Kenji Sagae</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Towards framework-independent evaluation of deep linguistic parsers.</title>
<date>2007</date>
<booktitle>In Proceedings of the GEAF 2007 Workshop,</booktitle>
<location>Palo Alto, California.</location>
<contexts>
<context position="30275" citStr="Miyao et al., 2007" startWordPosition="5002" endWordPosition="5005"> tags. It is possible that parsing accuracy is reasonably insensitive to tagger accuracy, but it is also possible that there is a better metric to look at, such as tag accuracy over frequently confused tags. 6 Discussion The results of Table 8 show that, using no human annotated data, we can get exact match results that are almost half way between our random baseline and our gold-standard-trained upperbound. EDM Fscores of 90% and 83% over in-domain data compare well with dependency-based scores from other parsers, although a direct comparison is very difficult to do (Clark and Curran, 2007a; Miyao et al., 2007). It still remains to see whether this level of accuracy is good enough to be useful. The main aim of this work is to bootstrap the treebanking process for new grammars, but to conclusively show the efficacy of our methods in that situation requires a long-term experiment that we are now starting, based on the results we have here. Another possible use for these methods was alluded to in Section 2: producing a new model for a new domain. Results at every stage have been much worse for the catb data set, compared to the other jhpstgt English data set. While sentence length plays some part, the </context>
</contexts>
<marker>Miyao, Sagae, Tsujii, 2007</marker>
<rawString>Yusuke Miyao, Kenji Sagae, and Jun’ichi Tsujii. 2007. Towards framework-independent evaluation of deep linguistic parsers. In Proceedings of the GEAF 2007 Workshop, Palo Alto, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Rune Sætre</author>
<author>Kenji Sagae</author>
<author>Takuya Matsuzaki</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Task-oriented evaluation of syntactic parsers and their representations.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the ACL,</booktitle>
<pages>46--54</pages>
<location>Columbus, USA.</location>
<contexts>
<context position="3925" citStr="Miyao et al., 2008" startWordPosition="627" endWordPosition="630">thods we examine are purely unsupervised, using nothing more than the grammars them694 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 694–704, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics selves and raw text. We find that, not only can we produce models that are suitable for kick-starting the treebanking process, but the accuracy of these models is comparable to parsers trained on gold standard data (Clark and Curran, 2007b; Miyao and Tsujii, 2008), which have been successfully used in applications (Miyao et al., 2008). 2 The problem The current method of training a parse selection model uses the [incr tsdb()] treebanking mechanism (Oepen, 2001) and works well for updating models for mature grammars, although even for these grammars, building a new model for a different domain requires a time-consuming initial treebanking effort. The treebanks used with DELPH-IN grammars are dynamic treebanks (Oepen et al., 2004) created by parsing text and having an annotator select the correct analysis (or discard all of them). The annotation process involves making binary decisions based on so-called parse discriminants </context>
</contexts>
<marker>Miyao, Sætre, Sagae, Matsuzaki, Tsujii, 2008</marker>
<rawString>Yusuke Miyao, Rune Sætre, Kenji Sagae, Takuya Matsuzaki, and Jun’ichi Tsujii. 2008. Task-oriented evaluation of syntactic parsers and their representations. In Proceedings of the 46th Annual Meeting of the ACL, pages 46–54, Columbus, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
<author>John Carroll</author>
</authors>
<title>Ambiguity packing in constraint-based parsing - practical results.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>162--169</pages>
<location>Seattle, USA.</location>
<contexts>
<context position="1317" citStr="Oepen and Carroll, 2000" startWordPosition="199" endWordPosition="202">reebank without a model requires more resources than subsequent treebanks. In this work, we show that, by taking advantage of the constrained nature of these BPSG grammars, we can learn a discriminative parse selection model from raw text in a purely unsupervised fashion. This allows us to bootstrap the treebanking process and provide better parsers faster, and with less resources. 1 Introduction Parsing with precision grammars is generally a twostage process: (1) the full parse yield of the precision grammar is calculated for a given item, often in the form of a packed forest for efficiency (Oepen and Carroll, 2000; Zhang et al., 2007); and (2) the individual analyses in the parse forest are ranked using a statistical model (“parse selection”). In the domain of treebank parsing, the Charniak and Johnson (2005) reranking parser adopts an analogous strategy, except that ranking and pruning are incorporated into the first stage, and the second stage is based on only the top-ranked parses from the first stage. For both styles of parsing, however, parse selection is based on a statistical model learned from a pre-existing treebank associated with the grammar. Our interest in this paper is in completely remov</context>
</contexts>
<marker>Oepen, Carroll, 2000</marker>
<rawString>Stephan Oepen and John Carroll. 2000. Ambiguity packing in constraint-based parsing - practical results. In Proceedings of the 1st Conference of the North American Chapter of the Association for Computational Linguistics, pages 162–169, Seattle, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
<author>Dan Flickinger</author>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
</authors>
<title>LinGO redwoods. a rich and dynamic treebank for HPSG.</title>
<date>2004</date>
<journal>Journal of Research in Language and Computation,</journal>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="4327" citStr="Oepen et al., 2004" startWordPosition="689" endWordPosition="692">rocess, but the accuracy of these models is comparable to parsers trained on gold standard data (Clark and Curran, 2007b; Miyao and Tsujii, 2008), which have been successfully used in applications (Miyao et al., 2008). 2 The problem The current method of training a parse selection model uses the [incr tsdb()] treebanking mechanism (Oepen, 2001) and works well for updating models for mature grammars, although even for these grammars, building a new model for a different domain requires a time-consuming initial treebanking effort. The treebanks used with DELPH-IN grammars are dynamic treebanks (Oepen et al., 2004) created by parsing text and having an annotator select the correct analysis (or discard all of them). The annotation process involves making binary decisions based on so-called parse discriminants (Carter, 1997). Whenever the grammar is changed, the treebank can be quickly updated by re-parsing and re-applying the old annotation decisions. This treebanking process not only produces gold standard trees, but also a set of non-gold trees which provides the negative training data necessary for a discriminative maximum entropy model. The standard process for creating a parse selection model is: 1.</context>
<context position="10474" citStr="Oepen et al., 2004" startWordPosition="1700" endWordPosition="1703">age word length per sentence, and also the ambiguity measured as the average number of parses found per sentence. Note that the ambiguity figures for the English test sets are under-estimates, since some of the longer sentences timed out before giving an analysis count. the same Tanaka Corpus (Tanaka, 2001) which was used for the Japanese training data. There is a wider variety of treebanked data available for the English grammar than for the Japanese. We use the jhpstgt data set, which consists of text from Norwegian tourism brochures, from the same LOGON corpus as the English training data (Oepen et al., 2004). In order to have some idea of domain effects, we also use the catb data set, the text of an essay on opensource development.3 We see here that the sentences are longer, particularly for the English data. Also, since we are not artificially limiting the parse ambiguity by ignoring those with 500 or more parses, the ambiguity is much higher. This ambiguity figure gives some indication of the difficulty of the parse selection task. Again we see that the English sentences are more ambiguous, much more in this case, making the parse selection task difficult. In fact, the English ambiguity figures</context>
</contexts>
<marker>Oepen, Flickinger, Toutanova, Manning, 2004</marker>
<rawString>Stephan Oepen, Dan Flickinger, Kristina Toutanova, and Christopher D. Manning. 2004. LinGO redwoods. a rich and dynamic treebank for HPSG. Journal of Research in Language and Computation, 2(4):575–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
</authors>
<title>incr tsdb()] – competence and performance laboratory. User manual,</title>
<date>2001</date>
<institution>Computational Linguistics, Saarland University,</institution>
<location>Saarbr¨ucken, Germany.</location>
<contexts>
<context position="4054" citStr="Oepen, 2001" startWordPosition="649" endWordPosition="650">ethods in Natural Language Processing, pages 694–704, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics selves and raw text. We find that, not only can we produce models that are suitable for kick-starting the treebanking process, but the accuracy of these models is comparable to parsers trained on gold standard data (Clark and Curran, 2007b; Miyao and Tsujii, 2008), which have been successfully used in applications (Miyao et al., 2008). 2 The problem The current method of training a parse selection model uses the [incr tsdb()] treebanking mechanism (Oepen, 2001) and works well for updating models for mature grammars, although even for these grammars, building a new model for a different domain requires a time-consuming initial treebanking effort. The treebanks used with DELPH-IN grammars are dynamic treebanks (Oepen et al., 2004) created by parsing text and having an annotator select the correct analysis (or discard all of them). The annotation process involves making binary decisions based on so-called parse discriminants (Carter, 1997). Whenever the grammar is changed, the treebank can be quickly updated by re-parsing and re-applying the old annota</context>
</contexts>
<marker>Oepen, 2001</marker>
<rawString>Stephan Oepen. 2001. [incr tsdb()] – competence and performance laboratory. User manual, Computational Linguistics, Saarland University, Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago, USA.</location>
<contexts>
<context position="2159" citStr="Pollard and Sag, 1994" startWordPosition="339" endWordPosition="342">pts an analogous strategy, except that ranking and pruning are incorporated into the first stage, and the second stage is based on only the top-ranked parses from the first stage. For both styles of parsing, however, parse selection is based on a statistical model learned from a pre-existing treebank associated with the grammar. Our interest in this paper is in completely removing this requirement of parse selection on explicitly treebanked data, ie the development of fully unsupervised parse selection models. The particular style of precision grammar we experiment with in this paper is HPSG (Pollard and Sag, 1994), in the form of the DELPH-IN suite of grammars (http://www.delph-in.net/). One of the main focuses of the DELPH-IN collaboration effort is multilinguality. To this end, the Grammar Matrix project (Bender et al., 2002) has been developed which, through a set of questionnaires, allows grammar engineers to quickly produce a core grammar for a language of their choice. Bender (2008) showed that by using and expanding on this core grammar, she was able to produce a broad-coverage precision grammar of Wambaya in a very short amount of time. However, the Grammar Matrix can only help with the first s</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar. University of Chicago Press, Chicago, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Jason Baldridge</author>
<author>Kevin Knight</author>
</authors>
<title>Minimized models and grammar-informed initialization for supertagging with highly ambiguous lexicons.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>495--503</pages>
<location>Uppsala,</location>
<contexts>
<context position="24356" citStr="Ravi et al. (2010" startWordPosition="4030" endWordPosition="4033">+morph versions of the tag set, we decided to use the basic lextypes as tags, since a smaller tag set should be easier to tag with. However, we first had to train a tagger, without using any gold standard data. 5.2 Unsupervised Supertagging Research into unsupervised part-of-speech tagging with a tag dictionary (sometimes called weakly supervised POS tagging) has been going on for many years (cf Merialdo (1994), Brill (1995)), but generally using a fairly small tag set. The only work we know of on unsupervised tagging for the more complex supertags is from Baldridge (2008), and more recently, Ravi et al. (2010a). In this work, the constraining nature of the (CCG) grammar is used to mitigate the problem of having a much more ambiguous tag set. Our method has a similar underlying idea, but the implementation differs both in the way we extract the word-to-tag mappings, and also how we extract and use the information from the grammar to initialise the tagger model. We chose to use a simple first-order Hidden Markov Model (HMM) tagger, using the implemen699 tation of Dekang Lin,6 which re-estimates probabilities, given an initial model, using the Baum-Welch variant of the Expectation-Maximisation (EM) a</context>
<context position="29454" citStr="Ravi et al. (2010" startWordPosition="4862" endWordPosition="4865">his insignificant tagger accuracy decrease for Japanese produced a significant increase in parser accuracy, while a more pronounced tagger accuracy decrease had no significant effect on parser accuracy in English. 700 Language Initial counts EM trained Japanese 84.4 83.3 English 71.7 64.6 Table 9: Tagger accuracy over the training data, using both the initial counts and the EM trained models. There is much potential for further work in this direction, experimenting with more training data or more estimation iterations, or even looking at different estimators as suggested in Johnson (2007) and Ravi et al. (2010b). There is also the issue of whether tag accuracy is the best measure for indicating potential parse accuracy. The Japanese parsing results are already equivalent to those achieved using gold standard tags. It is possible that parsing accuracy is reasonably insensitive to tagger accuracy, but it is also possible that there is a better metric to look at, such as tag accuracy over frequently confused tags. 6 Discussion The results of Table 8 show that, using no human annotated data, we can get exact match results that are almost half way between our random baseline and our gold-standard-traine</context>
</contexts>
<marker>Ravi, Baldridge, Knight, 2010</marker>
<rawString>Sujith Ravi, Jason Baldridge, and Kevin Knight. 2010a. Minimized models and grammar-informed initialization for supertagging with highly ambiguous lexicons. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 495–503, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Ashish Vaswani</author>
<author>Kevin Knight</author>
<author>David Chiang</author>
</authors>
<title>Fast, greedy model minimization for unsupervised tagging.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>940--948</pages>
<location>Beijing, China.</location>
<contexts>
<context position="24356" citStr="Ravi et al. (2010" startWordPosition="4030" endWordPosition="4033">+morph versions of the tag set, we decided to use the basic lextypes as tags, since a smaller tag set should be easier to tag with. However, we first had to train a tagger, without using any gold standard data. 5.2 Unsupervised Supertagging Research into unsupervised part-of-speech tagging with a tag dictionary (sometimes called weakly supervised POS tagging) has been going on for many years (cf Merialdo (1994), Brill (1995)), but generally using a fairly small tag set. The only work we know of on unsupervised tagging for the more complex supertags is from Baldridge (2008), and more recently, Ravi et al. (2010a). In this work, the constraining nature of the (CCG) grammar is used to mitigate the problem of having a much more ambiguous tag set. Our method has a similar underlying idea, but the implementation differs both in the way we extract the word-to-tag mappings, and also how we extract and use the information from the grammar to initialise the tagger model. We chose to use a simple first-order Hidden Markov Model (HMM) tagger, using the implemen699 tation of Dekang Lin,6 which re-estimates probabilities, given an initial model, using the Baum-Welch variant of the Expectation-Maximisation (EM) a</context>
<context position="29454" citStr="Ravi et al. (2010" startWordPosition="4862" endWordPosition="4865">his insignificant tagger accuracy decrease for Japanese produced a significant increase in parser accuracy, while a more pronounced tagger accuracy decrease had no significant effect on parser accuracy in English. 700 Language Initial counts EM trained Japanese 84.4 83.3 English 71.7 64.6 Table 9: Tagger accuracy over the training data, using both the initial counts and the EM trained models. There is much potential for further work in this direction, experimenting with more training data or more estimation iterations, or even looking at different estimators as suggested in Johnson (2007) and Ravi et al. (2010b). There is also the issue of whether tag accuracy is the best measure for indicating potential parse accuracy. The Japanese parsing results are already equivalent to those achieved using gold standard tags. It is possible that parsing accuracy is reasonably insensitive to tagger accuracy, but it is also possible that there is a better metric to look at, such as tag accuracy over frequently confused tags. 6 Discussion The results of Table 8 show that, using no human annotated data, we can get exact match results that are almost half way between our random baseline and our gold-standard-traine</context>
</contexts>
<marker>Ravi, Vaswani, Knight, Chiang, 2010</marker>
<rawString>Sujith Ravi, Ashish Vaswani, Kevin Knight, and David Chiang. 2010b. Fast, greedy model minimization for unsupervised tagging. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 940–948, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Rimell</author>
<author>Stephen Clark</author>
</authors>
<title>Adapting a lexicalized-grammar parser to contrasting domains.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>475--484</pages>
<location>Honolulu, USA.</location>
<contexts>
<context position="20957" citStr="Rimell and Clark, 2008" startWordPosition="3456" endWordPosition="3459">ll statistical significance tests in these experiments use the computationally-intensive randomisation test described in Yeh (2000), with P &lt; 0.05. 698 work has shown that tag sequences can be successfully used to restrict the set of parses produced, but generally are not discriminatory enough to distinguish a single best parse. Toutanova et al. (2002) present a similar exploration but also go on to include probabilities from a HMM model into the parse selection model as features. There has also been some work on using lexical probabilities for domain adaptation of a model (Hara et al., 2007; Rimell and Clark, 2008). In Dridan (2009), tag sequences from a supertagger are used together with other factors to re-rank the top 500 parses from the same parser and English grammar we use in this research, and achieve some improvement in the ranking where tagger accuracy is sufficiently high. We use a similar method, one level removed, in that we use the tag sequences to select the ‘gold’ parse(s) that are then used to train a model, as in the previous sections. 5.1 Gold Supertags In order to test the viability of this method, we first experimented using gold standard tags, extracted from the gold standard parses</context>
</contexts>
<marker>Rimell, Clark, 2008</marker>
<rawString>Laura Rimell and Stephen Clark. 2008. Adapting a lexicalized-grammar parser to contrasting domains. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP 2008), pages 475–484, Honolulu, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Aravind K Joshi</author>
</authors>
<title>Parsing with lexicalized tree adjoining grammar.</title>
<date>1991</date>
<booktitle>In Masaru Tomita, editor, Current Issues in Parsing Technology, chapter 3,</booktitle>
<pages>25--48</pages>
<publisher>Kluwer.</publisher>
<contexts>
<context position="19960" citStr="Schabes and Joshi, 1991" startWordPosition="3290" endWordPosition="3293">for the Edges method of tc-006 being at a level of statistical significance.5 The only statistically significant difference between the Edges and Branching methods is over the jhpstgt data set. While improvement over random is encouraging, the results were still uninspiring and so we moved on to slightly more complex methods, described in the next section. 5 Supertagging Experiments The term supertags was first used by Bangalore and Joshi (1999) to describe fine-grained part of speech tags which include some structural or dependency information. In that original work, the supertags were LTAG (Schabes and Joshi, 1991) elementary trees, and they were used for the purpose of speeding up parsing by restricting the allowable leaf types. Subsequent work involving supertags has mostly focussed on this efficiency goal, but they can also be used to inform parse selection. Dalrymple (2006) and Blunsom (2007) both look at how discriminatory a tag sequence is in filtering a parse forest. This 5All statistical significance tests in these experiments use the computationally-intensive randomisation test described in Yeh (2000), with P &lt; 0.05. 698 work has shown that tag sequences can be successfully used to restrict the</context>
</contexts>
<marker>Schabes, Joshi, 1991</marker>
<rawString>Yves Schabes and Aravind K. Joshi. 1991. Parsing with lexicalized tree adjoining grammar. In Masaru Tomita, editor, Current Issues in Parsing Technology, chapter 3, pages 25–48. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Melanie Siegel</author>
<author>Emily M Bender</author>
</authors>
<title>Efficient deep processing of japanese.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd Workshop on Asian Language Resources and International Standardization. Coling</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="8170" citStr="Siegel and Bender, 2002" startWordPosition="1316" endWordPosition="1319">lculating the supervised parse selection accuracy as an upperbound. The English Resource Grammar (ERG: 695 Average Average Language Sentences words parses Japanese 6769 10.5 49.6 English 4855 9.0 59.5 Table 1: Initial model training data, showing the average word length per sentence, and also the ambiguity measured as the average number of parses found per sentence. Flickinger (2002)) is an HPSG-based grammar of English that has been under development for many person years. In order to examine the cross-lingual applicability of our methods, we also use Jacy, an HPSG-based grammar of Japanese (Siegel and Bender, 2002). In both cases, we use grammar versions from the “Barcelona” release, from mid-2009. 3.1 Training Data Both of our grammars come with statistical models, and the parsed data and gold standard annotations used to create these models are freely available. As we are trying to simulate a fully unsupervised setup, we didn’t want any influence from these earlier models. Hence, in our experiments we used the parsed data from those sentences that received less than 500 parses and ignored any ranking, thus annulling the effects of the statistical model. This led to a reduced data set, both in the numb</context>
</contexts>
<marker>Siegel, Bender, 2002</marker>
<rawString>Melanie Siegel and Emily M. Bender. 2002. Efficient deep processing of japanese. In Proceedings of the 3rd Workshop on Asian Language Resources and International Standardization. Coling 2002 Post-Conference Workshop., Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yasuhito Tanaka</author>
</authors>
<title>Compilation of a multilingual parallel corpus.</title>
<date>2001</date>
<booktitle>In Proceedings of PACLING</booktitle>
<pages>265--268</pages>
<location>Kitakyushu, Japan.</location>
<contexts>
<context position="10163" citStr="Tanaka, 2001" startWordPosition="1648" endWordPosition="1649"> both gold and non-gold analyses (ie, had no correct parse, only one parse, or none) are not included in these figures. Average Average Test Set Language Sentences words parses tc-006 Japanese 904 10.7 383.9 jhpstgt English 748 12.8 4115.1 catb English 534 17.6 9427.3 Table 2: Test data, showing the average word length per sentence, and also the ambiguity measured as the average number of parses found per sentence. Note that the ambiguity figures for the English test sets are under-estimates, since some of the longer sentences timed out before giving an analysis count. the same Tanaka Corpus (Tanaka, 2001) which was used for the Japanese training data. There is a wider variety of treebanked data available for the English grammar than for the Japanese. We use the jhpstgt data set, which consists of text from Norwegian tourism brochures, from the same LOGON corpus as the English training data (Oepen et al., 2004). In order to have some idea of domain effects, we also use the catb data set, the text of an essay on opensource development.3 We see here that the sentences are longer, particularly for the English data. Also, since we are not artificially limiting the parse ambiguity by ignoring those </context>
</contexts>
<marker>Tanaka, 2001</marker>
<rawString>Yasuhito Tanaka. 2001. Compilation of a multilingual parallel corpus. In Proceedings of PACLING 2001, pages 265–268, Kitakyushu, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Chistopher D Manning</author>
<author>Stuart M Shieber</author>
<author>Dan Flickinger</author>
<author>Stephan Oepen</author>
</authors>
<title>Parse disambiguation for a rich HPSG grammar.</title>
<date>2002</date>
<booktitle>In First Workshop on Treebanks and Linguistic Theories (TLT2002),</booktitle>
<pages>253--263</pages>
<contexts>
<context position="20688" citStr="Toutanova et al. (2002)" startWordPosition="3409" endWordPosition="3412"> leaf types. Subsequent work involving supertags has mostly focussed on this efficiency goal, but they can also be used to inform parse selection. Dalrymple (2006) and Blunsom (2007) both look at how discriminatory a tag sequence is in filtering a parse forest. This 5All statistical significance tests in these experiments use the computationally-intensive randomisation test described in Yeh (2000), with P &lt; 0.05. 698 work has shown that tag sequences can be successfully used to restrict the set of parses produced, but generally are not discriminatory enough to distinguish a single best parse. Toutanova et al. (2002) present a similar exploration but also go on to include probabilities from a HMM model into the parse selection model as features. There has also been some work on using lexical probabilities for domain adaptation of a model (Hara et al., 2007; Rimell and Clark, 2008). In Dridan (2009), tag sequences from a supertagger are used together with other factors to re-rank the top 500 parses from the same parser and English grammar we use in this research, and achieve some improvement in the ranking where tagger accuracy is sufficiently high. We use a similar method, one level removed, in that we us</context>
</contexts>
<marker>Toutanova, Manning, Shieber, Flickinger, Oepen, 2002</marker>
<rawString>Kristina Toutanova, Chistopher D. Manning, Stuart M. Shieber, Dan Flickinger, and Stephan Oepen. 2002. Parse disambiguation for a rich HPSG grammar. In First Workshop on Treebanks and Linguistic Theories (TLT2002), pages 253–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of result differences.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics (COLING</booktitle>
<pages>947--953</pages>
<location>Saarbrcken, Germany.</location>
<contexts>
<context position="20465" citStr="Yeh (2000)" startWordPosition="3371" endWordPosition="3372">tural or dependency information. In that original work, the supertags were LTAG (Schabes and Joshi, 1991) elementary trees, and they were used for the purpose of speeding up parsing by restricting the allowable leaf types. Subsequent work involving supertags has mostly focussed on this efficiency goal, but they can also be used to inform parse selection. Dalrymple (2006) and Blunsom (2007) both look at how discriminatory a tag sequence is in filtering a parse forest. This 5All statistical significance tests in these experiments use the computationally-intensive randomisation test described in Yeh (2000), with P &lt; 0.05. 698 work has shown that tag sequences can be successfully used to restrict the set of parses produced, but generally are not discriminatory enough to distinguish a single best parse. Toutanova et al. (2002) present a similar exploration but also go on to include probabilities from a HMM model into the parse selection model as features. There has also been some work on using lexical probabilities for domain adaptation of a model (Hara et al., 2007; Rimell and Clark, 2008). In Dridan (2009), tag sequences from a supertagger are used together with other factors to re-rank the top</context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>Alexander Yeh. 2000. More accurate tests for the statistical significance of result differences. In Proceedings of the 18th International Conference on Computational Linguistics (COLING 2000), pages 947–953, Saarbrcken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Zhang</author>
<author>Stephan Oepen</author>
<author>John Carroll</author>
</authors>
<title>Efficiency in unification-based n-best parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the 10th international conference on parsing technologies (IWPT</booktitle>
<pages>48--59</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1338" citStr="Zhang et al., 2007" startWordPosition="203" endWordPosition="206">equires more resources than subsequent treebanks. In this work, we show that, by taking advantage of the constrained nature of these BPSG grammars, we can learn a discriminative parse selection model from raw text in a purely unsupervised fashion. This allows us to bootstrap the treebanking process and provide better parsers faster, and with less resources. 1 Introduction Parsing with precision grammars is generally a twostage process: (1) the full parse yield of the precision grammar is calculated for a given item, often in the form of a packed forest for efficiency (Oepen and Carroll, 2000; Zhang et al., 2007); and (2) the individual analyses in the parse forest are ranked using a statistical model (“parse selection”). In the domain of treebank parsing, the Charniak and Johnson (2005) reranking parser adopts an analogous strategy, except that ranking and pruning are incorporated into the first stage, and the second stage is based on only the top-ranked parses from the first stage. For both styles of parsing, however, parse selection is based on a statistical model learned from a pre-existing treebank associated with the grammar. Our interest in this paper is in completely removing this requirement </context>
<context position="5895" citStr="Zhang et al., 2007" startWordPosition="939" endWordPosition="942"> found; and at least one parse has been annotated as correct. That is, there needs to be both gold and non-gold trees for any sentence to be used in training the discriminative model. lhttp://tadm.sourceforge.net/ There are two issues with this process for new grammars. Firstly, treebanking takes many personhours, and is hence both time-consuming and expensive. Complicating that is the second issue: Nbest parsing requires a statistical model. While it is possible to parse exhaustively with no model, parsing is much slower, since the unpacking of results is time-consuming. Selective unpacking (Zhang et al., 2007) speeds this up a great deal, but requires a parse selection model. Treebanking is also much slower when the parser must be run exhaustively, since there are usually many more analyses to manually discard. This work hopes to alleviate both problems. By producing a statistical model without requiring human treebanking, we can have a working and efficient parser with less human effort. Even if the top1 parses this parser produces are not as accurate as those trained on gold standard data, this model can be used to produce the N-best analyses for the treebanker. Since our models are much better t</context>
</contexts>
<marker>Zhang, Oepen, Carroll, 2007</marker>
<rawString>Yi Zhang, Stephan Oepen, and John Carroll. 2007. Efficiency in unification-based n-best parsing. In Proceedings of the 10th international conference on parsing technologies (IWPT 2007), pages 48–59, Prague, Czech Republic.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>