<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002660">
<title confidence="0.988367">
Personalized Normalization for a Multilingual Chat System
</title>
<author confidence="0.772567">
Ai Ti Aw and Lian Hau Lee
</author>
<affiliation confidence="0.758144">
Human Language Technology
Institute for Infocomm Research
1 Fusionopolis Way, #21-01 Connexis, Singapore 138632
</affiliation>
<email confidence="0.99392">
aaiti@i2r.a-star.edu.sg
</email>
<sectionHeader confidence="0.995576" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999941555555555">
This paper describes the personalized
normalization of a multilingual chat system that
supports chatting in user defined short-forms or
abbreviations. One of the major challenges for
multilingual chat realized through machine
translation technology is the normalization of
non-standard, self-created short-forms in the
chat message to standard words before
translation. Due to the lack of training data and
the variations of short-forms used among
different social communities, it is hard to
normalize and translate chat messages if user
uses vocabularies outside the training data and
create short-forms freely. We develop a
personalized chat normalizer for English and
integrate it with a multilingual chat system,
allowing user to create and use personalized
short-forms in multilingual chat.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999918948717949">
Processing user-generated textual content on social
media and networking usually encounters
challenges due to the language used by the online
community. Though some jargons of the online
language has made their way into the standard
dictionary, a large portion of the abbreviations,
slang and context specific terms are still
uncommon and only understood within the user
community. Consequently, content analysis or
translation techniques developed for a more formal
genre like news or even conversations cannot
apply directly and effectively to the social media
content. In recent years, there are many works (Aw
et al., 2006; Cook et al., 2009; Han et al., 2011) on
text normalization to preprocess user generated
content such as tweets and short messages before
further processing. The approaches include
supervised or unsupervised methods based on
morphological and phonetic variations. However,
most of the multilingual chat systems on the
Internet have not yet integrated this feature into
their systems but requesting users to type in proper
language so as to have good translation. This is
because the current techniques are not robust
enough to model the different characteristics
featured in the social media content. Most of the
techniques are developed based on observations
and assumptions made on certain datasets. It is also
difficult to unify the language uniqueness among
different users into a single model.
We propose a practical and effective method,
exploiting a personalized dictionary for each user,
to support the use of user-defined short-forms in a
multilingual chat system - AsiaSpik. The use of this
personalized dictionary reduces the reliance on the
availability and dependency of training data and
empowers the users with the flexibility and
interactivity to include and manage their own
vocabularies during chat.
</bodyText>
<sectionHeader confidence="0.980942" genericHeader="method">
2 ASIASPIK System Overview
</sectionHeader>
<bodyText confidence="0.998142818181818">
AsiaSpik is a web-based multilingual instant
messaging system that enables online chats written
in one language to be readable in other languages
by other users. Figure 1 describes the system
process. It describes the process flow between
Chat Client, Chat Server, Translation Bot and
Normalization Bot whenever Chat Client starts
chat module.
When Chat Client starts chat module, the Chat
Client checks if the normalization option for that
language used by the user is active and activated. If
</bodyText>
<page confidence="0.999621">
31
</page>
<note confidence="0.684352">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 31–36,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.998821777777778">
so, any message sent by the user will be routed to
the Normalization Bot for normalization before
reaching the Chat Server. The Chat Server then
directs the message to the designated recipients.
Chat Client at each recipient invokes a translation
request to the Translation Bot to translate the
message to the language set by the recipient. This
allows the same source message to be received by
different recipients in different target languages.
</bodyText>
<figureCaption confidence="0.995904">
Figure 1. AsiaSpik Chat Process Flow
</figureCaption>
<bodyText confidence="0.999050789473684">
In this system, we use Openfire Chat Server by
Ignite Realtime as our Chat Server. We custom
build a web-based Chat Client to communicate
with the Chat Server based on Jabber/XMPP to
receive presence and messaging information. We
also develop a user management plug-in to
synchronize and authenticate user login. The
translation and normalization function used by the
Translation Bot and Normalization Bot are
provided through Web Services.
The Translation Web Service uses in-house
translation engines and supports the translation
from Chinese, Malay and Indonesian to English
and vice versa. Multilingual chat among these
languages is achieved through pivot translation
using English as the pivot language. The
Normalization Web Service supports only English
normalization. Both web services are running on
Apache Tomcat web server with Apache Axis2.
</bodyText>
<sectionHeader confidence="0.991922" genericHeader="method">
3 Personalized Normalization
</sectionHeader>
<bodyText confidence="0.999612">
Personalized Normalization is the main distinction
of AsiaSpik among other multilingual chat system.
It gives the flexibility for user to personalize
his/her short-forms for messages in English.
</bodyText>
<subsectionHeader confidence="0.831214">
3.1 Related Work
</subsectionHeader>
<bodyText confidence="0.993688">
The traditional text normalization strategy follows
the noisy channel model (Shannon, 1948). Suppose
the chat message is C and its corresponding
standard form is S , the approach aims to find
arg max P(S  |C) by computing
arg max P(C  |S) in which P(S) is usually a
language model and P(C  |S) is an error model.
The objective of using model in the chat message
normalization context is to develop an appropriate
error model for converting the non-standard and
unconventional words found in chat messages into
standard words.
</bodyText>
<equation confidence="0.97903">
^
S = arg max (  |) arg max (  |) ( )
P S C = P C S P S
S S
</equation>
<bodyText confidence="0.998910192307692">
Recently, Aw et al. (2006) model text message
normalization as translation from the texting
language into the standard language. Choudhury et
al. (2007) model the word-level text generation
process for SMS messages, by considering
graphemic/phonetic abbreviations and
unintentional typos as hidden Markov model
(HMM) state transitions and emissions,
respectively. Cook and Stevenson (2009) expand
the error model by introducing inference from
different erroneous formation processes, according
to the sample error distribution. Han and Baldwin
(2011) use a classifier to detect ill-formed words,
and generate correction candidates based on
morphophonemic similarity. These models are
effective on their experiments conducted, however,
much works remain to be done to handle the
diversity and dynamic of content and fast evolution
of words used in social media and networking.
As we notice that unlike spelling errors which
are made mostly unintentionally by the writers,
abbreviations or slangs found in chat messages are
introduced intentionally by the senders most of the
time. This leads us to suggest that if facilities are
given to users to define their abbreviations, the
dynamic of the social content and the fast
</bodyText>
<page confidence="0.99605">
32
</page>
<bodyText confidence="0.999766166666667">
evolution of words could be well captured and
managed by the user. In this way, the
normalization model could be evolved together
with the social media language and chat message
could also be personalized for each user
dynamically and interactively.
</bodyText>
<subsectionHeader confidence="0.996068">
3.2 Personalized Normalization Model
</subsectionHeader>
<bodyText confidence="0.9375785">
We employ a simple but effective approach for
chat normalization. We express normalization
using a probabilistic model as below
sbest  arg max P(s  |c)
</bodyText>
<subsubsectionHeader confidence="0.686864">
s
</subsubsectionHeader>
<bodyText confidence="0.9973075">
and define the probability using a linear
combination of features
</bodyText>
<equation confidence="0.99553525">
m
P s c
(  |)  exp khk(s,c)
k 1
</equation>
<bodyText confidence="0.9998136">
where hk (s, c) are two feature functions namely
the log probability P(si ,j  |ci) of a short-form, ci,
being normalized to a standard form, si ,j ; and the
language model log probability. k are weights of
the feature functions.
We define P(si ,j  |ci) as a uniform distribution
computed through a set of dictionary collected
from corpus, SMS messages and Internet sources.
A total of 11,119 entries are collected and each
entry is assigned with an initial probability,
</bodyText>
<equation confidence="0.9950386">
1
P s c  , where  |ci  |is the number of
s i j i c
( ,  |)
  ||i
</equation>
<bodyText confidence="0.979289111111111">
ci entries defined in the dictionary. We adjust the
probability manually for some entries that are very
common and occur more than a certain threshold,
t , in the NUS SMS corpus (How and Kan, 2005)
with a higher weight-age, w . This model, together
with the language model, forms our baseline
system for chat normalization.
To enable personalized real-time management
of user-defined abbreviations and short-forms, we
define a personalized model Puser _i (si ,j  |ci) for
each user based on his/her dictionary profile. Each
personalized model is loaded into the memory
once the user activates the normalization option.
Whenever there is a change in the entry, the entry’s
probability will be re-distributed and updated
based on the following model. This characterizes
the AsiaSpik system which supports personalized
and dynamic chat normalization.
</bodyText>
<equation confidence="0.988797666666666">
 N
 s i j i
P s c
(  |) if c , S
, i ,
 s D
N M
 i j 
1 if SD ,
c  s  SD
i i j,
M
if SD
c 
i
</equation>
<bodyText confidence="0.999475666666667">
where
SD denotes default dictionary;
N denotes the number of entries
</bodyText>
<equation confidence="0.440078">
c i
M denotes the number of entries
c i
</equation>
<bodyText confidence="0.999815">
The feature weights in the normalization model
are optimized by minimum error rate training
(Och, 2003), which searches for weights
maximizing the normalization accuracy using a
small development set. We use standard state-of-
the-art open source tools, Moses (Koehn, 2007), to
develop the system and the SRI language modeling
toolkit (Stolcke,2003) to train a trigram language
model on the English portion of the Europarl
Corpus (Koehn, 2005).
</bodyText>
<subsectionHeader confidence="0.966973">
3.3 Experiments
</subsectionHeader>
<bodyText confidence="0.999804">
We conducted a small experiment using 134 chat
messages sent by high school students. Out of
these messages, 73 short-forms are uncommon and
not found in our default dictionary. Most of these
</bodyText>
<figure confidence="0.994789942307692">
if
t
1
w
(si
|
,
|
) |
ci
 |ci
) 
t
(si
 |ci
(si
Ps
, j
1
w

,
|
) |
|
if
ci
,j
,j
,j
,ci) |t
(si
, ) |
c t

i
 |(si
 |ci





|

,j
Puser i  N
_  1

 M
in SD
in user dictionary.
</figure>
<page confidence="0.997742">
33
</page>
<bodyText confidence="0.999524">
short-forms are very irregular and hard to predict
their standard forms using morphological and
phonetic similarity. It is also hard to train a
statistical model if training data is not available.
We asked the students to define their personal
abbreviations in the system and run through the
system with and without the user dictionary. We
asked them to give a score of 1 if the output is
acceptable to them as proper English, otherwise a 0
will be given. We compared the results using both
the baseline model and the model implemented
using the same training data as in Aw et al. (2006).
Table 1 shows the number of accepted output
between the two models. Both models show
improvement with the use of user dictionary. It
also shows that it is very critical to have similar
training data for the targeted domain to have good
normalization performance. A simple model helps
if such training data is unavailable. Nevertheless,
the use of a dictionary driven by the user is an
alternative to improve the overall performance.
One reason for the inability of both models to
capture the variations fully is because many
messages require some degree of rephrasing in
addition to insertion and deletion to make it
readable and acceptable. For example, the ideal
output for “haiz, I wanna pontang school” is “Sigh,
I do not feel like going to school”, which may not
be just a normalization problem.
</bodyText>
<table confidence="0.9991542">
Baseline Baseline + Aw et al. Aw et al.
Model User (2006) (2006) +
Dictionary user
Dictionary
40 72 17 42
</table>
<tableCaption confidence="0.999796">
Table 1. Number of Correct Normalization Output
</tableCaption>
<bodyText confidence="0.999092666666667">
In the examples showed in Table 2, ‘din’ and
‘dnr’ are normalized to ‘didn’t’ and ‘do not reply’
based on the entries captured in the default
dictionary. With the extension of normalization
hypotheses in the user dictionary, the system
produces the correct expansion to ‘dinner’.
</bodyText>
<table confidence="0.989188136363636">
Chat Message Chat Message Chat Message
normalized normalized
using the with the
Default supplement of
dictionary user dictionary
buy din 4 Buy didn&apos;t for Buy dinner for
urself. yourself. yourself.
dun cook dnr 4 Don&apos;t cook do Don&apos;t cook
me 2nite not reply for me dinner for me
tonight tonight
gtg bb ttyl ttfn Got to go bb ttyl Got to go bye
ttfn talk to you later
bye bye
I dun feel lyk I don&apos;t feel lyk I don&apos;t feel like
riting riting writing
im gng hme 2 I&apos;m going hme I&apos;m going home
mug two mug to study
msg me wh u Message me wh Message me
rch you rch when you reach
so sian I dun So sian I don&apos;t So bored I don&apos;t
wanna do hw want to do how want to do
now now homework now
</table>
<tableCaption confidence="0.999376">
Table 2. Normalized chat messages
</tableCaption>
<subsectionHeader confidence="0.379726">
AsiaSpik Multilingual Chat
</subsectionHeader>
<bodyText confidence="0.983287285714286">
Figure 2 and Figure 3 show the personal lingo
defined by two users. Note that expansions for
“gtg” and “tgt” are defined differently and
expanded differently for the two users. ‘Me’ in the
message box indicates the message typed by the
user while ‘Expansion’ is the message expanded
by the system.
</bodyText>
<figureCaption confidence="0.74035">
Figure 2. Short-forms defined and messages
expanded for user 1
</figureCaption>
<page confidence="0.99063">
34
</page>
<figureCaption confidence="0.912844222222222">
Figure 3. Short-forms defined and messages
expanded for user 2
Figure 4 shows the multilingual chat exchange
between a Malay language user (Mahani) and an
English user (Keith). The figure shows the
messages are first expanded to the correct forms
before translated to the recipient language.
Figure 4. Conversion between a Malay user &amp; an
English user
</figureCaption>
<sectionHeader confidence="0.999267" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999989896551724">
AsiaSpik system provides an architecture for
performing chat normalization for each user such
that user can chat as usual and does not need to pay
special attention to type in proper language when
involving translation for multilingual chat. The
system aims to overcome the limitations of
normalizing social media content universally
through a personalized normalization model. The
proposed strategy makes user the active contributor
in defining the chat language and enables the
system to model the user chat language
dynamically.
The normalization approach is a simple
probabilistic model making use of the
normalization probability defined for each short-
form and the language model probability. The
model can be further improved by fine-tuning the
normalization probability and incorporate other
feature functions. The baseline model can also be
further improved with more sophisticated method
without changing the architecture of the full
system.
AsiaSpik is a demonstration system. We would
like to expand the normalization model to include
more features and support other languages such as
Malay and Chinese. We would also like to further
enhance the system to convert the translated
English chat messages back to the social media
language as defined by the user.
</bodyText>
<sectionHeader confidence="0.999435" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999914064516129">
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
Phrase-based statistical model for SMS text
normalization. In Proc. Of the COLING/ACL 2006
Main Conference Poster Sessions, pages 33-40.
Sydney.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure of
texting language. International Journal on Document
Analysis and Recognition, 10:157–174.
Paul Cook and Suzanne Stevenson. 2009. An
unsupervised model for text message normalization.
In CALC ’09: Proceedings of the Workshop on
Computational Approaches to Linguistic Creativity,
pages 71–78, Boulder, USA.
Bo Han and Timothy Baldwin. 2011. Leixcal
Normalisation of Short Text Messages: Makn Sens a
#twitter. In Proc. Of the 49th Annual Meeting of the
Association for Computational Linguistics, pages
368-378, Portland, Oregon, USA.
Yijue How and Min-Yen Kan. 2005. Optimizing
predictive text entry for short message service on
mobile phones. In Proceedings of HCII.
Philipp Koehn &amp;al. Moses: Open Source Toolkit for
Statistical Machine Translation, ACL 2007,
demonstration session.
Koehn, P. (2005). Europarl: A Parallel Corpus for
Statistical Machine Translation. In Machine
Translation Summit X (pp. 79{86). Phuket, Thailand.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
</reference>
<page confidence="0.983109">
35
</page>
<reference confidence="0.997578">
41th Annual Meeting of the Association for
Computational Linguistics, Sapporo, July.
C. Shannon. 1948. A mathematical theory of
communication. Bell System Technical Journal
27(3): 379-423
A. Stolcke. 2003 SRILM – an Extensible Language
Modeling Toolkit. In International Conference on
Spoken Language Processing, Denver, USA.
</reference>
<page confidence="0.998942">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.410278">
<title confidence="0.999567">Personalized Normalization for a Multilingual Chat System</title>
<author confidence="0.989638">Ai Ti Aw</author>
<author confidence="0.989638">Lian Hau</author>
<affiliation confidence="0.742696">Human Language Institute for Infocomm</affiliation>
<address confidence="0.787624">1 Fusionopolis Way, #21-01 Connexis, Singapore</address>
<email confidence="0.903288">aaiti@i2r.a-star.edu.sg</email>
<abstract confidence="0.998352105263158">This paper describes the personalized normalization of a multilingual chat system that supports chatting in user defined short-forms or abbreviations. One of the major challenges for multilingual chat realized through machine translation technology is the normalization of non-standard, self-created short-forms in the chat message to standard words before translation. Due to the lack of training data and the variations of short-forms used among different social communities, it is hard to normalize and translate chat messages if user uses vocabularies outside the training data and create short-forms freely. We develop a personalized chat normalizer for English and integrate it with a multilingual chat system, allowing user to create and use personalized short-forms in multilingual chat.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>AiTi Aw</author>
<author>Min Zhang</author>
<author>Juan Xiao</author>
<author>Jian Su</author>
</authors>
<title>A Phrase-based statistical model for SMS text normalization.</title>
<date>2006</date>
<booktitle>In Proc. Of the COLING/ACL 2006 Main Conference Poster Sessions,</booktitle>
<pages>33--40</pages>
<location>Sydney.</location>
<contexts>
<context position="1666" citStr="Aw et al., 2006" startWordPosition="237" endWordPosition="240">user-generated textual content on social media and networking usually encounters challenges due to the language used by the online community. Though some jargons of the online language has made their way into the standard dictionary, a large portion of the abbreviations, slang and context specific terms are still uncommon and only understood within the user community. Consequently, content analysis or translation techniques developed for a more formal genre like news or even conversations cannot apply directly and effectively to the social media content. In recent years, there are many works (Aw et al., 2006; Cook et al., 2009; Han et al., 2011) on text normalization to preprocess user generated content such as tweets and short messages before further processing. The approaches include supervised or unsupervised methods based on morphological and phonetic variations. However, most of the multilingual chat systems on the Internet have not yet integrated this feature into their systems but requesting users to type in proper language so as to have good translation. This is because the current techniques are not robust enough to model the different characteristics featured in the social media content</context>
<context position="5787" citStr="Aw et al. (2006)" startWordPosition="887" endWordPosition="890">ted Work The traditional text normalization strategy follows the noisy channel model (Shannon, 1948). Suppose the chat message is C and its corresponding standard form is S , the approach aims to find arg max P(S |C) by computing arg max P(C |S) in which P(S) is usually a language model and P(C |S) is an error model. The objective of using model in the chat message normalization context is to develop an appropriate error model for converting the non-standard and unconventional words found in chat messages into standard words. ^ S = arg max ( |) arg max ( |) ( ) P S C = P C S P S S S Recently, Aw et al. (2006) model text message normalization as translation from the texting language into the standard language. Choudhury et al. (2007) model the word-level text generation process for SMS messages, by considering graphemic/phonetic abbreviations and unintentional typos as hidden Markov model (HMM) state transitions and emissions, respectively. Cook and Stevenson (2009) expand the error model by introducing inference from different erroneous formation processes, according to the sample error distribution. Han and Baldwin (2011) use a classifier to detect ill-formed words, and generate correction candid</context>
<context position="10547" citStr="Aw et al. (2006)" startWordPosition="1729" endWordPosition="1732">in SD in user dictionary. 33 short-forms are very irregular and hard to predict their standard forms using morphological and phonetic similarity. It is also hard to train a statistical model if training data is not available. We asked the students to define their personal abbreviations in the system and run through the system with and without the user dictionary. We asked them to give a score of 1 if the output is acceptable to them as proper English, otherwise a 0 will be given. We compared the results using both the baseline model and the model implemented using the same training data as in Aw et al. (2006). Table 1 shows the number of accepted output between the two models. Both models show improvement with the use of user dictionary. It also shows that it is very critical to have similar training data for the targeted domain to have good normalization performance. A simple model helps if such training data is unavailable. Nevertheless, the use of a dictionary driven by the user is an alternative to improve the overall performance. One reason for the inability of both models to capture the variations fully is because many messages require some degree of rephrasing in addition to insertion and d</context>
</contexts>
<marker>Aw, Zhang, Xiao, Su, 2006</marker>
<rawString>AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A Phrase-based statistical model for SMS text normalization. In Proc. Of the COLING/ACL 2006 Main Conference Poster Sessions, pages 33-40. Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Monojit Choudhury</author>
<author>Rahul Saraf</author>
<author>Vijit Jain</author>
<author>Animesh Mukherjee</author>
<author>Sudeshna Sarkar</author>
<author>Anupam Basu</author>
</authors>
<title>Investigation and modeling of the structure of texting language.</title>
<date>2007</date>
<journal>International Journal on Document Analysis and Recognition,</journal>
<pages>10--157</pages>
<contexts>
<context position="5913" citStr="Choudhury et al. (2007)" startWordPosition="905" endWordPosition="908">message is C and its corresponding standard form is S , the approach aims to find arg max P(S |C) by computing arg max P(C |S) in which P(S) is usually a language model and P(C |S) is an error model. The objective of using model in the chat message normalization context is to develop an appropriate error model for converting the non-standard and unconventional words found in chat messages into standard words. ^ S = arg max ( |) arg max ( |) ( ) P S C = P C S P S S S Recently, Aw et al. (2006) model text message normalization as translation from the texting language into the standard language. Choudhury et al. (2007) model the word-level text generation process for SMS messages, by considering graphemic/phonetic abbreviations and unintentional typos as hidden Markov model (HMM) state transitions and emissions, respectively. Cook and Stevenson (2009) expand the error model by introducing inference from different erroneous formation processes, according to the sample error distribution. Han and Baldwin (2011) use a classifier to detect ill-formed words, and generate correction candidates based on morphophonemic similarity. These models are effective on their experiments conducted, however, much works remain</context>
</contexts>
<marker>Choudhury, Saraf, Jain, Mukherjee, Sarkar, Basu, 2007</marker>
<rawString>Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh Mukherjee, Sudeshna Sarkar, and Anupam Basu. 2007. Investigation and modeling of the structure of texting language. International Journal on Document Analysis and Recognition, 10:157–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Cook</author>
<author>Suzanne Stevenson</author>
</authors>
<title>An unsupervised model for text message normalization.</title>
<date>2009</date>
<booktitle>In CALC ’09: Proceedings of the Workshop on Computational Approaches to Linguistic Creativity,</booktitle>
<pages>71--78</pages>
<location>Boulder, USA.</location>
<contexts>
<context position="6150" citStr="Cook and Stevenson (2009)" startWordPosition="935" endWordPosition="938"> chat message normalization context is to develop an appropriate error model for converting the non-standard and unconventional words found in chat messages into standard words. ^ S = arg max ( |) arg max ( |) ( ) P S C = P C S P S S S Recently, Aw et al. (2006) model text message normalization as translation from the texting language into the standard language. Choudhury et al. (2007) model the word-level text generation process for SMS messages, by considering graphemic/phonetic abbreviations and unintentional typos as hidden Markov model (HMM) state transitions and emissions, respectively. Cook and Stevenson (2009) expand the error model by introducing inference from different erroneous formation processes, according to the sample error distribution. Han and Baldwin (2011) use a classifier to detect ill-formed words, and generate correction candidates based on morphophonemic similarity. These models are effective on their experiments conducted, however, much works remain to be done to handle the diversity and dynamic of content and fast evolution of words used in social media and networking. As we notice that unlike spelling errors which are made mostly unintentionally by the writers, abbreviations or s</context>
</contexts>
<marker>Cook, Stevenson, 2009</marker>
<rawString>Paul Cook and Suzanne Stevenson. 2009. An unsupervised model for text message normalization. In CALC ’09: Proceedings of the Workshop on Computational Approaches to Linguistic Creativity, pages 71–78, Boulder, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Timothy Baldwin</author>
</authors>
<title>Leixcal Normalisation of Short Text Messages: Makn Sens a #twitter.</title>
<date>2011</date>
<booktitle>In Proc. Of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>368--378</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="6311" citStr="Han and Baldwin (2011)" startWordPosition="957" endWordPosition="960">andard words. ^ S = arg max ( |) arg max ( |) ( ) P S C = P C S P S S S Recently, Aw et al. (2006) model text message normalization as translation from the texting language into the standard language. Choudhury et al. (2007) model the word-level text generation process for SMS messages, by considering graphemic/phonetic abbreviations and unintentional typos as hidden Markov model (HMM) state transitions and emissions, respectively. Cook and Stevenson (2009) expand the error model by introducing inference from different erroneous formation processes, according to the sample error distribution. Han and Baldwin (2011) use a classifier to detect ill-formed words, and generate correction candidates based on morphophonemic similarity. These models are effective on their experiments conducted, however, much works remain to be done to handle the diversity and dynamic of content and fast evolution of words used in social media and networking. As we notice that unlike spelling errors which are made mostly unintentionally by the writers, abbreviations or slangs found in chat messages are introduced intentionally by the senders most of the time. This leads us to suggest that if facilities are given to users to defi</context>
</contexts>
<marker>Han, Baldwin, 2011</marker>
<rawString>Bo Han and Timothy Baldwin. 2011. Leixcal Normalisation of Short Text Messages: Makn Sens a #twitter. In Proc. Of the 49th Annual Meeting of the Association for Computational Linguistics, pages 368-378, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yijue How</author>
<author>Min-Yen Kan</author>
</authors>
<title>Optimizing predictive text entry for short message service on mobile phones.</title>
<date>2005</date>
<booktitle>In Proceedings of HCII.</booktitle>
<contexts>
<context position="8253" citStr="How and Kan, 2005" startWordPosition="1292" endWordPosition="1295">normalized to a standard form, si ,j ; and the language model log probability. k are weights of the feature functions. We define P(si ,j |ci) as a uniform distribution computed through a set of dictionary collected from corpus, SMS messages and Internet sources. A total of 11,119 entries are collected and each entry is assigned with an initial probability, 1 P s c  , where |ci |is the number of s i j i c ( , |) ||i ci entries defined in the dictionary. We adjust the probability manually for some entries that are very common and occur more than a certain threshold, t , in the NUS SMS corpus (How and Kan, 2005) with a higher weight-age, w . This model, together with the language model, forms our baseline system for chat normalization. To enable personalized real-time management of user-defined abbreviations and short-forms, we define a personalized model Puser _i (si ,j |ci) for each user based on his/her dictionary profile. Each personalized model is loaded into the memory once the user activates the normalization option. Whenever there is a change in the entry, the entry’s probability will be re-distributed and updated based on the following model. This characterizes the AsiaSpik system which supp</context>
</contexts>
<marker>How, Kan, 2005</marker>
<rawString>Yijue How and Min-Yen Kan. 2005. Optimizing predictive text entry for short message service on mobile phones. In Proceedings of HCII.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Moses</author>
</authors>
<title>Open Source Toolkit for Statistical Machine Translation, ACL 2007, demonstration session.</title>
<marker>Moses, </marker>
<rawString>Philipp Koehn &amp;al. Moses: Open Source Toolkit for Statistical Machine Translation, ACL 2007, demonstration session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Machine Translation Summit X</booktitle>
<pages>79--86</pages>
<location>Phuket, Thailand.</location>
<contexts>
<context position="9554" citStr="Koehn, 2005" startWordPosition="1524" endWordPosition="1525">  s D N M  i j  1 if SD , c  s  SD i i j, M if SD c  i where SD denotes default dictionary; N denotes the number of entries c i M denotes the number of entries c i The feature weights in the normalization model are optimized by minimum error rate training (Och, 2003), which searches for weights maximizing the normalization accuracy using a small development set. We use standard state-ofthe-art open source tools, Moses (Koehn, 2007), to develop the system and the SRI language modeling toolkit (Stolcke,2003) to train a trigram language model on the English portion of the Europarl Corpus (Koehn, 2005). 3.3 Experiments We conducted a small experiment using 134 chat messages sent by high school students. Out of these messages, 73 short-forms are uncommon and not found in our default dictionary. Most of these if t 1 w (si | , | ) | ci |ci )  t (si |ci (si Ps , j 1 w  , | ) | | if ci ,j ,j ,j ,ci) |t (si , ) | c t  i |(si |ci      |  ,j Puser i  N _  1   M in SD in user dictionary. 33 short-forms are very irregular and hard to predict their standard forms using morphological and phonetic similarity. It is also hard to train a statistical model if training data is not availabl</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Koehn, P. (2005). Europarl: A Parallel Corpus for Statistical Machine Translation. In Machine Translation Summit X (pp. 79{86). Phuket, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Sapporo,</location>
<contexts>
<context position="9216" citStr="Och, 2003" startWordPosition="1473" endWordPosition="1474">d into the memory once the user activates the normalization option. Whenever there is a change in the entry, the entry’s probability will be re-distributed and updated based on the following model. This characterizes the AsiaSpik system which supports personalized and dynamic chat normalization.  N  s i j i P s c ( |) if c , S , i ,  s D N M  i j  1 if SD , c  s  SD i i j, M if SD c  i where SD denotes default dictionary; N denotes the number of entries c i M denotes the number of entries c i The feature weights in the normalization model are optimized by minimum error rate training (Och, 2003), which searches for weights maximizing the normalization accuracy using a small development set. We use standard state-ofthe-art open source tools, Moses (Koehn, 2007), to develop the system and the SRI language modeling toolkit (Stolcke,2003) to train a trigram language model on the English portion of the Europarl Corpus (Koehn, 2005). 3.3 Experiments We conducted a small experiment using 134 chat messages sent by high school students. Out of these messages, 73 short-forms are uncommon and not found in our default dictionary. Most of these if t 1 w (si | , | ) | ci |ci )  t (si |ci (si P</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training for statistical machine translation. In Proceedings of the 41th Annual Meeting of the Association for Computational Linguistics, Sapporo, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Shannon</author>
</authors>
<title>A mathematical theory of communication.</title>
<date>1948</date>
<journal>Bell System Technical Journal</journal>
<volume>27</volume>
<issue>3</issue>
<pages>379--423</pages>
<contexts>
<context position="5271" citStr="Shannon, 1948" startWordPosition="784" endWordPosition="785">nglish and vice versa. Multilingual chat among these languages is achieved through pivot translation using English as the pivot language. The Normalization Web Service supports only English normalization. Both web services are running on Apache Tomcat web server with Apache Axis2. 3 Personalized Normalization Personalized Normalization is the main distinction of AsiaSpik among other multilingual chat system. It gives the flexibility for user to personalize his/her short-forms for messages in English. 3.1 Related Work The traditional text normalization strategy follows the noisy channel model (Shannon, 1948). Suppose the chat message is C and its corresponding standard form is S , the approach aims to find arg max P(S |C) by computing arg max P(C |S) in which P(S) is usually a language model and P(C |S) is an error model. The objective of using model in the chat message normalization context is to develop an appropriate error model for converting the non-standard and unconventional words found in chat messages into standard words. ^ S = arg max ( |) arg max ( |) ( ) P S C = P C S P S S S Recently, Aw et al. (2006) model text message normalization as translation from the texting language into the </context>
</contexts>
<marker>Shannon, 1948</marker>
<rawString>C. Shannon. 1948. A mathematical theory of communication. Bell System Technical Journal 27(3): 379-423</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM – an Extensible Language Modeling Toolkit.</title>
<date>2003</date>
<booktitle>In International Conference on Spoken Language Processing,</booktitle>
<location>Denver, USA.</location>
<marker>Stolcke, 2003</marker>
<rawString>A. Stolcke. 2003 SRILM – an Extensible Language Modeling Toolkit. In International Conference on Spoken Language Processing, Denver, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>