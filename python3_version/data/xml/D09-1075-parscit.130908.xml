<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000425">
<title confidence="0.998081">
Unsupervised Tokenization for Machine Translation
</title>
<author confidence="0.998402">
Tagyoung Chung and Daniel Gildea
</author>
<affiliation confidence="0.9970535">
Computer Science Department
University of Rochester
</affiliation>
<address confidence="0.28999">
Rochester, NY 14627
</address>
<sectionHeader confidence="0.979715" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99970805882353">
Training a statistical machine translation
starts with tokenizing a parallel corpus.
Some languages such as Chinese do not in-
corporate spacing in their writing system,
which creates a challenge for tokenization.
Moreover, morphologically rich languages
such as Korean present an even bigger
challenge, since optimal token boundaries
for machine translation in these languages
are often unclear. Both rule-based solu-
tions and statistical solutions are currently
used. In this paper, we present unsuper-
vised methods to solve tokenization prob-
lem. Our methods incorporate informa-
tion available from parallel corpus to de-
termine a good tokenization for machine
translation.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999945193548387">
Tokenizing a parallel corpus is usually the first
step of training a statistical machine translation
system. With languages such as Chinese, which
has no spaces in its writing system, the main chal-
lenge is to segment sentences into appropriate to-
kens. With languages such as Korean and Hun-
garian, although the writing systems of both lan-
guages incorporate spaces between “words”, the
granularity is too coarse compared with languages
such as English. A single word in these lan-
guages is composed of several morphemes, which
often correspond to separate words in English.
These languages also form compound nouns more
freely. Ideally, we want to find segmentations for
source and target languages that create a one-to-
one mapping of words. However, this is not al-
ways straightforward for two major reasons. First,
what the optimal tokenization for machine trans-
lation should be is not always clear. Zhang et al.
(2008b) and Chang et al. (2008) show that get-
ting the tokenization of one of the languages in
the corpus close to a gold standard does not nec-
essarily help with building better machine trans-
lation systems. Second, even statistical methods
require hand-annotated training data, which means
that in resource-poor languages, good tokenization
is hard to achieve.
In this paper, we explore unsupervised methods
for tokenization, with the goal of automatically
finding an appropriate tokenization for machine
translation. We compare methods that have ac-
cess to parallel corpora to methods that are trained
solely using data from the source language. Unsu-
pervised monolingual segmentation has been stud-
ied as a model of language acquisition (Goldwater
et al., 2006), and as model of learning morphol-
ogy in European languages (Goldsmith, 2001).
Unsupervised segmentation using bilingual data
has been attempted for finding new translation
pairs (Kikui and Yamamoto, 2002), and for finding
good segmentation for Chinese in machine trans-
lation using Gibbs sampling (Xu et al., 2008). In
this paper, further investigate the use of bilingual
information to find tokenizations tailored for ma-
chine translation. We find a benefit not only for
segmentation of languages with no space in the
writing system (such as Chinese), but also for the
smaller-scale tokenization problem of normaliz-
ing between languages that include more or less
information in a “word” as defined by the writ-
ing system, using Korean-English for our exper-
iments. Here too, we find a benefit from using
bilingual information, with unsupervised segmen-
tation rivaling and in some cases surpassing su-
pervised segmentation. On the modeling side,
we use dynamic programming-based variational
Bayes, making Gibbs sampling unnecessary. We
also develop and compare various factors in the
model to control the length of the tokens learned,
and find a benefit from adjusting these parame-
ters directly to optimize the end-to-end translation
quality.
</bodyText>
<page confidence="0.941801">
718
</page>
<note confidence="0.997818">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 718–726,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<sectionHeader confidence="0.991354" genericHeader="introduction">
2 Tokenization
</sectionHeader>
<bodyText confidence="0.9999915">
Tokenization is breaking down text into lexemes
— a unit of morphological analysis. For relatively
isolating languages such as English and Chinese, a
word generally equals a single token, which is usu-
ally a clearly identifiable unit. English, especially,
incorporates spaces between words in its writing
system, which makes tokenization in English usu-
ally trivial. The Chinese writing system does not
have spaces between words, but there is less am-
biguity where word boundaries lie in a given sen-
tence compared to more agglutinative languages.
In languages such as Hungarian, Japanese, and
Korean, what constitutes an optimal token bound-
ary is more ambiguous. While two tokens are usu-
ally considered two separate words in English, this
may be not be the case in agglutinative languages.
Although what is considered a single morpholog-
ical unit is different from language to language,
if someone were given a task to align words be-
tween two languages, it is desirable to have one-
to-one token mapping between two languages in
order to have the optimal problem space. For ma-
chine translation, one token should not necessarily
correspond to one morphological unit, but rather
should reflect the morphological units and writing
system of the other language involved in transla-
tion.
For example, consider a Korean “word” meok-
eoss-da, which means ate. It is written as a sin-
gle word in Korean but consists of three mor-
phemes eat-past-indicative. If one uses morpho-
logical analysis as the basis for Korean tokeniza-
tion, meok-eoss-da would be split into three to-
kens, which is not desirable if we are translat-
ing Korean to English, since English does not
have these morphological counterparts. However,
a Hungarian word szekr´enyemben, which means in
my closet, consists of three morphemes closet-my-
inessive that are distinct words in English. In this
case, we do want our tokenizer to split this “word”
into three morphemes szekr´eny em ben.
In this paper, we use segmentation and to-
kenization interchangeably as blanket terms to
cover the two different problems we have pre-
sented here. The problem of segmenting Chinese
sentences into words and the problem of segment-
ing Korean or Hungarian “words” into tokens of
right granularity are different in their nature. How-
ever, our models presented in section 3 handle the
both problems.
</bodyText>
<sectionHeader confidence="0.992105" genericHeader="method">
3 Models
</sectionHeader>
<bodyText confidence="0.9999838">
We present two different methods for unsuper-
vised tokenization. Both are essentially unigram
tokenization models. In the first method, we try
learning tokenization from word alignments with
a model that bears resemblance to Hidden Markov
models. We use IBM Model 1 (Brown et al., 1993)
for the word alignment model. The second model
is a relatively simpler monolingual tokenization
model based on counts of substrings which serves
as a baseline of unsupervised tokenization.
</bodyText>
<subsectionHeader confidence="0.999928">
3.1 Learning tokenization from alignment
</subsectionHeader>
<bodyText confidence="0.989781444444444">
We use expectation maximization as our primary
tools in learning tokenization form parallel text.
Here, the observed data provided to the algorithm
are the tokenized English string en1 and the unto-
kenized string of foreign characters cm1 . The un-
observed variables are both the word-level align-
ments between the two strings, and the tokeniza-
tion of the foreign string. We represent the tok-
enization with a string sm1 of binary variables, with
si = 1 indicating that the ith character is the final
character in a word. The string of foreign words
fℓ1 can be thought of as the result of applying the
tokenization s to the character string c:
f = s ◦ c where E =
We use IBM Model 1 as our word-level align-
ment model, following its assumptions that each
foreign word is generated independently from one
English word:
</bodyText>
<equation confidence="0.99991">
P(f|e) = X P(f, a  |e)
a
P(fi  |eai)P(a)
P(fi  |ej)P(ai = j)
</equation>
<bodyText confidence="0.9982687">
and that all word-level alignments a are equally
likely: P(a) = n1 for all positions. While Model 1
has a simple EM update rule to compute posteri-
ors for the alignment variables a and from them
learn the lexical translation parameters P(f  |e),
we cannot apply it directly here because f itself is
unknown, and ranges over an exponential number
of possibilities depending on the hidden segmenta-
tion s. This can be addressed by applying dynamic
programing over the sequence s. We compute the
</bodyText>
<equation confidence="0.948226714285714">
si
Xm
i=1
X= Y
a i
Y= X
i j
</equation>
<page confidence="0.781709">
719
c4
</page>
<bodyText confidence="0.997915">
posterior probability of a word beginning at posi-
tion i, ending at position j, and being generated by
English word k:
</bodyText>
<equation confidence="0.998443">
P(si...j = (1, 0, ... , 0, 1), a = k  |e)
α(i)P(f  |ek)P(a = k)β(j)
=
P(c  |e)
</equation>
<bodyText confidence="0.9997455">
where f = ci ... cj is the word formed by con-
catenating characters i through j, and a is a vari-
able indicating which English position generated
f. Here α and β are defined as:
</bodyText>
<equation confidence="0.9989795">
α(i) = P(ci1, si = 1  |e)
β(j) = P(cmj+1, sj = 1  |e)
</equation>
<bodyText confidence="0.9987405">
These quantities resemble forward and backward
probabilities of hidden Markov models, and can
be computed with similar dynamic programming
recursions:
</bodyText>
<equation confidence="0.986676833333333">
L
α(i) = α(i − ℓ) � P(a)P(cii−ℓ  |ea)
ℓ=1 a
� P(a)P(cj+ℓ
j  |ea)β(j + ℓ)
a
</equation>
<bodyText confidence="0.9994866">
where L is the maximum character length for a
word.
Then, we can calculate the expected counts of
individual word pairs being aligned (cji, ek) by ac-
cumulating these posteriors over the data:
</bodyText>
<equation confidence="0.991824">
ec(cj i, ek) += α(i)P(a)P(cj
α(m)
i  |ek)β(j)
The M step simply normalizes the counts:
P˜ (f  |e) = ec(f, e)
Ee ec(f, e)
</equation>
<bodyText confidence="0.999986692307692">
Our model can be compared to a hidden Markov
model in the following way: a target word gen-
erates a source token which spans a zeroth order
Markov chain of characters in source sentence,
where a “transition” represents a segmentation and
a “emission” represents an alignment. The model
uses HMM-like dynamic programming to do in-
ference. For the convenience, we refer to this
model as the bilingual model in the rest of the
paper. Figure 1 illustrates our first model with
an small example. Under this model we are not
learning segmentation directly, but rather we are
learning alignments between two sentences. The
</bodyText>
<equation confidence="0.995351333333333">
c1 c2 c3
f1 f2
e1 e2
</equation>
<figureCaption confidence="0.960328">
Figure 1: The figure shows a source sentence
</figureCaption>
<bodyText confidence="0.956873666666667">
f = f1, f2 = s o c1 ... c4 where s = (0, 0,1,1)
and a target sentence e = e1, e2. There is a seg-
mentation between c3 and c4; thus c1, c2, c3 form
f1 and c3 forms f2. f1 is generated by e2 and f2 is
generated by e1.
segmentation is by-product of learning the align-
ment. We can find the optimal segmentation of
a new source language sentence using the Viterbi
algorithm. Given two sentences e and f,
</bodyText>
<equation confidence="0.795291">
a* = argmax P(f, a  |e)
a
</equation>
<bodyText confidence="0.9858175">
and segmentation s* implied by alignment a* is
the optimal segmentation of f found by this model.
</bodyText>
<subsectionHeader confidence="0.8511415">
3.2 Learning tokenization from substring
counts
</subsectionHeader>
<bodyText confidence="0.999949615384615">
The second tokenization model we propose is
much simpler. More sophisticated unsupervised
monolingual tokenization models using hierarchi-
cal Bayesian models (Goldwater et al., 2006)
and using the minimum description length prin-
ciple (Goldsmith, 2001; de Marcken, 1996) have
been studied. Our model is meant to serve as
a computationally efficient baseline for unsuper-
vised monolingual tokenization. Given a corpus
of only source language of unknown tokenization,
we want to find the optimal s given c — s that
gives us the highest P(s  |c). According to Bayes’
rule,
</bodyText>
<equation confidence="0.852926">
P(s  |c) a P(c  |s)P(s)
</equation>
<bodyText confidence="0.998952">
Again, we assume that all P(s) are equally likely.
Let f = s o c = f1 ... fℓ, where fi is a word under
some possible segmentation s. We want to find the
s that maximizes P(f). We assume that
</bodyText>
<equation confidence="0.981638">
P(f) = P(f1) x ... x P(fℓ)
To calculate P(fi), we count every possible
L
β(j) =
ℓ=1
720
substring — every possible segmentation of char-
acters — from the sentences. We assume that
count(fi)
P (fi) =
Ek count(fk)
</equation>
<bodyText confidence="0.999838571428571">
We can compute these counts by making a sin-
gle pass through the corpus. As in the bilingual
model, we limit the maximum size of f for prac-
tical reasons and to prevent our model from learn-
ing unnecessarily long f. With P(f), given a se-
quence of characters c, we can calculate the most
likely segmentation using the Viterbi algorithm.
</bodyText>
<equation confidence="0.9923775">
s∗ = argmax P(f)
s
</equation>
<bodyText confidence="0.999983857142857">
Our rationale for this model is that if a span of
characters f = ci ... cj is an independent token, it
will occur often enough in different contexts that
such a span of characters will have higher prob-
ability than other spans of characters that are not
meaningful. For the rest of the paper, this model
will be referred to as the monolingual model.
</bodyText>
<subsectionHeader confidence="0.999786">
3.3 Tokenizing new data
</subsectionHeader>
<bodyText confidence="0.9999161">
Since the monolingual tokenization only uses in-
formation from a monolingual corpus, tokenizing
new data is not a problem. However, with the
bilingual model, we are learning P(f  |e). We are
relying on information available from e to get the
best tokenization for f. However, the parallel sen-
tences will not be available for new data we want
to translate. Therefore, for the new data, we have
to rely only on P(f) to tokenize any new data,
which can be obtained by calculating
</bodyText>
<equation confidence="0.995751">
P(f) = � P(f  |e)P(e)
e
</equation>
<bodyText confidence="0.999535125">
With P(f) from the bilingual model, we can run
the Viterbi algorithm in the same manner as mono-
lingual tokenization model for monolingual data.
We hypothesize that we can learn valuable infor-
mation on which token boundaries are preferable
in language f when creating a statistical machine
translation system that translates from language f
to language e.
</bodyText>
<sectionHeader confidence="0.960532" genericHeader="method">
4 Preventing overfitting
</sectionHeader>
<bodyText confidence="0.999978428571429">
We introduce two more refinements to our word-
alignment induced tokenization model and mono-
lingual tokenization model. Since we are consid-
ering every possible token f that can be guessed
from our corpus, the data is very sparse. For the
bilingual model, we are also using the EM algo-
rithm to learn P(f  |e), which means there is a
danger of the EM algorithm memorizing the train-
ing data and thereby overfitting. We put a Dirichlet
prior on our multinomial parameter for P(f  |e)
to control this situation. For both models, we also
want a way to control the distribution of token
length after tokenization. We address this problem
by adding a length factor to our models.
</bodyText>
<subsectionHeader confidence="0.987741">
4.1 Variational Bayes
</subsectionHeader>
<bodyText confidence="0.999772666666667">
Beal (2003) and Johnson (2007) describe vari-
ational Bayes for hidden Markov model in de-
tail, which can be directly applied to our bilingual
model. With this Bayesian extension, the emission
probability of our first model can be summarized
as follows:
</bodyText>
<equation confidence="0.999439333333333">
θe  |α ∼ Dir(α),
fi  |ei = e ∼ Multi(θe).
e)
P˜(f|
=exp(0(ec(f, e) + α))
exp(0(Ee ec(f, e) + sα))
</equation>
<bodyText confidence="0.999650695652174">
where 0 is the digamma function, and s is the size
of the vocabulary from which f is drawn. Since
we do not accurately know s, we set s to be the
number of all possible tokens. As can be seen from
the equation, by setting α to a small value, we are
discounting the expected count with help of the
digamma function. Thus, having lower α leads to
a sparser solution.
Johnson (2007) and Zhang et al. (2008a) show
having small α helps to control overfitting. Fol-
lowing this, we set our Dirichlet prior to be as
sparse as possible. It is set at α = 10−6, the num-
ber we used as floor of our probability.
For the model incorporating the length factor,
which is described in the next section, we do not
place a prior on our transition probability, since
there are only two possible states, i.e. P(s = 1)
and P(s = 0). This distribution is not as sparse as
the emission probability.
Comparing variational Bayes to the traditional
EM algorithm, the E step stays the same but the
M step for calculating the emission probability
changes as follows:
</bodyText>
<subsectionHeader confidence="0.976971">
4.2 Token length
</subsectionHeader>
<bodyText confidence="0.9992115">
We now add a parameter that can adjust the to-
kenizer’s preference for longer or shorter tokens.
</bodyText>
<page confidence="0.973775">
721
</page>
<figure confidence="0.999584681818182">
1 2 3 4 5 6
ref
P(s)=0.55
lambda=3.16
0.6
ref
P(s)=0.58
lambda=2.13
0.5
0.4
0.3
0.2
0.1
0
1 2 3 4 5 6
0.6
0.5
0.4
0.3
0.2
0.1
0
</figure>
<figureCaption confidence="0.81105025">
Figure 2: Distribution of token length for (from left to right) Chinese, and Korean. “ref” is the empirical
distribution from supervised tokenization. Two length factors — φ1 and φ2 are also shown. For φ1, the
parameter to geometric distribution P(s) is set to the value learned from our bilingual model. For φ2, λ
is set using the criterion described in the experiment section.
</figureCaption>
<bodyText confidence="0.997718708333333">
This parameter is beneficial because we want our
distribution of token length after tokenization to
resemble the real distribution of token length. This
parameter is also useful because we also want to
incorporate information on the number of tokens
in the other language in the parallel corpus. This is
based on the assumption that, if tokenization cre-
ates a one-to-one mapping, the number of tokens
in both languages should be roughly the same. We
can force the two languages to have about the same
number of tokens by adjusting this parameter. The
third reason is to further control overfitting. Our
observation is that certain morphemes are very
common, such that they will be always observed
attached to other morphemes. For example, in Ko-
rean, a noun attached with nominative case marker
is very common. Our model is likely to learn a
noun attached with the morpheme — nominative
case marker — rather than noun itself. This is not
desirable when the noun occurs with less common
morphemes; in these cases the morpheme will be
split off creating inconsistencies.
We have experimented with two different length
factors, each with one adjustable parameter:
</bodyText>
<equation confidence="0.9998795">
φ1(ℓ) = P(s)(1 − P(s))t−1
φ2(ℓ) = 2−tλ
</equation>
<bodyText confidence="0.999704416666667">
The first, φ1, is the geometric distribution, where
l is length of a token and P(s) is probability of
segmentation between two characters. The second
length factor φ2 was acquired through several ex-
periments and was found to work well. As can
been seen from Figure 2, the second factor dis-
counts longer tokens more heavily than the geo-
metric distribution. We can adjust the value of λ
and P(s) to increase or decrease number of tokens
after segmentation.
For our monolingual model, incorporating these
factors is straightforward. We assume that
</bodyText>
<equation confidence="0.990395">
P(f) Oc P(f1)φ(ℓ1) x ... x P(f,)φ(ℓ,)
</equation>
<bodyText confidence="0.9993415">
where ℓi is the length of fi. Then, we use the same
Viterbi algorithm to select the f1 ... f, that max-
imizes P(f), thereby selecting the optimal s ac-
cording to our monolingual model with a length
factor. We pick the value of λ and P(s) that
produces about the same number of tokens in the
source side as in the target side, thereby incorpo-
rating some information about the target language.
For our bilingual model, we modify our model
slightly to incorporate φ1, creating a hybrid
model. Now, our forward probability of forward-
backward algorithm is:
</bodyText>
<equation confidence="0.911693">
L
α(i) = α(i − l)φ1(ℓ) � P(a)P(cii−t  |ea)
t=1 a
and the expected count of (c3i, ek) is
ec(c3i, ek) += α(m)
α(i)P(a)P(c3i  |ek)β(j)φ1(j − i)
</equation>
<bodyText confidence="0.74739725">
For φ1, we can learn P(s) for the geometric dis-
tribution from the model itself:&apos;
α(i)β(i)
α(m)
</bodyText>
<footnote confidence="0.528188">
&apos;The equation is for one sentence, but in practice, we sum
over all sentences in the training data to calculate P(s).
</footnote>
<equation confidence="0.9734834">
1
P(s) =
m
M
i
</equation>
<page confidence="0.987831">
722
</page>
<bodyText confidence="0.999975111111111">
We can also fix P(s) instead of learning it through
EM. We incorporate 02 into the bilingual model
as follows: after learning P(f) from the bilingual
model, we pick the A in the same manner as the
monolingual model and run the Viterbi algorithm.
After applying the length factor, what we have
is a log-linear model for tokenization, with two
feature functions with equal weights: the length
factor and P(f) learned from model.
</bodyText>
<sectionHeader confidence="0.999371" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.966054">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999998105263158">
We tested our tokenization methods on two differ-
ent language pairs: Chinese-English, and Korean-
English. For Chinese-English, we used FBIS
newswire data. The Korean-English parallel data
was collected from news websites and sentence-
aligned using two different tools described by
Moore (2002) and Melamed (1999). We used sub-
sets of each parallel corpus consisting of about 2M
words and 60K sentences on the English side. For
our development set and test set, Chinese-English
had about 1000 sentences each with 10 reference
translations taken from the NIST 2002 MT eval-
uation. For Korean-English, 2200 sentence pairs
were randomly sampled from the parallel corpus,
and held out from the training data. These were
divided in half and used for test set and develop-
ment set respectively. For all language pairs, very
minimal tokenization — splitting off punctuation
— was done on the English side.
</bodyText>
<subsectionHeader confidence="0.996626">
5.2 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999786666666667">
We used Moses (Koehn et al., 2007) to train
machine translation systems. Default parameters
were used for all experiments except for the num-
ber of iterations for GIZA++ (Och and Ney, 2003).
GIZA++ was run until the perplexity on develop-
ment set stopped decreasing. For practical rea-
sons, the maximum size of a token was set at three
for Chinese, and four for Korean.2 Minimum error
rate training (Och, 2003) was run on each system
afterwards and BLEU score (Papineni et al., 2002)
was calculated on the test sets.
For the monolingual model, we tested two ver-
sions with the length factor 01, and 02. We picked
A and P(s) so that the number of tokens on source
side (Chinese, and Korean) will be about the same
</bodyText>
<footnote confidence="0.879554666666667">
2In the Korean writing system, one character is actually
one syllable block. We do not decompose syllable blocks
into individual consonants and vowels.
</footnote>
<bodyText confidence="0.99995388">
as the number of tokens in the target side (En-
glish).
For the bilingual model, as explained in the
model section, we are learning P(f I e), but only
P(f) is available for tokenizing any new data. We
compared two conditions: using only the source
data to tokenize the source language training data
according to P(f) (which is consistent with the
conditions at test time), and using both the source
and English data to tokenize the source language
training data (which might produce better tok-
enization by using more information). For the first
length factor 01, we ran an experiment where the
model learns P(s) as described in the model sec-
tion, and we also had experiments where P(s) was
pre-set at 0.9, 0.7, 0.5, and 0.3 for comparison. We
also ran an experiment with the second length fac-
tor 02 where A was picked as the same manner as
the monolingual model.
We varied tokenization of development set and
test set to match the training data for each ex-
periment. However, as we have implied in the
previous paragraph, in the one experiment where
P(f I e) was used to segment training data, di-
rectly incorporating information from target cor-
pus, tokenization for test and development set is
not exactly consistent with tokenization of train-
ing corpus. Since we assume only source corpus
is available at the test time, the test and the devel-
opment set was tokenized only using information
from P(f).
We also trained MT systems using supervised
tokenizations and tokenization requiring a mini-
mal effort for the each language pair. For Chinese-
English, the minimal effort tokenization is maxi-
mal tokenization where every Chinese character is
segmented. Since a number of Chinese tokeniz-
ers are available, we have tried four different to-
kenizations for the supervised tokenizations. The
first one is the LDC Chinese tokenizer available at
the LDC website3, which is compiled by Zhibiao
Wu. The second tokenizer is a maxent-based to-
kenizer described by Xue (2003). The third and
fourth tokenizations come from the CRF-based
Stanford Chinese segmenter described by Chang
et al. (2008). The difference between third and
fourth tokenization comes from the different gold
standard, the third one is based on Beijing Uni-
versity’s segmentation (pku) and the fourth one is
based on Chinese Treebank (ctb). For Korean-
</bodyText>
<footnote confidence="0.988954">
3http://projects.ldc.upenn.edu/Chinese/LDC ch.htm
</footnote>
<page confidence="0.992015">
723
</page>
<table confidence="0.99967185">
Chinese Korean
BLEU F-score BLEU
Supervised 20.03 0.94 7.27
Rule-based morphological analyzer 23.02 0.96
LDC segmenter 21.69 0.96
Xue’s segmenter 22.45 1.00
Stanford segmenter (pku)
Stanford segmenter (ctb)
Unsupervised
Splitting punctuation only 6.04
Maximal (Character-based MT) 20.32 0.75
Bilingual P(f I e) with 01 P(s) = learned 19.25 6.93
Bilingual P(f) with 01 P(s) = learned 20.04 0.80 7.06
Bilingual P(f) with 01 P(s) = 0.9 20.75 0.87 7.46
Bilingual P(f) with 01 P(s) = 0.7 20.59 0.81 7.31
Bilingual P(f) with 01 P(s) = 0.5 19.68 0.80 7.18
Bilingual P(f) with 01 P(s) = 0.3 20.02 0.79 7.38
Bilingual P(f) with 02 22.31 0.88 7.35
Monolingual P(f) with 01 20.93 0.83 6.76
Monolingual P(f) with 02 20.72 0.85 7.02
</table>
<tableCaption confidence="0.7530645">
Table 1: BLEU score results for Chinese-English and Korean-English experiments and F-score of seg-
mentation compared against Chinese Treebank standard. The highest unsupervised score is highlighted.
</tableCaption>
<bodyText confidence="0.9998861875">
English, the minimal effort tokenization splitting
off punctuation and otherwise respecting the spac-
ing in the Korean writing system. A Korean mor-
phological analysis tool4 was used to create the su-
pervised tokenization.
For Chinese-English, since a gold standard for
Chinese segmentation is available, we ran an addi-
tional evaluation of tokenization from each meth-
ods we have tested. We tokenized the raw text
of Chinese Treebank (Xia et al., 2000) using all
of the methods (supervised/unsupervised) we have
described in this section except for the bilingual
tokenization using P(f I e) because the English
translation of the Chinese Treebank data was not
available. We compared the result against the gold
standard segmentation and calculated the F-score.
</bodyText>
<sectionHeader confidence="0.999982" genericHeader="conclusions">
6 Results
</sectionHeader>
<bodyText confidence="0.991821571428572">
Results from Chinese-English and Korean-English
experiments are presented in Table 1. Note that
nature of data and number of references are dif-
ferent for the two language pairs, and therefore
the BLEU scores are not comparable. For both
language pairs, our models perform equally well
as supervised baselines, or even better. We can
</bodyText>
<footnote confidence="0.743106">
4http://nlp.kookmin.ac.kr/HAM/eng/main-e.html
</footnote>
<bodyText confidence="0.999804407407407">
observe three things from the result. First, tok-
enization of training data using P(f I e) tested on
a test set tokenized with P(f) performed worse
than any other experiments. This affirms our be-
lief that consistency in tokenization is important
for machine translation, which was also mentioned
by Chang et al. (2008). Secondly, we are learning
valuable information by looking at the target lan-
guage. Compare the result of the bilingual model
with 02 as the length factor to the result of the
monolingual model with the same length factor.
The bilingual version consistently performed bet-
ter than the monolingual model in all language
pairs. This tells us we can learn better token
boundaries by using information from the target
language. Thirdly, our hypothesis on the need
for heavy discount for longer tokens is confirmed.
The value for P(s) learned by the model was 0.55,
and 0.58 for Chinese, and Korean respectively. For
both language pairs, this accurately reflects the
empirical distribution of token length, as can be
seen in Figure 2. However, experiments where
P(s) was directly optimized performed better, in-
dicating that this parameter should be optimized
within the context of a complete system. The sec-
ond length factor 02, which discounts longer to-
kens even more heavily, generally performed bet-
</bodyText>
<page confidence="0.992795">
724
</page>
<table confidence="0.997576625">
English the two presidents will hold a joint press conference at the end of their summit talks.
Untokenized Korean ‘m �9��} Œ ôt ]œ ¤ õ Ù0 •Œ ¬ D 4à Œ ô ° ü |õ `�17 &amp;quot;1\\ä .y
Supervised 7 �7 �7
‘m �/9 @ Œ ô t ]˜ L- T O Ù 0•4 ¬ D * à Œ ô °+T z õÝ &amp;quot;IR X L-r+ .
Bilingual P(f I e) with 01 ‘m -JÁ@ Œô t ]œ ¤ õÙ 0•Œ¬ D *à Œô °ü  |õÝ ~\\ ä .
Bilingual P(f) with 02 ‘m �9Á @ Œô t ]œ ¤ õÙ 0•Œ¬ D *à Œô °ü  |õÝ 1I\ \ä .
Monolingual P(f) with 01 ‘m ;xj Á @ Œô t ]œ ¤ õÙ 0•Œ¬ D *à Œô °ü |õÝ &amp;quot;E-\\ ä .
Monolingual P(f) with 02 ‘m ;XJÁ @ Œô t ]œ ¤ õÙ 0•Œ¬ D *à Œô °ü |õÝ &amp;quot;E-\ \ä .
</table>
<figureCaption confidence="0.8327195">
Figure 3: Sample tokenization results for Korean-English data. The underscores are added to clearly
visualize where the breaks are.
</figureCaption>
<bodyText confidence="0.999980741935484">
ter than the first length factor when used in con-
junction with the bilingual model. Lastly, F-scores
of Chinese segmentations compared against the
gold standard shows higher segmentation accuracy
does not necessarily lead to higher BLEU score.
F-scores presented in Table 1 are not directly com-
parable for all different experiments because the
test data (Chinese Treebank) is used in training for
some of the supervised segmenters, but these num-
bers do show how close unsupervised segmenta-
tions are to the gold standard. It is interesting to
note that our highest unsupervised segmentation
result does make use of bilingual information.
Sample tokenization results for Korean-English
experiments are presented in Figure 3. We observe
that different configurations produce different tok-
enizations, and the bilingual model produced gen-
erally better tokenizations for translation com-
pared to the monolingual models or the super-
vised tokenizer. In this example, the tokenization
obtained from the supervised tokenizer, although
morphologically correct, is too fine-grained for the
purpose of translation to English. For example,
it correctly tokenized the attributive suffix L- -n
however, this is not desirable since English has no
such counterpart. Both variations of the monolin-
gual tokenization have errors such as incorrectly
not segmenting °ü |gyeol-gwa-reul, which is
a compound of a noun and a case marker, into °
ü  |gyeol-gwa reul as the bilingual model was
able to do.
</bodyText>
<subsectionHeader confidence="0.989305">
6.1 Conclusion and future work
</subsectionHeader>
<bodyText confidence="0.999362263157895">
We have shown that unsupervised tokenization for
machine translation is feasible and can outperform
rule-based methods that rely on lexical analysis,
or supervised statistical segmentations. The ap-
proach can be applied both to morphological anal-
ysis of Korean and the segmentation of sentences
into words for Chinese, which may at first glace
appear to be quite different problems. We have
only shown how our methods can be applied to
one language of the pair, where one language is
generally isolating and the other is generally syn-
thetic. However, our methods could be extended
to tokenization for both languages by iterating be-
tween languages. We also used the most simple
word-alignment model, but more complex word
alignment models could be incorporated into our
bilingual model.
Acknowledgments This work was supported by
NSF grants IIS-0546554 and ITR-0428020.
</bodyText>
<sectionHeader confidence="0.998942" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999907923076923">
Matthew J. Beal. 2003. Variational Algorithms forAp-
proximate Bayesian Inference. Ph.D. thesis, Univer-
sity College London.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263–311.
Pi-Chuan Chang, Michel Galley, and Christopher Man-
ning. 2008. Optimizing Chinese word segmentation
for machine translation performance. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 224–232.
Carl de Marcken. 1996. Linguistic structure as compo-
sition and perturbation. In Meeting of the Associa-
tion for Computational Linguistics, pages 335–341.
Morgan Kaufmann Publishers.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
Linguistics, 27(2):153–198.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2006. Contextual dependencies in un-
supervised word segmentation. In Proceedings of
the International Conference on Computational Lin-
guistics/Association for Computational Linguistics
(COLING/ACL-06), pages 673–680.
</reference>
<page confidence="0.979166">
725
</page>
<reference confidence="0.999707534246576">
Mark Johnson. 2007. Why doesn’t EM find good
HMM POS-taggers? In 2007 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 296–305, Prague, Czech Republic,
June. Association for Computational Linguistics.
Genichiro Kikui and Hirofumi Yamamoto. 2002.
Finding translation pairs from english-japanese un-
tokenized aligned corpora. In Proceedings of the
40th Annual Conference of the Association for
Computational Linguistics (ACL-02) workshop on
Speech-to-speech translation: algorithms and sys-
tems, pages 23–30. Association for Computational
Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Confer-
ence of the Association for Computational Linguis-
tics (ACL-07), Demonstration Session, pages 177–
180.
I. Dan Melamed. 1999. Bitext maps and alignment
via pattern recognition. Computational Linguistics,
25:107–130.
Robert C. Moore. 2002. Fast and accurate sentence
alignment of bilingual corpora. In AMTA ’02: Pro-
ceedings of the 5th Conference of the Association for
Machine Translation in the Americas on Machine
Translation: From Research to Real Users, pages
135–144, London, UK. Springer-Verlag.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of the 41th Annual Conference of the Association for
Computational Linguistics (ACL-03).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Conference of the Association for
Computational Linguistics (ACL-02).
Fei Xia, Martha Palmer, Nianwen Xue, Mary Ellen
Okurowski, John Kovarik, Shizhe Huang, Tony
Kroch, and Mitch Marcus. 2000. Developing
Guidelines and Ensuring Consistency for Chinese
Text Annotation. In Proc. of the 2nd International
Conference on Language Resources and Evaluation
(LREC-2000), Athens, Greece.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Her-
mann Ney. 2008. Bayesiansemi-supervised chinese
word segmentation for statistical machine transla-
tion. In Proceedings of the 22nd International
Conference on Computational Linguistics (Coling
2008), pages 1017–1024, Manchester, UK, August.
Coling 2008 Organizing Committee.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. In International Journal of Com-
putational Linguistics and Chinese Language Pro-
cessing, volume 8, pages 29–48.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008a. Bayesian learning of non-
compositional phrases with synchronous parsing. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-08),
pages 97–105, Columbus, Ohio.
Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.
2008b. Improved statistical machine translation by
multiple Chinese word segmentation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 216–223.
</reference>
<page confidence="0.998412">
726
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.874203">
<title confidence="0.999209">Unsupervised Tokenization for Machine Translation</title>
<author confidence="0.932812">Chung</author>
<affiliation confidence="0.9997655">Computer Science University of</affiliation>
<address confidence="0.999065">Rochester, NY 14627</address>
<abstract confidence="0.996466055555555">Training a statistical machine translation starts with tokenizing a parallel corpus. Some languages such as Chinese do not incorporate spacing in their writing system, which creates a challenge for tokenization. Moreover, morphologically rich languages such as Korean present an even bigger challenge, since optimal token boundaries for machine translation in these languages are often unclear. Both rule-based solutions and statistical solutions are currently used. In this paper, we present unsupervised methods to solve tokenization problem. Our methods incorporate information available from parallel corpus to determine a good tokenization for machine translation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Matthew J Beal</author>
</authors>
<title>Variational Algorithms forApproximate Bayesian Inference.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University College London.</institution>
<contexts>
<context position="13672" citStr="Beal (2003)" startWordPosition="2306" endWordPosition="2307">lingual tokenization model. Since we are considering every possible token f that can be guessed from our corpus, the data is very sparse. For the bilingual model, we are also using the EM algorithm to learn P(f |e), which means there is a danger of the EM algorithm memorizing the training data and thereby overfitting. We put a Dirichlet prior on our multinomial parameter for P(f |e) to control this situation. For both models, we also want a way to control the distribution of token length after tokenization. We address this problem by adding a length factor to our models. 4.1 Variational Bayes Beal (2003) and Johnson (2007) describe variational Bayes for hidden Markov model in detail, which can be directly applied to our bilingual model. With this Bayesian extension, the emission probability of our first model can be summarized as follows: θe |α ∼ Dir(α), fi |ei = e ∼ Multi(θe). e) P˜(f| =exp(0(ec(f, e) + α)) exp(0(Ee ec(f, e) + sα)) where 0 is the digamma function, and s is the size of the vocabulary from which f is drawn. Since we do not accurately know s, we set s to be the number of all possible tokens. As can be seen from the equation, by setting α to a small value, we are discounting the</context>
</contexts>
<marker>Beal, 2003</marker>
<rawString>Matthew J. Beal. 2003. Variational Algorithms forApproximate Bayesian Inference. Ph.D. thesis, University College London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="6545" citStr="Brown et al., 1993" startWordPosition="1021" endWordPosition="1024">y as blanket terms to cover the two different problems we have presented here. The problem of segmenting Chinese sentences into words and the problem of segmenting Korean or Hungarian “words” into tokens of right granularity are different in their nature. However, our models presented in section 3 handle the both problems. 3 Models We present two different methods for unsupervised tokenization. Both are essentially unigram tokenization models. In the first method, we try learning tokenization from word alignments with a model that bears resemblance to Hidden Markov models. We use IBM Model 1 (Brown et al., 1993) for the word alignment model. The second model is a relatively simpler monolingual tokenization model based on counts of substrings which serves as a baseline of unsupervised tokenization. 3.1 Learning tokenization from alignment We use expectation maximization as our primary tools in learning tokenization form parallel text. Here, the observed data provided to the algorithm are the tokenized English string en1 and the untokenized string of foreign characters cm1 . The unobserved variables are both the word-level alignments between the two strings, and the tokenization of the foreign string. </context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pi-Chuan Chang</author>
<author>Michel Galley</author>
<author>Christopher Manning</author>
</authors>
<title>Optimizing Chinese word segmentation for machine translation performance.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>224--232</pages>
<contexts>
<context position="1790" citStr="Chang et al. (2008)" startWordPosition="269" endWordPosition="272">ems of both languages incorporate spaces between “words”, the granularity is too coarse compared with languages such as English. A single word in these languages is composed of several morphemes, which often correspond to separate words in English. These languages also form compound nouns more freely. Ideally, we want to find segmentations for source and target languages that create a one-toone mapping of words. However, this is not always straightforward for two major reasons. First, what the optimal tokenization for machine translation should be is not always clear. Zhang et al. (2008b) and Chang et al. (2008) show that getting the tokenization of one of the languages in the corpus close to a gold standard does not necessarily help with building better machine translation systems. Second, even statistical methods require hand-annotated training data, which means that in resource-poor languages, good tokenization is hard to achieve. In this paper, we explore unsupervised methods for tokenization, with the goal of automatically finding an appropriate tokenization for machine translation. We compare methods that have access to parallel corpora to methods that are trained solely using data from the sou</context>
<context position="22675" citStr="Chang et al. (2008)" startWordPosition="3862" endWordPosition="3865">nd tokenization requiring a minimal effort for the each language pair. For ChineseEnglish, the minimal effort tokenization is maximal tokenization where every Chinese character is segmented. Since a number of Chinese tokenizers are available, we have tried four different tokenizations for the supervised tokenizations. The first one is the LDC Chinese tokenizer available at the LDC website3, which is compiled by Zhibiao Wu. The second tokenizer is a maxent-based tokenizer described by Xue (2003). The third and fourth tokenizations come from the CRF-based Stanford Chinese segmenter described by Chang et al. (2008). The difference between third and fourth tokenization comes from the different gold standard, the third one is based on Beijing University’s segmentation (pku) and the fourth one is based on Chinese Treebank (ctb). For Korean3http://projects.ldc.upenn.edu/Chinese/LDC ch.htm 723 Chinese Korean BLEU F-score BLEU Supervised 20.03 0.94 7.27 Rule-based morphological analyzer 23.02 0.96 LDC segmenter 21.69 0.96 Xue’s segmenter 22.45 1.00 Stanford segmenter (pku) Stanford segmenter (ctb) Unsupervised Splitting punctuation only 6.04 Maximal (Character-based MT) 20.32 0.75 Bilingual P(f I e) with 01 P</context>
<context position="25336" citStr="Chang et al. (2008)" startWordPosition="4273" endWordPosition="4276">Table 1. Note that nature of data and number of references are different for the two language pairs, and therefore the BLEU scores are not comparable. For both language pairs, our models perform equally well as supervised baselines, or even better. We can 4http://nlp.kookmin.ac.kr/HAM/eng/main-e.html observe three things from the result. First, tokenization of training data using P(f I e) tested on a test set tokenized with P(f) performed worse than any other experiments. This affirms our belief that consistency in tokenization is important for machine translation, which was also mentioned by Chang et al. (2008). Secondly, we are learning valuable information by looking at the target language. Compare the result of the bilingual model with 02 as the length factor to the result of the monolingual model with the same length factor. The bilingual version consistently performed better than the monolingual model in all language pairs. This tells us we can learn better token boundaries by using information from the target language. Thirdly, our hypothesis on the need for heavy discount for longer tokens is confirmed. The value for P(s) learned by the model was 0.55, and 0.58 for Chinese, and Korean respect</context>
</contexts>
<marker>Chang, Galley, Manning, 2008</marker>
<rawString>Pi-Chuan Chang, Michel Galley, and Christopher Manning. 2008. Optimizing Chinese word segmentation for machine translation performance. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 224–232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl de Marcken</author>
</authors>
<title>Linguistic structure as composition and perturbation.</title>
<date>1996</date>
<booktitle>In Meeting of the Association for Computational Linguistics,</booktitle>
<pages>335--341</pages>
<publisher>Morgan Kaufmann Publishers.</publisher>
<marker>de Marcken, 1996</marker>
<rawString>Carl de Marcken. 1996. Linguistic structure as composition and perturbation. In Meeting of the Association for Computational Linguistics, pages 335–341. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Goldsmith</author>
</authors>
<title>Unsupervised learning of the morphology of a natural language.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="2595" citStr="Goldsmith, 2001" startWordPosition="395" endWordPosition="396">tatistical methods require hand-annotated training data, which means that in resource-poor languages, good tokenization is hard to achieve. In this paper, we explore unsupervised methods for tokenization, with the goal of automatically finding an appropriate tokenization for machine translation. We compare methods that have access to parallel corpora to methods that are trained solely using data from the source language. Unsupervised monolingual segmentation has been studied as a model of language acquisition (Goldwater et al., 2006), and as model of learning morphology in European languages (Goldsmith, 2001). Unsupervised segmentation using bilingual data has been attempted for finding new translation pairs (Kikui and Yamamoto, 2002), and for finding good segmentation for Chinese in machine translation using Gibbs sampling (Xu et al., 2008). In this paper, further investigate the use of bilingual information to find tokenizations tailored for machine translation. We find a benefit not only for segmentation of languages with no space in the writing system (such as Chinese), but also for the smaller-scale tokenization problem of normalizing between languages that include more or less information in</context>
<context position="10627" citStr="Goldsmith, 2001" startWordPosition="1760" endWordPosition="1761">ated by e1. segmentation is by-product of learning the alignment. We can find the optimal segmentation of a new source language sentence using the Viterbi algorithm. Given two sentences e and f, a* = argmax P(f, a |e) a and segmentation s* implied by alignment a* is the optimal segmentation of f found by this model. 3.2 Learning tokenization from substring counts The second tokenization model we propose is much simpler. More sophisticated unsupervised monolingual tokenization models using hierarchical Bayesian models (Goldwater et al., 2006) and using the minimum description length principle (Goldsmith, 2001; de Marcken, 1996) have been studied. Our model is meant to serve as a computationally efficient baseline for unsupervised monolingual tokenization. Given a corpus of only source language of unknown tokenization, we want to find the optimal s given c — s that gives us the highest P(s |c). According to Bayes’ rule, P(s |c) a P(c |s)P(s) Again, we assume that all P(s) are equally likely. Let f = s o c = f1 ... fℓ, where fi is a word under some possible segmentation s. We want to find the s that maximizes P(f). We assume that P(f) = P(f1) x ... x P(fℓ) To calculate P(fi), we count every possible</context>
</contexts>
<marker>Goldsmith, 2001</marker>
<rawString>John Goldsmith. 2001. Unsupervised learning of the morphology of a natural language. Computational Linguistics, 27(2):153–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Contextual dependencies in unsupervised word segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics/Association for Computational Linguistics (COLING/ACL-06),</booktitle>
<pages>673--680</pages>
<contexts>
<context position="2518" citStr="Goldwater et al., 2006" startWordPosition="381" endWordPosition="384">ot necessarily help with building better machine translation systems. Second, even statistical methods require hand-annotated training data, which means that in resource-poor languages, good tokenization is hard to achieve. In this paper, we explore unsupervised methods for tokenization, with the goal of automatically finding an appropriate tokenization for machine translation. We compare methods that have access to parallel corpora to methods that are trained solely using data from the source language. Unsupervised monolingual segmentation has been studied as a model of language acquisition (Goldwater et al., 2006), and as model of learning morphology in European languages (Goldsmith, 2001). Unsupervised segmentation using bilingual data has been attempted for finding new translation pairs (Kikui and Yamamoto, 2002), and for finding good segmentation for Chinese in machine translation using Gibbs sampling (Xu et al., 2008). In this paper, further investigate the use of bilingual information to find tokenizations tailored for machine translation. We find a benefit not only for segmentation of languages with no space in the writing system (such as Chinese), but also for the smaller-scale tokenization prob</context>
<context position="10559" citStr="Goldwater et al., 2006" startWordPosition="1748" endWordPosition="1751">us c1, c2, c3 form f1 and c3 forms f2. f1 is generated by e2 and f2 is generated by e1. segmentation is by-product of learning the alignment. We can find the optimal segmentation of a new source language sentence using the Viterbi algorithm. Given two sentences e and f, a* = argmax P(f, a |e) a and segmentation s* implied by alignment a* is the optimal segmentation of f found by this model. 3.2 Learning tokenization from substring counts The second tokenization model we propose is much simpler. More sophisticated unsupervised monolingual tokenization models using hierarchical Bayesian models (Goldwater et al., 2006) and using the minimum description length principle (Goldsmith, 2001; de Marcken, 1996) have been studied. Our model is meant to serve as a computationally efficient baseline for unsupervised monolingual tokenization. Given a corpus of only source language of unknown tokenization, we want to find the optimal s given c — s that gives us the highest P(s |c). According to Bayes’ rule, P(s |c) a P(c |s)P(s) Again, we assume that all P(s) are equally likely. Let f = s o c = f1 ... fℓ, where fi is a word under some possible segmentation s. We want to find the s that maximizes P(f). We assume that P(</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2006. Contextual dependencies in unsupervised word segmentation. In Proceedings of the International Conference on Computational Linguistics/Association for Computational Linguistics (COLING/ACL-06), pages 673–680.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Why doesn’t EM find good HMM POS-taggers?</title>
<date>2007</date>
<booktitle>In 2007 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>296--305</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="13691" citStr="Johnson (2007)" startWordPosition="2309" endWordPosition="2310">tion model. Since we are considering every possible token f that can be guessed from our corpus, the data is very sparse. For the bilingual model, we are also using the EM algorithm to learn P(f |e), which means there is a danger of the EM algorithm memorizing the training data and thereby overfitting. We put a Dirichlet prior on our multinomial parameter for P(f |e) to control this situation. For both models, we also want a way to control the distribution of token length after tokenization. We address this problem by adding a length factor to our models. 4.1 Variational Bayes Beal (2003) and Johnson (2007) describe variational Bayes for hidden Markov model in detail, which can be directly applied to our bilingual model. With this Bayesian extension, the emission probability of our first model can be summarized as follows: θe |α ∼ Dir(α), fi |ei = e ∼ Multi(θe). e) P˜(f| =exp(0(ec(f, e) + α)) exp(0(Ee ec(f, e) + sα)) where 0 is the digamma function, and s is the size of the vocabulary from which f is drawn. Since we do not accurately know s, we set s to be the number of all possible tokens. As can be seen from the equation, by setting α to a small value, we are discounting the expected count wit</context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>Mark Johnson. 2007. Why doesn’t EM find good HMM POS-taggers? In 2007 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 296–305, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Genichiro Kikui</author>
<author>Hirofumi Yamamoto</author>
</authors>
<title>Finding translation pairs from english-japanese untokenized aligned corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Conference of the Association for Computational Linguistics (ACL-02) workshop</booktitle>
<pages>23--30</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2723" citStr="Kikui and Yamamoto, 2002" startWordPosition="410" endWordPosition="413"> is hard to achieve. In this paper, we explore unsupervised methods for tokenization, with the goal of automatically finding an appropriate tokenization for machine translation. We compare methods that have access to parallel corpora to methods that are trained solely using data from the source language. Unsupervised monolingual segmentation has been studied as a model of language acquisition (Goldwater et al., 2006), and as model of learning morphology in European languages (Goldsmith, 2001). Unsupervised segmentation using bilingual data has been attempted for finding new translation pairs (Kikui and Yamamoto, 2002), and for finding good segmentation for Chinese in machine translation using Gibbs sampling (Xu et al., 2008). In this paper, further investigate the use of bilingual information to find tokenizations tailored for machine translation. We find a benefit not only for segmentation of languages with no space in the writing system (such as Chinese), but also for the smaller-scale tokenization problem of normalizing between languages that include more or less information in a “word” as defined by the writing system, using Korean-English for our experiments. Here too, we find a benefit from using bil</context>
</contexts>
<marker>Kikui, Yamamoto, 2002</marker>
<rawString>Genichiro Kikui and Hirofumi Yamamoto. 2002. Finding translation pairs from english-japanese untokenized aligned corpora. In Proceedings of the 40th Annual Conference of the Association for Computational Linguistics (ACL-02) workshop on Speech-to-speech translation: algorithms and systems, pages 23–30. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondrej Bojar,</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Conference of the Association for Computational Linguistics (ACL-07), Demonstration Session,</booktitle>
<pages>177--180</pages>
<location>Alexandra</location>
<contexts>
<context position="19771" citStr="Koehn et al., 2007" startWordPosition="3369" endWordPosition="3372">each parallel corpus consisting of about 2M words and 60K sentences on the English side. For our development set and test set, Chinese-English had about 1000 sentences each with 10 reference translations taken from the NIST 2002 MT evaluation. For Korean-English, 2200 sentence pairs were randomly sampled from the parallel corpus, and held out from the training data. These were divided in half and used for test set and development set respectively. For all language pairs, very minimal tokenization — splitting off punctuation — was done on the English side. 5.2 Experimental setup We used Moses (Koehn et al., 2007) to train machine translation systems. Default parameters were used for all experiments except for the number of iterations for GIZA++ (Och and Ney, 2003). GIZA++ was run until the perplexity on development set stopped decreasing. For practical reasons, the maximum size of a token was set at three for Chinese, and four for Korean.2 Minimum error rate training (Och, 2003) was run on each system afterwards and BLEU score (Papineni et al., 2002) was calculated on the test sets. For the monolingual model, we tested two versions with the length factor 01, and 02. We picked A and P(s) so that the nu</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Conference of the Association for Computational Linguistics (ACL-07), Demonstration Session, pages 177– 180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Bitext maps and alignment via pattern recognition.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<pages>25--107</pages>
<contexts>
<context position="19131" citStr="Melamed (1999)" startWordPosition="3264" endWordPosition="3265">gual model, we pick the A in the same manner as the monolingual model and run the Viterbi algorithm. After applying the length factor, what we have is a log-linear model for tokenization, with two feature functions with equal weights: the length factor and P(f) learned from model. 5 Experiments 5.1 Data We tested our tokenization methods on two different language pairs: Chinese-English, and KoreanEnglish. For Chinese-English, we used FBIS newswire data. The Korean-English parallel data was collected from news websites and sentencealigned using two different tools described by Moore (2002) and Melamed (1999). We used subsets of each parallel corpus consisting of about 2M words and 60K sentences on the English side. For our development set and test set, Chinese-English had about 1000 sentences each with 10 reference translations taken from the NIST 2002 MT evaluation. For Korean-English, 2200 sentence pairs were randomly sampled from the parallel corpus, and held out from the training data. These were divided in half and used for test set and development set respectively. For all language pairs, very minimal tokenization — splitting off punctuation — was done on the English side. 5.2 Experimental </context>
</contexts>
<marker>Melamed, 1999</marker>
<rawString>I. Dan Melamed. 1999. Bitext maps and alignment via pattern recognition. Computational Linguistics, 25:107–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>Fast and accurate sentence alignment of bilingual corpora.</title>
<date>2002</date>
<booktitle>In AMTA ’02: Proceedings of the 5th Conference of the Association for Machine Translation in the Americas on Machine Translation: From Research to Real Users,</booktitle>
<pages>135--144</pages>
<publisher>Springer-Verlag.</publisher>
<location>London, UK.</location>
<contexts>
<context position="19112" citStr="Moore (2002)" startWordPosition="3261" endWordPosition="3262">f) from the bilingual model, we pick the A in the same manner as the monolingual model and run the Viterbi algorithm. After applying the length factor, what we have is a log-linear model for tokenization, with two feature functions with equal weights: the length factor and P(f) learned from model. 5 Experiments 5.1 Data We tested our tokenization methods on two different language pairs: Chinese-English, and KoreanEnglish. For Chinese-English, we used FBIS newswire data. The Korean-English parallel data was collected from news websites and sentencealigned using two different tools described by Moore (2002) and Melamed (1999). We used subsets of each parallel corpus consisting of about 2M words and 60K sentences on the English side. For our development set and test set, Chinese-English had about 1000 sentences each with 10 reference translations taken from the NIST 2002 MT evaluation. For Korean-English, 2200 sentence pairs were randomly sampled from the parallel corpus, and held out from the training data. These were divided in half and used for test set and development set respectively. For all language pairs, very minimal tokenization — splitting off punctuation — was done on the English side</context>
</contexts>
<marker>Moore, 2002</marker>
<rawString>Robert C. Moore. 2002. Fast and accurate sentence alignment of bilingual corpora. In AMTA ’02: Proceedings of the 5th Conference of the Association for Machine Translation in the Americas on Machine Translation: From Research to Real Users, pages 135–144, London, UK. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="19925" citStr="Och and Ney, 2003" startWordPosition="3394" endWordPosition="3397">00 sentences each with 10 reference translations taken from the NIST 2002 MT evaluation. For Korean-English, 2200 sentence pairs were randomly sampled from the parallel corpus, and held out from the training data. These were divided in half and used for test set and development set respectively. For all language pairs, very minimal tokenization — splitting off punctuation — was done on the English side. 5.2 Experimental setup We used Moses (Koehn et al., 2007) to train machine translation systems. Default parameters were used for all experiments except for the number of iterations for GIZA++ (Och and Ney, 2003). GIZA++ was run until the perplexity on development set stopped decreasing. For practical reasons, the maximum size of a token was set at three for Chinese, and four for Korean.2 Minimum error rate training (Och, 2003) was run on each system afterwards and BLEU score (Papineni et al., 2002) was calculated on the test sets. For the monolingual model, we tested two versions with the length factor 01, and 02. We picked A and P(s) so that the number of tokens on source side (Chinese, and Korean) will be about the same 2In the Korean writing system, one character is actually one syllable block. We</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41th Annual Conference of the Association for Computational Linguistics (ACL-03).</booktitle>
<contexts>
<context position="20144" citStr="Och, 2003" startWordPosition="3434" endWordPosition="3435">vided in half and used for test set and development set respectively. For all language pairs, very minimal tokenization — splitting off punctuation — was done on the English side. 5.2 Experimental setup We used Moses (Koehn et al., 2007) to train machine translation systems. Default parameters were used for all experiments except for the number of iterations for GIZA++ (Och and Ney, 2003). GIZA++ was run until the perplexity on development set stopped decreasing. For practical reasons, the maximum size of a token was set at three for Chinese, and four for Korean.2 Minimum error rate training (Och, 2003) was run on each system afterwards and BLEU score (Papineni et al., 2002) was calculated on the test sets. For the monolingual model, we tested two versions with the length factor 01, and 02. We picked A and P(s) so that the number of tokens on source side (Chinese, and Korean) will be about the same 2In the Korean writing system, one character is actually one syllable block. We do not decompose syllable blocks into individual consonants and vowels. as the number of tokens in the target side (English). For the bilingual model, as explained in the model section, we are learning P(f I e), but on</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training for statistical machine translation. In Proceedings of the 41th Annual Conference of the Association for Computational Linguistics (ACL-03).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Conference of the Association for Computational Linguistics (ACL-02).</booktitle>
<contexts>
<context position="20217" citStr="Papineni et al., 2002" startWordPosition="3445" endWordPosition="3448">ctively. For all language pairs, very minimal tokenization — splitting off punctuation — was done on the English side. 5.2 Experimental setup We used Moses (Koehn et al., 2007) to train machine translation systems. Default parameters were used for all experiments except for the number of iterations for GIZA++ (Och and Ney, 2003). GIZA++ was run until the perplexity on development set stopped decreasing. For practical reasons, the maximum size of a token was set at three for Chinese, and four for Korean.2 Minimum error rate training (Och, 2003) was run on each system afterwards and BLEU score (Papineni et al., 2002) was calculated on the test sets. For the monolingual model, we tested two versions with the length factor 01, and 02. We picked A and P(s) so that the number of tokens on source side (Chinese, and Korean) will be about the same 2In the Korean writing system, one character is actually one syllable block. We do not decompose syllable blocks into individual consonants and vowels. as the number of tokens in the target side (English). For the bilingual model, as explained in the model section, we are learning P(f I e), but only P(f) is available for tokenizing any new data. We compared two conditi</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Conference of the Association for Computational Linguistics (ACL-02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Martha Palmer</author>
<author>Nianwen Xue</author>
<author>Mary Ellen Okurowski</author>
<author>John Kovarik</author>
<author>Shizhe Huang</author>
<author>Tony Kroch</author>
<author>Mitch Marcus</author>
</authors>
<title>Developing Guidelines and Ensuring Consistency for Chinese Text Annotation.</title>
<date>2000</date>
<booktitle>In Proc. of the 2nd International Conference on Language Resources and Evaluation (LREC-2000),</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="24321" citStr="Xia et al., 2000" startWordPosition="4118" endWordPosition="4121">or Chinese-English and Korean-English experiments and F-score of segmentation compared against Chinese Treebank standard. The highest unsupervised score is highlighted. English, the minimal effort tokenization splitting off punctuation and otherwise respecting the spacing in the Korean writing system. A Korean morphological analysis tool4 was used to create the supervised tokenization. For Chinese-English, since a gold standard for Chinese segmentation is available, we ran an additional evaluation of tokenization from each methods we have tested. We tokenized the raw text of Chinese Treebank (Xia et al., 2000) using all of the methods (supervised/unsupervised) we have described in this section except for the bilingual tokenization using P(f I e) because the English translation of the Chinese Treebank data was not available. We compared the result against the gold standard segmentation and calculated the F-score. 6 Results Results from Chinese-English and Korean-English experiments are presented in Table 1. Note that nature of data and number of references are different for the two language pairs, and therefore the BLEU scores are not comparable. For both language pairs, our models perform equally w</context>
</contexts>
<marker>Xia, Palmer, Xue, Okurowski, Kovarik, Huang, Kroch, Marcus, 2000</marker>
<rawString>Fei Xia, Martha Palmer, Nianwen Xue, Mary Ellen Okurowski, John Kovarik, Shizhe Huang, Tony Kroch, and Mitch Marcus. 2000. Developing Guidelines and Ensuring Consistency for Chinese Text Annotation. In Proc. of the 2nd International Conference on Language Resources and Evaluation (LREC-2000), Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Xu</author>
<author>Jianfeng Gao</author>
<author>Kristina Toutanova</author>
<author>Hermann Ney</author>
</authors>
<title>Bayesiansemi-supervised chinese word segmentation for statistical machine translation.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>1017--1024</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="2832" citStr="Xu et al., 2008" startWordPosition="428" endWordPosition="431">inding an appropriate tokenization for machine translation. We compare methods that have access to parallel corpora to methods that are trained solely using data from the source language. Unsupervised monolingual segmentation has been studied as a model of language acquisition (Goldwater et al., 2006), and as model of learning morphology in European languages (Goldsmith, 2001). Unsupervised segmentation using bilingual data has been attempted for finding new translation pairs (Kikui and Yamamoto, 2002), and for finding good segmentation for Chinese in machine translation using Gibbs sampling (Xu et al., 2008). In this paper, further investigate the use of bilingual information to find tokenizations tailored for machine translation. We find a benefit not only for segmentation of languages with no space in the writing system (such as Chinese), but also for the smaller-scale tokenization problem of normalizing between languages that include more or less information in a “word” as defined by the writing system, using Korean-English for our experiments. Here too, we find a benefit from using bilingual information, with unsupervised segmentation rivaling and in some cases surpassing supervised segmentat</context>
</contexts>
<marker>Xu, Gao, Toutanova, Ney, 2008</marker>
<rawString>Jia Xu, Jianfeng Gao, Kristina Toutanova, and Hermann Ney. 2008. Bayesiansemi-supervised chinese word segmentation for statistical machine translation. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1017–1024, Manchester, UK, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Chinese word segmentation as character tagging.</title>
<date>2003</date>
<booktitle>In International Journal of Computational Linguistics and Chinese Language Processing,</booktitle>
<volume>8</volume>
<pages>29--48</pages>
<contexts>
<context position="22555" citStr="Xue (2003)" startWordPosition="3846" endWordPosition="3847">set was tokenized only using information from P(f). We also trained MT systems using supervised tokenizations and tokenization requiring a minimal effort for the each language pair. For ChineseEnglish, the minimal effort tokenization is maximal tokenization where every Chinese character is segmented. Since a number of Chinese tokenizers are available, we have tried four different tokenizations for the supervised tokenizations. The first one is the LDC Chinese tokenizer available at the LDC website3, which is compiled by Zhibiao Wu. The second tokenizer is a maxent-based tokenizer described by Xue (2003). The third and fourth tokenizations come from the CRF-based Stanford Chinese segmenter described by Chang et al. (2008). The difference between third and fourth tokenization comes from the different gold standard, the third one is based on Beijing University’s segmentation (pku) and the fourth one is based on Chinese Treebank (ctb). For Korean3http://projects.ldc.upenn.edu/Chinese/LDC ch.htm 723 Chinese Korean BLEU F-score BLEU Supervised 20.03 0.94 7.27 Rule-based morphological analyzer 23.02 0.96 LDC segmenter 21.69 0.96 Xue’s segmenter 22.45 1.00 Stanford segmenter (pku) Stanford segmenter</context>
</contexts>
<marker>Xue, 2003</marker>
<rawString>Nianwen Xue. 2003. Chinese word segmentation as character tagging. In International Journal of Computational Linguistics and Chinese Language Processing, volume 8, pages 29–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Chris Quirk</author>
<author>Robert C Moore</author>
<author>Daniel Gildea</author>
</authors>
<title>Bayesian learning of noncompositional phrases with synchronous parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL-08),</booktitle>
<pages>97--105</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="1764" citStr="Zhang et al. (2008" startWordPosition="264" endWordPosition="267">although the writing systems of both languages incorporate spaces between “words”, the granularity is too coarse compared with languages such as English. A single word in these languages is composed of several morphemes, which often correspond to separate words in English. These languages also form compound nouns more freely. Ideally, we want to find segmentations for source and target languages that create a one-toone mapping of words. However, this is not always straightforward for two major reasons. First, what the optimal tokenization for machine translation should be is not always clear. Zhang et al. (2008b) and Chang et al. (2008) show that getting the tokenization of one of the languages in the corpus close to a gold standard does not necessarily help with building better machine translation systems. Second, even statistical methods require hand-annotated training data, which means that in resource-poor languages, good tokenization is hard to achieve. In this paper, we explore unsupervised methods for tokenization, with the goal of automatically finding an appropriate tokenization for machine translation. We compare methods that have access to parallel corpora to methods that are trained sole</context>
<context position="14410" citStr="Zhang et al. (2008" startWordPosition="2443" endWordPosition="2446">ilingual model. With this Bayesian extension, the emission probability of our first model can be summarized as follows: θe |α ∼ Dir(α), fi |ei = e ∼ Multi(θe). e) P˜(f| =exp(0(ec(f, e) + α)) exp(0(Ee ec(f, e) + sα)) where 0 is the digamma function, and s is the size of the vocabulary from which f is drawn. Since we do not accurately know s, we set s to be the number of all possible tokens. As can be seen from the equation, by setting α to a small value, we are discounting the expected count with help of the digamma function. Thus, having lower α leads to a sparser solution. Johnson (2007) and Zhang et al. (2008a) show having small α helps to control overfitting. Following this, we set our Dirichlet prior to be as sparse as possible. It is set at α = 10−6, the number we used as floor of our probability. For the model incorporating the length factor, which is described in the next section, we do not place a prior on our transition probability, since there are only two possible states, i.e. P(s = 1) and P(s = 0). This distribution is not as sparse as the emission probability. Comparing variational Bayes to the traditional EM algorithm, the E step stays the same but the M step for calculating the emissi</context>
</contexts>
<marker>Zhang, Quirk, Moore, Gildea, 2008</marker>
<rawString>Hao Zhang, Chris Quirk, Robert C. Moore, and Daniel Gildea. 2008a. Bayesian learning of noncompositional phrases with synchronous parsing. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL-08), pages 97–105, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruiqiang Zhang</author>
<author>Keiji Yasuda</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Improved statistical machine translation by multiple Chinese word segmentation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>216--223</pages>
<contexts>
<context position="1764" citStr="Zhang et al. (2008" startWordPosition="264" endWordPosition="267">although the writing systems of both languages incorporate spaces between “words”, the granularity is too coarse compared with languages such as English. A single word in these languages is composed of several morphemes, which often correspond to separate words in English. These languages also form compound nouns more freely. Ideally, we want to find segmentations for source and target languages that create a one-toone mapping of words. However, this is not always straightforward for two major reasons. First, what the optimal tokenization for machine translation should be is not always clear. Zhang et al. (2008b) and Chang et al. (2008) show that getting the tokenization of one of the languages in the corpus close to a gold standard does not necessarily help with building better machine translation systems. Second, even statistical methods require hand-annotated training data, which means that in resource-poor languages, good tokenization is hard to achieve. In this paper, we explore unsupervised methods for tokenization, with the goal of automatically finding an appropriate tokenization for machine translation. We compare methods that have access to parallel corpora to methods that are trained sole</context>
<context position="14410" citStr="Zhang et al. (2008" startWordPosition="2443" endWordPosition="2446">ilingual model. With this Bayesian extension, the emission probability of our first model can be summarized as follows: θe |α ∼ Dir(α), fi |ei = e ∼ Multi(θe). e) P˜(f| =exp(0(ec(f, e) + α)) exp(0(Ee ec(f, e) + sα)) where 0 is the digamma function, and s is the size of the vocabulary from which f is drawn. Since we do not accurately know s, we set s to be the number of all possible tokens. As can be seen from the equation, by setting α to a small value, we are discounting the expected count with help of the digamma function. Thus, having lower α leads to a sparser solution. Johnson (2007) and Zhang et al. (2008a) show having small α helps to control overfitting. Following this, we set our Dirichlet prior to be as sparse as possible. It is set at α = 10−6, the number we used as floor of our probability. For the model incorporating the length factor, which is described in the next section, we do not place a prior on our transition probability, since there are only two possible states, i.e. P(s = 1) and P(s = 0). This distribution is not as sparse as the emission probability. Comparing variational Bayes to the traditional EM algorithm, the E step stays the same but the M step for calculating the emissi</context>
</contexts>
<marker>Zhang, Yasuda, Sumita, 2008</marker>
<rawString>Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita. 2008b. Improved statistical machine translation by multiple Chinese word segmentation. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 216–223.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>