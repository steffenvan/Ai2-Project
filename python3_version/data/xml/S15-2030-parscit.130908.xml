<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000238">
<title confidence="0.9972625">
NeRoSim: A System for Measuring and Interpreting Semantic Textual
Similarity
</title>
<author confidence="0.9356675">
Rajendra Banjade*, Nobal B. Niraula*, Nabin Maharjan*, Vasile Rus, Dan Stefanescu†,
Mihai Lintean†, Dipesh Gautam
</author>
<affiliation confidence="0.9996565">
Department of Computer Science
The University of Memphis
</affiliation>
<address confidence="0.738779">
Memphis, TN
</address>
<email confidence="0.998933">
{rbanjade,nbnraula,nmharjan,vrus,dstfnscu,mclinten,dgautam}@memphis.edu
</email>
<sectionHeader confidence="0.998595" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998616555555556">
We present in this paper our system developed
for SemEval 2015 Shared Task 2 (2a - En-
glish Semantic Textual Similarity, STS, and
2c - Interpretable Similarity) and the results
of the submitted runs. For the English STS
subtask, we used regression models combin-
ing a wide array of features including semantic
similarity scores obtained from various meth-
ods. One of our runs achieved weighted mean
correlation score of 0.784 for sentence similar-
ity subtask (i.e., English STS) and was ranked
tenth among 74 runs submitted by 29 teams.
For the interpretable similarity pilot task, we
employed a rule-based approach blended with
chunk alignment labeling and scoring based
on semantic similarity features. Our system
for interpretable text similarity was among the
top three best performing systems.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994997553191489">
Semantic Textual Similarity (STS) is the task of
measuring the degree of semantic equivalence for
a given pair of texts. The importance of semantic
similarity in Natural Language Processing is high-
lighted by the diversity of datasets and shared task
evaluation campaigns over the last decade (Dolan et
al., 2004; Agirre et al., 2012; Agirre et al., 2013;
Agirre et al., 2014; Rus et al., 2014) and by many
uses such as in text summarization (Aliguliyev,
2009) and student answer assessment (Rus and Lin-
tean, 2012; Niraula et al., 2013).
∗* These authors contributed equally to this work
††Work done while at University of Memphis
This year’s SemEval shared task on semantic
textual similarity focused on English STS, Span-
ish STS, and Interpretable Similarity (Agirre et al.,
2015). We participated in the English STS and In-
terpretable Similarity subtasks. We describe in this
paper our systems participated in these two subtasks.
The English STS subtask was about assigning a
similarity score between 0 and 5 to pairs of sen-
tences; a score of 0 meaning the sentences are un-
related and 5 indicating they are equivalent. Our
three runs for this subtask combined a wide array
of features including similarity scores calculated us-
ing knowledge based and corpus based methods in
a regression model (cf. Section 2). One of our sys-
tems achieved mean correlation score of 0.784 with
human judgment on the test data.
Although STS systems measure the degree of se-
mantic equivalence in terms of a score which is use-
ful in many tasks, they stop short of explaining why
the texts are similar, related, or unrelated. They
do not indicate what kind of semantic relations ex-
ist among the constituents (words or chunks) of the
target texts. Finding explicit relations between con-
stituents in the paired texts would enable a mean-
ingful interpretation of the similarity scores. To this
end, Brockett (2007) and Rus et al. (2012) produced
datasets where corresponding words (or multiword
expressions) were aligned and in the later case their
semantic relations were explicitly labeled. Simi-
larly, this year’s pilot subtask called Interpretable
Similarity required systems to align the segments
(chunks) either using the chunked texts given by the
organizers or chunking the given texts and indicat-
ing the type of semantic relations (such as EQUI for
</bodyText>
<page confidence="0.977781">
164
</page>
<note confidence="0.9534445">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 164–171,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.99824575">
equivalent, OPPO for opposite) between each pair
of aligned chunks. Moreover, a similarity score for
each alignment (0 − unrelated, 5 − equivalent) had
to be assigned. We applied a set of rules blended
with similarity features in order to assign the labels
and scores for the chunk-level relations (cf. Section
3). Our system was among the top performing sys-
tems in this subtask.
</bodyText>
<sectionHeader confidence="0.997382" genericHeader="method">
2 System for English STS
</sectionHeader>
<bodyText confidence="0.999771125">
We used regression models to compute final
sentence-to-sentence similarity scores using various
features such as different sentence-to-sentence simi-
larity scores, presence of negation cues, lexical over-
lap measures etc. The sentence-to-sentence similar-
ity scores were calculated using word-to-word sim-
ilarity methods and optimal word and chunk align-
ments.
</bodyText>
<subsectionHeader confidence="0.98588">
2.1 Word-to-Word Similarity
</subsectionHeader>
<bodyText confidence="0.999982545454546">
We used knowledge based, corpus based, and hy-
brid methods to compute word-to-word similarity.
From the knowledge based category, we used Word-
Net (Fellbaum, 1998) based similarity methods from
SEMILAR Toolkit (Rus et al., 2013) which in-
clude Lin (Lin, 1998), Lesk (Banerjee and Pedersen,
2003), Hso (Hirst and St-Onge, 1998), Jcn (Jiang
and Conrath, 1997), Res (Resnik, 1995), Path, Lch
(Leacock and Chodorow, 1998), and Wup (Wu and
Palmer, 1994).
In corpus based category, we developed Latent
Semantic Analysis (LSA) (Landauer et al., 2007)
models1 from the whole Wikipedia articles as de-
scribed in Stefanescu et al. (2014a). We also used
pre-trained Mikolov word representations (Mikolov
et al., 2013)2 and GloVe word vectors (Pennington
et al., 2014)3. In these cases, each word was rep-
resented as a vector encoding and the similarity be-
tween words were computed as cosine similarity be-
tween corresponding vectors. We exploited the lex-
ical relations between words, i.e. synonymy and
antonymy, from WordNet 3.0. As such we computed
</bodyText>
<footnote confidence="0.999939">
1Models available at http://semanticsimilarity.org
2Downloaded from http://code.google.com/p/word2vec/
3Downloaded from http://nlp.stanford.edu/projects/glove/
</footnote>
<bodyText confidence="0.991876714285714">
similarity scores between two words a and b as:
1, if a and b are synonyms
0, if a and b are antonyms
A.B
otherwise
where A and B are vector representations of words
a and b respectively.
In hybrid approach, we developed a new
word-to-word similarity measure (hereafter referred
as Combined-Word-Measure) by combining the
WordNet-based similarity methods with corpus
based methods (using Mikolov’s word embeddings
and GloVe vectors) by applying Support Vector Re-
gression (Banjade et al., 2015).
</bodyText>
<subsectionHeader confidence="0.998021">
2.2 Sentence-to-Sentence Similarity
</subsectionHeader>
<bodyText confidence="0.999653">
We applied three different approaches to compute
sentence-to-sentence similarity.
</bodyText>
<subsectionHeader confidence="0.836643">
2.2.1 Optimal Word Alignment Method
</subsectionHeader>
<bodyText confidence="0.999988055555556">
Our alignment step was based on the optimal as-
signment problem, a fundamental combinatorial op-
timization problem which consists of finding a maxi-
mum weight matching in a weighted bipartite graph.
An algorithm, the Kuhn-Munkres method (Kuhn,
1955), can find solutions to the optimum assignment
problem in polynomial time.
In our case, we first computed the similarity of
word pairs (all possible combinations) using all sim-
ilarity methods described in Section 2.1. The sim-
ilarity score less than 0.3 (empirically set thresh-
old), was reset to 0 in order to avoid noisy align-
ments. Then the words were aligned so that the
overall alignment score between the full sentences
was maximum. Once the words were aligned opti-
mally, we calculated the sentence similarity score as
the sum of the word alignment scores normalized by
the average length of the sentence pair.
</bodyText>
<subsectionHeader confidence="0.705118">
2.2.2 Optimal Chunk Alignment Method
</subsectionHeader>
<bodyText confidence="0.9996905">
We created chunks and aligned them to calculate
sentence similarity as in Stefanescu et al. (2014b)
and applied optimal alignment twice. First, we ap-
plied optimal alignment of words in two chunks to
measure the similarity of the chunks. As before,
word similarity threshold was set to 0.3. We then
</bodyText>
<equation confidence="0.9763235">
sim(a, b) = {
|A||B|,
</equation>
<page confidence="0.991843">
165
</page>
<bodyText confidence="0.999951363636364">
normalized chunk similarity by the number of to-
kens in the shorter chunk such that it assigned higher
scores to pairs of chunks such as physician and gen-
eral physician. Second, we applied optimal align-
ment at chunk level in order to calculate the sentence
level similarity. We used chunk-to-chunk similarity
threshold 0.4 to prevent noisy alignments. In this
case, however, the similarity score was normalized
by the average number of chunks in the given texts
pair. All threshold values were set empirically based
on the performance on the training set.
</bodyText>
<subsectionHeader confidence="0.746717">
2.2.3 Resultant Vector Based Method
</subsectionHeader>
<bodyText confidence="0.999986583333333">
In this approach, we combined vector based word
representations to obtain sentence level representa-
tions through vector algebra. We added the vectors
corresponding to content words in each sentence to
create a resultant vector for each sentence and the
cosine similarity was calculated between the resul-
tant vectors. We used word vector representations
from Wiki LSA, Mikolov and GloVe models.
For a missing word, we used vector representa-
tion of one of its synonyms obtained from the Word-
Net. To compute the synonym list, we considered all
senses of the missing word given its POS category.
</bodyText>
<subsectionHeader confidence="0.873444">
2.3 Features for Regression
</subsectionHeader>
<bodyText confidence="0.999946">
We summarize the features used for regression next.
</bodyText>
<listItem confidence="0.990754235294118">
1. Similarity scores using optimal alignment of
words where word-to-word similarity was cal-
culated using vector based methods using word
representations from Mikolov, GloVe, LSA
Wiki models and Combined-Word-Measure
which combines knowledge based methods and
corpus based methods.
2. Similarity score using optimal alignment of
chunks where word-to-word similarity scores
were calculated using Mikolov’s word repre-
sentations.
3. Similarity scores based on the resultant vec-
tor method using word representations from
Mikolov, GloVe, and LSA Wiki models.
4. Noun-Noun, Adjective-Adjective, Adverb-
Adverb, and Verb-Verb similarity scores
and similarity score for other words using
</listItem>
<table confidence="0.999438666666667">
Data set Count Release time
SMTnews 351 STS2012-Test
Headlines 1500 STS2013-Test
Deft-forum 423 STS2014-Test
Deft-news 299 STS2014-Test
Images 749 STS2014-Test
</table>
<tableCaption confidence="0.999901">
Table 1: Summary of training data
</tableCaption>
<bodyText confidence="0.891335">
optimal word alignment and Mikolov’s word
representations.
</bodyText>
<listItem confidence="0.96490215">
5. Multiplication of noun-noun similarity score
and verb-verb similarity score (scores calcu-
lated as described in 4).
6. Whether there was any antonym pair present.
7. |Ci1 − Ci2 |where Ci1 and Ci2 are the counts
Ci1 + Ci2
of i E {all tokens, adjectives, adverbs, nouns,
and verbs} for sentence 1 and 2 respectively.
8. Presence of adjectives and adverbs in first sen-
tence, and in the second sentence.
9. Unigram overlap with synonym check, bi-
gram overlap and BLEU score (Papineni et al.,
2002).
10. Presence of negation cue (e.g. no, not, never)
in either of sentences.
11. Whether one sentence was a question while the
other was not.
12. Total number of words in each sentence. Sim-
ilarly, the number of adjectives, nouns, verbs,
adverbs, and others, in each sentence.
</listItem>
<subsectionHeader confidence="0.986656">
2.4 Experiments and Results
</subsectionHeader>
<bodyText confidence="0.999844909090909">
Data: For training, we used data released in pre-
vious shared tasks (summarized in Table 1). We
selected datasets that included texts from different
genres. However, some others, such as Tweet-news
and MSRPar were not included. Tweet-news data
were quite different from most other texts. MSRPar,
being more biased towards overlapping text (Rus et
al., 2014), was also a concern.
The test set included data (sentence pairs) from
Answers-forums (375), Answers-students (750),
Belief (375), Headlines (750), and Images (750).
</bodyText>
<page confidence="0.994627">
166
</page>
<bodyText confidence="0.99907">
Preprocessing: We removed stop words, la-
beled each word with Part-of-Speech (POS) tag and
lemmatized them using Stanford CoreNLP Toolkit
(Manning et al., 2014). We did spelling corrections
in student answers and forum data using Jazzy tool
(Idzelis, 2005) with WordNet dictionary. Moreover,
in student answers data, we found that the symbol
A (such as in bulb A and node A) typed in lower-
case was incorrectly labeled as a determiner ’a’ by
the POS tagger. We applied a rule to correct it. If
the token after ’a’ is not an adjective, adverb, or
noun, or the token is the last token in the sentence,
we changed its type to noun (NN). We then created
chunks as described by Stefuanescu et al. (2014b).
Regression: We generated various features as de-
scribed in Section 2.3 and applied regression meth-
ods in three different settings. In the first run (R1),
all features were used in Support Vector Regression
(SVR) with Radial Basis Function kernel. The sec-
ond run (R2) was same as R1 except that the features
in R2 did not include the count features (i.e., features
in 12). In the third run (R3), we used features same
as R2 but applied linear regression instead.
For SVR, we used LibSVM library (Chang and
Lin, 2011) in Weka (Holmes et al., 1994) and for the
linear regression we used Weka’s implementation.
The 10-fold cross validation results (r) of three dif-
ferent runs with the training data were 0.7734 (R1),
0.7662 (R2), and 0.7654 (R3).
</bodyText>
<table confidence="0.999887857142857">
Data set Baseline R1 R2 R3
Ans-forums 0.445 0.526 0.694 0.677
Ans-students 0.664 0.725 0.744 0.735
Belief 0.651 0.631 0.751 0.722
Headlines 0.531 0.813 0.807 0.812
Images 0.603 0.858 0.864 0.857
Mean 0.587 0.743 0.784 0.776
</table>
<tableCaption confidence="0.999702">
Table 2: Results of our submitted runs on test data.
</tableCaption>
<bodyText confidence="0.999882777777778">
The results on the test set have been presented
in Table 2. Though R1 had the highest correlation
score in a 10-fold cross validation process using the
training data, the results of R2 and R3 on the test
data were consistently better than the results of R1.
It suggests that absolute count features used in R1
tend to overfit the model. The weighted mean cor-
relation of R2 was 0.784 - the best among our three
runs and ranked 10th among 74 runs submitted by 29
</bodyText>
<figureCaption confidence="0.995272">
Figure 1: A graph showing similarity scores predicted by
our system (R2) and corresponding human judgment in
test data (sorted by gold score).
</figureCaption>
<bodyText confidence="0.999831076923077">
participating teams. The correlation score was very
close to the results of other best performing systems.
Moreover, we observed from Figure 1 that our sys-
tem worked fairly well at all range of scores. The ac-
tual variation of scores at extreme (very low and very
high) points is not very high though the regression
line seems to be more skewed. However, the corre-
lation scores of answer-forum, answer-students, and
belief data were found to be lower than those of
headlines and images data. The reason might be
the texts in the former data being not well-written
as compared to the latter. Also, more contextual in-
formation is required to fully understand them.
</bodyText>
<sectionHeader confidence="0.999557" genericHeader="method">
3 Interpretable STS
</sectionHeader>
<bodyText confidence="0.999803055555556">
For each sentence pair, participating systems had
to identify the chunks in each sentence or use the
given gold chunks, align corresponding chunks and
assign a similarity/relatedness score and type of the
alignment for each alignment. The alignment types
were EQUI (semantically equivalent), OPPO (oppo-
site in meaning), SPE (one chunk is more specific
than other), SIM (similar meanings, but no EQUI,
OPPO, SPE), REL (related meanings, but no SIM,
EQUI, OPPO, SPE), ALIC (does not have any cor-
responding chunk in the other sentence because of
the 1:1 alignment restriction), and NOALI (has no
corresponding chunk in the other sentence). Fur-
ther details about the task including type of relations
and evaluation criteria can be found in Agirre et al.
(2015).
Our system uses gold chunks of a given sentence
pair and maps chunks of the first sentence to those
</bodyText>
<page confidence="0.995414">
167
</page>
<bodyText confidence="0.999975083333334">
from the second by assigning different relations and
scores based on a set of rules. The system performs
stop word marking, POS tagging, lemmatization,
and named-entity recognition in the preprocessing
steps. It also uses lookups for synonym, antonym
and hypernym relations.
For synonym lookup, we created a strict synonym
lookup file using WordNet. Similarly, an antonym
lookup file was created by building an antonym set
for a given word from its direct antonyms and their
synsets. We further constructed another lookup file
for strict hypernyms.
</bodyText>
<subsectionHeader confidence="0.99655">
3.1 Rules
</subsectionHeader>
<bodyText confidence="0.999843166666667">
In this section, we describe the rules used for chunk
alignments and scoring. The scores given by each
rule are highlighted.
Conditions: We define below a number of condi-
tions for a given chunk pair that might be checked
before applying a rule.
</bodyText>
<listItem confidence="0.961422444444445">
C1: One chunk has a conjunction and other does not
C2: A content word in a chunk has an antonym in
the other chunk
C3: A word in either chunk is a NUMERIC entity
C4: Both chunks have LOCATION entities
C5: Any of the chunks has a DATE/TIME entity
C6: Both chunks share at least one content word
other than noun
C7: Any of the chunks has a conjunction
</listItem>
<bodyText confidence="0.999995142857143">
Next, we define a set of rules for each relation
type. For aligning a chunk pair (A, B), these rules
are applied in order of precedence as NOALIC,
EQUI, OPPO, SPE, SIMI, REL, and ALIC. Once a
chunk is aligned, it would not be considered for fur-
ther alignments. Moreover, there is a precedence of
rules within each relation type e.g. EQ2 is applied
only if EQ1 fails and EQ3 is applied if both EQ1
and EQ2 fail and so on. If a chunk does not get any
relation after applying all the rules, a NOALIC rela-
tion is assigned. Note that we frequently use sim-
Mikolov(A, B) to refer to the similarity score be-
tween the chunks A and B using Mikolov word vec-
tors as described in Section 2.2.2.
</bodyText>
<subsectionHeader confidence="0.50814">
3.1.1 NOALIC Rules
</subsectionHeader>
<bodyText confidence="0.724304">
NO1: If a chunk to be mapped is a single token and
is a punctuation, assign NOALIC.
</bodyText>
<subsectionHeader confidence="0.504054">
3.1.2 EQUI Rules
</subsectionHeader>
<bodyText confidence="0.88536">
EQUI Rules EQ1 − EQ3 are applied uncondition-
ally. The rest rules (EQ4 − EQ5) are applied only
if none of conditions C1 - C5 are satisfied.
EQ1 - Both chunks have same tokens (5) - e.g. to
compete ⇔ To Compete
EQ2 - Both chunks have same content words (5) -
e.g. in Olympics ⇔ At Olympics
EQ3 - All content words match using synonym
lookup (5) - e.g. to permit ⇔ Allowed
EQ4 : All content words of a chunk match and un-
matched content word(s) of the other chunk are all
of proper noun type (5) - e.g. Boeing 787 Dream-
liner ⇔ on 787 Dreamliner
EQ5 : Both chunks have equal number of content
words and sim − Mikolov(A, B) &gt; 0.6 (5) - e.g.
in Indonesia boat sinking ⇔ in Indonesia boat cap-
size
</bodyText>
<subsectionHeader confidence="0.510594">
3.1.3 OPPO Rules
</subsectionHeader>
<bodyText confidence="0.9850586">
OPPO rules are applied only when none of C3 and
C7 are satisfied.
OP1: A content word in a chunk has an antonym
in the other chunk (4) - e.g. in southern Iraq ⇔ in
northern Iraq
</bodyText>
<subsectionHeader confidence="0.555447">
3.1.4 SPE Rules
</subsectionHeader>
<bodyText confidence="0.675214285714286">
SP1: If chunk A but B has a conjunction and A con-
tains all the content words of B then A is SPE of B
(4) - e.g. Angelina Jolie ⇔ Angelina Jolie and the
complex truth.
SP2: If chunk A contains all content words of chunk
B plus some extra content words that are not verbs,
A is a SPE of B or vice-versa. If chunk B has mul-
tiple SPEs, then the chunk with the maximum token
overlap with B is selected as the SPE of B. (4) - e.g.
Blade Runner Pistorius ⇔ Pistorius.
SP3: If chunks A and B contain only one noun each
say n1 and n2 and n1 is hypernym of n2, B is SPE
of A or vice versa (4) - e.g. by a shop ⇔ outside a
bookstore.
</bodyText>
<subsectionHeader confidence="0.914954">
3.1.5 SIMI Rules
</subsectionHeader>
<bodyText confidence="0.7728878">
SI1: Only the unmatched content word in each
chunk is a CD type(3)-e.g. 6.9 magnitude earth-
quake ⇔ 5.6 magnitude earthquake
SI2: Each chunk has a token of DATE/TIME type
(3)- e.g. on Friday ⇔ on Wednesday
</bodyText>
<page confidence="0.990072">
168
</page>
<table confidence="0.999826333333333">
Run A T S T+S
Headlines Baseline 0.844 0.555 0.755 0.555
R1 0.898 0.654 0.826 0.638
R2 0.897 0.655 0.826 0.640
R3 0.897 0.666 0.815 0.642
Images Baseline 0.838 0.432 0.721 0.432
R1 0.887 0.614 0.787 0.584
R2 0.880 0.585 0.781 0.561
R3 0.883 0.603 0.783 0.575
</table>
<tableCaption confidence="0.98135975">
Table 3: Fl scores for Images and Headlines. A, T and
S refer to Alignment, Type, and Score respectively. The
highlighted scores are the best results produced by our
system.
</tableCaption>
<listItem confidence="0.6999865">
5I3: Each chunk has a token of LOCATION type
(3) - e.g. Syria ⇔ Iraq
</listItem>
<bodyText confidence="0.969746625">
5I4: When both chunks share at least one noun then
assign 3 if sim-Mikolov(A, B) &gt;= 0.4 and 2 oth-
erwise. - e.g. Nato troops ⇔ NATO strike
5I5: This rule is applied only if C6 is not
satisfied. Scores are assigned as : (i) 4 if
sim-Mikolov(A, B) ∈ [0.7, 1.0] (ii) 3 if sim-
Mikolov(A, B) ∈ [0.65, 0.7) (iii) 2 if sim-
Mikolov(A, B) ∈ [0.60, 0.65)
</bodyText>
<subsectionHeader confidence="0.924548">
3.1.6 REL Rules
</subsectionHeader>
<bodyText confidence="0.9996865">
RE1: If both chunks share at least one content word
other than noun then assign REL relation. Scores are
assigned as follows: (i) 4 if sim-Mikolov(A, B) ∈
[0.5, 1.0] (ii) 3 if sim-Mikolov(A, B) ∈ [0.4, 0.5)
(iii) 2 otherwise. e.g. to Central African Republic
⇔ in Central African capital
</bodyText>
<subsectionHeader confidence="0.973911">
3.1.7 ALIC Rules
</subsectionHeader>
<bodyText confidence="0.9996714">
AL1: If a chunk in a sentence X (Cx) is not
aligned yet but has a chunk in another pair-sentence
Y (Cy) that is already aligned and has sim-
Mikolov(Cx, Cy) &gt;= 0.6, assign ALIC relation to
Cx with a score of (0).
</bodyText>
<subsectionHeader confidence="0.999084">
3.2 Experiments and Results
</subsectionHeader>
<bodyText confidence="0.984989228571429">
We applied above mentioned rules in the training
data set by varying thresholds for sim-Mikolov
scores and selected the thresholds that produced the
best results in the training data set. Since three runs
were allowed to submit, we defined them as follows:
Run1(R1) : We applied our full set of rules with
limited stop words (375 words). However EQ4 was
modified such that it would apply when unmatched
content words of the bigger chunk were of noun
rather than proper noun type.
Run2(R2) : Same as R1 but with extended stop
words (686 words).
Run3(R3) : Applied full set of rules with extended
stop words.
The results corresponding to our three runs and
that of the baseline are presented in Table 3. In
Headlines test data, our system outperformed the
rest competing submissions in all evaluation met-
rics (except when alignment type and score were
ignored). In Images test data, R1 was the best in
alignment and type metrics. Our submissions were
among the top performing submissions for score and
type+score metrics.
R3 performed better among all runs in case of
Headlines data in overall. This was chiefly due to
modified EQ4 rule which reduced the number of
incorrect EQUI alignments. We also observed that
performance of our system was least affected by
size of stopword list for Headlines data as both R1
and R2 recorded similar F1-measures for all evalua-
tion metrics. However, R1 performed relatively bet-
ter than R2 in Images data-particularly in correctly
aligning chunk relations. It could be that images
are described mostly using common words and thus
were filtered by R2 as stop words.
</bodyText>
<sectionHeader confidence="0.99747" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.998542166666667">
In this paper we described our submissions to the
Semantic Text Similarity Task in SemEval Shared
Task 2015. Our system for the English STS subtask
used regression models that combined a wide array
of features including semantic similarity scores ob-
tained with various methods. For the Interpretable
Similarity subtask, we employed a rule-based ap-
proach for aligning chunks in sentence pairs and as-
signing relations and scores for the alignments. Our
systems were among the top performing systems in
both subtasks. We intend to publish our systems at
http://semanticsimilarity.org.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998980666666667">
This research was partially sponsored by University
of Memphis and the Institute for Education Sciences
under award R305A100875 to Dr. Vasile Rus.
</bodyText>
<page confidence="0.998364">
169
</page>
<sectionHeader confidence="0.995667" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99746201904762">
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot
on semantic textual similarity. In Proceedings of the
First Joint Conference on Lexical and Computational
Semantics-Volume 1: Proceedings of the main confer-
ence and the shared task, and Volume 2: Proceedings
of the Sixth International Workshop on Semantic Eval-
uation, pages 385–393. Association for Computational
Linguistics.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. sem 2013 shared task:
Semantic textual similarity, including a pilot on typed-
similarity. In In* SEM 2013: The Second Joint Con-
ference on Lexical and Computational Semantics. As-
sociation for Computational Linguistics. Citeseer.
Eneko Agirre, Carmen Baneab, Claire Cardiec, Daniel
Cerd, Mona Diabe, Aitor Gonzalez-Agirrea, Weiwei
Guof, Rada Mihalceab, German Rigaua, and Janyce
Wiebeg. 2014. Semeval-2014 task 10: Multilingual
semantic textual similarity. SemEval 2014, page 81.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo,
Iigo Lopez-Gazpio, Montse Maritxalar, Rada Mihal-
cea, German Rigau, Larraitz Uria, and Janyce Wiebe.
2015. SemEval-2015 Task 2: Semantic Textual Simi-
larity, English, Spanish and Pilot on Interpretability. In
Proceedings of the 9th International Workshop on Se-
mantic Evaluation (SemEval 2015), Denver, CO, June.
Association for Computational Linguistics.
Ramiz M Aliguliyev. 2009. A new sentence similarity
measure and sentence based extractive technique for
automatic text summarization. Expert Systems with
Applications, 36(4):7764–7772.
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlaps as a measure of semantic relatedness.
In IJCAI, volume 3, pages 805–810.
Rajendra Banjade, Nabin Maharjan, Nobal B. Niraula,
Vasile Rus, and Dipesh Gautam. 2015. Lemon and
tea are not similar: Measuring word-to-word similarity
combining different methods. In Proceedings of the
16th International Conference on Intelligent Text Pro-
cessing and Computational Linguistics, pages 335–
346.
Chris Brockett. 2007. Aligning the rte 2006 corpus. Mi-
crosoft Research.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
a library for support vector machines. ACM Trans-
actions on Intelligent Systems and Technology (TIST),
2(3):27.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources. In
Proceedings of the 20th international conference on
Computational Linguistics, page 350. Association for
Computational Linguistics.
Christiane Fellbaum. 1998. WordNet. Wiley Online Li-
brary.
Graeme Hirst and David St-Onge. 1998. Lexical chains
as representations of context for the detection and cor-
rection of malapropisms. WordNet: An electronic lex-
ical database, 305:305–332.
Geoffrey Holmes, Andrew Donkin, and Ian H Witten.
1994. Weka: A machine learning workbench. In Intel-
ligent Information Systems, 1994. Proceedings of the
1994 Second Australian and New Zealand Conference
on, pages 357–361. IEEE.
Mindaugas Idzelis. 2005. Jazzy: The java open source
spell checker.
Jay J Jiang and David W Conrath. 1997. Semantic simi-
larity based on corpus statistics and lexical taxonomy.
arXiv preprint cmp-lg/9709008.
Harold W Kuhn. 1955. The hungarian method for the as-
signment problem. Naval research logistics quarterly,
2(1-2):83–97.
Thomas K Landauer, Danielle S McNamara, Simon Den-
nis, and Walter Kintsch. 2007. Handbook of latent
semantic analysis. Psychology Press.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and wordnet similarity for word
sense identification. WordNet: An electronic lexical
database, 49(2):265–283.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In ICML, volume 98, pages 296–304.
Christopher D Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J Bethard, and David McClosky.
2014. The stanford corenlp natural language process-
ing toolkit. In Proceedings of 52nd Annual Meeting of
the Association for Computational Linguistics: System
Demonstrations, pages 55–60.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representations
of words and phrases and their compositionality. In
Advances in Neural Information Processing Systems,
pages 3111–3119.
Nobal B. Niraula, Rajendra Banjade, Dan S¸tef˘anescu,
and Vasile Rus. 2013. Experiments with semantic
similarity measures based on lda and lsa. In Statisti-
cal Language and Speech Processing, pages 188–199.
Springer.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th annual meeting on association for computational
linguistics, pages 311–318. Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.969463">
170
</page>
<reference confidence="0.99958476744186">
Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word rep-
resentation. Proceedings of the Empiricial Methods in
Natural Language Processing (EMNLP 2014), 12.
Philip Resnik. 1995. Using information content to eval-
uate semantic similarity in a taxonomy. arXiv preprint
cmp-lg/9511007.
Vasile Rus and Mihai Lintean. 2012. A comparison
of greedy and optimal assessment of natural language
student input using word-to-word similarity metrics.
In Proceedings of the Seventh Workshop on Building
Educational Applications Using NLP, pages 157–162.
Association for Computational Linguistics.
Vasile Rus, Mihai Lintean, Cristian Moldovan, William
Baggett, Nobal Niraula, and Brent Morgan. 2012.
The similar corpus: A resource to foster the qualita-
tive understanding of semantic similarity of texts. In
Semantic Relations II: Enhancing Resources and Ap-
plications, The 8th Language Resources and Evalua-
tion Conference (LREC 2012), May, pages 23–25.
Vasile Rus, Mihai Lintean, Rajendra Banjade, Nobal Ni-
raula, and Dan Stefanescu. 2013. Semilar: The se-
mantic similarity toolkit. In Proceedings of the 51st
Annual Meeting of the Association for Computational
Linguistics. Association for Computational Linguis-
tics.
Vasile Rus, Rajendra Banjade, and Mihai Lintean. 2014.
On paraphrase identification corpora. In Proceed-
ing on the International Conference on Language Re-
sources and Evaluation (LREC 2014).
Dan Stefanescu, Rajendra Banjade, and Vasile Rus.
2014a. Latent semantic analysis models on wikipedia
and tasa.
Dan S¸tef˘anescu, Rajendra Banjade, and Vasile Rus.
2014b. A sentence similarity method based on chunk-
ing and information content. In Computational Lin-
guistics and Intelligent Text Processing, pages 442–
453. Springer.
Zhibiao Wu and Martha Palmer. 1994. Verbs semantics
and lexical selection. In Proceedings of the 32nd an-
nual meeting on Association for Computational Lin-
guistics, pages 133–138. Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.998202">
171
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.248842">
<title confidence="0.9720595">NeRoSim: A System for Measuring and Interpreting Semantic Textual Similarity</title>
<author confidence="0.984997">Nobal B Nabin Vasile Rus</author>
<author confidence="0.984997">Dan</author>
<affiliation confidence="0.7604035">Dipesh Department of Computer The University of Memphis,</affiliation>
<abstract confidence="0.999820736842105">We present in this paper our system developed for SemEval 2015 Shared Task 2 (2a - English Semantic Textual Similarity, STS, and 2c - Interpretable Similarity) and the results of the submitted runs. For the English STS subtask, we used regression models combining a wide array of features including semantic similarity scores obtained from various methods. One of our runs achieved weighted mean correlation score of 0.784 for sentence similarity subtask (i.e., English STS) and was ranked tenth among 74 runs submitted by 29 teams. For the interpretable similarity pilot task, we employed a rule-based approach blended with chunk alignment labeling and scoring based on semantic similarity features. Our system for interpretable text similarity was among the top three best performing systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Mona Diab</author>
<author>Daniel Cer</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>Semeval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,</booktitle>
<pages>385--393</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1483" citStr="Agirre et al., 2012" startWordPosition="215" endWordPosition="218">29 teams. For the interpretable similarity pilot task, we employed a rule-based approach blended with chunk alignment labeling and scoring based on semantic similarity features. Our system for interpretable text similarity was among the top three best performing systems. 1 Introduction Semantic Textual Similarity (STS) is the task of measuring the degree of semantic equivalence for a given pair of texts. The importance of semantic similarity in Natural Language Processing is highlighted by the diversity of datasets and shared task evaluation campaigns over the last decade (Dolan et al., 2004; Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Rus et al., 2014) and by many uses such as in text summarization (Aliguliyev, 2009) and student answer assessment (Rus and Lintean, 2012; Niraula et al., 2013). ∗* These authors contributed equally to this work ††Work done while at University of Memphis This year’s SemEval shared task on semantic textual similarity focused on English STS, Spanish STS, and Interpretable Similarity (Agirre et al., 2015). We participated in the English STS and Interpretable Similarity subtasks. We describe in this paper our systems participated in these two subtasks. Th</context>
</contexts>
<marker>Agirre, Diab, Cer, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Mona Diab, Daniel Cer, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot on semantic textual similarity. In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, pages 385–393. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor GonzalezAgirre</author>
<author>Weiwei Guo</author>
</authors>
<title>sem 2013 shared task: Semantic textual similarity, including a pilot on typedsimilarity.</title>
<date>2013</date>
<booktitle>In In* SEM 2013: The Second Joint Conference on Lexical and Computational Semantics. Association for Computational Linguistics. Citeseer.</booktitle>
<contexts>
<context position="1504" citStr="Agirre et al., 2013" startWordPosition="219" endWordPosition="222">erpretable similarity pilot task, we employed a rule-based approach blended with chunk alignment labeling and scoring based on semantic similarity features. Our system for interpretable text similarity was among the top three best performing systems. 1 Introduction Semantic Textual Similarity (STS) is the task of measuring the degree of semantic equivalence for a given pair of texts. The importance of semantic similarity in Natural Language Processing is highlighted by the diversity of datasets and shared task evaluation campaigns over the last decade (Dolan et al., 2004; Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Rus et al., 2014) and by many uses such as in text summarization (Aliguliyev, 2009) and student answer assessment (Rus and Lintean, 2012; Niraula et al., 2013). ∗* These authors contributed equally to this work ††Work done while at University of Memphis This year’s SemEval shared task on semantic textual similarity focused on English STS, Spanish STS, and Interpretable Similarity (Agirre et al., 2015). We participated in the English STS and Interpretable Similarity subtasks. We describe in this paper our systems participated in these two subtasks. The English STS subtask</context>
</contexts>
<marker>Agirre, Cer, Diab, GonzalezAgirre, Guo, 2013</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, Aitor GonzalezAgirre, and Weiwei Guo. 2013. sem 2013 shared task: Semantic textual similarity, including a pilot on typedsimilarity. In In* SEM 2013: The Second Joint Conference on Lexical and Computational Semantics. Association for Computational Linguistics. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Carmen Baneab</author>
<author>Claire Cardiec</author>
<author>Daniel Cerd</author>
</authors>
<title>Mona Diabe, Aitor Gonzalez-Agirrea, Weiwei Guof, Rada Mihalceab, German Rigaua, and Janyce Wiebeg.</title>
<date>2014</date>
<booktitle>Semeval-2014 task 10: Multilingual semantic textual similarity. SemEval 2014,</booktitle>
<pages>81</pages>
<contexts>
<context position="1525" citStr="Agirre et al., 2014" startWordPosition="223" endWordPosition="226"> pilot task, we employed a rule-based approach blended with chunk alignment labeling and scoring based on semantic similarity features. Our system for interpretable text similarity was among the top three best performing systems. 1 Introduction Semantic Textual Similarity (STS) is the task of measuring the degree of semantic equivalence for a given pair of texts. The importance of semantic similarity in Natural Language Processing is highlighted by the diversity of datasets and shared task evaluation campaigns over the last decade (Dolan et al., 2004; Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Rus et al., 2014) and by many uses such as in text summarization (Aliguliyev, 2009) and student answer assessment (Rus and Lintean, 2012; Niraula et al., 2013). ∗* These authors contributed equally to this work ††Work done while at University of Memphis This year’s SemEval shared task on semantic textual similarity focused on English STS, Spanish STS, and Interpretable Similarity (Agirre et al., 2015). We participated in the English STS and Interpretable Similarity subtasks. We describe in this paper our systems participated in these two subtasks. The English STS subtask was about assigning </context>
</contexts>
<marker>Agirre, Baneab, Cardiec, Cerd, 2014</marker>
<rawString>Eneko Agirre, Carmen Baneab, Claire Cardiec, Daniel Cerd, Mona Diabe, Aitor Gonzalez-Agirrea, Weiwei Guof, Rada Mihalceab, German Rigaua, and Janyce Wiebeg. 2014. Semeval-2014 task 10: Multilingual semantic textual similarity. SemEval 2014, page 81.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Carmen Banea</author>
<author>Claire Cardie</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
</authors>
<title>Aitor Gonzalez-Agirre, Weiwei Guo, Iigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe.</title>
<date>2015</date>
<booktitle>SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015),</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Denver, CO,</location>
<contexts>
<context position="1931" citStr="Agirre et al., 2015" startWordPosition="289" endWordPosition="292">n Natural Language Processing is highlighted by the diversity of datasets and shared task evaluation campaigns over the last decade (Dolan et al., 2004; Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Rus et al., 2014) and by many uses such as in text summarization (Aliguliyev, 2009) and student answer assessment (Rus and Lintean, 2012; Niraula et al., 2013). ∗* These authors contributed equally to this work ††Work done while at University of Memphis This year’s SemEval shared task on semantic textual similarity focused on English STS, Spanish STS, and Interpretable Similarity (Agirre et al., 2015). We participated in the English STS and Interpretable Similarity subtasks. We describe in this paper our systems participated in these two subtasks. The English STS subtask was about assigning a similarity score between 0 and 5 to pairs of sentences; a score of 0 meaning the sentences are unrelated and 5 indicating they are equivalent. Our three runs for this subtask combined a wide array of features including similarity scores calculated using knowledge based and corpus based methods in a regression model (cf. Section 2). One of our systems achieved mean correlation score of 0.784 with human</context>
<context position="14798" citStr="Agirre et al. (2015)" startWordPosition="2352" endWordPosition="2355">ing chunks and assign a similarity/relatedness score and type of the alignment for each alignment. The alignment types were EQUI (semantically equivalent), OPPO (opposite in meaning), SPE (one chunk is more specific than other), SIM (similar meanings, but no EQUI, OPPO, SPE), REL (related meanings, but no SIM, EQUI, OPPO, SPE), ALIC (does not have any corresponding chunk in the other sentence because of the 1:1 alignment restriction), and NOALI (has no corresponding chunk in the other sentence). Further details about the task including type of relations and evaluation criteria can be found in Agirre et al. (2015). Our system uses gold chunks of a given sentence pair and maps chunks of the first sentence to those 167 from the second by assigning different relations and scores based on a set of rules. The system performs stop word marking, POS tagging, lemmatization, and named-entity recognition in the preprocessing steps. It also uses lookups for synonym, antonym and hypernym relations. For synonym lookup, we created a strict synonym lookup file using WordNet. Similarly, an antonym lookup file was created by building an antonym set for a given word from its direct antonyms and their synsets. We further</context>
</contexts>
<marker>Agirre, Banea, Cardie, Cer, Diab, 2015</marker>
<rawString>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Iigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe. 2015. SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), Denver, CO, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramiz M Aliguliyev</author>
</authors>
<title>A new sentence similarity measure and sentence based extractive technique for automatic text summarization. Expert Systems with Applications,</title>
<date>2009</date>
<pages>36--4</pages>
<contexts>
<context position="1610" citStr="Aliguliyev, 2009" startWordPosition="240" endWordPosition="241">d scoring based on semantic similarity features. Our system for interpretable text similarity was among the top three best performing systems. 1 Introduction Semantic Textual Similarity (STS) is the task of measuring the degree of semantic equivalence for a given pair of texts. The importance of semantic similarity in Natural Language Processing is highlighted by the diversity of datasets and shared task evaluation campaigns over the last decade (Dolan et al., 2004; Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Rus et al., 2014) and by many uses such as in text summarization (Aliguliyev, 2009) and student answer assessment (Rus and Lintean, 2012; Niraula et al., 2013). ∗* These authors contributed equally to this work ††Work done while at University of Memphis This year’s SemEval shared task on semantic textual similarity focused on English STS, Spanish STS, and Interpretable Similarity (Agirre et al., 2015). We participated in the English STS and Interpretable Similarity subtasks. We describe in this paper our systems participated in these two subtasks. The English STS subtask was about assigning a similarity score between 0 and 5 to pairs of sentences; a score of 0 meaning the se</context>
</contexts>
<marker>Aliguliyev, 2009</marker>
<rawString>Ramiz M Aliguliyev. 2009. A new sentence similarity measure and sentence based extractive technique for automatic text summarization. Expert Systems with Applications, 36(4):7764–7772.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>Extended gloss overlaps as a measure of semantic relatedness.</title>
<date>2003</date>
<booktitle>In IJCAI,</booktitle>
<volume>3</volume>
<pages>805--810</pages>
<contexts>
<context position="4749" citStr="Banerjee and Pedersen, 2003" startWordPosition="735" endWordPosition="738">nce-to-sentence similarity scores using various features such as different sentence-to-sentence similarity scores, presence of negation cues, lexical overlap measures etc. The sentence-to-sentence similarity scores were calculated using word-to-word similarity methods and optimal word and chunk alignments. 2.1 Word-to-Word Similarity We used knowledge based, corpus based, and hybrid methods to compute word-to-word similarity. From the knowledge based category, we used WordNet (Fellbaum, 1998) based similarity methods from SEMILAR Toolkit (Rus et al., 2013) which include Lin (Lin, 1998), Lesk (Banerjee and Pedersen, 2003), Hso (Hirst and St-Onge, 1998), Jcn (Jiang and Conrath, 1997), Res (Resnik, 1995), Path, Lch (Leacock and Chodorow, 1998), and Wup (Wu and Palmer, 1994). In corpus based category, we developed Latent Semantic Analysis (LSA) (Landauer et al., 2007) models1 from the whole Wikipedia articles as described in Stefanescu et al. (2014a). We also used pre-trained Mikolov word representations (Mikolov et al., 2013)2 and GloVe word vectors (Pennington et al., 2014)3. In these cases, each word was represented as a vector encoding and the similarity between words were computed as cosine similarity betwee</context>
</contexts>
<marker>Banerjee, Pedersen, 2003</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2003. Extended gloss overlaps as a measure of semantic relatedness. In IJCAI, volume 3, pages 805–810.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajendra Banjade</author>
<author>Nabin Maharjan</author>
<author>Nobal B Niraula</author>
<author>Vasile Rus</author>
<author>Dipesh Gautam</author>
</authors>
<title>Lemon and tea are not similar: Measuring word-to-word similarity combining different methods.</title>
<date>2015</date>
<booktitle>In Proceedings of the 16th International Conference on Intelligent Text Processing and Computational Linguistics,</booktitle>
<pages>335--346</pages>
<contexts>
<context position="6143" citStr="Banjade et al., 2015" startWordPosition="942" endWordPosition="945">nticsimilarity.org 2Downloaded from http://code.google.com/p/word2vec/ 3Downloaded from http://nlp.stanford.edu/projects/glove/ similarity scores between two words a and b as: 1, if a and b are synonyms 0, if a and b are antonyms A.B otherwise where A and B are vector representations of words a and b respectively. In hybrid approach, we developed a new word-to-word similarity measure (hereafter referred as Combined-Word-Measure) by combining the WordNet-based similarity methods with corpus based methods (using Mikolov’s word embeddings and GloVe vectors) by applying Support Vector Regression (Banjade et al., 2015). 2.2 Sentence-to-Sentence Similarity We applied three different approaches to compute sentence-to-sentence similarity. 2.2.1 Optimal Word Alignment Method Our alignment step was based on the optimal assignment problem, a fundamental combinatorial optimization problem which consists of finding a maximum weight matching in a weighted bipartite graph. An algorithm, the Kuhn-Munkres method (Kuhn, 1955), can find solutions to the optimum assignment problem in polynomial time. In our case, we first computed the similarity of word pairs (all possible combinations) using all similarity methods descri</context>
</contexts>
<marker>Banjade, Maharjan, Niraula, Rus, Gautam, 2015</marker>
<rawString>Rajendra Banjade, Nabin Maharjan, Nobal B. Niraula, Vasile Rus, and Dipesh Gautam. 2015. Lemon and tea are not similar: Measuring word-to-word similarity combining different methods. In Proceedings of the 16th International Conference on Intelligent Text Processing and Computational Linguistics, pages 335– 346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Brockett</author>
</authors>
<title>Aligning the rte</title>
<date>2007</date>
<journal>Microsoft Research.</journal>
<contexts>
<context position="3038" citStr="Brockett (2007)" startWordPosition="479" endWordPosition="480">n a regression model (cf. Section 2). One of our systems achieved mean correlation score of 0.784 with human judgment on the test data. Although STS systems measure the degree of semantic equivalence in terms of a score which is useful in many tasks, they stop short of explaining why the texts are similar, related, or unrelated. They do not indicate what kind of semantic relations exist among the constituents (words or chunks) of the target texts. Finding explicit relations between constituents in the paired texts would enable a meaningful interpretation of the similarity scores. To this end, Brockett (2007) and Rus et al. (2012) produced datasets where corresponding words (or multiword expressions) were aligned and in the later case their semantic relations were explicitly labeled. Similarly, this year’s pilot subtask called Interpretable Similarity required systems to align the segments (chunks) either using the chunked texts given by the organizers or chunking the given texts and indicating the type of semantic relations (such as EQUI for 164 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 164–171, Denver, Colorado, June 4-5, 2015. c�2015 Association </context>
</contexts>
<marker>Brockett, 2007</marker>
<rawString>Chris Brockett. 2007. Aligning the rte 2006 corpus. Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Libsvm: a library for support vector machines.</title>
<date>2011</date>
<booktitle>ACM Transactions on Intelligent Systems and Technology (TIST),</booktitle>
<pages>2--3</pages>
<contexts>
<context position="12251" citStr="Chang and Lin, 2011" startWordPosition="1921" endWordPosition="1924">nce, we changed its type to noun (NN). We then created chunks as described by Stefuanescu et al. (2014b). Regression: We generated various features as described in Section 2.3 and applied regression methods in three different settings. In the first run (R1), all features were used in Support Vector Regression (SVR) with Radial Basis Function kernel. The second run (R2) was same as R1 except that the features in R2 did not include the count features (i.e., features in 12). In the third run (R3), we used features same as R2 but applied linear regression instead. For SVR, we used LibSVM library (Chang and Lin, 2011) in Weka (Holmes et al., 1994) and for the linear regression we used Weka’s implementation. The 10-fold cross validation results (r) of three different runs with the training data were 0.7734 (R1), 0.7662 (R2), and 0.7654 (R3). Data set Baseline R1 R2 R3 Ans-forums 0.445 0.526 0.694 0.677 Ans-students 0.664 0.725 0.744 0.735 Belief 0.651 0.631 0.751 0.722 Headlines 0.531 0.813 0.807 0.812 Images 0.603 0.858 0.864 0.857 Mean 0.587 0.743 0.784 0.776 Table 2: Results of our submitted runs on test data. The results on the test set have been presented in Table 2. Though R1 had the highest correlati</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm: a library for support vector machines. ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>350</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1462" citStr="Dolan et al., 2004" startWordPosition="211" endWordPosition="214">4 runs submitted by 29 teams. For the interpretable similarity pilot task, we employed a rule-based approach blended with chunk alignment labeling and scoring based on semantic similarity features. Our system for interpretable text similarity was among the top three best performing systems. 1 Introduction Semantic Textual Similarity (STS) is the task of measuring the degree of semantic equivalence for a given pair of texts. The importance of semantic similarity in Natural Language Processing is highlighted by the diversity of datasets and shared task evaluation campaigns over the last decade (Dolan et al., 2004; Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Rus et al., 2014) and by many uses such as in text summarization (Aliguliyev, 2009) and student answer assessment (Rus and Lintean, 2012; Niraula et al., 2013). ∗* These authors contributed equally to this work ††Work done while at University of Memphis This year’s SemEval shared task on semantic textual similarity focused on English STS, Spanish STS, and Interpretable Similarity (Agirre et al., 2015). We participated in the English STS and Interpretable Similarity subtasks. We describe in this paper our systems participated in t</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In Proceedings of the 20th international conference on Computational Linguistics, page 350. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<date>1998</date>
<publisher>WordNet. Wiley Online Library.</publisher>
<contexts>
<context position="4618" citStr="Fellbaum, 1998" startWordPosition="716" endWordPosition="717"> the top performing systems in this subtask. 2 System for English STS We used regression models to compute final sentence-to-sentence similarity scores using various features such as different sentence-to-sentence similarity scores, presence of negation cues, lexical overlap measures etc. The sentence-to-sentence similarity scores were calculated using word-to-word similarity methods and optimal word and chunk alignments. 2.1 Word-to-Word Similarity We used knowledge based, corpus based, and hybrid methods to compute word-to-word similarity. From the knowledge based category, we used WordNet (Fellbaum, 1998) based similarity methods from SEMILAR Toolkit (Rus et al., 2013) which include Lin (Lin, 1998), Lesk (Banerjee and Pedersen, 2003), Hso (Hirst and St-Onge, 1998), Jcn (Jiang and Conrath, 1997), Res (Resnik, 1995), Path, Lch (Leacock and Chodorow, 1998), and Wup (Wu and Palmer, 1994). In corpus based category, we developed Latent Semantic Analysis (LSA) (Landauer et al., 2007) models1 from the whole Wikipedia articles as described in Stefanescu et al. (2014a). We also used pre-trained Mikolov word representations (Mikolov et al., 2013)2 and GloVe word vectors (Pennington et al., 2014)3. In the</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet. Wiley Online Library.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
<author>David St-Onge</author>
</authors>
<title>Lexical chains as representations of context for the detection and correction of malapropisms. WordNet: An electronic lexical database,</title>
<date>1998</date>
<pages>305--305</pages>
<contexts>
<context position="4780" citStr="Hirst and St-Onge, 1998" startWordPosition="740" endWordPosition="743">sing various features such as different sentence-to-sentence similarity scores, presence of negation cues, lexical overlap measures etc. The sentence-to-sentence similarity scores were calculated using word-to-word similarity methods and optimal word and chunk alignments. 2.1 Word-to-Word Similarity We used knowledge based, corpus based, and hybrid methods to compute word-to-word similarity. From the knowledge based category, we used WordNet (Fellbaum, 1998) based similarity methods from SEMILAR Toolkit (Rus et al., 2013) which include Lin (Lin, 1998), Lesk (Banerjee and Pedersen, 2003), Hso (Hirst and St-Onge, 1998), Jcn (Jiang and Conrath, 1997), Res (Resnik, 1995), Path, Lch (Leacock and Chodorow, 1998), and Wup (Wu and Palmer, 1994). In corpus based category, we developed Latent Semantic Analysis (LSA) (Landauer et al., 2007) models1 from the whole Wikipedia articles as described in Stefanescu et al. (2014a). We also used pre-trained Mikolov word representations (Mikolov et al., 2013)2 and GloVe word vectors (Pennington et al., 2014)3. In these cases, each word was represented as a vector encoding and the similarity between words were computed as cosine similarity between corresponding vectors. We exp</context>
</contexts>
<marker>Hirst, St-Onge, 1998</marker>
<rawString>Graeme Hirst and David St-Onge. 1998. Lexical chains as representations of context for the detection and correction of malapropisms. WordNet: An electronic lexical database, 305:305–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Holmes</author>
<author>Andrew Donkin</author>
<author>Ian H Witten</author>
</authors>
<title>Weka: A machine learning workbench.</title>
<date>1994</date>
<booktitle>In Intelligent Information Systems,</booktitle>
<pages>357--361</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="12281" citStr="Holmes et al., 1994" startWordPosition="1927" endWordPosition="1930">un (NN). We then created chunks as described by Stefuanescu et al. (2014b). Regression: We generated various features as described in Section 2.3 and applied regression methods in three different settings. In the first run (R1), all features were used in Support Vector Regression (SVR) with Radial Basis Function kernel. The second run (R2) was same as R1 except that the features in R2 did not include the count features (i.e., features in 12). In the third run (R3), we used features same as R2 but applied linear regression instead. For SVR, we used LibSVM library (Chang and Lin, 2011) in Weka (Holmes et al., 1994) and for the linear regression we used Weka’s implementation. The 10-fold cross validation results (r) of three different runs with the training data were 0.7734 (R1), 0.7662 (R2), and 0.7654 (R3). Data set Baseline R1 R2 R3 Ans-forums 0.445 0.526 0.694 0.677 Ans-students 0.664 0.725 0.744 0.735 Belief 0.651 0.631 0.751 0.722 Headlines 0.531 0.813 0.807 0.812 Images 0.603 0.858 0.864 0.857 Mean 0.587 0.743 0.784 0.776 Table 2: Results of our submitted runs on test data. The results on the test set have been presented in Table 2. Though R1 had the highest correlation score in a 10-fold cross va</context>
</contexts>
<marker>Holmes, Donkin, Witten, 1994</marker>
<rawString>Geoffrey Holmes, Andrew Donkin, and Ian H Witten. 1994. Weka: A machine learning workbench. In Intelligent Information Systems, 1994. Proceedings of the 1994 Second Australian and New Zealand Conference on, pages 357–361. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mindaugas Idzelis</author>
</authors>
<title>Jazzy: The java open source spell checker.</title>
<date>2005</date>
<contexts>
<context position="11293" citStr="Idzelis, 2005" startWordPosition="1749" endWordPosition="1750">me others, such as Tweet-news and MSRPar were not included. Tweet-news data were quite different from most other texts. MSRPar, being more biased towards overlapping text (Rus et al., 2014), was also a concern. The test set included data (sentence pairs) from Answers-forums (375), Answers-students (750), Belief (375), Headlines (750), and Images (750). 166 Preprocessing: We removed stop words, labeled each word with Part-of-Speech (POS) tag and lemmatized them using Stanford CoreNLP Toolkit (Manning et al., 2014). We did spelling corrections in student answers and forum data using Jazzy tool (Idzelis, 2005) with WordNet dictionary. Moreover, in student answers data, we found that the symbol A (such as in bulb A and node A) typed in lowercase was incorrectly labeled as a determiner ’a’ by the POS tagger. We applied a rule to correct it. If the token after ’a’ is not an adjective, adverb, or noun, or the token is the last token in the sentence, we changed its type to noun (NN). We then created chunks as described by Stefuanescu et al. (2014b). Regression: We generated various features as described in Section 2.3 and applied regression methods in three different settings. In the first run (R1), all</context>
</contexts>
<marker>Idzelis, 2005</marker>
<rawString>Mindaugas Idzelis. 2005. Jazzy: The java open source spell checker.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay J Jiang</author>
<author>David W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy. arXiv preprint cmp-lg/9709008.</title>
<date>1997</date>
<contexts>
<context position="4811" citStr="Jiang and Conrath, 1997" startWordPosition="745" endWordPosition="748">ifferent sentence-to-sentence similarity scores, presence of negation cues, lexical overlap measures etc. The sentence-to-sentence similarity scores were calculated using word-to-word similarity methods and optimal word and chunk alignments. 2.1 Word-to-Word Similarity We used knowledge based, corpus based, and hybrid methods to compute word-to-word similarity. From the knowledge based category, we used WordNet (Fellbaum, 1998) based similarity methods from SEMILAR Toolkit (Rus et al., 2013) which include Lin (Lin, 1998), Lesk (Banerjee and Pedersen, 2003), Hso (Hirst and St-Onge, 1998), Jcn (Jiang and Conrath, 1997), Res (Resnik, 1995), Path, Lch (Leacock and Chodorow, 1998), and Wup (Wu and Palmer, 1994). In corpus based category, we developed Latent Semantic Analysis (LSA) (Landauer et al., 2007) models1 from the whole Wikipedia articles as described in Stefanescu et al. (2014a). We also used pre-trained Mikolov word representations (Mikolov et al., 2013)2 and GloVe word vectors (Pennington et al., 2014)3. In these cases, each word was represented as a vector encoding and the similarity between words were computed as cosine similarity between corresponding vectors. We exploited the lexical relations be</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay J Jiang and David W Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. arXiv preprint cmp-lg/9709008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold W Kuhn</author>
</authors>
<title>The hungarian method for the assignment problem. Naval research logistics quarterly,</title>
<date>1955</date>
<pages>2--1</pages>
<contexts>
<context position="6545" citStr="Kuhn, 1955" startWordPosition="999" endWordPosition="1000">mbined-Word-Measure) by combining the WordNet-based similarity methods with corpus based methods (using Mikolov’s word embeddings and GloVe vectors) by applying Support Vector Regression (Banjade et al., 2015). 2.2 Sentence-to-Sentence Similarity We applied three different approaches to compute sentence-to-sentence similarity. 2.2.1 Optimal Word Alignment Method Our alignment step was based on the optimal assignment problem, a fundamental combinatorial optimization problem which consists of finding a maximum weight matching in a weighted bipartite graph. An algorithm, the Kuhn-Munkres method (Kuhn, 1955), can find solutions to the optimum assignment problem in polynomial time. In our case, we first computed the similarity of word pairs (all possible combinations) using all similarity methods described in Section 2.1. The similarity score less than 0.3 (empirically set threshold), was reset to 0 in order to avoid noisy alignments. Then the words were aligned so that the overall alignment score between the full sentences was maximum. Once the words were aligned optimally, we calculated the sentence similarity score as the sum of the word alignment scores normalized by the average length of the </context>
</contexts>
<marker>Kuhn, 1955</marker>
<rawString>Harold W Kuhn. 1955. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):83–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Danielle S McNamara</author>
<author>Simon Dennis</author>
<author>Walter Kintsch</author>
</authors>
<title>Handbook of latent semantic analysis.</title>
<date>2007</date>
<publisher>Psychology Press.</publisher>
<contexts>
<context position="4997" citStr="Landauer et al., 2007" startWordPosition="774" endWordPosition="777">larity methods and optimal word and chunk alignments. 2.1 Word-to-Word Similarity We used knowledge based, corpus based, and hybrid methods to compute word-to-word similarity. From the knowledge based category, we used WordNet (Fellbaum, 1998) based similarity methods from SEMILAR Toolkit (Rus et al., 2013) which include Lin (Lin, 1998), Lesk (Banerjee and Pedersen, 2003), Hso (Hirst and St-Onge, 1998), Jcn (Jiang and Conrath, 1997), Res (Resnik, 1995), Path, Lch (Leacock and Chodorow, 1998), and Wup (Wu and Palmer, 1994). In corpus based category, we developed Latent Semantic Analysis (LSA) (Landauer et al., 2007) models1 from the whole Wikipedia articles as described in Stefanescu et al. (2014a). We also used pre-trained Mikolov word representations (Mikolov et al., 2013)2 and GloVe word vectors (Pennington et al., 2014)3. In these cases, each word was represented as a vector encoding and the similarity between words were computed as cosine similarity between corresponding vectors. We exploited the lexical relations between words, i.e. synonymy and antonymy, from WordNet 3.0. As such we computed 1Models available at http://semanticsimilarity.org 2Downloaded from http://code.google.com/p/word2vec/ 3Dow</context>
</contexts>
<marker>Landauer, McNamara, Dennis, Kintsch, 2007</marker>
<rawString>Thomas K Landauer, Danielle S McNamara, Simon Dennis, and Walter Kintsch. 2007. Handbook of latent semantic analysis. Psychology Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>Combining local context and wordnet similarity for word sense identification. WordNet: An electronic lexical database,</title>
<date>1998</date>
<pages>49--2</pages>
<contexts>
<context position="4871" citStr="Leacock and Chodorow, 1998" startWordPosition="754" endWordPosition="757"> of negation cues, lexical overlap measures etc. The sentence-to-sentence similarity scores were calculated using word-to-word similarity methods and optimal word and chunk alignments. 2.1 Word-to-Word Similarity We used knowledge based, corpus based, and hybrid methods to compute word-to-word similarity. From the knowledge based category, we used WordNet (Fellbaum, 1998) based similarity methods from SEMILAR Toolkit (Rus et al., 2013) which include Lin (Lin, 1998), Lesk (Banerjee and Pedersen, 2003), Hso (Hirst and St-Onge, 1998), Jcn (Jiang and Conrath, 1997), Res (Resnik, 1995), Path, Lch (Leacock and Chodorow, 1998), and Wup (Wu and Palmer, 1994). In corpus based category, we developed Latent Semantic Analysis (LSA) (Landauer et al., 2007) models1 from the whole Wikipedia articles as described in Stefanescu et al. (2014a). We also used pre-trained Mikolov word representations (Mikolov et al., 2013)2 and GloVe word vectors (Pennington et al., 2014)3. In these cases, each word was represented as a vector encoding and the similarity between words were computed as cosine similarity between corresponding vectors. We exploited the lexical relations between words, i.e. synonymy and antonymy, from WordNet 3.0. A</context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Claudia Leacock and Martin Chodorow. 1998. Combining local context and wordnet similarity for word sense identification. WordNet: An electronic lexical database, 49(2):265–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In ICML,</booktitle>
<volume>98</volume>
<pages>296--304</pages>
<contexts>
<context position="4713" citStr="Lin, 1998" startWordPosition="732" endWordPosition="733">ompute final sentence-to-sentence similarity scores using various features such as different sentence-to-sentence similarity scores, presence of negation cues, lexical overlap measures etc. The sentence-to-sentence similarity scores were calculated using word-to-word similarity methods and optimal word and chunk alignments. 2.1 Word-to-Word Similarity We used knowledge based, corpus based, and hybrid methods to compute word-to-word similarity. From the knowledge based category, we used WordNet (Fellbaum, 1998) based similarity methods from SEMILAR Toolkit (Rus et al., 2013) which include Lin (Lin, 1998), Lesk (Banerjee and Pedersen, 2003), Hso (Hirst and St-Onge, 1998), Jcn (Jiang and Conrath, 1997), Res (Resnik, 1995), Path, Lch (Leacock and Chodorow, 1998), and Wup (Wu and Palmer, 1994). In corpus based category, we developed Latent Semantic Analysis (LSA) (Landauer et al., 2007) models1 from the whole Wikipedia articles as described in Stefanescu et al. (2014a). We also used pre-trained Mikolov word representations (Mikolov et al., 2013)2 and GloVe word vectors (Pennington et al., 2014)3. In these cases, each word was represented as a vector encoding and the similarity between words were </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In ICML, volume 98, pages 296–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The stanford corenlp natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="11197" citStr="Manning et al., 2014" startWordPosition="1732" endWordPosition="1735">ks (summarized in Table 1). We selected datasets that included texts from different genres. However, some others, such as Tweet-news and MSRPar were not included. Tweet-news data were quite different from most other texts. MSRPar, being more biased towards overlapping text (Rus et al., 2014), was also a concern. The test set included data (sentence pairs) from Answers-forums (375), Answers-students (750), Belief (375), Headlines (750), and Images (750). 166 Preprocessing: We removed stop words, labeled each word with Part-of-Speech (POS) tag and lemmatized them using Stanford CoreNLP Toolkit (Manning et al., 2014). We did spelling corrections in student answers and forum data using Jazzy tool (Idzelis, 2005) with WordNet dictionary. Moreover, in student answers data, we found that the symbol A (such as in bulb A and node A) typed in lowercase was incorrectly labeled as a determiner ’a’ by the POS tagger. We applied a rule to correct it. If the token after ’a’ is not an adjective, adverb, or noun, or the token is the last token in the sentence, we changed its type to noun (NN). We then created chunks as described by Stefuanescu et al. (2014b). Regression: We generated various features as described in Se</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J Bethard, and David McClosky. 2014. The stanford corenlp natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="5159" citStr="Mikolov et al., 2013" startWordPosition="799" endWordPosition="802"> similarity. From the knowledge based category, we used WordNet (Fellbaum, 1998) based similarity methods from SEMILAR Toolkit (Rus et al., 2013) which include Lin (Lin, 1998), Lesk (Banerjee and Pedersen, 2003), Hso (Hirst and St-Onge, 1998), Jcn (Jiang and Conrath, 1997), Res (Resnik, 1995), Path, Lch (Leacock and Chodorow, 1998), and Wup (Wu and Palmer, 1994). In corpus based category, we developed Latent Semantic Analysis (LSA) (Landauer et al., 2007) models1 from the whole Wikipedia articles as described in Stefanescu et al. (2014a). We also used pre-trained Mikolov word representations (Mikolov et al., 2013)2 and GloVe word vectors (Pennington et al., 2014)3. In these cases, each word was represented as a vector encoding and the similarity between words were computed as cosine similarity between corresponding vectors. We exploited the lexical relations between words, i.e. synonymy and antonymy, from WordNet 3.0. As such we computed 1Models available at http://semanticsimilarity.org 2Downloaded from http://code.google.com/p/word2vec/ 3Downloaded from http://nlp.stanford.edu/projects/glove/ similarity scores between two words a and b as: 1, if a and b are synonyms 0, if a and b are antonyms A.B oth</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nobal B Niraula</author>
<author>Rajendra Banjade</author>
<author>Dan S¸tef˘anescu</author>
<author>Vasile Rus</author>
</authors>
<title>Experiments with semantic similarity measures based on lda and lsa.</title>
<date>2013</date>
<booktitle>In Statistical Language and Speech Processing,</booktitle>
<pages>188--199</pages>
<publisher>Springer.</publisher>
<marker>Niraula, Banjade, S¸tef˘anescu, Rus, 2013</marker>
<rawString>Nobal B. Niraula, Rajendra Banjade, Dan S¸tef˘anescu, and Vasile Rus. 2013. Experiments with semantic similarity measures based on lda and lsa. In Statistical Language and Speech Processing, pages 188–199. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th annual meeting on association for computational linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10208" citStr="Papineni et al., 2002" startWordPosition="1577" endWordPosition="1580">S2014-Test Images 749 STS2014-Test Table 1: Summary of training data optimal word alignment and Mikolov’s word representations. 5. Multiplication of noun-noun similarity score and verb-verb similarity score (scores calculated as described in 4). 6. Whether there was any antonym pair present. 7. |Ci1 − Ci2 |where Ci1 and Ci2 are the counts Ci1 + Ci2 of i E {all tokens, adjectives, adverbs, nouns, and verbs} for sentence 1 and 2 respectively. 8. Presence of adjectives and adverbs in first sentence, and in the second sentence. 9. Unigram overlap with synonym check, bigram overlap and BLEU score (Papineni et al., 2002). 10. Presence of negation cue (e.g. no, not, never) in either of sentences. 11. Whether one sentence was a question while the other was not. 12. Total number of words in each sentence. Similarly, the number of adjectives, nouns, verbs, adverbs, and others, in each sentence. 2.4 Experiments and Results Data: For training, we used data released in previous shared tasks (summarized in Table 1). We selected datasets that included texts from different genres. However, some others, such as Tweet-news and MSRPar were not included. Tweet-news data were quite different from most other texts. MSRPar, b</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),</booktitle>
<pages>12</pages>
<contexts>
<context position="5209" citStr="Pennington et al., 2014" startWordPosition="807" endWordPosition="810"> we used WordNet (Fellbaum, 1998) based similarity methods from SEMILAR Toolkit (Rus et al., 2013) which include Lin (Lin, 1998), Lesk (Banerjee and Pedersen, 2003), Hso (Hirst and St-Onge, 1998), Jcn (Jiang and Conrath, 1997), Res (Resnik, 1995), Path, Lch (Leacock and Chodorow, 1998), and Wup (Wu and Palmer, 1994). In corpus based category, we developed Latent Semantic Analysis (LSA) (Landauer et al., 2007) models1 from the whole Wikipedia articles as described in Stefanescu et al. (2014a). We also used pre-trained Mikolov word representations (Mikolov et al., 2013)2 and GloVe word vectors (Pennington et al., 2014)3. In these cases, each word was represented as a vector encoding and the similarity between words were computed as cosine similarity between corresponding vectors. We exploited the lexical relations between words, i.e. synonymy and antonymy, from WordNet 3.0. As such we computed 1Models available at http://semanticsimilarity.org 2Downloaded from http://code.google.com/p/word2vec/ 3Downloaded from http://nlp.stanford.edu/projects/glove/ similarity scores between two words a and b as: 1, if a and b are synonyms 0, if a and b are antonyms A.B otherwise where A and B are vector representations of</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy. arXiv preprint cmp-lg/9511007.</title>
<date>1995</date>
<contexts>
<context position="4831" citStr="Resnik, 1995" startWordPosition="750" endWordPosition="751">imilarity scores, presence of negation cues, lexical overlap measures etc. The sentence-to-sentence similarity scores were calculated using word-to-word similarity methods and optimal word and chunk alignments. 2.1 Word-to-Word Similarity We used knowledge based, corpus based, and hybrid methods to compute word-to-word similarity. From the knowledge based category, we used WordNet (Fellbaum, 1998) based similarity methods from SEMILAR Toolkit (Rus et al., 2013) which include Lin (Lin, 1998), Lesk (Banerjee and Pedersen, 2003), Hso (Hirst and St-Onge, 1998), Jcn (Jiang and Conrath, 1997), Res (Resnik, 1995), Path, Lch (Leacock and Chodorow, 1998), and Wup (Wu and Palmer, 1994). In corpus based category, we developed Latent Semantic Analysis (LSA) (Landauer et al., 2007) models1 from the whole Wikipedia articles as described in Stefanescu et al. (2014a). We also used pre-trained Mikolov word representations (Mikolov et al., 2013)2 and GloVe word vectors (Pennington et al., 2014)3. In these cases, each word was represented as a vector encoding and the similarity between words were computed as cosine similarity between corresponding vectors. We exploited the lexical relations between words, i.e. sy</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. arXiv preprint cmp-lg/9511007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasile Rus</author>
<author>Mihai Lintean</author>
</authors>
<title>A comparison of greedy and optimal assessment of natural language student input using word-to-word similarity metrics.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP,</booktitle>
<pages>157--162</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1663" citStr="Rus and Lintean, 2012" startWordPosition="246" endWordPosition="250">Our system for interpretable text similarity was among the top three best performing systems. 1 Introduction Semantic Textual Similarity (STS) is the task of measuring the degree of semantic equivalence for a given pair of texts. The importance of semantic similarity in Natural Language Processing is highlighted by the diversity of datasets and shared task evaluation campaigns over the last decade (Dolan et al., 2004; Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Rus et al., 2014) and by many uses such as in text summarization (Aliguliyev, 2009) and student answer assessment (Rus and Lintean, 2012; Niraula et al., 2013). ∗* These authors contributed equally to this work ††Work done while at University of Memphis This year’s SemEval shared task on semantic textual similarity focused on English STS, Spanish STS, and Interpretable Similarity (Agirre et al., 2015). We participated in the English STS and Interpretable Similarity subtasks. We describe in this paper our systems participated in these two subtasks. The English STS subtask was about assigning a similarity score between 0 and 5 to pairs of sentences; a score of 0 meaning the sentences are unrelated and 5 indicating they are equiv</context>
</contexts>
<marker>Rus, Lintean, 2012</marker>
<rawString>Vasile Rus and Mihai Lintean. 2012. A comparison of greedy and optimal assessment of natural language student input using word-to-word similarity metrics. In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, pages 157–162. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasile Rus</author>
<author>Mihai Lintean</author>
<author>Cristian Moldovan</author>
<author>William Baggett</author>
<author>Nobal Niraula</author>
<author>Brent Morgan</author>
</authors>
<title>The similar corpus: A resource to foster the qualitative understanding of semantic similarity of texts.</title>
<date>2012</date>
<booktitle>In Semantic Relations II: Enhancing Resources and Applications, The 8th Language Resources and Evaluation Conference (LREC</booktitle>
<pages>23--25</pages>
<contexts>
<context position="3060" citStr="Rus et al. (2012)" startWordPosition="482" endWordPosition="485"> (cf. Section 2). One of our systems achieved mean correlation score of 0.784 with human judgment on the test data. Although STS systems measure the degree of semantic equivalence in terms of a score which is useful in many tasks, they stop short of explaining why the texts are similar, related, or unrelated. They do not indicate what kind of semantic relations exist among the constituents (words or chunks) of the target texts. Finding explicit relations between constituents in the paired texts would enable a meaningful interpretation of the similarity scores. To this end, Brockett (2007) and Rus et al. (2012) produced datasets where corresponding words (or multiword expressions) were aligned and in the later case their semantic relations were explicitly labeled. Similarly, this year’s pilot subtask called Interpretable Similarity required systems to align the segments (chunks) either using the chunked texts given by the organizers or chunking the given texts and indicating the type of semantic relations (such as EQUI for 164 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 164–171, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Ling</context>
</contexts>
<marker>Rus, Lintean, Moldovan, Baggett, Niraula, Morgan, 2012</marker>
<rawString>Vasile Rus, Mihai Lintean, Cristian Moldovan, William Baggett, Nobal Niraula, and Brent Morgan. 2012. The similar corpus: A resource to foster the qualitative understanding of semantic similarity of texts. In Semantic Relations II: Enhancing Resources and Applications, The 8th Language Resources and Evaluation Conference (LREC 2012), May, pages 23–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasile Rus</author>
<author>Mihai Lintean</author>
<author>Rajendra Banjade</author>
<author>Nobal Niraula</author>
<author>Dan Stefanescu</author>
</authors>
<title>Semilar: The semantic similarity toolkit.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4683" citStr="Rus et al., 2013" startWordPosition="724" endWordPosition="727">sh STS We used regression models to compute final sentence-to-sentence similarity scores using various features such as different sentence-to-sentence similarity scores, presence of negation cues, lexical overlap measures etc. The sentence-to-sentence similarity scores were calculated using word-to-word similarity methods and optimal word and chunk alignments. 2.1 Word-to-Word Similarity We used knowledge based, corpus based, and hybrid methods to compute word-to-word similarity. From the knowledge based category, we used WordNet (Fellbaum, 1998) based similarity methods from SEMILAR Toolkit (Rus et al., 2013) which include Lin (Lin, 1998), Lesk (Banerjee and Pedersen, 2003), Hso (Hirst and St-Onge, 1998), Jcn (Jiang and Conrath, 1997), Res (Resnik, 1995), Path, Lch (Leacock and Chodorow, 1998), and Wup (Wu and Palmer, 1994). In corpus based category, we developed Latent Semantic Analysis (LSA) (Landauer et al., 2007) models1 from the whole Wikipedia articles as described in Stefanescu et al. (2014a). We also used pre-trained Mikolov word representations (Mikolov et al., 2013)2 and GloVe word vectors (Pennington et al., 2014)3. In these cases, each word was represented as a vector encoding and the </context>
</contexts>
<marker>Rus, Lintean, Banjade, Niraula, Stefanescu, 2013</marker>
<rawString>Vasile Rus, Mihai Lintean, Rajendra Banjade, Nobal Niraula, and Dan Stefanescu. 2013. Semilar: The semantic similarity toolkit. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasile Rus</author>
<author>Rajendra Banjade</author>
<author>Mihai Lintean</author>
</authors>
<title>On paraphrase identification corpora.</title>
<date>2014</date>
<booktitle>In Proceeding on the International Conference on Language Resources and Evaluation (LREC</booktitle>
<contexts>
<context position="1544" citStr="Rus et al., 2014" startWordPosition="227" endWordPosition="230">yed a rule-based approach blended with chunk alignment labeling and scoring based on semantic similarity features. Our system for interpretable text similarity was among the top three best performing systems. 1 Introduction Semantic Textual Similarity (STS) is the task of measuring the degree of semantic equivalence for a given pair of texts. The importance of semantic similarity in Natural Language Processing is highlighted by the diversity of datasets and shared task evaluation campaigns over the last decade (Dolan et al., 2004; Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Rus et al., 2014) and by many uses such as in text summarization (Aliguliyev, 2009) and student answer assessment (Rus and Lintean, 2012; Niraula et al., 2013). ∗* These authors contributed equally to this work ††Work done while at University of Memphis This year’s SemEval shared task on semantic textual similarity focused on English STS, Spanish STS, and Interpretable Similarity (Agirre et al., 2015). We participated in the English STS and Interpretable Similarity subtasks. We describe in this paper our systems participated in these two subtasks. The English STS subtask was about assigning a similarity score </context>
<context position="10868" citStr="Rus et al., 2014" startWordPosition="1684" endWordPosition="1687">t, never) in either of sentences. 11. Whether one sentence was a question while the other was not. 12. Total number of words in each sentence. Similarly, the number of adjectives, nouns, verbs, adverbs, and others, in each sentence. 2.4 Experiments and Results Data: For training, we used data released in previous shared tasks (summarized in Table 1). We selected datasets that included texts from different genres. However, some others, such as Tweet-news and MSRPar were not included. Tweet-news data were quite different from most other texts. MSRPar, being more biased towards overlapping text (Rus et al., 2014), was also a concern. The test set included data (sentence pairs) from Answers-forums (375), Answers-students (750), Belief (375), Headlines (750), and Images (750). 166 Preprocessing: We removed stop words, labeled each word with Part-of-Speech (POS) tag and lemmatized them using Stanford CoreNLP Toolkit (Manning et al., 2014). We did spelling corrections in student answers and forum data using Jazzy tool (Idzelis, 2005) with WordNet dictionary. Moreover, in student answers data, we found that the symbol A (such as in bulb A and node A) typed in lowercase was incorrectly labeled as a determin</context>
</contexts>
<marker>Rus, Banjade, Lintean, 2014</marker>
<rawString>Vasile Rus, Rajendra Banjade, and Mihai Lintean. 2014. On paraphrase identification corpora. In Proceeding on the International Conference on Language Resources and Evaluation (LREC 2014).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Dan Stefanescu</author>
</authors>
<title>Rajendra Banjade, and Vasile Rus. 2014a. Latent semantic analysis models on wikipedia and tasa.</title>
<marker>Stefanescu, </marker>
<rawString>Dan Stefanescu, Rajendra Banjade, and Vasile Rus. 2014a. Latent semantic analysis models on wikipedia and tasa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan S¸tef˘anescu</author>
<author>Rajendra Banjade</author>
<author>Vasile Rus</author>
</authors>
<title>A sentence similarity method based on chunking and information content.</title>
<date>2014</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>442--453</pages>
<publisher>Springer.</publisher>
<marker>S¸tef˘anescu, Banjade, Rus, 2014</marker>
<rawString>Dan S¸tef˘anescu, Rajendra Banjade, and Vasile Rus. 2014b. A sentence similarity method based on chunking and information content. In Computational Linguistics and Intelligent Text Processing, pages 442– 453. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verbs semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd annual meeting on Association for Computational Linguistics,</booktitle>
<pages>133--138</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4902" citStr="Wu and Palmer, 1994" startWordPosition="760" endWordPosition="763">sures etc. The sentence-to-sentence similarity scores were calculated using word-to-word similarity methods and optimal word and chunk alignments. 2.1 Word-to-Word Similarity We used knowledge based, corpus based, and hybrid methods to compute word-to-word similarity. From the knowledge based category, we used WordNet (Fellbaum, 1998) based similarity methods from SEMILAR Toolkit (Rus et al., 2013) which include Lin (Lin, 1998), Lesk (Banerjee and Pedersen, 2003), Hso (Hirst and St-Onge, 1998), Jcn (Jiang and Conrath, 1997), Res (Resnik, 1995), Path, Lch (Leacock and Chodorow, 1998), and Wup (Wu and Palmer, 1994). In corpus based category, we developed Latent Semantic Analysis (LSA) (Landauer et al., 2007) models1 from the whole Wikipedia articles as described in Stefanescu et al. (2014a). We also used pre-trained Mikolov word representations (Mikolov et al., 2013)2 and GloVe word vectors (Pennington et al., 2014)3. In these cases, each word was represented as a vector encoding and the similarity between words were computed as cosine similarity between corresponding vectors. We exploited the lexical relations between words, i.e. synonymy and antonymy, from WordNet 3.0. As such we computed 1Models avai</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Zhibiao Wu and Martha Palmer. 1994. Verbs semantics and lexical selection. In Proceedings of the 32nd annual meeting on Association for Computational Linguistics, pages 133–138. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>