<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000369">
<title confidence="0.971971">
Estimating Compact Yet Rich Tree Insertion Grammars
</title>
<author confidence="0.971853">
Elif Yamangil and Stuart M. Shieber
</author>
<affiliation confidence="0.981025">
Harvard University
</affiliation>
<address confidence="0.522415">
Cambridge, Massachusetts, USA
</address>
<email confidence="0.998573">
{elif,shieber}@seas.harvard.edu
</email>
<sectionHeader confidence="0.996662" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999902117647059">
We present a Bayesian nonparametric model
for estimating tree insertion grammars (TIG),
building upon recent work in Bayesian in-
ference of tree substitution grammars (TSG)
via Dirichlet processes. Under our general
variant of TIG, grammars are estimated via
the Metropolis-Hastings algorithm that uses
a context free grammar transformation as a
proposal, which allows for cubic-time string
parsing as well as tree-wide joint sampling of
derivations in the spirit of Cohn and Blun-
som (2010). We use the Penn treebank for
our experiments and find that our proposal
Bayesian TIG model not only has competitive
parsing performance but also finds compact
yet linguistically rich TIG representations of
the data.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99984616">
There is a deep tension in statistical modeling of
grammatical structure between providing good ex-
pressivity — to allow accurate modeling of the data
with sparse grammars — and low complexity —
making induction of the grammars and parsing of
novel sentences computationally practical. Recent
work that incorporated Dirichlet process (DP) non-
parametric models into TSGs has provided an effi-
cient solution to the problem of segmenting train-
ing data trees into elementary parse tree fragments
to form the grammar (Cohn et al., 2009; Cohn and
Blunsom, 2010; Post and Gildea, 2009). DP infer-
ence tackles this problem by exploring the space of
all possible segmentations of the data, in search for
fragments that are on the one hand large enough so
that they incorporate the useful dependencies, and
on the other small enough so that they recur and have
a chance to be useful in analyzing unseen data.
The elementary trees combined in a TSG are, in-
tuitively, primitives of the language, yet certain lin-
guistic phenomena (notably various forms of modifi-
cation) “split them up”, preventing their reuse, lead-
ing to less sparse grammars than might be ideal.
For instance, imagine modeling the following set of
structures:
</bodyText>
<listItem confidence="0.9996812">
• [NP the [NN [NN [NN president] of the university] who
resigned yesterday]]
• [NP the [NN former [NN [NN president] of the univer-
sity]]]
• [NP the [NN [NN president] who resigned yesterday]]
</listItem>
<bodyText confidence="0.99969552631579">
A natural recurring structure here would be the
structure “[NP the [NN president]]”, yet it occurs
not at all in the data.
TSGs are a special case of the more flexible gram-
mar formalism of tree adjoining grammar (TAG)
(Joshi et al., 1975). TAG augments TSG with an
adjunction operator and a set of auxiliary trees in
addition to the substitution operator and initial trees
of TSG, allowing for “splicing in” of syntactic frag-
ments within trees. In the example, by augmenting a
TSG with an operation of adjunction, a grammar that
hypothesizes auxiliary trees corresponding to ad-
joining “[NN former NN]”, “[NN NN of the uni-
versity]”, and “[NN NN who resigned yesterday]”
would be able to reuse the basic structure “[NP the
[NN president]]”.
Unfortunately, TAG’s expressivity comes at the
cost of greatly increased complexity. Parsing com-
plexity for unconstrained TAG scales as O(n6), im-
</bodyText>
<page confidence="0.974699">
110
</page>
<note confidence="0.9020505">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 110–114,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<figureCaption confidence="0.993738666666667">
Figure 1: Example TIG derivation of an NP constituent:
One left insertion (at NN) and two simultaneous right in-
sertions (at NP).
</figureCaption>
<bodyText confidence="0.99975175">
practical as compared to CFG and TSG’s O(n3). In
addition, the model selection problem for TAG is
significantly more complicated than for TSG since
one must reason about many more combinatorial op-
tions with two types of derivation operators.1 This
has led researchers to resort to heuristic grammar ex-
traction techniques (Chiang, 2000; Carreras et al.,
2008) or using a very small number of grammar cat-
egories (Hwa, 1998).
Hwa (1998) first proposed to use tree-insertion
grammars (TIG), a kind of expressive compromise
between TSG and TAG, as a substrate on which to
build grammatical inference. TIG constrains the ad-
junction operation so that spliced-in material falls
completely to the left or completely to the right of
the splice point. By restricting the form of possible
auxiliary trees to only left or right auxiliary trees in
this way, TIG remains within the realm of context-
free formalisms (with cubic complexity) while still
modeling rich linguistic phenomena (Schabes and
Waters, 1995). Figure 1 depicts some examples of
TIG derivations.
Sharing the same intuitions, Shindo et al. (2011)
have provided a previous attempt at combining TIG
and Bayesian nonparametric principles, albeit with
severe limitations. Their TIG variant (which we will
refer to as TIG0) is highly constrained in the follow-
ing ways.
</bodyText>
<footnote confidence="0.690822666666667">
1. The foot node in an auxiliary tree must be the immediate
child of the root node.
2. Only one adjunction can occur at a given node.
1This can be seen by the fact that tree-path languages under
TAG are context free, whereas they are regular for TSG. (Sch-
abes and Waters, 1995)
</footnote>
<figureCaption confidence="0.8443915">
Figure 2: TIG-to-TSG transform: (a) and (b) illus-
trate transformed TSG derivations for two different TIG
</figureCaption>
<bodyText confidence="0.756041666666667">
derivations of the same parse tree structure. The TIG
nodes where we illustrate the transformation are in bold.
(We suppress the rest of the transformational nodes.)
</bodyText>
<listItem confidence="0.981088142857143">
3. Even modeling multiple adjunction with root adjunction
is disallowed. There is thus no recursion possibility with
adjunction, no stacking of auxiliary trees.
4. As a consequence of the prior two constraints, no adjunc-
tion along the spines of auxiliary trees is allowed.
5. As a consequence of the first constraint, all nonterminals
along the spine of an auxiliary tree are identical.
</listItem>
<bodyText confidence="0.999043857142857">
In this paper we explore a Bayesian nonparamet-
ric model for estimating a far more expressive ver-
sion of TIG, and compare its performance against
TSG and the restricted TIG0 variant. Our more gen-
eral formulation avoids these limitations by support-
ing the following features and thus relaxing four of
the five restrictions of TIG0.
</bodyText>
<listItem confidence="0.999676142857143">
1. Auxiliary trees may have the foot node at depth greater
than one.2
2. Both left and right adjunctions may occur at the same
node.
3. Simultanous adjunction (that is, more than one left or
right adjunction per node) is allowed via root adjunction.
4. Adjunctions may occur along the spines of auxiliary trees.
</listItem>
<bodyText confidence="0.99991625">
The increased expressivity of our TIG variant is
motivated both linguistically and practically. From
a linguistic point of view: Deeper auxiliary trees can
help model large patterns of insertion and potential
correlations between lexical items that extend over
multiple levels of tree. Combining left and right
auxiliary trees can help model modifiers of the same
node from left and right (combination of adjectives
</bodyText>
<footnote confidence="0.8313735">
2Throughout the paper, we will refer to the depth of an aux-
iliary tree to indicate the length of its spine.
</footnote>
<figure confidence="0.998842867924528">
NPR
who
ε
who
of
NP
ε
of
NPR
NPR
NP
NP* SBAR
WHNP S
NPL NP
NP
NPR
DT NN
the
NNL NN NNR
ε
NNL
NPR
NNR
president
NN
JJ NN*
former ε
ε
NP
NP
NP* PP
IN NP
SBAR
WHNP S
NP
NP* PP
IN NP
NPL
ε
the president
of
NN
JJ NN*
former
DT NN
NP
NP* SBAR
WHNP S
who
NP
NP* PP
IN NP
NP
</figure>
<page confidence="0.98756">
111
</page>
<bodyText confidence="0.99983">
and relative clauses for instance). Simultaneous in-
sertion allows us to deal with multiple independent
modifiers for the same constituent (for example, a
series of adjectives). From a practical point of view,
we show that an induced TIG provides modeling
performance superior to TSG and comparable with
TIG0. However we show that the grammars we in-
duce are compact yet rich, in that they succinctly
represent complex linguistic structures.
</bodyText>
<sectionHeader confidence="0.994564" genericHeader="method">
2 Probabilistic Model
</sectionHeader>
<bodyText confidence="0.9995466">
In the basic nonparametric TSG model, there is an
independent DP for every grammar category (such
as c = NP), each of which uses a base distribution
P0 that generates an initial tree by making stepwise
decisions.
</bodyText>
<equation confidence="0.96400025">
Ginit
c ∼DP(αinit
c , P init
0 (·  |c))
</equation>
<bodyText confidence="0.998825857142857">
The canonical P0 uses a probabilistic CFG P that
is fixed a priori to sample CFG rules top-down and
Bernoulli variables for determining where substitu-
tions should occur (Cohn et al., 2009; Cohn and
Blunsom, 2010).
We extend this model by adding specialized DPs
for left and right auxiliary trees.3
</bodyText>
<equation confidence="0.9267585">
Gright c∼ DP(αright c, P right
0 (·  |c))
</equation>
<bodyText confidence="0.859388">
Therefore, we have an exchangeable process for
generating right auxiliary trees
right right
</bodyText>
<equation confidence="0.948711">
nab + αc P0 (aj  |c) p(aj  |a&lt;j) = (1)
j − 1 + αright
c
</equation>
<bodyText confidence="0.966100933333333">
as for initial trees in TSG.
We must define three distinct base distributions
for initial trees, left auxiliary trees, and right aux-
iliary trees. Pinit
0 generates an initial tree with root
label c by sampling CFG rules from P and making
a binary decision at every node generated whether
to leave it as a frontier node or further expand (with
probability βc) (Cohn et al., 2009). Similarly, our
P0
right generates a right auxiliary tree with root la-
bel c by first making a binary decision whether to
generate an immediate foot or not (with probability
γc), and then sampling an appropriate CFG rule
right
</bodyText>
<footnote confidence="0.723798">
3We use right insertions for illustration; the symmetric ana-
log applies to left insertions.
</footnote>
<table confidence="0.491895857142857">
(VP (, ,) (VP PP (VP (, ,) VP*)))
(VP (SBAR (WHADVP (WRB (WRB When))) S) (VP (, ,) VP*))
(VP (PP (IN For) (NP NN )) (VP (, ,) VP*))
(VP (CC But) (VP PP (VP (, ,) VP*)))
(VP ADVP (VP (, ,) VP*))
(IN (ADVP (RB (RB particularly))) IN*)
(NP PP (NP (CC and) (NP PP NP*)))
</table>
<figureCaption confidence="0.869293166666667">
Figure 3: Example left auxiliary trees that occur in the
top derivations for Section 23. Simultaneous insertions
occur most frequently for the labels VP (85 times), NNS
(21 times), NNP (14 times).
from P. For the right child, we sample an initial tree
from P init
</figureCaption>
<bodyText confidence="0.995365875">
0 . For the left child, if decision to gener-
ate an immediate foot was made, we generate a foot
node, and stop. Otherwise we recur into Pright
0 which
generates a right auxiliary tree that becomes the left
child.
We bring together these three sets of processes
via a set of insertion parameters µleft
c , µright
c . In any
derivation, for every initial tree node labelled c (ex-
cept for frontier nodes) we determine whether or
not there are insertions at this node by sampling a
Bernoulli(µleft
c ) distributed left insertion variable and
a Bernoulli(µright
c ) distributed right insertion vari-
able. For left auxiliary trees, we treat the nodes that
are not along the spine of the auxiliary tree the same
way we treat initial tree nodes, however for nodes
that are along the spine (including root nodes, ex-
cluding foot nodes) we consider only left insertions
by sampling the left insertion variable (symmetri-
cally for right insertions).
</bodyText>
<sectionHeader confidence="0.999805" genericHeader="method">
3 Inference
</sectionHeader>
<bodyText confidence="0.999914125">
Given this model, our inference task is to explore
optimal derivations underlying the data. Since TIG
derivations are highly structured objects, a basic
sampling strategy based on local node-level moves
such as Gibbs sampling (Geman and Geman, 1984)
would not hold much promise. Following previ-
ous work, we design a blocked Metropolis-Hastings
sampler that samples derivations per entire parse
trees all at once in a joint fashion (Cohn and Blun-
som, 2010; Shindo et al., 2011). This is achieved by
proposing derivations from an approximating distri-
bution and stochastically correcting via accept/reject
to achieve convergence into the correct posterior
(Johnson et al., 2007).
Since our base distributions factorize over levels
of tree, CFG is the most convenient choice for a
</bodyText>
<page confidence="0.989915">
112
</page>
<figure confidence="0.976110074074074">
CFG rule CFG probability
Base distribution: Pinit
0
NP → NPinit «init�/(Dinit
NP + «cit)
NPinit → NPL NPinit NPR 1.0
NPinit → DT NN P˜ (NP → DT NN) × (1 − ODT) × (1 − ONN)
NPinit → DT NNinit P˜ (NP → DT NN) × (1 − ODT) × ONN
NPinit → DTinit NN P˜ (NP → DT NN) × ODT × (1 − ONN)
NPinit → DTinit NNinit P˜ (NP → DT NN) × ODT × ONN
Base distribution: P right
0
NPR → NPright ANPh��(«cght/(DI Pht + «cght)1
NPR → e 1 −
NPright → NPright NPR 1.0
NPright → NP* SBARinit P˜ (NP → NP SBAR  |NP → NP )
×(1 − ryright
NP ) × (1 − OSBAR)
NPright → NP* SBAR P˜ (NP → NP SBAR  |NP → NP )
×(1 − ryright
NP ) × OSBAR
NPright → NPright SBARinit P˜ (NP → NP SBAR  |NP → NP )
×ryNP × (1 − OSBAR)
right
NPright → NPright SBAR P˜ (NP → NP SBAR  |NP → NP )
×ryNP × OSBAR
right
</figure>
<figureCaption confidence="0.9600245">
Figure 4: Transformation CFG rules that represent infi-
nite base distributions. P init
</figureCaption>
<tableCaption confidence="0.460461142857143">
0 is taken from Cohn and Blun-
som (2010). Underscored labels (such as NPright as op-
posed to NPright) are used to differentiate the pre-insertion
nodes in Figure 2 from the post-insertion ones. Pleft
0 rules
are omitted for brevity and mirror the P right
0 rules above.
</tableCaption>
<table confidence="0.5563105">
Model FMeasure # Initial Trees # Auxiliary Trees (# Left)
TSG 77.51 6.2K -
TIG0 78.46 6.0K 251 (137)
TIG 78.62 5.6K 604 (334)
</table>
<figureCaption confidence="0.999825">
Figure 5: EVALB results after training on Section 2 and
testing on Section 23. Note that TIG finds a compact yet
rich representation. Elementary tree counts are based on
ones with count &gt; 1.
</figureCaption>
<bodyText confidence="0.999759777777778">
proposal distribution. Fortunately, Schabes and Wa-
ters (1995) provide an (exact) transformation from a
fully general TIG into a TSG that generates the same
string languages. It is then straightforward to repre-
sent this TSG as a CFG using the Goodman trans-
form (Goodman, 2002; Cohn and Blunsom, 2010).
Figure 4 lists the additional CFG productions we
have designed, as well as the rules used that trigger
them.
</bodyText>
<sectionHeader confidence="0.979769" genericHeader="evaluation">
4 Evaluation Results
</sectionHeader>
<bodyText confidence="0.999995638888889">
We use the standard Penn treebank methodology of
training on sections 2–21 and testing on section 23.
All our data is head-binarized and words occurring
only once are mapped into unknown categories of
the Berkeley parser. As has become standard, we
carried out a small treebank experiment where we
train on Section 2, and a large one where we train
on the full training set. All hyperparameters are re-
sampled under appropriate vague gamma and beta
priors. All reported numbers are averages over three
runs. Parsing results are based on the maximum
probability parse which was obtained by sampling
derivations under the transform CFG.
We compare our system (referred to as TIG) to
our implementation of the TSG system of (Cohn
and Blunsom, 2010) (referred to as TSG) and the
constrained TIG variant of (Shindo et al., 2011) (re-
ferred to as TIGo). The upshot of our experiments is
that, while on the large training set all models have
similar performance (85.6, 85.3, 85.4 for TSG, TIGo
and TIG respectively), on the small dataset inser-
tion helps nonparametric model to find more com-
pact and generalizable representations for the data,
which affects parsing performance (Figure 4). Al-
though TIGo has performance close to TIG, note that
TIG achieves this performance using a more suc-
cinct representation and extracting a rich set of aux-
iliary trees. As a result, TIG finds many chances to
apply insertions to test sentences, whereas TIGo de-
pends mostly on TSG rules. If we look at the most
likely derivations for the test data, TIGo assigns 663
insertions (351 left insertions) in the parsing of en-
tire Section 23, meanwhile TIG assigns 3924 (2100
left insertions). Some of these linguistically sophis-
ticated auxiliary trees that apply to test data are listed
in Figure 3.
</bodyText>
<sectionHeader confidence="0.998925" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999972">
We described a nonparametric Bayesian inference
scheme for estimating TIG grammars and showed
the power of TIG formalism over TSG for returning
rich, generalizable, yet compact representations of
data. The nonparametric inference scheme presents
a principled way of addressing the difficult model
selection problem with TIG which has been pro-
hibitive in this area of research. TIG still remains
within context free and both our sampling and pars-
ing techniques are highly scalable.
</bodyText>
<sectionHeader confidence="0.996326" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9326985">
The first author was supported in part by a Google
PhD Fellowship in Natural Language Processing.
</bodyText>
<page confidence="0.998955">
113
</page>
<sectionHeader confidence="0.98324" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999870343283582">
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for
efficient, feature-rich parsing. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning, CoNLL ’08, pages 9–16, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Proceedings of the 38th Annual Meeting on Associa-
tion for Computational Linguistics, ACL ’00, pages
456–463, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Trevor Cohn and Phil Blunsom. 2010. Blocked in-
ference in Bayesian tree substitution grammars. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, ACLShort ’10, pages 225–230, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Trevor Cohn, Sharon Goldwater, and Phil Blunsom.
2009. Inducing compact but accurate tree-substitution
grammars. In NAACL ’09: Proceedings of Human
Language Technologies: The 2009 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 548–556, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
S. Geman and D. Geman. 1984. Stochastic Relaxation,
Gibbs Distributions and the Bayesian Restoration of
Images. pages 6:721–741.
J. Goodman. 2002. Efficient parsing of DOP with
PCFG-reductions. Bod et al. 2003.
Rebecca Hwa. 1998. An empirical evaluation of prob-
abilistic lexicalized tree insertion grammars. In Pro-
ceedings of the 17th international conference on Com-
putational linguistics - Volume 1, pages 557–563, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Mark Johnson, Thomas Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics;
Proceedings of the Main Conference, pages 139–146,
Rochester, New York, April. Association for Compu-
tational Linguistics.
Aravind K. Joshi, Leon S. Levy, and Masako Takahashi.
1975. Tree adjunct grammars. Journal of Computer
and System Sciences, 10(1):136–163.
Matt Post and Daniel Gildea. 2009. Bayesian learn-
ing of a tree substitution grammar. In Proceedings
of the ACL-IJCNLP 2009 Conference Short Papers,
pages 45–48, Suntec, Singapore, August. Association
for Computational Linguistics.
Remko Scha and Rens Bod. 2003. Efficient parsing of
DOP with PCFG-reductions, October.
Yves Schabes and Richard C. Waters. 1995. Tree in-
sertion grammar: a cubic-time parsable formalism that
lexicalizes context-free grammar without changing the
trees produced. Comput. Linguist., 21:479–513, De-
cember.
Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata.
2011. Insertion operator for Bayesian tree substitution
grammars. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies: short papers - Volume
2, HLT ’11, pages 206–211, Stroudsburg, PA, USA.
Association for Computational Linguistics.
</reference>
<page confidence="0.998656">
114
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.886059">
<title confidence="0.99956">Estimating Compact Yet Rich Tree Insertion Grammars</title>
<author confidence="0.972043">M Yamangil</author>
<affiliation confidence="0.931798">Harvard</affiliation>
<address confidence="0.974676">Cambridge, Massachusetts,</address>
<abstract confidence="0.999399111111111">We present a Bayesian nonparametric model for estimating tree insertion grammars (TIG), building upon recent work in Bayesian inference of tree substitution grammars (TSG) via Dirichlet processes. Under our general variant of TIG, grammars are estimated via the Metropolis-Hastings algorithm that uses a context free grammar transformation as a proposal, which allows for cubic-time string parsing as well as tree-wide joint sampling of derivations in the spirit of Cohn and Blunsom (2010). We use the Penn treebank for our experiments and find that our proposal Bayesian TIG model not only has competitive parsing performance but also finds compact yet linguistically rich TIG representations of the data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twelfth Conference on Computational Natural Language Learning, CoNLL ’08,</booktitle>
<pages>9--16</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3873" citStr="Carreras et al., 2008" startWordPosition="612" endWordPosition="615">or Computational Linguistics, pages 110–114, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Figure 1: Example TIG derivation of an NP constituent: One left insertion (at NN) and two simultaneous right insertions (at NP). practical as compared to CFG and TSG’s O(n3). In addition, the model selection problem for TAG is significantly more complicated than for TSG since one must reason about many more combinatorial options with two types of derivation operators.1 This has led researchers to resort to heuristic grammar extraction techniques (Chiang, 2000; Carreras et al., 2008) or using a very small number of grammar categories (Hwa, 1998). Hwa (1998) first proposed to use tree-insertion grammars (TIG), a kind of expressive compromise between TSG and TAG, as a substrate on which to build grammatical inference. TIG constrains the adjunction operation so that spliced-in material falls completely to the left or completely to the right of the splice point. By restricting the form of possible auxiliary trees to only left or right auxiliary trees in this way, TIG remains within the realm of contextfree formalisms (with cubic complexity) while still modeling rich linguisti</context>
</contexts>
<marker>Carreras, Collins, Koo, 2008</marker>
<rawString>Xavier Carreras, Michael Collins, and Terry Koo. 2008. TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing. In Proceedings of the Twelfth Conference on Computational Natural Language Learning, CoNLL ’08, pages 9–16, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Statistical parsing with an automatically-extracted tree adjoining grammar.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, ACL ’00,</booktitle>
<pages>456--463</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3849" citStr="Chiang, 2000" startWordPosition="610" endWordPosition="611"> Association for Computational Linguistics, pages 110–114, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Figure 1: Example TIG derivation of an NP constituent: One left insertion (at NN) and two simultaneous right insertions (at NP). practical as compared to CFG and TSG’s O(n3). In addition, the model selection problem for TAG is significantly more complicated than for TSG since one must reason about many more combinatorial options with two types of derivation operators.1 This has led researchers to resort to heuristic grammar extraction techniques (Chiang, 2000; Carreras et al., 2008) or using a very small number of grammar categories (Hwa, 1998). Hwa (1998) first proposed to use tree-insertion grammars (TIG), a kind of expressive compromise between TSG and TAG, as a substrate on which to build grammatical inference. TIG constrains the adjunction operation so that spliced-in material falls completely to the left or completely to the right of the splice point. By restricting the form of possible auxiliary trees to only left or right auxiliary trees in this way, TIG remains within the realm of contextfree formalisms (with cubic complexity) while still</context>
</contexts>
<marker>Chiang, 2000</marker>
<rawString>David Chiang. 2000. Statistical parsing with an automatically-extracted tree adjoining grammar. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, ACL ’00, pages 456–463, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Phil Blunsom</author>
</authors>
<title>Blocked inference in Bayesian tree substitution grammars.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers, ACLShort ’10,</booktitle>
<pages>225--230</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="667" citStr="Cohn and Blunsom (2010)" startWordPosition="89" endWordPosition="93">rammars Elif Yamangil and Stuart M. Shieber Harvard University Cambridge, Massachusetts, USA {elif,shieber}@seas.harvard.edu Abstract We present a Bayesian nonparametric model for estimating tree insertion grammars (TIG), building upon recent work in Bayesian inference of tree substitution grammars (TSG) via Dirichlet processes. Under our general variant of TIG, grammars are estimated via the Metropolis-Hastings algorithm that uses a context free grammar transformation as a proposal, which allows for cubic-time string parsing as well as tree-wide joint sampling of derivations in the spirit of Cohn and Blunsom (2010). We use the Penn treebank for our experiments and find that our proposal Bayesian TIG model not only has competitive parsing performance but also finds compact yet linguistically rich TIG representations of the data. 1 Introduction There is a deep tension in statistical modeling of grammatical structure between providing good expressivity — to allow accurate modeling of the data with sparse grammars — and low complexity — making induction of the grammars and parsing of novel sentences computationally practical. Recent work that incorporated Dirichlet process (DP) nonparametric models into TSG</context>
<context position="8130" citStr="Cohn and Blunsom, 2010" startWordPosition="1344" endWordPosition="1347">TIG0. However we show that the grammars we induce are compact yet rich, in that they succinctly represent complex linguistic structures. 2 Probabilistic Model In the basic nonparametric TSG model, there is an independent DP for every grammar category (such as c = NP), each of which uses a base distribution P0 that generates an initial tree by making stepwise decisions. Ginit c ∼DP(αinit c , P init 0 (· |c)) The canonical P0 uses a probabilistic CFG P that is fixed a priori to sample CFG rules top-down and Bernoulli variables for determining where substitutions should occur (Cohn et al., 2009; Cohn and Blunsom, 2010). We extend this model by adding specialized DPs for left and right auxiliary trees.3 Gright c∼ DP(αright c, P right 0 (· |c)) Therefore, we have an exchangeable process for generating right auxiliary trees right right nab + αc P0 (aj |c) p(aj |a&lt;j) = (1) j − 1 + αright c as for initial trees in TSG. We must define three distinct base distributions for initial trees, left auxiliary trees, and right auxiliary trees. Pinit 0 generates an initial tree with root label c by sampling CFG rules from P and making a binary decision at every node generated whether to leave it as a frontier node or furth</context>
<context position="11032" citStr="Cohn and Blunsom, 2010" startWordPosition="1858" endWordPosition="1862">(including root nodes, excluding foot nodes) we consider only left insertions by sampling the left insertion variable (symmetrically for right insertions). 3 Inference Given this model, our inference task is to explore optimal derivations underlying the data. Since TIG derivations are highly structured objects, a basic sampling strategy based on local node-level moves such as Gibbs sampling (Geman and Geman, 1984) would not hold much promise. Following previous work, we design a blocked Metropolis-Hastings sampler that samples derivations per entire parse trees all at once in a joint fashion (Cohn and Blunsom, 2010; Shindo et al., 2011). This is achieved by proposing derivations from an approximating distribution and stochastically correcting via accept/reject to achieve convergence into the correct posterior (Johnson et al., 2007). Since our base distributions factorize over levels of tree, CFG is the most convenient choice for a 112 CFG rule CFG probability Base distribution: Pinit 0 NP → NPinit «init�/(Dinit NP + «cit) NPinit → NPL NPinit NPR 1.0 NPinit → DT NN P˜ (NP → DT NN) × (1 − ODT) × (1 − ONN) NPinit → DT NNinit P˜ (NP → DT NN) × (1 − ODT) × ONN NPinit → DTinit NN P˜ (NP → DT NN) × ODT × (1 − </context>
<context position="13080" citStr="Cohn and Blunsom, 2010" startWordPosition="2257" endWordPosition="2260">above. Model FMeasure # Initial Trees # Auxiliary Trees (# Left) TSG 77.51 6.2K - TIG0 78.46 6.0K 251 (137) TIG 78.62 5.6K 604 (334) Figure 5: EVALB results after training on Section 2 and testing on Section 23. Note that TIG finds a compact yet rich representation. Elementary tree counts are based on ones with count &gt; 1. proposal distribution. Fortunately, Schabes and Waters (1995) provide an (exact) transformation from a fully general TIG into a TSG that generates the same string languages. It is then straightforward to represent this TSG as a CFG using the Goodman transform (Goodman, 2002; Cohn and Blunsom, 2010). Figure 4 lists the additional CFG productions we have designed, as well as the rules used that trigger them. 4 Evaluation Results We use the standard Penn treebank methodology of training on sections 2–21 and testing on section 23. All our data is head-binarized and words occurring only once are mapped into unknown categories of the Berkeley parser. As has become standard, we carried out a small treebank experiment where we train on Section 2, and a large one where we train on the full training set. All hyperparameters are resampled under appropriate vague gamma and beta priors. All reported</context>
</contexts>
<marker>Cohn, Blunsom, 2010</marker>
<rawString>Trevor Cohn and Phil Blunsom. 2010. Blocked inference in Bayesian tree substitution grammars. In Proceedings of the ACL 2010 Conference Short Papers, ACLShort ’10, pages 225–230, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Sharon Goldwater</author>
<author>Phil Blunsom</author>
</authors>
<title>Inducing compact but accurate tree-substitution grammars.</title>
<date>2009</date>
<booktitle>In NAACL ’09: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>548--556</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1428" citStr="Cohn et al., 2009" startWordPosition="210" endWordPosition="213">so finds compact yet linguistically rich TIG representations of the data. 1 Introduction There is a deep tension in statistical modeling of grammatical structure between providing good expressivity — to allow accurate modeling of the data with sparse grammars — and low complexity — making induction of the grammars and parsing of novel sentences computationally practical. Recent work that incorporated Dirichlet process (DP) nonparametric models into TSGs has provided an efficient solution to the problem of segmenting training data trees into elementary parse tree fragments to form the grammar (Cohn et al., 2009; Cohn and Blunsom, 2010; Post and Gildea, 2009). DP inference tackles this problem by exploring the space of all possible segmentations of the data, in search for fragments that are on the one hand large enough so that they incorporate the useful dependencies, and on the other small enough so that they recur and have a chance to be useful in analyzing unseen data. The elementary trees combined in a TSG are, intuitively, primitives of the language, yet certain linguistic phenomena (notably various forms of modification) “split them up”, preventing their reuse, leading to less sparse grammars t</context>
<context position="8105" citStr="Cohn et al., 2009" startWordPosition="1340" endWordPosition="1343">nd comparable with TIG0. However we show that the grammars we induce are compact yet rich, in that they succinctly represent complex linguistic structures. 2 Probabilistic Model In the basic nonparametric TSG model, there is an independent DP for every grammar category (such as c = NP), each of which uses a base distribution P0 that generates an initial tree by making stepwise decisions. Ginit c ∼DP(αinit c , P init 0 (· |c)) The canonical P0 uses a probabilistic CFG P that is fixed a priori to sample CFG rules top-down and Bernoulli variables for determining where substitutions should occur (Cohn et al., 2009; Cohn and Blunsom, 2010). We extend this model by adding specialized DPs for left and right auxiliary trees.3 Gright c∼ DP(αright c, P right 0 (· |c)) Therefore, we have an exchangeable process for generating right auxiliary trees right right nab + αc P0 (aj |c) p(aj |a&lt;j) = (1) j − 1 + αright c as for initial trees in TSG. We must define three distinct base distributions for initial trees, left auxiliary trees, and right auxiliary trees. Pinit 0 generates an initial tree with root label c by sampling CFG rules from P and making a binary decision at every node generated whether to leave it as</context>
</contexts>
<marker>Cohn, Goldwater, Blunsom, 2009</marker>
<rawString>Trevor Cohn, Sharon Goldwater, and Phil Blunsom. 2009. Inducing compact but accurate tree-substitution grammars. In NAACL ’09: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 548–556, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Geman</author>
<author>D Geman</author>
</authors>
<title>Stochastic Relaxation, Gibbs Distributions and the Bayesian Restoration of Images.</title>
<date>1984</date>
<pages>6--721</pages>
<contexts>
<context position="10827" citStr="Geman and Geman, 1984" startWordPosition="1825" endWordPosition="1828">ght insertion variable. For left auxiliary trees, we treat the nodes that are not along the spine of the auxiliary tree the same way we treat initial tree nodes, however for nodes that are along the spine (including root nodes, excluding foot nodes) we consider only left insertions by sampling the left insertion variable (symmetrically for right insertions). 3 Inference Given this model, our inference task is to explore optimal derivations underlying the data. Since TIG derivations are highly structured objects, a basic sampling strategy based on local node-level moves such as Gibbs sampling (Geman and Geman, 1984) would not hold much promise. Following previous work, we design a blocked Metropolis-Hastings sampler that samples derivations per entire parse trees all at once in a joint fashion (Cohn and Blunsom, 2010; Shindo et al., 2011). This is achieved by proposing derivations from an approximating distribution and stochastically correcting via accept/reject to achieve convergence into the correct posterior (Johnson et al., 2007). Since our base distributions factorize over levels of tree, CFG is the most convenient choice for a 112 CFG rule CFG probability Base distribution: Pinit 0 NP → NPinit «ini</context>
</contexts>
<marker>Geman, Geman, 1984</marker>
<rawString>S. Geman and D. Geman. 1984. Stochastic Relaxation, Gibbs Distributions and the Bayesian Restoration of Images. pages 6:721–741.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>Efficient parsing of DOP with PCFG-reductions.</title>
<date>2002</date>
<journal>Bod</journal>
<contexts>
<context position="13055" citStr="Goodman, 2002" startWordPosition="2255" endWordPosition="2256"> right 0 rules above. Model FMeasure # Initial Trees # Auxiliary Trees (# Left) TSG 77.51 6.2K - TIG0 78.46 6.0K 251 (137) TIG 78.62 5.6K 604 (334) Figure 5: EVALB results after training on Section 2 and testing on Section 23. Note that TIG finds a compact yet rich representation. Elementary tree counts are based on ones with count &gt; 1. proposal distribution. Fortunately, Schabes and Waters (1995) provide an (exact) transformation from a fully general TIG into a TSG that generates the same string languages. It is then straightforward to represent this TSG as a CFG using the Goodman transform (Goodman, 2002; Cohn and Blunsom, 2010). Figure 4 lists the additional CFG productions we have designed, as well as the rules used that trigger them. 4 Evaluation Results We use the standard Penn treebank methodology of training on sections 2–21 and testing on section 23. All our data is head-binarized and words occurring only once are mapped into unknown categories of the Berkeley parser. As has become standard, we carried out a small treebank experiment where we train on Section 2, and a large one where we train on the full training set. All hyperparameters are resampled under appropriate vague gamma and </context>
</contexts>
<marker>Goodman, 2002</marker>
<rawString>J. Goodman. 2002. Efficient parsing of DOP with PCFG-reductions. Bod et al. 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
</authors>
<title>An empirical evaluation of probabilistic lexicalized tree insertion grammars.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguistics -</booktitle>
<volume>1</volume>
<pages>557--563</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3936" citStr="Hwa, 1998" startWordPosition="626" endWordPosition="627">July 2012. c�2012 Association for Computational Linguistics Figure 1: Example TIG derivation of an NP constituent: One left insertion (at NN) and two simultaneous right insertions (at NP). practical as compared to CFG and TSG’s O(n3). In addition, the model selection problem for TAG is significantly more complicated than for TSG since one must reason about many more combinatorial options with two types of derivation operators.1 This has led researchers to resort to heuristic grammar extraction techniques (Chiang, 2000; Carreras et al., 2008) or using a very small number of grammar categories (Hwa, 1998). Hwa (1998) first proposed to use tree-insertion grammars (TIG), a kind of expressive compromise between TSG and TAG, as a substrate on which to build grammatical inference. TIG constrains the adjunction operation so that spliced-in material falls completely to the left or completely to the right of the splice point. By restricting the form of possible auxiliary trees to only left or right auxiliary trees in this way, TIG remains within the realm of contextfree formalisms (with cubic complexity) while still modeling rich linguistic phenomena (Schabes and Waters, 1995). Figure 1 depicts some e</context>
</contexts>
<marker>Hwa, 1998</marker>
<rawString>Rebecca Hwa. 1998. An empirical evaluation of probabilistic lexicalized tree insertion grammars. In Proceedings of the 17th international conference on Computational linguistics - Volume 1, pages 557–563, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Bayesian inference for PCFGs via Markov chain Monte Carlo.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>139--146</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="11253" citStr="Johnson et al., 2007" startWordPosition="1890" endWordPosition="1893">imal derivations underlying the data. Since TIG derivations are highly structured objects, a basic sampling strategy based on local node-level moves such as Gibbs sampling (Geman and Geman, 1984) would not hold much promise. Following previous work, we design a blocked Metropolis-Hastings sampler that samples derivations per entire parse trees all at once in a joint fashion (Cohn and Blunsom, 2010; Shindo et al., 2011). This is achieved by proposing derivations from an approximating distribution and stochastically correcting via accept/reject to achieve convergence into the correct posterior (Johnson et al., 2007). Since our base distributions factorize over levels of tree, CFG is the most convenient choice for a 112 CFG rule CFG probability Base distribution: Pinit 0 NP → NPinit «init�/(Dinit NP + «cit) NPinit → NPL NPinit NPR 1.0 NPinit → DT NN P˜ (NP → DT NN) × (1 − ODT) × (1 − ONN) NPinit → DT NNinit P˜ (NP → DT NN) × (1 − ODT) × ONN NPinit → DTinit NN P˜ (NP → DT NN) × ODT × (1 − ONN) NPinit → DTinit NNinit P˜ (NP → DT NN) × ODT × ONN Base distribution: P right 0 NPR → NPright ANPh��(«cght/(DI Pht + «cght)1 NPR → e 1 − NPright → NPright NPR 1.0 NPright → NP* SBARinit P˜ (NP → NP SBAR |NP → NP ) ×(</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas Griffiths, and Sharon Goldwater. 2007. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 139–146, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Leon S Levy</author>
<author>Masako Takahashi</author>
</authors>
<title>Tree adjunct grammars.</title>
<date>1975</date>
<journal>Journal of Computer and System Sciences,</journal>
<volume>10</volume>
<issue>1</issue>
<contexts>
<context position="2542" citStr="Joshi et al., 1975" startWordPosition="402" endWordPosition="405">various forms of modification) “split them up”, preventing their reuse, leading to less sparse grammars than might be ideal. For instance, imagine modeling the following set of structures: • [NP the [NN [NN [NN president] of the university] who resigned yesterday]] • [NP the [NN former [NN [NN president] of the university]]] • [NP the [NN [NN president] who resigned yesterday]] A natural recurring structure here would be the structure “[NP the [NN president]]”, yet it occurs not at all in the data. TSGs are a special case of the more flexible grammar formalism of tree adjoining grammar (TAG) (Joshi et al., 1975). TAG augments TSG with an adjunction operator and a set of auxiliary trees in addition to the substitution operator and initial trees of TSG, allowing for “splicing in” of syntactic fragments within trees. In the example, by augmenting a TSG with an operation of adjunction, a grammar that hypothesizes auxiliary trees corresponding to adjoining “[NN former NN]”, “[NN NN of the university]”, and “[NN NN who resigned yesterday]” would be able to reuse the basic structure “[NP the [NN president]]”. Unfortunately, TAG’s expressivity comes at the cost of greatly increased complexity. Parsing comple</context>
</contexts>
<marker>Joshi, Levy, Takahashi, 1975</marker>
<rawString>Aravind K. Joshi, Leon S. Levy, and Masako Takahashi. 1975. Tree adjunct grammars. Journal of Computer and System Sciences, 10(1):136–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
<author>Daniel Gildea</author>
</authors>
<title>Bayesian learning of a tree substitution grammar.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>45--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="1476" citStr="Post and Gildea, 2009" startWordPosition="218" endWordPosition="221">G representations of the data. 1 Introduction There is a deep tension in statistical modeling of grammatical structure between providing good expressivity — to allow accurate modeling of the data with sparse grammars — and low complexity — making induction of the grammars and parsing of novel sentences computationally practical. Recent work that incorporated Dirichlet process (DP) nonparametric models into TSGs has provided an efficient solution to the problem of segmenting training data trees into elementary parse tree fragments to form the grammar (Cohn et al., 2009; Cohn and Blunsom, 2010; Post and Gildea, 2009). DP inference tackles this problem by exploring the space of all possible segmentations of the data, in search for fragments that are on the one hand large enough so that they incorporate the useful dependencies, and on the other small enough so that they recur and have a chance to be useful in analyzing unseen data. The elementary trees combined in a TSG are, intuitively, primitives of the language, yet certain linguistic phenomena (notably various forms of modification) “split them up”, preventing their reuse, leading to less sparse grammars than might be ideal. For instance, imagine modeli</context>
</contexts>
<marker>Post, Gildea, 2009</marker>
<rawString>Matt Post and Daniel Gildea. 2009. Bayesian learning of a tree substitution grammar. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 45–48, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Remko Scha</author>
<author>Rens Bod</author>
</authors>
<title>Efficient parsing of DOP with PCFG-reductions,</title>
<date>2003</date>
<marker>Scha, Bod, 2003</marker>
<rawString>Remko Scha and Rens Bod. 2003. Efficient parsing of DOP with PCFG-reductions, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Richard C Waters</author>
</authors>
<title>Tree insertion grammar: a cubic-time parsable formalism that lexicalizes context-free grammar without changing the trees produced.</title>
<date>1995</date>
<journal>Comput. Linguist.,</journal>
<pages>21--479</pages>
<contexts>
<context position="4511" citStr="Schabes and Waters, 1995" startWordPosition="715" endWordPosition="718">ery small number of grammar categories (Hwa, 1998). Hwa (1998) first proposed to use tree-insertion grammars (TIG), a kind of expressive compromise between TSG and TAG, as a substrate on which to build grammatical inference. TIG constrains the adjunction operation so that spliced-in material falls completely to the left or completely to the right of the splice point. By restricting the form of possible auxiliary trees to only left or right auxiliary trees in this way, TIG remains within the realm of contextfree formalisms (with cubic complexity) while still modeling rich linguistic phenomena (Schabes and Waters, 1995). Figure 1 depicts some examples of TIG derivations. Sharing the same intuitions, Shindo et al. (2011) have provided a previous attempt at combining TIG and Bayesian nonparametric principles, albeit with severe limitations. Their TIG variant (which we will refer to as TIG0) is highly constrained in the following ways. 1. The foot node in an auxiliary tree must be the immediate child of the root node. 2. Only one adjunction can occur at a given node. 1This can be seen by the fact that tree-path languages under TAG are context free, whereas they are regular for TSG. (Schabes and Waters, 1995) Fi</context>
<context position="12842" citStr="Schabes and Waters (1995)" startWordPosition="2215" endWordPosition="2219">and Blunsom (2010). Underscored labels (such as NPright as opposed to NPright) are used to differentiate the pre-insertion nodes in Figure 2 from the post-insertion ones. Pleft 0 rules are omitted for brevity and mirror the P right 0 rules above. Model FMeasure # Initial Trees # Auxiliary Trees (# Left) TSG 77.51 6.2K - TIG0 78.46 6.0K 251 (137) TIG 78.62 5.6K 604 (334) Figure 5: EVALB results after training on Section 2 and testing on Section 23. Note that TIG finds a compact yet rich representation. Elementary tree counts are based on ones with count &gt; 1. proposal distribution. Fortunately, Schabes and Waters (1995) provide an (exact) transformation from a fully general TIG into a TSG that generates the same string languages. It is then straightforward to represent this TSG as a CFG using the Goodman transform (Goodman, 2002; Cohn and Blunsom, 2010). Figure 4 lists the additional CFG productions we have designed, as well as the rules used that trigger them. 4 Evaluation Results We use the standard Penn treebank methodology of training on sections 2–21 and testing on section 23. All our data is head-binarized and words occurring only once are mapped into unknown categories of the Berkeley parser. As has b</context>
</contexts>
<marker>Schabes, Waters, 1995</marker>
<rawString>Yves Schabes and Richard C. Waters. 1995. Tree insertion grammar: a cubic-time parsable formalism that lexicalizes context-free grammar without changing the trees produced. Comput. Linguist., 21:479–513, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyuki Shindo</author>
<author>Akinori Fujino</author>
<author>Masaaki Nagata</author>
</authors>
<title>Insertion operator for Bayesian tree substitution grammars.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11,</booktitle>
<pages>206--211</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4613" citStr="Shindo et al. (2011)" startWordPosition="731" endWordPosition="734">s (TIG), a kind of expressive compromise between TSG and TAG, as a substrate on which to build grammatical inference. TIG constrains the adjunction operation so that spliced-in material falls completely to the left or completely to the right of the splice point. By restricting the form of possible auxiliary trees to only left or right auxiliary trees in this way, TIG remains within the realm of contextfree formalisms (with cubic complexity) while still modeling rich linguistic phenomena (Schabes and Waters, 1995). Figure 1 depicts some examples of TIG derivations. Sharing the same intuitions, Shindo et al. (2011) have provided a previous attempt at combining TIG and Bayesian nonparametric principles, albeit with severe limitations. Their TIG variant (which we will refer to as TIG0) is highly constrained in the following ways. 1. The foot node in an auxiliary tree must be the immediate child of the root node. 2. Only one adjunction can occur at a given node. 1This can be seen by the fact that tree-path languages under TAG are context free, whereas they are regular for TSG. (Schabes and Waters, 1995) Figure 2: TIG-to-TSG transform: (a) and (b) illustrate transformed TSG derivations for two different TIG</context>
<context position="11054" citStr="Shindo et al., 2011" startWordPosition="1863" endWordPosition="1866">xcluding foot nodes) we consider only left insertions by sampling the left insertion variable (symmetrically for right insertions). 3 Inference Given this model, our inference task is to explore optimal derivations underlying the data. Since TIG derivations are highly structured objects, a basic sampling strategy based on local node-level moves such as Gibbs sampling (Geman and Geman, 1984) would not hold much promise. Following previous work, we design a blocked Metropolis-Hastings sampler that samples derivations per entire parse trees all at once in a joint fashion (Cohn and Blunsom, 2010; Shindo et al., 2011). This is achieved by proposing derivations from an approximating distribution and stochastically correcting via accept/reject to achieve convergence into the correct posterior (Johnson et al., 2007). Since our base distributions factorize over levels of tree, CFG is the most convenient choice for a 112 CFG rule CFG probability Base distribution: Pinit 0 NP → NPinit «init�/(Dinit NP + «cit) NPinit → NPL NPinit NPR 1.0 NPinit → DT NN P˜ (NP → DT NN) × (1 − ODT) × (1 − ONN) NPinit → DT NNinit P˜ (NP → DT NN) × (1 − ODT) × ONN NPinit → DTinit NN P˜ (NP → DT NN) × ODT × (1 − ONN) NPinit → DTinit N</context>
<context position="14034" citStr="Shindo et al., 2011" startWordPosition="2417" endWordPosition="2420">he Berkeley parser. As has become standard, we carried out a small treebank experiment where we train on Section 2, and a large one where we train on the full training set. All hyperparameters are resampled under appropriate vague gamma and beta priors. All reported numbers are averages over three runs. Parsing results are based on the maximum probability parse which was obtained by sampling derivations under the transform CFG. We compare our system (referred to as TIG) to our implementation of the TSG system of (Cohn and Blunsom, 2010) (referred to as TSG) and the constrained TIG variant of (Shindo et al., 2011) (referred to as TIGo). The upshot of our experiments is that, while on the large training set all models have similar performance (85.6, 85.3, 85.4 for TSG, TIGo and TIG respectively), on the small dataset insertion helps nonparametric model to find more compact and generalizable representations for the data, which affects parsing performance (Figure 4). Although TIGo has performance close to TIG, note that TIG achieves this performance using a more succinct representation and extracting a rich set of auxiliary trees. As a result, TIG finds many chances to apply insertions to test sentences, </context>
</contexts>
<marker>Shindo, Fujino, Nagata, 2011</marker>
<rawString>Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata. 2011. Insertion operator for Bayesian tree substitution grammars. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11, pages 206–211, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>