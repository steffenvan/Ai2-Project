<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001199">
<title confidence="0.968856">
Personalized Recommendation of User Comments via Factor Models
</title>
<author confidence="0.84078">
Deepak Agarwal Bee-Chung Chen Bo Pang
</author>
<affiliation confidence="0.703735">
Yahoo! Research
</affiliation>
<address confidence="0.834313">
701 First Ave
Sunnyvale, CA 94089
</address>
<email confidence="0.967068">
{dagarwal,beechun,bopang}@yahoo-inc.com
</email>
<sectionHeader confidence="0.996164" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999857782608696">
In recent years, the amount of user-generated
opinionated texts (e.g., reviews, user com-
ments) continues to grow at a rapid speed: fea-
tured news stories on a major event easily at-
tract thousands of user comments on a popular
online News service. How to consume subjec-
tive information of this volume becomes an in-
teresting and important research question. In
contrast to previous work on review analysis
that tried to filter or summarize information
for a generic average user, we explore a dif-
ferent direction of enabling personalized rec-
ommendation of such information.
For each user, our task is to rank the comments
associated with a given article according to
personalized user preference (i.e., whether the
user is likely to like or dislike the comment).
To this end, we propose a factor model that
incorporates rater-comment and rater-author
interactions simultaneously in a principled
way. Our full model significantly outperforms
strong baselines as well as related models that
have been considered in previous work.
</bodyText>
<sectionHeader confidence="0.99891" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997923162790697">
Recent years have seen rapid growth in user-
generated opinions online. Many of them are user
reviews: a best-seller or a popular restaurant can
get over 1000 reviews on top review sites like Ama-
zon or Yelp. A large quantity of them also come in
the form of user comments on blogs or news arti-
cles. Most notably, during the short period of time
for which a major event is active, news stories on
one single event can easily attract over ten thousand
comments on a popular online news site like Yahoo!
News. One question becomes immediate: how can
we help people consume such gigantic amount of
opinionated information?
One possibility is to take the summarization route.
Briefly speaking (see Section 2 for a more detailed
discussion), previous work has largely formulated
review summarization as automatically or manually
identify ratable aspects, and present overall senti-
ment polarity for each aspect (Hu and Liu, 2004;
Popescu and Etzioni, 2005; Snyder and Barzilay,
2007; Titov and McDonald, 2008). A related line
of research looked into predicting helpfulness of re-
views in the hope of promoting those with better
quality, where helpfulness is usually defined as some
function over the percentage of users who found the
review to be helpful (Kim et al., 2006; Liu et al.,
2007; Danescu-Niculescu-Mizil et al., 2009). In
short, the focus of previous work has been on dis-
tilling subjective information for an average user.
Whether opinion consumers are looking for qual-
ity information or just wondering what other people
think, each may have different purposes or prefer-
ences that is not well represented by a generic av-
erage user. If we think about how we deal with in-
formation content overflow on the Web, there have
been two main frameworks to identify relevant infor-
mation for each person. One is search. Indeed many
top review sites allow users to search within reviews
for a given entity. But this is only useful when users
have explicit information needs that can be formu-
lated as queries. The other paradigm is recommen-
dation: based on what users have liked or disliked in
the past, the system will automatically recommend
</bodyText>
<page confidence="0.967683">
571
</page>
<note confidence="0.9583335">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 571–582,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999188727272728">
new items.
Can we provide similar recommendation mech-
anisms to help users consume large quantities of
subjective information? Many commenting environ-
ments allow users to mark “like” or “dislike” over
existing comments (e.g., Yahoo! News comments,
Facebook posts, or review sites that allow helpful-
ness votes). Can we learn from users’ past prefer-
ences, so that when a user is reading a new article,
we have a system that automatically ranks its com-
ments according to their likelihood of being liked by
the user? This can be used directly to create person-
alized presentation of comments (e.g., into a “like”
column and a “dislike” column), as well as enabling
down-stream applications such as personalized sum-
marization.
Recommending textual information has recently
attracted more attention. So far, the focus has been
mainly on recommending news articles (Ahn et al.,
2007; Das et al., 2007). Our task differs in several
aspects. Intuitively, recommending news articles is
largely about identifying the topics of interest to a
given user, and it is conceivable that unigram repre-
sentation of full-length articles can reasonably cap-
ture that information. In our case, most comments
for an article a user is reading are already of interest
to that user topically. Which ones the user ends up
liking may depend on several non-topical aspects of
the text: whether the user agrees with the viewpoint
expressed in the comment, whether the comment is
convincing and well-written, etc. Previous work has
shown that such analysis can be more difficult than
topic-based analysis (Pang and Lee, 2008), and we
have the additional challenge that comments are typ-
ically much shorter than full-length articles. How-
ever, the difficulty in analyzing the textual infor-
mation in comments can be alleviated by additional
contextual information such as author identities. If
between a pair of users one consistently likes or dis-
likes the other, then at least for the heavy users, this
authorship information alone could be highly infor-
mative. Indeed, previous work in collaborative filter-
ing has usually found no additional gain from lever-
aging content information when entity-level prefer-
ence information is abundant.
In this paper, we present a principled way of uti-
lizing multiple sources of information for the task of
recommending user comments, which significantly
outperforms strong baseline methods, as well as pre-
vious methods proposed for text recommendation.
While using authorship information alone tends to
provide stronger signal than using textual informa-
tion alone, to our surprise, even for heavy users,
adding textual information to the authorship infor-
mation yields additional improvements.
</bodyText>
<sectionHeader confidence="0.999815" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999840710526316">
There are two main bodies of related work: our
problem formulation is closer to collaborative filter-
ing, while the nature of the text we are dealing with
has more in common with opinion mining and sen-
timent analysis.
Our approach is related to a large body of work
in collaborative filtering. While a proper survey
is not possible here, we describe some of the ap-
proaches that are germane. Classical approaches in
collaborative filtering are based on item-item/user-
user similarity, these are nearest-neighbor methods
where the response for a user-item pair is predicted
based on a local neighborhood mean (Sarwar et al.,
2001; Wang et al., 2006). In general, neighbor-
hoods are defined by measuring similarities between
users/items through correlation measures like Pear-
son, cosine similarities, etc. Better approaches to
estimate similarities have also been proposed in Ko-
ren (2010). However, modern methods based on
matrix factorization have been shown to outperform
nearest neighbor methods (Salakhutdinov and Mnih,
2008a,b; Bell et al., 2007). Generalizations of ma-
trix factorization to include both features and past
ratings have been proposed (Agarwal and Chen,
2009; Stern et al., 2009). The approach in this pa-
per is an extension where in addition to interactions
among users and items (comments in our case), we
also consider the authorship information. Three-way
interactions were recently studied for personalized
tag recommendation (Rendle and Lars, 2010). Their
model was based on the sum of two-way interac-
tions, and was trained by using pairwise tag pref-
erences for each (user, item) pair. However, no fea-
tures were considered, which is an important consid-
eration for us. We show using both text and author-
ship provides the best performance.
Our work is also related to news personalization
that has received increasing attention in the last few
</bodyText>
<page confidence="0.995251">
572
</page>
<bodyText confidence="0.999983613333334">
years. For instance, Billsus and Pazanni (2007) de-
scribes an approach to build user profile models for
adaptive personalization in the context of mobile
content access. Their approach is based on a hybrid
model that combines content-based approaches with
similarity methods used in recommender systems.
This is further exemplified in the work by Ahn et al.
(2007) where text processing techniques are used to
build content profiles for users to recommend per-
sonalized news. In our experiments, we show that
such approaches are inferior to our method. A con-
tent agnostic approach based on collaborative filter-
ing techniques was proposed by Das et al. (2007);
cold-start for new items/users was not their focus,
but is important for our task — candidate comments
for recommendation are often not in training data.
As discussed in Section 1, previous work in opin-
ion mining and sentiment analysis has addressed the
information consumption challenge via review sum-
marization. Discussion of early work in that di-
rection can be found in Pang and Lee (2008). In
this line of work, opinions for each given aspect are
usually summarized as the average sentiment po-
larity associated with that aspect. Related to that,
people have looked into predicting review helpful-
ness given the textual information in reviews, where
helpfulness is either defined as the percentage of
users who have voted the review to be helpful (Kim
et al., 2006), or labeled by annotators according to
a set of criteria (Liu et al., 2007). Our goal dif-
fers in that we look for personalized ranking (what
a specific user might like) rather than generic qual-
ity (what an average user might like). Subsequently,
there has been work that tried to predict similarly
defined helpfulness scores using meta-information
over the reviewer. For instance, whether the au-
thor has used his/her true name or where the user
is from (Danescu-Niculescu-Mizil et al., 2009), as
well as graph structure in the social network be-
tween reviewers (Lu et al., 2010). In this work, we
simply use author identity to provide more context
to the short text; in future work, additional meta-
information over users can easily be incorporated
via our model.
As discussed in Section 1, whether a rater likes a
comment or not may depend on whether they agree
with the viewpoint expressed in the text and quality
of the text. While previous work has not looked into
the reader-comments relationship, there has been re-
lated work on identifying political orientations or
viewpoints (Lin and Hauptmann, 2006; Lin et al.,
2006; Mullen and Malouf, 2006, 2008; Laver et al.,
2003); whether a piece of text expresses support or
opposition in congressional debates (Thomas et al.,
2006) or online debates (Somasundaran and Wiebe,
2009, 2010); as well as identifying contrastive re-
lationship (Kawahara et al., 2010). Note that it is
not trivial to use previous work along this line to di-
rectly serve as sub-components in our setting. For
instance, for work on identifying political orienta-
tions or viewpoints, the training data consists of text
with the desired labels. In our setting, our labels
come in the form of whether users liked or disliked
a previous comment. In the simplest case, we might
have pair-wise constraints on whether two pieces of
text have the same viewpoints (i.e., liked or dis-
liked by the same rater), which would yield a dif-
ferent learning problem akin to the metric learning
problem; note, however, the complication that two
pieces of text receiving different labels from a given
user might not necessarily contain contrasting view-
points. Consequently, rather than trying to reduce
this problem to a set of known text classification
tasks, we address this task via a collaborative filter-
ing framework that incorporates textual features.
</bodyText>
<sectionHeader confidence="0.980076" genericHeader="conclusions">
3 Method
</sectionHeader>
<bodyText confidence="0.999818666666667">
In this section, we describe our model that predicts
rater affinity to comments. A key strength of our
model is the ability to incorporate rater-comment
and rater-author interactions simultaneously in a
principled fashion. Our model also provides a seam-
less mechanism to transition from cold-start (where
recommendations need to be made for users or items
with no or few past ratings) to warm-start scenarios
— with a large amount of data, it fits a per-rater (au-
thor) model; with increase in data sparsity, the model
applies a small sample size correction through fea-
tures (in our case, textual features). The exact for-
mula for such corrections in the presence of sparsity
is based on parameter estimates that are obtained by
applying an EM algorithm to the training data.
</bodyText>
<page confidence="0.99719">
573
</page>
<subsectionHeader confidence="0.995599">
3.1 Model
</subsectionHeader>
<bodyText confidence="0.997495983870968">
Notation: Let yij denote the rating that user i, called
the rater, gives to comment j. Since throughout, we
use suffix i to denote a rater and suffix j to denote a
comment, we slightly abuse notation and let xi (of
dimension pu) and xj (of dimension pc) denote fea-
ture vectors of user i and comment j respectively.
For example, xi can be the bag of words represen-
tation (a sparse vector) inferred through text anal-
ysis on comments voted positively by user i in the
past, and xj can be the bag of words representation
for comment j. We use a(j) to denote the author of
comment j, and use µij to denote the mean rating by
rater i on comment j, i.e., µij = E(yij). Of course it
is impossible to estimate µij empirically since each
user i usually rates a comment j at most once.
Model specification: We work in a generalized
linear model framework (McCullagh and Nelder,
1989) that assumes µij (or some monotone function
h of µij) is an additive function of (1) the rater bias
αi of user i since some users may have a tendency
of rating comments more positively or negatively
than others, (2) popularity βj of comment j, which
could reflect the quality of the comment in this set-
ting, and (3) the author reputation γa(j) of user a(j)
since comments by a reputed author may in general
get more positive ratings. Thus, the overall bias is
αi + βj + γa(j).
In addition to the bias, we include terms that
capture interactions among entities (raters, authors,
comments). Indeed, capturing such interactions is a
non-trivial part of our modeling procedure. In our
approach, we take recourse to factor models that
have been widely used in collaborative filtering ap-
plications in recent times. The basic idea is to at-
tach latent factors to each rater, author and comment.
These latent factors are finite dimensional Euclidean
vectors that are unknown and estimated from the
data. They provide a succinct representation of vari-
ous aspects that are important to explain interaction
among entities. In our case, we use the following
factors — (a) user factor vi of dimension rv(&gt; 1)
to model rater-author affinity, (b) user factor ui and
comment factor cj of dimension ru(&gt; 1) to model
rater-comment affinity. Intuitively, each could repre-
sent viewpoints of users or comments along different
i index for raters
j index for comments
a(j) author of comment j
yij rating given by rater i to comment j
µij mean rating given by rater i to comment j
xj feature vector of comment j
(e.g., textual features in comment j)
xi feature vector of user i
(e.g., comments voted positively by user i)
bias terms:
αi rater bias of user i
βj popularity of comment j
(e.g., quality of the comment)
γa(j) reputation of the author of comment j
interaction terms:
vi user factor for rater-author affinity
ui, cj factors for rater-comment affinity
</bodyText>
<tableCaption confidence="0.994874">
Table 1: Table of Notations.
</tableCaption>
<bodyText confidence="0.99921">
dimensions.
Affinity of rater i to comment j by author a(j)
is captured by (1) similarity between viewpoints of
users i and a(j), measured by v0iva(j); and (2) simi-
larity between the preferences of user i and the per-
spectives reflected in comment j, measured by u0icj.
The overall interaction is v0iva(j) + u0icj. Then, the
mean rating µij, or more precisely h(µij), is mod-
eled as the sum of bias and interaction terms. Math-
ematically, we assume:
</bodyText>
<equation confidence="0.993313333333333">
yij ∼ N(µij, σ2y) or Bernoulli(µij)
(1)
h(µij) = αi + βj + γa(j) + v0 iva(j) + u0 icj
</equation>
<bodyText confidence="0.99980375">
For numeric ratings, we use the Gaussian distri-
bution denoted by N(mean,var); for binary rat-
ings, we use the Bernoulli distribution. For Gaus-
sian, h(µij) = µij, and for Bernoulli, we assume
</bodyText>
<equation confidence="0.981142">
h(µij) = log µzj
</equation>
<bodyText confidence="0.8961388">
1−µzj , which is the commonly used
logistic transformation.
Table 1 summarizes the notations for easy refer-
ences. We denote the full model specified above as
vv+uc since both user-user interaction v0iva(j) and
user-comment interaction u0icj are modeled at the
same time.
Latent factors: A natural approach to estimate la-
tent factors in Equation 1 is through a maximum
likelihood estimation (MLE) approach. This does
</bodyText>
<page confidence="0.989419">
574
</page>
<bodyText confidence="0.999974052631579">
not work in our scenario since a large fraction of
entities have small sample size. For instance, if a
comment is rated only by one user and ru &gt; 1,
the model is clearly overparametrized and the MLE
of the comment factor would tend to learn idiosyn-
crasies in the training data. Hence, it is imperative
to impose constraints on the factors to obtain esti-
mates that generalize well on unseen data. We work
in a Bayesian framework where such constraints are
imposed through prior distributions. The crucial is-
sue is the selection of appropriate priors. In our sce-
nario, we need priors that provide a good backoff
estimate when interacting entities have small sam-
ple sizes. For instance, to estimate latent factors of
a user with little data, we provide a backoff estimate
that is obtained by pooling data across users with
the same user features. We perform such a pooling
through regression, the mathematical specification is
given below.
</bodyText>
<equation confidence="0.999783333333333">
αi — N(g0xi, σ2α), ui — N(Gxi, σ2u),
βj — N(d0xj, σ2β), cj — N(Dxj, σ2c),
γa(j) — N(0, σ2 γ), vi — N(0, σ2v),
</equation>
<bodyText confidence="0.999979047619048">
where gpu×1 and dp-×1 are regression weight vec-
tors, and Gru×pu and Dru×p- are regression weight
matrices. These regression weights are learnt from
data and provide the backoff estimate. Take the prior
distribution of ui for example. We can rewrite the
prior as ui = Gxi + Si, where Si — N(0, σ2u).
If user i has no rating in the training data, ui will
be predicted as the prior mean (backoff) Gxi, a lin-
ear projection from the feature vector xi through
matrix G learnt from data. This projection can be
thought of as a multivariate linear regression prob-
lem with weight matrix G, one weight vector per
dimension of ui. However, if user i has many rat-
ings in the training data, we will precisely estimate
the per-user residual Si that is not captured by the re-
gression Gxi. For sample sizes in between these two
extremes, the per user residual estimate is “shrunk”
toward zero — amount of shrinkage depends on the
sample size, past user ratings, variability in ratings
on comments rated by the user, and the value of vari-
ance components σ2· s.
</bodyText>
<subsectionHeader confidence="0.991079">
3.2 Special Cases of Our Model
</subsectionHeader>
<bodyText confidence="0.999653081081081">
Our full model (vv+uc) includes several existing
models explored in collaborative filtering and social
networks as special cases.
The matrix factorization model: This model as-
sumes the mean rating of user i on item j is given
by h(µij) = αi + βj + u0icj, and the mean of
the prior distributions on αi,βj, ui, cj are zero, i.e.,
g = d = G = D = 0. Recent work clearly illus-
trates that this method obtains better predictive accu-
racy than classical collaborative filtering techniques
based on item-item similarity (Bell et al. (2007)).
The uc model: This is also a matrix factorization
model but with priors based on regressions (i.e.,
non-zero g, d, G, D). It provides a mechanism to
deal with both cold and warm-start scenarios in rec-
ommender applications (Agarwal and Chen (2009)).
The vv model: This model assumes h(µij) = αi +
γa(j) + v0iva(j). It was first proposed by Hoff (2005)
to model interactions in social networks. The model
was fitted to small datasets (at most a few hundred
nodes) and the goal was to test certain hypotheses
on social behavior, out-of-sample prediction was not
considered.
The low-rank bilinear regression model: Here,
h(µij) = g0xi + d0xj + x0iG0Dxj. This is a re-
gression model purely based on features with no per-
user or per-comment latent factors. In a more gen-
eral form, x0iG0Dxj can be written as x0iAxj, where
Apu×p- is the matrix of regression weights (Chu and
Park, 2009). However, since xi and xj are typically
high dimensional, A can be a large matrix that needs
to be learnt from data. To reduce dimensionality, one
can decompose A as A = G0D, where the number
of rows in D and G are small. Thus, instead of learn-
ing A, we learn a low-rank approximation of A. This
ensures scalability and provides an attractive method
to avoid over-fitting.
</bodyText>
<subsectionHeader confidence="0.999376">
3.3 Model Fitting
</subsectionHeader>
<bodyText confidence="0.999206888888889">
Model fitting for our model is based on the
expectation-maximization (EM) algorithm (Demp-
ster et al., 1977). For ease of exposition and space
constraints, we only provide a sketch of the algo-
rithm for the Gaussian case, the logistic model can
be fitted along the same lines by using a variational
approximation (see Agarwal and Chen (2009)).
Let Y = {yij} denote the set of the observed
ratings. In the EM parlance, this is “incomplete”
</bodyText>
<page confidence="0.992094">
575
</page>
<bodyText confidence="0.924442083333333">
data that gets augmented with the latent factors
Θ = {ui, vi, ej} to obtain the “complete” data.
The goal of the EM algorithm is to find the param-
eter 77 = (g, d, G, D, a2α, a2β, a2u, a2v, a2y) that maxi-
mizes the “incomplete” data likelihood Pr(Y |77) =
f Pr(Y , Θ|77)dΘ that is obtained after marginaliza-
tion (taking expectation) over the distribution of Θ.
Since such marginalization is not available in closed
form for our model, we use the EM algorithm.
EM algorithm: The complete data log-likelihood
l(77; Y , Θ) for the full model in the Gaussian case
(where h(pij) = pij) is given by l(77; Y , Θ) =
</bodyText>
<equation confidence="0.972365166666667">
2 Eij((yij − pij)2/ay + log ay)
2 Ei ((αi − g,xi)2/a2 + log aα)
− 2 Ej I (βj − dxj)2/a2 + log a2 β
2 Ei ( k ui − Gxi k2/a2 + ru log a2)
2 Ej ( k ej − Dxj k2/a2 + ru log a2) ,
− 2 Ei (vZvi/a2v + rv log a2 + -yi /a2γ + log a2) , γ
</equation>
<bodyText confidence="0.9959508">
where ru is the dimension of factors ui and ej, and
rv is the dimension of vi. Let 77(t) denote the esti-
mated parameter setting at the tth iteration. The EM
algorithm iterates through the following two steps
until convergence.
</bodyText>
<listItem confidence="0.773463125">
• E-step: Compute ft(77) = EΘ[l(77; Y , Θ)  |77(t)]
as a function of 77, where the expectation is taken
over the posterior distribution of (Θ  |77(t), Y ).
Note that here 77 is the input variable of function
ft, but 77(t) consists of known quantities (deter-
mined in the previous iteration).
• M-step: Find the 77 that maximizes the expecta-
tion computed in the E-step.
</listItem>
<equation confidence="0.993173">
77 = arg max
(t+1) ft(77)
</equation>
<bodyText confidence="0.999941263157895">
Since the expectation in the E-step is not available in
a closed form, we use a Gibbs sampler to compute
the Monte Carlo expectation (Booth and Hobert,
1999). The Gibbs sampler repeats the following
procedure L times. It samples αi, -yi, βj, ui, vj,
and ej sequentially one at a time by sampling from
the corresponding full conditional distributions. The
full conditional distributions are all Gaussian, hence
they are easy to sample. Once a Monte Carlo ex-
pectation is calculated from the samples, an updated
estimate of 77 is obtained in the M-step. The opti-
mization of variance components a2· s in the M-step
is available in closed form, the regression param-
eters are estimated through off-the-shelf linear re-
gression routines. We note that the posterior distri-
bution of latent factors for known 77 is multi-modal,
we have found the Monte Carlo based EM method to
outperform other optimization methods like gradient
descent in terms of predictive accuracy.
</bodyText>
<sectionHeader confidence="0.999437" genericHeader="references">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.967011">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.982149166666667">
We obtained comment rating data between March
and May, 2010 from Yahoo! News, with all user IDs
anonymized. On this site, users can post comments
on news article pages and rate the comments made
by others through thumb-up (positive) or thumb-
down (negative) votes. Clearly, for articles with very
few comments, there is no need to recommend com-
ments. Also, we do not expect deep personalized
recommendations for users who have rated very few
comments in the past. To focus on instances of in-
terest to us, we restricted ourselves to a subset of the
rating data associated with relatively heavy raters.
In particular, we formed the experimental dataset
by randomly selecting 9,003 raters who provided at
least 200 ratings (of which at least 10 were posi-
tive and 10 were negative), 189,291 authors who re-
ceived at least 20 ratings, and 5,088 news articles
that received at least 40 comments in the raw dataset
during the three-month period. Note that the per en-
tity sample size in the experimental dataset can be
smaller than the thresholds specified above. For in-
stance, a rater with more than 200 ratings in the raw
dataset can have fewer than 200 in the experimental
dataset due to the removal of certain authors or news
articles. (See Figure 2 for a distribution of users with
different activity levels.) In total, we have 4,440,222
ratings on 1,197,098 comments.
The 5,088 news articles were split into training
articles (the earliest 50%), tuning articles (next 5%),
and test articles (the last 45%) based on their pub-
lication time. The ratings and comments were split
into training, tuning, and test sets according to the
article they were associated with. All tuning param-
eters are determined using the tuning set, and per-
formances are reported over the test set. Note that
η
</bodyText>
<page confidence="0.985799">
576
</page>
<bodyText confidence="0.999943">
this training-test split ensures that performance on
the test data best simulates our application scenar-
ios. It also creates a completely cold-start situation
for comments — no comment in the test set has any
past rating in the training set.
</bodyText>
<subsectionHeader confidence="0.966289">
4.2 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9999045">
Features: All comments were tokenized, lower-
cased, with stopwords and punctuations removed.
We limited the vocabulary to the 10K most frequent
tokens in all comments associated with the training
articles. (See Section 4.3.3 for a discussion on the
effect of the vocabulary size.) For a given comment
j, xj is its bag of words representation, L2 normal-
ized. For term weighting, we experimented with
both presence value and tf-idf weighting. The latter
gives slight better performance. Rater feature vector
xi is created by summing over the feature vectors
of all comments rated positively by rater i, which is
then L2 normalized.
Methods: We compare the following methods
based on our model: The full model vv+uc, as well
as the three main special cases, vv, uc, and bilin-
ear, as defined in Section 3. The dimensions of vi,
ui and cj (i.e., r„ and ru), and the rank of bilin-
ear are selected to obtain the best AUC on the tun-
ing set. In our experiments, r„ = 2, ru = 3 and
rank of bilinear is 3. In addition, we also evaluate
the following baseline methods that predict per-user
preferences in isolation, primarily based on textual
information.
</bodyText>
<listItem confidence="0.985806777777778">
• Cosine similarity (cos): xZxj. This is simply
based on how similar a new comment j is to the
comments rater i has liked in the past.
• Per-user SVM (svm): For each rater, train a sup-
port vector machine (SVM) classifier using only
comments (xj) rated by that user.
• Per-user Naive Bayes (nb): For each rater, train
a Naive Bayes classifier using only comments
(xj) rated by that user.1
</listItem>
<bodyText confidence="0.991066">
Note that SVMs typically yield the best performance
on text classification tasks; a Naive Bayes classifier
1As we mentioned in Section 4.1, not all users have training
data of both classes in the experimental dataset. For svm and
nb, we use the following backoff: for users with training data
from only ci, we predict ci; for users with no training data at
all, we predict the majority class, in this case, the positive class.
can be more robust over shorter text spans common
in user comments given the high variance. For fair
comparisons, for the three baseline methods, we use
a simple way of utilizing author information: the
feature space is augmented with author IDs and each
xj is augmented with a(j)2. In Section 4.3, we only
report results using the augmented feature vectors
since they yield better performance (though the dif-
ference is fairly small).
Performance metrics: We use two types of met-
rics to measure the performance of a method: (1)
A global metric based on Receiver Operating Char-
acteristic (ROC) and (2) Precision at rank k (P@k).
The former measures the overall correlation of pre-
dicted scores for a method with the observed rat-
ings in the test set, while the latter measures the
performance of a hypothetical top-k recommenda-
tion scenario using the method. To summarize an
ROC curve into a single number, we use the Area
Under the ROC Curve (AUC). Since random guess
yields AUC score of 0.5, regardless of the class dis-
tribution, using this measure makes it convenient for
us to compare the performance over different sub-
sets of the data (where class distributions could be
different). The P@k of a method is computed as
follows: (1) For each rater, rank comments that the
rater rated in the test set according to the scores pre-
dicted by the method, and compute the precision at
rank k for that rater; and then (2) average the per-
rater precision numbers over all raters. To report
P@k, for k = 5,10, 20, we only use raters who have
at least 50 ratings in the test set. Statistical signif-
icance based on a two-sample t-test across raters is
also reported.
</bodyText>
<subsectionHeader confidence="0.752415">
4.3 Results
4.3.1 Main comparisons
</subsectionHeader>
<bodyText confidence="0.999941375">
We first show the ROC curves of different meth-
ods on the test set in Figure 1, and the AUCs and
precisions in Table 2. Results from significance tests
are in Table 3.
First, note that while svm significantly outper-
forms random guesses and nb, it is worse than bi-
linear, which is also using (mostly) textual infor-
mation, but learns the model for all users together,
</bodyText>
<footnote confidence="0.7990665">
2We assign weight 1 to a(j), so that the author information
have the same impact as the textual features.
</footnote>
<page confidence="0.990806">
577
</page>
<figure confidence="0.952851">
0.0 0.2 0.4 0.6 0.8 1.0
False positive rate
</figure>
<figureCaption confidence="0.996424">
Figure 1: ROC curves of different models
</figureCaption>
<table confidence="0.986612375">
Method AUC P@5 P@10 P@20
vv+uc 0.8360 0.9152 0.9079 0.8942
vv 0.8090 0.8810 0.8807 0.8727
uc 0.7857 0.9046 0.8921 0.8694
bilinear 0.7701 0.9028 0.8894 0.8668
svm 0.6768 0.7814 0.7678 0.7497
nb 0.6465 0.7660 0.7486 0.7309
cos 0.5382 0.6834 0.6813 0.6754
</table>
<tableCaption confidence="0.999058">
Table 2: AUCs and precisions of different models.
</tableCaption>
<bodyText confidence="0.999870222222222">
rather than in isolation. Next, uc outperforms bilin-
ear (significantly in AUC, P@10 and P@20), show-
ing per-user and per-comment latent factors help.
Note that vv outperforms uc in ROC, AUC and
P@20, but is worse than uc in P@5 and P@10; we
will take a closer look at this later. Finally, the full
model vv+uc significantly outperforms both vv and
uc, achieving 0.83 in AUC, and close to 90% in pre-
cision at rank 20.
</bodyText>
<subsectionHeader confidence="0.817374">
4.3.2 Break-down by user activity level
</subsectionHeader>
<bodyText confidence="0.9991539">
Next, we investigate model performance in differ-
ent subsets of the test set. For succinctness, we use
AUC as our performance metric. In Figure 2(a), we
breakdown model performance by different author
activity levels. In Figure 2(b), we breakdown model
performance by different voter activity levels. We
also generated similar plots with the y-axis replaced
by P@5, P@10 and P@20, and observed the same
trend except that vv starts to outperform uc at differ-
ent user activity thresholds for different metrics.
</bodyText>
<table confidence="0.992476875">
Comparison Metrics p-value
vv+uc &gt; vv All &lt; 10−7
vv+uc &gt; uc All &lt; 10−20
uc &gt; bilinear All except P@5 &lt; 0.006
bilinear &gt; svm All &lt; 10−20
vv &gt; svm All &lt; 10−20
svm &gt; nb All &lt; 10−8
nb &gt; cos All &lt; 10−20
</table>
<tableCaption confidence="0.993689666666667">
Table 3: Paired t-test results. Note that uc is better than
bilinear in P@5, but not significant. The orders of uc
and vv are not consistent across different metrics.
</tableCaption>
<bodyText confidence="0.991326181818182">
Not surprisingly, vv performs poorly for raters or
authors with no ratings observed in the training data.
However, once we have a small amount of ratings, it
starts to outperform uc, even though intuitively, the
textual information in the comment should be more
informative than the authorship information alone.
Using paired t-tests with significance level 0.05, we
report when vv starts to significantly outperform uc
in the following table, which is interpreted as fol-
lows — vv is not significantly worse than uc in met-
ric M if the author of a test comment received at
least Neq ratings in the training set, and vv signifi-
cantly outperforms uc in metric M if the author re-
ceived at least Ngt ratings in the training set.
Metric M P@5 P@10 P@20 AUC
# Ratings Neq 50 5 5 5
Ngt 1000 50 5 5
Recall that our training/test split is by article. Since
we have never observed a rater’s preference over the
test articles before, it is rather surprising that author
information alone can yield 0.8 in AUC score, even
for very light authors who have received between 3
and 5 votes in total in the training data. This suggests
that users’ viewpoints are quite consistent: a large
portion of the ratings can be adequately explained
by the pair of user identities. One interesting obser-
vation is that the number of ratings required for vv to
outperform uc in P@5 is quite high. This suggests
that to obtain high precision at the top of a recom-
mended list, comment features are important.
Nonetheless, modeling textual information in ad-
dition to author information provides additional im-
provements. Based on paired t-tests with signifi-
</bodyText>
<figure confidence="0.992060431578947">
vv+uc
vv
uc
bilinear
svm
nb
cos
random
True positive rate
0.0 0.2 0.4 0.6 0.8 1.0
578
AUC
AUC
AUC
0−0
1−2
3−5
6−10
11−20
21−50
51−100
101−200
201−500
501−2000
0−0
1−5
6−10
11−20
21−50
51−100
101−200
201−500
501−2000
21K
19K
41K
29K
0.90
0.85
0.80
vv+uc
vv
uc
bilinear
svm
nb
0.75
0.70
50−100
101−200
201−500
501−2000
409K
168K
147K
143K
170K
231K
184K
131K
120K
82K
0.90
0.85
0.80
0.75
vv+uc
vv
uc
bilinear
svm
nb
0.70
0.65
0.60
25K
22K
15K
31K
96K
202K
404K
597K
403K
0.8
0.7
vv+uc
vv
uc
bilinear
svm
nb
0.6
0.5
(a) AUC by author activity levels (b) AUC by rater activity levels (c) AUC by user activity levels
</figure>
<figureCaption confidence="0.989286">
Figure 2: AUC of different models as a function of the activity level of authors or raters. The x-axis (bottom) has the
form m-n, meaning the subset of the test data in which the number of ratings that each author received (as in (a)) or
each rater gave (as in (b)) in the training set is between m and n. In (c), we select both authors and raters based on the
m-n criterion. The x-axis (top) denotes the number of ratings in the subset
</figureCaption>
<bodyText confidence="0.999965166666667">
cance level 0.05, vv+uc significantly outperforms vv
in all metrics if the author received &lt; 500 ratings
in the training set. Except for the very heavy au-
thors, even for cases where both raters and authors
are heavy users (Figure 2(c)), adding the comment
feature information still yields additional improve-
ment over the already impressive performance of us-
ing vv alone. In spite of the simple representation we
adopted for the textual information, the full model is
still capable of accounting for part of the residual
errors from vv model (that uses authorship informa-
tion alone) by using comment features — what was
actually written does matter.
Finally, if we breakdown the comparison be-
tween vv+uc and uc for different user activity lev-
els, vv+uc significantly outperforms uc (with level
0.05) in all metrics if the author received at least 5
ratings in the training set.
</bodyText>
<subsectionHeader confidence="0.984898">
4.3.3 Analysis of textual features
</subsectionHeader>
<bodyText confidence="0.9934005">
Recall that we limited the vocabulary size to the
10K most frequent terms for efficiency reasons. Is
this limitation likely to affect our model perfor-
mance significantly? We examined the effect of dif-
ferent numbers of features. In the following table,
#features = n means that both xi and xj are bags
of n words3. Since the vv model does not utilize
rater or comment features, we examine AUC of the
uc model.
#features 1K 3K 5K 10K
AUC 0.7713 0.7855 0.7872 0.7876
As can be seen, the performance improvement is in
the 4th decimal place when we increase from 5K
features to 10K features. Thus, we do not further
increase the number of features in our experiments.
Note that our full model does not require rater fea-
tures and comment features to be in the same feature
space. Each is projected into the hidden “viewpoint”
space, via G and D, separately. For simplicity and
easy comparison to other methods, we used all com-
ments liked by a rater in the past to build the feature
vector of the rater. But since the full model already
has information of the textual content of comments
from the comment features, and which comments
were liked by the users from the ratings, rater fea-
tures constructed this way do not provide any new
information. Indeed, if we model ui — N(1, σ2u),
instead of ui — N(Gxi, σ2u), this omission of xi
does not hurt the performance of the model. In fu-
ture work, other meta-information about the rater
</bodyText>
<footnote confidence="0.815798">
3Note that we used n most useful features in each case.
</footnote>
<page confidence="0.98622">
579
</page>
<bodyText confidence="0.997305425531915">
can easily be incorporated into xi to enrich rater rep- rater-comment and rater-author interactions simul-
resentation. taneously. Our full model significantly outperforms
Recall that comment features xj were projected to strong baseline methods, as well as previous meth-
comment factors cj via D. We envisioned that the ods proposed for text recommendation. In particu-
comment factors could be representing viewpoints. lar, learning weights over textual features across all
Does our model conform to this intuition? Let’s con- users outperforms learning for each user individu-
sider the simplest case, where we restrict ui and cj ally, which holds true even for heavy raters. Further-
to be one-dimensional vectors. In this case, each can more, while using authorship information alone pro-
be represented by scalars ui and cj. If ui and cj vides stronger signal than using textual information
are of the same sign, then the rater is likely to like alone, to our surprise, even for heavy users, adding
the comment. Words assigned high positive weights textual information yields additional improvements.
or low negative weights via D will have significant It is difficult to comprehensively capture user
contributions to the overall sign of cj. Now if we ex- affinity to comments using a finite number of rat-
amine such words, will we see any meaningful dif- ings observed during a certain time interval. News
ferences in the underlying viewpoints of these two and comments on news articles are dynamic in na-
groups of words? ture, novel aspects may emerge over time. To cap-
To address this question qualitatively, we manu- ture such dynamic behavior, comment factors have
ally sampled words with heavy weights, focusing on to be allowed to evolve over time and such an evolu-
politics-related ones (so that viewpoints are likely tion would also necessitate the re-estimation of user
to be polarized and easier to interpret). At one ex- factors. Incorporating such temporal dynamics into
treme, we observe words like repukes, repugs, which our modeling framework is a challenging research
seemed to be derogatory mentions of Republica- problem and requires significant elaboration of our
tions, and likely to represent an anti-Republication current approach.
point of view. At the other end, we observe terms Acknowledgments
like libtards, nobama, obummer. While terms like We thank the anonymous reviewers for useful sug-
nobama may appear to be typos at first sight, a gestions.
quick search online reveals that these are at least References
intentional typos expressing anti-Obama sentiments, D. Agarwal and B.C. Chen. Regression-based latent
which clearly represents an opposite underlying per- factor models. In Proceedings of the 15th ACM
spective from terms like repukes. SIGKDD international conference on Knowledge
These examples also illustrate the importance to discovery and data mining, pages 19–28. ACM,
learn directly from the data of interest to us. Such 2009.
indicative words would never have appeared in more Jae-Wook Ahn, Peter Brusilovsky, Jonathan Grady,
formal writings. While we do not have direct labels Daqing He, and Sue Y. Syn. Open user profiles
for perspectives, our model seems to be capturing for adaptive news systems: help or harm? In Pro-
the underlying perspectives (as much as a unigram- ceedings of the 16th international conference on
based model could) by learning from user preference World Wide Web (WWW), 2007.
labels across different users. This allows us to learn Robert Bell, Yehuda Koren, and Chris Volinsky.
the text features most relevant to our dataset, which Modeling relationships at multiple scales to im-
is particularly important given the time-sensitive and prove accuracy of large recommender systems. In
ever-evolving nature of news-related comments. KDD, 2007.
5 Conclusions D. Billsus and M. Pazanni. Adaptive news access.
In this paper, we promote personalized recommen- Springer, Berlin, 2007.
dation as a novel way of helping users to consume J.G. Booth and J.P Hobert. Maximizing generalized
large quantities of subjective information. We pro- linear mixed model likelihoods with an automated
pose using a principled way of incorporating both
580
</bodyText>
<reference confidence="0.995606711111111">
monte carlo EM algorithm. J.R.Statist. Soc. B,
1999.
Wei Chu and Seung T. Park. Personalized recom-
mendation on dynamic content using predictive
bilinear models. In Proceedings of the 18th inter-
national conference on World Wide Web (WWW),
2009.
Cristian Danescu-Niculescu-Mizil, Gueorgi
Kossinets, Jon Kleinberg, and Lillian Lee.
How opinions are received by online communi-
ties: A case study on Amazon.com helpfulness
votes. In Proceedings of WWW, pages 141–150,
2009.
Abhinandan S. Das, Mayur Datar, Ashutosh Garg,
and Shyam Rajaram. Google news personaliza-
tion: scalable online collaborative filtering. In
Proceedings of the 16th international conference
on World Wide Web (WWW), 2007.
A. P. Dempster, N. M. Laird, and D. B. Rubin. Max-
imum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Soci-
ety, Series B, 39:1–38, 1977.
Peter D. Hoff. Bilinear mixed-effects models for
dyadic data. Journal of the American Statistical
Association, 100(469):286–295, 2005.
Minqing Hu and Bing Liu. Mining and summa-
rizing customer reviews. In Proceedings of the
ACM SIGKDD Conference on Knowledge Dis-
covery and Data Mining (KDD), pages 168–177,
2004.
Daisuke Kawahara, Kentaro Inui, and Sadao Kuro-
hashi. Identifying contradictory and contrastive
relations between statements to outline web infor-
mation on a given topic. In Proceedings of the
23rd International Conference on Computational
Linguistics (COLING): Posters, 2010.
Soo-Min Kim, Patrick Pantel, Tim Chklovski, and
Marco Pennacchiotti. Automatically assessing re-
view helpfulness. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP), pages 423–430, Sydney,
Australia, July 2006. Association for Computa-
tional Linguistics.
Y. Koren. Factor in the neighbors: Scalable and ac-
curate collaborative filtering. ACM Transactions
</reference>
<page confidence="0.648363">
581
</page>
<reference confidence="0.999623816513761">
on Knowledge Discovery from Data (TKDD), 4
(1):1–24, 2010. ISSN 1556-4681.
Michael Laver, Kenneth Benoit, and John Garry. Ex-
tracting policy positions from political texts using
words as data. American Political Science Review,
97(2):311–331, 2003.
Wei-Hao Lin and Alexander Hauptmann. Are these
documents written from different perspectives? A
test of different perspectives based on sta tisti-
cal distribution divergence. In Proceedings of the
International Conference on Computational Lin-
guistics (COLING)/Proceedings of the Associa-
tion for Computational Linguistics (ACL), pages
1057–1064, Sydney, Australia, July 2006. Asso-
ciation for Computational Linguistics.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. Which side are you on?
identifying perspectives at the document and sen-
tence levels. In Proceedings of the Conference on
Natural Language Learning (CoNLL), 2006.
Jingjing Liu, Yunbo Cao, Chin-Yew Lin, Yalou
Huang, and Ming Zhou. Low-quality product
review detection in opinion summarization. In
Proceedings of the Joint Conference on Empir-
ical Methods in Natural Language Processing
and Computational Natural Language Learning
(EMNLP-CoNLL), pages 334–342, 2007. Poster
paper.
Yue Lu, Panayiotis Tsaparas, Alexandros Ntoulas,
and Livia Polanyi. Exploiting social context for
review quality prediction. In Proceedings of the
19th International World Wide Web Conference
(WWW), 2010.
P. McCullagh and J. A. Nelder. Generalized Linear
Models. Chapman &amp; Hall/CRC, 1989.
Tony Mullen and Robert Malouf. A preliminary in-
vestigation into sentiment analysis of informal po-
litical discourse. In AAAI Symposium on Compu-
tational Approaches to Analysing Weblogs (AAAI-
CAAW), pages 159–162, 2006.
Tony Mullen and Robert Malouf. Taking sides:
User classification for informal online political
discourse. Internet Research, 18:177–190, 2008.
Bo Pang and Lillian Lee. Opinion mining and sen-
timent analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1–135, 2008.
Ana-Maria Popescu and Oren Etzioni. Extracting
product features and opinions from reviews. In
Proceedings of the Human Language Technol-
ogy Conference and the Conference on Empir-
ical Methods in Natural Language Pro cessing
(HLT/EMNLP), 2005.
Steffen Rendle and Schmidt-Thie Lars. Pairwise
interaction tensor factorization for personalized
tag recommendation. In Proceedings of the third
ACM international conference on Web search and
data mining, WSDM ’10, pages 81–90, New
York, NY, USA, 2010. ACM.
R. Salakhutdinov and A. Mnih. Bayesian proba-
bilistic matrix factorization using Markov chain
Monte Carlo. In Proceedings of the 25th inter-
national conference on Machine learning, pages
880–887. ACM, 2008a.
R. Salakhutdinov and A. Mnih. Probabilistic ma-
trix factorization. Advances in neural information
processing systems, 20:1257–1264, 2008b.
Badrul Sarwar, George Karypis, Joseph Konstan,
and John Reidl. Item-based collaborative filter-
ing recommendation algorithms. In WWW ’01:
Proceedings of the 10th international conference
on World Wide Web, pages 285–295. ACM, 2001.
Benjamin Snyder and Regina Barzilay. Multiple as-
pect ranking using the Good Grief algorithm. In
Proceedings of the Joint Human Language Tech-
nology/North American Chapter of the ACL Con-
ference (HLT- NAACL), pages 300–307, 2007.
Swapna Somasundaran and Janyce Wiebe. Recog-
nizing stances in online debates. In Proceedings
of the Joint Conference of the 47th Annual Meet-
ing of the ACL and the 4th International Joint
Conference on Natural Language Processing of
the AFNLP, 2009.
Swapna Somasundaran and Janyce Wiebe. Recog-
nizing stances in ideological on-line debates. In
Proceedings of the NAACL HLT 2010 Workshop
on Computational Approaches to Analysis and
Generation of Emotion in Text, 2010.
David H. Stern, Ralf Herbrich, and Thore Grae-
pel. Matchbox: large scale online bayesian rec-
ommendations. In Proceedings of the 18th inter-
national conference on World Wide Web (WWW),
2009.
Matt Thomas, Bo Pang, and Lillian Lee. Get out
the vote: Determining support or opposition from
Congressional floor-debate transcripts. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
327–335, 2006.
Ivan Titov and Ryan McDonald. A joint model of
text and aspect ratings for sentiment summariza-
tion. In Proceedings of the Association for Com-
putational Linguistics (ACL), 2008.
Jun Wang, Arjen P. de Vries, and Marcel J. T. Rein-
ders. Unifying user-based and item-based collab-
orative filtering approaches by similarity fusion.
In SIGIR ’06: Proceedings of the 29th annual in-
ternational ACM SIGIR conference on Research
and development in information retrieval, pages
501–508, New York, NY, USA, 2006. ACM.
</reference>
<page confidence="0.997868">
582
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.594232">
<title confidence="0.993019">Personalized Recommendation of User Comments via Factor Models</title>
<author confidence="0.978041">Deepak Agarwal Bee-Chung Chen Bo</author>
<affiliation confidence="0.655111">Yahoo!</affiliation>
<address confidence="0.866771">701 First Sunnyvale, CA</address>
<abstract confidence="0.999607083333333">In recent years, the amount of user-generated opinionated texts (e.g., reviews, user comments) continues to grow at a rapid speed: featured news stories on a major event easily attract thousands of user comments on a popular online News service. How to consume subjective information of this volume becomes an interesting and important research question. In contrast to previous work on review analysis that tried to filter or summarize information a generic we explore a different direction of enabling personalized recommendation of such information. For each user, our task is to rank the comments associated with a given article according to personalized user preference (i.e., whether the user is likely to like or dislike the comment). To this end, we propose a factor model that incorporates rater-comment and rater-author interactions simultaneously in a principled way. Our full model significantly outperforms strong baselines as well as related models that have been considered in previous work.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J R Statist Soc B</author>
</authors>
<date>1999</date>
<marker>B, 1999</marker>
<rawString>monte carlo EM algorithm. J.R.Statist. Soc. B, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Chu</author>
<author>Seung T Park</author>
</authors>
<title>Personalized recommendation on dynamic content using predictive bilinear models.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th international conference on World Wide Web (WWW),</booktitle>
<contexts>
<context position="20251" citStr="Chu and Park, 2009" startWordPosition="3370" endWordPosition="3373">del: This model assumes h(µij) = αi + γa(j) + v0iva(j). It was first proposed by Hoff (2005) to model interactions in social networks. The model was fitted to small datasets (at most a few hundred nodes) and the goal was to test certain hypotheses on social behavior, out-of-sample prediction was not considered. The low-rank bilinear regression model: Here, h(µij) = g0xi + d0xj + x0iG0Dxj. This is a regression model purely based on features with no peruser or per-comment latent factors. In a more general form, x0iG0Dxj can be written as x0iAxj, where Apu×p- is the matrix of regression weights (Chu and Park, 2009). However, since xi and xj are typically high dimensional, A can be a large matrix that needs to be learnt from data. To reduce dimensionality, one can decompose A as A = G0D, where the number of rows in D and G are small. Thus, instead of learning A, we learn a low-rank approximation of A. This ensures scalability and provides an attractive method to avoid over-fitting. 3.3 Model Fitting Model fitting for our model is based on the expectation-maximization (EM) algorithm (Dempster et al., 1977). For ease of exposition and space constraints, we only provide a sketch of the algorithm for the Gau</context>
</contexts>
<marker>Chu, Park, 2009</marker>
<rawString>Wei Chu and Seung T. Park. Personalized recommendation on dynamic content using predictive bilinear models. In Proceedings of the 18th international conference on World Wide Web (WWW), 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristian Danescu-Niculescu-Mizil</author>
<author>Gueorgi Kossinets</author>
<author>Jon Kleinberg</author>
<author>Lillian Lee</author>
</authors>
<title>How opinions are received by online communities: A case study on Amazon.com helpfulness votes.</title>
<date>2009</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>141--150</pages>
<contexts>
<context position="2559" citStr="Danescu-Niculescu-Mizil et al., 2009" startWordPosition="404" endWordPosition="407">ing (see Section 2 for a more detailed discussion), previous work has largely formulated review summarization as automatically or manually identify ratable aspects, and present overall sentiment polarity for each aspect (Hu and Liu, 2004; Popescu and Etzioni, 2005; Snyder and Barzilay, 2007; Titov and McDonald, 2008). A related line of research looked into predicting helpfulness of reviews in the hope of promoting those with better quality, where helpfulness is usually defined as some function over the percentage of users who found the review to be helpful (Kim et al., 2006; Liu et al., 2007; Danescu-Niculescu-Mizil et al., 2009). In short, the focus of previous work has been on distilling subjective information for an average user. Whether opinion consumers are looking for quality information or just wondering what other people think, each may have different purposes or preferences that is not well represented by a generic average user. If we think about how we deal with information content overflow on the Web, there have been two main frameworks to identify relevant information for each person. One is search. Indeed many top review sites allow users to search within reviews for a given entity. But this is only usefu</context>
<context position="10058" citStr="Danescu-Niculescu-Mizil et al., 2009" startWordPosition="1603" endWordPosition="1606">information in reviews, where helpfulness is either defined as the percentage of users who have voted the review to be helpful (Kim et al., 2006), or labeled by annotators according to a set of criteria (Liu et al., 2007). Our goal differs in that we look for personalized ranking (what a specific user might like) rather than generic quality (what an average user might like). Subsequently, there has been work that tried to predict similarly defined helpfulness scores using meta-information over the reviewer. For instance, whether the author has used his/her true name or where the user is from (Danescu-Niculescu-Mizil et al., 2009), as well as graph structure in the social network between reviewers (Lu et al., 2010). In this work, we simply use author identity to provide more context to the short text; in future work, additional metainformation over users can easily be incorporated via our model. As discussed in Section 1, whether a rater likes a comment or not may depend on whether they agree with the viewpoint expressed in the text and quality of the text. While previous work has not looked into the reader-comments relationship, there has been related work on identifying political orientations or viewpoints (Lin and H</context>
</contexts>
<marker>Danescu-Niculescu-Mizil, Kossinets, Kleinberg, Lee, 2009</marker>
<rawString>Cristian Danescu-Niculescu-Mizil, Gueorgi Kossinets, Jon Kleinberg, and Lillian Lee. How opinions are received by online communities: A case study on Amazon.com helpfulness votes. In Proceedings of WWW, pages 141–150, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abhinandan S Das</author>
<author>Mayur Datar</author>
<author>Ashutosh Garg</author>
<author>Shyam Rajaram</author>
</authors>
<title>Google news personalization: scalable online collaborative filtering.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th international conference on World Wide Web (WWW),</booktitle>
<contexts>
<context position="4464" citStr="Das et al., 2007" startWordPosition="713" endWordPosition="716">tes that allow helpfulness votes). Can we learn from users’ past preferences, so that when a user is reading a new article, we have a system that automatically ranks its comments according to their likelihood of being liked by the user? This can be used directly to create personalized presentation of comments (e.g., into a “like” column and a “dislike” column), as well as enabling down-stream applications such as personalized summarization. Recommending textual information has recently attracted more attention. So far, the focus has been mainly on recommending news articles (Ahn et al., 2007; Das et al., 2007). Our task differs in several aspects. Intuitively, recommending news articles is largely about identifying the topics of interest to a given user, and it is conceivable that unigram representation of full-length articles can reasonably capture that information. In our case, most comments for an article a user is reading are already of interest to that user topically. Which ones the user ends up liking may depend on several non-topical aspects of the text: whether the user agrees with the viewpoint expressed in the comment, whether the comment is convincing and well-written, etc. Previous work</context>
<context position="8797" citStr="Das et al. (2007)" startWordPosition="1395" endWordPosition="1398">i (2007) describes an approach to build user profile models for adaptive personalization in the context of mobile content access. Their approach is based on a hybrid model that combines content-based approaches with similarity methods used in recommender systems. This is further exemplified in the work by Ahn et al. (2007) where text processing techniques are used to build content profiles for users to recommend personalized news. In our experiments, we show that such approaches are inferior to our method. A content agnostic approach based on collaborative filtering techniques was proposed by Das et al. (2007); cold-start for new items/users was not their focus, but is important for our task — candidate comments for recommendation are often not in training data. As discussed in Section 1, previous work in opinion mining and sentiment analysis has addressed the information consumption challenge via review summarization. Discussion of early work in that direction can be found in Pang and Lee (2008). In this line of work, opinions for each given aspect are usually summarized as the average sentiment polarity associated with that aspect. Related to that, people have looked into predicting review helpfu</context>
</contexts>
<marker>Das, Datar, Garg, Rajaram, 2007</marker>
<rawString>Abhinandan S. Das, Mayur Datar, Ashutosh Garg, and Shyam Rajaram. Google news personalization: scalable online collaborative filtering. In Proceedings of the 16th international conference on World Wide Web (WWW), 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<volume>39</volume>
<contexts>
<context position="20750" citStr="Dempster et al., 1977" startWordPosition="3457" endWordPosition="3461">more general form, x0iG0Dxj can be written as x0iAxj, where Apu×p- is the matrix of regression weights (Chu and Park, 2009). However, since xi and xj are typically high dimensional, A can be a large matrix that needs to be learnt from data. To reduce dimensionality, one can decompose A as A = G0D, where the number of rows in D and G are small. Thus, instead of learning A, we learn a low-rank approximation of A. This ensures scalability and provides an attractive method to avoid over-fitting. 3.3 Model Fitting Model fitting for our model is based on the expectation-maximization (EM) algorithm (Dempster et al., 1977). For ease of exposition and space constraints, we only provide a sketch of the algorithm for the Gaussian case, the logistic model can be fitted along the same lines by using a variational approximation (see Agarwal and Chen (2009)). Let Y = {yij} denote the set of the observed ratings. In the EM parlance, this is “incomplete” 575 data that gets augmented with the latent factors Θ = {ui, vi, ej} to obtain the “complete” data. The goal of the EM algorithm is to find the parameter 77 = (g, d, G, D, a2α, a2β, a2u, a2v, a2y) that maximizes the “incomplete” data likelihood Pr(Y |77) = f Pr(Y , Θ|7</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39:1–38, 1977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Hoff</author>
</authors>
<title>Bilinear mixed-effects models for dyadic data.</title>
<date>2005</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>100</volume>
<issue>469</issue>
<contexts>
<context position="19724" citStr="Hoff (2005)" startWordPosition="3282" endWordPosition="3283">he prior distributions on αi,βj, ui, cj are zero, i.e., g = d = G = D = 0. Recent work clearly illustrates that this method obtains better predictive accuracy than classical collaborative filtering techniques based on item-item similarity (Bell et al. (2007)). The uc model: This is also a matrix factorization model but with priors based on regressions (i.e., non-zero g, d, G, D). It provides a mechanism to deal with both cold and warm-start scenarios in recommender applications (Agarwal and Chen (2009)). The vv model: This model assumes h(µij) = αi + γa(j) + v0iva(j). It was first proposed by Hoff (2005) to model interactions in social networks. The model was fitted to small datasets (at most a few hundred nodes) and the goal was to test certain hypotheses on social behavior, out-of-sample prediction was not considered. The low-rank bilinear regression model: Here, h(µij) = g0xi + d0xj + x0iG0Dxj. This is a regression model purely based on features with no peruser or per-comment latent factors. In a more general form, x0iG0Dxj can be written as x0iAxj, where Apu×p- is the matrix of regression weights (Chu and Park, 2009). However, since xi and xj are typically high dimensional, A can be a lar</context>
</contexts>
<marker>Hoff, 2005</marker>
<rawString>Peter D. Hoff. Bilinear mixed-effects models for dyadic data. Journal of the American Statistical Association, 100(469):286–295, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD),</booktitle>
<pages>168--177</pages>
<contexts>
<context position="2159" citStr="Hu and Liu, 2004" startWordPosition="339" endWordPosition="342">otably, during the short period of time for which a major event is active, news stories on one single event can easily attract over ten thousand comments on a popular online news site like Yahoo! News. One question becomes immediate: how can we help people consume such gigantic amount of opinionated information? One possibility is to take the summarization route. Briefly speaking (see Section 2 for a more detailed discussion), previous work has largely formulated review summarization as automatically or manually identify ratable aspects, and present overall sentiment polarity for each aspect (Hu and Liu, 2004; Popescu and Etzioni, 2005; Snyder and Barzilay, 2007; Titov and McDonald, 2008). A related line of research looked into predicting helpfulness of reviews in the hope of promoting those with better quality, where helpfulness is usually defined as some function over the percentage of users who found the review to be helpful (Kim et al., 2006; Liu et al., 2007; Danescu-Niculescu-Mizil et al., 2009). In short, the focus of previous work has been on distilling subjective information for an average user. Whether opinion consumers are looking for quality information or just wondering what other peo</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. Mining and summarizing customer reviews. In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), pages 168–177, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Kentaro Inui</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Identifying contradictory and contrastive relations between statements to outline web information on a given topic.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING): Posters,</booktitle>
<contexts>
<context position="10974" citStr="Kawahara et al., 2010" startWordPosition="1755" endWordPosition="1758"> 1, whether a rater likes a comment or not may depend on whether they agree with the viewpoint expressed in the text and quality of the text. While previous work has not looked into the reader-comments relationship, there has been related work on identifying political orientations or viewpoints (Lin and Hauptmann, 2006; Lin et al., 2006; Mullen and Malouf, 2006, 2008; Laver et al., 2003); whether a piece of text expresses support or opposition in congressional debates (Thomas et al., 2006) or online debates (Somasundaran and Wiebe, 2009, 2010); as well as identifying contrastive relationship (Kawahara et al., 2010). Note that it is not trivial to use previous work along this line to directly serve as sub-components in our setting. For instance, for work on identifying political orientations or viewpoints, the training data consists of text with the desired labels. In our setting, our labels come in the form of whether users liked or disliked a previous comment. In the simplest case, we might have pair-wise constraints on whether two pieces of text have the same viewpoints (i.e., liked or disliked by the same rater), which would yield a different learning problem akin to the metric learning problem; note</context>
</contexts>
<marker>Kawahara, Inui, Kurohashi, 2010</marker>
<rawString>Daisuke Kawahara, Kentaro Inui, and Sadao Kurohashi. Identifying contradictory and contrastive relations between statements to outline web information on a given topic. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING): Posters, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Patrick Pantel</author>
<author>Tim Chklovski</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Automatically assessing review helpfulness.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>423--430</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="2502" citStr="Kim et al., 2006" startWordPosition="396" endWordPosition="399">e summarization route. Briefly speaking (see Section 2 for a more detailed discussion), previous work has largely formulated review summarization as automatically or manually identify ratable aspects, and present overall sentiment polarity for each aspect (Hu and Liu, 2004; Popescu and Etzioni, 2005; Snyder and Barzilay, 2007; Titov and McDonald, 2008). A related line of research looked into predicting helpfulness of reviews in the hope of promoting those with better quality, where helpfulness is usually defined as some function over the percentage of users who found the review to be helpful (Kim et al., 2006; Liu et al., 2007; Danescu-Niculescu-Mizil et al., 2009). In short, the focus of previous work has been on distilling subjective information for an average user. Whether opinion consumers are looking for quality information or just wondering what other people think, each may have different purposes or preferences that is not well represented by a generic average user. If we think about how we deal with information content overflow on the Web, there have been two main frameworks to identify relevant information for each person. One is search. Indeed many top review sites allow users to search </context>
<context position="9566" citStr="Kim et al., 2006" startWordPosition="1522" endWordPosition="1525">ta. As discussed in Section 1, previous work in opinion mining and sentiment analysis has addressed the information consumption challenge via review summarization. Discussion of early work in that direction can be found in Pang and Lee (2008). In this line of work, opinions for each given aspect are usually summarized as the average sentiment polarity associated with that aspect. Related to that, people have looked into predicting review helpfulness given the textual information in reviews, where helpfulness is either defined as the percentage of users who have voted the review to be helpful (Kim et al., 2006), or labeled by annotators according to a set of criteria (Liu et al., 2007). Our goal differs in that we look for personalized ranking (what a specific user might like) rather than generic quality (what an average user might like). Subsequently, there has been work that tried to predict similarly defined helpfulness scores using meta-information over the reviewer. For instance, whether the author has used his/her true name or where the user is from (Danescu-Niculescu-Mizil et al., 2009), as well as graph structure in the social network between reviewers (Lu et al., 2010). In this work, we sim</context>
</contexts>
<marker>Kim, Pantel, Chklovski, Pennacchiotti, 2006</marker>
<rawString>Soo-Min Kim, Patrick Pantel, Tim Chklovski, and Marco Pennacchiotti. Automatically assessing review helpfulness. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 423–430, Sydney, Australia, July 2006. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Koren</author>
</authors>
<title>Factor in the neighbors: Scalable and accurate collaborative filtering.</title>
<date>2010</date>
<journal>ACM Transactions on Knowledge Discovery from Data (TKDD),</journal>
<volume>4</volume>
<pages>1556--4681</pages>
<contexts>
<context position="7159" citStr="Koren (2010)" startWordPosition="1135" endWordPosition="1137">llaborative filtering. While a proper survey is not possible here, we describe some of the approaches that are germane. Classical approaches in collaborative filtering are based on item-item/useruser similarity, these are nearest-neighbor methods where the response for a user-item pair is predicted based on a local neighborhood mean (Sarwar et al., 2001; Wang et al., 2006). In general, neighborhoods are defined by measuring similarities between users/items through correlation measures like Pearson, cosine similarities, etc. Better approaches to estimate similarities have also been proposed in Koren (2010). However, modern methods based on matrix factorization have been shown to outperform nearest neighbor methods (Salakhutdinov and Mnih, 2008a,b; Bell et al., 2007). Generalizations of matrix factorization to include both features and past ratings have been proposed (Agarwal and Chen, 2009; Stern et al., 2009). The approach in this paper is an extension where in addition to interactions among users and items (comments in our case), we also consider the authorship information. Three-way interactions were recently studied for personalized tag recommendation (Rendle and Lars, 2010). Their model wa</context>
</contexts>
<marker>Koren, 2010</marker>
<rawString>Y. Koren. Factor in the neighbors: Scalable and accurate collaborative filtering. ACM Transactions on Knowledge Discovery from Data (TKDD), 4 (1):1–24, 2010. ISSN 1556-4681.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Laver</author>
<author>Kenneth Benoit</author>
<author>John Garry</author>
</authors>
<title>Extracting policy positions from political texts using words as data.</title>
<date>2003</date>
<journal>American Political Science Review,</journal>
<volume>97</volume>
<issue>2</issue>
<contexts>
<context position="10742" citStr="Laver et al., 2003" startWordPosition="1720" endWordPosition="1723">ers (Lu et al., 2010). In this work, we simply use author identity to provide more context to the short text; in future work, additional metainformation over users can easily be incorporated via our model. As discussed in Section 1, whether a rater likes a comment or not may depend on whether they agree with the viewpoint expressed in the text and quality of the text. While previous work has not looked into the reader-comments relationship, there has been related work on identifying political orientations or viewpoints (Lin and Hauptmann, 2006; Lin et al., 2006; Mullen and Malouf, 2006, 2008; Laver et al., 2003); whether a piece of text expresses support or opposition in congressional debates (Thomas et al., 2006) or online debates (Somasundaran and Wiebe, 2009, 2010); as well as identifying contrastive relationship (Kawahara et al., 2010). Note that it is not trivial to use previous work along this line to directly serve as sub-components in our setting. For instance, for work on identifying political orientations or viewpoints, the training data consists of text with the desired labels. In our setting, our labels come in the form of whether users liked or disliked a previous comment. In the simples</context>
</contexts>
<marker>Laver, Benoit, Garry, 2003</marker>
<rawString>Michael Laver, Kenneth Benoit, and John Garry. Extracting policy positions from political texts using words as data. American Political Science Review, 97(2):311–331, 2003.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Wei-Hao Lin</author>
<author>Alexander Hauptmann</author>
</authors>
<title>Are these documents written from different perspectives? A test of different perspectives based on sta tistical distribution divergence.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING)/Proceedings of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1057--1064</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="10672" citStr="Lin and Hauptmann, 2006" startWordPosition="1707" endWordPosition="1710">l., 2009), as well as graph structure in the social network between reviewers (Lu et al., 2010). In this work, we simply use author identity to provide more context to the short text; in future work, additional metainformation over users can easily be incorporated via our model. As discussed in Section 1, whether a rater likes a comment or not may depend on whether they agree with the viewpoint expressed in the text and quality of the text. While previous work has not looked into the reader-comments relationship, there has been related work on identifying political orientations or viewpoints (Lin and Hauptmann, 2006; Lin et al., 2006; Mullen and Malouf, 2006, 2008; Laver et al., 2003); whether a piece of text expresses support or opposition in congressional debates (Thomas et al., 2006) or online debates (Somasundaran and Wiebe, 2009, 2010); as well as identifying contrastive relationship (Kawahara et al., 2010). Note that it is not trivial to use previous work along this line to directly serve as sub-components in our setting. For instance, for work on identifying political orientations or viewpoints, the training data consists of text with the desired labels. In our setting, our labels come in the form</context>
</contexts>
<marker>Lin, Hauptmann, 2006</marker>
<rawString>Wei-Hao Lin and Alexander Hauptmann. Are these documents written from different perspectives? A test of different perspectives based on sta tistical distribution divergence. In Proceedings of the International Conference on Computational Linguistics (COLING)/Proceedings of the Association for Computational Linguistics (ACL), pages 1057–1064, Sydney, Australia, July 2006. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei-Hao Lin</author>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Alexander Hauptmann</author>
</authors>
<title>Which side are you on? identifying perspectives at the document and sentence levels.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Natural Language Learning (CoNLL),</booktitle>
<contexts>
<context position="10690" citStr="Lin et al., 2006" startWordPosition="1711" endWordPosition="1714">ph structure in the social network between reviewers (Lu et al., 2010). In this work, we simply use author identity to provide more context to the short text; in future work, additional metainformation over users can easily be incorporated via our model. As discussed in Section 1, whether a rater likes a comment or not may depend on whether they agree with the viewpoint expressed in the text and quality of the text. While previous work has not looked into the reader-comments relationship, there has been related work on identifying political orientations or viewpoints (Lin and Hauptmann, 2006; Lin et al., 2006; Mullen and Malouf, 2006, 2008; Laver et al., 2003); whether a piece of text expresses support or opposition in congressional debates (Thomas et al., 2006) or online debates (Somasundaran and Wiebe, 2009, 2010); as well as identifying contrastive relationship (Kawahara et al., 2010). Note that it is not trivial to use previous work along this line to directly serve as sub-components in our setting. For instance, for work on identifying political orientations or viewpoints, the training data consists of text with the desired labels. In our setting, our labels come in the form of whether users </context>
</contexts>
<marker>Lin, Wilson, Wiebe, Hauptmann, 2006</marker>
<rawString>Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and Alexander Hauptmann. Which side are you on? identifying perspectives at the document and sentence levels. In Proceedings of the Conference on Natural Language Learning (CoNLL), 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingjing Liu</author>
<author>Yunbo Cao</author>
<author>Chin-Yew Lin</author>
<author>Yalou Huang</author>
<author>Ming Zhou</author>
</authors>
<title>Low-quality product review detection in opinion summarization.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>334--342</pages>
<note>Poster paper.</note>
<contexts>
<context position="2520" citStr="Liu et al., 2007" startWordPosition="400" endWordPosition="403">ute. Briefly speaking (see Section 2 for a more detailed discussion), previous work has largely formulated review summarization as automatically or manually identify ratable aspects, and present overall sentiment polarity for each aspect (Hu and Liu, 2004; Popescu and Etzioni, 2005; Snyder and Barzilay, 2007; Titov and McDonald, 2008). A related line of research looked into predicting helpfulness of reviews in the hope of promoting those with better quality, where helpfulness is usually defined as some function over the percentage of users who found the review to be helpful (Kim et al., 2006; Liu et al., 2007; Danescu-Niculescu-Mizil et al., 2009). In short, the focus of previous work has been on distilling subjective information for an average user. Whether opinion consumers are looking for quality information or just wondering what other people think, each may have different purposes or preferences that is not well represented by a generic average user. If we think about how we deal with information content overflow on the Web, there have been two main frameworks to identify relevant information for each person. One is search. Indeed many top review sites allow users to search within reviews for</context>
<context position="9642" citStr="Liu et al., 2007" startWordPosition="1536" endWordPosition="1539"> analysis has addressed the information consumption challenge via review summarization. Discussion of early work in that direction can be found in Pang and Lee (2008). In this line of work, opinions for each given aspect are usually summarized as the average sentiment polarity associated with that aspect. Related to that, people have looked into predicting review helpfulness given the textual information in reviews, where helpfulness is either defined as the percentage of users who have voted the review to be helpful (Kim et al., 2006), or labeled by annotators according to a set of criteria (Liu et al., 2007). Our goal differs in that we look for personalized ranking (what a specific user might like) rather than generic quality (what an average user might like). Subsequently, there has been work that tried to predict similarly defined helpfulness scores using meta-information over the reviewer. For instance, whether the author has used his/her true name or where the user is from (Danescu-Niculescu-Mizil et al., 2009), as well as graph structure in the social network between reviewers (Lu et al., 2010). In this work, we simply use author identity to provide more context to the short text; in future</context>
</contexts>
<marker>Liu, Cao, Lin, Huang, Zhou, 2007</marker>
<rawString>Jingjing Liu, Yunbo Cao, Chin-Yew Lin, Yalou Huang, and Ming Zhou. Low-quality product review detection in opinion summarization. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 334–342, 2007. Poster paper.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Lu</author>
</authors>
<title>Panayiotis Tsaparas, Alexandros Ntoulas, and Livia Polanyi. Exploiting social context for review quality prediction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th International World Wide Web Conference (WWW),</booktitle>
<marker>Lu, 2010</marker>
<rawString>Yue Lu, Panayiotis Tsaparas, Alexandros Ntoulas, and Livia Polanyi. Exploiting social context for review quality prediction. In Proceedings of the 19th International World Wide Web Conference (WWW), 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P McCullagh</author>
<author>J A Nelder</author>
</authors>
<title>Generalized Linear Models.</title>
<date>1989</date>
<publisher>Chapman &amp; Hall/CRC,</publisher>
<contexts>
<context position="13592" citStr="McCullagh and Nelder, 1989" startWordPosition="2207" endWordPosition="2210">sion pc) denote feature vectors of user i and comment j respectively. For example, xi can be the bag of words representation (a sparse vector) inferred through text analysis on comments voted positively by user i in the past, and xj can be the bag of words representation for comment j. We use a(j) to denote the author of comment j, and use µij to denote the mean rating by rater i on comment j, i.e., µij = E(yij). Of course it is impossible to estimate µij empirically since each user i usually rates a comment j at most once. Model specification: We work in a generalized linear model framework (McCullagh and Nelder, 1989) that assumes µij (or some monotone function h of µij) is an additive function of (1) the rater bias αi of user i since some users may have a tendency of rating comments more positively or negatively than others, (2) popularity βj of comment j, which could reflect the quality of the comment in this setting, and (3) the author reputation γa(j) of user a(j) since comments by a reputed author may in general get more positive ratings. Thus, the overall bias is αi + βj + γa(j). In addition to the bias, we include terms that capture interactions among entities (raters, authors, comments). Indeed, ca</context>
</contexts>
<marker>McCullagh, Nelder, 1989</marker>
<rawString>P. McCullagh and J. A. Nelder. Generalized Linear Models. Chapman &amp; Hall/CRC, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony Mullen</author>
<author>Robert Malouf</author>
</authors>
<title>A preliminary investigation into sentiment analysis of informal political discourse.</title>
<date>2006</date>
<booktitle>In AAAI Symposium on Computational Approaches to Analysing Weblogs (AAAICAAW),</booktitle>
<pages>159--162</pages>
<contexts>
<context position="10715" citStr="Mullen and Malouf, 2006" startWordPosition="1715" endWordPosition="1718">e social network between reviewers (Lu et al., 2010). In this work, we simply use author identity to provide more context to the short text; in future work, additional metainformation over users can easily be incorporated via our model. As discussed in Section 1, whether a rater likes a comment or not may depend on whether they agree with the viewpoint expressed in the text and quality of the text. While previous work has not looked into the reader-comments relationship, there has been related work on identifying political orientations or viewpoints (Lin and Hauptmann, 2006; Lin et al., 2006; Mullen and Malouf, 2006, 2008; Laver et al., 2003); whether a piece of text expresses support or opposition in congressional debates (Thomas et al., 2006) or online debates (Somasundaran and Wiebe, 2009, 2010); as well as identifying contrastive relationship (Kawahara et al., 2010). Note that it is not trivial to use previous work along this line to directly serve as sub-components in our setting. For instance, for work on identifying political orientations or viewpoints, the training data consists of text with the desired labels. In our setting, our labels come in the form of whether users liked or disliked a previ</context>
</contexts>
<marker>Mullen, Malouf, 2006</marker>
<rawString>Tony Mullen and Robert Malouf. A preliminary investigation into sentiment analysis of informal political discourse. In AAAI Symposium on Computational Approaches to Analysing Weblogs (AAAICAAW), pages 159–162, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony Mullen</author>
<author>Robert Malouf</author>
</authors>
<title>Taking sides: User classification for informal online political discourse.</title>
<date>2008</date>
<journal>Internet Research,</journal>
<volume>18</volume>
<marker>Mullen, Malouf, 2008</marker>
<rawString>Tony Mullen and Robert Malouf. Taking sides: User classification for informal online political discourse. Internet Research, 18:177–190, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="5162" citStr="Pang and Lee, 2008" startWordPosition="825" endWordPosition="828">is largely about identifying the topics of interest to a given user, and it is conceivable that unigram representation of full-length articles can reasonably capture that information. In our case, most comments for an article a user is reading are already of interest to that user topically. Which ones the user ends up liking may depend on several non-topical aspects of the text: whether the user agrees with the viewpoint expressed in the comment, whether the comment is convincing and well-written, etc. Previous work has shown that such analysis can be more difficult than topic-based analysis (Pang and Lee, 2008), and we have the additional challenge that comments are typically much shorter than full-length articles. However, the difficulty in analyzing the textual information in comments can be alleviated by additional contextual information such as author identities. If between a pair of users one consistently likes or dislikes the other, then at least for the heavy users, this authorship information alone could be highly informative. Indeed, previous work in collaborative filtering has usually found no additional gain from leveraging content information when entity-level preference information is a</context>
<context position="9191" citStr="Pang and Lee (2008)" startWordPosition="1460" endWordPosition="1463"> for users to recommend personalized news. In our experiments, we show that such approaches are inferior to our method. A content agnostic approach based on collaborative filtering techniques was proposed by Das et al. (2007); cold-start for new items/users was not their focus, but is important for our task — candidate comments for recommendation are often not in training data. As discussed in Section 1, previous work in opinion mining and sentiment analysis has addressed the information consumption challenge via review summarization. Discussion of early work in that direction can be found in Pang and Lee (2008). In this line of work, opinions for each given aspect are usually summarized as the average sentiment polarity associated with that aspect. Related to that, people have looked into predicting review helpfulness given the textual information in reviews, where helpfulness is either defined as the percentage of users who have voted the review to be helpful (Kim et al., 2006), or labeled by annotators according to a set of criteria (Liu et al., 2007). Our goal differs in that we look for personalized ranking (what a specific user might like) rather than generic quality (what an average user might</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2):1–135, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Oren Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Pro cessing (HLT/EMNLP),</booktitle>
<contexts>
<context position="2186" citStr="Popescu and Etzioni, 2005" startWordPosition="343" endWordPosition="346"> short period of time for which a major event is active, news stories on one single event can easily attract over ten thousand comments on a popular online news site like Yahoo! News. One question becomes immediate: how can we help people consume such gigantic amount of opinionated information? One possibility is to take the summarization route. Briefly speaking (see Section 2 for a more detailed discussion), previous work has largely formulated review summarization as automatically or manually identify ratable aspects, and present overall sentiment polarity for each aspect (Hu and Liu, 2004; Popescu and Etzioni, 2005; Snyder and Barzilay, 2007; Titov and McDonald, 2008). A related line of research looked into predicting helpfulness of reviews in the hope of promoting those with better quality, where helpfulness is usually defined as some function over the percentage of users who found the review to be helpful (Kim et al., 2006; Liu et al., 2007; Danescu-Niculescu-Mizil et al., 2009). In short, the focus of previous work has been on distilling subjective information for an average user. Whether opinion consumers are looking for quality information or just wondering what other people think, each may have di</context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>Ana-Maria Popescu and Oren Etzioni. Extracting product features and opinions from reviews. In Proceedings of the Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Pro cessing (HLT/EMNLP), 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steffen Rendle</author>
<author>Schmidt-Thie Lars</author>
</authors>
<title>Pairwise interaction tensor factorization for personalized tag recommendation.</title>
<date>2010</date>
<booktitle>In Proceedings of the third ACM international conference on Web search and data mining, WSDM ’10,</booktitle>
<pages>81--90</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA,</location>
<contexts>
<context position="7743" citStr="Rendle and Lars, 2010" startWordPosition="1221" endWordPosition="1224">ave also been proposed in Koren (2010). However, modern methods based on matrix factorization have been shown to outperform nearest neighbor methods (Salakhutdinov and Mnih, 2008a,b; Bell et al., 2007). Generalizations of matrix factorization to include both features and past ratings have been proposed (Agarwal and Chen, 2009; Stern et al., 2009). The approach in this paper is an extension where in addition to interactions among users and items (comments in our case), we also consider the authorship information. Three-way interactions were recently studied for personalized tag recommendation (Rendle and Lars, 2010). Their model was based on the sum of two-way interactions, and was trained by using pairwise tag preferences for each (user, item) pair. However, no features were considered, which is an important consideration for us. We show using both text and authorship provides the best performance. Our work is also related to news personalization that has received increasing attention in the last few 572 years. For instance, Billsus and Pazanni (2007) describes an approach to build user profile models for adaptive personalization in the context of mobile content access. Their approach is based on a hybr</context>
</contexts>
<marker>Rendle, Lars, 2010</marker>
<rawString>Steffen Rendle and Schmidt-Thie Lars. Pairwise interaction tensor factorization for personalized tag recommendation. In Proceedings of the third ACM international conference on Web search and data mining, WSDM ’10, pages 81–90, New York, NY, USA, 2010. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Salakhutdinov</author>
<author>A Mnih</author>
</authors>
<title>Bayesian probabilistic matrix factorization using Markov chain Monte Carlo.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>880--887</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="7299" citStr="Salakhutdinov and Mnih, 2008" startWordPosition="1153" endWordPosition="1156">sical approaches in collaborative filtering are based on item-item/useruser similarity, these are nearest-neighbor methods where the response for a user-item pair is predicted based on a local neighborhood mean (Sarwar et al., 2001; Wang et al., 2006). In general, neighborhoods are defined by measuring similarities between users/items through correlation measures like Pearson, cosine similarities, etc. Better approaches to estimate similarities have also been proposed in Koren (2010). However, modern methods based on matrix factorization have been shown to outperform nearest neighbor methods (Salakhutdinov and Mnih, 2008a,b; Bell et al., 2007). Generalizations of matrix factorization to include both features and past ratings have been proposed (Agarwal and Chen, 2009; Stern et al., 2009). The approach in this paper is an extension where in addition to interactions among users and items (comments in our case), we also consider the authorship information. Three-way interactions were recently studied for personalized tag recommendation (Rendle and Lars, 2010). Their model was based on the sum of two-way interactions, and was trained by using pairwise tag preferences for each (user, item) pair. However, no featur</context>
</contexts>
<marker>Salakhutdinov, Mnih, 2008</marker>
<rawString>R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization using Markov chain Monte Carlo. In Proceedings of the 25th international conference on Machine learning, pages 880–887. ACM, 2008a.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Salakhutdinov</author>
<author>A Mnih</author>
</authors>
<title>Probabilistic matrix factorization. Advances in neural information processing systems,</title>
<pages>20--1257</pages>
<marker>Salakhutdinov, Mnih, </marker>
<rawString>R. Salakhutdinov and A. Mnih. Probabilistic matrix factorization. Advances in neural information processing systems, 20:1257–1264, 2008b.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Badrul Sarwar</author>
<author>George Karypis</author>
<author>Joseph Konstan</author>
<author>John Reidl</author>
</authors>
<title>Item-based collaborative filtering recommendation algorithms.</title>
<date>2001</date>
<booktitle>In WWW ’01: Proceedings of the 10th international conference on World Wide Web,</booktitle>
<pages>285--295</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="6902" citStr="Sarwar et al., 2001" startWordPosition="1097" endWordPosition="1100">are two main bodies of related work: our problem formulation is closer to collaborative filtering, while the nature of the text we are dealing with has more in common with opinion mining and sentiment analysis. Our approach is related to a large body of work in collaborative filtering. While a proper survey is not possible here, we describe some of the approaches that are germane. Classical approaches in collaborative filtering are based on item-item/useruser similarity, these are nearest-neighbor methods where the response for a user-item pair is predicted based on a local neighborhood mean (Sarwar et al., 2001; Wang et al., 2006). In general, neighborhoods are defined by measuring similarities between users/items through correlation measures like Pearson, cosine similarities, etc. Better approaches to estimate similarities have also been proposed in Koren (2010). However, modern methods based on matrix factorization have been shown to outperform nearest neighbor methods (Salakhutdinov and Mnih, 2008a,b; Bell et al., 2007). Generalizations of matrix factorization to include both features and past ratings have been proposed (Agarwal and Chen, 2009; Stern et al., 2009). The approach in this paper is a</context>
</contexts>
<marker>Sarwar, Karypis, Konstan, Reidl, 2001</marker>
<rawString>Badrul Sarwar, George Karypis, Joseph Konstan, and John Reidl. Item-based collaborative filtering recommendation algorithms. In WWW ’01: Proceedings of the 10th international conference on World Wide Web, pages 285–295. ACM, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
</authors>
<title>Multiple aspect ranking using the Good Grief algorithm.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Human Language Technology/North American Chapter of the ACL Conference (HLT- NAACL),</booktitle>
<pages>300--307</pages>
<contexts>
<context position="2213" citStr="Snyder and Barzilay, 2007" startWordPosition="347" endWordPosition="350">hich a major event is active, news stories on one single event can easily attract over ten thousand comments on a popular online news site like Yahoo! News. One question becomes immediate: how can we help people consume such gigantic amount of opinionated information? One possibility is to take the summarization route. Briefly speaking (see Section 2 for a more detailed discussion), previous work has largely formulated review summarization as automatically or manually identify ratable aspects, and present overall sentiment polarity for each aspect (Hu and Liu, 2004; Popescu and Etzioni, 2005; Snyder and Barzilay, 2007; Titov and McDonald, 2008). A related line of research looked into predicting helpfulness of reviews in the hope of promoting those with better quality, where helpfulness is usually defined as some function over the percentage of users who found the review to be helpful (Kim et al., 2006; Liu et al., 2007; Danescu-Niculescu-Mizil et al., 2009). In short, the focus of previous work has been on distilling subjective information for an average user. Whether opinion consumers are looking for quality information or just wondering what other people think, each may have different purposes or prefere</context>
</contexts>
<marker>Snyder, Barzilay, 2007</marker>
<rawString>Benjamin Snyder and Regina Barzilay. Multiple aspect ranking using the Good Grief algorithm. In Proceedings of the Joint Human Language Technology/North American Chapter of the ACL Conference (HLT- NAACL), pages 300–307, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Janyce Wiebe</author>
</authors>
<title>Recognizing stances in online debates.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<contexts>
<context position="10894" citStr="Somasundaran and Wiebe, 2009" startWordPosition="1743" endWordPosition="1746">formation over users can easily be incorporated via our model. As discussed in Section 1, whether a rater likes a comment or not may depend on whether they agree with the viewpoint expressed in the text and quality of the text. While previous work has not looked into the reader-comments relationship, there has been related work on identifying political orientations or viewpoints (Lin and Hauptmann, 2006; Lin et al., 2006; Mullen and Malouf, 2006, 2008; Laver et al., 2003); whether a piece of text expresses support or opposition in congressional debates (Thomas et al., 2006) or online debates (Somasundaran and Wiebe, 2009, 2010); as well as identifying contrastive relationship (Kawahara et al., 2010). Note that it is not trivial to use previous work along this line to directly serve as sub-components in our setting. For instance, for work on identifying political orientations or viewpoints, the training data consists of text with the desired labels. In our setting, our labels come in the form of whether users liked or disliked a previous comment. In the simplest case, we might have pair-wise constraints on whether two pieces of text have the same viewpoints (i.e., liked or disliked by the same rater), which wo</context>
</contexts>
<marker>Somasundaran, Wiebe, 2009</marker>
<rawString>Swapna Somasundaran and Janyce Wiebe. Recognizing stances in online debates. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Janyce Wiebe</author>
</authors>
<title>Recognizing stances in ideological on-line debates.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,</booktitle>
<marker>Somasundaran, Wiebe, 2010</marker>
<rawString>Swapna Somasundaran and Janyce Wiebe. Recognizing stances in ideological on-line debates. In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David H Stern</author>
<author>Ralf Herbrich</author>
<author>Thore Graepel</author>
</authors>
<title>Matchbox: large scale online bayesian recommendations.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th international conference on World Wide Web (WWW),</booktitle>
<contexts>
<context position="7469" citStr="Stern et al., 2009" startWordPosition="1180" endWordPosition="1183">ed on a local neighborhood mean (Sarwar et al., 2001; Wang et al., 2006). In general, neighborhoods are defined by measuring similarities between users/items through correlation measures like Pearson, cosine similarities, etc. Better approaches to estimate similarities have also been proposed in Koren (2010). However, modern methods based on matrix factorization have been shown to outperform nearest neighbor methods (Salakhutdinov and Mnih, 2008a,b; Bell et al., 2007). Generalizations of matrix factorization to include both features and past ratings have been proposed (Agarwal and Chen, 2009; Stern et al., 2009). The approach in this paper is an extension where in addition to interactions among users and items (comments in our case), we also consider the authorship information. Three-way interactions were recently studied for personalized tag recommendation (Rendle and Lars, 2010). Their model was based on the sum of two-way interactions, and was trained by using pairwise tag preferences for each (user, item) pair. However, no features were considered, which is an important consideration for us. We show using both text and authorship provides the best performance. Our work is also related to news per</context>
</contexts>
<marker>Stern, Herbrich, Graepel, 2009</marker>
<rawString>David H. Stern, Ralf Herbrich, and Thore Graepel. Matchbox: large scale online bayesian recommendations. In Proceedings of the 18th international conference on World Wide Web (WWW), 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Thomas</author>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Get out the vote: Determining support or opposition from Congressional floor-debate transcripts.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>327--335</pages>
<contexts>
<context position="10846" citStr="Thomas et al., 2006" startWordPosition="1736" endWordPosition="1739"> text; in future work, additional metainformation over users can easily be incorporated via our model. As discussed in Section 1, whether a rater likes a comment or not may depend on whether they agree with the viewpoint expressed in the text and quality of the text. While previous work has not looked into the reader-comments relationship, there has been related work on identifying political orientations or viewpoints (Lin and Hauptmann, 2006; Lin et al., 2006; Mullen and Malouf, 2006, 2008; Laver et al., 2003); whether a piece of text expresses support or opposition in congressional debates (Thomas et al., 2006) or online debates (Somasundaran and Wiebe, 2009, 2010); as well as identifying contrastive relationship (Kawahara et al., 2010). Note that it is not trivial to use previous work along this line to directly serve as sub-components in our setting. For instance, for work on identifying political orientations or viewpoints, the training data consists of text with the desired labels. In our setting, our labels come in the form of whether users liked or disliked a previous comment. In the simplest case, we might have pair-wise constraints on whether two pieces of text have the same viewpoints (i.e.</context>
</contexts>
<marker>Thomas, Pang, Lee, 2006</marker>
<rawString>Matt Thomas, Bo Pang, and Lillian Lee. Get out the vote: Determining support or opposition from Congressional floor-debate transcripts. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 327–335, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Ryan McDonald</author>
</authors>
<title>A joint model of text and aspect ratings for sentiment summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL),</booktitle>
<contexts>
<context position="2240" citStr="Titov and McDonald, 2008" startWordPosition="351" endWordPosition="354">e, news stories on one single event can easily attract over ten thousand comments on a popular online news site like Yahoo! News. One question becomes immediate: how can we help people consume such gigantic amount of opinionated information? One possibility is to take the summarization route. Briefly speaking (see Section 2 for a more detailed discussion), previous work has largely formulated review summarization as automatically or manually identify ratable aspects, and present overall sentiment polarity for each aspect (Hu and Liu, 2004; Popescu and Etzioni, 2005; Snyder and Barzilay, 2007; Titov and McDonald, 2008). A related line of research looked into predicting helpfulness of reviews in the hope of promoting those with better quality, where helpfulness is usually defined as some function over the percentage of users who found the review to be helpful (Kim et al., 2006; Liu et al., 2007; Danescu-Niculescu-Mizil et al., 2009). In short, the focus of previous work has been on distilling subjective information for an average user. Whether opinion consumers are looking for quality information or just wondering what other people think, each may have different purposes or preferences that is not well repre</context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>Ivan Titov and Ryan McDonald. A joint model of text and aspect ratings for sentiment summarization. In Proceedings of the Association for Computational Linguistics (ACL), 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Wang</author>
<author>Arjen P de Vries</author>
<author>Marcel J T Reinders</author>
</authors>
<title>Unifying user-based and item-based collaborative filtering approaches by similarity fusion.</title>
<date>2006</date>
<booktitle>In SIGIR ’06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>501--508</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA,</location>
<marker>Wang, de Vries, Reinders, 2006</marker>
<rawString>Jun Wang, Arjen P. de Vries, and Marcel J. T. Reinders. Unifying user-based and item-based collaborative filtering approaches by similarity fusion. In SIGIR ’06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 501–508, New York, NY, USA, 2006. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>