<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.997799">
Not-So-Latent Dirichlet Allocation:
Collapsed Gibbs Sampling Using Human Judgments
</title>
<author confidence="0.972515">
Jonathan Chang
</author>
<affiliation confidence="0.760782">
Facebook
</affiliation>
<address confidence="0.787484">
1601 S. California Ave.
Palo Alto, CA 94304
</address>
<email confidence="0.9985">
jonchang@facebook.com
</email>
<sectionHeader confidence="0.997384" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998393333333333">
Probabilistic topic models are a popular tool
for the unsupervised analysis of text, providing
both a predictive model of future text and a la-
tent topic representation of the corpus. Recent
studies have found that while there are sugges-
tive connections between topic models and the
way humans interpret data, these two often dis-
agree. In this paper, we explore this disagree-
ment from the perspective of the learning pro-
cess rather than the output. We present a novel
task, tag-and-cluster, which asks subjects to
simultaneously annotate documents and cluster
those annotations. We use these annotations
as a novel approach for constructing a topic
model, grounded in human interpretations of
documents. We demonstrate that these topic
models have features which distinguish them
from traditional topic models.
</bodyText>
<sectionHeader confidence="0.999514" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999533952380953">
Probabilistic topic models have become popular tools
for the unsupervised analysis of large document
collections (Deerwester et al., 1990; Griffiths and
Steyvers, 2002; Blei and Lafferty, 2009). These mod-
els posit a set of latent topics, multinomial distribu-
tions over words, and assume that each document
can be described as a mixture of these topics. With
algorithms for fast approximate posterior inference,
we can use topic models to discover both the topics
and an assignment of topics to documents from a
collection of documents. (See Figure 1.)
These modeling assumptions are useful in the
sense that, empirically, they lead to good models
of documents (Wallach et al., 2009). However, re-
cent work has explored how these assumptions corre-
spond to humans’ understanding of language (Chang
et al., 2009; Griffiths and Steyvers, 2006; Mei et al.,
2007). Focusing on the latent space, i.e., the inferred
mappings between topics and words and between
documents and topics, this work has discovered that
although there are some suggestive correspondences
</bodyText>
<page confidence="0.986548">
131
</page>
<bodyText confidence="0.998628571428571">
between human semantics and topic models, they are
often discordant.
In this paper we build on this work to further
explore how humans relate to topic models. But
whereas previous work has focused on the results
of topic models, here we focus on the process by
which these models are learned. Topic models lend
themselves to sequential procedures through which
the latent space is inferred; these procedures are in
effect programmatic encodings of the modeling as-
sumptions. By substituting key steps in this program
with human judgments, we obtain insights into the
semantic model conceived by humans.
Here we present a novel task, tag-and-cluster,
which asks subjects to simultaneously annotate a doc-
ument and cluster that annotation. This task simulates
the sampling step of the collapsed Gibbs sampler (de-
scribed in the next section), except that the posterior
defined by the model has been replaced by human
judgments. The task is quick to complete and is
robust against noise. We report the results of a large-
scale human study of this task, and show that humans
are indeed able to construct a topic model in this fash-
ion, and that the learned topic model has semantic
properties distinct from existing topic models. We
also demonstrate that the judgments can be used to
guide computer-learned topic models towards models
which are more concordant with human intuitions.
</bodyText>
<sectionHeader confidence="0.957833" genericHeader="method">
2 Topic models and inference
</sectionHeader>
<bodyText confidence="0.999834363636364">
Topic models posit that each document is expressed
as a mixture of topics. These topic proportions are
drawn once per document, and the topics are shared
across the corpus. In this paper we focus on Latent
Dirichlet allocation (LDA) (Blei et al., 2003) a topic
model which treats each document’s topic assign-
ment as a multinomial random variable drawn from
a symmetric Dirichlet prior. LDA, when applied to a
collection of documents, will build a latent space: a
collection of topics for the corpus and a collection of
topic proportions for each of its documents.
</bodyText>
<note confidence="0.98373">
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 131–138,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<figure confidence="0.870069625">
TOPIC 2
&amp;quot;BUSINESS&amp;quot;
TOPIC 1
&amp;quot;TECHNOLOGY&amp;quot;
Forget the
Bootleg, Just
Download the
Movie Legally
Red Light, Green
Light: A
2-Tone L.E.D. to
Simplify Screens
Internet portals
begin to distinguish
among themselves
as shopping malls
Stock Trades: A
Better Deal For
Investors Isn&apos;t
Simple
The Shape of
Cinema,
Transformed At
the Click of a
Mouse A Peaceful Crew
Puts Muppets
Where Its Mouth Is
Multiplex Heralded
As Linchpin To
Growth
TOPIC 3
&amp;quot;ENTERTAINMENT&amp;quot;
computer,
technology,
system,
service, site,
phone,
internet,
machine
TOPIC 1
TOPIC 2
TOPIC 3
sell, sale,
store, product,
business,
advertising,
market,
consumer
play, film,
movie, theater,
production,
star, director,
stage
(a) Topics (b) Document Assignments to Topics
m each document’s title shows the document’s position in the topic space.
LDA can
be described by the following generative
process:
—
2. For each document d,
(a) Draw topic proportions
—
(b) For each word
i. Draw topic assignment
ii. Draw word wd,n —
βk
Dir(η)
θd
Dir(α)
wd,n,
zd,n—Mult(θd)
Mult(βzd,n)
wd,n
zd,n
ed
N
D
βk
η
K
</figure>
<figureCaption confidence="0.998944">
Figure 1: The latent space of a topic model consists of topics, which are distributions over words, and a distribution
</figureCaption>
<figure confidence="0.670886777777778">
over these topics for each document. On the left are three topics from a fifty topic LDA model trained on articles from
the New York Times. On the right is a simplex depicting the distribution over topics associated with seven documents.
The line fro
1. For each topic k,
(a) Draw topic
This process is depicted graphically in Figure 2. The
parameters of the model are the number of topics,
K, as well as the Dirichlet priors on the topic-word
α
</figure>
<figureCaption confidence="0.87993">
Figure 2: A graphical model depiction of latent Dirichlet
allocation (LDA). Plates denote replication. The shaded
</figureCaption>
<bodyText confidence="0.993356571428571">
circle denotes an observed variable an
d unshaded circles
denote hidden variables.
the topic assignments zd,n, can be expressed as
p(z|α,η, w) a
Yd
where nd,k denotes the number of words in document
</bodyText>
<equation confidence="0.508943">
p(zd,i =
,
</equation>
<bodyText confidence="0.935219318181818">
distributions and document-topic distributions,
and
The only observed variables of the model are
the words, wd,n. The remaining variables must be
learned.
There are several techniques for performing pos-
terior inference, i.e., inferring the distribution over
hidden variables given a collection of documents, in-
cluding variational inference (Blei et al., 2003) and
Gibbs sampling (Griffiths and Steyvers, 2006). In the
sequel, we focus on the latter approach.
Collapsed Gibbs sampling for LDA treats the topic-
word and document-topic distributions,
and
as
nuisance variables to be marginali
α
η.
θd
βk,
zed out. The poste-
rior distribution over the remaining latent variables,
</bodyText>
<equation confidence="0.990413333333333">
� �
Γ(nd,k + αk)Γ(ηwd,n + nwd,n,k)
Γ(P w nw,k + ηw)
</equation>
<bodyText confidence="0.99757175">
d assigned to topic k an
d nw,k the number of times
word w is assigned to topic k. This leads to the
sampling equations,
</bodyText>
<equation confidence="0.904813533333333">
k|α, η, w, zy) a
-d,i
�d,i
wd,i + nw
,
,k ( )
(nd k +
) ,d i 1
nw,k +
η
d
i
αk
Pw
ηw
</equation>
<bodyText confidence="0.992902">
where the superscript ted, i indicates that these statis-
</bodyText>
<equation confidence="0.6527935">
Y
k
</equation>
<page confidence="0.978969">
132
</page>
<bodyText confidence="0.9996858">
tics should exclude the current variable under consid-
eration, zd,i.
In essence, the model performs inference by look-
ing at each word in succession, and probabilistically
assigning it to a topic according to Equation 1. Equa-
tion 1 is derived through the modeling assumptions
and choice of parameters. By replacing Equation 1
with a different equation or with empirical observa-
tions, we may construct new models which reflect
different assumptions about the underlying data.
</bodyText>
<sectionHeader confidence="0.995414" genericHeader="method">
3 Constructing topics using human judg-
ments
</sectionHeader>
<bodyText confidence="0.999971866666667">
In this section we propose a task which creates a
formal setting where humans can create a latent
space representation of the corpus. Our task, tag-and-
cluster, replaces the collapsed Gibbs sampling step
of Equation 1 with a human judgment. In essence,
we are constructing a gold-standard series of samples
from the posterior.1
Figure 3 shows how tag-and-cluster is presented
to users. The user is shown a document along with its
title; the document is randomly selected from a pool
of available documents. The user is asked to select
a word from the document which is discriminative,
i.e, a word which would help someone looking for
the document find it. Once the word is selected, the
user is then asked to assign the word to the topic
which best suits the sense of the word used in the
document. Users are specifically instructed to focus
on the meanings of words, not their syntactic usage
or orthography.
The user assigns a word to a topic by selecting an
entry out of a menu of topics. Each topic is repre-
sented by the five words occuring most frequently in
that topic. The order of the topics presented to the
user is determined by the number of words in that
document already assigned to each topic. Once an
instance of a word in a document has been assigned,
it cannot be reassigned and will be marked in red
when subsequent users encounter this document. In
practice, we also prohibit users from selecting infre-
quently occurring words and stop words.
</bodyText>
<footnote confidence="0.97923725">
1Additionally, since Gibbs sampling is by nature stochastic,
we believe that the task is robust against small perturbations in
the quality of the assignments, so long as in aggregate they tend
toward the mode.
</footnote>
<sectionHeader confidence="0.996798" genericHeader="method">
4 Experimental results
</sectionHeader>
<bodyText confidence="0.999995461538461">
We conducted our experiments using Amazon Me-
chanical Turk, which allows workers (our pool of
prospective subjects) to perform small jobs for a fee
through a Web interface. No specialized training
or knowledge is typically expected of the workers.
Amazon Mechanical Turk has been successfully used
in the past to develop gold-standard data for natural
language processing (Snow et al., 2008).
We prepare two randomly-chosen, 100-document
subsets of English Wikipedia. For convenience, we
denote these two sets of documents as set] and set2.
For each document, we keep only the first 150 words
for our experiments. Because of the encyclopedic
nature of the corpus, the first 150 words typically
provides a broad overview of the themes in the article.
We also removed from the corpus stop words and
words which occur infrequently2, leading to a lexicon
of 8263 words. After this pruning set] contained
11614 words and set2 contained 11318 words.
Workers were asked to perform twenty of the tag-
gings described in Section 3 for each task; workers
were paid $0.25 for each such task. The number of
latent topics, K, is a free parameter. Here we explore
two values of this parameter, K = 10 and K = 15,
leading to a total of four experiments — two for each
set of documents and two for each value of K.
</bodyText>
<subsectionHeader confidence="0.998846">
4.1 Tagging behavior
</subsectionHeader>
<bodyText confidence="0.999969">
For each experiment we issued 100 HITs, leading to
a total of 2000 tags per experiment. Figure 4 shows
the number of HITs performed per person in each
experiment. Between 12 and 30 distinct workers par-
ticipated in each experiment. The number of HITs
performed per person is heavily skewed, with the
most active participants completing an order of mag-
nitude more HITs than other particpants.
Figure 5 shows the amount of time taken per tag,
in log seconds. Each color represents a different
experiment. The bulk of the tags took less than a
minute to perform and more than a few seconds.
</bodyText>
<subsectionHeader confidence="0.996626">
4.2 Comparison with LDA
</subsectionHeader>
<bodyText confidence="0.7458105">
Learned topics As described in Section 3, the tag-
and-cluster task is a way of allowing humans to con-
</bodyText>
<footnote confidence="0.770443333333333">
2Infrequently occurring words were identified as those ap-
pearing fewer than eight times on a larger collection of 7726
articles.
</footnote>
<page confidence="0.995986">
133
</page>
<figureCaption confidence="0.9972883">
Figure 3: Screenshots of our task. In the center, the document along with its title is shown. Words which cannot be
selected, e.g., distractors and words previously selected, are shown in red. Once a word is selected, the user is asked to
find a topic in which to place the word. The user selects a topic by clicking on an entry in a menu of topics, where each
topic is expressed by the five words which occur most frequently in that topic.
Figure 4: The number of HITs performed (y-axis) by each participant (x-axis). Between 12 and 30 people participated
in each experiment.
Figure 6: A comparison of the entropy of distributions drawn from a Dirichlet distribution versus the entropy of the
topic proportions inferred by workers. Each column of the boxplot shows the distribution of entropies for 100 draws
from a Dirichlet distribution with parameter α. The two rightmost columns show the distribution of the entropy of the
topic proportions inferred by workers on set] and set2. The α of workers typically falls between 0.2 and 0.5.
</figureCaption>
<figure confidence="0.998588727272727">
0 5 10 15 20 25 30 0 2 4 6 8 10 12 0 5 10 15 20 0 5 10 15 20 25 30
(a) set], K = 10 (b) set], K = 15 (c) set2, K = 10 (d) set2, K = 15
25
20
15
10
5
0
60
50
40
30
20
10
0
50
40
30
20
10
0
20
15
10
5
0
0.05 0.1 0.2 0.5 1 2 set1 set2 0.05 0.1 0.2 0.5 1 2 set1 set2
α α
(a) K = 10 (b) K = 15
entropy
2.5
0.5
1.5
2
0
1
entropy
2.5
0.5
1.5
3
2
0
1
</figure>
<page confidence="0.99494">
134
</page>
<tableCaption confidence="0.994271">
Table 1: The five words with the highest probability mass in each topic inferred by humans using the task described in
Section 3. Each subtable shows the results for a particular experimental setup. Each row is a topic; the most probable
words are ordered from left to right.
</tableCaption>
<bodyText confidence="0.996470727272727">
railway lighthouse (a) set], K = 10 huddersfield station
rail
school college education history conference
catholic church film music actor
runners team championships match racing
engine company power dwight engines
university london british college county
food novel book series superman
november february april august december
paint photographs american austin black
war history army american battle
</bodyText>
<equation confidence="0.675509">
(b) set2, K = 10
</equation>
<bodyText confidence="0.6281999">
president emperor politician election government
american players swedish team zealand
war world navy road torpedo
system pop microsoft music singer
september 2007 october december 1999
television dog name george film
people malay town tribes cliff
diet chest enzyme hair therapy
british city london english county
school university college church center
</bodyText>
<equation confidence="0.814036">
(c) set], K = 15
</equation>
<bodyText confidence="0.986625733333333">
australia knee british israel set
catholic roman island village columbia
john devon michael austin charles
school university class community district
november february 2007 2009 2005
lighthouse period architects construction design
railway rail huddersfield ownership services
cyprus archdiocese diocese king miss
carson gordon hugo ward whitney
significant application campaign comic considered
born london american england black
war defense history military artillery
actor film actress band designer
york michigan florida north photographs
church catholic county 2001 agricultural
</bodyText>
<equation confidence="0.829589">
(d) set2, K = 15
</equation>
<bodyText confidence="0.999683133333333">
music pop records singer artist
film paintings movie painting art
school university english students british
drama headquarters chess poet stories
family church sea christmas emperor
dog broadcast television bbc breed
champagne regular character characteristic common
election government parliament minister politician
enzyme diet protein hair oxygen
war navy weapons aircraft military
september october december 2008 1967
district town marin america american
car power system device devices
hockey players football therapy champions
california zealand georgia india kolkata
</bodyText>
<figure confidence="0.988969">
log likelihood
−50000
−55000
−60000
−65000
−70000
log likelihood
−45000
−50000
−55000
−60000
−65000
−70000
log likelihood
−50000
−55000
−60000
−65000
−70000
log likelihood
−45000
−50000
−55000
−60000
−65000
−70000
0 10 20 30 40 0 10 20 30 40 0 10 20 30 40 0 10 20 30 40
iteration iteration iteration iteration
(a) set], K = 10 (b) set2, K = 10 (c) set], K = 15 (d) set2, K = 15
</figure>
<figureCaption confidence="0.9230165">
Figure 7: The log-likelihood achieved by LDA as a function of iteration. There is one series for each value of
α E {0.05, 0.1, 0.2, 0.5, 1.0, 2.01 from top to bottom.
</figureCaption>
<page confidence="0.998744">
135
</page>
<tableCaption confidence="0.778417333333333">
Table 2: The five words with the highest probability mass in each topic inferred by LDA, with cr = 0.2. Each subtable
shows the results for a particular experimental setup. Each row is a topic; the most probable words are ordered from left
to right.
</tableCaption>
<table confidence="0.665900904761905">
(a) set], K = 10 (b) set2, K = 10
born 2004 team award sydney
regiment army artillery served scouting
line station main island railway
region street located site knee
food february conference day 2009
pride greek knowledge portland study
catholic church roman black time
class series film actor engine
travel human office management defense
school born war world university
september english edit nord hockey
black hole current england model
training program war election navy
school university district city college
family word international road japan
publication time day india bridge
born pop world released march
won video microsoft project hungary
film hair bank national town
people name french therapy artist
</table>
<equation confidence="0.595975">
(c) set], K = 15 (d) set2, K = 15
</equation>
<bodyText confidence="0.992994133333333">
time michael written experience match
line station railway branch knowledge
film land pass set battle
william florida carson virginia newfoundland
war regiment british army south
reaction terminal copper running complex
born school world college black
food conference flight medium rail
township scouting census square county
travel defense training management edges
series actor engine november award
pride portland band northwest god
team knee 2004 sydney israel
catholic located site region church
class february time public king
family protein enzyme acting oxygen
england producer popular canadian sea
system death artist running car
character series dark main village
english word publication stream day
training program hair students electrical
district town city local kolkata
september edit music records recorded
black pop bank usually hole
people choir road diet related
war built navy british service
center million cut champagne players
born television current drama won
school university college election born
film nord played league hockey
</bodyText>
<figureCaption confidence="0.993995666666667">
Figure 5: The distribution of times taken per HIT. Each se-
ries represents a different experiment. The bulk of the tags
took less than one minute and more than a few seconds.
</figureCaption>
<bodyText confidence="0.999545409090909">
struct a topic model. One way of visualizing a learned
topic model is by examining its topics. Table 1 shows
the topics constructed by human judgments. Each
subtable shows a different experimental setup and
each row shows an individual topic. The five most
frequently occurring words in each topic are shown,
ordered from left to right.
Many of the topics inferred by humans have
straightforward interpretations. For example, the
{november, february, april, august,
december} topic for the set] corpus with K = 10
is simply a collection of months. Similar topics
(with years and months combined) can be found
in the other experimental configurations. Other
topics also cover specific semantic domains,
such as {president, emperor, politican,
election, government} or {music, pop,
records, singer, artist}. Several of the
topics are combinations of distinct concepts,
such as {catholic, church, film, music,
actor}, which is often indicative of the number of
clusters, K, being too low.
</bodyText>
<figure confidence="0.9969136">
0 500 1000 1500 2000
log(seconds between tags)
−2
12
10
8
6
4
2
0
</figure>
<page confidence="0.995552">
136
</page>
<bodyText confidence="0.999840489361702">
Table 2 shows the topics learned by LDA un-
der the same experimental conditions, with the
Dirichlet hyperparameter α = 0.2 (we justify
this choice in the following section). These
topics are more difficult to interpret than the
ones created by humans. Some topics seem
to largely make sense except for some anoma-
lous words, such as {district, town, city,
local, kolkata} or {school, university,
college, election, born}. But the small
amount of data means that it is difficult for a model
which does not leverage prior knowledge to infer
meaningful topic. In contrast, several humans, even
working independently, can leverage prior knowledge
to construct meaningful topics with little data.
There is another qualitative difference between
the topics found by the tag-and-cluster task and
LDA. Whereas LDA must rely on co-occurrence,
humans can use ontological information. Thus, a
topic which has ontological meaning, such as a list
of months, may rarely be discovered by LDA since
the co-occurrence patterns of months do not form a
strong pattern. But users in every experimental con-
figuration constructed this topic, suggesting that the
users were consistently leveraging information that
would not be available to LDA, even with a larger
corpus.
Hyperparameter values A persistent question
among practitioners of topic models is how to set or
learn the value of the hyperparameter α. α is a Dirich-
let parameter which acts as a control on sparsity —
smaller values of α lead to sparser document-topic
distributions. By comparing the sparsity patterns of
human judgments to those of LDA for different set-
tings of α, we can infer the value of α that would
best match human judgments.
Figure 6 shows a boxplot comparison of the en-
tropy of draws from a Dirichlet distribution (the gen-
erative process in LDA), versus the observed en-
tropy of the models learned by humans. The first
six columns show the distributions for the Dirichlet
draws for various values of α; the last two columns
show the observed entropy distributions on the two
corpora, set] and set2.
The empirical entropy distributions across the cor-
pora are comparable to those of a Dirichlet distri-
bution with α between approximately 0.2 and 0.5.
</bodyText>
<tableCaption confidence="0.772639">
Table 3: The five words with the highest probability mass
</tableCaption>
<bodyText confidence="0.926329895833334">
in each topic inferred by LDA on set] with α = 0.2,
K = 10, and initialized using human judgments. Each
row is a topic; the most probable words are ordered from
left to right.
line station lighthouse local main
school history greek knowledge university
catholic church city roman york
team club 2004 scouting career
engine knee series medium reaction
located south site land region
food film conference north little
february class born august 2009
pride portland time northwest june
war regiment army civil black
These settings of α are slightly higher than, but still in
line with a common rule-of-thumb of α = 1/K. Fig-
ure 7 shows the log-likelihood, a measure of model
fit, achieved by LDA for each value of α. Higher
log-likelihoods indicate better fits. Commensurate
with the rule of thumb, using log-likelihoods to se-
lect α would encourage values smaller than human
judgments.
However, while the entropy of the Dirichlet draws
increases significantly when the number of clusters
is increased from K = 10 to K = 15, the entropies
assigned by humans does not vary as dramatically.
This suggests that for any given document, humans
are likely to pull words from only a small number of
topics, regardless of how many topics are available,
whereas a model will continue to spread probability
mass across all topics even as the number of topics
increases.
Improving LDA using human judgments The re-
sults of the previous sections suggests that human be-
havior differs from that of LDA, and that humans con-
ceptualize documents in ways LDA does not. This
motivates using the human judgments to augment the
information available to LDA. To do so, we initialize
the topic assignments used by LDA’s Gibbs sampler
to those made by humans. We then run the LDA
sampler till convergence. This provides a method to
weakly incorporate human knowledge into the model.
Table 3 shows the topics inferred by LDA when
initialized with human judgments. These topics re-
semble those directly inferred by humans, although
as we predicted in the previous sections, the topic
consisting of months has largely disappeared. Other
semantically coherent topics, such as {located,
</bodyText>
<page confidence="0.956546">
137
</page>
<figure confidence="0.988944714285714">
0 10 20 30 40
log likelihood
−50000
−55000
−60000
−65000
−70000
</figure>
<bodyText confidence="0.999695285714286">
may motivate, as future work, the construction of dif-
ferent modeling assumptions which lead to sampling
equations which more closely match the empirically
observed sampling performed by humans. In effect,
our method constructs a series of samples from the
posterior, a gold standard which future topic models
can aim to emulate.
</bodyText>
<sectionHeader confidence="0.998943" genericHeader="method">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.84579425">
The author would like to thank Eytan Bakshy and
Professor Jordan Boyd-Graber-Ying for their helpful
comments and discussions.
iteration
</bodyText>
<figureCaption confidence="0.986667">
Figure 8: The log likelihood achieved by LDA on set1
</figureCaption>
<bodyText confidence="0.847047866666667">
with α = 0.2, K = 10, and initialized using human
judgments (blue). The red line shows the log likelihood
without incorporating human judgments. LDA with hu-
man judgments dominates LDA without human judgments
and helps the model converge more quickly.
south, site, land, region}, have appeared
in its place.
Figure 8 shows the log-likelihood course of LDA
when initialized by human judgments (blue), versus
LDA without human judgments (red). Adding hu-
man judgments strictly helps the model converge to
a higher likelihood and converge more quickly. In
short, incorporating human judgments shows promise
at improving both the interpretability and conver-
gence of LDA.
</bodyText>
<sectionHeader confidence="0.999893" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999991588235294">
We presented a new method for constructing topic
models using human judgments. Our approach re-
lies on a novel task, tag-and-cluster, which asks
users to simultaneously annotate a document with
one of its words and to cluster those annotations. We
demonstrate using experiments on Amazon Mechan-
ical Turk that our method constructs topic models
quickly and robustly. We also show that while our
topic models bear many similarities to traditionally
constructed topic models, our human-learned topic
models have unique features such as fixed sparsity
and a tendency for topics to be constructed around
concepts which models such as LDA typically fail to
find.
We also underscore that the collapsed Gibbs sam-
pling framework is expressive enough to use as the
basis for human-guided topic model inference. This
</bodyText>
<sectionHeader confidence="0.999678" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999915774193548">
David Blei and John Lafferty, 2009. Text Mining: Theory
and Applications, chapter Topic Models. Taylor and
Francis.
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent Dirichlet allocation. JMLR, 3:993–1022.
Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,
Chong Wang, and David M. Blei. 2009. Reading
tea leaves: How humans interpret topic models. In
NIPS.
Scott Deerwester, Susan Dumais, Thomas Landauer,
George Furnas, and Richard Harshman. 1990. Index-
ing by latent semantic analysis. Journal of the Ameri-
can Society of Information Science, 41(6):391–407.
Thomas L. Griffiths and Mark Steyvers. 2002. A prob-
abilistic approach to semantic representation. In Pro-
ceedings of the 24th Annual Conference of the Cogni-
tive Science Society.
T. Griffiths and M. Steyvers. 2006. Probabilistic topic
models. In T. Landauer, D. McNamara, S. Dennis, and
W. Kintsch, editors, Latent Semantic Analysis: A Road
to Meaning. Laurence Erlbaum.
Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai. 2007.
Automatic labeling of multinomial topic models. In
KDD.
R. Snow, Brendan O’Connor, D. Jurafsky, and A. Ng.
2008. Cheap and fast—but is it good? Evaluating
non-expert annotations for natural language tasks. In
EMNLP.
Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov,
and David Mimno. 2009. Evaluation methods for topic
models. In ICML.
</reference>
<page confidence="0.997346">
138
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.677423">
<title confidence="0.989423">Not-So-Latent Dirichlet Collapsed Gibbs Sampling Using Human Judgments</title>
<author confidence="0.988909">Jonathan</author>
<address confidence="0.8276395">1601 S. California Palo Alto, CA</address>
<email confidence="0.999596">jonchang@facebook.com</email>
<abstract confidence="0.999487894736842">Probabilistic topic models are a popular tool for the unsupervised analysis of text, providing both a predictive model of future text and a latent topic representation of the corpus. Recent studies have found that while there are suggestive connections between topic models and the way humans interpret data, these two often disagree. In this paper, we explore this disagreement from the perspective of the learning process rather than the output. We present a novel which asks subjects to simultaneously annotate documents and cluster those annotations. We use these annotations as a novel approach for constructing a topic model, grounded in human interpretations of documents. We demonstrate that these topic models have features which distinguish them from traditional topic models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>John Lafferty</author>
</authors>
<title>Text Mining: Theory and Applications, chapter Topic Models. Taylor and Francis.</title>
<date>2009</date>
<contexts>
<context position="1200" citStr="Blei and Lafferty, 2009" startWordPosition="173" endWordPosition="176">ctive of the learning process rather than the output. We present a novel task, tag-and-cluster, which asks subjects to simultaneously annotate documents and cluster those annotations. We use these annotations as a novel approach for constructing a topic model, grounded in human interpretations of documents. We demonstrate that these topic models have features which distinguish them from traditional topic models. 1 Introduction Probabilistic topic models have become popular tools for the unsupervised analysis of large document collections (Deerwester et al., 1990; Griffiths and Steyvers, 2002; Blei and Lafferty, 2009). These models posit a set of latent topics, multinomial distributions over words, and assume that each document can be described as a mixture of these topics. With algorithms for fast approximate posterior inference, we can use topic models to discover both the topics and an assignment of topics to documents from a collection of documents. (See Figure 1.) These modeling assumptions are useful in the sense that, empirically, they lead to good models of documents (Wallach et al., 2009). However, recent work has explored how these assumptions correspond to humans’ understanding of language (Chan</context>
</contexts>
<marker>Blei, Lafferty, 2009</marker>
<rawString>David Blei and John Lafferty, 2009. Text Mining: Theory and Applications, chapter Topic Models. Taylor and Francis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Andrew Ng</author>
<author>Michael Jordan</author>
</authors>
<date>2003</date>
<booktitle>Latent Dirichlet allocation. JMLR,</booktitle>
<pages>3--993</pages>
<contexts>
<context position="3723" citStr="Blei et al., 2003" startWordPosition="585" endWordPosition="588">s task, and show that humans are indeed able to construct a topic model in this fashion, and that the learned topic model has semantic properties distinct from existing topic models. We also demonstrate that the judgments can be used to guide computer-learned topic models towards models which are more concordant with human intuitions. 2 Topic models and inference Topic models posit that each document is expressed as a mixture of topics. These topic proportions are drawn once per document, and the topics are shared across the corpus. In this paper we focus on Latent Dirichlet allocation (LDA) (Blei et al., 2003) a topic model which treats each document’s topic assignment as a multinomial random variable drawn from a symmetric Dirichlet prior. LDA, when applied to a collection of documents, will build a latent space: a collection of topics for the corpus and a collection of topic proportions for each of its documents. Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 131–138, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics TOPIC 2 &amp;quot;BUSINESS&amp;quot; TOPIC 1 &amp;quot;TECHNOLOGY&amp;quot; Forget the Bootleg, Just Download the</context>
<context position="6541" citStr="Blei et al., 2003" startWordPosition="1039" endWordPosition="1042">ion (LDA). Plates denote replication. The shaded circle denotes an observed variable an d unshaded circles denote hidden variables. the topic assignments zd,n, can be expressed as p(z|α,η, w) a Yd where nd,k denotes the number of words in document p(zd,i = , distributions and document-topic distributions, and The only observed variables of the model are the words, wd,n. The remaining variables must be learned. There are several techniques for performing posterior inference, i.e., inferring the distribution over hidden variables given a collection of documents, including variational inference (Blei et al., 2003) and Gibbs sampling (Griffiths and Steyvers, 2006). In the sequel, we focus on the latter approach. Collapsed Gibbs sampling for LDA treats the topicword and document-topic distributions, and as nuisance variables to be marginali α η. θd βk, zed out. The posterior distribution over the remaining latent variables, � � Γ(nd,k + αk)Γ(ηwd,n + nwd,n,k) Γ(P w nw,k + ηw) d assigned to topic k an d nw,k the number of times word w is assigned to topic k. This leads to the sampling equations, k|α, η, w, zy) a -d,i �d,i wd,i + nw , ,k ( ) (nd k + ) ,d i 1 nw,k + η d i αk Pw ηw where the superscript ted, </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David Blei, Andrew Ng, and Michael Jordan. 2003. Latent Dirichlet allocation. JMLR, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Chang</author>
<author>Jordan Boyd-Graber</author>
<author>Sean Gerrish</author>
<author>Chong Wang</author>
<author>David M Blei</author>
</authors>
<title>Reading tea leaves: How humans interpret topic models.</title>
<date>2009</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="1814" citStr="Chang et al., 2009" startWordPosition="274" endWordPosition="277">009). These models posit a set of latent topics, multinomial distributions over words, and assume that each document can be described as a mixture of these topics. With algorithms for fast approximate posterior inference, we can use topic models to discover both the topics and an assignment of topics to documents from a collection of documents. (See Figure 1.) These modeling assumptions are useful in the sense that, empirically, they lead to good models of documents (Wallach et al., 2009). However, recent work has explored how these assumptions correspond to humans’ understanding of language (Chang et al., 2009; Griffiths and Steyvers, 2006; Mei et al., 2007). Focusing on the latent space, i.e., the inferred mappings between topics and words and between documents and topics, this work has discovered that although there are some suggestive correspondences 131 between human semantics and topic models, they are often discordant. In this paper we build on this work to further explore how humans relate to topic models. But whereas previous work has focused on the results of topic models, here we focus on the process by which these models are learned. Topic models lend themselves to sequential procedures </context>
</contexts>
<marker>Chang, Boyd-Graber, Gerrish, Wang, Blei, 2009</marker>
<rawString>Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish, Chong Wang, and David M. Blei. 2009. Reading tea leaves: How humans interpret topic models. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan Dumais</author>
<author>Thomas Landauer</author>
<author>George Furnas</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society of Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="1144" citStr="Deerwester et al., 1990" startWordPosition="165" endWordPosition="168">his paper, we explore this disagreement from the perspective of the learning process rather than the output. We present a novel task, tag-and-cluster, which asks subjects to simultaneously annotate documents and cluster those annotations. We use these annotations as a novel approach for constructing a topic model, grounded in human interpretations of documents. We demonstrate that these topic models have features which distinguish them from traditional topic models. 1 Introduction Probabilistic topic models have become popular tools for the unsupervised analysis of large document collections (Deerwester et al., 1990; Griffiths and Steyvers, 2002; Blei and Lafferty, 2009). These models posit a set of latent topics, multinomial distributions over words, and assume that each document can be described as a mixture of these topics. With algorithms for fast approximate posterior inference, we can use topic models to discover both the topics and an assignment of topics to documents from a collection of documents. (See Figure 1.) These modeling assumptions are useful in the sense that, empirically, they lead to good models of documents (Wallach et al., 2009). However, recent work has explored how these assumptio</context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Furnas, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan Dumais, Thomas Landauer, George Furnas, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society of Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>A probabilistic approach to semantic representation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 24th Annual Conference of the Cognitive Science Society.</booktitle>
<contexts>
<context position="1174" citStr="Griffiths and Steyvers, 2002" startWordPosition="169" endWordPosition="172">s disagreement from the perspective of the learning process rather than the output. We present a novel task, tag-and-cluster, which asks subjects to simultaneously annotate documents and cluster those annotations. We use these annotations as a novel approach for constructing a topic model, grounded in human interpretations of documents. We demonstrate that these topic models have features which distinguish them from traditional topic models. 1 Introduction Probabilistic topic models have become popular tools for the unsupervised analysis of large document collections (Deerwester et al., 1990; Griffiths and Steyvers, 2002; Blei and Lafferty, 2009). These models posit a set of latent topics, multinomial distributions over words, and assume that each document can be described as a mixture of these topics. With algorithms for fast approximate posterior inference, we can use topic models to discover both the topics and an assignment of topics to documents from a collection of documents. (See Figure 1.) These modeling assumptions are useful in the sense that, empirically, they lead to good models of documents (Wallach et al., 2009). However, recent work has explored how these assumptions correspond to humans’ under</context>
</contexts>
<marker>Griffiths, Steyvers, 2002</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2002. A probabilistic approach to semantic representation. In Proceedings of the 24th Annual Conference of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Griffiths</author>
<author>M Steyvers</author>
</authors>
<title>Probabilistic topic models.</title>
<date>2006</date>
<editor>In T. Landauer, D. McNamara, S. Dennis, and W. Kintsch, editors,</editor>
<contexts>
<context position="1844" citStr="Griffiths and Steyvers, 2006" startWordPosition="278" endWordPosition="281">osit a set of latent topics, multinomial distributions over words, and assume that each document can be described as a mixture of these topics. With algorithms for fast approximate posterior inference, we can use topic models to discover both the topics and an assignment of topics to documents from a collection of documents. (See Figure 1.) These modeling assumptions are useful in the sense that, empirically, they lead to good models of documents (Wallach et al., 2009). However, recent work has explored how these assumptions correspond to humans’ understanding of language (Chang et al., 2009; Griffiths and Steyvers, 2006; Mei et al., 2007). Focusing on the latent space, i.e., the inferred mappings between topics and words and between documents and topics, this work has discovered that although there are some suggestive correspondences 131 between human semantics and topic models, they are often discordant. In this paper we build on this work to further explore how humans relate to topic models. But whereas previous work has focused on the results of topic models, here we focus on the process by which these models are learned. Topic models lend themselves to sequential procedures through which the latent space</context>
<context position="6591" citStr="Griffiths and Steyvers, 2006" startWordPosition="1046" endWordPosition="1049">he shaded circle denotes an observed variable an d unshaded circles denote hidden variables. the topic assignments zd,n, can be expressed as p(z|α,η, w) a Yd where nd,k denotes the number of words in document p(zd,i = , distributions and document-topic distributions, and The only observed variables of the model are the words, wd,n. The remaining variables must be learned. There are several techniques for performing posterior inference, i.e., inferring the distribution over hidden variables given a collection of documents, including variational inference (Blei et al., 2003) and Gibbs sampling (Griffiths and Steyvers, 2006). In the sequel, we focus on the latter approach. Collapsed Gibbs sampling for LDA treats the topicword and document-topic distributions, and as nuisance variables to be marginali α η. θd βk, zed out. The posterior distribution over the remaining latent variables, � � Γ(nd,k + αk)Γ(ηwd,n + nwd,n,k) Γ(P w nw,k + ηw) d assigned to topic k an d nw,k the number of times word w is assigned to topic k. This leads to the sampling equations, k|α, η, w, zy) a -d,i �d,i wd,i + nw , ,k ( ) (nd k + ) ,d i 1 nw,k + η d i αk Pw ηw where the superscript ted, i indicates that these statisY k 132 tics should e</context>
</contexts>
<marker>Griffiths, Steyvers, 2006</marker>
<rawString>T. Griffiths and M. Steyvers. 2006. Probabilistic topic models. In T. Landauer, D. McNamara, S. Dennis, and W. Kintsch, editors, Latent Semantic Analysis: A Road to Meaning. Laurence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>Xuehua Shen</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Automatic labeling of multinomial topic models.</title>
<date>2007</date>
<booktitle>In KDD.</booktitle>
<contexts>
<context position="1863" citStr="Mei et al., 2007" startWordPosition="282" endWordPosition="285">ultinomial distributions over words, and assume that each document can be described as a mixture of these topics. With algorithms for fast approximate posterior inference, we can use topic models to discover both the topics and an assignment of topics to documents from a collection of documents. (See Figure 1.) These modeling assumptions are useful in the sense that, empirically, they lead to good models of documents (Wallach et al., 2009). However, recent work has explored how these assumptions correspond to humans’ understanding of language (Chang et al., 2009; Griffiths and Steyvers, 2006; Mei et al., 2007). Focusing on the latent space, i.e., the inferred mappings between topics and words and between documents and topics, this work has discovered that although there are some suggestive correspondences 131 between human semantics and topic models, they are often discordant. In this paper we build on this work to further explore how humans relate to topic models. But whereas previous work has focused on the results of topic models, here we focus on the process by which these models are learned. Topic models lend themselves to sequential procedures through which the latent space is inferred; these</context>
</contexts>
<marker>Mei, Shen, Zhai, 2007</marker>
<rawString>Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai. 2007. Automatic labeling of multinomial topic models. In KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>Brendan O’Connor</author>
<author>D Jurafsky</author>
<author>A Ng</author>
</authors>
<title>Cheap and fast—but is it good? Evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>R. Snow, Brendan O’Connor, D. Jurafsky, and A. Ng. 2008. Cheap and fast—but is it good? Evaluating non-expert annotations for natural language tasks. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna M Wallach</author>
<author>Iain Murray</author>
<author>Ruslan Salakhutdinov</author>
<author>David Mimno</author>
</authors>
<title>Evaluation methods for topic models.</title>
<date>2009</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="1689" citStr="Wallach et al., 2009" startWordPosition="254" endWordPosition="257">unsupervised analysis of large document collections (Deerwester et al., 1990; Griffiths and Steyvers, 2002; Blei and Lafferty, 2009). These models posit a set of latent topics, multinomial distributions over words, and assume that each document can be described as a mixture of these topics. With algorithms for fast approximate posterior inference, we can use topic models to discover both the topics and an assignment of topics to documents from a collection of documents. (See Figure 1.) These modeling assumptions are useful in the sense that, empirically, they lead to good models of documents (Wallach et al., 2009). However, recent work has explored how these assumptions correspond to humans’ understanding of language (Chang et al., 2009; Griffiths and Steyvers, 2006; Mei et al., 2007). Focusing on the latent space, i.e., the inferred mappings between topics and words and between documents and topics, this work has discovered that although there are some suggestive correspondences 131 between human semantics and topic models, they are often discordant. In this paper we build on this work to further explore how humans relate to topic models. But whereas previous work has focused on the results of topic m</context>
</contexts>
<marker>Wallach, Murray, Salakhutdinov, Mimno, 2009</marker>
<rawString>Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov, and David Mimno. 2009. Evaluation methods for topic models. In ICML.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>